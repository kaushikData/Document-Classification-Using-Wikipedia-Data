{"id": "54215490", "url": "https://en.wikipedia.org/wiki?curid=54215490", "title": "ACM SIGARCH", "text": "ACM SIGARCH\n\nACM SIGARCH is the Association for Computing Machinery's Special Interest Group on computer architecture, a community of computer professionals and students from academia and industry involved in research and professional practice related to computer architecture and design. The organization sponsors many prestigious international conferences in this area, including the International Symposium on Computer Architecture (ISCA), recognized as the top conference in this area since 1975. Together with IEEE Computer Society's Technical Committee on Computer Architecture (TCCA), it is one of the two main professional organizations for people working in computer architecture.\n\nACM SIGARCH was formed in August 1971, initially as a Special Interest Committee (a precursor to a SIG), with Michael J. Flynn as the founding chairman. Flynn was also the founding chairman of IEEE Computer Society's TCCA and encouraged from the beginning, joint cooperation between the two groups. Many of the joint symposiums and conferences are the leading events in the field.\n\n\"ACM SIGARCH Computer Architecture News\" is a newsletter, started in January 1972, that publishes refereed articles about computer hardware and its interactions with compilers and operating systems.\n\nACM SIGARCH sponsors many top international conferences related to computer architecture.\n\nSIGARCH offers a variety of awards for outstanding contributions to computer architecture:\n\n\n"}
{"id": "9436002", "url": "https://en.wikipedia.org/wiki?curid=9436002", "title": "Ann Rockley", "text": "Ann Rockley\n\nAnn Rockley is one of Canada's foremost experts in organizing and presenting information online. She is the founder and President of The Rockley Group, based in the greater Toronto Area. She regularly presents papers and workshops on subjects involving the efficient creation, management and delivery of content for organizations in North America and Europe. \nShe was the lead analyst for \"The XML & Component Content Management Report\" on Content Management Systems Watch.\n\nAt university, Ann Rockley took a Bachelor of Science degree in Astronomy. In the early 1980s, she got her first permanent job as a junior technical writer at I. P. Sharp Associates. She went on to work for Cemcorp, Unisys, and American Express, then formed Information Design Solutions with two partners, Heather Fawcett and Sam Ferdinand. According to Gerlinde Schuller, information design is a complex, interdisciplinary, and experimental art. The partnership consulted in usability, document analysis, SGML, and large-scale online documentation projects.\n\nIn 1995, Rockley left Information Design Solutions to start The Rockley Group. She took a master's degree in information science at the University of Toronto while continuing to work at The Rockley Group full-time.\n\nIn the mid-1980s, Rockley took the initiative to revive the Toronto chapter of the Society for Technical Communication, a professional organization for technical communicators, by getting in touch with the international organization's representative for her area, Rennie Charles. The chapter was founded in 1959 but had been dormant for several years. She and Michelle Hutchinson, another technical communicator, assembled a group of their colleagues and presented a plan to hold regular meetings, find speakers of interest, and develop services for technical communicators. The chapter has been active ever since. Among other things, Rockley served as chapter president, produced the newsletter, and was general manager for a very successful three-day, multi-stream conference in 1989. (Hutchinson served as chapter president, produced the newsletter, and organized the hosting of the international society's annual conference in 1997.)\n\nWhen Toronto hosted the international conference, Rockley proposed producing the conference proceedings on CD as well as in book form. She also volunteered to produce the CDs for the 2500 attendees. That was the first time it had been done. The machine-readable format proved to be so popular that it has been provided in one form or another ever since.\n\nRockley regularly presents papers and workshops at the annual meeting of the international Society for Technical Communication. She was named an Associate Fellow of the STC for her contributions to the profession, and in 2005 became a Fellow, the Society's highest honor.\n\nAnn Rockley helped to develop the Information Design certificate program at the University of Toronto. She has taught courses in the program (information design and enterprise content management).\n\nRockley is the lead author of a 2002 book, \"Managing Enterprise Content: A Unified Content Strategy\", co-authored with Pamela Kostur and Steve Manning, who both worked with Ann at The Rockley Group. It has become a standard reference manual for content management. Her methodology includes return on investment calculations, which can justify the content management effort to executives.\n\nShe is also the lead author of a 2009 book,\"DITA 101: Fundamentals of DITA for Authors and Managers\", co-authored with Charles Cooper and Steve Manning. It is a beginner's guide to understanding the Darwin Information Typing Architecture (DITA), an XML-based architecture for authoring, producing, and delivering information.\n\nRockley's 2015 book, \"Intelligent Content: A Primer\", was co-authored with Charles Cooper and Scott Abel. It is a beginner's guide to creating intelligent content—defined as content which is not limited to one purpose, technology or output—and overcoming the challenges to its adoption. Intelligent Content was published by XML Press.\n\nFacing challenges such as putting online 10,000+ pages of documentation for a nuclear power plant, Rockley has been an innovator in devising ways to handle large quantities of online information. She has progressed from information design and online documentation through single sourcing to content management for entire enterprises. She pioneered content reuse for a unified content strategy and incorporated the use of XML for implementing that strategy.\n\nAs her interests became more specialized, Rockley went on to help found another professional organization, the Content Management Professionals (CM Pros), in 2004. She served as the president in 2005.\n\nAnn Rockley is a member of the Organization for the Advancement of Structured Information Standards (OASIS). It is a \"not-for-profit consortium that drives the development, convergence and adoption of open standards for the global information society\". She is the Co-Chair of the DITA for Enterprise Business Documents Subcommittee\n\n"}
{"id": "1811772", "url": "https://en.wikipedia.org/wiki?curid=1811772", "title": "Atoms for Peace", "text": "Atoms for Peace\n\n\"Atoms for Peace\" was the title of a speech delivered by U.S. President Dwight D. Eisenhower to the UN General Assembly in New York City on December 8, 1953.\nThe United States then launched an \"Atoms for Peace\" program that supplied equipment and information to schools, hospitals, and research institutions within the U.S. and throughout the world. The first nuclear reactors in Iran, Israel and Pakistan were built under the program by American Machine and Foundry, a company more commonly known as a major manufacturer of bowling equipment.\n\nThe speech was part of a carefully orchestrated media campaign, called \"Operation Candor\", to enlighten the American public on the risks and hopes of a nuclear future. It was a propaganda component of the Cold War strategy of containment. Eisenhower's speech opened a media campaign that would last for years and that aimed at \"emotion management,\" balancing fears of continuing nuclear armament with promises of peaceful use of uranium in future nuclear reactors. The speech was a tipping point for international focus on peaceful uses of atomic energy, even during the early stages of the Cold War. Eisenhower, with some influence from J. Robert Oppenheimer, may have been attempting to convey a spirit of comfort to a terrified world after the horror of Hiroshima and Nagasaki and of the nuclear tests of the early 1950s.\n\nIt presented an ostensible antithesis to brinkmanship, the international intrigue that subsequently kept the world at the edge of war.\nHowever, recent historians have tended to see the speech as a cold war maneuver directed primarily at U.S. allies in Europe. Eisenhower wanted to make sure that the European allies would go along with the shift in NATO strategy from an emphasis on conventional weapons to cheaper nuclear weapons. Western Europeans wanted reassurance that the U.S. did not intend to provoke a nuclear war in Europe, and the speech was designed primarily to create that sense of reassurance. Eisenhower later said that he knew the Soviets would reject the specific proposal he offered in the speech.\n\nEisenhower's invoking of \"those same great concepts of universal peace and human dignity which are so clearly etched in\" the UN Charter placed new emphasis upon the U.S. responsibility for its nuclear actions—past, present and future. This address laid down the rules of engagement for the new kind of warfare: the cold war.\n\nTwo quotations from the speech follow:\n\nPrior to Eisenhower's speech, the state of atomic development in the world was under strict secrecy. The information and expertise needed for atomic development was bound by the secret Quebec Agreement of 1943 and thus not devoted to peaceful processes, but instead as a weapon to defend against other countries which were developing and using the same weaponry. With atomic development thus far under wraps, there were no safety protocols and no standards developed.\n\nEisenhower's speech was an important moment in political history as it brought the atomic issue which had been kept quiet for \"national security\" into the public eye, asking the world to support his solution. Eisenhower was determined to solve \"the fearful atomic dilemma\" by finding some way by which \"the miraculous inventiveness of man would not be dedicated to his death, but consecrated to his life.\" Unfortunately, Eisenhower was not completely effective in his repurposing; Eisenhower himself approved the National Security Council (NSC) document which stated that only a massive atomic weapon base would deter violence from the Soviet Union. The belief that to avoid a nuclear war, the United States must stay on the offensive, ready to strike at any time, is the same reason that the Soviet Union would not give up its atomic weapons either. During Eisenhower's time in office the nuclear holdings of the US rose from 1,005 to 20,000 weapons.\n\nThe \"Atoms for Peace\" program opened up nuclear research to civilians and countries that had not previously possessed nuclear technology. Eisenhower argued for a nonproliferation agreement throughout the world and argued for a stop of the spread of military use of nuclear weapons. Although the nations that already had atomic weapons kept their weapons and grew their supplies, very few other countries have developed similar weapons—in this sense, it has been very much contained. The \"Atoms for Peace\" program also created regulations for the use of nuclear power and through these regulations stopped other countries from developing weapons while allowing the technology to be used for positive means.\n\n\"Atoms for Peace\" created the ideological background for the creation of the International Atomic Energy Agency and the Treaty on the Non-Proliferation of Nuclear Weapons, but also gave political cover for the U.S. nuclear weapons build-up, and the backdrop to the Cold War arms race. Under programs related to \"Atoms for Peace,\" the U.S. exported over 25 tons of highly enriched uranium (HEU) to 30 countries, mostly to fuel research reactors, and is now regarded as a proliferation and terrorism risk. Under a similar program, the Soviet Union (now Russia and some countries which are separated from it ) exported over 11 tons of HEU.\n\n\n\n"}
{"id": "14003239", "url": "https://en.wikipedia.org/wiki?curid=14003239", "title": "Better Place (company)", "text": "Better Place (company)\n\nBetter Place was a venture-backed international company that developed and sold battery-charging and battery-switching services for electric cars. It was formally based in Palo Alto, California, but the bulk of its planning and operations were steered from Israel, where both its founder Shai Agassi and its chief investors resided.\n\nThe company opened its first functional charging station the first week of December 2008 at Cinema City in Pi-Glilot near Tel Aviv, Israel The first customer deliveries of Renault Fluence Z.E. electric cars enabled with battery switching technology began in Israel in the second quarter of 2012, and at peak in mid September 2012, there were 21 operational battery-swap stations open to the public in Israel.\n\nBetter Place filed for bankruptcy in Israel in May 2013. The company's financial difficulties were caused by mismanagement, wasteful efforts to establish toeholds and run pilots in too many countries, the high investment required to develop the charging and swapping infrastructure, and a market penetration far lower than originally predicted by Shai Agassi.\nLess than 1,000 Fluence Z.E. cars were deployed in Israel and around 400 units in Denmark, after spending about million in private capital. After two failed post-bankruptcy acquisition attempts, the bankruptcy receivers sold off the remaining assets in November 2013 to Grngy for only $450,000.\n\nThe company was publicly launched on October 29, 2007, as Project Better Place, by Shai Agassi, the company's founder and CEO at the time. According to Agassi, his vision was inspired by a question asked by Klaus Schwab at the 2005 World Economic Forum in Davos, Switzerland: \"How do you make the world a \"better place\" by 2020?\" As of January 2011 it had raised $700 million, and about a third was spent in setting up the battery switch stations. Also, several countries and states had offered tax breaks.\n\nBetter Place announced deployment of electric vehicle networks in Israel, Denmark and Hawaii in 2008 and 2009. The company planned to deploy the infrastructure on a country-by-country basis, and said it was in talks with more than 25 additional regions around the world. Australia, Ontario, Oregon, and California also announced deployment of Better Place electric car networks.\n\nIn January 2008, Better Place announced a Memorandum of Understanding (MOU) with Renault-Nissan to build the world's first Electric Recharge Grid Operator (ERGO) model for Israel. Under the agreement, Better Place would build the electric recharge grid, and Renault-Nissan would provide the electric vehicles.\n\nIn early October 2012, Agassi resigned from his role as worldwide Better Place CEO, and was replaced by Evan Thornley, CEO of Better Place-Australia. Agassi briefly remained on the company board, but a week later he resigned from that position as well. A few days after Thornley's appointment, Better Place asked its investors for a round of emergency funding, totalling about $100 million. On October 29, 2012, Ynet reported that Better Place would that week lay off 150 to 200 of its 400-person staff in Israel as it sought financing to combat its cash-flow problems.\n\nIn late January 2013, Thornley was fired by Chairman Idan Ofer, and Dan Cohen was named acting CEO by the board. As a consequence of the financial problems, the Australian rollout was put on hold, as the company decided to concentrate on its two existing markets.\nHowever, on 26 May 2013, Better Place filed for bankruptcy in Israel. Following the decision of the Board of Directors of the global company, Better Place Danmark A/S also decided to initiate bankruptcy proceedings on the same date.\n\nThe company's financial difficulties were caused by the high investment required to develop the charging and swapping infrastructure, about million in private capital, and market penetration significantly lower than originally predicted by Shai Agassi, who expected 100,000 cars on Israeli roads by 2010. Less than 1,000 Fluence Z.E. cars were deployed in Israel and around 400 cars in Denmark. Under Better Place's business model, the company owned the Fluence Z.E. batteries, so the court liquidator would have to decide what to do with customers who do not have ownership of the battery and risk being left with a useless car.\n\nIn July 2013, an attempt to acquire Better Place was made by the Sunrise group that comprised entrepreneur Yosef Abramowitz and the Association for the Advancement of Electric Transportation in Israel. Court filings showed that the acquisition would be worth 18 million Israeli shekels () for Better Place’s assets in Israel, and 25 million Israeli shekels () for its intellectual property, held by Better Place Switzerland. The deal was canceled by the court after the Sunrise group failed to make the first agreed payment of 3.52 million shekels (), even after an extension.\n\nIn August 2013, the Central District Court ruled that Better Place Israel would be sold to Success Assets Ltd., owned by Tsahi Merkur, for 11 million shekels (million). To effect the acquisition, Merkur was to sign a personal guarantee within seven days for the full amount of the acquisition, and a personal guarantee for a letter of indemnification covering the guarantees made by Better Place Israel's subsidiaries to the Ministry of Transport. Within 21 days, Merkur was to deposit with the company's special managers an opinion on a property on which a commitment to register a lien of up to 5 million shekels would be placed. By 30 September 2013 he was to make a payment of 2 million shekels. On 17 October 2013 the deal was canceled after Success Assets failed to make the required payments.\n\nIn November 2013, the court-appointed receivers decided to sell the remaining assets of Better Place in parts and liquidate the business.\n\nThe steam car, the internal combustion engine automobile, and the electric car emerged as the main competing technologies in the late 1890s until the 1920s. The concept of exchangeable battery service was first proposed as early as 1896 in order to overcome the limited operating range of electric cars and trucks.\n\nThe concept was first put into practice by Hartford Electric Light Company through the GeVeCo battery service and was initially available for electric trucks. The vehicle owner purchased the vehicle from General Vehicle Company (GeVeCo, a subsidiary of the General Electric Company) without a battery and the electricity was purchased from Hartford Electric through an exchangeable battery. The owner paid a variable per-mile charge and a monthly service fee to cover maintenance and storage of the truck. Both vehicles and batteries were modified to facilitate a fast battery exchange. The service was provided between 1910 and 1924 and during that period vehicles using it covered more than 6 million miles. Beginning in 1917 a similar service was operated in Chicago for owners of Milburn Light Electric cars who also could buy the vehicle without the batteries.\n\nElectric forklifts have used battery swapping since at least 1946 and a rapid battery replacement system was implemented to help maintain 50 electric buses at the 2008 Summer Olympics in China.\n\nBetter Place implemented a business model wherein customers entered into subscriptions to purchase driving distance similar to the mobile telephone industry from which customers contract for minutes of airtime. The initial cost of an electric vehicle might also have been subsidized by the ongoing per-distance revenue contract just as mobile handset purchases are subsidized by per-minute mobile service contracts. Better Place's goal was to enable electric cars to sell for $5,000 less than the price of the average gasoline car sold in the United States, or the impact of electric cars would be minimal. For example, the Prius hybrid had been sold for 13 years at a price of $4,000 more than other gasoline cars and had captured less than 2% of the worldwide car market.\n\nThe Better Place approach was to enable manufacturing and sales of different electric cars separately from their standardized batteries in the same way that petrol cars are sold separately from their fuel. Petrol is not purchased upfront, but is bought a few times a month when the fuel tank needs filling. Similarly, the Better Place monthly payment would cover electric \"fuel\" costs including battery, daily charging and battery swaps. Better Place was to allow customers to pay incrementally for battery costs including electric power, battery life, degradation, warranty problems, maintenance, capital cost, quality, technology advancement and anything else related to the battery. The per-distance fees would cover battery pack leasing, charging and swap infrastructure, purchasing sustainable electricity, profits, and the cost of investor capital. All battery problems would be handled by Better Place which would then bundle the costs and bill their customers monthly for providing all the infrastructure.\n\nThe Better Place electric car charging infrastructure network was based on a smart grid software platform using Intel Atom processors and .NET Framework, or comparable vendors. This platform was first of its kind in the world and was to enable Better Place to manage the charging of hundreds of thousands of electric cars simultaneously by automatically time-shifting recharging away from peak demand hours of the day, preventing overload of the electrical grid of the host country. Better Place would be able to provide electricity for millions of electric cars without adding a single electricity generator or transmission line by using smart software that oversaw and managed the recharging of electric cars connected with Better Place.\n\nBetter Place encouraged governments to mandate the use of international standards and open access to recharge across charging networks to facilitate competing networks. Standardization efforts such as SAE J1772, however, had not yet yielded global consensus . Better Place displayed Charge Spot charging stations that used a connector with the same pin layout as SAE J1772-2009 but housed in a non-standard, triangular plug. They also displayed a wall mounted charging station using IEC 62196 Type 2 receptacle . Battery pack switching outside of Better Place's network was not to be allowed. Better Place said it had pre-sold enough contracts to make its first deployed network in Israel profitable at launch.\n\nAgassi stated that the company's plan was to have the network's electricity generated entirely by renewable energy from solar arrays and wind farms if necessary, thus invalidating the \"long-smokestack\" accusation leveled against electric vehicles which rely on the nonrenewable sources of the electricity. However, achieving the 100% renewable energy goal would have depended on the local electric grid's energy sources.\n\nIn Israel, where the first Better Place deployment took place, the electric grid is based mostly on fossil fuels, rendering the renewable energy vision practically impossible in the short term.\n\nThe first prototype car was the Renault Laguna with a battery instead of a fuel tank and an electric motor instead of an internal combustion engine. The battery for electric vehicles was a Lithium iron phosphate ion device. The range of the car running on just one battery was from about to . By replacing the battery at a battery switch station, the range between longer charging stops was to be limited only by the geographical distribution of the battery-swapping infrastructure.\n\nThe second demo car was the Nissan eRogue, an electric car based on the Renault-Nissan Rogue, halfway between a sedan and an SUV in size.\n\nThe Renault Fluence Z.E. was announced at the Frankfurt Motor Show on September 15, 2009 as the first electric car to be available on the Better Place network using a switchable battery. Shai Agassi said that EVs had to be priced at $5,000 less than the price of the average gasoline car to be successful. In April 2010 Renault announced that sales of the Fluence Z.E. were scheduled for 2011 in Israel, Denmark and the rest of Europe. In August 2010 Better Place announced a non-binding order of 100,000 Renault Fluence ZE and four months later Better Place claimed to have sold 70,000 cars from that order, a year away from the public launch of its network.\n\nThe floor-mounted battery packs in these electric cars were designed to be changed out robotically in less than two minutes, which was quicker than the average petroleum refuel, allowing for battery-swap services like those proposed by Better Place and Tesla Motors. Better Place expected battery packs to cost between US 4¢ and 5¢ per mile over their life, provide the cars with a range per charge, perform for 2000 recharge cycles, and last for 8 years.\n\nWith areas around cities covered with battery switching stations, also called battery-swap stations, drivers would potentially have electric cars with an unlimited driving range for long distance trips. The QuickDrop battery switch system would enable Renault Fluence Z.E.'s battery, the only vehicle deployed in the Better Place network, to be swapped in approximately three minutes at dedicated battery exchange stations. The actual robotic battery-switching operation took about five minutes in the deployed stations. While each exchange station would cost $500,000, The then CEO of Better Place, Shai Agassi, said that cost would be half the price of a typical petroleum fuelling station.\n\nIn order to access the battery switch station, Better Place customers would have to swipe their membership card. The remaining process was fully automated, similar to going through a car wash, so the driver never had to leave the car. In Better Place's demonstration battery switch stations, a robotic arm removed the depleted battery and replaced it with a full one.\n\nDuring 2010, Better Place operated a demonstration battery switch station in Tokyo allowing three specially equipped cabs to exchange their car's depleted battery pack for a fully recharged one in 59.1 seconds on average. Better Place used the same technology to swap batteries that F-16 jet fighter aircraft use to load their bombs.\n\nBetter Place battery switch stations were claimed to support multiple battery types of all kinds of electric cars as long as the battery could be removed from under the car. A battery switch station using only 15 batteries allegedly had the ability to swap batteries for 2,500 EV's. Better Place claimed it had battery station installation teams who could install one battery switch station in just two days, one every 25 miles in every route and at the same cost of 7 days of oil in the United States, Better Place claimed it could cover all of the United States with battery switch stations and all the required infrastructure.\n\nThe main alternative technology to the battery-switching technique promoted by Better Place is DC fast charging. A nationwide fast charging infrastructure is/was being deployed in the United States that by 2013 would cover the entire nation. DC Fast Chargers are going to be installed at 45 BP and ARCO locations and will be made available to the public as early as March 2011.\n\nBetter Place claimed that its subscription model had customers effectively paying only the prorated mileage cost without any battery ownership problems, whereas the fast charging model involves the customer bearing all the battery purchase, ownership, maintenance, and replacement costs, in addition to the cost of the electricity to recharge the battery.\n\nBetter Place claimed that far fewer fast charging and battery switch stations would be needed than the current number of petroleum fuel refill stations, because drivers would usually recharge (\"refuel\") electric cars at home, offices, shopping centers, commercial areas, and the like. Drivers would only need alternatives if they forgot to recharge, couldn't get to a charge spot, had insufficient time at a charge spot, or were driving non-stop in excess of , typical of long distance vacations and business trips.\n\nDC fast charging was at the time considerably slower than Better Place's claimed 59 second battery switchover, but while Better Place battery switch stations would have cost around $500,000 each, DC fast chargers that the EV project is/was to deploy would cost only between $25,000 to $40,000.\n\nOn May 13, 2009, Better Place premiered their battery switching station to the public in Yokohama where they had been invited by the Japanese Ministry of the Environment. The battery switching station demonstrated was set up similarly to a gas station automatic car wash. The vehicle drove up on a ramp and was aligned on the swapping pad. The battery shuttle then engaged and rose up toward the bottom of the vehicle. It made contact with the battery, released it, lowered it, and moved the depleted battery pack away from the car. The charged battery pack was then inserted. The discharged battery was returned to the charging bay. The battery switch was complete in less than two minutes and the vehicle drove away. The battery swap was designed to require less time than filling a tank of gas. In order to keep electric vehicles in demand, Better Place was going to try to keep the vehicles competitive with the other cars on the market. By building infrastructure that made owning an electric car more practical, they hoped to increase demand.\n\nThe first prototype battery switch station opened in Yokohama, Japan on May 14, 2009, was designed by Yoav Heichal, chief engineer for Better Place research and development group.\n\nThe company signed an agreement with Dor Alon Energy to install battery replacement points, which would run alongside the petroleum refueling station' normal business. Dor Alon CEO, Israel Yaniv, said, \"Dor Alon is the first energy company that will enable owners of electric car owners of the future to obtain electric refueling services at its gas stations. We consider this agreement with Better Place to be a strategic partnership that will create real value and innovation for the company's activity.\"\n\nIn April 2010, a 90-day switchable-battery electric taxi demonstration project was launched in Tokyo, using three Nissan Rogue crossover utility vehicles, converted into electric cars with switchable batteries provided by A123 Systems. The battery switch station deployed in Tokyo was more advanced than the Yokohama switch system demonstrated in 2009. During the three-month field test the EV taxis accumulated over and swapped batteries 2,122 times, with an average battery swap time of 59.1 seconds. Nissan decided to continue the trial until late November 2010.\n\nIn October 2010 Better Place announced its commitment to launch a three-year demonstration program with electric-powered taxis in the San Francisco Bay Area, in partnership with the cities of San Francisco and San Jose, California, taxi operators and carsharing programs, regional and state agencies, consumer and EV organizations, and the San Francisco Public Utilities Commission. The program would deploy and operate four battery switching stations in the San Francisco to San Jose corridor to support a fleet of switchable-battery EV taxis. , Better Place had made no further statements of progress on this program.\n\nA battery-powered 10-taxi demonstration project was launched at Schiphol Airport, Amsterdam in 2012.\n\n, the company had raised from various sources including, VantagePoint Venture Partners, Israel Corporation (33% ownership), Israel Cleantech Ventures, HSBC, Morgan Stanley, Acorns to Oaks II, Esarbee Investments Canada, GC Investments LLC, Musea Ventures, Ofer Group, Vyikra Partners, Wolfensohn & Co. and Maniv Energy Capital. In late 2007, Agassi began raising in Series-A funding for the project, one of the largest and fastest seed rounds in history. Investors included VantagePoint Venture Partners, Israel Corporation, Israel Cleantech Ventures, Morgan Stanley, and private investors led by Michael Granoff of Maniv Energy Capital. In 2009, the company raised an additional for Better Place Denmark, including an investment from DONG Energy the leading utility in Denmark. Following the announcement in Israel, Better Place said it had launched its network in Denmark, Australia and in two United States locations - Hawaii and Northern California. The company said it was in talks with more than 25 countries around the world.\n\nIn Australia, Better Place announced agreements with AGL Energy and financial advisor Macquarie Capital Group to raise and begin deploying an electric vehicle (EV) network powered by renewable energy. According to Better Place, its model for sustainable mobility would help Australia move toward oil independence. With the world’s seventh highest per capita rate of car ownership, the country had nearly 15 million cars on the road after adding over a million new cars in 2007.\n\nIn January 2010, as Israel Corporation completed its investment of in the company, a consortium of investors signed a Series-B funding round to invest a further in Better Place, citing their confidence that \"Better Place has the technical and commercial solutions to allow for the mass adoption of electric cars in the near term.\" The Series-B round was led by HSBC, which invested , and included all Series-A investors plus Morgan Stanley Investment Management and Lazard Asset Management. The deal represented one of the largest financial investments of its kind by HSBC, which gained a seat on the Better Place board of directors and approximately 10% of the company's shares.\n\nFrom its early days, doubts were raised as to the effectiveness of Better Place's centralised model of providing charging infrastructure, with some anticipating that the model would not be widely adopted.\n\nIn May 2008, the company presented a prototype of its electric car at a press conference in Tel Aviv. Shai Agassi estimated that the company's partner, the Renault-Nissan alliance, would likely invest $500 million to $1 billion in developing the swappable-battery electric cars.\n\nFurther partnerships with other manufacturers were not announced, and Peter Rawlinson, VP and Chief Engineer for Vehicle Engineering at Tesla was quoted as saying “Different batteries suit different cars. It’s far too simplistic to look at batteries as isolation,”\n\nBetter Place also announced plans to develop electric recharge grids in the city of San Francisco and the state of Hawaii.\n\nAustralian finance group Macquarie said it would work with Better Place to fund the construction of plug-in stations, and Australian utility AGL Energy committed to powering those stations with renewable electricity.\n\nIn March 2008, Deutsche Bank analysts issued a glowing report on the company stating that its approach could be a \"paradigm shift\" that caused \"massive disruption\" to the auto industry, and which had \"the potential to eliminate the gasoline engine altogether.\" Three months later, the same institution issued a second report, finding \"electric vehicles destined for much more growth than is widely perceived\". The same report stated that \"[i]mprovements in battery technology will allow for increased power, increased electrical propulsion, and bigger gains in fuel economy.\"\n\nOn June 26, 2008, Shai Agassi testified before the United States House of Representatives Select Committee on Energy Independence and Global Warming. The hearing, titled \"$4 Gasoline and Fuel Economy: Auto Industry at a Crossroads,\" dealt with the future role of the auto industry and the federal government in fighting gas prices and the fuel economy standards proposed in response to the enactment of the Energy Independence and Security Act (EISA) of 2007.\n\nIn 2009, CBS Money Watch cast doubts on Better Place's business model, noting that it would cost up to $500,000 to construct a battery switching station\n\nIn Australia a roll out of 500 charge stations was planned to begin in the major eastern coast cities before expanding nationally. It was estimated that these would give comparable coverage to the existing 13,000 petrol stations then in operation. The total cost of this roll out was claimed to be between $1 to $1.25 billion AUD.\n\nThe first charge spot was installed in Canberra in late 2011, but in January 2013, after fewer than 20 public charge spots had been installed the rollout was halted and the board of Better Place decided to concentrate on its two existing markets, Israel and Denmark.\n\nBetter Place was also to be the preferred provider of home and dealership charging stations for the Holden Volt, with the partnership announced in July 2012.\n\nIn 2011, Better Place announced an agreement with China Southern Power Grid Company, the world’s eighth-largest utility company. Before the end of the year, Better Place was going to open a battery switch station and joint education center in the southern city of Guangzhou. Shai Agassi said that China Southern Grid was embracing battery switch as the primary means of range extension. China Southern Grid Chairman Zhao Jianguo said that the battery-switch model might become mainstream in China and that the joint visitor center and battery switch demonstration project with Better Place would help promote electric-car adoption in China by allowing potential customers to experience this innovative solution.\n\nChina Southern Power Grid pilot projects and other joint activities were supposed to explore the benefits that switchable-battery electric cars and the networked infrastructure that supports them might deliver to the electric grid in CSG's service area, which spanned five provinces, one million square kilometers, and 230 million people in Southern China.\n\nElectric utility State Grid Corporation of China planned to build over 2,351 electric-charging and battery-swap stations by 2016. These would have had 220,000 charging poles, but they did not indicate how many, if any, of them would have been battery-swap stations. The director of the State Grid smart grid research center commented \"The construction of a large-scale charging station costs 20 to 30 million yuan ($3.05-4.57 million) and a small-scale one costs less than 10 million yuan, but it costs more than 100 million yuan to build a battery-swap station.\"\n\nIn April 2010, Better Place signed a memorandum of understanding with Chery Automobile, China's biggest independent car maker, to develop prototypes for electric vehicles to be used in regional state-sponsored pilot projects.\n\nBetter Place partnered with Denmark's leading energy company; Dong Energy, in a €103 million Euro (770 million Danish Kroner) investment to introduce electric cars and infrastructure to Denmark. The country currently generates 20% of its electric power from wind energy, but much of it is exported because there is currently no way for utilities to store the excess power. Using the Better Place model, Dong hoped to take advantage of the existing electric grid and electric vehicle batteries to harness and store the abundance of wind-generated power, and distribute it appropriately for transportation consumption.\n\nThe Renault Fluence Z.E. sold in Denmark at a price of 205,000 DKK (€27,496 or ) including VAT plus the monthly fee for the switchable-battery. Consumers paid a one-time fee of 9,995 DKK (€1,341) for a private charging station and customers were offered a choice of five fixed-price switchable-battery packages based on kilometers driven per year. For more than a year the monthly fee was 2,995 DKK () per month. The network commercial launch was in late 2011.\n\nThe first battery switch station in Denmark, out of 20 which were planned to be deployed across the country as part of the network of charging infrastructure, was unveiled in June 2011 at in Gladsaxe, near Copenhagen. Sales of the Fluence Z.E. began in late 2011, and\n198 units were sold in Denmark through December 2012. Cumulative sales through April 2013 reached 234 units.\n\nAs of December 2012 there were 17 fully operational battery switch stations in the country, enabling Danish customers to drive anywhere across the country in an electric car. On 26 May 2013, and following the decision of the Board of Directors of Better Place's global company, Better Place Danmark A/S decided to begin bankruptcy proceedings. Because the batteries are owned by Better Place, Renault announced it would honor the existing agreement to around 500 customers that bought their electric cars through Better Place.\n\nBetter Place deployed about 80 charging stations and 154 charge points in Oahu, Maui, Kauai and the Big Island, and had almost 700 customers. The operation of the charging stations were acquired in March 2013 by OpConnect. No interruption of service was expected due to Better Place bankruptcy.\n\nIsrael was the first nation in the world to partner with Better Place to build an electric car infrastructure. Shai Agassi, former Better Place CEO, claimed that by 2016, plus or minus a year, more than 50% of cars sold in Israel would be electric.\n\nBattery switch stations were supposedly opening to customers almost weekly in 2012. That map indicates with \"orange circles\" the handful of battery switch stations available in June 2012, and shows with \"grey circles\" the full buildout of battery switch stations expected by year's end. The Baran Group signed an agreement with Better Place stating its intention to build 51 battery switch stations over the course of 2011 to cover all of Israel. However progress was not nearly as rapid as was planned.\n\nAccording to the Financial Times around 400 corporations in Israel signed letters of intent to begin switching their fleets to Better Place electric car network as soon as the service becomes available. This represented a potential of 80,000 electric cars. Out of the 100,000 Renault Fluence Z.E. that Better Place agreed to buy from Renault, the company claimed to have already signed around 70,000 orders, most of them from commercial fleet customers.\n\nBetter Place launched its first battery-swapping station in Israel, in Kiryat Ekron, near Rehovot in March 2011. The station was supposed to be the first of approximately 40 stations to begin operating in the near term. The battery exchange process took five minutes. The company also erected over 1,000 functional charging spots for the cars and thousands more were supposed to be put in place by the end of 2011, according to the CEO of Better Place Israel.\n\nOrders for the Renault Fluence ZE in Israel began in July 2011. According to Better Place, their customers regular maintenance costs would be about 40% less than for regular family cars, insurance also will be less at around NIS 3,700 a year, and its comprehensive solution of electric car and services would cut annual vehicle maintenance costs by 20%.\n\nThe first deliveries of the Renault Fluence Z.E. took place on the 22nd of January 2012 and around 100 electric cars were allocated to Better Place's employees. Better Place planned a staged delivery process as the infrastructure across the country was completed. Retail customer deliveries began in the second quarter of 2012. As of mid September 2012, there were 21 operational battery-swap stations open to the public in Israel. Cumulative sales through July 2012 reached 300 cars, and as of the end of October, just 490 cars had been sold, making the company's target of 4,000 customers by June 2013 a difficult goal to achieve. In October 2012, Better Place signed a deal with Elco to supply 125 cars worth NIS 15m. The 125 Renault Fluence ZEs were to be delivered through 2012 and 2013. , a total of 518 cars were sold in the country. In the first four months of 2013, a total 422 cars were sold, bringing the total to 940. Alan Gelman, chief financial officer said in January 2013 that the company had turned a corner in recent weeks with large sales to fleets and that the days of not selling cars were over Nevertheless, on 26 May 2013 the company filed for bankruptcy in Israel.\nBetter Place wanted electric car customers who could make a successful transition from range anxiety to ordinary range awareness within the company's growing infrastructure. During the sales process, Better Place aimed to educate and assess each customer's electric car suitability. The company was going to exclude drivers frequently traveling irregular routes that span the country. Eventually, Israel was supposed to have enough battery switch stations and recharging spots at parking garages, shopping centers, hotels, commercial areas, and elsewhere, to cater for most drivers.\n\nAt minimum, customers needed dedicated off-road parking at home and, for higher mileage drivers, Better Place was going to install charging spots at people's workplace. The preferred high-mileage customer might commute each way between home and office. Lower distance customers might only need their home charging spot, with battery switching being infrequent.\n\nElectric cars aren't allowed by law to directly plug into ordinary Israeli electrical outlets. Better Place charging stations were to have smart grid interactivity that automatically time shifted the charging process away from peak electrical demand hours. Most critics claimed it was an attempt to monopolize the charging of car batteries, and had in fact the reverse effect, and had discouraged many potential customers in Israel from buying Better Place's cars.\n\nEdmunds.com selected battery-charging infrastructure developers Coulomb Technologies and Better Place as recipients of its first annual Green Car Breakthrough Award. In 2010, Shai Agassi was included at number 28 in a list of the 100 Top Global Thinkers published by Foreign Policy magazine, for his efforts to make electric cars a mass-market success.\n\nA number of companies have announced plans to install charging station networks. In France, Électricité de France (EDF) and Toyota announced plans to provide recharging points for PHEVs on roads, streets and parking lots. EDF also announced a partnership with Elektromotive, Ltd. to install 250 new charging points over six months from October 2007 in London and elsewhere in the UK. Coulomb Technologies was aiming to deploy its ChargePoint charging station network throughout the USA.\n\nIn March 2009, Tesla Motors announced a partnership to deploy battery swap stations among their existing \"Supercharger\" network to service their Model S platform cars. Tesla abandoned battery swapping citing low demand.\n\nThe Nation-E's Angel Car system is a portable unit, containing a lithium-ion battery, that stores energy and is used as an emergency charger for electric cars that run out of power. It designed to be a solution to \"range anxiety\" without the deployment of significant new infrastructure, and provides fast-charging services for all known Evs equipped with a fast-charging socket, including hybrid cars.\n\n\nhttps://web.archive.org/web/20140714222625/http://www.globes.co.il/en/article-1000875072\n\nhttp://www.ynetnews.com/articles/0,7340,L-4403479,00.html\n\n"}
{"id": "52865540", "url": "https://en.wikipedia.org/wiki?curid=52865540", "title": "Bioremediation of PCBs", "text": "Bioremediation of PCBs\n\nPCB, polychorinated biphenyls, are a type of chemical that was widely used in the 1960s and 1970s, and which is a contamination source of soil and water. They are fairly stable and therefore persistent in the environment. Bioremediation of PCBs is the use of microorganisms to degrade PCBs from contaminated sites, which can be either a soil or aqueous environment. It is a process relying on multiple microorganisms' co-metabolism. Anaerobic microorganisms dechlorinate PCBs first, and other microorganisms that are capable of doing BH pathway can break down the dechlorinated PCBs to usable intermediates like acyl-CoA or carbon dioxide. If no BH pathway-caple microorganisms are present, dechlorinated PCBs can be mineralized with help of fungi and plants. However, there are multiple limiting factors for this co-metabolism.\n\nPolychlorinated biphenyls (PCBs) are various biphenyl based artificial products that are widely used as a dielectric fluid, industrial coolant, and lubricants in the 1960s and 1970s. There is no evidence its synthesis occurs naturally. They are classified as persistent organic pollutants. PCBs share the basic chemical structure of biphenyl and one or more of the hydrogen atoms on the aromatic rings are replaced by chlorine atoms.\n\nPCBs is in viscous liquid form at normal temperature and has a poor solubility in water. The aromatic hydrocarbon structure gives PCBs relatively high molecular stability. The chlorine substitution further reinforces its insolubility and chemical stability. Hence, the degradation of PCBs in the natural environment is very slow, which can range from 3 to 37 years depending on the number of chloride substitutions and their positions.\n\nBioremediation is a waste removal method that uses microorganisms to degrade or remove wastes like organic waste and heavy metal from contaminated sites including both soil and water. The advantages of bioremediation are that it is environment-friendly, inexpensive and can remove multiple wastes simultaneously comparing with traditional chemical and physical processes.\n\nVarious microorganisms are involved in a two-stage process of degradation of PCBs, which happens in aerobic and anaerobic environments. Degrading PCBs is similar to the degradation of biphenyl. However, the chlorines on PCBs prevent them from being utilized as a substrate of biphenyl degradation. Due to high chemical stability, PCBs cannot be used as energy sources. However, due to the chlorination, PCBs can be used as electron acceptors in anaerobic respiration to store energy, which is also the first stage of the degradation pathway, reductive dechlorination. Once the PCBs are dechlorinated to a certain degree, usually lower than five chlorines presenting in the structure and one aromatic ring has no chlorine, they can undergo the biphenyl degradation pathway (BP pathway) to be degraded to accessible carbon or CO in the aerobic environment. BP pathway is a pathway that utilizes series of enzymes (BphA, B, C, D, E, F, G) to convert biphenyl to TCA cycle intermediates (pyruvate and Acyl-CoA) and benzoate. However, there are few microorganisms that can dechlorinate substrate under natural conditions. Even with selective media, the accumulation of PCB dechlorinating microorganisms is still slow, which is one reason for the slow degradation rate. As a result, PCBs usually go through a co-metabolism pathway that involving different microorganism species.\n\nGenerally speaking, there are four steps in this process:\nThe Figure below shows the complete degradation pathway. \n\nPCBs have low water solubility, so they adhere tightly to soil and cannot be easily accessed by bacteria. Especially, if the contaminations site has been exposed to PCBs for long period, PCBs can be integrated into soil or sediment matrices, then further decrease their bioavailability. Some surfactants can help solubilize but cannot increase the rate of PCBs degrading. However, if PCBs are linked to surfactants tightly too, then this process cannot promote the absorption of PCBs and even lower it. Also, many surfactants have been proven to be toxic to cells and the high cost of surfactants is another issues.\n\nPCBs are toxic to bacterial communities above 1000 mg/kg. However, if the concentration is too low (lower than 50 mg/kg), the degradation slows down significantly, for there is not enough material to stimulate the expression of required genes and support the growth of competent microorganisms. PCBs includes various different compounds with slightly different structures. Those slight differences make big differences in metabolic rate. Generally speaking, the more chlorines in a PCB molecule, the harder for it to be degraded. In particular, microorganisms cannot degrade di- and tetra-ortho substituted well. It is possible that those structures prevent enzymes from accessing reaction sites.\n\nFirst, the soil and sediment structures will determine how tightly PCBs are adhered to them and affect the absorption of PCBs into cells. The PCBs’ availability suffers from increasing organic carbon and clay content, for they promote the absorption of PCBs into soil or sediment matrices. Second, the soil contains necessary nutrition for the growth of microbes and anaerobic and aerobic environments. Finally, the local microbial population has significant impacts on the rate of degradation of PCBs, which varies based on the microbial strains and their activities. Then, if there is no history of exposure to PCBs, it may take months for microbes to activates their ability to dechlorinate PCBs and break them down.\n\nPCBs or biphenyl cannot provide energy for microbes, so they are primary energy and carbon sources. As stated before it takes months sometimes for microorganisms to activate their gene for dichlorine after the first exposure to PCBs. It has been proposed to use analogs to promote the activation of genes. However, even after the metabolic pathway is activated, the intermediates of the pathway create a bottleneck effect due to their toxicity. Also, there is the possibility that BP pathway leads to protoanemonin which is a dead-end metabolite that cannot be utilized by cells. Due to the high energy cost of this pathway, if no preferred energy source present in the system, cells will not activate this pathway.\n"}
{"id": "43109870", "url": "https://en.wikipedia.org/wiki?curid=43109870", "title": "Carbon tissue", "text": "Carbon tissue\n\nCarbon tissue is a gelatin-based emulsion used as a photoresist in the chemical etching (photoengraving) of gravure cylinders for printing. This was introduced by British physicist and chemist Joseph Swan in 1864. It has been used in photographic reproduction since the early days of photography.\n\nCarbon materials marketing began in 1866 by Joseph Swan which he subsequently sold to the Autotype Company in 1868. His ready-made tissues were in three colours black, sepia and purple-brown. This method was used in Europe and USA throughout the 19th century and well into the 20th. This market was almost closed in the 1950s although some companies produce small amount of carbon tissue and transfer papers for monochrome and three-color work until around 1990\n\nThe gelatinous emulsion is applied to a paper backing, and is rendered sensitive to light when immersed in a 3:4% solution of potassium bichromate. After drying, it is ready for use. The carbon tissue is first exposed to a film positive. In those areas where the carbon tissue has received the most light (i.e., non-image areas and highlights) the emulsion becomes thick and hard, and the thickness and hardness decreases with decreasing exposure to the light source, the emulsion being thinnest and softest in image areas corresponding to shadows and solids. After developing the carbon tissue, it is adhered to the surface of the copper-plated cylinder.\n\nA solution of ferric chloride etchant is applied to the surface of the cylinder, where it eats away the copper through the carbon tissue. In the highly exposed areas, where the carbon tissue photoresist is thickest and hardest, the etchant takes a long time to eat through the hard emulsion, while in the least exposed, thinnest regions the etchant eats through the resist into the copper very quickly. Thus, in a given period of etching, the cells etched into the copper will be deepest (and thus will print the darkest) in those regions where the etchant has eaten through the quickest, while the cells etched into the copper will be the shallowest (and thus print the lightest) in those regions where the etchant has eaten through the slowest.\n\nCarbon tissue resists were the first chemical etching media, but have been replaced by photopolymers, and chemical etching as a whole is being increasingly replaced by electromechanical engraving and computer-to-cylinder laser-cutting processes. (See Gravure Engraving.)\n\nCarbon tissue resists have also been used extensively for the manufacture of photostencils in screen printing.\n\n"}
{"id": "183330", "url": "https://en.wikipedia.org/wiki?curid=183330", "title": "Chassis", "text": "Chassis\n\nA chassis (, ; plural \"chassis\" ) is the internal framework of an artificial object, which supports the object in its construction and use. An example of a chassis is a vehicle frame, the underpart of a motor vehicle, on which the body is mounted; if the running gear such as wheels and transmission, and sometimes even the driver's seat, are included, then the assembly is described as a rolling chassis.\n\nIn the case of vehicles, the term rolling chassis means the frame plus the \"running gear\" like engine, transmission, drive shaft, differential, and suspension.\n\nAn under body (sometimes referred to as \"coachwork\"), which is usually not necessary for integrity of the structure, is built on the chassis to complete the vehicle.\n\nFor commercial vehicles, a rolling chassis consists of an assembly of all the essential parts of a truck (without the body) to be ready for operation on the road. The design of a pleasure car chassis will be different than one for commercial vehicles because of the heavier loads and constant work use. Commercial vehicle manufacturers sell \"chassis only\", \"cowl and chassis\", as well as \"chassis cab\" versions that can be outfitted with specialized bodies. These include motor homes, fire engines, ambulances, box trucks, etc.\n\nIn particular applications, such as school buses, a government agency like National Highway Traffic Safety Administration (NHTSA) in the U.S. defines the design standards of chassis and body conversions.\n\nAn armoured fighting vehicle's hull serves as the chassis and comprises the bottom part of the AFV that includes the tracks, engine, driver's seat, and crew compartment. This describes the lower hull, although common usage might include the upper hull to mean the AFV without the turret. The hull serves as a basis for platforms on tanks, armoured personnel carriers, combat engineering vehicles, etc.\n\nIn an electronic device, the chassis consists of a frame or other internal supporting structure on which the circuit boards and other electronics are mounted.\n\nIn some designs, such as older sets, the chassis is mounted inside a heavy, rigid cabinet, while in other designs such as modern computer cases, lightweight covers or panels are attached to the chassis.\n\nThe combination of chassis and outer covering is sometimes called an enclosure.\n\nIn firearms, the chassis is a bedding frame on long guns such as rifles to replace the traditionally wooden stock, for the purpose of better accurizing the gun. The chassis is usually made from metallic material such as aluminium alloy due to the more superior stiffness and compressive strength of metals compared with wood or synthetic polymer, which are commonly used in conventional rifle stocks. The chassis essentially functions as a more extensive pillar bedding, providing a metal-on-metal bearing surface for the that has reduced potential shifting under the stress of recoil. A barreled action bedded into a metal chassis would theoretically operate more consistently during repeated firing, resulting in better precision.\n\n"}
{"id": "22628044", "url": "https://en.wikipedia.org/wiki?curid=22628044", "title": "Display pixel interface", "text": "Display pixel interface\n\nThe Display pixel interface (DPI) is the interface defined by the Mobile Industry Processor Interface (MIPI), which is used for Active-Matrix LCD displays for handheld devices. It is intended for the display modules in the mobile devices. \n"}
{"id": "1212755", "url": "https://en.wikipedia.org/wiki?curid=1212755", "title": "Edward Goldsmith", "text": "Edward Goldsmith\n\nEdward René David Goldsmith (8 November 1928 – 21 August 2009), widely known as Teddy Goldsmith, was an Anglo-French environmentalist, writer and philosopher.\n\nHe was a member the prominent Goldsmith family. The eldest son of Major Frank Goldsmith, and elder brother of the financier James Goldsmith. Edward Goldsmith was the founding editor and publisher of \"The Ecologist\". Known for his outspoken views opposing industrial society and economic development, he expressed a strong sympathy for the ways and values of traditional peoples.\n\nHe co-authored the influential \"A Blueprint for Survival\" with Robert Allen, becoming a founding member of the political party \"People\" (later renamed the Green Party), itself largely inspired by the\" Blueprint\". Goldsmith's conservative view of environmentalism put him at odds with the socialist currents of thought, which came to dominate the Green Party.\n\nA deep ecologist and systems theorist, Goldsmith was an early proponent of the Gaia hypothesis, having previously developed a similar cybernetic concept of a self-regulating biosphere.\n\nA talented after-dinner speaker and raconteur, Goldsmith was an articulate spokesman and campaigner, receiving a number of awards for his work protecting the natural world and highlighting the importance and plight of indigenous peoples, including an honorary Right Livelihood Award and the Chevalier de la Legion d'Honneur.\n\nGoldsmith (widely known as Teddy) was born in Paris in 1928 to a German Jewish father, Frank Goldsmith, and French mother, Marcelle Mouiller.\n\nHe entered Millfield School, Somerset, as a grammar student, and he later graduated with honours in Philosophy, Politics, and Economics at Magdalen College, Oxford (1947–1950). While studying at Oxford, Goldsmith rejected the reductionist and compartmentalised ideas taught at the time, and he sought a more holistic worldview with which to study societies and the problems facing the world at large.\n\nAfter fulfilling his National Service as a British Intelligence Officer in Hamburg and Berlin, he involved himself unsuccessfully in a number of business ventures and devoted most of his spare time to the study of the subjects that were to preoccupy him for the rest of his life.\n\nThroughout the 1960s, he spent time travelling the world with his close friend, John Aspinall, witnessing at first hand the destruction of traditional societies. He concluded that the spread of economic development and its accompanying industrialisation, far from being progressive as claimed, was actually the root cause of social and environmental destruction.\n\nIn London, at meetings of the Primitive People's Fund (the committee that founded Survival International), Goldsmith teamed up with the fund's treasurer Robert Prescott-Allen, the explorer Jean Liedloff, and a writer from \"World Medicine\", Peter Bunyard, to found \"The Ecologist\" in 1969.\n\nAfter rejecting what he saw as the excessively reductionist and compartmentalised approach of mainstream academia, he spent much of his time researching and developing his own theories for the unification of the sciences. The \"Theory of a Unified Science\" was heavily influenced by cybernetics, as well as the General Systems Theory of Ludwig von Bertalanffy, the holism of the early academic ecologists, and the functionalism employed by many anthropologists. His theory would later be published, in its final form, as \"The Way: an ecological world view.\" (see below)\n\nEarly on, Goldsmith had formulated a concept of the biosphere as an integrated cybernetic entity, the self-regulating parts (of which he included tribal societies) co-operating, largely unconsciously, for the mutual benefit of the whole, a view that anticipated aspects of the Gaia thesis, of which he was to become a leading proponent.\n\nGoldsmith was also a critic of neo-Darwinism. He claimed that it is a reductionist theory and that if you understand evolution, it is necessary to \"abandon the reductionistic and mechanistic paradigm of science\".\n\nHaving established \"The Ecologist\" in 1969 with founding editors Robert Allen, Jean Liedloff, and Peter Bunyard, Goldsmith was to use the journal as a platform for his theoretical concerns with regular articles appearing under the heading \"Towards a Unified Science\". The journal also became an important forum for the early green movement, with articles focusing on the relevance and survival of hunter-gatherer societies, alternative technology and organic farming, together with prescient articles about climate change, resource depletion, and nuclear accidents. They were accompanied by the usual gamut of articles examining pollution, overpopulation, deforestation, soil erosion, corporate power, large dams, and, not least, the World Bank's alleged role in \"financing the destruction of our planet\".\n\nSigned by over thirty of the leading scientists of the day—l, including Sir Julian Huxley, Sir Frank Fraser Darling, Sir Peter Medawar, Sir Peter Scott, and C. H. Waddington, Goldsmith and his fellow editor Robert Allen made headlines in January 1972 with \"A Blueprint for Survival\".\n\nThe \"Blueprint\" was a far reaching proposal for a radical transition to a largely decentralised and deindustrialised society, an attempt to prevent what the authors referred to as \" the breakdown of society and the irreversible disruption of the life-support systems on this planet\". It became a key text for the early Green movement, selling over half a million copies, and it was translated into 16 different languages. In many ways, it anticipated the concerns taken up by today's Transition Movement.\n\nGoldsmith and Allen argued that rather than devise imaginary utopias, as did Marxist and liberal political theorists of the time, they should instead look to the example of existing tribal peoples, who, the authors claimed, were real-life working models of societies perfectly adapted to both their long-term survival needs and the needs of the living world on which they depended. The tribal peoples alone, the authors argued, had demonstrated a viable means by which the most pressing problems facing humanity could be answered successfully.\n\nSuch societies were characterised by their small, human-scale communities, low-impact technologies, successful population controls, sustainable resource management, holistic and ecologically integrated worldviews and a high degree of social cohesion, physical health, psychological well-being and spiritual fulfilment of their members.\n\nThe \"Blueprint\" was a major inspiration for the embryonic political party called \"People\" (later to become the Green Party,) which invited Goldsmith to stand for the Eye constituency in Suffolk as their candidate in the February 1974 general election.\n\nThe campaign focused on the threat of desertification from the intensive farming practised in the area, which Goldsmith emphasised with the help of a Bactrian camel supplied by Aspinall. Goldsmith was in turn accompanied by bearded supporters dressed in the garb of Arab sheiks, the implication being that if modern oil-intensive farming practises were allowed to continue, the camel would be the only viable means of transport left in Suffolk. Goldsmith lost his deposit, but his unorthodox campaign succeeded in attracting the media's attention and highlighted the issues. He again stood for the now-renamed Ecology Party at the European elections in 1979, now winning a more respectable portion of the vote.\n\nIn 1973, buoyed by the success of the \"Blueprint\" and a sudden rise in public awareness of ecological issues, partly brought about by the Stockholm Conference and the publication of the Club of Rome's \"The Limits to Growth\" in the same year. Goldsmith and his editorial team moved from their offices in London to relocate to rural Cornwall, in the far west of England. Goldsmith and his colleagues bought themselves farms, and for the following 17 years, they attempted to form a small-scale, relatively self-sufficient community of their own, and \"The Ecologist\" continued to be produced on-site, in between their other chores.\n\nIn 1977, when the Central Electricity Generating Board (CEGB) threatened to site a nuclear reactor on farmland in Luxulyan, Cornwall, Goldsmith was among those who organised a continuous sit-in of the land, with local people blocking the entrance and staffing round-the-clock garrisons to prevent CEGB contractors from starting their drilling work. An early example of an environmental protest camp, the High Court of England and Wales eventually awarded in favour of CEGB allowing the drilling to go ahead. The CEGB never went on to develop the site, however.\n\nIn 1974, Goldsmith spent four months with the Gandhi Peace Foundation in New Delhi, comparing the Gandhian (\"Sarvodaya\") movement with the Ecology movement in Europe. This led Goldsmith to forge close links with Indian environmental activists, in particular with the Chipko movement, including Sunderlal Bahuguna and Vandana Shiva. That was to have a major influence on Goldsmith's approach to environmental activism and led to a special issue of \"The Ecologist\" on the subject.\n\nIn 1984, together with his colleague Nicholas Hildyard, Goldsmith authored a multi-volume report on the destructive effects of large-scale, hydroelectric dams. It was the beginning of a long attack against the International Monetary Fund and World Bank, which Goldsmith and his colleagues accused of financing the destruction of the planet.\n\nIn one episode, Goldsmith wrote an open letter to the then President of the World Bank, Alden W. Clausen, demanding that the bank \"stop financing the destruction of the tropical world, the devastation of its remaining forests, the extermination of its wildlife and the impoverishment and starvation of its human inhabitants\". At the time, the connection between large-scale development projects and social and environmental destruction had not been widely recognised, even within the environmental movement.\n\nIn 1989, Goldsmith helped to organise an international campaign calling for an immediate end to the destruction of the world's remaining forests with its detrimental effects on indigenous cultures, biodiversity and global climate. The campaign raised over 3 million signatures, which were taken in wheelbarrows to the UN's headquarters in New York City. Goldsmith and a party of activists subsequently occupied the main lobby, refusing to move until the Secretary General, Perez de Cuellar, agreed to see them. The group demanded for him to call an extraordinary general meeting of the Security Council to tackle the global crisis of deforestation. Although failing, the campaign managed to organise a meeting in the US Senate with a group of senators, headed by Al Gore, whom the activists called upon to end their support of the World Bank.\n\nIn 1991, with the financial support of his brother James, Goldsmith established the Goldsmith (JMG) Foundation supporting a diverse range of non-governmental organisations campaigning against environmentally destructive activities, along with organisations providing sustainable alternatives.\n\nIn 1990, urged on by Arne Næss, Goldsmith left the editorship of \"The Ecologist\" to Nicholas Hildyard, while taking time off to write his philosophical magnum opus \"The Way: an ecological worldview\". \"The Way\" (1992) was the culmination and synthesis of more than four decades of theoretical development, embodying a \"coherent worldview\" by which Goldsmith would attempt to explain the self-inflicted problems facing the world and to propose a way out of them. Much of the work was already mature in Goldsmith's mind by the time that he published the first issues of \"The Ecologist\" in 1970.\n\nIn addition to the UK \"Ecologist\", Goldsmith later helped to found and support \"The Ecologist\" as independent enterprises in many parts of the world:\n\nHe continued to attend key meetings around the world and involved himself with a variety of campaign organisations by becoming President of the \"Climate Initiatives Fund\", Richmond, London; a board member of the \"International Forum on Globalization\", San Francisco, USA; a founder member of \"Marunui Conservation Ltd.\", Mangawhai, New Zealand (1987); and a founder member and vice-president of ECOROPA, a European ecological club and think tank (1975).\n\n\nIn 1997, after an acrimonious split with his editorial team, most notably with his former friend and colleague Nicholas Hildyard, Goldsmith was left to run \"The Ecologist\" on his own. Having been absent for some years, he brought in the International Society for Ecology and Culture (ISEC) to act as the editorial team. His nephew Zac, who was then working for ISEC, eventually took over the editorship on their behalf.\n\nThe split with Hildyard led to a period of often-bitter criticism from some members of the political left in the environmental movement, which, compounded with failing health, resulted in a period of isolation from the British scene.\n\nGoldsmith was accused of having affiliated himself with the Nouvelle Droite, an intellectual voice of the European \"New Right\", after addressing a symposium on Green issues organised in Paris by the GRECE (Research and Study Group on European Culture), a school of political thought founded largely on the works of Alain de Benoist. It was the attending of that and another similar event that had led to rising tensions with his colleague Nicholas Hildyard. The title of Goldsmith's contribution in Paris being simply \"Une société écologique: la seule alternative\" (\"An Ecological Society: The Only Alternative\").\n\nLater, in a controversial article for the \"Guardian\" newspaper, entitled \"Black Shirts in Green Trousers\", George Monbiot (a cofounder of the left-wing political party Respect) accused Goldsmith of having \"advocated the enforced separation of Tutsis and Hutus in Rwanda and Protestants and Catholics in Ulster, on the grounds that they constitute 'distinct ethnic groups' and are thus culturally incapable of co-habitation\" (a point rejected by Goldsmith). That, along with other attacks, eventually led Goldsmith to counter his critics with his indepth rebuttal \"My Answer\".\n\nGoldsmith's close association with his brother, Sir James Goldsmith, his lifelong friendship with the controversial casino owner and conservationist John Aspinall, along with his anti-modernist stance and support for indigenous peoples, ensured that Goldsmith had many detractors throughout his life. Still, Goldsmith received affectionate support and respect from across the full spectrum of the environmental movement and from many of the people whose views and preoccupations were the focus of his theoretical and philosophical critique.\n\nGoldsmith's message continued to be sponsored around the world, in particular through his work with the International Forum on Globalization (IFG), and, regardless of their previous acrimony, Hildyard and Goldsmith went on to restore their former friendship.\n\n\nWith his first wife, Gillian Marion Pretty (later wife of \"Comte\" Jean-Baptiste de Monpezat, brother of Prince Consort Henrik of Denmark), he had two daughters and a son:\n\nWith his second wife, Katherine Victoria James, he had two sons.\n\nHe had a brother, James Goldsmith, through whom he is the uncle of Zac Goldsmith, Jemima Khan and Ben Goldsmith.\n\n\n\n\n\n\n\n"}
{"id": "890943", "url": "https://en.wikipedia.org/wiki?curid=890943", "title": "Elitegroup Computer Systems", "text": "Elitegroup Computer Systems\n\nElitegroup Computer Systems Co., Ltd. (ECS;) is a Taiwan-based electronics firm. It is the fifth largest PC motherboard manufacturer in the world (after Asus, Gigabyte Technology, ASRock, and MSI), with production reaching 24 million units in 2002.\n\nThe company has since concentrated on broadening its product range. After ECS's purchase of laptop manufacturer Uniwill in 2006, the company has been involved in the design and manufacture of laptops, desktop replacement computers and multimedia products. ECS computers are sold by Fry's Electronics under the “Great Quality” (“GQ”) brand.\n\nWhile Elitegroup Computer Systems is headquartered in Taiwan, the company has production facilities all over the world:\n\n\nDesign of the products is probably done in Taiwan.\n\nMany of these motherboards have been produced for OEM customers and are used in systems assembled and sold by such brand-name companies as IBM, Compaq and Zoostorm. Its main competitors are Micro-Star International and ASRock.\n\nECS also produces Acer computers.\n\nFounded in 1987, ECS is headquartered in Taiwan with operations in North America, Europe and the Pacific Rim. The company merged with PCChips (Hsing Tech Enterprise Co., Ltd), a major manufacturer of low-cost motherboards, in 2005. In June 2003, ECS was selected for two years in a row for \"Business Week\" magazine’s exclusive Information Technology 100 list.\n\nOn 10 April 2013, ECS unveiled \"Durathon\", where all ECS motherboards are tested beyond industry standards for durability, stability, and reliability, as well as materials used in the manufacture of its motherboards. The name \"Durathon\" derives from the words durable and marathon, which refers to each motherboard's rigorous testing methods in extreme environmental conditions.\n\n\n"}
{"id": "610000", "url": "https://en.wikipedia.org/wiki?curid=610000", "title": "European Organisation for the Exploitation of Meteorological Satellites", "text": "European Organisation for the Exploitation of Meteorological Satellites\n\nThe European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT) is an intergovernmental organisation created through an international convention agreed by a current total of 30 European Member States: Austria, Belgium, Bulgaria, Croatia, the Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Ireland, Iceland, Italy, Latvia, Lithuania, Luxembourg, the Netherlands, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, Switzerland, Turkey, and the United Kingdom. These States fund the EUMETSAT programs and are the principal users of the systems. The convention establishing EUMETSAT was opened for signature in 1983 and entered into force on 19 June 1986.\n\nEUMETSAT's primary objective is to establish, maintain and exploit European systems of operational meteorological satellites. EUMETSAT is responsible for the launch and operation of the satellites and for delivering satellite data to end-users as well as contributing to the operational monitoring of climate and the detection of global climate changes.\n\nThe activities of EUMETSAT contribute to a global meteorological satellite observing system coordinated with other space-faring nations.\n\nSatellite observations are an essential input to numerical weather prediction systems and also assist the human forecaster in the diagnosis of potentially hazardous weather developments. Of growing importance is the capacity of weather satellites to gather long-term measurements from space in support of climate change studies.\n\nEUMETSAT is not part of the European Union, but became a signatory to the International Charter on Space and Major Disasters in 2012, thus providing for the global charitable use of its space assets.\n\nThe national mandatory contributions of member states are proportional to their gross national income. However, the cooperating countries contribute only half of the fee they would pay for full membership.\n\nTwo generations of active Meteosat geostationary satellites, Meteosat Transition Programme (MTP) and Meteosat Second Generation (MSG), provide images of the full Earth disc, and data for weather forecasts. More detail can be found on the Meteosat wiki.\n\nWhile geostationary satellites provide a continuous view of the earth disc from a stationary position in space, the instruments on polar-orbiting satellites, flying at a much lower altitude, provide more precise details about atmospheric temperature and moisture profiles, although with less frequent global coverage.\n\nThe lack of observational coverage in certain parts of the globe, particularly the Pacific Ocean and continents of the southern hemisphere, has led to the increasingly important role for polar-orbiting satellite data in numerical weather prediction and climate monitoring.\n\nEUMETSAT Polar System (EPS) Metop mission consists of three polar orbiting Metop satellites, to be flown successively for more than 14 years. The first, Metop-A, was launched by a Russian Soyuz-2.1a rocket from Baikonur on October 19, 2006, at 22:28 Baikonur time (16:28 UTC). Metop-A was initially controlled by ESOC for the LEOP phase immediately following launch, with control handed over to EUMETSAT 72 hours after lift-off. EUMETSAT's first commands to the satellite were sent at 14:04 UTC on October 22, 2006.\n\nThe second EPS satellite, Metop-B, was launched from Baikonur on 17 September 2012, with the third, Metop-C, scheduled for launch in November 2018.\n\nPositioned at approximately 817 km above the Earth, special instruments on board Metop-A can deliver far more precise details about atmospheric temperature and moisture profiles than a geostationary satellite.\n\nThe satellites also ensure that the more remote regions of the globe, particularly in Northern Europe as well as the oceans in the Southern hemisphere, are fully covered.\n\nThe EPS programme is also the European half of a joint program with NOAA, called the International Joint Polar System. NOAA has operated a continuous series of low earth orbiting meteorological satellite since April 1960. Many of the instruments on Metop are also operated on NOAA/POES satellites, providing similar data types across the IJPS.\n\n\nThe Jason-2 programme is an international partnership across multiple organisations, including EUMETSAT, CNES, and the US agencies NASA and NOAA.\n\nJason-2 was launched successfully from Vandenberg Air Force Base aboard a Delta-II rocket on 20 June 2008, 7:46 UTC. \n\nJason-2 reliably delivers detailed oceanographic data vital to our understanding of weather forecasting and climate change monitoring. Jason-2 provides data on the decadal (10-yearly) oscillations in large ocean basins, such as the Atlantic Ocean; mesoscale variability, and surface wind and wave conditions. Jason-2 measurements contribute to the European Centre for Medium-Range Weather Forecasts (ECMWF) satellite data assimilation, helping improve global atmosphere and ocean forecasting.\n\nAltimetric data from Jason-2 have also helped create detailed decade-long global observations and analyses of the El Niño and La Niña phenomena, opening the way to new discoveries about ocean circulation and its effects on climate, and providing new insights into ocean tides, turbulent ocean eddies and marine gravity.\n\nThe next step is the Jason-3 Programme, which has been approved. It will ensure continuation of the series of measurements made by the Jason-2 satellite, and its predecessors, in support of meteorology, operational oceanography and, in particular, the monitoring of the sea-level trend, a key indicator of climate change.\n\n\n"}
{"id": "33072106", "url": "https://en.wikipedia.org/wiki?curid=33072106", "title": "Federated Millers and Manufacturing Grocers Union", "text": "Federated Millers and Manufacturing Grocers Union\n\nFederated Millers and Manufacturing Grocers Union was an Australian trade union. It represented workers in food processing and manufacturing industries.\n\nThe Federated Millers and Manufacturing Grocers Union was formed through the amalgamation of two pre-existing unions, the Federated Millers and Mill Employees' Association of Australasia and the Manufacturing Grocers' Employees' Federation of Australia. Both unions represented relatively small memberships, and amalgamated as part of a period of union rationalisation encouraged by the ACTU.\n\nThe new union did not last long as a separate entity, and in 1992 it amalgamated with the recently formed National Union of Workers. The new union was formed from several pre-existing organisations, the largest and most influential being the Federated Storemen and Packers Union.\n"}
{"id": "42529963", "url": "https://en.wikipedia.org/wiki?curid=42529963", "title": "Flexicar (carsharing)", "text": "Flexicar (carsharing)\n\nFlexicar is an Melbourne membership-based carsharing service owned by Hertz. The company provides automobile reservations to its members, billable by the hour or day. Flexicar was co-founded in 2004 by Monique Conheady and five other partners. In 2008, Conheady was awarded the Winston Churchill Fellowship to investigate public transport systems all over the world that use the latest technology and incorporate new forms of transport like bicycle sharing.\n\nFlexicar has been a partner of Honda in 2009 for sharing there mutual commitment to sustainability. Honda provided two Honda Civic Hybrids, a low fuel emission car, to Flexicar as an initiative for sustainable driving.\n\nIn 2010, Flexicar won an award for Transportation, Warehousing and Logistics – Mindful Movement (supported by Banksia Environmental Foundation).\n\n\nFlexicar has won and been a finalist in several environmental awards including:\n\n"}
{"id": "144143", "url": "https://en.wikipedia.org/wiki?curid=144143", "title": "Gallium arsenide", "text": "Gallium arsenide\n\nGallium arsenide (GaAs) is a compound of the elements gallium and arsenic. It is a III-V direct bandgap semiconductor with a Zinc blende crystal structure.\n\nGallium arsenide is used in the manufacture of devices such as microwave frequency integrated circuits, monolithic microwave integrated circuits, infrared light-emitting diodes, laser diodes, solar cells and optical windows.\n\nGaAs is often used as a substrate material for the epitaxial growth of other III-V semiconductors including indium gallium arsenide, aluminum gallium arsenide and others.\n\nIn the compound, gallium has a +3 oxidation state. Gallium arsenide single crystals can be prepared by three industrial processes:\n\nAlternative methods for producing films of GaAs include:\n\nOxidation of GaAs occurs in air and degrades performance of the semiconductor. The surface can be passivated by depositing a cubic gallium(II) sulfide layer using a tert-butyl gallium sulfide compound such as (.\n\nIf a GaAs boule is grown with excess arsenic present, it gets certain defects, in particular arsenic antisite defects (an arsenic atom at a gallium atom site within the crystal lattice). The electronic properties of these defects (interacting with others) cause the Fermi level to be pinned to near the center of the bandgap, so that this GaAs crystal has very low concentration of electrons and holes. This low carrier concentration is similar to an intrinsic (perfectly undoped) crystal, but much easier to achieve in practice. These crystals are called \"semi-insulating\", reflecting their high resistivity of 10–10 Ω·cm (which is quite high for a semiconductor, but still much lower than a true insulator like glass).\n\nWet etching of GaAs industrially uses an oxidizing agent such as hydrogen peroxide or bromine water, and the same strategy has been described in a patent relating to processing scrap components containing GaAs where the is complexed with a hydroxamic acid (\"HA\"), for example:\nThis reaction produces arsenic acid.\n\nGaAs can be used for various transistor types:\n\nThe HBT can be used in integrated injection logic (IL).\nThe earliest GaAs logic gate used Buffered FET Logic (BFL).\n\nFrom ~1975 to 1995 the main logic families used were:\n\nSome electronic properties of gallium arsenide are superior to those of silicon. It has a higher saturated electron velocity and higher electron mobility, allowing gallium arsenide transistors to function at frequencies in excess of 250 GHz. GaAs devices are relatively insensitive to overheating, owing to their wider energy bandgap, and they also tend to create less noise (disturbance in an electrical signal) in electronic circuits than silicon devices, especially at high frequencies. This is a result of higher carrier mobilities and lower resistive device parasitics. These superior properties are compelling reasons to use GaAs circuitry in mobile phones, satellite communications, microwave point-to-point links and higher frequency radar systems. It is also used in the manufacture of Gunn diodes for the generation of microwaves.\n\nAnother advantage of GaAs is that it has a direct band gap, which means that it can be used to absorb and emit light efficiently. Silicon has an indirect bandgap and so is relatively poor at emitting light.\n\nAs a wide direct band gap material with resulting resistance to radiation damage, GaAs is an excellent material for outer space electronics and optical windows in high power applications.\n\nBecause of its wide bandgap, pure GaAs is highly resistive. Combined with a high dielectric constant, this property makes GaAs a very good substrate for Integrated circuits and unlike Si provides natural isolation between devices and circuits. This has made it an ideal material for monolithic microwave integrated circuits, MMICs, where active and essential passive components can readily be produced on a single slice of GaAs.\n\nOne of the first GaAs microprocessors was developed in the early 1980s by the RCA corporation and was considered for the Star Wars program of the United States Department of Defense. These processors were several times faster and several orders of magnitude more radiation proof than silicon counterparts, but were more expensive. Other GaAs processors were implemented by the supercomputer vendors Cray Computer Corporation, Convex, and Alliant in an attempt to stay ahead of the ever-improving CMOS microprocessor. Cray eventually built one GaAs-based machine in the early 1990s, the Cray-3, but the effort was not adequately capitalized, and the company filed for bankruptcy in 1995.\n\nComplex layered structures of gallium arsenide in combination with aluminium arsenide (AlAs) or the alloy AlGaAs can be grown using molecular beam epitaxy (MBE) or using metalorganic vapor phase epitaxy (MOVPE). Because GaAs and AlAs have almost the same lattice constant, the layers have very little induced strain, which allows them to be grown almost arbitrarily thick. This allows extremely high performance and high electron mobility HEMT transistors and other quantum well devices.\n\nConcerns over GaAs's susceptibility to heat damage have been raised, but it has been speculated that certain manufacturers would benefit from such limitations, considering the planned obsolescence cycle that many consumer electronics are designed to follow.\n\nSilicon has three major advantages over GaAs for integrated circuit manufacture. First, silicon is abundant and cheap to process in the form of silicate minerals. The economies of scale available to the silicon industry has also hindered the adoption of GaAs.\n\nIn addition, a Si crystal has a very stable structure and can be grown to very large diameter boules and processed with very good yields. It is also a fairly good thermal conductor, thus enabling very dense packing of transistors that need to get rid of their heat of operation, all very desirable for design and manufacturing of very large ICs. Such good mechanical characteristics also make it a suitable material for the rapidly developing field of nanoelectronics. Naturally, a GaAs surface cannot withstand the high temperatures needed for diffusion; however a viable and actively pursued alternative as of the 1980's was ion implanation.\n\nThe second major advantage of Si is the existence of a native oxide (silicon dioxide, SiO), which is used as an insulator. Silicon dioxide can be incorporated onto silicon circuits easily, and such layers are adherent to the underlying silicon. SiO is not only a good insulator (with a band gap of 8.9 eV), but the Si-SiO interface can be easily engineered to have excellent electrical properties, most importantly low density of interface states. GaAs does not have a native oxide, does not easily support a stable adherent insulating layer, and does not possess the dielectric strength or surface passivating qualities of the Si-SiO.\n\nAluminum oxide (AlO) has been extensively studied as a possible gate oxide for GaAs (as well as InGaAs).\n\nThe third advantage of silicon is that it possesses a higher hole mobility compared to GaAs (500 versus 400 cmVs). This high mobility allows the fabrication of higher-speed P-channel field effect transistors, which are required for CMOS logic. Because they lack a fast CMOS structure, GaAs circuits must use logic styles which have much higher power consumption; this has made GaAs logic circuits unable to compete with silicon logic circuits.\n\nFor manufacturing solar cells, silicon has relatively low absorptivity for sunlight, meaning about 100 micrometers of Si is needed to absorb most sunlight. Such a layer is relatively robust and easy to handle. In contrast, the absorptivity of GaAs is so high that only a few micrometers of thickness are needed to absorb all of the light. Consequently, GaAs thin films must be supported on a substrate material.\n\nSilicon is a pure element, avoiding the problems of stoichiometric imbalance and thermal unmixing of GaAs.\n\nSilicon has a nearly perfect lattice; impurity density is very low and allows very small structures to be built (currently down to 16 nm). In contrast, GaAs has a very high impurity density, which makes it difficult to build integrated circuits with small structures, so the 500 nm process is a common process for GaAs.\n\nGallium arsenide (GaAs) is an important semiconductor material for high-cost, high-efficiency solar cells and is used for single-crystalline thin film solar cells and for multi-junction solar cells.\n\nThe first known operational use of GaAs solar cells in space was for the Venera 3 mission, launched in 1965. The GaAs solar cells, manufactured by Kvant, were chosen because of their higher performance in high temperature environments. GaAs cells were then used for the Lunokhod rovers for the same reason.\n\nIn 1970, the GaAs heterostructure solar cells were developed by the team led by Zhores Alferov in the USSR, achieving much higher efficiencies. In the early 1980s, the efficiency of the best GaAs solar cells surpassed that of conventional, crystalline silicon-based solar cells. In the 1990s, GaAs solar cells took over from silicon as the cell type most commonly used for photovoltaic arrays for satellite applications. Later, dual- and triple-junction solar cells based on GaAs with germanium and indium gallium phosphide layers were developed as the basis of a triple-junction solar cell, which held a record efficiency of over 32% and can operate also with light as concentrated as 2,000 suns. This kind of solar cell powers the Mars Exploration Rovers Spirit and Opportunity, which are exploring Mars' surface. Also many solar cars utilize GaAs in solar arrays.\n\nGaAs-based devices hold the world record for the highest-efficiency single-junction solar cell at 28.8%. This high efficiency is attributed to the extreme high quality GaAs epitaxial growth, surface passivation by the AlGaAs, and the promotion of photon recycling by the thin film design.\n\nComplex designs of AlGaAs-GaAs devices using quantum wells can be sensitive to infrared radiation (QWIP).\n\nGaAs diodes can be used for the detection of X-rays.\n\nGaAs has been used to produce near-infrared laser diodes since 1962.\n\nFor this purpose an optical fiber tip of an optical fiber temperature sensor is equipped with a gallium arsenide crystal. Starting at a light wavelength of 850 nm GaAs becomes optically translucent. Since the position of the band gap is temperature dependent, it shifts about 0.4 nm/K. The measurement device contains a light source and a device for the spectral detection of the band gap. With the changing of the band gap, (0.4 nm/K) an algorithm calculates the temperature (all 250 ms).\n\nGaAs may have applications in spintronics as it can be used instead of platinum in spin-charge converters and may be more tunable.\n\nThe environment, health and safety aspects of gallium arsenide sources (such as trimethylgallium and arsine) and industrial hygiene monitoring studies of metalorganic precursors have been reported. California lists gallium arsenide as a carcinogen, as do IARC and ECA, and it is considered a known carcinogen in animals. On the other hand, a 2013 review (funded by industry) argued against these classifications, saying that when rats or mice inhale fine GaAs powders (as in previous studies), they get cancer from the resulting lung irritation and inflammation, rather than from a primary carcinogenic effect of the GaAs itself—and that, moreover, fine GaAs powders are unlikely to be created in the production or use of GaAs.\n\n"}
{"id": "72727", "url": "https://en.wikipedia.org/wiki?curid=72727", "title": "Green Revolution", "text": "Green Revolution\n\nThe Green Revolution, or Third Agricultural Revolution, refers to a set of research and the development of technology transfer initiatives occurring between 1950 and the late 1960s, that increased agricultural production worldwide, particularly in the developing world, beginning most markedly in the late 1960s. The initiatives resulted in the adoption of new technologies, including high-yielding varieties (HYVs) of cereals, especially dwarf wheats and rices, in association with chemical fertilizers and agro-chemicals, and with controlled water-supply (usually involving irrigation) and new methods of cultivation, including mechanization. All of these together were seen as a 'package of practices' to supersede 'traditional' technology and to be adopted as a whole.\n\nBoth the Ford Foundation and the Rockefeller Foundation were heavily involved. \nOne key leader was Norman Borlaug, the \"Father of the Green Revolution\", who received the Nobel Peace Prize in 1970. He is credited with saving over a billion people from starvation. The basic approach was the development of high-yielding varieties of cereal grains, expansion of irrigation infrastructure, modernization of management techniques, distribution of hybridized seeds, synthetic fertilizers, and pesticides to farmers.\n\nThe term \"Green Revolution\" was first used in a March 8, 1968, speech by the administrator of the U.S. Agency for International Development (USAID), William S. Gaud, who noted the spread of the new technologies: \"These and other developments in the field of agriculture contain the makings of a new revolution. It is not a violent Red Revolution like that of the Soviets, nor is it a White Revolution like that of the Shah of Iran. I call it the Green Revolution.\"\n\nIt has been argued that \"during the twentieth century two 'revolutions' transformed rural Mexico: the Mexican Revolution (1910–1920) and the Green Revolution (1950–1970)\". With the support of the Mexican government, the U.S. government, the United Nations, the Food and Agriculture Organization (FAO), and the Rockefeller Foundation, Mexico made a concerted effort to transform agricultural productivity, particularly with irrigated rather than dry-land cultivation in its northwest, to solve its problem of lack of food self-sufficiency. In the center and south of Mexico, where large-scale production faced challenges, agricultural production languished. Increased production meant food self-sufficiency in Mexico to feed its growing and urbanizing population, with the number of calories consumed per Mexican increasing. Technology was seen as a valuable way to feed the poor, and would relieve some pressure of the land redistribution process.\n\nMexico was the recipient of Green Revolution knowledge and technology, and it was an active participant with financial support from the government for agriculture as well as Mexican agronomists. Although the Mexican Revolution had broken the back of the hacienda system and land reform in Mexico had by 1940 distributed a large expanse of land in central and southern Mexico, agricultural productivity had fallen. During the administration of Manuel Avila Camacho (1940–46), the government put resources into developing new breeds of plants and partnered with the Rockefeller Foundation. In 1943, the Mexican government founded the International Maize and Wheat Improvement Center (CIMMYT), which became a base for international agricultural research.\n\nAgriculture in Mexico had been a sociopolitical issue, a key factor in some regions' participation in the Mexican Revolution. It was also a technical issue, enabled by a cohort of trained agronomists, who were to advise peasants how to increase productivity. In the post-World War II era, the government sought development in agriculture that bettered technological aspects of agriculture in regions that were not dominated by small-scale peasant cultivators. This drive for agricultural transformation would have the benefit of keeping Mexico self-sufficient in food and in the political sphere with the Cold War, potentially stem unrest and the appeal of Communism. Technical aid can be seen as also serving political ends in the international sphere. In Mexico, it also served political ends, separating peasant agriculture based on the ejido and considered one of the victories of the Mexican Revolution, from agribusiness that requires large-scale land ownership, irrigation, specialized seeds, fertilizers, and pesticides, machinery, and a low-wage paid labor force.\n\nThe government created the Mexican Agricultural Program (MAP) to be the lead organization in raising productivity. One of their successes was wheat production, with varieties the agency's scientists helped create dominating wheat production as early as 1951 (70%), 1965 (80%), and 1968 (90%). Mexico became the showcase for extending the Green Revolution to other areas of Latin America and beyond, into Africa and Asia. New breeds of maize, beans, and wheat produced bumper crops with proper inputs (such as fertilizer and pesticides) and careful cultivation. Many Mexican farmers who had been dubious about the scientists or hostile to them (often a mutual relationship of discord) came to see the scientific approach to agriculture as worth adopting.\n\nIn 1960, the Government of the Republic of the Philippines with the Ford Foundation and the Rockefeller Foundation established the International Rice Research Institute (IRRI). A rice crossing between Dee-Geo-woo-gen and Peta was done at IRRI in 1962. In 1966, one of the breeding lines became a new cultivar, IR8. IR8 required the use of fertilizers and pesticides, but produced substantially higher yields than the traditional cultivars. Annual rice production in the Philippines increased from 3.7 to 7.7 million tons in two decades. The switch to IR8 rice made the Philippines a rice exporter for the first time in the 20th century.\n\nIn 1961, India was on the brink of mass famine. Norman Borlaug was invited to India by the adviser to the Indian minister of agriculture Dr.M.S Swaminathan. Despite bureaucratic hurdles imposed by India's grain monopolies, the Ford Foundation and Indian government collaborated to import wheat seed from the International Maize and Wheat Improvement Center (CIMMYT). Punjab was selected by the Indian government to be the first site to try the new crops because of its reliable water supply and a history of agricultural success. India began its own Green Revolution program of plant breeding, irrigation development, and financing of agrochemicals.\n\nIndia soon adopted IR8 – a semi-dwarf rice variety developed by the International Rice Research Institute (IRRI) that could produce more grains of rice per plant when grown with certain fertilizers and irrigation. In 1968, Indian agronomist S.K. De Datta published his findings that IR8 rice yielded about 5 tons per hectare with no fertilizer, and almost 10 tons per hectare under optimal conditions. This was 10 times the yield of traditional rice. IR8 was a success throughout Asia, and dubbed the \"Miracle Rice\". IR8 was also developed into Semi-dwarf IR36.\n\nIn the 1960s, rice yields in India were about two tons per hectare; by the mid-1990s, they had risen to six tons per hectare. In the 1970s, rice cost about $550 a ton; in 2001, it cost under $200 a ton. India became one of the world's most successful rice producers, and is now a major rice exporter, shipping nearly 4.5 million tons in 2006.\n\nIn 1970, foundation officials proposed a worldwide network of agricultural research centers under a permanent secretariat. This was further supported and developed by the World Bank; on 19 May 1971, the Consultative Group on International Agricultural Research (CGIAR) was established. co-sponsored by the FAO, IFAD and UNDP. CGIAR has added many research centers throughout the world.\n\nCGIAR has responded, at least in part, to criticisms of Green Revolution methodologies. This began in the 1980s, and mainly was a result of pressure from donor organizations. Methods like Agroecosystem Analysis and Farming System Research have been adopted to gain a more holistic view of agriculture.\n\nBrazil's vast inland cerrado region was regarded as unfit for farming before the 1960s because the soil was too acidic and poor in nutrients, according to Norman Borlaug. However, from the 1960s, vast quantities of lime (pulverised chalk or limestone) were poured on the soil to reduce acidity. The effort went on for decades; by the late 1990s, between 14 million and 16 million tonnes of lime were being spread on Brazilian fields each year. The quantity rose to 25 million tonnes in 2003 and 2004, equalling around five tonnes of lime per hectare. As a result, Brazil has become the world's second biggest soybean exporter. Soybeans are also widely used in animal feed, and the large volume of soy produced in Brazil has contributed to Brazil's rise to become the biggest exporter of beef and poultry in the world. Several parallels can also be found in Argentina's boom in soybean production as well.\n\nThere have been numerous attempts to introduce the successful concepts from the Mexican and Indian projects into Africa. These programs have generally been less successful. Reasons cited include widespread corruption, insecurity, a lack of infrastructure, and a general lack of will on the part of the governments. Yet environmental factors, such as the availability of water for irrigation, the high diversity in slope and soil types in one given area are also reasons why the Green Revolution is not so successful in Africa.\n\nA recent program in western Africa is attempting to introduce a new high-yielding 'family' of rice varieties known as \"New Rice for Africa\" (NERICA). NERICA varieties yield about 30% more rice under normal conditions, and can double yields with small amounts of fertilizer and very basic irrigation. However, the program has been beset by problems getting the rice into the hands of farmers, and to date the only success has been in Guinea, where it currently accounts for 16% of rice cultivation.\n\nAfter a famine in 2001 and years of chronic hunger and poverty, in 2005 the small African country of Malawi launched the \"Agricultural Input Subsidy Program\" by which vouchers are given to smallholder farmers to buy subsidized nitrogen fertilizer and maize seeds. Within its first year, the program was reported to have had extreme success, producing the largest maize harvest of the country's history, enough to feed the country with tons of maize left over. The program has advanced yearly ever since. Various sources claim that the program has been an unusual success, hailing it as a \"miracle\".\n\nThe Green Revolution spread technologies that already existed, but had not been widely implemented outside industrialized nations. Two kinds of technologies were used in the Green Revolution and aim at cultivation and breeding area respectively. The technologies in cultivation are targeted at providing excellent growing conditions, which included modern irrigation projects, pesticides, and synthetic nitrogen fertilizer. The breeding technologies aimed at improving crop varieties developed through the conventional, science-based methods available at the time. These technologies included hybrids, combining modern genetics with selections.\n\nThe novel technological development of the Green Revolution was the production of novel wheat cultivars. Agronomists bred cultivars of maize, wheat, and rice that are generally referred to as HYVs or \"high-yielding varieties\". HYVs have higher nitrogen-absorbing potential than other varieties. Since cereals that absorbed extra nitrogen would typically lodge, or fall over before harvest, semi-dwarfing genes were bred into their genomes. A Japanese dwarf wheat cultivar Norin 10 developed by a Japanese agronomist Gonjiro Inazuka, which was sent to Orville Vogel at Washington State University by Cecil Salmon, was instrumental in developing Green Revolution wheat cultivars. IR8, the first widely implemented HYV rice to be developed by IRRI, was created through a cross between an Indonesian variety named \"Peta\" and a Chinese variety named \"Dee-geo-woo-gen\". In the 1960s, when a food crisis happened in Asia, the spread of HYV rice was aggravated intensely.\n\nDr. Norman Borlaug, who is usually recognized as the \"Father of the Green Revolution\", bred rust-resistant cultivars which have strong and firm stems, preventing them from falling over under extreme weather at high levels of fertilization. CIMMYT(Centro Internacional de Mejoramiento de Maiz y Trigo—International Center for Maize and Wheat Improvements) conducted these breeding programs and helped spread high-yielding varieties in Mexico and countries in Asia like India and Pakistan. These programs successfully led the harvest double in these countries.\n\nPlant scientists figured out several parameters related to the high yield and identified the related genes which control the plant height and tiller number. With advances in molecular genetics, the mutant genes responsible for \"Arabidopsis thaliana\" genes (GA 20-oxidase, \"ga1\", \"ga1-3\"), wheat reduced-height genes (\"Rht\") and a rice semidwarf gene (\"sd1\") were cloned. These were identified as gibberellin biosynthesis genes or cellular signaling component genes. Stem growth in the mutant background is significantly reduced leading to the dwarf phenotype. Photosynthetic investment in the stem is reduced dramatically as the shorter plants are inherently more stable mechanically. Assimilates become redirected to grain production, amplifying in particular the effect of chemical fertilizers on commercial yield.\n\nHYVs significantly outperform traditional varieties in the presence of adequate irrigation, pesticides, and fertilizers. In the absence of these inputs, traditional varieties may outperform HYVs. Therefore, several authors have challenged the apparent superiority of HYVs not only compared to the traditional varieties alone, but by contrasting the monocultural system associated with HYVs with the polycultural system associated with traditional ones.\n\nCereal production more than doubled in developing nations between the years 1961–1985. Yields of rice, maize, and wheat increased steadily during that period. The production increases can be attributed roughly equally to irrigation, fertilizer, and seed development, at least in the case of Asian rice.\n\nWhile agricultural output increased as a result of the Green Revolution, the energy input to produce a crop has increased faster, so that the ratio of crops produced to energy input has decreased over time. Green Revolution techniques also heavily rely on chemical fertilizers, pesticides, herbicides, and defoliants and rely on machines, which as of 2014 rely on or are derived from crude oil, making agriculture increasingly reliant on crude oil extraction. Proponents of the Peak Oil theory fear that a future decline in oil and gas production would lead to a decline in food production or even a Malthusian catastrophe.\n\nThe effects of the Green Revolution on global food security are difficult to assess because of the complexities involved in food systems.\n\nThe world population has grown by about five billion since the beginning of the Green Revolution and many believe that, without the Revolution, there would have been greater famine and malnutrition. India saw annual wheat production rise from 10 million tons in the 1960s to 73 million in 2006. The average person in the developing world consumes roughly 25% more calories per day now than before the Green Revolution. Between 1950 and 1984, as the Green Revolution transformed agriculture around the globe, world grain production increased by about 160%.\n\nThe production increases fostered by the Green Revolution are often credited with having helped to avoid widespread famine, and for feeding billions of people.\n\nThere are also claims that the Green Revolution has decreased food security for a large number of people. One claim involves the shift of subsistence-oriented cropland to cropland oriented towards production of grain for export or animal feed. For example, the Green Revolution replaced much of the land used for pulses that fed Indian peasants for wheat, which did not make up a large portion of the peasant diet.\n\nSome criticisms generally involve some variation of the Malthusian principle of population. Such concerns often revolve around the idea that the Green Revolution is unsustainable, and argue that humanity is now in a state of overpopulation or overshoot with regards to the sustainable carrying capacity and ecological demands on the Earth.\n\nAlthough 36 million people die each year as a direct or indirect result of hunger and poor nutrition, Malthus's more extreme predictions have frequently failed to materialize. In 1798 Thomas Malthus made his prediction of impending famine. The world's population had doubled by 1923 and doubled again by 1973 without fulfilling Malthus's prediction. Malthusian Paul R. Ehrlich, in his 1968 book \"The Population Bomb\", said that \"India couldn't possibly feed two hundred million more people by 1980\" and \"Hundreds of millions of people will starve to death in spite of any crash programs.\" Ehrlich's warnings failed to materialize when India became self-sustaining in cereal production in 1974 (six years later) as a result of the introduction of Norman Borlaug's dwarf wheat varieties.\n\nHowever, Borlaug was well aware of the implications of population growth. In his Nobel lecture he repeatedly presented improvements in food production within a sober understanding of the context of population. \"The green revolution has won a temporary success in man's war against hunger and deprivation; it has given man a breathing space. If fully implemented, the revolution can provide sufficient food for sustenance during the next three decades. But the frightening power of human reproduction must also be curbed; otherwise the success of the green revolution will be ephemeral only. Most people still fail to comprehend the magnitude and menace of the \"Population Monster\"...Since man is potentially a rational being, however, I am confident that within the next two decades he will recognize the self-destructive course he steers along the road of irresponsible population growth...\"\n\nTo some modern Western sociologists and writers, increasing food production is not synonymous with increasing food security, and is only part of a larger equation. For example, Harvard professor Amartya Sen wrote that large historic famines were not caused by decreases in food supply, but by socioeconomic dynamics and a failure of public action. Economist Peter Bowbrick disputes Sen's theory, arguing that Sen relies on inconsistent arguments and contradicts available information, including sources that Sen himself cited. Bowbrick further argues that Sen's views coincide with that of the Bengal government at the time of the Bengal famine of 1943, and the policies Sen advocates failed to relieve the famine.\n\nSome have challenged the value of the increased food production of Green Revolution agriculture. Miguel A. Altieri, (a pioneer of agroecology and peasant-advocate), writes that the comparison between traditional systems of agriculture and Green Revolution agriculture has been unfair, because Green Revolution agriculture produces monocultures of cereal grains, while traditional agriculture usually incorporates polycultures.\n\nThese monoculture crops are often used for export, feed for animals, or conversion into biofuel. According to Emile Frison of Bioversity International, the Green Revolution has also led to a change in dietary habits, as fewer people are affected by hunger and die from starvation, but many are affected by malnutrition such as iron or vitamin-A deficiencies. Frison further asserts that almost 60% of yearly deaths of children under age five in developing countries are related to malnutrition.\n\nThe strategies developed by the Green Revolution focused on fend off starvation and was very successful in raising overall yields of cereal grains, but did not give sufficient relevance to nutritional quality. High yield-cereal crops have low quality proteins, with essential amino acid deficiencies, are high in carbohydrates, and lack balanced essential fatty acids, vitamins, minerals and other quality factors.\n\nHigh-yield rice (HYR), introduced since 1964 to poverty-ridden Asian countries, such as the Philippines, was found to have inferior flavor and be more glutinous and less savory than their native varieties. This caused its price to be lower than the average market value.\n\nIn the Philippines the introduction of heavy pesticides to rice production, in the early part of the Green Revolution, poisoned and killed off fish and weedy green vegetables that traditionally coexisted in rice paddies. These were nutritious food sources for many poor Filipino farmers prior to the introduction of pesticides, further impacting the diets of locals.\n\nA major critic of the Green Revolution, U.S. investigative journalist Mark Dowie, writes:\nThe primary objective of the program was geopolitical: to provide food for the populace in undeveloped countries and so bring social stability and weaken the fomenting of communist insurgency.\n\nCiting internal Foundation documents, Dowie states that the Ford Foundation had a greater concern than Rockefeller in this area.\n\nThere is significant evidence that the Green Revolution weakened socialist movements in many nations. In countries such as India, Mexico, and the Philippines, \"technological solutions\" were sought as an alternative to expanding \"agrarian reform\" initiatives, the latter of which were often linked to socialist politics.\n\nThe transition from traditional agriculture, in which inputs were generated on-farm, to Green Revolution agriculture, which required the purchase of inputs, led to the widespread establishment of rural credit institutions. Smaller farmers often went into debt, which in many cases results in a loss of their farmland. The increased level of mechanization on larger farms made possible by the Green Revolution removed a large source of employment from the rural economy. Because wealthier farmers had better access to credit and land, the Green Revolution increased class disparities, with the rich–poor gap widening as a result. Because some regions were able to adopt Green Revolution agriculture more readily than others (for political or geographical reasons), interregional economic disparities increased as well. Many small farmers are hurt by the dropping prices resulting from increased production overall. However, large-scale farming companies only account for less than 10% of the total farming capacity. This is a criticism held by many small producers in the food sovereignty movement.\n\nThe new economic difficulties of small holder farmers and landless farm workers led to increased rural-urban migration. The increase in food production led to a cheaper food for urban dwellers, and the increase in urban population increased the potential for industrialization.\n\nAccording to a 2018 paper, a 10 percentage points increase in the use of high-yielding crop varieties in developing countries in the period 1960-2000 led to increases in GDP per capita of approximately 15 percent.\n\nThe spread of Green Revolution agriculture affected both agricultural biodiversity (or agrodiversity) and wild biodiversity. There is little disagreement that the Green Revolution acted to reduce agricultural biodiversity, as it relied on just a few high-yield varieties of each crop.\n\nThis has led to concerns about the susceptibility of a food supply to pathogens that cannot be controlled by agrochemicals, as well as the permanent loss of many valuable genetic traits bred into traditional varieties over thousands of years. To address these concerns, massive seed banks such as Consultative Group on International Agricultural Research’s (CGIAR) International Plant Genetic Resources Institute (now Bioversity International) have been established (see Svalbard Global Seed Vault).\n\nThere are varying opinions about the effect of the Green Revolution on wild biodiversity. One hypothesis speculates that by increasing production per unit of land area, agriculture will not need to expand into new, uncultivated areas to feed a growing human population. However, land degradation and soil nutrients depletion have forced farmers to clear up formerly forested areas in order to keep up with production. A counter-hypothesis speculates that biodiversity was sacrificed because traditional systems of agriculture that were displaced sometimes incorporated practices to preserve wild biodiversity, and because the Green Revolution expanded agricultural development into new areas where it was once unprofitable or too arid. For example, the development of wheat varieties tolerant to acid soil conditions with high aluminium content, permitted the introduction of agriculture in sensitive Brazilian ecosystems such as Cerrado semi-humid tropical savanna and Amazon rainforest in the geoeconomic macroregions of Centro-Sul and Amazônia. Before the Green Revolution, other Brazilian ecosystems were also significantly damaged by human activity, such as the once 1st or 2nd main contributor to Brazilian megadiversity Atlantic Rainforest (above 85% of deforestation in the 1980s, about 95% after the 2010s) and the important xeric shrublands called Caatinga mainly in Northeastern Brazil (about 40% in the 1980s, about 50% after the 2010s — deforestation of the Caatinga biome is generally associated with greater risks of desertification). This also caused many animal species to suffer due to their damaged habitats.\n\nNevertheless, the world community has clearly acknowledged the negative aspects of agricultural expansion as the 1992 Rio Treaty, signed by 189 nations, has generated numerous national Biodiversity Action Plans which assign significant biodiversity loss to agriculture's expansion into new domains.\n\nThe Green Revolution has been criticized for an agricultural model which relied on a few staple and market profitable crops, and pursuing a model which limited the biodiversity of Mexico. One of the critics against these techniques and the Green Revolution as a whole was Carl O. Sauer, a geography professor at the University of California, Berkeley. According to Sauer these techniques of plant breeding would result in negative effects on the country's resources, and the culture:\n\n\"A good aggressive bunch of American agronomists and plant breeders could ruin the native resources for good and all by pushing their American commercial stocks… And Mexican agriculture cannot be pointed toward standardization on a few commercial types without upsetting native economy and culture hopelessly... Unless the Americans understand that, they'd better keep out of this country entirely. That must be approached from an appreciation of native economies as being basically sound\".\n\nAccording to a study published in 2013 in PNAS, in the absence of the crop germplasm improvement associated with the Green Revolution, greenhouse gas emissions would have been 5.2-7.4 Gt higher than observed in 1965–2004. High yield agriculture has dramatic effects on the amount of carbon cycling in the atmosphere. The way in which farms are grown, in tandem with the seasonal carbon cycling of various crops, could alter the impact carbon in the atmosphere has on global warming. Wheat, rice, and soybean crops account for a significant amount of the increase in carbon in the atmosphere over the last 50 years.\n\nMost high intensity agricultural production is highly reliant on non-renewable resources. Agricultural machinery and transport, as well as the production of pesticides and nitrates all depend on fossil fuels. Moreover, the essential mineral nutrient phosphorus is often a limiting factor in crop cultivation, while phosphorus mines are rapidly being depleted worldwide. The failure to depart from these non-sustainable agricultural production methods could potentially lead to a large scale collapse of the current system of intensive food production within this century.\n\nThe consumption of the pesticides used to kill pests by humans in some cases may be increasing the likelihood of cancer in some of the rural villages using them. Poor farming practices including non-compliance to usage of masks and over-usage of the chemicals compound this situation. In 1989, WHO and UNEP estimated that there were around 1 million human pesticide poisonings annually. Some 20,000 (mostly in developing countries) ended in death, as a result of poor labeling, loose safety standards etc.\n\nContradictory epidemiologic studies in humans have linked phenoxy acid herbicides or contaminants in them with soft tissue sarcoma (STS) and malignant lymphoma, organochlorine insecticides with STS, non-Hodgkin's lymphoma (NHL), leukemia, and, less consistently, with cancers of the lung and breast, organophosphorous compounds with NHL and leukemia, and triazine herbicides with ovarian cancer.\n\nThe Indian state of Punjab pioneered green revolution among the other states transforming India into a food-surplus country. The state is witnessing serious consequences of intensive farming using chemicals and pesticides. A comprehensive study conducted by Post Graduate Institute of Medical Education and Research (PGIMER) has underlined the direct relationship between indiscriminate use of these chemicals and increased incidence of cancer in this region. An increase in the number of cancer cases has been reported in several villages including Jhariwala, Koharwala, Puckka, Bhimawali, and Khara.\n\nEnvironmental activist Vandana Shiva has written extensively about the social, political and economic impacts of the Green Revolution in Punjab. She claims that the Green Revolution's reliance on heavy use of chemical inputs and monocultures has resulted in water scarcity, vulnerability to pests, and incidents of violent conflict and social marginalization.\n\nA Greenpeace Research Laboratories investigation of 50 villages in Muktsar, Bathinda and Ludhiana districts revealed that twenty percent of the sampled wells had nitrate levels above WHO limits for drinking water. The 2009 study linked the nitrate pollution with high use of synthetic nitrogen fertilizers.\n\nBorlaug dismissed certain claims of critics, but also cautioned, \"There are no miracles in agricultural production. Nor is there such a thing as a miracle variety of wheat, rice, or maize which can serve as an elixir to cure all ills of a stagnant, traditional agriculture.\"\n\nOf environmental lobbyists, he said:\n\n\"some of the environmental lobbyists of the Western nations are the salt of the earth, but many of them are elitists. They've never experienced the physical sensation of hunger. They do their lobbying from comfortable office suites in Washington or Brussels...If they lived just one month amid the misery of the developing world, as I have for fifty years, they'd be crying out for tractors and fertilizer and irrigation canals and be outraged that fashionable elitists back home were trying to deny them these things\".\n\nAlthough the Green Revolution has been able to improve agricultural output in some regions in the world, there was and is still room for improvement. As a result, many organizations continue to invent new ways to improve the techniques already used in the Green Revolution. Frequently quoted inventions are the System of Rice Intensification, marker-assisted selection, agroecology, and applying existing technologies to agricultural problems of the developing world. Current challenges for nations trying to modernize their agriculture include closing the urban-rural income gap, integration of smallholders into value chains, and maintaining competitiveness in the market. However, in low-income countries, chronic problems such as poverty and hunger cause agricultural modernization efforts constrained. It is projected that global populations by 2050 will increase by one-third and as such will require a 70% increase in the production of food. As such, the Second Green Revolution will likely focus on improving tolerances to pests and disease in addition to technological input use efficiency.\n\n\n\n"}
{"id": "41293019", "url": "https://en.wikipedia.org/wiki?curid=41293019", "title": "Grok Learning", "text": "Grok Learning\n\nGrok Learning is an online interactive platform that offers coding classes in programming languages like Python and HTML. Grok Learning has a focus on educating high school students, but also has content targeted specifically for professionals. Grok Learning provides courses suitable for use in a classroom setting where a computing trained teacher may or may not be present.\n\nThe site teaches students through a combination of notes and accompanying coding problems to solve. These problems can coded and run using the in-browser programming environment and are marked instantaneously by an auto-marker. The site offers feedback for solved problems, and badges for completing certain tasks, as well as leaderboards for certain courses. Grok has created a motto to encourage coders to keep going. This motto is \"Grok On\".\n\nGrok Learning was founded in 2013 by Tara Murphy, James Curran, Tim Dawborn, and Nicky Ringland. At the time of foundation, James was an associate professor in the School of IT at The University of Sydney, Tara was a senior lecturer in astroinformatics at The University of Sydney, and Tim and Nicky were computer science PhD candidates at The University of Sydney. Since then, Tim and Nicky have completed their PhDs in 2015.\n\n"}
{"id": "32693095", "url": "https://en.wikipedia.org/wiki?curid=32693095", "title": "Gude Cause 1909 and 2009", "text": "Gude Cause 1909 and 2009\n\nGude Cause was the name of a feminist project, based at the Peace and Justice Centre in Edinburgh, Scotland, which inspired over 60 events and projects throughout Scotland between 2007 and 2009. \nGude Cause aimed to commemorate the work of Scotswomen involved in the suffrage movement, to celebrate women's achievements in the 100 years since the Women's Suffrage Procession which had taken place in Edinburgh in 1909, and to re-energise women's commitment to political representation and action in Scotland.\n\nThe work culminated in the re-enactment on 10 October 2009 of the 1909 Edinburgh procession, and was organised by volunteers, women's historians and community workers, in association with The Edinburgh Peace and Justice Resource Centre, achieving the main goal of recreating the original procession in all its glory, while drawing attention to the problems that still need to be faced up to around the world, such as tackling domestic violence, forced marriage, sex trafficking and equal pay.\nThousands of people from groups across Scotland had prepared for the day by creating banners, learning old and specially written songs, researching women's struggles in the past and discussing current issues and future aims.\n\nThe crowd on the day included women, men and children; students, activists, 'roller derby girls', the University of the Third Age, political parties, faith groups and trade unions; artists and academics; professionals and campaigners for women's rights, social justice and environmental justice, representing a wide spectrum of ages, ethnicities, attitudes and activism and a mixture of beliefs, traditions and movements.\n\nIn common with the Suffrage Procession a century before, the Gude Cause Procession 2009 was led by a lone woman piper, Pipe Major Louise Marshall Millington, and featured a band, the Forth Bridges Accordion Band. Two mounted policewomen represented the women on horseback of the 1909 parade. Groups of drummers, including SheBoom, and singers led each section of the Procession, representing the past, the present and the future for women in Scotland.\n\nIn the weeks before the 2007 Scottish Parliamentary election, statistics indicated that a large percentage of women intended to abstain from using the vote that women a century ago had fought so hard to secure. Women – especially young women – appeared to be disillusioned and increasingly apathetic about party politics and representative democracy. However, the same study showed they did still care about the issues and political decisions that affect people's lives in Scotland and around the world.\n\nThe research also showed that, when looked at as a group, women often had differing views to men on a wide range subjects, including issues such as the war in Iraq or the replacement of the Trident nuclear missile system. It followed, therefore, that the under representation of women and their views in the political process was not just an academic problem, but that this was likely to result in real deficiencies in government policy and decision-making in not taking into account of the needs of this group of the population.\nIn April 2007, the Edinburgh Peace & Justice Centre organised a procession along Princes Street in Edinburgh, to raise the profile of women's concerns, to encourage women to re-engage with the political process, and at the most basic level, to make women think about using their vote.\n\nDespite the short notice, several hundred women turned up, walking along Princes Street and climbing Calton Hill to sing \"Bread and Roses\" and other feminist songs. Participants agreed that this spirit of feminist co-operation could inspire the planning of a more substantial and high-profile public campaign.\nThe plan was to recall and celebrate the courage, passion and persistence of the women's suffrage activists who, for around sixty years from 1867 until 1928, had campaigned for the vote. A key event for the movement in Scotland was the great suffrage procession through the streets of Edinburgh on 9 October 1909. It was decided it would be good to re-enact that event one hundred years on, in 2009.\n\nAmong the many of banners seen at the 1909 procession was one which read \"A Gude Cause Maks a Strong Arm\"; a rallying call the organisers felt was as true in 2009 as it was in 1909. And so \"Gude Cause\" was adopted as the name for the organisation.\n\nThe organisation's aims were to commemorate the 1909 Suffrage Parade in Edinburgh, and the importance of Scotswomen in the suffrage movement; to encourage Scotswomen to re-connect with political processes at a local, regional and national level; to celebrate all the achievements and progress made by Scotswomen during the last hundred years; and to highlight the violence, lower pay, discrimination and other inequalities which the Scottish Government's own Gender Audit of 2007 revealed still blight many women's lives in the 21st century.\n\nThe project was launched in October 2008 at the Scottish Parliament.\n\nThe 1909 Edinburgh Pageant, which so inspired the creation of Gude Cause, took place in the midst of great political tension on Scotland. Four key by-elections provided ideal campaigning opportunities for the suffragettes, and in the weeks leading up to the event, several women were arrested and imprisoned. Some refused to eat. Meanwhile, the constitutional campaigners were organising exhausting caravan tours the length and breadth of the country – chalking details of meetings on pavements and enduring jeers on the streets.\nThe 1909 procession was organised by the Women's Social and Political Union (WSPU), who billed it as \"The Great Procession and Women's Demonstration\" with the theme of \"What women have done and can and will do\". In charge of planning, and at the helm of the procession itself, was Flora Drummond, aka 'the General', astride a horse. Playing the bagpipes was nine-year-old Bessie Watson. A \"Historical Pageant\" of women dressed as well-known female historical figures, including the Countess of Buchan, and groups from far & near processed along Princes Street, while a large proportion of the local population turned out to watch.\n\nThe leader of the British suffragette movement, Emmeline Pankhurst and her daughter Christabel, were keynote speakers; participants included some of the first female graduates from Edinburgh University and a group of fishwives from Musselburgh.\n\n'The Edinburgh \"Evening Dispatch\" wrote of \"a solid phalanx of resolute and unflinching womanhood bent upon obtaining the vote\"'.\n\nMany taking part wore the WSPU colours of white, violet, and green, others were dressed in their graduate robes or the attire of their profession or trade. It was a demonstration of women's recent and hard-won achievements, as well as their aspirations for future equality and rights.\n\nEdinburgh \"Evening Dispatch\" wrote in its 11 October 1909 issue, \"The imposing display achieved its object. It advertised to tens of 1000s the aim and objects of the suffragettes, and it made it abundantly apparent to all who had eyes to see, ears to hear, and minds to understand, that behind this movement there is a solid phalanx of resolute and unflinching womanhood bent upon obtaining the vote, and fully determined that they will triumph over every obstacle!”\n\nOn Saturday 10 October 2009 5000 people paraded through Edinburgh in autumn sunshine to commemorate the work of the suffrage movement, to celebrate women's achievements in the intervening 100 years, and to re-energise women's commitment to political representation and action in Scotland. \"The suffragettes wanted votes for women; these re-enactors want women to value and use the votes for which their great grannies fought\".\n\nThe predominant colours were violet, green and white and many were dressed in period costume from 1909, wearing sashes and carrying banners demanding Votes for Women, reflecting the look and aims of the earlier procession, as well as banners from groups currently involved in political and social activism.\nThe groups included \n\nThe 2009 procession assembled on Bruntsfield Links and travelled along the following route: Bruntsfield Links – Whitehouse Loan – Bruntsfield Place – Glengyle Terrace – Leven Terrace – Melville Drive – Middle Meadow Walk crossing The Meadows (park) – Forest Road – George IV Bridge – The High Street AKA The Royal Mile – North Bridge – Waterloo Place – Regent Road – Calton Hill access road – to the summit of Calton Hill. The original route had to be changed and the part along Princes Street had to be omitted because of the tram works.\n\nThe procession paused on the High Street, where Jenny Dawe, the City of Edinburgh Council leader, spoke from the Mercat cross, tracing the history of women's struggles for recognition in Edinburgh. On behalf of the City of Edinburgh Council, Jenny presented the Gude Cause Committee with a banner which was made by the City of Edinburgh Council Museum Volunteers to commemorate the occasion. MSPs, from across the political spectrum, who joined the procession included Fiona Hyslop, Sarah Boyack, Marilyn Glen, Shirley Anne Somerville and Patrick Harvie. Former MSP and MP Donald Gorrie, whose aunts Belle and Mary Gorrie played Mary Queen of Scots and Catherine Barlass in the original parade, also took part.\n\nCouncillors joined the procession at this point and walked to Calton Hill where the Minister for Education, Fiona Hyslop MSP spoke on behalf of the Scottish Government. Cathy Peattie MSP sang 'Bread and Roses', and Janet Fenton spoke about the aims, aspirations and activities of the Gude Cause movement.\n\nGude Cause had developed a network of women's organisations to spread the word and work in partnership to promote interest in women's history and political action. Over 100 organisations provided practical and financial support. In particular, the Gude Cause committee collaborated with \n\nOther Gude Cause initiatives included: \n\nOne particular initiative is still ongoing. The Gude Cause Media Project (originally called the New Media Group) aims to bring together crowd-sourced photographs and videos of the event, alongside video presentations talking about Scottish women's history and the suffrage movement in Scotland.\n\nThe crowd-sourced photographs and videos are being hosted by Scran, an online resource for educational use by schools, further education, higher education, libraries, museums, and the public. Scran is part of RCAHMS, the Royal Commission on the Ancient and Historical Monuments of Scotland.\n\nThere is also a Gude Cause Group on Flickr which contains many of these images.\n"}
{"id": "47507934", "url": "https://en.wikipedia.org/wiki?curid=47507934", "title": "International Ground Source Heat Pump Association", "text": "International Ground Source Heat Pump Association\n\nEstablished in 1987, the International Ground Source Heat Pump Association (IGSHPA) is a nonprofit, membership-based organization that promotes geothermal heat pump technology. It is an outreach unit of the College of Engineering, Architecture and Technology (CEAT) at Oklahoma State University. \n\nIGSHPA is the main organization for establishing standards of practice and standards of design for Geothermal Heat Pump (GHP) systems in the US. Related organizations have been formed in other countries on four continents, including Australia, Canada, China, India, South Korea, and Sweden.\n\nEach year the association hosts an annual conference for people such as manufacturers, contractors, distributors, and drillers.\n\nIGSHPA sets and revises standards for Geothermal Heat Pump (GHP) system installs based on ongoing research and field application results.\n\n\n"}
{"id": "29741016", "url": "https://en.wikipedia.org/wiki?curid=29741016", "title": "Juraj Šimlovič", "text": "Juraj Šimlovič\n\nJuraj Šimlovič (born January 9, 1982) is a freelance software developer living in Prague, Czech Republic. He is a co-author of the extension for Wikipedia and author of a number of free software utilities, of which the most popular is TED Notepad. In December 2008, his team won the 2K Australia's Botprize artificial intelligence competition.\n\n"}
{"id": "2830398", "url": "https://en.wikipedia.org/wiki?curid=2830398", "title": "Knee replacement", "text": "Knee replacement\n\nKnee replacement, also known as knee arthroplasty, is a surgical procedure to replace the weight-bearing surfaces of the knee joint to relieve pain and disability. It is most commonly performed for osteoarthritis, and also for other knee diseases such as rheumatoid arthritis and psoriatic arthritis. In patients with severe deformity from advanced rheumatoid arthritis, trauma, or long-standing osteoarthritis, the surgery may be more complicated and carry higher risk. Osteoporosis does not typically cause knee pain, deformity, or inflammation and is not a reason to perform knee replacement\n\nOther major causes of debilitating pain include meniscus tears, cartilage defects, and ligament tears. Debilitating pain from osteoarthritis is much more common in the elderly.\n\nKnee replacement surgery can be performed as a partial or a total knee replacement. In general, the surgery consists of replacing the diseased or damaged joint surfaces of the knee with metal and plastic components shaped to allow continued motion of the knee.\n\nThe operation typically involves substantial postoperative pain, and includes vigorous physical rehabilitation. The recovery period may be 6 weeks or longer and may involve the use of mobility aids (e.g. walking frames, canes, crutches) to enable the patient's return to preoperative mobility.\n\nKnee replacement surgery is most commonly performed in people with advanced osteoarthritis and should be considered when conservative treatments have been exhausted. Total knee replacement is also an option to correct significant knee joint or bone trauma in young patients. Similarly, total knee replacement can be performed to correct mild valgus or varus deformity. Serious valgus or varus deformity should be corrected by osteotomy. Physical therapy has been shown to improve function and may delay or prevent the need for knee replacement. Pain is often noted when performing physical activities requiring a wide range of motion in the knee joint.\n\nRisks and complications in knee replacement are similar to those associated with all joint replacements. The most serious complication is infection of the joint, which occurs in <1% of patients. Risk factors for infection are related to both patient and surgical factors. Deep vein thrombosis occurs in up to 15% of patients, and is symptomatic in 2–3%. Nerve injuries occur in 1–2% of patients. Persistent pain or stiffness occurs in 8–23% of patients. Prosthesis failure occurs in approximately 2% of patients at 5 years.\n\nThere is increased risk of complications for obese people going through total knee replacement. The morbidly obese should be advised to lose weight before surgery and, if medically eligible, would probably benefit from bariatric surgery.\n\nFracturing or chipping of the polyethylene platform between the femoral and tibial components may be of concern. These fragments may become lodged in the knee and create pain or may move to other parts of the body. Advancements in implant design have greatly reduced these issues but the potential for concern is still present over the lifespan of the knee replacement.\n\nAccording to the American Academy of Orthopedic Surgeons (AAOS), deep vein thrombosis in the leg is \"the most common complication of knee replacement surgery... prevention... may include periodic elevation of patient's legs, lower leg exercises to increase circulation, support stockings and medication to thin your blood.\"\n\nPeriprosthetic fractures are becoming more frequent with the aging patient population and can occur intraoperatively or postoperatively. Depending on the location of the fracture and the stability of the prosthesis, these can be treated surgically with open reduction and internal fixation or revision of the prosthesis.\n\nThe knee at times may not recover its normal range of motion (0–135 degrees usually) after total knee replacement. Much of this is dependent on pre-operative function. Most patients can achieve 0–110 degrees, but stiffness of the joint can occur. In some situations, manipulation of the knee under anesthetic is used to reduce post operative stiffness. There are also many implants from manufacturers that are designed to be \"high-flex\" knees, offering a greater range of motion.\n\nIn some patients, the kneecap is unrevertable post-surgery and dislocates to the outer side of the knee. This is painful and usually needs to be treated by surgery to realign the kneecap. However this is quite rare.\n\nIn the past, there was a considerable risk of the implant components loosening over time as a result of wear. As medical technology has improved however, this risk has fallen considerably.\n\nThe current classification of AAOS divides prosthetic infections into four types.\n\nWhile it is relatively rare, periprosthetic infection remains one of the most challenging complications of joint arthroplasty.\nA detailed clinical history and physical remain the most reliable tool to recognize a potential periprosthetic infection. In some cases the classic signs of fever, chills, painful joint, and a draining sinus may be present, and diagnostic studies are simply done to confirm the diagnosis. In reality though, most patients do not present with those clinical signs, and in fact the clinical presentation may overlap with other complications such as aseptic loosening and pain. In those cases diagnostic tests can be useful in confirming or excluding infection.\n\nModern diagnosis of infection around a total knee replacement is based on the Musculoskeletal Infection Society (MSIS) criteria. They are:\n\n1.There is a sinus tract communicating with the prosthesis; or\n2. A pathogen is isolated by culture from at least two separate tissue or fluid samples obtained from the affected prosthetic joint; \nor\n\nFour of the following six criteria exist:\n\n1.Elevated serum erythrocyte sedimentation rate (ESR>30mm/hr) and serum C-reactive protein (CRP>10 mg/L) concentration,\n\n2.Elevated synovial leukocyte count,\n\n3.Elevated synovial neutrophil percentage (PMN%),\n\n4.Presence of purulence in the affected joint,\n\n5.Isolation of a microorganism in one culture of periprosthetic tissue or fluid, or\n\n6.Greater than five neutrophils per high-power field in five high-power fields observed from histologic analysis of periprosthetic tissue at ×400 magnification.\n\nNone of the above laboratory tests has 100% sensitivity or specificity for diagnosing infection. Specificity improves when the tests are performed in patients in whom clinical suspicion exists. ESR and CRP remain good 1st line tests for screening (high sensitivity, low specificity). Aspiration of the joint remains the test with the highest specificity for confirming infection.\n\nThe choice of treatment depends on the type of prosthetic infection.\n\n\nAppropriate antibiotic doses can be found at the following instructional course lecture by AAOS \n\nTo indicate knee replacement in case of osteoarthritis, its radiographic classification and severity of symptoms should both be substantial. Such radiography should consist of weightbearing X-rays of both knees- AP, Lateral, and 30 degrees of flexion. AP and lateral views may not show joint space narrowing, but the 30 degree flexion view is most sensitive for narrowing. Full length projections are also used in order to adjust the prosthesis to provide a neutral angle for the distal lower extremity. Two angles used for this purpose are:\n\nThe patient is to perform range of motion exercises and hip, knee and ankle strengthening as directed daily. Before the surgery is performed, pre-operative tests are done: usually a complete blood count, electrolytes, APTT and PT to measure blood clotting, chest X-rays, ECG, and blood cross-matching for possible transfusion. About a month before the surgery, the patient may be prescribed supplemental iron to boost the hemoglobin in their blood system. Accurate X-rays of the affected knee are needed to measure the size of components which will be needed. Medications such as warfarin and aspirin will be stopped some days before surgery to reduce the amount of bleeding. Patients may be admitted on the day of surgery if the pre-op work-up is done in the pre-anesthetic clinic or may come into hospital one or more days before surgery. Currently there is insufficient quality evidence to support the use of pre-operative physiotherapy in older adults undergoing total knee arthroplasty.\n\nPreoperative education is currently an important part of patient care. There is some evidence that it may slightly reduce anxiety before knee replacement surgery, with low risk of detrimental effects.\n\nWeight loss surgery before a knee replacement does not appear to change outcomes.\n\nThe surgery involves exposure of the front of the knee, with detachment of part of the quadriceps muscle (vastus medialis) from the patella. The patella is displaced to one side of the joint, allowing exposure of the distal end of the femur and the proximal end of the tibia. The ends of these bones are then accurately cut to shape using cutting guides oriented to the long axis of the bones. The cartilages and the anterior cruciate ligament are removed; the posterior cruciate ligament may also be removed but the tibial and fibular collateral ligaments are preserved. Whether the posterior cruciate ligament is removed or preserved depends on the type of implant used, although there appears to be no clear difference in knee function or range of motion favouring either approach. Metal components are then impacted onto the bone or fixed using polymethylmethacrylate (PMMA) cement. Alternative techniques exist that affix the implant without cement. These cement-less techniques may involve osseointegration, including porous metal prostheses.\n\nA round ended implant is used for the femur, mimicking the natural shape of the joint. On the tibia the component is flat, although it sometimes has a stem which goes down inside the bone for further stability. A flattened or slightly dished high density polyethylene surface is then inserted onto the tibial component so that the weight is transferred metal to plastic not metal to metal. During the operation any deformities must be corrected, and the ligaments balanced so that the knee has a good range of movement and is stable and aligned. In some cases the articular surface of the patella is also removed and replaced by a polyethylene button cemented to the posterior surface of the patella. In other cases, the patella is replaced unaltered.\n\nThe regional analgesia techniques (neuraxial anesthesia or continuous femoral nerve block or adductor canal block) are used most commonly. Local anesthesia infiltration in the pericapsular area using liposomal bupivacaine provides good analgesia in the post-operative period without increasing the risk for instability or nerve injury. A combined approach of local infiltration analagesia and femoral nerve block to achieve multimodal analagesia is common.\n\nDifferent implant manufacturers require slightly different instrumentation and technique. No consensus has emerged over which one is the best. Clinical studies are very difficult to perform, requiring large numbers of cases followed over many years. The most significant variations are between cemented and uncemented components and between resurfacing the patella or not. Among those who do not resurface the patella, there is also variation between denervating the patella using electrocautery or not. In theory, this technique could disrupt the superficial pain receptors near the patella in hopes of relieving anterior knee pain, a common post-operative complaint. No consensus exists, but a recent randomized controlled trial indicates that while both methods provide relief, patellar denervation results in a modest benefit compared to no denervation in the short-term. The patient satisfaction was higher with more number of patients rating the procedure as excellent in the denervation group (74.6% vs 50.8%). The benefit does not, however, persist mid- to long-term post-operatively. Anterior knee pain component within the patellar score and Visual Analogue Scale for anterior knee pain were significantly better in the denervation group at 3 months (4.5 vs 5.1) but not at 12 months (4.4 vs 4.9) and 24 months (2.1 vs. 2.2).\n\nSome also study patient satisfaction data associated with pain. Retaining the posterior cruciate ligament (PCL) has been shown to be beneficial for patients. Removal of the PCL has been shown to reduce the maximal force that the individual can place on that knee. Typically individuals who have the PCL removed will lean forward while climbing in order to maximize the force of the quadriceps. A variation in the total knee replacement procedure is to permit movement in the prostheses using a polyethylene insert, an approach called a mobile bearing total knee arthroplasty. There is no strong evidence that this approach improves a persons knee function, mortality, number of adverse effects, or amount of pain compared to a fixed bearing approach for total knee replacement that retains the posterior cruciate ligament.\n\nMinimally invasive procedures have been developed in total knee replacement (TKR) that do not cut the quadriceps tendon. There are different definitions of minimally invasive knee surgery, which may include a shorter incision length, retraction of the patella (kneecap) without eversion (rotating out), and specialized instruments. There are few randomized trials, but studies have found less postoperative pain, shorter hospital stays, and shorter recovery. However, no studies have shown long-term benefits.\n\nIn 2015 The OGAAP team from Sydney Australia led by Dr Al Muderis presented a revolutionary technology for the first time enabling the use of knee replacement in combination with percutaneous bone anchoring device enabling amputees with short residual tibia and or knee joint arthritis to mobilise with ease. This technology provided a solution for individuals with amputation who are unable to wear a traditional socket prosthesis.\n\nAnother variation in the total knee replacement procedure is to permit movement in the prostheses using a polyethylene insert, an approach called a mobile bearing total knee arthroplasty. There is no strong evidence that this approach improves a persons function compared to the fixed bearing approach for total knee replacement that retains the cruciate ligament.\n\nThe mobile (meniscal or ro-\n\ntating) bearing TKA with a polyethylene insert has some freedom\n\nof movement and is an example of such a new development. The\n\nmain goal of the mobile bearing insert is to de crease contact stresses\n\nat the implant interface (\n\nMatsuda 1998; Szivek 1996). Contradic-\n\ntory views exist as to whether the mobile bearing prosthesis will\n\nimprove functionality as compared with the fixed bearing pros-\n\nthesis for cruciate retaining TKA.\n\nUnicompartmental arthroplasty (UKA), also called partial knee replacement, is an option for some patients. The knee is generally divided into three \"compartments\": medial (the inside part of the knee), lateral (the outside), and patellofemoral (the joint between the kneecap and the thighbone). Most patients with arthritis severe enough to consider knee replacement have significant wear in two or more of the above compartments and are best treated with total knee replacement. A minority of patients (the exact percentage is hotly debated but is probably between 10 and 30 percent) have wear confined primarily to one compartment, usually the medial, and may be candidates for unicompartmental knee replacement. Advantages of UKA compared to TKA include smaller incision, easier post-op rehabilitation, better post-operative range of motion, shorter hospital stay, less blood loss, lower risk of infection, stiffness, and blood clots, but a harder revision if necessary. While most recent data suggests that UKA in properly selected patients has survival rates comparable to TKA, most surgeons believe that TKA is the more reliable long term procedure. Persons with infectious or inflammatory arthritis (Rheumatoid, Lupus, Psoriatic), or marked deformity are not candidates for this procedure.\n\nKnee replacement is routinely evaluated by X-ray, including the following measures:\n\nThe length of post-operative hospitalization is 5 days on average depending on the health status of the patient and the amount of support available outside the hospital setting. Protected weight bearing on crutches or a walker is required until specified by the surgeon because of weakness in the quadriceps muscle\n\nTo increase the likelihood of a good outcome after surgery, multiple weeks of physical therapy is necessary. In these weeks, the therapist will help the patient return to normal activities, as well as prevent blood clots, improve circulation, increase range of motion, and eventually strengthen the surrounding muscles through specific exercises. Whether techniques such as neuromuscular electrical stimulation are effective at promoting gains in knee muscle strength after surgery are unclear.\nOften range of motion (to the limits of the prosthesis) is recovered over the first two weeks (the earlier the better). Over time, patients are able to increase the amount of weight bearing on the operated leg, and eventually are able to tolerate full weight bearing with the guidance of the physical therapist. After about ten months, the patient should be able to return to normal daily activities, although the operated leg may be significantly weaker than the non-operated leg.\n\nFor post-operative knee replacement patients, immobility is a factor precipitated by pain and other complications. Mobility is known as an important aspect of human biology that has many beneficial effects on the body system. It is well documented in literature that physical immobility affects every body system and contributes to functional complications of prolonged illness. In most medical-surgical hospital units that perform knee replacements, ambulation is a key aspect of nursing care that is promoted to patients. Early ambulation can decrease the risk of complications associated with immobilization such as pressure ulcers, deep vein thrombosis (DVT), impaired pulmonary function, and loss of functional mobility. Nurses’ promotion and execution of early ambulation on patients has found that it greatly reduces the complications listed above, as well as decreases length of stay and costs associated with further hospitalization. Nurses may also work with teams such as physical therapy and occupational therapy to accomplish ambulation goals and reduce complications.\n\nContinuous passive motion (CPM) is a postoperative therapy approach that uses a machine to move the knee continuously through a specific range of motion, with the goal of preventing joint stiffness and improving recovery. There is no evidence that CPM therapy leads to a clinically significant improvement in range of motion, pain, knee function, or quality of life. CPM is inexpensive, convenient, and assists patients in therapeutic compliance. However, CPM should be used in conjunction with traditional physical therapy. In unusual cases where the person has a problem which prevents standard mobilization treatment, then CPM may be useful.\n\nCryotherapy, also known as 'cold therapy' is sometimes recommended after surgery for pain relief and to limit swelling of the knee. Cryotherapy involves the application of ice bags or cooled water to the skin of the knee joint. However, the evidence that cryotherapy reduces pain and swelling is very weak and the benefits after total knee replacement surgery have been shown to be very small.\n\nSome physicians and patients may consider having ultrasonography for deep venous thrombosis after knee replacement. However, this kind of screening should be done only when indicated because to perform it routinely would be unnecessary health care. If a medical condition exists that could cause deep vein thrombosis, a physician can choose to treat patients with cryotherapy and intermittent pneumatic compression as a preventive measure.\n\nNeither gabapentin nor pregabalin have been found to be useful for pain following a knee replacement.\n\nWith 718,000 hospitalizations, knee arthroplasty accounted for 4.6% of all United States operating room procedures in 2011—making it one of the most common procedures performed during hospital stays. The number of knee arthroplasty procedures performed in U.S. hospitals increased 93% between 2001 and 2011. A study of United States community hospitals showed that in 2012, among hospitalizations that involved an OR procedure, knee arthroplasty was the OR procedure performed most frequently during hospital stays paid by Medicare (10.8 percent of stays) and by private insurance (9.1 percent). Knee arthroplasty was not among the top five most frequently performed OR procedures for stays paid by Medicaid or for uninsured stays.\n\nBy 2030, the demand for primary total knee arthroplasty is projected to increase to 3.48 million surgeries performed annually in the U.S.\n\n\n"}
{"id": "41326", "url": "https://en.wikipedia.org/wiki?curid=41326", "title": "Loading coil", "text": "Loading coil\n\nA loading coil or load coil is an inductor that is inserted into an electronic circuit to increase its inductance. The term originated in the 19th century for inductors used to prevent signal distortion in long-distance telegraph transmission cables. The term is also used for inductors in radio antennas, or between the antenna and its feedline, to make an electrically short antenna resonant at its operating frequency.\n\nThe concept of loading coils was discovered by Oliver Heaviside in studying the problem of slow signalling speed of the first transatlantic telegraph cable in the 1860s. He concluded additional inductance was required to prevent amplitude and time delay distortion of the transmitted signal. The mathematical condition for distortion-free transmission is known as the Heaviside condition. Previous telegraph lines were overland or shorter and hence had less delay, and the need for extra inductance was not as great. Submarine communications cables are particularly subject to the problem, but early 20th century installations using balanced pairs were often continuously loaded with iron wire or tape rather than discretely with loading coils, which avoided the sealing problem.\n\nLoading coils are historically also known as Pupin coils after Mihajlo Pupin, especially when used for the Heaviside condition and the process of inserting them is sometimes called \"pupinization\".\n\nA common application of loading coils is to improve the voice-frequency amplitude response characteristics of the twisted balanced pairs in a telephone cable. Because twisted pair is a balanced format, half the loading coil must be inserted in each leg of the pair to maintain the balance. It is common for both these windings to be formed on the same core. This increases the flux linkages, without which the number of turns on the coil would need to be increased. Despite the use of common cores, such loading coils do not comprise transformers, as they do not provide coupling to other circuits.\n\nLoading coils inserted periodically in series with a pair of wires reduce the attenuation at the higher voice frequencies up to the cutoff frequency of the low-pass filter formed by the inductance of the coils (plus the distributed inductance of the wires) and the distributed capacitance between the wires. Above the cutoff frequency, attenuation increases rapidly. The shorter the distance between the coils, the higher the cut-off frequency.\n\nIt should be emphasised that the cutoff effect is an artifact of using lumped inductors. With loading methods using continuous distributed inductance there is no cutoff.\n\nWithout loading coils, the line response is dominated by the resistance and capacitance of the line with the attenuation gently increasing with frequency. With loading coils of exactly the right inductance, neither capacitance nor inductance dominate: the response is flat, waveforms are undistorted and the characteristic impedance is resistive up to the cutoff frequency. The coincidental formation of an audio frequency filter is also beneficial in that noise is reduced.\n\nWhen loading coils are in place, signal attenuation remains low for signals within the passband of the transmission line but increases rapidly for frequencies above the audio cutoff frequency. Thus, if the pair is subsequently reused to support applications that require higher frequencies (such as analog or digital carrier systems or DSL), any loading coils that were present on the line must be removed or replaced with ones which are transparent to DSL. Using coils with parallel capacitors will form a filter with the topology of an m-derived filter and a band of frequencies above the cut-off will also be passed.\n\nIf the coils are not removed, and the subscriber is an extended distance (e.g. over 4 miles or 6.4 km) from the Central Office, DSL can not be supported. This sometimes happens in dense, growing areas such as Southern California in the late 1990s and early 21st century.\n\nAmerican early and middle 20th century telephone cables had load coils at intervals of a mile (1.61 km), usually in coil cases holding many. The coils had to be removed to pass higher frequencies, but the coil cases provided convenient places for repeaters of digital T-carrier systems, which could then transmit a 1.5 Mbit/s signal that distance. Due to narrower streets and higher cost of copper, European cables had thinner wires and used closer spacing. Intervals of a kilometer allowed European systems to carry 2 Mbit/s.\n\nAnother type of loading coil is used in radio antennas. Monopole and dipole radio antennas are designed to act as resonators for radio waves; the power from the transmitter, applied to the antenna through the antenna's transmission line, excites standing waves of voltage and current in the antenna element. To be resonant, the antenna must have a physical length of one quarter of the wavelength of the radio waves used (or a multiple of that length). At resonance the antenna acts electrically as a pure resistance, absorbing all the power applied to it from the transmitter.\n\nIn many cases for practical reasons it is necessary to make the antenna shorter than the resonant length, this is called an electrically short antenna. An antenna shorter than a quarter wavelength presents capacitive reactance to the transmission line. Some of the applied power is reflected back into the transmission line and travels back toward the transmitter (called “\"backlash current\"”). The two currents at the same frequency running in opposite directions causes standing waves on the transmission line (measured as a standing wave ratio (SWR) greater than one) which waste energy, and can even overheat the transmitter.\n\nSo to make an electrically short antenna resonant, a loading coil is inserted in series with the antenna. The inductive reactance of the coil is equal and opposite to, and cancels, the capacitive reactance of the antenna, so the loaded antenna presents a pure resistance to the transmission line, preventing energy from being reflected. The loading coil is often placed at the base of the antenna, between it and the transmission line (\"base loading\"), but for better efficiency, it is sometimes inserted in the center of the antenna element itself (\"center loading\").\n\nLoading coils for powerful transmitters can have challenging design requirements, especially at low frequencies. Since the radiation resistance of short antennas can be very low, as low a few ohms in the LF or VLF bands, the coil must have extremely low AC resistance at the operating frequency. To reduce skin effect losses the coil is often made of tubing or Litz wire, with single layer windings with turns spaced apart to reduce proximity effect and arcing. They must often handle high voltages. To reduce power lost in dielectric losses, the coil is often suspended in air supported on thin ceramic strips. The capacitively loaded antennas used at low frequencies have extremely narrow bandwidths, and therefore if the frequency is changed the loading coil must be adjustable to tune the antenna to resonance with the new transmitter frequency. Variometers are often used.\n\nThe Campbell equation is a relationship due to George Ashley Campbell for predicting the propagation constant of a loaded line. It is stated as;\n\nA more engineer friendly rule of thumb is that the approximate requirement for spacing loading coils is ten coils per wavelength of the maximum frequency being transmitted. This approximation can be arrived at by treating the loaded line as a constant k filter and applying image filter theory to it. From basic image filter theory the angular cutoff frequency and the characteristic impedance of a low-pass constant k filter are given by;\n\nFrom these basic equations the necessary loading coil inductance and coil spacing can be found;\n\nExpressing this in terms of number of coils per cutoff wavelength yields;\n\nSince formula_14 then\n\nCampbell arrived at this expression by analogy with a mechanical line periodically loaded with weights described by Charles Godfrey in 1898 who obtained a similar result. Mechanical loaded lines of this sort were first studied by Joseph-Louis Lagrange (1736–1813).\n\nThe phenomenon of cutoff whereby frequencies above the cutoff frequency are not transmitted is an undesirable side effect of loading coils (although it proved highly useful in the development of filters). Cutoff is avoided by the use of continuous loading since it arises from the lumped nature of the loading coils.\n\nThe origin of the loading coil can be found in the work of Oliver Heaviside on the theory of transmission lines. Heaviside (1881) represented the line as a network of infinitesimally small circuit elements. By applying his operational calculus to the analysis of this network he discovered (1887) what has become known as the Heaviside condition. This is the condition that must be fulfilled in order for a transmission line to be free from distortion. The Heaviside condition is that the series impedance, Z, must be proportional to the shunt admittance, Y, at all frequencies. In terms of the primary line coefficients the condition is:\n\nHeaviside was aware that this condition was not met in the practical telegraph cables in use in his day. In general, a real cable would have,\n\nThis is mainly due to the low value of leakage through the cable insulator, which is even more pronounced in modern cables which have better insulators than in Heaviside's day. In order to meet the condition, the choices are therefore to try to increase G or L or to decrease R or C. Decreasing R requires larger conductors. Copper was already in use in telegraph cables and this is the very best conductor available short of using silver. Decreasing R means using more copper and a more expensive cable. Decreasing C would also mean a larger cable (although not necessarily more copper). Increasing G is highly undesirable; while it would reduce distortion, it would at the same time increase the signal loss. Heaviside considered, but rejected, this possibility which left him with the strategy of increasing L as the way to reduce distortion.\n\nHeaviside immediately (1887) proposed several methods of increasing the inductance, including spacing the conductors further apart and loading the insulator with iron dust. Finally, Heaviside made the proposal (1893) to use discrete inductors at intervals along the line. However, he never succeeded in persuading the British GPO to take up the idea. Brittain attributes this to Heaviside's failure to provide engineering details on the size and spacing of the coils for particular cable parameters. Heaviside's eccentric character and setting himself apart from the establishment may also have played a part in their ignoring of him.\n\nJohn S. Stone worked for the American Telephone & Telegraph Company (AT&T) and was the first to attempt to apply Heaviside's ideas to real telecommunications. Stone's idea (1896) was to use a bimetallic iron-copper cable which he had patented. This cable of Stone's would increase the line inductance due to the iron content and had the potential to meet the Heaviside condition. However, Stone left the company in 1899 and the idea was never implemented. Stone's cable was an example of continuous loading, a principle that was eventually put into practice is other forms, see for instance Krarup cable later in this article.\n\nGeorge Campbell was another AT&T engineer working in their Boston facility. Campbell was tasked with continuing the investigation into Stone's bimetallic cable, but soon abandoned it in favour of the loading coil. His was an independent discovery, Campbell was aware of Heaviside's work in discovering the Heaviside condition, but unaware of Heaviside's suggestion of using loading coils to enable a line to meet it. The motivation for the change of direction was Campbell's limited budget.\n\nCampbell was struggling to set up a practical demonstration over a real telephone route with the budget he had been allocated. After considering that his artificial line simulators used lumped components rather than the distributed quantities found in a real line, he wondered if he could not insert the inductance with lumped components instead of using Stone's distributed line. When his calculations showed that the manholes on telephone routes were sufficiently close together to be able to insert the loading coils without the expense of either having to dig up the route or lay in new cables he changed to this new plan. The very first demonstration of loading coils on a telephone cable was on a 46-mile length of the so-called Pittsburgh cable (the test was actually in Boston, the cable had previously been used for testing in Pittsburgh) on 6 September 1899 carried out by Campbell himself and his assistant. The first telephone cable using loaded lines put into public service was between Jamaica Plain and West Newton in Boston on 18 May 1900.\n\nCampbell's work on loading coils provided the theoretical basis for his subsequent work on filters which proved to be so important for frequency-division multiplexing. The cut-off phenomena of loading coils, an undesirable side-effect, can be exploited to produce a desirable filter frequency response.\n\nMichael Pupin, inventor and Serbian immigrant to the USA, also played a part in the story of loading coils. Pupin filed a rival patent to the one of Campbell's. This patent of Pupin's dates from 1899. There is an earlier patent (1894, filed December 1893) which is sometimes cited as Pupin's loading coil patent but is, in fact, something different. The confusion is easy to understand, Pupin himself claims that he first thought of the idea of loading coils while climbing a mountain in 1894, although there is nothing from him published at that time.\n\nPupin's 1894 patent \"loads\" the line with capacitors rather than inductors, a scheme that has been criticised as being theoretically flawed and never put into practice. To add to the confusion, one variant of the capacitor scheme proposed by Pupin does indeed have coils. However, these are not intended to compensate the line in any way. They are there merely to restore DC continuity to the line so that it may be tested with regular equipment. Pupin states that the inductance is to be so large that it will block all AC signals above 50 Hz. Consequently, only the capacitor is adding any significant impedance to the line and \"the coils will not exercise any material influence on the results before noted\".\n\nHeaviside never patented his idea; indeed, he took no commercial advantage of any of his work. Despite the legal disputes surrounding this invention, it is unquestionable that Campbell was the first to actually construct a telephone circuit using loading coils. There also can be little doubt that Heaviside was the first to publish and many would dispute Pupin's priority.\n\nAT&T fought a legal battle with Pupin over his claim. Pupin was first to patent but Campbell had already conducted practical demonstrations before Pupin had even filed his patent (December 1899). Campbell's delay in filing was due to the slow internal machinations of AT&T.\n\nHowever, AT&T foolishly deleted from Campbell's proposed patent application all the tables and graphs detailing the exact value of inductance that would be required before the patent was submitted. Since Pupin's patent contained a (less accurate) formula, AT&T was open to claims of incomplete disclosure. Fearing that there was a risk that the battle would end with the invention being declared unpatentable due to Heaviside's prior publication, they decided to desist from the challenge and buy an option on Pupin's patent for a yearly fee so that AT&T would control both patents. By January 1901 Pupin had been paid $200,000 ($13 million in 2011) and by 1917, when the AT&T monopoly ended and payments ceased, he had received a total of $455,000 ($25 million in 2011).\n\nThe invention was of enormous value to AT&T. Telephone cables could now be used to twice the distance previously possible, or alternatively, a cable of half the previous quality (and cost) could be used over the same distance. When considering whether to allow Campbell to go ahead with the demonstration, their engineers had estimated that they stood to save $700,000 in new installation costs in New York and New Jersey alone. It has been estimated that AT&T saved $100 million in the first quarter of the 20th century. Heaviside, who began it all, came away with nothing. He was offered a token payment but would not accept, wanting the credit for his work. He remarked ironically that if his prior publication had been admitted it would \"interfere ... with the flow of dollars in the proper direction ...\".\n\nLoading coils were not without their problems. In heavy submarine cables, loading coils were difficult to lay. Discontinuities where the coils were installed caused stresses in the cable during laying. Without great care, the cable might part and would be difficult to repair. A second problem was that the material science of the time had difficulties sealing the joint between coil and cable against ingress of seawater. When this occurred the cable was ruined.\n\nA Danish engineer, Carl Emil Krarup, invented a form of continuously loaded cable which solved these problems and the cable is named for him. Krarup cable has iron wires continuously wound around the central copper conductor with adjacent turns in contact with each other. This cable was the first use of continuous loading on any telecommunication cable. In 1902, Krarup both wrote his paper on this subject and saw the installation of the first cable between Helsingør (Denmark) and Helsingborg (Sweden).\n\nEven though the Krarup cable added inductance to the line, this was insufficient to meet the Heaviside condition. AT&T searched for a better material with higher magnetic permeability. In 1914, Gustav Elmen discovered permalloy, a magnetic nickel-iron annealed alloy. In c. 1915, Oliver E. Buckley, H. D. Arnold, and Elmen, all at Bell Labs, greatly improved transmission speeds by suggesting a method of constructing submarine communications cable using permalloy tape wrapped around the copper conductors.\n\nThe cable was tested in a trial in Bermuda in 1923. The first permalloy cable placed in service connected New York City and Horta (Azores) in September 1924. Permalloy cable enabled signalling speed on submarine telegraph cables to be increased to 400 words/min at a time when 40 words/min was considered good. The first transatlantic cable achieved only two words/min.\n\nMu-metal has similar magnetic properties to permalloy but the addition of copper to the alloy increases the ductility and allows the metal to be drawn into wire. Mu-metal cable is easier to construct than permalloy cable, the mu-metal being wound around the core copper conductor in much the same way as the iron wire in Krarup cable. A further advantage with mu-metal cable is that the construction lends itself to a variable loading profile whereby the loading is tapered towards the ends.\n\nMu-metal was invented in 1923 by The Telegraph Construction and Maintenance Company Ltd., London, who made the cable, initially, for the Western Union Telegraph Co. Western Union were in competition with AT&T and the Western Electric Company who were using permalloy. The patent for permalloy was held by Western Electric which prevented Western Union from using it.\nContinuous loading of cables is expensive and hence is only done when absolutely necessary. Lumped loading with coils is cheaper but has the disadvantages of difficult seals and a definite cutoff frequency. A compromise scheme is patch loading whereby the cable is continuously loaded in repeated sections. The intervening sections are left unloaded.\n\nLoaded cable is no longer a useful technology for submarine communication cables, having first been superseded by co-axial cable using electrically powered in-line repeaters and then by fibre-optic cable. Manufacture of loaded cable declined in the 1930s and was then superseded by other technologies post-war. Loading coils can still be found in some telephone landlines today but new installations use more modern technology.\n\n\n\n"}
{"id": "18254411", "url": "https://en.wikipedia.org/wiki?curid=18254411", "title": "MHealth", "text": "MHealth\n\nmHealth (also written as m-health) is an abbreviation for mobile health, a term used for the practice of medicine and public health supported by mobile devices.\nThe term is most commonly used in reference to using mobile communication devices, such as mobile phones, tablet computers and PDAs, and wearable devices such as smart watches, for health services, information, and data collection. The mHealth field has emerged as a sub-segment of eHealth, the use of information and communication technology (ICT), such as computers, mobile phones, communications satellite, patient monitors, etc., for health services and information. mHealth applications include the use of mobile devices in collecting community and clinical health data, delivery of healthcare information to practitioners, researchers, and patients, real-time monitoring of patient vital signs, and direct provision of care (via mobile telemedicine).\n\nWhile mHealth certainly has application for industrialized nations, the field has emerged in recent years as largely an application for developing countries, stemming from the rapid rise of mobile phone penetration in low-income nations. The field, then, largely emerges as a means of providing greater access to larger segments of a population in developing countries, as well as improving the capacity of health systems in such countries to provide quality healthcare.\nWithin the mHealth space, projects operate with a variety of objectives, including increased access to healthcare and health-related information (particularly for hard-to-reach populations); improved ability to diagnose and track diseases; timelier, more actionable public health information; and expanded access to ongoing medical education and training for health workers.\n\nAccording to an analyst firm, around 2.8 million patients worldwide were using a home monitoring service based on equipment with integrated connectivity at the end of 2012. The figure does not include patients that use monitoring devices connected to a PC or mobile phone. It only includes systems that rely on monitors with integrated connectivity or systems that use monitoring hubs with integrated cellular or fixed-line modems. It forecast that the number of home monitoring systems with integrated communication capabilities will grow at a compound annual growth rate (CAGR) of 26.9 percent between 2011 and 2017 reaching 9.4 million connections globally by the end of the forecast period. The number of these devices that have integrated cellular connectivity increased from 0.73 million in 2011 to about 1.03 million in 2012, and is projected to grow at a CAGR of 46.3 percent to 7.10 million in 2017.\n\nA growing percentage of health-related smartphone apps are available, and some estimates predicted that 500 million patients would be using such apps by the year 2015.\n\nThere are concerns about the accuracy and unregulated status of health apps.\n\nmHealth broadly encompasses the use of mobile telecommunication and multimedia technologies as they are integrated within increasingly mobile and wireless health care delivery systems. The field broadly encompasses the use of mobile telecommunication and multimedia technologies in health care delivery. The term mHealth was coined by Robert Istepanian as use of \"emerging mobile communications and network technologies for healthcare\".\nA definition used at the 2010 mHealth Summit of the Foundation for the National Institutes of Health (FNIH) was \"the delivery of healthcare services via mobile communication devices\".\n\nWhile there are some projects that are considered solely within the field of mHealth, the linkage between mHealth and eHealth is unquestionable. For example, an mHealth project that uses mobile phones to access data on HIV/AIDS rates would require an eHealth system in order to manage, store, and assess the data. Thus, eHealth projects many times operate as the backbone of mHealth projects.\n\nIn a similar vein, while not clearly bifurcated by such a definition, eHealth can largely be viewed as technology that supports the functions and delivery of healthcare, while mHealth rests largely on providing healthcare access. Because mHealth is by definition based on mobile technology such as smartphones, healthcare, through information and delivery, can better reach areas, people, and/or healthcare practitioners with previously limited exposure to certain aspects of healthcare.\n\nmHealth is one aspect of eHealth that is pushing the limits of how to acquire, transport, store, process, and secure the raw and processed data to deliver meaningful results. mHealth offers the ability of remote individuals to participate in the health care value matrix, which may not have been possible in the past. Participation does not imply just consumption of health care services. In many cases remote users are valuable contributors to gather data regarding disease and public health concerns such as outdoor pollution, drugs and violence.\n\nThe motivation behind the development of the mHealth field arises from two factors. The first factor concerns the myriad constraints felt by healthcare systems of developing nations. These constraints include high population growth, a high burden of disease prevalence, low health care workforce, large numbers of rural inhabitants, and limited financial resources to support healthcare infrastructure and health information systems. The second factor is the recent rapid rise in mobile phone penetration in developing countries to large segments of the healthcare workforce, as well as the population of a country as a whole. With greater access to mobile phones to all segments of a country, including rural areas, the potential of lowering information and transaction costs in order to deliver healthcare improves.\n\nThe combination of these two factors has motivated much discussion of how greater access to mobile phone technology can be leveraged to mitigate the numerous pressures faced by developing countries' healthcare systems. Both factors are discussed here.\n\nMiddle income and especially low-income countries face a plethora of constraints in their healthcare systems. These countries face a severe lack of human and physical resources, as well as some of the largest burdens of disease, extreme poverty, and large population growth rates. Additionally, healthcare access to all reaches of society is generally low in these countries. \n\nAccording to a World Health Organization (WHO) report from June 2011, higher-income countries show more mHealth activity than do lower-income countries (as consistent with eHealth trends in general). Countries in the European Region are currently the most active and those in the African Region the least active. The WHO report findings also included that mHealth is most easily incorporated into processes and services that historically use voice communication through conventional telephone networks. The report was the result of a mHealth survey module designed by researchers at the Earth Institute's Center for Global Health and Economic Development, Columbia University.\n\nThe WHO notes an extreme deficit within the global healthcare workforce. The WHO notes critical healthcare workforce shortages in 57 countries—most of which are characterized as developing countries—and a global deficit of 2.4 million doctors, nurses, and midwives. The WHO, in a study of the healthcare workforce in 12 countries of Africa, finds an average density of physicians, nurses and midwives per 1000 population of 0.64. The density of the same metric is four times as high in the United States, at 2.6.\n\nThe burden of disease is additionally much higher in low- and middle-income countries than high-income countries. The burden of disease, measured in disability-adjusted life year (DALY), which can be thought of as a measurement of the gap between current health status and an ideal situation where everyone lives into old age, free of disease and disability, is about five times higher in Africa than in high-income countries. In addition, low- and middle-income countries are forced to face the burdens of both extreme poverty and the growing incidence of chronic diseases, such as diabetes and heart disease, an effect of new-found (relative) affluence.\n\nConsidering poor infrastructure and low human resources, the WHO notes that the healthcare workforce in sub-Saharan Africa would need to be scaled up by as much as 140% to attain international health development targets such as those in the Millennium Declaration.\n\nThe WHO, in reference to the healthcare condition in sub-saharan Africa, states:\n\nThe problem is so serious that in many instances there is simply not enough human capacity even to absorb, deploy and efficiently use the substantial additional funds that are considered necessary to improve health in these countries.\nMobile technology has made a recent and rapid appearance into low- and middle-income nations. While, in the mHealth field, mobile technology usually refers to mobile phone technology, the entrance of other technologies into these nations to facilitate healthcare are also discussed here.\n\nThe link between health and development can be found in three of the Millennium Development Goals (MDGs), as set forth by the United Nations Millennium Declaration in 2000. The MDGs that specifically address health include reducing child mortality; improving maternal health; combating HIV and AIDS, malaria, and other diseases; and increasing access to safe drinking water. A progress report published in 2006 indicates that childhood immunization and deliveries by skilled birth attendants are on the rise, while many regions continue to struggle to achieve reductions in the prevalence of the diseases of poverty including malaria, HIV and AIDS and tuberculosis.\n\nIn developed countries, healthcare systems have different policies and goals in relation to the personal and population health care goals.\n\nIn US and EU many patients and consumers use their cell phones and tablets to access health information and look for healthcare services. In parallel the number of mHealth applications grew significantly the last years.\n\nDoctors, nurses and clinicians use mobile devices to access patient information and other databases and resources.\n\nBasic SMS functions and real-time voice communication serve as the backbone and the current most common use of mobile phone technology. The broad range of potential benefits to the health sector that the simple functions of mobile phones can provide should not be understated.\n\nThe appeal of mobile communication technologies is that they enable communication in motion, allowing individuals to contact each other irrespective of time and place. This is particularly beneficial for work in remote areas where the mobile phone, and now increasingly wireless infrastructure, is able to reach more people, faster. As a result of such technological advances, the capacity for improved access to information and two-way communication becomes more available at the point of need.\n\nWith the global mobile phone penetration rate drastically increasing over the last decade, mobile phones have made a recent and rapid entrance into many parts of the low- and middle-income world. Improvements in telecommunications technology infrastructure, reduced costs of mobile handsets, and a general increase in non-food expenditure have influenced this trend. Low- and middle-income countries are utilizing mobile phones as \"leapfrog technology\" (see leapfrogging). That is, mobile phones have allowed many developing countries, even those with relatively poor infrastructure, to bypass 20th century fixed-line technology and jump to modern mobile technology.\n\nThe number of global mobile phone subscribers in 2007 was estimated at 3.1 billion of an estimated global population of 6.6 billion (47%). These figures are expected to grow to 4.5 billion by 2012, or a 64.7% mobile penetration rate. The greatest growth is expected in Asia, the Middle East, and Africa. In many countries, the number of mobile phone subscribers has bypassed the number of fixed-line telephones; this is particularly true in developing countries. Globally, there were 4.1 billion mobile phones in use in December 2008. See List of countries by number of mobile phones in use.\n\nWhile mobile phone penetration rates are on the rise, globally, the growth within countries is not generally evenly distributed. In India, for example, while mobile penetration rates have increased markedly, by far the greatest growth rates are found in urban areas. Mobile penetration, in September 2008, was 66% in urban areas, while only 9.4% in rural areas. The all India average was 28.2% at the same time. So, while mobile phones may have the potential to provide greater healthcare access to a larger portion of a population, there are certainly within-country equity issues to consider.\n\nMobile phones are spreading because the cost of mobile technology deployment is dropping and people are, on average, getting wealthier in low- and middle-income nations. Vendors, such as Nokia, are developing cheaper infrastructure technologies (CDMA) and cheaper phones (sub $50–100, such as Sun's Java phone). Non-food consumption expenditure is increasing in many parts of the developing world, as disposable income rises, causing a rapid increase spending on new technology, such as mobile phones. In India, for example, consumers have become and continue to become wealthier. Consumers are shifting their expenditure from necessity to discretionary. For example, on average, 56% of Indian consumers' consumption went towards food in 1995, compared to 42% in 2005. The number is expected to drop to 34% by 2015. That being said, although total share of consumption has declined, total consumption of food and beverages increased 82% from 1985 to 2005, while per-capita consumption of food and beverages increased 24%. Indian consumers are getting wealthier and they are spending more and more, with a greater ability to spend on new technologies.\n\nMore advanced mobile phone technologies are enabling the potential for further healthcare delivery.\nSmartphone technologies are in now in the hands of a large number of physicians and other healthcare workers in low- and middle-income countries. Although far from ubiquitous, the spread of smartphone technologies opens up doors for mHealth projects such as technology-based diagnosis support, remote diagnostics and telemedicine, web browsing, GPS navigation, access to web-based patient information, post-visit patient surveillance, and decentralized health management information systems (HMIS).\n\nWhile uptake of smartphone technology by the medical field has grown in low- and middle-income countries, it is worth noting that the capabilities of mobile phones in low- and middle-income countries has not reached the sophistication of those in high-income countries. The infrastructure that enables web browsing, GPS navigation, and email through smartphones is not as well developed in much of the low- and middle-income countries. Increased availability and efficiency in both voice and data-transfer systems in addition to rapid deployment of wireless infrastructure will likely accelerate the deployment of mobile-enabled health systems and services throughout the world.\n\nBeyond mobile phones, wireless-enabled laptops and specialized health-related software applications are currently being developed, tested, and marketed for use in the mHealth field. Many of these technologies, while having some application to low- and middle-income nations, are developing primarily in high-income countries. However, with broad advocacy campaigns for free and open source software (FOSS), applications are beginning to be tailored for and make inroads in low- and middle-income countries.\n\nSome other mHealth technologies include:\n\n\nTechnologies relate to the operating systems that orchestrate mobile device hardware while maintaining confidentiality, integrity and availability are required to build trust. This may foster greater adoption of mHealth technologies and services, by exploiting lower cost multi purpose mobile devices such as tablets, PCs, and smartphones. Operating systems that control these emerging classes of devices include Google's Android, Apple's iPhone OS, Microsoft's Windows Mobile, and RIM's BlackBerry OS.\n\nOperating systems must be agile and evolve to effectively balance and deliver the desired level of service to an application and end user, while managing display real estate, power consumption and security posture. As advances in capabilities such as integrating voice, video and Web 2.0 collaboration tools into mobile devices, significant benefits can be achieved in the delivery of health care services. New sensor technologies such as HD video and audio capabilities, accelerometers, GPS, ambient light detectors, barometers and gyroscopes can enhance the methods of describing and studying cases, close to the patient or consumer of the health care service. This could include diagnosis, education, treatment and monitoring.\n\nEnvironmental conditions have a significant impact to public health. Per the World Health Organization, outdoor air pollution accounts for about 1.4% of total mortality. Utilizing Participatory sensing technologies in mobile telephone, public health research can exploit the wide penetration of mobile devices to collect air measurements, which can be utilized to assess the impact of pollution. Projects such as the Urban Atmospheres are utilizing embedded technologies in mobile phones to acquire real time conditions from millions of user mobile phones. By aggregating this data, public health policy shall be able to craft initiatives to mitigate risk associated with outdoor air pollution.\n\nData has become an especially important aspect of mHealth. Data collection requires both the collection device (mobile phones, computer, or portable device) and the software that houses the information. Data is primarily focused on visualizing static text but can also extend to interactive decision support algorithms, other visual image information, and also communication capabilities through the integration of e-mail and SMS features. Integrating use of GIS and GPS with mobile technologies adds a geographical mapping component that is able to \"tag\" voice and data communication to a particular location or series of locations. These combined capabilities have been used for emergency health services as well as for disease surveillance, health facilities and services mapping, and other health-related data collection.\n\nWhile others exist, the 2009 UN Foundation and Vodafone Foundation report presents seven application categories within the mHealth field.\n\n\nEach application category as well as a specific project within the category will be described.\n\nEducation and awareness programs within the mHealth field are largely about the spreading of mass information from source to recipient through short message services (SMS). In education and awareness applications, SMS messages are sent directly to users' phones to offer information about various subjects, including testing and treatment methods, availability of health services, and disease management. SMSs provide an advantage of being relatively unobtrusive, offering patients confidentiality in environments where disease (especially HIV/AIDS) is often taboo. Additionally, SMSs provide an avenue to reach far-reaching areas—such as rural areas—which may have limited access to public health information and education, health clinics, and a deficit of healthcare workers.\n\nHelpline typically consists of a specific phone number that any individual is able to call to gain access to a range of medical services. These include phone consultations, counseling, service complaints, and information on facilities, drugs, equipment, and/or available mobile health clinics.\n\nDiagnostic and treatment support systems are typically designed to provide healthcare workers in remote areas advice about diagnosis and treatment of patients. While some projects may provide mobile phone applications—such as a step-by-step medical decision tree systems—to help healthcare workers diagnosis, other projects provide direct diagnosis to patients themselves. In such cases, known as telemedicine, patients might take a photograph of a wound or illness and allow a remote physician diagnose to help treat the medical problem. Both diagnosis and treatment support projects attempt to mitigate the cost and time of travel for patients located in remote areas.\n\nmHealth projects within the communication and training for healthcare workers subset involve connecting healthcare workers to sources of information through their mobile phone. This involves connecting healthcare workers to other healthcare workers, medical institutions, ministries of health, or other houses of medical information. Such projects additionally involve using mobile phones to better organize and target in-person training. Improved communication projects attempt to increase knowledge transfer amongst healthcare workers and improve patient outcomes through such programs as patient referral processes.\n\nProjects within this area operate to utilize mobile phones' ability to collect and transmit data quickly, cheaply, and relatively efficiently. Data concerning the location and levels of specific diseases (such as malaria, HIV/AIDS, TB, Avian Flu) can help medical systems or ministries of health or other organizations identify outbreaks and better target medical resources to areas of greatest need. Such projects can be particularly useful during emergencies, in order to identify where the greatest medical needs are within a country\n\nPolicymakers and health providers at the national, district, and community level need accurate data in order to gauge the effectiveness of existing policies and programs and shape new ones. In the developing world, collecting field information is particularly difficult since many segments of the population are rarely able to visit a hospital, even in the case of severe illness. A lack of patient data creates an arduous environment in which policy makers can decide where and how to spend their (sometimes limited) resources. While some software within this area is specific to a particular content or area, other software can be adapted to any data collection purpose.\n\nRemote monitoring and treatment support allows for greater involvement in the continued care of patients. Recent studies seem to show also the efficacy of inducing positive and negative affective states, using smart phones. Within environments of limited resources and beds—and subsequently a 'outpatient' culture—remote monitoring allows healthcare workers to better track patient conditions, medication regimen adherence, and follow-up scheduling. Such projects can operate through either one- or two-way communications systems. Remote monitoring has been used particularly in the area of medication adherence for AIDS, cardiovascular disease, chronic lung disease, diabetes, antenatal mental health, and tuberculosis. Technical process evaluations have confirmed the feasibility of deploying dynamically tailored, SMS-based interventions designed to provide ongoing behavioral reinforcement for persons living with HIV. among others.\n\nIn conclusion, the use of the mobile phone technology (in combination with a web-based interface) in health care results in an increase in convenience and efficiency of data collection, transfer, storage and analysis management of data as compared with paper-based systems. Formal studies and preliminary project assessments demonstrate this improvement of efficiency of healthcare delivery by mobile technology. Nevertheless, mHealth should not be considered as a panacea for healthcare. Possible organizational issues include the ensuring of appropriate use and proper care of the handset, lost or stolen phones, and the important consideration of costs related to the purchase of equipment. There is therefore a difficulty in comparison in weighing up mHealth interventions against other priority and evidence-based interventions.\n\n\nAccording to Vodafone Group Foundation on February 13, 2008, a partnership for emergency communications was created between the group and United Nations Foundation. Such partnership will increase the effectiveness of the information and communications technology response to major emergencies and disasters around the world.\n\nM-health has raised serious legal issues especially in developing countries that lack privacy and data protection laws. This makes medical records like the electronic health record vulnerable to third party abuses.\n\n"}
{"id": "3305496", "url": "https://en.wikipedia.org/wiki?curid=3305496", "title": "MOSE Project", "text": "MOSE Project\n\nMOSE (MOdulo Sperimentale Elettromeccanico, \"Experimental Electromechanical Module\") is a project intended to protect the city of Venice, Italy and the Venetian Lagoon from flooding.\n\nThe project is an integrated system consisting of rows of mobile gates installed at the Lido, Malamocco, and Chioggia inlets that are able to isolate the Venetian Lagoon temporarily from the Adriatic Sea during acqua alta high tides. Together with other measures, such as coastal reinforcement, the raising of quaysides, and the paving and improvement of the lagoon, MOSE is designed to protect Venice and the lagoon from tides of up to .\n\nThe \"Consorzio Venezia Nuova\" is responsible for the work on behalf of the Ministry of Infrastructure and Transport – Venice Water Authority.\nConstruction began simultaneously in 2003 at all three lagoon inlets, and , more than 85% of the project has been completed. It is expected to be fully completed in 2022.\n\nBefore the acronym was used to describe the entire flood protection system, MOSE referred to the 1:1 scale prototype of a gate that had been tested between 1988 and 1992 at the Lido inlet.\nThe name also holds a secondary meaning: \"MOSE\" alludes to the biblical character Moses (\"Mosè\" in Italian), who is remembered for parting the Red Sea.\n\nMOSE is part of a General Plan of Interventions to safeguard Venice and the lagoon commenced in 1987 by the Ministry of Infrastructure through the Venice Water Authority (the Ministry's operational arm in the lagoon) and the concessionary \"Consorzio Venezia Nuova\". The measures already completed or underway along the coastline and in the lagoon are the most important environmental defense, restoration, and improvement program ever implemented by the Italian State.\n\nIn parallel with the construction of MOSE, the Venice Water Authority and Venice Local Authority are raising quaysides and paving in the city in order to protect built-up areas in the lagoon from medium high tides (below , the height at which the mobile barriers will come into operation). These measures are extremely complex, particularly in urban settings such as Venice and Chioggia where the raising must take account of the delicate architectural and monumental context.\nMeasures to improve the shallow lagoon environment are aimed at slowing degradation of the morphological structures caused by subsidence, eustatism, and erosion due to waves and wash. Work is underway throughout the lagoon basin to protect, reconstruct, and renaturalise salt marshes, mud flats and shallows; restore the environment of the smaller islands; and dredge lagoon canals and channels.\n\nImportant activities are also underway to redress pollution in the industrial area of Porto Marghera, at the edge of the central lagoon. Islands formerly used as rubbish dumps are being secured while industrial canals are being consolidated and sealed after removal of their polluted sediments.\n\nThe aim of MOSE is to protect the lagoon, its towns, villages and inhabitants along with its iconic historic, artistic and environmental heritage from floods, including extreme events.\nAlthough the tide in the lagoon basin is lower than in other areas of the world (where it may reach as high as ), the phenomenon may become significant when associated with atmospheric and meteorological factors such as low pressure and the bora, a north-easterly wind coming from Trieste, or the Sirocco, a hot south-easterly wind. Those conditions push waves into the gulf of Venice. High water is also worsened by rain and water flowing into the lagoon from the drainage basin at 36 inflow points associated with small rivers and canals.\n\nFloods have caused damage since ancient times and have become ever more frequent and intense as a result of the combined effect of eustatism (a rise in sea level) and subsidence (a drop in land level) caused by natural and man-induced phenomena. Today, towns and villages in the lagoon are an average of lower with respect to the water level than at the beginning of the 1900s and each year, thousands of floods cause serious problems for the inhabitants as well as deterioration of architecture, urban structures and the ecosystem. Over the entire lagoon area, there is also a constant risk of an extreme catastrophic event such as that of 4November 1966 when a tide of submerged Venice, Chioggia and the other built-up areas.\nFloods effects are exacerbated due to greater erosion by the sea caused by human interventions to facilitate port activities (e.g. through the construction of jetties and artificial canals); establishment of the industrial Porto Marghera area; and increased wash from motorized boats, which all aggravate erosion of morphological structures and the foundations of quaysides and buildings.\nIn the future, the high water phenomenon may be further aggravated by the predicted rise in sea level as a result of global warming.\n\nIn this context, MOSE, together with reinforcement of the barrier island, has been designed to provide protection from tides of up to in height. MOSE will therefore effectively protect the lagoon, even if the most pessimistic hypotheses are proven true, such as a rise in sea level of at least (recent estimates from the IPCC (Intergovernmental Panel on Climate Change) predict a rise in sea level of between over the next 100 years).\nMOSE is flexible and can be operated in different ways according to the characteristics and height of the tide. Given that the gates are independent and can be operated separately, all three inlets can be closed in the case of an exceptional event, the inlets can be closed one at a time according to the winds, atmospheric pressure and height of tide forecast, or again, each inlet can be partially closed.\n\nExceptionally high waters have struck the city during the 20th century: the flood of November 1966 (), 1979 (), 2007 (), 1986 (), 1951 and 2012 (), 1936 and 2002 (), 1960 (), 1968 and 2000 (), 1992 (), 1979 (). All values were recorded at the Punta della Salute (Venice) station and refer to the 1897 tidal datum point.\n\nFollowing the flood of 4November 1966 when Venice, Chioggia and the other built-up areas in the lagoon were submerged by a tide of , the first Special Law for Venice declared the problem of safeguarding the city to be of \"priority national interest\". This marked the beginning of a long legislative and technical process to guarantee Venice and the lagoon an effective sea defence system.\n\nTo this end, in 1975 the State Ministry of Public Works issued a competitive tender, but the process ended without a project being chosen from those presented as no hypothesis for action satisfied all the mandated requirements. The Ministry subsequently acquired documents presented during the call for tender and passed them to a group of experts commissioned to draw up a project to preserve the hydraulic balance of the lagoon and protect Venice from floods (the \"Progettone\" of 1981).\n\nA few years later, a further Special Law (Law no. 798/1984) emphasised the need for a unified approach to safeguarding measures, set up a committee for policy, coordination and control of these activities (the \"Comitatone\", chaired by the President of the Council of Ministers and consisting of representatives of the competent national and local authorities and institutions) and entrusted design and implementation to a single body, the \"Consorzio Venezia Nuova\", recognising its ability to manage the safeguarding activities as a whole.\n\"The Venice Water Authority – Consorzio Venezia Nuova\" presented a complex system of interventions to safeguard Venice (the REA \"Riequilibrio E Ambiente\", \"Rebalancing and the Environment\" Project), which included mobile barriers at the inlets to regulate tides in the lagoon. In this context, between 1988 and 1992, experiments were carried out on a prototype gate (\"MOdulo Sperimentale Elettromeccanico\", hence the name MOSE) and in 1989, a conceptual design for the mobile barriers was drawn up. This was completed in 1992 and subsequently approved by the Higher Council of Public Works then subjected to an Environmental Impact Assessment procedure and further developed as requested by the \"Comitatone\". In 2002 the final design was presented and on 3April 2003, the Comitatone gave the go-ahead for its implementation. The same year, construction sites opened at the three lagoon inlets of Lido, Malamocco and Chioggia.\n\nMOSE consists of rows of mobile gates at the three inlets, which temporarily separate the lagoon from the sea in the event of a high tide. There will be a total of 78 gates divided into four barriers. At the Lido inlet, the widest, there will be two rows of gates of 21 and 20 elements respectively linked by an artificial island (the island connecting the two rows of gates at the centre of the Lido inlet will also accommodate the technical buildings housing the system operating plant); one row of 19 gates at the Malamocco inlet and one row of 18 gates at the Chioggia inlet.\nThe gates consist of metal box-type structures wide for all rows, with a length varying between and from thick, connected to the concrete housing structures with hinges, the technological heart of the system, which constrain the gates to the housing structures and allow them to move.\n\nUnder normal tidal conditions, the gates are full of water and rest in their housing structures. When a high tide is forecast, compressed air is introduced into the gates to empty them of water, causing them to rotate around the axis of the hinges and rise up until they emerge above the water to stop the tide from entering the lagoon. When the tide drops, the gates are filled with water again and return to their housing.\n\nThe inlets are closed for an average of between four and five hours, including the time taken for the gates to be raised (about 30 minutes) and lowered (about 15 minutes).\n\nTo guarantee navigation and avoid interruption of activities in the Port of Venice, when the mobile barriers are in operation, a main lock is under construction at the Malamocco inlet to allow the transit of large ships, while at the Lido and Chioggia inlets there will be smaller locks to allow emergency vessels, fishing boats and pleasure craft to shelter and transit.\n\nOperating procedure dictates that the gates will be raised for tides of more than high. The competent authorities have established this as the optimum height with respect to current sea levels, but the gates can be operated for any level of tide. The MOSE system is also flexible and depending on the winds, atmospheric pressure and level of tide, it can oppose the high water in different ways – with simultaneous closure of all three inlets in the case of exceptional tides, by closing just one inlet at a time, or by partially closing each inletgiven that the gates are independentfor medium-high tides.\n\nConstruction of MOSE was authorised by the \"Comitatone\" on 3April 2003 and the associated construction sites opened the same year. Work began simultaneously and continues in parallel at the three inlets of Lido, Malamocco and Chioggia. Work on the structural parts (foundations, mobile barrier abutments, gate housing structures), associated structures (breakwaters, small craft harbours, locks) and parts for operating the system (technical buildings, plant) is now at an advanced stage.\nCurrently about 4000 people are employed in the construction of MOSE.\nAs well as the construction sites at the inlets, fabrication of the main components of MOSE (the hinges, the technological heart of the system which constrain the gates to their housing and allow them to move, and the gates) is also proceeding. Restructuring of the buildings and spaces in the area of the Venice Arsenal where maintenance of MOSE and management of the system will be located is also underway.\n\nConstruction of MOSE at the inlets necessitates complex logistical organisation. These are located in a highly delicate environmental context so as to avoid interfering with the surrounding area as far as possible. The sites have been set up on temporary areas of water in order to limit occupation of the land adjacent to the inlets and reduce as far as possible the effect on activities taking place there. Materials (for example, site supplies) and machines are also moved via sea to avoid overloading the road system along the coast. Since the sites opened, all work has been carried out without interrupting transit through the inlet channels.\n\nBelow is a description of the work underway and already completed at each inlet.\n\nThere will be two rows of gates at the Lido inlet (21 mobile gates for the North barrier Lido-Treporti and 20 mobile gates for the South barrier Lido-San Nicolò).\n\nTo the north of the inlet (Treporti), a small craft harbour consisting of two basins communicating through a lock, will allow small craft and emergency vessels to shelter and transit when the gates are raised. The sea-side basin was temporarily drained and sealed for use as the site to construct the gate housing structures for this barrier. Once the housing structures had been completed, the area was flooded with water to allow the housing structures to be floated out.\n\nThe housing structures for the gates in the north barrier (seven housing structures and two for the abutment connections) were positioned on the seabed. Four of this barrier's gates were installed and manoeuvred for the first time in October 2013; at the end of 2014, the installation of 21 gates will be complete and operational for functional testing purposes (the so-called \"blank tests\").\n\nAt the south of the inlet (San Nicolò), the launch and the positioning of seven housing structures and two for the abutment connections has been completed (the structures have been fabricated on a temporary raised area in the Malamocco inlet and will be taken out to sea by a giant mobile platform which functions as a giant elevator).\n\nAt the centre of the inlet, a new island has been constructed to act as an intermediate structure between the two rows of mobile gates. This island will accommodate the buildings and plant for operating the gates (construction underway).\n\nOutside the inlet, a long curved breakwater is almost complete.\n\nA temporary construction site has been set up alongside the basin to fabricate the gate housing structures to be positioned on the sea bed (Malamocco and Lido San Nicolò barriers, seven housing structures and two for the abutment connections for each barrier have been built).\n\nIn April 2014, the lock for the transit of large ships becomes operative to avoid interference with port activities when the gates are in operation.\n\nPositioning of the gate housing structures for the Malamocco barrier started in June 2014 and is now underway (expected to end in October 2014).\n\nThe seabed in the area where the 19 gates will be installed has been reinforced.\n\nOutside the inlet, a long curved breakwater designed to attenuate tidal currents and define a basin of calm water to protect the lock has been completed.\n\nWork has been completed to construct a small craft harbour with double lock to guarantee transit of a large number of fishing vessels when the gates are in operation.\n\nThe sea-side basin has been temporarily drained and sealed for use as a construction site to fabricate the gate housing structures, as for the Lido north inlet barrier.\n\nPositioning for the gate housing structures started in June 2014 (expected to end in October 2014).\n\nIn the inlet channel, the seabed in the area where the 18 gates will be installed has been reinforced.\n\nOutside the inlet, a long curved breakwater has been completed.\n\nThe hinges form the technological heart of the sea defence system. They constrain the gates to the housing structures, allow them to move and connect the gates to the operating plant.\nThe steel gates consist of a male element ( high and weighing ) connected to the gate, a female element ( high and weighing ) fastened to the housing structure and an attachment assembly to connect the male and female elements.\nA total of 156 hinges (two for each gate) will be fabricated, together with a number of reserve elements. Fabrication is currently underway.\n\nDuring 2014, both barriers at the Lido inlet will be completed. At the same time, work will continue to launch and position the gate housing structures for the Malamocco and Chioggia barriers.\nMOSE is expected to be complete and in operation by 2022.\n\nSince 2011, the Mose control centre and management functions for the lagoon system have been located in the Venice Arsenal, symbol of the former trading and military might of the historic \"Serenissima\" or \"Serene Republic\". Numerous historical buildings, in a state of decay and abandonment for decades, have already been restored and reorganisation of the area is underway to accommodate these new activities.\nRestoration has enabled a heritage of extraordinary historical and architectural value to be safeguarded and allowed buildings to be recovered and re-utilised. As home to MOSE management and control the arsenal will receive a new lease of life after years of abandonment, allowing its renaissance as a place of innovation and production, with important economic repercussions for the city and local area.\n\nThe historic arsenal buildings before and after restoration and construction of infrastructure to accommodate the new functions are shown below.\n\nIn the control centre, key decisions will be taken on raising and lowering the mobile barriers according to measurements made by tide gauges positioned in front of the lagoon inlets to record the rising tide in real time. The command to raise the gate will be given when water reaches the level established by the procedure to begin the manoeuvre and guarantee that the water level in the lagoon does not exceed the requisite safe level.\n\n\nThe MOSE project is estimated to cost €5.496 billion, up €1.3 billion from initial cost projections. Testing will commence in January 2019, and last two years; full completion and operations are now expected in 2022.\n\nThe project has met resistance from environmental and conservation groups such as Italia Nostra, and the World Wide Fund for Nature, who have made negative comments about the project.\n\nCriticisms of the MOSE project, which environmentalists and certain political forces have opposed since its inception, relate to the costs to the Italian State of construction, management, and maintenance, which are said to be much higher than those for alternative systems employed by the Netherlands and England to resolve similar problems. In addition, according to the project's opponents, the monolithic integrated system is not \"gradual, experimental and reversible\" as required by the Special Law for the Safeguarding of Venice. There have also been criticisms of the environmental impact of the barriers, not just at the inlets where complex leveling will be carried out (the seabed must be flat at the barrier installation sites) and the lagoon bed reinforced to accommodate the gates (which will rest on thousands of concrete piles driven deep underground), but also on the hydrogeological balance and delicate ecosystem of the lagoon.\nThe \"NO MOSE front\" also emphasises what could be a number of critical points in the structure of the system and its inability to cope with predicted rises in sea level.\n\nOver the years, nine appeals have been presented, eight of which have been rejected by the TAR (\"Tribunale Amministrativo Regionale\", regional administrative court) and the Council of State. The ninth, currently under evaluation by the Administrative Tribunal, was presented by the Venice Local Authority and contests the favourable opinion of the Safeguard Venice Commission on the commencement of work at the Pellestrina site in the Malamocco inlet. Here, part of the MOSE gate housing caissons will be made using processes which, according to the local authority, could damage a site of special natural interest.\n\nEnvironmental associations have also requested the intervention of the European Union, as project activities affect sites protected by the \"Nature 2000 Network\" and the \"European Directive on birds\". Following a report of 5March 2004 by the Venetian MP Luana Zanella, on 19December 2005 the European Commission opened an infraction procedure against Italy for \"pollution of the habitat\" of the lagoon. The European Environmental Commission Directorate General considers that as it has \"neither identified nor adoptedin relation to the impacts on the area 'IBA 064-Venice Lagoon' resulting from construction of the MOSE projectappropriate measures to prevent pollution and deterioration of the habitat, together with harmful disturbance of birds with significant consequences in the light of the objectives of article 4 of EEC Directive 79/409, the Italian Republic has not fulfilled its obligations under Article 4, Paragraph 4, of EEC Directive 79/409 of the Council of 2April 1979 on the conservation of wild birds.\nAlthough the European Environmental Commission has said that the initiative is not intended to stop MOSE going ahead, the body has called on the Italian Government to produce new information on the impact of the sites and the environmental mitigation structures. The Water Authority and \"Consorzio Venezia Nuova\" both confirm that the construction sites are temporary and will be completely restored at the end of the work.\n\nIn 2014, 35 people, including Giorgio Orsoni, the Mayor of Venice, were arrested in Italy on corruption charges in connection with the MOSE Project. Orsoni was accused of receiving illicit funds from the Consorzio Venezia Nuova, the consortium behind the construction of the project, which he then used in his campaign to be elected mayor. There were allegations that 20 million euros in public funds had been sent to foreign bank accounts and used to finance political parties.\n\nFollowing the legal proceedings occurred between 2013 and 2014, that involved part of the management bodies of the Consorzio Venezia Nuova and its Companies, the State intervened in order to ensure the conclusion of the flood defense system: in December 2014, the ANAC (National Anti-Corruption Authority) proposes the extraordinary management of the Consorzio, which follows the appointment of three Special Chief Executive Officers.\nThe Special Administration of the Consorzio pursues its task to guarantee the proper completion of Mose and to ensure the conclusion of the defense system on 2018.\n\nOver the years, various proposals have been presented as an alternative to MOSE. Some offer widely different technological solutions while others suggest technologies to improve the efficiency of the system of mobile gates. At the request of the Mayor of Venice, Massimo Cacciari, approximately ten of these projects were examined in 2006 by round tables of experts appointed by individual responsible bodies, including the Higher Council of Public Works. In November 2006, negative assessments of the alternative proposals by these round tables led the government to give definitive approval for the MOSE project with the alternative proposals deemed ineffective or inappropriate to guarantee the defence of Venice.\n\n\n\n"}
{"id": "56182351", "url": "https://en.wikipedia.org/wiki?curid=56182351", "title": "Managed access (corrections)", "text": "Managed access (corrections)\n\nManaged access is a term for managing cellular network access from contraband phones within a corrections facility. Managed access differs from cellular jamming technologies which are outlawed in the United States. A managed access system functions like a femtocell or low power cell tower which passes calls to cellular carriers, however, only communications from approved devices and emergency calling is allowed. The managed access signal appears as an extension of nearby commercial cellular signals, once a phone connects to the network its identifying information is compared with approved devices and communications are accepted or denied. Managed access networks work with commercial cellular signals including 2G, 3G, 4G/LTE, and WiMAX.\n\nIn 2010, the Mississippi Department of Corrections tested the first managed access system at Parchman Mississippi State Penitentiary, during just one month the system blocked more than 216,000 texts and 600 phone calls. In 2013, the FCC recommended that prisons be allowed to managed their own network access without having to seek approval from the agency, saying that the process of inspecting the systems is \"time-consuming and complex\" and \"discourages their use.\" In a 2016 Op-ed, FCC Chairman Ajit Pai requested that the reforms proposed in 2013 aimed at loosening regulations on managed access and other solutions used to prevent the use of contraband cell phones should be enacted.\n\nAs of 2016, only California, Maryland, Mississippi, South Carolina, and Texas had tested managed access systems.\n\nManaged access systems are unable to stop the use of contraband devices using Wi-Fi to connect to the internet. Deployment of managed access systems requires FCC approval as well as consent from cellular network carriers. The devices can also cause interference outside of the prison if they are not properly implemented.\n\n"}
{"id": "10454903", "url": "https://en.wikipedia.org/wiki?curid=10454903", "title": "Ministry of Food Processing Industries", "text": "Ministry of Food Processing Industries\n\nThe Ministry of Food Processing Industries (MOFPI) is a ministry of the Government of India responsible for formulation and administration of the rules and regulations and laws relating to food processing in India. The ministry was set up in the year 1988, with a view to develop a strong and vibrant food processing industry, to create increased employment in rural sector and enable farmers to reap the benefits of modern technology and to create a surplus for exports and stimulating demand for processed food. The ministry is currently headed by Harsimrat Kaur Badal, a Cabinet Minister\n\n\n\n\nThe strategic role and functions of the Ministry fall under three categories -\n\n\nIt is concerned with the formulation & implementation of policies and plans for all the industries under its domain within the overall national priorities and objectives. Its main focus areas include—development of infrastructure, technological up gradation, development of backward linkages, enforcement of quality standards and expanding domestic as well as export markets for processed food products.\n\nThe Ministry acts as a catalyst and facilitator for attracting domestic & foreign investments towards developing large integrated processing capacities, by creating conducive policy environment, including rationalization of taxes & duties. It processes applications for foreign collaborations, Export Oriented Units (EOUs) etc. and assists/guides prospective entrepreneur in his endeavour.\n\nPost liberalization, it has approved a large no. of joint ventures, foreign collaborations, industrial licenses and 100% EOU proposals in different food processing areas and has taken major policy initiatives to facilitate an accelerated growth of the industry.The functions of the Ministry can be broadly classified as follows:\n\n\n\nEarlier the regulatory responsibilities of MoFPI were to implement Fruit Products Order (FPO), However, by the enactment of Food Safety and Standards Act, 2006, these regulatory responsibilities are transferred to Food Safety Authority of India, New Delhi which is under control of Ministry of Health and Family Welfare.\n\n\nFor achieving its objectives, the Ministry, apart from various Ministries of the Government of India like Agriculture, Industry, Commerce and Health interacts with various State Governments through their Ministries / Ministry of Food Processing Industries or nominated nodal agencies which are responsible for implementing programmes relating to this sector in the concerned State Governments.\n\nThe Ministry also interacts with various promotional organizations like\n\n"}
{"id": "1110936", "url": "https://en.wikipedia.org/wiki?curid=1110936", "title": "Photoelectrochemical cell", "text": "Photoelectrochemical cell\n\nPhotoelectrochemical cells or PECs are solar cells that produce electrical energy or hydrogen in a process similar to the electrolysis of water.\n\nThis type of cell electrolizes water to hydrogen and oxygen gas by irradiating the anode with electromagnetic radiation. This has been referred to as artificial photosynthesis and has been suggested as a way of storing solar energy in hydrogen for use as fuel.\n\nIncoming sunlight excites free electrons near the surface of the silicon electrode. These electrons flow through wires to the stainless steel electrode, where four of them react with four water molecules to form two molecules of hydrogen and 4 OH groups. The OH groups flow through the liquid electrolyte to the surface of the silicon electrode. There they react with the four holes associated with the four photoelectrons, the result being two water molecules and an oxygen molecule. Illuminated silicon immediately begins to corrode under contact with the electrolytes. The corrosion consumes material and disrupts the properties of the surfaces and interfaces within the cell.\n\nTwo types of photochemical systems operate via photocatalysis. One uses semiconductor surfaces as catalysts. In these devices the semiconductor surface absorbs solar energy and acts as an electrode for water splitting. The other methodology uses in-solution metal complexes as catalysts.\n\nPhotogeneration cells have passed the 10 percent economic efficiency barrier. Corrosion of the semiconductors remains an issue, given their direct contact with water. Research is now ongoing to reach a service life of 10000 hours, a requirement established by the United States Department of Energy.\n\nDye-sensitized solar cells or Grätzel cells use dye-adsorbed highly porous nanocrystalline titanium dioxide (nc-) to produce electrical energy.\n\nPECs convert light energy into electricity within a two-electrode cell. In theory, three arrangements of photo-electrodes in the assembly of PECs exist:\n\n\nThe two basic requirements for materials used as photo-electrodes are optical function, required to obtain maximal absorption of solar energy, and catalytic function, required for other reactions such as water decomposition.\n\nGaN is another option, because metal nitrides usually have a narrow band gap that could encompass almost the entire solar spectrum. GaN has a narrower band gap than but is still large enough to allow water splitting to occur at the surface. GaN nanowires exhibited better performance than GaN thin films, because they have a larger surface area and have a high single crystallinity which allows longer electron-hole pair lifetimes. Meanwhile, other non-oxide semiconductors such as GaAs, , and are used as n-type electrode, due to their stability in chemical and electrochemical steps in the photocorrosion reactions.\n\nIn 2013 a cell with 2 nanometers of nickel on a silicon electrode, paired with a stainless steel electrode, immersed in an aqueous electrolyte of potassium borate and lithium borate operated for 80 hours without noticeable corrosion, versus 8 hours for titanium dioxide. In the process, about 150 ml of hydrogen gas was generated, representing the storage of about 2 kilojoules of energy.\n\nIn 1967, Akira Fujishima discovered the Honda-Fujishima effect, (the photocatalytic properties of titanium dioxide).\n\n"}
{"id": "58337706", "url": "https://en.wikipedia.org/wiki?curid=58337706", "title": "Polyurethane urea elastomer", "text": "Polyurethane urea elastomer\n\nThe polyurethane urea elastomer (PUU), or poly(urethane urea) elastomer, is a flexible polymeric material that is composed of linkages made out of polyurethane and polyurea compounds. Due to its hyperelastic properties, it is capable of bouncing back high-speed ballistic projectiles as if the material had “hardened” upon impact. PUUs were developed by researchers from the U.S. Army Research Laboratory (ARL) and the Army’s Institute for Soldier Nanotechnology at the Massachusetts Institute of Technology (MIT) to potentially replace polyethylene materials in body armor and other protective gear, such as combat helmets, face shields, and ballistic vests.\n\nIn general, PUUs are composed of both hard and soft segments that each play a role in the material’s physical properties. The soft segments consist of two types of chemical compounds, long-chain polyols and diisocyanates, that react and connect together with urethane linkages. On the other hand, the short-chain diamines react with the diisocyanates to form the hard segments that are held together with urea linkages. The mechanical properties of the PUU largely depend on the specific diisocyanates, long-chain polyols, and short-chain diamines in play, because how these components interact determines how well the soft and hard segments of the elastomers  both crystallize and undergo microphase separation. As a result, variations in this molecular arrangement of chemical compounds have been shown to greatly affect the elastomer’s morphology and the macroscopic, mechanical properties that it exhibits.\n\nIn 2017, researchers from the Army Research Laboratory and MIT reported that PUUs are capable of demonstrating hyperelastic properties, meaning that the material becomes extremely hardened upon being deformed within a very short time. As a result, the material may withstand ballistic impacts at exceptionally high speeds.\n\nFor the study, the researchers investigated the performance of different PUU variants where 4,4’-dicyclohexylmethane diisocyanate (HMDI) was chosen as the diisocyanate compound, diethyltoluenediamine (DETA) was chosen as the short-chain diamine compound, and poly(tetramethyleneoxide) (PTMO) was chosen as the long-chain polyol compound. Despite consisting of the same chemical compounds with the same stoichiometric ratio of 2:1:1 of [HDMI]:[DETA]:[PTMO], the samples differed regarding the molecular weight of their respective PTMO component, namely 650 g/mol, 1000 g/mol, and 2000 g/mol, for the soft segments of the elastomers.\n\nEach of the three samples were subjected to a laser-induced projectile impact test (LIPIT), which tested the dynamic response of the material by using a pulsed laser to shoot it with microparticles made of silica at speeds ranging from 200 m/s to 800 m/s. The researchers found that the sample with the 650 g/mol PTMO was the most rigid variant with the particle exhibiting a shallow penetration of about 4 micrometers upon impact despite travelling at 790 m/s before rebounding at 195 m/s. In contrast, the sample with the 2000 g/mol PTMO displayed a deeper penetration of about 9 micrometers but had a slower particle rebound of 80 m/s, making it the most rubber-like among the PUU samples. The strain-rates associated with these impacts were on the order of 2.0 x 10^8/s for the former and 8.1 x 10^7/s for the latter.\n\nHowever, all three PUU variants demonstrated rebound capabilities with no signs of post-mortem damage after impact from the microparticles. In contrast, when the LIPIT was performed on a ductile, glassy polycarbonate at similar speeds to that of the 650 g/mol PTMO PUU variant, the polycarbonate displayed predominant deformation upon impact, despite its high fracture toughness and ballistic strength. According to the researchers, the effectiveness of the PUUs may come from how the molecules “resonate” similar to chain-mail upon impact with each oscillations at specific frequencies dissipate the absorbed energy. In comparison, the polycarbonate lacked the broad range of relaxation times, a characteristic that reflects how efficiently the molecules in the polymer chains respond to an external impulse, that PUUs are known to have. As a result, the researchers concluded that even the most rubber-like variant of the PUU, specifically the 2000 g/mol PTMO sample, demonstrated greater robustness and dynamic stiffening than the glassy polycarbonate.\n\nARL researchers have stated that the PUU’s primary benefit comes not from its extra strength but its fabric-like flexibility, which demonstrates its potential as a replacement material for the rigid ceramic and metal plates generally found in military battle armor. However, as of 2018, the PUU is still under development in the testing phase.\n"}
{"id": "12085248", "url": "https://en.wikipedia.org/wiki?curid=12085248", "title": "Pumping (oil well)", "text": "Pumping (oil well)\n\nIn the context of oil wells, pumping is a routine operation involving injecting fluids into the well. Pumping may either be done by rigging up to the kill wing valve on the Xmas tree or, if an intervention rig up is present pumping into the riser through a T-piece (a small section of riser with a connection on the side). Pumping is most routinely done to protect the well against scale and hydrates through the pumping of scale inhibitors and methanol. Pumping of kill weight brine may be done for the purposes of well kills and more exotic chemicals may be pumped from surface for cleaning the lower completion or stimulating the reservoir (though these types are jobs are more frequently done with coiled tubing for extra precision).\n\nWork involving wells is fraught with difficulties as there is often very little information about the real time condition of the completion. This lack of knowledge also covers potential damage and even loss of well integrity. Therefore, it is essential for the operator to pay attention to the pressures as recorded and to the quantity pumped. A premature increase in pressure is sign of a potential blockage and continuing to pump risks burst pressure retaining components. Pumping more than an anticipated amount of fluid is a sign of a loss of integrity and a potential leak path somewhere. In either of these two situations, pumping must be stopped and the potential causes analysed.\n\nIt is vital to know the effective capacity of the completion being filled in order to understand what are sensible volumes. If pumping is to continue until reaching a desired pressurisation, then the compressibility of the fluid will become significant. It is therefore important to know how much the fluid will compress under pressure to know how much extra fluid is expected to be required.\n\nAs a rule of thumb in the oilfield, compression is governed by the equation:\nformula_1\nwhere ΔV is the change in volume, P is the pressure at surface and V is the volume of fluid unpressurised. k is a compression factor approximately 3.5×10 psi.\n\nFor example, a volume of 300 bbl is to be filled with brine and pressurised to 3000 psi at the surface. The compression is\nformula_2\nformula_3\nformula_4\n\nTherefore, it is expected that 303.15 bbl are required to accomplish this task. If 3000 psi is achieved prior to this quantity being pumped, a blockage is to be suspected. If after pumping 303 bbl, pressurisation is not achieved, a leak is to be suspected.\n"}
{"id": "20747004", "url": "https://en.wikipedia.org/wiki?curid=20747004", "title": "RailRider", "text": "RailRider\n\nThe RailRider is a Global Positioning System (GPS) tracking and monitoring device used on railroad freight cars and locomotives. In earlier days of railroading a rail rider was known as a person to ride on a railcar to make sure it arrived unscathed.\n\nThe modern electronic RailRiders are deployed worldwide on railcars, and locomotives. The patented RailRider collects data from a GPS and sensors. The information is communicated through cellular (GPRS), radio, or satellite modem to servers for further processing. Customers can access data from anywhere through the Internet.\n\nShippers use the location and monitoring data to improve utilization, productivity, reduce maintenance, increase security, and improve customer support. Managers can quickly get answers to their logistics questions by running reports or viewing maps on the website. Data can also be passed directly to corporate fleet management computer systems for a complete integrated system with all lading information.\n\nRailRider is a registered trademark of Lat-Lon, LLC. Lat-Lon was founded in 1999 by Dave Baker and Steve Tautz. Membership interest was purchased by BSM Technologies U.S. Holdings, Inc. effective June 1, 2014limited liability company.\n\n"}
{"id": "46238", "url": "https://en.wikipedia.org/wiki?curid=46238", "title": "Refrigeration", "text": "Refrigeration\n\nRefrigeration is a process of removing heat from a low-temperature reservoir and transferring it to a high-temperature reservoir. The work of heat transfer is traditionally driven by mechanical means, but can also be driven by heat, magnetism, electricity, laser, or other means. Refrigeration has many applications, including, but not limited to: household refrigerators, industrial freezers, cryogenics, and air conditioning. Heat pumps may use the heat output of the refrigeration process, and also may be designed to be reversible, but are otherwise similar to air conditioning units. Refrigeration has had a large impact on industry, lifestyle, agriculture, and settlement patterns. The idea of preserving food dates back to at least the ancient Roman and Chinese empires. However, mechanical refrigeration technology has rapidly evolved in the last century, from ice harvesting to temperature-controlled rail cars. The introduction of refrigerated rail cars contributed to the westward expansion of the United States, allowing settlement in areas that were not on main transport channels such as rivers, harbors, or valley trails. Settlements were also developing in infertile parts of the country, filled with newly discovered natural resources. These new settlement patterns sparked the building of large cities which are able to thrive in areas that were otherwise thought to be inhospitable, such as Houston, Texas, and Las Vegas, Nevada. In most developed countries, cities are heavily dependent upon refrigeration in supermarkets, in order to obtain their food for daily consumption. The increase in food sources has led to a larger concentration of agricultural sales coming from a smaller percentage of existing farms. Farms today have a much larger output per person in comparison to the late 1800s. This has resulted in new food sources available to entire populations, which has had a large impact on the nutrition of society.\n\nAs quite similar criteria shall be fulfilled by working fluids (refrigerants) applied to heat pumps, refrigeration and ORC cycles, several working fluids are applied by all these technologies. Ammonia was one of the first refrigerants. Refrigeration can be defined as \"The science of providing and maintaining temperature below that of surrounding atmosphere\". It means continuous extraction of heat from a body whose temperature is already below the temperature of its surroundings.\n\nThe seasonal harvesting of snow and ice is an ancient practice estimated to have begun earlier than 1000 B.C. A Chinese collection of lyrics from this time period known as the \"Shijing\", describes religious ceremonies for filling and emptying ice cellars. However, little is known about the construction of these ice cellars or what the ice was used for. The next ancient society to harvest ice may have been the Jews according to the book of Proverbs, which reads, “As the cold of snow in the time of harvest, so is a faithful messenger to them who sent him.” Historians have interpreted this to mean that the Jews used ice to cool beverages rather than to preserve food. Other ancient cultures such as the Greeks and the Romans dug large snow pits insulated with grass, chaff, or branches of trees as cold storage. Like the Jews, the Greeks and Romans did not use ice and snow to preserve food, but primarily as a means to cool beverages. The Egyptians also developed methods to cool beverages, but in lieu of using ice to cool water, the Egyptians cooled water by putting boiling water in shallow earthen jars and placing them on the roofs of their houses at night. Slaves would moisten the outside of the jars and the resulting evaporation would cool the water. The ancient people of India used this same concept to produce ice. The Persians stored ice in a pit called a Yakhchal and may have been the first group of people to use cold storage to preserve food. In the Australian outback before a reliable electricity supply was available where the weather could be hot and dry, many farmers used a \"Coolgardie safe\". This consisted of a room with hessian \"curtains\" hanging from the ceiling soaked in water. The water would evaporate and thereby cool the hessian curtains and thereby the air circulating in the room. This would allow many perishables such as fruit, butter, and cured meats to be kept that would normally spoil in the heat.\nBefore 1830, few Americans used ice to refrigerate foods due to a lack of ice-storehouses and iceboxes. As these two things became more widely available, individuals used axes and saws to harvest ice for their storehouses. This method proved to be difficult, dangerous, and certainly did not resemble anything that could be duplicated on a commercial scale.\n\nDespite the difficulties of harvesting ice, Frederic Tudor thought that he could capitalize on this new commodity by harvesting ice in New England and shipping it to the Caribbean islands as well as the southern states. In the beginning, Tudor lost thousands of dollars, but eventually turned a profit as he constructed icehouses in Charleston, Virginia and in the Cuban port town of Havana. These icehouses as well as better insulated ships helped reduce ice wastage from 66% to 8%. This efficiency gain influenced Tudor to expand his ice market to other towns with icehouses such as New Orleans and Savannah. This ice market further expanded as harvesting ice became faster and cheaper after one of Tudor’s suppliers, Nathaniel Wyeth, invented a horse-drawn ice cutter in 1825. This invention as well as Tudor’s success inspired others to get involved in the ice trade and the ice industry grew.\n\nIce became a mass-market commodity by the early 1830s with the price of ice dropping from six cents per pound to a half of a cent per pound. In New York City, ice consumption increased from 12,000 tons in 1843 to 100,000 tons in 1856. Boston’s consumption leapt from 6,000 tons to 85,000 tons during that same period. Ice harvesting created a “cooling culture” as majority of people used ice and iceboxes to store their dairy products, fish, meat, and even fruits and vegetables. These early cold storage practices paved the way for many Americans to accept the refrigeration technology that would soon take over the country.\n\nThe history of artificial refrigeration began when Scottish professor William Cullen designed a small refrigerating machine in 1755. Cullen used a pump to create a partial vacuum over a container of diethyl ether, which then boiled, absorbing heat from the surrounding air. The experiment even created a small amount of ice, but had no practical application at that time.\n\nIn 1758, Benjamin Franklin and John Hadley, professor of chemistry, collaborated on a project investigating the principle of evaporation as a means to rapidly cool an object at Cambridge University, England. They confirmed that the evaporation of highly volatile liquids, such as alcohol and ether, could be used to drive down the temperature of an object past the freezing point of water. They conducted their experiment with the bulb of a mercury thermometer as their object and with a bellows used to quicken the evaporation; they lowered the temperature of the thermometer bulb down to , while the ambient temperature was . They noted that soon after they passed the freezing point of water (32 °F), a thin film of ice formed on the surface of the thermometer's bulb and that the ice mass was about a quarter inch thick when they stopped the experiment upon reaching . Franklin wrote, \"From this experiment, one may see the possibility of freezing a man to death on a warm summer's day\". In 1805, American inventor Oliver Evans described a closed vapor-compression refrigeration cycle for the production of ice by ether under vacuum.\n\nIn 1820 the English scientist Michael Faraday liquefied ammonia and other gases by using high pressures and low temperatures, and in 1834, an American expatriate to Great Britain, Jacob Perkins, built the first working vapor-compression refrigeration system in the world. It was a closed-cycle that could operate continuously, as he described in his patent:\n\nHis prototype system worked although it did not succeed commercially.\n\nIn 1842, a similar attempt was made by American physician, John Gorrie, who built a working prototype, but it was a commercial failure. Like many of the medical experts during this time, Gorrie thought too much exposure to tropical heat led to mental and physical degeneration, as well as the spread of diseases such as malaria. He conceived the idea of using his refrigeration system to cool the air for comfort in homes and hospitals to prevent disease. American engineer Alexander Twining took out a British patent in 1850 for a vapour compression system that used ether.\n\nThe first practical vapour-compression refrigeration system was built by James Harrison, a British journalist who had emigrated to Australia. His 1856 patent was for a vapour-compression system using ether, alcohol, or ammonia. He built a mechanical ice-making machine in 1851 on the banks of the Barwon River at Rocky Point in Geelong, Victoria, and his first commercial ice-making machine followed in 1854. Harrison also introduced commercial vapour-compression refrigeration to breweries and meat-packing houses, and by 1861, a dozen of his systems were in operation. He later entered the debate of how to compete against the American advantage of unrefrigerated beef sales to the United Kingdom. In 1873 he prepared the sailing ship \"Norfolk\" for an experimental beef shipment to the United Kingdom, which used a cold room system instead of a refrigeration system. The venture was a failure as the ice was consumed faster than expected.\nThe first gas absorption refrigeration system using gaseous ammonia dissolved in water (referred to as \"aqua ammonia\") was developed by Ferdinand Carré of France in 1859 and patented in 1860. Carl von Linde, an engineer specializing in steam locomotives and professor of engineering at the Technological University of Munich in Germany, began researching refrigeration in the 1860s and 1870s in response to demand from brewers for a technology that would allow year-round, large-scale production of lager; he patented an improved method of liquefying gases in 1876. His new process made possible using gases such as ammonia, sulfur dioxide (SO) and methyl chloride (CHCl) as refrigerants and they were widely used for that purpose until the late 1920s.\n\nThaddeus Lowe, an American balloonist, held several patents on ice-making machines. His \"Compression Ice Machine\" would revolutionize the cold-storage industry. In 1869 other investors and he purchased an old steamship onto which they loaded one of Lowe's refrigeration units and began shipping fresh fruit from New York to the Gulf Coast area, and fresh meat from Galveston, Texas back to New York, but because of Lowe's lack of knowledge about shipping, the business was a costly failure.\n\nIn 1842 John Gorrie created a system capable of refrigerating water to produce ice. Although it was a commercial failure, it inspired scientists and inventors around the world. France’s Ferdinand Carre was one of the inspired and he created an ice producing system that was simpler and smaller than that of Gorrie. During the Civil War, cities such as New Orleans could no longer get ice from New England via the coastal ice trade. Carre’s refrigeration system became the solution to New Orleans ice problems and by 1865 the city had three of Carre’s machines. In 1867, in San Antonio, Texas, a French immigrant named Andrew Muhl built an ice-making machine to help service the expanding beef industry before moving it to Waco in 1871. In 1873, the patent for this machine was contracted by the Columbus Iron Works, a company acquired by the W. C. Bradley Co., which went on to produce the first commercial ice-makers in the US.\n\nBy the 1870s breweries had become the largest users of harvested ice. Though the ice-harvesting industry had grown immensely by the turn of the 20th century, pollution and sewage had begun to creep into natural ice, making it a problem in the metropolitan suburbs. Eventually, breweries began to complain of tainted ice. Public concern for the purity of water, from which ice was formed, began to increase in the early 1900s with the rise of germ theory. Numerous media outlets published articles connecting diseases such as typhoid fever with natural ice consumption. This caused ice harvesting to become illegal in certain areas of the country. All of these scenarios increased the demands for modern refrigeration and manufactured ice. Ice producing machines like that of Carre’s and Muhl’s were looked to as means of producing ice to meet the needs of grocers, farmers, and food shippers.\n\nRefrigerated railroad cars were introduced in the US in the 1840s for short-run transport of dairy products, but these used harvested ice to maintain a cool temperature.\nThe new refrigerating technology first met with widespread industrial use as a means to freeze meat supplies for transport by sea in reefer ships from the British Dominions and other countries to the British Isles. The first to achieve this breakthrough was an entrepreneur who had emigrated to New Zealand. William Soltau Davidson thought that Britain's rising population and meat demand could mitigate the slump in world wool markets that was heavily affecting New Zealand. After extensive research, he commissioned the \"Dunedin\" to be refitted with a compression refrigeration unit for meat shipment in 1881. On February 15, 1882, the \"Dunedin\" sailed for London with what was to be the first commercially successful refrigerated shipping voyage, and the foundation of the refrigerated meat industry.\n\n\"The Times\" commented \"Today we have to record such a triumph over physical difficulties, as would have been incredible, even unimaginable, a very few days ago...\". The \"Marlborough\"—sister ship to the \"Dunedin\" – was immediately converted and joined the trade the following year, along with the rival New Zealand Shipping Company vessel \"Mataurua\", while the German Steamer \"Marsala\" began carrying frozen New Zealand lamb in December 1882. Within five years, 172 shipments of frozen meat were sent from New Zealand to the United Kingdom, of which only 9 had significant amounts of meat condemned. Refrigerated shipping also led to a broader meat and dairy boom in Australasia and South America. J & E Hall of Dartford, England outfitted the 'SS Selembria' with a vapor compression system to bring 30,000 carcasses of mutton from the Falkland Islands in 1886. In the years ahead, the industry rapidly expanded to Australia, Argentina and the United States.\n\nBy the 1890s refrigeration played a vital role in the distribution of food. The meat-packing industry relied heavily on natural ice in the 1880s and continued to rely on manufactured ice as those technologies became available. By 1900, the meat-packing houses of Chicago had adopted ammonia-cycle commercial refrigeration. By 1914 almost every location used artificial refrigeration. The big meat packers, Armour, Swift, and Wilson, had purchased the most expensive units which they installed on train cars and in branch houses and storage facilities in the more remote distribution areas. \nBy the middle of the 20th century, refrigeration units were designed for installation on trucks or lorries. Refrigerated vehicles are used to transport perishable goods, such as frozen foods, fruit and vegetables, and temperature-sensitive chemicals. Most modern refrigerators keep the temperature between –40 and –20 °C, and have a maximum payload of around 24,000 kg gross weight (in Europe). \nAlthough commercial refrigeration quickly progressed, it had limitations that prevented it from moving into the household. First, most refrigerators were far too large. Some of the commercial units being used in 1910 weighed between five and two hundred tons. Second, commercial refrigerators were expensive to produce, purchase, and maintain. Lastly, these refrigerators were unsafe. It was not uncommon for commercial refrigerators to catch fire, explode, or leak toxic gases. Refrigeration did not become a household technology until these three challenges were overcome.\n\nDuring the early 1800s consumers preserved their food by storing food and ice purchased from ice harvesters in iceboxes. In 1803, Thomas Moore patented a metal-lined butter-storage tub which became the prototype for most iceboxes. These iceboxes were used until nearly 1910 and the technology did not progress. In fact, consumers that used the icebox in 1910 faced the same challenge of a moldy and stinky icebox that consumers had in the early 1800s. \nGeneral Electric (GE) was one of the first companies to overcome these challenges. In 1911 GE released a household refrigeration unit that was powered by gas. The use of gas eliminated the need for an electric compressor motor and decreased the size of the refrigerator. However, electric companies that were customers of GE did not benefit from a gas-powered unit. Thus, GE invested in developing an electric model. In 1927, GE released the Monitor Top, the first refrigerator to run on electricity.\n\nIn 1930, Frigidaire, one of GE’s main competitors, synthesized Freon. With the invention of synthetic refrigerants based mostly on a chlorofluorocarbon (CFC) chemical, safer refrigerators were possible for home and consumer use. Freon led to the development of smaller, lighter, and cheaper refrigerators. The average price of a refrigerator dropped from $275 to $154 with the synthesis of Freon. This lower price allowed ownership of refrigerators in American households to exceed 50%. Freon is a trademark of the DuPont Corporation and refers to these CFCs, and later hydro chlorofluorocarbon (HCFC) and hydro fluorocarbon (HFC), refrigerants developed in the late 1920s. These refrigerants were considered at the time to be less harmful than the commonly-used refrigerants of the time, including methyl formate, ammonia, methyl chloride, and sulfur dioxide. The intent was to provide refrigeration equipment for home use without danger. These CFC refrigerants answered that need. In the 1970s, though, the compounds were found to be reacting with atmospheric ozone, an important protection against solar ultraviolet radiation, and their use as a refrigerant worldwide was curtailed in the Montreal Protocol of 1987.\n\nIn the last century refrigeration allowed new settlement patterns to emerge. This new technology has allowed for new areas to be settled that are not on a natural channel of transport such as a river, valley trail or harbor that may have otherwise not been settled. Refrigeration has given opportunities to early settlers to expand westward and into rural areas that were unpopulated. These new settlers with rich and untapped soil saw opportunity to profit by sending raw goods to the eastern cities and states. In the 20th century, refrigeration has made “Galactic Cities” such as Dallas, Phoenix and Los Angeles possible.\n\nThe refrigerated rail car (refrigerated van or refrigerator car), along with the dense railroad network, became an exceedingly important link between the marketplace and the farm allowing for a national opportunity rather than a just a regional one. Before the invention of the refrigerated rail car it was impossible to ship perishable food products long distances. The beef packing industry made the first demand push for refrigeration cars. The railroad companies were slow to adopt this new invention because of their heavy investments in cattle cars, stockyards, and feedlots. Refrigeration cars were also complex and costly compared to other rail cars, which also slowed the adoption of the refrigerated rail car. After the slow adoption of the refrigerated car, the beef packing industry dominated the refrigerated rail car business with their ability to control ice plants and the setting of icing fees. The United States Department of Agriculture estimated that in 1916 over sixty-nine percent of the cattle killed in the country was done in plants involved in interstate trade. The same companies that were also involved in the meat trade later implemented refrigerated transport to include vegetables and fruit. The meat packing companies had much of the expensive machinery, such as refrigerated cars, and cold storage facilities that allowed for them to effectively distribute all types of perishable goods. During World War I, a national refrigerator car pool was established by the United States Administration to deal with problem of idle cars and was later continued after the war. The idle car problem was the problem of refrigeration cars sitting pointlessly in between seasonal harvests. This meant that very expensive cars sat in rail yards for a good portion of the year while making no revenue for the car’s owner. The car pool was a system where cars were distributed to areas as crops matured ensuring maximum use of the cars. Refrigerated rail cars moved eastward from vineyards, orchards, fields, and gardens in western states to satisfy Americas consuming market in the east. The refrigerated car made it possible to transport perishable crops hundreds and even thousands of miles. The most noticeable effect the car gave was a regional specialization of vegetables and fruits. The refrigeration rail car was widely used for the transportation of perishable goods up until the 1950s. By the 1960s the nation's interstate highway system was adequately complete allowing for trucks to carry the majority of the perishable food loads and to push out the old system of the refrigerated rail cars.\n\nThe widespread use of refrigeration allowed for a vast amount of new agricultural opportunities to open up in the United States. New markets emerged throughout the United States in areas that were previously uninhabited and far-removed from heavily populated areas. New agricultural opportunity presented itself in areas that were considered rural such as states in the south and in the west. Shipments on a large scale from the south and California were both made around the same time although natural ice was used from the Sierras in California rather than manufactured ice in the south. Refrigeration allowed for many areas to specialize in the growing of specific fruits. California specialized in several fruits, grapes, peaches, pears, plums, and apples while Georgia became famous for specifically its peaches. In California, the acceptance of the refrigerated rail carts lead to an increase of car loads from 4,500 carloads in 1895 to between 8,000 and 10,000 carloads in 1905. The Gulf States, Arkansas, Missouri and Tennessee entered into strawberry production on a large-scale while Mississippi became the center of the tomato industry. New Mexico, Colorado, Arizona, and Nevada grew cantaloupes. Without refrigeration this would have not been possible. By 1917, well-established fruit and vegetable areas that were close to eastern markets felt the pressure of competition from these distant specialized centers. Refrigeration was not limited to meat, fruit and vegetables but it also encompassed dairy product and dairy farms. In the early twentieth century large cities got their dairy supply from farms as far as 400 miles. Dairy products were not as easily transported great distances like fruits and vegetables due to greater perishability. Refrigeration made production possible in the west far from eastern markets, so much in fact that dairy farmers could pay transportation cost and still undersell their eastern competitors. Refrigeration and the refrigerated rail gave opportunity to areas with rich soil far from natural channel of transport such as a river, valley trail or harbors.\n\n\"Edge city\" was a term coined by Joel Garreau, whereas the term \"galactic city\" was coined by Lewis Mumford. These terms refer to a concentration of business, shopping, and entertainment outside a traditional downtown or central business district in what had previously been a residential or rural area. There were several factors contributing to the growth of these cities such as Los Angeles, Las Vegas, Houston, and Phoenix. The factors that contributed to these large cities include reliable automobiles, highway systems, refrigeration, and agricultural production increases. Large cities such as the ones mentioned above have not been uncommon in history but what separates these cities from the rest are that these cities are not along some natural channel of transport, or at some crossroad of two or more channels such as a trail, harbor, mountain, river, or valley. These large cities have been developed in areas that only a few hundred years ago would have been uninhabitable. Without a cost efficient way of cooling air and transporting water and food great distances these large cities would have never developed. The rapid growth of these cities was influenced by refrigeration and an agricultural productivity increase, allowing more distant farms to effectively feed the population.\n\nAgriculture’s role in developed countries has drastically changed in the last century due to many factors, including refrigeration. Statistics from the 2007 census gives information on the large concentration of agricultural sales coming from a small portion of the existing farms in the United States today. This is a partial result of the market created for the frozen meat trade by the first successful shipment of frozen sheep carcasses coming from New Zealand in the 1880s. As the market continued to grow, regulations on food processing and quality began to be enforced. Eventually, electricity was introduced into rural homes in the United States, which allowed refrigeration technology to continue to expand on the farm, increasing output per person. Today, refrigeration’s use on the farm reduces humidity levels, avoids spoiling due to bacterial growth, and assists in preservation.\n\nThe introduction of refrigeration and evolution of additional technologies drastically changed agriculture in the United States. During the beginning of the 20th century, farming was a common occupation and lifestyle for United States citizens, as most farmers actually lived on their farm. In 1935, there were 6.8 million farms in the United States and a population of 127 million. Yet, while the United States population has continued to climb, citizens pursuing agriculture continue to decline. Based on the 2007 US Census, less than one percent of a population of 310 million people claim farming as an occupation today. However, the increasing population has led to an increasing demand for agricultural products, which is met through a greater variety of crops, fertilizers, pesticides, and improved technology. Improved technology has decreased the risk and time involved if agricultural management and allows larger farms to increase their output per person to meet society’s demand.\n\nPrior to 1882, the South Island of New Zealand had been experimenting with sowing grass and crossbreeding sheep, which immediately gave their farmers economic potential in the exportation of meat. In 1882, the first successful shipment of sheep carcasses was sent from Port Chalmers in Dunedin, New Zealand to London. By the 1890s, the frozen meat trade became increasingly more profitable in New Zealand, especially in Canterbury, where 50% of exported sheep carcasses came from in 1900. It wasn’t long before Canterbury meat was known for the highest quality, creating a demand for New Zealand meat around the world. In order to meet this new demand, the farmers improved their feed so sheep could be ready for the slaughter in only seven months. This new method of shipping led to an economic boom in New Zealand by the mid 1890s.\n\nIn the United States, the Meat Inspection Act of 1891 was put in place in the United States because local butchers felt the refrigerated railcar system was unwholesome. When meat packing began to take off, consumers became nervous about the quality of the meat for consumption. Upton Sinclair's 1906 novel \"The Jungle\" brought negative attention to the meat packing industry, by drawing to light unsanitary working conditions and processing of diseased animals. The book caught the attention of President Theodore Roosevelt, and the 1906 Meat Inspection Act was put into place as an amendment to the Meat Inspection Act of 1891. This new act focused on the quality of the meat and environment it is processed in.\n\nIn the early 1930s, 90 percent of the urban population of the United States had electric power, in comparison to only 10 percent of rural homes. At the time, power companies did not feel that extending power to rural areas (rural electrification) would produce enough profit to make it worth their while. However, in the midst of the Great Depression, President Franklin D. Roosevelt realized that rural areas would continue to lag behind urban areas in both poverty and production if they were not electrically wired. On May 11, 1935, the president signed an executive order called the Rural Electrification Administration, also known as REA. The agency provided loans to fund electric infrastructure in the rural areas. In just a few years, 300,000 people in rural areas of the United States had received power in their homes.\n\nWhile electricity dramatically improved working conditions on farms, it also had a large impact on the safety of food production. Refrigeration systems were introduced to the farming and food distribution processes, which helped in food preservation and kept food supplies safe. Refrigeration also allowed for production of perishable commodities, which could then be shipped throughout the United States. As a result, the United States farmers quickly became the most productive in the world, and entire new food systems arose.\n\nIn order to reduce humidity levels and spoiling due to bacterial growth, refrigeration is used for meat, produce, and dairy processing in farming today. Refrigeration systems are used the heaviest in the warmer months for farming produce, which must be cooled as soon as possible in order to meet quality standards and increase the shelf life. Meanwhile, dairy farms refrigerate milk year round to avoid spoiling.\n\nIn the late 19th Century and into the very early 20th Century, except for staple foods (sugar, rice, and beans) that needed no refrigeration, the available foods were affected heavily by the seasons and what could be grown locally. Refrigeration has removed these limitations. Refrigeration played a large part in the feasibility and then popularity of the modern supermarket. Fruits and vegetables out of season, or grown in distant locations, are now available at relatively low prices. Refrigerators have led to a huge increase in meat and dairy products as a portion of overall supermarket sales. As well as changing the goods purchased at the market, the ability to store these foods for extended periods of time has led to an increase in leisure time. Prior to the advent of the household refrigerator, people would have to shop on a daily basis for the supplies needed for their meals.\n\nThe introduction of refrigeration allowed for the hygienic handling and storage of perishables, and as such, promoted output growth, consumption, and the availability of nutrition. The change in our method of food preservation moved us away from salts to a more manageable sodium level. The ability to move and store perishables such as meat and dairy led to a 1.7% increase in dairy consumption and overall protein intake by 1.25% annually in the US after the 1890s.\n\nPeople were not only consuming these perishables because it became easier for they themselves to store them, but because the innovations in refrigerated transportation and storage led to less spoilage and waste, thereby driving the prices of these products down. Refrigeration accounts for at least 5.1% of the increase in adult stature (in the US) through improved nutrition, and when the indirect effects associated with improvements in the quality of nutrients and the reduction in illness is additionally factored in, the overall impact becomes considerably larger. Recent studies have also shown a negative relationship between the number of refrigerators in a household and the rate of gastric cancer mortality.\n\nProbably the most widely used current applications of refrigeration are for air conditioning of private homes and public buildings, and refrigerating foodstuffs in homes, restaurants and large storage warehouses. The use of refrigerators in kitchens for storing fruits and vegetables has allowed adding fresh salads to the modern diet year round, and storing fish and meats safely for long periods.\nOptimum temperature range for perishable food storage is .\n\nIn commerce and manufacturing, there are many uses for refrigeration. Refrigeration is used to liquify gases - oxygen, nitrogen, propane and methane, for example. In compressed air purification, it is used to condense water vapor from compressed air to reduce its moisture content. In oil refineries, chemical plants, and petrochemical plants, refrigeration is used to maintain certain processes at their needed low temperatures (for example, in alkylation of butenes and butane to produce a high octane gasoline component). Metal workers use refrigeration to temper steel and cutlery. When transporting temperature-sensitive foodstuffs and other materials by trucks, trains, airplanes and seagoing vessels, refrigeration is a necessity.\n\nDairy products are constantly in need of refrigeration, and it was only discovered in the past few decades that eggs needed to be refrigerated during shipment rather than waiting to be refrigerated after arrival at the grocery store. Meats, poultry and fish all must be kept in climate-controlled environments before being sold. Refrigeration also helps keep fruits and vegetables edible longer.\n\nOne of the most influential uses of refrigeration was in the development of the sushi/sashimi industry in Japan. Before the discovery of refrigeration, many sushi connoisseurs were at risk of contracting diseases. The dangers of unrefrigerated sashimi were not brought to light for decades due to the lack of research and healthcare distribution across rural Japan. Around mid-century, the Zojirushi corporation, based in Kyoto, made breakthroughs in refrigerator designs, making refrigerators cheaper and more accessible for restaurant proprietors and the general public.\n\nMethods of refrigeration can be classified as \"non-cyclic\", \"cyclic\", \"thermoelectric\" and \"magnetic\".\n\nThis refrigeration method cools a contained area by melting ice, or by sublimating dry ice. Perhaps the simplest example of this is a portable cooler, where items are put in it, then ice is poured over the top. Regular ice can maintain temperatures near, but not below the freezing point, unless salt is used to cool the ice down further (as in a traditional ice-cream maker). Dry ice can reliably bring the temperature well below freezing.\n\nThis consists of a refrigeration cycle, where heat is removed from a low-temperature space or source and rejected to a high-temperature sink with the help of external work, and its inverse, the thermodynamic power cycle. In the power cycle, heat is supplied from a high-temperature source to the engine, part of the heat being used to produce work and the rest being rejected to a low-temperature sink. This satisfies the second law of thermodynamics.\n\nA \"refrigeration cycle\" describes the changes that take place in the refrigerant as it alternately absorbs and rejects heat as it circulates through a refrigerator. It is also applied to heating, ventilation, and air conditioning HVACR work, when describing the \"process\" of refrigerant flow through an HVACR unit, whether it is a packaged or split system.\n\nHeat naturally flows from hot to cold. Work is applied to cool a living space or storage volume by pumping heat from a lower temperature heat source into a higher temperature heat sink. Insulation is used to reduce the work and energy needed to achieve and maintain a lower temperature in the cooled space. The operating principle of the refrigeration cycle was described mathematically by Sadi Carnot in 1824 as a heat engine.\n\nThe most common types of refrigeration systems use the reverse-Rankine vapor-compression refrigeration cycle, although absorption heat pumps are used in a minority of applications.\n\nCyclic refrigeration can be classified as:\n\nVapor cycle refrigeration can further be classified as:\n\nThe vapor-compression cycle is used in most household refrigerators as well as in many large commercial and industrial refrigeration systems. Figure 1 provides a schematic diagram of the components of a typical vapor-compression refrigeration system. The thermodynamics of the cycle can be analyzed on a diagram as shown in Figure 2. In this cycle, a circulating refrigerant such as Freon enters the compressor as a vapor. From point 1 to point 2, the vapor is compressed at constant entropy and exits the compressor as a vapor at a higher temperature, but still below the vapor pressure at that temperature. From point 2 to point 3 and on to point 4, the vapor travels through the condenser which cools the vapor until it starts condensing, and then condenses the vapor into a liquid by removing additional heat at constant pressure and temperature. Between points 4 and 5, the liquid refrigerant goes through the expansion valve (also called a throttle valve) where its pressure abruptly decreases, causing flash evaporation and auto-refrigeration of, typically, less than half of the liquid. That results in a mixture of liquid and vapor at a lower temperature and pressure as shown at point 5. The cold liquid-vapor mixture then travels through the evaporator coil or tubes and is completely vaporized by cooling the warm air (from the space being refrigerated) being blown by a fan across the evaporator coil or tubes. The resulting refrigerant vapor returns to the compressor inlet at point 1 to complete the thermodynamic cycle.\n\nThe above discussion is based on the ideal vapor-compression refrigeration cycle, and does not take into account real-world effects like frictional pressure drop in the system, slight thermodynamic irreversibility during the compression of the refrigerant vapor, or non-ideal gas behavior, if any.\n\nMore information about the design and performance of vapor-compression refrigeration systems is available in the classic \"Perry's Chemical Engineers' Handbook\".\n\nIn the early years of the twentieth century, the vapor absorption cycle using water-ammonia systems was popular and widely used. After the development of the vapor compression cycle, the vapor absorption cycle lost much of its importance because of its low coefficient of performance (about one fifth of that of the vapor compression cycle). Today, the vapor absorption cycle is used mainly where fuel for heating is available but electricity is not, such as in recreational vehicles that carry LP gas. It is also used in industrial environments where plentiful waste heat overcomes its inefficiency.\n\nThe absorption cycle is similar to the compression cycle, except for the method of raising the pressure of the refrigerant vapor. In the absorption system, the compressor is replaced by an absorber which dissolves the refrigerant in a suitable liquid, a liquid pump which raises the pressure and a generator which, on heat addition, drives off the refrigerant vapor from the high-pressure liquid. Some work is needed by the liquid pump but, for a given quantity of refrigerant, it is much smaller than needed by the compressor in the vapor compression cycle. In an absorption refrigerator, a suitable combination of refrigerant and absorbent is used. The most common combinations are ammonia (refrigerant) with water (absorbent), and water (refrigerant) with lithium bromide (absorbent).\n\nWhen the working fluid is a gas that is compressed and expanded but doesn't change phase, the refrigeration cycle is called a \"gas cycle\". Air is most often this working fluid. As there is no condensation and evaporation intended in a gas cycle, components corresponding to the condenser and evaporator in a vapor compression cycle are the hot and cold gas-to-gas heat exchangers in gas cycles.\n\nThe gas cycle is less efficient than the vapor compression cycle because the gas cycle works on the reverse Brayton cycle instead of the reverse Rankine cycle. As such the working fluid does not receive and reject heat at constant temperature. In the gas cycle, the refrigeration effect is equal to the product of the specific heat of the gas and the rise in temperature of the gas in the low temperature side. Therefore, for the same cooling load, a gas refrigeration cycle needs a large mass flow rate and is bulky.\n\nBecause of their lower efficiency and larger bulk, \"air cycle\" coolers are not often used nowadays in terrestrial cooling devices. However, the air cycle machine is very common on gas turbine-powered jet aircraft as cooling and ventilation units, because compressed air is readily available from the engines' compressor sections. Such units also serve the purpose of pressurizing the aircraft.\n\nThermoelectric cooling uses the Peltier effect to create a heat flux between the junction of two types of material. This effect is commonly used in camping and portable coolers and for cooling electronic components and small instruments.\n\nMagnetic refrigeration, or adiabatic demagnetization, is a cooling technology based on the magnetocaloric effect, an intrinsic property of magnetic solids. The refrigerant is often a paramagnetic salt, such as cerium magnesium nitrate. The active magnetic dipoles in this case are those of the electron shells of the paramagnetic atoms.\n\nA strong magnetic field is applied to the refrigerant, forcing its various magnetic dipoles to align and putting these degrees of freedom of the refrigerant into a state of lowered entropy. A heat sink then absorbs the heat released by the refrigerant due to its loss of entropy. Thermal contact with the heat sink is then broken so that the system is insulated, and the magnetic field is switched off. This increases the heat capacity of the refrigerant, thus decreasing its temperature below the temperature of the heat sink.\n\nBecause few materials exhibit the needed properties at room temperature, applications have so far been limited to cryogenics and research.\n\nOther methods of refrigeration include the air cycle machine used in aircraft; the vortex tube used for spot cooling, when compressed air is available; and thermoacoustic refrigeration using sound waves in a pressurized gas to drive heat transfer and heat exchange; steam jet cooling popular in the early 1930s for air conditioning large buildings; thermoelastic cooling using a smart metal alloy stretching and relaxing. Many Stirling cycle heat engines can be run backwards to act as a refrigerator, and therefore these engines have a niche use in cryogenics. In addition there are other types of cryocoolers such as Gifford-McMahon coolers, Joule-Thomson coolers, pulse-tube refrigerators and, for temperatures between 2 mK and 500 mK, dilution refrigerators.\n\nAnother potential solid-state refrigeration technique and a relatively new area of study comes from a special property of super elastic materials. These materials undergo a temperature change when experiencing an applied mechanical stress (called the elastocaloric effect). Since super elastic materials deform reversibly at high strains, the material experiences a flattened elastic region in its stress-strain curve caused by a resulting phase transformation from an austenitic to a martensitic crystal phase.\n\nWhen a super elastic material experiences a stress in the austenitic phase, it undergoes an exothermic phase transformation to the martensitic phase, which causes the material to heat up. Removing the stress reverses the process, restores the material to its austenitic phase, and absorbs heat from the surroundings cooling down the material.\n\nThe most appealing part of this research is how potentially energy efficient and environmentally friendly this cooling technology is. The different materials used, commonly shape-memory alloys, provide a non-toxic source of emission free refrigeration. The most commonly studied materials studied are shape-memory alloys, like nitinol and Cu-Zn-Al. Nitinol is of the more promising alloys with output heat at about 66 J/cm and a temperature change of about 16-20 K. Due to the difficulty in manufacturing some of the shape memory alloys, alternative materials like natural rubber have been studied. Even though rubber may not give off as much heat per volume (12 J/cm ) as the shape memory alloys, it still generates a comparable temperature change of about 12 K and operates at a suitable temperature range, low stresses, and low cost.\n\nThe main challenge however comes from potential energy losses in the form of hysteresis, often associated with this process. Since most of these losses comes from incompatibilities between the two phases, proper alloy tuning is necessary to reduce losses and increase reversibility and efficiency. Balancing the transformation strain of the material with the energy losses enables a large elastocaloric effect to occur and potentially a new alternative for refrigeration.\n\nThe Fridge Gate method is a theoretical application of using a single logic gate to drive a refrigerator in the most energy efficient way possible without violating the laws of thermodynamics. It operates on the fact that there are two energy states in which a particle can exist: the ground state and the excited state. The excited state carries a little more energy than the ground state, small enough so that the transition occurs with high probability. There are three components or particle types associated with the fridge gate. The first is on the interior of the fridge, the second on the outside and the third is connected to a power supply which heats up every so often that it can reach the E state and replenish the source. In the cooling step on the inside of the fridge, the g state particle absorbs energy from ambient particles, cooling them, and itself jumping to the e state. In the second step, on the outside of the fridge where the particles are also at an e state, the particle falls to the g state, releasing energy and heating the outside particles. In the third and final step, the power supply moves a particle at the e state, and when it falls to the g state it induces an energy-neutral swap where the interior e particle is replaced by a new g particle, restarting the cycle.\n\nMIT researchers have devised a new way of providing cooling on a hot sunny day, using inexpensive materials and requiring no fossil fuel-generated power. The passive system, which could be used to supplement other cooling systems to preserve food and medications in hot, off-grid locations, is essentially a high-tech version of a parasol.\n\nThe measured capacity of refrigeration is always dimensioned in units of power. Domestic and commercial refrigerators may be rated in kJ/s, or Btu/h of cooling. For commercial and industrial refrigeration systems, the kilowatt (kW) is the basic unit of refrigerationexcept in North America, where the ton of refrigeration (TR) is used. (Nominally the capacity to freeze one short ton of water per day, the TR is defined as 12,000 Btu/hr (3.517 kW).)\n\nA refrigeration system's coefficient of performance (CoP) is very important in determining a system's overall efficiency. It is defined as refrigeration capacity in kW divided by the energy input in kW. While CoP is a very simple measure of performance, it is typically not used for industrial refrigeration in North America. Owners and manufacturers of these systems typically use performance factor (PF). A system's PF is defined as a system's energy input in horsepower divided by its refrigeration capacity in TR. Both CoP and PF can be applied to either the entire system or to system components. For example, an individual compressor can be rated by comparing the energy needed to run the compressor versus the expected refrigeration capacity based on inlet volume flow rate. It is important to note that both CoP and PF for a refrigeration system are only defined at specific operating conditions, including temperatures and thermal loads. Moving away from the specified operating conditions can dramatically change a system's performance.\n\n\n"}
{"id": "2533985", "url": "https://en.wikipedia.org/wiki?curid=2533985", "title": "Serial cable", "text": "Serial cable\n\nA serial cable is a cable used to transfer information between two devices using a serial communication protocol. The form of connectors depends on the particular serial port used. A cable wired for connecting two DTEs directly is known as a null modem cable.\n\nThe maximum working length of a cable varies depending on the characteristics of the transmitters and receivers, the baud rate on the cable, and the capacitance and electrical impedance of the cable. The RS-232 standard states that a compliant port must provide defined signal characteristics for a capacitive load of pF. This does not correspond to a fixed length of cable since varying cables have different characteristics. Empirically tested combinations of bit rate, serial ports, cable type, and lengths may provide reliable communications, but generally RS-232-compatible ports are intended to be connected by, at the most, a few tens of metres of cable. Other serial communications standards are better adapted to drive hundreds or thousands of metres of cable.\n\nThis cable has short transmission distance because of noise limiting the transmission of high numbers of bits per second when the cable is more than 15m long. This means that the transmitting and receiving lines are referenced to ground. It is cheap to purchase and is simple to join and connect. It is suitable for unbalanced circuits. Each end of the cable can be connected to only one device.\n\n\n"}
{"id": "20104338", "url": "https://en.wikipedia.org/wiki?curid=20104338", "title": "Sinovac Biotech", "text": "Sinovac Biotech\n\nSinovac Biotech Ltd. (, ) is a biopharmaceutical company that focuses on the research, development, manufacture and commercialization of vaccines that protect against human infectious diseases. The company is based in Beijing, China. \n\nSinovac's commercialized vaccines include Healive (hepatitis A), Bilive (combined hepatitis A and B), Anflu (influenza), Panflu (H5N1) and PANFLU.1 (H1N1). Sinovac is currently developing Universal Pandemic Influenza vaccine and Japanese encephalitis vaccine.\n\nSinovac is developing vaccines for enterovirus 71, universal pandemic influenza, Japanese encephalitis, and human rabies. Its wholly owned subsidiary, Tangshan Yian, is conducting field trials for independently developed inactivated animal rabies vaccines.\n\n"}
{"id": "8778890", "url": "https://en.wikipedia.org/wiki?curid=8778890", "title": "Sparkling wine production", "text": "Sparkling wine production\n\nSparkling wine is a wine (usually white) that becomes carbonated, either through fermentation or by addition of carbon dioxide. The oldest known production of sparkling wine took place in 1531 with the \"ancestral method\". Champagne is the most well-known variant, but there are other variations such as Italian Prosecco and Spumante, Spanish Cava, French Crémant and German Sekt.\n\nOverall, more or less dry white wines are most common among the sparkling wines, but sparkling rosé and red wines are also produced, as well as wines of varying sweetness.\n\nIn popular parlance and also in the title of this article the term \"sparkling\" is used for all wines that produce bubbles at the surface after opening. Under EU law the term \"sparkling\" has a special meaning that does not include all wines that produce bubbles. For this reason the terms \"fizzy\" and \"effervescent\" are sometimes used to include all bubbly wines.\n\nThe following terms are increasingly used to designate different bottle pressures:\n\nFermentation of sugar into alcohol during winemaking always means that carbon dioxide is released. Carbon dioxide has the property of being very soluble in water (the main constituent of wine), a property that is utilized in sparkling wines. Production always starts from a base wine (where the carbon dioxide from the first fermentation has been gasified). In Champagne production, the base wine is usually a blend of wines from different grape varieties and different wineries, where the distribution gives the final wine its special character, called \"cuvée\". In some commonly used methods the base wine undergoes a secondary fermentation, which encloses the resulting carbon dioxide under excess pressure and binds it to the liquid in the sparkling wine. In this way the carbon dioxide content is created which, after opening the bottle, produces the bubbles. The dead yeast cells form a precipitate called \"lees\", that often helps with appealing aromas to the sparkling wine but looks unappetizing. The lees is therefore normally removed before the wine is distributed.\n\nThe main methods used to make sparkling wines are often given as seven, with some variations. What or which production methods that give the best wines is not a moot point, but there is some consensus that the first four methods, where the second fermentation occurs in the bottle, are usually preferable to the latter three. The following table shows the main features of each method. The methods are then described in the text. It should be noted that within each method there may be variations from one producer to another.\n\nThe so-called classic way (though not the oldest) to produce sparkling wine is popularly known as the \"Champagne method\" or \"méthode classique\" which is the official EU designation. Formerly, the designation was \"méthode champenoise\" but this designation is no longer permitted, as it involves a certain amount of renomée passing off. As the former designation suggests, the method is used for the production of most Champagne, and it is slightly more expensive than the Charmat method. Champagne in bottles of 375 ml, 750 ml and 1.5 liters must be produced with the traditional method, but smaller and larger bottles are usually produced with the \"transfer method\".\n\nThe wine is fermented once in the barrel and then undergoes a second fermentation in the bottle after the addition of yeast, nutrients for the yeast, and sugar (known as \"tirage\"). The second fermentation results in a natural sparkling wine. Yeast precipitate (lees) must then be removed. This begins with \"riddling\" (\"remuage\" in French) which means that the bottles are turned with the neck downwards and lightly shaken to move the lees to the neck of the bottle. This is done in small steps where the bottle orientation gradually changes. Finally the inverted bottle necks are cooled so that the precipitation freezes to a small block of ice, the bottles are turned upright and the temporary closure (normally a crown cap) is opened so that the precipitate is pushed out by the pressure in the bottle. Then the bottle is filled to replace the missing volume, and fitted with a plain Champagne cork and halter. The process to remove lees is called \"disgorging\".\n\nHistorically the various stages were performed manually but the whole process is now automated for most wines. In connection with the filling of the missing volume, it is common to add a certain amount of sugar dissolved in wine to give the sparkling wine a smoother taste. Sugar addition is called \"dosage\" and the added liquid \"liqueur d'expedition\".\n\nIn many cases the wine is stored on the lees – \"sur lie\" – under carbon dioxide pressure for a long time before disgorging takes place, to get a more mature character. The requirement for non-vintage Champagne is at least 15 months of storage on the lees, and for vintage Champagne at least three years.\n\nThe traditional method is used for Champagne, all European wines with the designation \"Crémant\", \"Cava\", better varieties of \"Sekt\" and other sparkling wines that have names such as \"méthode classique\" or \"fermented in this bottle\" on the label (note, however, that the unusual \"ancestral\" and \"dioise\" methods also ferment the wine exclusively in the bottle).\n\nThe ancestral method () goes under many local names in the various French regions, such as \"rurale\", \"artisanale\" and \"gaillacoise\". This is by far the oldest method of making sparkling wine and preceded the \"traditional method\" by almost 200 years, or possibly even more. The wine that is now called Blanquette de Limoux is considered by wine historians to be the world's first sparkling wine, and was produced in Limoux in 1531 by monks in the monastery of Saint-Hilaire. Wines produced using the ancestral method include among others French wines from Gaillac, Bugey Cerdon and Blanquette de Limoux, German wines from a few vineyards where the method is usually called \"méthode rural\", and North American wines.\n\nWines made with the ancestral method are sometimes called \"pétillant-naturel\", popularly \"pét-nat\". Since French wine labels ban the word \"natural\" the appellation from Montlouis-sur-Loire is instead called \"pétillant originel\".\n\nAlcoholic fermentation is not completely finished when the wine is bottled. It follows that the carbon dioxide is produced by the fermenting yeasts, and where malolactic fermentation has not yet taken place. Unlike the \"traditional method\" there is no \"disgorging\" and \"dosage\", but the wine is normally filtered. To accomplish this the bottles are emptied, then cleaned and refilled.\n\nThe method generally produces wines that are highly aromatic with low alcohol content, sometimes as low as 6%. The wines are sometimes somewhat obscure from remaining lees. They taste best 1–3 years after bottling and do not develop with further storage. In general, the wines are slightly sweet but \"brut\" (dry) varieties are also produced. The method's main weakness is that the production process is difficult to control and therefore requires great skill by the winemaker. The produced volumes are very modest. High-quality wines produced with the ancestral method, often by small growers using organic farming principles, can be complex and very distinctive. They are mainly used as aperitifs or dessert wine with fruit dishes.\n\nThe transfer method follows the first steps of the traditional method in that after primary fermentation the cuvée is transferred to bottles to complete secondary fermentation, which allows for additional complexity. When the secondary fermentation is complete and the wine has spent the desired amount of time in bottle on yeast lees (six months is the requirement to label a wine 'bottle fermented') then the individual bottles are transferred (hence the name) into a larger tank. The wine is then filtered, \"liqueur de dosage\" added, and then filled back into new bottles for sale. This method allows for complexity to be built into the wine, but also gives scope for blending options after the wine has gone into bottle and reduces the bottle-to-bottle variations that can be hard to control in the traditional method.\n\nThe name \"transversage method\" is often used as a synonym to transfer method, but is actually a slight twist to the latter. In the transfer method proper, the wine is transferred to a tank directly after ageing on lees, while in the transversage method, the wine is riddled and disgorged before transfer to a tank. Consequently the transversage method doesn't need additional clarification before bottling.\n\nThe transfer method gives better results than the \"charmat\" method, particularly in terms of bubble size and durability. Sparkling wines from New Zealand and Australia often use the transfer method. The method is used for sparkling wine sold in unusually small or unusually large bottles.\n\nThis method is used for Clairette de Die AOC where the method is officially designated as the \"original dioise process\". In contrast to the \"ancestral method\" the yeast production is controlled by cooling. The dioise method is used in among others the Drôme valley in France, as well as for Asti Spumante produced in Canelli in the Piedmont region in Italy.\n\nCharmat () was developed and patented in 1895 by the Italian Federico Martinotti (1860–1924). The method was further developed with a new patent by the inventor Eugène Charmat in 1907. The method is now named after the latter, but is also called \"cuve close\", \"metodo Italiano\" or \"the tank method\". The wine is mixed in a stainless steel pressure tank, together with sugar and yeast. When the sugar is converted into alcohol and carbon dioxide, the yeast is filtered and removed, and the wine is bottled. The duration of fermentation affects the quality; longer fermentation preserves the wine's aromas better and gives finer and more durable bubbles.\n\nThis production method is widely used in the U.S., in Italy, especially in the Asti province, and in Prosecco wines, and in Germany to produce cheap variants of Sekt. Charmat method sparkling wines can be produced at a considerably lower cost than traditional method wines.\n\nThe continuous method is also called the \"Russian method\". The secondary fermentation takes place in steel tanks with special rings, or with added oak chips. The wine circulates slowly and becomes reasonably clear before it is bottled.\n\n\"Sovetskoye Shampanskoye\", \"Soviet Champagne\", or under European Union law \"Soviet sparkling wine\", is produced by the continuous method in Russia and former Soviet Union countries. In 1975 Moët & Chandon bought a licence for production of sparkling wine by the continuous method.\n\nSimpler, cheaper sparkling wines are manufactured simply by adding CO to the wine from a carbonator. The bubbles created by using this method will be relatively large and volatile. In the European Union sparkling wines made via this method must use terms 'aerated sparkling wine' and 'aerated semi-sparkling wine', supplemented where necessary with the words 'obtained by adding carbon dioxide' or 'obtained by adding carbon anhydride.'\n\nSeveral wine faults can occur in sparkling wine production. Some that were present in early production methods include \"yeux de crapauds\" (toad's eyes) which was a condition of big, viscous bubbles that resulted from the wine spending too much time in wooden casks. Another fault could occur when the wine is exposed to bacteria or direct sunlight, leaving the wine with murky colouring and an oily texture.\n"}
{"id": "18273020", "url": "https://en.wikipedia.org/wiki?curid=18273020", "title": "Spy basket", "text": "Spy basket\n\nThe Spy gondola, Spy basket, Observation car or sub-cloud car ( or ) was a byproduct of \"Peilgondel\" development (a gondola to weight an airship's radio-locating antenna). They were used almost entirely by the Germans in the First World War on their military airships. The spy basket could be lowered from above through the cloud deck several hundred metres, in order to inconspicuously observe the ground and to help navigate the airship.\n\nThe \"Peilgondel\" was developed by Paul Jaray to act as a heavy plumbbob for an airship's radio antenna. A free-hanging antenna wire would move and flex in the wind hindering communications; the added weight reduced this movement. Jaray then developed the \"Peilgondel\" further into a manned spy gondola.\n\nSpy baskets were used on, among others, Schütte-Lanz and Zeppelin airships. , it was not always certain which airships used them: the blueprints for LZ 62 (L 30) and LZ 72 (L 31) included the spy basket operating plant but the German Navy was no longer installing them at that time; however a fish-shaped spy basket can be seen on photographs of the German Army LZ 83 (tactical number LZ 113).After the war the Americans briefly experimented with a spy basket on the .\n\nCaptain Ernst A. Lehmann, the German airship captain, described in his book \"The Zeppelins\" how he and Baron Gemmingen, Count Zeppelin's nephew, had developed the device. To test the prototype he blindfolded the helmsman of the airship and allowed himself to be lowered by a winch from the bombroom in a modified cask, equipped with a telephone. Hanging some below the airship using a compass he could tell the helmsman which bearing to take and effectively drive the airship. He later recounted how, while returning from the aborted raid on London in March 1916 in the Z 12, Baron Gemmingen insisted on being the first to use it on their secondary target, Calais. The basket was equipped with a wicker chair, chart table, electric lamp, compass, telephone, and lightning conductor. With the Zeppelin sometimes within, sometimes above the clouds and unable to see the ground, Gemmingen in the hanging basket would relay orders on navigation and when and which bombs to drop. The Calais defenders could hear the engines but their searchlights and artillery fire did not reach the airship.\n\nLZ26's basket was lowered from the airship on a specially constructed tether long; other airships may have used one approximately long. The tether was high grade steel with a brass core insulated with rubber to act as the telephone cable.\n\nDespite Gemmingen reporting a feeling of loneliness while being lowered and losing sight of the airship, crewmen would nevertheless volunteer for this duty because it was the one place they could smoke.\n\nThe Imperial War Museum in London exhibits a Zeppelin observation car that was found near Colchester after the Zeppelin air raid on the night of the September 2-3, 1916. It is believed to have been carried by the LZ 90 and was being deployed unmanned when the winch accidentally ran out of control. It was found along with 1500 metres of cable. The winch was jettisoned near Bury St. Edmunds. \n\nThe spy basket's use is dramatized in the 1930 film \"Hell's Angels\". In the long running British comic Charley's War, a ruthless German airship commander orders the jettisoning of his ship's\ncloud car (with the observer still in it) in order to save weight when his airship comes under attack.\n"}
{"id": "19049004", "url": "https://en.wikipedia.org/wiki?curid=19049004", "title": "Stump-jump plough", "text": "Stump-jump plough\n\nThe stump-jump plough is a kind of plough invented in South Australia in the late 19th century by Richard Bowyer Smith to solve the particular problem of preparing mallee lands for cultivation.\n\nMallee scrub originally covered large parts of southern Australia, and when farmers moved into these areas they found it peculiarly difficult to clear the land. After the trees were cut down, the roots energetically produced regrowth. These young shoots and the old roots could be killed by repeated burning (see below), but the large roots remained in the ground, making it impossible to plough the soil. Grubbing the roots out was a slow and labour-intensive activity, and the problem was seriously hindering agricultural expansion.\n\nIn South Australia, land was being offered under the \"Scrub Act\" of 1866 to farmers on lease, with the option of purchasing after 21 years at the price of £1 per acre. However, grubbing the scrublands was proving costly, at approximately £2 per acre, and solutions to the problem were desperately sought.\n\nThe situation had grown to be so frustrating by 1878 that the South Australian government offered a reward of £200 to anyone who could develop an effective mechanical stump puller; although myriad devices were developed, none proved to be a breakthrough success. Many of these machines were trialled in contests near Gawler in the same year, but none were as effective as three skilled axemen.\n\nPending the development of an effective machine, a technique known as mullenizing (after a farmer from Wasleys named Charles Mullens) became popular as a means of clearing the scrub. Mullenizing involved dragging a heavy roller over roughly cleared ground to crush young shoots; the field was then burnt, and a spiked log was run over the ground, and a crop of wheat sown. The next season, the stubble and any mallee regrowth was again burnt, and eventually the mallee died, though stumps remained underground.\n\nIn 1876 a special plough was invented by agricultural machinery apprentice Richard Bowyer Smith, and later developed and perfected by his brother, Clarence Herbert Smith, on the Yorke Peninsula (where the problem was particularly acute). The plough consisted of any number of hinged shares: when the blade encountered an underground obstacle like a mallee stump, it would rise out of the ground. Attached weights forced the blade back into the ground after the root was passed, allowing as much of the ground to be furrowed as possible. Although a little unorthodox, the plough in action appearing \"like a ship in a storm\", it proved remarkably effective, and was dubbed the \"stump-jump\" plough.\n\nThe invention was hailed as a \"complete revolution\" and, in combination with the process of mullenizing, was adopted almost universally across the mallee lands, even proving as useful in stony ground as it was in mallee country. There is a plough on display in Ardrossan, with a Historic Engineering Marker plaque attached.\n\nAnother successful stump jump plough was invented in 1877 by James Winchester Stott (1830-1907) who was a very prolific inventor in Alma in the mid North of South Australia. Not only did he invent a stump jump plough he also invented a cultivator, slasher, scarifier and double furrow plough. Stott and Mellor Brothers, who had refined Stott's design, were jointly the first to patent a stump-jumping plough in Victoria.\n\nA working model of the R.B. Smith Double Furrow Plow is held by the Powerhouse Museum in Sydney. It is very finely crafted and was made in 1883 by Albert Arnold, then a young Journeyman blacksmith. His family moved to Dowlingville (near Ardrossan) on the Yorke Peninsula in 1876 <7> to take up land. He and his father John, also a blacksmith, made numerous modifications over several years and participated in local ploughing competitions. Albert moved to Sydney in 1882 and endeavoured to have his new employer manufacture the plow, hence the model. Albert left considerable family papers including diagrams and photos of his work. Eventually the model and another 25 models and NSWGR inventions were given to the then Technological Museum and are available to viewers on request.\n\n 7. Arnold Family Papers including newspaper articles held by G.Stevenson\n"}
{"id": "28491", "url": "https://en.wikipedia.org/wiki?curid=28491", "title": "Submachine gun", "text": "Submachine gun\n\nA submachine gun (SMG) is a magazine-fed, automatic carbine designed to fire pistol cartridges. The term \"submachine gun\" was coined by John T. Thompson, the inventor of the Thompson submachine gun.\n\nThe submachine gun was developed during World War I (1914–1918). At its zenith during World War II (1939–1945), millions of SMGs were made. After the war, new SMG designs appeared frequently. However, by the 1980s, SMG usage decreased. Today, submachine guns have been largely replaced by assault rifles, which have a greater effective range and are capable of penetrating the helmets and body armor used by modern infantry. However, submachine guns are still used by military special forces and police SWAT teams for close quarters battle (CQB) because they are \"a pistol-caliber weapon that's easy to control, and less likely to over-penetrate the target.\"\n\nDuring World War I, the Austrians introduced the world's first machine pistol the Steyr Repetierpistole M1912/P16. The Germans also experimented with machine pistols by converting pistols such as the Mauser C96 and Luger P-08 from semiautomatic to fully automatic operation and adding detachable stocks. Carbine-type automatic weapons firing pistol rounds were developed during the latter stages of World War I by Italy, Germany and the United States. Their improved firepower and portability offered an advantage in trench warfare.\n\nIn 1915, the Italians introduced the Villar-Perosa aircraft machine gun. It fired pistol-caliber 9mm Glisenti ammunition, but was not a true submachine gun, as it was originally designed as a mounted weapon. This odd design was then modified into the OVP 1918 carbine-type submachine gun, which then evolved into the 9×19mm Parabellum Beretta Model 1918 after the end of World War I. Both the OVP 1918 and the Beretta 1918 had a traditional wooden stock, a 25-round top-fed box magazine, and had a cyclic rate of fire of 900 rounds per minute.\nThe Germans initially used heavier versions of the P08 pistol equipped with a detachable stock, larger-capacity snail-drum magazine and a longer barrel. By 1918, Bergmann Waffenfabrik had developed the 9 mm Parabellum MP 18, the first practical submachine gun. This weapon used the same 32-round snail-drum magazine as the Luger P-08. The MP 18 was used in significant numbers by German stormtroopers employing infiltration tactics, achieving some notable successes in the final year of the war. However, these were not enough to prevent Germany's collapse in November 1918. After World War I, the MP 18 would evolve into the MP28/II SMG, \nwhich incorporated a simple 32-round box magazine, a semi & full auto selector, and other minor improvements.\nThe .45 ACP Thompson submachine gun had been in development at approximately the same time as the Bergmann and the Beretta. However, the war ended before prototypes could be shipped to Europe. Although it had missed its chance to be the first purpose-designed submachine gun to enter service, it became the basis for later weapons and had the longest active service life of the three.\n\nIn the interwar period the \"Tommy Gun\" or \"Chicago Typewriter\" became notorious in the U.S. as a gangster's weapon; the image of pinstripe-suited James Cagney types wielding drum-magazine Thompsons caused some military planners to shun the weapon. However, the FBI and other U.S. police forces themselves showed no reluctance to use and prominently display these weapons. Eventually, the submachine gun was gradually accepted by many military organizations, especially as World War II loomed, with many countries developing their own designs.\n\nThe Italians were among the first to develop submachine guns during World War I. However, they were slow to produce them during World War II. The 9 mm Parabellum Beretta Model 1938 was not available in large numbers until 1943. The 38 was made in a successive series of improved and simplified models all sharing the same basic layout. The Beretta has two triggers, the front for semi-auto and rear for full-auto. Most models use standard wooden stocks, although some models were fitted with an MP 40-style under-folding stock and are commonly mistaken for the German SMG. The 38 series was extremely robust and proved very popular with both Axis forces and Allied troops (who used captured Berettas). It is considered the most successful and effective Italian small arm of World War II. The 38 series is the longest serving of the world's SMGs, as later models can still be seen in the hands of Italian military and police forces.\nIn 1939, the Germans introduced the 9 mm Parabellum MP38 during the invasion of Poland. However, the MP38 production was still just starting and only a few thousand were in service at the time. It proved to be far more practical and effective in close quarters combat than the standard-issue German Kar 98K bolt-action rifle. From it, the nearly identical MP40 (commonly and erroneously referred to as Schmeisser) was developed and made in large numbers; about a million were made during World War II. The MP40 was lighter than the MP38. It also used more stamped parts, making it faster and cheaper to produce. The MP38 and MP40 were the first SMGs to use plastic furniture and a practical folding stock. They would set the fashion for all future SMG designs.\nDuring the Winter War, the badly outnumbered Finnish used the Suomi KP/-31 in large numbers against the Russians with devastating effect. Finnish ski troops became known for appearing out of the woods on one side of a road, raking Soviet columns with SMG fire and disappearing back into the woods on the other side. During the Continuation War, the Finnish Sissi patrols would often equip every soldier with KP/-31s. The Suomi fired 9 mm Parabellum ammo from a 71-round drum magazine (although often loaded with 74 rounds). \"This SMG showed to the world the importance of the submachine gun to the modern warfare\", prompting the development, adoption and mass production of submachine guns by most of the World's armies. The Suomi was used in combat until the end of the Lapland war, was widely exported and remained in service to the late 1970s.\nIn 1940, the Russians introduced the 7.62×25mm PPD-40 and later the more easily manufactured PPSh-41 in response to their experience during the Winter War against Finland. The PPSh's 71-round drum magazine is a copy of the Suomi's. Later in the war they developed the even more readily mass-produced PPS submachine gun. The USSR would go on to make over 6 million PPSh-41s and 2 million PPSs by the end of World War II. Thus, the Soviet Union could field huge numbers of submachine guns against the Wehrmacht, with whole infantry battalions being armed with little else. Even in the hands of conscripted soldiers with minimal training, the volume of fire produced by massed submachine guns could be overwhelming.\nIn 1941, Britain adopted the 9 mm Parabellum Lanchester submachine gun. Following the Dunkirk evacuation, and with no time for the usual research and development for a new weapon, it was decided to make a direct copy of the German MP 28. However this gun, the Lanchester, proved to be difficult and expensive to manufacture. Shortly thereafter, the simpler STEN submachine gun was developed, which was much cheaper and faster to make. Over 4 million STEN Guns were made during World War II. The STEN was so cheap and easy to make that Germany started manufacturing their own copy (the MP 3008) towards the end of World War II. After the war, the British replaced the STEN with the Sterling submachine gun. Britain also used many M1928 Thompson submachine guns during World War II.\nThe United States and its allies used the Thompson submachine gun, especially the simplified M1. However, the Thompson was still expensive and slow to produce. Therefore, the U.S. developed the M3 submachine gun or \"Grease Gun\" in 1942, followed by the improved M3A1 in 1944. While the M3 was no more effective than the Tommy Gun, it was made primarily of stamped parts and welded together, and so, it could be produced much faster and at fraction of the cost of a Thompson. It could be configured to fire either .45 ACP or 9mm Luger ammunition. The M3A1 was among the longest serving submachine guns designs, being produced into the 1960s and serving in US forces into the 1990s.\nThe Owen Gun is a 9mm Parabellum Australian submachine gun designed by Evelyn Owen in 1939. The Owen is a simple, highly reliable, open bolt, blowback SMG. It was designed to be fired either from the shoulder or the hip. It's easily recognisable, owing to its unconventional appearance, including a quick-release barrel and butt-stock, double pistol grips, top-mounted magazine, and unusual offset right-side mounted sights. The Owen was the only entirely Australian-designed and constructed service submachine gun of World War II and was used by the Australian Army from 1943 until the mid-1960s, when it was replaced by the F1 submachine gun. Only about 45,000 Owens were produced during the war for a unit cost of about A$30.\n\nAfter World War II, \"new submachine gun designs appeared almost every week to replace the admittedly rough and ready designs which had appeared during the war. Some (the better ones) survived, most rarely got past the glossy brochure stage.\" Most of these survivors were cheaper, easier and faster to make than their predecessors. As such, they were widely distributed.\nIn 1945, Sweden introduced the 9 mm Parabellum Carl Gustav M/45 with a design borrowing from and improving on many design elements of earlier submachine-gun designs. It has a tubular stamped steel receiver with a side folding stock. The M/45 was widely exported, and especially popular with CIA operatives and U.S. Special Forces during the Vietnam War. In U.S. service it was known as the \"Swedish-K\". In 1966, the Swedish government blocked the sale of firearms to the United States because it opposed the Vietnam War. As a result, in the following year Smith & Wesson began to manufacture an M/45 clone called the M76.\n\nIn 1946, Denmark introduced the Madsen M-46, and in 1950, an improved model the Madsen M-50. These 9 mm Parabellum stamped steel SMGs featured a unique clamshell type design, a side folding stock and a grip-safety on the magazine housing. The Madsen was widely exported and especially popular in Latin America, with variants made by several countries.\nIn 1948, Czechoslovakia introduced the Sa vz. 23 series. This 9 mm Parabellum SMG introduced several innovations: a progressive trigger for selecting between semi-automatic and full auto fire, a telescoping bolt that extends forward wrapping around the barrel and a vertical handgrip housing the magazine and trigger mechanism. The vz. 23 series was widely exported and especially popular in Africa and the Middle East with variants made by several countries. The vz. 23 inspired the development of the Uzi submachine gun.\nIn 1949, France introduced the MAT-49 to replace the hodgepodge of French, American, British, German and Italian SMGs in French service after World War II. The 9 mm Parabellum MAT-49 is an inexpensive stamped steel SMG with a telescoping wire stock, a pronounced folding magazine housing and a grip safety. This \"wildebeast like design\" proved to be an extremely reliable and effective SMG, and was used by the French well into the 1980s. It was also widely exported to Africa, Asia and the Middle East.\n\nIn 1954, Israel introduced a 9 mm Parabellum open-bolt, blowback-operated submachine gun called the Uzi (after its designer Uziel Gal). The Uzi was one of the first weapons to use a telescoping bolt design with the magazine housed in the pistol grip for a shorter weapon. The Uzi has become the most popular submachine gun in the world, with over 10 million units sold, more\nthan any other submachine gun.\nIn 1959, Beretta introduced the Model 12. This 9 mm Parabellum submachine gun was a complete break with previous Beretta designs. It is a small, compact, very well made SMG and among the first to use telescoping bolt design. The M12 was designed for mass production and was made largely of stamped steel and welded together. It is identified by its tubular shape receiver, double pistol grips, a side folding stock and the magazine housed in front of the trigger guard. The M12 uses the same magazines as the Model 38 series.\n\nIn the 1960s, Heckler & Koch developed the 9 mm Parabellum MP5 submachine gun. The MP5 is based on the G3 rifle and uses the same closed-bolt roller-delayed blowback operation system. This makes the MP5 more accurate than open-bolt SMGs, such as the UZI. The MP5 is also one of the most widely used submachine guns in the world, having been adopted by 40 nations and numerous military, law enforcement, intelligence, and security organizations.\nIn 1969, Steyr introduced the MPi 69. This 9 mm Parabellum open-bolt, blowback-operated SMG has telescoping bolt and is similar in appearance to the Uzi SMG. It has a vertical pistol-grip into which the magazine is inserted, a longer horizontal front grip area and a telescoping wire butt-stock. The receiver is a squared stamped steel tube which partly nestles inside a large plastic molding (resembling a lower receiver) which contains the forward hand-grip, vertical pistol-grip and the fire control group. Making the MPi 69 one of the first firearms to use a plastic construction in this way. It has a progressive trigger and is also unusual among modern SMGs, as the MPi 69 is cocked by a dual-purpose lever also used as the front sling attachment point.\n\nIn the 1970s, extremely compact submachine guns, such as the .45ACP Mac-10 and .380 ACP Mac-11, were developed to be used with silencers or suppressors. While these SMGs received enormous publicity, and were prominently displayed in films and television, they were not widely adopted by military or police forces. These smaller weapons led other manufacturers to develop their own compact SMGs, such as the Micro-UZI and the H&K MP5K.\n\nBy the 1980s, the demand for new submachine guns was very low and could be easily met by existing makers with existing designs. However, following H&Ks lead, other manufacturers began designing submachine guns based on their existing assault rifle patterns. These new SMGs offered a high degree of parts commonality with parent weapons, thereby easing logistical concerns.\n\nIn 1987, Colt introduced the Colt 9mm SMG based on the M16 rifle. The Colt SMG is a closed bolt, blowback operated SMG and the overall aesthetics are identical to most M16 type rifles. The magazine well is modified using a special adapter to allow the use of smaller 9mm magazines. The magazines themselves are a copy of the Israeli UZI SMG magazine, modified to fit the Colt and lock the bolt back after the last shot. The Colt is widely used by U.S. police forces and the USMC.\n\nIn 1998, H&K introduced the last widely distributed SMG, the UMP \"Universal Machine Pistol\". The UMP is a 9mm, .40 S&W, or .45 ACP, closed-bolt blowback-operated SMG, based on the H&K G36 assault rifle. It features a predominantly polymer construction and was designed to be a lighter and cheaper alternative to the MP5. The UMP has a side-folding stock and is available with four different trigger group configurations. It was also designed to use a wide range of Picatinny rail mounted accessories \n\nIn 2004, Izhmash introduced the Vityaz-SN a 9 mm Parabellum, closed bolt straight blowback operated submachine gun. It is based on the AK-74 rifle and offers a high degree of parts commonality with the AK-74. It is the standard submachine gun for all branches of Russian military and police forces.\n\nBy 2010, compact assault rifles and personal defense weapons had replaced submachine guns in most roles. Factors such as the increasing use of body armor and logistical concerns have combined to limit the appeal of submachine guns. However, SMGs are still used by police (especially SWAT teams) for dealing with heavily armed suspects and by military special forces units for close quarters combat, due to their reduced size, recoil and muzzle blast. Submachine guns also lend themselves to the use of suppressors, particularly when loaded with subsonic ammunition. Variants of the Sterling and Heckler & Koch MP5 have been manufactured with integral suppressors.\n\nDeveloped during the late 1980s, the personal defense weapon (PDW) is touted as a further evolution of the submachine gun. The PDW was created in response to a NATO request for a replacement for 9×19mm Parabellum submachine guns. The PDW is a compact automatic weapon that can defeat enemy body armor and which can be used conveniently by non-combatant and support troops, and as a close quarters battle weapon for special forces and counter-terrorist groups.\n\nIntroduced in 1991, the FN P90 features a bullpup design with a futuristic appearance. It has a 50-round magazine housed horizontally above the barrel, an integrated reflex sight and fully ambidextrous controls. A simple blowback automatic weapon, it was designed to fire the FN 5.7×28mm cartridge which can penetrate soft body armor. The P90 was designed to have a length no greater than a man's shoulder width, to allow it to be easily carried and maneuvered in tight spaces, such as the inside of an armored vehicle. The P90 is currently in service with military and police forces in over 40 nations.\n\nIntroduced in 2001, the Heckler & Koch MP7 is a direct rival to the FN P90. It is a more conventional-looking design. The MP7 uses a short-stroke piston gas system as used on H&K's G36 and HK416 assault rifles, in place of a blowback system traditionally seen on submachine guns. The MP7 uses 20-, 30- and 40-round magazines and fires 4.6×30mm ammunition which can penetrate soft body armor. Due to the heavy use of polymers in its construction, the MP7 is much lighter than older SMG designs, being only with 20-round empty magazine. The MP7 is currently in service with military and police forces in over 20 nations\n\nThere are some inconsistencies in the classification of submachine guns. British Commonwealth sources often refer to SMGs as \"machine carbines\". Other sources refer to SMGs as \"machine pistols\" because they fire pistol-caliber ammunition, for example, the MP-40 and MP5, where \"MP\" stands for \"Maschinenpistole\" (\"Submachine gun\" in German, but cognate with the English term \"Machine pistol\"). However, the term \"machine pistol\" is also used to describe a handgun-style firearm capable of fully automatic or burst fire, such as the Stechkin, Beretta 93R and the H&K VP70. Also, Personal Defense Weapons such as the FN P90 and H&K MP7 are often called submachine guns. In addition, some compact assault rifles, such as the Colt XM177, HK53 and AKS-74U, have been historically referred to as submachine guns as they served in the latter's role.\n\n\n"}
{"id": "1616701", "url": "https://en.wikipedia.org/wiki?curid=1616701", "title": "Tactile signing", "text": "Tactile signing\n\nTactile signing is a common means of communication used by people with both a sight and hearing impairment (see Deafblindness), which is based on a sign language or other system of manual communication.\n\n\"Tactile signing\" refers to the mode or medium i.e. signing (using some form of signed language or code) using \"touch\". It does not indicate whether the signer is using a tactile form of a natural language e.g. American Sign Language (ASL) a modified form of such a visual sign language, a modified form of a manual code for English Manually Coded English or something else. It has also been referred to as \"hand over hand\" referring to the position of the listener vis. the signer.\n\nSeveral methods of Deafblind communication may be referred to as tactile signing, including:\n\nWe all adapt to our imperfect body, using glasses, crutches and so on. Human beings also adapt to an environment, for example by modifying our diet to fit the local ecology. (See adaptation.)\nUntil the 1970s it was rare for a person to be both deaf and blind and most people who were deaf and blind lived lives of isolation. As professionals became aware of this population attempts were made to serve both adults and children who are deaf and blind by modifying the manual alphabet mentioned above, or the Sign Language used by deaf-sighted people. See, for example Helen Keller National Center, LightHouse for the Blind and Visually Impaired and Alabama Institute for the Deaf and Blind.\n\nProfessionals developed ways to communicate with their clients/students/patients who were deafblind. These are listed below.\n\nAdditionally, simple ways of responding such as a tap for 'yes' or a rubbing motion for 'no' may be included. In Japan, a system developed by a deafblind woman is in use to represent the five vowels and five major consonants of the Japanese language on the fingers, where the signer 'types' onto a table and the receiver places their hands on top to 'listen' (see this page for more info).\n\nWhat was especially challenging was communicating with children or babies born deaf and blind who had not had an opportunity to learn a natural (spoken or signed) language. Below are listed some of these attempts.\n\nAs the decades progressed, deafblind people began to form communities where a tactile language is being born. Just as deaf people brought together in community first used an invented form of the spoken language and then created/evolved their own natural language that suits the life of a deaf-sighted person (i.e. a visual language) so too, deafblind people in community have first used a modified form of a visual language and are now creating/evolving their own natural tactile/tactual language. For the development of visual sign languages see for example: Deaf Education; List of sign languages; Nicaraguan Sign Language.\nOne of the most active communities is in the Seattle area of Washington State. See Washington State DeafBlind Citizens .\n\nLittle data exists on the specifics of variation between visual and tactile sign language use. However, what studies there are suggest a significant degree of difference. In hand-over-hand signing, elements of deaf sign languages known as 'non-manual features' (such as facial expression) will not be received, and will need to be substituted with supplementary information produced manually. Common non-manual features used in Deaf Sign languages that are absent in tactile signing include raised eyebrows as a question marker and a shaking head as a negation.\n\nTactile signing is also contained within a smaller space than is typical in visual sign language. Signs that touch the body may be moved forward into a more neutral space. Other signs which are usually produced in an 'out of range' location (such as the leg) may be modified (either spelled or a variant sign used).\n\nDifferent rules govern turn taking, greetings and goodbyes.\n\nWhen interacting with deaf-blind people, a number of considerations can smooth the interaction.\n\nMany deafblind people make the most of their remaining sight, so the right lighting is vital. Mostly bright, even light is best (avoid glare), but some prefer dim light, so it is best to ask.\n\nSusie Morgan suggests the following guidelines for appearance and attire of interpreters working with deafblind clients:\n\nWear clothes that provide contrast for your hands. Consider the following when selecting clothing:\n\nIt is better to avoid jewelry which can be distracting, either tactually (e.g. rings and bracelets) or visually (e.g. sparkling drop earrings). Fingernails should also be short and smooth. A neutral color of nail polish may be worn, but bright reds and dark colors can be too strong. Working in close proximity to clients when using tactile sign, interpreters need to be aware of strong smells such as perfumes, cigarette smoke or onion breath.\n\nTactile signing can also be exhausting for both the interpreter and the deafblind client. Breaks are even more important than with regular interpreting, and should be taken more often. Correct seating can also reduce the risk of strain of injury; both communication partners should be comfortable and at an equal height. Specially designed cushioned tables for tactile signing can be employed.\n\nIn 1648 in England, John Bulwer wrote of a couple who were proficient in tactile sign communication: \"A pregnant example of the officious nature of the Touch in supplying the defect or temporall incapacity of the other senses we have in one Master Babington of Burntwood in the County of Essex, an ingenious gentleman, who through some sicknesse becoming deaf, doth notwithstanding feele words, and as if he had an eye in his finger, sees signes in the darke; whose Wife discourseth very perfectly with him by a strange way of Arthrologie or Alphabet contrived on the joynts of his Fingers; who taking him by the hand in the night, can so discourse with him very exactly; for he feeling the joynts which she toucheth for letters, by them collected into words, very readily conceives what shee would suggest unto him. By which examples [referring to this case and to that of an abbot who became deaf, dumb, and blind, who understood writing traced upon his naked arm] you may see how ready upon any invitation of Art, the Tact is, to supply the defect, and to officiate for any or all of the other senses, as being the most faithful sense to man, being both the Founder, and Vicar generall to all the rest.\"\n\n\n\n"}
{"id": "38094824", "url": "https://en.wikipedia.org/wiki?curid=38094824", "title": "Target indicator", "text": "Target indicator\n\nTarget Indicators, also known as target markers or TI's for short, were flares used by the RAF's Bomber Command during World War II. TI's were normally dropped by the Pathfinders on the target, providing an easily seen visual aiming point for the following bombers to aim at. After their introduction, the use of TI's expanded to include en-route markers to gather up lost aircraft, additional drops of TI's to keep the target lit over long periods, and various changes in technique to address German defences.\n\nThe use of TI's allowed the RAF to concentrate its advanced navigational systems in the Pathfinder units. Most widely used were the H2S radar and Oboe system, the former requiring considerable training to be useful, the latter able to guide only a single aircraft at a time. The limited number of navigational units meant that spreading them through the force would have limited effects. By concentrating these in a single Group and having them drop TI's, the accurate fixes could be used to guide the entire attack. The same basic system had been used by the Luftwaffe's Kampfgruppe 100 during The Blitz, for similar reasons.\n\nTarget Indicators were available in various colours, and during a raid bomb aimers would be instructed by the Master Bomber to drop their bombs on the target indicators of a specified colour, marker aircraft carrying different colours should the initial target indicators be dropped off target. The first target indicators could be cancelled over the radio by the Master Bomber and the Marker crews instructed to drop new target indicators of a different colour, until the correct aiming point was correctly marked. The Main Force bombers would then be instructed by the Master Bomber to bomb the colour of the most accurate target indicators.\n\nTarget Indicators could be fuzed for both air and ground burst, the air burst markers - referred to as 'sky marking' by the RAF - resembling bunches of grapes or upside down fir trees when detonated in the air. These the Germans called 'Christmas trees' due to their shape.\n\nMarking of targets was carried out by the following methods:\n\n\nThe code words were initially chosen by asking three Bomber Command personnel in the operations room where they came from. One was from Newhaven, England, one from Parramatta, New South Wales, and one from Wanganui, New Zealand.\n\nOboe was usually carried by Pathfinder de Havilland Mosquitoes.\n"}
{"id": "43068645", "url": "https://en.wikipedia.org/wiki?curid=43068645", "title": "The Kitchen Library", "text": "The Kitchen Library\n\nThe Kitchen Library was the first non-profit lending library of kitchen appliances in Canada. Since opening its doors inside the Toronto Tool Library in October 2013, The Kitchen Library has received local, national, and international media attention and community support. The Kitchen Library moved to Yonge and Eglinton (inside Living City Health) in October 2014 where they lend appliances and teach workshops. It closed Sep 1, 2016.\n\nAs population increases and the average size of condos decrease, they believe that space and income shouldn't be barriers to cooking and healthy eating. By providing access to otherwise costly and space-consuming appliances they are building a more shareable city for the future.\n\nIn 2014 they were named by Canadian Living as one of the 7 Canadian inventions that make your life better, and have been featured in Toronto Life, The Toronto Star, The National Post, and CBC News.\n\n\n"}
{"id": "15565698", "url": "https://en.wikipedia.org/wiki?curid=15565698", "title": "They're Made Out of Meat", "text": "They're Made Out of Meat\n\n\"They're Made Out of Meat\" is a short story by Terry Bisson. It was originally published in \"OMNI\". It consists entirely of dialogue between two characters. Bisson's website hosts a theatrical adaptation. A film adaptation won the Grand Prize at the Seattle Science Fiction Museum's 2006 film festival.\n\nThe story was collected in the 1993 anthology \"Bears Discover Fire and Other Stories\", and has circulated widely on the Internet, which Bisson finds \"flattering\". It has been quoted in cognitive, cosmological, and philosophical scholarship.\n\nThe two characters are intelligent beings capable of traveling faster than light, on a mission to \"contact, welcome and log in any and all sentient races or multibeings in this quadrant of the Universe.\" Bisson's stage directions represent them as \"two lights moving like fireflies among the stars\" on a projection screen. One of them tells the incredulous other about the recent discovery of carbon-based lifeforms \"made up entirely of meat\". After conversing briefly about it, they both deem such beings and communication with them too bizarre and agree to \"erase the records and forget the whole thing\", marking the Solar System \"unoccupied\".\n\nIn 2005, Stephen O'Regan wrote and directed a live film adaptation starring Tom Noonan and Ben Bailey. The film was made as a final project for the New York Film Academy. The main action takes place inside of a diner full of teenagers. The music for the film was scored by Bob Reynolds.\n\nIn 2010, Jeff Frumess and Trevor Scott wrote and directed their own iteration, adding the character of a homeless conspiracy theorist with an original score by musician Sam Belkin. The film was shot at the Hartsdale train platform on the Merto North in Westchester, NY.\n\nIn 2015, musical producer Jose Hoffman created a song based on the story.\n\n\n"}
{"id": "34417396", "url": "https://en.wikipedia.org/wiki?curid=34417396", "title": "Thomas Thorp (scientific instrument manufacturer)", "text": "Thomas Thorp (scientific instrument manufacturer)\n\nThomas Thorp (1850–1914) was an English manufacturer of scientific instruments credited with inventing the first practical coin-in-the-slot gas meter, with innovations in the field of photography, including that involving colour, and for producing an early example of what has since been developed into the modern spectrohelioscope. He began his working life as an apprentice to a firm of architects and ended it as a Fellow of the Royal Astronomical Society, having had a keen interest in astronomy since childhood.\n\nThomas Thorp was born on 26 October 1850, the son of a farmer who worked land around Narrow Lane (now known as Victoria Avenue), Besses o' th' Barn, Whitefield, Lancashire. He was educated first at the local Park Lane School and subsequently at Manchester Grammar School, following which he was apprenticed to Maycock and Bell, a firm of architects and surveyors in Manchester. During his apprenticeship, he had a role in the town-planning of New Brighton, which was then rising as a Victorian resort town. He had an interest in the practical application of scientific knowledge, as did his inventive father, and a consequence of this trait was an inclination towards civil and mechanical engineering. Later in life Thorp was to be the appointed engineer for both the Local Board of Whitefield and also its District Council.\n\nAs early as 1872, while still working for Maycock and Bell, he was sole patentee of a mechanism for tap valves. In 1875, he was jointly named with A. P. Bell, architect, on a patent application regarding improvements to the automation of gas lighting, and continued to design items for the gas industry thereafter. In 1880 he established a business for the manufacture of his designs, originally known as Thomas Thorp & Co., with premises at Narrow Lane, Whitefield.\n\nOne significant innovation was the design, patented in 1889, for the first practical mechanism that allowed gas to be dispensed by insertion of a penny into a meter, an early point-of-use prepayment system. Although frequently attributed as being his invention alone, the patent was jointly held with Thomas Gardiner Marsh. The device was a major advance and, for example, the Manchester Institution of the Association of Gas Engineers and Manufacturers reported in 1898 that \n\nDespite its practicality, the meter design of Thorp and Marsh was rapidly superseded and by 1894 J. Nasmith was reporting that it was \"practically out of the running\". The Edinburgh-based William Cowan and the Manchester business of Sawyer and Purves were among those who had been granted patents for improvements to the mechanism by that time.\n\nAround the same time, Thorp and Marsh were among the many people experimenting with the possibilities of using acetylene gas as an alternate to coal gas. Acetylene – produced by mixing calcium carbide and water – was thought to produce a better quality of light but there were practical difficulties to overcome. They discovered that adding the carbide to the water was less dangerous than adding the water to the carbide, and they surmounted problems related to gradual \"choking\" of the burners when in use by mixing air with the product. They also invented a safety valve that they claimed cut off the burner outlet in the not unusual event that the supply to it was interrupted, thus preventing unlit gas from being emitted when the supply was restored. This valve was capable of use in both acetylene and coal gas installations.\n\nThorp also invented the rotary gas meter in 1902, and, what his obituary describes as \"a tiny device ... for economical regulation of gas delivery [that] has been very widely employed by manufacturers and public authorities.\" Other inventions included a push-tap for water and improvements to gas lamps and pneumatic tools.\n\nThorp retired from business at the age of 50 to devote more time to his scientific studies. An advertisement of 1906 shows Marsh as chairman of the Manchester-based Rotary Meter Company Ltd and director of a similarly named business in New York. The latter had bought the sole US and Canadian manufacturing rights for all products regarding which Thorp held or might in future hold US patents.\n\nThorp was a \"scientific and mechanical genius\". In pursuit of his childhood interest in astronomy, he developed considerable skills in the manufacture of optical glass and both reflector and refractor telescopes. He also created celluloid diffraction grating replicas, polarising solar eyepieces and prominence spectroscopes that were widely used, as well as objective prisms. His high resolution diffracting grating replicas were significant advances of the ideas developed by Henry Rowland, whilst his multi-slit spectroscope was the first to enable the showing of both celestial and terrestrial objects and has formed the basis for the present day spectrohelioscope. Father Aloysius Cortie, of Stonyhurst College, Lancashire, was one whom achieved good results from an example of the objective prisms. Eventually owning houses in Whitefield and also in Prestatyn, North Wales, he constructed observatories at each of these and favoured the Cooke equatorial telescope for his own observations. This device was fitted with a photo-visual objective, a spectroscope and other attachments that enabled it to be used for photography.\n\nThorp's innovative design for celluloid diffraction grating replicas won him the Wilde Premium for Original Research in 1901, principally in recognition of his paper of 1899 entitled \"Grating Films and their application to Colour Photography\". He was also recognised by awards from other bodies, including international exhibitions. His diffraction gratings were particularly noted in an obituary, which said that they made Prior to his developments, details of which he published but did not patent, interested people had to rely upon the photographic methods that had been proposed by Lord Rayleigh and Izarn. Thorp's \"brilliant idea\", announced in 1898 and improved thereafter through experimentation and the development of new materials, was to take \"a cast of the ruled surface in a transparent medium [comprising] a thin solution of celluloid in amyl acetate.\" He described his method on numerous occasions, including in a letter published in the edition of 29 December 1905 of the \"British Journal of Photography\" (reproduced in \"Popular Astronomy\", 1906), He emphasised the need for a dust-free environment and a uniform drying of the solution as well as asserting that his was the \"first method of producing optically useful replicas of gratings and one which after all has not in my opinion been superseded.\" By 1900 he was able to apply his developments to colour photography and he went on to demonstrate to the British Astronomical Association a method for projecting natural colours that used \"three replicas adjusted to diffract the proper colour sensations to the eye\".\nHis work with celluloid was not limited to investigation of its uses in astronomy. He reported in 1902 that he had demonstrated the explosive nature of the substance by substituting it for the cordite in a bullet and then firing that bullet through a thick piece of wood. Similarly, although he used partial vacuums in his development of his diffraction gratings, he also investigated their use elsewhere as, for example, in his paper of 1903 entitled \"On the Production of Polished Metallic Surfaces having the Properties of Japanese \"Magic\" Mirrors\". These and numerous other contributions – including experiments in soldering aluminium and the use of optics for gunsights – were reported to the Manchester Literary and Philosophical Society, of which he had been elected a member on 21 January 1896. He served as member of the Society's Council for all but one year between 1902 and 1912, was a vice-president for three years and was offered, but refused, a nomination to be president.\n\nThorp was a member of expeditions to view the total solar eclipses of 1900 and 1905, which visited Algiers and Burgos, respectively. Examples of his inventions were used by various members of the parties.\n\nProposed by David G. Simpson, Thorp was elected a Fellow of the Royal Astronomical Society on 11 April 1902. He was also a vice-president of the Manchester Astronomical Society, of which he had been a member since 1892.\n\nThorp died at Prestatyn on 13 June 1914, leaving a widow and three sons, one of whom – Franklin – also had involvements in the gas industry. He was buried at All Saints' Church, Stand, a few days later. He also left a partially developed scheme for the manufacture of a ruling machine that obviated the need for a screw drive, being controlled instead by a mercury clepsydra and micrometre valve, and had been working on the design of a cinema screen. Thorp's Cooke telescope was given by the family to Stand Grammar School. One obituarist remarked on his \"unassuming and genial manner to all with whom he came into contact, and his readiness to explain and to make suggestions on any subject in which his wide knowledge could be of any assistance.\"\n\nThe business that he founded, Thomas Thorp & Co. Ltd, was finally liquidated in 1976.\n\nNotes\n\nCitations\nBibliography\n\n"}
{"id": "4701565", "url": "https://en.wikipedia.org/wiki?curid=4701565", "title": "Title 21 of the Code of Federal Regulations", "text": "Title 21 of the Code of Federal Regulations\n\nTitle 21 is the portion of the Code of Federal Regulations that governs food and drugs within the United States for the Food and Drug Administration (FDA), the Drug Enforcement Administration (DEA), and the Office of National Drug Control Policy (ONDCP).\n\nIt is divided into three chapters:\n\nMost of the Chapter I regulations are based on the Federal Food, Drug, and Cosmetic Act.\n\nNotable sections:\n\n\nThe 100 series are regulations pertaining to food:\n\n\nThe 200 and 300 series are regulations pertaining to pharmaceuticals :\n\n\nThe 500 series are regulations for animal feeds and animal medications:\n\n\nThe 600 series covers biological products (e.g. vaccines, blood):\n\n\nThe 700 series includes the limited regulations on cosmetics:\n\n\nThe 800 series are for medical devices:\n\n\nThe 900 series covers mammography quality requirements enforced by CDRH.\n\nThe 1000 series covers radiation-emitting device (e.g. cell phones, lasers, x-ray generators); requirements enforced by the Center for Devices and Radiological Health. It also talks about the FDA citizen petition.\n\nThe 1100 series includes updated rules deeming items that statutorily come under the definition of \"tobacco product\" to be subject to the Federal Food, Drug, and Cosmetic Act as amended by the Tobacco Control Act. The items affected include E-cigarettes, Hookah tobacco, and pipe tobacco.\n\nThe 1200 series consists of rules primarily based in laws other than the Food, Drug, and Cosmetic Act:\n\n\nNotable sections:\n\n\nNotable sections:\n\n\n\n"}
{"id": "23584473", "url": "https://en.wikipedia.org/wiki?curid=23584473", "title": "Vacuum engine", "text": "Vacuum engine\n\nA vacuum engine (also called flame-licker engine, flame-engine, flame-dancer) derives its force from air pressure against one side of the piston, which has a partial vacuum on the other side of it. At the beginning of an outstroke, a valve in the head of the cylinder opens and admits a charge of burning gas and air, which is trapped by the closing of the valve and expands. Towards the end of the stroke the charge comes into contact with a water- or air-cooled part of the cylinder and is chilled, causing a sudden drop in pressure sufficient to suck the piston – which is open towards the crank – back on the return stroke. The valve opens again in time for the piston to expel the burnt gases before the next outstroke begins.\n\nSome early gas engines worked on the \"vacuum\" or \"atmospheric\" principle in a similar way to the Newcomen steam engine. A mixture of gas and air was drawn into the cylinder and ignited; the mixture expanded and part of it escaped through the exhaust valve; the valve then closed, the mixture cooled and contracted, and atmospheric pressure pushed the piston in. Such engines were very inefficient and were superseded by engines working on the Otto cycle.\n\nIn a vacuum motor, the partial vacuum is created by an external pump. These motors were commonly used to power railway turntables in the UK, using vacuum created by a steam locomotive's vacuum brake ejector. The operating principle is similar to a steam engine – in both cases power is extracted from a difference in pressure. \n\nSmall vacuum motors were also used to operate windscreen wipers in automobiles. In this case, the motors were powered by manifold vacuum. This arrangement was not very satisfactory because, if the throttle were wide open, the wipers would slow down, or even stop. Modern automobiles use electrically powered wipers. Modern automobiles still use a vacuum motor of a kind, however, the vacuum servo. Brakes are operated by a hydraulic system, but they use a ‘vacuum motor’ to amplify the force provided by the driver. Small vacuum motors were also used from the late 1960s to control servomechanisms such as door locks, heater controls or movable bonnet ventilation flaps.\n\nYou could say that the global Industrial Revolution arose because of a 'vacuum motor', because all the early steam engines, especially the pioneering Boulton and Watt engines, operated with almost atmospheric pressure steam. You can easily make a demonstration vacuum engine using a flywheel, simple plumbing parts and a few other simple components, as Neil A Downie shows in the reference.\n\nA vacuum system can be used for power transmission, although the maximum power that can be transmitted to a vacuum motor is less than conventional pneumatics. There is an optimum pressure for the operation of a vacuum power transmission system, of around 0.4 bar (8 psig), as Downie also shows. Although less efficient than pneumatics, it can be perfectly workable. For example a 22 mm (7/8\") pipe on vacuum can transmit as much power on 0.4 bar (8 psig) as a 6 mm (1/4\") pipe on 8 bar (100 psig). The system is efficient enough that Boulton and Watt used vacuum power transmission in their factory. They called the vacuum main in the factory the ‘spirit pipe’. \n\nUnlike the ideal Otto cycle engine, the vacuum engine relies on a constant heat source provided by burning fuel. As mentioned above, a valve allows an intake of heat into the piston chamber. Estimating the heat in or Qin is constant in the controlled volume space, the ideal gas equation PV = nRT implies an increase in the pressure of the piston chamber. After the valve closes, the piston undergoes an adiabatic process during the downward stroke. Once the piston reaches the bottom of its stroke, the chamber is cooled either by the surrounding air or water, and the resulting Qout forces the pressure in the piston to decrease. The system then undergoes another adiabatic compression of the gas in the chamber, which is subsequently released by the valve at the top of the cylinder's stroke, while simultaneously allowing new heated gas to enter the chamber.\n\nOne of the major issues that this engine encountered while being developed was that the efficiency of this model was extremely poor in real applications. Because the heat source is not contained to a specific area, only a small portion of the potential fuel is being consumed to power the engine. Because Engine efficiency is defined by the relationship between the amount of work done and the potential energy in the fuel consumed, it can be seen that in the vacuum engine only a small amount of the burning fuel is being used to power the engine. The rest of the fuel energy is lost to the surrounding atmosphere.\n\n\n"}
{"id": "46268782", "url": "https://en.wikipedia.org/wiki?curid=46268782", "title": "Volume combustion synthesis", "text": "Volume combustion synthesis\n\nVolume combustion synthesis (VCS) is method of chemical synthesis in which the reactants are heated uniformly in a controlled manner until a reaction ignites throughout the volume of the reaction chamber. The VCS mode is typically used for weakly exothermic reactions that require preheating prior to ignition.\n"}
{"id": "24168588", "url": "https://en.wikipedia.org/wiki?curid=24168588", "title": "Wallpaper steamer", "text": "Wallpaper steamer\n\nA wallpaper steamer is an electrical device which boils water continuously in order to produce steam. This steam is then allowed to pass through a narrow bore tube to a face plate. This face plate is then held against a wallpapered wall. The steam passes through the paper, saturates the backing paper and partially dissolves the desiccated wallpaper paste. This then allows the wallpaper to be pulled from, or scraped off the wall.\n\nThe wallpaper steamer was invented by Peter Ravenscroft Wilkins from Blackwell, Worcestershire, England. Its European patent was filed 19 November 1987.\n\nThe basic form of a wallpaper steamer is a semi-sealed container, usually made of plastic, that contains a heating element. A quantity of water is added to the container and the water comes into contact with the heating element. Once electrical power is connected to the element, it heats up which then in turn heats the water. The water reaches its boiling point, evaporates and produces steam. As the container is sealed, the steam pressure builds up but is released via a small opening in the container to which is connected a length of pressure pipe. At the end of the pressure pipe is a flat plate about square and 1 cm deep. The plate has a handle on its reverse side. This plate is then held up to the papered wall. The steam being released under pressure from the container travels along the pressure pipe and is released into the forward face of the steam plate. As the plate is held against the wall, a slight seal is maintained which holds steam long enough to force the moisture into the wallpaper, saturating it and rehydrating the wallpaper paste. Once the paste is rehydrated, it starts to lose its adhesive properties, allowing the wallpaper to be either pulled or scraped from the wall.\n\nPenetration of steam, especially with vinyl-coated wallpapers, may be accelerated by first perforating the surface of the wallpaper. A variety of spiked roller and spiked wheel hand tools are available for this.\n\nDuring use, the operator's hands are close to the steam source and so protective, heat- and vapour-proof gloves, such as silicone oven gloves, may be helpful.\n\nWallpaper steamers have been more popular in countries, such as the UK or Europe, using 230V domestic electricity. This provides around 3kW of steam generating capacity from all sockets around the house.\n\nA commercial wallpaper steamer is a conveniently packaged source of steam. This can be used for other purposes too, such as steam-bending wood.\n"}
{"id": "51435969", "url": "https://en.wikipedia.org/wiki?curid=51435969", "title": "Walter Fricke", "text": "Walter Fricke\n\nWalter Ernst Fricke (1April 191521March 1988) was a German distinguished professor of theoretical astronomy at the University of Heidelberg. He was a mathematician and cryptanalyst during the time of World War II in Inspectorate 7/VI from 1941-1942 (which would later become the General der Nachrichtenaufklärung, then in 1942 posted to Inspectorate 7/IV, then later transferred to OKW/Chi Section IIb. His speciality was the production of codes and ciphers, and the security studies of Army systems. After the war he was director of the Astronomical Calculation Institute (German: \"Astronomisches Rechen-Institut\") in Heidelberg, Germany.\n\nWalter Fricke was born in Leimbach-Mansfeld near Merseburg, Germany. His father was a carpenter who worked as a miner in the copper-schist mines at Mansfeld. Walter Fricke attended a high school Stephaneum in Aschersleben and passed the final examination (\"Abitur\") in 1934. After high school, he enrolled as a student at Frederick William University in East Berlin, studying astronomy, mathematics and physics. His teachers there included Paul ten Bruggencate and August Kopff in astronomy, Erhard Schmidt in mathematics and Max von Laue in physics.\n\nIn 1935, he published his first astronomy papers. These were critiques of Edwin Hubble's studies made at Mount Wilson Observatory on the distribution of spiral nebulae. In 1939, while resident at the Göttingen Observatory, he received his doctorate with a thesis titled \"Influence of a resisting agent in the dynamics of dense stellar systems\" (\"Einfluß eines widerstehenden Mittels in der Dynamik dichter Sternsysteme\"). He obtained a scholarship to Edinburgh University in Scotland, which was arranged with the help of British theoretical cosmologist Dr George C. McVittie, which was due to start in 1 October 1939 but which had to be cancelled because of the start of World War II in September 1939. On 1 May 1940 he started work at Hamburg Observatory, but later in that year was drafted into the Signal Corps (\"Nachrichtentruppe\"). On 15 May 1941 he was posted to Inspektorate 7, the cipher bureau of the Wehrmacht (German Armed Forces) high command, although as an astronomer he knew nothing about cryptography and cryptanalysis. Professor Otto Heckmann, director of the Hamburg Observatory, tried to lure him back to continue working on problems specifically related to war work that he had been occupied with before he was drafted: tables of air and ship navigation, aerodynamic problems for aeroplanes traveling at speeds over 1300 km/hour as well as rockets flying at speeds of more than 3000 km/hour. These were purely solutions to differential equations which were allocated to various institutions for solving.\n\nA minor planet discovered on 15 February 1941 by Karl Wilhelm Reinmuth in Heidelberg was named \"1561 Fricke\" in his honour. In 1942 he was appointed the assistant astronomer at the Hamburg Observatory at Bergedorf, but could only take the position up in 1946 due to conscription.\n\nIn 1943 Fricke married Marianne Fricke (\"née\" Traute). They had a daughter, Maxi-Marianne Fricke. His wife Marianne died in 1987.\n\nIn 1951, he received his habilitation from University of Hamburg. In 1953, after receiving a fellowship from the German Science Foundation, he went to the United States for a year, working at the Yerkes, Mount Wilson, Palomar and Princeton University Observatories. After returning, he became a tenured member (\"Wissenschaftlicher Rat\") of the Hamburg Observatory. In December 1954, Dr Fricke was made the provisional director of the Astronomical Calculation Institute in Heidelberg. Heidelberg University appointed him an honorary professor in 1955 and a personal full professor in 1958. In 1961 he became a regular full professor of theoretical astronomy at Heidelberg.\n\nFricke served as president of the International Astronomical Union Commissions 4 (1958–1964) and 8 (1970–1973), and as vice president of the IAU from 1964 to 1967.\n\nOn 1 April 1983 he retired as professor emeritus. Dr. Drh.c.mult Fricke stayed on as Director of the Institute until 30 September 1985, and continued his scientific work until he was hospitalized with cancer in 1987.\n\nFricke had a wide interest in astronomy. His first publications dealt with problems in theoretical and observational astronomy. Then for the next two decades, from the time of his thesis onwards, his interests focused on stellar dynamics, working from the observatory in Bergedorf. His favourite subject was the photographic surface photometry survey of the Andromeda Nebula.\n\nIn addition, in 1951 he published with Otto Heckmann and Pascual Jordan an important work for the extension of Einstein's theory of gravity.\n\nAfter being appointed to the Astronomical Calculation Institute, Fricke concentrated on fundamental astrometry. He worked specifically to improve the fundamental reference system, a series of measurements of the position and motions of a series of fundamental stars that is extremely important for study of kinematics and dynamics of objects within the Galactic system. The production of this kind of fundamental catalogue, which provided the astronomical representation of an inertial system, was part of the Institute's important work. His finest contribution to astronomy was the derivation, together with his colleagues and his predecessor August Kopff, of the Fourth Fundamental Catalogue (FK4), published in 1963. The FK5 catalogue was strongly associated with his name, but he was unable to view it when it was published; it used a new constant of precession which he derived himself and adopted by the IAU in 1976.\n\nFricke made significant contributions toward the establishment of the European Southern Observatory in 1962 and the Max Planck Institute for Astronomy (\"Max-Planck-Institut für Astronomie\") in Heidelberg, with its observatory in Calar Alto, Spain.\n\n\"See also\" Crytanalysis of Double Playfair.\n\nWhile in the military, Fricke studied German cipher methods and devised new ones.\n\nFricke's initial task was working on the solving of the double stop system, NS 42, code named the Double Playfair (\"Doppelkastenschlüssel\"). The Army, Air Force and Police used the Double Playfair system as a medium-grade hand cipher in World War II. The Playfair TS 42 single stop system had already been broken at that point if there were more than 3000 letters of traffic a day. After a year's work, they solved the problem by using vertical bigram frequencies. As the text was written in double lines of 17 or 21 and the substitutions taken vertically, plain text bigram frequencies could not be used. Combined frequencies of pairs of single letters showed a sharp drop after the top three values, EE, EN and NE (the last two had the same frequency). Of a text length of 10000, they could place the three values only initially. Hollerith counts (frequency analysis) were undertaken against messages from the Polish War, but as these were of a stereotypical nature, words could only be guessed after high-frequency digraphs (i.e. pairs of letters) had been created. Using this method led to the recovery of more pairs, and the guessing of words. However, the solution was never used, as even though they believed 3000 letters would be enough to break a message, the Army never informed them what the actual volume of traffic was, so the system continued in use.\n\nAround the same time, he worked on the French C36 cipher machine with fixed lugs, designed by Boris Hagelin, which was solved by cribs. He later heard from others that cribs were no longer needed.\n\nThe head of his section at OKH/Chi (Army Cipher Office) was Dr Pietsch, who had eight mathematicians under him. He remained there until 1 November 1944, when he was transferred to OKW/Chi. At this time Inspektorate 7 was forbidden to create new systems, which was strictly restricted to OKW/Chi. At OKW/Chi, Dr Fricke managed Section IIb of Chi II (Group 2), the main group managing OKW/Chi's interception services. Section IIb, which developed German code and cypher systems (camouflage, codes and cyphers, and telephone secrecy) and also advised on the production of keys and the supervision of production, had a staff of 14. OKW/Chi cipher bureau was a strictly military organization.\n\nIn 1942, he developed the codebook (\"Schlüsseltafel\") for enciphering tables for 3 letter field codes. Before that were used without encipherment. Daily changing trigraphic substitution tables were introduced, initially made by Hollerith machines. He stated in his TICOM interview that there were two master decks of 500 cards each, with a trigraph on each card. Late in the war, the Hollerith machine section moved to Weimar, so Walter Fricke told the printer to make up a set of three-letter slugs of type corresponding to the code groups, which were called \"Logotypen\". The printer had the plain code values in alphabetical order in a form, with blank spaces for the encrypted values. He was told to draw two logos from the mixed batch and place the second one opposite that plain value which was the same as the first. This procedure was repeated until the reciprocal table was created. It also had the effect of enabled untrained workers to make fewer mistakes.\n\nHe went on to design and develop the raster key hand cipher (\"Rasterschluessel 44\"), which was to replace the \"double playfair\" cipher. A study was made on the British cipher raster, Cysquare which was created by John Tiltman in 1941. The Cysquare cipher had been pinched from Britain, when Rommel's Afrika Korps overran British units and captured the Cysquare and pads with their instruction booklets. Fricke found it to be excellent, a very secure and practical hand cipher, but he didn't know if it had been broken, with the English using 40 letters and large number of abbreviations. He knew that if German forces used the cipher as it was, it soon would be broken because of longer messages. In order to use longer messages, 26 rows were created, of which 24 were used at any one time. The requirement was to choose all rasters from a systematically constructed field, and to satisfy the following requirements simultaneously:\n\n\nThese conditions were difficult to achieve, since as soon as one requirement was achieved the others would go wrong. The purpose of making all from the same field was to avoid special cases. However, they were eventually required to be made from 20 to 40 master fields. As regards changing keys, the printer was given lead strips bearing the pattern of each of the 36 rows, of which 24 were chosen for each raster. The minimum message length was initially set to 60 characters but was lowered to 45 by the army after some use. Fricke asked the TICOM interrogators\n\nThey reflected on the fact that the work on Russian systems showed that these systems were secure if properly used, but if the cryptographers in Moscow could only see how they were used they would be very unhappy.\n\nParts of this article have been sourced from TICOM document:\n\n"}
{"id": "1555022", "url": "https://en.wikipedia.org/wiki?curid=1555022", "title": "Web 2.0", "text": "Web 2.0\n\nWeb 2.0, also called Participative (or Participatory) and Social Web, refers to World Wide Web websites that emphasize user-generated content, usability (ease of use, even by non-experts), participatory culture and interoperability (this means that a website can work well with other products, systems, and devices) for end users. The term was invented by Darcy DiNucci in 1999 and popularized several years later by Tim O'Reilly and Dale Dougherty at the O'Reilly Media Web 2.0 Conference in late 2004. Web 2.0 does not refer to an update to any technical specification, but to changes in the way Web pages are designed and used. The transition was progressive and there is no precise date on which the change occurred.\n\nA Web 2.0 website may allow users to interact and collaborate with each other in a social media dialogue as creators of user-generated content in a virtual community, in contrast to the first generation of Web 1.0-era websites where people were limited to the passive viewing of content. Examples of Web 2.0 features include social networking sites and social media sites (e.g., Facebook), blogs, wikis, folksonomies (\"tagging\" keywords on websites and links), video sharing sites (e.g., YouTube), hosted services, Web applications (\"apps\"), collaborative consumption platforms, and mashup applications.\n\nWhether Web 2.0 is substantively different from prior Web technologies has been challenged by World Wide Web inventor Tim Berners-Lee, who describes the term as jargon. His original vision of the Web was \"a collaborative medium, a place where we [could] all meet and read and write.\" On the other hand, the term Semantic Web (sometimes referred to as Web 3.0) was coined by Berners-Lee to refer to a web of content where the meaning can be processed by machines.\n\nWeb 1.0 is a retronym referring to the first stage of the World Wide Web's evolution. According to Cormode, G. and, Krishnamurthy, B. (2008): \"content creators were few in Web 1.0 with the vast majority of users simply acting as consumers of content.\" Personal web pages were common, consisting mainly of static pages hosted on ISP-run web servers, or on free web hosting services such as GeoCities. With the advent of Web 2.0, it was more common for the average web user to have social networking profiles on sites such as Myspace and Facebook, as well as personal blogs on one of the new low-cost web hosting services or a dedicated blog host like Blogger or LiveJournal. The content for both was generated dynamically from stored content, allowing for readers to comment directly on pages in a way that was not previously common.\n\nSome Web 2.0 capabilities were present in the days of Web 1.0, but they were implemented differently. For example, a Web 1.0 site may have had a guestbook page to publish visitor comments, instead of a comment section at the end of each page. Server performance and bandwidth considerations had to be taken into account, and a long comments thread on each page could potentially slow down the site. Terry Flew, in his 3rd edition of \"New Media,\" described the differences between Web 1.0 and Web 2.0:\n\nFlew believed it to be the above factors that form the basic change in trends that resulted in the onset of the Web 2.0 \"craze\".\n\nSome design elements of a Web 1.0 site include:\n\nThe term \"Web 2.0\" was first used in January 1999 by Darcy DiNucci, an information architecture consultant. In her article, \"Fragmented Future\", DiNucci writes:\nWriting when Palm Inc. was introducing its first Web-capable personal digital assistant, supporting Web access with WAP, DiNucci saw the Web \"fragmenting\" into a future that extended beyond the browser/PC combination it was identified with. She focused on how the basic information structure and hyperlinking mechanism introduced by HTTP would be used by a variety of devices and platforms. As such, her use of the \"2.0\" designation refers to a next version of the Web that does not directly relate to the term's current use.\n\nThe term Web 2.0 did not resurface until 2002. These authors focus on the concepts currently associated with the term where, as Scott Dietzen puts it, \"the Web becomes a universal, standards-based integration platform\". In 2004, the term began its rise in popularity when O'Reilly Media and MediaLive hosted the first Web 2.0 conference. In their opening remarks, John Battelle and Tim O'Reilly outlined their definition of the \"Web as Platform\", where software applications are built upon the Web as opposed to upon the desktop. The unique aspect of this migration, they argued, is that \"customers are building your business for you\". They argued that the activities of users generating content (in the form of ideas, text, videos, or pictures) could be \"harnessed\" to create value. O'Reilly and Battelle contrasted Web 2.0 with what they called \"Web 1.0\". They associated this term with the business models of Netscape and the Encyclopædia Britannica Online. For example,\n\nThe popularity of Web 2.0 was acknowledged by 2006 \"TIME magazine\" Person of The Year (\"You\"). That is, \"TIME\" selected the masses of users who were participating in content creation on social networks, blogs, wikis, and media sharing sites.\nIn the cover story, Lev Grossman explains:\nInstead of merely reading a Web 2.0 site, a user is invited to contribute to the site's content by commenting on published articles or creating a user account or profile on the site, which may enable increased participation. By increasing emphasis on these already-extant capabilities, they encourage the user to rely more on their browser for user interface, application software (\"apps\") and file storage facilities. This has been called \"network as platform\" computing. Major features of Web 2.0 include social networking websites, self-publishing platforms (e.g., WordPress' easy-to-use blog and website creation tools), \"tagging\" (which enables users to label websites, videos or photos in some fashion), \"like\" buttons (which enable a user to indicate that they are pleased by online content), and social bookmarking. Users can provide the data that is on a Web 2.0 site and exercise some control over that data. These sites may have an \"architecture of participation\" that encourages users to add value to the application as they use it. Users can add value in many ways, such as by commenting on a news story on a news website, by uploading a relevant photo on a travel website, or by adding a link to a video or TED talk which is pertinent to the subject being discussed on a website. Some scholars argue that cloud computing is an example of Web 2.0 because cloud computing is simply an implication of computing on the Internet.\n\nWeb 2.0 offers almost all users the same freedom to contribute. While this opens the possibility for serious debate and collaboration, it also increases the incidence of \"spamming\", \"trolling\", and can even create a venue for racist hate speech, cyberbullying and defamation. The impossibility of excluding group members who do not contribute to the provision of goods (i.e., to the creation of a user-generated website) from sharing the benefits (of using the website) gives rise to the possibility that serious members will prefer to withhold their contribution of effort and \"free ride\" on the contributions of others. This requires what is sometimes called radical trust by the management of the Web site.\nAccording to Best, the characteristics of Web 2.0 are: rich user experience, user participation, dynamic content, metadata, Web standards, and scalability. Further characteristics, such as openness, freedom and collective intelligence by way of user participation, can also be viewed as essential attributes of Web 2.0. Some websites require users to contribute user-generated content to have access to the website, to discourage \"free riding\".The key features of Web 2.0 include:\n\nIn 2005, Tim O'Reilly and Dale Dougherty held a brainstorming session to elucidate characteristics and components of the Web 1.0 to Web 2.0 transition and what changed:\n\nThe client-side (Web browser) technologies used in Web 2.0 development include Ajax and JavaScript frameworks. Ajax programming uses JavaScript and the Document Object Model to update selected regions of the page area without undergoing a full page reload. To allow users to continue to interact with the page, communications such as data requests going to the server are separated from data coming back to the page (asynchronously). Otherwise, the user would have to routinely wait for the data to come back before they can do anything else on that page, just as a user has to wait for a page to complete the reload. This also increases overall performance of the site, as the sending of requests can complete quicker independent of blocking and queueing required to send data back to the client. The data fetched by an Ajax request is typically formatted in XML or JSON (JavaScript Object Notation) format, two widely used structured data formats. Since both of these formats are natively understood by JavaScript, a programmer can easily use them to transmit structured data in their Web application. When this data is received via Ajax, the JavaScript program then uses the Document Object Model (DOM) to dynamically update the Web page based on the new data, allowing for a rapid and interactive user experience. In short, using these techniques, Web designers can make their pages function like desktop applications. For example, Google Docs uses this technique to create a Web-based word processor.\n\nAs a widely available plugin independent of W3C standards (the World Wide Web Consortium is the governing body of Web standards and protocols), Adobe Flash is capable of doing many things that were not possible pre-HTML5. Of Flash's many capabilities, the most commonly used is its ability to integrate streaming multimedia into HTML pages. With the introduction of HTML5 in 2010 and growing concerns with Flash's security, the role of Flash is decreasing. In addition to Flash and Ajax, JavaScript/Ajax frameworks have recently become a very popular means of creating Web 2.0 sites. At their core, these frameworks use the same technology as JavaScript, Ajax, and the DOM. However, frameworks smooth over inconsistencies between Web browsers and extend the functionality available to developers. Many of them also come with customizable, prefabricated 'widgets' that accomplish such common tasks as picking a date from a calendar, displaying a data chart, or making a tabbed panel. On the server-side, Web 2.0 uses many of the same technologies as Web 1.0. Languages such as Perl, PHP, Python, Ruby, as well as Enterprise Java (J2EE) and Microsoft.NET Framework, are used by developers to output data dynamically using information from files and databases. This allows websites and web services to share machine readable formats such as XML (Atom, RSS, etc.) and JSON. When data is available in one of these formats, another website can use it to integrate a portion of that site's functionality.\n\nWeb 2.0 can be described in three parts:\n\nAs such, Web 2.0 draws together the capabilities of client- and server-side software, content syndication and the use of network protocols. Standards-oriented Web browsers may use plug-ins and software extensions to handle the content and the user interactions. Web 2.0 sites provide users with information storage, creation, and dissemination capabilities that were not possible in the environment now known as \"Web 1.0\".\n\nWeb 2.0 sites include the following features and techniques, referred to as the acronym SLATES by Andrew McAfee:\n\n\nWhile SLATES forms the basic framework of Enterprise 2.0, it does not contradict all of the higher level Web 2.0 design patterns and business models. It includes discussions of self-service IT, the long tail of enterprise IT demand, and many other consequences of the Web 2.0 era in enterprise uses.\n\nA third important part of Web 2.0 is the social web. The social Web consists of a number of online tools and platforms where people share their perspectives, opinions, thoughts and experiences. Web 2.0 applications tend to interact much more with the end user. As such, the end user is not only a user of the application but also a participant by:\n\nThe popularity of the term Web 2.0, along with the increasing use of blogs, wikis, and social networking technologies, has led many in academia and business to append a flurry of 2.0's to existing concepts and fields of study, including Library 2.0, Social Work 2.0,\nEnterprise 2.0, PR 2.0, Classroom 2.0, Publishing 2.0, Medicine 2.0, Telco 2.0, Travel 2.0, Government 2.0, and even Porn 2.0. Many of these 2.0s refer to Web 2.0 technologies as the source of the new version in their respective disciplines and areas. For example, in the Talis white paper \"Library 2.0: The Challenge of Disruptive Innovation\", Paul Miller argues\n\nBlogs, wikis and RSS are often held up as exemplary manifestations of Web 2.0. A reader of a blog or a wiki is provided with tools to add a comment or even, in the case of the wiki, to edit the content. This is what we call the Read/Write web. Talis believes that Library 2.0 means harnessing this type of participation so that libraries can benefit from increasingly rich collaborative cataloging efforts, such as including contributions from partner libraries as well as adding rich enhancements, such as book jackets or movie files, to records from publishers and others.\n\nHere, Miller links Web 2.0 technologies and the culture of participation that they engender to the field of library science, supporting his claim that there is now a \"Library 2.0\". Many of the other proponents of new 2.0s mentioned here use similar methods. The meaning of Web 2.0 is role dependent. For example, some use Web 2.0 to establish and maintain relationships through social networks, while some marketing managers might use this promising technology to \"end-run traditionally unresponsive I.T. department[s].\" There is a debate over the use of Web 2.0 technologies in mainstream education. Issues under consideration include the understanding of students' different learning modes; the conflicts between ideas entrenched in informal on-line communities and educational establishments' views on the production and authentication of 'formal' knowledge; and questions about privacy, plagiarism, shared authorship and the ownership of knowledge and information produced and/or published on line.\n\nWeb 2.0 is used by companies, non-profit organizations and governments for interactive marketing. A growing number of marketers are using Web 2.0 tools to collaborate with consumers on product development, customer service enhancement, product or service improvement and promotion. Companies can use Web 2.0 tools to improve collaboration with both its business partners and consumers. Among other things, company employees have created wikis—Web sites that allow users to add, delete, and edit content — to list answers to frequently asked questions about each product, and consumers have added significant contributions. Another marketing Web 2.0 lure is to make sure consumers can use the online community to network among themselves on topics of their own choosing. Mainstream media usage of Web 2.0 is increasing. Saturating media hubs—like \"The New York Times, PC Magazine\" and \"Business Week\" — with links to popular new Web sites and services, is critical to achieving the threshold for mass adoption of those services. User web content can be used to gauge consumer satisfaction. In a recent article for Bank Technology News, Shane Kite describes how Citigroup's Global Transaction Services unit monitors social media outlets to address customer issues and improve products. According to Google Timeline, the term Web 2.0 was discussed and indexed most frequently in 2005, 2007 and 2008. Its average use is continuously declining by 2–4% per quarter since April 2008.\n\nIn tourism industries, social media is an effective channel to attract travellers and promote the tourism product and services by engaging with customers. The brand of tourist destinations can be built through the marketing campaigns on social media by engaging with customers. For example, the “Snow at First Sight” campaign launched by the State of Colorado aimed at a brand awareness of Colorado as a winter destination. The campaign used social media platforms, for examples, Facebook and Twitter, to promote this competition, and requested the participates to share experiences, pictures and videos on social medias. As a result, Colorado enhanced the image of winter destination and a campaign worth was about $2.9 million.\n\nThe tourism organisation can earn a brand royalty from interactive marketing campaigns on social media with engaging passive communication tactics. For example, “Moms” advisors of the Walt Disney World are responsibilities for offer suggestions and reply answers about the family trips at the Walt Disney World. Due to its characteristic of experts in Disney, the “Moms” was chosen to represent the campaign. The social networking such as Facebook can be used as a platform for providing a detailed information about the marketing campaign, as well as real-time online communication with customers. Korean Airline Tour created and maintained a relationship with customers by using Facebook for individual communication purposes.\n\nTravel 2.0 refers a model of Web 2.0 on tourism industries which provided virtual travel communities. Travel 2.0 model allow user to create their own contents and exchange their words through globally interactive features on websites. The users also can contribute their experiences, images and suggestions regarding their trips on online travel communities. For example, TripAdvisor is an online travel community which enables user to rate and share autonomously their reviews and feedbacks on hotels and tourist destinations. Non pre-associate users can interact socially and discuss through discussion forums on Tripadvior. Social media, especially Travel 2.0 website, plays a crucial role in decision-making behaviours of travellers. The user-generated contents on social media tools have a significant impact on travellers’ choices and organisation preferences. The travel 2.0 emerged a radical change in receiving information methods of travellers from business-to-customer marketing into peer-to-peer reviews. The user-generated contents become a vital tool for helping a number of travellers manage their international travels for the first time visiting. The travellers tend to trust and rely on peer-to-peer reviews and virtual communications on social media rather than the information provided by travel supplier. In addition, An autonomous review feature on social media would help traveller reduce risks and uncertainties before purchasing stages. Social media is also a channel for customer complaints and negative feedbacks which can damage images of organisations and destinations. For example, a majority of UK travellers read customer reviews before booking hotels and the bookings of hotels receiving negative feedbacks would be refrained by half of customers. Therefore, the organisations should develop strategic plans to handle and manage the negative feedbacks on social media. Although the user-generated content and rating system on social media are out of business controls, the businesses can monitor those conversations and participate in communities to enhance a customer loyalty and maintain customer relationships.\n\nWeb 2.0 could allow for more collaborative education. For example, blogs give students a public space to interact with one another and the content of the class. Some studies suggest that Web 2.0 can increase the public's understanding of science, which could improve governments' policy decisions. A 2012 study by researchers at the University of Wisconsin-Madison notes that \"...the internet could be a crucial tool in increasing the general public’s level of science literacy. This increase could then lead to better communication between researchers and the public, more substantive discussion, and more informed policy decision.\"\nAjax has prompted the development of Web sites that mimic desktop applications, such as word processing, the spreadsheet, and slide-show presentation. WYSIWYG wiki and blogging sites replicate many features of PC authoring applications. Several browser-based services have emerged, including EyeOS and YouOS.(No longer active.) Although named operating systems, many of these services are application platforms. They mimic the user experience of desktop operating-systems, offering features and applications similar to a PC environment, and are able to run within any modern browser. However, these so-called \"operating systems\" do not directly control the hardware on the client's computer. Numerous web-based application services appeared during the dot-com bubble of 1997–2001 and then vanished, having failed to gain a critical mass of customers.\n\nMany regard syndication of site content as a Web 2.0 feature. Syndication uses standardized protocols to permit end-users to make use of a site's data in another context (such as another Web site, a browser plugin, or a separate desktop application). Protocols permitting syndication include RSS (really simple syndication, also known as Web syndication), RDF (as in RSS 1.1), and Atom, all of which are XML-based formats. Observers have started to refer to these technologies as Web feeds. Specialized protocols such as FOAF and XFN (both for social networking) extend the functionality of sites and permit end-users to interact without centralized Web sites.\n\nWeb 2.0 often uses machine-based interactions such as REST and SOAP. Servers often expose proprietary Application programming interfaces (API), but standard APIs (for example, for posting to a blog or notifying a blog update) have also come into use. Most communications through APIs involve XML or JSON payloads. REST APIs, through their use of self-descriptive messages and hypermedia as the engine of application state, should be self-describing once an entry URI is known. Web Services Description Language (WSDL) is the standard way of publishing a SOAP Application programming interface and there are a range of Web service specifications.\n\nCritics of the term claim that \"Web 2.0\" does not represent a new version of the World Wide Web at all, but merely continues to use so-called \"Web 1.0\" technologies and concepts. First, techniques such as Ajax do not replace underlying protocols like HTTP, but add an additional layer of abstraction on top of them. Second, many of the ideas of Web 2.0 were already featured in implementations on networked systems well before the term \"Web 2.0\" emerged. Amazon.com, for instance, has allowed users to write reviews and consumer guides since its launch in 1995, in a form of self-publishing. Amazon also opened its API to outside developers in 2002. Previous developments also came from research in computer-supported collaborative learning and computer supported cooperative work (CSCW) and from established products like Lotus Notes and Lotus Domino, all phenomena that preceded Web 2.0. Tim Berners-Lee, who developed the initial technologies of the Web, has been an outspoken critic of the term, while supporting many of the elements associated with it. In the environment where the Web originated, each workstation had a dedicated IP address and always-on connection to the Internet. Sharing a file or publishing a web page was as simple as moving the file into a shared folder.\n\nPerhaps the most common criticism is that the term is unclear or simply a buzzword. For many people who work in software, version numbers like 2.0 and 3.0 are for software versioning or hardware versioning only, and to assign 2.0 arbitrarily to many technologies with a variety of real version numbers has no meaning. The web does not have a version number. For example, in a 2006 interview with IBM developerWorks podcast editor Scott Laningham, Tim Berners-Lee described the term \"Web 2.0\" as a jargon:\n\"Nobody really knows what it means... If Web 2.0 for you is blogs and wikis, then that is people to people. But that was what the Web was supposed to be all along... Web 2.0, for some people, it means moving some of the thinking [to the] client side, so making it more immediate, but the idea of the Web as interaction between people is really what the Web is. That was what it was designed to be... a collaborative space where people can interact.\" Other critics labeled Web 2.0 \"a second bubble\" (referring to the Dot-com bubble of 1997–2000), suggesting that too many Web 2.0 companies attempt to develop the same product with a lack of business models. For example, \"The Economist\" has dubbed the mid- to late-2000s focus on Web companies as \"Bubble 2.0\".\n\nIn terms of Web 2.0's social impact, critics such as Andrew Keen argue that Web 2.0 has created a cult of digital narcissism and amateurism, which undermines the notion of expertise by allowing anybody, anywhere to share and place undue value upon their own opinions about any subject and post any kind of content, regardless of their actual talent, knowledge, credentials, biases or possible hidden agendas. Keen's 2007 book, \"Cult of the Amateur\", argues that the core assumption of Web 2.0, that all opinions and user-generated content are equally valuable and relevant, is misguided. Additionally, \"Sunday Times\" reviewer John Flintoff has characterized Web 2.0 as \"creating an endless digital forest of mediocrity: uninformed political commentary, unseemly home videos, embarrassingly amateurish music, unreadable poems, essays and novels... [and that Wikipedia is full of] mistakes, half truths and misunderstandings\". In a 1994 \"Wired\" interview, Steve Jobs, forecasting the future development of the web for personal publishing, said \"The Web is great because that person can't foist anything on you – you have to go get it. They can make themselves available, but if nobody wants to look at their site, that's fine. To be honest, most people who have something to say get published now.\" Michael Gorman, former president of the American Library Association has been vocal about his opposition to Web 2.0 due to the lack of expertise that it outwardly claims, though he believes that there is hope for the future.\n\"The task before us is to extend into the digital world the virtues of authenticity, expertise, and scholarly apparatus that have evolved over the 500 years of print, virtues often absent in the manuscript age that preceded print\".\n\nThere is also a growing body of critique of Web 2.0 from the perspective of political economy. Since, as Tim O'Reilly and John Batelle put it, Web 2.0 is based on the \"customers... building your business for you,\" critics have argued that sites such as Google, Facebook, YouTube, and Twitter are exploiting the \"free labor\" of user-created content. Web 2.0 sites use Terms of Service agreements to claim perpetual licenses to user-generated content, and they use that content to create profiles of users to sell to marketers. This is part of increased surveillance of user activity happening within Web 2.0 sites. Jonathan Zittrain of Harvard's Berkman Center for the Internet and Society argues that such data can be used by governments who want to monitor dissident citizens. The rise of AJAX-driven web sites where much of the content must be rendered on the client has meant that users of older hardware are given worse performance versus a site purely composed of HTML, where the processing takes place on the server. Accessibility for disabled or impaired users may also suffer in a Web 2.0 site.\n\nOthers have noted that Web 2.0 technologies are tied to particular political ideologies. \"Web 2.0 discourse is a conduit for the materialization of neoliberal ideology.\" The technologies of Web 2.0 may also \"function as a disciplining technology within the framework of a neoliberal political economy.\"\n\nIn November 2004, CMP Media applied to the USPTO for a service mark on the use of the term \"WEB 2.0\" for live events. On the basis of this application, CMP Media sent a cease-and-desist demand to the Irish non-profit organization IT@Cork on May 24, 2006, but retracted it two days later. The \"WEB 2.0\" service mark registration passed final PTO Examining Attorney review on May 10, 2006, and was registered on June 27, 2006. The European Union application (which would confer unambiguous status in Ireland) was declined on May 23, 2007.\n\n\n"}
{"id": "11776760", "url": "https://en.wikipedia.org/wiki?curid=11776760", "title": "Xandria Collection", "text": "Xandria Collection\n\nXandria was one of the largest distributors of sex toys and adult videos, books and novelty items in the United States. It began as a catalog of sexual aids for people with disabilities at a time when the disability rights movement was in its infancy.\nXandria was also one of the first companies to sell sex-related items online.\n\nThe business closed in 2016 when Lawrence Research Group, Inc., which operated the business, filed for dissolution with the Nevada Secretary of State.\n\nXandria was founded in 1974 by Gaye Raymond, a San Francisco physical therapy professor, and her husband, Victoria's Secret founder Roy Raymond. Gaye Raymond’s philosophy was that people with disabilities should be able to enjoy as much of life as possible, including an active sex life. Xandria began as a catalog-based distributor of sexual aids for people with physical and developmental disabilities.\n\nThe original Xandria Collection offered mostly educational adult videos and books, as well as vibrators and sex toys that were easier to grasp and maneuver by people who had physical impairments due to an amputation or illness.\n\nThe company’s guarantee of privacy, product quality and personal satisfaction helped it develop a following among the general public, especially among those who didn’t want to be seen entering their local porn shop. \n\nXandria’s products were shipped in discreet packages that did not bear the company’s name. Customers could request a refund within 60 days if they were not satisfied with their sex toys for any reason.\n\nGaye and Roy Raymond sold the Xandria Collection in 1984 and the company's inventory was expanded to appeal to the general public. In addition to a catalog for people with disabilities, the company began selling condoms, erotica, dildos, lubricants, vibrating cock rings, triple crowns, penis pumps, lingerie, masturbation aids and other sex-related products. In 1999 it also began carrying explicit sex videos distributed by sister company 1st Skin. \n\nThe company's new owners also created an advisory board of health and sex experts. Members included sex educator, activist and author Betty Dodson; journalist and author Michael Castleman, who penned the Playboy Magazine Advisor from 1991 to 1995; and clinical psychologist Louanne Cole, who now serves as resident expert for WebMD's \"Sex Matters.\"\n\nXandria was initially headquartered in San Francisco. It moved to Brisbane, California in 1993 and to its final location in the Las Vegas Valley in 2005.\nIn 1997, the company launched Xandria.com, one of the first online stores dedicated to selling sex toys and adult videos. The site and the company itself have generated positive press from well-known publications such as Playboy, Esquire, Hustler and Men’s Fitness.\n\nIn its final phase customers could order items through the mail and by calling a toll free number, but most of the company’s sales were generated through its website. Xandria did not have a physical store. Xandria.com carried a long list of sex-related items as well as articles on sexual health written by well-known authors. \n\nSeveral sex toys from the Xandria Collection were featured in the film Stash, an indie comedy starring Tim Kazurinsky and Marilyn Chambers. The film is about \"Discreet Removals,\" a company that will remove people's stashes of pornography and sex toys after their death, so that unsuspecting spouses and family members can \"concentrate on grieving\". Stash won the 2007 Gold Remi Award for excellence in the category of Mature Audience Narrative Feature\" at the WorldFest Houston International Film Festival.\n\n\n"}
{"id": "9020357", "url": "https://en.wikipedia.org/wiki?curid=9020357", "title": "Xceive XC3028", "text": "Xceive XC3028\n\nXceive's XC3028 and XC3028L are single-chip TV tuners by Xceive Inc., capable of receiving analogue cable, analogue terrestrial, digital cable and digital terrestrial TV (PAL, NTSC, SECAM, DVB-t, ATSC, DVB-c). It is commonly used in USB digital TV receivers for Windows and Mac due to its small size and power consumption, but the manufacturer also markets it as suitable for portable TVs and notebook computers.\n\nIts main competitor is the Microtune MT2060.\n\nSupport for this tuner has been added in the 2.6.25 Linux kernel.\n\nIt is a variant that only supports analog reception and DVB-T digital reception.\n\n\n"}
{"id": "1215588", "url": "https://en.wikipedia.org/wiki?curid=1215588", "title": "Yokneam Illit", "text": "Yokneam Illit\n\nYokneam Illit (), also \"Yoqne'am Illit\" and \"Jokneam Illit\", is a city in northern Israel. It is located in a hilly region of the lower Galilee at the base of the Carmel Mountains and overlooks the Jezreel Valley, from Haifa and from Tel Aviv. Yokneam is known as Israel's \"Startup Village\" because its high-tech hub is surrounded by forest and small communities.\nYokneam Illit was founded in 1950 and became a local authority in 1967, and a city in 2007. The city is located alongside the country’s major highways – Highway 70 and Highway 6. In it had a population of .\n\nStarting in 1989 when a new mayor, Simon Alfassi, was elected, the economic structure of Yokneam Illit changed from a centralized dependence on two large factories to a dispersed base of many small high-tech companies. As the number and size of the companies grew, Yokneam and the small communities around it began to attract young entrepreneurs and developers who were looking for a less urban alternative to the Tel Aviv area. It now has over 160 high-tech companies and exports of approximately 6 billion US dollars annually.\n\nThe policy of the municipality is to build low-density, spacious homes to preserve the landscape and views from every home. Although real estate prices are low relative to the Tel-Aviv area, its high rate of growth in recent years has pushed prices up faster than in similar sized cities.\n\nAncient Yokneam appears in the list of 119 cities conquered by the Egyptian Pharaoh Thutmose III following the victory in the battle of Megiddo (1468 BC).\n\nYokneam (or Jokneam) is mentioned in the Hebrew Bible as a city of Levites within the territory of the Israelite tribe of Zebulun (Joshua 12:1, 12:22, 19:10-11, 21:34). It is located near Megiddo (Armageddon). The Crusaders called Yokneam \"Cain Mons\", or \"Mountain of Cain\" in keeping with the tradition that Cain, son of Adam (Genesis 4: 23-24), was murdered at this site. Yokneam was populated throughout the Persian, Hellenistic, Roman/Byzantine, Arab, Crusaders, Mameluk, and Ottoman periods.\n\nYokneam was the site of a Palestinian Arab village named Qira, depopulated in the lead up to the 1948 Arab–Israeli war.\n\nThe Yokneam Moshava was established in 1935. In July 1950, a tent camp (Maabara) was set up to absorb 250 new immigrant families and remained part of Yokneam Moshava until 1967. By the start of 1952, another 400 families were housed in the tent camp. Toward the end of 1951, the first 285 families moved from the tent camp to permanent housing on the hill above the Yokneam Moshava. In 1952, Yokneam was considered unusual in that it had a single school for all regardless of whether they were religious or secular, or from the moshava or the tent camp.\n\nIn 1964, Yokneam was occupied by 4,300 residents, with 160 of the families living in the moshava. In 1967, Yokneam was split into two local councils (Yokneam and Yokneam Illit).\n\nIn the 1980s there were two factories in Yokneam, employing 90% of its residents. One collapsed and unemployment soared creating a challenging situation for residents.\n\nIn the 1990s, Yokneam absorbed a large wave of immigrants from Ethiopia and the former Soviet Union. During that time, Yokneam's industrial parks became a magnet for high-tech firms.\n\nYokneam Illit was formally declared a city in 2006. In 2009, Yokneam was linked to Highway 6 and also received recognition as Israel's first \"Green City\" in the Cleantech competition. These two events significantly boosted Yokneam's attraction as a center for companies involved in medical, cleantech and product development.\n\nIn 2011, some 16,000 workers entered the industrial parks every day. Of these, only 4,000 were local residents.\n\nOn Dec. 31, 2014, Yokneam had a population of 21,100 residents, with a population density of 2,576.5. The rate of unemployment is 5 percent, lower than the national average. The median age in the Yokneam region is 32. 76% of Yokneam Illit's residents are 40 years old or under. Eighty percent of high-school students in Yokneam graduate with a bagrut matriculation certificate, one of the highest rates in Israel. 40% of Yokneam's population are college graduates.\n\nSince the 1950s when Yokneam Moshava and Yokneam Illit shared a single school, social integration and coexistence has been an integral part of the local culture. The first wave of immigrants to settle in the Maabara (tent camp) had major contingents from at least 6 different Jewish communities (Iran, Iraq, Kurdistan, Romania, Yemen and India), all of whom were absorbed by the Moshava which had been settled in 1935 by Jews from Germany and the Netherlands.\n\nThe next wave of new immigrants came in the 1960s from North Africa, followed by immigrants from the Soviet Union in the 1970s. In the 1990s, Yokneam Illit absorbed large numbers of immigrants from Ethiopia and the former Soviet Union.\n\nStarting in the 1990s, the growth of high-tech companies in Yokneam Illit also brought an influx of Israeli-born residents along with an increasing number of immigrants from Western Europe, South Africa, North America and South America. During this period, the economic problems of the Kibbutz and Moshav movements led many of the young families in the Megiddo Regional Council to move to nearby Yokneam Illit.\n\nYokneam Illit's accelerated demand for day care and Kindergartens led to a shortage of space in local day care and Kindergartens at the same time that there was a shortage of children at some of the Kibbutzim. In order to avoid sending their children to other small communities in the area, Kibbutz children's houses and Kindergartens began accepting children from nearby Yokneam Illit, and in some cases the number of children from Yokneam Illit outnumbered the Kibbutz children. This phenomenon later extended to the elementary and secondary schools until the building of new schools in Yokneam Illit was able to catch up with demand.\n\nMany of the children from Yokneam Illit and their families remained in the Megiddo school system, while families who moved from the Megiddo communities to Yokneam Illit were educating their children in the Yokneam Illit school system. The family ties and school friendships led to mixed Yokneam-Megiddo parents groups in both communities and nearly all sports activities and many other after-school activities being fully integrated. Today, it is extremely rare to find a local team representing only Yokneam Illit or only the Megiddo Regional Council.\n\nThe Partnership2Gether program, which partnered Yokneam Illit and the Megiddo Regional Council on the Israeli side, with the Jewish communities of Atlanta, GA and St. Louis, MO on the US side, helped institutionalize cooperation between the Yokneam Illit municipality and the Megiddo Regional Council. The impact on joint community leadership not only affected Yokneam-Megiddo, but also created an ongoing relationship between the Jewish communities of Atlanta and St. Louis.\n\nThis cross-pollination has led to an integrated economic ecosystem that help support the high-tech companies in Yokneam Illit. Examples of this cooperation can be seen in the activities of the Southeast chapter of the US-Israel Chamber of Commerce in Atlanta, and the success of companies like Given Imaging. Given Imaging, which began as a small startup in Yokneam Illit, leveraged its special relationship with the Southeast chapter of the US-Israel Chamber of Commerce when it opened its Marketing headquarters in Atlanta, GA.\n\nYokneam's proximity to two major universities (Technion and University of Haifa), location at the crossroads between northern Israel and the major urban areas of Tel-Aviv and Jerusalem, tax benefits and investment grants associated with its status as a \"National Priority Area A\", and its small town surroundings and lower cost of housing all worked together to attract Israel's highest concentration of R&D companies outside of Tel-Aviv and give it the nickname of Startup Village.\n\nUnder the Israeli Law for the Encouragement of Capital Investment, approved enterprises in Yokneam Illit enjoy the highest level of tax benefits and investment grants (currently up to 20% of the investment or 10 year tax exemption). In addition, companies relocating to the region can apply for concessions in local taxes over a three-year period from the Yokneam Illit Local Authority.\n\nYokneam's industrial parks are home to more than 160 high-tech companies with exports of approximately 6 billion US dollars annually. Companies specialize in a wide range of technologies, including semiconductors, biotechnology, pharmaceuticals and medical devices. These include Intel, Panasonic Avionics Corporation, Medtronic, Given Imaging, Naiot Venture Accelerator, Mellanox Technologies, Marvell Technology Group, Biosense Webster and Lumenis. An additional high-tech park, Mevo Carmel Jewish-Arab Industrial Park, is already underway that will be a joint venture of Yokneam Illit, Megiddo Regional Council and the Druze villages of Daliyat al-Karmel and Isfiya.\n\nThe growth of the high-tech companies in Yokneam has attracted speakers and members of specialized professions, such as technical writers, to come from other parts of Israel to meet and exchange ideas.\n\nIn addition to technology-based industry, Yokneam has a number of production facilities, including Osem, which established a plant in Yokneam in 1974; and a boutique winery, Morad Winery, known for its special wines made from Pomegranates, Passion Fruit, Red Grapefruits, Coffee, Coconuts, Carub and many other fruits. Initially, the Osem plant produced mainly pasta products, but the scope of activities broadened over the years. In 1997, the facility doubled in size and three production lines were installed for baked goods such as crackers, biscuits, cookies and pretzels. Bamba, a popular Israeli snack food, is also produced in Yokneam. Many Yokneam residents are employed at the plant, which won awards from the Council for a Beautiful Israel for its attractive premises.\n\nA 2018 study found that employees in Yokneam have the highest salaries of any development town, and was one of only two development towns with an average salary that exceeded the national average. The percentage of employees who earned less than minimum wage was the lowest among development towns and over 20% less than the national average. \n\nNearly 150 high technology companies are headquartered in Yokneam or have R&D facilities there. These include:\nMajor startup exits include BTG's recent $110 million acquisition of Galil Medical, Amazon's acquisition of Annapurna Labs for a reported $350–375 million and Mellanox Technologies acquisition of EZchip Semiconductor for a reported $811 million, Covidien's 2014 acquisition of Given Imaging for a reported $860 million, Intel's acquisition of Oplus Technologies Inc in 2005 for nearly $50 million, and Marvell's acquisition of Galileo Technology Ltd. for $2.7 billion in stock in 2000. In addition to these exits, Lumenis recently merged with an affiliate of China's XIO group for a reported $510 million.\n\nAs the number of hi-tech companies in Yokneam grow, technology incubators have begun opening in or moving to Yokneam, each specializing in different types of technology.\n\nYokneam is a very green city. Not just in terms of ecological activism, but also in terms of its physical surroundings. Roughly two-thirds of the land within Yokneam's city limits are green space (parks, archeological digs, gardens, playgrounds). It has five major public parks, and outdoor amphitheater and over 80 playgrounds distributed throughout the city. Tel Yokneam, which includes the ruins of a crusader church and its intricate water system, is used as an educational site where Yokneam's school children perform restoration work under the direction of the Antiquities Authority and the National Parks Authority.\nNearly of bicycle paths connect all of the parks and public spaces. The city is also surrounded by the Ramot Menashe Biosphere Reserve of the Megiddo Regional Council, which is currently in the process of becoming recognized by UNESCO as a biosphere reserve.\n\nEach year thousands of Israelis participate in Yokneam's annual walking event which began in 1991. In 2013 there were 35,000 participants. The event takes place in the Spring, with a short route of and a longer route of through the hills and valleys that surround Yokneam. The 2014 walking event is scheduled for April 17, 2014.\n\nYokneam sits at the intersections of Highway 70 and Highway 6, which connect all of Israel with both Tel-Aviv and Jerusalem, and will soon be connected to Highway 77 leading to the Galilee. The Yokneam–Kfar Yehoshua railway station on the Jezreel Valley railway which stretches from Haifa to Beit She'an is located only 5 minutes from Yokneam's high-tech parks. Yokneam is 20–25 minutes from Haifa Airport and 12 minutes from the Ramat David airbase, which is reported to be the most likely location for a new International airport to complement Ben Gurion Airport.\n\nThe bus service in Yokneam is provided by 3 bus companies. Superbus provides the local service in Yokneam and intercity service to Haifa and Afula. Superbus also runs night lines (including Friday nights) to Haifa, Kiryat Tivon and Ramat Ishai. All bus stops are handicapped-accessible. Egged and Nateev Express provide express intercity services to Tel Aviv, Bnei Brak and Jerusalem, as well as service with stops along the way.\n\nThere is a taxi stand at the shopping mall at the eastern entrance to Yokneam Illit.\n\nThe Yokneam Illit Municipality encourages the use of bicycles in the city. It currently has of bicycle paths that connect all of it public parks and buildings.\n\nYokneam allocates 42% of its budget to education. With 75% of its students passing the national matriculation exams and a dropout rate of only 0.8%, Yokneam Illit was awarded the National Education Prize and the Regional Education Prize several times in recent years. Preschool from age 3 has been free in Yokneam Illit for 20 years. According to a 2018 report by the Knesset, Yokneam has the 2nd highest percentage of residents with an academic of any city in Israel. That same study showed that Yokneam had the 2nd highest rate of high school matriculation among development town and the highest rate of those with matriculation at a level high enough for University acceptance, both of which were 205 higher than the national average and 30% higher than the average for development towns. \n\nThe municipality of Yokneam has developed a special relationship with the Jewish communities of Atlanta, GA and Saint Louis, MO, whereby all three communities work together to create programs for bridging the gaps between communities and cultures, and to provide all students with equal opportunities.\n\nThe Environmental Protection Ministry (Israel) awarded a number of Yokneam's kindergartens and elementary schools the “green label” because of the way they integrated study of the environment into the local curriculum. In grades three through six, elementary school students spend a full week of school at the city's Science Museum.\n\nThe IDF Head of Manpower Planning recently awarded the city a special excellence prize for achieving an 87% level of enlistment in the IDF and the eighth highest level of recruitment to elite units and officers courses in all of Israel.\n\nConstruction of a \"breakthrough\" educational campus is nearly complete, with an educational infrastructure for study from kindergarten to high school that will eliminate the transitions that affect students’ lives. The campus will have sports halls, day care centers, kindergartens, elementary and high schools, and academic courses prior to IDF enlistment.\n\nIn addition to Yokneam's proximity to Technion(23.8 km), University of Haifa(22 km), and Oranim Academic College(7.5 km), Yokneam has a branch of the Open University for bachelor's and master's degrees. The municipality is currently trying to expand on this by working with major universities to open branches in Yokneam.\n\nResidents of Yokneam Illit are avid theater goers and there are two local theaters, Yokneam City Theater (seats 475) and Tarbuta (seats 200).\n\nIsrael's best-known theater groups come to perform at The Yokneam City Theater operates 6 days a week. The subscription series with 8 different performances a year from Israel's best-known theater companies has grown from one day of the week to five in order to accommodate a total audience of over 2,000, or 10% of its population. The theater also runs two subscription series for children with a total of 600 subscribers, one for ages 3–6 and the other for ages 6–10. A separate \"Fringe\" series has 200 subscribers for one-man or small theater companies and nontraditional theater performances. Low cost and subsidized performances that are outside of the subscription series include performances in Yiddish by the \"Yiddish Shpiel\" company, events for the Ethiopian community, children's concerts, and animation theater. The Jewish Federation subsidizes children's theater subscriptions for members of the Ethiopian community to bring their cost of a child accompanied by one parent down to 100 ILS (approximately $30 USD) for the entire series. The local Artists' Guild makes use of the theater to display artwork from local and outside artists.\n\nA second theater house, Tarbuta, seats 200. The Family Empowerment Center also runs a Drama-Therapy group for Ethiopian women that performs in Hebrew and Amharic.\nDuring the summer months, musicians from all over Israel perform in the open air Amphitheater.\n\nThere are also many dance performances held at Kibbutz Ein HaShofet, just outside Yokneam. One of Israel's largest movie houses, Yes Planet, is only 15 minutes away.\n\nYokneam Illit does not have any bars or pubs. Young people looking for more than wine or beer at a restaurant or coffee shop go to nearby Ramat Ishai.\nYokneam is known for its religious tolerance, with many extended families consisting of a mixture of secular and religious Jews. Despite its small size, Yokneam Illit is an ethnically diverse city, with immigrants from all parts of the world and their descendants and many active synagogues, each with its own special character.\nYokneam has two indoor Basketball Courts (Dahari Sports Center and Hapoel Sports Center), a Tennis Association, a new Country Club including an indoor and outdoor swimming pool, an additional community swimming pool, and a Soccer field, which is run by the Hapoel sports association and is often used with the school age children. Local teams compete in league for Basketball, Soccer, Baseball, , and Bicycle Racing. The Indoor basketball court at Hapoel seats 500. The outdoor soccer field seats 200.\n\nThe Yokneam-Megiddo Region has two baseball teams that represent their Sister Cities (the Atlanta Braves and the St. Louis Cardinals). These teams have participated in many tournaments: Jr Macabbiah Games, European Tournaments and the Israeli Junior Olympics where they received the Bronze Medal. One of the local Kibutz' has donated the land necessary to build a new Baseball/Softball Stadium in conjunction with the Israel Softball Association and the Jewish National Fund. The Israel Softball Association is helping to start a Special Education Program for the families in the area. In 2012-2013, Yokneam had a player make the Junior National Team which traveled to Denmark that summer.\n\nStarting in 2012, 12 youngsters from Daliyat al-Karmel began taking part in a weekly co-existence program at the Israel Tennis Center as part of a tennis school financed by the Freddie Krivine Foundation.\n\nIn 2009, the Cleantech Authority of the Environmental Protection Ministry (Israel) awarded the city of Yokneam Illit a prize for its overall strategy of protecting and nurturing green spaces and encouraging environmental awareness and education. It was again awarded the \"Green City\" prize in 2014, where the parameters measured included: energy efficiency, reduction of greenhouse gas emissions, recycling and waste separation, water and sewage, environment-friendly \"green\" construction, public gardens, environmental education, beauty of the city, environment-friendly \"green\" commerce and industry, care of urban public grounds and nature, and urban management.\n\nThe city has its own environmental protection committee responsible for defining and implementing an overall strategy for improving the quality of life and protecting the environment. The strategy emphasizes: planned development, environmental management, educational programs for environmental protection, urban sanitation, cause and effect management, and conservation of energy and water. It actively promotes walking and use of bicycles for transportation, and all bus stops are handicapped-accessible.\n\nYokneam Illit's 500 dunams of public parks and its 50 m/person of open urban space is 5 times the minimum recommendation for urban planning. In addition to the annual \"Yokneam Walk\", which attracts thousands of participants, the works closely with the Israel National Fund (Keren Kayemet) to further promote nature preservation.\n\nYokneam Illit is one of 31 municipalities that separates garbage at the source, as well as having waste separation centers distributed throughout the city. The drainage systems work well and stockpile water for reuse in city parks and gardens. The city also uses a computerized irrigation system to save water and achieve optimal results. Sewage is treated at the water treatment plants that are located in nearby Kiryat Tivon.\n\nAs part of Yokneam's commitment to adopting green energy it will soon be one of the first two cities in northern Israel where electric buses will operate.\n\nThe municipality invests both time and money to maintain the natural beauty of its green spaces. Its building codes have become stricter and greener as the demand for housing has increased. A \"green\" neighborhood is currently under construction which includes environment friendly lighting, irrigation and waste management. At the same time, the municipality has actively worked to make the older neighborhoods more attractive.\n\nThe city runs environmental leadership workshops. Jewish immigrants from Ethiopia run a program to teach and preserve traditional farming methods. The education system is defined as “green”, with environmental studies built into the curriculum.\n\nYokneam Municipality’s policies stress savings and proper management. Over the years, it has won financial prizes for proper management, which have been invested in city employees and local improvement. The municipality has privatized a range of services including gardening, garbage collection, and rates collection, resulting in substantial savings for reinvestment. The city’s annual budget of NIS 112 million is balanced and is collected from 95% of residents and 97% of businesses and industrial enterprises.\nThe municipal budget for 2014 is 135,889,000.\n\nSource: \n\nThere is one radio station, Radio Kol Rega 96.fm, and five local (Hebrew) newspapers that regularly cover news about Yokneam Illit.\n\nYokneam averages 650 millimeters of rain per year due to its proximity to Mount Carmel. The temperatures are the same as the other communities in the Jezre'el Valley. In winter, minimum temperatures typically range from . Temperatures can dip below when cold winds come from the north with clear skies. Morning fog is common in Yokneam and throughout the Jezre'el Valley.\n\nSummers in Yokneam are hot and dry, with maximum temperatures reaching . Since the introduction of weather station, the hottest recorded temperature was that was measured on August 7, 2010; and the lowest recorded temperature was on December 15, 2013.\n\nUp to of snow fell in Yokneam Illit in 1950, 1992, 1999 and 2000.\n\nRecord rainfall in recent years:\n2009-2010 (85% of average)\n2010-2011 (75% of average)\n2011-2012 (82% of average)\n2012-2013 (103% of average)\n\nYokneam Illit is twinned with:\n\n\n"}
