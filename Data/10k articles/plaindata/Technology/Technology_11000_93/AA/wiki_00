{"id": "9748160", "url": "https://en.wikipedia.org/wiki?curid=9748160", "title": "100 Pine Center", "text": "100 Pine Center\n\n100 Pine Center is a class-A office building at the northwest corner of Pine Street and Front Street in San Francisco's Financial District. The building is with 33 floors, and of rentable office space, a 150-car garage including 30 valet parking spaces.\n\nCompleted in 1972 as headquarters of the Continental Insurance Company, the building is among the earlier of San Francisco's modern skyscrapers. By the start of the dot-com bubble the property, then owned by Grosvenor Properties, was outdated and suffered from deferred capital maintenance.\n\nIn 1997, the building was purchased by Walton Capital, which invested heavily in upgrades, re-measured the building at larger than before, then aggressively re-leased space at higher rates. After two failed transactions, Walton Capital sold the building to investors led by Unico Properties in May 2000 for US$156 million. Unico re-sold the property to the Alaska Permanent Fund based on a US$149 million price in August 2005, but retained a minority ownership share. The property was acquired by the Rockpoint Group in February 2017 for US$287.5 million.\n\nThe tower houses many daytime office workers. Major tenants include the New York Life Insurance Company. As of 2016, it is tied with 45 Fremont Center as the twenty-sixth tallest building in San Francisco. It was certified as being operated and maintained as a green building under the US Green Building Council's Leadership in Energy and Environmental Design for Existing Buildings (LEED EB 2.0) standards in July 2008.\n\nThe Consulate-General of Ireland is in the building.\n\n"}
{"id": "3503154", "url": "https://en.wikipedia.org/wiki?curid=3503154", "title": "AIM Fight", "text": "AIM Fight\n\nAIM Fight was a program designed to compare the popularity of two AOL Instant Messenger (AIM) screen names. AIM Fight was not an official AOL product.\n\nTo \"fight,\" two screen names are entered into the Web page and then each screen name was run through an algorithm that runs through the buddy lists of everybody signed on at that particular moment. The algorithm returns a score that represents the sum of the number of people who have those screen names listed as a buddy out to the third degree. As explained on the AIM Fight Web page, the score cannot increase by adding people to that user's buddy list, but rather having other people add the user to their buddy list. Depending on how well-connected these people are, the increase in the score can vary.\n\nAlthough the score represents a numerical sum of the people who have that screen name listed as a buddy and are currently signed on, the help page later states that the actual score was relative to how many people are connected to AIM. Although the term rank usually assumes that each person has a unique position, multiple users can share the same AIM Fight rank, including rank 1.\n\nThe API consists of a single URL called by the Adobe Flash applet that returns the scores and ranks of the users. Additionally, it returns heights for the bars for the applet to show.\n\nIt was accessed at codice_1\n\nBy replacing codice_2 and codice_3 with the screen names of the individuals to fight, percent-encoded data was returned in the following format:\n\ncodice_4\n\n\n\n"}
{"id": "22707753", "url": "https://en.wikipedia.org/wiki?curid=22707753", "title": "Aquatic timing system", "text": "Aquatic timing system\n\nAquatic timing systems are designed to automate the process of timing, judging, and scoring in competitive swimming and other aquatic sports, including diving, water polo, and synchronised swimming. These systems are also used in the training of athletes, and many add-on products have been developed to assist with the training process.\n\nCompanies that currently manufacture aquatic timing and systems include Stramatel, ALGE-TIMING, Superior Swim Timing, Wylas Timing, Colorado Time Systems, Seiko, Daktronics, Quince electronics and Omega/Swiss Timing.\n\nPrior to the 1950s, competitive swimmers relied on the sound of a starting pistol to start their races and mechanical stopwatches to record their times at the end of a race. Unfortunately, these analog watches could not record times accurately beyond one tenth (0.1) of a second. The invention of automatic timing systems brought more accuracy and credibility to aquatic sports.\n\n\n"}
{"id": "39186043", "url": "https://en.wikipedia.org/wiki?curid=39186043", "title": "BIM Task Group", "text": "BIM Task Group\n\nThe Building Information Modelling (BIM) Task Group is a UK Government-funded group, managed through the Cabinet Office, created in 2011 (and holding its first meeting in May 2011) and chaired by Mark Bew. It was founded to \"drive adoption of BIM across government\" in support of the \"Government Construction Strategy\". It aimed to strengthen the public sector’s capabilities in BIM implementation to that all central government departments can adopt, as a minimum, collaborative 'Level 2' BIM by 2016.\n\nThe core BIM task force, to which companies seconded employees, identified four work streams, each led by a core team member: stakeholder and media engagement, delivery and productivity, commercial and legal, and training and academia. Working parties were established to focus on particular areas including: training and education, COBie data set requirements, Plan of Works, software vendors (the BIM Technologies Alliance), contractors (UK Contractors Group, now superseded by Build UK), and materials and products suppliers (Construction Products Association).\n\nIn early 2014, it was announced that the BIM Task Group would be wound down during 2015, with a \"managed handover\" during 2015 to a newly created \"legacy group\", though there was speculation that the group's life might be extended to help achieve a new BIM Level 3 target.\n\nIn October 2016, an updated BIM Task Group delivering the February 2015 \"Digital Built Britain\" strategy was announced at the Institution of Civil Engineers BIM 2016 Conference in a keynote speech by Mark Bew.\n"}
{"id": "1156603", "url": "https://en.wikipedia.org/wiki?curid=1156603", "title": "Biofilter", "text": "Biofilter\n\nBiofiltration is a pollution control technique using a bioreactor containing living material to capture and biologically degrade pollutants. Common uses include processing waste water, capturing harmful chemicals or silt from surface runoff, and microbiotic oxidation of contaminants in air.\n\nExamples of biofiltration include:\n\nWhen applied to air filtration and purification, biofilters use microorganisms to remove air pollution. \nThe air flows through a packed bed and the pollutant transfers into a thin biofilm on the surface of the packing material. Microorganisms, including bacteria and fungi are immobilized in the biofilm and degrade the pollutant. Trickling filters and bioscrubbers rely on a biofilm and the bacterial action in their recirculating waters.\n\nThe technology finds greatest application in treating malodorous compounds and water-soluble volatile organic compounds (VOCs). Industries employing the technology include food and animal products, off-gas from wastewater treatment facilities, pharmaceuticals, wood products manufacturing, paint and coatings application and manufacturing and resin manufacturing and application, etc. Compounds treated are typically mixed VOCs and various sulfur compounds, including hydrogen sulfide. Very large airflows may be treated and although a large area (footprint) has typically been required—a large biofilter (>200,000 acfm) may occupy as much or more land than a football field—this has been one of the principal drawbacks of the technology. Engineered biofilters, designed and built since the early 1990s, have provided significant footprint reductions over the conventional flat-bed, organic media type.\n\nOne of the main challenges to optimum biofilter operation is maintaining proper moisture throughout the system. The air is normally humidified before it enters the bed with a watering (spray) system, humidification chamber, bioscrubber, or biotrickling filter. Properly maintained, a natural, organic packing media like peat, vegetable mulch, bark or wood chips may last for several years but engineered, combined natural organic and synthetic component packing materials will generally last much longer, up to 10 years. A number of companies offer these types or proprietary packing materials and multi-year guarantees, not usually provided with a conventional compost or wood chip bed biofilter.\n\nAlthough widely employed, the scientific community is still unsure of the physical phenomena underpinning biofilter operation, and information about the microorganisms involved continues to be developed. A biofilter/bio-oxidation system is a fairly simple device to construct and operate and offers a cost-effective solution provided the pollutant is biodegradable within a moderate time frame (increasing residence time = increased size and capital costs), at reasonable concentrations (and lb/hr loading rates) and that the airstream is at an organism-viable temperature. For large volumes of air, a biofilter may be the only cost-effective solution. There is no secondary pollution (unlike the case of incineration where additional CO and NO are produced from burning fuels) and degradation products form additional biomass, carbon dioxide and water. Media irrigation water, although many systems recycle part of it to reduce operating costs, has a moderately high biochemical oxygen demand (BOD) and may require treatment before disposal. However, this \"blowdown water\", necessary for proper maintenance of any bio-oxidation system, is generally accepted by municipal publicly owned treatment works without any pretreatment.\n\nBiofilters are being utilized in Columbia Falls, Montana at Plum Creek Timber Company's fiberboard plant. The biofilters decrease the pollution emitted by the manufacturing process and the exhaust emitted is 98% clean. The newest, and largest, biofilter addition to Plum Creek cost $9.5 million, yet even though this new technology is expensive, in the long run it will cost less overtime than the alternative exhaust-cleaning incinerators fueled by natural gas (which are not as environmentally friendly). The biofilters use trillions of microscopic bacteria that cleanse the air being released from the plant.\n\nBiofiltration was first introduced in England in 1893 as a trickling filter for wastewater treatment and has since been successfully used for the treatment of different types of water. Biological treatment has been used in Europe to filter surface water for drinking purposes since the early 1900s and is now receiving more interest worldwide. Biofiltration is also common in wastewater treatment, aquaculture and greywater recycling, as a way to minimize water replacement while increasing water quality.\n\nA biofilter is a bed of media on which microorganisms attach and grow to form a biological layer called biofilm. Biofiltration is thus usually referred to as a fixed–film process. Generally, the biofilm is formed by a community of different microorganisms (bacteria, fungi, yeast, etc.), macro-organisms (protozoa, worms, insect’s larvae, etc.) and extracellular polymeric substances (EPS) (Flemming and Wingender, 2010). The aspect of the biofilm is usually slimy and muddy.\n\nWater to be treated can be applied intermittently or continuously over the media, via upflow or downflow. Typically, a biofilter has two or three phases, depending on the feeding strategy (percolating or submerged biofilter):\n\n\nOrganic matter and other water components diffuse into the biofilm where the treatment occurs, mostly by biodegradation. Biofiltration processes are usually aerobic, which means that microorganisms require oxygen for their metabolism. Oxygen can be supplied to the biofilm, either concurrently or countercurrently with water flow. Aeration occurs passively by the natural flow of air through the process (three phase biofilter) or by forced air supplied by blowers.\n\nMicroorganisms' activity is a key-factor of the process performance. The main influencing factors are the water composition, the biofilter hydraulic loading, the type of media, the feeding strategy (percolation or submerged media), the age of the biofilm, temperature, aeration, etc.\n\nMost biofilters use media such as sand, crushed rock, river gravel, or some form of plastic or ceramic material shaped as small beads and rings. \n\nAlthough biological filters have simple superficial structures, their internal hydrodynamics and the microorganisms' biology and ecology are complex and variable. These characteristics confer robustness to the process. In other words, the process has the capacity to maintain its performance or rapidly return to initial levels following a period of no flow, of intense use, toxic shocks, media backwash (high rate biofiltration processes), etc.\n\nThe structure of the biofilm protects microorganisms from difficult environmental conditions and retains the biomass inside the process, even when conditions are not optimal for its growth. Biofiltration processes offer the following advantages: (Rittmann et al., 1988):\n\n\nBecause filtration and growth of biomass leads to an accumulation of matter in the filtering media, this type of fixed-film process is subject to bioclogging and flow channeling. Depending on the type of application and on the media used for microbial growth, bioclogging can be controlled using physical and/or chemical methods. Whenever possible, backwash steps can be implemented using air and/or water to disrupt the biomat and recover flow. Chemicals such as oxidizing (peroxide, ozone) or biocide agents can also be used.\n\nFor drinking water, biological water treatment involves the use of naturally occurring microorganisms in the surface water to improve water quality. Under optimum conditions, including relatively low turbidity and high oxygen content, the organisms break down material in the water and thus improve water quality. Slow sand filters or carbon filters are used to provide a support on which these microorganisms grow. These biological treatment systems effectively reduce water-borne diseases, dissolved organic carbon, turbidity and color in surface water, thus improving overall water quality.\n\nBiofiltration is used to treat wastewater from a wide range of sources, with varying organic compositions and concentrations. Many examples of biofiltration applications are described in the literature. Bespoke biofilters have been developed and commercialized for the treatment of animal wastes, landfill leachates, dairy wastewater, domestic wastewater.\n\nThis process is versatile as it can be adapted to small flows (< 1 m3/d), such as onsite sewage as well as to flows generated by a municipality (> 240 000 m3/d). For decentralized domestic wastewater production, such as for isolated dwellings, it has been demonstrated that there are important daily, weekly and yearly fluctuations of hydraulic and organic production rates related to modern families' lifestyle. In this context, a biofilter located after a septic tank constitutes a robust process able to sustain the variability observed without compromising the treatment performance.\n\nThe use of biofilters is common in closed aquaculture systems, such as recirculating aquaculture systems (RAS). Many designs are used, with different benefits and drawbacks, however the function is the same: reducing water exchanges by converting ammonia to nitrate. Ammonia (NH and NH) originates from the brachial excretion from the gills of aquatic animals and from the decomposition of organic matter. As ammonia-N is highly toxic, this is converted to a less toxic form of nitrite (by \"Nitrosomonas\" sp.) and then to an even less toxic form of nitrate (by \"Nitrobacter\" sp.). This \"nitrification\" process requires oxygen (aerobic conditions), without which the biofilter can crash. Furthermore, as this nitrification cycle produces H, the pH can decrease which necessitates the use of buffers such as lime.\n\n\n"}
{"id": "7367566", "url": "https://en.wikipedia.org/wiki?curid=7367566", "title": "CollectSPACE", "text": "CollectSPACE\n\ncollectSPACE is an online publication and community for space history enthusiasts featuring articles and photos about space artifacts and memorabilia, information on past, current, and upcoming space events, space history collecting resources, and links to other space-related websites. It also provides an array of message boards where registered members can discuss various aspects of space history and the space collecting hobby; buy, sell, or trade items; or pose \"what if?\" historical questions. Users often abbreviate the website's name as \"cS,\" and members often refer to each other as \"cSers.\"\n\ncollectSPACE, founded and edited by Robert Pearlman, has published articles and reviews by authors Andrew Chaikin (\"A Man on the Moon\"), Kris Stoever (\"For Spacious Skies\"), James Oberg (\"Red Star in Orbit\"), Frederick Ordway III (\"Imagining Space\"), Francis French (\"In the Shadow of the Moon\"), David Hitt (\"Homesteading Space\"), Russell Still (\"Relics of the Space Race\"), Colin Burgess (\"Into That Silent Sea\"), Jay Gallentine (\"Ambassadors From Earth\") and Apollo astronaut Walt Cunningham, among others.\n\nThe website's intended name was spacememorabilia.com, for which a logo had been designed; however, the URL was owned (though not in use) by former Gemini and Apollo astronaut Pete Conrad. Pearlman instead bought the URL collectSPACE.com, which came online on July 20, 1999, the 30th anniversary of the Apollo 11 moon landing (Conrad died unexpectedly July 8).\n\ncollectSpace originally contained a photo gallery, drawing on Pearlman's personal collection; \"Sightings,\" a calendar of astronaut appearances; and a short article about Apollo 11 anniversary toys. \"Sightings\" was chosen to show up in Internet searches for \"Sightings\", a TV series about UFOs. The site's original tagline was \"memorabilia from the conquest of the final frontier,\" which became \"The Source for Space History & Artifacts.\"\n\ncollectSPACE earned national media attention later in 1999 for its role in halting a controversial eBay auction for space shuttle \"Challenger\" debris. In September 1999, it first covered a space memorabilia auction—Christie's East—followed by Superior Galleries of Beverly Hills, California the following month. collectSPACE was the first to webcast space memorabilia auctions, providing live audio (and one year, video) from Superior Gallery's auction floor, as well as live hammer results (auction houses subsequently added their own webcast capabilities or partnered with eBay for live online bidding).\n\nThe site's message board went online in November 1999. Among those posting and replying to messages have been former Apollo (EECOM flight controller) Sy Liebergot; Stephen Clemmons, a member of the Apollo 1 ground support crew; Project Mercury astronaut Scott Carpenter's daughter Kris Stoever; astronaut Pete Conrad's son, Pete Conrad, III; National Air and Space Museum curator Allan Needell, space historian Dwayne A. Day, \"Who's Who in Space\" authors Michael Cassutt and Rex Hall, Kraig McNutt of \"Today In Space History,\" and The Surfaris' former bassist Andrew Lagomarsino, among others. A number of astronauts are known to be cS readers.\n\ncollectSPACE was nominated for \"The Houston Chronicle\"<nowiki>'</nowiki>s best blog in its Ultimate Houston Readers Pick for 2005.\n\nIn 2006, collectSPACE was the first to reveal the name of NASA's next planned manned spacecraft, Orion, and publish its logo; as well as the name Altair for the next planned lunar lander.\n\nIn the wake of the 9-11 terrorist attacks, collectSPACE organized \"Heroes Helping Heroes\", an online auction benefiting the American Red Cross. In partnership with Yahoo! Auctions, the site offered bidders the chance to have an item of their choice signed by one of 22 retired astronauts, who volunteered to participate. $12,686 was raised.\n\nBetween 2003 and 2006, collectSPACE hosted annual silent auctions benefiting the Astronaut Scholarship Foundation. The astronaut experiences and artifacts auctions have raised more than $180,000 for exceptional college students seeking degrees in science and engineering.\n"}
{"id": "1630673", "url": "https://en.wikipedia.org/wiki?curid=1630673", "title": "Decay heat", "text": "Decay heat\n\nDecay heat is the heat released as a result of radioactive decay. This heat is produced as an effect of radiation on materials: the energy of the alpha, beta or gamma radiation is converted into the thermal movement of atoms.\n\nDecay heat occurs naturally from decay of long-lived radioisotopes that are primordially present from the Earth's formation.\n\nIn nuclear reactor engineering, decay heat continues to be generated after the reactor has been shut down (see SCRAM), and nuclear chain reactions have been suspended. The decay of the short-lived radioisotopes created in fission continues at high power, for a time after shut down. The major source of heat production in a newly shut down reactor is due to the beta decay of new radioactive elements recently produced from fission fragments in the fission process.\n\nQuantitatively, at the moment of reactor shutdown, decay heat from these radioactive sources is still 6.5% of the previous core power, if the reactor has had a long and steady power history. About 1 hour after shutdown, the decay heat will be about 1.5% of the previous core power. After a day, the decay heat falls to 0.4%, and after a week it will be only 0.2%. Because radioisotopes of all half life lengths are present in nuclear waste, enough decay heat continues to be produced in spent fuel rods to require them to spend a minimum of one year, and more typically 10 to 20 years, in a spent fuel pool of water, before being further processed. However, the heat produced during this time is still only a small fraction (less than 10%) of the heat produced in the first week after shutdown.\n\nIf no cooling system is working to remove the decay heat from a crippled and newly shut down reactor, the decay heat may cause the core of the reactor to reach unsafe temperatures within a few hours or days, depending upon the type of core. These extreme temperatures can lead to minor fuel damage (e.g. a few fuel particle failures (0.1 to 0.5%) in a graphite moderated gas-cooled design) or even major core structural damage (meltdown) in a light water reactor or liquid metal fast reactor. Chemical species released from the damaged core material may lead to further explosive reactions (steam or hydrogen) which may further damage the reactor.\n\nNaturally occurring decay heat is a significant source of the heat in the interior of the Earth. Radioactive isotopes of uranium, thorium and potassium are the primary contributors to this decay heat, and this radioactive decay is the primary source of heat from which geothermal energy derives.\n\nIn a typical nuclear fission reaction, 187 MeV of energy are released instantaneously in the form of kinetic energy from the fission products, kinetic energy from the fission neutrons, instantaneous gamma rays, or gamma rays from the capture of neutrons. An additional 23 MeV of energy are released at some time after fission from the beta decay of fission products. About 10 MeV of the energy released from the beta decay of fission products is in the form of neutrinos, and since neutrinos are very weakly interacting, this 10 MeV of energy will not be deposited in the reactor core. This results in 13 MeV (6.5% of the total fission energy) being deposited in the reactor core from delayed beta decay of fission products, at some time after any given fission reaction has occurred. In a steady state, this heat from delayed fission product beta decay contributes 6.5% of the normal reactor heat output.\n\nWhen a nuclear reactor has been shut down, and nuclear fission is not occurring at a large scale, the major source of heat production will be due to the delayed beta decay of these fission products (which originated as fission fragments). For this reason, at the moment of reactor shutdown, decay heat will be about 6.5% of the previous core power if the reactor has had a long and steady power history. About 1 hour after shutdown, the decay heat will be about 1.5% of the previous core power. After a day, the decay heat falls to 0.4%, and after a week it will be only 0.2%. The decay heat production rate will continue to slowly decrease over time; the decay curve depends upon the proportions of the various fission products in the core and upon their respective half-lives.  An approximation for the decay heat curve valid from 10 seconds to 100 days after shutdown is\nwhere formula_2 is the decay power, formula_3 is the reactor power before shutdown, formula_4 is the time since reactor startup and formula_5 is the time of reactor shutdown measured from the time of startup (in seconds). For an approach with a more direct physical basis, some models use the fundamental concept of radioactive decay. Used nuclear fuel contains a large number of different isotopes that contribute to decay heat, which are all subject to the radioactive decay law, so some models consider decay heat to be a sum of exponential functions with different decay constants and initial contribution to the heat rate. A more accurate model would consider the effects of precursors, since many isotopes follow several steps in their radioactive decay chain, and the decay of daughter products will have a greater effect longer after shutdown.\n\nThe removal of the decay heat is a significant reactor safety concern, especially shortly after normal shutdown or following a loss-of-coolant accident. Failure to remove decay heat may cause the reactor core temperature to rise to dangerous levels and has caused nuclear accidents, including the nuclear accidents at Three Mile Island and Fukushima I. The heat removal is usually achieved through several redundant and diverse systems, from which heat is removed via heat exchangers. Water is passed through the secondary side of the heat exchanger via the essential service water system which dissipates the heat into the 'ultimate heat sink', often a sea, river or large lake. In locations without a suitable body of water, the heat is dissipated into the air by recirculating the water via a cooling tower. The failure of ESWS circulating pumps was one of the factors that endangered safety during the 1999 Blayais Nuclear Power Plant flood.\n\nAfter one year, typical spent nuclear fuel generates about 10 kW of decay heat per tonne, decreasing to about 1 kW/t after ten years. Hence effective active or passive cooling for spent nuclear fuel is required for a number of years.\n\n\n"}
{"id": "21051994", "url": "https://en.wikipedia.org/wiki?curid=21051994", "title": "Delay composition", "text": "Delay composition\n\nDelay composition, also called delay charge or delay train, is a pyrotechnic composition, a sort of pyrotechnic initiator, a mixture of oxidizer and fuel that burns in a slow, constant rate that should not be significantly dependent on temperature and pressure. Delay compositions are used to introduce a delay into the firing train, e.g. to properly sequence firing of fireworks, to delay firing of ejection charges in e.g. model rockets, or to introduce a few seconds of time between triggering a hand grenade and its explosion. Typical delay times range between several milliseconds and several seconds.\n\nA popular delay charge is a tube of pressed black powder. The mechanical assembly prevents the outright detonation of the charge.\n\nWhile delay compositions are principally similar to other fuel-oxidizer compositions, larger grain sizes and less aggressively reacting chemicals are used. Many of the compositions generate little or no gas during burning. Typical materials used are:\n\nThe burn rates are dependent on: \n\nExamples of some compositions are: \n"}
{"id": "51500017", "url": "https://en.wikipedia.org/wiki?curid=51500017", "title": "Design for additive manufacturing", "text": "Design for additive manufacturing\n\nDesign for additive manufacturing (DfAM or DFAM) is design for manufacturability as applied to additive manufacturing (AM). It is a general type of design methods or tools whereby functional performance and/or other key product life-cycle considerations such as manufacturability, reliability, and cost can be optimized subjected to the capabilities of additive manufacturing technologies.\n\nThis concept emerges due to the enormous design freedom provided by AM technologies. To take full advantages of unique capabilities from AM processes, DFAM methods or tools are needed. Typical DFAM methods or tools includes topology optimization, design for multiscale structures (lattice or cellular structures), multi-material design, mass customization, part consolidation, and other design methods which can make use of AM-enabled features.\n\nDFAM is not always separate from broader DFM, as the making of many objects can involve both additive and subtractive steps. Nonetheless, the name \"DFAM\" has value because it focuses attention on the way that commercializing AM in production roles is not just a matter of figuring out how to switch existing parts from subtractive to additive. Rather, it is about redesigning entire objects (assemblies, subsystems) in view of the newfound availability of advanced AM. That is, it involves redesigning them because their entire earlier design—including even how, why, and at which places they were originally divided into discrete parts—was conceived within the constraints of a world where advanced AM did not yet exist. Thus instead of just modifying an existing part design to allow it to be made additively, full-fledged DFAM involves things like reimagining the overall object such that it has fewer parts or a new set of parts with substantially different boundaries and connections. The object thus may no longer be an assembly at all, or it may be an assembly with many fewer parts. Many examples of such deep-rooted practical impact of DFAM have been emerging in the 2010s, as AM greatly broadens its commercialization. For example, in 2017, GE Aviation revealed that it had used DFAM to create a helicopter engine with 16 parts instead of 900, with great potential impact on reducing the complexity of supply chains. It is this radical rethinking aspect that has led to themes such as that \"DFAM requires 'enterprise-level disruption'.\" In other words, the disruptive innovation that AM can allow can logically extend throughout the enterprise and its supply chain, not just change the layout on a machine shop floor.\n\nDFAM involves both broad themes (which apply to many AM processes) and optimizations specific to a particular AM process. For example, DFM analysis for stereolithography maximizes DFAM for that modality.\n\nAdditive manufacturing is defined as a material joining process, whereby a product can be directly fabricated from its 3D model, usually layer upon layer. Comparing to traditional manufacturing technologies such as CNC machining or casting, AM processes have several unique capabilities. It enables the fabrication of parts with a complex shape as well as complex material distribution. These unique capabilities significantly enlarge the design freedom for designers. However, they also bring a big challenge. Traditional Design for manufacturing (DFM) rules or guidelines deeply rooted in designers’ mind and severely restrict designers to further improve product functional performance by taking advantages of these unique capabilities brought by AM processes. Moreover, traditional feature-based CAD tools are also difficult to deal with irregular geometry for the improvement of functional performance. To solve these issues, design methods or tools are needed to help designers to take full advantages of design freedom provide by AM processes. These design methods or tools can be categorized as Design for Additive Manufacturing\n\nTopology optimization is a type of structural optimization technique which can optimize material layout within a given design space. Compared to other typical structural optimization techniques, such as size optimization or shape optimization, topology optimization can update both shape and topology of a part. However, the complex optimized shapes obtained from topology optimization are always a headache for traditional manufacturing processes such as CNC machining. To solve this issue, additive manufacturing processes can be applied to fabricate topology optimization result. However, it should be noticed, some manufacturing constraints such as minimal feature size also need to be considered during the topology optimization process. Since the topology optimization can help designers to get an optimal complex geometry for additive manufacturing, this technique can be considered one of DFAM methods.\n\nDue to the unique capabilities of AM processes, parts with multiscale complexities can be realized. This provides a great design freedom for designers to use cellular structures or lattice structures on micro or mesoscales for the preferred properties. For example, in the aerospace field, lattice structures fabricated by AM process can be used for weight reduction. In the bio-medical field, bio-implant made of lattice or cellular structures can enhance osseointegration.\n\nParts with multi-material or complex material distribution can be achieved by additive manufacturing processes. To help designers to take use of this advantage, several design and simulation methods has been proposed to support design a part with multiple materials or Functionally Graded Materials . These design methods also bring a challenge to traditional CAD system. Most of them can only deal with homogeneous materials now.\n\nSince additive manufacturing can directly fabricate parts from products’ digital model, it significantly reduces the cost and leading time of producing customized products. Thus, how to rapidly generate customized parts becomes a central issue for mass customization. Several design methods have been proposed to help designers or users to obtain the customized product in an easy way. These methods or tools can also be considered as the DFAM methods.\n\nDue to the constraints of traditional manufacturing methods, some complex components are usually separated into several parts for the ease of manufacturing as well as assembly. This situation has been changed by the using of additive manufacturing technologies. Some case studies have been done to shows some parts in the original design can be consolidated into one complex part and fabricated by additive manufacturing processes. This redesigning process can be called as parts consolidation. The research shows parts consolidation will not only reduce part count, it can also improve the product functional performance. The design methods which can guide designers to do part consolidation can also be regarded as a type of DFAM methods.\n\nLattice structures is type of cellular structures. Thanks to the free-form manufacturing capability of additive manufacturing technology, it is now possible to design complex forms. These structures were previously difficult to manufacture, hence not widely used. Lattice structures have high-strength low mass mechanical properties and multifunctionality. Lattice structures can be found in parts in the aerospace and bio-medical industries \n"}
{"id": "9142025", "url": "https://en.wikipedia.org/wiki?curid=9142025", "title": "Developing tank", "text": "Developing tank\n\nA developing tank is a light-tight container used for developing film. A developing tank allows photographic film to be developed in a daylight environment. This is necessary because most film is panchromatic and therefore can not be exposed to any light during processing. Depending upon the size and type, a developing tank can hold one to many roll or sheet films.\nFamous brands include Paterson, Yankee, Jobo and Nikor.\n\nA film reel holds roll films in a spiral shape. The film is held evenly spaced so that the chemicals in the developing tank reach all of the film.\n\nGeneral tank support 110,126,135,120,620 format films\nDeveloping tanks and film reels for roll films come in two varieties: plastic and stainless steel. With stainless steel reels, the film is clipped to the center and then gently pinched while the reel is turned so that the film falls into the reel's grooves. With a plastic reel, the film is loaded from the outside and then wound onto the reel by rotating the reel with a back-and-forth motion.\n\nThe user begins by opening the film canister (in the case of 35 mm film) or separating the film from a paper backing (in the case of medium format film, e.g. 120/220 format). The film is then loaded onto a film reel in a completely dark environment; this can be a light-tight room or a changing bag. Care must be taken during this step, as improperly loading the film may result in parts of the film not getting developed. Once the film is on the reel it is put into the developing tank, and then a lid is placed on the developing tank. Because the lid prevents light from reaching the film, the rest of the film developing process may be carried out in daylight. In addition to protecting the film from light, the lid contains an opening for rapidly pouring liquids into and out of the developing tank. Finally, a separate cap seals the canister, which prevents the contents of the tank from spilling when agitated by inverting the tank.But the tank should be cleaned by water every time after development as any residual fixer may not lead to proper development of the next film.\n"}
{"id": "1263553", "url": "https://en.wikipedia.org/wiki?curid=1263553", "title": "Dexit", "text": "Dexit\n\nDexit was a publicly traded company in Toronto, Ontario, Canada. It offered a rechargeable, contactless, stored-value smart key tag used for electronic payment in on-line or off-line systems locally in 2003, until it stopped operating in 2006. Dexit was rebranded in 2007 and its name changed to Hosted Data Transaction Solutions Inc (HDX) and is currently now known as Posera.\n\nInstead of coins or cards (and PIN), Dexit used an RFID key tag device associated with funds transferred from ordinary bank accounts. There is no link to access the accounts from the key tag, a feature to guard against the abuse of lost key tags. Accounts can be filled up from the Dexit website, by telephone, at participating merchants, or through pre-approved bank account or credit card balance transfers.\n\nA partnership between small retailers, Dexit Inc., TD Canada Trust, National Bank of Canada, Telus Mobility, Bell Canada in Toronto's downtown, and a few retailers in and around Toronto. There were plans to expand to the rest of Toronto in 2005; however, this does not appear to have occurred.\n\nIn the summer of 2006, Dexit announced a restructuring, and nearly all payment terminals have been removed from stores. Dexit is also offering refunds of all funds that have been stored on Dexit Tags.\n\nDexit later became HDX Solutions Inc.\n\n\n"}
{"id": "970374", "url": "https://en.wikipedia.org/wiki?curid=970374", "title": "Eating utensil etiquette", "text": "Eating utensil etiquette\n\nEating utensil etiquette covers the prescriptive systems of rules, etiquette, in various cultures for using eating utensils.\n\nIn many Asian cultures, it is impolite to point with chopsticks.\n\nWhen used in conjunction with a knife to cut and consume food in Western social settings, two forms of fork etiquette are common. In the \"European style\", the diner keeps the fork in his or her left hand, while in the \"American style\" the fork is shifted between the left and right hands. The \"American style\" is most common in the United States, but the European style is considered proper in other countries.\n\nOriginally, the traditional European method, once the fork was adopted as a utensil, was to transfer the fork to the right hand after cutting food, as it had been considered proper for all utensils to be used with the right hand only. This tradition was brought to America by British colonists and is still in use in the United States. Europe adopted the more rapid style of eating in relatively modern times. \n\nThe European style is to hold the fork in the left hand and the knife in the right. Once a bite-sized piece of food has been cut, it is conducted straight to the mouth by the left hand. For other items, such as potatoes, vegetables or rice, the blade of the knife is used to assist or guide placement of the food on the back of the fork. The tines remain pointing down.\n\nThe knife and fork are both held with the handle running along the palm and extending out to be held by thumb and forefinger. This style is sometimes called \"hidden handle\" because the palm conceals the handle.\n\nIn the American style, also called the \"zig-zag method\" or \"fork switching\", the knife is initially held in the right hand and the fork in the left. Holding food to the plate with the fork tines-down, a single bite-sized piece is cut with the knife. The knife is then set down on the plate, the fork transferred from the left hand to the right hand, and the food is brought to the mouth for consumption. The fork is then transferred back to the left hand and the knife is picked up with the right. In contrast to the European hidden handle grip, in the American style the fork is held much like a spoon or pen once it is transferred to the right hand to convey food to the mouth. Though called \"American style\", this style originated in Europe. \n\nEtiquette experts have noted that the American style of fork-handling is in flux, often being replaced by a hybrid of the traditional American and European styles. In this new style, the fork is not switched between hands between cutting and eating, but may be deployed \"tines-up\" as a scoop when convenient.\n\nThe South East Asian style is similar to the European style, wherein the spoon is held in the right hand throughout consumption (except with certain dishes when a fork is more suitable). The difference is that a spoon is often used in the right hand and knives are rarely used. Rice and soups are a staple of the diet in South East Asian countries, so using a spoon would be practical in such dishes. The spoon is the main utensil in bringing food into the mouth, in tandem with using a fork. The spoon could also be used for manipulating food in the plate and as an alternative for a knife. Often dishes require slicing before serving or sliced into small before cooking to relinquish the use of a knife.\n\nTables are often set with two or more forks, meant to be used for different courses; for example, a salad fork, a meat fork, and a dessert fork. Some institutions wishing to give an impression of high formality set places with many different forks for meals of several courses, although many etiquette authorities regard this as vulgar and prefer that the appropriate cutlery be brought in with each course.\n\nIt should not be necessary for the diner to distinguish between types of forks; forks are used in order from outside to inside, with the exception of oyster forks, which are placed on the right side, the tines nested in the bowl of a spoon.\n\nSetting the knife and fork handles in a 5 and 7 o'clock position with the fork tines turned downwards on the plate is used to indicate to the server or host that the diner has not yet finished with the meal, while placing them together, with the fork tines turned up and the blade of the knife facing inwards, is used to indicate that the diner has finished.\n\n\n"}
{"id": "3295123", "url": "https://en.wikipedia.org/wiki?curid=3295123", "title": "Engineering bill of materials", "text": "Engineering bill of materials\n\nAn engineering bill of materials (EBOM) is a type of bill of materials (BOM) reflecting the product as designed by engineering, referred to as the \"as-designed\" bill of materials. \n\nThe EBOM is not related to modular BOM or configurable BOM (CBOM) concepts, as modular and configurable BOMs are used to reflect selection of items to create saleable end-products.\n\nThe EBOM concept aligns to sales BOMs (as sold), service BOMs (as changed based on changes due to field service).\n\nThis BOM includes all substitute and alternate part numbers, and includes parts that are contained in drawing notes.\n\n"}
{"id": "1621419", "url": "https://en.wikipedia.org/wiki?curid=1621419", "title": "Ensign-Bickford Company", "text": "Ensign-Bickford Company\n\nThe Ensign-Bickford Aerospace & Defense Company (formerly The Ensign-Bickford Company) is a manufacturer of hardware and energetic systems for use in spacecraft, military, and industrial applications. It is a wholly owned subsidiary of Ensign-Bickford Industries.\n\nThe Ensign-Bickford Company (EBCo) was started in 1836 in Simsbury, Connecticut as a manufacturer of William Bickford's safety fuse for use in mining. Safety fuse was a great advance in mining technology over the practice of filling holes with black powder. \n\nThe next step in mining technology was detonating cord. Ensign-Bickford and other companies developed different versions of detonating cord. In 1937, Ensign-Bickford trademarked \"Primacord\" , which became the functional generic name for detonating cord in North America. In May 2003, Ensign-Bickford sold the trademarks and processes to Dyno Nobel Inc of Australia (formerly of Norway).\n\nIn 1956, EBCo began providing research and development work for Frankford Arsenal and Sandia National Laboratories to develop linear shaped charge, a product critical to the early strategic missile and launch vehicle programs. In 1965, the Space Ordnance Division was formed, making contributions to early NASA programs such as Mercury, Gemini, and Apollo. In 1987, the Space Ordnance Division became Ensign-Bickford Aerospace Company, a wholly owned subsidiary of EBCo, later becoming The Ensign-Bickford Aerospace & Defense Company.\n\nIn 2008, EBA&D acquired Shock Tube Systems, and in 2010 acquired SDI Defense and Aerospace and NEA Electronics.\n\nIn addition to its headquarters and manufacturing operations in Simsbury, CT, EBA&D has manufacturing facilities in Graham, KY and Moorpark, CA. Subsidiary NEA Electronics is located within EBA&D's Moorpark site.\n\n"}
{"id": "33385570", "url": "https://en.wikipedia.org/wiki?curid=33385570", "title": "Envelope tracking", "text": "Envelope tracking\n\nEnvelope tracking (ET) describes an approach to radio frequency (RF) amplifier design in which the power supply voltage applied to the RF power amplifier is continuously adjusted to ensure that the amplifier is operating at peak efficiency for power required at each instant of transmission.\n\nA conventional RF amplifier designed with a fixed supply voltage operates most efficiently only when operating in compression.\n\nAmplifiers operating with a constant supply voltage become less efficient as the crest factor of the signal increases, because the amplifier spends more time operating below peak power and, therefore, spends more time operating below its maximum efficiency.\n\nThe need for greater efficiency arises particularly as modulation schemes become more complicated and their peak to average power ratio increases. Older modulation schemes based on phase or frequency modulation with no amplitude information carried on the signal can use amplifiers that are driven into compression and offer high levels of efficiency. As of 2014 mobile communications basestations consumed ~1% of global electricity.\n\nHowever many new communications systems from WiMAX to LTE do use amplitude information. The amplifier cannot be run into compression, because the amplitude information becomes distorted. These amplifiers can only achieve their peak efficiency on the peaks of the amplitude. The remainder of the time power is being dissipated unnecessarily.\n\nThus signals with a high peak to average power ratio mean that low efficiency levels are achieved.\n\nEnvelope tracking adjusts the voltage applied to an RF power amplifier to deliver the power needed at that instant. Envelope information is derived from the IQ modem and is passed to an envelope tracking power supply to provide the required voltage.\n\nIn 2013, Qualcomm became the first company to ship a chip with such technology, which it claimed to be the industry’s first for 3G and 4G LTE mobile devices. R2 Semiconductor became the industry's first ET company to ship a phone with ET in the Samsung Galaxy S5 Mini.\n\nAs of September 2014, at least 16 phones employ ET, including the Samsung Galaxy Note 3, Galaxy S5 Mini, Nexus 5, and iPhone 6. Other component makers evaluating the technology include R2 Semiconductor, Mediatek, RF Micro Devices, Skyworks, Texas Instruments, Analog Devices, Nujira and Eta Devices.\n\nEta Devices, an MIT spinoff based in Cambridge, Massachusetts, is preparing a base station module and a chip that it claims decreases battery drain and work well in high-bandwidth applications. The company says the chip helps lower electricity consumption by 20 percent and helps reduce heat generation by up to 30 percent. Eta's approach increases efficiency at the cost of greater signal noise. The company uses advanced digital signal processing to handle the problem. The Eta basestation is a little smaller than a shoebox, is the first 4G LTE transmitter to achieve average efficiency greater than 70 percent, up from the typical 45 to 55 percent.\n\n"}
{"id": "43595346", "url": "https://en.wikipedia.org/wiki?curid=43595346", "title": "Federal Polytechnic, Bauchi", "text": "Federal Polytechnic, Bauchi\n\nFederal Polytechnic, Bauchi is a polytechnic based in Bauchi State, North-East of Nigeria. It is situated at Gwallameji village Bauchi. The current Rector of the polytechnic is Arch. Sanusi Waziri Gumau. \n"}
{"id": "11666975", "url": "https://en.wikipedia.org/wiki?curid=11666975", "title": "Fermentation starter", "text": "Fermentation starter\n\nA fermentation starter (called simply starter within the corresponding context, sometimes called a mother) is a preparation to assist the beginning of the fermentation process in preparation of various foods and s. A starter culture is a microbiological culture which actually performs fermentation. These starters usually consist of a cultivation medium, such as grains, seeds, or nutrient liquids that have been well colonized by the microorganisms used for the fermentation.\n\nIn descriptions of national cuisines, fermentation starters may be referred to by their national names: \n\nThese starters are formed using a specific cultivation medium and a specific mix of fungal and bacterial strains. \n\nTypical microorganisms used in starters include various bacteria and fungi (yeasts and molds): \"Rhizopus\", \" Aspergillus\", \"Mucor\", \"Amylomyces\", \"Endomycopsis\", \"Saccharomyces\", \"Hansenula anomala\", \"Lactobacillus\", \"Acetobacter\", etc. Various national cultures have various active ingredients in starters, and often involve mixed microflora.\n\nIndustrial starters include various enzymes, in addition to microflora.\n\n"}
{"id": "20828361", "url": "https://en.wikipedia.org/wiki?curid=20828361", "title": "Forest railway", "text": "Forest railway\n\nA forest railway, forest tram, timber line, logging railway or logging railroad is a mode of railway transport which is used for forestry tasks, primarily the transportation of felled logs to sawmills or railway stations.\n\nIn most cases this form of transport utilised narrow gauges, and were temporary in nature, and in rough and sometimes difficult to access terrain.\n\nBefore the railway was invented, logs were transported in large numbers from the forest down rivers either freely or on wooden rafts. This was not without its problems and wood was often damaged in transit, lost in floods or stranded in shallow water. Suitable rivers were often unavailable in mountainous terrain.\n\nSimple wagonways, using horses and wooden rails, were used from the 18th century. However the invention of the steam locomotive and steel rails soon led to these being employed for forestry. However the difficult terrain within forests meant that narrow-gauge railways, which took up less space, were lighter and easier to build and enabled tight curves to be laid, were preferred. These were the so-called forest railways. In particularly large areas of forest or forests of unusually large trees, such as in the northwestern USA, extensive forest railways were even built using standard gauge exclusively for forestry tasks. Special geared locomotives such as the Shay and Climax locomotive were developed for high tractive effort on rough track. Some forest railways became common carriers when cleared forest land was converted to agricultural or recreational use. \nIn cases where the railway itself was considered very short-term, or the region was extremely difficult to access, logs would often be laid into the ground as a pole road, rather than the cost and logistics of laying steel rails and sleepers. Pole roads could be extensive; several examples in the southeastern United States extended up to 20 miles at the end of the nineteenth century, and used purpose-built steam locomotives.\n\nIn addition to steam traction, diesel and petrol-driven locomotives were also used later on. These largely brought animal-hauled transportation to an end on the forest railways. Also common were routes that just used gravity. Wagons loaded with wood would simply roll downhill in a controlled fashion under the pull of gravity. Foresters also travelled on these, at some risk to their lives on occasions – as brakemen. Empty wagons were hauled uphill again by horses.\n\nFrom the second half of the 20th century forest railways were threatened by road transportation and by the end of the 1960s they had practically disappeared from western Europe. Roads were often laid in their place on the old trackbeds.\n\nIn a few Eastern European countries forest railways survived longer, particularly in Russia where there are still some today. In Hungary too there are several forest railways in active service today, some are also used for tourist traffic. The numerous forest railway operations in Romania were closed, with a few exceptions, by the 1990s. In Western Europe there are very few which are even preserved as museum railways.\n\nIn Asia and Oceania (Australia and New Zealand) the history and fate of logging tramways/forest railways is similar to Europe, with most lines either converted to motorised truck transport or closing down in the 1960s. Significant numbers of locomotives and other remnants of the former lines are found in museums and museum railways in Australia.\n\n\n\n\n\n\n\n\nA logging railroad describes railroads, pole roads, tram roads, or similar infrastructure used to transport harvested timber from a logging site to a sawmill. Logging railroads vary in gauge and length, with most forested regions of the world supporting a railroad of this type at some point.\n\nWhile most railroads of this variety were temporary, it was not uncommon for permanent railroads to take their place as a complement to logging operations or as an independent operation once logging ended.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "50449380", "url": "https://en.wikipedia.org/wiki?curid=50449380", "title": "Forrest Pritchard", "text": "Forrest Pritchard\n\nForrest Pritchard (born June 1, 1974) is a \"New York Times\" bestselling author and seventh generation sustainable farmer, living at Smithfield Farm in Berryville, Virginia, United States. He is a graduate of Episcopal High School and The College of William and Mary, where he won the Academy of American Poets prize in 1996. His farm, called Smith Meadows, was one of the first \"grass-finished\" farms in the country, and sells at farmers' markets in the Washington, DC area. His books include the \"New York Times\" bestseller \"Gaining Ground, A Story of Farmers' Markets, Local Food and Saving the Family Farm\" (Lyons, 2013, , no. 19 in E-books non-fiction) named a top read by NPR and Publishers Weekly, and \"Growing Tomorrow: Behind The Scenes With 18 Sustainable Farmers Who Are Changing The Way We Eat\" (with Molly Peterson, The Experiment, 2015 ).\n"}
{"id": "30871229", "url": "https://en.wikipedia.org/wiki?curid=30871229", "title": "Glazing (window)", "text": "Glazing (window)\n\nGlazing, which derives from the Middle English for 'glass', is a part of a wall or window, made of glass. Glazing also describes the work done by a professional \"glazier\". Glazing is also less commonly used to describe the insertion of ophthalmic lenses into an eyeglass frame.\n\nCommon types of glazing that are used in architectural applications include clear and tinted float glass, tempered glass, and laminated glass as well as a variety of coated glasses, all of which can be glazed singly or as double, or even triple, glazing units. Ordinary clear glass has a slight green tinge but special colorless glasses are offered by several manufacturers.\n\nGlazing can be mounted on the surface of a window sash or door stile, usually made of wood, aluminium or PVC. The glass is fixed into a rabbet (rebate) in the frame in a number of ways including triangular glazing points, putty, etc. Toughened and laminated glass can be glazed by bolting panes directly to a metal framework by bolts passing through drilled holes.\n\nGlazing is commonly used in low temperature solar thermal collectors because it helps retain the collected heat.\n"}
{"id": "17674024", "url": "https://en.wikipedia.org/wiki?curid=17674024", "title": "HBT (explosive)", "text": "HBT (explosive)\n\nHBT is a bistetrazole. It is an explosive approximately as powerful as HMX or CL-20, but it releases less toxic reaction products when detonated: ammonia and hydrogen cyanide. When combined with ADM or AN oxidizers, the amount of HCN produced by a deflagration may be reduced. The compound is thus considered by its advocates to be a more environmentally friendly explosive than traditional nitroamine-based explosives.\n\n"}
{"id": "47255995", "url": "https://en.wikipedia.org/wiki?curid=47255995", "title": "Homework gap", "text": "Homework gap\n\nThe homework gap refers to the difficulty students experience completing homework when they lack internet access at home, compared to those who have access. According to a Pew Research Center analysis of the U.S. Census Bureau’s American Community Survey data from 2013, there were approximately 5 million households with school-age children in the United States that lacked access to high-speed Internet service. Low-income households and minority households made up a “disproportionate” share of those 5 million households; 31.4% of households with school-age children whose incomes fell below $50,000 fell into the group without internet access at home. According to Pew, this group makes up about 40% of all families with school-age children in the United States.\n\nOne of the most commonly cited reasons for students’ lack of internet access at home is that it costs too much for their families to afford. In one study of K-12 students who did not have home internet access, 38% of all the families who were surveyed reported that it was too expensive for them. Although the homework gap is commonly thought of as a problem that mostly affects rural students, its greatest impact falls on students who live in urban areas, who have internet plans available to them but whose families cannot afford the price. Out of 64 million Americans with no internet access, only 16 million live in areas where there is no infrastructure. Some students who report having internet access at home are only able to use the internet through smartphones and mobile hotspots, in which case their usage depends on having enough data on their family’s monthly data plan. Studies suggest that students whose families rely on mobile phones for internet usage often run out of data quickly or periodically lose access to phone service due to bills not being paid, making it an unreliable internet source for students needing to complete school work.\n\nSome students in rural areas of the United States are unable to access the internet at home because the infrastructure for internet access does not exist where they live. A study conducted by the Institute of Education Sciences in 2015 found that 18% of students living in remote rural areas had either no internet access or they only had dial-up internet access. Even if students in rural areas are able to access the internet at home, it is likely that their internet speed is inadequate. According to a 2015 FCC report, 53% of Americans living in rural areas could not reach 25 Mbps download speed, which is the benchmark broadband speed set by the FCC.\n\nStudies have shown that a student’s likeliness of having no internet access at home is increased by several socioeconomic factors. An important factor is the level of education of students’ parents: 71% of children whose parents had a bachelor’s degree had internet access in their homes in 2015. Comparatively, only 42% of children whose parents had not completed high school had internet access. For students whose families earned a higher income, they were more likely to have internet access at home compared with students whose families earned a lower income or were below the poverty line. Students who are racial or ethnic minorities tend to have a lower percentage of internet usage at home than students who are white.\n\nStudents who lack internet access at home risk falling behind on schoolwork, as well as being unable to learn technology skills that are needed for many jobs. Being unable to learn internet skills at a young age could put students at a disadvantage in the future. 94% of low income school districts utilize homework that is online, which makes internet access a necessity for students. In order to complete homework, students may resort to such measures as waiting for hours to use a public library computer or standing outside of their school after hours to try and pick up a WiFi signal. Research on the correlation between internet access and school performance has been limited. However, one study has shown a significant difference between states that have higher broadband access and states that do not, with the latter having students that tend to achieve lower scores in academic subjects. Another study found that in 2015, almost 50% of students surveyed said that they had missed assignments due to lack of internet access. With the homework gap impacting low-income populations, it is possible that the achievement gap between different socioeconomic groups of students will grow.\n\nPolicymakers in the United States have highlighted the homework gap as an issue of national importance. President Barack Obama cited the homework gap in launching the ConnectHome initiative in July 2015. Senator Angus King (I-ME) and Senator Shelley Moore Capito (R-WV) introduced legislation on the topic. FCC Commissioner Jessica Rosenworcel, who is credited with coining the term the “homework gap,” has encouraged changes to U.S. broadband internet and wi-fi policies to address this problem.\n\nThe homework gap has been referred to some as a civil rights issue, as students are being denied information. In 2012, the United States government created a division called the Open Technology Fund. This program was created by the government to help support internet freedom technologies that ensure access, circumvent censorship, surveillance, and to promote human rights in the United States and on a global scale. This program is solely funded by grants and donations by interested parties. The goal is to have the access to the internet not be restricted by outside parties, attempting to ensure there are no conflicts of interest in regards to content being delivered to consumers. Currently, the program is working on completing and funding approximately 83 projects in upwards of 20 different countries, with most of the projects focusing on the United States.\n\nMobile hotspot lending programs have been an effective strategy for increasing access to the internet. These programs, while opening access to the internet to those that can not afford it, are limited to only being helpful to students with access to smart devices, or some form of laptop computer.  There are a number of different mobile hotspot lending programs attempting to increase the availability to the internet for students. For example, there is the Kajeet SmartSpot.  With this device, students get filtered Internet access anytime, in any location. Additionally, educators receive reports into student use of digital resources that can inform future academic strategies.\n\nOne attempt to connect students to the internet is the implementation of WiFi on certain school buses.  In North Carolina and Missouri, WiFi was added to school buses so students could finish their homework while they commuted to school. An example of this program is offered via Kajeet Smartbus. The benefit of this program has been linked to not only students completing their homework assignments more often, but also to better overall behavior on the daily bus commutes, offering relief for teachers, school employees, parents, students, and bus drivers. While North Carolina and Missouri were test states, the program is expecting to expand to other states that struggle with the homework gap. However, as with the mobile hotspot programs, these programs are only efficient if the students have access to hardware like a computer, tablet, or in some cases smartphones, that allow them to access the internet.  \n"}
{"id": "188545", "url": "https://en.wikipedia.org/wiki?curid=188545", "title": "Hydrogen vehicle", "text": "Hydrogen vehicle\n\nA hydrogen vehicle is a vehicle that uses hydrogen as its onboard fuel for motive power. Hydrogen vehicles include hydrogen-fueled space rockets, as well as automobiles and other transportation vehicles. The power plants of such vehicles convert the chemical energy of hydrogen to mechanical energy either by burning hydrogen in an internal combustion engine, or by reacting hydrogen with oxygen in a fuel cell to run electric motors. Widespread use of hydrogen for fueling transportation is a key element of a proposed hydrogen economy.\n\n, there are three hydrogen cars publicly available in select markets: the Toyota Mirai, the Hyundai Nexo, and the Honda Clarity. Several other companies are working to develop hydrogen cars. As of 2014, 95% of hydrogen is made from natural gas. It can be produced using renewable sources, but that is an expensive process. Integrated wind-to-hydrogen (power-to-gas) plants, using electrolysis of water, are exploring technologies to deliver costs low enough, and quantities great enough, to compete with hydrogen production using natural gas. The drawbacks of hydrogen use are high carbon emissions intensity when produced from natural gas, capital cost burden, low energy content per unit volume, production and compression of hydrogen, and the large investment in infrastructure that would be required to fuel vehicles.\n\nAutomobiles, buses, forklifts, trains, PHB bicycles, canal boats, cargo bikes, golf carts, motorcycles, wheelchairs, ships, airplanes, submarines, and rockets can already run on hydrogen, in various forms. NASA used hydrogen to launch Space Shuttles into space. A working toy model car runs on solar power, using a regenerative fuel cell to store energy in the form of hydrogen and oxygen gas. It can then convert the fuel back into water to release the solar energy. Since the advent of hydraulic fracturing the key concern for hydrogen fuel cell vehicles is consumer and public policy confusion concerning the adoption of natural gas powered hydrogen vehicles with heavy hidden emissions to the detriment of environmentally friendly transportation.\n\nA land-speed record for a hydrogen-powered vehicle of was set by Ohio State University's Buckeye Bullet 2, which achieved a \"flying-mile\" speed of at the Bonneville Salt Flats in August 2008. A record of was set by a prototype Ford Fusion Hydrogen 999 Fuel Cell Race Car at the Bonneville Salt Flats, in August 2007, using a large compressed oxygen tank to increase power.\n\n, there are 3 hydrogen cars publicly available in select markets: the Toyota Mirai, the Hyundai Nexo, and the Honda Clarity.\n\nIn 2013 the Hyundai Tucson FCEV was the first commercially mass-produced hydrogen fuel cell vehicle in the world. It could run in the range of nearly 600km. Hyundai Nexo succeeded it in 2018.\n\nToyota launched its first production fuel cell vehicle (FCV), the Mirai, in Japan at the end of 2014 and began sales in California, mainly the Los Angeles area, in 2015. The car has a range of and takes about five minutes to refill its hydrogen tank. The initial sale price in Japan was about 7 million yen ($69,000). Former European Parliament President Pat Cox estimated that Toyota would initially lose about $100,000 on each Mirai sold. Many automobile companies have introduced demonstration models in limited numbers (see List of fuel cell vehicles and List of hydrogen internal combustion engine vehicles). One disadvantage of hydrogen compared to other automobile fuels is its low density.\n\nIn 2013 BMW leased hydrogen technology from Toyota, and a group formed by Ford Motor Company, Daimler AG, and Nissan announced a collaboration on hydrogen technology development. By 2017, however, Daimler had abandoned hydrogen vehicle development, and most of the automobile companies developing hydrogen cars had switched their focus to battery electric vehicles.\nFuel-cell buses (as opposed to hydrogen fueled buses) are being trialed by several manufacturers in different locations, for example, Ursus Lublin. The Fuel Cell Bus Club is a global fuel cell bus testing collaboration.\n\nIn March 2015, China South Rail Corporation (CSR) demonstrated the world's first hydrogen fuel cell-powered tramcar at an assembly facility in Qingdao. The chief engineer of the CSR subsidiary CSR Sifang Co Ltd., Liang Jianying, said that the company is studying how to reduce the running costs of the tram. Tracks for the new vehicle have been built in seven Chinese cities. China plans to spend 200 billion yuan ($32 billion) through 2020 to increase tram tracks to more than 1,200 miles.\n\nIn northern Germany in 2018 the first fuel-cell powered Coradia iLint trains were placed into service; excess power is stored in lithium-ion batteries.\n\nIn 2007, Pearl Hydrogen Power Sources of Shanghai, China, unveiled a hydrogen bicycle at the 9th China International Exhibition on Gas Technology, Equipment, and Applications.\n\nGeneral Motors' military division, GM Defense, focuses on hydrogen fuel cell vehicles. Its SURUS (Silent Utility Rover Universal Superstructure) is a flexible fuel cell electric platform with autonomous capabilities. Since April 2017, the U.S. Army has been testing the commercial Chevrolet Colorado ZH2 on its U.S. bases to determine the viability of hydrogen-powered vehicles in military mission tactical environments.\n\nENV develops electric motorcycles powered by a hydrogen fuel cell, including the Crosscage and Biplane. Other manufacturers as Vectrix are working on hydrogen scooters. Finally, hydrogen-fuel-cell-electric-hybrid scooters are being made such as the Suzuki Burgman fuel-cell scooter and the FHybrid. The Burgman received \"whole vehicle type\" approval in the EU. The Taiwanese company APFCT conducted a live street test with 80 fuel-cell scooters for Taiwan's Bureau of Energy.\n\nAutostudi S.r.l's H-Due is a hydrogen-powered quad, capable of transporting 1-3 passengers. A concept for a hydrogen-powered tractor has been proposed.\n\nCompanies such as Boeing, Lange Aviation, and the German Aerospace Center pursue hydrogen as fuel for manned and unmanned airplanes. In February 2008 Boeing tested a manned flight of a small aircraft powered by a hydrogen fuel cell. Unmanned hydrogen planes have also been tested. For large passenger airplanes, \"The Times\" reported that \"Boeing said that hydrogen fuel cells were unlikely to power the engines of large passenger jet airplanes but could be used as backup or auxiliary power units onboard.\"\n\nIn July 2010, Boeing unveiled its hydrogen-powered Phantom Eye UAV, powered by two Ford internal-combustion engines that have been converted to run on hydrogen.\n\nIn Britain, the Reaction Engines A2 has been proposed to use the thermodynamic properties of liquid hydrogen to achieve very high speed, long distance (antipodal) flight by burning it in a precooled jet engine.\n\nA HICE forklift or HICE lift truck is a hydrogen fueled, internal combustion engine-powered industrial forklift truck used for lifting and transporting materials. The first production HICE forklift truck based on the Linde X39 Diesel was presented at an exposition in Hannover on May 27, 2008. It used a 2.0 litre, diesel internal combustion engine converted to use hydrogen as a fuel with the use of a compressor and direct injection.\n\nA fuel cell forklift (also called a fuel cell lift truck) is a fuel cell powered industrial forklift truck. In 2013 there were over 4,000 fuel cell forklifts used in material handling in the US. The global market was estimated at 1 million fuel cell powered forklifts per year for 2014–2016. Fleets are being operated by companies around the world. Pike Research stated in 2011 that fuel-cell-powered forklifts will be the largest driver of hydrogen fuel demand by 2020.\n\nMost companies in Europe and the US do not use petroleum powered forklifts, as these vehicles work indoors where emissions must be controlled and instead use electric forklifts. Fuel-cell-powered forklifts can provide benefits over battery powered forklifts as they can be refueled in 3 minutes. They can be used in refrigerated warehouses, as their performance is not degraded by lower temperatures. The fuel cell units are often designed as drop-in replacements.\n\nMany large rockets use liquid hydrogen as fuel, with liquid oxygen as an oxidizer (LH2/LOX). An advantage of hydrogen rocket fuel is the high effective exhaust velocity compared to kerosene/LOX or UDMH/NTO engines. According to the Tsiolkovsky rocket equation, a rocket with higher exhaust velocity uses less propellant to accelerate. Also the energy density of hydrogen is greater than any other fuel. LH2/LOX also yields the greatest efficiency in relation to the amount of propellant consumed, of any known rocket propellant.\n\nA disadvantage of LH2/LOX engines is the low density and low temperature of liquid hydrogen, which means bigger and insulated and thus heavier fuel tanks are needed. This increases the rocket's structural mass which reduces its delta-v significantly. Another disadvantage is the poor storability of LH2/LOX-powered rockets: Due to the constant hydrogen boil-off, the rocket must be fueled shortly before launch, which makes cryogenic engines unsuitable for ICBMs and other rocket applications with the need for short launch preparations.\n\nOverall, the delta-v of a hydrogen stage is typically not much different from that of a dense fuelled stage, but the weight of a hydrogen stage is much less, which makes it particularly effective for upper stages, since they are carried by the lower stages. For first stages, dense fuelled rockets in studies may show a small advantage, due to the smaller vehicle size and lower air drag.\n\nLH2/LOX were also used in the Space Shuttle to run the fuel cells that power the electrical systems. The byproduct of the fuel cell is water, which is used for drinking and other applications that require water in space.\n\nIn 2016 Nikola Motor Company introduced a hydrogen-powered Class 8 heavy truck powered by a 320 kWh EV battery. Nikola plans two versions of the hydrogen powered truck, long haul Nikola One and day cab Nikola Two. United Parcel Service began testing of a hydrogen powered delivery vehicle in 2017. US Hybrid, Toyota, and Kenworth have also announced plans to test Class 8 drayage hydrogen fuel cell trucks.\n\nHydrogen internal combustion engine cars are different from hydrogen fuel cell cars. The hydrogen internal combustion car is a slightly modified version of the traditional gasoline internal combustion engine car. These hydrogen engines burn fuel in the same manner that gasoline engines do; the main difference is the exhaust product. Gasoline combustion results in carbon dioxide and water vapor, while the only exhaust product of hydrogen combustion is water vapor.\n\nIn 1807 Francois Isaac de Rivaz designed the first hydrogen-fueled internal combustion engine. In 1965, Roger Billings, then a high school student, converted a Model A to run on hydrogen. In 1970 Paul Dieges patented a modification to internal combustion engines which allowed a gasoline-powered engine to run on hydrogen .\n\nMazda has developed Wankel engines burning hydrogen. The advantage of using an internal combustion engine, like Wankel and piston engines, is the lower cost of retooling for production.\n\nHICE forklift trucks have been demonstrated based on converted diesel internal combustion engines with direct injection.\n\nHydrogen fuel cells are relatively expensive to produce, as their designs require rare substances such as platinum as a catalyst, In 2014, Toyota said it would introduce its Toyota Mirai in Japan for less than $70,000 in 2015. Former European Parliament President Pat Cox estimates that Toyota will initially lose about $100,000 on each Mirai sold.\n\nThe problems in early fuel-cell designs at low temperatures concerning range and cold start capabilities have been addressed so that they \"cannot be seen as show-stoppers anymore\". Users in 2014 said that their fuel cell vehicles perform flawlessly in temperatures below zero, even with the heaters blasting, without significantly reducing range. Studies using neutron radiography on unassisted cold-start indicate ice formation in the cathode, three stages in cold start and Nafion ionic conductivity. A parameter, defined as coulomb of charge, was also defined to measure cold start capability.\n\nThe service life of fuel cells is comparable to that of other vehicles. PEM service life is 7,300 hours under cycling conditions.\n\nHydrogen does not come as a pre-existing source of energy like fossil fuels but is first produced and then stored as a carrier, much like a battery. A suggested benefit of large-scale deployment of hydrogen vehicles is that it could lead to decreased emissions of greenhouse gasses and ozone precursors. However, as of 2014, 95% of hydrogen is made from methane. It can be produced using renewable sources, but that is an expensive process. Integrated wind-to-hydrogen (power to gas) plants, using electrolysis of water, are exploring technologies to deliver costs low enough, and quantities great enough, to compete with traditional energy sources.\n\nAccording to Ford Motor Company, \"when FCVs are run on hydrogen reformed from natural gas using this process, they do not provide significant environmental benefits on a well-to-wheels basis (due to GHG emissions from the natural gas reformation process).\" While methods of hydrogen production that do not use fossil fuel would be more sustainable, currently renewable energy represents only a small percentage of energy generated, and power produced from renewable sources can be used in electric vehicles and for non-vehicle applications.\n\nThe challenges facing the use of hydrogen in vehicles include production, storage, transport, and distribution. The well-to-wheel efficiency for hydrogen is less than 25%. More recent analyses confirm this.\n\nThe molecular hydrogen needed as an onboard fuel for hydrogen vehicles can be obtained through many thermochemical methods utilizing natural gas, coal (by a process known as coal gasification), liquefied petroleum gas, biomass (biomass gasification), by a process called thermolysis, or as a microbial waste product called biohydrogen or Biological hydrogen production. 95% of hydrogen is produced using natural gas, and 85% of hydrogen produced is used to remove sulfur from gasoline. Hydrogen can also be produced from water by electrolysis at working efficiencies in the 50–60% range for the smaller electrolysers and around 65–70% for the larger plants. Hydrogen can also be made by chemical reduction using chemical hydrides or aluminum. Current technologies for manufacturing hydrogen use energy in various forms, totaling between 25 and 50 percent of the higher heating value of the hydrogen fuel, used to produce, compress or liquefy, and transmit the hydrogen by pipeline or truck.\n\nEnvironmental consequences of the production of hydrogen from fossil energy resources include the emission of greenhouse gasses, a consequence that would also result from the on-board reforming of methanol into hydrogen. Analyses comparing the environmental consequences of hydrogen production and use in fuel-cell vehicles to the refining of petroleum and combustion in conventional automobile engines do not agree on whether a net reduction of ozone and greenhouse gasses would result. Hydrogen production using renewable energy resources would not create such emissions, but the scale of renewable energy production would need to be expanded to be used in producing hydrogen for a significant part of transportation needs. As of 2016, 14.9 percent of U.S. electricity was produced from renewable sources. In a few countries, renewable sources are being used more widely to produce energy and hydrogen. For example, Iceland is using geothermal power to produce hydrogen, and Denmark is using wind.\n\nCompressed hydrogen in hydrogen tanks at 350 bar (5,000 psi) and 700 bar (10,000 psi) is used for hydrogen tank systems in vehicles, based on type IV carbon-composite technology.\n\nHydrogen has a very low volumetric energy density at ambient conditions, equal to about one-third that of methane. Even when the fuel is stored as liquid hydrogen in a cryogenic tank or in a compressed hydrogen storage tank, the volumetric energy density (megajoules per liter) is small relative to that of gasoline. Hydrogen has three times higher specific energy by mass compared to gasoline (143 MJ/kg versus 46.9 MJ/kg). In 2011, scientists at Los Alamos National Laboratory and University of Alabama, working with the U.S. Department of Energy, found a single-stage method for recharging ammonia borane, a hydrogen storage compound. In 2018, researchers at CSIRO in Australia powered a Toyota Mirai and Hyundai Nexo with hydrogen separated from ammonia using a membrane technology. Ammonia is easier to transport safely in tankers than pure hydrogen.\n\nThe hydrogen infrastructure for consists of hydrogen-equipped filling stations, which are supplied with hydrogen via compressed hydrogen tube trailers, liquid hydrogen tank trucks or dedicated onsite production, and some industrial hydrogen pipeline transport. The distribution of hydrogen fuel for vehicles throughout the U.S. would require new hydrogen stations that would cost between 20 billion dollars in the US, (4.6 billion in the EU). and half trillion dollars in the US.\n\n, there were 40 publicly accessible hydrogen refueling stations in the US, most which are in located in California (compared with 19,000 electric charging stations). By 2017, there were 91 hydrogen fueling stations in Japan.\n\nHydrogen codes and standards, as well as codes and technical standards for hydrogen safety and the storage of hydrogen, have been identified as an institutional barrier to deploying hydrogen technologies and developing a hydrogen economy. To enable the commercialization of hydrogen in consumer products, new codes and standards must be developed and adopted by federal, state and local governments.\n\nIn 2003, George W. Bush announced an initiative to promote hydrogen powered vehicles. In 2009, President Obama and his Department of Energy Secretary Steven Chu stripped the funding of fuel cell technology due to their belief that the technology was still decades away. Under heavy criticism, the funding was partially restored. In 2014 the Obama administration announced they want to speed up production and development of hydrogen powered vehicles. The Department of Energy planned to spread a $7.2 million investment among the states of Georgia, Kansas, Pennsylvania, and Tennessee to support projects that fuel vehicles and support power systems. The Center for Transportation and The Environment, Fed Ex Express, Air Products and Chemicals, and Sprint have invested in the development of fuel cells. Fuel cells could also be used in handling equipment such as forklifts as well as telecommunications infrastructure.\n\nSenator Byron L. Dorgan stated in 2013: “The Energy and Water Appropriations bill makes investments in our nation’s efforts to develop safe, homegrown energy sources that will reduce our reliance on foreign oil. And, because ongoing research and development is necessary to develop game-changing technologies, this bill also restores funding for Hydrogen energy research”. In June 2013, the U.S. Department of Energy gave 9 million dollars in grants to speed up technology development, 4.5 million for advanced fuel cell membranes, $3 million to 3M corporation to work on membranes with improved durability and performance, and 1.5 million to the Colorado School of Mines for work on simpler and more affordable fuel cell membranes.\n\nIn Japan, hydrogen is mainly to be sourced from outside Japan.\n\nNorway plans a series of hydrogen refueling stations along the main roads.\n\nCritics claim the time frame for overcoming the technical and economic challenges to implementing wide-scale use of hydrogen cars is likely to last for at least several decades, and hydrogen vehicles may never become broadly available. They claim that the focus on the use of the hydrogen car is a dangerous detour from more readily available solutions to reducing the use of fossil fuels in vehicles. In May 2008, \"Wired News\" reported that \"experts say it will be 40 years or more before hydrogen has any meaningful impact on gasoline consumption or global warming, and we can't afford to wait that long. In the meantime, fuel cells are diverting resources from more immediate solutions.\"\n\nCritiques of hydrogen vehicles are presented in the 2006 documentary, \"Who Killed the Electric Car?\". According to former U.S. Department of Energy official Joseph Romm, \"A hydrogen car is one of the least efficient, most expensive ways to reduce greenhouse gases.\" Asked when hydrogen cars will be broadly available, Romm replied: \"Not in our lifetime, and very possibly never.\" The \"Los Angeles Times\" wrote, in 2009, \"Hydrogen fuel-cell technology won't work in cars. ... Any way you look at it, hydrogen is a lousy way to move cars.\"\n\n\"The Economist\" magazine, in 2008, quoted Robert Zubrin, the author of \"Energy Victory\", as saying: \"Hydrogen is 'just about the worst possible vehicle fuel'\". The magazine noted the withdrawal of California from earlier goals: \"In [2008] the California Air Resources Board, an agency of California's state government and a bellwether for state governments across America, changed its requirement for the number of zero-emission vehicles (ZEVs) to be built and sold in California between 2012 and 2014. The revised mandate allows manufacturers to comply with the rules by building more battery-electric cars instead of fuel-cell vehicles.\" The magazine also noted that most hydrogen is produced through steam reformation, which creates at least as much emission of carbon per mile as some of today's gasoline cars. On the other hand, if the hydrogen could be produced using renewable energy, \"it would surely be easier simply to use this energy to charge the batteries of all-electric or plug-in hybrid vehicles.\"\n\nA 2009 study at UC Davis, published in the \"Journal of Power Sources\", similarly found that, over their lifetimes, hydrogen vehicles will emit more carbon than gasoline vehicles. This agrees with a 2014 analysis. \"The Washington Post\" asked in 2009, \"[W]hy would you want to store energy in the form of hydrogen and then use that hydrogen to produce electricity for a motor, when electrical energy is already waiting to be sucked out of sockets all over America and stored in auto batteries\"? \"The Motley Fool\" stated in 2013 that \"there are still cost-prohibitive obstacles [for hydrogen cars] relating to transportation, storage, and, most importantly, production.\"\n\nVolkswagen's Rudolf Krebs said in 2013 that \"no matter how excellent you make the cars themselves, the laws of physics hinder their overall efficiency. The most efficient way to convert energy to mobility is electricity.\" He elaborated: \"Hydrogen mobility only makes sense if you use green energy\", but ... you need to convert it first into hydrogen \"with low efficiencies\" where \"you lose about 40 percent of the initial energy\". You then must compress the hydrogen and store it under high pressure in tanks, which uses more energy. \"And then you have to convert the hydrogen back to electricity in a fuel cell with another efficiency loss\". Krebs continued: \"in the end, from your original 100 percent of electric energy, you end up with 30 to 40 percent.\" \"The Business Insider\" commented: Pure hydrogen can be industrially derived, but it takes energy. If that energy does not come from renewable sources, then fuel-cell cars are not as clean as they seem. ... Another challenge is the lack of infrastructure. Gas stations need to invest in the ability to refuel hydrogen tanks before FCEVs [fuel cell electric vehicles] become practical, and it's unlikely many will do that while there are so few customers on the road today. ... Compounding the lack of infrastructure is the high cost of the technology. Fuel cells are \"still very, very expensive\".\n\nIn 2014, Joseph Romm devoted three articles to updating his critiques of hydrogen vehicles. He stated that fuel cell vehicles still have not overcome the following issues: high cost of the vehicles, high fueling cost, and a lack of fuel-delivery infrastructure. \"It would take several miracles to overcome all of those problems simultaneously in the coming decades.\" Moreover, he wrote, \"FCVs aren't green\" because of escaping methane during natural gas extraction and when hydrogen is produced, as 95% of it is, using the steam reforming process. He concluded that renewable energy cannot economically be used to make hydrogen for an FCV fleet \"either now or in the future.\" GreenTech Media's analyst reached similar conclusions in 2014. In 2015, \"Clean Technica\" listed some of the disadvantages of hydrogen fuel cell vehicles as did \"Car Throttle\". Another \"Clean Technica\" writer concluded that \"while hydrogen may have a part to play in the world of energy storage (especially seasonal storage), it looks like a dead end when it comes to mainstream vehicles.\" A 2016 study in the November issue of the journal \"Energy\" by scientists at Stanford University and the Technical University of Munich concluded that, even assuming local hydrogen production, \"investing in all-electric battery vehicles is a more economical choice for reducing carbon dioxide emissions, primarily due to their lower cost and significantly higher energy efficiency.\"\n\nA 2017 analysis published in \"Green Car Reports\" concluded that the best hydrogen-fuel-cell vehicles consume \"more than three times more electricity per mile than an electric vehicle ... generate more greenhouse-gas emissions than other powertrain technologies ... [and have] very high fuel costs. ... Considering all the obstacles and requirements for new infrastructure (estimated to cost as much as $400 billion), fuel-cell vehicles seem likely to be a niche technology at best, with little impact on U.S. oil consumption. The US Department of Energy agrees, for fuel produced by grid electricity via electrolysis, but not for most other pathways for generation. Argonne National Laboratory developed a model of these emission pathways, to communicate the impact of potential fuel cell vehicle advantages and disadvantages. In 2017, Michael Barnard, writing in \"Forbes\", listed the continuing disadvantages of hydrogen fuel cell cars and concluded that \"by about 2008, it was very clear that hydrogen was and would be inferior to battery technology as a storage of energy for vehicles. [B]y 2025 the last hold-outs should likely be retiring their fuel cell dreams.\"\n\nHydrogen vehicles compete with various proposed alternatives to the modern fossil fuel powered vehicle infrastructure.\n\nPlug-in hybrid electric vehicles, or PHEVs, are hybrid vehicles that can be plugged into the electric grid and contain an electric motor and also an internal combustion engine. The PHEV concept augments standard hybrid electric vehicles with the ability to recharge their batteries from an external source, enabling increased use of the vehicle's electric motors while reducing their reliance on internal combustion engines. The infrastructure required to charge PHEVs is already in place, and transmission of power from grid to car is about 93% efficient. This, however, is not the only energy loss in transferring power from grid to wheels. AC/DC conversion must take place from the grids AC supply to the PHEV's DC. This is roughly 98% efficient. The battery then must be charged. As of 2007, the Lithium iron phosphate battery was between 80-90% efficient in charging/discharging. The battery needs to be cooled; the GM Volt's battery has four coolers and two radiators. As of 2009, \"the total well-to-wheels efficiency with which a hydrogen fuel cell vehicle might utilize renewable electricity is roughly 20% (although that number could rise to 25% or a little higher with the kind of multiple technology breakthroughs required to enable a hydrogen economy). The well-to-wheels efficiency of charging an onboard battery and then discharging it to run an electric motor in a PHEV or EV, however, is 80% (and could be higher in the future)—four times more efficient than current hydrogen fuel cell vehicle pathways.\" A 2006 article in \"Scientific American\" argued that PHEVs, rather than hydrogen vehicles, would become standard in the automobile industry. A December 2009 study at UC Davis found that, over their lifetimes, PHEVs will emit less carbon than current vehicles, while hydrogen cars will emit more carbon than gasoline vehicles.\n\nInternal combustion engine-based compressed natural gas(CNG), HCNG or LNG vehicles (Natural gas vehicles or NGVs) use methane (Natural gas or Biogas) directly as a fuel source. Natural gas has a higher energy density than hydrogen gas. NGVs using biogas are nearly carbon neutral. Unlike hydrogen vehicles, CNG vehicles have been available for many years, and there is sufficient infrastructure to provide both commercial and home refueling stations. Worldwide, there were 14.8 million natural gas vehicles by the end of 2011. The other use for natural gas is in steam reforming which is the common way to produce hydrogen gas for use in electric cars with fuel cells.\n\nA 2008 \"Technology Review\" article stated, \"Electric cars—and plug-in hybrid cars—have an enormous advantage over hydrogen fuel-cell vehicles in utilizing low-carbon electricity. That is because of the inherent inefficiency of the entire hydrogen fueling process, from generating the hydrogen with that electricity to transporting this diffuse gas long distances, getting the hydrogen in the car, and then running it through a fuel cell—all for the purpose of converting the hydrogen back into electricity to drive the same exact electric motor you'll find in an electric car.\" Thermodynamically, each additional step in the conversion process decreases the overall efficiency of the process.\n\nA 2013 comparison of hydrogen and battery electric vehicles agreed with the 25% figure from Ulf Bossel in 2006 and stated that the cost of an electric vehicle battery \"is rapidly coming down, and the gap will widen further\", while there is little \"existing infrastructure to transport, store and deliver hydrogen to vehicles and would cost billions of dollars to put into place, everyone's household power sockets are \"electric vehicle refueling\" station and the \"cost of electricity (depending on the source) is at least 75% cheaper than hydrogen.\" In 2013 the National Academy of Sciences and DOE stated that even under optimistic conditions by 2030 the price for the battery is not expected to go below $17,000 ($200–$250/kWh) on 300 miles of range. In 2013 Matthew Mench, from the University of Tennessee stated: \"If we are sitting around waiting for a battery breakthrough that will give us four times the range than we have now, we are going to be waiting for a long time\". Navigant Research, (formerly Pike research), on the other hand, forecasts that “lithium-ion costs, which are tipping the scales at about $500 per kilowatt hour now, could fall to $300 by 2015 and to $180 by 2020.” In 2013 Takeshi Uchiyamada, a designer of the Toyota Prius stated: \"Because of its shortcomings – driving range, cost and recharging time – the electric vehicle is not a viable replacement for most conventional cars\".\n\nMany electric car designs offer limited driving range causing range anxiety. For example, the 2013 Nissan Leaf has a range of , the 2014 Mercedes-Benz B-Class Electric Drive has an estimated range of and the Tesla Model S has a range of up to . However, most US commutes are per day round trip and in Europe, most commutes are around round-trip\n\nIn 2013, \"The New York Times\" stated that there are only 10 publicly accessible hydrogen-filling stations in the U.S., eight of which are in Southern California, and that BEVs' cost-per-mile expense in 2013 is one-third as much as hydrogen cars when comparing electricity from the grid and hydrogen at a filling station. The \"Times\" commented: \"By the time Toyota sells its first fuel-cell sedan, there will be about half-million plug-in vehicles on the road in the United States – and tens of thousands of E.V. charging stations.\" In 2013 John Swanton of the California Air Resources Board, who sees them as complementary technologies, stated that EVs have the jump on fuel-cell autos, which \"are like electric vehicles were 10 years ago. EVs are for real consumers, no strings attached. With EVs you have a lot of infrastructure in place. \"The Business Insider\", in 2013 commented that if the energy to produce hydrogen \"does not come from renewable sources, then fuel-cell cars are not as clean as they seem. ... Gas stations need to invest in the ability to refuel hydrogen tanks before FCEVs become practical, and it's unlikely many will do that while there are so few customers on the road today. ... Compounding the lack of infrastructure is the high cost of the technology. Fuel cells are \"still very, very expensive\", even compared to battery-powered EVs.\n\n \n\n"}
{"id": "1952865", "url": "https://en.wikipedia.org/wiki?curid=1952865", "title": "HyperTransport Consortium", "text": "HyperTransport Consortium\n\nThe HyperTransport Consortium is an industry consortium responsible for specifying and promoting the computer bus technology called HyperTransport. \n\nThe Technical Working Group along with several Task Forces manage the HyperTransport specification and drive new developments. A Marketing Working Group promotes the use of the technology and the consortium.\n\nIt was founded in 2001 by Advanced Micro Devices, Alliance Semiconductor, Apple Computer, Broadcom Corporation, Cisco Systems, NVIDIA, PMC-Sierra, Sun Microsystems, and Transmeta. As of 2009 it has over 50 members.\n\nAs of 2009, Mike Uhler of AMD is the President of the Consortium, Mario Cavalli is the General Manager, Brian Holden of PMC-Sierra is both the Vice President and the Chair of the Technical Working Group, Deepika Sarai is the Treasurer.\n\n"}
{"id": "9993236", "url": "https://en.wikipedia.org/wiki?curid=9993236", "title": "Indoor antenna", "text": "Indoor antenna\n\nAn Indoor antenna is a type of radio or TV antenna placed indoors, as opposed to being mounted on the roof. These days, indoor antennas are a common solution for cord cutting, and there are a variety of commercial options. They are usually considered a simple and cheap solution that may work well when the receiver is relatively near to the broadcasting transmitter and the building walls do not shield the radio waves too much.\n\nBeing close to other electric or electronic equipment in the building, an indoor antenna is prone to picking up more electrical noise that may interfere with a clear (analog) reception. Used for digital broadcast, the noise is less of a factor, which recently makes this type of antenna a more popular solution.\n\n\n"}
{"id": "8535925", "url": "https://en.wikipedia.org/wiki?curid=8535925", "title": "Interdigital transducer", "text": "Interdigital transducer\n\nAn interdigital transducer (IDT) is a device that consists of two interlocking comb-shaped arrays of metallic electrodes (in the fashion of a zipper). These metallic electrodes are deposited on the surface of a piezoelectric substrate, such as quartz or lithium niobate, to form a periodic structure.\n\nIDTs primary function is to convert electric signals to surface acoustic waves (SAW) by generating periodically distributed mechanical forces via piezoelectric effect (an input transducer). The same principle is applied to the conversion of SAW back to electric signals (an output transducer). These processes of generation and reception of SAW can be used in different types of SAW signal processing devices, such as band pass filters, delay lines, resonators, sensors, etc. IDT was first proposed by Richard M. White and Voltmer in 1965. \n\n"}
{"id": "39438647", "url": "https://en.wikipedia.org/wiki?curid=39438647", "title": "Laser scanning vibrometry", "text": "Laser scanning vibrometry\n\nThe scanning laser vibrometer or scanning laser Doppler vibrometer, introduced in the 1980s, is an instrument for rapid non-contact measurement and imaging of vibration.\n\nFields where they are applied include automotive, medical, aerospace, micro system and information technology as well as for quality and production control. The optimization of vibration and acoustic behavior are important goals of product development in all of these fields because they are often among the key characteristics that determine a product’s success in the market. They are also in widespread use throughout many universities conducting basic and applied research in areas that include structural dynamics, modal analysis, acoustic optimization and non-destructive evaluation.\n\nThe operating principle is based on the Doppler effect, which occurs when light is back-scattered from a vibrating surface. Both velocity and displacement can be determined by analyzing the optical signals in different ways. A scanning laser vibrometer integrates computer-controlled X,Y scanning mirrors and a video camera inside an optical head. The laser is scanned point-by-point over the test object’s surface to provide a large number of very high spatial resolution measurements. This sequentially measured vibration data can be used to calculate and visualize animated deflection shapes in the relevant frequency bands from frequency domain analysis. Alternatively, data can be acquired in the time domain to, for example, generate animations showing wave propagation across structures. In contrast to contact measuring methods, the test object is unaffected by the vibration measuring process.\nToday, vibrometry covers a huge range of applications such as the study of microstructures moving only a few pm at frequencies up to 30 MHz, all the way up to the intense dynamics occurring in Formula 1 engines with vibration velocities approaching 30 m/s.\n\nA 3D scanning vibrometer combines three optical sensors that accurately detect dynamic movement from different directions in space in order to completely determine the 3D vectors of motion. The software allows each individual x-, y- or z-direction component to be displayed independently, or combined into a single representation. Data can be exported for finite element model validation at nodes previously imported from the model for scan grid definition.\n\n"}
{"id": "8386159", "url": "https://en.wikipedia.org/wiki?curid=8386159", "title": "Ledeburite", "text": "Ledeburite\n\nIn iron and steel metallurgy, ledeburite is a mixture of 4.3% carbon in iron and is a eutectic mixture of austenite and cementite. Ledeburite is not a type of steel as the carbon level is too high although it may occur as a separate constituent in some high carbon steels. It is mostly found with cementite or pearlite in a range of cast irons.\n\nIt is named after the metallurgist Karl Heinrich Adolf Ledebur (1837–1906). He was the first professor of metallurgy at the Bergakademie Freiberg and discovered ledeburite in 1882.\n\nLedeburite arises when the carbon content is between 2.06% and 6.67%. The eutectic mixture of austenite and cementite is 4.3% carbon, FeC:2Fe, with a melting point of 1147 °C.\n\nLedeburite-II (at ambient temperature) is composed of cementite-I with recrystallized secondary cementite (which separates from austenite as the metal cools) and (with slow cooling) of pearlite. The pearlite results from the eutectoidal decay of the austenite that comes from the ledeburite-I at 723 °C. During more rapid cooling, bainite can develop instead of pearlite, and with very rapid cooling martensite can develop.\n\n"}
{"id": "18379", "url": "https://en.wikipedia.org/wiki?curid=18379", "title": "Linear timecode", "text": "Linear timecode\n\nLinear (or Longitudinal) Timecode (LTC) is an encoding of SMPTE timecode data in an audio signal, as defined in SMPTE 12M specification. The audio signal is commonly recorded on a VTR track or other storage media. The bits are encoded using the biphase mark code (also known as \"FM\"): a 0 bit has a single transition at the start of the bit period. A 1 bit has two transitions, at the beginning and middle of the period. This encoding is self-clocking. Each frame is terminated by a 'sync word' which has a special predefined sync relationship with any video or film content.\n\nA special bit in the linear timecode frame, the \"biphase mark correction\" bit, ensures that there are an even number of AC transitions in each timecode frame.\n\nThe sound of linear timecode is a jarring and distinctive noise and has been used as a sound-effects shorthand to imply \"telemetry\" or \"computers\".\n\nIn broadcast video situations, the LTC generator should be tied into house black burst, as should all devices using timecode, to ensure correct color framing and correct synchronization of all digital clocks. When synchronizing multiple clock-dependent digital devices together with video, such as digital audio recorders, the devices must be connected to a common word clock signal that is derived from the house black burst signal. This can be accomplished by using a generator that generates both black burst and video-resolved word clock, or by synchronizing the master digital device to video, and synchronizing all subsequent devices to the word clock output of the master digital device (and to LTC).\n\nMade up of 80 bits per frame, where there may be 24, 25 or 30 frames per second, LTC timecode varies from 960 Hz (binary zeros at 24 frames/s) to 2400 Hz (binary ones at 30 frames/s), and thus is comfortably in the audio frequency range. LTC can exist as either a balanced or unbalanced signal, and can be treated as an audio signal in regards to distribution. Like audio, LTC can be distributed by standard audio wiring, connectors, distribution amplifiers, and patchbays, and can be ground-isolated with audio transformers. It can also be distributed via 75 ohm video cable and video distribution amplifiers, although the voltage attenuation caused by using a 75 ohm system may cause the signal to drop to a level that can not be read by some equipment.\n\nCare has to be taken with analog audio to avoid audible 'breakthrough' (aka \"crosstalk\") from the LTC track to the audio tracks.\n\nLTC care:\n\n\nLongitudinal SMPTE timecode should be played back at a middle-level when recorded on an audio track, as both low and high levels will introduce distortion.\n\nThe basic format is an 80-bit code that gives the time of day to the second, and the frame number within the second. Values are stored in binary-coded decimal, least significant bit first.\nThere are thirty-two bits of user data, usually used for a reel number and date.\n\n\n"}
{"id": "50277898", "url": "https://en.wikipedia.org/wiki?curid=50277898", "title": "List of South African inventions and discoveries", "text": "List of South African inventions and discoveries\n\nThe following is a list and timeline of innovations as well as inventions and discoveries that involved South African people or South Africa including predecessor states in the history of the formation of South Africa. This list covers innovation and invention in the mechanical, electronic, and industrial fields, as well as medicine, military devices and theory, artistic and scientific discovery and innovation, and ideas in religion and ethics.\n\n\n\n\nRetinal Cryosurgery\n• While working at Soweto's Baragwanath Hospital in 1965, South Africa's Dr Selig Percy Amoils unveiled the Amoils Cryo Pencil, which is the world's first surgical tool that uses extreme cold (nitrous oxide) to destroy unwanted tissue.\nDr Amoils' pencil has made retinal detachment surgery and cataract extraction simple and safe - it has been used to treat Margaret Thatcher and Nelson Mandela's eyes. His invention has transformed cryosurgery (the use of extreme cold produced by liquid nitrogen) for gynaecology, lung, heart, mouth, liver and prostate surgery.\n• Pratley’s Putty, George Pratley invented Pratley’s Putty while trying to create a glue that would hold components in an electrical box. Pratley’s Glue had a part in the success of the Moon Landing. In 1969 the substance was used to hold bits of the Apollo XI mission’s Eagle landing craft together.\n\n\n"}
{"id": "40602914", "url": "https://en.wikipedia.org/wiki?curid=40602914", "title": "List of unmanned aerial vehicles of China", "text": "List of unmanned aerial vehicles of China\n\nThis is a list of unmanned aerial vehicles (UAVs, or 'drones'), of China. There are further categories and sub-categories following the main, comprehensive list, and some of the UAVs appear in more than one category/ sub-category when they can be classified by more than one. As late of 2010, there are more than a hundred UAV developers/manufacturers currently in China. By 2014, that number is increased to over two hundred thirty UAV developers / manufacturers in China, with over two third of them are private enterprises (PE), and the remaining are government owned enterprises (GOE). Most of the GOEs are fully capable of indigenously completing the entire development of UAVs of various sizes, from the initial design at the very beginning, all the way to the final completion of UAVs of various sizes, ranging from the smallest micro air vehicle (MAV)s to the largest UAVs. In contrast, most PEs lack such capability because they are assemblers purchasing existing commercial off-the-shelf (COTS) subsystems such as airframes, flight control systems (FCS) and propulsion systems and integrating these subsystems together into final products of their own. Compounded with transparency and language hurdles, this often creates confusion among observers and analysts outside Chinese because the same UAV appears to be shown by different firms, while in reality, the only thing common is the COTS airframe chosen by different firms, which choose different COTS for other subsystems when assembling UAV of their own. For those PEs do have the full capability to indigenously completing the entire development of UAVs, their products are often limited to unmanned blimp/airships, unmanned helicopters/multirotors and micro air vehicle (MAV)s, which consist of the majority of Chinese UAVs. The Chinese UAV market is projected to be ¥ 10 billion in 2015, and within next two decades, it is expected to increase to ¥ 46 billion. As the end of 2013, there are over fifteen thousand UAVs operating in China in the civilian sector alone.\n\nThis main list including miniature, micro (MAVs), and unmanned combat air vehicles (UCAVs), unmanned blimps, rotary-wing UAVs of the People's Republic of China.\n\nThis is a list of unmanned UAVs of the People's Republic of China from the main list above, which are designed to be launched amphibiously or from water.\n\nThis is a list of unmanned UAVs of the People's Republic of China from the main list above, which are designed to be launched by various artilleries.\n\nThis is a list of unmanned experimental UAVs of the People's Republic of China from the main list above.\n\nThis is a list of unmanned experimental UAVs of the People's Republic of China from the main list above, and they are developed to explore the technologies necessary for aircraft carrier landing for ship-borne aircraft.\n\nThis is a list of unmanned experimental UAVs of the People's Republic of China from the main list above, and they are developed to explore the technologies needed to deploy UAV from submarines.\n\nThis is a list of unmanned experimental UAVs of the People's Republic of China from the main list above, and they are developed to explore the stealth technologies and associated flight control systems, particularly those of flying wing design.\n\nThis is the list of UAVs of Forward-swept wing design of the People's Republic of China from the main list above.\nThis is a list of unmanned experimental UAVs of the People's Republic of China from the main list above, and they are developed to explore the technologies of inflatable UAVs.\n\nThis is a list of fuel cell powered UAVs of the People's Republic of China from the main list above.\n\nThis is a list of jet-powered (including rocket-powered) UAVs of the People's Republic of China from the main list above.\n\nThis is the list of UAVs of jointed wing design of the People's Republic of China from the main list above.\n\nThis is a list of unmanned micro air vehicles (MAV) of the People's Republic of China from the main list above. Majority of Chinese fixed- and rotary-wing UAVs are MAVs.\n\nThis is a list of UAVs with parasol wing of the People's Republic of China from the main list above.\n\nIn addition to conventional layout, twin boom design is the second highest number of layout adopted by Chinese UAVs, and this is the list of UAVs in twin boom design of the People's Republic of China from the main list above.\n\nThis is the list of twin engine UAVs of the People's Republic of China from the main list above.\n\nThis is a list of UAVs of the People's Republic of China controlled by smartphones from the main list above.\nThis is a list of unmanned blimps and airships of the People's Republic of China from the main list above.\n\nThis is a list of unmanned cyclogyros of the People's Republic of China from the main list above.\n\nThis is a list of unmanned helicopters of the People's Republic of China from the main list above, excluding unmanned coaxial helicopters, which are listed separately in their own subcategory. Unmanned helicopters consist of a significant portion of Chinese UAVs and most of these Chinese unmanned helicopters are developed as a direct result of an incident in 2007, when Japanese government arrested three officials of Yamaha Motor Company in early 2007 for exporting nine Yamaha unmanned helicopters to China in 2005, allegedly could be converted to military application from its original crop dusting role. This alleged charge is denied by both Yamaha and China, and Yahama has claimed that these unmanned helicopters only have a range of only 200 meters, or 656 feet, from the person who is controlling them and are therefore unlikely to be used to carry weapons of mass destruction, but the Japanese government nonetheless pressed charges. The action of Japanese government triggered a massive Chinese response in a nationwide effort to develop domestic Chinese unmanned helicopters to replace those imported from Japan, including the rapid acceleration of existing unmanned helicopters programs in China. The massive nationwide effort has resulted in more than a hundred domestic Chinese unmanned helicopters, most of which are completed by integrating existing commercial off-the-shelf (COTS) airframes with COTS autopilots and flight control systems (FCS). Eventually these Chinese unmanned helicopters evolved into models with all subsystems such airframe and flight control systems indigenously developed in China.\n\nThis is the list of unmanned coaxial helicopters of the People's Republic of China from the main list above.\n\nThis is a list of unmanned multirotors of the People's Republic of China from the main list above.\n\nThis is a list of unmanned tricopters of the People's Republic of China from the main list above.\n\nThis is a list of unmanned quadcopters of the People's Republic of China from the main list above.\n\nThis is a list of unmanned hexacopters of the People's Republic of China from the main list above.\n\nThis is a list of unmanned octocopters of the People's Republic of China from the main list above.\n\nThis is the list of Unmanned ornithopters of the People's Republic of China from the main list above.\nThis is the list of unmanned powered paragliders of the People's Republic of China from the main list above.\n\nThis is a list of V/STOL UAVs of the People's Republic of China from the main list above.\n\nThis is a list of lift augmented ducted fan V/STOL UAVs of the People's Republic of China from the main list above.\nThis is a list of tiltrotor V/STOL UAVs of the People's Republic of China from the main list above.\n"}
{"id": "41174897", "url": "https://en.wikipedia.org/wiki?curid=41174897", "title": "Ministry of Oil (Kuwait)", "text": "Ministry of Oil (Kuwait)\n\nThe Ministry of Oil is one of the governmental bodies of Kuwait and part of the cabinet.\n\nThe production and export of oil in Kuwait were among the responsibilities of Kuwait's finance department until 1962 when the department was reorganized under the name of the ministry of finance and oil. Then the body was renamed as the ministry of finance and economy and dealt with oil-related policies until April 1975 when the ministry of oil was established. The ministry was established to protect the state’s natural resources along with two other major public bodies, namely the Supreme Petroleum Council and the Kuwait Petroleum Corporation. The ministry is headquartered in Kuwait City. \n\nIn 1986 the ministry was separated from the Ministry of Finance. At the same time on 12 August 1986 the mission of the ministry was refined and includes the following: Protecting, exploiting and developing the petroleum resources, and raising the share of petroleum in the national income and securing the safety of workers, environment and structures. The ministry was reorganized as part of the Ministry of Energy in 2003. Following this reorganization, energy policy of the country is overseen by the ministry. The Kuwait Petroleum Corporation is the major state-run body governed by the ministry.\n\nThe minister of oil is the major advisor to the Amir of Kuwait. The minister is also a member of the Supreme Petroleum Council.\n\nFrom 1978 to June 1990 Ali Khalifa Al Sabah was the minister of oil. Saud Nasser Al Sabah served in the post between 1998 and 2000. He resigned from the post in June 2000 due to an explosion that killed five workers at three oil refineries. Adel Al Subaih served as oil minister until February 2002 when he resigned from the post. He was replaced by Ahmed Fahd Al Sabah as oil minister.\n\nThe ministry was headed by five different officials between February 2006 and February 2009. In late 2007 Bader Mishari Al Humaidhi was appointed oil minister. However, he resigned from office only eight days after his appointment due to harsh criticisms of the members of the Kuwaiti parliament. Mohammad Al Olaim was the oil minister until his resignation in November 2008. Abdulmohsen Al Madaj also served as Kuwaiti oil minister. \n\nFrom February 2009 to May 2011 Ahmad Al Abdullah Al Sabah served as the minister of oil. Mohammad Al Busairi was the oil minister from May 2011 to February 2012. Hani Hussein was named the oil minister in a cabinet reshuffle in February 2012. Hussein resigned from office on 26 May 2013 due to tensions with members of the Kuwaiti parliament. Mustafa Jassem Al Shamali served as oil minister from 4 August 2013 to January 2014, when Ali al-Omair replaced him in the post and became the ninth oil minister since 2004. In November 2015, Al-Omair was replaced by Anas Khalid Al Saleh as acting oil minister.\n"}
{"id": "39892426", "url": "https://en.wikipedia.org/wiki?curid=39892426", "title": "Neenah Foundry", "text": "Neenah Foundry\n\nNeenah Foundry is a manufacturing company in the north central United States, based in Neenah, Wisconsin. The company manufactures cast iron manhole covers, gratings, and similar items for municipal and construction applications. Neenah Enterprises, Inc. manufactures iron castings for the heavy truck, agriculture, construction, and related markets.\n\nNeenah Foundry was established in 1872 by William Aylward, Sr., as Aylward Plow Works. The name was changed to Aylward and Sons in 1904 and to Neenah Foundry Co. in 1922. In 2003, the company filed for bankruptcy. In 2010, the foundry's parent company again filed for and emerged from bankruptcy.\n\nA major customer over the years is the city of Chicago.\n"}
{"id": "13361564", "url": "https://en.wikipedia.org/wiki?curid=13361564", "title": "Organic clothing", "text": "Organic clothing\n\nOrganic clothing is clothing made from materials raised in or grown in compliance with organic agricultural standards. Organic clothing may be composed of Cotton, Jute, Silk, Ramie, or Wool. Textiles do not need to be 100% organic to use the organic label. A more general term is \"organic textiles\", which includes both apparel and home textiles. The technical requirements in terms of certification and origin generally remain same for organic clothing and organic textiles. \n\n"}
{"id": "53587467", "url": "https://en.wikipedia.org/wiki?curid=53587467", "title": "Outline of machine learning", "text": "Outline of machine learning\n\nThe following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\n\n\nSubfields of machine learning\n\nCross-disciplinary fields involving machine learning\n\nApplications of machine learning\n\nMachine learning hardware\n\nMachine learning tools   (list)\n\nMachine learning framework\n\nProprietary machine learning frameworks\n\nOpen source machine learning frameworks\n\nMachine learning library   \n\nMachine learning algorithm\n\n\nMachine learning method   (list)\n\nDimensionality reduction\n\nEnsemble learning\n\nMeta learning\n\nReinforcement learning\n\nSupervised learning\n\nBayesian statistics\n\nDecision tree algorithm\n\nLinear classifier\n\nUnsupervised learning\n\nArtificial neural network\n\nAssociation rule learning\n\nHierarchical clustering\n\nCluster analysis\n\nAnomaly detection\n\nSemi-supervised learning\n\nDeep learning\n\n\n\nHistory of machine learning\n\nMachine learning projects\n\nMachine learning organizations\n\n\nBooks about machine learning\n\n\n\n\n\n\n\n\n"}
{"id": "12461193", "url": "https://en.wikipedia.org/wiki?curid=12461193", "title": "PCell", "text": "PCell\n\nPCell stands for parameterized cell, a concept used widely in the automated design of analog integrated circuits. A PCell represents a part or a component of the circuit whose structure is dependent on one or more parameters. Hence, it is a cell which is automatically generated by electronic design automation (EDA) software based on the values of these parameters. For example, one can create a transistor PCell and then use different instances of the same with different user defined lengths and widths. Vendors of EDA software sometimes use different names for the concept of parameterized cells, e.g. \"T-Cell\" and \"Magic Cell\".\n\nIn electronic circuit designs, cells are basic units of functionality. A given cell may be placed or instantiated many times. A PCell is more flexible than a non-parameterized cell because different instances may have different parameter values and, therefore, different structures. For example, rather than having many different cell definitions to represent the variously sized transistors in a given design, a single PCell may take a transistor's dimensions (width and length) as parameters. Different instances of a single PCell can then represent transistors of different sizes, but otherwise similar characteristics.\n\nThe structures within an integrated circuit and the rules (design rules) governing their physical dimensions are often complex, thereby making the structures tedious to draw by hand. By using PCells a circuit designer can easily generate a large number of various structures that only differ in a few parameters, thus increasing design productivity and consistency.\n\nMost often, PCell implies a \"physical PCell\", i.e., a physical representation of an electronic component describing its physical structure inside an integrated circuit (IC). Although most PCells are physical PCells, device symbols in circuit schematics may also be implemented as PCells.\n\nUnderlying characteristics of all PCells are a dependence on (input) parameters and the ability to generate design data based on these parameters.\n\nA PCell is a piece of programming code. This code is responsible for the process of creating the proper structure of the PCell variants based on its (input) parameters. For the example of a physical PCell, this code generates (draws) the actual shapes of the mask design that comprise the circuit.\n\nSince one piece of PCell code can create many different objects (with different parameter values), it is referred to as a \"PCell Master\". The object/shapes/data that this code creates is called an \"instance\" of the \"PCell\". Typically, one Master PCell produces many instances/variants. This is not only helpful during design entry and specification but also in reducing memory resources required to represent the design data.\n\nAlthough the programming language in which a PCell is written is not of importance, SKILL or Python are most often used to write PCell's code. Alternatively, PCells can be generated using a graphical user interface (GUI) or specialized PCell design tools based on a library of predefined functions.\n\n"}
{"id": "2916361", "url": "https://en.wikipedia.org/wiki?curid=2916361", "title": "Photographic studio", "text": "Photographic studio\n\nA photographic studio (also known as a photography studio or photo studio) is a workspace to take, develop, print and duplicate photographs. Photographic training and the display of finished photographs may also be accommodated in a photographic studio. The studio may have a darkroom, storage space, a studio proper where photographs are taken, a display room and space for other related work.\n\nA photographic studio is often a business owned and represented by one or more photographers, possibly accompanied by assistants and pupils, who create and sell their own and sometimes others’ photographs.\n\nSince the early years of the 20th century the business functions of a photographic studio have increasingly been called a “photographic agency,” leaving the term “photographic studio” to refer almost exclusively to the workspace.\n\nThe history of photographic studios and photography dates back to 19th century with the first camera. The earliest photographic studios made use of painters' lighting techniques to create photographic portraits. During that era the nothing was better than the sunlight with open window as the primary source of light of painters. Photographic studios started using flashes in 1840. However, not everyone could afford it as they were quite expensive and dangerous. These flashes were also known as 'hot lights' and could have exploded. By 1860s they were in common use in professional studios. 'Tungsten Lights' or 'Hot Lights' were still in use. In around 70s even smaller studios got access to flash lights or strobes.\n\nPeople tried many things from time to time when setting up studios to cope up with different hurdles in photography. However lighting was a big hurdle. Flash powder was the first means of artificial lighting that allowed to produce sufficient brightness to capture the action of the film . However this industry developed at a faster rate. With advancement in camera lenses, lighting and other techniques and equipment, studio photography gained hold and it became quite easier to produce images within a studio.\n\nThe first commercial use of photography was in the production of portraits. Photography replaced painting completely by 40s. with fully equipped studios in existence. The photography process was much shorter and simple as compared to painting, in which the subject and even the painter used to suffer.\n\n'Calotypes' was introduced in 1840s. With the introduction of calotypes the production of negative enabled the photographers to print as many copies as customer required, hence strengthening the very base for the studios. In 1850s small portraits called 'Ambrotypes' were being produced. The exposure time varied between 2 and 20 seconds in comparison to 8 hr long exposure when the first still portrait photograph was takenin 1826. With the time passing by, saw the advancement in the photography. However, trick photography has always been around from as long as photography is. Trick photography was later replaced by Photoshop.\n\nIt became easy and cheap to set up the photographic studios. Modern studios are equipped with new age lighting, gears and technology. They are capable of producing high quality images in bulk. From the past 15 years the studio setup has changed drastically and still getting more digital.\n\n"}
{"id": "26092568", "url": "https://en.wikipedia.org/wiki?curid=26092568", "title": "Plan Calcul", "text": "Plan Calcul\n\nPlan Calcul was a French governmental program to promote a national or European computer industry and associated research and education activities.\n\nThe plan was approved in July 1966 by President Charles de Gaulle, in the aftermath of two key events that made his government worry about French dependence on the US computer industry. In the mid-1960s, the United States denied export licenses for American-made IBM and CDC computers to the French Commissariat à l'énergie atomique in order to prevent it from perfecting its H bomb. Meanwhile, in 1964, General Electric had acquired a majority of Compagnie des Machines Bull, the largest French computer manufacturer, which had the second highest market share in France, after IBM, and was a leading IT equipment maker in Europe. Following this partial takeover, known as \"Affaire Bull\", GE-Bull dropped two Bull computers from its product line.\n\nResponsibility for administering the plan was given to a newly created government agency, \"Délégation à l'informatique\", answering directly to the prime minister.\n\nAs part of the program, in December 1966, the \"Compagnie internationale pour l'informatique\" (CII) was established as a manufacturer of commercial and scientific computers, initially under licence from Scientific Data Systems. The new company was intended to compete not only in the process control and military market, where its staff was already seasoned, but also in the office computing sector of the French market, where IBM and Bull were dominant at the time. The plan enacted government subsidies for CII between 1967 and 1971, and was reconducted for another four years. A minor side of the plan was devoted to peripherals, while CII's main parent company, Thomson-CSF, received government support to develop its semiconductor plants and R & D. Overall, while CII mainframes benefitted from preferential procurement by the French government, the Plan Calcul left peripherals, components and small computers makers compete on the free market. The same went for software companies, which were already thriving in France.\n\nOn the research side, the program also led to the creation of \"L'Institut de recherche en informatique et en automatique\" (IRIA) in 1967, which later became INRIA. It was accompanied with a vast educational effort in programming and computer science.\n\nIn the late 1960s, CII shipped its new, internally designed mainframes (Iris 50 and Iris 80), and developed a mini-computer, Mitra 15, which became a commercial success in the following decade. The company also produced competitive magnetic peripherals in cooperation with CDC.\n\nIn 1971, CII began negotiations with Siemens and Philips to form a joint European company, Unidata, which shipped its first computers in 1974. Yet a new President of the Republic was elected then, former Finance minister Giscard d'Estaing, who was a strong opponent of the Plan Calcul; meanwhile, CII's sleeping partner, CGE-Alcatel, woke up to oppose the domination of its archrival Siemens over the European computer industry. Unidata was terminated and CII was absorbed into Honeywell-Bull in 1976.\n"}
{"id": "21484572", "url": "https://en.wikipedia.org/wiki?curid=21484572", "title": "Remote Sensing Center", "text": "Remote Sensing Center\n\nThe Remote Sensing Center (RSC) at the Naval Postgraduate School was established to bring together a range of capabilities and expertise to address problems of military and intelligence importance, as well as environmental and civil concerns. It is specialized in a variety of remote sensing technologies designed to enable people to look beyond the range of human vision in range or in spectral perception.\n\nMembers of the RSC come from the physics, electrical and computer engineering, computer science, meteorology, and oceanography departments. They are collaborating to develop new remote sensing systems, as well as use and exploit current systems in air and space. It is part of a larger activity in the Monterey Bay area that provides expertise in topical areas outside the technical disciplines available at NPS.\n\nThe Naval Postgraduate School, and specifically the Remote Sensing Center, has the ability to handle classified data, as well as access to a Sensitive Compartmented Information Facility (SCIF) that is fully equipped with comms, storage, and processing capabilities. The RSC has pre-established cooperative research with government, academia, and industry in the remote sensing sector ranging from local to international partners. Highly experienced military officers, intelligence analysts, and faculty are a critical part of the NPS research staff.\n\n\"Lidar\" (LIght raDAR) works as an optical analog to radar in the visible spectrum of light with advantages related to the smaller wavelengths of the laser pulse. Lidar ranges in wavelength from ultra-violet (0.3-0.45 µm) to visible (0.45-0.70 µm) to the infrared (1-15 µm). Lidar can detect much smaller particles than radar in the atmosphere (which cannot detect things smaller than cloud particles), and thus can be used for aerosol detection.\n\nThe raw form of data is a set of x,y,z coordinate points. With recent advances, resolution has improved dramatically. Raw data can be processed to remove unwanted areas or features. Outputs such as topographic maps with contour lines can also be derived from lidar. Programs to manipulate lidar data include ENVI, ERDAS IMAGINE, ArcInfo, and ESRI ArcView (with 3D analyst ext.) One useful derivation of lidar data is the DEM (Digital Elevation Model). DEMs are displayed in a raster format with a matrix. The DEM has a specified cell size that corresponds to the earth’s surfaces. The cell contains the average elevation of the points within it. \n\nThe Remote Sensing Center is planning research projects that undertake the modeling and testing of analytical processing and using more fieldwork to obtain ground-truth measurements. Projects have been completed and are currently underway in terrain classification including Elkhorn Slough and hidden trail identification. Other future projects include a collaboration with the MOdeling, Virtual Environments, and Simulation (MOVES) institute on lidar standards for data structure and visualization tools and modeling new lidar analysis tools.\n\nSpectral imagery measures the spectral character of materials within the visible range and beyond. Two objects may appear visually identical but may be distinguished through examination of their spectral properties. Computer software can use a color scheme to make them visible.\n\nA subset of spectral imagery, hyperspectral imaging data, is produced when \"solar electromagnetic energy reflected from the earth's surface is dispersed into many contiguous narrow spectral bands by an airborne spectrometer\" (Stefanou, 1997, p. 2). Our current research and projects include environmental mapping, target detection, and change detection.\n\nThe Remote Sensing Center works with airborne and satellite systems including IKONOS/Quickbird multispectral imagery (MSI), and airborne hyperspectral imaging (HSI) systems including AVIRIS, HYDICE, CASI, and HYMAP. Classification and analysis, including atmospheric compensation is performed using standard industry research tools; notably ENVI and ERDAS Imagine. The RSC has acquired a polarimetric camera for expanding experimentation in the visible spectrum.\n\nThe Remote Sensing Center benefits from the secure facilities at NPS. Having the ability to process classified data with an on-site, fully equipped Sensitive Compartmented Information Facility (SCIF) allows students and faculty to pursue lines of research and work with technologies unavailable to the public.\n\nThe sustained efforts of fully funded graduate students, both military and civilian with an average of eight to ten years of field experience, have conducted research in an array of topics related to remote sensing.\n\n\nThe Remote Sensing Research and Education Program (RS-REP) is an interdisciplinary program to promote Remote Sensing technical education and research advancement to ensure that the Intelligence Community is fully supported for technology evolution. The curriculum is in hiatus as of 2017.\n\n\n\n\nAnthony W Davis Jr.- Lieutenant, United States Navy\nSeptember 2007\nAdvisor: Richard Olsen\nSecond Reader: David Trask\nMarcus Stavros Stefanou, Electrical Engineering, June, 1997.\nFermin Espinoza-Lieutenant Commander, United States Navy\nRobb E. Owens-Major, United States Air Force\nSeptember 2007\nAdvisor: Richard Christopher Olsen\nSecond Reader: Mark C. Abrams\nMark A. Camacho-Lieutenant, United States Naval Reserve\nSeptember 2006\nAdvisor: Dr. Daria Siciliano\nCo-Advisor: Dr. Richard C. Olsen\n"}
{"id": "57047389", "url": "https://en.wikipedia.org/wiki?curid=57047389", "title": "RemoveDEBRIS", "text": "RemoveDEBRIS\n\nRemoveDEBRIS is a satellite research project intending to demonstrate various space debris removal technologies. The satellite's platform was manufactured by Surrey Satellite Technology Ltd (SSTL) and is a variant of the SSTL X50 series. Partners on the project include Airbus, ArianeGroup, Swiss Center for Electronics and Microtechnology, Inria, Innovative Solutions In Space, Surrey Space Centre, and Stellenbosch University.\n\nRather than engaging in active debris removal (ADR) of real space debris, the RemoveDEBRIS mission plan is to test the efficacy of several ADR technologies on mock targets in low Earth orbit. In order to complete its planned experiments the platform is equipped with a net, a harpoon, a laser ranging instrument, a dragsail, and two CubeSats (miniature research satellites). The experiments are as follows:\n\nRemoveDEBRIS was launched aboard the SpaceX Dragon spacecraft on 2 April 2018 as part of the CRS-14 mission, arriving at the ISS on 4 April. Deployment of the satellite from the station's Kibo module via robotic Canadarm-2 took place on 20 June 2018. At approximately 100 kg, RemoveDEBRIS is the largest satellite to have ever been deployed from the ISS. The full lifespan of the mission from launch to re-entry is estimated at 1.5 years.\n\nOn 16 September 2018, it demonstrated its ability to use net to capture a deployed simulated target.\n\n"}
{"id": "1222993", "url": "https://en.wikipedia.org/wiki?curid=1222993", "title": "Society for Technical Communication", "text": "Society for Technical Communication\n\nThe Society for Technical Communication (STC) is a professional association dedicated to the advancement of the theory and practice of technical communication, with more than 4,500 members in the United States, Canada, and the world. The society publishes a quarterly journal and a magazine eight times a year and hosts an annual international conference (STC Technical Communication Summit). STC also provides online education in the form of live Web seminars, multi-week online certificate courses, virtual conferences, recorded seminars, and more.\n\nHeadquartered in Fairfax, Virginia, US, STC is the largest organization of its type in the world according to its website. It includes 50 chapters, 12 Special Interest Groups (SIGs), and over 4,500 members worldwide. STC members work in a wide range of roles, including:\n\n\nMost STC members belong to one or more \"communities,\" which are either geographic chapters or special interest groups (SIGs). Most chapters are in the United States, but STC includes members from 14 countries. The largest group outside the U.S. is the chapter in Toronto, Ontario, Canada.\n\nSTC publishes a quarterly journal, \"Technical Communication\", and a monthly magazine, \"Intercom\".\n\nThe organization traces its roots to the Society of Technical Writers (STW) in Boston and the Association of Technical Writers and Editors (ATWE) in New York. Both were founded in the United States in 1953. These organizations merged in 1957 to form the Society of Technical Writers and Editors (STWE). In 1960, this group merged with the Technical Publishing Society (TPS), based in Los Angeles, to become the Society of Technical Writers and Publishers. In 1971, the organization's name was changed to the Society for Technical Communication.\n\nThe organization's main journal developed from the \"TWE Journal\" to the \"STWE Review\" to the \"STWP Review\" to \"Technical Communications\" to \"Technical Communication\". Editors of this journal have included Douglas E. Knight, Allan H. Lytel, A. Stanley Higgins, Frank R. Smith, George Hayhoe, and Menno de Jong.\n\nOther important leaders in the history of STC include Robert T. Hamlett (first president of ATWE), A. E. Tyler (first president of TPS), Samuel A. Miles (president of the Society of Technical Writers and Editors, which became ATWE's New York chapter in 1955), Vernon R. Root, Robert O. Shockney, and Stello Jordan. In 2011, Alan Houser was elected vice president of the organization; per their by-laws, he became president in 2012, and was succeeded by his own vice president Nicky Bleiel in 2013.\n\nSTC's annual publications competition for 2012-2013 was held in Washington, D.C. The organization also has branches internationally. On November 12, 2012, STC's branch in India held its 14th annual conference in Bangalore.\n\nIn May or June of each year, STC holds the Technical Communication Summit, the largest conference for technical communicators in the world. The Summit includes over 80 education session broken up into relevant subject areas or tracks; networking events such as the Opening General Session, Welcome Reception, Communities Reception, and Closing Lunch; an Honors Banquet; and an exhibit hall with dozens of companies offering technical communication products or services. The Summit also includes preconference education for an additional price.\n\nIn addition to the Technical Communication Summit, STC offers online education both to its members and nonmembers, with members receiving discounted registration rates. STC offers both live and recorded online education.\n\nSTC recognizes outstanding individuals by conferring the titles of Fellow, Associate Fellow, and Honorary Fellow. STC's Honorary Fellow for 2009 was \"Jimmy Wales, the co-founder of Wikipedia.\"\n\nSTC also sponsors honorary societies for technical communication students with a grade point average of 3.5 or above: \n\n"}
{"id": "1172932", "url": "https://en.wikipedia.org/wiki?curid=1172932", "title": "Sodium azide", "text": "Sodium azide\n\nSodium azide is the inorganic compound with the formula NaN. This colorless salt is the gas-forming component in many car airbag systems. It is used for the preparation of other azide compounds. It is an ionic substance, is highly soluble in water, and is very acutely toxic.\n\nSodium azide is an ionic solid. Two crystalline forms are known, rhombohedral and hexagonal. Both adopt layered structures. The azide anion is very similar in each form, being centrosymmetric with N–N distances of 1.18 Å. The ion has octahedral geometry. Each azide is linked to six Na+ centers, with three Na-N bonds to each terminal nitrogen center.\n\nThe common synthesis method is the \"Wislicenus process,\" which proceeds in two steps from ammonia. In the first step, ammonia is converted to sodium amide:\nThe sodium amide is subsequently combined with nitrous oxide:\nThese reactions are the basis of the industrial route, which produced about 250 tons/y in 2004, with production increasing owing to the popularization of airbags.\n\nCurtius and Thiele developed another production process where a nitrite ester is converted to sodium azide using hydrazine. This method is suited for laboratory preparation of sodium azide:\nAlternatively the salt can be obtained by the reaction of sodium nitrate with sodium amide.\n\nTreatment of sodium azide with strong acids gives hydrazoic acid, which is also extremely toxic:\n\nAqueous solutions contain minute amounts of hydrogen azide, the formation of which is described by the following equilibrium:\n\nSodium azide can be destroyed by treatment with nitrous acid solution:\n\nOlder airbag formulations contained mixtures of oxidizers and sodium azide and other agents including ignitors and accelerants. An electronic controller detonates this mixture during an automobile crash:\nThe same reaction occurs upon heating the salt to approximately 300 °C. The sodium that is formed is a potential hazard alone and, in automobile airbags, it is converted by reaction with other ingredients, such as potassium nitrate and silica. In the latter case, innocuous sodium silicates are generated. Sodium azide is also used in airplane escape chutes. Newer generation air bags contain nitroguanidine or similar less sensitive explosives.\n\nDue to its explosion hazard, sodium azide is of only limited value in industrial scale organic chemistry. In the laboratory, it is used in organic synthesis to introduce the azide functional group by displacement of halides. The azide functional group can thereafter be converted to an amine by reduction with either SnCl in ethanol or lithium aluminium hydride or a tertiary phosphine, such as triphenylphosphine in the Staudinger reaction, with Raney nickel or with hydrogen sulfide in pyridine.\n\nSodium azide is a versatile precursor to other inorganic azide compounds, \"e.g.\", lead azide and silver azide, which are used in explosives.\n\nSodium azide is a useful probe reagent and a preservative.\n\nIn hospitals and laboratories, it is a biocide; it is especially important in bulk reagents and stock solutions which may otherwise support bacterial growth where the sodium azide acts as a bacteriostatic by inhibiting cytochrome oxidase in gram-negative bacteria; gram-positive (streptococci, pneumococci, lactobacilli) are intrinsically resistant.\n\nIt is used in agriculture for pest control of soil-borne pathogens such as \"Meloidogyne incognita\" or \"Helicotylenchus dihystera\".\n\nIt is also used as a mutagen for crop selection of plants such as rice, barley or oats.\n\nSodium azide has caused deaths for decades. It is a severe poison. It possesses the NFPA 704's highest rating of 4 on the heath scale. It may be fatal in contact with skin or if swallowed. Even minute amounts can cause symptoms. The toxicity of this compound is comparable to that of soluble alkali cyanides. No toxicity has been reported from spent airbags.\n\nIt produces extrapyramidal symptoms with necrosis of the cerebral cortex, cerebellum, and basal ganglia. Toxicity may also include hypotension, blindness and hepatic necrosis. Sodium azide increases cyclic GMP levels in brain and liver by activation of guanylate cyclase.\n\n"}
{"id": "59188245", "url": "https://en.wikipedia.org/wiki?curid=59188245", "title": "Talan Products", "text": "Talan Products\n\nTalan Products is a contract manufacturing company specializing in progressive die metal stamping and extruded aluminum supply and fabrication, founded in 1986 and located in Cleveland Ohio. Talan Products manufactures stampings and fabricated aluminum extrusions to the Building Products, Automotive, Fastener, Appliance, LED & Lighting, Defense, and Solar Power industries. Talan Products supplies component parts and assemblies to companies in North America, South America, Middle East, Australia, and Europe.\n\nSteve Peplin, John Talan and Rich Peplin collaborated together to create a manufacturing company in 1986 with a $2,100 investment. They had no manufacturing equipment but landed a customer, contracted out production, and shipped parts. Upon landing their second customer in the next year 1987, they purchased a Niagara punch press machine.\n\nThe company started out on Chatfield Ave in the neighborhood of West Park, Cleveland and moved to Cleveland's Detroit-Shoreway neighborhood in 1991 In 2006 the company moved into a building formely occupied by TRW Inc. in Cleveland's Collinwood Neighborhood. Each relocation is within the city of Cleveland's urban manufacturing areas and was facilitated by growth.\nPete Accorti joined the company as a partner in 1988. John Talan left the company in 1996. Talan Products manufactures component parts for medium and large OEM companies across many industries.\nIn 1993 the company was first recognized for the \"green initiatives\" it had adopted and Pete Accorti was awarded the \"Green Lantern\" award by Crain Communications, Crain's Cleveland Business in 2010.\n\n"}
{"id": "39085480", "url": "https://en.wikipedia.org/wiki?curid=39085480", "title": "TiLite", "text": "TiLite\n\nTiLite designs and manufactures titanium and aluminum wheelchairs. TiLite specializes in wheelchair customization (a process called TiFit) in which each wheelchair is made to users' exact specifications.\n\nOne of TiLite’s first models to market was called the “CrossSport”. Since then, TiLite has developed a full range of titanium and aluminum wheelchairs. TiLite wheelchairs are sold in 38 countries; TiLite is a division of TiSport, LLC, which is headquartered in Pasco, Washington. In 2014, TiLite was acquired by Permobil.\n\nTiLite uses titanium for the frames of several lines of its high-performance wheelchairs. Titanium has the highest strength-to-weight ratio of any metal, it is highly durable, and it absorbs vibrations better than other common frame materials such as aluminum. These unique properties make it desirable, but because titanium is difficult to refine and requires expertise and precision in welding and bending, it is a more costly material. TiLite makes wheelchairs in both titanium and aluminum. While the company's titanium chairs are slightly lighter in weight and more durable, their aluminum wheelchairs are more economical and sufficient for most users. \nThe design and manufacturing process uses parametric modelling, computer aided design and finite element analysis technologies to optimize design and material choices.\n\n"}
{"id": "31344578", "url": "https://en.wikipedia.org/wiki?curid=31344578", "title": "Train inspection system", "text": "Train inspection system\n\nA train inspection system is one of various systems of inspection which are essential to maintain the safe running of rail transport.\n\nBecause safety is of high importance when train cars move across the rails, there must be inspections. The cars are heavy and have moving parts that can break or become defective. Worn or broken parts can drag, pound, and generally destroy the cars and the track structure they run on. Parts and loads must not extend outside the limits of the car, and there should be no leaking of the cars' contents.\n\nQuality inspections are needed not just before a train is moved, but also as it travels to its destination.\n\nThere are several levels of inspection on railroad equipment. Inspections are continuous, starting when the car is being built and repeated at regular intervals.\n\nBefore the train begins its journey, the locomotives and the cars are checked. This is done by mechanical department workers, sometimes cameras and scanners are also used. Some of the items certified \"good to go\" are things like the braking system including hand-brake release, brake application, condition of the air valves, piping and hoses, communication equipment, and many more. Very much like the checklist an airline might use. When the pretrip work is done, the train is considered safe to move.\n\nOnce the movement begins, the train is continuously watched by employees, scanners and monitors. These are sometimes called \"in motion defect detectors\" or defect detectors (DD).\nAs the railroad has evolved from block stations and control towers to a centralized dispatching system, they have also moved to more advanced inspection tools. The technologies in use today vary from a simplistic paddle and switch to infrared thermography, laser scanning, and even ultrasonic audio analysis. These devices are used to inspect engines, cars and the loads on them.\n\nThe systems used on North American and other railways fall into a dozen or so major groups, some are listed below.\n\nHigh impact wheels have some defect where it does not roll smoothly along the track, A flaw in a wheel causes vibrations or banging. This is very destructive to the track structure and the rolling stock.\n\n\nChecks for overloads or shifted loads that can be dangerous.\n\n\nLooks for \"hunting oscillation\" of the trucks or wheelset; a lateral movement in the gauge of the track, like drifting back and forth in a lane of traffic. This action can increase above a certain speed to the point the wheel flanges impact the rails, potentially causing damage to both. There is also force absorbed by this action that will affect the energy consumption of train operations.\n\n\nIs stiff or poorly steering or axle sets that don't follow the path of the track correctly. Presents itself like 'dog tracking\" where the trucks are biased to run against the flange on one side or the other. Truck performance defects include tracking position and angle of attack on a per-axle basis, as well as rotation, shift, inter-axle misalignment, and tracking error on a per-bogie basis. Truck performance detectors can provide early detection of bogie defects, and early warning of derailment risks through flange climb or rail break.\n\nListens with special microphones for internal bearing defects as the equipment moves across the detector. These are very sensitive and can detect problems before the bearing fails.\n\n\nLook for hot and cold wheels which are generally caused by braking equipment failures.\n\nThis type of defect detector uses ultra sensitive infrared cameras called pyrometers. These devices take the temperature of each bearing as it passes by the scanner. This data is then compared to preset \"Alarm limits\". If the whole consist is without problems, the train is passed. If however there is overheating in one or more of the bearings (a hot box), an alarm is given.\n\nThese messages can be by radio to the train crew or might be sent to the dispatch center for handling. The data is also used to detect a trend over several locations that might predict a future failure. If an alarm is sent, the train is stopped, and either the defect is corrected or the offending car is removed for repair. Many derailments are avoided by these devices which is why thousands of them are in use on rail systems across the world.\n\nAs the name suggests, they are able to detect things hanging or dragging under the cars. These consist of a series of plates mounted on a pivot shaft. An object hanging or dragging will contact the plates, moving them and breaking a circuit. This will trigger an alarm and alert the crew. Many are stand-alone detectors, but most often are integrated into bearing temperature scanning locations.\n\nThese are able to measure the height and width of rolling stock, so that cars and loads which won't fit under bridges or through tunnels are stopped. Most often they are optical line of sight devices that trigger an alarm when the beam is broken. They are often seen at a bearing temperature scanning locations, they then report out as part of the detectors regular train inspection report. Mounted on poles or a bridge structure, the optical line is adjusted for a set height and width, it can then send an alarm when something too big to fit the clearance limits ahead passes its view.\n\nIs a simple \"broken wire\" type device that notifies the train crew they are off the track somewhere in their train.\n\n\nA system which monitors the integrity of the pantograph. These are normally based on a vision system which takes pictures of the pantographs and performs an analysis based on computer vision algorithms. A pantograph uses a carbon strip to conduct electricity between the catenary and the pantograph. When these are damaged or worn out, the pantograph can tear down the catenary causing train delays.\n\n\nThese laser and optical scanning devices make images of the flange and tread. These measurements are compared with acceptable dimensions. When worn beyond limits, the wheels are scheduled for replacement. If beyond a safe level the axle set is removed from service.\n\nThese back office systems are predictive, finding equipment problems as they start to develop.\n\n\n"}
{"id": "1838507", "url": "https://en.wikipedia.org/wiki?curid=1838507", "title": "United States President's Commission on CIA Activities within the United States", "text": "United States President's Commission on CIA Activities within the United States\n\nThe United States President's Commission on CIA Activities within the United States was set up under President Gerald Ford in 1975 to investigate the activities of the Central Intelligence Agency and other intelligence agencies within the United States. The commission was led by the Vice President, Nelson Rockefeller, and is sometimes referred to as the Rockefeller Commission.\n\nThe commission was created in response to a December 1974 report in \"The New York Times\" that the CIA had conducted illegal domestic activities, including experiments on U.S. citizens, during the 1960s. The commission issued a single report in 1975, touching upon certain CIA abuses including mail opening and surveillance of domestic dissident groups. It publicized Project MKUltra, a CIA mind control study.\n\nIt also studied issues relating to the John F. Kennedy assassination, specifically the head snap as seen in the Zapruder film (first shown on television in 1975), and the possible presence of E. Howard Hunt and Frank Sturgis in Dallas, Texas.\n\nA larger investigation, the Church Committee, was set up on 27 January 1975 by the U.S. Senate. The Nedzi Committee was created in the U.S. Congress on 19 February 1975. It was replaced by the Pike Committee five months later.\n\nIn July 1975, \"The New York Times\" reported that unnamed staff sources within the Rockefeller Commission said that Sidney Gottlieb commanded the CIA's LSD experimentation program, was personally involved in the experiment that killed researcher Frank Olson, then destroyed the program's records in 1973.\n\n\n\n"}
{"id": "5611262", "url": "https://en.wikipedia.org/wiki?curid=5611262", "title": "Uranium in the environment", "text": "Uranium in the environment\n\nUranium in the environment refers to the science of the sources, environmental behaviour, and effects of uranium on humans and other animals. Uranium is weakly radioactive and remains so because of its long physical half-life (4.468 billion years for uranium-238). The biological half-life (the average time it takes for the human body to eliminate half the amount in the body) for uranium is about 15 days. Normal functioning of the kidney, brain, liver, heart, and numerous other systems can be affected by uranium exposure, because uranium is a toxic metal. The use of depleted uranium (DU) in munitions is controversial because of questions about potential long-term health effects.\n\nUranium is a naturally occurring element found in low levels within all rock, soil, and water. This is the highest-numbered element to be found naturally in significant quantities on earth. According to the United Nations Scientific Committee on the Effects of Atomic Radiation the normal concentration of uranium in soil is 300 μg/kg to 11.7 mg/kg.\n\nIt is considered to be more plentiful than antimony, beryllium, cadmium, gold, mercury, silver, or tungsten and is about as abundant as tin, arsenic or molybdenum. It is found in many minerals including uraninite (most common uranium ore), autunite, uranophane, torbernite, and coffinite. Significant concentrations of uranium occur in some substances such as phosphate rock deposits, and minerals such as lignite, and monazite sands in uranium-rich ores (it is recovered commercially from these sources).\n\nSeawater contains about 3.3 parts per billion of uranium by weight, approximately (3.3 µg/kg) or, 3.3 micrograms per liter of seawater. as uranium(VI) forms soluble carbonate complexes. The extraction of uranium from seawater has been considered as a means of obtaining the element.\n\nThe radiation hazards of uranium mining and milling were not appreciated in the early years, resulting in workers exposed to high levels of radiation. Conventional uranium ore treatment mills create radioactive waste in the form of tailings, which contain uranium, radium, and polonium. Consequently, uranium mining results in \"the unavoidable radioactive contamination of the environment by solid, liquid and gaseous wastes\". Inhalation of radon gas caused sharp increases in lung cancers among underground uranium miners employed in the 1940s and 1950s.\n\nIn the 1940s and 1950s, uranium mill tailings were released with impunity into water sources, and the radium leached from these tailings contaminated thousands of miles of the Colorado River system. Between 1966 and 1971, thousands of homes and commercial buildings in the Colorado Plateau region were \"found to contain anomalously high concentrations of radon, after being built on uranium tailings taken from piles under the authority of the Atomic Energy Commission\".\n\nDepleted uranium (DU) is useful because of its very high density of 19.1 g/cm (68.4% denser than lead). Civilian uses include counterweights in aircraft, radiation shielding in medical radiation therapy and industrial radiography equipment, and containers used to transport radioactive materials. Military uses include defensive armor plating and armor-piercing projectiles.\n\nUranium metal can disperse into the air and water, United Nations Environment Programme (UNEP) study says in part:\n\nStudies of depleted uranium aerosol exposure suggest that uranium combustion product particles would quickly settle out of the air, and thus could not affect populations more than a few kilometres from target areas.\n\nThe U.S. has admitted that there have been over 100 \"friendly fire\" incidents in which members of the U.S. military have been struck by DU munitions, and that an unknown number have been exposed to DU via inhalation of combustion products from burning DU munitions.\n\nIt has been reported that the corrosion of uranium in a silica rich aqueous solution forms both uranium dioxide and uranium trioxide.\n\nIn pure water, schoepite {(UO)O(OH).12(HO)} is formed in the first week and then after four months studtite {(UO)O·4(HO)} was formed.\n\nUranium metal reacts with water to form hydrogen gas, this reaction forms uranium dioxide and 2% to 9% uranium hydride. It is important to note that the rate of corrosion due to water is far greater than that caused by oxygen at temperatures around . At pH values below 2 the corrosion rate at 100 °C goes down greatly, while as pH values go from 7 upwards the corrosion rate declines. Gamma irradiation has little effect on the corrosion rate.\n\nOxygen gas inhibits the corrosion of uranium by water.\n\nSpent uranium dioxide fuel is very insoluble in water, it is likely to release uranium (and fission products) even more slowly than borosilicate glass when in contact with water.\n\nNote that while the vast majority of the uranium is removed by PUREX nuclear reprocessing, a small amount of uranium is left in the raffinate from the first cycle of the PUREX process. In addition because of the decay of the transplutonium minor actinides and the residual plutonium in the waste the concentration of uranium will increase on the waste. This will occur on a time scale of hundreds and thousands of years.\n\nSoluble uranium salts are toxic, though less so than those of other heavy metals such as lead or mercury. The organ which is most affected is the kidney. Soluble uranium salts are readily excreted in the urine, although some accumulation in the kidneys does occur in the case of chronic exposure. The World Health Organization has established a daily \"tolerated intake\" of soluble uranium salts for the general public of 0.5 μg/kg body weight (or 35 μg for a 70 kg adult): exposure at this level is not thought to lead to any significant kidney damage.\n\nThe antidote for uranium in humans is bicarbonate, which is used because uranium (VI) forms complexes with carbonate. An alternative is to use tiron (sodium 4,5-dihydroxybenzene-1,3-disulfonate).\n\nIn 1950, the US Public Health service began a comprehensive study of uranium miners, leading to the first publication of a statistical correlation between cancer and uranium mining, released in 1962. The federal government eventually regulated the standard amount of radon in mines, setting the level at 0.3 WL on January 1, 1969.\n\nOut of 69 69 present and former uranium milling sites in 12 states, 24 have been abandoned, and are the responsibility of the US Department of Energy. Accidental releases from uranium mills include the 1979 Church Rock uranium mill spill in New Mexico, called the largest accident of nuclear-related waste in US history, and the 1986 Sequoyah Corporation Fuels Release in Oklahoma.\n\nIn 1990, Congress passed the Radiation Exposure Compensation Act (RECA), granting reparations for those affected by mining, with amendments passed in 2000 to address criticisms with the original act.\n\nThe use of depleted uranium (DU) in munitions is controversial because of questions about potential long-term health effects. Normal functioning of the kidney, brain, liver, heart, and numerous other systems can be affected by uranium exposure, because uranium is a toxic metal. The aerosol produced during impact and combustion of depleted uranium munitions can potentially contaminate wide areas around the impact sites leading to possible inhalation by human beings. During a three-week period of conflict in 2003 in Iraq, 1,000 to 2,000 tonnes of DU munitions were used.\n\nThe actual acute and chronic toxicity of DU is also a point of medical controversy. Multiple studies using cultured cells and laboratory rodents suggest the possibility of leukemogenic, genetic, reproductive, and neurological effects from chronic exposure.\nA 2005 epidemiology review concluded: \"In aggregate the human epidemiological evidence is consistent with increased risk of birth defects in offspring of persons exposed to DU.\" The World Health Organization, the directing and coordinating authority for health within the United Nations which is responsible for setting health research norms and standards, providing technical support to countries and monitoring and assessing health trends, states that no risk of reproductive, developmental, or carcinogenic effects have been reported in humans due to DU exposure. This report has been criticized by Dr. Keith Baverstock for not including possible long-term effects of DU on human body.\n\nMost scientific studies have found no link between uranium and birth defects, but some claim statistical correlations between soldiers exposed to DU, and those who were not, concerning reproductive abnormalities.\n\nOne study concluded that epidemiological evidence is consistent with an increased risk of birth defects in the offspring of persons exposed to DU. Environmental groups and others have expressed concern about the health effects of depleted uranium, and there is some debate over the matter. Some people have raised concerns about the use of this material, particularly in munitions, because of its mutagenicity, teratogenicity in mice, and neurotoxicity, and its suspected carcinogenic potential. Additional concerns address unexploded DU munitions leeching into groundwater over time.\n\nSeveral sources have attributed the increase in the rate of birth defects in the children of Gulf War veterans and in Iraqis to depleted uranium inhalation exposure, A 2001 study of 15,000 February 1991 U.S. Gulf War combat veterans and 15,000 control veterans found that the Gulf War veterans were 1.8 (fathers) to 2.8 (mothers) times more likely to have children with birth defects.\nIn a study of UK troops, \"Overall, the risk of any malformation among pregnancies reported by men was 50% higher in Gulf War Veterans (GWV) compared with Non-GWVs\". The conclusion of the study stated \"We found no evidence for a link between paternal deployment to the Gulf war and increased risk of stillbirth, chromosomal malformations, or congenital syndromes. Associations were found between fathers' service in the Gulf war and increased risk of miscarriage and less well-defined malformations, but these findings need to be interpreted with caution as such outcomes are susceptible to recall bias. The finding of a possible relationship with renal anomalies requires further investigation. There was no evidence of an association between risk of miscarriage and mothers' service in the gulf.\"\n\nIt has been reported that uranium has caused reproductive defects, and other health problems in rodents, frogs and other animals. Uranium was shown to have cytotoxic, genotoxic and carcinogenic effects in animal studies. It has been shown in rodents and frogs that water-soluble forms of uranium are teratogenic.\n\nIt has been shown that bacteria, and proteobacteria such as Geobacter and \"Burkholderia fungorum\" \"(strain Rifle)\", can reduce and fix uranium in soil and groundwater. These bacteria change soluble U(VI) into the highly insoluble complex-forming U(IV) ion, hence stopping chemical leaching.\n\nIt has been suggested that it is possible to form a \"reactive barrier\" by adding something to the soil which will cause the uranium to become fixed. One method of doing this is to use a mineral (apatite) while a second method is to add a food substance such as acetate to the soil. This will enable bacteria to reduce the uranium (VI) to uranium (IV) which is much less soluble. In peat-like soils the uranium will tend to bind to the humic acids, this tends to fix the uranium in the soil.\n"}
{"id": "1462927", "url": "https://en.wikipedia.org/wiki?curid=1462927", "title": "User expectations", "text": "User expectations\n\nUser expectations refers to the consistency that users expect from products. Interaction design is very concerned with this topic. For example, our user expectations for traffic behavior is one of the more consistent ones because it is governed by traffic laws that are enforced. Software, or even worse small consumer electronics, on the other hand, can have widely varying degrees of consistency.\n\nA good rule of thumb or \"design principle\" to use in interaction design is to follow the \"Principle of least astonishment\". In the case of interactive software applications, for example, users form expectations based on their experience with similar kinds of software. Effective software design aims to conform with prevailing norms for aspects of behavior such as the software's user interface and its responsiveness.\n\nA commonly noted sign of poor usability and human factors due to the violation of user expectations is when signs are needed for common things like doors. This was made famous by Donald A. Norman in his seminal book The Psychology of Everyday Things. There is also a website devoted to such Bad Designs .\n\n\n"}
{"id": "22982961", "url": "https://en.wikipedia.org/wiki?curid=22982961", "title": "Ward-Beck Systems", "text": "Ward-Beck Systems\n\nWard-Beck Systems \"commonly referred to as Ward-Beck or simply WBS\", is a Canadian manufacturer of broadcast audio and video equipment. It was founded in a garage in April 1967 by Ron W. Ward and director of engineering, Rodger K. Beck.\n\nThe first two digits of the module or consoles serial number dictates the year of manufacturing.\n\n\n\n\nThe Ward-Beck Systems Preservation Society was founded in 2005 by Tony Kuzub. The goal and objective of the WBSPS is to keep vintage Ward-Beck equipment running and working by supplying a database of documentation, knowledge, and support. There are only a handful of consoles still in existence and use, and the WBSPS is dedicated to keeping these consoles in use and maintained. Many manuals have been scanned in their entirety and posted for all to learn from. The WBSPS was given permission by Eugene Johnson of Ward-Beck to publish information regarding vintage WBS equipment. The WBSPS is not directly or in-directly associated with Ward-Beck Systems Incorporated. A Letter of permission from Ward-Beck Systems\n\n"}
{"id": "58971694", "url": "https://en.wikipedia.org/wiki?curid=58971694", "title": "Wells Print Shop", "text": "Wells Print Shop\n\nThe Wells Print Shop was located at 27 Cuna Street in St. Augustine, Florida. It operated as part of the Historic St. Augustine Preservation Board's 18th century museum village, San Agustín Antiguo, demonstrating the colonial printmaking process.<mapframe latitude=\"29.8949\" longitude=\"-81.3152\" zoom=\"15\" width=\"250\" height=\"200\" align=\"right\">\n</mapframe>\n\nWilliam Charles Wells was born in 1757 in Charleston, South Carolina, the son of a Tory printer, publisher, and bookseller. He was formally trained as a doctor, receiving a degree in medicine from the University of Edinburgh in 1780. When the British left Charleston in 1782, William and his brother John moved to St. Augustine where they established a print shop. In St. Augustine the Wells brothers are most well known for publishing the \"East Florida Gazette\", the first newspaper in Florida. After the 1783 Treaty of Paris returned Florida to the Spanish, William Wells went to England and practiced medicine for the rest of his life. He died in London in 1817.\n\nThe Wells brothers' publication began on February 1, 1783 and lasted a little over one year, with the last issue published on March 22, 1784. London's Public Record Office only contains three issues of the newspaper, those from March 1, May 3, and May 17, 1783. The St. Augustine Historical Society resurrected the name \"East Florida Gazette\" with its newsletter, which is indexed on the Historical Society Research Library's online catalog.\n\nThe Historic St. Augustine Preservation Board reconstructed the Wells Print Shop in 1968 on Cuna Street. The original Wells printing press owned by William and John was located on Treasury Lane. The all-wooden shop was constructed using board-and-batten building method, which the British settlers in St. Augustine preferred for its quick construction and ease of repair. The building is 220 square feet. \n\nInside the print shop was a replica of the type of printing press used towards the end of the 18th century. The press was operated daily to demonstrate the printing process to visitors, but the Preservation Board also used it to make reproductions of historic St. Augustine maps and replicas of the \"East Florida Gazette,\" which were sold as souvenirs. \n\nToday the site once occupied by the Wells Print Shop operates as a jewelry store.\n \n"}
