{"id": "30595821", "url": "https://en.wikipedia.org/wiki?curid=30595821", "title": "AN/AAR-47 Missile Approach Warning System", "text": "AN/AAR-47 Missile Approach Warning System\n\nThe AN/AAR-47 Missile Warning System is a Missile Approach Warning system used on slow moving aircraft such as helicopters and military transport aircraft to notify the pilot of threats and to trigger the aircraft's countermeasures systems. Its main users are the U.S. Army, Navy and Air Force, but is also operated by other countries. Originally developed by Loral (now part of BAE Systems), and later dual-source procured from Loral Infrared & Imaging Systems and Honeywell Electro-Optics Div., both in Lexington, MA, it has been a product of Alliant Techsystems (ATK) since 2002. 100 to 300 sets have been manufactured per annum.\n\nThe AN/AAR-47 passively detects missiles by their Ultraviolet signature, and uses algorithms to differentiate between incoming missiles and false alarms. Newer versions also have laser warning sensors and are capable of detecting a wider range of threats. After processing the nature of the threat, the system gives the pilot an audio and visual warning, and indicates the direction of the incoming threat. It also sends a signal to the aircraft's infrared countermeasures system, which can then for example deploy flares.\n\nThe system's algorithms include looking for temporal variations in a signal's strength, such as the brightening of an incoming missile. It also evaluates the spectral bandpass of the threat to reduce false alarms and has software for detecting events, such as the launch of a surface-to-air missile.\n\nThe AN/AAR-47 is a line of missile warning systems by Loral and ATK Alliant Techsystems. The development of the original AN/AAR-47(V)1 began in 1983 by Loral. ATK became a second production source in the mid 90s and eventually became the prime contractor. In 1998 ATK began production of the improved AN/AAR-47(V)2 version, which added laser warning functionality. As of 2005, over 5000 of these sets have been manufactured. In 2006, production of the AN/AAR-47A(V)2 began, also developed by ATK. It has further improved missile and laser warning capabilities.\n\nA further developed model, AAR-47B(V)2, reached initial operating capability in 2008. It adds Hostile Fire Indication (HFI), which is the ability to detect incoming rocket propelled grenades and tracer ammunition in addition to an overall improvement in missile detection. The U.S. Navy placed orders for over 1600 in 2009.\n\nThe AAR-47 missile warning system consists of 4 Optical Sensor Converters (OSC), a Computer Processor and a Control Indicator. The system is relatively light at a total weight of approximately 32 pounds.\n\nThere is one optical sensor converter for each side of the aircraft. They have an infrared camera for detecting incoming missiles. The Optical modules since version AAR-47(V)2 include a laser warning sensor, and versions since AAR-47A(V)2 further incorporate an ultraviolet sensor for improved dynamic blanking laser warning detection.\n\nThe computer processor evaluates the data from the OSC:s and analyzes whether a detected event is an incoming missile. If a threat is detected, it sends a signal to the control indicator which informs the crew, and the aircraft's infrared countermeasures system.\n\n\n\n"}
{"id": "17219909", "url": "https://en.wikipedia.org/wiki?curid=17219909", "title": "ATM Controller", "text": "ATM Controller\n\nAn ATM controller (ATMC) is a system used in financial institutions to route financial transactions between ATMs, core banking systems and other banks. An ATMC is sometimes referred to as an \"EFTPOS Switch.\" An ATM controller is key infrastructure in an interbank network.\n\nA message may enter an ATMC from an ATM, another ATMC or a third party. When receiving a message, the ATMC will examine the message, validate the PIN block if present, and then route the message according to the leading digits of the account number referenced.\n\nThis routing may be to a core banking system to check the available balances and to authorise the transaction, or to another bank's ATMC. For example, if a customer of Bank A used their card at an ATM belonging to Bank B, the message would be forwarded to Bank B's ATMC. The ATMC would examine the message, and based upon the account number determine that the appropriate ATMC to contact would be Bank A. It would then forward the message to Bank A's ATMC for authorisation.\n\nAn important aspect of an ATMC system is its ability to perform stand-in processing when core banking systems are unavailable. This allows a bank's ATMs to operate (usually with reduced limits) during periods of outage or maintenance on core banking systems. ATMCs make use of a SAF (store and forward) queue to ensure transactions are not lost.\n\nAn ATMC will usually have at least one attached hardware security module to manage the keys relating to PIN validation and encryption of connections.\n\n"}
{"id": "7888008", "url": "https://en.wikipedia.org/wiki?curid=7888008", "title": "Activstudio", "text": "Activstudio\n\nActivstudio is a software application designed specifically for teachers and presenters who use an Activboard Interactive Whiteboard, Promethean's interactive whiteboard. Activstudio and the derivative product Activprimary were designed and implemented by Nigel Pearce together with a software development team at Promethean (Blackburn, England).\n\n\nActivstudio provides a suite of ‘interactive whiteboard centric’ tools. The main feature is to \nallow the user to prepare and present files known as flipcharts, an electronic document that can contain a combination of vector and raster object data including lines, shapes, rich text, images, video, FLASH media and other third party document types.\n\nThe use of the term 'flipchart' in describing these electronic documents derives from the similarity in the way a physical flipchart is commonly used, effectively starting with a set of 'blank canvas' pages upon which to present. The very early versions of Activstudio had a much reduced functionality set, comprising simple pens, highlighters and page turning and hence the analogy with 'flipcharts' was born and has remained.\n\nThe flipchart also captures and stores any notes (termed ‘annotations’) which may be written on the surface of the interactive whiteboard using an electronic pen. Additionally the program allows the user to write (or ‘annotate’) directly over other applications, WEB Browser content and live video clips.\n\nOn the Windows platform, Activstudio also comprises the program Activmarker. This product allows for pen annotations to be written over Microsoft Word documents, Excel spreadsheets and Powerpoint presentations. The subsequent markup is then automatically committed into the content of the Office document for storage.\n\nActivstudio includes integrated tools to centrally manage additional interactive inputs including remote annotation and telepointing using wireless handheld Activslates and Activote Student Response Devices.\n\nActivstudio incorporates many of today’s standard ‘whiteboard centric’ concepts, including the spotlight, revealer and zoom functions, a sound recorder, resource library and the interactive protractor, ruler and compass tools. A large selection of editable Restrictors, Properties and Actions that can be edited on screen through simple clicks enable the creation of interactive multimedia.\n\nActivstudio version 3 added point and click object authoring capabilities, gesture recognition, an optional Windows style interface and integrated links to Promethean Planet via a digital dashboard.\n\nActivprimary is virtually identical in functionality to Activstudio but is aimed at the younger learner environment, differing only in its user interface.\n\nOther derivatives of the applications:\n\n\nActivstudio and Activprimary are both available in 32 languages and for most versions of the Windows and MAC operating systems.\n\nAll authored flipcharts will work in either program and there are many freely available flipcharts uploaded to the internet. A quick search should provide a good starting point for any teacher just starting out with an Activboard.\n\nWhilst Activstudio and Activprimary were originally designed for use on the Activboard Interactive Whiteboard, the programs can also be used with the standard computer mouse for preparation purposes and are also suitable for running with any other type of pen input device or on any other make of interactive whiteboard.\n\nActivStudio and ActivPrimary use a proprietary 'flipchart' format ending in the file extension '.flp'. This utilizes PKZip compression technology to reduce flipchart file sizes. This technique is also employed in other applications, such as the Microsoft PowerPoint pptx file format.\n\nIn 2009, Promethean released ActivInspire which can be used for free. This software package now uses the new 'flipchart' file format ending in the file extension '.flipchart'. ActivInspire can open the older '.flp' format but it can only save in the new '.flipchart' format. This results in ActivInspire's files from no longer being backwards compatible. In other words, Activstudio (and Activprimary) cannot open the flipcharts created by ActivInspire.\n\nThe ActivInspire 'flipchart' file uses a proprietary compression format. Accessing these files in a hex-editor you will find the file header information for this format is 'Bamboo'. One of the consequences of using a proprietary file format is that it makes it difficult for third-party software to\nuse\npen flipcharts.\n"}
{"id": "294387", "url": "https://en.wikipedia.org/wiki?curid=294387", "title": "Airlock", "text": "Airlock\n\nAn airlock is a device which permits the passage of people and objects between a pressure vessel and its surroundings while minimizing the change of pressure in the vessel and loss of air from it. The lock consists of a small chamber with two airtight doors in series which do not open simultaneously.\n\nAn airlock may be used for passage between environments of different gases rather than different pressures, to minimize or prevent the gases from mixing.\n\nAn airlock may also be used underwater to allow passage between an air environment in a pressure vessel and the water environment outside, in which case the airlock can contain air or water. This is called a floodable airlock or an underwater airlock, and is used to prevent water from entering a submersible vessel or an underwater habitat.\n\nBefore opening either door, the air pressure of the airlock—the space between the doors—is equalized with that of the environment beyond the next door to open. This is analogous to a waterway lock: a section of waterway with two watertight gates, in which the water level is varied to match the water level on either side.\n\nA gradual pressure transition minimizes air temperature fluctuations (see Boyle's law), which helps reduce fogging and condensation, decreases stresses on air seals, and allows safe verification of pressure suit and space suit operation.\n\nWhere a person who is not in a pressure suit moves between environments of greatly different pressures, an airlock changes the pressure slowly to help with internal air cavity equalization and to prevent decompression sickness. This is critical in scuba diving, and a diver may have to wait in an airlock for some hours, in accordance with decompression tables.\n\nAirlocks are used in\n\n\nA four-door airlock (with three interior chambers) was proposed by science fiction writer H. Beam Piper in his novel \"Uller Uprising\". The atmosphere inside the fictional structure was human-breathable, while the outside atmosphere was highly toxic. Only one door of the airlock opened at a time, and the middle chamber of the three would always contain a vacuum to minimize traces of the exterior atmosphere reaching the habitat.\n\nIn the 1979 spy film \"Moonraker\", James Bond dispatches the villain Hugo Drax aboard his own space station by first shooting him in the chest with a cyanide dart, then pushing him out an airlock into space (\"Take a giant leap for mankind.\").\n\nIn the 2014 science-fiction film \"Interstellar\", Airlocks are featured several times, beginning with the initial journey into space. In the film's climax, Dr. Mann attempts to maroon Cooper and Brand by stealing their spacecraft \"The Endurance\". When Mann tries docking his own ship onto The Endurance, he does so imperfectly. Despite this, he decides to board The Endurance from his own spacecraft regardless of the circumstances. In the midst of an arrogant monologue, he opens the airlock door. Due to the pressure change caused by the vacuum of space, the airlock explodes, killing Mann and critically damaging The Endurance. As a result of this, Cooper and Brand are forced to pursue the damaged Endurance in their own ship, as it is their only chance of survival. Due to the explosion, the circular ship is spinning out of control toward the stratosphere of a nearby planet. In a last ditch effort to save the mission, Cooper attempts to match the RPM of The Endurance with that of his own spacecraft. Due to the intense g-forces being subjected to Cooper and Brand, they must rely on their versatile AI known as TARS to accurately dock the ship with the Endurance while spinning. \n\nIn the 2015 science-fiction film \"The Martian\", airlocks are used in the \"Hab\", a base of operations on Mars, as well as on space-faring vessels. Mark Watney, an astronaut stranded on Mars improvises a farm in the \"Hab\" where he is living. Subsequently, the failure of an airlock and the depressurization of the environment kills the potato crops he was growing. Additionally, the crew of the \"Hermes\" vessel tasked with rescuing Watney deliberately breached an airlock to produce reverse thrust, in order to slow their vessel down enough to intercept Watney's capsule.\n\nIn \"Star Trek\", \"Star Wars\" and some other genres of fiction, conventional airlocks may be replaced by forcefields which hold in air while allowing solid matter like spacecraft to pass through. Airlocks of this type usually have pressure doors as a backup.\n\nAirlocks are commonly used in science fiction as a form of execution, often referred to as \"spacing\" or \"airlocking\". \"Battlestar Galactica\", \"Babylon 5\", and the \"Honorverse\" books both make frequent use or reference to it.\n\nAirlocks are also used in the following video games:\n\n\n"}
{"id": "27697009", "url": "https://en.wikipedia.org/wiki?curid=27697009", "title": "Application programming interface", "text": "Application programming interface\n\nIn computer programming, an application programming interface (API) is a set of subroutine definitions, communication protocols, and tools for building software. In general terms, it is a set of clearly defined methods of communication among various components. A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer.\n\nAn API may be for a web-based system, operating system, database system, computer hardware, or software library.\n\nAn API specification can take many forms, but often includes specifications for routines, data structures, object classes, variables, or remote calls. POSIX, Windows API and ASPI are examples of different forms of APIs. Documentation for the API usually is provided to facilitate usage and implementation.\n\nJust as a graphical user interface (GUI) makes it easier for people to use programs, application programming interfaces make it easier for developers to use certain technologies in building applications. An API simplifies programming by abstracting the underlying implementation and only exposing objects or actions the developer needs. While a graphical interface for an email client might provide a user with a button that performs all the steps for fetching and highlighting new emails, an API for file input/output might give the developer a function that copies a file from one location to another without requiring that the developer understand the file system operations occurring behind the scenes.\n\nThe modern API management helps the enterprises expose individual components in well-documented services that the internal developers and partners can use to rapidly iterate new features.\n\nAn API usually is related to a software library. The API describes and prescribes the \"expected behavior\" (a specification) while the library is an \"actual implementation\" of this set of rules.\n\nA single API can have multiple implementations (or none, being abstract) in the form of different libraries that share the same programming interface.\n\nThe separation of the API from its implementation can allow programs written in one language to use a library written in another. For example, because Scala and Java compile to compatible bytecode, Scala developers can take advantage of any Java API.\n\nAPI use can vary depending on the type of programming language involved.\nAn API for a procedural language such as Lua could consist primarily of basic routines to execute code, manipulate data or handle errors while an API for an object-oriented language, such as Java, would provide a specification of classes and its class methods.\n\nLanguage bindings are also APIs. By mapping the features and capabilities of one language to an interface implemented in another language, a language binding allows a library or service written in one language to be used when developing in another language. Tools such as SWIG and F2PY, a Fortran-to-Python interface generator, facilitate the creation of such interfaces.\n\nAn API can also be related to a software framework: a framework can be based on several libraries implementing several APIs, but unlike the normal use of an API, the access to the behavior built into the framework is mediated by extending its content with new classes plugged into the framework itself.\n\nMoreover, the overall program flow of control can be out of the control of the caller and in the hands of the framework by inversion of control or a similar mechanism.\n\nAn API can specify the interface between an application and the operating system. POSIX, for example, specifies a set of common APIs that aim to enable an application written for a POSIX conformant operating system to be compiled for another POSIX conformant operating system.\n\nLinux and Berkeley Software Distribution are examples of operating systems that implement the POSIX APIs.\n\nMicrosoft has shown a strong commitment to a backward-compatible API, particularly within its Windows API (Win32) library, so older applications may run on newer versions of Windows using an executable-specific setting called \"Compatibility Mode\".\n\nAn API differs from an application binary interface (ABI) in that an API is source code based while an ABI is binary based. For instance, POSIX provides APIs while the Linux Standard Base provides an ABI.\n\nRemote APIs allow developers to manipulate remote resources through protocols, specific standards for communication that allow different technologies to work together, regardless of language or platform.\nFor example, the Java Database Connectivity API allows developers to query many different types of databases with the same set of functions, while the Java remote method invocation API uses the Java Remote Method Protocol to allow invocation of functions that operate remotely, but appear local to the developer.\n\nTherefore, remote APIs are useful in maintaining the object abstraction in object-oriented programming; a method call, executed locally on a proxy object, invokes the corresponding method on the remote object, using the remoting protocol, and acquires the result to be used locally as return value.\n\nA modification on the proxy object also will result in a corresponding modification on the remote object.\n\nWeb APIs are the defined interfaces through which interactions happen between an enterprise and applications that use its assets, which also is a Service Level Agreement (SLA) to specify the functional provider and expose the service path or URL for its API users. An API approach is an architectural approach that revolves around providing a program interface to a set of services to different applications serving different types of consumers.\n\nWhen used in the context of web development, an API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format. An example might be a shipping company API that can be added to an eCommerce-focused website to facilitate ordering shipping services and automatically include current shipping rates, without the site developer having to enter the shipper's rate table into a web database. While \"web API\" historically virtually has been synonymous for web service, the recent trend (so-called Web 2.0) has been moving away from Simple Object Access Protocol (SOAP) based web services and service-oriented architecture (SOA) towards more direct representational state transfer (REST) style web resources and resource-oriented architecture (ROA). Part of this trend is related to the Semantic Web movement toward Resource Description Framework (RDF), a concept to promote web-based ontology engineering technologies. Web APIs allow the combination of multiple APIs into new applications known as mashups.\nIn the social media space, web APIs have allowed web communities to facilitate sharing content and data between communities and applications. In this way, content that is created in one place dynamically can be posted and updated to multiple locations on the web. For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data.\n\nThe design of an API has significant impact on its usage. The principle of information hiding describes the role of programming interfaces as enabling modular programming by hiding the implementation details of the modules so that users of modules need not understand the complexities inside the modules. Thus, the design of an API attempts to provide only the tools a user would expect. The design of programming interfaces represents an important part of software architecture, the organization of a complex piece of software.\n\nSeveral authors have created recommendations for how to design APIs, such as Joshua Bloch, Kin Lane, and Michi Henning.\n\nAPIs are one of the more common ways technology companies integrate with each other. Those that provide and use APIs are considered as being members of a business ecosystem.\n\nThe main policies for releasing an API are:\n\n\nAn important factor when an API becomes public is its \"interface stability\". Changes by a developer to a part of it—for example adding new parameters to a function call—could break compatibility with clients that depend on that API.\n\nWhen parts of a publicly presented API are subject to change and thus not stable, such parts of a particular API should be documented explicitly as \"unstable\". For example, in the Google Guava library, the parts that are considered unstable, and that might change in the near future, are marked with the Java annotation codice_1.\n\nA public API can sometimes declare parts of itself as \"deprecated\" or rescinded. This usually means that part of the API should be considered a candidate for being removed, or modified in a backward incompatible way. Therefore, these changes allows developers to transition away from parts of the API that will be removed or not supported in the future.\n\nFor any library with a significant user base, as soon as an element becomes part of the public API, it starts being used in diverse ways. In this perspective, client code sometimes contain original or opportunistic usages that were never envisioned by the API designers.\n\nAPI documentation describes what services an API offers and how to use those services, aiming to cover everything a client would need to know for practical purposes.\n\nDocumentation is crucial for the development and maintenance of applications using the API.\nAPI documentation is traditionally found in documentation files but can also be found in social media such as blogs, forums, and Q&A websites.\n\nTraditional documentation files are often presented via a documentation system, such as Javadoc or Pydoc, that has a consistent appearance and structure.\nHowever, the types of content included in the documentation differs from API to API.\n\nIn the interest of clarity, API documentation may include a description of classes and methods in the API as well as \"typical usage scenarios, code snippets, design rationales, performance discussions, and contracts\", but implementation details of the API services themselves are usually omitted.\n\nRestrictions and limitations on how the API can be used are also covered by the documentation. For instance, documentation for an API function could note that its parameters cannot be null, that the function itself is not thread safe, or that a decrement and cancel protocol averts self-trading.\nBecause API documentation tends to be comprehensive, it is a challenge for writers to keep the documentation updated and for users to read it carefully, potentially yielding bugs.\n\nAPI documentation can be enriched with metadata information like Java annotations. This metadata can be used by the compiler, tools, and by the \"run-time\" environment to implement custom behaviors or custom handling.\n\nIt is possible to generate API documentation in data-driven manner. By observing a large number of programs that use a given API, it is possible to infer the typical usages, as well the required contracts and directives. Then, templates can be used to generate natural language from the mined data.\n\nIn 2010, Oracle Corporation sued Google for having distributed a new implementation of Java embedded in the Android operating system. Google had not acquired any permission to reproduce the Java API, although permission had been given to the similar OpenJDK project. Judge William Alsup ruled in the \"Oracle v. Google\" case that APIs cannot be copyrighted in the U.S, and that a victory for Oracle would have widely expanded copyright protection and allowed the copyrighting of simple software commands:\n\nTo accept Oracle's claim would be to allow anyone to copyright one version of code to carry out a system of commands and thereby bar all others from writing its own different versions to carry out all or part of the same commands.\n\nIn 2014, however, Alsup's ruling was overturned on appeal, though the question of whether such use of APIs constitutes fair use was left unresolved.\n\nIn 2016, following a two-week trial, a jury determined that Google's reimplementation of the Java API constituted fair use, but Oracle vowed to appeal the decision. Oracle won on its appeal, with the Court of Appeals for the Federal Circuit ruling that Google's use of the APIs did not qualify for fair use.\n\n"}
{"id": "37850", "url": "https://en.wikipedia.org/wiki?curid=37850", "title": "Beam-powered propulsion", "text": "Beam-powered propulsion\n\nBeam-powered propulsion, also known as directed energy propulsion, is a class of aircraft or spacecraft propulsion that uses energy beamed to the spacecraft from a remote power plant to provide energy. The beam is typically either a microwave or a laser beam and it is either pulsed or continuous. A continuous beam lends itself to thermal rockets, photonic thrusters and light sails, whereas a pulsed beam lends itself to ablative thrusters and pulse detonation engines.\n\nThe rule of thumb that is usually quoted is that it takes a megawatt of power beamed to a vehicle per kg of payload while it is being accelerated to permit it to reach low earth orbit.\n\nOther than launching to orbit, applications for moving around the world quickly have also been proposed.\n\nRockets are momentum machines; they use mass ejected from the rocket to provide momentum to the rocket. Momentum is the product of mass and velocity, so rockets generally attempt to put as much velocity into their working mass as possible, thereby minimizing the amount of working mass that is needed. In order to accelerate the working mass, energy is required. In a conventional rocket, the fuel is chemically combined to provide the energy, and the resulting fuel products, the ash or exhaust, are used as the working mass.\n\nThere is no particular reason why the same fuel has to be used for both energy and momentum. In the jet engine, for instance, the fuel is used only to produce energy, the working mass is provided from the air that the jet aircraft flies through. In modern jet engines, the amount of air propelled is much greater than the amount of air used for energy. This is not a solution for the rocket, however, as they quickly climb to altitudes where the air is too thin to be useful as a source of working mass.\n\nRockets can, however, carry their working mass and use some other source of energy. The problem is finding an energy source with a power-to-weight ratio that competes with chemical fuels. Small nuclear reactors can compete in this regard, and considerable work on nuclear thermal propulsion was carried out in the 1960s, but environmental concerns and rising costs led to the ending of most of these programs.\n\nA further improvement can be made by removing the energy creation from the spacecraft. If the nuclear reactor is left on the ground and its energy transmitted to the spacecraft, the weight of the reactor is removed as well. The issue then is to get the energy into the spacecraft. This is the idea behind beamed power.\n\nWith beamed propulsion one can leave the power-source stationary on the ground, and directly (or via a heat exchanger) heat propellant on the spacecraft with a maser or a laser beam from a fixed installation. This permits the spacecraft to leave its power-source at home, saving significant amounts of mass, greatly improving performance.\n\nSince a laser can heat propellant to extremely high temperatures, this potentially greatly improves the efficiency of a rocket, as exhaust velocity is proportional to the square root of the temperature. Normal chemical rockets have an exhaust speed limited by the fixed amount of energy in the propellants, but beamed propulsion systems have no particular theoretical limit (although in practice there are temperature limits).\n\nIn microwave thermal propulsion, an external microwave beam is used to heat a refractory heat exchanger to >1,500 K, in turn heating a propellant such as hydrogen, methane or ammonia. This improves the specific impulse and thrust/weight ratio of the propulsion system relative to conventional rocket propulsion. For example, hydrogen can provide a specific impulse of 700–900 seconds and a thrust/weight ratio of 50-150.\n\nA variation, developed by brothers James Benford and Gregory Benford, is to use thermal desorption of propellant trapped in the material of a very large microwave sail. This produces a very high acceleration compared to microwave pushed sails alone.\n\nSome proposed spacecraft propulsion mechanisms use power in the form of electricity. Usually these schemes assume either solar panels, or an on-board reactor. However, both power sources are heavy.\n\nBeamed propulsion in the form of laser can be used to send power to a photovoltaic panel, for \"Laser electric propulsion.\" In this system, careful design of the panels is necessary as the extra power tends to cause a fall-off of the conversion efficiency due to heating effects.\n\nA microwave beam could be used to send power to a rectenna, for \"microwave electric propulsion\". Microwave broadcast power has been practically demonstrated several times (e.g. Goldstone, California in 1974), rectennas are potentially lightweight and can handle high power at high conversion efficiency. However, rectennas tend to need to be very large for a significant amount of power to be captured.\n\nA beam could also be used to provide impulse by directly \"pushing\" on the sail.\n\nOne example of this would be using a solar sail to reflect a laser beam. This concept, called a \"laser-pushed lightsail,\" was initially proposed by Marx but first analyzed in detail, and elaborated on, by physicist Robert L. Forward in 1989 as a method of Interstellar travel that would avoid extremely high mass ratios by not carrying fuel. Further analysis of the concept was done by Landis, Mallove and Matloff, Andrews and others.\n\nIn a later paper, Forward proposed pushing a sail with a microwave beam. This has the advantage that the sail need not be a continuous surface. Forward tagged his proposal for an ultralight sail \"Starwisp\". A later analysis by Landis suggested that the Starwisp concept as originally proposed by Forward would not work, but variations on the proposal might be implemented.\n\nThe beam has to have a large diameter so that only a small portion of the beam misses the sail due to diffraction and the laser or microwave antenna has to have a good pointing stability so that the craft can tilt its sails fast enough to follow the center of the beam. This gets more important when going from interplanetary travel to interstellar travel, and when going from a fly-by mission, to a landing mission, to a return mission. The laser or the microwave sender would probably be a large phased array of small devices, which get their energy directly from solar radiation. The size of the array negates the need for a lens or mirror.\n\nAnother beam-pushed concept would be to use a magnetic sail or MMPP sail to divert a beam of charged particles from a particle accelerator or plasma jet. Landis proposed a particle beam pushed sail in 1989, and analyzed in more detail in a 2004 paper. Jordin Kare has proposed a variant to this whereby a \"beam\" of small laser accelerated light sails would transfer momentum to a magsail vehicle.\n\nAnother beam-pushed concept uses pellets or projectiles of ordinary matter. A stream of pellets from a stationary mass-driver is \"reflected\" by the spacecraft, cf. mass driver. The spacecraft neither needs energy nor reaction mass for propulsion of its own.\n\nA lightcraft is a vehicle currently under development that uses an external pulsed source of laser or maser energy to provide power for producing thrust.\n\nThe laser shines on a parabolic reflector on the underside of the vehicle that concentrates the light to produce a region of extremely high temperature. The air in this region is heated and expands violently, producing thrust with each pulse of laser light. In space, a lightcraft would need to provide this gas itself from onboard tanks or from an ablative solid. By leaving the vehicle's power source on the ground and by using ambient atmosphere as reaction mass for much of its ascent, a lightcraft would be capable of delivering a very large percentage of its launch mass to orbit. It could also potentially be very cheap to manufacture.\n\nEarly in the morning of 2 October 2000 at the High Energy Laser Systems Test Facility (HELSTF), Lightcraft Technologies, Inc. (LTI) with the help of Franklin B. Mead of the U.S. Air Force Research Laboratory and Leik Myrabo set a new world's altitude record of 233 feet (71 m) for its 4.8 inch (12.2 cm) diameter, , laser-boosted rocket in a flight lasting 12.7 seconds. Although much of the 8:35 am flight was spent hovering at 230+ feet, the Lightcraft earned a world record for the longest ever laser-powered free flight and the greatest \"air time\" (i.e., launch-to-landing/recovery) from a light-propelled object. This is comparable to Robert Goddard's first test flight of his rocket design. Increasing the laser power to 100 kilowatts will enable flights up to a 30-kilometer altitude. Their goal is to accelerate a one-kilogram microsatellite into low Earth orbit using a custom-built, one megawatt ground-based laser. Such a system would use just about 20 dollars' worth of electricity, placing launch costs per kilogram to many times less than current launch costs (which are measured in thousands of dollars).\n\nMyrabo's \"lightcraft\" design is a reflective funnel-shaped craft that channels heat from the laser, towards the center, using a reflective parabolic surface causing the laser to literally explode the air underneath it, generating lift. Reflective surfaces in the craft focus the beam into a ring, where it heats air to a temperature nearly five times hotter than the surface of the sun, causing the air to expand explosively for thrust.\n\nA laser thermal rocket is a thermal rocket in which the propellant is heated by energy provided by an external laser beam. \nIn 1992, the late Jordin Kare proposed a simpler, nearer term concept which has a rocket containing liquid hydrogen. The propellant is heated in a heat exchanger that the laser beam shines on before leaving the vehicle via a conventional nozzle. This concept can use continuous beam lasers, and the semiconductor lasers are now cost effective for this application.\n\nIn 2002, Kevin L.G. Parkin proposed a similar system using microwaves. In May 2012, the DARPA/NASA Millimeter-wave Thermal Launch System (MTLS) Project began the first steps toward implementing this idea. The MTLS Project was the first to demonstrate a millimeter-wave absorbent refractory heat exchanger, subsequently integrating it into the propulsion system of a small rocket to produce the first millimeter-wave thermal rocket. Simultaneously, it developed the first high power cooperative target millimeter-wave beam director and used it to attempt the first millimeter-wave thermal rocket launch. Several launches were attempted but problems with the beam director could not be resolved before funding ran out in March 2014.\n\nMotivation to develop beam-powered propulsion systems comes from the economic advantages that would be gained as a result of improved propulsion performance. In the case of beam-powered launch vehicles, better propulsion performance enables some combination of increased payload fraction, increased structural margins and fewer stages. JASON's 1977 study of laser propulsion, authored by Freeman Dyson, succinctly articulates the promise of beam-powered launch: \"Laser propulsion as an idea that may produce a revolution in space technology. A single laser facility on the ground can in theory launch single-stage vehicles into low or high earth orbit. The payload can be 20% or 30% of the vehicle take-off weight. It is far more economical in the use of mass and energy than chemical propulsion, and it is far more flexible in putting identical vehicles into a variety of orbits.\"This promise was quantified in a 1978 Lockheed Study conducted for NASA:\"The results of the study showed that, with advanced technology, laser rocket system with either a space- or ground-based laser transmitter could reduce the national budget allocated to space transportation by 10 to 345 billion dollars over a 10-year life cycle when compared to advanced chemical propulsion systems (LO-LH) of equal capability.\"\n\nThe 1970s-era studies and others since have cited beam director cost as a possible impediment to beam-powered launch systems. A recent cost-benefit analysis estimates that microwave (or laser) thermal rockets would be economical once beam director cost falls below 20 $/Watt. The current cost of suitable lasers is <100 $/Watt and the current cost of suitable microwave sources is <$5/Watt. Mass production has lowered the production cost of microwave oven magnetrons to <0.01 $/Watt and some medical lasers to <10 $/Watt, though these are thought to be unsuitable for use in beam directors.\n\nIn 1964 William C. Brown demonstrated a miniature helicopter equipped with a combination antenna and rectifier device called a rectenna. The rectenna converted microwave power into electricity, allowing the helicopter to fly.\n\nIn 2002 a Japanese group propelled a tiny aluminium airplane by using a laser to vaporize a water droplet clinging to it, and in 2003 NASA researchers flew an 11-ounce (312 g) model airplane with a propeller powered with solar panels illuminated by a laser. It is possible that such beam-powered propulsion could be useful for long-duration high altitude unmanned aircraft or balloons, perhaps designed to serve – like satellites do today – as communication relays, science platforms, or surveillance platforms.\n\nA \"laser broom\" has been proposed to sweep space debris from Earth orbit. This is another proposed use of beam-powered propulsion, used on objects that were not designed to be propelled by it, for example small pieces of scrap knocked off (\"spalled\") satellites. The technique works since the laser power ablates one side of the object, giving an impulse that changes the eccentricity of the object's orbit. The orbit would then intersect the atmosphere and burn up.\n\n\n"}
{"id": "766507", "url": "https://en.wikipedia.org/wiki?curid=766507", "title": "Becta", "text": "Becta\n\nBecta, originally known as the British Educational Communications and Technology Agency, was a non-departmental public body (popularly known as a Quango) funded by the Department for Education and its predecessor departments, in the United Kingdom. It was a charity and a company limited by guarantee. The abolition of Becta was announced in the May 2010 post-election spending review. Government funding was discontinued in March 2011. Becta went into liquidation in April 2011. \n\nBecta was the lead agency in the United Kingdom for promotion and integration of information and communications technology (ICT) in education. Becta was a company limited by guarantee with charitable status. It was established in 1998 through the reconstitution of the National Council for Educational Technology (NCET), which oversaw the procurement of all ICT equipment and e-learning strategy for schools. \n\nForemost among the 2005–2008 Becta strategic objectives were \"to influence strategic direction and development of national education policy to best take advantage of technology\" and \"to develop a national digital infrastructure and resources strategy leading to greater national coherence.\" \n\nThe National Grid for Learning (NGfL) was managed by Becta and was set up as a gateway to educational resources on to support schools and colleges across the UK. The NGfL portal was launched in November 1998, as one of several new programmes initiated by the new Labour government which took office in May 1997 and had a linked budget of earmarked funds to be spent on schools' internet connections and ICT.\n\nBecta awarded certain vendors placement on approved \"purchasing frameworks\": \nThe purchasing frameworks were criticised as being outdated, and for effectively denying schools the option of benefiting from both free and open source and the value and experience of small and medium ICT companies. Participating companies had to have a net worth of at least £700,000 to qualify and had to satisfy a list of functional requirements. A concern was raised about the \"over-comfortable relationship the government has with some of the bigger players.\"\n\nIn January 2007, Crispin Weston, who had helped Becta draw up the criteria used to select suppliers, asked the EC Competition Commission to investigate his allegation that a significant number of the successful tenders had failed to implement the mandatory functional requirements, including particular aspects of inter-operability. He also added in his letter to the Commission that they should take action on the further issue of: \n\n\n"}
{"id": "490588", "url": "https://en.wikipedia.org/wiki?curid=490588", "title": "Bernard T. Feld", "text": "Bernard T. Feld\n\nBernard Taub Feld (December 21, 1919 – February 19, 1993) was a professor of physics at the Massachusetts Institute of Technology. He helped develop the atomic bomb, and later led an international movement among scientists to banish nuclear weapons.\nHis life could be effectively summed up with the following famous quotation:\n\nFeld was born in Brooklyn, New York. He graduated from the City College of New York with a bachelor of science degree in 1939. He began graduate school at Columbia University, but suspended his studies to join the American war effort. He spent the war serving as an assistant to Enrico Fermi and Leó Szilárd working on the Manhattan Project. After World War II, he returned to Columbia University to receive his PhD in 1945 with thesis advisor Willis Lamb.\n\nFeld was on the faculty of MIT from 1948 until he retired in 1990. During this time, he was President of the Albert Einstein Peace Foundation, editor of the Bulletin of the Atomic Scientists, and head of the American Pugwash Committee.\n\nThe Pugwash Conferences on Science and World Affairs won the Nobel Peace Prize in 1995. Feld was a leader in these conferences, serving as U.S. Chairman from 1963 to 1973 and as International Chairman from 1973 to 1978. It was in this role that he attracted the anger of Richard Nixon's White House. He was eleventh on Nixon's list of enemies, a fact that pleased him tremendously.\n\n\"One month after the election of Ronald Reagan, Feld being an editor of 'Bulletin of the American Atomic Scientists' reported that his publication had decided to move the hands on the Doomsday Clock featured on its cover from seven to four minutes to midnight, because, as 'the year drew to a close, the world seemed to be moving unevenly but inexorably closer to nuclear disaster' \".\n\n"}
{"id": "2514679", "url": "https://en.wikipedia.org/wiki?curid=2514679", "title": "Burt's Bees", "text": "Burt's Bees\n\nBurt's Bees is an American personal care products company that markets its products internationally. The company is a subsidiary of Clorox that describes itself as an \"Earth friendly, Natural Personal Care Company\"\nmaking products for personal care, health, beauty, and personal hygiene. Its products are distributed globally.\n\nBurt's Bees manufactures their products with natural ingredients and use minimal processing such as distillation/condensation, extraction/steamed distillation/pressure cooking, and hydrolysis to maintain the purity of those ingredients. In addition, every product has a \"natural bar\" which gives a percentage of natural ingredients in that product, often with detailed ingredient descriptions.\n\nOriginating in Maine in the 1980s, the business began when co-founder Roxanne Quimby started making candles from Burt Shavitz's leftover beeswax.\nThis eventually led to their bottling and selling of honey, a practice that slowly diminished as the company evolved as a corporation. Eventually, other products using honey and beeswax, including edible spreads and furniture polish, were sold, before a move into the personal care line. In late 2007, Clorox purchased Burt's Bees for $925 \nmillion USD.\n\nBurt's Bees originated in Maine in 1984 as a candle making partnership between Roxanne Quimby and Burt Shavitz. Shavitz had a honey business which provided the excess beeswax needed for the candles and Quimby's focus on maintaining high quality helped to grow their business from an initial $200 at the Dover-Foxcroft Junior High School craft fair to $20,000 by the end of their first year. Their first headquarters was an abandoned one-room schoolhouse rented from a friend for $150 a year.\n\nBurt's Bees increased production in 1989 after a New York boutique, Zona, ordered hundreds of their beeswax candles. Forty additional employees were hired and an abandoned bowling alley became their new manufacturing location. During this time, Quimby read one of Burt Shavitz's 19th-century books about bee-keeping which included home-made personal care recipes and Burt's Bees entered into the personal care products industry.\n\nBurt's Bees became incorporated in 1991 and had a product offering including candles, natural soaps, perfumes, and eventually lip balm, which became their best-selling product. The lip balm comes in various flavors including beeswax and strawberry.\n\nIn 1993, Quimby threatened to sue Shavitz over personal issues and essentially forced Shavitz out of the company's operations. Increasing demand and product offerings necessitated a move from Maine headquarters to North Carolina where other personal care product manufacturers were also situated. Burt's Bees changed its focus to exclusively personal care products.\n\nIn 1995, the company moved its manufacturing operations into an former garment factory in Creedmoor, North Carolina. Although Burt's Bees continued to focus on the \"home-made\" product theme, automated machines, such as a former cafeteria mixer from Duke University, were introduced to increase production. Chapel Hill was the site of the first Burt's Bees retail store, which offered 50 natural personal care products. Distribution and demand of products had also reached the Japanese market.\n\nIn 1998, Burt's Bees was offering over 100 natural personal care products in 4,000 locations with sales in excess of $8 million. Distribution had reached national retailers such as Whole Foods Market and restaurants like Cracker Barrel. New product offerings branched into travel-sized skin care and hair care products. In 1999, with increasing demand and an increase product offerings, including sugar and milk-based body lotions and bath products, Burt's Bees relocated to Durham among the many other enterprises located in the Research Triangle area of North Carolina. An eCommerce website was launched allowing distribution in a much larger, nationwide scale.\n\nIn 1999, Quimby bought out Shavitz's one-third stake in the company in exchange for a house in Maine, worth approximately $130,000.\n\nIn 2002 and 2003, Burt's Bees launched its first toothpaste, first shampoo, and successful Baby Bee product line of infant personal care products. The co-founder, Roxanne Quimby, also used company-earned profits to preserve of forest land in Maine, marking the beginning of a relationship with The Nature Conservancy, an international organization engaged in environmental protection and conservation.\n\nIn 2004, private equity firm, AEA Investors, purchased 80% of Burt's Bees for $173,000,000 US, with co-founder, Roxanne Quimby, retaining a 20% share and a seat on the board. Upon seeking compensation from Quimby after the deal, Shavitz was paid $4 million.\n\nAs of 2007, they manufactured over 197 products distributed globally. In late 2007, Clorox acquired Burt's Bees for a reported sum of $925 million USD. The company subsequently released a statement to their customers.\n\nJohn Replogle, the CEO of Burt's Bees (2006-2011), left to become CEO and President of Seventh Generation Inc. in February 2011. In 2011 Nick Vlahos, a 15-year veteran of The Clorox Company was named Vice President and General Manager of Burt's Bees, effective April 2011. In 2017, Nick Vlahos left Clorox and was named CEO of the Honest Company. \n\nBy February 2014, the Clorox Company had increased \"advertising and sales promotion spending for Burt’s Bees, particularly for its lip care lines.\" At the time, \"Burt’s Bees sales [were] outpacing volume due to price increases.\" In August 2014, a documentary detailing the story between the two co-founders was released. Co-founder Shavitz died in July 2015 at the age of 80, and was buried in Bangor, Maine. In his final years, he had lived on a plot of land in Parkman, Maine.\n\nIn 2017, Burt's Bees introduced a full cosmetics line, including products such as foundation, mascara, eyeshadow, and blush.\n\nAs of March 2018 Burt's Bees lip balm reached such a level of popularity that some reports claimed one tube of the lip balm was purchased every second.\n\n\n"}
{"id": "17353312", "url": "https://en.wikipedia.org/wiki?curid=17353312", "title": "Cement render", "text": "Cement render\n\nCement rendering is the application of a premixed layer of sand and cement to brick, cement, stone, or mud brick. It is often textured, colored, or painted after application. It is generally used on exterior walls but can be used to feature an interior wall.\n\nDepending on the 'look' required, rendering can be fine or coarse, textured or smooth, natural or colored, pigmented or painted.\n\nThe cement rendering of brick, concrete and mud houses has been used for centuries to improve the appearance (and sometimes weather resistance) of exterior walls. It can be seen in different forms all over southern Europe. Different countries have their own styles and traditional colors.\n\nDifferent finishes can be created by using different tools such as trowels, sponges, or brushes. The art in traditional rendering is (apart from getting the mix right) the appearance of the top coat. Different tradesmen have different finishing styles and are able to produce different textures and decorative effects. Some of these special finishing effects may need to be created with a thin finishing 'top coat' or a finishing wash.\n\nCement render consists of 6 parts clean sharp fine sand, 1 part cement, and 1 part lime. The lime makes the render more workable and reduces cracking when the render dries. Any general purpose cement can be used. Various additives can be added to the mix to increase adhesion. Coarser sand is used in the base layer and slightly finer sand in the top layer.\n\nThe application process resembles the process for applying paint. To ensure adhesion, the surface to be rendered is initially hosed off to ensure it is free of any dirt and loose particles. Old paint or old render is scraped away. The surface is roughened to improve adhesion. For large areas, vertical battens are fixed to the wall every 1 to 1.5 meters, to keep the render flat and even.\n\nThere is also a wide variety of premixed renders for different situations. Some have a polymer additive to the traditional cement, lime and sand mix for enhanced water resistance, flexibility and adhesion.\n\nAcrylic premixed renders have superior water resistance and strength. They can be used on a wider variety of surfaces than cement render, including concrete, cement blocks, and AAC concrete paneling. With the right preparation, they can be used on smoother surfaces like cement sheeting, new high tech polymer exterior cladding such as Uni-Base, and expanded polystyrene. A few of these require activation with cement just prior to application.\n\nSome premixed acrylic renders have a smoother complexion than traditional renders. There are also many various acrylic-bound pigmented 'designer' finishing coats that can be applied over acrylic render. Various finishes, patterns and textures are possible such as sand, sandstone, marble, stone, stone chip, lime wash or clay like finishes. There are stipple, glistening finishes, and those with enhanced water resistance and anti fungal properties. Depending upon the product, they can be rolled, troweled or sponged on. A limited number can also be sprayed on. Acrylic renders take only 2 days to dry and cure—much faster than the 28 days for traditional render.\n\nA significant disadvantage of acrylic render vs. traditional rendering is that acrylic render lacks the sustainability and environmental compatibility of traditional cement-and-mineral render. All buildings have a finite lifetime, and their materials will eventually be either recycled or absorbed into the environment. Acrylic being a synthetic polymer material, it does not break down by natural weathering the way that a cement, sand, and lime mixture will, persisting in the natural environment for centuries as synthetic chemical compounds that have unknown long-term effects on ecosystems. Also, the application and drying process of acrylic resin render involves the atmospheric evaporation of pollutant solvents—necessary for the application of the resin—which are hazardous to the health of humans and of many organisms on which humans depend. Synthetic polymers such as acrylic are manufactured from chemical feedstocks such as acetone, hydrogen cyanide, ethylene, isobutylene, and other petroleum derivatives. The polymer products cannot be fully recycled (using present technology or any that can be confidently expected to be developed), so new raw materials, taken from the finite and diminishing supply of raw natural resources, must always be put into their manufacture, making the process unsustainable. Traditional cement-based render does not have these problems, making it an arguably better choice in many cases, despite its working limitations.\n\n"}
{"id": "41681886", "url": "https://en.wikipedia.org/wiki?curid=41681886", "title": "Collabrify", "text": "Collabrify\n\nCollabrification came from the foundation of making mobile education more collaborative. From the start, it was designed to make something that was inherently non-collaborative and make it collaborative. The infrastructure has been used in the IMLC, University of Michigan, University of Texas, and the department of Mathematics and Information Technology at The Hong Kong Institute of Education. By building real-time, synchronous collaboration into mobile infrastructure, people build on the Four C's: critical thinking, communications, collaboration, and creativity. Collabrification of mobile and web applications allows people to build relationships, increase shared understanding, increase productivity, organize thoughts, and make mobile technology, specifically related to mobile applications, more fun and engaging.\n\nThe first set of collabrified mobile apps were WeKWL, WeMap, WeSketch, WeCollabrify, YesWeKhan,\n\n"}
{"id": "13117577", "url": "https://en.wikipedia.org/wiki?curid=13117577", "title": "Compact (cosmetics)", "text": "Compact (cosmetics)\n\nA compact (also powder box and powder case) is a cosmetic product. It is usually a small round metal case and contains two or more of the following: a mirror, pressed or loose face powder with a gauze sifter and a powder puff.\n\nCompacts date from the early 1900s, a time when make-up had not gained widespread social acceptance and the first powder cases were often concealed within accessories such as walking sticks, jewellery or hatpins.\n\nFrom 1896, American handbag manufacturer Whiting & Davis created lidded compartments in its bags where powder rouge and combs could be stowed. In 1908, Sears' catalogue advertised a silver-plated case with mirror and powder puff (price 19 cents) and described it as small enough to fit in a handbag. \n\nIn the US, manufacturers such as Evans and Elgin American produced metal compacts with either finger chains or longer tango chains. Designed to be displayed rather than fitted in a handbag, they required more ornate designs and many from this era are examples of sleek Art Deco styling. \n\nAs make-up became more mainstream and women were increasingly active outside the home, compacts became more popular. British manufacturer Stratton began importing part-finished powder boxes from the US for assembly at its Birmingham plant in 1923 and by the 1930s it was creating them from scratch and producing half the compacts used by the UK cosmetics industry. The company developed self-opening inner lids in 1948, designed to protect the powder and prevent damage to fingernails, and by the 1960s it was exporting to agents worldwide.\n\nCompacts were heavily influenced by prevailing fashions – for instance, the 1922 discovery of Tutankhamun's tomb spawned Egypt-inspired obelisks, sphinxes and pyramids, while the growing popularity of the car meant compacts were incorporated into visors, steering wheels and gears. Jewellers such as Van Cleef & Arpels, Tiffany and Cartier began producing minaudières, metal evening bags/vanity cases carried on a metal or silk cord that contained a compact plus space for a few other small items, many were inlaid with jewels or personalised.\n\nBy the 1930s, compacts were regularly updated to match the season's fashion trends and gimmicks such as watches and even miniature windscreen wipers were included in designs. Later, compacts became popular souvenir items, both the Chicago and New York world's fairs of the 1930s included souvenir powder cases, and during holidays.\n\nAlthough compacts continued to be in widespread production up to the 1960s, their popularity diminished as the cosmetics industry created plastic containers that were designed to be discarded once the powder ran out. These began to be heavily advertised from the 1950s. Writing in \"Americana\", Deirdre Clemente suggested that changing make-up trends, notably for natural rather than pale and powdered complexions from the late 1950s on, contributed to the declining popularity of the compact.\n\n\n"}
{"id": "50891745", "url": "https://en.wikipedia.org/wiki?curid=50891745", "title": "Context Labs", "text": "Context Labs\n\nContext Labs (CXL) is a company that provides blockchain enabled platform solutions such as secure distributed ledgers, network graph analytics, and data interoperability and visualization for publishing, financial, trading and supply chain industries. Headquartered in Amsterdam, Netherlands and Cambridge, Massachusetts, the company was founded in 2013.\n\nContext Labs, BV was founded in 2013 by Daniel Harple, a technology entrepreneur and internet pioneer in web streaming and VoIP. With offices in Amsterdam, Netherlands and Cambridge, Massachusetts, the company was formed with the objective of extending research and development begun at MIT Sloan and the MIT Media Lab focusing on \"innovation dynamics\". According to the company, it offers \"enterprise grade platform solutions that assist in the development of new market channels and reducing channel friction, while retaining and growing direct customer relationships\".\n\nPartnering with architectural design firm Rogers Partners in 2014, the company applied innovation dynamics methods to collaborate on the design for the Connect Kendall Square project.\n\nIn March 2016, R. R. Donnelley & Sons announced it would partner with the company to integrate Context Labs blockchain enabled platform technology within RR Donnelley’s print-to-digital supply chain solutions.\n\nIn June 2016, Context Labs announced that as a co-founding member, the company would provide operational, strategic, and technical guidance for the Open Music Initiative, a digital rights framework for the music industry.\n\n\n\n"}
{"id": "47664987", "url": "https://en.wikipedia.org/wiki?curid=47664987", "title": "Density meter", "text": "Density meter\n\nA density meter, also known as a densimeter, is a device that measures the density. Density is usually abbreviated as either formula_1 or formula_2. Typically, density either has the units of \"formula_3\" or \"formula_4\". The most basic principle of how density is calculated is by the formula:\n\nWhere:\n\nMany density meters can measure both the wet portion and the dry portion of a sample. The wet portion comprises the density from all liquids present in the sample. The dry solids comprise solely of the density of the solids present in the sample.\n\nA density meter does not measure the specific gravity of a sample directly. However, the specific gravity can be inferred from a density meter. The specific gravity is defined as the density of a sample compared to the density of a reference. The reference density is typically of that of water. The specific gravity is found by the following equation:\n\nWhere:\n\nDensity meters come in many varieties. Different types include: nuclear, coriolis, ultrasound, microwave, and gravitic. Each type measures the density differently. Each type has its advantages and drawbacks.\n\nDensity meters have many applications in various parts of various industries. Density meters are used to measure slurries, sludges, and other liquids that flow through the pipeline. Industries such as mining, dredging, wastewater treatment, paper, oil, and gas all have uses for density meters at various points during their respective processes.\n\nGravimetric density meters work on the principle of gravity to calculate the density of a sample. A flexible hose is used to determine the change in weight. Using the principle of beam deflection of two fixed ends, the weight can be calculated. Increases in weight result in a larger deflection. Decreases in weight result in a smaller deflection. The volume inside of the hose never changes. Since the volume is constant and the weight is known, the density is easily calculated from this information.\n\nDisplacement is measured with a high precision displacement laser. Micron scale deflections can be read by the density meter. Minute changes in weight are seen at this scale.\n\nThe entire volume is measured using gravimetric methods. This means that the sample size is the entire volume of what needs to be measured.\n\nGravimetric density meters use the least amount of theory, making them the most accurate choice, depending on the application. Using the equation formula_5 the volume is known, once we find the mass, we can find the density.\n\nCoriolis density meters, also known as \"mass flow meters\" or \"inertial flow meters\", work on the principle of vibration to measure phase shifts in the vibration of a bent thin walled tube. The bent thin walled tube is rotated around a central axis. When there is no mass in the bent section, the tube remains untwisted. However, when the density inside the bent section increases, the inbound flow portion of the bent pipe drags behind the out flow portion. This twisting causes phase shifts which result in changes in the resonant frequency of the thin walled tube. Therefore, the resonant frequency is directly affected by the density. Higher density media causes a larger Coriolis effect if the volumetric flow rate is constant. \nFlowing media causes a frequency and a phase shift of the bent pipe, which are proportional to the mass flow rate of the sample.\n\nCoriolis meters measure the mass flow of the system. They do not measure the volumetric flow. \nHowever, a volumetric flow can be inferred from the mass flow measurement. \nThese measurements are restricted to small diameters for flow tubes. However, this measurement technique results in high accuracy and high repeatability. Coriolis meters also have a fast response time.\n\nCoriolis meters need to be calibrated for temperature and pressure. The zero points for these values are used to calibrate the system. Coriolis meters cannot be calibrated while in use. The span difference is used to see how temperature and pressure have changed.\n\nNuclear density meters work on the principle of measuring gamma radiation. Gamma radiation is emitted from a source. This source is typically caesium-137 (half-life: ~30 years). The radiation is seen by a scintillator device. The radiation is converted into flashes of light. The number of flashes of light is counted. Radiation that is absorbed by the mass is not seen by the scintillator device. Therefore, the density of the media is inversely proportional to the radiation captured and seen by the scintillator.\n\nNuclear density meters are limited in scope to what is seen by the gamma radiation beam. The sample size is a single, thin column with small longitudinal length.\n\nNuclear equipment requires certified and licensed staff in order to operate the instruments.\n\nMicrowave density meters have various ways to measure what solids are in the sample. All microwave meters measure microwaves but some use different methods such as measuring the microwave propagation speed change, amplitude reduction, time of flight, single phase difference, or dual phase shift. Each technique has certain accuracies.\n\nSome microwave meters use a ceramic probe that is directly inserted into the sample. This allows the meter to have direct contact to the sample in question. However, this limits the types of slurries and sludges that can flow through the pipe line. Abrasive slurries with particulates can damage the sensor probe.\n\nMicrowave meters are also limited to liquids with unvarying dielectric constants. The percentage of solids of the slurry affects the dielectric constant for the entire sample. Typically, percent solids greater than 20% result in large errors. Similar inconsistencies happen with large pipe \ndiameters.\n\nMicrowave meters are very good at detecting dissolved solids. Homogeneous solutions are easily seen by microwave meters. This makes them a fit for applications where the solution is consistent and non-abrasive.\n\nUltrasonic density meters work on various principles to calculate the density. One of the methods is transit-time principle (also known as the time of flight principle). In this technique, two transducers are mounted to the sides of the pipe walls. The transducers alternate between sending and receiving ultrasonic signals. From this transit time measurement, the flow velocity and volume flow based on the diameter of the pipe are calculated.\n\nAnother method this is used is ultrasonic attenuation method. This method measures the count of various signals with certain amplitudes. The density of the media flowing through the pipe affects the signal sent through the pipe. This changes the strength of the signal, causing a weaker signal and smaller amplitude.\n\nAnother method that is utilized in ultrasonic meters is the envelope energy average method. This method is based on not only the amplitude of the signal but also the shape of the signal. These packets of information are called envelopes.\n\nDoppler ultrasonic meters measure the suspension flow where the concentration of solids in the slurry is above 100ppm and the particles that are suspended are larger than 100 microns in diameter. However, the Doppler method only works on concentrations of less than 10% solids.\n\nTemperature affects the density of fluids. In most cases, an increase in temperature indicates that the density of the media will decrease. This indicates that temperature and density are inversely proportional to each other. Temperature also affects the meters themselves. Mass flow meters have different resonant frequencies at different temperatures.\n\nPressure changes the rigidity of the mass flow tube. Pressure affects the rigidity of gravimetric meters.\n\nVibration from plant noise can be filtered out. Vibration is apparent in microwave, ultrasonic, gravimetric, and Coriolis meters. Vibration causes these types of meters to accumulate error\n\nCoriolis meters have compensations from pitting, cracking, coating, erosion, and corrosion. These damages affect the way that the tube resonates. These changes affect the baseline. Compensations cannot be made dynamically. These damages typically cause offsets that can be added to the existing calibration factors that will ensure that a consistent reading is still acquired.\n"}
{"id": "22728417", "url": "https://en.wikipedia.org/wiki?curid=22728417", "title": "Distinguo", "text": "Distinguo\n\nDistinguo is a proprietary software application for Semantic search based on description logic that enables users to search for meaning instead of just keywords. This API permits developers to integrate into their applications a tool to parse natural language (generating an XML summary), and then measure the semantic \"distance\" between a query and a target text. \"Guha et al.\" distinguish two major forms of search: Navigational and Research. In navigational search, the user is using the search engine as a navigation tool to navigate to a particular intended document. Semantic Search is not directly applicable to navigational searches. In Research Search, the user enters a phrase which is intended to denote the object of the research. The user will not know in advance which particular documents will contain the information; the task is to locate any number of documents which together will produce the sought information.\n\nThe program is not available as a stand-alone or consumer application; it is a powerful software component for inclusion in other text analysis solutions such as any project requiring the comparison of meanings or the measurement of the difference between words and longer texts. Users can compare a simple query to a large database of texts, locating texts containing similar meanings and ranking them according to their similarity.\n\nIn addition to measuring the similarity between words, sentences, or texts written in natural language, semantic searching can also distill the meanings of a set of texts and then provide comparative information about those meanings. Users enter a word or phrase which represents a subject for which similar related information is required. Possible typical applications would be for comparing reports, minutes of meetings, insurance claims, medical histories, research papers, and legal decisions from many court cases.\n\nDistinguo is a C++ Application Programming Interface (API), in two different applications:\n\nDistinguo Index: a tool for expanding search keywords to include inflected forms, synonyms, hypernyms, hyponyms, and other words related by meaning.\n\nDistinguo Context: a tool for analyzing the meaning of full sentences or even of full texts; it can then match this text with other texts containing the same or similar ideas.\n\nThey are delivered for integration into other solutions. Distinguo tools are supplied as C++ libraries, and can be integrated into software solutions for its own features, or to supplement or refine statistical search methods.\n\nThe result of the syntactic analysis, as well as the format of the ontologies, is represented in XML. The calculation of semantic similarities may be in the form of a numerical coefficient, or an ontology showing the information present in the first ontology and missing in the second. The format of the texts and of the XML is a string of characters in the programming language 'C'.\n\nDistinguo Index and Distinguo Context are based on algorithms for the parsing of language and the matching and ranking semantic results developed by Semantica Software of Luxembourg in association with Ultralingua and is in constant further research and development. Other uses could be integrated into electronic dictionary or phrasebook applications.\n\n\n\n"}
{"id": "21130192", "url": "https://en.wikipedia.org/wiki?curid=21130192", "title": "Elecom", "text": "Elecom\n\nElecom Co., Ltd. (エレコム株式会社) is a Japanese electronics company, headquartered in Osaka, Japan. They are best known for their modern design, especially with their mice. They also manufacture USB flash drives, keyboards and many other peripheral devices. Though not very commonly seen in the United States, they control a large percent of the market share in Japan in various product categories, including #1 in computer mice, keyboards, and speakers as of 2010. The company produces many electronics along with other Japanese computer-peripheral manufacturers such as I-O Data, Buffalo, and Century micro that are shipped to many outside and larger companies, as an OEM (e.g. Century Micro is known to ship their memory chips to Samsung).\n\n\n"}
{"id": "4319717", "url": "https://en.wikipedia.org/wiki?curid=4319717", "title": "Electrostatic fieldmeter", "text": "Electrostatic fieldmeter\n\nAn electrostatic fieldmeter, also called a static meter is a tool used in the static control industry. It is used for non-contact measurement of electricity charge on an object. It measures the electrostatic field of an object in volts, measuring both the initial peak voltage and the rate at which it falls away.\n\n\n"}
{"id": "6541883", "url": "https://en.wikipedia.org/wiki?curid=6541883", "title": "Fishing float", "text": "Fishing float\n\nA fishing float (or bobber in the US) is an item of angling equipment. Usually attached to a fishing line, it can serve several purposes. Firstly, it can suspend the bait at a predetermined depth; secondly, due to its buoyancy, it can carry the baited hook to otherwise inaccessible areas of water by allowing the float to drift in the prevailing current; and thirdly, a float also serves as a visual bite indicator. Fishing with a float is sometimes called float fishing.\n\nFloats come in different sizes and shapes, and can be made from various materials, such as balsa wood, cork, plastic, Indian sarcanda reed (Erianthus family), or even bird/porcupine quills. The float is used to enable the angler to cast out a bait away from the shore or boat while maintaining a reference point to where the bait is unlike bottom or leger fishing. The angler will select an appropriate float after taking into account the strength of the current (if any), the wind speed, the size of the bait he or she is using, the depth the angler wishes to present that bait at and the distance the bait is to be cast. Usually, the line between the float and hook will have small weights attached, ensuring that the float sits vertically in the water with only a small brightly coloured tip remaining visible. The rest of the float is usually finished in a dull neutral colour to render it as inconspicuous as possible to the fish. Each float style is designed to be used in certain types of conditions such as slow or fast rivers, windy or still water or small confined waters such as canals.\n\nIt is impossible to say with any degree of accuracy who first used a float for indicating that a fish had taken the bait, but it can be said with some certainty that people used pieces of twig, bird feather quills or rolled leaves as bite indicators, many years before any documented evidence. The first known mention of using a float appears in the book \"Treatyse of fysshynge wyth an Angle\" written by Juliana Berners in 1496.\n\n\"All maner lynes that be not for the grounde must haue flotes, and the rennyng ground lyne must haue a flote, the lyeng ground lyne must haue a flote.\"\n\nThe method described, involved boring a hole through a cork so the line could be passed through and trapped with a quill. Later books such as \"the Art of Angling\"by Gerald Eades Bentley in 1577 and the classic work \"The Compleat Angler\" first published in 1653, written by Isaak Walton gave greater detail on fishing and using floats.\n\nPrior to about 1800, anglers made their own floats, a practice that many still carry on today. As angling became more popular, companies started to make floats in different styles to supply the growing demand. By 1921, companies such as Wadhams had at least 250 mainly celluloid floats in their catalog.\n\nSince those early days, the fishing float has become the subject of much practical and theoretical change. English anglers such as Peter Drennan (Drennan International) and Kenneth Middleton (Middy Tackle) and American fishermen like Chicago's ex World Champion Mick Thill (Thill Floats) have built up large companies designing and marketing fishing floats. The English companies have been supported by major league anglers such as Ivan Marks, Benny Ashurst and Billy Lane.\n\nThe Avon float is a straight float with a body at the top. It was designed to cope with the fast flow conditions of the English River Avon. Many early floats were Avon style having a cork body pushed onto a crow quill. It is fished attached to the line top and bottom.\n\nBubble floats are small hollow balls which are used to control the fishing line. They may have the facility to be partially filled with water to control how much float is above the water. They are used in situations where a normal float cannot be cast, such a working close to the edge of reeds or heavy surface plant growth. The bubble float can be allowed to drift into the area without tangling.\n\nThe Dink Float is most commonly made of a cylinder of dark foam with a smaller cylinder of cork on the top painted for indicator. The line is run through the top, wrapped around the cylinder and through the bottom. Main advantage is that the float needs no stopper on the main line, the wrap of line between the top of bottom of float will hold it in place.\n\nA popper float, commonly called a 'popping cork' is designed to mimic a large fish feeding at the surface with rod action. There are different styles of popper floats, some use a metal wire with beads at each end to make a clicking noise when pulled through the water, while more modern floats make use of a concave top, which make a deep chugging sound when pulled through the water, imitating the sound of large predator fish feeding at the surface. Some popping corks also have pellets inside, designed to mimic bait fish jumping at the surface when rattled.\n\nThe quill is one of the earliest floats, originally it was a bird feather quill but with the opening up of new worlds, porcupine quills from Africa became a standard for the float. It is fished in the same way as a stick float.\n\nSelf-cocking floats can be of many styles but they are all weighted so that in the water they automatically stand upright without the use of shot or weights on the fishing line.\n\nThe Stick Float is a straight float with a taper. It is always attached to the line both top and bottom. They are made from two different materials, a light, buoyant top section of balsa wood and a heavy stem of hard grade cane, non-buoyant hardwood, or plastic. Unlike the Avon float, the stick has no body; it is just a tapered rod.\n\nA waggler float \nis the term given to any float which is attached only at the bottom to the line. They come in two different types, straight or bodied. These two types can come both with and without inserts (antennas). They are made from a variety of materials including quills (such as peacock), balsa wood, cane, plastic and reed.\n\nFloats with direction control change direction by planing or moving to one side when given a tug.\n"}
{"id": "938385", "url": "https://en.wikipedia.org/wiki?curid=938385", "title": "Flight recorder", "text": "Flight recorder\n\nA flight recorder is an electronic recording device placed in an aircraft for the purpose of facilitating the investigation of aviation accidents and incidents. Flight recorders are also known by the misnomer black box—they are in fact bright orange to aid in their recovery after accidents.\n\nThere are two different flight recorder devices: the flight data recorder (FDR) preserves the recent history of the flight through the recording of dozens of parameters collected several times per second; the cockpit voice recorder (CVR) preserves the recent history of the sounds in the cockpit, including the conversation of the pilots. The two devices may be combined in a single unit. Together, the FDR and CVR give an accurate testimony, narrating the aircraft's flight history, to assist in any later investigation. \n\nThe two flight recorders are required by international regulation, overseen by the International Civil Aviation Organization, to be capable of surviving the conditions likely to be encountered in a severe aircraft accident. For this reason, they are typically specified to withstand an impact of 3400 \"g\" and temperatures of over , as required by EUROCAE ED-112. They have been a mandatory requirement in commercial aircraft in the United States since 1967.\n\nOne of the earliest and proven attempts was made by François Hussenot and Paul Beaudouin in 1939 at the Marignane flight test center, France, with their \"type HB\" flight recorder; they were essentially photograph-based flight recorders, because the record was made on a scrolling photographic film long by wide. The latent image was made by a thin ray of light deviated by a mirror tilted according to the magnitude of the data to record (altitude, speed, etc.). A pre-production run of 25 \"HB\" recorders was ordered in 1941 and HB recorders remained in use in French flight test centers well into the 1970s. \n\nIn 1947, Hussenot founded the Société Française des Instruments de Mesure with Beaudouin and another associate, so as to market his invention, which was also known as the \"hussenograph\". This company went on to become a major supplier of data recorders, used not only aboard aircraft but also trains and other vehicles. SFIM is today part of the Safran group and is still present on the flight recorder market. The advantage of the film technology was that it could be easily developed afterwards and provides a durable, visual feedback of the flight parameters without needing any playback device. On the other hand, unlike magnetic tapes or later flash memory-based technology, a photographic film cannot be erased and recycled, and so it must be changed periodically. As such, this technology was reserved for one-shot uses, mostly during planned test flights; and it was not mounted aboard civilian aircraft during routine commercial flights. Also, the cockpit conversation was not recorded.\n\nAnother form of flight data recorder was developed in the UK during World War II. Len Harrison and Vic Husband developed a unit that could withstand a crash and fire to keep the flight data intact. This unit used copper foil as the recording medium with various styli indicating various instruments / aircraft controls which indented the copper foil. The copper foil was periodically advanced at set periods of time therefore giving a history of the instruments / control settings of the aircraft. This unit was developed at Farnborough for the Ministry of Aircraft Production. At the war's end the Ministry got Harrison and Husband to sign over their invention to it and the Ministry patented it under British patent 19330/45. This unit was the forerunner of today's recorders being able to withstand conditions that aircrew could not.\n\nThe first modern flight recorder, called \"Mata Hari\", was created in 1942 by Finnish aviation engineer Veijo Hietala. This black high-tech mechanical box was able to record all important details during test flights of fighter aircraft that the Finnish army repaired or built in its main aviation factory in Tampere, Finland.\n\nIn 1953, while working at the Aeronautical Research Laboratories (ARL) of the Defence Science and Technology Organisation, in Melbourne, Australian research scientist David Warren conceived a device that would record not only the instrument readings, but also the voices in the cockpit. In 1954 he published a report entitled \"A Device for Assisting Investigation into Aircraft Accidents\".\n\nWarren built a prototype FDR called \"The ARL Flight Memory Unit\" in 1956, and in 1958 he built the first combined FDR/CVR prototype, which was designed with civilian aircraft in mind, for explicit post-crash examination purposes. Aviation authorities from around the world were largely uninterested at first, but this changed in 1958 when Sir Robert Hardingham, the Secretary of the British Air Registration Board, visited the ARL and was introduced to David Warren. Hardingham realised the significance of the invention and arranged for Warren to demonstrate the prototype in the UK.\n\nThe ARL assigned an engineering team to help Warren develop the prototype to airborne stage. The team, consisting of electronics engineers Lane Sear, Wally Boswell and Ken Fraser, developed a working design that incorporated a fire-resistant and shockproof case, a reliable system for encoding and recording aircraft instrument readings and voice on one wire, and a ground-based decoding device. The ARL system, made by the British firm of S. Davall & Sons Ltd, in Middlesex, was named the \"Red Egg\" because of its shape and bright red colour.\n\nThe units were redesigned in 1965 and relocated at the rear of aircraft to increase the probability of successful data retrieval after a crash.\n\nThe \"Flight Recorder\" was invented and patented in the United States by Professor James J. \"Crash\" Ryan, a professor of mechanical engineering at the University of Minnesota from 1931 to 1963. Ryan's \"Flight Recorder\" patent was filed in August 1953 and approved on November 8, 1960 as US Patent 2,959,459. A second patent by Ryan for a \"Coding Apparatus For Flight Recorders and the Like\" is US Patent 3,075,192 dated January 22, 1963. An early prototype of the Ryan Flight Data Recorder is described in the January 2013 \"Aviation History Magazine\" article \"Father of the Black Box\" by Scott M. Fisher. Ryan, also the inventor of the retractable safety seat belt now required in automobiles, began working on the idea of a flight recorder in 1946, and invented the device in response to the 1948 request from the Civil Aeronautics Board for development of a flight recorder as a means of accumulating data that could be used to get information useful in arriving at operating procedures designed to reduce air mishaps. The original device was known as the \"General Mills Flight Recorder\". The benefits of the flight recorder and the coding apparatus for flight recorders were outlined by Ryan in his study entitled \"Economies in Airline Operation with Flight Recorders\" which was entered into the Congressional Record in 1956. Ryan's Flight Recorder maintained a continuing recording of aircraft flight data such as engine exhaust, temperature, fuel flow, aircraft velocity, altitude, control surfaces positions, and rate of descent.\n\nA \"Cockpit Sound Recorder\" (CSR) was independently invented and patented by Edmund A. Boniface, Jr., an aeronautical engineer at Lockheed Aircraft Corporation. He originally filed with the US Patent Office on February 2, 1961, as an \"Aircraft Cockpit Sound Recorder\". The 1961 invention was viewed by some as an \"invasion of privacy\". Subsequently Boniface filed again on February 4, 1963 for a \"Cockpit Sound Recorder\" (US Patent 3,327,067) with the addition of a spring-loaded switch which allowed the pilot to erase the audio/sound tape recording at the conclusion of a safe flight and landing. Boniface's participation in aircraft crash investigations in the 1940s and in the accident investigations of the loss of one of the wings at cruise altitude on each of two Lockheed Electra turboprop powered aircraft (Flight #542 operated by Braniff Airlines in 1959 and Flight #710 operated by Northwest Orient Airlines in 1961) led to his wondering what the pilots may have said just prior to the wing loss and during the descent as well as the type and nature of any sounds or explosions that may have preceded or occurred during the wing loss. His patent was for a device for recording audio of pilot remarks and engine or other sounds to be \"contained with the in-flight recorder within a sealed container that is shock mounted, fireproofed and made watertight\" and \"sealed in such a manner as to be capable of withstanding extreme temperatures during a crash fire\". The CSR was an analog device which provided a progressive erasing/recording loop (lasting 30 or more minutes) of all sounds (explosion, voice, and the noise of any aircraft structural components undergoing serious fracture and breakage) which could be overheard in the cockpit.\n\nThe origin of the term \"black box\" is uncertain. In a systems engineering context (since the 1960s when the term was spreading), the meaning is that the aircraft is modeled as a black box, and its behaviour can be understood from its recorded inputs, such as pilot instructions, and outputs, such as flight level data.\n\nThe term \"black box\" is almost never used within the flight safety industry or aviation, which prefers the term \"flight recorder\". The recorders are not permitted to be black in colour, and must be bright orange, as they are intended to be spotted and recovered after incidents. The term \"black box\" has been popularised by the media in general.\n\nOne explanation for popularization of the term \"black box\" comes from the early film-based design of flight data recorders, which required the inside of the recorder to be perfectly dark to prevent light leaks from corrupting the record, as in a photographer's darkroom.\n\nAnother possible origin of the term is World War II RAF jargon. Prior to the end of the war in 1945, new electronic innovations, such as Oboe, GEE and H2S, were added to bombers on a regular basis. The prototypes were roughly covered in hand-made metal boxes, painted black to prevent reflections. After a time any piece of \"new\" electronics was referred to as the \"box-of-tricks\" (as illusionist box) or the \"black box\".\n\nThe first recorded use of the term \"black box\" in reference to flight data recorders and cockpit voice recorders was by Mr E. Newton of the AAIB at a meeting of the Aeronautical Research Council in August 1958.\n\nA flight data recorder (FDR; also ) is an electronic device employed to record instructions sent to any electronic systems on an aircraft.\n\nThe data recorded by the FDR are used for accident and incident investigation. Due to their importance in investigating accidents, these ICAO-regulated devices are carefully engineered and constructed to withstand the force of a high speed impact and the heat of an intense fire. Contrary to the popular term \"black box\", the exterior of the FDR is coated with heat-resistant bright orange paint for high visibility in wreckage, and the unit is usually mounted in the aircraft's tail section, where it is more likely to survive a severe crash. Following an accident, the recovery of the FDR is usually a high priority for the investigating body, as analysis of the recorded parameters can often detect and identify causes or contributing factors.\n\nModern day FDRs receive inputs via specific data frames from the Flight Data Acquisition Units (FDAU). They record significant flight parameters, including the control and actuator positions, engine information and time of day. There are 88 parameters required as a minimum under current US federal regulations (only 29 were required until 2002), but some systems monitor many more variables. Generally each parameter is recorded a few times per second, though some units store \"bursts\" of data at a much higher frequency if the data begin to change quickly. Most FDRs record approximately 17–25 hours of data in a continuous loop. It is required by regulations that an FDR verification check (readout) is performed annually in order to verify that all mandatory parameters are recorded.\n\nModern FDRs are typically double wrapped in strong corrosion-resistant stainless steel or titanium, with high-temperature insulation inside. Modern FDRs are accompanied by an underwater locator beacon that emits an ultrasonic \"ping\" to aid in detection when submerged. These beacons operate for up to 30 days and are able to operate while immersed to a depth of up to .\n\nA cockpit voice recorder (CVR) is a flight recorder used to record the audio environment in the flight deck of an aircraft for the purpose of investigation of accidents and incidents. This is typically achieved by recording the signals of the microphones and earphones of the pilots' headsets and of an area microphone in the roof of the cockpit. The current applicable FAA TSO is C123b titled Cockpit Voice Recorder Equipment.\n\nWhere an aircraft is required to carry a CVR and uses digital communications the CVR is required to record such communications with air traffic control unless this is recorded elsewhere. it is an FAA requirement that the recording duration is a minimum of two hours.\n\nA standard CVR is capable of recording 4 channels of audio data for a period of 2 hours. The original requirement was for a CVR to record for 30 minutes, but this has been found to be insufficient in many cases, significant parts of the audio data needed for a subsequent investigation having occurred more than 30 minutes before the end of the recording.\n\nThe earliest CVRs used analog wire recording, later replaced by analog magnetic tape. Some of the tape units used two reels, with the tape automatically reversing at each end. The original was the ARL Flight Memory Unit produced in 1957 by Australian David Warren and an instrument maker named Tych Mirfield.\n\nOther units used a single reel, with the tape spliced into a continuous loop, much as in an 8-track cartridge. The tape would circulate and old audio information would be overwritten every 30 minutes. Recovery of sound from magnetic tape often proves difficult if the recorder is recovered from water and its housing has been breached. Thus, the latest designs employ solid-state memory and use digital recording techniques, making them much more resistant to shock, vibration and moisture. With the reduced power requirements of solid-state recorders, it is now practical to incorporate a battery in the units, so that recording can continue until flight termination, even if the aircraft electrical system fails.\n\nLike the FDR, the CVR is typically mounted in the rear of the airplane fuselage to maximize the likelihood of its survival in a crash.\n\nWith the advent of digital recorders, the FDR and CVR can be manufactured in one fireproof, shock proof, and waterproof container as a combined digital Cockpit Voice and Data Recorder (CVDR). Currently, CVDRs are manufactured by L-3 Communications, as well as by other manufacturers.\n\nSolid state recorders became commercially practical in 1990, having the advantage of not requiring scheduled maintenance and making the data easier to retrieve. This was extended to the two-hour voice recording in 1995.\n\nSince the 1970s, most large civil jet transports have been additionally equipped with a \"quick access recorder\" (QAR). This records data on a removable storage medium. Access to the FDR and CVR is necessarily difficult because of the requirement that they survive an accident. They also require specialized equipment to read the recording. The QAR recording medium is readily removable and is designed to be read by equipment attached to a standard desktop computer. In many airlines, the quick access recordings are scanned for 'events', an event being a significant deviation from normal operational parameters. This allows operational problems to be detected and eliminated before an accident or incident results.\n\nMany modern aircraft systems are digital or digitally controlled. Very often, the digital system will include Built-In Test Equipment which records information about the operation of the system. This information may also be accessed to assist with the investigation of an accident or incident.\n\nThe design of today's FDR is governed by the internationally recognized standards and recommended practices relating to flight recorders which are contained in ICAO Annex 6 which makes reference to industry crashworthiness and fire protection specifications such as those to be found in the European Organisation for Civil Aviation Equipment documents EUROCAE ED55, ED56 fiken A and ED112 (Minimum Operational Performance Specification for Crash Protected Airborne Recorder Systems). In the United States, the Federal Aviation Administration (FAA) regulates all aspects of US aviation, and cites design requirements in their Technical Standard Order, based on the EUROCAE documents (as do the aviation authorities of many other countries).\n\nCurrently, EUROCAE specifies that a recorder must be able to withstand an acceleration of 3400 \"g\" (33 km/s²) for 6.5 milliseconds. This is roughly equivalent to an impact velocity of and a deceleration or crushing distance of 45 cm. Additionally, there are requirements for penetration resistance, static crush, high and low temperature fires, deep sea pressure, sea water immersion, and fluid immersion.\n\nEUROCAE ED-112 (Minimum Operational Performance Specification for Crash Protected Airborne Recorder Systems) defines the minimum specification to be met for all aircraft requiring flight recorders for recording of flight data, cockpit audio, images and CNS / ATM digital messages and used for investigations of accidents or incidents. When issued in March 2003 ED-112 superseded previous ED-55 and ED-56A that were separate specifications for FDR and CVR. FAA TSOs for FDR and CVR reference ED-112 for characteristics common to both types.\n\nIn order to facilitate recovery of the recorder from an aircraft accident site they are required to be coloured bright yellow or orange with reflective surfaces. All are lettered \"FLIGHT RECORDER DO NOT OPEN\" on one side in English and the same in French on the other side. To assist recovery from submerged sites they must be equipped with an underwater locator beacon which is automatically activated in the event of an accident.\n\nIn the investigation of the 1960 crash of Trans Australia Airlines Flight 538 at Mackay (Queensland), the inquiry judge strongly recommended that flight recorders be installed in all Australian airliners. Australia became the first country in the world to make cockpit-voice recording compulsory.\nThe United States' first CVR rules were passed in 1964, requiring all turbine and piston aircraft with four or more engines to have CVRs by March 1, 1967. it is an FAA requirement that the CVR recording duration is a minimum of two hours, following the NTSB recommendation that it should be increased from its previously-mandated 30-minute duration. , the United States requires flight data recorders and cockpit voice recorders on aircraft that have 20 or more passenger seats, or those that have six or more passenger seats, are turbine-powered, and require two pilots.\n\nFor US air carriers and manufacturers, the National Transportation Safety Board (NTSB) is responsible for investigating accidents and safety-related incidents. The NTSB also serves in an advisory role for many international investigations not under its formal jurisdiction. The NTSB does not have regulatory authority, but must depend on legislation and other government agencies to act on its safety recommendations. In addition, 49 USC Section 1114(c) prohibits the NTSB from making the audio recordings public except by written transcript.\n\nThe ARINC Standards are prepared by the Airlines Electronic Engineering Committee (AEEC). The 700 Series of standards describe the form, fit, and function of avionics equipment installed predominately on transport category aircraft. The FDR is defined by ARINC Characteristic 747. The CVR is defined by ARINC Characteristic 757.\n\nThe NTSB recommended in 1999 that operators be required to install two sets of CVDR systems, with the second CVDR set being \"deployable or ejectable\". The \"deployable\" recorder combines the cockpit voice/flight data recorders and an emergency locator transmitter (ELT) in a single unit. The \"deployable\" unit would depart the aircraft before impact, activated by sensors. The unit is designed to \"eject\" and \"fly\" away from the crash site, to survive the terminal velocity of fall, to float on water indefinitely, and would be equipped with satellite technology for immediate location of crash impact site. The \"deployable\" CVDR technology has been used by the US Navy since 1993. The recommendations would involve a massive retrofit program. However, government funding would negate cost objections from manufacturers and airlines. Operators would get both sets of recorders for free: they would not have to pay for the one set they are currently required by law to carry. The cost of the second \"deployable/ejectable CVDR\" (or \"Black Box\") was estimated at US$30 million for installation in 500 new aircraft (about $60,000 per new commercial plane).\n\nIn the United States, the proposed SAFE Act calls for implementing the NTSB 1999 recommendations. However, so far the SAFE ACT legislation has failed to pass Congress, having been introduced in 2003 (H.R. 2632), in 2005 (H.R. 3336), and in 2007 (H.R. 4336). Originally the \"Safe Aviation Flight Enhancement (SAFE) Act of 2003\" was introduced on June 26, 2003 by Congressman David Price (NC) and Congressman John Duncan (Tennessee) in a bipartisan effort to ensure investigators have access to information immediately following commercial accidents.\n\nOn July 19, 2005, a revised SAFE Act was introduced and referred to the Committee on Transportation and Infrastructure of the US House of Representatives. The bill was referred to the House Subcommittee on Aviation during the 108th, 109th, and 110th Congresses.\n\nThe NTSB has asked for the installation of cockpit image recorders in large transport aircraft to provide information that would supplement existing CVR and FDR data in accident investigations. They have recommended that image recorders be placed into smaller aircraft that are not required to have a CVR or FDR. The rationale is that what is seen on an instrument by the pilots of an aircraft is not necessarily the same as the data sent to the display device. This is particularly true of aircraft equipped with electronic displays (CRT or LCD). A mechanical instrument is likely to preserve its last indication, but this is not the case with an electronic display. Such systems, estimated to cost less than $8,000 installed, typically consist of a camera and microphone located in the cockpit to continuously record cockpit instrumentation, the outside viewing area, engine sounds, radio communications, and ambient cockpit sounds. As with conventional CVRs and FDRs, data from such a system is stored in a crash-protected unit to ensure survivability. Since the recorders can sometimes be crushed into unreadable pieces, or even located in deep water, some modern units are self-ejecting (taking advantage of kinetic energy at impact to separate themselves from the aircraft) and also equipped with radio emergency locator transmitters and sonar underwater locator beacons to aid in their location.\n\nOn March 12, 2014, in response to the missing Malaysia Airlines Flight 370, David Price re-introduced the SAFE Act in the US House of Representatives.\n\nThe disappearance of Malaysia Airlines Flight 370 demonstrated the limits of the contemporary flight recorder technology, namely how physical possession of the flight recorder device is necessary to help investigate the cause of an aircraft incident. Considering the advances of modern communication, technology commentators called for flight recorders to be supplemented or replaced by a system that provides \"live streaming\" of data from the aircraft to the ground. Furthermore, commentators called for the underwater locator beacon's range and battery life to be extended, as well as the outfitting of civil aircraft with the deployable flight recorders typically used in military aircraft. Previous to MH370, the investigators of the 2009 Air France Flight 447 urged to extend the battery life as \"rapidly as possible\" after the crash's flight recorders went unrecovered for over a year.\n\nOn December 28, 2014, Indonesia AirAsia Flight 8501, \"en route\" from Surabaya, Indonesia, to Singapore, crashed in bad weather, killing all 155 passengers and seven crew on board.\n\nOn January 12 and 13, 2015, following the recovery of the flight recorders, an anonymous ICAO representative said: \"The time has come that deployable recorders are going to get a serious look.\" Unlike military recorders, which jettison away from an aircraft, signaling their location to search and rescue bodies, recorders on commercial aircraft remain inside the fuselage. A second ICAO official said that public attention had \"galvanized momentum in favour of ejectable recorders on commercial aircraft\".\n\nThe artwork for the band Rammstein's album \"Reise, Reise\" is made to look like a CVR; it also includes a recording from a crash. The recording is from the last 1–2 minutes of the CVR of Japan Airlines Flight 123, which crashed on August 12, 1985, killing 520 people; JAL 123 is the deadliest single-aircraft disaster in history.\n\nMembers of the performing arts collective made a theatrical presentation of a play called \"Charlie Victor Romeo\" with a script based on transcripts from CVR voice recordings of nine aircraft emergencies. The play features the famous United Airlines Flight 232 that landed in a cornfield near Sioux City, Iowa after suffering a catastrophic failure of one engine and most flight controls.\n\n\"Survivor\", a novel by Chuck Palahniuk, is about a cult member who dictates his life story to a flight recorder before the plane runs out of fuel and crashes.\n\n\n\n"}
{"id": "8293740", "url": "https://en.wikipedia.org/wiki?curid=8293740", "title": "History of forensic photography", "text": "History of forensic photography\n\nForensic science holds the branch of forensic photography which encompasses documenting both suspected and convicted criminals, and also the crime scenes, victims, and other evidence needed to make a conviction. Although photography was widely acknowledged as the most accurate way to depict and document people and objects, it was not until key developments in the late 19th century that it came to be widely accepted as a forensic means of identification.\n\nForensic photography resulted from the modernization of criminal justice systems and the power of photographic realism. During the nineteenth and twentieth centuries, these two developments were significant to both forensic photography and police work in general. They can be attributed to a desire for accuracy. First, government bureaucracies became more professionalized and thus collected much more data about their citizens. Then, criminal justice systems began incorporating science into the procedures of police and judiciaries. The main reason, however, for the acceptance of police photography, is a conventional one. Other than its growing popularity, the widespread notion of photography was the prominent belief in the realism of the medium.\n\nThe earliest evidence of photographic documentation of prison inmates dates back to 1843-44 in Belgium and 1851 in Denmark. This, however, was solely experimental and was yet to be ruled by technical or legal regulations. The shots ranged from mug shot resemblances, to prisoners in their cells; and the purpose of them also varied from documentation to experimentation. There was no training required and pictures were often taken by amateurs, commercial photographers, and even policemen or prison officials.\n\nBy the 1870s, the practice had spread to many countries, though limited to larger cities. Professional photographers would then be employed to take posed portraits of the criminals. This was early evidence that led to the standard mug shot known today and was unlike any previously known portraiture. Though there was no set standard yet, there was rarely creativity employed with lighting or angle. This was not like photographing portraits of families or children. These were documenting criminals. It was one of the first times people saw portraiture being used for something other than art. Though these were slowly adapted to police regulations, photographing criminals and suspects was widespread until the latter part of the 19th century, when the process of having one's picture taken and archived was limited to individuals convicted of serious offenses. This was, of course, by discretion of the police.\n\nAs the number of criminals climbed, so too did the number of photographs. Organizing and storing the archives became a problem. Collections called, \"Rogues Galleries\" classified criminals according to types of offenses. The earliest evidence of these galleries was found in Birmingham, England in the 1850s. Shortly after this were initial attempts at standardizing the photographs.\n\nFrench photographer, Alphonse Bertillon was the first to realize that photographs were futile for identification if they were not standardized by using the same lighting, scale and angles. He wanted to replace traditional photographic documentation of criminals with a system that would guarantee reliable identification. He suggested anthropological studies of profiles and full-face shots to identify criminals. He published La Photographie Judiciaire (1890), which contained rules for a scientifically exact form of identification photography. He stated that the subjects should be well lit, photographed full face and also in profile, with the ear visible. Bertillon maintained that the precepts of commercial portraits should be forgotten in this type of photography. By the turn of the century, both his measurement system and photographic rules had been accepted and introduced in almost all states. Thus, Bertillon is credited with the invention of the mug shot. \n\nSome people believe that Bertillon's methods were influenced by crude Darwinian ideas and attempted to confirm assumptions that criminals were physically distinguishable from law-abiding citizens. It is speculated in the article, \"Most Wanted Photography,\" that it is from this system that many of the stereotype looks (skin color, eye color, hair color, body type and more) of criminals in movies, books and comics were founded. Although the measurement system was soon replaced by fingerprinting, the method of standardized photographs survived.\n\nOn the other side of the spectrum of forensic photography, is the crime photography that involves documenting the scene of the crime, rather than the criminal. Though this type of forensic photography was also created for the purpose of documenting, identifying and convicting, it allows more room for creative interpretation and variance of style. It includes taking pictures of the victim (scars, wounds, birthmarks, etc.) for the purpose of identification or conviction; and pictures of the scene (placement of objects, position of body, photos of evidence and fingerprints). The development of this type of forensic photography is responsible for radical changes in the field, including public involvement (crime photos appearing in the newspaper) and new interpretations and purposes of the field.\n\nBertillon was also the first to methodically photograph and document crime scenes. He did this both at ground level and overhead, which he called \"God's-eye-view.\" While his mug shots encourage people to find differences (from themselves) in physical characteristics of criminals, his crime scene photographs revealed similarities to the public. This made people question, when looking in a newspaper at pictures of a murder that took place in a home that resembles their own, \"could this happen to me?\" For the first time, people other than criminologists, police or forensic photographers were seeing the effects of crime through forensic photography.\n\nAmong the more famous, and arguably the most famous crime photographer, is Arthur Fellig, better known as \"Weegee\". He was known for routinely arriving at crime scenes before other reporters, or often even before the police, The nickname is speculated to come from an alternate spelling of the word \"Ouija\", implying that Fellig had a supernatural force telling where the action was going to occur. His first exhibition was a solo exhibition, entitled, \"\" and showed in 1941 at the Photo League in New York. The Museum of Modern Art purchased five of his photos and showed them in an exhibit called \"Action Photography.\" Forensic photography had now transcended mere documentation. It was considered an art. Weegee did not consider his photos art, but many perceived them that way. He is a prime example of the different purposes of forensic photography. His photographs were intended as documentation and were viewed that way in the paper by many people, but were shown in museums and seen as art by many others. His first book was published in 1945 and was titled, \"Naked City\".\n\nWith technology like digital photography becoming more common, forensic photography continues to advance and now includes many categories where specialists are required to perform more sophisticated tasks. The use of infrared and ultraviolet light is used for trace evidence photography of fingerprints, tiny blood samples and many other things. Necropsy photographs, or photographs taken both before and after the victim's clothing is removed. These photos include close-ups of scars, tattoos, wounds, teeth marks and anything else that would help in identifying the victim, or determining his or her time and cause of death. \n"}
{"id": "1300528", "url": "https://en.wikipedia.org/wiki?curid=1300528", "title": "Industrial district", "text": "Industrial district\n\nIndustrial district concept was initially used by Alfred Marshall to describe some aspects of the industrial organisation of nations. Industrial district (ID) is a place where workers and firms, specialised in a main industry and auxiliary industries, live and work. At the end of the 1990s the industrial districts in developed or developing countries had gained a recognised attention in international debates on industrialisation and policies of regional development.\n\nThe term was used the first time by Alfred Marshall in \"The Principles of Economics\" (1890, 1922). and in \"Industry and Trade\". Marshall talks of a... \"thickly peopled industrial district\".\n\nThe term was also used in political struggle. The 1917 handbook of the Industrial Workers of the World states:-\n\nThe term also appears in English literature. For instance, in a short story of 1920 by D. H. Lawrence, \"You Touched Me\" (aka 'Hadrian'):-\n\nThe strong specialisation of the workers and an appropriate support of public goods and institutions are supported by an \"Industrial Atmosphere\" related to a locally developing division of labour. Competences and knowledge are shared in informal way with processes of learning by doing and learning by using, and this promotes innovation over time. Local firms, families and civic organisations are connected by way of both market mechanisms and non-market mechanisms, like trust within bilateral or team exchanges, and collective action supporting the availability of local industrial, social and environmental infrastructure. Also, the notion that firms located in geographical proximity benefit from agglomeration effects in having a common or collective infrastructure is frequently mentioned as one of the main bases in the industrial district literature.\n\nWithin the study of economics, the term has evolved. Giacomo Becattini rediscovered the concept to describe the Italian industrial configuration of the middle of the 20th century. Since the 1980s, the dynamic industrial development in NEC (North, East and Centre) of Italy, where after the Second World War geographical concentration of specialised small and medium-sized enterprises (SME) raised up, led to an increasing attention to the Marshall' seminal works. A growing literature with an accompanying cloud of definitions of what is meant as an industrial district characterised the international dabate, e.g. Cluster. Industrial districts in Italy have a coherent location and a narrow specialisation profile, e.g. Prato in woollen fabric, Sassuolo in ceramic tiles or Brenta in ladies' footwear.\n\nThe success of SME-based Italian districts in the last century and the alternate fortunes of the current ones led to investigate more thoroughly some related aspects. The general characteristics of the ID are consistent with gradual change supported by processes of innovation from below, or decentralized industrial creativity. However, the globalisation processes asked non-gradual changes to the historical IDs and technical and organisational difficulties could hit them.\n\nIn the Industry 4.0 era, the specialised capabilities of these area seem to have possibility to encourage the emergence of the New Artisans, Maker in the context of adapted models like the \"ID mark 3.0\".\n\n"}
{"id": "39951275", "url": "https://en.wikipedia.org/wiki?curid=39951275", "title": "Jablotron", "text": "Jablotron\n\nJablocom is a Czech company that produces mobile phones. In 2005 it was settled as mobile spinoff of the Czech electrotechnical holding Jablotron Group. In 2017 company was sold by Jablotron to private hands (to one of its founders) and in spring 2018 it was renamed to NOABE s.r.o. Nevertheless Jablocom brand is still used on all products.\nJablocom aroused in 2005 a great interest in its model \"the world's largest mobile phone\". Mobile desktop phones (mobile phone in the shape of a desk phone) was originally designed for old people, however, it aroused great interest in regions with poor condition of landline and at mobile operators, who see it as another tool in competition with fixed lines.\nToday majority of phones are used in offices and other work places. Phone designs changed accordingly, in 2014 first desktop smartphone with Android was introduced and cloud administration tools for all current Jablocom phones was launched.\n\n\n"}
{"id": "5430766", "url": "https://en.wikipedia.org/wiki?curid=5430766", "title": "John Paul Mitchell Systems", "text": "John Paul Mitchell Systems\n\nJohn Paul Mitchell Systems (\"JPMS\") is an American manufacturer of hair care products and styling tools through several brands including Paul Mitchell, Tea Tree, and MITCH. It was founded in 1980 by John Paul DeJoria and Paul Mitchell. The company was formerly located in Beverly Hills, California; its world headquarters is now in Century City, California, with its operations facility in Santa Clarita, California. The company is also well known for its Paul Mitchell Schools. With 109 locations throughout the United States, Paul Mitchell Schools is the largest cosmetology and barber school franchise globally.\nJPMS was the first professional hair care company to publicly oppose animal testing, and remains privately owned and independent.\n\nWith just $700, John Paul DeJoria and Paul Mitchell launch JPMS. The first Paul Mitchell products are Shampoo One, Shampoo Two, and The Conditioner, which debut in black-and-white packaging since color printing was too expensive at the time. \n\nJohn Paul Mitchell Systems establishes a self-sustaining, solar-powered awapuhi farm in Hawaii—all of the awapuhi used in its products is still harvested there today.\n\nPaul Mitchell stars in a national advertising campaign with the tagline, “This man does not want you to buy his products. Anywhere.” The ad firmly establishes the company’s commitment to the professional beauty industry and fighting diversion.\n\nPaul Mitchell enters the world of lifestyle sports marketing and sponsors Team Paul Mitchell Karate, a martial arts team based in Rhode Island. \n\nPaul Mitchell passes away at age 53 after losing his battle with pancreatic cancer. His share of the company is passed to his son, Angus Mitchell.\n\nThe company receives a Humanitarian Award from People magazine for its ethical treatment of animals.\n\nPaul Mitchell introduces Paul Mitchell the color, their first professional hair color brand.\n\nThe first Paul Mitchell School opens its doors in Costa Mesa, California and sets a new standard for education in the pro beauty industry.\n\nPaul Mitchell revolutionizes the blowout and reduces drying time with the launch of Super Skinny Serum (still one of the company’s bestselling products).\n\nThe Tea Tree brand partners with leading environmental organizations and pledges to help make the planet a greener place.\n\nPaul Mitchell Schools raise $1.5 million for various charities at its annual FUNraising Gala, bringing its 14-year total to $18.2 million.\n\n"}
{"id": "18446887", "url": "https://en.wikipedia.org/wiki?curid=18446887", "title": "Key finder", "text": "Key finder\n\nKey finders, also known as keyfinders, key locators or electronic finders, are small electronic devices used to recover misplaced or lost key sets. Their goal is to reduce the time it takes to locate keys or other personal items, while remaining unobtrusive. Some key finders beep on-demand.\n\nThe first generation key finders were sound based, and listened for a clap or a whistle (or a sequence of same) then beeped to the user to find them. Determining what was a clap or a whistle proved difficult, resulting in poor performance and false alarms. As a result of this low quality and reliability, these early key finders were soon discarded and were unpopular for serious needs.\n\nSecond generation key finders eliminated the problem of false alarms, and some have fairly long battery lives. As electronics became smaller and cheaper, radio became viable to locate the keys which were fitted with a small receiver. A separate transmitter is used to activate the receiver(s). All wireless key finders have to \"listen\" for a searching transmission, resulting in battery replacement at intervals ranging from 3 months to a year. Some distributors include a cost-effective key-return service that assists in returning the keys should they be lost in a taxi, bus or other public place, provided the customer registered their details. The transmitter can also contain information to help return the card to its rightful owner.\n\nThird generation key finders no longer require a separate \"base\"—they are all functionally identical and based on a peer-to-peer system where each can find all the others individually. The user can, for example, use a wallet to find misplaced keys and vice versa, or a mobile phone to find a lost TV remote control or glasses. In addition, since each has its own transmitter, it can reply by radio as well as beeping and flashing a light to attract attention. The seeking unit can then follow this beacon to find even a buried set of keys. Having a transmitter in each unit also means that, unlike second generation units, losing the single transmitter does not result in total loss of all items.\n\nA typical usage is to have the key finder device attached to a set of keys, and if these keys are lost or misplaced an action is performed to initiate the search functionality (such as clapping or pressing a button on the locator device). Modern key finder devices are able to: 1) generate a distinct sound, such as a beep or tone; 2) flash a light, or; 3) somehow guide a user towards the lost item.\n\nKey finders have also been found to be useful for the visually challenged, as well as those with memory problems associated with Alzheimer's disease or problems paying attention typical of attention deficit hyperactivity disorder (ADHD). Military veterans suffering from post-traumatic stress disorder (PTSD) are finding key finders useful in combating the memory effects of the disease.\n\nModern life requires most urban dwellings, cars and offices to be generally accessed via a key. The stress, cost and inconvenience of losing one's key to those environments is high. Key finders are a practical way of alleviating these effects.\n\n\n\n"}
{"id": "3069948", "url": "https://en.wikipedia.org/wiki?curid=3069948", "title": "Letter scale", "text": "Letter scale\n\nA letter scale is a weighing scale used for weighing letters in order to determine the correct amount of postage. Until the 1990s most letter scales were mechanical, but today electronic scales are the most common.\nArticle Comparing and explaining letter scales\n"}
{"id": "7352502", "url": "https://en.wikipedia.org/wiki?curid=7352502", "title": "Manfrotto", "text": "Manfrotto\n\nManfrotto is a brand of camera and lighting supports, including tripods, monopods, and other accessories, that is manufactured by Lino Manfrotto + Co. Spa, a company headquartered in Cassola, Italy. Manfrotto is owned by the UK company \"The Vitec Group\" (Vinten), which purchased Lino Manfrotto + Co. Spa in 1989.\n\nIn 2010 Manfrotto established the Manfrotto School of Xcellence, an educational resource to support and help everyone to get closer to photography and videography.\n\nLino Manfrotto, an Italian photojournalist, began designing and selling light stands, booms, and telescopic rods under the name \"Manfrotto\" in the late 1960s. In 1972 Lino Manfrotto met Gilberto Battocchio, a technician working for a Bassano mechanical firm. With the collaboration, the company introduced its first tripod in 1974. By 1986 Manfrotto already had six manufacturing plants in Bassano, and in the following two years they would build five more plants in Villapaiera, the industrial zone of Feltre.\n\nVitec Group Plc. purchased Manfrotto in 1989, followed by the French company Gitzo in 1992 and the American company Bogen Photo Corp. in 1993. Vitec Group Plc. chose to maintain the brands as separate lines in its portfolio. Manfrotto products are distributed in Germany, France, Italy, Japan, the United Kingdom and the United States by Manfrotto Distribution. In Canada, Manfrotto products are distributed by Gentec International.\n\nLino Manfrotto died on February 5, 2017, at the age of 80.\n\nIn 2010 Manfrotto changed its brand slogan from \"Manfrotto Proven Professional\" to \"Manfrotto Imagine More\". In May 2011 Manfrotto launched an online competition asking people to share short posts about imagination on Twitter and Facebook for the Manfrotto Imagine More Manifesto, as well as to submit pictures representing imagination on the Manfrotto Imagine More website. A short film was then created based around the winning posts and pictures, debuting at the 68th Venice International Film Festival. In July 2012 Manfrotto launched the Manfrotto Imagine More blog, a web resource with tips on how to get the best out of pictures and how to share them with other people more effectively.\n\nManfrotto's main competition is from such brands as Benro, Slik and Velbon on the full range, and on the high end from Miller and Gitzo (which, as noted above, is part of the same group).\n\n"}
{"id": "21024709", "url": "https://en.wikipedia.org/wiki?curid=21024709", "title": "Margaret Hutchinson Rousseau", "text": "Margaret Hutchinson Rousseau\n\nMargaret Hutchinson Rousseau (October 27, 1910 – January 12, 2000) was an American chemical engineer who designed the first commercial penicillin production plant. She was the first female member of the American Institute of Chemical Engineers.\n\nMargaret Hutchinson was born in 1910 in Houston, Texas, the daughter of a clothing store owner. She received her Bachelor of Science degree from Rice Institute in 1932 and her Doctor of Science degree in chemical engineering from MIT in 1937, the first woman to earn a doctorate in the subject in the USA. Her thesis topic was \"The effect of solute on the liquid film resistance in gas absorption.\"\n\nOn May 1, 1939, she married William Caubu Rousseau, a co-worker at E.B. Badger & Sons, who was later a chemical engineering lecturer at Massachusetts Institute of Technology (MIT). They had one son, William.\n\nShe died January 12, 2000, aged 89, at her home in Weston, Massachusetts.\n\nHutchinson started her professional career with E. B. Badger (where she met her husband-to-be). During the Second World War she oversaw the design of production plants for the strategically important materials of penicillin and synthetic rubber. Her development of deep-tank fermentation of Penicillium mold enabled large-scale production of penicillin. She worked on the development of high-octane gasoline for aviation fuel. Her later work included improved distillation column design and plants for the production of ethylene glycol and glacial acetic acid.\n\nShe retired in 1961 and became an overseer of the Boston Symphony Orchestra.\n\nIn 1945, Hutchinson became the first woman to be accepted as a member of the American Institute of Chemical Engineers.\n\nIn 1955 she received the Achievement Award of the Society of Women Engineers.\n\nIn 1983 she was the first female recipient of the prestigious Founders Award of the AIChE.\n\n"}
{"id": "6885779", "url": "https://en.wikipedia.org/wiki?curid=6885779", "title": "Mobile radio", "text": "Mobile radio\n\nMobile radio or mobiles refer to wireless communications systems and devices which are based on radio frequencies(using commonly UHF or VHF frequencies), and where the path of communications is movable on either end. There are a variety of views about what constitutes mobile equipment. For US licensing purposes, mobiles may include hand-carried, (sometimes called \"portable\"), equipment. An obsolete term is radiophone.\n\nA sales person or radio repair shop would understand the word \"mobile\" to mean vehicle-mounted: a transmitter-receiver (transceiver) used for radio communications from a vehicle. Mobile radios are mounted to a motor vehicle usually with the microphone and control panel in reach of the driver. In the US, such a device is typically powered by the host vehicle's 12 volt electrical system.\n\nSome mobile radios are mounted in aircraft (aeronautical mobile), shipboard (maritime mobile), on motorcycles, or railroad locomotives. Power may vary with each platform. For example, a mobile radio installed in a locomotive would run off of 72- or 30-volt DC power. A large ship with 117V AC power might have a base station mounted on the ship's bridge.\n\nThe distinction between radiotelephones and two-way radio is becoming blurred as the two technologies merge. The backbone or infrastructure supporting the system defines which category or taxonomy applies. A parallel to this concept is the convergence of computing and telephones.\n\n\"Radiotelephones\" are full-duplex (simultaneous talk and listen), circuit switched, and primarily communicate with telephones connected to the public switched telephone network. The connection sets up based on the user dialing. The connection is taken down when the \"end\" button is pressed. They run on telephony-based infrastructure such as AMPS or GSM.\n\n\"Two-way radio\" is primarily a dispatch tool intended to communicate in simplex or half-duplex modes using push-to-talk, and primarily intended to communicate with other radios rather than telephones. These systems run on push-to-talk-based infrastructure such as Nextel's iDEN, Specialized Mobile Radio (SMR), MPT-1327, Enhanced Specialized Mobile Radio (ESMR) or conventional two-way systems. Certain modern two-way radio systems may have full-duplex telephone capability.\n\nEarly users of mobile radio equipment included transportation and government. These systems used one-way broadcasting instead of two-way conversations. Railroads used medium frequency range (MF) communications (similar to the AM broadcast band) to improve safety. Instead of hanging out of a locomotive cab and grabbing train orders while rolling past a station, voice communications with rolling trains became possible. Radios linked the caboose with the locomotive cab. Early police radio systems were initially one way using MF frequencies above the AM broadcast band, (1.7 MHz). Some early systems talked back to dispatch on a 30-50 MHz link, (called \"crossband\").\n\nEarly mobile radios used amplitude modulation (AM) to convey intelligence through the communications channel. In time, problems with sources of electrical noise showed that frequency modulation (FM) was superior for its ability to cope with vehicle ignition and power line noise. The frequency range used by most early radio systems, 25 to 50 MHz (vhf \"low band\") is particularly susceptible to the problem of electrical noise. This plus the need for more channels led to the eventual expansion of two-way radio communications into the VHF \"high band\" (150 to 174 MHz) and UHF (450 to 470 MHz). The UHF band has since been expanded again.\n\nOne of the major challenges in early mobile radio technology was that of converting the six or twelve volt power supply of the vehicle to the high voltage needed to operate the vacuum tubes in the radio. Early tube-type radios used dynamotors - essentially a six or twelve volt motor that turned a generator to provide the high voltages required by the vacuum tubes. Some early mobile radios were the size of a suitcase or had separate boxes for the transmitter and receiver. As time went on, power supply technology evolved to use first electromechanical vibrators, then solid-state power supplies to provide high voltage for the vacuum tubes. These circuits, called \"inverters\", changed the 6V or 12V direct current (DC) to alternating current (AC) which could be passed through a transformer to make high voltage. The power supply then rectified this high voltage to make the high voltage DC required for the vacuum tubes, (called \"valves\" in British English). The power supplies needed to power vacuum tube radios resulted in a common trait of tube-type mobile radios: their heavy weight due to the iron-core transformers in the power supplies. These high voltage power supplies were inefficient, and the filaments of the vacuum tubes added to current demands, taxing vehicle electrical systems. Sometimes, a generator or alternator upgrade was needed to support the current required for a tube-type mobile radio.\n\nExamples of US 1950s-1960s tube-type mobile radios with no transistors:\n\nEquipment from different US manufacturers had similar traits. This was partly dictated by Federal Communications Commission (FCC) regulations. The requirement that unauthorized persons be prohibited from using the radio transmitter meant that many radios were wired so they could not transmit unless the vehicle ignition was on. Persons without a key to the vehicle could not transmit. Equipment had to be \"type accepted\", or technically approved, by the FCC before it could be offered for sale. In order to be type accepted, the radio set had to be equipped with an indicator light, usually green or yellow, that showed power was applied and the radio was ready to transmit. Radios were also required to have a lamp (usually red) indicating when the transmitter was on. These traits continue in the design of modern radios.\n\nEarly tube-type radios operated on 50 kHz channel spacing with plus-or-minus fifteen kilohertz modulation deviation. This meant that the number of radio channels that could be accommodated in the available radio frequency spectrum were limited to a certain number, dictated by the bandwidth of the signal on each channel.\n\nSolid state equipment arrived in the 1960s, with more efficient circuitry and smaller size. Channel spacing narrowed to 20–30 kHz with modulation deviation dropping to plus-or-minus five kilohertz. This was done to allow more radio spectrum availability to accommodate the rapidly growing national group of two-way radio users. By the mid 1970s, tube-type transmitter power amplifiers had been replaced with high-power transistors. From the 1960s to the 1980s, large system users with specialized requirements often had custom built radios designed for their unique systems. Systems with multiple-CTCSS tone encoders and more than two channels were unusual. Manufacturers of mobile radios built customized equipment for large radio fleets such as the California Department of Forestry and the California Highway Patrol.\n\nExamples of US hybrid partially solid state mobile radios:\n\nCustom design for a particular customer is a thing of the past. Modern mobile radio equipment is \"feature rich\". A mobile radio may have 100 or more channels, be microprocessor controlled and have built-in options such as unit ID. A computer and software is typically required to program the features and channels of the mobile radio. Menus of options may be several levels deep and offer a complicated array of possibilities. Some mobile radios have alphanumeric displays that translate channel numbers (F1, F2) to a phrase more meaningful to the user, such as \"Providence Base\", \"Boston Base\", etc. Radios are now designed with a myriad of features to preclude the need for custom design.\n\nExamples of US microprocessor-controlled mobile radios:\n\nAs use of mobile radio equipment has virtually exploded, channel spacing has had to be narrowed again to 12.5–15 kHz with modulation deviation dropping to plus-or-minus 2.5 kilohertz. In order to fit into smaller, more economical vehicles, today's radios are trending toward radically smaller sizes than their tube-type ancestors.\nThe traditional analogue radio communications have been surpassed by digital radio voice communications capabilities(examples:Project 25(APCO-25), Terrestrial Trunked Radio (TETRA), DMR) that provide greater clarity of transmission, enable security features such as encryption and, within the network, allow low band data transmissions to accommodate simple text or picture messaging as an example.\n\nCommercial and professional mobile radios are often purchased from an equipment supplier or dealer whose staff will install the equipment into the user's vehicles. Large fleet users may buy radios directly from an equipment manufacturer and may even employ their own technical staff for installation and maintenance.\n\nA modern mobile radio consists of a radio transceiver, housed in a single box, and a microphone with a push-to-talk button. Each installation would also have a vehicle-mounted antenna connected to the transceiver by a coaxial cable. Some models may have an external, separate speaker which can be positioned and oriented facing the driver to overcome ambient road noise present when driving. The installer would have to locate this equipment in a way that does not interfere with the vehicle's sun roof, electronic engine management system, vehicle stability computer, or air bags.\n\nMobile radios installed on motorcycles are subject to extreme vibration and weather. Professional equipment designed for use on motorcycles is weather and vibration resistant. Shock mounting systems are used to reduce the radio's exposure to vibration imparted by the motorcycle's modal, or resonant, shaking.\n\nSome mobile radios use noise-canceling microphones or headsets. At speeds over 100 MPH, the ambient road and wind noise can make radio communications difficult to understand. For example, California Highway Patrol mobile radios have noise-canceling microphones which reduce road and siren noise heard by the dispatcher. Most fire engines and radios in heavy equipment use noise-canceling headsets. These protect the occupant's hearing and reduce background noise in the transmitted audio. Noise-canceling microphones require the operator speak directly into the front of the microphone. Hole arrays in the back of the microphone pick up ambient noise. This is applied, out-of-phase, to the back of the microphone, effectively reducing or canceling any sound which is present both in front and back of the microphone. Ideally, only the voice present on the front side of the microphone goes out on the air.\n\nMany radios are equipped with transmitter time-out timers which limit the length of a transmission. A bane of push-to-talk systems is the stuck microphone: a radio locked on transmit which disrupts communications on a two-way radio system. One example of this problem occurred in a car with a concealed two-way radio installation where the microphone and coiled cord were hidden inside the glove box. An operator tossed the mike into the glove box and shut it, causing the push-to-talk button to be depressed and locking the transmitter on. On taxi systems, a driver may be upset when a dispatcher assigns a call (s)he wanted to another driver and may deliberately hold the transmit button down (for which the owner can be fined by the FCC). Radios with time-out timers transmit for the preset amount of time, usually 30- or 60-seconds, after which the transmitter automatically turns off and a loud tone comes out of the radio speaker. The volume level of the tone on some radios is loud and cannot be adjusted. As soon as the push-to-talk button is released, the tone stops and the timer resets.\n\nMobile radio equipment is manufactured to specifications developed by the Electronic Industries Association/Telecommunications Industry Association (EIA/TIA). These specifications have been developed to help assure the user that mobile radio equipment performs as expected and to prevent the sale and distribution of inferior equipment which could degrade communications.\n\nA mobile radio must have an associated antenna. The most common antennas are stainless steel wire or rod whips which protrude vertically from the vehicle. Physics defines the antenna length: length relates to frequency and cannot be arbitrarily lengthened or shortened (more likely) by the end user. The standard \"quarter wave\" antenna in the 25-50 MHz range can be over nine feet long. A 900 MHz antenna may be three inches long for a quarter wavelength. A transit bus may have a ruggedized antenna, which looks like a white plastic blade or fin, on its roof. Some vehicles with concealed radio installations have antennas designed to look like the original AM/FM antenna, a rearview mirror, or may be installed inside windows, or hidden on the floor pan or underside of a vehicle. Aircraft antennas look like blades or fins, the size and shape being determined by frequencies used. Microwave antennas may look like flat panels on the aircraft's skin. Temporary installations may have antennas which clip on to vehicle parts or are attached to steel body parts by a strong magnet.\n\nThough initially relatively inexpensive mobile radio system components, frequently damaged antennas can be costly to replace since they are usually not included in maintenance contracts for mobile radio fleets. Some types of vehicles in 24-hour use, with stiff suspensions, tall heights, or rough diesel engine idle vibrations may damage antennas quickly. The location and type of antenna can affect system performance drastically. Large fleets usually test a few vehicles before making a commitment to a certain antenna location or type.\n\nUS Occupational Safety and Health Administration guidelines for non-ionizing radio energy generally say the radio antenna must be two feet from any vehicle occupants. (Read the OSHA guidelines before attempting to install an antenna.) This rule of thumb is intended to result in passengers being exposed to safe levels of radio frequency energy in the event the radio transmits.\n\nDispatch-reliant services, such as tow cars or ambulances, may have several radios in each vehicle. For example, tow cars may have one radio for towing company communications and a second for emergency road service communications. Ambulances may have a similar arrangement with one radio for government emergency medical services dispatch and one for company dispatch.\n\nUS ambulances often have radios with dual controls and dual microphones allowing the radio to be used from the patient care area in the rear or from the vehicle's cab.\n\nBoth tow cars and ambulances may have an additional radio which transmits and receives to support a mobile data terminal. A data terminal radio allows data communications to take place over the separate radio. In the same way that a facsimile machine has a separate phone line, this means data and voice communication can take place simultaneously over a separate radio. Early Federal Express (FedEx) radio systems used a single radio for data and voice. The radio had a \"request-to-speak\" button which, when acknowledged, allowed voice communication to the dispatch center.\n\nEach radio works over a single band of frequencies. If a tow car company had a frequency on the same band as its auto club, a single radio with scanning might be employed for both systems. Since a mobile radio typically works on a single frequency band, multiple radios may be required in cases where communications take place over systems on more than one frequency band.\n\nIntended as a cost savings, some systems employ vehicular chargers instead of a mobile radio. Each radio user is issued a walkie talkie. Each vehicle is equipped with a charger system console. The walkie talkie inserted into a vehicular charger or converter while the user is in the vehicle. The charger or converter 1) connects the walkie talkie to the vehicle's two-way radio antenna, 2) connects an amplified speaker, 3) connects a mobile microphone, and 4) charges the walkie talkie's battery. The weak point of these systems has been connector technology which has been proven unreliable in some installations. Receiver performance is a problem in congested radio signal and urban areas. These installations are sometimes referred to as \"jerk-and-run\" systems.\n"}
{"id": "19806932", "url": "https://en.wikipedia.org/wiki?curid=19806932", "title": "ModeMapping", "text": "ModeMapping\n\nModeMapping is a research technique developed by Stuart Karten Design (SKD), a Los Angeles, United States, based industrial design firm. It is a method of interpreting standard consumer research to uncover areas of unmet needs.\n\nIt can be understood as a visualization tool that tracks the state of mind of consumers over time. Designers can then look for patterns by using a colorcoding system of categorizing these states of mind, or “modes,” that describe activity (“work mode” or “play mode”). The color-coded patterns of consumer behavior allow designers to look for shared experiences and then use these observations to suggest solutions that will appeal to customers. This also creates a topography of sorts of consumer activities as well as how they are thinking and feeling during those activities.\n\nSKD has used ModeMapping to drive product lines for companies including Johnson Controls and Avery Dennison. For example, for Johnson Controls, SKD found that the drivers they observed all made quick, frequent transitions from role to role (parent at school, friend meeting peers at a restaurant) throughout the day. Seeing this pattern led the designers to suggest products such as a modular storage system that can easily be loaded into a vehicle and a reminder system (using RFID tags) that would alert drivers when important items (say, kids' sports equipment before a game) wasn’t brought into the car.\n\nModeMapping has won a Silver International Design Excellence Award in 2006.\n\n\"BusinessWeek\" featured ModeMapping as an “innovation tool worth trying now.”\n\n"}
{"id": "324498", "url": "https://en.wikipedia.org/wiki?curid=324498", "title": "Mortar (masonry)", "text": "Mortar (masonry)\n\nMortar is a workable paste used to bind building blocks such as stones, bricks, and concrete masonry units together, fill and seal the irregular gaps between them, and sometimes add decorative colors or patterns in masonry walls. In its broadest sense mortar includes pitch, asphalt, and soft mud or clay, such as used between mud bricks. \"Mortar\" comes from Latin \"mortarium\" meaning crushed.\n\nCement mortar becomes hard when it cures, resulting in a rigid aggregate structure; however the mortar is intended to be weaker than the building blocks and the sacrificial element in the masonry, because the mortar is easier and less expensive to repair than the building blocks. Mortars are typically made from a mixture of sand, a binder, and water. The most common binder since the early 20th century is Portland cement but the ancient binder lime mortar is still used in some new construction. Lime and gypsum in the form of plaster of Paris are used particularly in the repair and repointing of buildings and structures because it is important the repair materials are similar to the original materials. The type and ratio of the repair mortar is determined by a \"mortar analysis\". There are several types of cement mortars and additives.\n\nThe first mortars were made of mud and clay. Because of a lack of stone and an abundance of clay, Babylonian constructions were of baked brick, using lime or pitch for mortar. According to Roman Ghirshman, the first evidence of humans using a form of mortar was at the Mehrgarh of Baluchistan in Pakistan, built of sun-dried bricks in 6500 BCE. The ancient sites of Harappan civilization of third millennium BCE are built with kiln-fired bricks and a \"gypsum mortar\". Gypsum mortar, also called plaster of Paris, was used in the construction of the Egyptian pyramids and many other ancient structures. It is made from gypsum, which requires a lower firing temperature. It is therefore easier to make than lime mortar and sets up much faster which may be a reason it was used as the typical mortar in ancient, brick arch and vault construction. Gypsum mortar is not as durable as other mortars in damp conditions.\n\nIn early Egyptian pyramids, which were constructed during the Old Kingdom (~2600–2500 BCE), the limestone blocks were bound by mortar of mud and clay, or clay and sand. In later Egyptian pyramids, the mortar was made of either gypsum or lime. Gypsum mortar was essentially a mixture of plaster and sand and was quite soft.\n\nIn the Indian subcontinent, multiple cement types have been observed in the sites of the Indus Valley Civilization, such as the Mohenjo-daro city-settlement that dates to earlier than 2600 BCE. Gypsum cement that was \"\"light grey and contained sand, clay, traces of calcium carbonate, and a high percentage of lime\" was used in the construction of wells, drains and on the exteriors of \"important looking buildings\".\" Bitumen mortar was also used at a lower-frequency, including in the Great Bath at Mohenjo-daro.\n\nHistorically, building with concrete and mortar next appeared in Greece. The excavation of the underground aqueduct of Megara revealed that a reservoir was coated with a pozzolanic mortar 12 mm thick. This aqueduct dates back to c. 500 BCE. Pozzolanic mortar is a lime based mortar, but is made with an additive of volcanic ash that allows it to be hardened underwater; thus it is known as hydraulic cement. The Greeks obtained the volcanic ash from the Greek islands Thira and Nisiros, or from the then Greek colony of Dicaearchia (Pozzuoli) near Naples, Italy. The Romans later improved the use and methods of making what became known as pozzolanic mortar and cement. Even later, the Romans used a mortar without pozzolana using crushed terra cotta, introducing aluminum oxide and silicon dioxide into the mix. This mortar was not as strong as pozzolanic mortar, but, because it was denser, it better resisted penetration by water.\n\nHydraulic mortar was not available in ancient China, possibly due to a lack of volcanic ash. Around 500 CE, sticky rice soup was mixed with slaked lime to make an inorganic−organic composite sticky rice mortar that had more strength and water resistance than lime mortar.\n\nIt is not understood how the art of making hydraulic mortar and cement, which was perfected and in such widespread use by both the Greeks and Romans, was then lost for almost two millennia. During the Middle Ages when the Gothic cathedrals were being built, the only active ingredient in the mortar was lime. Since cured lime mortar can be degraded by contact with water, many structures suffered from wind blown rain over the centuries.\n\nOrdinary Portland cement mortar, commonly known as OPC mortar or just cement mortar, is created by mixing powdered Ordinary Portland Cement, fine aggregate and water.\n\nIt was invented in 1794 by Joseph Aspdin and patented on 18 December 1824, largely as a result of efforts to develop stronger mortars. It was made popular during the late nineteenth century, and had by 1930 became more popular than lime mortar as construction material. The advantages of Portland cement is that it sets hard and quickly, allowing a faster pace of construction. Furthermore, fewer skilled workers are required in building a structure with Portland cement.\n\nAs a general rule, however, Portland cement should not be used for the repair or repointing of older buildings built in lime mortar, which require the flexibility, softness and breathability of lime if they are to function correctly.\nIn the United States and other countries, five standard types of mortar (available as dry pre-mixed products) are generally used for both new construction and repair. Strengths of mortar change based on the ratio of cement, lime, and sand used in mortar. The ingredients and the mix ratio for each type of mortars are specified under the ASTM standards. These premixed mortar products are designated by one of the five letters, M, S, N, O, and K. Type M mortar is the strongest, and Type K the weakest. These type letters are apparently taken from the alternate letters of the words \"MaSoN wOrK\".\n\nPolymer cement mortars (PCM) are the materials which are made by partially replacing the cement hydrate binders of conventional cement mortar with polymers. The polymeric admixtures include latexes or emulsions, redispersible polymer powders, water-soluble polymers, liquid thermoset resins and monomers. It has low permeability, and it reduces the incidence of drying shrinkage cracking, mainly designed for repairing concrete structures. One brand of PCM is MagneLine.\n\nThe setting speed can be increased by using impure limestone in the kiln, to form a hydraulic lime that will set on contact with water. Such a lime must be stored as a dry powder. Alternatively, a pozzolanic material such as calcined clay or brick dust may be added to the mortar mix. Addition of a pozzolanic material will make the mortar set reasonably quickly by reaction with the water.\n\nIt would be problematic to use Portland cement mortars to repair older buildings originally constructed using lime mortar. Lime mortar is softer than cement mortar, allowing brickwork a certain degree of flexibility to adapt to shifting ground or other changing conditions. Cement mortar is harder and allows little flexibility. The contrast can cause brickwork to crack where the two mortars are present in a single wall.\n\nLime mortar is considered breathable in that it will allow moisture to freely move through and evaporate from the surface. In old buildings with walls that shift over time, cracks can be found which allow rain water into the structure. The lime mortar allows this moisture to escape through evaporation and keeps the wall dry. Re−pointing or rendering an old wall with cement mortar stops the evaporation and can cause problems associated with moisture behind the cement.\n\nPozzolana is a fine, sandy volcanic ash. It was originally discovered and dug at Pozzuoli, nearby Mount Vesuvius in Italy, and was subsequently mined at other sites, too. The Romans learned that pozzolana added to lime mortar allowed the lime to set relatively quickly and even under water. Vitruvius, the Roman architect, spoke of four types of pozzolana. It is found in all the volcanic areas of Italy in various colours: black, white, grey and red. Pozzolana has since become a generic term for any siliceous and/or aluminous additive to slaked lime to create hydraulic cement.\n\nFinely ground and mixed with lime it is a hydraulic cement, like Portland cement, and makes a strong mortar that will also set under water.\n\nFirestop mortars are mortars most typically used to firestop large openings in walls and floors required to have a fire-resistance rating. They are passive fire protection items. Firestop mortars differ in formula and properties from most other cementitious substances and cannot be substituted with generic mortars without violating the listing and approval use and compliance.\n\nFirestop mortar is usually a combination of powder mixed with water, forming a cementatious stone which dries hard. It is sometimes mixed with lightweight aggregates, such as perlite or vermiculite. It is sometimes pigmented to distinguish it from generic materials in an effort to prevent unlawful substitution and to enable verification of the certification listing.\n\nAs the mortar hardens, the current atmosphere is encased in the mortar and thus provides a sample for analysis. Various factors affect the sample and raise the margin of error for the analysis.\nThe possibility to use radiocarbon dating as a tool for mortar dating was introduced as early as the 1960s, soon after the method was established (Delibrias and Labeyrie 1964; Stuiver and Smith 1965; Folk and Valastro 1976). The very first data were provided by van Strydonck et al. (1983), Heinemeier et al.(1997) and Ringbom and Remmer (1995). Than the methodological aspect were developed by different groups (an international team headed by Åbo Akademi University, and teams from CIRCE, CIRCe, ETHZ, Poznań, RICH and Milano-Bicocca laboratory. \nTo evaluate the different anthropogenic carbon extraction methods for radiocarbon dating as well as to compare the different dating methods, i.e. radiocarbon and OSL, the first intercomparison study (MODIS) was set up and published in 2017.\n\n\n"}
{"id": "402105", "url": "https://en.wikipedia.org/wiki?curid=402105", "title": "NASA Earth Observatory", "text": "NASA Earth Observatory\n\nNASA Earth Observatory is an online publishing outlet for NASA which was created in 1999. It is the principal source of satellite imagery and other scientific information pertaining to the climate and the environment which are being provided by NASA for consumption by the general public. It is funded with public money, as authorized by the United States Congress, and is part of the EOS Project Science Office located at Goddard Space Flight Center.\n\n\n"}
{"id": "29278882", "url": "https://en.wikipedia.org/wiki?curid=29278882", "title": "Osmothèque", "text": "Osmothèque\n\nThe Osmothèque (from Greek \"osmē\" \"scent\" patterned on French \"bibliothèque\" \"library\") is the world’s largest scent archive, a leading international research institution tracing the history of perfumery, based in Versailles with conference centers in New York City and Paris. Founded in 1990 by Jean Kerléo and other senior perfumers including Jean-Claude Ellena and Guy Robert, the Osmothèque is internationally responsible for the authentication, registration, preservation, documentation and reproduction of thousands of perfumes gathered from the past two millennia, archived at the Osmothèque repository and consultable by the public.\n\nExclusive to the collection are countless rare masterpieces elsewhere discontinued or reformulated, including François Coty’s \"Chypre\" (Coty), Paul Parquet’s \"Fougère Royale\" (Houbigant) and Aimé Guerlain's \"Jicky\" (Guerlain) as well as numerous personalized fragrances worn by historical figures such as Elizabeth of Poland, Napoleon and Eugénie de Montijo. Since 2008, Patricia de Nicolaï has served as the institution’s president.\n\nThe founding of the world’s first international scent archive was initially proposed to the Société Française des Parfumeurs in 1976 by Jean Kerléo, then head perfumer at Jean Patou, in an effort to formally record and preserve the history of perfumery. Kerléo envisioned reconstituting various discontinued classics according to their original formulae, working in collaboration with the world’s foremost perfumers and perfume houses. An advisory committee was thus assembled, composed of experts Jean-François Blayn, Raymond Chaillan, Jean-Claude Ellena, Yuri Gutsatz, Jeannine Mongin, Raymond Pouliquen, Guy Robert and Henri Sebag.\n\nAfter successfully reproducing the discontinued perfumes of Jean Patou, Jean Kerléo and his team were entrusted in 1986 with the formulae of the defunct F. Millot, among them the 1925 classic \"Crêpe de Chine\" by Jean Desprez. Kerléo’s reconstitution, completed a year later with perfumer Aimable Duhayon, impressed many within the industry, a major catalyst for the launch of the proposed scent archive. When in 1988 the project won the support of both the Chambre de Commerce et d’Industrie de Versailles and the Comité Français du Parfum, a repository facility was provided on the premises of the Institut Supérieur International du Parfum, de la Cosmétique et de l'Aromatique alimentaire. There the Osmothèque was officially founded on 26 April 1990 with an initial collection of 400 perfumes, both those reproduced by the Osmothèque and those supplied by external perfumers at houses such as Chanel, Guerlain and Lanvin.\n\nThe Osmothèque is the world’s largest scent archive, storing over 3,000 perfumes from the past and present, all preserved at a constant temperature under argon gas. Perfumes accepted into the collection are either those reconstituted using archived formulae by the Osmothèque’s internal perfumers (known as osmocurators) or those supplied by external perfume houses. As a legal deposit archive, the Osmothèque receives a supply of all new perfumes produced in France and much of the world, in addition to those obtained through its program for content acquisitions. The role of chief archivist is currently filled by Patricia de Nicolaï, having assumed the presidency of the Osmothèque from Jean Kerléo in 2008.\n\nThe institution also maintains a substantial reference library of fragrance bases and aromatic sources, both natural and synthetic, historical and contemporary, as well as a vault inaccessible to the public containing historical perfume formulae, many unusable due to a lack of corresponding raw materials.\n\nRarities from the Osmothèque’s collections include examples of ancient perfumery, such as the \"Parfum Royal\" of the Parthian kings as described by Pliny the Elder in the 1st century, medieval toilet waters such as the 14th century \"Eau de la reine de Hongrie\" of Elizabeth of Poland and 18th century powders such as the \"Poudre de Chypre\".\n\nEqually unique is a major collection of 19th century perfumes from leading houses such as Farina, Guerlain, Houbigant, Lubin, F. Millot, L. T. Piver and Roger & Gallet. Also from the period is the eau de cologne made for Napoleon in 1815 during his exile on Saint Helena.\n\nThe largest portion of the Osmothèque’s archives is devoted to modern perfumery (beginning in the late 19th century), presenting innumerable original masterpieces now discontinued or reformulated, including:\nThe Osmothèque maintains an active website, including an online database detailing the institution’s collections.\n\nVarious conferences for professionals, researchers, students and members of the public are regularly offered at the Osmothèque’s headquarters in Versailles, as well as at conference centers at the Galerie de Nicolaï in Paris and the Academy of Perfumery & Aromatics in New York City. In addition, the Osmothèque frequently organizes partner exhibitions and conferences with museums around the world, including the Carrousel du Louvre, the Palace of Versailles and the Smithsonian Institution.\n\nThe Osmothèque also publishes books on the subject of perfume, in addition to a bilingual periodical titled \"Les Nouvelles de l’Osmothèque\", available online and at the Osmothèque’s bookshop.\n\n\n"}
{"id": "24432532", "url": "https://en.wikipedia.org/wiki?curid=24432532", "title": "Randox Laboratories", "text": "Randox Laboratories\n\nRandox Laboratories is a company in the \"in vitro\" diagnostics industry, developing diagnostic solutions for hospitals, clinical, research and molecular labs, food testing, forensic toxicology, veterinary labs and life sciences. Randox develops, manufactures and markets diagnostic reagents and equipment for laboratory medicine, with a distribution network of 145 countries.\n\nIt is the official sponsor of the Grand National from 2017.\n\nRandox was established in 1982 by its Managing Director, Dr Peter FitzGerald in Crumlin, and has since expanded globally.\n\nRandox manufactures more clinical diagnostic products than any other company in the world. It invests more than 16% of profits into R&D, and almost a quarter of its staff are research scientists and engineers. \n\nFollowing the development of Randox Health - the first public facing division - the company became the title sponsor of the Randox Health Grand National. Ex-jockey Sir Anthony McCoy is the main brand ambassador, along with Olympic sailor Matt McGovern and Featherweight Celtic, Irish, WBO European & Intercontinental Champion Marco McCullough.\n\nBeginning with a team of six employees, the company now has 1400 employees around the world. It is now moving into the Randox Science Park, a 45 acre R&D and manufacturing site housed on the former Massereene Barracks. \n\nIt has also recently invested €25m in developing a site in Dungloe, Donegal. Randox Teoranta aims to create more than 470 jobs in research, engineering and life sciences by 2020. \n\nRandox's range of reagents covers many different techniques such as colorimetric, UV and immunoturbidimetric methods. As a result of heavy investment in R&D, Randox's portfolio has grown to provide a wider variety of reagents.\n\nRandox has become a leading specialist in developing dedicated reagents to work on competitor systems, including Dimension and Hitachi.\n\nRandox developed the world's first biochip array technology (BAT) in 2002. BAT is a multi-analyte testing platform which allows simultaneous quantitative or qualitative detection of a wide range of analytes from a single patient sample. It screens biological samples in a rapid, accurate and easy-to-use format.\n\n£180 million was invested in research and development of BAT.\n\nWith the development of the biochip, analysers were created to handle the biochip in a high throughput routine laboratory. The analyser range expanded from the evidence, to include the evidence evolution, evidence investigator and evidence multistat.\n\nAs well as producing 5% of the world's clinical chemistry products, the Randox range of clinical chemistry analysers, the RX Series, accommodates low- to high-throughput laboratories. The RX monza, RX daytona, RX daytona plus, RX imola, RX monaco and RX suzuka attempt to consolidate a laboratory's requirements onto one platform. The throughput of the range varies, with the RX monza having the capability of up to 10,000 tests per year and the RX suzuka having up to 1,000,000 tests per year.\n\nRandox is the 3rd largest manufacturer of Quality Controls and Calibrators in the world. They specialise in third party controls that combine lots of analytes in a single control with the aim of consolidation. Covering over 390 parameters the Acusera branded portfolio of QC supplies 60,000 customers worldwide with QC material. Principle control products include Clinical Chemistry, Immunoassay, Urine, Cardiac and many more as well numerous other research based areas.\n\nAcusera 24.7 Live Online is an inter-laboratory data management programme, with the purpose of supporting the Acusera range of controls.\n\nRandox International Quality Assessment Scheme (RIQAS) is the external quality assessment (EQA) scheme supplied by Randox. RIQAS is the largest international EQA scheme with more than 35,000 laboratory participants in over 123 countries. There are currently 32 programmes available spanning most areas of clinical testing.\n\nThe large number of participants using RIQAS ensures an extensive database for many analytical methods directly increasing statistical validity as a result. RIQAS helps maintain and improve analytical quality; improve inter-laboratory agreement; detect reagent and equipment problems; and compares different analytical methods. RIQAS is also ISO 13485 and UKAS accredited.\n\nRandox offer more than 100 clinical biochemistry tests, covering testing panels such as antioxidants, basic and comprehensive metabolic profiles, cardiac, coagulation, diabetes, drugs of abuse testing, hepatic function, lipids, renal function, specific proteins, therapeutic drug monitoring and veterinary.\n\nFurthermore, Randox re-invest significantly in research and development to develop unique tests such as sLDL, Lipoprotein(a), H-FABP, Cystatin C, TxBCardio, Adiponectin, Bile Acids, Copper, D-3- Hydroxybutyrate, G-6-PDH, Non-Esterified Fatty Acids, Total Antioxidant Status and Zinc.\n\nIn February 2017, two Randox employees were arrested on suspicion of perverting the course of justice amid allegations of data tampering within Randox Testing Services, used by many Police Forces in England and Wales for forensic toxicology. Randox acquired this laboratory in Manchester from Trimega laboratories which went into administration in 2014. As of November 2017, around 50 criminal prosecutions for driving offences had been dropped in what BBC home affairs correspondent, Danny Shaw, described as \"the biggest forensic science scandal in the UK for decades\". Police forces have begun reviewing over 10,000 criminal cases that may be affected by the alleged data manipulation, including sexual and violent crimes.\n"}
{"id": "4224543", "url": "https://en.wikipedia.org/wiki?curid=4224543", "title": "Rocket garden", "text": "Rocket garden\n\nWith rare exceptions, rockets are expendable, so rockets in displays have not been flown. As in the case of the Saturn V, later planned missions were cancelled, leaving unneeded rockets for the museums. For displays of early American space hardware, such as Project Mercury and Project Gemini, surplus missiles have been painted to look like manned space launch vehicles. Engineering test articles (such as the \"Pathfinder\" space shuttle stack in Huntsville) or purpose-built full-scale replicas are also displayed in rocket gardens.\n\n\n\n"}
{"id": "729339", "url": "https://en.wikipedia.org/wiki?curid=729339", "title": "Snow blower", "text": "Snow blower\n\nA snow blower or snow thrower is a machine for removing snow from an area where it is not wanted, such as a driveway, sidewalk, roadway, railroad track, ice rink, or runway. The commonly-used term \"snow blower\" is a misnomer, as the snow is moved using an auger or impeller instead of being blown (by air). It can use either electric power (line power or battery), or a gasoline or diesel engine to throw snow to another location or into a truck to be hauled away. This is in contrast with the action of snow plows, which push snow to the front or side. Typically, the snow is discharged to one side.\n\nSnow blowers range from the very small, capable of removing only a few inches (a few more cm) of light snow in an path, to the very large, mounted onto heavy-duty winter service vehicles and capable of moving wide, or wider, swaths of heavy snow up to deep. \n\nSnow blowers can generally be divided into two classes: single-stage and two-stage. On a single-stage snow blower, the auger (the paddle mechanism visible from the front) pulls snow into the machine and directs it out of a discharge chute. The auger contacts the ground, making single-stage snow blowers unsuitable for use on unpaved surfaces. On a two-stage snow blower, the auger pulls snow into the machine and feeds it into a high-speed impeller, which in turn directs it out of a discharge chute. Two-stage snow blowers can generally handle deeper snow depths than single-stage ones, and because their augers don't touch the ground, they can be used on unpaved surfaces.\n\nRobert Carr Harris of Dalhousie, New Brunswick patented a \"Railway Screw Snow Excavator\" in 1870. In 1923, Robert E. Cole patented a snowplow that operated by using cutters and a fan to blow snow from a surface. Various other innovations also occurred. However, it is (1876–1946) who is generally credited as the inventor of the first practical snow blower. In 1925 Sicard completed his first prototype, based on a concept he described in 1894. He founded Sicard Industries in Sainte-Thérèse, Quebec and by 1927 his vehicles were in use removing snow from the roadways of the town of Outremont, now a borough of Montreal. His company is now a division of SMI-Snowblast, Inc. of Watertown, New York.\n\nThe U.S. Consumer Product Safety Commission estimates that each year there are approximately 5,740 snowblower related injuries in the United States which require medical attention. One problem with the design of the snow blower is that snow can build up in the auger, jamming it and stalling the motor. This is complicated by the fact that the auger could deform before applying enough resistance to the motor to turn it off. If the jam is cleared by hand, it is possible for the auger to return to its natural shape suddenly and with great force, possibly injuring the user; snow blowers are a leading cause of traumatic hand and finger amputations. The correct procedure is to turn off the engine, disengage the clutch and then clear the jam with a broom handle or other long object.. In an effort to improve safety, many manufacturers now include a plastic tool to be used to clear jams, often mounted directly to the snow blower.\n\nMost modern machines mitigate this problem by including a safety system known as the \"Dead man's switch\" to prevent the mechanism from rotating when the operator is not at the controls. They are mandatory in some jurisdictions.\n\nJet engines and other gas turbines are used for large scale propelling and melting of snow over rails and roads. These blowers first were used in Russia and Canada in the 1960s as the large amounts of snow fall were becoming problematic for their train tracks and road ways, and were later introduced into the U.S. by the Boston Transportation Authority.\n\nThe jet engine both melts and blows the snow, clearing the tracks faster than other methods. While offering considerably greater power in a relatively lightweight machine, this method is much more expensive than traditional snow removing methods. In Russia, the high cost is partially negated by utilizing retired military jet engines, such as the Klimov VK-1.\n\n"}
{"id": "936821", "url": "https://en.wikipedia.org/wiki?curid=936821", "title": "Sociocybernetics", "text": "Sociocybernetics\n\nSociocybernetics is an independent chapter of science in sociology based upon the general systems theory and cybernetics. \n\nIt also has a basis in organizational development (OD) consultancy practice and in theories of communication, theories of psychotherapies and computer sciences. The International Sociological Association has a specialist research committee in the area – RC51 – which publishes the (electronic) \"Journal of Sociocybernetics\". \n\nThe term \"socio\" in the name of sociocybernetics refers to any social system (as defined, among others, by Talcott Parsons and Niklas Luhmann).\n\nThe idea to study society as a system can be traced back to the origin of sociology when the emergent idea of functional differentiation has been applied for the first time to society by Auguste Comte. \n\nThe basic goal for which sociocybernetics was created, is the production of a theoretical framework as well as information technology tools for responding to the basic challenges individuals, couples, families, groups, companies, organizations, countries, international affairs are facing today.\n\nOne of the tasks of sociocybernetics is to map, measure, harness, and find ways of intervening in the parallel network of social forces that influence human behavior. Sociocyberneticists' task is to understand the guidance and control mechanisms that govern the operation of society (and the behavior of individuals more generally) in practice and then to devise better ways of harnessing and intervening in them – that is to say to devise more effective ways to operate these mechanisms, or to modify them according to the opinions of the cyberneticist.\n\nSociocybernetics aims to generate a general theoretical framework for understanding cooperative behavior. It claims to give a deep understanding of the general theory of evolution.\nThe outlook that sociocybernetics uses when analyzing any living system lies in a basic law of sociocybernetics. \nIt says: All living systems go through six levels of interrelations (social contracts) of its subsystems:\n\n\nGoing through these six phases of relationship theoretically gives the framework for the sociocybernetic study of any evolutionary system. It serves as an \"equation for life.\"\nSociocybernetics can be defined as \"Systems Science in Sociology and Other Social Sciences\" – systems science, because sociocybernetics is not limited to theory but includes application, empirical research, methodology, axiology (i.e., ethics and value research), and epistemology. In general use, \"systems theory\" and \"cybernetics\" are frequently interchangeable or appear in combination. Hence, they can be considered as synonyms, although the two terms come from different traditions and are not used uniformly in different languages and national traditions. Sociocybernetics includes both what are called first order cybernetics and second order cybernetics. Cybernetics, according to Wiener's original definition, is the science of \"control and communication in the animal and the machine\". Heinz von Foerster went on to distinguish a first order cybernetics, \"the study of observed systems\", and a second order cybernetics, \"the study of observing systems\". Second order cybernetics is explicitly based on a constructivist epistemology and is concerned with issues of self-reference, paying particular attention to the observer-dependence of knowledge, including scientific theories. In the interdisciplinary and holistic spirit of systems science, although sociology is clearly at the centre of interest of sociocybernetics, the other social sciences, such as psychology, anthropology, political science, economics, are addressed as well, with emphases depending on the particular research question to be dealt with.\n\nRecent research from the Santa Fe Institute presents the idea that social systems like cities don't behave like organisms as has been proposed by some in sociocybernetics.\n\n\n\n\n\n"}
{"id": "35196799", "url": "https://en.wikipedia.org/wiki?curid=35196799", "title": "SoundBite Hearing System", "text": "SoundBite Hearing System\n\nSoundBite Hearing System is a non-surgical bone conduction prosthetic device that transmits sound via the teeth. It is an alternative to surgical bone conduction prosthetic devices, which require surgical implantation into the skull to conduct sound.\n\nSoundBite uses the tooth instead of the implanted component and eliminates the need for surgery. It is therefore typically lower in complications and in cost than the prevalent surgical treatments.\n\nSoundBite Hearing System has two principal components: a behind-the-ear (BTE) microphone unit that is worn on the impaired ear and a removable, custom-made in-the-mouth (ITM) device worn on the upper, left or right back teeth. Both components have rechargeable batteries and a charger is included with the system.\n\nThe BTE microphone captures and processes sound, and wirelessly transmits the sound signals to the ITM device. The ITM receives these signals and converts them into sound vibrations. These subtle sound vibrations travel via the teeth, through bones in the skull, to the functioning inner ear or cochlea, bypassing problems in the outer or middle ear entirely. Although the vibrations are strong enough to be picked up by the cochlea, they are so subtle as to not be felt by the wearer.\n\nIn the United States, the device has FDA clearance to treat patients with single-sided deafness (SSD) or conductive hearing loss (CHL). SoundBite also has CE mark approval for SSD, CHL, and mixed hearing loss (MHL).\n\nSoundBite was developed and marketed by Sonitus Medical, Inc. The company filed for bankruptcy on Thursday, January 15, 2015, as a result of the US Centers for Medicare & Medicaid Services' decision not to cover the device.\n\nSingle-sided deafness (SSD) and conductive hearing loss (CHL) are life-altering conditions where patients often have anxiety, depression, social isolation, and reduced quality of life. SSD patients have one cochlea that is virtually non-functional. It does not hear sound even when using conventional hearing aids, which are amplification devices that simply “turn up the volume” on air-conducted sound. CHL patients have a problem with the ear (outer, middle or canal) that prohibits air conducted sound from reaching an otherwise functional cochlea. Conventional hearing aids which amplify sound can cause distortion for these patients. Therefore, the traditional treatment approach has been a prosthetic device called Baha, which replaces the function of the impaired ear by using a well-established principle called bone conduction to re-route sound through the skull bones to the functional cochlea.\n\nThe Baha bone conduction prosthetic devices are used rather than hearing aids because conventional hearing aids are clinically inappropriate for these patients. The Baha surgery can cause complications that range from skin reaction to infection, to abscess, to complete re-implantation or revision of the Baha post.\n\nIn the United States, the Medicare Benefit Policy Manual distinguishes between hearing aids and prosthetic devices, and indicates that certain devices (including Baha) are payable by Medicare as prosthetic devices when hearing aids are medically inappropriate.\n\nThe principle of bone conduction has been used for many years to treat patients with single-sided deafness and conductive hearing loss. The principle is based on decades of research showing that bone conduction stimulation of the teeth initiates auditory sensations. Evidence shows that teeth vibrations lead to audio-frequency vibration transmissions via soft tissue. Those transmissions then travel through skull foramina into the skull cavity. From there, they channel into the inner ear fluids, stimulating the cochlea. Subsequently, Sonitus Medical developed SoundBite Hearing System to use those principles in a non-surgical, removable hearing system.\n\nA multi-center clinical trial conducted in 2011 validated that SoundBite is safe and effective and provides substantial benefit for individuals with single-sided deafness (SSD). Trial participants wore SoundBite for 30 days, using the device an average of 8.2 hours per day. Based on the clinical trial results, SoundBite improves the ability of individuals with SSD to understand speech in an environment with background noise by an average of 25%. One-third of the trial participants found that the system improved their hearing ability by more than 30%. The results of this extensive clinical trial showed SoundBite to be as effective as surgically implanted bone conduction systems in improving the ability to understand speech in an environment with background noise.\n\nIn 2013, another multi-center clinical study confirmed previous findings that SoundBite is safe, effective, and provides significant benefit for patients. Study participants wore SoundBite for 6 month. Benefit was determined through a standard audiological questionnaire called the Abbreviated Profile of Hearing Aid Benefit (APHAB), and a patient survey. Results from the APHAB questionnaire showed patients had significant improvement in Ease of Communication, Reverberation, Background Noise, and Global Benefit. Additionally, the patient survey showed strong satisfaction with the device: 100% would recommend SoundBite to a friend or familty member with similar hearing loss.\n\nSound vibrations travel through a medium, and sound is heard when sound waves travel through the medium of air or bones/teeth to arrive at the inner ears. The SoundBite Hearing System uses sound waves travelling through bone, known as bone conduction to transmit subtle vibrations through bones to the inner ears.\n\nThe SoundBite Hearing System is a non-surgical and removable bone conduction hearing prosthetic device that re-routes sound through the teeth and skull bone directly to the functioning inner ear or cochlea. By-passing problems in the outer and middle ears entirely. For patients suffering from single-sided deafness, SoundBite re-routes sound from the deaf side, to the functioning cochlea, by-passing the non-hearing side.\n\nSoundBite uses the same mechanism of action as Baha devices (bone conduction), however it places a transducer on the tooth —a “naturally osseointegrated” post — and thereby eliminates the need for a surgical implant.\n\nSoundBite relies on two primary components to deliver sound:\n\nThe BTE unit has a 12-to-15-hour operational life when fully charged. Each ITM hearing device has a 6-to-8 hour operational life when fully charged. The SoundBite Hearing System includes 1 BTE, 2 ITMs, and a system charger.\n\nThe BTE unit delivers a broader frequency bandwidth (up to 18,000 Hz) as compared to existing devices for single-sided deafness. This broader bandwidth enhances spatial hearing ability, which is a key limitation for SSD patients.\n\nThe patient is evaluated by an audiologist to determine degree of hearing loss to determine if the patient is a candidate for SoundBite. A dentist then performs a dental screening and takes a partial impression of the patient's teeth, which is used to create a customized SoundBite ITM device. The ITM and BTE are fitted and adjusted, and the system is programmed by a hearing professional.\n\nIn the United States, SoundBite is appropriate for patients who are 18 years or older, with good oral health, with:\n\nIn Europe and Canada, SoundBite is appropriate for patients who are 18 years or older, with good oral health, with:\n\nThe SoundBite Hearing System was manufactured by Sonitus Medical Inc., a privately held medical device company based in San Mateo, California founded in June 2006. In addition to receiving FDA clearance for SoundBite, Sonitus Medical has received CE Mark certification. The company filed for bankruptcy in January 2015 leaving users with no service and many who paid for these expensive devices never received them. Other companies have looked into buying them out but to date nobody has.\n"}
{"id": "8334805", "url": "https://en.wikipedia.org/wiki?curid=8334805", "title": "Steam jet cooling", "text": "Steam jet cooling\n\nSteam jet cooling uses a high-pressure jet of steam to cool water or other fluid media. Typical uses include industrial sites, where a suitable steam supply already exists for other purposes or, historically, for air conditioning on passenger trains which use steam for heating. Steam jet cooling experienced a wave of popularity during the early 1930s for air conditioning large buildings. Steam ejector refrigeration cycles were later supplanted by systems using mechanical compressors.\n\nSteam is passed through a vacuum ejector of high efficiency to exhaust a separate, closed vessel which forms part of a cooling water circuit. The partial vacuum in the vessel causes some of the water to evaporate, thus giving up heat through evaporative cooling. The chilled water is pumped through the circuit to air coolers, while the evaporated water from the ejector is recovered in separate condensers and returned to the cooling circuit.\n\nThe AT&SF railroad (Santa Fe) used this method, which they called \"Steam Ejector Air Conditioning\", on both heavyweight and lightweight passenger cars, built until the mid-1950s.\n\n\n"}
{"id": "33273723", "url": "https://en.wikipedia.org/wiki?curid=33273723", "title": "StudySync", "text": "StudySync\n\nStudySync is an educational software suite created by BookheadEd Learning, LLC for middle school, high school, and college-level education. StudySync was created by Robert Romano in partnership with Jay King in Sonoma County, California, USA.\n"}
{"id": "18886199", "url": "https://en.wikipedia.org/wiki?curid=18886199", "title": "Suhua Highway Improvement Project", "text": "Suhua Highway Improvement Project\n\nThe Suhua Highway Improvement Project (; colloquially 蘇花改, ) is a major highway project in northeast Taiwan to improve and bypass dangerous sections of the Suhua Highway, part of Provincial Highway 9.\n\nThe Suhua Highway is the main road connecting the Taiwanese communities of Su'ao and Hualien, and a portion of it is built alongside very steep cliffs high above the Pacific Ocean. Because of the rugged terrain, the road is often closed due to heavy rain, typhoons, or landslides, leading to injuries and deaths.\nIn the 1990s, the Ministry of Transportation and Communications (MOTC) started planning a new freeway to connect Su'ao and Hualien, as part of Freeway 5. However, it was controversial because of its huge projected construction costs and environmental impact. Instead, the MOTC developed a scaled-down project, which would construct bridges and tunnels in three dangerous sections: Su'ao–Dong'ao (), Nan'ao–Heping (), and Heping–Qingshui (). Compared to a freeway, the improved highway would have a lower speed limit and still have only one lane in each direction. The new alignment will cut travel time along the coastline from 2.5 hours to 80 minutes. Some parts of the old alignment will be kept open for bicycles and small vehicles, with a speed limit of .\n\nThe project was named the Suhua Highway Alternative Project (; colloquially 蘇花替 ) in 2008. Its name was changed to its current name in 2010.\n\nConstruction started in 2011 and was expected to take five years and cost 46.5 billion New Taiwan dollars. Due to difficulties in construction, the MOTC now expects to complete it in 2019.\n\n"}
{"id": "18100479", "url": "https://en.wikipedia.org/wiki?curid=18100479", "title": "Swivel (drill rig)", "text": "Swivel (drill rig)\n\nA Swivel is a mechanical device used on a drilling rig that hangs directly under the traveling block and directly above the kelly drive, that provides the ability for the kelly (and subsequently the drill string) to rotate while allowing the traveling block to remain in a stationary rotational position (yet allow vertical movement up and down the derrick) while simultaneously allowing the introduction of drilling fluid into the \"drill string\".\n\nSee Drilling rig (petroleum) for a diagram of a drilling rig.\n"}
{"id": "40863107", "url": "https://en.wikipedia.org/wiki?curid=40863107", "title": "TALOS (uniform)", "text": "TALOS (uniform)\n\nTALOS (Tactical Assault Light Operator Suit) was the name given to a robotic exoskeleton that United States Special Operations Command intended to design with the help of universities, laboratories, and the technology industry. The brief for TALOS states that it must be bulletproof, weaponized, have the ability to monitor vitals and give the wearer enhanced strength and perception. The suit would comprise layers of smart material and sensors. The suit may have not been intended for an entire squad, but for a lead operator who would breach a door first, to protect them as they are the most vulnerable team operator in that situation. \n\nDespite recent reports that United States Special Operations Command missed the initial deadline to deliver a prototype for the high-profile tactical assault light operator suit, command leadership is confident that it will test a powered exoskeleton by summer 2019. Partners involved in the project include Defense Advanced Research Projects Agency, U.S. Army Research, Development and Engineering Command, and the Army Research Laboratory. \n\nTALOS was first presented by Admiral William McRaven, then-commanding officer of the United States Special Operations Command at a conference on May 2013. He said that the protective suit was inspired by one of his troops in Afghanistan.\n\nUnited States Special Operations Command expected \"1st gen capability\" inside a year, though it has taken longer than that. Three unpowered prototype suits were to be assembled and delivered in June 2014. Development of the suit is a collaborative effort between 56 corporations, 16 government agencies, 13 universities, and 10 national laboratories. They are working together to incorporate features including a powered exoskeleton, full-body armor, and situational-awareness displays. SOCOM plans to hold a \"monster garage\" event to encourage mechanics and craftsmen to create components for the suit. They may seek permission from The Pentagon to distribute prize money to generate interest. Admiral McRaven expects a system to be fielded by August 2018.\n\nSpecial Operations Command per Lieutenant Commander Li Cohen started and completed the selection process using prototypes that competed for the contract. Admiral William McRaven, who leads the Special Operations Command, says in the video, \"I am very committed to this because I'd like that last operator that we lost to be the last one that we ever lose, in this fight or in the fight of the future.\" According to a video produced by the command, the program is on track to have a \"first-generation capability\" by summer 2014.\n\n\nAlthough the objective of the program was to incorporate new technologies into a fully powered and integrated suit, components developed under it could be issued individually to troops in the short term to enhance their effectiveness. Non-lethal weapons, new armor materials, more compact communications gear, advanced night vision, and 3-D audio can be used as individual pieces of equipment before they are all put together in one powered exoskeleton. Items developed for TALOS including an increased tactical data storage capability which allows for ten times the capacity of current data storage has transitioned to fulfill an immediate operational requirement, as well as a new armor solution being used for special operations non-standard commercial vehicles. Others systems that will be transitioned include a small, individual soldier SATCOM antenna, an unpowered loadbearing exoskeleton, a powered cooling vest to sustain body temperature, a next generation antenna that includes dynamic tuning, the Future Interoperable Radio Enclosure (FIRE), a tactical radio sleeve for cell phones, lightweight multi-hit ceramic-metallic hybrid armor, and a biosensor-equipped combat shirt that can monitor a soldier’s physiological status.\n\n\nDefense industry leaders expressed skepticism about SOCOM's financial outlook and development schedule for TALOS technologies. Admiral McRaven planned to have portions of a prototype by June 2014, with the first \"independently operational combat suit prototype\" delivered by July 2018. Science and technology officials believe that technologies envisioned for the suit won't be achievable before around 2026; new technologies that need to be developed include next-generation full-body ballistic armor protection materials, powered exoskeletons for mobility and agility, conformable and wearable antennae and computers, soldier combat-ready displays with non-visual means of information display, power generation and thermal management, and embedded medical monitoring and biomechanical modeling. Components made by different companies will have to be made to work together in one suit. Power generation is the biggest problem, as there was no light-weight, low-bulk power generation system able to fuel TALOS components. Funding is also a concern, as SOCOM planned to spend $20 million per year on development to total $80 million, which some see as far too low. In previous endeavors to create \"digitized\" soldiers such as Land Warrior, the U.S. Army spent $500 million on three major contracts from 1996 to 2006 before its features became reliable.\n\nAt the 2015 Special Operations Forces Industry Conference, Revision Military displayed its prototype Kinetic Operations Suit on a full-sized mannequin. Launched a year prior, the suit features a powered, lower-body exoskeleton to transfer the weight down to the waist belt and supports it with motorized actuators on each leg. The exoskeleton supports a body armor system capable of stopping rifle rounds that surrounds 60 percent of the operator, compared to 18 percent with current armor vests. To relieve weight, the leg actuators pick up each leg and moves it as the person moves, and takes the weight of the helmet, armor, and vest down through a rigid, articulated spine, transferring weight from weak areas of the neck and lower back. A small power pack powers the suit, and a cooling vest pumps water through three yards of tubing under the suit to maintain core temperature; the power pack has a cooling fan that can be heard in close proximity, but it is thought that won't matter after breaching a door. The Kinetic Operations Suit has undergone live-fire testing and combat scenarios and successfully performed the same tasks as currently-outfitted operators in similar amounts of time.\n\n"}
{"id": "32154042", "url": "https://en.wikipedia.org/wiki?curid=32154042", "title": "Time domain vernier method", "text": "Time domain vernier method\n\nSpread Spectrum Time Domain Vernier Method or SSTDV, is a time domain reflectometry method that uses a spread spectrum time domain reflectometry signal to locate intermittent faults in wires (such as aircraft wires) by measuring time delay between incident and reflected signals. SSTDR uses a pseudo noise (PN) code as the test signal. This code travels down the length of the wire, where it is reflected off of impedance discontinuities such as open or short circuits. The reflected signal is correlated with the incident signal to identify the locations of these discontinuities. In the case of SSTDV, the correlation is accomplished by adding one bit to the PN code each cycle, thus creating a vernier sequence that can be used to resolve the time delay between incident and reflected signals. \n\n"}
{"id": "3488152", "url": "https://en.wikipedia.org/wiki?curid=3488152", "title": "Treadle pump", "text": "Treadle pump\n\nA treadle pump is a human-powered suction pump that sits on top of a well and is used for irrigation. It is designed to lift water from a depth of seven metres or less. The pumping is activated by stepping up and down on a treadle, which are levers, which drive pistons, creating cylinder suction that draws groundwater to the surface.\nTreadle pumps free farmers from dependence on rain-fed irrigation and helps farmers maximize return on their small plots of land. The treadle pump can do most of the work of a motorized pump, but costs considerably less. Pump prices including installation range between US$20 and $100. Because it needs no fossil fuel (it is driven by the operator's body weight and leg muscles), it can also cost less (50%) to operate than a motorized pump. It can lift five to seven cubic metres of water per hour from wells and boreholes up to seven metres deep and can also be used to draw water from lakes and rivers. Many treadle pumps are manufactured locally, but they can be challenging to produce consistently without highly skilled welders and production hardware.\n\nTreadle pumps are most commonly used by farmers on small plots of land, typically about the size of an acre. They are also used in poor countries and small villages such as: villages in Africa, small farmers in Asia, and anywhere else where money is an issue.\n\nCompared to bucket irrigation, the treadle pump can greatly increase the income that farmers generate from their land by increasing the number of growing seasons, by expanding the types of crops that can be cultivated, and improving on the quality of grown crops.\n\nhttps://commons.wikimedia.org/wiki/File:Treadle_pump,_original.jpg\n\nRDRS then a program of Lutheran World Federation\\World Service in northern Bangladesh had begun the search for efficient, low-cost irrigation technology using local materials from 1975, experimenting with many varieties and the model developed by Norwegian intermediate technologist Gunnar Barnes was developed in 1979. Later claims that the treadle pump was invented in 1980 by Mr. Narendra Nath Deb in Bangladesh (), with input from Dan Jenkins, USAID engineer were somewhat inaccurate although both contributed to its further development and replication\n\nWorking with the poor, RDRS endeavored to produce an affordable manual pump for irrigation. The main criteria were that it should be able to irrigate at least 0.5 ha of wheat, the total cost of purchase and installation was not to be more than the price of one bag of paddy, and the pump was to be simple enough to make and repair locally. This led to the use of bamboo tubewell and frame, and other locally available materials. \n\nBeginning in 1976, they came up with various designs of foot-operated pumps which, because they only used one leg, were not comfortable and had low output. The last design before the treadle pump was a “Y-pump”, having two cylinders welded together in a Y shape, and a hand-driven rocking frame so the hands could help the foot. The plungers were also of a special and more efficient design, which were subsequently adopted by the handpump industry in Bangladesh and India. The improvements started to gain interest for the pump. With the development of the treadle pump, using components of the Y pump but having the use of both legs, there was immediate interest and demand. \n\nThe treadle pump was introduced in December, 1980, and thereafter the RDRS workshop produced 20 pumps a day to meet demands. By 1982 there were different models of the pump: twin tubewell, twin dugwell, twin low-lift, twin tubewell with drinking spout, and household model.\n\nIn March, 1988, the cost of a treadle pump, installed, was about $20 U.S.\n\nAs the small RDRS workshop in Rangpur was unable to keep up with demand, RDRS helped local entrepreneurs set up workshops to make the treadle pump (known early on as the “twin treadle pump”). The first of the workshops was the North Bengal Agriculture Workshop in Lalmonirhat (NBAW), started in 1981. The fourth workshop was that of Mr. Narendra Nath Deb. Mr. Deb was already making pumps of his own design, but contracted in 1984 for his workers to be trained in making the treadle pump. By the end of 1984, 26,701 treadle pumps had been sold. Since 1985, 84 manufacturers now produce treadle pumps and have currently sold 1.4 million treadle pumps to small plot Bangladeshi farmers.\n\nhttps://commons.wikimedia.org/wiki/File:Treadle_pump,_original,_drawing.jpg\n\nOne of the first instances of the treadle pump moving out of Bangladesh was its promotion by the International Rice Research Institute (IRRI) in the Philippines in 1984, under Robert Stickney. There it was called the “Tapak-Tapak” Pump.\n\nIn 1986, iDE identified the treadle pump as a technology which could help raise income and productivity on small farms, and entered into the field of marketing the pumps. iDE also started helping to set up workshops to make the pump, and has gone on to be one of the main players in disseminating the treadle pump technology throughout the world.\n\nSince its inception, the treadle pump has had many modifications. One of the most useful has been the pressure pump, which enables water to be pumped above the height of the pump. A main player in its development was Carl Bielenberg, whose work (based on a 1985 design by Dan Jenkins) was supported by Appropriate Technology International, and CARE. This model has also been adapted to suit local conditions and material available. Pressure treadle pumps allow farmers to spray water and run sprinklers, eliminating the need for an elevated water storage tank and suction pump system. Pressure pumps are widely in use in East Africa though KickStart International and in Myanmar through Proximity Designs. Many Non-Governmental Organizations (NGOs) (IDE, IDE-India, iDE UK, KickStart International, Proximity Designs, Practical Action (formally ITDG)) have been active in developing treadle pumps, as have student and researcher teams at universities.\n"}
{"id": "4539079", "url": "https://en.wikipedia.org/wiki?curid=4539079", "title": "Underactuation", "text": "Underactuation\n\nUnderactuation is a technical term used in robotics and control theory to describe mechanical systems that cannot be commanded to follow arbitrary trajectories in configuration space. This condition can occur for a number of reasons, the simplest of which is when the system has a lower number of actuators than degrees of freedom. In this case, the system is said to be \"trivially underactuated\".\n\nThe class of underactuated mechanical systems is very rich and includes such diverse members as automobiles, airplanes, and even animals.\n\nTo understand the mathematical conditions which lead to underactuation, one must examine the dynamics that govern the systems in question. Newton's laws of motion dictate that the dynamics of mechanical systems are inherently second order. In general, these dynamics can be described by a second order differential equation:\nformula_1\n\nWhere:\n\nformula_2 is the position state vector formula_3 is the vector of control inputs formula_4 is time.\n\nFurthermore, in many cases the dynamics for these systems can be rewritten to be affine in the control inputs:\nformula_5\n\nWhen expressed in this form, the system is said to be underactuated if:\nformula_6\n\nWhen this condition is met, there are acceleration directions that can not be produced no matter what the control vector is.\n\nNote that formula_7 does not explicitly represent the number of actuators present in the system. Indeed, there may be more actuators than degrees of freedom and the system may still be underactuated. Also worth noting is the dependence of formula_7 on the state formula_9. That is, there may exist states in which an otherwise fully actuated system becomes underactuated.\n\nThe classic inverted pendulum is an example of a trivially underactuated system: it has two degrees of freedom (one for its support's motion in the horizontal plane, and one for the angular motion of the pendulum), but only one of them (the cart position) is actuated, and the other is only indirectly controlled. Although naturally extremely unstable, this underactuated system is still controllable.\n\nA standard automobile is underactuated due to the nonholonomic constraints imposed by the wheels. That is, a car cannot accelerate in a direction perpendicular to the direction the wheels are facing. A similar argument can be made for boats, planes and most other vehicles.\n\n\n\n"}
{"id": "252725", "url": "https://en.wikipedia.org/wiki?curid=252725", "title": "Wheelbarrow", "text": "Wheelbarrow\n\nA wheelbarrow is a small hand-propelled vehicle, usually with just one wheel, designed to be pushed and guided by a single person using two handles at the rear, or by a sail to push the ancient wheelbarrow by wind. The term \"wheelbarrow\" is made of two words: \"wheel\" and \"barrow.\" \"Barrow\" is a derivation of the Old English \"bearwe\" which was a device used for carrying loads.\n\nThe wheelbarrow is designed to distribute the weight of its load between the wheel and the operator, so enabling the convenient carriage of heavier and bulkier loads than would be possible were the weight carried entirely by the operator. As such it is a second-class lever. Traditional Chinese wheelbarrows, however, had a central wheel supporting the whole load. Use of wheelbarrows is common in the construction industry and in gardening. Typical capacity is approximately 100 liters (4 cubic feet) of material.\n\nA two-wheel type is more stable on level ground, while the almost universal one-wheel type has better maneuverability in small spaces, on planks, in water, or when tilted ground would throw the load off balance. The use of one wheel also permits greater control of the deposition of the load upon emptying.\n\nThe earliest wheelbarrows with archaeological evidence in the form of a one-wheel cart come from second century Han Dynasty Emperor Hui's tomb murals and brick tomb reliefs. The painted tomb mural of a man pushing a wheelbarrow was found in a tomb at Chengdu, Sichuan province, dated precisely to 118 AD. The stone carved relief of a man pushing a wheelbarrow was found in the tomb of Shen Fujun in Sichuan province, dated circa 150 AD. And then there is the story of the pious Dong Yuan pushing his father around in a single-wheel \"lu che\" barrow, depicted in a mural of the Wu Liang tomb-shrine of Shandong (dated to 147 AD). However, there are even earlier accounts than this that date back to the 1st century BC and 1st century AD. The 5th century \"Book of Later Han\" stated that the wife of the once poor and youthful imperial censor Bao Xuan helped him push a \"lu che\" back to his village during their feeble wedding ceremony, around 30 BC. Later, during the Red Eyebrows Rebellion (c. 20 AD) against Xin dynasty's Wang Mang (45 BC–23 AD), the official Zhao Xi saved his wife from danger by disguising himself and pushing her along in his \"lu che\" barrow, past a group of brigand rebels who questioned him, and allowed him to pass after he convinced them that his wife was terribly ill. The first recorded description of a wheelbarrow appears in Liu Xiang's work \"Lives of Famous Immortals\". Liu describes the invention of the wheelbarrow by the legendary Chinese mythological figure Ko Yu, who builds a \"Wooden ox\".\n\nNevertheless, the Chinese historical text of the \"Sanguozhi\" (Records of the Three Kingdoms), compiled by the ancient historian Chen Shou (233–297 AD), credits the invention of the wheelbarrow to Prime Minister Zhuge Liang (181–234 AD) of Shu Han from 197–234. It was written that in 231 AD, Zhuge Liang developed the vehicle of the wooden ox and used it as a transport for military supplies in a campaign against Cao Wei. Further annotations of the text by Pei Songzhi (430 AD) described the design in detail as a large single central wheel and axle around which a wooden frame was constructed in representation of an ox. Writing later in the 11th century, the Song Dynasty (960–1279) scholar Gao Cheng wrote that the small wheelbarrow of his day, with shafts pointing forward (so that it was pulled), was the direct descendent of Zhuge Liang's wooden ox. Furthermore, he pointed out that the third century 'gliding horse' wheelbarrow featured the simple difference of the shaft pointing backwards (so that it was pushed instead).\n\nWheelbarrows in China came in two types. The more common type after the third century has a large, centrally mounted wheel. Prior types were universally front-wheeled wheelbarrows. The central-wheeled wheelbarrow could generally transport six human passengers at once, and instead of a laborious amount of energy exacted upon the animal or human driver pulling the wheelbarrow, the weight of the burden was distributed equally between the wheel and the puller. European visitors to China from the 17th century onwards had an appreciation for this, and it was given a considerable amount of attention by a member of the Dutch East India Company, Andreas Everardus van Braam Houckgeest, in his writings of 1797 (who accurately described its design and ability to hold large amounts of heavy baggage). However, the lower carrying surface made the European wheelbarrow clearly more useful for short-haul work. As of the 1960s, traditional wheelbarrows in China were still in wide use.\n\nAlthough there are records of Chinese sailing carriages from the 6th century these land sailing vehicles were not wheelbarrows, and the date of which the sail assisted wheelbarrow was invented is uncertain. Engravings are found in van Braam Houckgeest's 1797 book.\n\nEuropean interest in the Chinese sailing carriage is also seen in the writings of Andreas Everardus van Braam Houckgeest in 1797, who wrote:\n\nNear the southern border of Shandong one finds a kind of wheelbarrow much larger than that which I have been describing, and drawn by a horse or a mule. But judge by my surprise when today I saw a whole fleet of wheelbarrows of the same size. I say, with deliberation, a fleet, for each of them had a sail, mounted on a small mast exactly fixed in a socket arranged at the forward end of the barrow. The sail, made of matting, or more often of cloth, is five or six feet high, and three or four feet broad, with stays, sheets, and halyards, just as on a Chinese ship. The sheets join the shafts of the wheelbarrow and can thus be manipulated by the man in charge.\n\nThese sailing wheelbarrows continued in use into the twentieth century.\n\nM. J. T. Lewis surmised the wheelbarrow may have existed in ancient Greece in the form of a one-wheel cart. Two building material inventories for 408/407 and 407/406 BC from the temple of Eleusis list, among other machines and tools, \"1 body for a one-wheeler (hyperteria monokyklou)\", although there is no archaeological evidence to prove this hypothesis.(\"ὑπερτηρία μονοκύκλου\" in Greek):\n\nSince dikyklos (\"δίκυκλος\") and tetrakyklos (\"τετράκυκλος\") mean nothing but \"two-wheeler\" and \"four-wheeler,\" and since the monokyklos (\"μονόκυκλος\") body is sandwiched in the Eleusis inventory between a four-wheeler body and its four wheels, to take it as anything but a one-wheeler strains credulity far beyond breaking point. It can only be a wheelbarrow, necessarily guided and balanced by a man...what does now emerge as certainty is that the wheelbarrow did not, as is universally claimed, make its European debut in the Middle Ages. It was there some sixteen centuries before.\n\nM. J. T. Lewis admits that the current consensus among technology historians, including Bertrand Gille, Andrea Matthies, and Joseph Needham, is that the wheelbarrow was invented in China around 100 AD. However, Lewis proposes that the wheelbarrow could have also existed in ancient Greece. Based on the Eleusis list, Lewis states that it is possible that wheelbarrows were used on Greek construction sites, but admits that archaeological evidence for the wheelbarrow in ancient farming and mining is absent. He surmised that wheelbarrows were not uncommon on Greek construction sites for carrying moderately light loads. He speculates the possibility of wheelbarrows in the Roman Empire and the later Eastern Roman, or Byzantine Empire, although Lewis concludes that the evidence is scarce, and that \"most of this scenario, perforce, is pure speculation.\" The 4th century Historia Augusta reports emperor Elagabalus to have used a wheelbarrow (Latin: \"pabillus\" from \"pabo\", one-wheeled vehicle) to transport women in his frivolous games at court. While the present evidence does not indicate any use of wheelbarrows into medieval times, the question of continuity in the Byzantine Empire is still open, due to a lack of research yet. Currently, there is no archaeological evidence for the wheelbarrow in ancient Greece and Rome.\n\nThe first wheelbarrow in Europe appeared sometime between 1170 and 1250. Medieval wheelbarrows universally featured a wheel at or near the front (in contrast to their Chinese counterparts, which typically had a wheel in the center of the barrow), the arrangement now universally found on wheelbarrows.\n\nResearch on the early history of the wheelbarrow is made difficult by the marked absence of a common terminology. The English historian of science M.J.T. Lewis has identified in English and French sources, four mentions of wheelbarrows between 1172 and 1222, three of them designated with a different term. According to the medieval art historian Andrea Matthies, the first archival reference to a wheelbarrow in medieval Europe is dated 1222, specifying the purchase of several wheelbarrows for the English king's works at Dover. The first depiction appears in an English manuscript, Matthew Paris's \"Vitae duorum Offarum\", completed in 1250.\n\nBy the 13th century, the wheelbarrow proved useful in building construction, mining operations, and agriculture. However, going by surviving documents and illustrations the wheelbarrow remained a relative rarity until the 15th century. It also seemed to be limited to England, France, and the Low Countries. Eastward diffusion of the technology was uneven and not especially fast: the wheelbarrow was still unknown in Russia and its neighbors as late as the reign of Peter the Great. The conscript laborers who dug millions of cubic yards of earth to create the city of St. Petersburg--with its extensive system of canals and the levees and embankments required to keep the city dry--carried dirt either in handbaskets or the fronts of their long, tunic-like shirts. On the occasion of Peter's first visit to England, the young tsar and his traveling companions found a wheelbarrow in the garden of the house where they lodged; not knowing its purpose, they used it for drunken wheelbarrow races.\n\nIn the 1970s, British inventor James Dyson introduced the Ballbarrow, an injection molded plastic wheelbarrow with a spherical ball on the front end instead of a wheel. Compared to a conventional design, the larger surface area of the ball made the wheelbarrow easier to use in soft soil, and more laterally stable with heavy loads on uneven ground.\n\nThe Honda HPE60, an electric power-assisted wheelbarrow, was produced in 1998.\nPower assisted wheelbarrows are now widely available from a number of different manufacturers. Powered wheelbarrows are used in a range of applications; the technology has improved to enable them to take much heavier loads, beyond weights that a human could transport alone without assistance. Motorized wheelbarrows are generally either diesel powered or electric battery powered. Often used in small scale construction applications where access for larger plant machinery might be restricted.\n\n\n\n"}
{"id": "37824778", "url": "https://en.wikipedia.org/wiki?curid=37824778", "title": "Willis Whitfield", "text": "Willis Whitfield\n\nWillis Whitfield (December 6, 1919 – November 12, 2012) was an American physicist and inventor of the modern cleanroom, a room with a low level of pollutants used in manufacturing or scientific research. His invention earned him the nickname, \"Mr. Clean,\" from \"Time Magazine\".\n\nWhitfield was born in Rosedale, Oklahoma, the son of a cotton farmer.\n\nAn employee of the Sandia National Laboratories in New Mexico, Whitfield created the initial plans for the cleanroom in 1960. Prior to Whitfield's invention, earlier cleanrooms often had problems with particles and unpredictable airflows. Whitfield solved this problem by designing his cleanrooms with a constant, highly filtered air flow to flush out impurities in the air. Within a few years of its invention, sales of Whitfield's modern cleanroom had generated more than $50 billion in sales worldwide.\n\nWhitfield retired from Sandia in 1984.\n\nWhitfield died in Albuquerque, New Mexico, on November 12, 2012, at the age of 92. His death was announced by officials at Sandia National Laboratories.\n"}
{"id": "889043", "url": "https://en.wikipedia.org/wiki?curid=889043", "title": "World War II cryptography", "text": "World War II cryptography\n\nCryptography was used extensively during World War II, with a plethora of code and cipher systems fielded by the nations involved. In addition, the theoretical and practical aspects of \"cryptanalysis\", or \"codebreaking\", was much advanced.\n\nProbably the most important codebreaking event of the war was the successful decryption by the Allies of the German \"Enigma\" Cipher. The first complete break into Enigma was accomplished by Poland around 1932; the techniques and insights used were passed to the French and British Allies just before the outbreak of the war in 1939. They were substantially improved by British efforts at the Bletchley Park research station during the war. Decryption of the Enigma Cipher allowed the Allies to read important parts of German radio traffic on important networks and was an invaluable source of military intelligence throughout the war. Intelligence from this source (and other high level sources, including the Fish ciphers) was eventually called Ultra.\n\nA similar break into the most secure Japanese diplomatic cipher, designated Purple by the US Army Signals Intelligence Service, started before the US entered the war. Product from this source was called Magic.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "29017011", "url": "https://en.wikipedia.org/wiki?curid=29017011", "title": "Wyss Institute for Biologically Inspired Engineering", "text": "Wyss Institute for Biologically Inspired Engineering\n\nThe Wyss Institute for Biologically Inspired Engineering is a cross-disciplinary research institute at Harvard University which focuses on developing new bioinspired materials and devices for applications in healthcare, manufacturing, robotics, energy, and sustainable architecture. The Institute has two sites: one in the Center for Life Sciences Boston building in Boston’s Longwood Medical Area, and one on Harvard's main campus in Cambridge, Massachusetts. The Wyss Institute was launched in January 2009 with a $125 million gift to Harvard—at the time, the largest single philanthropic gift in its history—from Hansjörg Wyss. In 2013, Hansjorg Wyss doubled his gift to $250 million.\n\nThe Institute works as an alliance among Harvard Medical School, Harvard School of Dental Medicine, Harvard School of Engineering and Applied Sciences, Harvard Faculty of Arts and Sciences, Children’s Hospital Boston, Dana-Farber Cancer Institute, Beth Israel Deaconess Medical Center, Boston University, Brigham and Women's Hospital, Massachusetts General Hospital, Spaulding Rehabilitation Hospital, Tufts University, Charité - Universitätsmedizin Berlin, University of Zurich, Massachusetts Institute of Technology and the University of Massachusetts Medical School. Translating technological discoveries into commercial products and therapies is an important part of the organization's mission.\n\nThe Wyss Institute’s scientific operations are organized around six Enabling Technology Platforms that focus on development of new core technologies and capabilities that will facilitate the explosion of major R&D areas in the field of bioinspired engineering. The platforms integrate multiple faculty members with the advanced technology team, clinical experts, and industrial partners. The Institute platforms are:\n\n\n\n"}
{"id": "39138053", "url": "https://en.wikipedia.org/wiki?curid=39138053", "title": "X-cite by Alghanim Electronics", "text": "X-cite by Alghanim Electronics\n\nX-cite by Alghanim Electronics (part of Alghanim Industries) is an electronics store currently based in Kuwait. Their product line includes computers & tablets, mobile phones, cameras, TVs, audio & MP3, games & toys, major appliances, small housewares, watches, fitness gear, and home furnishing.\n\nXcite Kuwait has 20 showrooms located in different areas in Kuwait - Al Rai, Fahaheel, Hawally, Salmiya, Avenues Mall, Marina Mall, Sahari Mall, Beitak Tower, Makhiyal Mall (Jahra) (Kuwait City), Jleeb Al-Shuyoukh, Jahra, Qurain CO-OP, Jabriya CO-OP, Khaldiya CO-OP, Souk Al Salmiya, Liwan Mall, Shuwaik , and Cube Mall\n\nXcite Saudi Arabia has 4 showrooms located in different areas in Riyadh, King Abdullah Road, Granada District, Khurais Road and Imall.\n\nE-Commerce Portal\n\nIn December 2011, X-cite launched their e-commerce portal xcite.com. in Kuwait and xcite.com.sa in KSA \n\nAwards\n\nX-cite has received the following awards:\n"}
