{"id": "22576371", "url": "https://en.wikipedia.org/wiki?curid=22576371", "title": "AMD Lance Am7990", "text": "AMD Lance Am7990\n\nAMD Lance Am7990 IEEE 802.3 Ethernet Media Access Controller (MAC) controller were introduced in 1985. Its architecture is the basis for AMD’s PCnet Family of highly integrated single-chip Ethernet controllers. The one exception is the Am79C940 MAC.\nThe Am7990 chip was fabricated in NMOS technology and has no integrated Manchester encoder/decoder (ENDEC) nor does it have an integrated 10BASE-T transceiver.\n\nA later refabricated chip called the C-LANCE Am79C90 is made with 0.8 micrometre CMOS technology. The original NMOS version Am7990 and the CMOS Am79C90 version are differ in some details which may affect device driver compatibility.\n\nThe datasheet for the CMOS version writes that the CMOS and NMOS versions are the same. But the \"Table B-1. Comparison Summary of the C-LANCE and LANCE Devices\" in the datasheet shows they differ. These differences are not likely to require modifications of any device driver.\n\nThe PCnet family of Ethernet controllers (PCnet-ISA II, PCnet-32, PCnet-PCI II and PCnet-FAST) is LANCE software compatible. This means you should be able use the original 16-bit software on these members of the PCnet family of single-chip Ethernet controllers.\n\nThe Am7990 can handle 10BASE-5 Type A, 10BASE-2 Type B, and 10BASE-T. Back-to-back packet reception with as little as 0,5 µs interframe spacing. DMA/Bus mastering 24-bit (16M) address capable. Up to 128 ring buffers can be used. 48 byte receive/transmit FIFO. Operates with supply and logic. Features a Time-domain reflectometer (TDR) with a granularity of 30 meter. maximum frequency. \nPhysically a DIP-48 or PLCC-68 package is used.\nCSR0 slave read data can cause timing violations on DAL lines.\n\nThe old LANCE (Rev. C) chips have a bug which causes garbage to be inserted in front of the received packet. The workaround is to ignore packets with an invalid destination address (garbage will usually not match). Of course, this precludes multicast support.\nThe Amiga SANA-II network interface API has poor multicast support. And this chip bug might be the reason.\n\nNo capability for transmit buffer byte count of zero.\nReceive lockup may occur if bus latency is large.\nExternal loopback on a live network may cause reception of invalid loopback failure indications.\nReceive descriptor zero byte count buffer interpreted as 4096 available bytes.\nWill poll computer memory every 1.6 ms for new packets to transmit.\n\n\n"}
{"id": "48470575", "url": "https://en.wikipedia.org/wiki?curid=48470575", "title": "American Safety Razor Company", "text": "American Safety Razor Company\n\n\"For the company of the same name that was founded in 1901 and renamed as the Gillette Safety Razor Company, see Gillette (brand)\"\nAmerican Safety Razor Company is a personal care brand founded in the early 20th century (1906) by a merging of the Gem Cutlery Company & Ever-Ready and has been a principal competitor to Gillette for a century and more. Unrelated to the Gillette company which also used the name \" 'American Safety Razor Company' \" in 1901 until 1904 before it was renamed for its founder, King C. Gillette. \n\nThe company produces a wide range of personal care, medical, industrial blades and cutting tools with international operations in Canada, England, Mexico, Puerto Rico, and Israel.\n\nFrederick and Otto Kampfe were immigrants from Saxony, Germany. In 1875 they applied for a U.S. patent and introduced the Star Razor, the first safety razor made in the United States. The brothers continued to improve their design and sold their products under the Star Safety Razor name in Brooklyn, New York.\n\nJerry Reichard, after working for the Kampfe Brothers for 23 years, leaves to start his own company – the Gem Cutlery Company in 1898. Its first product, the Gem Safety Razor, borrowed heavily from the Star Razor in design but soon outpaces the Star in sales.\n\nIn 1903 Jerry Reichard leaves Gem Safety Razor Company to form yet another razor and blade producer, along with August Scheuber the company was briefly named 'Reichard & Scheuber Manufacturing Company' before it became 'The Yankee Company'. The Yankee Company made wedge-blade razors under the name Yankee, Mohican & Winner.\nThe Yankee Company, is renamed Ever-Ready in 1905. Gem & Ever-Ready merge in 1906 and is incorporated as the Personna American Safety Razor Company. In 1906, abandoning the wedge-blade design, they introduced the standard single edge rib-back blade that is still used today. In 1915 Ever-Ready Shaving Brushes were first introduced and continued to be produced until the early 1990s.\n\nIn December 1919 The Safetee Soap Corporation formed as a subsidiary of American Safety Razor Corporation and produced a line of shaving soaps, creams, powders, talc and aftershave lotions to compliment the safety razor business.\nThe Safetee Soap line was cross-promoted in pamphlets included in other ASR products.\n\nGem & Ever-Ready merged with Star to become the American Safety Razor Corporation in 1919. It was chartered in Virginia, while razor and blade production remained in Brooklyn. By 1921, it had produced 1,800,000 safety razors; 110,000,000 razor blades; 1,000,000 shaving brushes; 2,000,000 cakes of shaving soap.\nBy 1942, it had introduced and popularized the phrase \"five o'clock shadow\". In 1953, it acquired the Pal, Treet and Personna brands after purchasing the Pal Blade Company. These two product names continue to be made today.\nIn 1954 the factory relocated to Staunton, Virginia, after the Brooklyn City Planning Commission's planned civic center encroached on the factory with plans to redevelop the industrial area into office and residential use.\nThe American Safety Razor factory building at 333 Jay Street then became the new campus for the Polytechnic Institute of Brooklyn.\n\nSeeking diversification Philip Morris acquires American Safety Razor in 1960. In 1963 American Safety Razor is the first maker of stainless steel blades, which were sold under the Personna brand name. In 1968, Philip Morris, purchased the Burma-Vita Company, makers of Burma-Shave. In 1970, the first blade made with tungsten steel was introduced, the Personna 74. In 1974 American Line Brand of industrial products was introduced, expanding the company into industrial blades. In 1977, executives purchased Personna American Safety Razor Company from Philip Morris in a management buyout.\n\nEnergizer bought the privately held American Safety Razor in 2011, when it filed for bankruptcy.\nIn 2015, Energizer spun off the personal care division as Edgewell Personal Care. Edgewell announced the sale of its Personna Industrial Division to an investment group, which renamed it AccuTec Blades.\n"}
{"id": "2235613", "url": "https://en.wikipedia.org/wiki?curid=2235613", "title": "Anal bleaching", "text": "Anal bleaching\n\nAnal bleaching is the process of lightening the color of the skin around the anus. It is done for cosmetic purposes, to make the color of the anus more uniform with the surrounding area. Some treatments are applied in an office or salon by a cosmetic technician and others are sold as cream that can be applied at home.\n\nPornography actresses were the first to undergo the anal bleaching process, in an effort to lighten the color of their anuses to match the rest of their skin, although it has been suggested by Kristina Rose that this is not the case. As Brazilian waxing became popular, due to the popularity of smaller swimsuits and lingerie, the spread of pornography into the mainstream, and endorsement of the procedure by celebrities, women began noticing that their anuses were darker than the rest of their skin. The increase in the number of women engaging in anal sex has also contributed to women's concern over the appearance of their anus. To combat this perceived problem, genital bleaching began to gain appeal. Gay men also make use of this procedure.\n\nThe procedure was briefly shown in 2004 near the end of an episode \"Cosmetic Surgery Live\". One salon that performed the procedure received an increase in queries in 2005 attributed to an episode of \"Dr. 90210\" on E!, when porn star Tabitha Stevens was filmed having her anus bleached. It garnered several mentions in movies such as \"Brüno\", and \"Bridesmaids\", and in magazines.\n\nThe treatment was apparently first offered in the US in California in 2005; it was reported to be available at the same time in Australia. Spas outside of Hollywood were slow to begin offering it as a beauty treatment, with just one New York spa offering the service by 2007. Creams are now sold for use at home, and genital lightening is offered as a laser-based treatment in cosmetic surgery centers. Although the popularity of anal bleaching has not approached that of Brazilian waxing, it has garnered mainstream recognition over the past several years.\n\nThere are several methods to carry out the anal lightening process. The most common method is to simply use an at-home lotion or gel to target the darkened anal and genital area and gradually fade the darkened area over time. Most of the other methods that are used for skin lightening, like hydroquinone used to lighten skin and usually found in products marketed to darker skinned people to even out their skin tone, and other popular methods like cryosurgery and laser lightening treatments, are also used for anal bleaching.\n\nMany of these cosmetics contained ingredients that would irritate the sensitive anal area, creating temporary discomfort and even burning, scarring, or incontinence.\n\nThe process performed with creams containing hydroquinone is banned in some countries, such as the member states of the EU. In 2006, the FDA removed previous advice that stated hydroquinone was considered generally safe, as hydroquinone has been linked to ochronosis, where skin becomes permanently discoloured and disfigured, and because it may also be a carcinogen. However, its use is not banned in the United States and it is still in use.\n\nOther principal ingredients that are used in skin lightening cosmetics are arbutin and kojic acid. Arbutin, often also called \"bearberry\", can be converted by the body into hydroquinone. Azelaic acid with products like aloe vera and vitamin C tablets is another method. Kojic acid was developed as a safer alternative to hydroquinone, however it is less effective at lightening and also carcinogenic.\n\nLaser-based and cryogenic anal or genital lightening does come with its own set of warnings. Results are not always consistent, the process may be a painful one, and those with darker skin tones may have issues with these processes . Lasers also have the added disadvantage of leaving scars, so cryogenic skin lightening is the option usually used for the anal and genital area.\n\n"}
{"id": "51758248", "url": "https://en.wikipedia.org/wiki?curid=51758248", "title": "AnswerDash", "text": "AnswerDash\n\nAnswerDash is a B2B software company that facilitates customer service for e-commerce businesses. AnswerDash was founded in Seattle, Washington in 2012 as a spin-off from the Information school of the University of Washington. Its software-as-a-service utilizes machine learning to create databases of context-sensitive support answers for end-users of webpages and mobile applications, thus reducing the need for human customer service. AnswerDash claims to be the first, and as of 2015 the world's leading provider of contextual point-and-click answer technology.\n\nThe company was founded under name ″Qazzow″ in 2012 by three University of Washington employees: Jacob O. Wobbrock, a professor and the director of Mobile & Accessible Design Lab, Andy Ko, an associate professor the director of Code & Cognition Lab, and Parmit Chilana, a research assistant and a graduate student at the time, now a professor at Simon Fraser University. Jake Wobbrock assumed the responsibilities of CEO, and Andy Ko became the company's CTO. The idea of the key company's service stemmed from Parmit Chilana's dissertation on human–computer interaction. Her research has highlighted a very high proportion (up to 95%) of users' questions that are triggered by something seen on a website and should benefit from contextual help.\n\nIn November 2013 the company received $500,000 seed investment from W Fund to further develop its contextual Q&A service and in December 2013 additional $2.4 million from WRF Capital and Voyager Capital investment funds. In May 2014, the company re-branded itself to ″AnswerDash″ reacting to customers' confusion over the unusual sequence of letters in the original name.\n\nIn January 2015, Kevin Knoepp has replaced Andy Ko as AnswerDash new CTO. AnswerDash service came on-line in May 2015. At the same time, after serving as a CEO for three years, Jake Wobbrock stepped down as the CEO. His performance was generally favored by the board, but Wobbrok has admitted his tendency to focusing on interesting scientific details that might not be relevant to business success. Wobbrock was replaced by William Colleran, a former CEO Impinj.\n\nBy the end of 2015 the company has \"matured dramatically\". After announcing the successful deployment of its software to Zendesk, in September AnswerDash raised additional $2.9 million. In 2016, AnswerDash has recorded the biggest jump in GeekWire 200 rating, a ranked index of Pacific Northwest startups.\n\nOffered under SaaS model by subscription, the service aims to eliminate the need for separate FAQs or ″help islands\" offering contextual answers to end-users' questions. The service can be used on websites, web applications, and mobile applications. It employs a natural-language engine and machine learning technology that allows website and application owners to build the database of answers in a matter of several days.\nIn addition, AnswerDash utilizes data mining technology to provide business owners with valuable analytics on their consumers' behavior that leads to further improvement of their e-commerce services. AnswerDash claims to reduce the need of traditional customer support by 30 to 50 percent.\n\n"}
{"id": "1867804", "url": "https://en.wikipedia.org/wiki?curid=1867804", "title": "Base load", "text": "Base load\n\nThe baseload (also base load) on a grid is the minimum level of demand on an electrical grid over a span of time, for example, one week. This demand can be met by unvarying power plants, dispatchable generation, or by a collection of smaller intermittent energy sources, depending on which approach has the best mix of low cost, availability and high reliability in any particular market. The remainder of demand, varying throughout a day, is met by dispatchable generation, load following power plants, and peaking power plants, which can be turned up or down quickly, operating reserve, demand response and energy storage.\n\nWhen the cheapest power was from large coal and nuclear plants which could not be turned up or down quickly, they were used to generate baseload, since it is constant, and they were called \"baseload plants.\" Large standby reserves were needed in case of sudden failure of one of these large plants. Unvarying power plants are no longer always the cheapest way to meet baseload. The grid now includes many wind turbines which have such low marginal costs that they can bid lower prices than coal or nuclear, so they can provide some of the baseload when the wind blows. Using wind turbines in areas with varying wind conditions, and supplementing them with solar in the day time, dispatchable generation and storage, handles the intermittency of individual wind sources.\n\nGrid operators take long and short term bids to provide electricity over various time periods and balance supply and demand continuously. The detailed adjustments are known as the unit commitment problem in electrical power production.\n\nWhile historically large power grids used unvarying power plants to meet the base load, there is no specific technical requirement for this to be so. The base load can equally well be met by the appropriate quantity of intermittent power sources and dispatchable generation.\n\nUnvarying power plants can be coal, nuclear, combined cycle plants, which may take several days to start up and shut down, hydroelectric, geothermal, biogas, biomass, solar thermal with storage and ocean thermal energy conversion. \n\nSupply interruptions can affect all plants from breakdowns, and also hydroelectric plants from droughts, coal plants if their coal stockpiles freeze, and gas plants from pipeline leaks and closures.\n\nThe desirable attribute of dispatchability applies to some gas plants, wind (through blade pitch) and hydroelectricity. Grid operators also use \"curtailment\" to shut plants out of the grid when their energy is not needed.\n\nThere are 195,000 megawatts of grid storage installed world-wide; 94% is pumped-storage hydroelectricity; 2% is in batteries. Pumped storage uses cheap power at times of low demand, usually night, to pump water from a lower reservoir to an upper reservoir, then lets it drop back through turbines during peak demand times, usually in the day. Availability of solar power in peak hours of the day can reduce the need for storage. The biggest storage facility in the world is on the Virginia-West Virginia border, with 50% more capacity than the Hoover Dam.\n\nGrid operators solicit bids to find the cheapest sources of electricity over short and long term buying periods.\n\nNuclear and coal plants have very high fixed costs, high plant load factor but very low marginal costs, though not as low as solar, wind, and hydroelectric. On the other hand, peak load generators, such as natural gas, have low fixed costs, low plant load factor and high marginal costs.\n\nCoal and nuclear power plants do not change production to match power consumption demands since it is more economical to operate them at constant production levels. Use of higher cost combined-cycle plants or combustion turbines is thus minimized, and these plants can be cycled up and down to match more rapid fluctuations in consumption. Some nuclear power stations, such as those in France, do operate as load following power plants and alter their output to meet varying demands. \n\nDifferent plants and technologies may have differing capacities to increase or decrease output on demand: nuclear plants are generally run at close to maximum output continuously (apart from maintenance, refueling and periodic refurbishment), while coal-fired plants may be cycled over the course of a day to meet demand. Plants with multiple generating units may be used as a group to improve the \"fit\" with demand, by turning units on and off.\n\nAccording to National Grid plc chief executive officer Steve Holliday in 2015, baseload is \"outdated\", as microgrids would become the primary means of production, and large powerplants relegated to supply the remainder.\n\nIn 2016, Ambrose Evans-Pritchard of the Daily Telegraph wrote that, with advances in energy storage, 'there ceases to be much point in building costly \"baseload\" power plants' and goes on to argue 'Nuclear reactors cannot be switched on and off as need demands - unlike gas plants. They are useless as a back-up for the decentralized grid of the future, when wind, solar, hydro, and other renewables will dominate the power supply'.\n\n\n"}
{"id": "13315035", "url": "https://en.wikipedia.org/wiki?curid=13315035", "title": "Bomber gap", "text": "Bomber gap\n\nThe bomber gap was the Cold War belief that the Soviet Union's Long Range Aviation department had gained an advantage in deploying jet-powered strategic bombers. Widely accepted for several years, the gap was used as a political talking point in the United States to justify a great increase in defense spending.\n\nOne result was a massive buildup of the US Air Force bomber fleet, which peaked at over 2500 bombers to counter the perceived Soviet threat. Surveillance flights by the U-2 aircraft indicated that the bomber gap did not exist.\n\nRealizing that the belief in the gap was an extremely effective funding source, the US military fabricated a series of similarly-nonexistent Soviet military advances. That tactic is now known as policy by press release. Some of the claims were a nuclear-powered bomber, supersonic VTOL flying saucers, and, only a few years later, the missile gap.\n\nOn February 15, 1954, \"Aviation Week\" published an article describing new Soviet jet bombers capable of carrying a nuclear bomb from their bases to the US. The aircraft was the Myasishchev M-4 \"Bison\". Over the next year and a half, the rumors were debated publicly in the press and soon in Congress.\n\nAdding to the concerns was an infamous event in July 1955. At the Soviet Aviation Day demonstrations at the Tushino Airfield, ten Bison bombers were flown past the reviewing stand, flew out of sight, quickly turned around, and then flew past the stands again with eight more. That presented the illusion that there were 28 aircraft in the flyby. Western analysts, extrapolating from the illusionary 28 aircraft, judged that by 1960, the Soviets would have 800.\n\nAt the time, the Air Force had just introduced its own strategic jet bomber, the B-52 Stratofortress, and the shorter-range B-47 Stratojet was still suffering from a variety of technical problems that limited its availability. Its staff started pressing for accelerated production of the B-52 but also grudgingly accepted calls for expanded air defense.\n\nThe Air Force was generally critical of spending effort on defense after it had studied the results of the World War II bombing campaigns and concluded that British Prime Minister Stanley Baldwin's pre-war thinking on the fruitlessness of air defense was mostly correct: \"The bomber will always get through.\" Like the British, the US Air Force concluded that money would better be spent on making the offensive arm larger to deter an attack. The result was a production series consisting of thousands of aircraft. Over 2,000 B-47s and almost 750 B-52s were built to match the imagined fleet of Soviet aircraft.\n\nUS President Dwight Eisenhower had always been skeptical of the gap. However, with no evidence to disprove it, he agreed to the development of the U-2 to find out for sure.\n\nThe first U-2 flights started in 1956. One early mission, Mission 2020, flown by Martin Knutson on July 4, 1956, flew over Engels-2 airfield, near Saratov, and photographed 20 M-4 Bison bombers on the ramp. Multiplying by the number of Soviet bomber bases, the intelligence suggested the Soviets were already well on their way to deploying hundreds of aircraft. The U-2 had actually photographed the entire Bison fleet; there was no bomber at any of the other bases. Similar missions over the next year finally proved that. At least in official circles, the gap had been disproved.\n\nOther newly declassified information indicates that Engels was never overflown, and nothing was confirmed about Bison aircraft numbers until December 1959 The downward revision in Bison numbers was caused by new information from the factories that was received in 1958. It showed that production rate had slowed down.\n\nAs it was later discovered, the M-4 was unable to meet its original range goals and was limited to about . Unlike the US, the Soviets still lacked overseas bases in the Western Hemisphere and so the M-4 could not attack the US and then land at a friendly airbase. Interest in the M-4 waned, and a total of only 93 were produced before the assembly lines were shut down in 1963.\n\nThe vast majority were used as tankers or maritime reconnaissance aircraft; only the original ten shown at the air show and nine newer 3MD13 models served on nuclear alert.\n\n\n"}
{"id": "2550316", "url": "https://en.wikipedia.org/wiki?curid=2550316", "title": "Bowmanville Foundry", "text": "Bowmanville Foundry\n\nBowmanville Foundry Co. Ltd. is a foundry located in Bowmanville, Ontario, Canada. It was established in 1901.\n\nThe company has a long history in the manufacture of ductile, gray iron and malleable iron castings. \n\nA 2005 book, \"Iron in the Blood\" by local authors Helen Bajorek MacDonald and Helen Lewis Schmid, discusses the history of the company in the context of family, community, labour and economic history.\n\nThe official launch of \"Iron in the Blood\" was mentioned in the Ontario Legislature by John O'Toole, the member for Durham.\n\n"}
{"id": "48723288", "url": "https://en.wikipedia.org/wiki?curid=48723288", "title": "Cloaca Circi Maximi", "text": "Cloaca Circi Maximi\n\nThe Cloaca Circi Maximi or Cloaca Circi was a sewer in ancient Rome.\n\nIt was originally a small stream fed by various sources from around the Porta Capena right through the valley between the Palatine Hill and Aventine Hill, running down to the river Tiber. According to tradition, games and horse races were held in this vallery from right after the founding of Rome in the 8th century. Over the centuries the Circus Maximus was built over the stream, with a channel named Euripus running across it halfway and two bridges carrying the track over it. The channel also served as the spina down the middle of the track.\n\nUnder Julius Caesar and Augustus the circus and its surroundings were greatly enlarged, covering over the channel, which became a sewer. It was connected to a tunnel modelled on that of the Cloaca Maxima and now terminated on the Tiber upstream of the Cloaca Maxima.\n\n"}
{"id": "4385366", "url": "https://en.wikipedia.org/wiki?curid=4385366", "title": "Cocktail strainer", "text": "Cocktail strainer\n\nA cocktail strainer is a metal bar accessory used to remove ice from a mixed drink as it is poured into the serving glass. A type of sieve, the strainer is placed over the mouth of the glass or shaker in which the beverage was prepared; small holes in the device allow only liquids to pass as the beverage is poured.\n\nThere are two common types of strainers. The Hawthorne strainer is a disc (called the \"rim\") with a handle and two or more stabilizing prongs. A metal spring fixed around the edge of the rim rolls inward to fit inside the glass. The rim of the strainer does not need to touch the rim of the glass, as the spring inside filters out the ice. \n\nThe Julep strainer is shaped like a bowl with a handle, and will fit tightly into a mixing glass or shaker when inserted at the proper angle. Liquid passes through holes or slits in the bowl.\n\n"}
{"id": "3444907", "url": "https://en.wikipedia.org/wiki?curid=3444907", "title": "Core drill", "text": "Core drill\n\nA core drill is a drill specifically designed to remove a cylinder of material, much like a hole saw. The material left inside the drill bit is referred to as the \"core\".\n\nCore drills used in metal are called annular cutters. Core drills used for concrete are generally called Diamond Core Drills and are water cooled. For drilling masonry, carbide core drills can be used.\n\nThe earliest core drills were those used by the ancient Egyptians, invented in 3000 BC. Core drills are used for many applications, either where the core needs to be preserved (the drilling apparatus used in obtaining a core sample is often referred to as a corer), or where drilling can be done more rapidly since much less material needs to be removed than with a standard bit. This is the reason that diamond-tipped core drills are commonly used in construction to create holes for pipes, manholes, and other large-diameter penetrations in concrete or stone.\n\nCore drills are used frequently in mineral exploration where the coring may be several hundred to several thousand feet in length. The core samples are recovered and examined by geologists for mineral percentages and stratigraphic contact points. This gives exploration companies the information necessary to begin or abandon mining operations in a particular area.\n\nBefore the start of World War Two, Branner Newsom, a California mining engineer, invented a core drill that could take out large diameter cores up to 16 feet in length for mining shafts. This type of core drill is no longer in use as modern drill technology allows standard drilling to accomplish the same at a much cheaper cost.\n\nCore drills come with several power choices including electric, pneumatic, hydraulic (all of which require power sources, such as a generator).\n\n"}
{"id": "40311953", "url": "https://en.wikipedia.org/wiki?curid=40311953", "title": "Crunchfish", "text": "Crunchfish\n\nCrunchfish is a Swedish technology company in Malmö that develops gesture recognition software for the mobile phone and tablet market. Crunchfish was founded in 2010 with an initial focus to create innovations for the iOS and Android app markets. Gesture recognition using a standard webcam as main gesture sensor was one of their core innovations and the company is now focusing on touchless interaction based on camera based gestures. In 2013, April, the company was selected a '2013 Red Herring Top 100' company by Red Herring (magazine). Crunchfish produces gesture sensing software, a set of customized mid-air gesture recognition solutions, named A3D™, to global mobile device manufacturers and app developers. Crunchfish cooperates with smartphone manufacturers to enable Crunchfish gesture sensing technology in their partners mobile devices. Crunchfish developed the touchless functions in Chinese Gionee's smartphone Elife E6, launched in China, July, 2013 and in India and Africa in August, 2013\n\nCrunchfish integrates touchless functions in for e.g. smartphones and the technology is portable to other mobile platform devices. Their gesture recognition technology can be used in\nmobile phones, tablets and DTVs equipped with a front-facing camera. Typical uses for Crunchfish touchless technology are devices in an intelligent home as smartphones, tablets, Digital TV, games and other home electronics that have a built-in standard camera. The gesture based functionality adds alternative options to a standard touch interface. The user can for example put a tablet in front of him/her and be free to do mid-air gesture with both hands and interacting with a media player without touching it. Smartphone users can answer and reject phone calls, start slideshows in photo gallery, use video scrubber etc. without touching the device. The device can pause a video playback if the users look away from the screen.\n\nCrunchfish's Touchless A3D software is able to detect and track objects (fingers, hand gestures and face movements) in three dimensions based on the video stream from an embedded standard camera in e.g. a mobile device.\n\n"}
{"id": "5831971", "url": "https://en.wikipedia.org/wiki?curid=5831971", "title": "DECtalk", "text": "DECtalk\n\nDECtalk was a speech synthesizer and text-to-speech technology developed by Digital Equipment Corporation in 1984, based largely on the work of Dennis Klatt at MIT, whose source-filter algorithm was variously known as KlattTalk or MITalk.\n\nThe first DECtalk units were seen in 1984. They were standalone units that connected to any device with an asynchronous serial port. These units were also able to connect to the telephone system by having two telephone jacks. One connected to a phone line, the other to a telephone. The DECtalk units could recognize and generate any telephone touch tone. With that capability the units could be used to automate various telephone-related tasks by handling both incoming and outgoing calls. This included acting as an interface to an email system and the capability to function as an alerting system by utilizing the ability to place calls and interact via touch tones with the person answering the phone.\n\nLater units were produced for PCs with ISA bus slots. In addition, various software implementations were produced, most notably the DECtalk Access32. Certain versions of the synthesizer were prone to undesirable characteristics. For example, the alveolar stops were often assimilated as sounding more like dental stops. Also, versions such as Access32 would produce faint electronic beeps at the end of phrases.\n\nIn the final years of DEC, early/mid-2000, the DECtalk IP was sold to Force Computers, Inc. In December 2001, the IP was sold from Force Computers, Inc, to Fonix Speech, Inc. (now SpeechFX, Inc.), which offers DECtalk as a small-footprint TTS system.\n\n\n"}
{"id": "14013974", "url": "https://en.wikipedia.org/wiki?curid=14013974", "title": "DISCover", "text": "DISCover\n\nDigital Interactive Systems Corporation (or DISCover) is a company specializing in gaming technology for PCs. They are the creators of the DISCover technology which allow PC games to be played like a video game console. The technology, which features the \"Drop and Play\" engine, auto-plays CDs or DVDs and automates scripts for installing and updating games. Consoles with the engine connect to the Internet for game updates. This technology debuted at the 2003 Electronic Entertainment Expo.\n\nMachines using DISCover technology include the Apex Extreme, Alienware DHS 2 and DHS 5.\n\nIn August 2007, DISCover announced that their Hardcore White-Label Gaming System, or HAWGS, technology would be used for FiringSquad's Ammo digital distribution service. The following month, on September 2007, the company disclosed the InstaPlay desktop client, which improves the ease of use for accessing games. DISCover chief executive officer David Ferrigno addressed Instaplay concerns and comparisons with other digital distribution services such as Direct2Drive and Steam.\n\n\n"}
{"id": "564685", "url": "https://en.wikipedia.org/wiki?curid=564685", "title": "Digital control", "text": "Digital control\n\nDigital control is a branch of control theory that uses digital computers to act as system controllers.\nDepending on the requirements, a digital control system can take the form of a microcontroller to an ASIC to a standard desktop computer.\nSince a digital computer is a discrete system, the Laplace transform is replaced with the Z-transform.\nAlso since a digital computer has finite precision (\"See quantization\"), extra care is needed to ensure the error in coefficients, A/D conversion, D/A conversion, etc. are not producing undesired or unplanned effects.\n\nThe application of digital control can readily be understood in the use of feedback.\nSince the creation of the first digital computer in the early 1940s the price of digital computers has dropped considerably, which has made them key pieces to control systems for several reasons:\n\nA digital controller is usually cascaded with the plant in a feedback system. The rest of the system can either be digital or analog.\n\nTypically, a digital controller requires:\n\n\nThe programs can take numerous forms and perform many functions\n\nAlthough a controller may be stable when implemented as an analog controller, it could be unstable when implemented as a digital controller due to a large sampling interval. During sampling the aliasing modifies the cutoff parameters. Thus the sample rate characterizes the transient response and stability of the compensated system, and must update the values at the controller input often enough so as to not cause instability.\n\nWhen substituting the frequency into the z operator, regular stability criteria still apply to discrete control systems. Nyquist criteria apply to z-domain transfer functions as well as being general for complex valued functions. Bode stability criteria apply similarly.\nJury criterion determines the discrete system stability about its characteristic polynomial.\n\nThe digital controller can also be designed in the s-domain (continuous). The Tustin transformation can transform the continuous compensator to the respective digital compensator. The digital compensator will achieve an output which approaches the output of its respective analog controller as the sampling interval is decreased.\n\nformula_1\n\nTustin is the Padé approximation of the exponential function formula_2 :\n\nAnd its inverse\n\nWe must never forget that the digital control theory is the technique to design strategies in discrete time, (and/or) quantized amplitude (and/or) in (binary) coded form to be implemented in computer systems (microcontrollers, microprocessors) that will control the analog (continuous in time and amplitude) dynamics of analog systems. From this consideration many errors from classical digital control were identified and solved and new methods were proposed:\n\n\nhttp://mtc-m18.sid.inpe.br/col/sid.inpe.br/mtc-m18@80/2008/03.17.15.17.24/doc/mirrorget.cgi?languagebutton=pt-BR&metadatarepository=sid.inpe.br/mtc-m18@80/2009/02.09.14.45.33&index=0&choice=full\n\nhttp://mtc-m05.sid.inpe.br/col/sid.inpe.br/deise/1999/09.14.15.39/doc/homepage.pdf\n\nhttp://www.sae.org/technical/papers/2002-01-3468\n\n\nand\n\n\n\n"}
{"id": "9592774", "url": "https://en.wikipedia.org/wiki?curid=9592774", "title": "European Federation of Food Science and Technology", "text": "European Federation of Food Science and Technology\n\nThe European Federation of Food Science and Technology (EFFoST) is a European-based non-governmental organization devoted to the advancement of food science and technology. It consists of eighty different societies in 21 different European countries. They are a regional group of the International Union of Food Science and Technology.\n\n\n\nThe Executive Committee consists of a President, Past President, President-Elect, four other elected officials, and twelve Members-At-Large.\n\nEFFoST is headquartered in Wageningen, Netherlands.\n\n"}
{"id": "24222593", "url": "https://en.wikipedia.org/wiki?curid=24222593", "title": "Fédération nationale des boulangers et pâtissiers du Burkina", "text": "Fédération nationale des boulangers et pâtissiers du Burkina\n\nFédération nationale des boulangers et pâtissiers du Burkina (FNBPB) is a trade union of bakery workers in Burkina Faso. The union was founded on April 4, 1960. FNBPB is affiliated to \"Confédération générale du travail du Burkina\" (CGT-B). Djigimdé Tiga is the president of FNBPB.\n\nFNBPB is a member of the International Union of Food, Agricultural, Hotel, Restaurant, Catering, Tobacco and Allied Workers' Association.\n"}
{"id": "36033904", "url": "https://en.wikipedia.org/wiki?curid=36033904", "title": "George W. Snyder", "text": "George W. Snyder\n\nGeorge W. Snyder (1780–1841) was a watchmaker and inventor.\n\nGeorge W. Snyder was a 19th-century reel maker from Paris, Kentucky. He is credited for inventing the very first American-made fishing reel in 1820.\n\nSnyder's reel was designed for fly fishing, and named the Kentucky Reel. Without patent or trademark protection, the Kentucky reel was quickly copied by many others, including Meek, Milam, Sage, Hardman and Gayle. These artisans were trained in Jewelry fabrication and were experienced in cutting gears, constructing small parts, and doing precision work. In time, the Kentucky Reel was mass-produced by the emerging factories located in the Northeast, where they could be produced at a fraction of the cost and time required for hand-built construction. The availability of more affordable fly reels greatly stimulated the sales and popularity of fly fishing equipment, and was soon applied to bait casting reels, resulting in a surge in the popularity of fishing as a pastime among all levels of American society.\n\nGeorge Snyder was born in the same county as Daniel Boone—Bucks, Pa. He later went to Paris, Ky., then called Hopewell, about 1803, and died there on February 10, 1841, aged sixty years. He was a skillful watchmaker and silversmith; being\na good practical angler, and seeing the necessity for a rapid multiplying reel for black-bass fishing with the live minnow, he proceeded to invent one. Snyder's first reel was made for his own use, about 1810. He afterward made reels for members of his club, and others.\n\nDuring the middle of the 19th century, another fly reel was invented in New York City. The \"New York Reel\" was usually made out of brass or nickel silver, and it was much heavier than the Kentucky Reel. It had a serpentine crank or a \"ball-handle\".\n\n"}
{"id": "255041", "url": "https://en.wikipedia.org/wiki?curid=255041", "title": "Gutenberg Museum", "text": "Gutenberg Museum\n\nThe Gutenberg Museum is one of the oldest museums of printing in the world, located opposite the cathedral in the old part of Mainz, Germany. It is named after Johannes Gutenberg, the inventor of printing from movable metal type in Western Europe. The collections include printing equipment and examples of printed materials from many cultures.\n\nA group of people founded the museum in 1900, 500 years after Johannes Gutenberg’s birth, to honor the inventor and present his technical and artistic achievements to the public at large. They also aimed to exhibit the writing and printing of as many different cultures as possible.\n\nPublishers, manufacturers of printing machines and printing houses donated books, apparatus and machines, which formed the basis of the collection. In its first few years the museum was part of the city library, meaning that the most beautiful and characteristic volumes from the library’s extensive collection could be requisitioned for the museum. Visitors were thus presented with a survey of almost 500 years of the printed book. In time the museum expanded to include sections on printing techniques, book art, job printing and ex-libris, graphics and posters, paper, the history of writing of all cultures of the world and modern artists’ books.\n\nThe Gutenberg Museum was originally laid out in two rooms at the Kurfürstliches Schloß (Electoral Palace Mainz), which also accommodated the city library. The museum moved into the new library building on the Rheinallee in 1912. The same year, 1925, saw the installation of a reconstruction of Gutenberg’s workshop which soon became one of the museum’s main attractions. Type founding, typesetting and printing could now be demonstrated visually. The replica of Gutenberg’s printing press, rebuilt according to the 15th- and 16th-century woodcuts, proved an object of great interest to visitors and was henceforth shown at a large number of exhibitions all over the world.\n\nIn 1927 the museum was able to move into the \"Zum Römischen Kaiser\" (1664), one of the most beautiful buildings in Mainz. This is now where the museum’s administration, the restoration workshop, library and Gutenberg Society are housed. The Late Renaissance building was heavily bombed in 1945; the museum’s contents had been stored in a safe place and thus remained intact. In 1962, the restoration of the Römischer Kaiser was complete. A new, modern exhibition building was also opened in the place where once the guest house \"König von England\" stood.\n\nThe museum made several important acquisitions in the following years, among them a second Gutenberg Bible, the Shuckburgh Bible in two volumes (1978), and two block books printed using wooden formes and today extremely rare. Another major change was the introduction of the museum’s educational unit in 1989. In 2000, the old museum building was restored and extended.\n\n\n\n"}
{"id": "51180310", "url": "https://en.wikipedia.org/wiki?curid=51180310", "title": "Hawks family", "text": "Hawks family\n\nThe Hawks family (c.1750 – 1889) was one of the largest and most powerful British dynasties to arise during the British Industrial Revolution. It owned several companies in the North and the City of London - including Hawks and Co., Hawks, Crawshay, and Stanley, and Hawks, Crawshay and Sons - all of which featured the Hawks name in the company name and had iron manufacture and engineering as their main enterprises.\n\nThe Hawks company reached the apogee of its power in the early Victorian period, during which it employed over 2000 persons, and its reputation for engineering and bridge building was worldwide. Its Gateshead factories were termed New Deptford and New Woolwich after the location of two of its warehouses on the River Thames, Deptford and Woolwich. The company owned its own ships, which it used to transport its manufacture. It built the striking High Level Bridge across the River Tyne that was opened by Queen Victoria in 1849, bridges as far afield as Constantinople and India, and lighthouses in France. It produced ironclad warships and other materials for the Royal Navy to exponential profits during the Napoleonic Wars and completed several large contracts for the East India Company. It also produced the first iron boat, the Vulcan, in 1821.\n\nSeveral members of the Hawks family were involved in merchant banking, several in Freemasonry, and several in the advocacy of Whig free-trade politics. Notable members included Sir Robert Shafto Hawks (1768-1840); Joseph Hawks (1791-1873), merchant banker and Sheriff of Newcastle; George Hawks (1801-1863), Freemason and Grand Master of the Grand Cross Chapter of the Holy Temple of Jerusalem \n(Knights Templar), and Mayor of Gateshead; Mary Hawks (b.1829), who was the wife of Richard Clement Moody, the founder of British Columbia; and Colonel Richard Stanley Hawks Moody (1854-1930).\n\nThe family developed areas of West London, including Pembroke Square, Kensington.\nThe poet Joseph Skipsey worked for the Hawks company, at their Gateshead ironworks, from 1859 to 1863, until one of his children was killed in a fatal accident at the works in 1863. The job was obtained for Skipsey by James Thomas Clephan, the editor of the \"Gateshead Observer\".\n\nThe Hawks company was established by William Hawks (1708–1755), who worked for Sir Ambrose Crowley, Sheriff of London. In the late 1740s, Hawks established a set of workshops on waste ground along the river at Gateshead. When Hawks died - at Gateshead on 23 February 1755 - the works passed to the eldest son, William Hawks (bap. 1730, d. 1810), who, with his first wife, Elizabeth Dixon, established the Hawks' industrial empire. William (d.1810) formed a partnership with Thomas Longridge (bap. 1751, d. 1803), in 1770, and shortly afterwards acquired a plating forge at Beamish, County Durham, which was the first of four separate metalworking sites operated by Hawks and Longridge along Beamish Burn.\n\nIn the 1780s, a forge at Lumley, in County Durham, and slitting and rolling mills, on the River Blyth in Northumberland, were acquired by the company.\n\nBy 1790, the works at Gateshead consisted of a substantial industrial complex that produced steel, anchors, heavy chains, steam-engine components, and a great diversity of smaller iron wares. The supply of ironware to the Board of Admiralty became a speciality. The firm's products were transported by the Gordon and Stanley families, the latter of whom had links to the old ordnance industry of the Weald and the naval yards of the River Thames and the Medway. The family also owned the Bedlington Ironworks during this period.\n\nOn 4 December 1810, the estate and works of William Hawks passed to his surviving sons: George Hawks (1766–1820) of Blackheath, Sir Robert Shafto Hawks (1768–1840), and John Hawks (1770–1830).\nThe Hawks Company built Hawks Cottages in the 1830s in the Saltmeadows district of Gateshead for its workers. \nThe Hawks works covered 44 acres by the end of the 1830s, and employed between 800 and 900 persons. At the time of the visit of the British Association to Newcastle in 1863, it employed 1500 people, and owned 92 marine engines and 58 land engines, which together provided 5000 horse power, and 33 puddling furnaces. \n\nThe family's New Greenwich ironworks at Gateshead was the town's largest employer until its closure, in mysterious circumstances, in 1889. \n\nThe poet Joseph Skipsey worked for the Hawks company, at their Gateshead ironworks, from 1859 to 1863, until one of his children was killed in a fatal accident at the works in 1863. The job was obtained for Skipsey by James Thomas Clephan, the editor of the \"Gateshead Observer\".\nThe first iron boat to be built, which was a rowing boat that was named the Vulcan, was constructed, in 1821, at the Hawks's ironworks. When Sir Robert Shafto Hawks was informed of the purpose for which Samuel Tyne, the boat's inventor, had purchased iron from the Hawks company, he proffered all of the iron required for the task free of charge. Sir Robert arranged for cannons to be fired at the launch of the boat, which subsequently won races against wooden boats of the same capacity.\n\nHowever, on Ascension Day, 1826, when, laden with 12 persons including the rowers of the Vulcan, a boat accompanying the Mayor's barge down the river was hit by a steam vessel, two of the Vulcan's rowers were killed, and the Vulcan was subsequently abandoned.\n\nAt around the year 1842, the Hawks company erected a cast iron bridge at York, which spans the river Ouse in one arch of 172 feet in width. They company also reconstructed the Rowland Burdon iron bridge at Sunderland, which consists of a single arch of a width of 237 feet. The company also constructed the wrought iron gates for the Northumberland Docks; the iron lighthouses at Gunfleet, Harwich, and Calais; and the iron pier at Madras. The company also built bridges in Constantinople. Sir Robert Hawks financed the construction of St John's Church, Gateshead Fell. The company built the High Level Bridge over the Tyne, which consisted of 5050 tons of iron. George Hawks, the Mayor of Gateshead (see below), drove in the last key of the structure on 7 June 1849. The bridge was opened by Queen Victoria later that year. \n\nThe company produced ironclad warships and other materials for the Royal Navy to exponential profits during the Napoleonic Wars, and completed several large contracts for the East India Company It also built paddle steamers and hydraulic dredgers.\n\nThe family developed also developed areas of London, including Pembroke Square, Kensington.\n\nRobert Shafto Hawks became the director of the firm subsequent to the death of his father. He was knighted by the Prince Regent, in 1817, for his role in suppressing riots in the winter of that year. Shafto Hawks was involved in Freemasonry and served as Worshipful Master of the oldest lodge in Northumberland.\n\nShafto Hawks is commemorated in Newcastle Cathedral, and his portrait hangs in Shipley Art Gallery. \n\nSir Robert married Hannah Pembroke Akenhead (1766 - 1863) in 1790 and had two sons. One son, William, entered the church. Sir Robert and Lady Hannah's blind son, David, was a musical prodigy who composed and published marches for military bands at 9 years of age. He later specialized in the composition of Tyrolean, Scottish, and Welsh airs. He was described as showing 'a most amazing proof of musical genius and early proficiency' when 17 years of age and as a 'true musical genius'.\n\nSir Robert's nephew George Hawks (1801–1863) , of Redheugh Hall, succeeded him as head of the Hawks company. George Hawks was an advocate of free-trade, and a vehement supporter of the Whigs and Sir William Hutt MP, who was MP for Gateshead after 1841. Hutt campaigned to have George Hawks knighted. Hawks's home, Redheugh Hall, became one of the organizing centres of Liberalism in the north east of England. \n\nGeorge Hawks served as the first Mayor of Gateshead in 1836, and, subsequently, served in the same position again in 1848 and 1849.\n\nGeorge Hawks was extensively involved in Freemasonry. George Hawks served as Grand Master of the Grand Cross Chapter of the Holy Temple of Jerusalem \n(Knights Templar). He also served as Past Master of the Lodge of Newcastle upon Tyne, Deputy of the Provincial Grand Lodge of Northumberland, and a Full Affiliated Member of The Celtic Lodge of Edinburgh and Leith, No.291. Hawks was made a Freemason in Guernsey, and was later described as 'an excellent mason'.\n\nSir Robert's nephew Joseph Hawks, , of Jesmond House, Newcastle upon Tyne, was a merchant banker who served as Sheriff of Newcastle. He married Mary Elizabeth Boyd, daughter of William Boyd, merchant banker of the prominent Boyd merchant banking family that founded the bank of Newcastle. Mary Boyd's brother was industrialist Edward Fenwick Boyd. William Boyd was a descendant of Sir Francis Liddell, the second son of Sir Thomas Liddell, 1st Baronet, whose family monopolized the government of the North of England in the 16th and 17th centuries, and his second wife, Frances Forster, previously wife of Nicholas Forster of Bamburgh Castle, Northumberland, and daughter of Sir William Chaytor of Croft.\n\nJoseph's daughter, Mary Susannah Hawks, married Richard Clement Moody, the founder of British Columbia. Her 13 children included Josephine 'Zeffie' Moody, who married Arthur Newall, son of Robert Stirling Newall, a close business associate of the Hawks family, Colonel Richard Stanley Hawks Moody, and Colonel Henry de Clervaulx Moody. Richard Clement Moody named the 400-foot hill in Port Coquitlam, British Columbia, \"Mary Hill\" after his wife, Mary. The Royal British Columbia Museum possesses a trove of 42 letters written by Mary Moody from various colonies of the British Empire, mostly from the Colony of British Columbia (1858–66), to her mother and her sister, Emily Hawks, in England. Mary Moody was highly literate, having been tutored in literature, penmanship, and French, and her letters have been of great interest to scholars studying the perspective of the English ruling class in the colonies of the British Empire.\n\nColonel Richard Stanley Hawks Moody was a distinguished officer of the British Army.\n\nHannah, Lady Hawks (d. 1863), the widow of Sir Robert Shafto Hawks, and her two sons sold their shares, in 1840, to George Crawshay a member of a prominent iron-making family of south Wales. George Crawshay had been bought out of his family's iron merchanting business in London by his brother, William Crawshay II. The business developed by William Hawks had been divided between his three eldest surviving sons in 1810. When he was able to acquire the shares of Joseph Hawks, the only surviving son of George Hawks of Blackheath, he obtained a second third of the company. The Bedlington works passed eventually to a cousin of the Hawkses, Michael Longridge (1785–1853), a pioneer of railway technology, and an associate of Robert Stephenson, under whose superintendence the works became a training ground for a generation of celebrated engineers; including Sir Daniel Gooch.\n\nThe dominant influence over the firm of Hawks, Crawshay & Sons in its last years was George Crawshay (1821–1896), the son of George Crawshay and his wife, Like George Hawks, George Crawshay became a considerable political figure in the north-east. However, Crawshay's management of the company was incompetent: he devoted much of his time to political pursuits and failed to modernize the company, which manufactured diverse products by diverse processes, to enable it to outcompete the newer specialist companies. The specialist companies, such as those owned by William Armstrong, 1st Baron Armstrong and the William Galloway, the nail manufacturer, flourished as the Hawks company declined.\n\nIn 1889, New Greenwich was suddenly closed, and Hawks, Crawshay, and Sons was liquidated. The circumstances surrounding the closure are mysterious: all of the firm's creditors were paid in full, but every single document of the company's archive was systematically destroyed by burning.\n\n"}
{"id": "1606301", "url": "https://en.wikipedia.org/wiki?curid=1606301", "title": "Hunter process", "text": "Hunter process\n\nThe Hunter process was the first industrial process to produce pure ductile metallic titanium. It was invented in 1910 by Matthew A. Hunter, a chemist born in New Zealand, who worked in the US. The process involves reducing titanium tetrachloride (TiCl) with sodium (Na) in a batch reactor with an inert atmosphere at a temperature of 1,000°C. Dilute hydrochloric acid is then used to leach the salt from the product. \n\nPrior to the Hunter process, all efforts to produce Ti metal afforded highly impure material, often titanium nitride (which resembles a metal). The Hunter process was replaced by the more economical Kroll process in the 1940s. In the Kroll Process, TiCl is reduced by magnesium.\n"}
{"id": "29160726", "url": "https://en.wikipedia.org/wiki?curid=29160726", "title": "Impulse generator", "text": "Impulse generator\n\nAn impulse generator is an electrical apparatus which produces very short high-voltage or high-current surges. Such devices can be classified into two types: impulse voltage generators and impulse current generators. High impulse voltages are used to test the strength of electric power equipment against lightning and switching surges. Also, steep-front impulse voltages are sometimes used in nuclear physics experiments. High impulse currents are needed not only for tests on equipment such as lightning arresters and fuses but also for many other technical applications such as lasers, thermonuclear fusion, and plasma devices.\n\nIn 1863 Hungarian physicist Ányos Jedlik discovered the possibility of voltage multiplication and in 1868 demonstrated it with a \"tubular voltage generator\", which was successfully displayed at the Vienna World Exposition in 1873. It was an early form of the impulse generators now applied in nuclear research.\n\nThe jury of the World Exhibition of 1873 in Vienna awarded his voltage multiplying condenser of cascade connection with prize \"For Development\". Through this condenser, Jedlik framed the principle of surge generator of cascaded connection. (The Cascade connection was another important invention of Ányos Jedlik.)\n\nOne form is the Marx generator, after E. Marx who first proposed it in 1923. This consists of multiple capacitors that are first charged in parallel through charging resistors by a high-voltage, direct-current source and then connected in series and discharged through a test object by a simultaneous spark-over of the spark gaps. The impulse current generator comprises many capacitors that are also charged in parallel by a high-voltage, low-current, direct-current source, but it is discharged in parallel through resistances, inductances, and a test object by a spark gap.\n\n\n"}
{"id": "31916669", "url": "https://en.wikipedia.org/wiki?curid=31916669", "title": "Irene Muloni", "text": "Irene Muloni\n\nIrene Nafuna Muloni is a Ugandan electrical engineer, businesswoman and politician. She is the Cabinet Minister for Energy and Minerals in the Ugandan Cabinet. She was first appointed to that position on 27 May 2011. In the new cabinet list released after the 2016 national elections, she maintained her position. She served as the elected Member of Parliament for Bulambuli District Women's Representative, from 2011 until 2016. In 2016, she lost her seat to Sarah Nambozo Wekomba, an Independent.\n\nShe was born on 18 November 1960, in what is known today, as Bulambuli District.She attended \"Budadiri Girls' Primary School\" before entering Gayaza High School. In 1982, she entered Makerere University, the oldest university in East Africa, to study Engineering. In 1986, she graduated with an honours Bachelor of Science in Electrical Engineering (BSc.E.Eng) degree. Later, she graduated with the degree of Master of Business Administration (MBA), from Capella University in Minneapolis, Minnesota, United States. She is also a Certified Public-Private Partnership Specialist, accredited by The Institute for Public-Private Partnerships, Inc. (IP3) and the Water, Engineering and Development Centre (WEDC) of Loughborough University.\n\nFrom 2002 until 2011, Irene Muloni worked as the Managing Director of the Uganda Electricity Distribution Company Limited (UEDCL), a Ugandan parastatal company, responsible for distribution of electrical power to both commercial and retail customers nationwide. In 2011, she entered elective politics, by successfully contesting for the Bulambuli District Women's Representative in the 9th Ugandan Parliament (2011 - 2016). On 27 May 2011, she was appointed Minister of Energy & Minerals, by President Yoweri Museveni. She replaced Hilary Onek, who was appointed Minister of Internal Affairs. In the cabinet changes made on 6 June 2016, she maintained her cabinet appointment.\n\n\n"}
{"id": "350578", "url": "https://en.wikipedia.org/wiki?curid=350578", "title": "Julie Payette", "text": "Julie Payette\n\nJulie Payette (born October 20, 1963) is the current Governor General of Canada, the 29th since Canadian Confederation. Before assuming office, she was a businesswoman, former member of the Canadian Astronaut Corps, and engineer. Payette has completed two spaceflights, STS-96 and STS-127, logging more than 25 days in space. She served as chief astronaut for the Canadian Space Agency (CSA), and has served as capsule communicator at NASA Mission Control Center in Houston.\n\nIn July 2013, Payette was named chief operating officer for the Montreal Science Centre, and in April 2014, she was appointed to the board of directors of the National Bank of Canada. On July 13, 2017, Prime Minister Justin Trudeau announced that Queen Elizabeth II had approved the appointment of Payette as the next Governor General of Canada. She was sworn in on October 2, 2017.\n\nAs Governor General, Payette is entitled to be styled Her Excellency while in office and The Right Honourable for life.\n\nPayette was born on October 20, 1963, in Montreal, Quebec, and lived in the Ahuntsic neighbourhood, attending Collège Mont-Saint-Louis and Collège Regina Assumpta. In 1982 she completed an International Baccalaureate diploma at the United World College of the Atlantic in South Wales, United Kingdom.\n\nFor her undergraduate studies, Payette enrolled in McGill University where she completed a Bachelor of Engineering degree in electrical engineering in 1986, after which she completed a Master of Applied Science degree in computer engineering at the University of Toronto in 1990. Her thesis focused on computational linguistics, a field of artificial intelligence. She is a member of the Ordre des ingénieurs du Québec\n\nDuring her schooling, between 1986 and 1988, Payette also worked as a systems engineer for IBM Canada's Science Engineering division. From 1988 to 1990, as a graduate student at the University of Toronto, she was involved in a high-performance computer architecture project and worked as a teaching assistant. At the beginning of 1991, Payette joined the Communications and science department of the IBM Zurich Research Laboratory in Switzerland, for a one-year visiting scientist appointment. When she returned to Canada, in January 1992, she joined the Speech Research Group of Bell-Northern Research in Montreal where she was responsible for a project in telephone speech comprehension using computer voice recognition.\n\nPayette was married twice, first to François Brissette in the 1990s, and secondly to William Flynn, with whom she had a son in 2003, and from whom she divorced in 2015. She is fluent in French (her mother tongue) and English, and can converse in Spanish, German, Italian and Russian. She plays the piano and has sung with the Montreal Symphony Orchestra, Tafelmusik Chamber Choir and several others. She practices running, skiing, racquet sports and scuba diving.\n\nPayette was selected by the Canadian Space Agency (CSA) as one of four astronauts from a field of 5,330 applicants in June 1992. After undergoing basic training in Canada, she worked as a technical advisor for the Mobile Servicing System, an advanced robotics system and Canada's contribution to the International Space Station. In 1993, Payette established the Human-Computer Interaction Group at the Canadian Astronaut Program and served as a technical specialist on the NATO International Research Study Group on speech processing.\n\nIn preparation for a space assignment, Payette obtained her commercial pilot licence and logged 120 hours as a research operator on reduced gravity aircraft. In April 1996, Payette was certified as a one-atmosphere deep sea diving suit operator. Payette obtained her captaincy on the CT-114 Tutor military jet at CFB Moose Jaw in February 1996 and her military instrument rating in 1997. Payette has logged more than 1,300 hours of flight time, including 600 hours on high-performance jet aircraft.\n\nPayette reported to the Johnson Space Center in August 1996 to begin mission specialist training. After completing one year of training, she was assigned to work on the Mobile Servicing System. Payette completed the initial astronaut training in April 1998.\n\nPayette served as chief astronaut for the Canadian Space Agency from 2000 to 2007. She also worked as capsule communicator at the Mission Control Center in Houston for several years, including the return to flight mission STS-114. She was lead capsule communicator during STS-121.\n\nPayette flew on the Space Shuttle \"Discovery\" from May 27 to June 6, 1999, as part of the crew of STS-96. During the mission, the crew performed the first manual docking of the shuttle to the International Space Station, and delivered four tons of logistics and supplies to the station. On \"Discovery\", Payette served as a mission specialist. Her main responsibility was to operate the Canadarm robotic arm from the space station. The STS-96 mission was accomplished in 153 orbits of the Earth, traveling over in 9 days, 19 hours and 13 minutes. Payette became the first Canadian to participate in an ISS assembly mission and to board the Space Station.\n\nPayette visited the space station again in 2009 as a mission specialist aboard Space Shuttle \"Endeavour\" during mission STS-127 from July 15 to 31, 2009, and was the flight engineer and lead robotic operator during the mission. At that time, Robert Thirsk was a member of Expedition 20 on the space station. \"Endeavour\"s docking at the space station marked the first time two Canadians met in space.\n\nDuring her second mission, Payette brought a signed sweater of the famed Montreal Canadiens player Maurice Richard, stating she had brought Richard, who was known as \"The Rocket\", into the rocket to celebrate the hockey team's 100th anniversary.\n\nDuring 2010–2011, she worked at the Woodrow Wilson International Center for Scholars in Washington, D.C. and was also a scientific delegate to the United States for the Quebec Government.\n\nPayette has served on boards of directors, at Queen's University, Canada's Own the Podium Olympic program, Montréal Science Centre foundation, Robotique FIRST Québec, Drug Free Kids Canada, the Montreal Bach Festival, the National Bank of Canada, Développement Aéroport Saint-Hubert de Longueuil, and others. She was recently appointed to the International Olympic Committee's Women in Sports Commission. She is a member of the Ordre des ingénieurs du Québec and a fellow of the International Academy of Astronautics. As well, Payette is a member of the Faculty of Engineering Advisory Board of McGill University.\n\nPayette was announced on July 13, 2017, as Prime Minister Justin Trudeau's recommendation to be the 29th Governor General of Canada. Her term was scheduled to begin October 2, 2017, after the completion of briefings from the incumbent, David Johnston. Her salary was set at $290,660 per annum and an official residence at Rideau Hall. After the announcement was made, Johnston issued a statement congratulating Payette and welcoming \"a Canadian of extraordinary achievement, admired by all\".\n\nAs the designate, Payette had her first official meeting with Queen Elizabeth II on September 20, 2017, at Balmoral Castle when she was also invested as an Extraordinary Companion of the Order of Canada (CC), an Extraordinary Commander of the Order of Military Merit (CMM) and a Commander of the Order of Merit of the Police Forces (COM) by Her Majesty.\n\nPayette was installed as Canada's 29th Governor General on October 2, 2017. Afterwards, she urged Canadians to work together on issues such as climate change, migration and poverty. \"Anyone can accomplish anything and rise to the challenge as long as they are willing to work with others, to let go of the personal agenda, to reach a higher goal and to do what is right for the common good. This is exactly what I hope my mandate as the Governor General will reflect,\" Payette said. At the Canadian Science Policy Conference the next month, she argued strongly for greater public acceptance of science, saying that too many people believe in astrology, deny climate change, and believe that \"maybe taking a sugar pill will cure cancer.\" Committee for Skeptical Inquiry concerning climate change, commented that her remarks were \"refreshing\". George Dvorsky from Gizmodo.org stated '\"Her words were a breath of fresh air\"'.\n\nAs she was completing her first year as the viceregal representative in September 2018, Payette faced some criticism about controversial comments she had made against those who believe in creationism and those who did not believe in climate change. Maintaining her position on the issues, Payette emphasized the importance of debate and critical thinking but admitted that she was still growing into her role and needed more time to adapt to the position. \"I learned that you have to be careful about how you say things, but not what you say,\" she added. Some time later, she offered an additional explanation to CPAC. \"I made a speech as I had as an astronaut and I'm not an astronaut any longer, I'm governor general. I represent all Canadians. I've learned those lessons.\"\n\nWeeks later, she faced criticism about her work ethic, with some suggesting that she had not devoted enough time and dedication to the role of GG and had not visited several of the provinces in her nearly 12-month tenure. The \"Toronto Star\" published specifics confirming the more numerous appearances her two predecessors had made per year. Rideau Hall spokesperson Marie-Ève Létourneau said that \"The first year of every mandate is a period of learning, adjustment and adaptation from both the Governor General and Rideau Hall staff.\" The \"Globe and Mail\" opinion piece offered this conclusion on Payette's tenure to date. \"Canadians need to know that the Governor-General is fulfilling her duties. Her biggest challenge right now is to convince us that she is worthy of our trust\".\n\nLater in September, the GG's office confirmed that Payette would not preside over the 2018 Governor General's History Awards ceremony. This may have been only one of several traditional roles that she would not fulfill; a full plan for her involvement in such discretionary responsibilities with various organizations was to be published but was not yet available at the time.\n\nOn September 27, Payette acknowledged the articles that had painted an \"unfavourable image of our work\" in an e-mail to staff, expressed regret about the effect of the criticisms on morale, and assured them that she was \"very proud of all we have achieved together to date\".\n\n\n\n\n\n\nPayette holds 28 honorary doctorates, Some of the honorary degrees she has received:\n\n\nSchools\n\nPayette assisted in the carrying of the Olympic flag in the opening ceremonies of the 2010 Olympic Winter Games in Vancouver, British Columbia.\n\n\n"}
{"id": "22261589", "url": "https://en.wikipedia.org/wiki?curid=22261589", "title": "List of lighting design applications", "text": "List of lighting design applications\n\nThis is a list of Lighting Design software for use in analyzing photometrics, BIM (Building Information Modeling), and 3D modeling. The software is typically used by importing the structural design via CAD files. Then lighting elements are inserted. And finally, the lighting objects are associated with a photometry via IES files. The photometry of a light fixture describes the way it distributes its light into space. Once this process is completed, the illuminance and luminance produced by each fixture in the space can be calculated. The output is typically a diagram indicating these by means of colors or numbers. This typically is the goal of technical photometry software.\n\nIn marketing and higher-level design, 3D photometric analysis is useful to give a graphical (no numerics) output of a proposed design.\n\n\n"}
{"id": "49761762", "url": "https://en.wikipedia.org/wiki?curid=49761762", "title": "List of video games derived from mods", "text": "List of video games derived from mods\n\nThis is a list of standalone video games that have been ported from a modification of another video game, and/or that are entirely based on a modification of another video game. A game is considered standalone when it does not require the purchase or installation of any other game (including separate engine software such as the Source SDK) in order to run.\n\n"}
{"id": "1342872", "url": "https://en.wikipedia.org/wiki?curid=1342872", "title": "Memory management controller", "text": "Memory management controller\n\nMulti-memory controllers or memory management controllers (MMC) are different kinds of special chips designed by various video game developers for use in Nintendo Entertainment System (NES) cartridges. These chips extend the capabilities of the original console and make it possible to create NES games with features the original console cannot offer alone.\n\nThese chips are also known as \"mappers\".\n\n\nEarly NES MMCs are composed of 7400 series discrete logic chips. The UNROM, implemented with two such chips, divides the program space into two 16 KB banks. The MMC allows a program to switch one bank while keeping one bank always available. Instead of a dedicated ROM chip to hold graphics data (called CHR by Nintendo), games using UNROM store graphics data on the program ROM and copy it to a RAM on the cartridge at run time.\n\nThe MMC1 is Nintendo's first custom MMC integrated circuit to incorporate support for saved games and multi-directional scrolling configurations.\nThe chip comes in at least five different versions: \"MMC1A\", \"MMC1B1\", \"MMC1B2\", \"MMC1B3\" and \"MMC1C\". The differences between the different versions are slight, mostly owing to savegame memory protection behavior. The MMC1 chip allows for switching of different memory banks. Program ROM can be selected in 16KB or 32KB chunks, and character ROM can be selected in 4KB or 8KB chunks. An unusual feature of this memory controller is that its input is serial, rather than parallel, so 5 sequential writes (with bit shifting) are needed to send a command to the circuit.\n\nThe MMC2 is only used in \"Mike Tyson's Punch-Out!!\" and the later rerelease which replaced Mike Tyson. A single 8KB bank of program ROM can be selected (with the remaining 24KB locked) and character ROM can be selected in \"two pairs\" of 4KB banks, which would be automatically switched when the video hardware attempts to load particular graphic tiles from memory, thus allowing a larger amount of graphics to be used on the screen without the need for the game itself to manually switch them.\n\nThe MMC3 is Nintendo's most popular MMC chip. It comes in MMC3A, B, and C revisions. The MMC3 adds a scanline based IRQ counter to make split screen scrolling easier to perform (mainly to allow the playfield to scroll while the status bar remains motionless at the top or bottom of the screen), along with two selectable 8KB program ROM banks and two 2KB+four 1KB selectable character ROM banks.\n\nThis chip is only used in three games, all of which were released only for the Famicom in Japan, and were developed by Intelligent Systems. Functionally, it is nearly identical to the MMC2, with the only difference being that the MMC4 switches program ROM in 16KB banks instead of 8KB banks and has support for a battery-backed SRAM to save game data.\n\n\nThe MMC5 is Nintendo's largest MMC. It was originally also the most expensive. Only Koei used this chip regularly. The chip has 1 KB of extra RAM, two extra square wave sound channels, one extra PCM sound channel, support for vertical split screen scrolling, improved graphics capabilities (making 16,384 different tiles available per screen rather than only 256, and allowing each individual 8x8-pixel background tile to have its own color assignment instead of being restricted to one color set per 2x2 tile group), highly configurable program ROM and character ROM bank switching, and a scanline-based IRQ counter.\n\nThe MMC6 is similar to the MMC3, with an additional 1 KB of RAM which can be saved with battery backup.\n\nThe Famicom Disk System's ASIC is an extended audio chip, which supports one channel of single-cycle (6-bit × 64 step) wavetable-lookup synthesis with a built in phase modulator (PM) for sound generation similar to frequency modulation synthesis.\n\nThe A*ROM MMC, named after the AMROM, ANROM, and AOROM cartridge boards that use it, was developed by Chris Stamper of Rare, and manufactured by Nintendo. It is found in games developed by Rare for Nintendo, Tradewest, GameTek, Acclaim, and Milton Bradley. It uses 32 KB ROM switch and a CHR RAM. Unlike other chips, it uses one screen mirroring.\n\nAs Nintendo's NES licensing program allowed North America and Europe distribution to include only first-party hardware in its cartridges, these third party chips were supposed to be used only in Japan. However, some of them had been found in a few North American cartridges as well.\n\n\nThe VRC2 is a chip from Konami that allows program ROM to be switched in 8KB banks, and character ROM to be switched in 1KB banks. This MMC has two known revisions: VRC2a and VRC2b.\n\nExclusively used in the Japanese version of \"Salamander\".\n\n\nThe VRC6 (Virtual Rom Controller) is an advanced MMC chip from Konami, supporting bank switching for both program code and graphics as well as a CPU cycle–based IRQ counter, which can also act as a scanline counter. The chip also contains support for three extra sound channels (two square waves of eight duty cycles each, and one sawtooth wave). It is used in \"Akumajō Densetsu\" (the Japanese version of \"\"), while the Western version uses the MMC5 from Nintendo. Since the Nintendo Entertainment System does not allow cartridges to add additional sound channels, the Famicom version's soundtrack was reworked to follow those specifications; thus, the soundtrack on the Western version is implemented by the five sound channels built into the stock NES.\n\nThe VRC7 is a very advanced MMC chip from Konami, not only supporting bank switching and IRQ counting equivalent to the VRC6 but also containing a YM2413 derivative providing 6 channels of FM Synthesis audio. This advanced audio is used only in the Famicom game \"Lagrange Point\"; while the Japanese version of \"Tiny Toon Adventures 2\" also used the VRC7, it does not make use of the extended audio.\n\nThe 163 has been only used in games exclusive to Japan. Its capabilities were a little better than Nintendo's MMC3. A variant contained extra sound hardware that plays 4-bit wave samples. It supports 1 to 8 extra sound channels, but audible aliasing appears when a sufficiently large number of channels are enabled.\n\nThe FME-7 is a memory mapping circuit developed by Sunsoft for use in NES and Famicom cartridges. It switches program ROM in 8KB banks and switches the character ROM 1KB banks. It also contains hardware to generate IRQ signals after a specified number of CPU clock cycles, thus achieving split-screen effects with minimal use of processing power. A special version of this MMC, labeled as \"SUNSOFT 5B\" rather than \"FME-7\", contains a version of the widely used Yamaha YM2149. This sound generation hardware is used on only one Famicom title: \"Gimmick!\".\n\n\nSome individual (homebrew) and unlicensed developers have made custom MMCs for the NES, most of which simply expand the available memory.\n\n228 is a simple bank switching MMC developed for use in the games \"Action 52\" and \"Cheetahmen II\". It does not have a nametable control bit. In the \"Action 52\" multicart, it also contains a small 16-bit register area that contains the old menu selection when exiting a game.\n\n\n"}
{"id": "2277747", "url": "https://en.wikipedia.org/wiki?curid=2277747", "title": "Metal clay", "text": "Metal clay\n\nMetal clay is a crafting medium consisting of very small particles of metal such as silver, gold, bronze, or copper mixed with an organic binder and water for use in making jewelry, beads and small sculptures. Originating in Japan in 1990, metal clay can be shaped just like any soft clay, by hand or using molds. After drying, the clay can be fired in a variety of ways such as in a kiln, with a handheld gas torch, or on a gas stove, depending on the type of clay and the metal in it. The binder burns away, leaving the pure sintered metal. Shrinkage of between 8% and 30% occurs (depending on the product used). Alloys such as bronze, sterling silver, and steel also are available.\n\nMetal clay first came out in Japan in 1990 to allow craft jewelry makers to make sophisticated looking jewelry without the years of study needed to make fine jewelry.\n\nFine silver metal clay results in objects containing 99.9% pure silver, which is suitable for enameling. Lump metal clay is sold in sealed packets to keep it moist and workable. The silver versions are also available as a softer paste in a pre-filled syringe which can be used to produce extruded forms, in small jars of slip and as paper-like sheets, from which most of the moisture has been removed. Common brands of silver metal clay include Precious Metal Clay (PMC) and Art Clay Silver (ACS).\n\nMetal clay artists looking for more strength in their silver creations can also mix PMC fine silver clay with an equal part of PMC Sterling clay. The firing of this alloy is found to be up to for two hours.\n\nAnother available alloy, EZ960 Sterling Silver Metal Clay was invented by Bill Struve from Metal Adventures, the inventor of BRONZclay™ and COPPRclay™. Because the clay is a sterling silver alloy, one of its best attributes is its post firing strength, in comparison to fine silver. This clay is fired open shelf on a raised hard ceramic kiln shelf at for 2 hours, full ramp. No carbon required. Its shrinkage rate is smaller than other clays, at 10–11%.\n\nPMC was developed in the early 1990s in Japan by metallurgist Masaki Morikawa. As a solid-phase sintered product of a precious metal powder used to form a precious metal article, the material consists of microscopic particles of pure silver or fine gold and a water-soluble, non-toxic, organic binder that burns off during firing. Success was first achieved with gold and later duplicated with silver. \nThe PMC brand includes the following products:\n\nACS was developed by AIDA Chemical Industries, also a Japanese company. ACS followed PMC Standard with their Art Clay Original clay (more like PMC+ than PMC Standard), which allows the user to fire with a handheld torch or on a gas hob. Owing to subtle differences in the binder and suggested firing times, this clay shrinks less than the PMC versions, approximately 8–10%.\n\nFurther developments introduced the Art Clay Slow Dry, a clay with a longer working time. Art Clay 650 and Art Clay 650 Slow Dry soon followed; both clays can be fired at , allowing the user to combine the clay with glass and sterling silver, which are affected negatively by the higher temperatures needed to fire the first generation clays. AIDA also manufacturers Oil Paste, a product used only on fired metal clay or milled fine silver, and Overlay Paste, which is designed for drawing designs on glass and porcelain.\n\nIn 2006 AIDA introduced the Art Clay Gold Paste, a more economical way to work with gold. The paste is painted onto the fired silver clay, then refired in a kiln, or with a torch or gas stove. When fired, it bonds with the silver, giving a 22-carat gold accent. The same year also saw Art Clay Slow Tarnish introduced, a clay that tarnishes less rapidly than the other metal clays.\n\nLump metal clay in bronze was introduced in 2008 by Metal Adventures Inc. and in 2009 by Prometheus. Lump metal clays in copper were introduced in 2009 by Metal Adventures Inc. and Aida. Because of the lower cost, the bronze and copper metal clays are used by artists more often than the gold and silver metal clays in the American market place. The actual creation time of a bronze or copper piece is also far greater than that of its silver counterpart. Base metal clays, such as bronze, copper, and steel metal clays are best fired in the absence of oxygen to eliminate the oxidation of the metal by atmospheric oxygen. A means to accomplish this –- to place the pieces in activated carbon inside a container – was developed by Bill Struve.\n\nMetal clays are also available as dry powders to which water is added to hydrate and kneaded to attain a clay consistency. One advantage to the powders are their unlimited shelf life. The first silver clay in powder form was released in 2006 as Silver Smiths' Metal Clay Powder. In the following years base metal clays by Hadar Jacobson and Goldie World released several variation containing copper, brass, and even steel.\n\n"}
{"id": "13591771", "url": "https://en.wikipedia.org/wiki?curid=13591771", "title": "Monochrome monitor", "text": "Monochrome monitor\n\nA monochrome monitor is a type of CRT computer monitor which was very common in the early days of computing, from the 1960s through the 1980s, before color monitors became popular. They are still widely used in applications such as computerized cash register systems, owing to the age of many registers. Green screen was the common name for a monochrome monitor using a green \"P1\" phosphor screen.\n\nAbundant in the early-to-mid-1980s, they succeeded Teletype terminals and preceded color CRTs and later LCDs as the predominant visual output device for computers.\n\nUnlike color monitors, which display text and graphics in multiple colors through the use of alternating-intensity red, green, and blue phosphors, monochrome monitors have only one color of phosphor (\"mono\" means \"one\", and \"chrome\" means \"color\"). All text and graphics are displayed in that color. Some monitors have the ability to vary the brightness of individual pixels, thereby creating the illusion of depth and color, exactly like a black-and-white television.\n\nTypically, only a limited set of brightness levels was provided to save display memory which was very expensive in the '70s and '80s. Either normal/bright or normal/dim (1 bit) per character as in the VT100 or black/white per pixel in the Macintosh 128K or black, dark gray, light gray, white (2bit) per pixel like the NeXT MegaPixel Display.\n\nMonochrome monitors are commonly available in three colors: if the P1 phosphor is used, the screen is green monochrome. If the P3 phosphor is used, the screen is amber monochrome. If the P4 phosphor is used, the screen is white monochrome (known as \"paper white\"); this is the same phosphor as used in early television sets.\nAn amber screen was claimed to give improved ergonomics, specifically by reducing eye strain; this claim appears to have little scientific basis. However, the color amber is a softer light, and would be less disruptive to a user's circadian rhythm. \n\nWell-known examples of early monochrome monitors are the VT100 from Digital Equipment Corporation, released in 1978, the Apple Monitor III in 1980, and the IBM 5151, which accompanied the IBM PC model 5150 upon its 1981 release.\n\nThe 5151 was designed to work with the PC's Monochrome Display Adapter (MDA) text-only graphics card, but the third-party Hercules Graphics Card became a popular companion to the 5151 screen because of the Hercules' comparatively high-resolution bitmapped 720×348 pixel monochrome graphics capability, much used for business presentation graphics generated from spreadsheets like Lotus 1-2-3. This was much higher resolution than the alternative IBM Color Graphics Adapter 320×200 pixel, or 640×200 pixel graphic standard. It could also run most programs written for the CGA card's standard graphics modes. Monochrome monitors continued to be used, even after the introduction of higher resolution color IBM Enhanced Graphics Adapter and Video Graphics Array standards in the late 1980s, for dual-monitor applications.\n\nPixel for pixel, the monochrome monitors produce sharper text and images than color CRT monitors. This is because a monochrome monitor is made up of a continuous coating of phosphor and the sharpness can be controlled by focusing the electron beam; whereas on a color monitor, each pixel is made up of three phosphor dots (one red, one blue, one green) separated by a mask. Monochrome monitors were used in almost all dumb terminals and are still widely used in text-based applications such as computerized cash registers and point of sale systems because of their superior sharpness and enhanced readability.\n\nSome green screen displays were furnished with a particularly full/intense phosphor coating, making the characters very clear and sharply defined (thus easy to read) but generating an afterglow-effect (sometimes called a \"ghost image\") when the text scrolled down the screen or when a screenful of information was quickly replaced with another as in word processing page up/down operations. Other green screens avoided the heavy afterglow-effects, but at the cost of much more pixelated character images. The 5151, amongst others, had brightness and contrast controls to allow the user to set their own compromise.\n\nThe ghosting effects of the now-obsolete green screens have become an eye-catching visual shorthand for computer-generated text, frequently in \"futuristic\" settings. The opening titles of the first Ghost in the Shell film and the Matrix source code of the Matrix trilogy science fiction films prominently feature computer displays with ghosting green text. Green text is also featured in 's computer in \"Lost\" series.\n\nMonochrome monitors are particularly susceptible to screen burn (hence the advent, and name, of the screensaver), because the phosphors used are very high intensity. Another effect of the high-intensity phosphors is an effect known as \"ghosting\", wherein a dim afterglow of the screen's contents is briefly visible after the screen has been blanked. This has a certain place in pop culture, as evidenced in movies such as \"The Matrix\".\n\nThis ghosting effect is deliberate on some monitors, known as \"long persistence\" monitors. These use the relatively long decay period of the phosphor glow to reduce flickering and eye strain.\n"}
{"id": "18273210", "url": "https://en.wikipedia.org/wiki?curid=18273210", "title": "NFPA 1123", "text": "NFPA 1123\n\nNFPA 1123, subtitled \"Code for Fireworks Display\" is a code administered, copyrighted, and published by the National Fire Protection Association (NFPA). NFPA 1123 is the registered trademark of an American consensus standard which, like many NFPA documents, is systematically revised on a three year cycle.\n\nThe standard, despite its title, is not a legal code, it is not published as an instrument of law and has no statutory authority unless adopted by the authority having jurisdiction (AHJ). The standard, widely adopted in the United States, is however deliberately crafted with language suitable for mandatory application to facilitate adoption into law by those empowered to do so.\n\nThe scope of the document is described as below on the NFPA website:\n\n\"Document Scope: 1.1.1 This code shall apply to the construction, handling, and use of fireworks and equipment intended for outdoor fireworks display. It also shall apply to the general conduct and operation of the display. (See definition 1.4.21, Fireworks Display.) 1.1.2 This code shall not apply to the manufacture, transportation, or storage of fireworks at a manufacturing facility. Similarly, this code shall not apply to the testing of fireworks under the direction of its manufacturer, provided permission for such testing has been obtained from the authority having jurisdiction, which shall be in accordance with NFPA 1124, Code for the Manufacture, Transportation, and Storage of Fireworks and Pyrotechnic Articles. 1.1.3 This code shall not apply to the use of consumer fireworks by the general public. 1.1.4 This code shall not apply to the transportation, handling, or use of fireworks by the armed forces of the United States. 1.1.5 This code shall not apply to the transportation, handling, or use of industrial pyrotechnic devices or fireworks, such as railroad torpedoes, fusees, automotive, aeronautical, and marine flares, and smoke signals. 1.1.6 This code shall not apply to the use of pyrotechnic devices or materials in the performing arts at distances less than those specified in this code and used in conformance with NFPA 1126, Standard for the Use of Pyrotechnics before a Proximate Audience. 1.1.7 This code shall not apply to the use of flame special effects in the performing arts when used in conformance with NFPA 160, Standard for Flame Effects before an Audience. 1.1.8 This code shall not apply to the sale and use of model rockets, model rocket motors, motor reloading kits, pyrotechnic modules, or components used in conformance with NFPA 1122, Code for Model Rocketry, or other propulsion devices as classified by the U.S. Department of Transportation as Rocket Motors (UN0186), or Cartridges, power device (UN0275). 1.1.9 This code shall not apply to the use of explosives, firearms, or flammable special effects used in motion pictures, television, or other entertainment industries.\" \n\nThe NFPA 1123 was created to help prevent damage of property and the injury or death of individuals during outdoor firework displays.\n\nThis listing of sections from the 2006 edition shows the scope of the Code.\n\n\n2014\n2010 \n2006\n2000\n1995\n\nSalt Lake City, Utah \n\nNFPA 1123 v2010 Booklet on NFPA.org \nNFPA 1123 v2006 Booklet on ConstructionBook.com \n\n"}
{"id": "40494714", "url": "https://en.wikipedia.org/wiki?curid=40494714", "title": "Neurolixis", "text": "Neurolixis\n\nNeurolixis is a biopharmaceutical company focused on novel drugs for the treatment of human central nervous system diseases.\n\nNeurolixis Inc. was founded in 2011 and is managed by two pharmaceutical industry professionals, Mark A. Varney, PhD (Chief Executive Officer) and Adrian Newman-Tancredi, PhD, DSc (Chief Scientific Officer). The company's therapeutic focus is on CNS disorders including Parkinson's disease, neurological orphan disorders, depression and cognitive deficits of schizophrenia. The company has offices in Southern California and in France.\n\nIn September 2013, Neurolixis announced that it had in-licensed two clinical-phase drugs from Pierre Fabre Laboratories, a French pharmaceutical company. The drugs (befiradol and F-15599) are targeted to the treatment of dyskinesia in Parkinson's disease and to breathing deficits in Rett syndrome, respectively.\n\nNeurolixis has been awarded a series of research grants by the Michael J. Fox Foundation to undertake research examining the effects of novel, highly selective and efficacious serotonergic drugs targeting 5-HT1A receptors in brain regions relevant to therapeutic properties in Parkinson's disease. The Michael J. Fox Foundation subsequently announced that it was supporting proof-of-principle studies on befiradol (also known as NLX-112) in models of Parkinson's disease and showcased Neurolixis in its Partnering Program.. In January 2018, the British charity Parkinson's UK announced that it had awarded Neurolixis a grant to advance development of befiradol up to clinical phase in Parkinson's disease patients.\n\nF-15599 (also known as NLX-101) was awarded Orphan Drug Status by the United States Food and Drug Administration (FDA) in October 2013 and Orphan Medicinal Product designation by the European Medicines Agency in March 2014. \nIn collaboration with researchers at the University of Bristol, Neurolixis has been awarded a grant by the International Rett Syndrome Foundation to study F-15599 in animal models of Rett syndrome. In June 2015, Neurolixis was awarded a grant by the Rett Syndrome Research Trust to advance F-15599 to clinical development.\n"}
{"id": "7073120", "url": "https://en.wikipedia.org/wiki?curid=7073120", "title": "Nuclear criticality safety", "text": "Nuclear criticality safety\n\nNuclear criticality safety is a field of nuclear engineering dedicated to the prevention of nuclear and radiation accidents resulting from an inadvertent, self-sustaining nuclear chain reaction. Nuclear criticality safety is concerned with mitigating the consequences of a nuclear criticality accident. \nA nuclear criticality accident occurs from operations that involve fissile material and results in a sudden and potentially lethal release of radiation. Nuclear criticality safety practitioners attempt to prevent nuclear criticality accidents by analyzing normal and credible abnormal conditions in fissile material operations and designing safe arrangements for the processing of fissile materials. A common practice is to apply a double contingency analysis to the operation in which two or more independent, concurrent and unlikely changes in process conditions must occur before a nuclear criticality accident can occur. For example, the first change in conditions may be complete or partial flooding and the second change a re-arrangement of the fissile material. Controls (requirements) on process parameters (e.g., fissile material mass, equipment) result from this analysis. These controls, either passive (physical), active (mechanical), or administrative (human), are implemented by inherently safe or fault-tolerant plant designs, or, if such designs are not practicable, by administrative controls such as operating procedures, job instructions and other means to minimize the potential for significant process changes that could lead to a nuclear criticality accident.\n\nA system will be exactly critical if the rate of neutron production from fission is exactly balanced by the rate at which neutrons are either absorbed or lost from the system due to leakage. Safely subcritical systems can be designed by ensuring that the potential combined rate of absorption and leakage always exceeds the potential rate of neutron production. (this is a textbook explanation, similar to electron \"clouds\" being taught in highschool, and is misleading.)\n\nThe parameters affecting the criticality of the system may be remembered using the phrase MAGICMERV, where each letter constitute a parameter important to nuclear criticality safety. It is important to note that these parameters are not independent from one another. For example, as you increase mass of a unit, its geometry and volume are also changing. \nMass: The probability of fission increases as the total number of fissile nuclei increases. The relationship is not linear. If a fissile body has a given size and shape but varying density and mass, there is a threshold below which criticality can not occur. This threshold is called the critical mass.\n\nAbsorption: Absorption removes neutrons from the system. Large amounts of absorbers are used to control or reduce the probability of a criticality. Good absorbers are boron, cadmium, gadolinium, silver, and indium.\n\nGeometry/shape of the fissile material: If neutrons escape (leak from) the fissile system they are not available to cause fission events in the fissile material. Therefore, the shape of the fissile material affects the probability of occurrence of fission events. A shape with a large surface area, such as a thin slab, favors leakage and is safer than the same amount of fissile material in a small, compact shape such as a cube or sphere.\n\nInteraction of units: Neutrons leaking from one unit can enter another. Two units, which by themselves are sub-critical, could interact with each other to form a critical system. The distance separating the units and any material between them influences the effect.\n\nConcentration/Density: Neutron reactions leading to scattering, capture or fission reactions are more likely to occur in dense materials; conversely neutrons are more likely to escape (leak) from low density materials.\n\nModeration: Neutrons resulting from fission are typically fast (high energy). These fast neutrons do not cause fission as readily as slower (less energetic) ones. Neutrons are slowed down (moderated) by collision with atomic nuclei. The most effective moderating nuclei are hydrogen, deuterium, beryllium and carbon. Hence hydrogenous materials including oil, polyethylene, water, wood, paraffin, and the human body are good moderators. Note that moderation comes from collisions; therefore most moderators are also good reflectors.\n\nEnrichment: The probability of a neutron reacting with a fissile nucleus is influenced by the relative numbers of fissile and non-fissile nuclei in a system. The process of increasing the relative number of fissile nuclei in a system is called enrichment. Typically, low enrichment means less likelihood of a criticality and high enrichment means a greater likelihood.\n\nReflection: When neutrons collide with other atomic particles (primarily nuclei) and are not absorbed, they are scattered (i.e. they change direction). If the change in direction is large enough, neutrons that have just escaped from a fissile body may be deflected back into it, increasing the likelihood of fission. This is called ‘reflection’. Good reflectors include hydrogen, beryllium, carbon, lead, uranium, water, polyethylene, concrete, Tungsten carbide and steel.\n\nVolume: For a body of fissile material in any given shape, increasing the size of the body increases the average distance that neutrons must travel before they can reach the surface and escape. Hence, increasing the size of the body increases the likelihood of fission and decreases the likelihood of leakage. Hence, for any given shape (and reflection conditions - see below) there will be a size that gives an exact balance between the rate of neutron production and the combined rate of absorption and leakage. This is the critical size.\n\nTemperature is another parameter that affects the criticality of the system. This particular parameter is less common for the criticality safety practitioner, as in a typical operating environment, where the variation in temperature is minimal, or where the increase in temperature does not adversely affect the criticality of the system, often, it is assumed that room temperate is bounding of the actual temperature of the system being analyzed. This is however only an assumption, it is important for the criticality safety practitioner to understand where this not apply, such as high temperature reactors, or low temperature cryogenic experiments.\n\nTo determine if any given system containing fissile material is safe, its neutron balance must be calculated. In all but very simple cases, this usually requires the use of computer programs to model the system geometry and its material properties.\n\nThe analyst describes the geometry of the system and the materials, usually with conservative or pessimistic assumptions. The density and size of any neutron absorbers is minimised while the amount of fissile material is maximised. As some moderators are also absorbers, the analyst must be careful when modelling these to be pessimistic. Computer codes allow analysts to describe a three-dimensional system with boundary conditions. These boundary conditions can represent real boundaries such as concrete walls or the surface of a pond, or can be used to represent an artificial infinite system using a periodic boundary condition. These are useful when representing a large system consisting of many repeated units.\n\nComputer codes used for criticality safety analyses include COG (US), MONK (UK), KENO (US), MCNP (US), and CRISTAL (France).\n\nTraditional criticality analyses assume that the fissile material is in its most reactive condition, which is usually at maximum enrichment, with no irradiation. For spent nuclear fuel storage and transport, burnup credit may be used to allow fuel to be more closely packed, reducing space and allowing more fuel to be handled safely. In order to implement burnup credit, fuel is modeled as irradiated using pessimistic conditions which produce an isotopic composition representative of all irradiated fuel. Fuel irradiation produces actinides consisting of both neutron absorbers and fissionable isotopes as well as fission products which absorb neutrons.\n\nIn fuel storage pools using burnup credit, separate regions are designed for storage of fresh and irradiated fuel. In order to store fuel in the irradiated fuel store it must satisfy a loading curve which is dependent on initial enrichment and irradiation.\n\n"}
{"id": "20726470", "url": "https://en.wikipedia.org/wiki?curid=20726470", "title": "Pauly &amp; C. – Compagnia Venezia Murano", "text": "Pauly &amp; C. – Compagnia Venezia Murano\n\nPauly & C. – Compagnia Venezia Murano is a Venetian company that produces glass art, most notably Roman murrine, mosaics and chandeliers.\n\nThe company was formed in 1919 by a merger of Pauly & C (founded 1902) and the Compagnia di Venezia e Murano (founded 1866). It has since expanded in 1932 with the acquisition of MVM Cappellin and in 1990 by the addition of the drawings and back catalogues of Toso Vetri d’Arte. The company maintains retail showrooms as well as exhibiting antiques.\n\nCompagnia di Venezia e Murano began as Salviati &C. in London in 1866 under the direction of Vicenza attorney Antonio Salviati and with the backing of two British men: archaeologist Austen Henry Layard and antiquarian Sir William Drake. The company was dedicated to using ancient techniques and utilized master glassblowers in its efforts to do so. It called in specialists from other fields like goldsmithing and engraving to ensure authenticity and employed artist Giuseppe Devers to teach the techniques of enamelling and heat-applied glass gilding to company artisans. Archaeologist Layard was particularly interested in the mosaic glass techniques of Roman and pre-Roman artists, and he spent years personally overseeing the work of the company's technicians and glassblowers in attempting to revive those techniques. In 1872, the company was successful, managing to replicate the type of glass commonly known as \"murrina\" (plural, \"murrine\").\n\nThe company was renamed Venice and Murano Glass and Mosaic Company Limited in 1872, and, in 1877, Layard purchased Salviati's interest so that Salviati could pursue other interests. The company quickly earned a reputation for quality original glass art and reproductions as well as its many mural mosaics in Great Britain and elsewhere in Europe. In 1878, the murrine produced by Compagnia di Venezia e Murano was included in its exhibit at the International Exhibition in Paris, which was the chief attraction in Italian glass. In its observations of the display, the United States Commission to the Paris exposition commented not only on \"Roman murrhine glass\", but also particularly on the mural glass mosaics, the \"perfection of which\" had \"engaged the earnest attention of the company.\" Mosaics produced by the company during the time period are still in existence in diverse areas such as Gonville and Caius College Chapel in Cambridge; St Paul's Within the Walls in Rome, the Victoria and Albert Museum, Westminster Cathedral in London, Old South Church in Boston the Chamberlain Memorial in Birmingham, Palazzo Barbarigo and the Senate House rooms in the United States. The last specimen, a portrait of Abraham Lincoln, was produced and donated in 1866.\n\nThrough the last years of the 19th century, Compagnia di Venezia e Murano took part in many other displays. Prior to the 1878 exhibition in Paris, it had shown at the Maritime Exhibition in Naples and the Trieste Exhibition in 1871 and at the International Exhibition in Vienna in 1873, where it won 13 prizes for decorative arts. It unveiled a new focus in 1881 with the display of the first of its glass phoenixes at the National Exhibition in Milan. It exported several thousand works for display at the World's Columbian Exposition in Chicago in 1893, also setting up a kiln so that the public could observe the company's glass blowing techniques. In 1895, it exhibited at the first Venice Biennale (an event at which it would also feature later), with artisan Vincenzo Moretti taking prizes and artisan Attilio Spaccarelli earning special note for his engraving.\n\n1900 saw a change in the company when its British owners sold their interest to a businessman from Venice named Tosolini, who was the owner of shops in St. Mark's Square. Under Tosoloni's ownership, the company stopped production in 1909, though it continued commercial distribution at St. Mark's Square.\n\nPauly & C was formed in 1903 by Emilio Pauly, Alessandro Hirscber Hellman, Vittorio Emanuele Toldo and Ernesto Graziadei, opening showrooms in Palazzo Trevisan Cappello, which would remain the headquarters of the company until its closure in 2007 for restoration. In 1919, Pauly & C. and Compagnia di Venezia e Murano were both purchased by the Milan Società Anonima Sanitaria, which resold them the following year to Gaetano Ceschina of Milan. The newly merged company, retitled to its present name, continued display in its previous locations of St. Mark's Square and the Palazzo Trevisan Cappello. In 1925, the merged company resumed production of glassworks on Murano and began exhibiting again at the Monza Tirennale.\n\nThe company grew in 1933 with the acquisition of Maestri Vetrai Muranesi Cappellin, or MVM Cappellin, a glass company formed in 1925 by Paolo Venini and Giacomo Cappellin, which transferred to them rights to the works and designs of the MVM Cappellin artists such as Vittorio Zecchin and Carlo Scarpa. The company expanded into commissioned chandeliers, with notable pieces from the period being placed in such locations as the Palazzo del Quirinale in Rome; the Vatican Palace, and the Royal Palace in Copenhagen. More recently, interior lighting has been designed for such places as the Palazzo Bezzi in Ravenna and the Al Assawi family palace. In 1990, the company again expanded with the acquisition of Toso Vetri d’Arte glassworks.\n\nCeschina sold his interest in the company in 1963 to the Barbon family, who retained it until 1976, when they sold to Andrea Boscaro. Boscaro owned the company for almost 30 years before 2005.\n\nThe company has a long history of association with international artisans. In the period following the second World War, the company undertook long-term associations with master glassblowers Anzolo Fuga and Alfredo Barbini, engraver Francesco Andolfato and painter Enzo Scarpa. Shorter associations include sculptor Napoleone Martinuzzi in the 1960s, painter Libero Vitali in 1971, architect Franz Prati between 1996 and 1997, and fashion designer Romeo Gigli in 1997. In the later 1990s, the company also featured exclusive designs by Heinz Oestergaard, Maria Teresa Lorella Gnutti and Berit Johansson, whose association with the company persisted. Since the turn of the century, associations have included Venetian sculptor Livio de Marchi and Chinese artist Xiao Fan Ru.\n\nGlass art produced by Pauly & C. – Compagnia Venezia Murano is on display in a number of museums. Among them:\n\n\n"}
{"id": "24080", "url": "https://en.wikipedia.org/wiki?curid=24080", "title": "PostScript", "text": "PostScript\n\nPostScript (PS) is a page description language in the electronic publishing and desktop publishing business. It is a dynamically typed, concatenative programming language and was created at Adobe Systems by John Warnock, Charles Geschke, Doug Brotz, Ed Taft and Bill Paxton from 1982 to 1984.\n\nThe concepts of the PostScript language were seeded in 1976 when John Warnock was working at Evans & Sutherland, a computer graphics company. At that time John Warnock was developing an interpreter for a large three-dimensional graphics database of New York Harbor. Warnock conceived the Design System language to process the graphics.\n\nConcurrently, researchers at Xerox PARC had developed the first laser printer and had recognized the need for a standard means of defining page images. In 1975-76 Bob Sproull and William Newman developed the Press format, which was eventually used in the Xerox Star system to drive laser printers. But Press, a data format rather than a language, lacked flexibility, and PARC mounted the Interpress effort to create a successor.\n\nIn 1978 Warnock left Evans & Sutherland and joined Xerox PARC to work with Martin Newell. They rewrote Design System to create the interpretive language, J & M or JaM (for \"John and Martin\") which was used for VLSI design and the investigation of type and graphics printing. This work later evolved and expanded into the Interpress language.\n\nWarnock left with Chuck Geschke and founded Adobe Systems in December 1982. They, together with Doug Brotz, Ed Taft and Bill Paxton created a simpler language, similar to Interpress, called PostScript, which went on the market in 1984. At about this time they were visited by Steve Jobs, who urged them to adapt PostScript to be used as the language for driving laser printers.\n\nIn March 1985, the Apple LaserWriter was the first printer to ship with PostScript, sparking the desktop publishing (DTP) revolution in the mid-1980s. The combination of technical merits and widespread availability made PostScript a language of choice for graphical output for printing applications. For a time an interpreter (sometimes referred to as a RIP for Raster Image Processor) for the PostScript language was a common component of laser printers, into the 1990s.\n\nHowever, the cost of implementation was high; computers output raw PS code that would be interpreted by the printer into a raster image at the printer's natural resolution. This required high performance microprocessors and ample memory. The LaserWriter used a 12 MHz Motorola 68000, making it faster than any of the Macintosh computers to which it attached. When the laser printer engines themselves cost over a thousand dollars the added cost of PS was marginal. But as printer mechanisms fell in price, the cost of implementing PS became too great a fraction of overall printer cost; in addition, with desktop computers becoming more powerful, it no longer made sense to offload the rasterisation work onto the resource-constrained printer. By 2001, few lower-end printer models came with support for PostScript, largely due to growing competition from much cheaper non-PostScript ink jet printers, and new software-based methods to render PostScript images on the computer, making them suitable for any printer; PDF, a descendant of PostScript, provides one such method, and has largely replaced PostScript as \"de facto\" standard for electronic document distribution.\n\nOn high-end printers, PostScript processors remain common, and their use can dramatically reduce the CPU work involved in printing documents, transferring the work of rendering PostScript images from the computer to the printer.\n\nThe first version of the PostScript language was released to the market in 1984. The suffix \"Level 1\" was added when Level 2 was introduced.\n\nPostScript Level 2 was introduced in 1991, and included several improvements: improved speed and reliability, support for in-RIP separations, image decompression (for example, JPEG images could be rendered by a PostScript program), support for composite fonts, and the form mechanism for caching reusable content.\n\nPostScript 3 (Adobe dropped the \"level\" terminology in favor of simple versioning) came at the end of 1997, and along with many new dictionary-based versions of older operators, introduced better color handling and new filters (which allow in-program compression/decompression, program chunking, and advanced error-handling).\n\nPostScript 3 was significant in terms of replacing the existing proprietary color electronic prepress systems, then widely used for magazine production, through the introduction of smooth shading operations with up to 4096 shades of grey (rather than the 256 available in PostScript Level 2), as well as DeviceN, a color space that allowed the addition of additional ink colors (called spot colors) into composite color pages.\n\nPrior to the introduction of PostScript, printers were designed to print character output given the text—typically in ASCII—as input. There were a number of technologies for this task, but most shared the property that the glyphs were physically difficult to change, as they were stamped onto typewriter keys, bands of metal, or optical plates.\n\nThis changed to some degree with the increasing popularity of dot matrix printers. The characters on these systems were drawn as a series of dots, as defined by a font table inside the printer. As they grew in sophistication, dot matrix printers started including several built-in fonts from which the user could select, and some models allowed users to upload their own custom glyphs into the printer.\n\nDot matrix printers also introduced the ability to print raster graphics. The graphics were interpreted by the computer and sent as a series of dots to the printer using a series of escape sequences. These printer control languages varied from printer to printer, requiring program authors to create numerous drivers.\n\nVector graphics printing was left to special-purpose devices, called plotters. Almost all plotters shared a common command language, HPGL, but were of limited use for anything other than printing graphics. In addition, they tended to be expensive and slow, and thus rare.\n\nLaser printers combine the best features of both printers and plotters. Like plotters, laser printers offer high quality line art, and like dot-matrix printers, they are able to generate pages of text and raster graphics. Unlike either printers or plotters, however, a laser printer makes it possible to position high-quality graphics and text on the same page. PostScript made it possible to fully exploit these characteristics, by offering a single control language that could be used on any brand of printer.\n\nPostScript went beyond the typical printer control language and was a complete programming language of its own. Many applications can transform a document into a PostScript program whose execution will result in the original document. This program can be sent to an interpreter in a printer, which results in a printed document, or to one inside another application, which will display the document on-screen. Since the document-program is the same regardless of its destination, it is called \"device-independent\".\n\nPostScript is noteworthy for implementing on-the fly rasterization; everything, even text, is specified in terms of straight lines and cubic Bézier curves (previously found only in CAD applications), which allows arbitrary scaling, rotating and other transformations. When the PostScript program is interpreted, the interpreter converts these instructions into the dots needed to form the output. For this reason PostScript interpreters are occasionally called PostScript raster image processors, or RIPs.\n\nAlmost as complex as PostScript itself is its handling of fonts. The font system uses the PS graphics primitives to draw glyphs as curves, which can then be rendered at any resolution. A number of typographic issues had to be considered with this approach.\n\nOne issue is that fonts do not actually scale linearly at small sizes; features of the glyphs will become proportionally too large or small and they start to look wrong. PostScript avoided this problem with the inclusion of font hinting, in which additional information is provided in horizontal or vertical bands to help identify the features in each letter that are important for the rasterizer to maintain. The result was significantly better-looking fonts even at low resolution; it had formerly been believed that hand-tuned bitmap fonts were required for this task.\n\nAt the time, the technology for including these hints in fonts was carefully guarded, and the hinted fonts were compressed and encrypted into what Adobe called a \"Type 1 Font\" (also known as \"PostScript Type 1 Font\", \"PS1\", \"T1\" or \"Adobe Type 1\"). Type 1 was effectively a simplification of the PS system to store outline information only, as opposed to being a complete language (PDF is similar in this regard). Adobe would then sell licenses to the Type 1 technology to those wanting to add hints to their own fonts. Those who did not license the technology were left with the \"Type 3 Font\" (also known as \"PostScript Type 3 Font\", \"PS3\" or \"T3\"). Type 3 fonts allowed for all the sophistication of the PostScript language, but without the standardized approach to hinting.\n\nThe Type 2 font format was designed to be used with Compact Font Format (CFF) charstrings, and was implemented to reduce the overall font file size. The CFF/Type2 format later became the basis for handling PostScript outlines in OpenType fonts.\n\nThe CID-keyed font format was also designed, to solve the problems in the OCF/Type 0 fonts, for addressing the complex Asian-language (CJK) encoding and very large character set issues. The CID-keyed font format can be used with the Type 1 font format for standard CID-keyed fonts, or Type 2 for CID-keyed OpenType fonts.\n\nTo compete with Adobe's system, Apple designed their own system, TrueType, around 1991. Immediately following the announcement of TrueType, Adobe published the specification for the Type 1 font format. Retail tools such as Altsys Fontographer (acquired by Macromedia in January 1995, owned by FontLab since May 2005) added the ability to create Type 1 fonts. Since then, many free Type 1 fonts have been released; for instance, the fonts used with the TeX typesetting system are available in this format.\n\nIn the early 1990s there were several other systems for storing outline-based fonts, developed by Bitstream and METAFONT for instance, but none included a general-purpose printing solution and they were therefore not widely used.\n\nIn the late 1990s, Adobe joined Microsoft in developing OpenType, essentially a functional superset of the Type 1 and TrueType formats. When printed to a PostScript output device, the unneeded parts of the OpenType font are omitted, and what is sent to the device by the driver is the same as it would be for a TrueType or Type 1 font, depending on which kind of outlines were present in the OpenType font.\n\nIn the 1980s, Adobe drew most of its revenue from the licensing fees for their implementation of PostScript for printers, known as a raster image processor or \"RIP\". As a number of new RISC-based platforms became available in the mid-1980s, some found Adobe's support of the new machines to be lacking.\n\nThis and issues of cost led to third-party implementations of PostScript becoming common, particularly in low-cost printers (where the licensing fee was the sticking point) or in high-end typesetting equipment (where the quest for speed demanded support for new platforms faster than Adobe could provide). At one point, Microsoft licensed to Apple a PostScript-compatible interpreter it had bought called TrueImage, and Apple licensed to Microsoft its new font format, TrueType. Apple ended up reaching an accord with Adobe and licensed genuine PostScript for its printers, but TrueType became the standard outline font technology for both Windows and the Macintosh.\n\nToday, third-party PostScript-compatible interpreters are widely used in printers and multifunction peripherals (MFPs). For example, CSR plc's IPS PS3 interpreter, formerly known as PhoenixPage, is standard in many printers and MFPs, including those developed by Hewlett-Packard and sold under the LaserJet and Color LaserJet lines. Other third-party PostScript solutions used by print and MFP manufacturers include Jaws and the Harlequin RIP, both by Global Graphics. A free software version, with several other applications, is Ghostscript. Several compatible interpreters are listed on the Undocumented Printing Wiki.\n\nSome basic, inexpensive laser printers do not support PostScript, instead coming with drivers that simply rasterize the platform's native graphics formats rather than converting them to PostScript first. When PostScript support is needed for such a printer, Ghostscript can be used. There are also a number of commercial PostScript interpreters, such as TeleType Co.'s T-Script.\n\nPostScript became commercially successful due to the introduction of the graphical user interface, allowing designers to directly lay out pages for eventual output on laser printers. However, the GUI's own graphics systems were generally much less sophisticated than PostScript; Apple's QuickDraw, for instance, supported only basic lines and arcs, not the complex B-splines and advanced region filling options of PostScript. In order to take full advantage of PostScript printing, applications on the computers had to re-implement those features using the host platform's own graphics system. This led to numerous issues where the on-screen layout would not exactly match the printed output, due to differences in the implementation of these features.\n\nAs computer power grew, it became possible to host the PS system in the computer rather than the printer. This led to the natural evolution of PS from a printing system to one that could also be used as the host's own graphics language. There were numerous advantages to this approach; not only did it help eliminate the possibility of different output on screen and printer, but it also provided a powerful graphics system for the computer, and allowed the printers to be \"dumb\" at a time when the cost of the laser engines was falling. In a production setting, using PostScript as a display system meant that the host computer could render low-resolution to the screen, higher resolution to the printer, or simply send the PS code to a smart printer for offboard printing.\n\nHowever, PostScript was written with printing in mind, and had numerous features that made it unsuitable for direct use in an interactive display system. In particular, PS was based on the idea of collecting up PS commands until the codice_1 command was seen, at which point all of the commands read up to that point were interpreted and output. In an interactive system this was clearly not appropriate. Nor did PS have any sort of interactivity built in; for example, supporting hit detection for mouse interactivity obviously did not apply when PS was being used on a printer.\n\nWhen Steve Jobs left Apple and started NeXT, he pitched Adobe on the idea of using PS as the display system for his new workstation computers. The result was Display PostScript, or DPS. DPS added basic functionality to improve performance by changing many string lookups into 32 bit integers, adding support for direct output with every command, and adding functions to allow the GUI to inspect the diagram. Additionally, a set of \"bindings\" was provided to allow PS code to be called directly from the C programming language. NeXT used these bindings in their NeXTStep system to provide an object oriented graphics system. Although DPS was written in conjunction with NeXT, Adobe sold it commercially and it was a common feature of most Unix workstations in the 1990s.\n\nSun Microsystems took another approach, creating NeWS. Instead of DPS's concept of allowing PS to interact with C programs, NeWS instead extended PS into a language suitable for running the entire GUI of a computer. Sun added a number of new commands for timers, mouse control, interrupts and other systems needed for interactivity, and added data structures and language elements to allow it to be completely object oriented internally. A complete GUI, three in fact, were written in NeWS and provided for a time on their workstations. However, the ongoing efforts to standardize the X11 system led to its introduction and widespread use on Sun systems, and NeWS never became widely used.\n\nPostScript is a Turing-complete programming language, belonging to the concatenative group. Typically, PostScript programs are not produced by humans, but by other programs. However, it is possible to write computer programs in PostScript just like any other programming language.\n\nPostScript is an interpreted, stack-based language similar to Forth but with strong dynamic typing, data structures inspired by those found in Lisp, scoped memory and, since language level 2, garbage collection. The language syntax uses reverse Polish notation, which makes the order of operations unambiguous, but reading a program requires some practice, because one has to keep the layout of the stack in mind. Most \"operators\" (what other languages term \"functions\") take their arguments from the stack, and place their results onto the stack. \"Literals\" (for example, numbers) have the effect of placing a copy of themselves on the stack. Sophisticated data structures can be built on the \"array\" and \"dictionary\" types, but cannot be declared to the type system, which sees them all only as arrays and dictionaries, so any further typing discipline to be applied to such user-defined \"types\" is left to the code that implements them.\n\nThe character \"%\" is used to introduce comments in PostScript programs. As a general convention, every PostScript program should start with the characters \"%!PS\" as an interpreter directive so that all devices will properly interpret it as PostScript.\n\nA Hello World program, the customary way to show a small example of a complete program in a given language, might look like this in PostScript (level 2):\nor if the output device has a console\n\nPostScript uses the point as its unit of length. However, unlike some of the other versions of the point, PostScript uses exactly 72 points to the inch. Thus:\n\nFor example, in order to draw a vertical line of 4 cm length, it is sufficient to type:\n\nMore readably and idiomatically, one might use the following equivalent, which demonstrates a simple procedure definition and the use of the mathematical operators codice_2 and codice_3:\n\nMost implementations of PostScript use single-precision reals (24-bit mantissa), so it is not meaningful to use more than 9 decimal digits to specify a real number, and performing calculations may produce unacceptable round-off errors.\n\nList of software which can be used to render the PostScript documents:\n\n\n\n"}
{"id": "19006711", "url": "https://en.wikipedia.org/wiki?curid=19006711", "title": "Putcher fishing", "text": "Putcher fishing\n\nPutcher fishing is a type of fishing (usually of salmon) which employs a large number of putcher baskets, set in a fixed wooden frame, against the tide in a river estuary, notably on the River Severn, in England and South East Wales. Putchers are placed in rows, standing four or five high, in a wooden \"rank\" set out against the incoming and/or outgoing tides.\n\nTraditionally the putcher was made of hazel rods with withy (willow) plait, both materials being grown locally on the Caldicot and Wentloog Levels. Modern baskets made of steel or aluminium wire were introduced in the 1940s and 1950s.\n\nThe trapping of fish is probably one of the oldest known forms of fishing. The exact origin of putcher fishing along the River Severn is not known, but a memorial song recorded in 1663 mentions two fixed engines operating \"between the Hill and the Pile\" – undoubtedly references to Hill Farm and the Pill Reen or \"Monksditch\" at Goldcliff. Evidence for the use of putchers in medieval times has recently been revealed during construction of the Second Severn Crossing \n\nA salmon fishery of some kind was probably in operation at the time of the Priory at Goldcliff and then passed, at the time of the Dissolution, to Eton College. In the 1920s there were ranks holding some 2,400 baskets. The revenue from this enterprise provided an endowment to the Church of the Blessed Virgin at Eton. In addition, fresh salmon were provided for the breakfast of the scholars at the college. The fishery was owned for much of the later part of the twentieth century by Mr John Williams who employed one full-time fisherman Mr Wyndham Howells. The lease for the fishery was subsequently taken over by a Newport fishmonger. The fishery at Porton was acquired in 1902 by the Pontypool Park Estate and was for many years run during the fishing season run by Mr Keyte, a member of a long-established family of Goldcliff farmers.\n\nDuring the later half of the twentieth century numerous fixed weirs, installed in rivers and shores throughout Wales for the purpose of catching fish, fell into disuse. In addition to fixed traps, however, Wales had a large number of removable traps, also known as \"hecks\", \"crucks\", \"cribs\" and \"inscale\". The basket traps used at Goldcliff and Porton in the Severn Estuary were known as \"putchers\".\n\nFishing for salmon using many of these devices was forbidden in England and Wales by the Salmon Fishery Acts of 1861 and 1865, except under grant or charter, or by the right of \"immemorial usage\". Indeed, Special Constables were appointed by the Act of 1865 to enquire into the legality of all \"fixed engines\" being used to catch salmon. Only if proven legal would the constables then issue a certificate of legality. In this way the number, size and position of all salmon weirs became fixed for all time.\n\nIn South East Wales the baskets at Porton and Goldcliff came under the jurisdiction of the Usk River Authority and were permitted to be used as \"privileged fixed engines\". Two others at nearby Undy and Redwick, also on the shoreline of the Severn Estuary, fell out of use on the 1930s. The Wye River Authority operated a weir of 500 baskets at Beachley in Gloucestershire and a number of other weirs were operated by the Severn River Authority on both sides of the River Severn.\n\nFrom 1913 to 1950 fisherman George Whittaker was employed at the Goldcliff Fishery, first by Mr Fennell and later by Mr Burge. Whittaker was to become a notable source of information about putcher fishing there. The Putchers at Goldcliff were traditionally open in weave, being constructed from hazel rods and withy saplings cut in the autumn from a withy plantation at Llanwern. The baskets were made and repaired by the fishermen themselves during the closed season between 14 August and 1 May. The wooden putcher was gradually superseded, from 1942, by galvanised steel wire baskets and then later by baskets of aluminium wire, both of which were more seawater resistant. After 1952 the baskets at Goldcliff and Porton were exclusively aluminium, although the traditional measurements were strictly adhered to by the modern manufacturers. The galvanised putchers with square mesh were supplied flat and wired into shape by the fishermen. The aluminium putchers, with a diamond mesh, were supplied ready-formed, although at Goldcliff they were strengthened with additional wire bands. All-wire putchers were fixed to the wooden ranks by means of the two staples hammered into the wooden rails on the ranks.\n\nSince the 18th century salmon fisheries were associated with the Clifford estate at Yearsey and Black Rock at West Stretcholt. By the mid 19th century the de Mauley family claimed three sites in the River Parrett near Pawlett, Cannington, and Black Rock where four men had 1,000 putchers, also called \"butts\". Between 1868 and 1873 the number of licensed butts fell from 1,360 to 450. Salmon putchers remained in the river in 1920 and were sold with the de Mauley estate.\n\nWillow was harvested using traditional methods of pollarding, where a tree would be cut back to the main trunk. New shoots of willow, called \"withies\", would grow out of the trunk and these would be cut periodically for use.\n\nIn 1970 a few dozen of the traditional willow baskets were made and installed by the fishermen at Goldcliff. To make a putcher by hand, a low bench was used approximately high and square into which 9 holes were made, in a circle in diameter. Green withy or hazel rods were then split into three using an oak cleaver held in the maker’s hand. The split lengths were then inserted into the holes to make a conical shape and a withy ring was plaited round them close to the surface of the bench. Nine shorter rods, either complete or in thirds, were then inserted into the ring, with two more rings being plaited around to secure them, one halfway up and another near the top. A nose ring was then plaited. The putcher was then pulled out of the bench, the nose ring attached and a spiral `worm' was added from the narrow end to the middle. A base ring was woven at the end of the basket. Any longer rods could then be trimmed and the basket was ready for use. The basket was expected to last for two fishing seasons, possibly also for a third following repair.\n\nA strong timber frame or \"rank\" was required to hold the rows of putchers, built across the main tidal flow of the river. The fish, with fins trapped by the weave and unable to easily swim backwards, were then trapped in the conical baskets. The very high difference between high and low tide in the Severn Estuary enabled the ranks to be long and thus economically productive. Goldcliff originally had three ranks – the \"Flood\", the \"Ebb\" and the \"Putt\", able to carry a total of 2,327 baskets. A number of smaller baskets were especially constructed to fill any smaller spaces in the rank. Although there was no great advantage to be had from trapping smaller fish, the lack of any holes in the rank presented a more complete \"wall of traps\" through which it was more difficult for any fish to pass. The single putcher rank at Porton could carry some 600 baskets. Although the fishermen’s intimate knowledge of the seasonal height of the tides meant wasted that un-submerged baskets were not set, once fixed the baskets were typically removed only if the fish could not be released, or to allow repairs. \n\nThe uprights of the ranks were generally made of green larch or green elm, some long and from 8 to in diameter and sunk into the foreshore by between 6 and . Green timber was preferred as the salt seawater would preserve it and allow use for perhaps up to ten years. Elm tended to be more durable then larch, although all would eventually start to rot from the top and would need to be replaced. Some poles at Goldcliff, probably those at the seaward ends of the ranks, were reported to have lasted for forty years. The post holes were bored using a rock auger and bar, with the debris removed using a long-handle implement called a \"spoon\". The uprights were then beaten into the ground, in pairs, five feet apart, using an iron lined yew beetle.\n\nBetween each pair of upstream and downstream uprights a space of six feet was left for laying the baskets, with each pair braced with diagonals and transverse beams top and bottom. To the outside of this structure horizontal rails were then nailed, the first some from the ground, and then at regular two foot intervals above Two or three narrow gaps were left in the lower rails to allow access. Rows of baskets were then stapled to the rails, at a downward angle of about 20 degrees. All the baskets in a rank would point the same way, with the open end set either against the incoming or outgoing tide. Added strength was given to some ranks by using angled props against the front post of every fourth pair.\n\nFor reasons of religious observance, until the early years of the nineteenth century, the putchers' open ends had to be closed at least between noon on Saturday until 6 am the following Monday. The practice suggested strongly the origins of the fishery with the Benedictine Priory at Goldcliff.\n\nOn normal fishing days, as soon as the tide had exposed the baskets, they needed to be inspected by the fisherman who used a pronged staff to push any trapped fish back towards the basket entrance. Returning along the other side of the rank, the fish were then collected in a sack. At Goldcliff a bicycle, with trays front and back, was used to make collection easier. Running repairs could also be carried out at the same time with hammer and nails.\n\nFish from the ebb-ranks were by necessity fresher than those taken from the flood-ranks, since on the latter the tide had to both rise and fall before the fishermen could collect the catch.\nSince the fisherman would need to work with the tides, inspection and collection needed to occur around the clock and thus the brick workshop was equipped with bed and cooking equipment. A fish store was also required where the freshly collected fish could be stored in tight-lidded lead-lined boxes packed with broken ice. The fish from both Goldcliff and Porton were sent to Billingsgate market in London. The Porton fishery was also equipped with a smokery at one time, using oak shavings, but this venture was apparently not long lasting.\n\nThe \"kype\", a basket similar to the putcher but more elaborate, for catching shrimp and eel, seems also to have been constructed for use at Goldcliff.\n\n\n"}
{"id": "42628804", "url": "https://en.wikipedia.org/wiki?curid=42628804", "title": "Quasi-crystals (supramolecular)", "text": "Quasi-crystals (supramolecular)\n\nQuasi-crystals are supramolecular aggregates exhibiting both crystalline (solid) properties as well as amorphous, liquid-like properties.\nSelf-organized structures termed \"quasi-crystals\" were originally described in 1978 by the Israeli scientist Valeri A. Krongauz of the Weizmann Institute of Science, in the \"Nature\" paper, \"Quasi-crystals from irradiated photochromic dyes in an applied electric field\". \"\n\nIn his 1978 paper Krongauz coined the term “Quasi-Crystals” for the new self-organized colloidal particles . The Quasi-crystals are supramolecular aggregates manifesting both crystalline properties e.g. Bragg scattering, as well as amorphous, liquid-like properties i.e. drop-like shapes, fluidity, extensibility and elasticity in electric field. The supramolecular Quasi-crystals are produced in photochemical reaction by exposing solutions of photochromic spiropyran molecules to UV radiation. The ultraviolet light induces the conversion of the spiropyrans to merocyanine molecules that manifest electric dipole moments. (see Scheme 1). The quasi-crystals have external shape of submicron globules and their internal structure consists of crystals enveloped by an amorphous matter(see Fig. 1). The crystals are formed by self-assembled stacks of the merocyanine molecular dipoles aligning themselves in a parallel manner, while amorphous envelopes consist of the same merocyanine dipoles aligned in an anti-parallel manner (Fig.1, Scheme2). In an applied electrostatic field, quasi-crystals form macroscopic threads that show linear optical dichroism.\n\nLater Krongauz described unusual phase transitions of molecules composedof mesogenic and spiropyran moieties, which he named \"quasi-liquidcrystals.\" A micrograph of their mesophase appeared on the cover of \"Nature \"in a 1984 paper, “Quasi-Liquid Crystals.” The investigation of spiropyran-merocyanineself-organized systems, including macromolecules (see, for example, Fig. 2),has continued over the years. \n\nThese studies have resulted in discoveries of unusual and practically significant phenomena. Thus, in the electrostatic field, quasi-crystals and quasi-liquid crystals have exhibited 2nd order non-linear optical properties.\n\nPotential applications of these fascinating materials have been described and patented.\n\nWork on spiropyran-merocyanine self-assemblies currently continues in several laboratories –see, for example,reference [18] and the references cited in that study.\n"}
{"id": "27847679", "url": "https://en.wikipedia.org/wiki?curid=27847679", "title": "Reactive nitrogen", "text": "Reactive nitrogen\n\nReactive nitrogen (\"Nr\") is a term used for a variety of nitrogen compounds that support growth directly or indirectly. Representative species include the gases nitrogen oxides (NO), ammonia (NH), nitrous oxide (NO), as well as the anion nitrate (NO). Most of these species are the result of intensive farming, especially the (mis)use of fertilizers. Although required for life, nitrogen is stored in the biosphere in an unreactive (\"unfixed\") form N, which supports on a few life forms. Reactive nitrogen is however \"fixed\" and is readily converted into protein, which supports life, leading to depletion of oxygen in fresh waters by eutrophication.\n\nIn the environmental context, reactive nitrogen compounds include the following classes:\nAll of these compounds enter into the nitrogen cycle.\n\nAs a consequence, an excess of Nr can affect the environment relatively quickly. This also means that nitrogen-related problems need to be looked at in an integrated manner.\n\n"}
{"id": "32985157", "url": "https://en.wikipedia.org/wiki?curid=32985157", "title": "Robot economics", "text": "Robot economics\n\nRobot economics is the study of the market for robots. Robot markets function through the interaction of robot makers and robot users. As (in part) a factor of production, robots are complements and/or substitutes for other factors, such as labor and (non-robot) capital goods. Another part of robot economics considers the effects of the introduction of robots on the markets for those other factors and on the products that robots help produce.\n\nRobots are spreading throughout the economy, in fields such as agriculture, medicine and retail.\n\nThe use of robots in agriculture began with automated milking systems. More recently, agricultural robots have begun to be used in harvesting, pruning, seeding, spraying and materials handling.\n\nMedical robots can be categorized in five segments: surgery, rehabilitation, non-invasive radiosurgery, hospital and pharmacy and others. Robot-assisted surgery can improve accuracy. However, medical robots can increase costs.\n\nThe market was valued at $1,781 million in 2013. Hospital and pharmacy robots segment grew fastest.\n\nRobots can be used in retail for helping customers find items, keep track of product inventory on shelves, and even interact with customers.\n\nMajor participants operating in the global robotic market include Samsung Electronics, iRobot, Toyota Motor Corporation, AB Electrolux, Hanool Robotics, Fujitsu Frontech Limited, LG Electronics, Fujitsu, Sony Corporation, Yujin Robot and GeckoSystems.\n"}
{"id": "27367532", "url": "https://en.wikipedia.org/wiki?curid=27367532", "title": "SEVEN Networks", "text": "SEVEN Networks\n\nSEVEN Networks, Inc. is a privately funded American corporation founded in 2000. It had about 265 employees in 2010. As of 2017, the company has research and development centers in Texas and Finland.\n\nSEVEN mobile messaging products are turnkey multi-device, multi-service computer software for operators and device manufacturers. The company claims its products have a desktop-like experience for core messaging applications like email, instant messagings and social networking.\n\nThe company was formerly known as Leap Corporation and changed its name to SEVEN Networks, Inc. in December 2000. \nIn 2004 the company was selected for FierceWireless' list of 15 promising and innovative wireless startups of the year. \nBy 2005, CEO Bill Nguyen had left to start another company.\nIn 2006, the company announced Sprint as a customer.\n\nSince then, the company expanded its products to support email services, added mobile instant messaging applications, analytics and social networking.\nIn 2010, the company announced it was selected by Samsung Electronics to provide push technology for Samsung Social Hub, a social networking and integrated messaging service available on several of the company’s handsets. In January 2010, the company claimed in a press release to have more than eight million accounts actively synchronized on mobile devices using its software. In early 2011, the company announced Verizon Wireless as a customer and also announced Open Channel.\n\nIn 2012, the company announced a combined email, instant messaging and social media product, Ping.\n\nThe Open Channel software product line focuses on mobile traffic management and optimization. There are Open Channel products for wireless signaling optimization, carrier network policy enforcement, and mobile data offloading.\nOpen Channel was launched in February 2011 to help carriers manage the impact of push technology for message notifications on their networks. It works by monitoring all requests for data from smartphone applications, such as Facebook, email, Twitter, which make up to hundreds of requests per hour, with only a small fraction of them actually returning data.\n\nThe platform acts as a buffer in the network, determining when content for a particular app is available and then allowing the phone to get that content. Early tests estimated mobile devices might reduce their time on a network by up to 40 percent and mobile traffic by up to 70 percent while boosting battery life by up to 25 percent.\n\nOpen Channel is transparent to connected applications and requires no changes or special integration by mobile developers. Additionally, it does not require changes to the network and can work in conjunction with new standards for fast network dormancy, smart signaling and other network optimizations. In February 2011, Open Channel received the GSMA Global Mobile Award for Best Mobile Technology Breakthrough in 2011.\n\nIn February 2013, Open Channel added offerings for policy enforcement and offloading. Also in early 2013, Toronto-based wireless operator Public Mobile selected Open Channel to manage network signaling and help reduce service costs stemming from non-optimized mobile applications and unnecessary data traffic that was creating excess network congestion.\n\nIn September 2015, Open Channel was made available directly to consumers.\n\nThe System SEVEN software allows consumers and enterprises to access information, such as business and personal email, calendar, corporate directories, personal contacts, and documents, as well as allows users to deliver mobile data, applications, and services to a portfolio of devices. SEVEN's push notification platform, System SEVEN, is deployed as a SaaS (software-as a service) solution. SEVEN Mobile Email and SEVEN Mobile IM are SEVEN's own applications built on top of its push platform and its Ping Services allow operators and device manufacturers to use the SEVEN push notification technology for messaging services and mobile applications. They provide mobile operators and device manufacturers with a solution for integrated messaging services.\n\nSystem SEVEN mobile email is a server-assisted solution, where access to user's email account appears to originate from IP addresses hosted by SEVEN (208.87.200.0 - 208.87.207.255) or its customers. Although done with user's permission, email service providers may flag these as potential hacking attempts and have raised security concerns, most recently with Microsoft Outlook for Android and iOS\n\nThe firm works with mobile platform providers, device manufacturers, email messaging solutions and providers of services in the cloud, and infrastructure partners, to sell mobile messaging services.\nIts systems use commonly deployed mobile platforms including Android, bada, BREW, J2ME, Symbian and Windows Mobile. They work on products from device manufacturers, including: HTC, INQ, LG, Motorola, Nokia, Sanyo, Samsung, and Sony Ericsson; and are embedded on more than 550 device types. The firm has partnered with many of the top Internet Service Providers including Google, Microsoft (Exchange and Windows Live) and Yahoo!, and infrastructure providers such as Equinix, Savvis and Oracle.\n\n\n"}
{"id": "41565445", "url": "https://en.wikipedia.org/wiki?curid=41565445", "title": "Science and Engineering Research Board", "text": "Science and Engineering Research Board\n\nScience and Engineering Research Board is a statutory body under the Department of Science and Technology, Government of India, established by an Act of the Parliament of India in 2009. The Board is chaired by the Secretary to the Government of India in the Department of Science and Technology and shall have other senior government officials and eminent scientists as members. The Board was set up for promoting basic research in science and engineering and to provide financial assistance to scientists, academic institutions, R&D laboratories, industrial concerns and other agencies for such research.\n\nThe Board has schemes for funding extramural research, for providing grants for start-up research and for using the scientific expertise of retired scientists. The Board also has programmes for intensifying research in high priority areas, for supporting international travel of scientists, for giving assistance to professional bodies for conducting seminars and symposia, and for awarding fellowships.\n\nThe following persons are (January 2014) the members of the Board.\n\n\n"}
{"id": "216795", "url": "https://en.wikipedia.org/wiki?curid=216795", "title": "Slurry wall", "text": "Slurry wall\n\nA slurry wall is a civil engineering technique used to build reinforced concrete walls in areas of soft earth close to open water, or with a high groundwater table. This technique is typically used to build diaphragm (water-blocking) walls surrounding tunnels and open cuts, and to lay foundations.\n\nWhile a trench is excavated to create a form for a wall, it is simultaneously filled with slurry (usually a mixture of bentonite and water). The dense but liquid slurry prevents the trench from collapsing by providing outward pressure, which balances the inward hydraulic forces and also retards water flow into the trench.\n\nSlurry walls are typically constructed by starting with a set of guide walls, typically deep and thick. The guide walls are constructed on the ground surface to outline the desired slurry trench and guide the excavation machinery. Excavation is done using a special clamshell-shaped digger or a hydromill trench cutter, suspended from a crane. The excavator digs down to design depth (or bedrock) for the first wall segment. The excavator is then lifted and moved along the trench guide walls to continue the trench with successive cuts as needed. The trench is at all times kept filled with slurry to prevent its collapse, but the liquid filling allows the excavation machinery and excavation spoil to be moved without hindrance.\n\nOnce a particular length of trench is reached, a reinforcing cage is lowered into the slurry-filled pit and the pit is filled with concrete from the bottom up using tremie pipes. The heavier concrete displaces the bentonite slurry, which is pumped out, filtered, and stored in tanks for use in the next wall segment, or recycled. \n\nSlurry walls are successively extended to enclose an area, blocking water and softened earth from flowing into it. Once the concrete has hardened, excavation within the now concrete-wall-enclosed area can proceed. To prevent the concrete wall from collapsing into the newly open area, temporary supports such as tiebacks or internal crossbeams are installed. When completed, the structure built within the walled-off area supports the wall, so that tiebacks or other temporary bracing may be removed.\nStability of the trench is essential in trench cutting.\nUsage of bentonite with precise density prevents collapse of trench walls\n\nThe slurry wall technique was first introduced during the excavation of Line 1 on the underground rapid transit system of Milan, Italy by the company ICOS (Impresa Costruzioni Opere Specializzate) just after the end of World War II. This new technology became an important component of the top-down tunnelling method also known as \"Metodo Milano\" (\"Milan method\"). \n\nSlurry wall construction was used to construct the \"bathtub\" that surrounded the foundations of most of the World Trade Center site in New York City. In the 1980s, the Red Line Northwest Extension project in Boston was one of the first projects in the US to use the modern form of the technology, with hydromill trench cutters and the \"Milan method\". Slurry walls were also used extensively in Boston's later Big Dig tunnel project.\n\n\n"}
{"id": "4151837", "url": "https://en.wikipedia.org/wiki?curid=4151837", "title": "Synthetic vision system", "text": "Synthetic vision system\n\nA synthetic vision system (SVS) is a computer-mediated reality system for aerial vehicles, that uses 3D to provide pilots with clear and intuitive means of understanding their flying environment. \n\nSynthetic vision is also a generic term, which may pertain to computer vision systems using artificial intelligence methods for visual learning, see \"Synthetic Vision using Volume Learning and Visual DNA\".\n\nSynthetic vision provides situational awareness to the operators by using terrain, obstacle, geo-political, hydrological and other databases. A typical SVS application uses a set of databases stored on board the aircraft, an image generator computer, and a display. Navigation solution is obtained through the use of GPS and inertial reference systems.\n\nHighway In The Sky (HITS), or Path-In-The-Sky, is often used to depict the projected path of the aircraft in perspective view. Pilots acquire instantaneous understanding of the current as well as the future state of the aircraft with respect to the terrain, towers, buildings and other environment features.\n\nSynthetic vision was developed by NASA and the U.S. Air Force in the late 1970s and 1980s in support of advanced cockpit research, and in 1990s as part of the Aviation Safety Program. Development of the High Speed Civil Transport fueled NASA research in the 1980s and 1990s. In the early 1980s, the USAF recognized the need to improve cockpit situation awareness to support piloting ever more complex aircraft, and pursued SVS (also called pictorial format avionics) as an integrating technology for both manned and remotely piloted systems.\n\nIn 1980 the FS1 Flight Simulator by Bruce Artwick for the Apple II microcomputer introduced recreational uses of synthetic vision.\n\nNASA used synthetic vision for remotely piloted vehicles (RPVs), such as the High Maneuvability Aerial Testbed or HiMAT. According to the report by NASA, the aircraft was flown by a pilot in a remote cockpit, and control signals up-linked from the flight controls in the remote cockpit on the ground to the aircraft, and aircraft telemetry downlinked to the remote cockpit displays (see photo). The remote cockpit could be configured with either nose camera video or with a 3D synthetic vision display. SV was also used for simulations of the HiMAT. Sarrafian reports that the test pilots found the visual display to be comparable to output of camera on board the RPV.\n\nThe 1986 RC Aerochopper simulation by Ambrosia Microcomputer Products, Inc. used synthetic vision to aid aspiring RC aircraft pilots in learning to fly. The system included joystick flight controls which would connect to an Amiga computer and display. The software included a three-dimensional terrain database for the ground as well as some man-made objects. This database was basic, representing the terrain with relatively small numbers of polygons by today's standards. The program simulated the dynamic three-dimensional position and attitude of the aircraft using the terrain database to create a projected 3D perspective display. The realism of this RPV pilot training display was enhanced by allowing the user to adjust the simulated control system delays and other parameters.\n\nSimilar research continued in the U.S. military services, and at Universities around the world. In 1995-1996, North Carolina State University flew a 17.5% scale F-18 RPV using Microsoft Flight Simulator to create the three-dimensional projected terrain environment.\n\nIn 2005 a synthetic vision system was installed on a Gulfstream V test aircraft as part of NASA's \"Turning Goals Into Reality\" program. Much of the experience gained during that program led directly to the introduction of certified SVS on future aircraft. NASA initiated industry involvement in early 2000 with major avionics manufacturers. \n\nEric Theunissen, a researcher at Delft University of Technology in the Netherlands, contributed to the development of SVS technology.\n\nAt the end of 2007 and early 2008, the FAA certified the Gulfstream Synthetic Vision-Primary flight display (SV-PFD) system for the G350/G450 and G500/G550 business jet aircraft, displaying 3D color terrain images from the Honeywell EGPWS data overlaid with the PFD symbology.\nIt replaces the traditional blue-over-brown artificial horizon.\n\nIn 2017, Avidyne Corporation certified Synthetic Vision capability for its air navigation avionics.\nOther glass cockpit systems such as the Garmin G1000 and the Rockwell Collins Pro Line Fusion offer synthetic terrain.\n\nLower-cost, non-certified avionics offer synthetic vision like apps available for Android or iPad tablet computers from ForeFlight, Garmin, or Hilton Software\n\n\n\n"}
{"id": "12810149", "url": "https://en.wikipedia.org/wiki?curid=12810149", "title": "Task lighting", "text": "Task lighting\n\nOften task lighting refers to increasing illuminance to better accomplish a specific activity. However, the illuminance level is not the only factor governing visibility. Contrast is also important, and a poorly positioned light source may cause contrast reduction, resulting in loss of visibility. The most important purpose of task lighting in the office is not increasing illuminance, but improving contrast. General lighting can be reduced because task lighting provides focused light where needed.\n\nDifferent strategies for task lighting exist. The three main approaches are:\nThere are also other approaches to task lighting, for example under-shelf luminaires. \n\nOther instances of task lighting are in machinery, where a specific work area needs illumination, and in workshops, where a task light may illuminate the actual working area. Special instances of task lighting are examination and operation lights for medicine and surgery, as well as the dentist's lamp. Task lamps are also used for many home tasks such as sewing, reading, small repairs, model construction, crafts, writing, and many other activities. The actual task may range from very small up to about as far as you may reach with your hands or available tools. Lighting of larger areas is beyond the scope of task lighting.\n\nLocalized lighting consists of a luminaire that provides ambient light as well as task light. Often it is an uplighter with a light source that is directed downward. It is intended to be mounted immediately over the workplace,\nlusu\n\nFixed task lighting refers to a non-movable light source dedicated to lighting a specific task. In kitchens, a homeowner may install several recessed \"can\" lights or under cabinet lighting to provide clear lighting onto the counters for cutting and preparing food. Having proper lighting when working with sharp knives is a critical component of injury prevention. Another form of fixed task lighting may simply be a table lamp positioned over one's reading chair.\n\nThe main feature of the freely adjustable task light is evident; one may adjust it freely at any whim or to suit one's needs. The lamp presents few limits to how one may position or orient the light. A freely adjustable lamp may include means for glare control, as a honeycomb or parabolic louvre that restricts the light output angle. \n\nA common form of home task lighting is a goose-neck lamp or swing arm light fixture. The adjustable neck allows light to be focused on the exact task needed, and the swing-arm wall sconces can be positioned next to a bed or chair, and adjusted to shine light on a printed page. Free standing, adjustable desk lamps are commonly used in home office applications.\n\nSome task lights also come with built in magnifying glasses for detailed tasks. It is hard to overstate the benefits of having focused light alongside magnification for small, precise operations such as model building, sewing, or other high-detail activities. Dentists often use a task light with magnification to perform dental cleanings.\n\nThe asymmetric task light is intended to be placed at the side of the actual task. The luminaire directs the light obliquely over the desk, with the highest illuminance typically about 1' to 1½' to the side of the lamp head. It mostly has an arm system that holds the lamp head horizontally irrespective of the arm movement - a \"parallel arm\". Asymmetric lamps often cause more reflected glare than other lamps. In workplaces where people use different table heights, an asymmetric lamp may cause direct glare due to its absence of means for glare control (ref:1).\n\nTask lighting can also be applied to home uses. Many people benefit from dedicated lighting for specific tasks such as cooking, sewing, reading, or paperwork. Home task lighting can be found in fixed or adjustable forms.\n\nContrast reduction in the office workplace refers to reading objects having decreased contrast compared to an estimated ideal contrast. If a lamp is placed so that printed letters reflect some of the light, their contrast against the paper background will decrease. This happens when a light source is reflected as in a mirror from the print into the eyes of the observer. A poorly placed lamp may render text illegible, regardless of illuminance level. For older persons, increased lighting and contrast is a necessary aid to performing daily tasks such as paying bills or reading.\n\n"}
{"id": "3518436", "url": "https://en.wikipedia.org/wiki?curid=3518436", "title": "Transistor model", "text": "Transistor model\n\nTransistors are simple devices with complicated behavior. In order to ensure the reliable operation of circuits employing transistors, it is necessary to scientifically model the physical phenomena observed in their operation using transistor models. There exists a variety of different models that range in complexity and in purpose. Transistor models divide into two major groups: models for device design and models for circuit design.\n\nThe modern transistor has an internal structure that exploits complex physical mechanisms. Device design requires a detailed understanding of how device manufacturing processes such as ion implantation, impurity diffusion, oxide growth, annealing, and etching affect device behavior. Process models simulate the manufacturing steps and provide a microscopic description of device \"geometry\" to the device simulator. \"Geometry\" does not mean readily identified geometrical features such as a planar or wrap-around gate structure, or raised or recessed forms of source and drain (see Figure 1 for a memory device with some unusual modeling challenges related to charging the floating gate by an avalanche process). It also refers to details inside the structure, such as the doping profiles after completion of device processing.\n\nWith this information about what the device looks like, the device simulator models the physical processes taking place in the device to determine its electrical behavior in a variety of circumstances: DC current-voltage behavior, transient behavior (both large-signal and small-signal), dependence on device layout (long and narrow versus short and wide, or interdigitated versus rectangular, or isolated versus proximate to other devices). These simulations tell the device designer whether the device process will produce devices with the electrical behavior needed by the circuit designer, and is used to inform the process designer about any necessary process improvements. Once the process gets close to manufacture, the predicted device characteristics are compared with measurement on test devices to check that the process and device models are working adequately.\n\nAlthough long ago the device behavior modeled in this way was very simple mainly drift plus diffusion in simple geometries today many more processes must be modeled at a microscopic level; for example, leakage currents in junctions and oxides, complex transport of carriers including velocity saturation and ballistic transport, quantum mechanical effects, use of multiple materials (for example, Si-SiGe devices, and stacks of different dielectrics) and even the statistical effects due to the probabilistic nature of ion placement and carrier transport inside the device. Several times a year the technology changes and simulations have to be repeated. The models may require change to reflect new physical effects, or to provide greater accuracy. The maintenance and improvement of these models is a business in itself.\n\nThese models are very computer intensive, involving detailed spatial and temporal solutions of coupled partial differential equations on three-dimensional grids inside the device.\nSuch models are slow to run and provide detail not needed for circuit design. Therefore, faster transistor models oriented toward circuit parameters are used for circuit design.\n\nTransistor models are used for almost all modern electronic design work. Analog circuit simulators such as SPICE use models to predict the behavior of a design. Most design work is related to integrated circuit designs which have a very large tooling cost, primarily for the photomasks used to create the devices, and there is a large economic incentive to get the design working without any iterations. Complete and accurate models allow a large percentage of designs to work the first time.\n\nModern circuits are usually very complex. The performance of such circuits is difficult to predict without accurate computer models, including but not limited to models of the devices used. The device models include effects of transistor layout: width, length, interdigitation, proximity to other devices; transient and DC current-voltage characteristics; parasitic device capacitance, resistance, and inductance; time delays; and temperature effects; to name a few items.\n\nNonlinear, or large signal transistor models fall into three main types:\n\nSmall-signal or linear models are used to evaluate stability, gain, noise and bandwidth, both in the conceptual stages of circuit design (to decide between alternative design ideas before computer simulation is warranted) and using computers. A small-signal model is generated by taking derivatives of the current-voltage curves about a bias point or Q-point. As long as the signal is small relative to the nonlinearity of the device, the derivatives do not vary significantly, and can be treated as standard linear circuit elements. \nA big advantage of small signal models is they can be solved directly, while large signal nonlinear models are generally solved iteratively, with possible convergence or stability issues. By simplification to a linear model, the whole apparatus for solving linear equations becomes available, for example, simultaneous equations, determinants, and matrix theory (often studied as part of linear algebra), especially Cramer's rule. Another advantage is that a linear model is easier to think about, and helps to organize thought.\n\nA transistor’s parameters represent its electrical properties. Engineers employ transistor parameters in production-line testing and in circuit design. A group of a transistor’s parameters sufficient to predict circuit gain, input impedance, and output impedance are components in its small-signal model.\n\nA number of different two-port network parameter sets may be used to model a transistor. These include:\n\n\nScattering parameters, or S parameters, can be measured for a transistor at a given bias point with a vector network analyzer. S parameters can be converted to another parameter set using standard matrix algebra operations.\n\n\n"}
{"id": "45274146", "url": "https://en.wikipedia.org/wiki?curid=45274146", "title": "VRVis", "text": "VRVis\n\nThe VRVis Zentrum für Virtual Reality und Visualisierung (VRVis) is the largest independent research center in the area of Visual Computing in Austria, and one of the largest in Europe. It is one of currently 22 centrally funded COMET – Competence Centers for Excellent Technologies of Austria. The VRVis Center is located in Ares Tower in Vienna.\n\nThe VRVis Center for Virtual Reality and Visualization was founded in January 2000 funded by the Austrian Kplus Competence Center program. The main initiator was Werner Purgathofer from TU Wien, with 3 other Austrian institutes as co-investigators. In 2010 and in 2017 VRVis received new fundings from the Austrian COMET program.\nVRVis was located in the science and technology park Tech Gate Vienna since 2001, in 2017 it moved to Ares Tower in Vienna.\n\nVRVis functions as a cooperation between scientific and industry partners, partly funded by the Austrian and Viennese governments. It is organized as a non-profit limited company owned by an association that has the only purpose to administrate VRVis. The members of this association are currently (2017):<ref name=\"Geschaeftsbericht 2016/17\">\"VRVis Geschäftsbericht 2016/17\"</ref>\n\n\nThe main business of VRVis is strategic and application oriented research in Visual Computing, with an emphasis on transferring state-of-the-art scientific results to companies in all areas. This includes projects financed by other funding sources, such as European projects and the Austrian Science Foundation. More than 50 FTE researchers from various fields cooperate with scientific and industrial partners to produce software and system solutions. Around 500 scientific publications have been published and 20 best paper awards have been achieved so far.\n\nThe scientific cooperation network of VRVis is quite large, including ETH Zurich (CH), University of Bergen (N), University of Rostock (D), KAUST (Saudi Arabia), University Medical Center Freiburg (D), Institute Claudio Regaud (F), Fondazione Santa Lucia (I), Delft University of Technology (NL), St Thomas' Hospital (UK), Virginia Tech (USA), Arizona State University (USA), City University London (UK), European Space Agency ESA, Fraunhofer IGD (D), Stanford University (USA), University of Konstanz (D) and many more.\n\nIndustry Partners include Agfa Healthcare (B/A), AVL List, Geodata, Austrian Federal Railways Infrastruktur AG, Kapsch TrafficCom AG, IMP - Research Institute of Molecular Pathology, and many more.\n\nThe VRVis Center does research and technology transfer in the following four areas:\n\n\"Visualization\" \n\n\"Visual Analytics\" \n\n\"Rendering\" \n\n\"Computer Vision\"\n\n\n"}
{"id": "52574367", "url": "https://en.wikipedia.org/wiki?curid=52574367", "title": "Virtual reality sex", "text": "Virtual reality sex\n\nVirtual reality sex (in short: VR sex) is a technology that allows the user to receive tactile sensations from remote participants, or fictional characters through the use of computer-controlled sex toys. Usually the user also wears a virtual reality headset so he/she can see and interact with the partner.\n\nThe BKK Cybersex Cup consists of a virtual reality headset and masturbation cup. It also comes with a mobile app which allows the users to customize their own 3D girlfriend based on their preferred body, skin tone, hairstyle and outfit. The masturbation cup has a built in motion sensor which is capable of simulating the movement and transmission of the action via Bluetooth to any smartphone that runs the 3D girlfriend app.\n\nThe American company Eos created the VirtuaDolls which consists a silicone sheath into which a man inserts his penis. In concert with a mechanized gripper for “intelligent stroke movement,” this allows for the VirtuaDolls controller to sync up the sensation to the action. For immersive experience, it also comes with six interchangeable sleeves with different textures to allow for varying sensations and contains a pressure sensor that determines the occupant’s position within the sheath. An optional vacuum attachment adds suction capabilities and an easy-clean system. The VirtuaDolls controller is also bundled with a video game titled \"Girls of Arcadia\" in which users carry out missions to rescue girls and are rewarded by the end of the missions in having sex with them. In January 2016, the company launched an Indiegogo campaign to raise funds for the product, but after being swamped with orders, the creators suspended the campaign. In February 2016, it was relaunched on Indiegogo.\n"}
{"id": "28443539", "url": "https://en.wikipedia.org/wiki?curid=28443539", "title": "World Fireworks Championship", "text": "World Fireworks Championship\n\nThe World Fireworks Championship was held in Oman in 2010 over three successive weekends and featured six firework companies. A reported 750,000 spectators attended the competition which ultimately was won by French company Lacroix Ruggieri.\n\nThe Oman World Fireworks Championship 2010 was part of the 40th National Day Celebrations of the Sultanate of Oman. The competition was held at Al Qurum Natural Park, located in the city of Muscat. Each pyromusical display was broadcast live over radio.\n\n\n"}
{"id": "58858459", "url": "https://en.wikipedia.org/wiki?curid=58858459", "title": "XADO Snipex", "text": "XADO Snipex\n\nXADO Snipex is a line of large-caliber sniper rifles manufactured by XADO company (Kharkiv, Ukraine).\n\nXADO company has been designing and manufacturing weapons since 2014. The prototype of the first Ukrainian rifle for long-range target shooting Snipex Rhino Hunter .50 BMG was officially presented at an exhibition in 2016. In 2017, the models Snipex M75 12.7x108 and Snipex 14.5x114 were displayed to the public. In 2018, the manufacturer presented the flagship model of the Snipex series — 12.7×108 caliber single-shot rifle Snipex M100 with automatic case ejection.\n\nSnipex M — single-shot rifles with automatic case ejection. Caliber: .50 BMG (12.7x99) or 12.7×108. Barrel length: 750; 850; 1,000; 1,100; 1,200 mm.\n\nThe weapon has the bullpup design, where the bolt carrier group is located behind the trigger. The rifle has short recoil. The inertia system provides operation of the automatics. Barrel locking is achieved through a rotating bolt.\n\nLow-level recoil when shooting. Recoil reduction is achieved due to the originally designed inertia recoil system, which includes moving spring-loaded assemblies, and two hydraulic shock absorbers located horizontally on both sides of the barrel, on the same level as the upper receiver. Barrel locking is achieved through a rotating bolt.\n\nThe rifle is designed for shooting resting and, to that end, is fitted with collapsible bipods and an additional rear support under the buttstock. The rifle is elevated on the bipods (barrel axis is located below mounting points of the bipods). A specially designed additional support provides the possibility of rough and precise adjustment.\n\nThe rifle is equipped with a special grip safety, which is located directly above the pistol grip on both sides of the weapon.\nOwn-produced barrel. As the manufacturer claims, barrel quality provides shooting accuracy of at least 0.5 MOA. Barrel operation life is stated to amount to 5,000 shots. The rifle is equipped with two Picatinny rails. The top rail with a 35 MOA gradient is located along the entire upper receiver length.\n\nThe most critical parts of the rifle are chrome-plated to ensure the maximum service life and reliability.\n\nExtra durable Cerakote coating is applied to the rifle.\n\nSnipex M100 is one of the modifications of the large-caliber single-shot rifle Snipex М with the barrel length of 1,000 mm. Caliber: 12.7x108; 12.7×99 mm NATO (.50 BMG).\n\nSnipex M75 is another modification of the large-caliber single-shot rifle Snipex М with the barrel length of 750 mm. Caliber: 12.7x108.\n\nSnipex .50 BMG — is the modification of the large-caliber single-shot rifle Snipex М with the barrel length of 750 mm. Caliber: 12.7×99 mm NATO (.50 BMG).\n\nSnipex 14,5 is a 14.5×114 caliber single-shot semi-automatic rifle whose main function is long-range target shooting (up to 5,000 m).\n\nSnipex MSA is the company’s latest development, which was presented at the exhibition “Arms and Security 2018”. It is a 5 to 10 shot rifle with automatic case ejection. Caliber: 12.7×99 mm NATO (.50 BMG); 12.7x108 or 14.5x114. Barrel length: 750; 850; 1,000; 1,100; 1,200 mm.\n\nThe weapon has the bullpup design, where the bolt carrier group is located behind the trigger. The rifle has short recoil. The lever accelerator ensures the automatics operation. Barrel locking is achieved through a rotating bolt. Recoil reduction is achieved due to the original design so that recoil does not create any unpleasant load for the shooter. Considerable part of the recoil energy is used to actuate the automatic reloading system. Free-floating barrel, gun carriage system.\n\nCivil variant\n\n\n"}
