{"id": "14580230", "url": "https://en.wikipedia.org/wiki?curid=14580230", "title": "American Software Testing Qualifications Board", "text": "American Software Testing Qualifications Board\n\nThe American Software Testing Qualifications Board (ASTQB) was founded in 2003 as the American Testing Board. In April 2005, the name was changed to the American Software Testing Qualifications Board.\n\nASTQB is a non-profit organization whose members comprise a group of highly experienced experts in software testing who volunteer their time to the development, maintenance, and promotion of the ISTQB Certified Tester program in the U.S. They also represent U.S. interests internationally as the national board for the U.S. within the International Software Testing Qualifications Board (ISTQB). The ISTQB is responsible for the international qualification scheme called \"ISTQB Certified Tester\".\n\nISTQB certificates at Foundation Level are also dual certified by ISEB, which was established in 1967 and is the world's leading issuer of Software Testing certifications and the only certifications aligned with the worldwide Professionalism in IT campaign.\n\nASTQB's exam and accreditation fees are charged to cover the cost connected with the administration of exams, applications for accreditation, the maintenance of a physical office, exhibits at leading software testing conventions, and the employment of administrative staff.\n\n\n"}
{"id": "761263", "url": "https://en.wikipedia.org/wiki?curid=761263", "title": "Antifuse", "text": "Antifuse\n\nAn antifuse is an electrical device that performs the opposite function to a fuse. Whereas a fuse starts with a low resistance and is designed to permanently break an electrically conductive path (typically when the current through the path exceeds a specified limit), an antifuse starts with a high resistance and is designed to permanently create an electrically conductive path (typically when the voltage across the antifuse exceeds a certain level). This technology has many applications.\n\nAntifuses are best known for their use in mini-light (or miniature) style low-voltage Christmas tree lights. Ordinarily (for operation from mains voltages), the lamps are wired in series. (The larger, traditional, C7 and C9 style lights are wired in parallel and are rated to operate directly at mains voltage.) Because the series string would be rendered inoperable by a single lamp failing, each bulb has an antifuse installed within it. When the bulb blows, the entire mains voltage is applied across the single blown lamp. This rapidly causes the antifuse to short out the blown bulb, allowing the series circuit to resume functioning, albeit with a larger proportion of the mains voltage now applied to each of the remaining lamps. The antifuse is made using wire with a high resistance coating and this wire is coiled over the two vertical filament support wires inside the bulb. The insulation of the antifuse wire withstands the ordinary low voltage imposed across a functioning lamp but rapidly breaks down under the full mains voltage, giving the antifuse action. Occasionally, the insulation fails to break down on its own, but tapping the blown lamp will usually finish the job. Often a special bulb with no antifuse and often a slightly different rating (so it blows first as the voltage gets too high) known as a \"fuse bulb\" is incorporated into the string of lights to protect against the possibility of severe overcurrent if too many bulbs fail.\n\nAntifuses are widely used to permanently program integrated circuits (ICs).\n\nCertain programmable logic devices (PLDs), such as structured ASICs, use antifuse technology to configure logic circuits and create a customized design from a standard IC design. Antifuse PLDs are one time programmable in contrast to other PLDs that are SRAM based and which may be reprogrammed to fix logic bugs or add new functions. Antifuse PLDs have advantages over SRAM based PLDs in that like ASICs, they do not need to be configured each time power is applied. They may be less susceptible to alpha particles which can cause circuits to malfunction. Also circuits built via the antifuse's permanent conductive paths may be faster than similar circuits implemented in PLDs using SRAM technology. QuickLogic Corporation refers to their antifuses as \"ViaLinks\" because blown fuses create a connection between two crossing layers of wiring on the chip in the same way that a via on a printed circuit board creates a connection between copper layers.\n\nAntifuses may be used in programmable read-only memory (PROM). Each bit contains both a fuse and an antifuse and is programmed by triggering one of the two. This programming, performed after manufacturing, is permanent and irreversible.\n\nDielectric antifuses employ a very thin oxide barrier between a pair of conductors. Formation of the conductive channel is performed by a dielectric breakdown forced by a high voltage pulse. Dielectric antifuses are usually employed in CMOS and BiCMOS processes as the required oxide layer thickness is lower than those available in bipolar processes.\n\nOne approach for the ICs that use antifuse technology employs a thin barrier of non-conducting amorphous silicon between two metal conductors. When a sufficiently high voltage is applied across the amorphous silicon it is turned into a polycrystalline silicon-metal alloy with a low resistance, which is conductive.\n\nAmorphous silicon is a material usually not used in either bipolar or CMOS processes and requires an additional manufacturing step.\n\nThe antifuse is usually triggered using an approximately 5 mA current. With a poly-diffusion antifuse, the high current density creates heat, which melts a thin insulating layer between polysilicon and diffusion electrodes, creating a permanent resistive silicon link.\n\nZener diodes can be used as antifuses. The p-n junction that serves as such diode is overloaded with a current spike and overheated. At temperatures above 100 °C and current densities above 10 A/cm the metallization undergoes electromigration and forms spikes through the junction, shorting it out; this process is known as Zener zap in the industry. The spike is formed on and slightly below the silicon surface, just below the passivation layer without damaging it. The conductive shunt therefore does not compromise integrity and reliability of the semiconductor device. Typically a few-millisecond pulse at 100-200 mA is sufficient for common bipolar devices, for a non-optimized antifuse structure; specialized structures will have lower power demands. The resulting resistance of the junction is in the range of 10 ohms.\n\nThe Zener antifuses can be made without additional manufacturing steps with most CMOS, BiCMOS and bipolar processes; hence their popularity in analog and mixed-signal circuits. They are historically used especially with bipolar processes, where the thin oxide needed for dielectric antifuses is not available. Their disadvantage, however, is lower area efficiency compared to other types.\n\nA standard NPN transistor structure is often used in common bipolar processes as the antifuse. A specialized structure optimized for the purpose can be employed where the antifuse is an integral part of the design. The terminals of the antifuses are usually accessible as bonding pads and the trimming process is performed before wire-bonding and encapsulating the chip. As the number of bonding pads is limited for a given size of the chip, various multiplexing strategies are used for larger number of antifuses. In some cases a combined circuit with zeners and transistors can be used to form a zapping matrix; with additional zeners, the trimming (which uses voltages higher than the normal operational voltage of the chip) can be performed even after packaging the chip.\n\nZener zap is frequently employed in mixed-signal circuits for trimming values of analog components. For example a precision resistor can be manufactured by forming several series resistors with Zeners in parallel (oriented to be nonconductive during normal operation of the device) and then shorting selected Zeners to shunt the unwanted resistors. By this approach, it is possible only to lower the value of the resulting resistor. It is therefore necessary to shift the manufacturing tolerances so that the lowest-value typically made is equal to or larger than the desired value. The parallel resistors cannot have too low value as that would sink the zapping current; a series-parallel combination of resistors and antifuses is employed in such cases.\n\nIn a similar fashion to that of Christmas tree lights, before the advent of high-intensity discharge lamps, street light circuits using incandescent light bulbs were often operated as high-voltage series circuits. Each individual street-lamp was equipped with a \"film cutout\"; a small disk of insulating film that separated two contacts connected to the two wires leading to the lamp. In the same fashion as with the Christmas lights described above, if the lamp failed, the entire voltage of the street lighting circuit (thousands of volts) was imposed across the insulating film in the cutout, causing it to rupture. In this way, the failed lamp was bypassed and illumination restored to the rest of the street. Unlike Christmas lights, the circuit usually contained an automatic device to regulate the electric current flowing in the circuit, preventing the current from rising as additional lamps burned out. When the failed lamp was finally changed, a new piece of film was also installed, once again separating the electrical contacts in the cutout. This style of street lighting was recognizable by the large porcelain insulator that separated the lamp and reflector from the light's mounting arm; the insulator was necessary because the two contacts in the lamp's base may have routinely operated at a potential of several thousands of volts above ground/earth.\n\n\n"}
{"id": "73664", "url": "https://en.wikipedia.org/wiki?curid=73664", "title": "Astrolabe", "text": "Astrolabe\n\nAn astrolabe ( \"astrolabos\"; \"al-Asturlāb\"; \"Akhtaryab\") is an elaborate inclinometer, historically used by astronomers and navigators to measure the inclined position in the sky of a celestial body, day or night. The word astrolabe means \"the one that catches the heavenly bodies.\" It can thus be used to identify stars or planets, to determine local latitude given local time (and vice versa), to survey, or to triangulate. It was used in classical antiquity, the Islamic Golden Age, the European Middle Ages and the Age of Discovery for all these purposes.\n\nThe astrolabe's importance not only comes from the early development of astronomy, but is also effective for determining latitude on land or calm seas. Although it is less reliable on the heaving deck of a ship in rough seas, the mariner's astrolabe was developed to solve that problem.\n\nOED gives the translation \"star-taker\" for the English word \"astrolabe\" and traces it through medieval Latin to the Greek word \"astrolabos\", from \"astron\" \"star\" and \"lambanein\" \"to take\". In the medieval Islamic world the Arabic word \"al-Asturlāb\" (i.e. astrolabe) was given various etymologies. In Arabic texts, the word is translated as \"ākhdhu al-Nujuum\" (, lit. \"star-taker\"), a direct translation of the Greek word.\n\nAl-Biruni quotes and criticizes medieval scientist Hamzah al-Isfahani who stated: \"asturlab is an arabization of this Persian phrase\" (\"sitara yab\", meaning \"taker of the stars\"). In medieval Islamic sources, there is also a folk etymology of the word as \"lines of lab\", where \"Lab\" refers to a certain son of Idris (Enoch). This etymology is mentioned by a 10th-century scientist named al-Qummi but rejected by al-Khwarizmi.\n\nAn early astrolabe was invented in the Hellenistic civilization by Apollonius of Perga between 220 and 150 BC, often attributed to Hipparchus. The astrolabe was a marriage of the planisphere and dioptra, effectively an analog calculator capable of working out several different kinds of problems in astronomy. Theon of Alexandria (c. 335 – c. 405) wrote a detailed treatise on the astrolabe, and Lewis argues that Ptolemy used an astrolabe to make the astronomical observations recorded in the \"Tetrabiblos\". The invention of the plane astrolabe is sometimes wrongly attributed to Theon's daughter Hypatia ( 350–370; died 415 AD), but it is, in fact, known to have already been in use at least 500 years before Hypatia was born. The misattribution comes from a misinterpretation of a statement in a letter written by Hypatia's pupil Synesius ( 373 – 414), which mentions that Hypatia had taught him how to construct a plane astrolabe, but does not state anything about her having invented it herself.\n\nAstrolabes continued in use in the Greek-speaking world throughout the Byzantine period. About 550 AD, Christian philosopher John Philoponus wrote a treatise on the astrolabe in Greek, which is the earliest extant treatise on the instrument. Mesopotamian bishop Severus Sebokht also wrote a treatise on the astrolabe in the Syriac language in the mid-7th century. Sebokht refers to the astrolabe as being made of brass in the introduction of his treatise, indicating that metal astrolabes were known in the Christian East well before they were developed in the Islamic world or in the Latin West.\n\nAstrolabes were further developed in the medieval Islamic world, where Muslim astronomers introduced angular scales to the design, adding circles indicating azimuths on the horizon. It was widely used throughout the Muslim world, chiefly as an aid to navigation and as a way of finding the Qibla, the direction of Mecca. Eighth-century mathematician Muhammad al-Fazari is the first person credited with building the astrolabe in the Islamic world.\n\nThe mathematical background was established by Muslim astronomer Albatenius in his treatise \"Kitab az-Zij\" (c. 920 AD), which was translated into Latin by Plato Tiburtinus (\"De Motu Stellarum\"). The earliest surviving astrolabe is dated AH 315 (927–28 AD). In the Islamic world, astrolabes were used to find the times of sunrise and the rising of fixed stars, to help schedule morning prayers (salat). In the 10th century, al-Sufi first described over 1,000 different uses of an astrolabe, in areas as diverse as astronomy, astrology, navigation, surveying, timekeeping, prayer, Salat, Qibla, etc.\n\nThe spherical astrolabe was a variation of both the astrolabe and the armillary sphere, invented during the Middle Ages by astronomers and inventors in the Islamic world.\nThe earliest description of the spherical astrolabe dates back to Al-Nayrizi (fl. 892–902). In the 12th century, Sharaf al-Dīn al-Tūsī invented the \"linear astrolabe\", sometimes called the \"staff of al-Tusi\", which was \"a simple wooden rod with graduated markings but without sights. It was furnished with a plumb line and a double chord for making angular measurements and bore a perforated pointer\". The geared mechanical astrolabe was invented by Abi Bakr of Isfahan in 1235.\n\nHerman Contractus, the abbot of Reichman Abbey, examined the use of the astrolabe in \"Mensura Astrolai\" during the 11th century. Peter of Maricourt wrote a treatise on the construction and use of a universal astrolabe in the last half of the 13th century entitled \"Nova compositio astrolabii particularis\". Universal astrolabes can be found at the History of Science Museum in Oxford.\n\nEnglish author Geoffrey Chaucer (c. 1343–1400) compiled \"A Treatise on the Astrolabe\" for his son, mainly based on Messahalla. The same source was translated by French astronomer and astrologer Pélerin de Prusse and others. The first printed book on the astrolabe was \"Composition and Use of Astrolabe\" by Christian of Prachatice, also using Messahalla, but relatively original.\n\nIn 1370, the first Indian treatise on the astrolabe was written by the Jain astronomer Mahendra Suri.\n\nA simplified astrolabe, known as a \"balesilha\", was used by sailors to get an accurate reading of latitude while out to sea. The use of the \"balesilha\" was promoted by Prince Henry (1394–1460) while out navigating for Portugal.\n\nThe first known metal astrolabe in Western Europe is the Destombes astrolabe made from brass in tenth-century Spain. Metal astrolabes avoided the warping that large wooden ones were prone to, allowing the construction of larger and therefore more accurate instruments. Metal astrolabes were heavier than wooden instruments of the same size, making it difficult to use them in navigation.\n\nThe astrolabe was almost certainly first brought north of the Pyrenees by Gerbert of Aurillac (future Pope Sylvester II), where it was integrated into the quadrivium at the school in Reims, France sometime before the turn of the 11th century. In the 15th century, French instrument maker Jean Fusoris (c. 1365–1436) also started remaking and selling astrolabes in his shop in Paris, along with portable sundials and other popular scientific devices of the day. Thirteen of his astrolabes survive to this day. One more special example of craftsmanship in early 15th-century Europe is the astrolabe designed by Antonius de Pacento and made by Dominicus de Lanzano, dated 1420.\n\nIn the 16th century, Johannes Stöffler published \"Elucidatio fabricae ususque astrolabii\", a manual of the construction and use of the astrolabe. Four identical 16th-century astrolabes made by Georg Hartmann provide some of the earliest evidence for batch production by division of labor.\n\nMechanical astronomical clocks were initially influenced by the astrolabe; they could be seen in many ways as clockwork astrolabes designed to produce a continual display of the current position of the sun, stars, and planets. For example, Richard of Wallingford's clock (c. 1330) consisted essentially of a star map rotating behind a fixed rete, similar to that of an astrolabe.\n\nMany astronomical clocks use an astrolabe-style display, such as the famous clock at Prague, adopting a stereographic projection (see below) of the ecliptic plane. In recent times, astrolabe watches have become popular. For example, Swiss watchmaker Dr. Ludwig Oechslin designed and built an astrolabe wristwatch in conjunction with Ulysse Nardin in 1985. Dutch watchmaker Christaan van der Klauuw also manufactures astrolabe watches today.\n\nAn astrolabe consists of a disk, called the \"mater\" (mother), which is deep enough to hold one or more flat plates called \"tympans\", or \"climates\". A tympan is made for a specific latitude and is engraved with a stereographic projection of circles denoting azimuth and altitude and representing the portion of the celestial sphere above the local horizon. The rim of the mater is typically graduated into hours of time, degrees of arc, or both.\n\nAbove the mater and tympan, the \"rete\", a framework bearing a projection of the ecliptic plane and several pointers indicating the positions of the brightest stars, is free to rotate. These pointers are often just simple points, but depending on the skill of the craftsman can be very elaborate and artistic. There are examples of astrolabes with artistic pointers in the shape of balls, stars, snakes, hands, dogs' heads, and leaves, among others. The names of the indicated stars were often engraved on the pointers in Arabic or Latin. Some astrolabes have a narrow \"rule\" or \"label\" which rotates over the rete, and may be marked with a scale of declinations.\n\nThe rete, representing the sky, functions as a star chart. When it is rotated, the stars and the ecliptic move over the projection of the coordinates on the tympan. One complete rotation corresponds to the passage of a day. The astrolabe is therefore a predecessor of the modern planisphere.\n\nOn the back of the mater there is often engraved a number of scales that are useful in the astrolabe's various applications. These vary from designer to designer, but might include curves for time conversions, a calendar for converting the day of the month to the sun's position on the ecliptic, trigonometric scales, and a graduation of 360 degrees around the back edge. The \"alidade\" is attached to the back face. An alidade can be seen in the lower right illustration of the Persian astrolabe above. When the astrolabe is held vertically, the alidade can be rotated and the sun or a star sighted along its length, so that its altitude in degrees can be read (\"taken\") from the graduated edge of the astrolabe; hence the word's Greek roots: \"astron\" (ἄστρον) = star + \"lab-\" (λαβ-) = to take.\n\n\n\n\n\n"}
{"id": "56814029", "url": "https://en.wikipedia.org/wiki?curid=56814029", "title": "Business machine", "text": "Business machine\n\nBusiness machine is a somewhat obsolete term for a machine that assists in the clerical activities common in business companies. Examples include:\n\n\n"}
{"id": "6119008", "url": "https://en.wikipedia.org/wiki?curid=6119008", "title": "Cambridge Scientific Instrument Company", "text": "Cambridge Scientific Instrument Company\n\nCambridge Scientific Instrument Company was a company founded in 1881 by Horace Darwin (1851–1928) and Albert George Dew-Smith (1848–1903) to manufacture scientific instruments.\n\nDarwin was first apprenticed to an engineering firm in Kent, and returned to Cambridge in 1875. Dew-Smith was an engineer, photographer and instrument maker who was at Trinity College, Cambridge with Darwin. Darwin's grandson Erasmus Darwin Barlow was later chairman.\n\nDesigned between 1884/85, The rocking microtome was one of Darwin's most successful designs which continued to be manufactured until the 1970s.\n\nTheir partnership became a Limited Liability Company in 1895. In 1920 it took over the R.W. Paul Instrument Company of London, and became The Cambridge and Paul Instrument Company Ltd. The name was shortened to the Cambridge Instrument Company Ltd. in 1924 when it was converted to a Public limited company. The company was finally taken over by the George Kent Group in 1968, forming the largest independent British manufacturer of industrial instruments.\n\nSeveral early employees went on to further renown, including Robert Stewart Whipple, who was appointed personal assistant to Horace Darwin in 1898, and later became Managing Director and Chairman of the company. His collection of scientific instruments later formed the basis of the Whipple Museum of the History of Science in 1944. William G. Pye, who had joined as foreman in 1880, left in 1898 to form the W.G. Pye Instrument Company with his son, which ultimately become the Pye group of companies.\n\n\n"}
{"id": "1503768", "url": "https://en.wikipedia.org/wiki?curid=1503768", "title": "Chocolate fountain", "text": "Chocolate fountain\n\nA chocolate fountain is a device for serving chocolate fondue. Typical examples resemble a stepped cone, standing 2–4 feet tall with a crown at the top and stacked tiers over a basin at the bottom. The basin is heated to keep the chocolate in a liquid state so it can be pulled into a center cylinder then vertically transported to the top of the fountain by an Archimedes screw. From there it flows over the tiers creating a chocolate \"waterfall\" in which food items like strawberries or marshmallows can be dipped.\n\n\"Chocolate fountain machines\" were invented by Ben Brisman and popularized in 1991 by a Canadian company called Design & Realisation. They did not become very popular until Design & Realisation began displaying these fountains at the National Restaurant Show in Chicago, starting in the early 1990s.\n\nJay Harlan (an entrepreneur and former Marriott catering executive) collaborated with D & R to introduce the chocolate fountain to the U.S. hotel and resort industry. Jay Harlan's company (Buffet Enhancements International) began distributing the D & R fountain in the U.S. in 2001. It wasn't long after 2001 that Buffet Enhancements copied and improved the D & R fountain idea and began manufacturing their own American-made version.\n\nChocolate fountains can be categorized as commercial-use and personal-use.\nCommercial Chocolate Fountains can be categorized as Auger system and Pump system.\n\nCommercial chocolate fountains usually range in size from about 20\" inches to 50\" inches tall and are designed for use in professional environments such as catering. These fountains are normally constructed of food-grade stainless steel and, as such, are quite durable. Depending on the model, commercial chocolate fountains may hold as much as 35 pounds of chocolate. Originally, chocolate fountains consisted of multiple tiers that formed curtains of chocolate. New models on the market now have a 'cup' design, dropping chocolate from cup to cup which can be much more chocolate efficient than traditional models, and save quite a lot of money.\n\nHome chocolate fountains range in size up to about 19\" tall. They are primarily made of plastic and may have some stainless steel components (although all-stainless models do exist) and are usually dishwasher safe. Home chocolate fountains normally hold 6 or fewer pounds of chocolate.\nMelted chocolate is very temperamental, so rich couverture chocolate, which is high in cocoa butter, is commonly used to ensure consistent flow. If the cocoa butter content of the chocolate is too low, an additive must be mixed in to decrease viscosity. (Vegetable oil is most commonly used to do this.) But even couverture chocolate—unless specifically designed for fountains—often still requires an additive to make it flow smoothly. Because of this, it is highly recommended that chocolate formulated specifically for fountains be used to avoid the need for the addition of vegetable oil, as the oil gives a slimy, gritty taste and texture to the chocolate. Few chocolate fountains are capable of melting chocolate directly in the basin, so chocolate is typically melted in a microwave or double boiler before pouring it into the fountain.\n\nThe fluid dynamics of home chocolate fountains are particularly problematic (it is a so-called non Newtonian fluid ), since the gravitational forces are much lower than the viscous forces; this means that small chocolate fountains need a very high proportion of vegetable oil. There is a trade-off between looks and taste: if perfect smooth flow may be sacrificed, it's possible to use non-oily chocolate.\n\nA cheaper alternative to couverture chocolate is chocolate-flavored syrup, also called \"chocolate coating\". Chocolate coating is already in liquid form and costs much less than couverture chocolate - although many would argue that the extra expense of gourmet Belgian couverture chocolate is more than worth it. Another alternative is to use dark chocolate, such as 70% cocoa chocolate. This contains much less sugar, so it isn't so sticky; a small quantity of hot water may be added to thin it further.\n\nOriginally, the market was entirely commercial, with chocolate fountains costing a large amount of money and requiring significant upkeep. The popularity of chocolate fountains grew to a point of demand at a consumer retail level as people who had seen commercial models at catered events inquired about purchasing their own fountains. Then, at the end of 2004, the Hellmann Group began marketing the Nostalgia Chocolate Fountain for personal use.\n\nThis expansion into the retail market caused the demand to peak drastically. As a result, the catering industry saw more requests for chocolate fountains at events. Flavoring oils such as mint, orange, and cappuccino were developed to give the chocolate extra taste. Caterers began adding food coloring to white chocolate to make it coincide with special holidays or events. Caterers and home users created special recipes for a variety of fondues that would flow well in a fountain; some of the more popular recipes included caramel, cheese, maple syrup, ranch dressing, and BBQ sauce. Because of the growing practice of using chocolate fountains for other types of fondue, chocolate fountains became interchangeably referred to as \"fondue fountains\".\n\nToday, commercial chocolate fountains are a common fixture at well-to-do galas, weddings, and catered parties; while home chocolate fountains are more commonly seen at birthday parties and friendly gatherings.\n\nNew models often incorporate more than one flavour of chocolate and a 'cup' design in place of the traditional tiered models. These models are designed to cater to new customer desires, to decrease the amount of chocolate needed and chocolate wastage.\n\nThe Jean-Philippe Patisserie at the Bellagio Casino in Paradise, Nevada is home to the world's tallest chocolate fountain.\n\n"}
{"id": "29504901", "url": "https://en.wikipedia.org/wiki?curid=29504901", "title": "Computer says no", "text": "Computer says no\n\n\"Computer Says No\", or the \"Computer says no attitude\", is the popular name given to an attitude seen in some public-facing organisations where the default response to a customer’s request is to check with information stored on or generated by a computer, and then make decisions based on that, often in the face of common sense.\n\nThere may also be an element of deliberate unhelpfulness towards customers and service-users, whereby more \"could\" be done to reach a mutually satisfactory outcome, but is not.\n\nThe name gained popularity through the British sketch comedy \"Little Britain\".\n\nIn \"Little Britain\", \"Computer Says No\" is the catchphrase of the character Carol Beer (played by David Walliams), a bank worker and later holiday rep and hospital receptionist, who always responds to a customer's enquiry by typing it into her computer and responding with \"Computer Says No\" to even the most reasonable of requests. When asked to do something aside from asking the computer, she would shrug and remain obstinate in her unhelpfulness, and ultimately cough in the customer's face. The phrase was also used in the Australian soap opera \"Neighbours\" in 2006 as a reference to \"Little Britain\".\n\nThe \"Computer Says No\" attitude often comes from larger companies that rely on information stored electronically. When this information is not updated, it can often lead to refusals of financial products or incorrect information being sent out to customers. These situations can often be resolved by an employee updating the information; however, when this cannot be done easily, the \"Computer Says No\" attitude can be viewed as becoming prevalent when there is unhelpfulness as a result. This attitude can also occur when an employee fails to read human emotion in the customer and reacts according to his or her professional training or relies upon a script. This attitude also crops up when larger companies rely on computer credit scores and do not meet with a customer to discuss his or her individual needs, instead basing a decision upon information stored in computers. Some organisations attempt to offset this attitude by moving away from reliance on electronic information and using a human approach towards requests.\n\n\"Computer Says No\" happens in a more literal sense when computer systems employ filters that prevent messages being passed along, as when these messages are perceived to include obscenities. When information is not passed through to the person operating the computer, decisions may be made without seeing the whole picture.\n\n"}
{"id": "4529151", "url": "https://en.wikipedia.org/wiki?curid=4529151", "title": "Detector (radio)", "text": "Detector (radio)\n\nIn radio, a detector is a device or circuit that extracts information from a modulated radio frequency current or voltage. The term dates from the first three decades of radio (1888-1918). Unlike modern radio stations which transmit sound (an audio signal) on an uninterrupted carrier wave, early radio stations transmitted information by \"radiotelegraphy\". The transmitter was switched on and off to produce long or short periods of radio waves, spelling out text messages in Morse code. Therefore, early radio receivers had only to distinguish between the presence or absence of a radio signal. The device that performed this function in the receiver circuit was called a \"detector\". A variety of different detector devices, such as the coherer, electrolytic detector, magnetic detector and the crystal detector were used during the wireless telegraphy era until superseded by vacuum tube technology.\n\nAfter sound (amplitude modulation, AM) transmission began around 1920, the term evolved to mean a demodulator, (usually a vacuum tube) which extracted the audio signal from the radio frequency carrier wave. This is its current meaning, although modern detectors usually consist of semiconductor diodes, transistors, or integrated circuits.\n\nIn a superheterodyne receiver the term is also sometimes used to refer to the mixer, the tube or transistor which converts the incoming radio frequency signal to the intermediate frequency. The mixer is called the first detector, while the demodulator that extracts the audio signal from the intermediate frequency is called the second detector.\n\nIn microwave and millimeter wave technology the terms \"detector\" and \"crystal detector\" refer to waveguide or coaxial transmission line components, used for power or SWR measurement, that typically incorporate point contact diodes or surface barrier Schottky diodes.\n\nOne major technique is known as envelope detection. The simplest form of envelope detector is the diode detector that consists of a diode connected between the input and output of the circuit, with a resistor and capacitor in parallel from the output of the circuit to the ground to form a low pass filter. If the resistor and capacitor are correctly chosen, the output of this circuit will be a nearly identical voltage-shifted version of the original signal.\n\nAn early form of envelope detector was the crystal detector, which was used in the crystal set radio receiver. A later version using a crystal diode is still used in crystal radio sets today. The limited frequency response of the headset eliminates the RF component, making the low pass filter unnecessary. \n\nMore sophisticated envelope detectors include the grid-leak detector, the plate detector, the infinite-impedance detector, transistor equivalents of them and precision rectifiers using operational amplifiers.\n\nA product detector is a type of demodulator used for AM and SSB signals, where the original carrier signal is removed by multiplying the received signal with a signal at the carrier frequency (or near to it). Rather than converting the envelope of the signal into the decoded waveform by rectification as an envelope detector would, the product detector takes the product of the modulated signal and a local oscillator, hence the name. By heterodyning, the received signal is mixed (in some type of nonlinear device) with a signal from the local oscillator, to give sum and difference frequencies to the signals being mixed, just as a \"first mixer\" stage in a superhet would produce an intermediate frequency; the beat frequency in this case, the low frequency modulating signal is recovered and the unwanted high frequencies filtered out from the output of the product detector. \n\nProduct detector circuits are and so essentially ring modulators or synchronous detectors and closely related to some phase-sensitive detector circuits. They can be implemented using something as simple as ring of diodes or a single dual-gate Field Effect Transistor to anything as sophisticated as an Integrated Circuit containing a Gilbert cell.\n\nAM detectors cannot demodulate FM and PM signals because both have a constant amplitude. However an AM radio may detect the sound of an FM broadcast by the phenomenon of slope detection which occurs when the radio is tuned slightly above or below the nominal broadcast frequency. Frequency variation on one sloping side of the radio tuning curve gives the amplified signal a corresponding local amplitude variation, to which the AM detector is sensitive. Slope detection gives inferior distortion and noise rejection compared to the following dedicated FM detectors that are normally used.\n\nA phase detector is a nonlinear device whose output represents the phase difference between the two oscillating input signals. It has two inputs and one output: a reference signal is applied to one input and the phase or frequency modulated signal is applied to the other. The output is a signal that is proportional to the phase difference between the two inputs.\n\nIn phase demodulation the information is contained in the amount and rate of phase shift in the carrier wave.\n\nThe Foster-Seeley discriminator is a widely used FM detector. The detector consists of a special center-tapped transformer feeding two diodes in a full wave DC rectifier circuit. When the input transformer is tuned to the signal frequency, the output of the discriminator is zero. When there is no deviation of the carrier, both halves of the center tapped transformer are balanced. As the FM signal swings in frequency above and below the carrier frequency, the balance between the two halves of the center-tapped secondary is destroyed and there is an output voltage proportional to the frequency deviation.\n\nThe ratio detector is a variant of the Foster-Seeley discriminator, but one diode conducts in an opposite direction, and using a tertiary winding in the preceding transformer. The output in this case is taken between the sum of the diode voltages and the center tap. The output across the diodes is connected to a large value capacitor, which eliminates AM noise in the ratio detector output. The ratio detector has the advantage over the Foster-Seeley discriminator that it will not respond to AM signals, thus potentially saving a limiter stage; however the output is only 50% of the output of a discriminator for the same input signal. The ratio detector has wider bandwidth but more distortion than the Foster-Seeley discriminator.\n\nIn quadrature detectors, the received FM signal is split into two signals. One of the two signals is then passed through a high-reactance capacitor, which shifts the phase of that signal by 90 degrees. This phase-shifted signal is then applied to an LC circuit, which is resonant at the FM signal's unmodulated, \"center,\" or \"carrier\" frequency. If the received FM signal's frequency equals the center frequency, then the two signals will have a 90-degree phase difference and they are said to be in \"phase quadrature\" — hence the name of this method. The two signals are then multiplied together in an analog or digital device, which serves as a phase detector; that is, a device whose output is proportional to the phase difference between two signals. In the case of an unmodulated FM signal, the phase detector's output is — after the output has been filtered; that is, averaged over time — constant; namely, zero. However, if the received FM signal has been modulated, then its frequency will vary from the center frequency. In this case, the resonant LC circuit will further shift the phase of the signal from the capacitor, so that the signal's total phase shift will be the sum of the 90 degrees that's imposed by the capacitor and the positive or negative phase change that's imposed by the LC circuit. Now the output from the phase detector will differ from zero, and in this way, one recovers the original signal that was used to modulate the FM carrier.\n\nThis detection process can also be accomplished by combining, in an exclusive-OR (XOR) logic gate, the original FM signal and a square wave whose frequency equals the FM signal's center frequency. The XOR gate produces an output pulse whose duration equals the difference between the times at which the square wave and the received FM signal pass through zero volts. As the FM signal's frequency varies from its unmodulated center frequency (which is also the frequency of the square wave), the output pulses from the XOR gate become longer or shorter. (In essence, this quadrature detector converts an FM signal into a pulse-width modulated (PWM) signal.) When these pulses are filtered, the filter's output rises as the pulses grow longer and its output falls as the pulses grow shorter. In this way, one recovers the original signal that was used to modulate the FM carrier.\n\nLess common, specialized, or obsolescent types of detectors include:\n\nThe phase-locked loop detector requires no frequency-selective LC network to accomplish demodulation. In this system, a voltage controlled oscillator (VCO) is phase locked by a feedback loop, which forces the VCO to follow the frequency variations of the incoming FM signal. The low-frequency error voltage that forces the VCO's frequency to track the frequency of the modulated FM signal is the demodulated audio output.\n\n\nSimple block diagrams and descriptions of key circuits for FM transmitters and receivers: \"\" \n"}
{"id": "46794289", "url": "https://en.wikipedia.org/wiki?curid=46794289", "title": "Diana Napolis", "text": "Diana Napolis\n\nDiana Louisa Napolis (born 1955), also known by her on-line pseudonym Karen Curio Jones or more often simply Curio, is an American former social worker. Between the late 1990s and 2000, Napolis posted a series of pseudonymous accusations alleging that individuals skeptical of the satanic ritual abuse moral panic were involved in a conspiracy to cover-up the sexual abuse and murder of children. The pseudonymous poster's real life identity was confirmed as Napolis in 2000.\n\nIn 2001, she was charged with stalking film director Steven Spielberg, and in 2002 faced more charges for making death threats against actress Jennifer Love Hewitt, and was committed to a state hospital until fit to stand trial. After a year in prison Napolis pleaded guilty to stalking and was released on probation.\n\nNapolis originally worked as a child protection worker for nearly 10 years (leaving the position in 1996), becoming involved in the satanic ritual abuse (SRA) moral panic that arose in the early 1980s. By the late 1990s the phenomenon was rejected by mainstream scholars and law enforcement experts, but Napolis continued to believe in the existence of SRA. Napolis held that those who had discredited the phenomenon were themselves child abusers involved in a conspiracy to conceal their activities from the public.\n\nPosting under the screen name \"Curio\", Napolis began a pattern of on-line harassment against those she believed were involved in the conspiracy, posting information about the individuals. Among those she targeted were Carol Hopkins, a school administrator who was part of a grand jury in San Diego, California that criticized social workers for removing children from their home without reason; Michael Aquino, a member of the Temple of Set and a lieutenant colonel in the United States Army Reserve against whom accusations of SRA were made but dropped as the accusations proved to be impossible; and Elizabeth Loftus, a professor who studied memory who believed coercive questioning techniques by poorly-trained investigators led to young children making false allegations of child sexual abuse. Loftus was confronted at a New Zealand academic conference by a group of people who accused her of conspiring to help child molesters, with information consisting largely of the postings made by Napolis. Napolis' actions against Aquino also led to the first ever lawsuit in the state of California that attempted to place responsibility for a Usenet poster's actions on their internet service provider, and one of the first to be filed under the United States Communications Decency Act.\n\nUsing public computers in internet cafes and libraries, Napolis concealed her identity for five years while continuing to post information on-line about those she believed were involved in the conspiracy. In 2000, private researcher Michelle Devereaux and the San Diego State University police tracked Napolis and caught her in the act of posting information as Curio on-line from a campus lab. No charges were filed, but by revealing her identity, those Napolis had harassed ceased to consider her a serious threat. The story was reported in \"The San Diego Union-Tribune\", which was added to her on-line list of harassing parties.\n\nIn the fall of 2001, Steven Spielberg filed a restraining order against Napolis after she made harassing telephone calls to him. Napolis claimed Spielberg and his wife were part of a satanic cult operating out of his basement that had implanted a microchip called \"soulcatcher\" in her brain, an accusation to which Spielberg replied \"To state the obvious, I am not involved with any form of manipulating Ms. Napolis's mind or body through remote technology or otherwise.\" Spielberg also expressed concern for the safety and security of his family. His security team indicated they believed Napolis to be suffering from a delusional disorder and posed \"a serious risk of violent confrontation\". The judge ruled Napolis was barred from approaching within 150 meters of Spielberg and believed her to be a \"credible threat\" to the director.\n\nOn September 18, 2002, Napolis \"verbally confronted\" Jennifer Love Hewitt while entering the 2002 Grammy Awards, and the subsequent day attempted to pose as a friend of the actress to enter the premiere of \"The Tuxedo\". On October 10 Napolis again tried to confront Hewitt at a filming, and e-mailed several death threats to the actress later that month. In December 2002, Napolis was arrested for stalking and making death threats against Hewitt, charged with six felonies, and remanded to San Diego County Jail on $500,000 bail. At her hearing, Napolis also admitted to becoming involved in a shoving match with Hewitt's mother while confronting the actress. Napolis accused the actress and Spielberg of being part of a satanic conspiracy and using mind controlling \"cybertronic\" technology to manipulate her body. Napolis was committed to Patton State Hospital in 2003 for three years or until fit to stand trial.\n\nAfter nearly a year in jail (including five months in a state psychiatric facility where she was judged delusional but fit to stand trial), Napolis pleaded guilty to stalking on September 29, 2003, receiving five years probation; in addition, she was required to enroll in a counseling program, surrender all weapons and firearms, abstain from drugs and alcohol and refrain from using computers. Napolis was also barred from any contact with Spielberg, Hewitt and their families for 10 years and was required by the judge to continue taking prescribed medication.\n\n"}
{"id": "8705426", "url": "https://en.wikipedia.org/wiki?curid=8705426", "title": "Digital Juice, Inc.", "text": "Digital Juice, Inc.\n\nDigital Juice, Inc is a privately held royalty-free content provider for professional video, print, and presentations. The company sells packaged content products direct to customers. Digital Juice was founded in 1992 and was originally based out of Ocala, Florida. Digital Juice has offices in Florida and Bangalore, India.\n\nThe company was founded in 1992 by David Hebel under the name Dimension Technologies Media Group as a developer of third party products for the Video Toaster. In 1995 the company released \"Club Toaster\" which was a monthly CD-ROM based product for the Amiga. The product contained animated backgrounds, still graphics, music, photos, articles, and product reviews. By 1997 Dimension Technologies was looking to move from developing for the Amiga and instead focus on Windows and Mac based content. The company renamed itself in 1997 to \"Digital Juice\" after the release of their first non-Amiga based product, \"Digital Juice for PowerPoint\".\n\nIn 2003, Digital Juice introduced their own online network called Digital Juice Television. At first it was limited to simple product demos and trade show coverage, but was expanded in early 2006 to include several new series involving tutorials and training videos.\n\nIn October 2007, Digital Juice finalized acquisition of The Ballistic Pixel Labs.\n\nIn 2002, the company received the top award, out of 250 nominees, in the \"New Business\" category from the University of Tampa's, John H Sykes School Of Business.\n\n"}
{"id": "2616345", "url": "https://en.wikipedia.org/wiki?curid=2616345", "title": "Drillship", "text": "Drillship\n\nA drillship is a merchant vessel designed for use in exploratory offshore drilling of new oil and gas wells or for scientific drilling purposes. In most recent years the vessels are used in deepwater and ultra-deepwater applications, equipped with the latest and most advanced dynamic positioning systems.\n\nThe first drillship was the CUSS I, designed by Robert F. Bauer of Global Marine in 1955. The CUSS I had drilled in 400 feet deep waters by 1957.\nRobert F. Bauer became the first president of the Global Marine in 1958.\n\nIn 1961 Global Marine started a new drillship era. They ordered several self-propelled drillships each with a rated centerline drilling of 20,000 foot-wells in water depths of 600 feet. The first was named CUSS (Glomar) II, a 5,500-deadweight-ton vessel, Costing around $4.5 million. Built by a Gulf Coast shipyard, the vessel was almost twice the size of the CUSS I, and became the world’s first drillship built as new construction which set sail in 1962.\n\nIn 1962 The Offshore Company elected to build a new type of drillship, larger than that of the Glomar class. This new drillships would feature a first ever anchor mooring array based on a unique turret system. The vessel was named Discoverer I. The Discoverer I had no main propulsion engines, meaning they needed to be towed out to the drill site.\n\nThe drillship can be used as a platform to carry out well maintenance or completion work such as casing and tubing installation, subsea tree installations and well capping. Drillships are often built to the design specification to meet the requirements set by the oil production company and/or investors.\n\nFrom the first drillship CUSS I to the Deepwater Asgard the fleet size has been growing ever since. In 2013 the worldwide fleet of drillships tops 80 ships, more than double its size in 2009. Drillships are not only growing in size but also in capability with new technology assisting operations from academic research to ice-breaker class drilling vessels. U.S. President Barack Obama's decision in late March 2010 to expand U.S. domestic exploratory drilling seems likely to increase further developments of drillship technology.\n\nDrillships are just one way to perform various types of drilling. This function can also be performed by semi-submersibles, jackups, barges, or platform rigs.\n\nDrillships have the functional ability of semi-submersible drilling rigs and also have a few unique features that separate them from all others. First being the ship-shaped design. A drillship has greater mobility and can move quickly under its own propulsion from drill site to drill site in contrast to semi-submersibles and jackup barges and platforms. Drillships have the ability to save time sailing between oilfields worldwide. A drillship takes 20 days to move from the Gulf of Mexico to the Offshore Angola. Whereas, a semi-submersible drilling unit must be towed and takes 70 days.\nDrillship construction cost is much higher than that of a semi-submersible. But although mobility comes at a high price, the drillship owners can charge higher day rates and get the benefit of lower idle times between assignments.\n\nThe table below depicts the industry’s way of classifying drill sites into different vintages, depending on their age and water depth.\nThe drilling operations are very detailed and in depth. A simple way to understand what a drillship is to do in order to drill, a marine riser is lowered from the drillship to the seabed with a blowout preventer (BOP) at the bottom that connects to the wellhead. The BOP is used to quickly disconnect the riser from the wellhead in times of emergency or in any needed situation.\nUnderneath the derrick is a moonpool, an opening through the hull covered by the rig floor. Some of the modern drillships have larger derricks that allow dual activity operations, for example simultaneous drilling and casing handling.\n\nThere are different types of offshore drilling units such as the oil platform, jackup rig, submersible drilling rig, semi-submersible platform and of course drillships. All drillships have what is called a ”moon pool”. The Moon pool is an opening on the base of the hull and depending on the mission the vessel is on, drilling equipment, small submersible crafts and divers may pass through the moon pool. Since the drillship is also a vessel, it can easily relocate to any desired location. But due to its mobility, drillships are not as stable compared to semi-submersible platforms. To maintain its position, drillships may utilize their anchors or use the ship’s computer-controlled system on board to run off their Dynamic positioning.\n\nOne of the world’s renowned drillship is Japan’s ocean-going drilling vessel the \"Chikyū\", which actually is a research vessel. The Chikyū has the remarkable ability to drill four miles down the seabed, which brings it at a depth of 23,000 feet below the seabed, bringing that to two to four times that of any other drillship.\n\nIn 2011 the Transocean drillship the \"Dhirubhai Deepwater KG1\" set the world water-depth record at 10,194 feet of water (3,107 meters) while working for Reliance - LWD and Directional drilling done by Sperry Drilling in India.\n\n"}
{"id": "10389895", "url": "https://en.wikipedia.org/wiki?curid=10389895", "title": "ESignal", "text": "ESignal\n\neSignal, a Windows-based application, uses JavaScript as the basis for the scripting language that programmers and traders can use for building custom indicators. This, in effect, includes eSignal users in the base from which to draw programmers for writing indicators.\n\neSignal provides streaming, real-time market data, news and analytics. The other products offered under the eSignal brand include eSignal, Advanced GET, eSignal OnDemand, \n\neSignal, Advanced GET, couples eSignal’s market data, back testing and trading strategy tools with a proprietary set of indicators, including the Elliott Oscillator, eXpert Trend Locator and False Bar Stochastic. Its rules-based set-ups include the signature Advanced GET Type 1 and 2 trades.\n"}
{"id": "6539374", "url": "https://en.wikipedia.org/wiki?curid=6539374", "title": "EditDroid", "text": "EditDroid\n\nThe EditDroid is a computerized analog NLE (non-linear editing system), which was developed by Lucasfilm spin-off company, the Droid Works and Convergence Corporation who formed a joint venture company. The company existed up through the mid-80's to the early 90's in an attempt to move from analog editing methods to digital. EditDroid debuted at the National Association of Broadcasters (NAB) 62nd Annual meeting in Las Vegas in 1984 concurrent with another editing tool that would compete with the EditDroid for all its years in production, the Montage Picture Processor. The EditDroid was never a commercial success and after the close of \"The Droid Works\" in 1987 and subsequent redevelopment of the product for seven years, the software was eventually sold to Avid Technology in 1993. Only 24 EditDroid systems were ever produced.\n\nThe system is LaserDisc-based, relying on several LaserDisc players and a database system which queues up the clips in the order needed from the LaserDisc players in the most efficient way, so as to minimize skipping. This however isn't always possible. So if the edits aren't sufficiently close, the system isn't always fast enough to cue up the next clip.\n\nIt has three screens connected to it: one Sun-1 computer display as the graphical UI for the product, one small preview video monitor, and one large rear-projected monitor containing \"the cut\" which was controlled by a custom controller. The controller, called the TouchPad, features a KEM-style shuttle knob, a trackball, and a host of buttons with LED labels that changes in function depending on what the system was doing. The EditDroid pioneered the use of the graphical display for editing—introducing the timeline as well as digital picture icons to identify raw source clips.\n\nOnce the entire movie has been edited, an Edit Decision List of marked frames is turned over to a film laboratory where the actual pieces of film are spliced together in the correct order.\n\nThe EditDroid is obsolete by market standards, as the market for nonlinear editing systems has changed radically since its inception, with computer-based products like Final Cut Pro ranging entirely from the consumer to professional markets. In many respects the EditDroid was a concept demonstration of the future of editing, with a LaserDisc being a good 1980s simulation of what digital access would eventually become, and an editing interface and workflow that was more like today's methods than any of the videotape linear or analog nonlinear products leading up to the Avid/1 in 1990.\n\nThere are numerous advantages of using a digital editing solution over the older analog solutions, such as the Moviola. Not only is it much faster to locate the clips needed, keeping track of what can in some cases amount to a staggering amount of footage, is also much easier digitally. Also, editing film digitally is a non-destructive process, whereas the analog process requires the actual cutting and taping together of pieces of film as well as manual syncing of sound.\n\nAside from the technological advantages of digital editing, in his book \"In the Blink of an Eye\", editor Walter Murch mourns the loss of the older analog solutions. Analog editing requires the editor to frequently move back and forth or scrub in the source material to gain an overview, thus increasing one's familiarity with it. Since undoing an edit is such a laborious process, there is a high incentive to get the best edit cut the first time. This process which is not necessary to the same extent with NLE solutions in which one edit point can be made and undone very quickly.\n\nFurthermore, LaserDisc has a fixed resolution, whereas film can be focused to look ideal at any display size.\n\nWhile the LaserDisc format was brought to market in the late 1970s, first with the name of DiscoVision and later as LaserVision, and despite persistent promises from the Music Corporation of America, a cheap method of recording LaserDiscs never surfaced. This lack made it exceedingly difficult and cumbersome to create the needed LaserDiscs for the EditDroid. Also at this time, the storage available on a hard disk was prohibitively small and extremely expensive.\n\nFurthermore, many potential customers of the EditDroid were disappointed by the fact that while Lucasfilm Ltd. were the creators of the EditDroid, George Lucas had never in fact used the EditDroid on a movie. This fact stood in contrast to the fact that the EditDroid had been shown with \"Return of the Jedi\" clips on numerous occasions at tradeshows and at demonstrations. Lucas eventually used his EditDroids in the early '90s on his series \"The Young Indiana Jones Chronicles\".\n\n\n\n"}
{"id": "39556869", "url": "https://en.wikipedia.org/wiki?curid=39556869", "title": "Elisa Leonida Zamfirescu", "text": "Elisa Leonida Zamfirescu\n\nElisa Leonida Zamfirescu (10 November 1887 – 25 November 1973) was a Romanian engineer who was one of the first women to obtain a degree in engineering. She was born in the Romanian town of Galați but qualified in Berlin. During World War I she managed a hospital in Romania.\n\nElisa Zamfirescu was born in Galați, Romania on 10 November 1887. Her father, Atanase Leonida, was a career officer while her mother, Matilda Gill, was the daughter of a French-born engineer. Her brother Dimitrie Leonida was also an engineer.\n\nDue to prejudices against women in the sciences, Zamfirescu was rejected by the School of Bridges and Roads in Bucharest. In 1909 she was accepted at the Royal Academy of Technology Berlin, Charlottenburg. She graduated in 1912, with a degree in engineering. It has been claimed that Zamfirescu was the world's first female engineer, but Englishwoman Nina Cameron Graham also gained a degree in civil engineering in 1912, from the University of Liverpool and the Irish engineer Alice Perry graduated six years before either of them: in 1906.\n\nReturning to Romania, Zamfirescu worked as an assistant at the Geological Institute of Romania. During World War I, she joined the Red Cross and ran a hospital at Mărășești Romania. In 1917 her hospital received the wounded from the Battle of Mărășești between the German and the Romanian armies. It was a victory by Romania over 28 days during which there were over 12,000 Romanian and over 10,000 of the invaders who were wounded.\n\nAround this time, she met and married chemist Constantin Zamfirescu, brother of the politician and writer Duiliu Zamfirescu.\n\nAfter the war, Zamfirescu returned to the Geological Institute. She led several geology laboratories and participated in various field studies, including some that identified new resources of coal, shale, natural gas, chromium, bauxite and copper. Zamfirescu also taught physics and chemistry.\n\nZamfirescu retired in 1963, aged 75. In retirement she was involved in activism for disarmament. She died at the age of 86 on 25 November 1973.\n\nAn award for women working in science and technology was established in her name, the Premiul Elisa Leonida-Zamfirescu.\n\nZamfirescu was the first woman member of A.G.I.R. (General Association of Romanian Engineers). A street in Sector 1 of Bucharest bears her name, and she was honoured with a Google Doodle on the anniversary of her birthday in 2018.\n\n"}
{"id": "46952948", "url": "https://en.wikipedia.org/wiki?curid=46952948", "title": "Exnovation", "text": "Exnovation\n\nIn commerce and management, exnovation, an opposite of innovation, can occur when products and processes that have been tested and confirmed to be best-in-class are standardized to ensure that they are not innovated further. Companies that have followed exnovation as a strategy to improve organizational performance include General Electric, Ford Motor Company and American Airlines. \n\nOne of the earliest usages of the term came in 1981, when John Kimberly referred to \"removal of innovation from an organisation\". In 1996 A. Sandeep provided the modern definition of exnovation as the philosophy of not innovating – in other words, ensuring that best-in-class entities are not innovated further. Since then \"exnovation\" has become a notable parlance in various practices, from management to medicine.\n"}
{"id": "732339", "url": "https://en.wikipedia.org/wiki?curid=732339", "title": "External Short Messaging Entity", "text": "External Short Messaging Entity\n\nExternal Short Messaging Entity (ESME) is a term originally coined by Aldiscon to describe an external application that connects to a Short Message Service Center (SMSC) to engage in the sending and/or receiving of SMS messages.\n\nSME is a term used in many cellular circles to describe a network entity (mobile/cell phone) that can send/receive messages. ESME (pronounced EZ-mee) is essentially one of these but without all the wireless aspects; i.e. it is connected via TCP/IP, X.25 or similar. On SMPP 3.4 protocol specifications ESME refers only to external sources and sinks of short messages as Voice Processing Systems, WAP Proxy Servers or Message Handling computers, and it specifically excludes SMEs which are located within the Mobile Network, i.e., a mobile station (MS).\n\nTypical examples of ESMEs are systems that send automated marketing messages to mobile users and voting systems that process SMS votes (\"Pop Idol\", \"Big Brother\").\n\nSMSC uses protocols such as SMPP, UCP, OIS, CIMD, SMCI all of which denote the concept of an ESME connecting to an SMSC.\n\nESME always connects to SMSC using a TCP/IP, X.25, etc. and then binds for the service it needs from SMSC.\n\nFor SMPP it can bind for Receiving only service, Transmitting only service or both (Transceiver service). Before SMPP 3.4 it was required to have two different connections, one for Transmitting and the other one for Receiving. Starting with SMPP 3.4 a Tranceiver connection is enough for both.\n\nThe relation between ESME and SMSC is somehow a master-slave relation because SMSC is providing services to ESME, and usually ESME just uses these services from SMSC.\nOne of the functions of the SMSC is to store and forward the messages while the ESME doesn't have this function. When a message is sent by an ESME to SMSC towards its destination, this message may stay in a SMSC queue until its destination will become available. During this time the ESME has the options to cancel the message in queue, to replace it or to check its status. ESME can also send a message to multiple destinations which will be handled by SMSC.\n\nESME are usually termination points of an SMS network while SMSC are the core of it. SMSC can connect between them while ESME only connects to an SMSC.\nSMPP protocol is designed exactly in this manner for connecting a small end of the SMS network (which is an ESME) to the entire SMS network (which is done through the SMSC)\n\nESME is submitting MTs to SMSC, while SMSC is delivering MOs to ESME.\n\nAn example of how the routing can be done at SMSC level, but not mandatory as this depends a lot on the implementation of SMSC and the way the connection inside the SMSC is between routing part of the SMSC and SMPP interface can be as below:\nDuring the service agreement between ESME and service provider(SMSC side) one unique short code will be allocated to ESME. At the SMSC end smpp server will have list of all ESME address and active connection. When you send any message to short code, messages first comes to SMSC, SMSC decodes it according to GSM 3.4 spec, then one of the modules in SMSC checks the destination address and if it is short code then that module routes messages to SMPP server part of the SMSC. Now SMPP server will have all active connection, according to destination address it selects the ESME - SMPP server connection object, that object will be responsible to encode message according to SMPP protocol and forward to ESME.\n\nCommunication between SMSC and ESME can be on either SMPP or HTTP.\nIf you have SMPP account, you could connect to the SMPP IP+Port on TCP/IP and the SMPP will push MOs to ESME on SMPP connection, and ESME will push MTs on the same connection in reverse.\nIf you have HTTP account with the operator's SMSC, then the SMSC will submit MO to an URL given by you and to push MTs SMSC will give you on URL.\n"}
{"id": "12182296", "url": "https://en.wikipedia.org/wiki?curid=12182296", "title": "FRUMEL", "text": "FRUMEL\n\nFleet Radio Unit, Melbourne (FRUMEL) was a United States–Australian–British signals intelligence unit, founded in Melbourne, Australia, during World War II. It was one of two major Allied signals intelligence units, called Fleet Radio Units, in the Pacific theatres, the other being FRUPAC (also known as Station HYPO), in Hawaii. FRUMEL was a US Navy organisation, reporting directly to CiCPAC (Admiral Nimitz) in Hawaii and the Chief of Naval Operations (Admiral King) in Washington, and hence to the central crypographic organization. The separate Central Bureau in Melbourne (later Brisbane) was attached to (and reported to) MacArthur's Allied South West Pacific Area command headquarters.\n\nFRUMEL was established at the Monterey Apartments in Queens Road, in early 1942, and was made up of three main groups. First was Lieutenant Rudolph J. (Rudi) Fabian's 75-man codebreaker unit, previously based at the United States Navy's Station CAST in the Philippines before being evacuated by submarine on 8 April 1942. The second was Commander Eric Nave's small Royal Australian Navy-supported cryptography unit, which had moved to the Monterey Apartments from Victoria Barracks in February 1942. Nave's unit was made up of a core of naval personnel, heavily assisted by university academics and graduates specialising in linguistics and mathematics (including from June 1941 a \"cipher group\" of four from Sydney University). These included Thomas Room, Dale Trendall, Athanasius Treweek, Eric Barnes, Jack Davies and Ronald Bond. The third group was a trio of British Foreign Office linguists (Henry Archer, Arthur Cooper and Hubert Graves), and Royal Navy support staff, evacuated from Singapore, particularly from the Far East Combined Bureau (FECB) there. IBM (punched-card) tabulating machines were obtained in 1942 to replace that left behind in Manila Bay on leaving Corregidor.\n\nNave and Fabian had a difficult relationship, and Nave eventually joined the Army's Central Bureau at Brisbane. According to Jenkins, Fabian wasted no time in getting rid of the civilian supernumaries at Monterey, many of them British service wives who had been evacuated from Singapore. He also squeezed out the British diplomatic corps types like Cooper and Archer. Eventually he seems to have succeeded in ousting Nave, who went to Central Bureau, the joint Australian-US Army codebreaking unit in Brisbane. Men like Jamieson (A. B. Jamieson, Nave’s second recruit) and (Athanasius) Treweek, who had cordial relations with the Americans, remained with FRUMEL throughout the war.\n\nFabian or his deputy John Lietweller were always in the office, 24 hours a day. Fabian was \"a highly professional officer with an air of authority and a hint of Central European sophistication\", although he was born in Butte, Montana, in 1908. But he \"regarded co-operation with anyone who was not in the US Navy or under its command as poor security\". One senior British officer said the atmosphere at FRUMEL was \"What is yours is mine, and what is mine is my own\", and Fabian (backed by Redman) was not interested in any exchange of material with the Army's Central Bureau. Once Fabian burnt a document in front of MacArthur's Intelligence Officer (G-2), Major General Charles A. Willoughby, to demonstrate that only MacArthur himself and Sutherland could be present at FRUMEL briefings and that Willoughby was not allowed to see it.\n\nThe major (naval) Intercept Stations which carried out intercept and D/F (direction finding) but not cryptographic work were:\n\n"}
{"id": "606845", "url": "https://en.wikipedia.org/wiki?curid=606845", "title": "Frosting spatula", "text": "Frosting spatula\n\nA frosting spatula or palette knife is a kitchen utensil designed especially for the use of spreading a substance onto a flat surface, such as frosting on a cake. It is also an ideal tool for applying spreads onto sandwiches in mass quantities.\n\nThe term 'palette knife' is common outside the USA, where the term 'frosting' is not generally used. However a palette knife as a culinary tool is not the same as a palette knife as used by artists. In Canada, the terms metal spatula and leveler are used also.\n\nThe English television cook Delia Smith refers to the joys of owning a \"palette knife with a serrated edge\", such that it provides ease of slicing cake as well as the spreading of icing (frosting) upon them. \n\nThe traditionally accepted British source \"Mrs Beeton's Book of Household Management\" however refers only to the use of a \"broad knife\" for laying on icing (frosting).\n"}
{"id": "25244954", "url": "https://en.wikipedia.org/wiki?curid=25244954", "title": "Fuel Cell Development Information Center", "text": "Fuel Cell Development Information Center\n\nThe Fuel Cell Development Information Center (FCDIC) is a Japanese center established in July 1986 to exchange information among its members on fuel cell research, development and deployment to speed up the introduction and penetration of fuel cells into the market.\n\nThe FCDIC consists of 154 organizations, 40 academic individuals, and 3 foreign members. The organisation publishes the quarterly journal \"The Journal of Fuel Cell Technology\" and the latest fuel cell news in Japan\n\n\n"}
{"id": "1179236", "url": "https://en.wikipedia.org/wiki?curid=1179236", "title": "Gas cylinder", "text": "Gas cylinder\n\nA gas cylinder or tank is a pressure vessel for storage and containment of gases at above atmospheric pressure. High-pressure gas cylinders are also called \"bottles\". Inside the cylinder the stored contents may be in a state of compressed gas, vapor over liquid, supercritical fluid, or dissolved in a substrate material, depending on the physical characteristics of the contents. A typical gas cylinder design is elongated, standing upright on a flattened bottom end, with the valve and fitting at the top for connecting to the receiving apparatus.\n\nIn the United States, \"bottled gas\" typically refers to liquefied petroleum gas. \"Bottled gas\" is sometimes used in medical supply, especially for portable oxygen tanks. Packaged industrial gases are frequently called \"cylinder gas\", though \"bottled gas\" is sometimes used.\n\nThe United Kingdom and other parts of Europe more commonly refer to \"bottled gas\" when discussing any usage whether industrial, medical, or liquefied petroleum. However, in contrast, what the United States calls liquefied petroleum gas is known generically in the United Kingdom as \"LPG\"; and it may be ordered by using one of several trade names, or specifically as butane or propane depending on the required heat output.\n\nFor a detailed discussion about the materials for gas cylinders see pressure vessel.\n\nDesign codes and application standards and the cost of materials dictated the choice of steel with no welds for most gas cylinders, treated to be anticorrosive. Some newly developed lightweight gas cylinders have been made from stainless steel and composite materials. Due to the very high tensile strength of carbon fiber, these vessels can be very light, but are much more difficult to manufacture.\n\nThe transportation of high-pressure cylinders is regulated by many governments throughout the world. Various levels of testing are generally required by the governing authority for the country in which it is to be transported. In the United States, this authority is the United States Department of Transportation (DOT). Similarly in the UK, the European transport regulations (ADR) are implemented by the Department for Transport (DfT). For Canada, this authority is Transport Canada (TC). Cylinders may have additional requirements placed on design and or performance from independent testing agencies such as Underwriter's Laboratory (UL). Each manufacturer of high-pressure cylinders is required to have an independent quality agent that will inspect the product for quality and safety.\n\nWithin the UK the \"competent authority\" — the DfT — implements the regulations and appointment of authorised cylinder testers is conducted by UKAS, who make recommendations to the VCA for approval of individual bodies.\n\nThere are a variety of tests that may be performed on various cylinders. Some of the most common types of tests are hydrostatic test, burst test, tensile strength, Charpy impact test and pressure cycling.\n\nDuring the manufacturing process, vital information is usually stamped or permanently marked on the cylinder. This information usually includes the type of cylinder, the working or service pressure, the serial number, date of manufacture, the manufacture's registered code and sometimes the test pressure. Other information may also be stamped depending on the regulation requirements.\n\nHigh-pressure cylinders that are used multiple times — as most are — can be hydrostatically or ultrasonically tested and visually examined every few years. In the United States, hydrostatic/ultrasonic testing is required either every five years or every ten years, depending on cylinder and its service. Helium gas cylinders have the highest pressures possible when full, around 1000 atmospheres.\n\nGas cylinders have a stop angle valve at the end on top. During storage, transportation, and handling when the gas is not in use, a cap may be screwed over the protruding valve to protect it from damage or breaking off in case the cylinder were to fall over. Instead of a cap, cylinders commonly have a protective collar or neck ring around the service valve assembly.\n\nWhen the gas in the cylinder is to be used at low pressure, the cap is taken off and a pressure-regulating assembly is attached to the stop valve. This attachment typically has a pressure regulator with upstream (inlet) and downstream (outlet) pressure gauges and a further downstream needle valve and outlet connection. For gases that remain gaseous under ambient storage conditions, the upstream pressure gauge can be used to estimate how much gas is left in the cylinder according to pressure. For gases that are liquid under storage, e.g., propane, the outlet pressure is dependent on the vapor pressure of the gas, and does not fall until the cylinder is nearly exhausted although it will vary according to the temperature of the cylinder contents. The regulator is adjusted to control the downstream pressure, which will limit the maximum flow of gas out of the cylinder at the pressure shown by the downstream gauge. The outlet connection is attached to whatever needs the gas supply, such as a balloon for example. For some purposes, such as welding, the regulator will also have a flowmeter on the downstream side.\n\nThe valves on industrial, medical and diving cylinders are usually of different sizes and types, as are the valves for different categories of gases, making it more difficult to mistakenly misuse a gas. For example, a hydrogen cylinder does not fit an oxygen supply line which would end in catastrophic failure. Some fittings use a right-hand thread, while others use a left-hand thread; left-hand thread fittings are usually identifiable by notches or grooves cut into them.\n\nIn the United States, valve connections are sometimes referred to as \"CGA connections\", since the Compressed Gas Association (CGA) publishes guidelines on what connections to use for what products; e.g., in the United States, an argon cylinder will have a CGA 580 connection on the valve.\n\nHigh purity gases will sometimes use CGA-DISS (\"Diameter Index Safety System\") connections.\n\nMedical gases may use the pin-index system to prevent incorrect connection of gases to services.\n\nIn the EU, DIN connections are more common than in the United States.\n\nIn the UK, the British Standards Institution sets the standards. Included among the standards is the use left-hand threaded valves for flammable gas cylinders (most commonly brass, BS4, valves for non corrosive cylinder contents or stainless steel, BS15, valves for corrosive contents). Non flammable gas cylinders are fitted with right-hand threaded valves (most commonly brass, BS3, valves for non corrosive components or stainless steel, BS14, valves for corrosive components).\n\nBecause the contents are under pressure and are sometimes hazardous materials, handling bottled gases are regulated. Regulations may include chaining bottles to prevent falling and damaging the valve, proper ventilation to prevent injury or death in case of leaks and signage to indicate the potential hazards If a compressed gas cylinder tips over, causing the valve block to be sheared off, the rapid release of high-pressure gas may cause the cylinder to be violently accelerated, potentially causing property damage, injury, or death. To prevent this, cylinders are normally secured to a fixed object or transport cart with a strap or chain.\n\nIn a fire, the pressure in a gas cylinder rises in direct proportion to its temperature. If the internal pressure exceeds the mechanical limitations of the cylinder and there are no means to safely vent the pressurized gas to the atmosphere, the vessel will fail mechanically. If the vessel contents are flammable, this event may result in a \"fireball\". Oxidisers such as oxygen and fluorine will produce a similar effect by accelerating combustion in the area affected. If the cylinder's contents are liquid, but become a gas at ambient conditions, this is commonly referred to as a boiling liquid expanding vapour explosion (BLEVE).\n\nMedical gas cylinders in the UK and some other countries have a fusible plug of Wood's metal in the valve block between the valve seat and the cylinder. This plug melts at a comparatively low temperature (70 °C) and allows the contents of the cylinder to escape to the surroundings before the cylinder is significantly weakened by the heat, lessening the risk of explosion.\n\nMore common pressure relief devices are a simple burst disc installed in the base of the valve between the cylinder and the valve seat. A burst disc is a small metal gasket engineered to rupture at a pre-determined pressure. Some burst discs are backed with a low-melting-point metal, so that the valve must be exposed to excessive heat before the burst disc can rupture.\n\nThe Compressed Gas Association publishes a number of booklets and pamphlets on safe handling and use of bottled gases.\n\nThere is a wide range of standards relating to the manufacture, use and testing of pressurised gas cylinders and related components. Some examples are listed here.\n\nGas cylinders are often color-coded, but the codes are not standard across different jurisdictions, and sometimes are not regulated. Cylinder color can not safely be used for positive product identification; cylinders have labels to identify the gas they contain.\n\nIn scuba diving, the United States measures cylinder volume by the amount of free air that can be compressed into the cylinder; Europe and most of the rest of the world measure the cylinder volume as the internal volume of the cylinder: e.g. United States 19 cubic feet = European 3 liter at 180 bar.\n\nThe below are example cylinder sizes and do not constitute an industry standard.\n\n\n"}
{"id": "4049168", "url": "https://en.wikipedia.org/wiki?curid=4049168", "title": "Glass cloth", "text": "Glass cloth\n\nGlass cloth is a textile material, originally developed to be used in greenhouse paneling, allowing sunlight's ultraviolet rays to be filtered out, while still allowing visible light through to plants. The cloth is usually woven with the plain weave, and may be patterned in various ways, though checked cloths are the most common. The original cloth was made from linen, but a large quantity is made with cotton warp and tow weft, and in some cases they are composed entirely of cotton. Short fibres of the cheaper kind are easily detached from the cloth.\n\nDue to properties of glass such as heat resistance and an inability to ignite, glass has been used to create fire barriers in hazardous environments such as inside of racecars. Its poor flexibility, and its being a source of skin irritation, made the fibers inadequate for apparel uses. \n\nDuring the Dust Bowl which were storms of the 1930s in the Southern Plains, states' health officials recommended attaching translucent glass cloth to the inside frames of windows to help in keeping the dust out of buildings, although people also used cardboard, canvas or blankets. Eyewitness accounts indicate they were not completely successful.\n\n"}
{"id": "48384756", "url": "https://en.wikipedia.org/wiki?curid=48384756", "title": "GoTenna", "text": "GoTenna\n\ngoTenna (goTenna Inc.) is a Brooklyn, New York-based startup that designs and develops technologies for off-grid and decentralized communications. goTenna devices pair with smartphones and, through intelligent mobile ad hoc networking protocols, enable users to send texts and share locations on a peer-to-peer basis, foregoing the need for centralized communications infrastructure of any kind.\n\nThe idea for goTenna came to siblings Daniela and Jorge Perdomo in November 2012 after Hurricane Sandy knocked out 25 percent of cell towers, and caused outages for 25 percent of Internet services, across 10 states on the East Coast. Officially incorporated in April 2013, the company’s stated goal is to build “people-powered peer-to-peer communication systems…reducing our reliance on cell towers and wifi routers, and providing anyone the ability to create a network on their terms.”\n\nIn September 2016, goTenna launched goTenna Plus, a, subscription-based upgrade to the goTenna applications, which includes the capability to use other goTenna users as gateways to relay messages through to traditional SMS networks. The company also released its software development kit, enabling developers to create new applications using goTenna hardware. Around the same time, goTenna unveiled a second-generation device: goTenna Mesh, the first consumer-ready mesh network of its kind, available to 49 countries. In March 2017, the company announced its goTenna Pro line, for professional mobile radio communications needs.\n\n"}
{"id": "13646290", "url": "https://en.wikipedia.org/wiki?curid=13646290", "title": "GrIDsure", "text": "GrIDsure\n\nGrIDsure was a personal identification system which extends the standard ‘shared-secret’ authentication model to create a secure methodology whereby a dynamic ‘one-time’ password or PIN can be generated by a user. It was invented by Jonathan Craymer and Stephen Howes in November 2005. It has received positive media reception.\n\nGrIDsure went into liquidation in October 2011 after investor funding dried up.\n\nOn 18 November 2011 Cryptocard announced it has acquired the intellectual property of GrIDsure which includes 8 patents that have been granted and a further 16 pending. Cryptocard was already a GrIDsure OEM partner and uses the product in their portfolio.\n\nIn order to authenticate, the user is asked to input a series of numbers based on a preregistered pattern on a grid (that the user knows) and a grid of pseudo-random numbers generated by the authenticator. This results in a different series of numbers each time the user authenticates.\n\nA study was carried out on the statistical security of GrIDsure by Richard Weber in the Statistical Laboratory of the University of Cambridge. He concluded \"This is one of the most beautiful ideas I have seen in many years of looking at algorithms and optimisation problems.\" \n\nIn March 2008, an independent security researcher, Mike Bond, identified flaws in the Gridsure authentication scheme, specifically commenting on Weber's analysis, and concluded:\n\n\"The Gridsure authentication mechanism remains largely unproven. Studies so far are flawed or taken out of context; my own initial studies indicate further weaknesses.\"\n\nThe introduction to Dr Bond's paper states \"This document is not intended to be a fully representative or balanced appraisal of the scheme.\"\n\nUniversity College London conducted a usability trial.\nIn a covering letter to the study report, Professor Sasse states:\n\n\"Having looked at many mechanisms which have been proposed in recent years to overcome users' problems with PINs and passwords, this is the first one that has the potential to offer good usability and increased security at the same time.\"\n\n"}
{"id": "47834320", "url": "https://en.wikipedia.org/wiki?curid=47834320", "title": "Green Petrol SEI", "text": "Green Petrol SEI\n\nGreen Petrol, (GPSEI) is one of the Iranian largest providers of fuel retailing solutions.\nIt's a supplier of fuel dispensers, point of sale systems, Station Equipment, and Fuel Tanker Level Gauge systems and support services. The company's headquarters is in Tehran, IRAN with sales, manufacturing, research, development and service locations in different parts of Iran.\n\nThe company was founded under the name Green Petrol in 2005 by Javad Hajibeygi and Masoud Tajrishi.In 2006 Green Petrol started to cooperate with NPS co.\n\n\n"}
{"id": "40740455", "url": "https://en.wikipedia.org/wiki?curid=40740455", "title": "Gregory Charvat", "text": "Gregory Charvat\n\nGregory L. Charvat is author of \"Small and Short-Range Radar Systems\", Co-Founder of Butterfly Network Inc, and advisor to the Camera Culture Group at Massachusetts Institute of Technology MIT Media Lab.\n\nCharvat is best known for his through-wall radar imaging system and his project-based MIT short-course on radar, where each student builds their own radar system. This radar course has been adopted by numerous other universities and institutions. Charvat is also well known in the hacker and maker community for developing radar devices and imaging systems in his garage.\n\nCharvat grew up in the metro Detroit area, where he would take apart old televisions & radios. He built amateur radio equipment in high school, a radio telescope for which he won second place at the 1997 International Science and Engineering Fair in Louisville, KY, and developed many radar sensors in college. He earned PhD (2007), MSc (2003), and BSc (2002) degrees in electrical engineering from Michigan State University. He was a member of the technical staff at MIT Lincoln Laboratory from Sept 2007 to Nov 2011, and has taught short radar courses at MIT where his ‘Build a Small Radar Sensor...’ course was top-ranked MIT Professional Education course in 2011.\n\nCharvat has authored or co-authored numerous journals, proceedings, magazine articles, and seminars on topics including applied electromagnetics, synthetic aperture radar (SAR), and phased array radar systems, radio frequency (RF) and analog design. He has developed numerous rail SAR imaging sensors, phased array radar systems, impulse radar systems and other radar sensors, and as well has designed his own amateur radio station. Charvat won best 2010 paper at the Military Sensing Symposia (MSS) Tri-Services Radar Symposium for his work on through wall radar. For fun he develops vacuum tube audio equipment and restores antique radios and watches, among hobbies.\n\n\nRecently, Gregory Charvat provided explanations of advanced sensing technologies that the general public could understand during a series of interviews on the missing Malaysian Flight 370 in March 2014:\n\nhttp://www.cbsnews.com/videos/flight-370-search-using-state-of-the-art-sonar-and-radar-tech/\n\nhttp://www.cnn.com/video/data/2.0/video/bestoftv/2014/03/21/pmt-bill-weir-malaysia-airlines-missing-flight-370.cnn.html\n\nhttp://edition.cnn.com/video/data/2.0/video/bestoftv/2014/03/18/pmt-greg-charvat.cnn.html\n\nSky News Television on Sunday morning 3/23/14 (afternoon in UK).\n\nMalaysia's local NPR-style radio station Business FM 89.9, on 3/24/14.\n\nNewstalk1010 Moore in the Morning with John Moore (Toronto Canada) on 3/24/14\n\nThe Arlene Bynon Show on SiriusXM Canada on 3/18/14.\n\nGregory Charvat is also a contributing author to Hack-a-Day blog, writing on the subject of using small radar technology for your next project\nhttp://hackaday.com/2014/02/24/guest-post-try-radar-for-your-next-project/\n\nand how Synthetic Aperture Radar imaging works:\nhttp://hackaday.com/2014/03/17/radar-imaging-in-your-garage-synthetic-aperture-radar/\n\nGregory L. Charvat is a visiting research scientist at MIT Media Lab.\n\nCharvat is the Series Editor of the, \"Modern and Practical Approaches to Electrical Engineering,\" book series. Author Albert Sabban published the most recent part in the series, \"Low-Visibility Antennas for Communication Systems,\" on September 18, 2015.\n\nLow-Visibility Antennas for Communication Systems is currently available for purchase here:\n\nhttps://www.crcpress.com/Low-Visibility-Antennas-for-Communication-Systems/Sabban/9781482246438\n\n9. Charvat, Gregory.(2015-10-23).\"Time-of-Flight Microwave Camera\" Scientific Reports 5, Article number: 14709 (2015). Retrieved 2015-11-23.\n\n10. Venkratraman, Vijee. (2015-10-06). \"A camera than can see through walls\" The Boston Globe. MIT Media Lab. Retrieved 2015-11-23.\n\n11. Ackerman, Evan. (2015-10-14). \"MIT's 3-D Microwave Camera Can See Through Walls\" Spectrum.ieee.org. Tech Talk. Retrieved 2015-11-23.\n\n12. Muoio, Danielle. (2015-10-15). \"This camera can see through walls and could help driverless cars navigate fog\" Techinside.io. Tech Insider. Retrieved 2015-11-23.\n\n13. Sorrel, Charlie. (2015-10-23). \"MIT's New Microwave Camera Can See Through Walls\" fastcoexist.com. Exist. Retrieved 2015-11-23.\n\n14. Sabban, Albert. (2015-9-18). \"Low-Visibility Antennas for Communication Systems.\" crcpress.com. CRC Net Base. Retrieved 2015-11-23.\n\n\nV. Venkatraman, “A camera that can see through walls.” Beta Boston, the Boston Globe, October 6, 2015. Retrieved 2015-11-23.\n\nE. Ackerman, “MIT’s 3-D Microwave Camera Can See Through Walls.” IEEE Spectrum, October 2015. Retrieved 2015-11-23.\n\nD. Muoio, “This camera can see through walls and could help driverless cars navigate fog.” Tech Insider, October 15, 2015. Retrieved 2015-11-23.\n\nC. Sorrel, “MIT’s new microwave camera can see through walls.” Co.Exist, Fast Company, October 23, 2015. Retrieved 2015-11-23.\n"}
{"id": "7126735", "url": "https://en.wikipedia.org/wiki?curid=7126735", "title": "Heater core", "text": "Heater core\n\nA heater core is a radiator-like device used in heating the cabin of a vehicle. Hot coolant from the vehicle's engine is passed through a winding tube of the core, a heat exchanger between coolant and cabin air. Fins attached to the core tubes serve to increase surface for heat transfer to air that is forced past them, by a fan, thereby heating the passenger compartment.\n\nThe internal combustion engine in most cars and trucks is cooled by a water and antifreeze mixture that is circulated through the engine and radiator by a water pump to enable the radiator to give off engine heat to the atmosphere. Some of that water can be diverted through the heater core to give some engine heat to the cabin, or adjust the temperature of the conditioned air.\n\nA heater core is a small radiator located under the dashboard of the vehicle, and it consists of conductive aluminium or brass tubing with cooling fins to increase surface area. Hot coolant passing through the heater core gives off heat before returning to the engine cooling circuit.\n\nThe squirrel cage fan of the vehicle's ventilation system forces air through the heater core to transfer heat from the coolant to the cabin air, which is directed into the vehicle through vents at various points.\n\nOnce the engine has warmed up, the coolant is kept at a more or less constant temperature by the thermostat. The temperature of the air entering the vehicle's interior can be controlled by using a valve limiting the amount of coolant that goes through the heater core. Another method is blocking off the heater core with a door, directing part (or all) of the incoming air around the heater core completely, so it does not get heated (or re-heated if the air conditioning compressor is active). Some cars use a combination of these systems.\n\nSimpler systems allow the driver to control the valve or door directly (usually by means of a rotary knob, or a lever). More complicated systems use a combination of electromechanical actuators and thermistors to control the valve or doors to deliver air at a precise temperature value selected by the user.\n\nCars with dual climate function (allowing driver and passenger to each set a different temperature) may use a heater core split in two, where different amounts of coolant flow through the heater core on either side to obtain the desired heating.\n\nIn a car equipped with air conditioning, outside air, or cabin air if the recirculation flap has been set to close the external air passages, is first forced, often after being filtered by a cabin air filter, through the air conditioner's evaporator coil. This can be thought of as a heater core filled with very cold liquid that is undergoing a phase change to gas (the evaporation), a process which cools rather than heats the incoming air. In order to obtain the desired temperature incoming air may first be cooled by the air conditioning and then heated again by the heater core. In a vehicle fitted with manual controls for the heater and air conditioning compressor, using both systems together will dehumidify the air in the cabin, as the evaporator coil removes moisture from the air due to condensation. This can result in increased air comfort levels inside the vehicle. Automatic temperature control systems can take the best course of action in regulating the compressor operation, amount of reheating and blower speed depending upon the external air temperature, the internal one and the cabin air temperature value or a rapid defrost effect requested by the user.\n\nBecause the heater core cools the heated coolant from the engine by transferring its heat to the cabin air, it can also act as an auxiliary radiator for the engine. If the radiator is working improperly, the operator may turn the heat on (together with the cabin blower fan placed on full speed, and with the windows opened) in the passenger cabin, resulting in a certain cooling effect on the overheated engine coolant. This idea only works to a certain degree, as the heater core is not large enough nor does it have enough cold air going through it to cool large amounts of coolant significantly.\n\nThe heater core is made up of small piping that has numerous bends. Clogging of the piping may occur if the coolant system is not flushed or if the coolant is not changed regularly. If clogging occurs the heater core will not work properly. If coolant flow is restricted, heating capacity will be reduced or even lost altogether if the heater core becomes blocked. Control valves may also clog or get stuck. Where a blend door is used instead of a control valve as a method of controlling the air's heating amount, the door itself or its control mechanism can become stuck due to thermal expansion. If the climate control unit is automatic, actuators can also fail.\n\nAnother possible problem is a leak in one of the connections to the heater core. This may first be noticeable by smell (ethylene glycol is widely used as coolant and has a sweet smell); it may also cause (somewhat greasy) fogging of the windshield above the windshield heater vent. Glycol may also leak directly into the car, causing wet upholstery or carpeting.\n\nElectrolysis can cause excessive corrosion leading to the heater core rupturing. Coolant will spray directly into the passenger compartment followed with white colored smoke, a significant driving hazard.\n\nBecause the heater core is usually located under the dashboard inside of the vehicle and is enclosed in the ventilation system's ducting, servicing it often requires disassembling a large part of the dashboard, which can be labour-intensive and therefore expensive.\n\nSince the heater core relies on the coolant's heat to warm the cabin air up, it obviously won't begin working until the engine's coolant warms up enough. This problem can be resolved by equipping the vehicle with an auxiliary heating system, which can either use electricity or burn the vehicle's fuel in order to rapidly bring the engine's coolant to operating temperatures.\n\nEngines that do not have a water cooling system cannot heat the cabin via a heater core; one alternative is to guide air around the (very hot) engine exhaust manifold and then into the vehicle's interior. Temperature control is achieved by mixing with unheated outside air. Air-cooled Volkswagen engines use this method. Depending on the design, this can cause a safety issue where a leak in the exhaust system will begin to fill the passenger cabin with deadly fumes.\n\nCar heat cores are also used for D.I.Y. projects, such as for cooling homemade liquid cooling systems for computers.\n\n\n"}
{"id": "34754752", "url": "https://en.wikipedia.org/wiki?curid=34754752", "title": "Hotel Hell", "text": "Hotel Hell\n\nHotel Hell is an American reality television series created, hosted and narrated by Gordon Ramsay, which ran on the Fox network for three seasons from 2012 to 2016. It aired on Monday nights at 8 pm ET/PT. It was Ramsay's fourth series for the Fox network.\n\nThe series features Ramsay visiting various struggling lodging establishments throughout the United States in an attempt to reverse their misfortunes, following a similar concept established in Ramsay's other programs, \"Ramsay's Kitchen Nightmares\" and its American counterpart, \"Kitchen Nightmares\".\n\nOriginally scheduled to premiere on Fox at 8 pm ET/PT on Friday, April 6, 2012, the series was first rescheduled to Monday, June 4, 2012 at 8 pm ET/PT, in order to accommodate the move of \"The Finder\" to Fridays, then rescheduled to August 13, due to Ramsay's other two series, \"Hell's Kitchen\" and \"MasterChef\", being scheduled for Monday nights during the summer.\n\nThe series' first season, which consisted of six episodes, ended on September 3, 2012. On August 31, 2012, Fox renewed \"Hotel Hell\" for a second season, which premiered on July 21, 2014.\n\n\nGeneral references\n"}
{"id": "47190810", "url": "https://en.wikipedia.org/wiki?curid=47190810", "title": "Human Longevity", "text": "Human Longevity\n\nHuman Longevity is a San Diego-based venture launched by Craig Venter and Peter Diamandis in 2013. Its goal is to build the world's most comprehensive database on human genotypes and phenotypes, and then subject it to machine learning so that it can help develop new ways to fight diseases associated with aging. The company received in investments in its Series A offering in summer 2014 and announced a further $220 million Series B investment offering in April 2016. It has made deals with drug companies Celgene and AstraZeneca to collaborate in its research.\n\nWhile it is conducting research, the company is offering a wellness service known as \"Health Nucleus,\" which offers customers a range of medical tests such as a full genome sequencing and tests for early indications of cancers, Alzheimer's and heart disease. This testing is meant to help people catch diseases earlier than otherwise possible and to identify risk factors for diseases later in life. \n\nAt the start of 2017 the company hired Cynthia Collins from GE Healthcare, and Venter became Executive Chair. The company's chief operating office, Mark Winham, left the company in mid-2017, and Collins and the company's chief medical officer, Brad Perkins, left in December. Venter stepped back into the CEO role, but announced in May 2018 that he was leaving the company to return to the J. Craig Venter Institute.\n\n"}
{"id": "19252063", "url": "https://en.wikipedia.org/wiki?curid=19252063", "title": "Integrated flowthrough", "text": "Integrated flowthrough\n\nIntegrated flowthrough, in telecommunications, refers to the disciplines and techniques by which a Communications Service Provider (CSP) may achieve and maintain high levels of automation, efficiency and accuracy in operational processes such as fulfilment and assurance.\n\nTypically, this involves optimization and integration of various Operations Support Systems (OSS) or Business Support Systems (BSS) as well as structuring and optimizing the activities of personnel involved in handling order fallout, with the goal of both reducing the number of orders that “fallout” and require manual handling and reducing the time and effort required to handle orders that do fallout.\n\nThe growth in the order management market is due to CSPs having higher expectations about guaranteed order completion times, thus making issues such as the management of order fallout a priority for CSPs. (Order fallout occurs when the progression of an order has been or rejected, resulting in the order not being successfully completed.)\n\nWhile many CSPs have spent decades on continuous improvement to achieve high levels of order flowthrough for traditional services, many find that converged, triple-play or quad-play operations environments require renewed focus on this discipline due to the increased complexity of orders in this environment.\n\nIntegrated Flowthrough employs a methodology that blends the service design, network technologies, OSSs, fulfillment and assurance processes, enabling CSPs to examine assurance and fulfillment operations from every dimension, including leadership, people processes, technology and systems/data.\n\nIntegrated Flowthrough can help CSPs realize the following benefits:\n\n\n"}
{"id": "39800443", "url": "https://en.wikipedia.org/wiki?curid=39800443", "title": "Intermittent control", "text": "Intermittent control\n\nIntermittent control is a feedback control method which not only explains some human control systems but also has applications to control engineering.\n\nIn the context of control theory, intermittent control provides a spectrum of possibilities between the two extremes of continuous-time and discrete-time control: the control signal consists of a sequence of (continuous-time) parameterised trajectories whose parameters are adjusted intermittently. It is different from discrete-time control in that the control is not constant between samples; it is different from continuous-time control in that the trajectories are reset intermittently. As a class of control theory, intermittent predictive control is more general than continuous control and provides a new paradigm incorporating continuous predictive and optimal control with intermittent, open loop (ballistic) control.\n\nThere are at least three areas where intermittent control is relevant. Firstly, continuous-time model-based predictive control where the intermittency is associated with on-line optimisation. Secondly, event-driven control systems where the intersample interval is time varying and determined by the event times. Thirdly, explanation of physiological control systems which, in some cases, have an intermittent character. This intermittency may be due to the “computation” in the central nervous system.\n\nConventional sampled-data control uses a zero-order hold, which produces a piecewise-constant control signal and can be used to give a\nsampled-data implementation which approximates previously-designed continuous-time controller. In contrast to conventional sampled data control, intermittent control explicitly embeds the underlying continuous-time closed-loop system in a \"system-matched\" hold which generates an open-loop intersample control trajectory based on the underlying continuous-time closed-loop control system.\n\nIntermittent control initially evolved separately in the engineering and physiological literature.\n\nThe concept of \"intermittent control\" appeared in a posthumous paper by Kenneth Craik which states “The human operator behaves basically as an intermittent correction servo”. A colleague of Kenneth Craik, Margaret Vince, related the concept of intermittency to the Psychological refractory period and provided experimental verification of intermittency. Fernando Navas and James Stark showed experimentally that human hand movements were synchronised to input signals rather than to an internal clock: in other words the hand control system is event-driven not clock-driven. The first detailed mathematical model of intermittency was presented by Peter Neilson, Megan Neilson, and Nicholas O’Dwyer.\nA more recent mathematical model of intermittency is given by PeterGawthrop, Ian Loram, Martin Lakie and Henrik Gollee.\n\nIn the context of Control Engineering, the term intermittent control was used by Eric Ronco, Taner Arsan and Peter Gawthrop.\nThey stated that “A conceptual, and practical difficulty with the continuous-time generalised predictive controller is solved by replacing the continuously moving horizon by an intermittently moving horizon. This allows slow optimisation to occur concurrently with a fast control action.” The concept of intermittent model predictive control was refined by Peter Gawthrop working with Liuping Wang, who also looked at event-driven intermittent control.\n\nIn a separate line of development Tomas Estrada, Hai Lin and Panos Antsaklis developed the concept of model-based control with intermittent feedback in the context of a networked control system.\n"}
{"id": "469799", "url": "https://en.wikipedia.org/wiki?curid=469799", "title": "Land rehabilitation", "text": "Land rehabilitation\n\nLand rehabilitation is the process of returning the land in a given area to some degree of its former state, after some process (industry, natural disasters, etc.) has resulted in its damage. Many projects and developments will result in the land becoming degraded, for example mining, farming and forestry.\n\nModern mine rehabilitation aims to minimize and mitigate the environmental effects of modern mining, which may in the case of open pit mining involve movement of significant volumes of rock. Rehabilitation management is an ongoing process, often resulting in open pit mines being backfilled.\n\nAfter mining finishes, the mine area must undergo rehabilitation. \n\nFor underground mines, rehabilitation is not always a significant problem or cost. This is because of the higher grade of the ore and lower volumes of waste rock and tailings. In some situations, stopes are backfilled with concrete slurry using waste, so that minimal waste is left at surface.\n\nThe removal of plant and infrastructure is not always part of a rehabilitation programme, as many old mine plants have cultural heritage and cultural value. Often in gold mines, rehabilitation is performed by scavenger operations which treat the soil within the plant area for spilled gold using modified placer mining gravity collection plants.\n\nAlso possible is that the section of the mine that is below ground, is kept and used to provide heating, water and/or methane. Heat extraction can be done using heat exchangers, that convey the heat to a nearby city (hence making it be used for district heating purposes.\nWater can be harvested from the mine as well (mines are often filled with water once the mine has been shut down and the pumps no longer operate). Methane is also often present in the mine shafts, in small quantities (often around 0,1%). This can still be recovered though with specialised systems. An added advantage of recovering the methane finally is that the methane does not come into the atmosphere, and so does not contribute to global warming.\n\n\n"}
{"id": "53950069", "url": "https://en.wikipedia.org/wiki?curid=53950069", "title": "Level shifter", "text": "Level shifter\n\nA level shifter in digital electronics, also called a logic-level shifter, is a circuit used to translate signals from one logic level or voltage domain to another, allowing compatibility between ICs with different voltage requirements, such as TTL and CMOS. Many modern full featured systems use level shifters to bridge domains between low-power application processors running at 1.8 V and other system functions like sensors or other analog intensive applications running at 3.3 or 5V.\n\nUni-directional – All input pins are dedicated to one voltage domain, all output pins are dedicated to the other.\nBi-directional with Dedicated Ports – Each voltage domain has both input and output pins, but the data direction of a pin does not change.\nBi-directional with External direction indicator – When an external signal is changed, inputs become outputs and vice versa.\nBi-directional, auto-sensing – A pair of I/O spanning voltage domains can act as either inputs or outputs depending on external stimulus without the need for a dedicated direction control pin.\n\nFixed Function Level Shifter ICs - these ICs provide several different types of level shift in fixed function devices. Often lumped into 2-bit, 4-bit, or 8-bit level shift configurations offered with various VDD and VDD2 ranges, these devices translate logic levels without any additional integrated logic or timing adjustment.\nConfigurable Mixed-signal ICs (CMICs) – Level shifter circuitry can also be implemented in a CMIC. The no-code programmable nature of CMICs allows designers to implement fully customizable level shifters with the added option to integrate configurable logic or timing adjustments in the same device.\n\nSince level shifters are used to resolve the voltage incompatibility between various parts of a system, they have a wide range of applications as well. Level shifters are widely used in interfacing legacy devices and also in SD cards, SIM cards, CF cards, Audio codecs and UARTs. \n\n"}
{"id": "56212", "url": "https://en.wikipedia.org/wiki?curid=56212", "title": "Linen", "text": "Linen\n\nLinen is a textile made from the fibers of the flax plant. Linen is laborious to manufacture, but the fiber is very strong, absorbent and dries faster than cotton. Garments made of linen are valued for their exceptional coolness and freshness in hot and humid weather. \n\nThe word \"linen\" is of West Germanic origin and cognate to the Latin name for the flax plant, \"linum\", and the earlier Greek λινόν (\"linón\"). This word history has given rise to a number of other terms in English, most notably \"line,\" from the use of a linen (flax) thread to determine a straight line. Many products are made of linen: aprons, bags, towels (swimming, bath, beach, body and wash towels), napkins, bed linens, tablecloths, runners, chair covers, and men's and women's wear.\n\nThe collective term \"linens\" is still often used generically to describe a class of woven or knitted bed, bath, table and kitchen textiles traditionally made of linen. In the past, \"linens\" also referred to lightweight undergarments such as shirts, chemises, waist-shirts, lingerie (a word also cognate with \"linen\"), and detachable shirt collars and cuffs, all of which were historically made almost exclusively out of linen. The inner layer of fine composite cloth garments (as for example jackets) was traditionally made of linen, hence the word \"lining\".\n\nTextiles in a linen weave texture, even when made of cotton, hemp and other non-flax fibers, are also loosely referred to as \"linen\". Such fabrics generally also have their own specific names, for example fine cotton yarn in a linen-style weave is called Madapolam.\n\nLinen textiles appear to be some of the oldest in the world: their history goes back many thousands of years. Fragments of straw, seeds, fibers, yarns, and various types of fabrics dating to about 8000 BC have been found in Swiss lake dwellings. Dyed flax fibers found in a prehistoric cave in Georgia suggest the use of woven linen fabrics from wild flax may date back even earlier to 36,000 BP.\n\nLinen was sometimes used as a form of currency in ancient Egypt. Egyptian mummies were wrapped in linen as a symbol of light and purity, and as a display of wealth. Some of these fabrics, woven from hand-spun yarns, were very fine for their day, but are coarse compared to modern linen. In 1923 the German city Bielefeld issued banknotes printed on linen.\nToday, linen is usually an expensive textile produced in relatively small quantities. It has a long staple (individual fiber length) relative to cotton and other natural fibers.\n\nThe word \"linen\" is derived from the Latin for the flax plant, which is \"linum\", and the earlier Greek \"λίνον\" (\"linon\"). This word history has given rise to a number of other terms:\n\n\nThe discovery of dyed flax fibers in a cave in Georgia dated to thirty-six thousand years ago suggests that ancient people used wild flax fibers to create linen-like fabrics from an early date.\n\nIn ancient Mesopotamia, flax was domesticated and linen was first produced. It was used mainly by the wealthier class of the society, including priests. The Sumerian poem of the courtship of Inanna and Dumuzi (Tammuz), translated by Samuel Noah Kramer and Diane Wolkstein and published in 1983, mentions flax and linen. It opens with briefly listing the steps of preparing linen from flax, in a form of questions and answers between Inanna and her brother Utu.\n\nIn ancient Egypt, linen was used for mummification and for burial shrouds. It was also worn as clothing on a daily basis; white linen was worn because of the extreme heat. \nThe use of linen for priestly vestments was not confined to the Israelites; Plutarch wrote that the priests of Isis also wore linen because of its purity.\n\nLinen fabric has been used for table coverings, bed coverings and clothing for centuries. The significant cost of linen derives not only from the difficulty of working with the thread, but also because the flax plant itself requires a great deal of attention. In addition flax thread is not elastic, and therefore it is difficult to weave without breaking threads. Thus linen is considerably more expensive to manufacture than cotton.\n\nThere is a long history of the production of linen in Ireland. The Living Linen Project was set up in 1995 as an oral archive of the knowledge of the Irish linen industry, which was at that time still available within a nucleus of people who formerly worked in the industry in Ulster.\n\nIn December 2006, the General Assembly of the United Nations proclaimed 2009 to be the International Year of Natural Fibres in order to raise people's awareness of linen and other natural fibers.\n\nWhen the tomb of the Pharaoh Ramses II, who died in 1213 BC, was discovered in 1881, the linen wrappings were in a state of perfect preservation after more than 3000 years. When the tomb of Tutankhamen was opened, the linen curtains were found to be intact.\n\nIn the Ulster Museum, Belfast there is the mummy of 'Takabuti' the daughter of a priest of Amun, who died 2,500 years ago. The linen on this mummy is also in a perfect state of preservation.\n\nThe earliest records of an established linen industry are 4,000 years old, from Egypt, The earliest written documentation of a linen industry comes from the Linear B tablets of Pylos, Greece, where linen is depicted as an ideogram and also written as \"li-no\" (Greek: λίνον, \"linon\"), and the female linen workers are cataloged as \"li-ne-ya\" (λίνεια, \"lineia\").\n\nThe Phoenicians, who, with their merchant fleet, opened up new channels of commerce to the peoples of the Mediterranean, and developed the tin mines of Cornwall, introduced flax growing and the making of linen into Ireland before the common era. It is not until the twelfth century that we can find records of a definite attempt to systematize flax production.\n\nWhen the Edict of Nantes was revoked, in 1685, many of the Huguenots who fled France settled in the British Isles, and amongst them was Louis Crommelin, who settled in the town of Lisburn, about ten miles from Belfast. Belfast itself is perhaps the most famous linen producing center throughout history; during the Victorian era the majority of the world's linen was produced in the city which gained it the name Linenopolis.\n\nAlthough the linen industry was already established in Ulster, Louis Crommelin found scope for improvement in weaving, and his efforts were so successful that he was appointed by the Government to develop the industry over a much wider range than the small confines of Lisburn and its surroundings. The direct result of his good work was the establishment, under statute, of the Board of Trustees of the Linen Manufacturers of Ireland in the year 1711. Several grades were produced from the coarsest lockram to the finest sasheen.\n\nIn Judaism, the only law concerning which fabrics may be interwoven together in clothing concerns the mixture of linen and wool, called \"shaatnez\"; it is restricted in \"Thou shalt not wear a mingled stuff, wool and linen together\" and , \"'...neither shall there come upon thee a garment of two kinds of stuff mingled together.'\" There is no explanation for this in the Torah itself and is categorized as a type of law known as \"chukim\", a statute beyond man's ability to comprehend. Josephus suggested that the reason for the prohibition was to keep the laity from wearing the official garb of the priests, while Maimonides thought that the reason was because heathen priests wore such mixed garments. Others explain that it is because God often forbids mixtures of disparate kinds, not designed by God to be compatible in a certain way, with mixing animal and vegetable fibers being similar to having two different types of plowing animals yoked together. And that such commands serve both a practical as well as allegorical purpose, perhaps here preventing a priestly garment that would cause discomfort (or excessive sweat) in a hot climate. \n\nLinen is also mentioned in the Bible in Proverbs 31, a passage describing a noble wife. says, \"She makes coverings for her bed; she is clothed in fine linen and purple.\" Fine white linen is also worn by angels in the Bible ().\n\nLinen is a bast fiber. Flax fibers vary in length from about 25 to 150 mm (1 to 6 in) and average 12–16 micrometers in diameter. There are two varieties: shorter tow fibers used for coarser fabrics and longer line fibers used for finer fabrics. Flax fibers can usually be identified by their “nodes” which add to the flexibility and texture of the fabric.\n\nThe cross-section of the linen fiber is made up of irregular polygonal shapes which contribute to the coarse texture of the fabric.\n\nLinen fabric feels cool to touch, a phenomenon which indicates its higher conductivity (the same principle that makes metals feel \"cold\"). It is smooth, making the finished fabric lint-free, and gets softer the more it is washed. However, constant creasing in the same place in sharp folds will tend to break the linen threads. This wear can show up in collars, hems, and any area that is iron creased during laundering. Linen has poor elasticity and does not spring back readily, explaining why it wrinkles so easily.\n\nLinen fabrics have a high natural luster; their natural color ranges between shades of ivory, ecru, tan, or grey. Pure white linen is created by heavy bleaching. Linen fabric typically varies somewhat in thickness and is crisp and textured, but it can in some cases feel stiff and rough, and in other cases feel soft and smooth. When properly prepared, linen fabric has the ability to absorb and lose water rapidly. Linen can absorb a fair amount of moisture without feeling unpleasantly damp to the skin, unlike cotton.\nLinen is a very durable, strong fabric, and one of the few that are stronger wet than dry. The fibers do not stretch, and are resistant to damage from abrasion. However, because linen fibers have a very low elasticity, the fabric eventually breaks if it is folded and ironed at the same place repeatedly over time.\n\nMildew, perspiration, and bleach can also damage the fabric, but it is resistant to moths and carpet beetles. Linen is relatively easy to take care of, since it resists dirt and stains, has no lint or pilling tendency, and can be dry-cleaned, machine-washed or steamed. It can withstand high temperatures, and has only moderate initial shrinkage.\n\nLinen should not be dried too much by tumble drying, and it is much easier to iron when damp. Linen wrinkles very easily, and thus some more formal garments require ironing often, in order to maintain perfect smoothness. Nevertheless, the tendency to wrinkle is often considered part of linen's particular \"charm\", and many modern linen garments are designed to be air-dried on a good clothes hanger and worn without the necessity of ironing.\n\nA characteristic often associated with linen yarn is the presence of \"slubs\", or small knots which occur randomly along its length. In the past, slubs were traditionally considered to be defects, and were associated with low quality linen. However, in the case of many present-day linen fabrics, particularly in the decorative furnishing industry, slubs are considered as part of the aesthetic appeal of an expensive natural product. In addition, slubs do not compromise the integrity of the fabric, and therefore they are not viewed as a defect. However, the very finest linen has very consistent diameter threads, with no slubs at all.\n\nThe standard measure of bulk linen yarn is the \"lea\", which is the number of yards in a pound of linen divided by 300. For example, a yarn having a size of 1 lea will give 300 yards per pound. The fine yarns used in handkerchiefs, etc. might be 40 lea, and give 40x300 = 12,000 yards per pound. This is a specific length therefore an indirect measurement of the fineness of the linen, i.e., the number of length units per unit mass. The symbol is NeL.(3) The metric unit, Nm, is more commonly used in continental Europe. This is the number of 1,000 m lengths per kilogram. In China, the English Cotton system unit, NeC, is common. This is the number of 840 yard lengths in a pound.\n\nThe quality of the finished linen product is often dependent upon growing conditions and harvesting techniques. To generate the longest possible fibers, flax is either hand-harvested by pulling up the entire plant or stalks are cut very close to the root. After harvesting, the plants are dried, and the seeds are removed through a mechanized process called “rippling” (threshing) and winnowing.\n\nThe fibers must then be loosened from the stalk. This is achieved through retting. This is a process which uses bacteria to decompose the pectin that binds the fibers together. Natural retting methods take place in tanks and pools, or directly in the fields. There are also chemical retting methods; these are faster, but are typically more harmful to the environment and to the fibers themselves.\n\nAfter retting, the stalks are ready for scutching, which takes place between August and December. Scutching removes the woody portion of the stalks by crushing them between two metal rollers, so that the parts of the stalk can be separated. The fibers are removed and the other parts such as linseed, shive, and tow are set aside for other uses. Next the fibers are heckled: the short fibers are separated with heckling combs by 'combing' them away, to leave behind only the long, soft flax fibers.\n\nAfter the fibers have been separated and processed, they are typically spun into yarns and woven or knit into linen textiles. These textiles can then be bleached, dyed, printed on, or finished with a number of treatments or coatings.\n\nAn alternate production method is known as “cottonizing” which is quicker and requires less equipment. The flax stalks are processed using traditional cotton machinery; however, the finished fibers often lose the characteristic linen look.\n\nFlax is grown in many parts of the world, but top quality flax is primarily grown in Western European countries and Ukraine. In recent years bulk linen production has moved to Eastern Europe and China, but high quality fabrics are still confined to niche producers in Ireland, Italy and Belgium, and also in countries including Poland, Austria, France, Germany, Sweden, Denmark, Belarus, Lithuania, Latvia, the Netherlands, Spain, Switzerland, Britain and Kochi in India. High quality linen fabrics are now produced in the United States for the upholstery market and in Belgium. Russia is currently the major flax cultivating nation.\n\nOver the past 30 years the end use for linen has changed dramatically. Approximately 70% of linen production in the 1990s was for apparel textiles, whereas in the 1970s only about 5% was used for fashion fabrics.\n\nLinen uses range across bed and bath fabrics (tablecloths, bath towels, dish towels, bed sheets); home and commercial furnishing items (wallpaper/wall coverings, upholstery, window treatments); apparel items (suits, dresses, skirts, shirts); and industrial products (luggage, canvases, sewing thread). It was once the preferred yarn for handsewing the uppers of moccasin-style shoes (loafers), but has been replaced by synthetics.\n\nA linen handkerchief, pressed and folded to display the corners, was a standard decoration of a well-dressed man's suit during most of the first part of the 20th century.\n\nCurrently researchers are working on a cotton/flax blend to create new yarns which will improve the feel of denim during hot and humid weather.\n\nLinen fabric is one of the preferred traditional supports for oil painting. In the United States cotton is popularly used instead, as linen is many times more expensive there, restricting its use to professional painters. In Europe, however, linen is usually the only fabric support available in art shops; in the UK both are freely available with cotton being cheaper. Linen is preferred to cotton for its strength, durability and archival integrity.\n\nLinen is also used extensively by artisan bakers. Known as a couche, the flax cloth is used to hold the dough into shape while in the final rise, just before baking. The couche is heavily dusted with flour which is rubbed into the pores of the fabric. Then the shaped dough is placed on the couche. The floured couche makes a \"non stick\" surface to hold the dough. Then ridges are formed in the couche to keep the dough from spreading.\n\nIn the past, linen was also used for books (the only surviving example of which is the Liber Linteus). Due to its strength, in the Middle Ages linen was used for shields, gambesons, and bowstrings; in classical antiquity it was used to make a type of body armour, referred to as a linothorax.\n\nBecause of its strength when wet, Irish linen is a very popular wrap of pool/billiard cues, due to its absorption of sweat from hands. Paper made of linen can be very strong and crisp, which is why the United States and many other countries print their currency on paper made from 25% linen and 75% cotton. \n\n"}
{"id": "17873133", "url": "https://en.wikipedia.org/wiki?curid=17873133", "title": "List of devices with assisted GPS", "text": "List of devices with assisted GPS\n\nThis is an incomplete list of devices which contain Assisted GPS.\n\n\nblackberry Z3\n\n\n\n\n\n\n\n\n\n\nHuawei honor 4c\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "12896417", "url": "https://en.wikipedia.org/wiki?curid=12896417", "title": "Material take off", "text": "Material take off\n\nMaterial take off (MTO) is a term used in engineering and construction, and refers to a list of materials with quantities and types (such as specific grades of steel) that are required to build a designed structure or item. This list is generated by analysis of a blueprint or other design document. The list of required materials for construction is sometimes referred to as the material take off list (MTOL).\n\nMaterial take off is not limited to the amount of required material, but also the weight of the items taken off. This is important when dealing with larger structures, allowing the company that does the take off to determine total weight of the item and how best to move the item (if necessary) when construction is completed.\n\nA material take off (MTO) is the process of analyzing the drawings and determining all the materials required to accomplish the design. We then use the material take off to create a bill of materials (BOM). Inspection does not aid in creating a bill of materials. Procurement and requisition are activities that occur after the bill of materials is complete.\n\nThe final stages of the MTO is as instrumentally visible in the GAD (General Arrangement Drawing) for specific equipment. The MTO sheet is such an important document in projects as it presents a huge detail such as list of all Materials, quantities, weights, material types, material codes etc."}
{"id": "4916038", "url": "https://en.wikipedia.org/wiki?curid=4916038", "title": "Mechanical biological treatment", "text": "Mechanical biological treatment\n\nA mechanical biological treatment system is a type of waste processing facility that combines a sorting facility with a form of biological treatment such as composting or anaerobic digestion. MBT plants are designed to process mixed household waste as well as commercial and industrial wastes.\n\nThe terms \"mechanical biological treatment\" or \"mechanical biological pre-treatment\" relate to a group of solid waste treatment systems. These systems enable the recovery of materials contained within the mixed waste and facilitate the stabilisation of the biodegradable component of the material.\n\nThe sorting component of the plants typically resemble a materials recovery facility. This component is either configured to recover the individual elements of the waste or produce a refuse-derived fuel that can be used for the generation of power.\n\nThe components of the mixed waste stream that can be recovered include:\n\nMBT is also sometimes termed biological mechanical treatment (BMT), however this simply refers to the order of processing (i.e., the biological phase of the system precedes the mechanical sorting). MBT should not be confused with mechanical heat treatment (MHT).\n\nThe \"mechanical\" element is usually an automated mechanical sorting stage. This either removes recyclable elements from a mixed waste stream (such as metals, plastics, glass, and paper) or processes them. It typically involves factory style conveyors, industrial magnets, eddy current separators, trommels, shredders, and other tailor made systems, or the sorting is done manually at hand picking stations. The mechanical element has a number of similarities to a materials recovery facility (MRF).\n\nSome systems integrate a wet MRF to separate by density and flotation and to recover and wash the recyclable elements of the waste in a form that can be sent for recycling. MBT can alternatively process the waste to produce a high calorific fuel termed refuse derived fuel (RDF). RDF can be used in cement kilns or thermal combustion power plants and is generally made up from plastics and biodegradable organic waste. Systems which are configured to produce RDF include the Herhof and Ecodeco processes. It is a common misconception that all MBT processes produce RDF; this is not the case, and depends strictly on system configuration and suitable local markets for MBT outputs.\n\nThe \"biological\" element refers to either:\n\nAnaerobic digestion harnesses anaerobic microorganisms to break down the biodegradable component of the waste to produce biogas and soil improver. The biogas can be used to generate electricity and heat.\n\nBiological can also refer to a composting stage. Here the organic component is broken down by naturally occurring aerobic microorganisms. They breakdown the waste into carbon dioxide and compost. There is no green energy produced by systems employing only composting treatment for the biodegradable waste.\n\nIn the case of biodrying, the waste material undergoes a period of rapid heating through the action of aerobic microbes. During this partial composting stage the heat generated by the microbes result in rapid drying of the waste. These systems are often configured to produce a refuse-derived fuel where a dry, light material is advantageous for later transport and combustion.\n\nSome systems incorporate both anaerobic digestion and composting. This may either take the form of a full anaerobic digestion phase, followed by the maturation (composting) of the digestate. Alternatively a partial anaerobic digestion phase can be induced on water that is percolated through the raw waste, dissolving the readily available sugars, with the remaining material being sent to a windrow composting facility.\n\nBy processing the biodegradable waste either by anaerobic digestion or by composting MBT technologies help to reduce the contribution of greenhouse gases to global warming.\n\nUsable wastes for this system:\n\nPossible products of this system:\n\nFurther advantages:\n\nMBT systems can form an integral part of a region's waste treatment infrastructure. These systems are typically integrated with kerbside collection schemes. In the event that a refuse-derived fuel is produced as a by-product then a combustion facility would be required. This could either be an incineration facility or a gasifier.\n\nAlternatively MBT solutions can diminish the need for home separation and kerbside collection of recyclable elements of waste. This gives the ability of local authorities, municipalities and councils to reduce the use of waste vehicles on the roads and keep recycling rates high.\n\nFriends of the Earth suggests that the best environmental route for residual waste is to firstly maximise removal of remaining recyclable materials from the waste stream (such as metals, plastics and paper). The amount of waste remaining should be composted or anaerobically digested and disposed of to landfill, unless sufficiently clean to be used as compost.\n\nA report by Eunomia undertook a detailed analysis of the climate impacts of different residual waste technologies. It found that an MBT process that extracts both the metals and plastics prior to landfilling is one of the best options for dealing with our residual waste, and has a lower impact than either MBT processes producing RDF for incineration or incineration of waste without MBT.\n\nFriends of the Earth does not support MBT plants that produce refuse derived fuel (RDF), and believes MBT processes should occur in small, localised treatment plants.\n\n\n"}
{"id": "39903888", "url": "https://en.wikipedia.org/wiki?curid=39903888", "title": "Medio", "text": "Medio\n\nMedio is a business-to-business mobile analytics provider based in Seattle, WA. The company processes pre-existing data to provide historic and predictive analytics. Medio is built on a cloud-based Hadoop platform and is designed to interpret big data for mobile enterprise. Medio has had various partners including: IBM, Rovio, Verizon, T-Mobile, ABC, and Disney\n\nMedio was founded in 2004 by Brian Lent, Bill Bryant, David Bluhm, and Michael Libes and employed 40 people. Founded to be the 'Google' of mobile search engines, Medio was backed by $30 Million in initial venture funding from various tech companies including: Accel Partners, Mohr Davidow Ventures, and Frazier Technology Ventures.\n\nMedio received $11 Million more in 2006 to create a mobile analytics search engine capable of searching for ringtones, graphics, and internet-delivered information. This sparked employment to over 100 employees for some time, but in 2009 Google released their new mobile search engine. Rob Lilleness, who joined the company as President and COO in 2007 and was subsequently named CEO in 2009, took that as an opportunity to refocus as a predictive analytics and data science provider, using their recommendations engine as a key component of their newly focused company. The shift resulted in lay-offs of much of the staff, scaling back to nearly 60 employees.\n\nBy the end of 2010 the company became profitable, nearly tripling its sales from previous years. With the latest version of the Medio Platform and the release of products like K-Invite, Medio has grown to 70 employees with a total of $44 Million in venture funding.\n\nOn July 1, 2014, Medio was acquired by Nokia and the company plans to grow their presence in Seattle.\n\n"}
{"id": "662879", "url": "https://en.wikipedia.org/wiki?curid=662879", "title": "Mobile device", "text": "Mobile device\n\nA mobile device (or handheld computer) is a computing device small enough to hold and operate in the hand. Typically, any handheld computer device will have an LCD flatscreen interface, providing a touchscreen interface with digital buttons and keyboard or physical buttons along with a physical keyboard. Many such devices can connect to the Internet and interconnect with other devices such as car entertainment systems or headsets via Wi-Fi, Bluetooth, cellular networks or near field communication (NFC). Integrated cameras, digital media players, the ability to place and receive telephone calls, video games, and Global Positioning System (GPS) capabilities are common. Power is typically provided by a lithium battery. Mobile devices may run mobile operating systems that allow third-party apps specialized for said capabilities to be installed and run.\n\nEarly smartphones were joined in the late 2000s by larger, but otherwise essentially the same, tablets. Input and output is now usually via a touch-screen interface. Phones/tablets and personal digital assistants may provide much of the functionality of a laptop/desktop computer but more conveniently, in addition to exclusive features. Enterprise digital assistants can provide additional business functionality such as integrated data capture via barcode, RFID and smart card readers.\nBy 2010, mobile devices often contained sensors such as accelerometers, magnetometers and gyroscopes, allowing detection of orientation and motion. Mobile devices may provide biometric user authentication such as face recognition or fingerprint recognition.\n\nMajor global manufacturers of mobile devices include Apple, Samsung, Sony, Google, HTC, LG, and Motorola Mobility.\n\nDevice mobility can be viewed in the context of several dimensions:\n\nStrictly speaking, many so-called mobile devices are not mobile. It is the host that is mobile, i.e., a mobile human host carries a non-mobile smartphone device. An example of a true mobile computing device, where the device itself is mobile, is a robot. Another example is an autonomous vehicle. There are three basic ways mobile devices can be physically bound to mobile hosts: accompanied, surface-mounted or embedded into the fabric of a host, e.g., an embedded controller embedded in a host device. Accompanied refers to an object being loosely bound and accompanying a mobile host, e.g., a smartphone can be carried in a bag or pocket but can easily be misplaced. Hence, mobile hosts with embedded devices such as an autonomous vehicle can appear larger than pocket-sized.\n\nAs stated earlier, the most common size of mobile computing device is pocket-sized that can be hand-held, but other sizes for mobile devices exist, too. Mark Weiser, known as the father of ubiquitous computing, computing everywhere, referred to device sizes that are tab-sized, pad and board sized, where \"tabs\" are defined as accompanied or wearable centimetre-sized devices, e.g. smartphones and smart cards, and \"pads\" are defined as hand-held decimetre-sized devices, e.g., laptops and tablets. If one changes the form of the mobile devices in terms of being non-planar, one can also have skin devices and tiny dust-sized devices. \"Dust\" refers to miniaturised devices without direct HCI interfaces, e.g., micro electro-mechanical systems (MEMS), ranging from nanometres through micrometers to millimetres. See also Smart dust. \"Skin\": fabrics based upon light emitting and conductive polymers and organic computer devices. These can be formed into more flexible non-planar display surfaces and products such as clothes and curtains, see OLED display. Also see smart device.\n\nAlthough mobility is often regarded as synonymous with having wireless connectivity, these terms are different. Not all network access by mobile users, applications and devices need be via wireless networks and vice versa. Wireless access devices can be static and mobile users can move in between wired and wireless hotspots such as in Internet cafés. Some mobile devices can be used as mobile Internet devices to access the Internet while moving but they do not need to do this and many phone functions or applications are still operational even while disconnected to the Internet. What makes the mobile device unique compared to other technologies is the inherent flexibility in the hardware and also the software. Flexible applications include video chat, Web browsing, payment systems, NFC, audio recording etc. As mobile devices become ubiquitous there, will be a proliferation of services which include the use of the cloud. Although a common form of mobile device, a smartphone, has a display, another perhaps even more common form of smart computing device, the smart card, e.g., used as a bank card or travel card, does not have a display. This mobile device often has a CPU and memory but needs to connect, or be inserted into a reader in order to display its internal data or state.\n\nThere are many kinds of mobile devices, designed for different applications. This includes:\n\nHandheld devices have become ruggedized for use in mobile field management. Uses include digitizing notes, sending and receiving invoices, asset management, recording signatures, managing parts, and scanning barcodes. In 2009, developments in mobile collaboration systems enabled the use of handheld devices that combine video, audio and on-screen drawing capabilities to enable multi-party conferencing in real-time, independent of location. Handheld computers are available in a variety of form factors, including smartphones on the low end, handheld PDAs, Ultra-Mobile PCs and Tablet PCs (Palm OS, WebOS). Users can watch television through Internet by IPTV on some mobile devices. Mobile television receivers have existed since the 1960s, and in the 21st century mobile phone providers began making television available on cellular phones.\nIn the 2010s, mobile devices can sync and share many data despite the distance or specifications of said devices. In the medical field, mobile devices are quickly becoming essential tools for accessing clinical information such as drugs, treatment, even medical calculation. Due to the popularity of mobile gaming, the gambling industry started offering casino games on mobile devices, which in turn lead to inclusion of these devices in anti hazard legislature as devices that could potentially be used in illegal gambling. Other potentially illegal activities might include the use of mobile devices in distributing child pornography and the legal sex industry use of mobile apps and hardware to promote its activities, as well as the possibility of using mobile devices to perform trans-border services, which are all issues that need to be regulated. In the military, mobile devices have created new opportunities for the armed forces to deliver training and educational materials to soldiers, regardless of where they are stationed.\n\n\n"}
{"id": "145095", "url": "https://en.wikipedia.org/wiki?curid=145095", "title": "Motor vehicle", "text": "Motor vehicle\n\nA motor vehicle, also known as motorized vehicle or automotive vehicle, is a self-propelled vehicle, commonly wheeled, that does not operate on rails (such as trains or trams) and is used for the transportation of people or cargo.\n\nThe vehicle propulsion is provided by an engine or motor, usually an internal combustion engine or an electric motor, or some combination of the two, such as hybrid electric vehicles and plug-in hybrids. For legal purposes, motor vehicles are often identified within a number of vehicle classes including cars, buses, motorcycles, off-road vehicles, light trucks and regular trucks. These classifications vary according to the legal codes of each country. ISO 3833:1977 is the standard for road vehicle types, terms and definitions. Generally, to avoid requiring handicapped persons from having to possess an operator's license to use one, or requiring tags and insurance, powered wheelchairs will be specifically excluded by law from being considered motor vehicles.\n\nAs of 2010, there were more than one billion motor vehicles in use in the world, excluding off-road vehicles and heavy construction equipment. Global vehicle ownership per capita in 2010 was 148 vehicles in operation per 1000 people. The United States has the largest fleet of motor vehicles in the world, with 258 million in 2014. Vehicle ownership per capita in the US is also the highest in the world, with 769 vehicles in operation per 1000 people. The People's Republic of China has the second largest fleet in the world, with slightly more than 78 million vehicles, and in 2009 became the world's largest new car market. In 2011, a total of 80 million cars and commercial vehicles were built, led by China, with 18.4 million motor vehicles manufactured.\n\nThe US publisher Ward's estimates that as of 2010, there were 1.015 billion motor vehicles in use in the world. This figure represents the number of cars, trucks (light, medium and heavy duty), and buses, but does not include off-road vehicles or heavy construction equipment. The world vehicle population passed the 500 million-unit mark in 1986, from 250 million motor vehicles in 1970. Between 1950 and 1970, the vehicle population doubled roughly every 10 years. Two US researchers estimate that the world's fleet will reach 2 billion motor vehicles by 2020, with cars representing at least 50% of all vehicles. China’s and India’s automobile fleets are expected to grow at an annual rate of around 7 or 8%, while the slowest growth is expected in the United States, with less than 1% a year, and Western Europe, with 1 to 2%. Navigant Consulting forecasts that the global stock of light-duty motor vehicles will reach 2 billion units in 2035.\n\nGlobal vehicle ownership in 2010 was 148 vehicles in operation per 1000 inhabitants, a ratio of 1:6.75 vehicles to people, slightly down from 150 vehicles per 1000 inhabitants in 2009, a rate of 1:6.63 vehicles to people. The global rate of motorization increased in 2013 to 174 vehicles per 1000 inhabitants. In developing countries vehicle ownership rates rarely exceed 200 cars per 1,000 population.\n\nThe following table summarizes the evolution of vehicle registrations in the world from 1960 to 2012:\n\nThe 27 European Union (EU-27) member countries had a fleet of over 256 million in 2008, and passenger cars accounted for 87% of the union's fleet. The five largest markets, Germany (17.7%), Italy (15.4%), France (13.3%), the UK (12.5%), and Spain (9.5%), accounted for 68% of the region's total registered fleet in 2008. The EU-27 member countries had in 2009 an estimated ownership rate of 473 passenger cars per 1000 people.\n\nAccording to Ward's, Italy had the second highest (after the U.S.) vehicle ownership per capita in 2010, with 690 vehicles per 1000 people. Germany had a rate of motorization of 534 vehicles per 1000 people and the UK of 525 vehicles per 1000 inhabitants, both in 2008. France had a rate of 575 vehicles per 1000 people and Spain 608 vehicles per 1000 people in 2007. Portugal, between 1991 and 2002 grew up 220% on its motorization rate, having had in 2002, 560 cars per 1000 people. Italy also leads in alternative fuel vehicles, with a fleet of 779,090 natural gas vehicles , the largest NGV fleet in Europe. Sweden, with 225,000 flexible-fuel vehicles, has the largest flexifuel fleet in Europe by mid-2011.\n\nAccording to Ward's, the United States has the largest fleet of motor vehicles in the world, with 239.8 million by 2010. Vehicle ownership per capita in the U.S. is also the highest in the world with 769 vehicles in operation per 1000 inhabitants, or a ratio of 1:1.3 vehicles to people. The U.S. Department of Energy reports an even higher motorization rate of 828 vehicles per 1000 people, and a total of 245,441,000 vehicles by 2009. According to USDoE, the rate of motorization peaked in 2007 at 842.6 vehicles per 1000 people. In terms of licensed drivers, as of 2009 the country had 1.0 vehicle for every licensed driver, and 1.87 vehicles per household.\n\nThe stock of alternative fuel vehicles in the United States includes almost 10 million E85 flexible-fuel vehicles, the world's second largest after Brazil, but actual use of ethanol fuel is significantly limited due to the lack of E85 refueling infrastructure. The fleet of hybrid electric vehicles in the United States is the largest in the world, with more than 3.0 million units by October 2013. The country's fleet also includes 123,000 natural gas vehicles , mainly transit buses and delivery fleets. Despite its relative small size, natural gas use accounted for about 52% of all alternative fuels consumed by alternative transportation fuel vehicles in the U.S. in 2009.\n\nIn the US a motor vehicle is specifically defined as a contrivance used for commercial purposes. As defined in US Code\n18 U.S.C. § 31 : US Code - Section 31: Definitions (6) Motor vehicle. - The term \"motor vehicle\" means every description of carriage or other contrivance propelled or drawn by mechanical power and used for commercial purposes on the highways in the transportation of passengers, passengers and property, or property or cargo.\n\nThe People's Republic of China has the second largest fleet in the world, with slightly more than 78 million vehicles, overtaking Japan in 2010. About 13.6 million vehicles were sold in 2009, and motor vehicle registrations in 2010 increased to more than 16.8 million units, representing nearly half the world’s fleet increase in 2010.\n\nThe number of cars and motorcycles in China increased 20 times between 2000 and 2010. This explosive growth has allowed China to become the world's largest new car market, overtaking the US in 2009. Nevertheless, ownership per capita is 58 vehicles per 1000 inhabitants, or a ratio of 1:17.2 vehicles to people, well below the rate of motorization of developed countries.\n\nJapan had 73.9 million vehicles by 2010, and had the world's second largest motor vehicle fleet until 2009. With more than 1.1 million hybrid electric vehicles, Japan has the second largest hybrid fleet in the world after the US.\n\nThe Brazilian vehicle fleet reached 64.8 million vehicles in 2010, up from 29.5 million units in 2000, representing a 119% growth in ten years, and reaching a motorization rate of 340 vehicles per 1000 people. In 2010 Brazil experienced the second largest fleet increase in the world after China, with 2.5 million vehicle registrations.\n\n, Brazil has the largest alternative fuel vehicle fleet in the world with over 27 million alternative fuel motor vehicles in the road. The clean vehicle stock includes 20 million flexible-fuel cars and light utility vehicles by June 2013; over 3 million flex-fuel motorcycles by October 2013; between 2.4 and 3.0 million neat ethanol vehicles still in use, out of 5.7 million ethanol only light-vehicles produced since 1979; and, , a total of 1.69 million natural gas vehicles. In addition, all the Brazilian gasoline-powered fleet is designed to operate with high ethanol blends, up to 25% ethanol fuel (E25).\n\nIndia’s vehicle fleet had the second-largest growth rate after China in 2010, with 8.9%. The fleet went from 19.1 million in 2009 to 20.8 million units in 2010. India's vehicle fleet has increased to 210 million in March 2015. India has a fleet of 1.1 million natural gas vehicles \n\nAs of January 2011, the Australian motor vehicle fleet had 16.4 million registered vehicles, with an ownership rate of 730 motor vehicles per 1000 people, up from 696 vehicles per 1000 residents in 2006. The motor vehicle fleet grew 14.5% since 2006, for an annual rate of 2.7% during this five-year period.\n\nThe following table compares vehicle ownership rates by region with the US, the country with the highest motorization rate in the world, and how it has evolved from 1999 to 2009.\n\nIn 2014, a total of 89.7 million cars and commercial vehicles were built worldwide, led by China, with about 23.7 million motor vehicles manufactured, followed by the United States with 11.7 million, and Japan with 9.8 million. The following table shows the top 15 manufacturing countries for 2014 and their corresponding annual production between 2004 and 2014.\n\n"}
{"id": "1699604", "url": "https://en.wikipedia.org/wiki?curid=1699604", "title": "Parabolic torus reflector antenna", "text": "Parabolic torus reflector antenna\n\nA parabolic torus reflector antenna is a quasi-parabolic antenna, where the defining parabola is not rotated around the main transmission axis, but around an axis which stands vertically to this axis.\n\nSimulsat is a trademark for such antennas designed and manufactured by Antenna Technology Communications.\n\nWhereas parabolic satellite dishes with one low-noise block converter (LNB) are able to receive a satellite television broadcast from one communications satellite at a time, parabolic torus reflector antennas are capable of establishing views to more than 40 C and K band satellites simultaneously, by employing multiple LNBs.\n\n\n"}
{"id": "35036579", "url": "https://en.wikipedia.org/wiki?curid=35036579", "title": "Race engineer", "text": "Race engineer\n\nA race engineer is a motorsport team member whose role is to communicate with the data analyst as well as the mechanics to determine the changes to be made to the vehicle. Off the race track, the race engineer analyzes historical data to determine the initial set up for the next race event. The race engineer function includes hands-on management of the vehicle mechanics, organizing testing schedule, as well as studying the regulations. The race engineer seeks to make all these activities occur as seamlessly as possible for their driver.\n\nIn addition, the vehicle set-up must stay within the regulations for the race event. The primary goal of the race engineer is to achieve the best performance from the vehicle and from the driver at the race track.\n\nA good race engineer must have good interpersonal relationship skills. To be effective, the race engineer must have a good working relationship with not only their driver, but also the rest of the team, both at and away from the track. Many times the race engineer is also \"the face\" of the team for the media; this is especially true during the race while the driver is inaccessible. This makes the race engineers media skills a priority. \n\nThe role of the race engineer on racing teams has grown in importance since the adoption of on-board sensors in vehicles to collect performance data. The race engineer's job is to evaluate both the vehicle's performance, gathered from the telemetry in addition to their driver's feedback. The race engineer then seeks to improve the performance, and the drivers desires, by adjusting suspension, engine calibrations, aerodynamics, and other variables which affect the vehicle's performance on the race track.\n\nRace engineers tend to travel extensively, especially during the racing season of their motorsport teams. At the highest level of professional motorsports, international travel is common. Off season travel for race engineers is usually for testing, training, and visiting vendors.\n\n\n"}
{"id": "5084677", "url": "https://en.wikipedia.org/wiki?curid=5084677", "title": "SODAR", "text": "SODAR\n\nSODAR (SOnic Detection And Ranging), also written as sodar, is a meteorological instrument used as a wind profiler to measure the scattering of sound waves by atmospheric turbulence. SODAR systems are used to measure wind speed at various heights above the ground, and the thermodynamic structure of the lower layer of the atmosphere.\nSodar systems are in fact nothing more than sonar systems used in the air rather than in water; more specifically, since they operate using the Doppler effect with a multi-beam configuration to determine wind speed, they are the exact in-air equivalent to a subclass of sonar systems known as acoustic Doppler current profilers. Other names used for sodar systems include sounder, echosounder and acoustic radar.\n\nCommercial sodars operated for the purpose of collecting upper-air wind measurements consist of antennas that transmit and receive acoustic signals. A mono-static system uses the same antenna for transmitting and receiving, while a bi-static system uses separate antennas. The difference between the two antenna systems determines whether atmospheric scattering is by temperature fluctuations (in mono-static systems), or by both temperature and wind velocity fluctuations (in bi-static systems). \n\nMono-static antenna systems can be divided into two categories: those using multiple axis, individual antennas and those using a single phased array antenna. The multiple-axis systems generally use three individual antennas aimed in specific directions to steer the acoustic beam. Using three independent (i.e. non-colinear) axes is enough to retrieve the three components of the wind speed, although using more axes would add redundancy and increase robustness to noise when estimating the wind speed, using a least-squares approach. One antenna is generally aimed vertically, and the other two are tilted slightly from the vertical at an orthogonal angle. Each of the individual antennas may use a single transducer focused into a parabolic reflector to form a parabolic loudspeaker, or an array of speaker drivers and horns (transducers) all transmitting in-phase to form a single beam. Both the tilt angle from the vertical and the azimuth angle of each antenna are fixed when the system is set up. \n\nPhased-array antenna systems use a single array of speaker drivers and horns (transducers), and the beams are electronically steered by phasing the transducers appropriately. To set up a phased-array antenna, the pointing direction of the array is either level, or oriented as specified by the manufacturer.\n\nThe horizontal components of the wind velocity are calculated from the radially measured Doppler shifts and the specified tilt angle from the vertical. The tilt angle, or zenith angle, is generally 15 to 30 degrees, and the horizontal beams are typically oriented at right angles to one another. Since the Doppler shift of the radial components along the tilted beams includes the influence of both the horizontal and vertical components of the wind, a correction for the vertical velocity is needed in systems with zenith angles less than 20 degrees. In addition, if the system is located in a region where vertical velocities may be greater than about 0.2 m/s, corrections for the vertical velocity are needed, regardless of the beam's zenith angle.\n\nThe vertical range of sodars is approximately 0.2 to 2 kilometers (km) and is a function of frequency, power output, atmospheric stability, turbulence, and, most importantly, the noise environment in which a sodar is operated. Operating frequencies range from less than 1000 Hz to over 4000 Hz, with power levels up to several hundred watts. Due to the attenuation characteristics of the atmosphere, high power, lower frequency sodars will generally produce greater height coverage. Some sodars can be operated in different modes to better match vertical resolution and range to the application. This is accomplished through a relaxation between pulse length and maximum altitude.\n\nTraditionally used in atmospheric research, sodars are now being applied as an alternative to traditional wind monitoring for the development of wind power projects. Sodars used for wind power applications are typically focused on a measurement range from 50m to 200m above ground level, corresponding to the size of modern wind turbines. Some sodar products, such as the REMTECH PA-XS Sodar and the AQ510 Sodar, have been specifically developed for this market.\n\nCompact-beam sodars are more accurate in complex terrain where the wind vector can change across the measurement area of the sodar. By providing a more compact beam angle, these sodars reduce the effect of any change in the wind vector. This provides a more accurate estimate of wind flow and therefore energy production of a wind turbine. Compact beam sodars also reduce the effect of fixed echos and allow a more compact unit design.\n\nMultiple-axis sodars provide the capability for simultaneous firing of all three sound beams, unlike single-axis sodars which must fire each sound beam sequentially. Simultaneous firing can provide three times the number of sample points in any given period, resulting in a higher signal to noise ratio (SNR), higher data availability and greater accuracy.\n\nSodars designed for the wind energy industry also differ in important aspects such as the traceability of data as a number of manufacturers do not return full signal and noise spectrum data from the sodar unit, but rather, only return processed wind speed data. This means the raw data cannot be re-analysed or reprocessed.\n\nThe underlying physical principles behind the two devices are exactly the same. Both devices use sound waves to determine remote properties of the environment. Both devices use the Doppler effect to measure radial speeds on at least three non-colinear beams, which after simple computations yield the three vector components of the speed of the transmitting medium (air or water) at different altitudes. Both sodars and ADCPs can use either separate transducers for each beam or use phased arrays. Finally, both devices may use piezoelectric transducers to produce and receive the sound. \n\nHowever, the operating frequencies between sodars and ADCPs are typically different. Commercial ADCPs as manufactured \"e.g.\" by Teledyne RDI (the \"de facto\" leader of this market) typically use carrier frequencies that are in the hundred of kilohertz range (300 kHz, 600 kHz, 1200 kHz) while sodars transmit only in the low kilohertz range. Transmitting at a higher frequency is possible for ADCPs due to the better sound transmitting qualities of water, and this also benefits to the compacity of the device (a diameter of typically 25 cm / 10\" or less for ADCPs). Also, the acoustic impedance of the transducers is not the same, because they do not operate in the same medium: air for sodars, water for ADCPs; said differently, an ADCP would not work in the air, and a sodar would not work underwater. Finally, it is more common for ADCPs to use four beams, even when they are not using a phased array. This has the benefit of adding some form of redundancy, thus making the estimate of the water currents more robust to noise. This is feasible for sodars too, but for the cost of adding a fourth transducer. The operating range of typical ADCPs is less than two hundred meters (this lowers as frequency increases, as in air).\n\n\n"}
{"id": "7529770", "url": "https://en.wikipedia.org/wiki?curid=7529770", "title": "Screw conveyor", "text": "Screw conveyor\n\nA screw conveyor or auger conveyor is a mechanism that uses a rotating helical screw blade, called a \"\"flighting\", usually within a tube, to move liquid or granular materials. They are used in many bulk handling industries. Screw conveyors in modern industry are often used horizontally or at a slight incline as an efficient way to move semi-solid materials, including food waste, wood chips, aggregates, cereal grains, animal feed, boiler ash, meat and bone meal, municipal solid waste, and many others. The first type of screw conveyor was the Archimedes' screw, used since ancient times to pump irrigation water.\n\nThey usually consist of a trough or tube containing either a spiral blade coiled around a shaft, driven at one end and held at the other, or a \"shaftless spiral\"\", driven at one end and free at the other. The rate of volume transfer is proportional to the rotation rate of the shaft. In industrial control applications the device is often used as a variable rate feeder by varying the rotation rate of the shaft to deliver a measured rate or quantity of material into a process.\n\nA flexible screw conveyor works by using the internal friction within a powder or bulk solid to transfer the forward motion of the powder in contact with the spiral to the whole tube contents. With an angled system, a dynamic equilibrium is set up with the spiral action moving some particles upwards. \n\nScrew conveyors can be operated with the flow of material inclined upward. When space allows, this is a very economical method of elevating and conveying. As the angle of inclination increases, the capacity of a given unit rapidly decreases.\n\nThe rotating part of the conveyor is sometimes called simply an auger.\n\nThe \"grain auger\" is used in agriculture to move grain from trucks, grain carts or grain trailers into grain storage bins (from where it is later removed by gravity chutes at the bottom). A grain auger may be powered by an electric motor; a tractor, through the power take-off; or sometimes an internal combustion engine mounted on the auger. The helical rotates inside a long metal tube, moving the grain upwards. On the lower end, a hopper receives grain from the truck or grain cart. A chute on the upper end guides the grain into the destination location.\nThe modern grain auger of today's farming communities was invented by Peter Pakosh. His grain mover employed a screw-type auger with a minimum of moving parts, a totally new application for this specific use. At Massey Harris (later Massey Ferguson), young Pakosh approached the design department in the 1940s with his auger idea, but was scolded and told that his idea was unimaginable and that once the auger aged and bent that the metal on metal would, according to a head Massey designer, \"start fires all across Canada\". Pakosh, however, went on to design and build a first prototype auger in 1945, and 8 years later start selling tens of thousands under the 'Versatile' name, making it the standard for modern grain augers.\n\nA specialized form of grain auger is used to transfer grain into a seed drill, and is usually quite a lot smaller in both length and diameter than the augers used to transfer grain to or from a truck, grain cart or bin. This type of auger is known as a \"drill fill\". Grain augers with a small diameter, regardless of the use they are put to, are often called \"pencil augers\".\n\nVarious other applications of the screw or auger conveyor include its use in snowblowers, to move snow towards an impeller, where it is thrown into the discharge chute. Combine harvesters use both enclosed and open augers to move the unthreshed crop into the threshing mechanism and to move the grain into and out of the machine's hopper. Ice resurfacers use augers to remove loose ice particles from the surface of the ice. An auger is also a central component of an injection molding machine. An auger is used in some rubbish compactors to push the rubbish into a lowered plate at one end for compaction.\n\nAugers are also present in food processing. They are a tool of choice in powder processing, when it comes to convey or dose precisely bulk solids (powders, pellets...). In a conventional meat grinder, chunks of meat are led by the auger through a spinning blade and a holed plate. This method emulsifies the fat in beef to soften hamburger patties, and is also used to produce a wide variety of sausages and loaves. Augers are also used to force food products through dies to produce pellets. These are then processed further to produce products such as bran flakes. \n\nAugers are also used in oil fields as a method of transporting rock cuttings away from the shakers to skips. Augers are also used in some types of pellet stoves and barbecue grills, to move fuel from a storage hopper into the firebox in a controlled manner. Augers are often used in machining, wherein the machine tools may include an auger to direct the swarf (scrap metal or plastic) away from the work piece.\n\nScrew conveyors can also be found in waste water treatment plants to evacuate solid waste from the treatment process.\n\nThe amphibious infantry fighting vehicle BMP-3 uses an auger-type propulsion unit in water.\n\n\n"}
{"id": "53405853", "url": "https://en.wikipedia.org/wiki?curid=53405853", "title": "Sticky mat", "text": "Sticky mat\n\nA sticky mat, also called a tacky mat or cleanroom mat, is a mat with an adhesive surface that is placed at the entrances or exits to certain workplaces to remove contaminants from the bottoms of footwear and wheeled carts such as hand trucks. They are an example of an engineering control within the hierarchy of hazard controls.\n\nSticky mats are typically used in cleanrooms and construction sites. Their purpose is to prevent contaminants from entering the site with personnel, and hazardous materials from exiting. In a cleanroom setting, airborne particles that are not removed by the ventilation system deposit themselves onto a surface, where they can be transported by personnel walking on or past them.\n\nSticky mats can be temporary or permanent. Temporary sticky mats are made of a stack of adhesive plastic film layers that are periodically peeled off and discarded. Permanent mats are made of a polymer, usually polyester- or polyvinyl chloride-based, that binds particles through electrostatic forces. The peeling process for temporary mats may dislodge particles from the mat, causing inhalation risk. However, permanent mats must be washed with a mop and detergent, which is more time-consuming and may be done less often.\n\nA 2012 study found that temporary adhesive mats reduced the particle level on shoes and overshoes by 20–50% while permanent polymeric flooring reduced it by approximately 80%, and that adhesive mats released more particles when they were dirtier and when they were peeled quickly. However, sticky mats placed outside the entrance to an operating room or suite have not been shown to reduce the number of organisms on shoes or stretcher wheels, nor do they reduce the risk of surgical site infections.\n"}
{"id": "25286840", "url": "https://en.wikipedia.org/wiki?curid=25286840", "title": "Taxation of digital goods", "text": "Taxation of digital goods\n\nDigital goods are software programs, music, videos or other electronic files that users download exclusively from the Internet. Some digital goods are free, others are available for a fee. The taxation of digital goods is partially governed by a federal statute and has been the area of significant state legislative and rule-making activity.\n\nIn 1997, the United States federal government decided to limit taxation of Internet activity for a period of time. The Internet Tax Freedom Act (ITFA) prohibits taxes on Internet access, which is defined as a service that allows users access to content, information, email or other services offered over the Internet and may include access to proprietary content, information, and other services as part of a package offered to customers. The Act has exceptions for taxes levied before the statute was written and for sales taxes on online purchases of physical goods.\n\nThe statute has been amended three times since its enactment to extend this prohibition. The first amendment solely extended the Act's duration. The second extended it again and clarified the definition of Internet access as including certain telecommunication services, as well as reorganizing sections within the Act. The third amendment again extended the prohibition but narrowed the definition of Internet access to \"not include voice, audio or video programming, or other products and services . . . that utilize Internet protocol . . . and for which there is a charge\" except those related to a homepage, email, instant messaging, video clips, and personal storage capacity.\n\nIn 2009, Anna Eshoo, Congresswoman from California's 14th District (which includes most of Silicon Valley), introduced a bill to make the Act permanent in its most recent permutation. However, this bill died in committee.\n\nStates levying a tax on digital goods may be violating the ITFA. The states using their original tax code may fall within the grandfather clause of the ITFA, but there has been no litigation to clarify this or other aspects of the Act. One of the few cases brought under the ITFA involved Community Telecable of Seattle suing the city of Seattle in Washington state court, where Telecable claimed it should not have to pay a telephone utility tax because it was an Internet access provider under the ITFA. The Washington State Supreme Court held that Telecable could not be taxed as a telephone provider when it was providing Internet access under the ITFA.\n\nEvery digital-specific tax created by a state has been enacted after the ITFA became law. These laws may be preempted because the ITFA bars taxes on Internet access, and multiple or discriminatory taxes on electronic commerce. Courts have yet to clarify whether the existing laws compound taxes or are discriminatory. Although, it is likely that these laws can survive scrutiny under the ITFA because they can be interpreted to only tax services that fit within the exception to Internet access described in the statute and to be the only taxes on these digital products. On the other hand, there may be problems with these taxes because they may cover products and services dealing with homepages, email, personal storage, or video clips.\n\nWithout litigation, it may be difficult to distinguish the difference between the definitions of content given by the ITFA, such as between a video clip and video programming. iTunes, for example, could be designated as video programming for the videos it sells based on the definition found in the federal statute regulating cable companies, and as video clips for its previews. These laws may also run into trouble if they tax a download that is already taxed by another state, because multiple taxes are defined as taxing property that has been taxed once before by another state or political subdivision. VIDEO software>\n\nAnother possible federal limitation on Internet taxation is the United States Supreme Court case, \"Quill Corp. v. North Dakota\", 504 U.S. 298 (1992), which held that under the dormant commerce clause, goods purchased through mail order cannot be subject to a state's sales tax unless the vendor has a substantial nexus with the state levying the tax. The dormant commerce clause could also apply to any efforts to tax downloads. Since most downloads are from companies that are centralized in a small number of states, it is likely that there will not be many states with a substantial nexus to download providers. At present, no litigation has arisen to determine what will be defined as a proper nexus for a distributor of digital content within a state. It is possible that a state would argue that servers are enough of a nexus to tax the content passing through, although the Supreme Court has already ruled that communication by common carrier is not enough to form a substantial nexus.\n\nStates initially were slow to enact taxes on downloads, but with recent downturns in tax revenue caused by consumers purchasing more digital downloads, many states have sought ways to impose taxes on purely digital transactions. There are multiple ways that downloads are taxed. Some states use their existing franchise, sales, and use taxes to tax purchases/uses/transactions of consumers of Internet goods and services. Other states enacted laws specifically aimed at digital downloads.\n\nSome states presume that downloads are automatically covered by their existing tax statutes based on the common law definition of tangible personal property, which is anything that holds value on its own that is not real property.\n\nIn other states, state tax boards have released bulletins to explain what products are subject to sales and use taxes, tax administrative boards have handed down revenue rulings, and statutes have been amended to define \"tangible personal property\" to include digital goods and therefore subject them to sales tax.\n\nThe remaining states that tax downloads have specific statutes that define exactly what is to be taxed and what is not. The similarity in these taxes is that they are based on a sales-type scheme, where each download (or group of downloads) is taxed like a purchase in physical space.\n\n\nSome of these laws specifically address the taxation of software, which may or may not be interpreted by those states' courts to include downloadable content, i.e. music and video files.\n\nThe EU operates Value Added Tax (VAT) and electronic goods and services are subject to VAT at the applicable rate. Each member state may set its own rate of VAT if they want\n\nVAT regulations are very complicated and the intent of this article is not to provide definitive guidance but rather to list some of the relevant factors.\n\nIf a business is located within an EU member state and its turnover through internet sales or otherwise exceeds that member state's VAT threshold then the business must register for VAT. It is then obliged to collect VAT on its sales (outputs) and remit it to the tax authorities having deducted the VAT it pays on its purchases (inputs).\n\nIf a business makes sales of physical goods to a member state that exceeds that member states distance selling threshold (typically either EUR 30,000 or EUR 100,000) then it must register to pay VAT in that member state and collect VAT at that member state's VAT rate.\n\nIf sales are below the distance selling threshold VAT must be collected at the VAT rate in the business' own member state.\n\nIf a business is located within an EU member state and supplies e-services to an individual who is not VAT registered in another EU member state then VAT rules of the state where the business is located apply. If the business supplies e-services to a VAT-registered individual in another state then the business is not obliged to pay VAT in its state and thus the individual must pay VAT in its state. If the business supplies e-services to a VAT-registered individual yet the individual receives the e-services in a state where neither the business nor the individual has their establishment then the business is obliged to register for VAT in the state where the e-services are delivered to. The 2015 EU VAT legislation requires two non-conflicting pieces of evidence to be produced so as to determine what VAT rate should be applied to these digital goods sales.\n\nA business must always charge VAT to non-VAT registered entities (i.e. consumers) but should not charge VAT to foreign EU VAT registered businesses who provide them with a VAT number. These foreign EU businesses are required to declare their purchase and the tax due to their own tax authorities.\n\nIn his budget of May 12, 2015, the then Australian Federal Government Treasurer Joe Hockey revealed details of a new 10% goods and services tax (GST) to be introduced on \"certain electronic supplies\".\n\nThe proposed GST has already been dubbed the 'Netflix Tax' in Australia as on-demand video-streaming is one of the services that will come under the scope of the new rules. The Australian GST on digital services is due to come into effect in July 2017.\n\nOn Wednesday, February 10, 2016, the draft bill outlining Australia's new digital GST was introduced with Treasurer Scott Morrison telling the Australian Parliament that the new rules would: \"ensure Australian businesses selling digital products and services are not disadvantaged relative to overseas businesses that sell equivalent products in Australia.\"\n\n"}
{"id": "40076768", "url": "https://en.wikipedia.org/wiki?curid=40076768", "title": "Telepharmacy", "text": "Telepharmacy\n\nTelepharmacy is the delivery of pharmaceutical care via telecommunications to patients in locations where they may not have direct contact with a pharmacist. It is an instance of the wider phenomenon of telemedicine, as implemented in the field of pharmacy. Telepharmacy services include drug therapy monitoring, patient counseling, prior authorization and refill authorization for prescription drugs, and monitoring of formulary compliance with the aid of teleconferencing or videoconferencing. Remote dispensing of medications by automated packaging and labeling systems can also be thought of as an instance of telepharmacy. Telepharmacy services can be delivered at retail pharmacy sites or through hospitals, nursing homes, or other medical care facilities.\n\nThe term can also refer to the use of videoconferencing in pharmacy for other purposes, such as providing education, training, and management services to pharmacists and pharmacy staff remotely.\n\nA primary appeal of telepharmacy is its potential to expand access to pharmacy care in smaller rural communities, some of which cannot support a full-time pharmacist or cannot easily recruit a pharmacist to reside in their region. Telepharmacy can potentially give patients in remote locations access to professional pharmacy care that could not be received locally, which can lower costs and improve patient safety through better patient counseling, drug administration monitoring, and compliance monitoring. Sharing of pharmacists between sites can also decrease costs in existing facilities, which might no longer need to employ a full-time pharmacist.\n\nThe potential costs of telepharmacy are broadly the same as those associated with all forms of telemedicine: potentially decreased human interaction between medical professionals and patients, an increased risk of error when medical services are delivered in the absence of a registered professional, and an increased risk that protected health information may be compromised through electronic information storage and transmission.\n\nThe implementation of telepharmacy varies by region and jurisdiction. Factors including geography, laws and regulations, and economics influence its implementation.\n\nA form of telepharmacy has been in use by Australia's Royal Flying Doctor Service since 1942. Medical chests containing medications and equipment are placed in remote communities where they can be administered to patients during a telehealth consultation. Some 3,500 chests were distributed around Australia as of 2006. In one year, Queensland recorded 21,470 telehealth consultations, of which 13.7% resulted in administration of a medication from a medical chest. The medication types administered most often are antibiotics, analgesics and gastrointestinal medications. This system improves access to both emergency and routine medical care in remote parts of Australia and reduces the need for patients to travel to seek medical care.\n\nAnother application of telepharmacy in Queensland has been the provision of pharmaceutical reviews in rural hospitals that lack on-staff pharmacists. Although broader use of telepharmacy could help alleviate a shortage of pharmacists, Australia has lagged the United States in its implementation of telepharmacy, partly because doctors, nurses, and other health care workers provide pharmacy services in rural and remote areas where there are no pharmacists.\n\nImplementation of telepharmacy in the United States began in the 2000s. A combination of factors, including changes in Medicare reimbursement for medications and the recession of 2007–8, led to a decline in the number of independent pharmacies in rural areas. In response to the need for alternative means of delivering pharmacy in services in rural communities lacking a full-time pharmacist, several midwestern and northwestern states with extensive rural areas have led much of the development of policy and implementation methods for telepharmacy.\n\nIn 2001, North Dakota became the first U.S. state to pass regulations allowing retail pharmacies to operate without requiring a pharmacist to be physically present. The next year, state agencies and grants established the North Dakota Telepharmacy Project, which now supports more than fifty remote retail and hospital pharmacy sites throughout North Dakota. In this program, a licensed pharmacist at a central site communicates with remote site pharmacy technicians and patients through videoconferencing. A 2004 study of the program found that telepharmacy delivered the same quality of pharmacy services as traditional facilities, and a study of the operation of one North Dakota telepharmacy business from 2002 through 2004 found that, while medication inventory turnover was lower than the industry average, the remote sites were able to be operated profitably. The success and expansion of this program were an inspiration and model for programs and laws in other states.\n\nThe Community Health Association of Spokane, a network of community health centers in Spokane, Washington, started a telepharmacy program in 2001. The program delivers remote medication dispensing and health counseling to patients at six urban and rural clinics; remote site personnel are connected to pharmacists at the base site by videoconferencing. A survey found that most patients at the remote sites strongly agreed or agreed that they would have had difficulty affording their medications without this program.\n\nThe Alaska Native Medical Center, a hospital in Anchorage, Alaska, providing telehealth services to Alaska Native populations, established a telepharmacy program in 2003 to improve its pharmaceutical services in rural native settlements. The American Society of Health-System Pharmacists gave the program its 2006 Award for Excellence in Medication-Use Safety, concluding that the use of telepharmacy had improved access to pharmaceutical care and enabled pharmacists to monitor medication safety and encourage medication adherence, as well as making pharmacy care more cost-effective.\n\nThe U.S. Navy Bureau of Medicine operates a large-scale telepharmacy program for the use of service personnel. After piloting the program in 2006 at Naval Hospital Pensacola in Florida and Naval Hospital Bremerton in Washington, in 2010 the Navy expanded it to more sites throughout the world. This program represents the largest implementation of telepharmacy to date.\n\nCalifornia passed a Telehealth Advancement Act in 2011 to update the state's legal definitions of telehealth, simplify approval processes for telehealth services, and broaden the range of medical services that may be provided via telehealth. The law establishes legal parity between the direct and remote delivery of pharmacy care. Iowa's first telepharmacy opened in September 2012 after receiving a three-year waiver from the Iowa Board of Pharmacy that allows the facility to operate without a pharmacist on-site.\n\nA 2010 study of the various American states' rural health offices found that telepharmacy in rural medical facilities varied in prevalence across the United States but was still not widespread, and that many states had not yet clearly defined regulations for telepharmacy in hospitals. Adoption and implementation of telepharmacy methods has been slow compared to the spread of the basic technologies involved (internet access, audio/video compression algorithms, microphones and video cameras), despite periodic predictions of a forthcoming boom in the industry. Aside from more intangible factors (such as physicians' and pharmacists' personal uneasiness with the lack of physical interaction with patients), the major obstacles to telepharmacy implementation appear to have been the lack of clear legal regulations for telepharmacy, and the lack of network and software systems to manage (and secure) all of the data used in a professional pharmacy. As of 2010, many of the telepharmacy facilities in active operation were operating as pilot programs or under temporary waivers issued by state regulators because many states still had no clear legal framework for the regulation of remote pharmaceutical sites without pharmacists. Even in states that had regulated retail telepharmacy practices, regulations were often not in place to permit the implementation of telepharmacy in hospital settings. For some pharmacy facilities that might otherwise consider telepharmacy, the cost and complexity of the infrastructure needed to manage patient data across multiple sites can be prohibitive. In addition to the computer hardware required for patient data storage, distribution and teleconferencing, telepharmacy programs must deploy network security tools and procedures adequate to protect patient medical information in compliance with HIPAA and other patient privacy regulations. In 2010 the North Dakota Telepharmacy Project estimated that the computer hardware needed for a typical retail installation costs US$17,300 per site, with an additional cost of US$5,000 to buy a mobile cart for a hospital installation.\n\nAdoption of telepharmacy in Canada began as a response to a nationwide shortage of pharmacists. Canada's first telepharmacy service was started by a hospital in Cranbrook, British Columbia, in June 2003 in order to assist a hospital in a nearby town that was unable to hire a pharmacist. To meet the need for service, a hospital pharmacist in Cranbrook began using telepharmacy technology to oversee pharmacy technicians at the other hospital. A similar service was subsequently extended to other small hospitals in the province; it is also used to provide coverage when a hospital's sole pharmacist is absent due to illness or vacation. Remote dispensing machines for medication began operation in Ontario, Canada, in 2007. After a patient inserts a prescription into the dispensing machine, the prescription is scanned and the patient is connected by telephone videoconference to a pharmacist at a remote site. The pharmacist reviews the prescription, discusses the patient's medication history, and authorizes the machine to dispense medication to the patient. The machines proved successful, with one assessment revealing that 96% of patients using them had their prescription filled in under five minutes. As of 2009, a hospital in Ontario, Canada, was using telepharmacy services in addition to retaining a pharmacist at the hospital; the telepharmacist reviews medication orders, while the on-site pharmacist works with patients and oversees medication safety in the facility. Thus telepharmacy support allows the on-site pharmacist to focus on the more sensitive and nuanced tasks for which physical presence is most helpful.\n\nAfter their success in Canada, remote medication dispensing machines were scheduled to be tested at several hospital locations in the United Kingdom beginning in 2010. In 2013, Maxor National Pharmacy Services, a U.S. company, reported that its remote dispensing machines for medication were being used in Bahrain, Belgium, Cuba, England, Germany, Guam, Italy, Japan, Spain and Venezuela.\n\nIn 2010, Mannings drugstores became the first in Hong Kong to use videoconferencing to allow patients at outlets without full-time pharmacists to consult with pharmacists at other sites.\n"}
{"id": "45289335", "url": "https://en.wikipedia.org/wiki?curid=45289335", "title": "Terminated merchant file", "text": "Terminated merchant file\n\nA terminated merchant file (TMF) is a tool used by credit card processing companies to screen potential merchants before giving them a merchant account. In the case of Mastercard and American Express it is known as the MATCH list.\n\nThe terminated merchant files are shared among processors and act as blacklists, where merchants with high-risk accounts or excessive chargebacks are put on the list and prevented from opening an account with a different credit card processor.\n\nOther reasons for being put on a terminated merchant file include:\n"}
{"id": "31803478", "url": "https://en.wikipedia.org/wiki?curid=31803478", "title": "The Icon Bar", "text": "The Icon Bar\n\nThe Icon Bar (also referred to as TIB) is a computing and technology website with a focus on the RISC OS computer operating system.\n\n\"The Icon Bar\" was founded in 2000 by Tim Fountain, Alasdair Bailey and Richard Goodwin. In 2004, co-founder Richard Goodwin was nominated for the \"Drobe\" awards for keeping the \"popular forum\" online. It was further developed by the same people who developed \"Acorn Arcade\", the contents of which were incorporated in 2006. At this time, it broadened its remit to also cover alternative platforms and new technologies, while still keeping abreast of the scene.\n\nWhen \"Drobe\" closed as a news site in 2009, \"The Icon Bar\" was cited as a notable alternative and took over running the annual awards scheme for the scene. It has been selected for inclusion by editors in at least one web directory,\n\nThe site features articles, news, forums and other media. It also hosts a \"Media Watch\" page, where users can share any relevant items they spot in the media.\n"}
{"id": "37953707", "url": "https://en.wikipedia.org/wiki?curid=37953707", "title": "WATIAC", "text": "WATIAC\n\nWATIAC was a virtual computer developed for teaching the principles of assembly language programming to undergraduates.\nWATIAC, and the WATMAP assembly language that ran on it were developed in 1973 by the newly founded Computer Systems Group,\nat the University of Waterloo, under the direction of Wes Graham.\n\nIn the 1970s most programming was conducted through batch stream processing, where the operating systems of the day, like IBM`s OS-360, would allow a single program to use all the resources of a large computer, for a limited period of time.\nSince student programs were only run a few times, possibly only once, after they had been successfully written, and debugged, efficient running of those programs was of relatively little importance, compared with quick compilation and relatively good error messages.\n\nWaterloo had been a leader in writing single pass, compile-and-go teaching compilers, with first its WATFOR FORTRAN compiler, and its WATBOL COBOL compiler.\nWATMAP was developed to be a similar compile-and-go teaching compiler.\n"}
{"id": "2954054", "url": "https://en.wikipedia.org/wiki?curid=2954054", "title": "Yeoman plotter", "text": "Yeoman plotter\n\nThe Yeoman Plotter was a plotter used on ships and boats to transfer GPS coordinates or RADAR echo locations onto a paper navigation chart and to read coordinates from the chart. It was manufactured from 1985 to 2014/2015 and was an intermediary step between traditional paper chart navigation and full electronic chart displays. It was easy to understand for people that were accustomed to paper charts and much cheaper than electronic chart displays available at the time. The continuing fall in prices of electronic chart displays, their increase in functionality such as radar overlay and the advent of cheap tablets eventually made the Yeoman plotter uncompetitive.\n\nThe plotter consists of a plotting surface impregnated with fine wires and a moveable \"mouse\" containing a sensing ring. The mouse's location on the surface can be electronically sensed.\n\nTo use the plotter, a conventional paper chart is first fixed in position on the surface (the method of fixing varies between the different models of plotter; see below). It is then \"registered\" - the mouse is placed over three known points on the chart and a button pressed at each. Knowing the location on the surface of these points, the system can interpolate the mouse's position anywhere on the chart. In the 1990s charts in some countries had plotter reference points printed on them, and the coordinates of these points were pre-programmed in the Yeoman unit. Other charts needed the input of the reference points to be manually added.\n\nAlthough the device can provide some useful data on its own (positions of charted objects, distances and bearings between points), in practice it is invariably linked to a source of position data such as GPS. This enables many useful features, the most important of which is the fixing on the chart of the vessel's current position. In position mode, four illuminated arrows around a transparent area on the mouse are used. The device is pushed across the chart, following the arrows, until all four of them are extinguished. This indicates that the mouse is directly over the vessel's current position on the chart; if required, a pencil can be poked through a small hole to mark the fix on the chart. After a couple of goes, this procedure becomes instinctive and very fast.\n\nThe mouse can also be used to obtain the range and bearing from the vessel's current position to any other object, simply by placing it over that object on the chart and reading the figures from the display. If a suitable radar is fitted it can be combined with the Yeoman plotter for navigation in poor visibility. In one mode, the position of the mouse on the chart is reflected in a cursor on the radar display - the mouse can be placed over a charted object to identify its echo on the radar, which is very useful for objects whose reflection may not be obvious. Similarly, a position from the radar screen can be sent to the Yeoman - the mouse is moved by hand following the illuminated arrows as for plotting a fix, and is guided to the location of the radar return.\n\nThe way in which the Yeoman plotter combines speedy GPS position fixing with a paper chart is cited as a benefit for two principal reasons. The first is one of safety and reliability - even the best marine electronics fail from time to time, and if the boat's electronic instruments should cease to work then a recent pencil plot on a paper chart becomes extremely valuable. Because such a fix can be made on a Yeoman in around two seconds, it is likely to be updated far more frequently. The fact that the correct chart is guaranteed to be on board, up to date, and open on the chart table ready for reversion to traditional navigation is also an aspect to consider as video chart plotters become more widely used.\n\nThe second benefit of the Yeoman plotter over an electronic chart display is cited less often, but to most of its users is the more important - they find it easier to use. While video plotter technology is improving every year, they still must display their charts on relatively small screens, with input via a single small joystick and a few buttons. By contrast, a printed chart on a Yeoman Navigator provides a 35-inch \"display\" at extremely high resolution, that one can draw on, make notes upon, and use for traditional operations with dividers and rulers, combined with instant (permanent!) position fixes and range-and-bearing information from the Yeoman mouse. This mixing of technology and traditional techniques was quite effective.\n\nYeoman made four versions of their plotter for different situations:\n\n\nThe manufacturing rights for Yeoman Plotters were purchased by Precision Navigation Ltd in 2004 and production has been relocated to East Anglia (UK). In 2014 the ownership of Precision Navigation transferred to Charity & Taylor and production ceased.\n\n\nYeoman Sport User Manual\n"}
