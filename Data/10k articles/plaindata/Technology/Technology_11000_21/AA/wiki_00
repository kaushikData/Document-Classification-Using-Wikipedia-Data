{"id": "3579510", "url": "https://en.wikipedia.org/wiki?curid=3579510", "title": "AN/TPS-59", "text": "AN/TPS-59\n\nThe AN/TPS-59 is an active electronically scanned array, transportable air search 3D radar.\n\nThe radar uses active beam steering in elevation and mechanical steering in azimuth.\n\nIt is similar to the AN/FPS-117.\n\nThe Lockheed Martin GE-592 radar is a version of the US Marine Corps AN/TPS-59 which is very similar to the AN/FPS-117.\n\nThe AN/TPS-59 is used by the United States Marine Corps, Egypt, and Bahrain. The United States Marine Corps currently uses AN/TPS-59 Version 3. It has seen action in Operation Desert Shield/Desert Storm, Operation Iraqi Freedom, Operation Enduring Freedom, and has deployed to the Republic of Korea.\n\nThe GE-592 is used by Taiwan.\n\n"}
{"id": "3664546", "url": "https://en.wikipedia.org/wiki?curid=3664546", "title": "Accumulator (energy)", "text": "Accumulator (energy)\n\nAn accumulator is an energy storage device: a device which accepts energy, stores energy, and releases energy as needed. Some accumulators accept energy at a low rate (low power) over a long time interval and deliver the energy at a high rate (high power) over a short time interval. Some accumulators accept energy at a high rate over a short time interval and deliver the energy at a low rate over longer time interval. Some accumulators typically accept and release energy at comparable rates. Various devices can store thermal energy, mechanical energy, and electrical energy. Energy is usually accepted and delivered in the same form. Some devices store a different form of energy than what they receive and deliver performing energy conversion on the way in and on the way out.\n\nExamples of accumulators include steam accumulators, mainsprings, flywheel energy storage, hydraulic accumulators, rechargeable batteries, capacitors, compensated pulsed alternators (compulsators), and pumped-storage hydroelectric plants.\n\nIn general usage in an electrical context, the word \"accumulator\" normally refers to a lead–acid battery.\n\nThe London Tower Bridge is operated via an accumulator. The original raising mechanism was powered by pressurised water stored in several hydraulic accumulators. In 1974, the original operating mechanism was largely replaced by a new electro-hydraulic drive system.\n\n"}
{"id": "46824194", "url": "https://en.wikipedia.org/wiki?curid=46824194", "title": "Aquantia Corporation", "text": "Aquantia Corporation\n\nAquantia Corporation is a manufacturer of high-speed transceivers. In 2004, Aquantia was founded to revolutionize Ethernet and accelerate connectivity. Aquantia first delivered products for Data Center connectivity, and in 2012 developed the world's first integrated 10GBASE-T MAC/PHY for servers. In 2014, Aquantia introduced a new Multi-Gig Ethernet technology into the Enterprise Infrastructure market and was joined by Cisco and others in co-founding the NBASE-T Alliance. Their Multi-Gig technology served as the baseline for the 802.3bz standard that was ratified by the IEEE in September 2016. This standard is now the basis for all new Multi-Gig implementations on Cat 5e and Cat 6 cabling in the Enterprise, SMB and SoHo environments.\n\nIn 2016 Aquantia introduced the first 100G technology, QuantumStream. In 2017, the company introduced a class of Multi-Gig products combining their Multi-Gig PHY technologies with an Enterprise-class controller to address the next generation of Client Connectivity in PCs by collaborating with numerous PC OEMs.\n\nEvery year, more than a billion ports of Ethernet ship into a variety of platforms. These platforms have started to convert from outdated Gigabit Ethernet to Multi-Gigabit Ethernet as the demand for bandwidth and data continues to grow. The Company currently addresses the Multi-Gig upgrade cycles across their existing (Data Center, Enterprise Infrastructure and Access applications) and new markets.\n\nIt was a privately held company which had raised $178.9 million over eight rounds of funding as of mid-2015. It won Company of the Year at the 2014 annual Creativity in Electronics awards, and was ranked by Deloitte Fast 500 as the fastest-growing semiconductor company in North America in 2014, 2015, and 2016. In 2016, Aquantia was named a finalist in UBM Tech’s EE Times and EDN Annual Creativity in Electronics (ACE) Awards for “Company of the Year\".\n\nThe company is now public, as of November 2017, and trades on the NYSE under the ticker symbol AQ.\n\nAquantia acquired the 10GBASE-T assets of PLX Technology in September 2012. PLX had picked them up in September 2010 from Teranetics.\n"}
{"id": "5280273", "url": "https://en.wikipedia.org/wiki?curid=5280273", "title": "Biochemical Society", "text": "Biochemical Society\n\nThe Biochemical Society is a learned society in the United Kingdom in the field of biochemistry, including all the cellular and molecular biosciences.\n\nIt currently has around 7000 members, two-thirds in the UK. It is affiliated with the European body, Federation of European Biochemical Societies (FEBS). The Society's current President (2016) is Sir David Baulcombe. The Society's headquarters are in London.\n\nThe society was founded in 1911 by Benjamin Moore, W.D. Halliburton and others, under the name of the Biochemical Club. It acquired the existing \"Biochemical Journal\" in 1912. The society name changed to the Biochemical Society in 1913.\n\nIn 2005, the headquarters of the society moved from Portland Place to purpose-built offices in Holborn. In 2009, the headquarters moved again to Charles Darwin House, near Gray's Inn Road.\n\nPast presidents include Professor Ron Laskey, Sir Philip Cohen, and Sir Tom Blundell.\n\nThe society makes a number of merit awards, four annually and others either biennially or triennially, to acknowledge excellence and achievement in both specific and general fields of science. The annual awards comprise the Morton Lecture, the Colworth Medal, the Centenary Award and the Novartis Medal and Prize. \n\nThe Society's wholly owned publishing subsidiary, Portland Press, publishes books, a magazine, \"The Biochemist\", and several print and online academic journals:\n\n\nThe Society's flagship publication, the \"Biochemical Journal\", celebrated its centenary in 2006 with the launch of a free online archive back to its first issue in 1906.\n\n\n"}
{"id": "904341", "url": "https://en.wikipedia.org/wiki?curid=904341", "title": "Bored cylindrical lock", "text": "Bored cylindrical lock\n\nA bored cylindrical lock is a lockset which is installed by boring two circular holes in the door. Door handles may also use the same installation.\n\nTwo holes are bored, perpendicular to one another, into the door. The \"face bore\" is the larger hole which is bored into the door face and a smaller \"edge bore\" hole is bored into the door edge. The edge may require additional preparation to receive the latch assembly, typically by routing or chiseling a shallow mortise. Some commercially-sold doors may come prepared to receive one or more bored cylindrical locks, such as entry doors, which typically require both a door knob and dead bolt.\n\nIn the United States, typically, the face bore is sized from in diameter and is centered at from the leading edge of the door. This distance is referred to as the \"backset.\" Other, less popular, backsets are at . Residential doors are normally prepared with a backset and commercial doors have a backset. The edge bore is typically centered on the edge.\n\nThe cylindrical lock was invented by Walter Schlage in 1923. The bored cylindrical lock arose from a need for a more cost-effective method of locking doors. The previous norm, the mortise lock, is a more complex device, and its higher manufacturing cost as well as its more labor-intensive installation make the bored cylindrical lock an ideal substitute, both in price and functionality. Because the mortise lock has a larger lock case, a larger and more complex volume must be removed from the door before it can be installed, but the mortise lock may offer additional functions compared to a cylindrical lock; for instance, the mortise lock may include a deadbolt in a single unit, while the cylindrical lock would require separate face bores for a deadbolt and doorknob. The 1923 patent evolved from an earlier Schlage patent filed in 1920 for a lock whose installation required a face bore and surface rabbet, which simplified door preparation compared to a mortise lock.\n"}
{"id": "1490168", "url": "https://en.wikipedia.org/wiki?curid=1490168", "title": "Butter churn", "text": "Butter churn\n\nA butter churn is a device used to convert cream into butter. This is done through a mechanical process, frequently via a pole inserted through the lid of the churn, or via a crank used to turn a rotating device inside the churn.\n\nThe word “butter” is believed to be derived from the Greek word bou-tyron, the approximate meaning of which is “cow cheese”. However, some believe the word came from the Scythian culture, as the ancient Greeks tended to herd sheep and goats, whose milk is not as good for butter making as that of cows, which the Scythians primarily herded. The word \"churn\" is from the Old English \"ċyrin,\" to churn. This is probably derived from the Old English \"cyrnel,\" \"kernel,\" due to the appearance of butter grains after milk has been churned.\n\nThe butter churn gave its name to the milk churn, early examples of which were based on butter churns. The milk churn is not, however, used for the act of churning, but rather to transport milk.\n\nEvidence for the use of butter dates back as early as 2000 BC, and there is mention of it in biblical works. The butter churn itself may have existed as early as the 6th century AD, as can be seen by what appears to be a churn lid dating from that era.\nIn the European tradition, the butter churn was primarily a device used by women, and the churning of butter was an essential responsibility along with other household chores. In earlier traditions of butter making, nomadic cultures placed milk in skin bags and produced butter either by shaking the bag manually, or possibly by attaching the bag to a pack animal, and producing butter simply through the movement of the animal. Some theorists believe this is how the butter creation process was discovered. Some cultures still use a process similar to this, whereby a bag is filled with milk, tied to a stick, and vigorously shaken.\n\nThe most historically prominent types of butter churns are the plunge churn, which is a container, usually made out of wood, where the butter-making action is created by moving in a vertical motion a staff that is inserted into the top. This type of churn is also known as an ‘up and down’ churn, churning tub, plunger churn, plumping churn, knocker churn, plump-kirn, or plowt-kirn (where “kirn\" is a Scots/Northern English word for churn). The staff used in the churn is known as the dash, dasher-staff, churn-staff, churning-stick, plunger, plumper, or kirn-staff.\n\nAnother prominent type of churn was the paddle churn, which was a container that contained a paddle, which was operated by a handle. The paddle churned the butter inside the container when the handle was turned.\nThe barrel churn was also used extensively. This type of churn was a barrel turned onto its side with a crank attached. The crank either turned a paddle device inside the churn, as in the paddle churn, or turned the whole barrel, whose action converted the milk to butter. The barrel churn was one of the agricultural innovations of 18th century Europe.\n\nOne particularly novel invention of note was the rocking chair butter churn. This device, invented by Alfred Clark, consisted of a barrel attached to a rocking chair. While the rocking chair moved, the barrel moved and churned the milk within into butter.\n\n\n"}
{"id": "7347661", "url": "https://en.wikipedia.org/wiki?curid=7347661", "title": "BytesForAll", "text": "BytesForAll\n\nBytesForAll (also known as B4A or BfA) is a South Asian initiative to focus on how information technology and the internet can help in taking up social development issues. It is one of the oldest ICT4D (information and communication technologies for development) networks in South Asia. It was launched at a time when ICT4D was yet to become a buzz-word on the development circuit, and was still largely unnoticed for its potential.\n\n BytesForAll was founded in July 1999 by Frederick Noronha of Goa, India and Partha Pratim Sarkar of Dhaka, Bangladesh. Photo alongside shows Noronha (left) and Sarkar at an April 2006 APC-organized meet in Dhaka, Bangladesh. It is inspired by the Free/Libre and Open Source Software (FLOSS) model, and says it prefers to be an unfunded, volunteer-driven project.\n\nIn 2005, BytesForAll became the lone member of the Association for Progressive Communications or APC in South Asia.\n\nBytesForAll_Readers had some 1,548 subscribers as of October 2006, and describes itself saying it \"is particularly designed for BytesForAll readers and supporters who want to take part and want to be updated about ICT and development-related issues in South Asia.\"\n\nThe BytesForAll mailings and discussions via its mailing list are summarised and published each month in the i4d magazine published from near New Delhi each month. BytesForAll has also shared contents and columns with the Spider internet magazine of Karachi, Pakistan (Dawn group), and earlier with the Express Computer magazine of the Indian Express group of Mumbai, India.\n\nBytesForAll also describes itself as \"being one of the oldest network on ICT4D issues in South Asia\". Its mailing list, which is watched by a number of ICT enthusiasts and academics across the globe sees itself as a forum that \"encourages a free flow of information and a lively debate and discussion on people-oriented IT practices.\"\n\nThe first issue of BytesForAll e-zine (July 1999) is archived online by the Inti.be website.\n\nRun by a volunteer administration team.\n\nOne of the goals of the BytesForAll project is to build people-to-people links across the diverse yet similar countries of South Asia, a region which is home to a huge population, but also faces a lot of political trouble at times.\n\nBytesForAll is a virtual organisation which does not have a physical office or entity. Its members and volunteers interact via cyberspace, through email and occasional group chat sessions.\n\nBytesForAll has emerged as one of the credible sources of news and criticism of the ICT4D networks in South Asia. It is read by campaigners, activists at the grassroots, people involved with actual projects, funders, key players in the development debate and also academics in both South and North.\n\nOne of the early, tone-setting essays is titled When a Modem Costs More than a Cow and is by noted Dhaka, Bangladesh-based photographer Shahidul Alam who has founded Drik based in the locality of Dhanmondi. In it, Alam argues strongly a case for ICT and technology to reach the poor. He says, \"Where information is power, denying information to marginalized communities, actively prevents the rural poor from overcoming the unequal power structures that they are trapped within. While it is in the interest of the powerful in society to restrict such access, it is also in the interest of the powerful nations to deny access and maintain domination. The unrestricted flow of general information is an essential pre-requisite for an egalitarian society.\"\n\nOver the years, BytesForAll has been intensively covering the field, mainly through its volunteer network of journalists and writers. BytesForAll has highlighted and tracked the growth of certain projects from South Asia, including the Hole In The Wall project, the Simputer, and free software (or FLOSS). BytesForAll has actively debated the growth of Free Software and Open Source in South Asia.\n\nIn mid-2006, BytesForAll set up its BytesForAll_FLOSS network to build links among supporters of FLOSS and techies dabbling in it, in South Asia.\n\nOver the years, BytesForAll has been able to build links and bridges among techies and development activists (specially those with a tech focus) in the South Asian region.\n\nMassachusetts Institute of Technology's Andrew W. Mellon Professor of Human Development in the Program in Science, Technology, and Society and Director of the MIT India Program Prof. Kenneth Kenniston has called BytesForAll \"the single most valuable source of information on 'IT for the people' projects anywhere in South Asia.\"\n\nBytesForAll has argued that it goes \"beyond the popular concept of digital divide. We not only recognize that there is a widespread disparity between information have and have-nots (in terms of access and distribution) but also raise and analyze the issues that put them trapped into it and bring about solutions wherever they exist.\" It adds that this network \"want(s) to see how IT are being used for the benefit of the dis-empowered, one who has no purchasing power to tempt the market to build solutions for him or her.\"\n\nBytesForAll has taken online issues relating to IT and public health, disaster mitigation, non-English computing, mass education and the like. It described its activities thus; \"Each month an offline E-zine goes out to its interested readers. In doing so, it has managed to highlight a surprising number of often-unnoticed success stories from a region where access to computers is still a class privilege.\"\n\nIt has focused repeatedly on issues such as efforts to promote computing in regional languages through GNU/Linux; the Learn Foundation's experience in laying a knowledge pipeline in rural Bangladesh; PraDeshta's idea of deploying a broadband communication network in Bangladesh; SDNP Pakistan's success in developing a knowledge network within the country; Kothmale's implementation of innovative \"community radio\" services in rural Sri Lanka, among others.\n\nThemes it has also focused on include IT-for-public health, Free/Libre and Open Source Software (FLOSS), ICT and human rights, emerging ICT technologies, community radio concerns, ICT for poverty alleviation, ICT for mass education, the knowledge society, local language computing initiatives, the \"digital divide\" generally, ongoing conferences and seminars in the region, and e-governance issues.\n\n"}
{"id": "26663730", "url": "https://en.wikipedia.org/wiki?curid=26663730", "title": "Critical Mass Energy Project", "text": "Critical Mass Energy Project\n\nThe Critical Mass Energy Project was formed by Ralph Nader in 1974 as a national anti-nuclear umbrella group. It was probably the largest national anti-nuclear group in the United States, with several hundred local affiliates and an estimated 200,000 supporters. Part of Nader's support comes from a Green agenda and the belief that \"the most important office in America for anyone to achieve is full-time citizen.\" The organization's main efforts were directed at lobbying activities and providing local groups with scientific and other resources to campaign against nuclear power.\n\nThe first national anti-nuclear conference, \"Critical Mass '74\" was held in Washington D.C. under the sponsorship of Ralph Nader. Workshops were held and groups throughout the United States learned about forming anti-nuclear organizations. At about the same time, Karen Silkwood, a nuclear plant worker, was killed in a car accident while investigating her nuclear energy company. There was speculation that the accident may have been intended.\n\nThe second Critical Mass conference was held in November 1975, and this involved a candlelight vigil in front of the White House for Karen Silkwood.\n\n"}
{"id": "2349940", "url": "https://en.wikipedia.org/wiki?curid=2349940", "title": "Cyclooctatetraene", "text": "Cyclooctatetraene\n\n1,3,5,7-Cyclooctatetraene (COT) is an unsaturated derivative of cyclooctane, with the formula CH. It is also known as [8]annulene. This polyunsaturated hydrocarbon is a colorless to light yellow flammable liquid at room temperature. Because of its stoichiometric relationship to benzene, COT has been the subject of much research and some controversy.\n\nUnlike benzene, CH, cyclooctatetraene, CH, is not aromatic, although its dianion, (cyclooctatetraenide), is. Its reactivity is characteristic of an ordinary polyene, i.e. it undergoes addition reactions. Benzene, by contrast, characteristically undergoes substitution reactions, not additions.\n\n1,3,5,7-Cyclooctatetraene was initially synthesized by Richard Willstätter in Munich in 1905:\n\nWillstätter noted that the compound did not exhibit the expected aromaticity. Between 1939 and 1943, chemists throughout the US unsuccessfully attempted to synthesize COT. They rationalized their lack of success with the conclusion that Willstätter had not actually synthesized the compound but instead its isomer, styrene. Willstätter responded to these reviews in his autobiography, where he noted that the American chemists were 'untroubled' by the reduction of his cyclooctatetraene to cyclooctane (a reaction impossible for styrene). During World War 2, Walter Reppe at BASF Ludwigshafen developed a simple, one-step synthesis of cyclooctatetraene from acetylene, providing material identical to that prepared by Willstätter. Any remaining doubts on the accuracy of Willstätter's original synthesis were resolved when Arthur C. Cope and co-workers at MIT reported, in 1947, a complete repetition of the Willstätter synthesis, step by step, using the originally reported techniques. They obtained the same cyclooctatetraene, and they subsequently reported modern spectral characterization of many of the intermediate products, again confirming the accuracy of Willstätter's original work.\n\nEarly studies demonstrated that COT did not display the chemistry of an aromatic compound. \nThen, early electron diffraction experiments concluded that the C-C bond distances were identical. \nHowever, X-ray diffraction data from H. S. Kaufman demonstrated cyclooctatetraene to adopt several conformations and to contain two distinct C–C bond distances. \nThis result indicated that COT is an annulene with fixed alternating single and double C-C bonds.\n\nIn its normal state, cyclooctatetraene is non-planar and adopts a tub conformation with angles C=C−C = 126.1° and C=C−H = 117.6°. The point group of cyclooctatetraene is D.\n\nIn its planar transition state, D transitional state is more stable than D transitional state due to Jahn–Teller effect.\n\nRichard Willstätter's original synthesis (4 consecutive elimination reactions on a cyclooctane framework) gives relatively low yields. Reppe's synthesis of cyclooctatetraene, which involves treating acetylene at high pressure with a warm mixture of nickel cyanide and calcium carbide, was much better, with chemical yields near 90%:\n\nCOT can also be prepared by photolysis of barrelene, one of its structural isomers, the reaction proceeding via another isolable isomer, semibullvalene. COT derivatives can also be synthesised by way of semibullvalene intermediates. In the sequence illustrated below, octaethylcyclooctatetraene (CEt) is formed by thermal isomerisation of octaethylsemibullvalene, itself formed by copper(I) bromide mediated cyclodimerisation of 1,2,3,4-tetraethyl-1,4-dilithio-1,3-butadiene.\n\nBecause COT is unstable and easily forms explosive organic peroxides, a small amount of hydroquinone is usually added to commercially available material. Testing for peroxides is advised when using a previously opened bottle; white crystals around the neck of the bottle may be composed of the peroxide, which may explode when mechanically disturbed.\n\nCyclooctatetraene has been isolated from certain fungi.\n\nThe π bonds in COT react as usual for olefins, rather than as aromatic ring systems. Mono- and polyepoxides can be generated by reaction of COT with peroxy acids or with dimethyldioxirane. Various other addition reactions are also known. Furthermore, polyacetylene can be synthesized via the ring-opening polymerization of cyclooctatetraene. COT itself—and also analogs with side-chains—have been used as metal ligands and in sandwich compounds.\n\nCyclooctatetraene also undergoes rearrangement reactions to form aromatic ring systems. For instance, oxidation with aqueous mercury(II) sulfate forms phenylacetaldehyde and photochemical rearrangement of its mono-epoxide forms benzofuran.\n\nCOT readily reacts with potassium metal to form the salt KCOT, which contains the dianion . The dianion is both planar and octagonal in shape and aromatic with a Hückel electron count of 10.\n\nCyclooctatetraene forms organometallic complexes with some metals, including yttrium and lanthanides. One-dimensional Eu–COT sandwiches have been described as nanowires. The sandwich compounds uranocene (U(COT)) and bis(cyclooctatetraene)iron (Fe(COT)) are known.\n\nFe(COT), when refluxed in toluene with dimethyl sulfoxide and dimethoxyethane for 5 days, is found to form magnetite and crystalline carbon also containing carbon nanotubes.\n\n"}
{"id": "6354378", "url": "https://en.wikipedia.org/wiki?curid=6354378", "title": "Drug delivery", "text": "Drug delivery\n\nDrug delivery refers to approaches, formulations, technologies, and systems for transporting a pharmaceutical compound in the body as needed to safely achieve its desired therapeutic effect. It may involve scientific site-targeting within the body, or it might involve facilitating systemic pharmacokinetics; in any case, it is typically concerned with both quantity and duration of drug presence. Drug delivery is often approached via a drug's chemical formulation, but it may also involve medical devices or drug-device combination products. Drug delivery is a concept heavily integrated with dosage form and route of administration, the latter sometimes even being considered part of the definition.\n\nDrug delivery technologies modify drug release profile, absorption, distribution and elimination for the benefit of improving product efficacy and safety, as well as patient convenience and compliance. Drug release is from: diffusion, degradation, swelling, and affinity-based mechanisms. Some of the common routes of administration include the enteral (gastrointestinal tract), parenteral (via injections), inhalation, transdermal, topical and oral routes. . Many medications such as peptide and protein, antibody, vaccine and gene based drugs, in general may not be delivered using these routes because they might be susceptible to enzymatic degradation or can not be absorbed into the systemic circulation efficiently due to molecular size and charge issues to be therapeutically effective. For this reason many protein and peptide drugs have to be delivered by injection or a nanoneedle array. \nFor example, many immunizations are based on the delivery of protein drugs and are often done by injection.\n\nCurrent efforts in the area of drug delivery include the development of targeted delivery in which the drug is only active in the target area of the body (for example, in cancerous tissues), sustained release formulations in which the drug is released over a period of time in a controlled manner from a formulation, and methods to increase survival of peroral agents which must pass through the stomach's acidic environment. In order to achieve efficient targeted delivery, the designed system must avoid the host's defense mechanisms and circulate to its intended site of action. Types of sustained release formulations include liposomes, drug loaded biodegradable microspheres and drug polymer conjugates. Survival of agents as they pass through the stomach typically is an issue for agents which cannot be encased in a solid tablet; one research area has been around the utilization of lipid isolates from the acid-resistant archaea \"Sulfolobus islandicus\", which confers on the order of 10% survival of liposome-encapsulated agents.\n\n\n"}
{"id": "49256303", "url": "https://en.wikipedia.org/wiki?curid=49256303", "title": "EcoRI", "text": "EcoRI\n\n\"Eco\"RI (pronounced \"eco R one\") is a restriction endonuclease enzyme isolated from species \"E. coli.\" The \"Eco\" part of the enzyme's name originates from the species from which it was isolated, while the R represents the particular strain, in this case RY13. The last part of its name, the I, denotes that it was the first enzyme isolated from this strain. \"Eco\"RI is a restriction enzyme that cleaves DNA double helices into fragments at specific sites. It is also a part of the restriction modification system.\n\nIn molecular biology it is used as a restriction enzyme. \"Eco\"RI creates 4 nucleotide sticky ends with 5' end overhangs of AATT. The nucleic acid recognition sequence where the enzyme cuts is G/AATTC, which has a palindromic, complementary sequence of CTTAA/G. The / in the sequence indicates which phosphodiester bond the enzyme will break in the DNA molecule. Other restriction enzymes, depending on their cut sites, can also leave 3' overhangs or blunt ends with no overhangs.\n\n\"Eco\"RI contains the PD..D/EXK motif within its active site like many restriction endonucleases.\n\nThe enzyme is a homodimer of a 31 kilodalton subunit consisting of one globular domain of the α/β architecture. Each subunit contains a loop which sticks out from the globular domain and wraps around the DNA when bound.\n\"Eco\"RI has been cocrystallized with the sequence it normally cuts. This crystal was used to solve the structure of the complex . The solved crystal structure shows that the subunits of the enzyme homodimer interact with the DNA symmetrically. In the complex, two α-helices from each subunit come together to form a four-helix bundle. On the interacting helices are residues Glu144 and Arg145, which interact together, forming a crosstalk ring that is believed to allow the enzyme's two active sites to communicate.\n\nRestriction enzymes, such as \"Eco\"RI, are used in a wide variety of molecular genetics techniques including cloning, DNA screening and deleting sections of DNA \"in vitro\". Restriction enzymes, like \"Eco\"RI, that generate sticky ends of DNA are often used to cut DNA prior to ligation, as the sticky ends make the ligation reaction more efficient. \"Eco\"RI can exhibit non-site-specific cutting, known as star activity, depending on the conditions present in the reaction. Conditions that can induce star activity when using \"Eco\"RI include low salt concentration, high glycerol concentration, excessive amounts of enzyme present in the reaction, high pH and contamination with certain organic solvents. The cut made by the Eco RI enzyme produces sticky ends on the vector mainly plasmids or viral DNA's. \n\n"}
{"id": "5421368", "url": "https://en.wikipedia.org/wiki?curid=5421368", "title": "Elsie Eaves", "text": "Elsie Eaves\n\nElsie Eaves (May 5, 1898 – March 27, 1983) was the first female associate member of the American Society of Civil Engineers (ASCE) and a founding member of the American Association of Cost Engineers (now AACE International; the Association for the Advancement of Cost Engineering).\n\nThe Idaho Springs, Colorado-born Eaves earned her civil engineering degree at the University of Colorado at Boulder in 1920. Eaves began her engineering experience before she received her university degree. She was a draftsman for the United States Bureau of Public Roads in Denver, Colorado, and then the Denver and Rio Grande Western Railroad Company. In 1926 she started working for McGraw-Hill in New York City for the Engineering News-Record Department.Also Eaves was publication and sales manager of the McGraw-Hill Construction Daily. In 1927, she was the first woman admitted to full membership to the American Society of Civil Engineers. In 1945, she became the manager of \"Business News\". She was the first woman to be admitted to the American Association of Cost Engineers in 1957. She retired in 1963, but continued practicing as an adviser to the National Commission on Urban Affairs on the subject of housing costs. She advised the International Executive Service Corps about construction costs in Iran.\n\nIn 1974, she received the George Norlin Silver Medal, the highest alumni award given by the University of Colorado and, in 1979, she was the first woman to receive an honorary lifetime membership to the American Association of Cost Engineers. Elsie Eaves died March 27, 1983 in Roslyn, New York, aged 84.\n\n\n\n"}
{"id": "3574347", "url": "https://en.wikipedia.org/wiki?curid=3574347", "title": "Endohedral hydrogen fullerene", "text": "Endohedral hydrogen fullerene\n\nEndohedral hydrogen fullerene (H@C) is an endohedral fullerene containing molecular hydrogen. This chemical compound has a potential application in molecular electronics and was synthesized in 2005 at Kyoto University by the group of Koichi Komatsu. Ordinarily the payload of endohedral fullerenes are inserted at the time of the synthesis of the fullerene itself or is introduced to the fullerene at very low yields at high temperatures and high pressure. This particular fullerene was synthesised in an unusual way in three steps starting from pristine C fullerene: cracking open the carbon framework, insert hydrogen gas and zipping up by organic synthesis methods.\n\n\"Scheme 1\" presents an overview of the first step, the creation of a 13 membered ring orifice on the fullerene surface. A 1,2,4-triazine 2 is fitted with two phenyl groups and a pyridine group for reasons of solubility and reacted in 1,2-dichlorobenzene with pristine C fullerene 2 in a Diels-Alder reaction at high temperature and for an extended reaction time. In this reaction nitrogen is expulsed and an 8-membered ring is formed (3). This orifice is further extended by reaction with singlet oxygen in carbon tetrachloride which causes one of the ring alkene groups to oxidize to a ketone. The 12-ring is extended to a 13-ring by reaction with elemental sulfur in presence of tetrakis(dimethylamino)ethylene.\n\nThe proposed reaction mechanism is depicted in a plat surface rendition in \"scheme 2\". In the first step the triazine reacts with the fullerene in a Diels-Alder reaction. In the second step nitrogen is expulsed from the DA adduct 2 resulting in the formation of a fused aza-cyclohexadiene ring followed by a [4+4]cycloaddition to an intermediate 4 with two cyclopropane rings. This intermediate quickly rearranges in a retro [2+2+2]cycloaddition to the 8 membered ring product 5. In silico calculations show that the electrons in the HOMO reside primarily in the double bonds of the butadiene part of the ring and indeed singlet oxygen reacts at these positions through the dioxetane intermediate 6 with alkene cleavage to diketone 7 (only one isomer shown). Elemental sulfur S is inserted into the single bond of the diene group leading to the extension of the ring to 13 atoms (structures 8 and 9 are identical). Tetrakis(dimethylamino)ethylene activates this bond for electrophilic sulfur addition either by one-electron reduction or by complexation.\n\nFrom X-ray crystallography it is determined that the shape of the orifice in the sulfur compound is roughly a circle. Inserting hydrogen in this compound is an easy step taking place with 100% efficiency. Zipping up the orifice is a reversal of the steps required to open the cage. Care must be taken to keep the reaction conditions below 160 °C on order to prevent hydrogen from escaping. m-CPBA oxidizes the sulfur group to a sulfoxide group which can then be extracted from the ring by a photochemical reaction under visible light in toluene. The two ketone groups are re-coupled in a McMurry reaction with titanium tetrachloride and elemental zinc. The reverse cycloadditions take place at 340 °C in a vacuum splitting of 2-cyanopyridine and diphenylacetylene resulting in the formation of H@C at a 40% chemical yield starting from pristine fullerene.\n\nH@C is found to be a stable molecule. it survives 10 minutes at 500 °C and shows the same chemical reactivity as empty C. The electronic properties are also largely unaffected.\n\nThe process of hydrogen introduction and release can be facilitated by increasing the size of the orifice. This can be done by replacing sulfur by selenium (sodium thiolate, Se) exploiting larger C-Se bond length. Filling cracked-open fullerene now takes 8 hours at 190 °C at 760 atmospheres (77 MPa) of hydrogen and release between 150 °C and 180 °C is three times as fast compared to the sulfur analogue. The activation energy for release is lowered by 0.7 kcal/mol to 28.2 kcal/mol (2.9 to 118 kJ/mol).\n\nThere is evidence that hydrogen in the fullerene cage is not completely shielded from the outside world as one study found that H@C is more efficient at quenching singlet oxygen than empty C.\n"}
{"id": "3442755", "url": "https://en.wikipedia.org/wiki?curid=3442755", "title": "Express Payment System", "text": "Express Payment System\n\nThe Express Payment System, more commonly known as the EPS, was the EFTPOS system originally of the ATM cards of Bank of the Philippine Islands and its subsidiaries, BPI Family Savings Bank and BPI Direct Savings Bank. Today, it is the EFTPOS system of the Expressnet interbank network in the Philippines. The system is the most popular EFTPOS system for ATM cardholders in the Philippines and is accepted nationwide. Rivals of the network include MegaLink's PayLink and the similarly named BancNet Payment System (BPS).\n\nAlthough the Philippines is largely still a cash-based society, the EPS is slowly getting widespread prominence. Since its launch, it has expanded to be found in many stores nationwide and has since expanded from its roots with BPI, which also pioneered Expressnet. In fact, the EPS is no longer limited to shopping: the EPS can also be used for things such as paying doctor's fees or paying for a haircut with the swipe of one's ATM card. Currently, there are more than 13,000 EPS merchants throughout the Philippines.\n\nIn 2005, the EPS expanded to include the ATM cards of Banco de Oro and Land Bank of the Philippines, both Expressnet members. However, many EPS merchants claim that their EPS terminal works only with BPI ATM cards or do not know how to use Banco de Oro or Landbank cards with the EPS terminal; this is because virtually all EPS terminals are BPI terminals (Banco de Oro and Landbank EPS terminals debuted just recently). Also, Banco de Oro and Landbank ATM cards, unlike BPI ATM cards, cannot be directly swiped on an EPS terminal. In order to use these cards, an EPS merchant must enter the number 7 on the main screen then swipe the card to use these ATM cards. As of 2005, HSBC and its subsidiary, HSBC Savings Bank, although both members of Expressnet, do not participate in the EPS system, meaning that HSBC and HSBC Savings Bank ATM cards cannot be used on an EPS terminal.\n\nIn addition to functioning as an ATM EFTPOS terminal, EPS terminals also accept BPI credit, debit and prepaid cards. However, these transactions are considered as BPI Express Credit transactions and not as EPS transactions.\n\nBanco de Oro has its own terminal independent of the EPS either branded under the SM name or under its own name. Terminals under either name accept all Banco de Oro cards (Smarteller ATM card and derivatives, BDO Gift Card, BDO Cash Card and BDO credit cards) and have the functions of an EPS terminal. Terminals under the SM name, however, have extended capabilities such as having the capabilities of an EPS, PayLink and BPS terminal (and even the ATM cards of non-affiliated banks) all in one as well as the acceptance of other banks' credit, debit and prepaid cards.\n\nThe terminal can be found depending on the name. Terminals under the SM name can be found in all SM stores, such as their supermarkets, department stores and stores such as Surplus Shop and Toy Kingdom. Terminals under the Banco de Oro name can be found in businesses that accept Banco de Oro cards before Banco de Oro joined the EPS. Some merchants though still use the Banco de Oro terminal.\n\n\n"}
{"id": "27547393", "url": "https://en.wikipedia.org/wiki?curid=27547393", "title": "For Inspiration and Recognition of Science and Technology", "text": "For Inspiration and Recognition of Science and Technology\n\nFor Inspiration and Recognition of Science and Technology (FIRST) is an international youth organization that operates the FIRST Robotics Competition, FIRST LEGO League, FIRST LEGO League Jr., and FIRST Tech Challenge competitions.\nFounded by Dean Kamen and Woodie Flowers in 1989, its expressed goal is to develop ways to inspire students in engineering and technology fields. Its philosophy is expressed by the organization as \"coopertition\" and \"gracious professionalism\".\nFIRST also operates FIRST Place, a research facility at FIRST headquarters in Manchester, New Hampshire, where it holds educational programs and day camps for students and teachers.\n\nFIRST operates as a non-profit public charity corporation. It licenses qualified teams, usually affiliated with schools or other youth organizations, to participate in its competitions. The teams in turn pay a fee to FIRST; these fees, the majority of which are redistributed to pay for teams' kit of parts and other services, consist of the majority of FIRST's revenue.\n\nThe supreme body of FIRST is its board of directors, which includes corporate executives and former government officials. FIRST also has an executive advisory board and several senior advisors; these advisors include engineers, involved volunteers, and other senior organizers. Day-to-day operations are run by a senior management team, consisting of a president and five vice presidents.\n\nThe first and highest-scale program developed through FIRST is the FIRST Robotics Competition (FRC), which is designed to inspire high school students to become engineers by giving them real world experience working with engineers to develop a robot. The inaugural FIRST Robotics Competition was held in 1992 in the Manchester Memorial High School gymnasium. , over 3,000 high school teams totaling over 46,000 students from Australia, Brazil, Canada, France, Turkey, Israel, Mexico, the Netherlands, the United States, the United Kingdom, and more compete in the annual competition.\n\nThe competition challenge changes each year, and the teams can only reuse certain components from previous years. The robots weigh at most , without batteries and bumpers. The kit issued to each team contains a base set of parts. Registration and the kit of parts together cost about US$6,000. In addition to that, teams are allowed to spend another $3,500 on their robot. The purpose of this rule is to lessen the influence of money on teams' competitiveness. Details of the game have been released on the first Saturday in January (except when that Saturday falls on January 1 or 2), and the teams have been given six weeks to construct a robot that can accomplish the game's tasks.\n\nIn 2011, teams participated in 48 regional and district competitions throughout March in an effort to qualify for the FIRST Championship in St. Louis in April. Previous years' Championships have been held in Atlanta, Georgia, Houston, Texas and at Walt Disney World's Epcot. On October 7, 2009, FIRST announced that the Championship Event will be held in St. Louis, Missouri for 2011 through 2013.\nEach year the FIRST Robotics Competition has scholarships for the participants in the program. In 2011, there were over $14 million worth of scholarships from more than 128 colleges and universities, associations, and corporations.\n\nThe district competition system was introduced in Michigan and as of 2017 has expanded to include districts in the Pacific Northwest, the Mid-Atlantic, the Washington DC area, New England, Georgia, North Carolina, Ontario, and Israel. When they were created in 2017, the Ontario and Israel districts became the first districts outside of the United States. The district competition system changed the traditional \"regional\" events by allowing teams to compete in multiple smaller events and using an associated ranking algorithm to determine which teams would advance to the next level of the competition. In general, there have been pushes to move more regions to the districts system; California, Texas, and New York have especially been pushed to move to the district system.\n\nThe FIRST Tech Challenge (FTC), formerly FIRST Vex Challenge (FVC), is a mid-level robotics competition announced by FIRST on March 22, 2005. According to FIRST, this competition was designed to be a more accessible and affordable option for schools. FIRST has also said that the FTC program was created for those of an intermediate skill level. FIRST Tech Challenge robots are approximately one-third the scale of their FRC counterparts. The FTC competition is meant to provide a transition for students from the FLL competition to the FRC competition. FTC was developed for the Vex Robotics Design System, which is available commercially.\n\nThe 2005 FVC pilot season featured a demonstration of the FIRST Vex Challenge using a 1/3 linear scale mock-up of the 2004 FRC Competition, . For their 2005-2006 Pilot Season, FVC teams played the Half-Pipe Hustle game using racquet balls and ramps.\n\nFor the 2006-2007 FTC Season, the FIRST Tech Challenge teams competed in the Hangin'-A-Round challenge using softballs, rotating platforms, a hanging bar, and a larger 'Atlas' ball which is significantly larger than most Vex robots and harder to manipulate. Competitions were held around the United States, Canada, and Mexico.\n\nFor the 2008-2009 FTC season, a new kit was introduced, as FIRST moved away from the VEX platform and worked with several different vendors to create a custom kit and control system for FTC known as Tetrix. Based around the LEGO Mindstorms NXT \"brain\" and including secondary specialized controllers to overcome the limitations of the NXT, teams use a Bluetooth link between the NXT and a laptop running FTC driver station software. A team's drivers then use either one or two USB gamepads to control their robots.\n\nFor the 2015-2016 FTC season, in a partnership with Qualcomm, the LEGO Mindstorms NXT was replaced as the \"brain\" of the robot by an android device which communicates to a separate \"driver station\" android device via Wifi Direct. In addition, students were allowed to use either MIT App Inventor or Android Studio (Java language) to program their robots.\n\nIn 1998, the FIRST LEGO League (FLL), a program similar to the FIRST Robotics Competition, was formed. It is aimed at 9 to 14-year-old students and utilizes LEGO Mindstorms sets (EV3, NXT, RCX) to build palm-sized LEGO robots, which are then programmed using either the ROBOLAB software (RCX-based systems) or Mindstorms NXT or EV3 software (for NXT or EV3-based systems respectively) to autonomously compete against other teams. The ROBOLAB software is based on National Instruments' LabVIEW industrial control engineering software. The combination of interchangeable LEGO parts, computer 'bricks', sensors, and the aforementioned software, provide preteens and teenagers with the capability to build simple models of real-life robotic systems. This competition also utilizes a research element that is themed with each year's game, and deals with a real-world situation for students to learn about through the season.\n\nThe simplistic nature of its games, its relatively low team startup costs, and its association with the Lego Group mean that it is the most extensive of all FIRST competitions, despite a lower profile and fewer sponsors than FIRST Tech Challenge or FIRST Robotics Competition. In 2009, 14,725 teams from 56 countries participated in local, regional, national, and international competitions, compared with around 1,600 teams in roughly 10 countries for FRC.\n\nFIRST LEGO League Jr. is a variation of the FIRST LEGO League, aimed towards elementary school children, in which kids ages 5 to 8 build LEGO models dealing with that year's FLL challenge. At least one part of a model has a moving component. The teams participate in exhibitions around the country, where they demonstrate and explain their models and research for award opportunities.\n\nThe FIRST Championship is the annual event which celebrates the finale of all of their programs by bringing them all together for their final rounds in the same event. The FIRST Championship was split into two locations: St. Louis, Missouri and Houston, Texas in 2017 due to the rise in teams. From 2018 through 2020, the FIRST Championships will be held in Detroit, Michigan and Houston, Texas At the 2014 Championship, FIRST announced changes to the 2015 structure that will bring a more \"Olympic Village\" feeling, and involves a rearrangement of the programs around the city.\n\nFIRST itself is a self-supporting organization; however, individual teams typically rely on outside funding sources. It also takes significant outside funds to run regional events and the FIRST Championship. In 2010, FIRST was a recipient of a Google Project 10^100 grant.\n\nTeams may request that team members, whether mentors or students, contribute to the costs of running a team. For example, members may pay a fee or donate tools and facilities.\n\nTeams frequently give other teams support. This may mean providing funds, tools, or facilities. Gracious professionalism and Coopertition are core tenets of the FIRST philosophy.\n\nGracious Professionalism is a major belief in the FIRST community. At every regional and national competition, the judges look for teams to be graciously professional. What gracious professionalism is all about is \"competing on an even playing field\". That means that each team wants their competition at the best. The way the team system is set up is that every team is matched up with two other teams per match at random. Therefore, a team's ally in one match may become an opponent in the next match. Traditionally, outside of FIRST, when one shares resources in a competition, one only does so with their allies.\n\nHowever, with the element of gracious professionalism, one would share resources with their opponent as well. For example, if a team needs a part or tool to fix their robot, it is expected that any team, even an opposing team would give that team a hand in order to compete. \nThis helps student learn that success is in learning and helping others no matter the circumstances. With this in mind, the judges give a Gracious Professionalism award at every FIRST Robotics Competition tournament, to a team that shows outstanding gracious professionalism.\n\nThe term \"Gracious Professionalism\" was created by Dr. Woodie Flowers, FIRST National Advisor and Pappalardo Professor Emeritus of Mechanical Engineering, Massachusetts Institute of Technology.\n\nThe most common method of monetary and resource sponsorship teams comes through the community surrounding the team. Since the majority of teams are based around a school or a school district, schools often provide the infrastructure needed to run a team. Local governments and individual citizens may provide funds and other support to teams. Local universities and colleges often give significant funds to teams.\n\nCorporate donations and grants usually provide the majority of a mature team's funds. Major donors include BAE Systems, Google, Raytheon, and National Instruments.\n\nEach year during his speech at the kickoff event, founder Dean Kamen gives the student participants a homework assignment. It often involves spreading the word about FIRST in various ways, such as increasing attendance at regionals (2005), mentoring rookie teams, making sure that FIRST-specific scholarships are applied for (2004), and researching the capabilities of motors and disseminating that information to other teams (2006). In 2007, Dean's homework was for each team to contact their government officials (e.g. mayors, legislators, governors, federal officials) and invite them to a FIRST regional or the championship to expose them to the competition and increase the level of political awareness of FIRST. In 2008, it was to inform the media more about FIRST. In 2009, the homework was for each team to have all students, mentors, and other persons involved with their team (past or present) register with FIRST. One goal of this registration process was to provide FIRST with data to demonstrate that many people had benefited from their experiences in FIRST robotics and to encourage more funding of robotics-related events.\n\nAt the World Championship in Atlanta, speakers have included former President of the United States George Herbert Walker Bush in 2008, and United States Secretary of Education Arne Duncan in 2010. In 2010, former U.S. Undersecretary of Commerce and Director of the U.S. Patent and Trademark Office Jon Dudas was selected to be the President of FIRST.\n\nAt the Championship in St. Louis, President of the United States Barack Obama has spoken via a pre-recorded message every year from 2011-2014.\n\nFIRST has received the attention of politicians in Canada as well. Ontario MPP Bob Delaney and Ontario MPP Vic Fedeli have made remarks in the Legislative Assembly of Ontario regarding their FRC experiences and showing their support.\n\nNASA, through its Robotics Alliance Project, is a major supporter of FIRST.\n\nFIRST seeks to promote a philosophy of teamwork and collaboration among engineers and encourages competing teams to remain friendly, helping each other out when necessary. Terms frequently applied to this ethos are \"Gracious Professionalism\" and \"Coopertition\"; terms coined by Woodie Flowers and Kamen that support respect towards one's competitors and integrity in one's actions. The concept of Gracious Professionalism grew from a robotics class that Flowers taught at Massachusetts Institute of Technology. Coopertition is patented under US Patent 7,507,169 by Dean Kamen.\n\nNote: All years indicate the year that the championship for that game was held.\n\n"}
{"id": "314366", "url": "https://en.wikipedia.org/wiki?curid=314366", "title": "H-infinity methods in control theory", "text": "H-infinity methods in control theory\n\nH\" (i.e. \"H\"-infinity\") methods are used in control theory to synthesize controllers to achieve stabilization with guaranteed performance. To use \"H\" methods, a control designer expresses the control problem as a mathematical optimization problem and then finds the controller that solves this optimization. \"H\" techniques have the advantage over classical control techniques in that they are readily applicable to problems involving multivariate systems with cross-coupling between channels; disadvantages of \"H\" techniques include the level of mathematical understanding needed to apply them successfully and the need for a reasonably good model of the system to be controlled. It is important to keep in mind that the resulting controller is only optimal with respect to the prescribed cost function and does not necessarily represent the best controller in terms of the usual performance measures used to evaluate controllers such as settling time, energy expended, etc. Also, non-linear constraints such as saturation are generally not well-handled. These methods were introduced into control theory in the late 1970s-early 1980s\nby George Zames (sensitivity minimization), J. William Helton (broadband matching),\nand Allen Tannenbaum (gain margin optimization).\n\nThe phrase \"H\" \"control\" comes from the name of the mathematical space over which the optimization takes place: \"H\" is the \"Hardy space\" of matrix-valued functions that are analytic and bounded in the open right-half of the complex plane defined by Re(\"s\") > 0; the \"H\" norm is the maximum singular value of the function over that space. (This can be interpreted as a maximum gain in any direction and at any frequency; for SISO systems, this is effectively the maximum magnitude of the frequency response.) \"H\" techniques can be used to minimize the closed loop impact of a perturbation: depending on the problem formulation, the impact will either be measured in terms of stabilization or performance.\n\nSimultaneously optimizing robust performance and robust stabilization is difficult. One method that comes close to achieving this is \"H\" loop-shaping, which allows the control designer to apply classical loop-shaping concepts to the multivariable frequency response to get good robust performance, and then optimizes the response near the system bandwidth to achieve good robust stabilization.\n\nCommercial software is available to support \"H\" controller synthesis.\n\nFirst, the process has to be represented according to the following standard configuration:\n\nThe plant \"P\" has two inputs, the exogenous input \"w\", that includes reference signal and disturbances, and the manipulated variables \"u\". There are two outputs, the error signals \"z\" that we want to minimize, and the measured variables \"v\", that we use to control the system. \"v\" is used in \"K\" to calculate the manipulated variables \"u\". Notice that all these are generally vectors, whereas P and K are matrices.\n\nIn formulae, the system is:\n\nIt is therefore possible to express the dependency of \"z\" on \"w\" as:\n\nCalled the \"lower linear fractional transformation\", formula_4 is defined (the subscript comes from \"lower\"):\n\nTherefore, the objective of formula_6 control design is to find a controller formula_7 such that formula_8 is minimised according to the formula_6 norm. The same definition applies to formula_10 control design. The infinity norm of the transfer function matrix formula_8 is defined as:\n\nwhere formula_13 is the maximum singular value of the matrix formula_14.\n\nThe achievable \"H\" norm of the closed loop system is mainly given through the matrix \"D\" (when the system \"P\" is given in the form (\"A\", \"B\", \"B\", \"C\", \"C\", \"D\", \"D\", \"D\", \"D\")). There are several ways to come to an \"H\" controller:\n\n\n"}
{"id": "35166137", "url": "https://en.wikipedia.org/wiki?curid=35166137", "title": "Heat and moisture exchanger", "text": "Heat and moisture exchanger\n\nHeat and Moisture Exchangers (HME) are devices used in mechanically ventilated patients intended to help prevent complications due to \"drying of the respiratory mucosa, such as mucus plugging and endotracheal tube (ETT) occlusion.\" HMEs are one type of commercial humidification system, which also include non-heated-wire humidifiers and heated-wire humidifiers.\n\nHMEs have been in clinical use for over 30 years.\n\nAn HME cassette plays a central part of lung rehabilitation after a total laryngectomy.\n\nHumidification and suctioning are necessary to manage secretions in patients on mechanical ventilation. According to Branson (2007), the optimal humidification level \"has been not well defined, but it is clear that in a patient with thick and copious secretions a heated humidifier is preferred to an HME\".\n\nIn patients with acute lung injury and acute respiratory distress syndrome conventional humidifiers are preferred to HMEs for improved elimination of carbon dioxide.\n\nAn HME has three purposes in laryngectomy:\nIn the lungs a temperature of 37 °C and 100% relative humidity (RH) is the ideal condition for the ciliary activity. If the conditions are too warm or cold, the cilia beat slower and at some point not at all. During normal nasal inspiration, air of 22 °C and 40% RH is conditioned into air of 32 °C and 99% RH at the level of the trachea.\n\nThe effect of the increased resistance (compared to stoma breathing without HME) in laryngectomy patients is poorly understood, but HMEs add a variable resistance to the airflow resistance, depending on the flow rate, though the outcomes of studies are not consistent.\n\nHME cassettes with an electrostatic filter are designed to enhance the protection against airborne microbes to help to reduce the transfer of viruses and bacteria. Wearing an HME cassette does not compensate for the loss of upper airway filtration of smaller particles such as bacteria and viruses; the pores of the HME filter are larger than the diameter of the infectious particles. Only larger particles are filtered by the HME.\n\nThe basic components of heat and moisture exchangers are foam, paper, or a substance which acts as a condensation and absorption surface. The material is often impregnated with hygroscopic salts such as calcium chloride, to enhance the water-retaining capacity. HMEs used for laryngectomees are mostly hygroscopic. HMEs can vary in size but they are designed to fit all adhesives or other attachment devices within a certain product line. HME cassettes for tracheotomy patients vary in size and are usually a bit larger than for laryngectomy patients. Air openings are at the side or at the front of the HME. Some designs use crossbars to prevent clothing from blocking. Usually a rim on the lid helps to find the correct finger position for occlusion.\n\nA hands-free HME enables laryngectomees to speak without requiring finger occlusion. The device consists of a combination of HME and an automatic speaking valve, which closes automatically, when exhaling air for speaking, enabling the pulmonary air to be diverted through the voice prosthesis into the esophagus. It reopens automatically, when exhalation decreases. Beside that the hands-free HME enables easy removal in case of coughing, or even an adjustable cough relief valve, to release the air that is built up during coughing. In some devices, speech membranes in different strengths can accommodate different speaking pressures.\n\nHME devices with a lower airflow resistance make them suitable for physical exercise or when adapting to the breathing resistance for patients that have not used any stoma protection before and start using an HME or have not used an HME for a longer time.\n\nAs antimicrobial filters, an HME is not considered to be an efficient barrier for microorganisms due to a relatively poor bacterial filtration capacity. Some HMEs provide an electrostatic filter for some protection from small particles and airborne microorganisms.\n"}
{"id": "8938988", "url": "https://en.wikipedia.org/wiki?curid=8938988", "title": "Hyfrecator", "text": "Hyfrecator\n\nThe word \"hyfrecator\" is a portmanteau derived from “high-frequency eradicator.” It was introduced as a brand name for a device introduced in 1940 by the Birtcher Corporation of Los Angeles. Birtcher also trademark registered the name Hyfrecator in 1939, and rights to the registered trademark were acquired by ConMed Corporation when it acquired Birtcher in 1995. Today, machines with the name \"Hyfrecator\" are sold only by ConMed Corporation. However, the word \"hyfrecator\" is sometimes used as a genericized trademark to refer to any dedicated non-ground-return electrosurgical apparatus, and a number of manufacturers now produce such machines, although not by this name.\n\nThe hyfrecator differs from other electrosurgical devices primarily in being low-powered that is not intended for cutting tissue, or for use in other than conscious patients. The reason is that the hyfrecator does not use a dispersive return pad or \"patient plate\" (also sometimes loosely referred to in electrosurgery as \"ground pad.\") It thus either passes a very low-powered current between forceps tips (bipolar output), or else passes an A.C. current between a single pointed metal electrode probe and the patient, with only the patient's self-capacitance providing a current sink (this is equivalent to considering displacement current to be the return current).\n\nIn the latter mode, the patient must sit or lie on an insulated table, much as in the case with objects to be charged electrostatically with high-voltage D.C. (as from a Van de Graaff generator, for example). Stray ground paths between the patient and foreign conductors (such as a metal table leading somewhere to earth-ground) can offer another capacitative reservoir besides the patient, and burns out of the area of treatment may thus result, from current passing between patient and the earth-ground. For this reason, hyfrecation and all non-ground-pad electrosurgery is performed only on conscious patients, who would be aware of the burn and discomfort from an unwanted earth-ground path. (In types of electrosurgery which \"do\" employ a ground-pad, the ground-pad path serves as such a low resistance ground to the machine, that extraneous other ground paths become unimportant, and thus with proper precautions these methods can, and often are, used on anesthetized patients).\n\nBecause hyfrecation is always a relatively low-power modality, it can be used in some situations (such as very small nevus removal or skin tag removal) without local anaesthesia. In many other uses to destroy larger lesions, a local anesthetic injection or regional nerve block is used. The pain from hyfrecation is due to the burning of tissue, and the pain of electric current is absent, due to the high (radio) frequency which does not directly cause discharge of nerves.\n\nAlthough the hyfrecator is not used primarily to cut tissue, it may be used in a secondary capacity to control bleeding, after tissue is cut by a standard surgical scalpel, or else it may be used to partly destroy superficial tissue, that is then removed by the scraping action of a curette. These are done under local anesthesia. An example of such a combination procedure is the standard method of electrodesiccation and curettage used by dermatologists to destroy skin cancers.\n\nHyfrecators are used in two principal modes:\n\n\n\n"}
{"id": "25306618", "url": "https://en.wikipedia.org/wiki?curid=25306618", "title": "IAEA safeguards", "text": "IAEA safeguards\n\nInternational Atomic Energy Agency (IAEA) Safeguards are a system of inspection and verification of the peaceful uses of nuclear materials as part of the Nuclear Non-Proliferation Treaty (NPT), supervised by the International Atomic Energy Agency.\n\nSafeguards activities are undertaken by the Department of Safeguards, a separate department within the International Atomic Energy Agency. The Department is headed by Deputy Director General and Head of the Department of Safeguards Massimo Aparo. The mission statement of the Department of Safeguards is: \"The primary role of the {Safeguard} Department is to administer and implement IAEA safeguards. It also contributes to nuclear arms control and disarmament, by responding to requests for verification and technical assistance associated with related agreements and arrangements.\" The Department is organized into operations divisions, which include the inspectors that conduct safeguards inspections in the IAEA's member states to confirm that they are living up to their NPT commitments, and support divisions, that provide the tools and services for the safeguards inspectors to complete their mission. Safeguards inspections compare a state's nuclear program, as declared to the IAEA, to observed nuclear activities in the country. The Divisions of Operations are organized as follows:\n\n• Operations A: conducting safeguards inspections in East Asia and Australasia<br>\n• Operations B: conducting safeguards inspections in the Middle East (Southwest Asia), South Asia, Africa and the Americas; this geographic region also includes non-EU European states<br>\n• Operations C: conducting safeguards inspection in the European Union states, Russia and Central Asia<br>\n• Operations for verification in Iran (as stated in the 2015 Joint Comprehensive Plan of Action, known commonly as the Iranian Nuclear Deal)\n\nThe history of the IAEA safeguards begins at the foreground of the nuclear regime to which debate over the disposal of leftover fissile material was the primary concern. Dwight Eisenhower’s Atoms for Peace speech in 1953 was the first step towards establishing regulation of nuclear activity to ensure only peaceful purposes were driving scientific development. It proposed that states with leftover fissile material contribute to an international fuel bank. The IAEA was proposed in 1954 with the mission to control the distribution and disposal of used nuclear material. Negotiations of safeguards were controversial due to the idea that they would inhibit the promotion of nuclear energy. However, safeguards help solidify the line between using nuclear energy for peaceful purposes and creating weapons-grade material that could serve militant purposes. Though safeguards are only one part of the nuclear non-proliferation regime, they underpin inspection and verification, and provide assurance that proliferation is not occurring in states declared to be nuclear weaponized, as well as non-nuclear weapons states.\n\nInformation Circular 66 (INFCIRC 66) is an agreement between the IAEA and member states that provides for the conduct of limited safeguards within the member state. The member states identifies facilities that are made available for inspection.\n\nThe Treaty on the Nonproliferation of Nuclear Weapons (NPT) opened for signature in 1968 and entered into force in 1970. The NPT defines nuclear weapons states as the United States of America, the United Kingdom of Great Britain and Northern Ireland, the People’s Republic of China, the Russian Federation, and France. The treaty requires signatories to become members of the IAEA. Nuclear weapons states are responsible for working toward disarmament and non-nuclear weapons states must submit to IAEA safeguards. The treaty requires that non-nuclear weapons states conclude comprehensive safeguards agreements under INFCIRC 153. The NPT is the centerpiece of global efforts to prevent the further spread of nuclear weapons.\n\nSafeguards inspectors are first appointed by the IAEA’s Director General, then approved by the Board of Governors, designated by the State, and granted privileges and immunities by the member states in which they designated to perform inspections. Inspectors are responsible for conducting three verification activities which are; Design Information Verification (DIV), Inspection, and Complementary Access (CA).\n\nDesign Information Verification (DIV) entails confirmation of design features of a facility and verification of the design features to be accurate and valid. This activity is performed under the comprehensive safeguards agreement, by which all signatories adhere to the provision and regulation of safeguards.\n\nThe second activity required by comprehensive safeguards agreements is the inspection of facilities. The objective of an inspection is to verify that nuclear material is not diverted and facilities are not misused to make undeclared nuclear material.\n\nFinally, Complementary Access (CA) is performed under the allowance of the Additional Protocol. The objective for complementary access is to confirm the absence of undeclared nuclear activities/material, answer questions, resolve inconsistencies, and confirm decommissioned status.\n\nSafeguards are implemented on an annual cycle and include four fundamental processes:\n\n\nThe IAEA prepares a Safeguards Evaluation Report (SER) for each country and draws safeguards conclusions based on the information collected during inspections and through remote monitoring and information collection. Safeguards conclusions provide the international community assurance that States are complying with their agreements by following the safeguards obligations. In some cases, the conclusion is that safeguards were not conclusive. Safeguards conclusions are documented in the annual Safeguards Implementation Report which is presented to the Board of Governors at its June meeting.\n\nThe IAEA offers several useful services to member states including aide for officiating required documentation and assistance with safeguards measures.\n\n"}
{"id": "33848849", "url": "https://en.wikipedia.org/wiki?curid=33848849", "title": "ISO/TS 80004", "text": "ISO/TS 80004\n\nThe ISO/TS 80004 series of standards, from the International Organization for Standardization, describe vocabulary for nanotechnology and its applications. These were largely motivated by health, safety and environment concerns, many of them originally elaborated by Eric Drexler in his 1985 Engines of Creation and echoed in more recent research. The ISO standards simply describe vocabulary or terminology by which a number of critical discussions between members of various stakeholder communities, including the public and political leaders, can begin. Drexler, in Chapter 15 of his 1985 work, explained how such consultation and the evolution of new social media and mechanisms to make objective scientific determinations regardless of political and industrial and public pressures, would be important to the evolution of the field. Nonetheless, it took a quarter-century for the ISO to agree and eventually standardize on this terminology.\n\nReviews of the field often need to distinct various definitions of nanomaterials vs. mesomaterials, nanoscale objects from nanostructured materials (including nanoporous materials), and other confused terms. The intent of the ISO standards is to remove most potential for terminology clash especially when dealing with international regulatory synchronization. The standard currently consists of 11 published parts, while more parts are under preparation which addresses graphene and quantum phenomena.\n\nISO/TR 18401 provides plain language explanations of selected terms from the ISO 80004 series.\n\n\nISO/TS 80004-7:2011 provides consistent and unambiguous use of terms for healthcare professionals, manufacturers, consumers, technologists, patent agents, regulators, NGOs, and researchers, etc.\n\n"}
{"id": "271981", "url": "https://en.wikipedia.org/wiki?curid=271981", "title": "Ismail al-Jazari", "text": "Ismail al-Jazari\n\nBadīʿ az-Zaman Abū l-ʿIzz ibn Ismāʿīl ibn ar-Razāz al-Jazarī (1136–1206, , ) was a Muslim polymath: a scholar, inventor, mechanical engineer, artisan, artist and mathematician. He is best known for writing \"The Book of Knowledge of Ingenious Mechanical Devices\" () in 1206, where he described 100 mechanical devices, some 80 of which are trick vessels of various kinds, along with instructions on how to construct them.\n\nThe only biographical information known about him is contained in his famed \"Book of Knowledge of Ingenious Mechanical Devices\". Like his father before him, he served as chief engineer at the Artuklu Palace, the residence of the Mardin branch of the Artuqids which ruled across eastern Anatolia as vassals of the Zengid dynasty of Mosul and later of Ayyubid general Saladin.\n\nAl-Jazari was part of a tradition of artisans and was thus more a practical engineer than an inventor who appears to have been \"more interested in the craftsmanship necessary to construct the devices than in the technology which lay behind them\" and his machines were usually \"assembled by trial and error rather than by theoretical calculation.\" His \"Book of Knowledge of Ingenious Mechanical Devices\" appears to have been quite popular as it appears in a large number of manuscript copies, and as he explains repeatedly, he only describes devices he has built himself. According to Mayr, the book's style resembles that of a modern \"do-it-yourself\" book.\n\nSome of his devices were inspired by earlier devices, such as one of his monumental water clocks, which was based on that of a Pseudo-Archimedes. He also cites the influence of the Banū Mūsā brothers for his fountains, al-Saghani for the design of a candle clock, and Hibatullah ibn al-Husayn (d. 1139) for musical automata. Al-Jazari goes on to describe the improvements he made to the work of his predecessors, and describes a number of devices, techniques and components that are original innovations which do not appear in the works by his precessors.\n\nThe most significant aspect of al-Jazari's machines are the mechanisms, components, ideas, methods, and design features which they employ.\n\nA camshaft, a shaft to which cams are attached, was introduced in 1206 by al-Jazari, who employed them in his automata, water clocks (such as the candle clock) and water-raising machines. The cam and camshaft also appeared in European mechanisms from the 14th century.\n\nThe eccentrically mounted handle of the rotary quern-stone in fifth century BCE Spain that spread across the Roman Empire constitutes a crank. The earliest evidence of a crank and connecting rod mechanism dates to the 3rd century AD Hierapolis sawmill in the Roman Empire. The crank also appears in the mid-9th century in several of the hydraulic devices described by the Banū Mūsā brothers in their \"Book of Ingenious Devices\".\n\nIn 1206, al-Jazari invented an early crankshaft, which he incorporated with a crank-connecting rod mechanism in his twin-cylinder pump. Like the modern crankshaft, al-Jazari's mechanism consisted of a wheel setting several crankpins into motion, with the wheel's motion being circular and the pins moving back-and-forth in a straight line. The crankshaft described by al-Jazari transforms continuous rotary motion into a linear reciprocating motion, and is central to modern machinery such as the steam engine, internal combustion engine and automatic controls.\n\nHe used the crankshaft with a connecting rod in two of his water-raising machines: the crank-driven saqiya chain pump and the double-action reciprocating piston suction pump. His water pump also employed the first known crank-slider mechanism.\n\nEnglish technology historian Donald Hill writes:\n\nAl-Jazari invented a method for controlling the speed of rotation of a wheel using an escapement mechanism.\n\nAccording to Donald Hill, al-Jazari described several early mechanical controls, including \"a large metal door, a combination lock and a lock with four bolts.\"\n\nA segmental gear is \"a piece for receiving or communicating reciprocating motion from or to a cogwheel, consisting of a sector of a circular gear, or ring, having cogs on the periphery, or face.\" Professor Lynn Townsend White, Jr. wrote:\n\nAl-Jazari invented five machines for raising water, as well as watermills and water wheels with cams on their axle used to operate automata, in the 12th and 13th centuries, and described them in 1206. It was in these water-raising machines that he introduced his most important ideas and components.\n\nThe first known use of a crankshaft in a chain pump was in one of al-Jazari's saqiya machines. The concept of minimizing intermittent working is also first implied in one of al-Jazari's \"saqiya\" chain pumps, which was for the purpose of maximising the efficiency of the saqiya chain pump. Al-Jazari also constructed a water-raising saqiya chain pump which was run by hydropower rather than manual labour, though the Chinese were also using hydropower for chain pumps prior to him. Saqiya machines like the ones he described have been supplying water in Damascus since the 13th century up until modern times, and were in everyday use throughout the medieval Islamic world.\n\nCiting the Byzantine siphon used for discharging Greek fire as an inspiration, al-Jazari went on to describe the first suction pipes, suction pump, double-action pump, and made early uses of valves and a crankshaft-connecting rod mechanism, when he invented a twin-cylinder reciprocating piston suction pump. This pump is driven by a water wheel, which drives, through a system of gears, an oscillating slot-rod to which the rods of two pistons are attached. The pistons work in horizontally opposed cylinders, each provided with valve-operated suction and delivery pipes. The delivery pipes are joined above the centre of the machine to form a single outlet into the irrigation system. This water-raising machine had a direct significance for the development of modern engineering. This pump is remarkable for three reasons:\n\nAl-Jazari's suction piston pump could lift 13.6 metres of water, with the help of delivery pipes. This was more advanced than the suction pumps that appeared in 15th-century Europe, which lacked delivery pipes. It was not, however, any more efficient than the noria commonly used by the Muslim world at the time.\n\nal-Jazari developed the earliest water supply system to be driven by gears and hydropower, which was built in 13th century Damascus to supply water to its mosques and Bimaristan hospitals. The system had water from a lake turn a scoop-wheel and a system of gears which transported jars of water up to a water channel that led to mosques and hospitals in the city.\n\nAl-Jazari built automated moving peacocks driven by hydropower. He also invented the earliest known automatic gates, which were driven by hydropower, created automatic doors as part of one of his elaborate water clocks, and invented water wheels with cams on their axle used to operate automata. According to \"Encyclopædia Britannica\", the Italian Renaissance inventor Leonardo da Vinci may have been influenced by the classic automata of al-Jazari.\n\nMark E. Rosheim summarizes the advances in robotics made by Muslim engineers, especially al-Jazari, as follows:\n\nOne of al-Jazari's humanoid automata was a waitress that could serve water, tea or drinks. The drink was stored in a tank with a reservoir from where the drink drips into a bucket and, after seven minutes, into a cup, after which the waitress appears out of an automatic door serving the drink.\n\nAl-Jazari invented a hand washing automaton incorporating a flush mechanism now used in modern flush toilets. It features a female humanoid automaton standing by a basin filled with water. When the user pulls the lever, the water drains and the female automaton refills the basin.\n\nAl-Jazari's \"peacock fountain\" was a more sophisticated hand washing device featuring humanoid automata as servants which offer soap and towels. Mark E. Rosheim describes it as follows:\n\nAl-Jazari's work described fountains and musical automata, in which the flow of water alternated from one large tank to another at hourly or half-hourly intervals. This operation was achieved through his innovative use of hydraulic switching.\n\nAl-Jazari created a musical automaton, which was a boat with four automatic musicians that floated on a lake to entertain guests at royal drinking parties. Professor Noel Sharkey has argued that it is quite likely that it was an early programmable automata and has produced a possible reconstruction of the mechanism; it has a programmable drum machine with (cams) that bump into little levers that operated the percussion. The drummer could be made to play different rhythms and different drum patterns if the pegs were moved around.\n\nAl-Jazari constructed a variety of water clocks and candle clocks. These included a portable water-powered scribe clock, which was a meter high and half a meter wide, reconstructed successfully at the Science Museum in 1976 Al-Jazari also invented monumental water-powered astronomical clocks which displayed moving models of the Sun, Moon, and stars.\n\nAccording to Donald Hill, al-Jazari described the most sophisticated candle clocks known to date. Hill described one of al-Jazari's candle clocks as follows:\n\nAl-Jazari's candle clock also included a dial to display the time and, for the first time, employed a bayonet fitting, a fastening mechanism still used in modern times.\n\nThe elephant clock described by al-Jazari in 1206 is notable for several innovations. It was the first clock in which an automaton reacted after certain intervals of time (in this case, a humanoid robot striking the cymbal and a mechanical robotic bird chirping) and the first water clock to accurately record the passage of the temporal hours to match the uneven length of days throughout the year.\n\nAl-Jazari's largest astronomical clock was the \"castle clock\", which was a complex device that was about high, and had multiple functions besides timekeeping. It included a display of the zodiac and the solar and lunar orbits, and an innovative feature of the device was a pointer in the shape of the crescent moon which travelled across the top of a gateway, moved by a hidden cart, and caused automatic doors to open, each revealing a mannequin, every hour. Another innovative feature was the ability to reprogram the length of day and night in order to account for their changes throughout the year.\n\nAnother feature of the device was five automata musicians who automatically play music when moved by levers operated by a hidden camshaft attached to a water wheel. Other components of the castle clock included a main reservoir with a float, a float chamber and flow regulator, plate and valve trough, two pulleys, crescent disc displaying the zodiac, and two falcon automata dropping balls into vases. Al-Jazari's castle clock is considered to be the earliest programmable analog computer.\n\nAl-Jazari invented water clocks that were driven by both water and weights. These included geared clocks and a portable water-powered scribe clock, which was a meter high and half a meter wide. The scribe with his pen was synonymous to the hour hand of a modern clock. Al-Jazari's famous water-powered scribe clock was reconstructed successfully at the Science Museum, London in 1976.\n\nAlongside his accomplishments as an inventor and engineer, al-Jazari was also an accomplished artist. In \"The Book of Knowledge of Ingenious Mechanical Devices\", he gave instructions of his inventions and illustrated them using miniature paintings, a medieval style of Islamic art.\n\n\n\n"}
{"id": "19285762", "url": "https://en.wikipedia.org/wiki?curid=19285762", "title": "LTE Advanced", "text": "LTE Advanced\n\nLTE Advanced is a mobile communication standard and a major enhancement of the Long Term Evolution (LTE) standard. It was formally submitted as a candidate 4G to ITU-T in late 2009 as meeting the requirements of the IMT-Advanced standard, and was standardized by the 3rd Generation Partnership Project (3GPP) in March 2011 as 3GPP Release 10.\n\nThe LTE format was first proposed by NTT DoCoMo of Japan and has been adopted as the international standard. LTE standardization has matured to a state where changes in the specification are limited to corrections and bug fixes. The first commercial services were launched in Sweden and Norway in December 2009 followed by the United States and Japan in 2010. More LTE networks were deployed globally during 2010 as a natural evolution of several 2G and 3G systems, including Global system for mobile communications (GSM) and Universal Mobile Telecommunications System (UMTS) in the 3GPP family as well as CDMA2000 in the 3GPP2 family.\n\nThe work by 3GPP to define a 4G candidate radio interface technology started in Release 9 with the study phase for LTE-Advanced. Being described as a 3.9G (beyond 3G but pre-4G), the first release of LTE did not meet the requirements for 4G (also called IMT Advanced as defined by the International Telecommunication Union) such as peak data rates up to 1 Gb/s. The ITU has invited the submission of candidate Radio Interface Technologies (RITs) following their requirements in a circular letter, 3GPP Technical Report (TR) 36.913, \"Requirements for Further Advancements for E-UTRA (LTE-Advanced).\" These are based on ITU's requirements for 4G and on operators’ own requirements for advanced LTE.\nMajor technical considerations include the following:\n\nLikewise, 'WiMAX 2', 802.16m, has been approved by ITU as the IMT Advanced family. WiMAX 2 is designed to be backward compatible with WiMAX 1 devices. Most vendors now support conversion of 'pre-4G', pre-advanced versions and some support software upgrades of base station equipment from 3G.\n\nThe mobile communication industry and standards organizations have therefore started work on 4G access technologies, such as LTE Advanced. At a workshop in April 2008 in China, 3GPP agreed the plans for work on Long Term Evolution (LTE). A first set of specifications were approved in June 2008. Besides the peak data rate 1 Gb/s as defined by the ITU-R, it also targets faster switching between power states and improved performance at the cell edge. Detailed proposals are being studied within the working groups.\n\nThe target of 3GPP LTE Advanced is to reach and surpass the ITU requirements. LTE Advanced should be compatible with first release LTE equipment, and should share frequency bands with first release LTE. In the feasibility study for LTE Advanced, 3GPP determined that LTE Advanced would meet the ITU-R requirements for 4G. The results of the study are published in 3GPP Technical Report (TR) 36.912.\n\nOne of the important LTE Advanced benefits is the ability to take advantage of advanced topology networks; optimized heterogeneous networks with a mix of macrocells with low power nodes such as picocells, femtocells and new relay nodes. The next significant performance leap in wireless networks will come from making the most of topology, and brings the network closer to the user by adding many of these low power nodes — LTE Advanced further improves the capacity and coverage, and ensures user fairness. LTE Advanced also introduces multicarrier to be able to use ultra wide bandwidth, up to 100 MHz of spectrum supporting very high data rates.\n\nIn the research phase many proposals have been studied as candidates for LTE Advanced (LTE-A) technologies. The proposals could roughly be categorized into:\n\nWithin the range of system development, LTE-Advanced and WiMAX 2 can use up to 8x8 MIMO and 128-QAM in downlink direction. Example performance: 100 MHz aggregated bandwidth, LTE-Advanced provides almost 3.3 Gbit peak download rates per sector of the base station under ideal conditions. Advanced network architectures combined with distributed and collaborative smart antenna technologies provide several years road map of commercial enhancements.\n\nThe 3GPP standards Release 12 added support for 256-QAM.\n\nA summary of a study carried out in 3GPP can be found in TR36.912.\n\nOriginal standardization work for LTE-Advanced was done as part of 3GPP Release 10, which was frozen in April 2011. Trials were based on pre-release equipment. Major vendors support software upgrades to later versions and ongoing improvements.\n\nIn order to improve the quality of service for users in hotspots and on cell edges, heterogenous networks (HetNet) are formed of a mixture of macro-, pico- and femto base stations serving corresponding-size areas. Frozen in December 2012, 3GPP Release 11 concentrates on better support of HetNet. Coordinated Multi-Point operation (CoMP) is a key feature of Release 11 in order to support such network structures. Whereas users located at a cell edge in homogenous networks suffer from decreasing signal strength compounded by neighbor cell interference, CoMP is designed to enable use of a neighboring cell to also transmit the same signal as the serving cell, enhancing quality of service on the perimeter of a serving cell. In-device Co-existence (IDC) is another topic addressed in Release 11. IDC features are designed to ameliorate disturbances within the user equipment caused between LTE/LTE-A and the various other radio subsystems such as WiFi, Bluetooth, and the GPS receiver. Further enhancements for MIMO such as 4x4 configuration for the uplink were standardized.\n\nThe higher number of cells in HetNet results in user equipment changing the serving cell more frequently when in motion.\nThe ongoing work on LTE-Advanced in Release 12, amongst other areas, concentrates on addressing issues that come about when users move through HetNet, such as frequent hand-overs between cells. It also included use of 256-QAM.\n\nThis list covers technology demonstrations and field trials up to the year 2014, paving the way for a wider commercial deployment of the VoLTE technology worldwide. From 2014 onwards various further operators trialled and demonstrated the technology for future deployment on their respective networks. These are not covered here. Instead a coverage of commercial deployments can be found in the section below.\n\nDeployment of LTE-Advanced in progress in various LTE networks.\n\n\n\n\n"}
{"id": "10067585", "url": "https://en.wikipedia.org/wiki?curid=10067585", "title": "Membrane roofing", "text": "Membrane roofing\n\nMembrane roofing is a type of roofing system for buildings and tanks. It is used to move water off the roof. Membrane roofs are most commonly made from synthetic rubber, thermoplastic (PVC or similar material), or modified bitumen. Membrane roofs are most commonly used in commercial application, though they are becoming increasingly common in residential application.\n\nSynthetic Rubber (Thermoset) – This type of membrane roof is made of large, flat pieces of synthetic rubber or similar materials. These pieces are bonded together at the seams to form one continuous membrane. The finished roof’s thickness is usually between 30 and 60 mils(thousandths of an inch) (0.75 mm to 1.50 mm). The most commonly used thermoset membrane is EDPM. Other types of related materials are CSPE, CR, and ECR. Thermosets are widely used roofing materials due to their ability to withstand damaging effects of sun-rays and chemicals found on roofs. \n\nThermoplastic Membrane – This is similar to synthetic rubber, but the seams are typically heat-fused (welded) to form a continuous membrane. The 'lap' seams can also be fused with solvents instead of heat, and can be as strong as the rest of the membrane. Other related materials are CPA, CPE, EIP, NBP, PIB, and TPO. Thermoplastic membranes include a reinforcement layer that provides more strength and stability. The most common thermoplastic membranes are PVC (polyvinyl chloride) and TPO (thermoplastic polyolefin).\n\nModified Bitumen – This type of roofing is an evolution of asphalt roofing. It is made from asphalt and a variety of rubber modifiers and solvents. There are several ways of connecting pieces of this material. In a heat application process the seams are heated to melt the asphalt together and create a seal. There is also hot-mopped application, similar to how conventional built-up roofs are installed. Cold-applied adhesives and self-adhesive membranes are two of the more recent options. This material is also referred to as APP, SBS, and SEBS.\n\n These three application types of membrane roofing show distinct advantages over the previously more common flat roofing method of asphalt and gravel. In asphalt and gravel application, it can be very difficult to create a proper seal at all seams and connection points. This can cause a roof to leak early in its lifespan, and require much more maintenance. When installed correctly, newer materials are either seamless, or have seams as strong as the body. This eliminates most of the leakage concerns associated with flat roofing systems. \n\n"}
{"id": "2237679", "url": "https://en.wikipedia.org/wiki?curid=2237679", "title": "Methyl nitrate", "text": "Methyl nitrate\n\nMethyl nitrate is the methyl ester of nitric acid and has the chemical formula CHNO. It is a colourless volatile liquid that is explosive.\n\nIt can be produced by the condensation of nitric acid and methanol:\n\nMethyl nitrate can be produced on a laboratory or industrial scale either through the distillation of a mixture of methanol and nitric acid, or by the nitration of methanol by a mixture of sulfuric and nitric acids. The first procedure is not preferred due to the great explosion danger presented by the methyl nitrate vapour. The second procedure is essentially identical to that of making nitroglycerin. However, the process is usually run at a slightly higher temperature and the mixture is stirred mechanically on an industrial scale instead of with compressed air.\n\nMethyl nitrate is a sensitive explosive. When ignited it burns extremely fiercely with a gray-blue flame. Methyl nitrate is a very strong explosive, like Nitroglycerin, Ethylene glycol dinitrate, including other nitrate esters. The sensitivity of methyl nitrate to initiation by detonation is among the greatest known, with even a number one blasting cap, the lowest power available, producing a near full detonation of the explosive.\n\nDespite the superior explosive properties of methyl nitrate, it has not received application as an explosive due mostly to its high volatility, which prevents it from being stored or handled safely. It was used as a rocket fuel by Germany in World War II, in a mixture containing 25% methanol, which was named \"myrol\". This mixture would evaporate at a constant rate and so its composition would not change over time. It presents a slight explosive danger (it is somewhat difficult to detonate) and does not detonate easily via shock.\n\nAs well as being an explosive, methyl nitrate is toxic and causes headaches when inhaled.\n"}
{"id": "57923814", "url": "https://en.wikipedia.org/wiki?curid=57923814", "title": "Natalie Villalobos", "text": "Natalie Villalobos\n\nNatalie Villalobos is the Head of Global Programs for Women Techmakers at Google, an external facing initiative supporting women in technology working to improve the visibility and contributions of women at Google and in particular at the annual developer conference Google I/O. \n\nNatalie is a History graduate. She studied for her B.A in History at Sonoma State University in California, US, including one year abroad at the University of Hull, in England. \n\nEarly in her career, Natalie worked with the communities at Digg and Yahoo! (MASH and Live) as an Associate Community Manager, and managed StyleMob, a DIY social network for fashion enthusiasts\".\n\nFrom 2008 to 2009 Natalie consulted for the Institute for the Future where she co-led the Signtific Lab, an open source massive multi-player \"thought experiment\" platform for scientists and technologists to help forecast future disruptions in their fields. And n 2009 she worked as the Arts and Culture Manager for The Seasteading Institute on the development of Ephemerisle - \"a floating festival of politics, community, and art\". \n\nIn 2010 Natalie became the Community Manager for Google+. After working in this role for a couple of years she saw a need to support women in technology and improve their visibility and contributions at their developer conferences and events. In 2013 the position Head of Global Programs for Women Techmakers at Google was created at her request. The team's mission is to provide visibility, community, and resources for women in tech globally, engaging 100,000 women annually across 190 countries. Since it's creation, the team has grown Google's 'Women Techmakers' event from an annual event into a scalable program operating in over 50 countries.\n\nNatalie is also involved in mentoring underrepresented people in tech through her activities as an Advisory Board Member for the Center for Gender and Refugee Studies at UC Hastings Law School, and as an advisor for XPrize on the Anu & Naveen JainWomen's Safety Prize.\n\nNatalie has authored two patents for Google, and ran a successful viral campaign after attending the Google Earth Outreach/Indigenous Mapping Summit, to save 300 acres of the Amazon rainforest for the Quichua Nation.\n\nIn 2011 Natalie was promoted as one of 35 personalities to add to your Google+ by the Huffington Post and one of the top 13 Google insiders to follow on Google+ by Business Insider. And in 2012 wasvoted one of the top 20 Women in Tech to Follow on Google+ by CBS News.\n"}
{"id": "1017391", "url": "https://en.wikipedia.org/wiki?curid=1017391", "title": "Nikon Coolpix series", "text": "Nikon Coolpix series\n\nThe Nikon Coolpix series are digital compact cameras in many variants produced by Nikon. It includes superzoom, bridge, travel-zoom, miniature compact and waterproof/rugged cameras.\n\nNikon Coolpix cameras are organized into five different lines. The line in which a particular camera is placed is indicated by the letter which is the first character of its model number. The lines are: the (A) series, the (AW) all weather series, the (L) life series, the (P) performance series, and the (S) style series.\n\nThe Coolpix A Series is Nikon's new flagship point and shoot camera.\n\nNote some cameras are numbered 5xxx on front, and E5xxx on bottom.\n\nThe following Coolpix cameras support raw image files: \n\nSome Coolpix cameras which are not advertised as supporting a raw file format can produce usable raw files if switched to a maintenance mode. Note that switching to this mode can invalidate a camera's guarantee. Nikon models with this capability: E700, E800, E880, E900, E950, E990, E995, E2100, E2500, E3700, E4300, E4500.\n\nOfficial Nikon Coolpix Pages\n"}
{"id": "54960861", "url": "https://en.wikipedia.org/wiki?curid=54960861", "title": "One Touch Make Ready", "text": "One Touch Make Ready\n\nOne Touch Make Ready (also known as One Touch, and often abbreviated as OTMR) is the various statutes and local ordinances passed by various local governments and utilities in the United States, which require the owners of utility poles to allow a single construction crew to make changes to multiple utility wires.\n\nAcross the United States, utility poles in a given area may be owned by the local government, a municipal electric company, a private entity, or any combination of the three. In most cases, the poles are owned by a private entity, like a local incumbent phone company or electric company. \nBefore an Internet Service Provider (or any company) can add a new attachment or line to a utility pole, the existing attachments may need to be moved around so that the pole can be made ready to handle a new attachment or line. This is known as 'Make Ready work.' The reason Make Ready work is necessary is that, under Federal Law, to prevent the risk of outages or other issues, lines on utility poles must be spaced a certain distance apart from each other based on how many lines are on the pole. \n\nUnder federal guidelines, Make Ready Work must occur sequentially, meaning that attachments can only be moved in the order with which they were originally placed on the line. This process can create massive delays, as well as other large disruptions in high traffic areas, such as alongside major roadways. In addition, the make ready work can take months, or even years, to complete as every company involved must send out their own approved contractor to move only their respective attachment. Each contractor must also schedule their work to not conflict with other contractors performing Make Ready Work, as well as taking into account other local factors, such as weather, traffic, and maintenance work (such as road paving). These factors must be considered as the United States primarily uses aerial work platforms to perform Make Ready Work.\n\nTo rectify these issues, several local governments have passed One Touch Make Ready legislation. While specific portions of the statutes vary, all carry a unifying theme that under these laws, certified construction crews chosen either by the pole owners or local governments are allowed to make all the necessary changes to a utility pole to make it ready for a new attachment.\n\nIn a location which has adopted one touch make-ready, companies that own utility poles must agree on one or more common contractors that have permission to move existing attachments on a pole, allowing a single crew to move all attachments on a pole on a single visit, rather than sending in a unique crew to move each attachment sequentially.\n\nNashville's OTMR ordinance is unique in that it includes a clause which requires a company wishing to install a new line (henceforth referred to as Company A), after having reached an agreement with the pole's owner, to give every company already on the pole 30 days notice to move their equipment. If those companies do not comply after 30 days, then Company A can perform the complex make-ready work. If there are any errors or problems from Company A's make-ready work, then the companies already on the pole can recoup expenses from Company A.\n\nAs of August 2017, only three cities in the United States have One Touch Make Ready statutes: Louisville, Kentucky; Nashville, Tennessee; and San Antonio, Texas. Numerous other cities are in various stages along the process of implementing the One Touch Make Ready system.\n\nOn February 11, 2016, the City Council of Louisville, Kentucky voted 23-0 to adopt a One Touch Make-Ready Statute, making it the first city in the country to adopt such legislation. According to city councilman Bill Hollander, who sponsored the legislation \"This will help businesses locate here and grow here. It will create jobs, and will retain and attract our young people and make Louisville broadband ready.\" These sentiments were echoed by Louisville Mayor Greg Fischer, who stated that it would help lay the groundwork for entities like Google Fiber, and said \"Tonight's vote puts Louisville one step closer toward becoming a Google Fiber city.\" The legislation states that an applicant for attachment must first receive approval from the existing pole owners, at which point it may contract a pre-approved construction crew to perform all make ready work at its own expense. Pole owners and pre-existing providers whose wires were moved may choose to do post-make ready work inspections and call for remedial work if needed, at the new provider’s expense.\n\nOn May 6, 2016, San Antonio's municipal utility, CPS Energy, adopted broad new guidelines which included many of the fundamental principles of One Touch Make Ready legislation. The standards include specific requirements for OTMR, and include what needs to happen before, during and after the process. The 128-page standards are far more detailed than the legislation passed by Louisville, Kentucky a few months prior. This is mainly due to CPS being an electric company. The Introduction of the standards states: \n\nThe standards go on to lay out technical provisions, administrative procedures, as well as various specific provisions for both wired and wireless utility pole attachments. They incorporate recommendations from the FCC on how best to expand broadband while also addressing safety concerns by working into the guidelines various safety standards recommended by the Occupational Safety and Health Administration (OSHA).\n\nOn August 2, 2016, the Metropolitan Council of Nashville and Davidson County began to debate its own One Touch Make Ready Ordinance. The measure, sponsored by Councilman Anthony Davis, was nearly identical to the measure passed by the city of Louisville several months prior. On September 6, 2016, the Nashville City Council Approved the Measure 32-7 on a roll-call vote, as the Council's computer systems were down. On September 20, 2016, Councilwoman Tanaka Vercher, a critic of the bill, proposed a resolution that would delay voting on the bill for two Council meetings. This resolution was defeated 26-12. A few hours later, the City Council passed the OTMR bill via voice vote. After the vote, Nashville Mayor Megan Barry, stated she would sign the bill.\n\nOn September 21, 2016, the Mayor of Nashville, Tennessee, Megan Barry, signed into law the city's OTMR ordinance. The ordinance exists under Nashville Law as Ordinance BL2016-343 Notably, throughout the entire process of the debate amongst council lawmakers on the specifics of the bill, Barry remained neutral on the bill's contents, refusing to comment on the bill's status, and only encouraging lawmakers and providers to seek a compromise.\n\nSupporters, such as the Fiber to the Home Council (FTTHC) and the National Cable and Telecommunications Association (NCTA), argue that OTMR policies drastically reduce the cost of adding additional lines, noting that allowing a single crew to work on a line over the course of a single workday, as opposed to multiple crews working on multiple, often non-sequential days, reduces the financial cost of adding additional lines, by decreasing the number of workers needed to install a new line, thereby decreasing the total number of man-hours worked. Further, the FTTHC argues that OTMR policies reduce the traffic disruption caused by the large Aerial work platforms used by the overwhelming majority of utility pole maintenance companies in the United States.\n\nSupporters, such as Google Fiber and several lawmakers, have argued that OTMR policies create and encourage competition, as they can theoretically allow any outside Internet service provider to lay networks of Fiber optic cables, thus leading to a breakup of the various local internet monopolies which exist across the United States. \n\nNumerous groups have criticized OTMR since its implementation in several cities. The American Legislative Exchange Council (ALEC), along with several lobbying groups, have critiqued the proposal. In addition, the measure has received extremely vocal opposition from cable and telephone providers, mainly AT&T and Comcast, who together have thus far filed lawsuits against every city which has successfully passed OTMR ordinances.\n\nOpponents, including AT&T, Comcast, and the Nashville Electric Service (NES), have argued that the regulation of telecommunications poles falls under the purview of the Federal Communications Commission. To back this up, they cite federal pole attachment rules, which mandate the Federal Communications Commission, not local municipalities, regulate telecommunications poles. However, on October 30, 2016, in response to the case \"AT&T v. Louisville Metro\", the Federal Communications Commission stated that states which have opted out of the FCC guidelines (including Kentucky) are not subject to the Federal Pole Attachment rules.\n\nOpponents, including the American Legislative Exchange Council, have criticized the policy as a blatant attempt by local officials to \"bend over backward\" to ensure the continued support of Google Fiber's expansion within their respective cities. ALEC further contends that cities are willing to spend millions of taxpayer dollars to defend legislation in court that they see as only a way to appease Google. They cite Google's willingness to join as defendants in every OTMR lawsuit filed as of August 2017.\n\nOpponents such as the American Legislative Exchange Council have argued that OTMR ordinances violate the property rights of corporations to control what happens on their respective lines. They further claim that OTMR ordinances violate federal property law by allegedly allowing any third party to relocate equipment on a pole, without notifying the equipment's owners, unless the third party believes there will be a service outage. However, so far, all cities which have successfully passed OTMR ordinances have included provisions which mandate that only contractors approved by the pole's owners may work on the lines.\n\nOpponents, such as the Communications Workers of America (the largest communications labor union in the United States), have argued against One Touch Make Ready proposals on the grounds that the new proposals have the potential to create unsafe working conditions, as well as violate existing labor contracts. In their filing with the Federal Communications Commission on the matter, the union argues that since new attachers would not be required to submit formal applications, this could lead to a situation where existing infrastructure could be damaged by workers who are unfamiliar with the lines. The union further emphasizes that pole owners should be able to conduct a thorough review of the lines before any work can be done.\n\nUnder the guidelines of the Federal Communications Commission, states are presented with two options for the regulation of their utility poles. They can either pass legislation regognizing the right of the FCC to regulate their utility poles, or states can form their own regulatory agencies and maintain their right to self-regulate their utility poles. As of 2016, thirty states have turned over regulatory power to the FCC, while the remaining twenty states have maintained their right to self-regulate. All Territories of the United States and Insular areas must adopt the federal guidelines.\n\nAccording to the Federal Communications Commission in 2016, under current regulatory statutes, localities that reside in states which have forfeited their right to self-regulate their utility poles to the federal government are unable to implement One Touch Make Ready legislation in its current form. The twenty states which maintained their right to self-regulate, and thus can pass OTMR legislation are:\n\nOn February 25, 2016, AT&T (which owns an estimated 25-40% of Louisville, Kentucky's utility poles) filed a lawsuit in federal court against the Lousiville Metro Government. In the lawsuit, officially titled \"BellSouth Telecommunications LLC vs. Louisville/Jefferson County Metro Government\", AT&T argued that the city's One Touch Make Ready ordinance violated state and federal law. In the 11-page filing, AT&T asked a federal judge to clarify that the power to regulate pole attachments lies solely with the Kentucky Public Service Commission, as well as with the Federal Communications Commission. In response to the lawsuit, Louisville Mayor Greg Fischer stated: \"We will vigorously defend the lawsuit filed today by AT&T, as Gigabit fiber is too important to our city's future.\" AT&T responded by stating that the ordinance allowed third parties to seize its property temporarily. In a statement, an AT&T spokesman said: \"Unless the court declares the ordinance invalid and permanently enjoins Louisville Metro from enforcing it, AT&T will suffer irreparable harm that cannot be addressed by recovery of damages.\"\n\nOn February 26, 2016, in an official blog post titled \"Standing With Louisville,\" Chris Levendos, Google Fiber's Director of National Deployment and Operations stood by the city, stating: \"Google Fiber stands with the city of Louisville and the other cities across the country that are taking steps to bring faster, better broadband to their residents.\" \n\nOn June 29, 2016, Attorneys for Frontier Communications filed a briefing in federal court asking to join AT&T as plaintiffs in the case. In their briefing, Frontier argued that, even though they do not serve the state of Kentucky, the ordinance's \"unprecedented scope\" could set a precedent that could harm Frontier's business in the future. Frontier also noted that they were not paid by AT&T to assist in the suit, nor were they asked about it.\n\nOn October 6, 2016, Attorneys for Google Fiber filed a court briefing that, if approved, would allow the company to assist in defending against the suit. In their filing, they claim the ordinance is: “a valid exercise of Louisville Metro’s unquestioned authority to manage construction activities in public rights-of-way.” Further, the attorneys argued that the ordinance would “enhance public safety while reducing disturbance and congestion.”\n\nOn October 31, 2016, in response to the lawsuit, the Federal Communications Commission released a statement, which guaranteed that the city's OTMR ordinance does not conflict with federal law. In the filing, the FCC stated that “[AT&T] maintains in its motion for summary judgment that the Louisville Ordinance conflicts with, and is therefore preempted by, the federal pole-attachment rules promulgated by the Commission under Section 224. That argument is wrong as a matter of law.”\n\nOn August 16, 2017, US District Court Judge David Hayes rejected AT&T's arguments and threw out the lawsuit. \"A one-touch make-ready approach inherently regulates public rights-of-way because it reduces the number of encumbrances or burdens placed on public rights-of-way,\" Hale wrote. \"The one-touch make-ready ordinance requires that all necessary make-ready work be performed by a single crew, lessening the impact of make-ready work on public rights-of-way.\" Hale also ruled that FCC guidelines do not apply in Kentucky, as the state exempted itself from those laws.\n\nOn September 22, 2016, AT&T filed a lawsuit in federal court against the Metro Government of Nashville, Tennessee, less than 24 hours after the city had passed its One Touch Make Ready Ordinance. At the time, AT&T owned approximately 20% of the utility poles within the Nashville Metro area. In the lawsuit, titled \"BellSouth Telecommunications, LLC, D/B/A AT&T, Tennessee versus The Metropolitan Government of Nashville and Davidson County, Tennessee; In her official capacity as Mayor; and Mark Sturtevant, in his official capacity as transitional interim Director of the Department of Public Works\", AT&T argued that the sole authority to regulate utility poles lies with the Federal Communications Commission. The lawsuit sought from the U.S. District Court for the Middle District of Tennessee a permanent injunction against the city, preventing the ordinance from ever going into effect. Notably, the lawsuit differed from the one filed against the city of Louisville in that it did not claim that the state had the power to regulate utility poles.\n\nOn October 26, 2016, Comcast filed a lawsuit in federal court against the Metro Government of Nashville over the city's One Touch ordinance. Comcast, like AT&T, sought a permanent injunction to prevent the ordinance from going into effect. In response to the filing, Nashville Mayor Megan Barry said: “One Touch Make Ready has been litigated in the court of public opinion, and the public overwhelmingly supports this measure designed to speed up the deployment of high-speed fiber in Nashville. Now, we hope that this federal litigation is quickly resolved so that we can get on with the business of expanding access to gigabit Internet throughout Davidson County.”\n\nOn October 31, 2016, Nashville Metro filed a request with a federal judge to consolidate the two lawsuits. In the filing, attorneys representing the city argued for why the cases should be consolidated:\n\nOn November 10, 2016, Ken Sharp, the federal judge overseeing the case, approved the request to merge the lawsuits. He also granted an extension to the city's deadline to file a response to the initial filing of the lawsuit, extending their deadline to November 14. \n\nOn November 14, 2016, the city of Nashville filed a motion to dismiss the lawsuit, citing that AT&T did not provide information regarding exactly where the local ordinance conflicted with federal law. \"The Metropolitan Government determined on a municipal level what approach to rights-of-way management would best serve local needs, and enacted an ordinance that aligns with and supports federal policy,\" Nashville's filing supporting the motion said.\n\nOn October 5, 2016, Charter Communications, a utility company that owns Time Warner Cable and does business in Louisville, filed a lawsuit against the city's Metro Government. In the lawsuit, Charter alleges that the city's OTMR ordinance violates Charter's fifth amendment rights, specifically over what is referred to as the \"Taking's Clause.\" The takings clause states that \"\"...private property [shall not] be taken for public use, without just compensation.\"\" Charter's lawsuit also hinges on the argument that their right to \"speak\" as a corporation is being violated, however, Charter's lawyers did not specifically reference any infringement by the local government.\n\nOn June 29, 2017, it was revealed through court filings that the case, officially titled \"Insight Kentucky Partners versus Louisville/Jefferson County Metro Government\" would be going to trial. The trial is set to be overseen by U.S District Senior Judge Charles Ralph Simpson III.\n\nAfter a federal judge threw out AT&T's lawsuit against the city of Louisville, it was revealed that Charter's lawsuit would be unaffected.\n\nOn October 21, 2016, the Nashville Electric Service (abbreviated NES), which is one of the twelve largest public electric utilities in the United States, filed a lawsuit against the Metro Government of Nashville, Tennessee. At the time, NES owned 80 percent of the utility poles in Nashville. After filing the lawsuit, a spokesman for the company said the OTMR ordinance forces the company to choose between complying with the new law or its existing contracts with AT&T, Comcast, as well as other internet providers. According to the court filing, the declaratory judgment is meant to clarify NES's rights and obligations. The company also stated that it had received a letter from AT&T, threatening to sue them if they complied with the law. In response to the lawsuit being filed, Nashville Mayor Megan Barry stated: “We believe in One Touch Make Ready and look forward to defending its legality. Our focus is simply on providing services to our citizens, who have expressed their desire for those services through their elected representatives.\" \n\nOn February 3, 2017, NES announced that it had reached an out-of-court settlement with Google Fiber, as well as the city of Nashville, over the OTMR policy. In the agreement, \"Google Fiber [has] promised NES it would pay any potential damages that result from NES abiding by the ordinance in relation to the deployment of Google Fiber,” NES President and CEO Decosta Jenkins said in a statement issued Friday. She further clarified “This is a win for NES, Google Fiber, Nashville and residents who are looking forward to having access to Google Fiber.” As part of this settlement, NES dropped its pending lawsuits against Google and the city of Nashville.\n\n\n\n"}
{"id": "45628053", "url": "https://en.wikipedia.org/wiki?curid=45628053", "title": "Open College (Toronto)", "text": "Open College (Toronto)\n\nOpen College was a radio-based university-credit distance education provider based in Toronto, Canada; it primarily served listeners in Ontario. \n\nFounded in 1971, the courses were accredited by Ryerson Polytechnic Institute and York University's Atkinson College and broadcast throughout Toronto and much of southern Ontario on what was then Ryerson owned radio station CJRT which produced and administered the courses. \n\nThe service was conceived of by the Dean of Arts of Ryerson Polytechnic Institute who was inspired by the creation of Open University in the United Kingdom, which used radio and television to broadcast some of its credit courses. In 1969, CJRT began broadcasting a non-credit education series which led to the development of credit courses two years later. Ryerson sociology professor Margaret Norquay volunteered for the project and produced its first course, Introduction to Sociology, which commenced in January 1971. Norquay went on to become Open College's first director in 1972. \n\nIn 1974, Ryerson divested itself of CJRT and the radio station became an independent government funded corporation with Open College as one of its departments. The service used the name \"Open College\" due to its open admissions policy.\n\nAt its peak, Open College offered 28 credit courses, produced at the radio station and accredited by Ryerson and Atkinson College. An estimated 15,000 students took courses using the service during its existence with student enrollment peaking in 1995. By the 1980s Open College broadcasts were heard throughout Ontario as CJRT was added to the FM services offered by cable systems throughout the province. Alberta's public radio broadcaster, CKUA, purchased and broadcast several Open College courses which were broadcast in conjunction with credit courses at Athabasca University; CKUA has continued to develop educational broadcasts in conjunction with Athabasca. \n\nIn 1999, the administration of Open College was transferred to Ryerson University. In 2003, as a result in the development of the internet as a mode of transmission for distance education and the resulting decline of interest in taking radio courses, as well as CJRT's transformation into an all-jazz radio station, radio broadcasts ended and the service was transferred to Ryerson's G. Raymond Chang School of Continuing Education which now offers distance education through the internet instead of by radio. In its last years, Open College broadcasts were aired Sunday mornings from 6am to 8am.\n\n"}
{"id": "27638832", "url": "https://en.wikipedia.org/wiki?curid=27638832", "title": "Oxford University Computing Services", "text": "Oxford University Computing Services\n\nOxford University Computing Services (OUCS) until 2012 provided the central Information Technology services for the University of Oxford. The service was based at 7-19 Banbury Road in central north Oxford, England, near the junction with Keble Road. OUCS became part of IT Services, when the new department was created at the University of Oxford on 1 August 2012 through a merger of the three previous central IT departments: Oxford University Computing Services (OUCS), Business Services and Projects (BSP) and ICT Support Team (ICTST).\n\nAt the time when Oxford University Computing Services ceased to operate as an independent department, it offered facilities, training and advice to members of the University in all aspects of academic computing. OUCS was responsible for the core networks reaching all departments and colleges of Oxford University. OUCS was made up of 5 technical and one administration group. Each group had responsibility for different aspects of OUCS services supplied to the University. At the time of the merger, the 5 technical groups were: Learning Technologies, Information and Support, Network Systems Management Services, Infrastructure Systems and Services Group, and Network and Telecommunications.\n\nA lease on a house was obtained in 1957 and operation started in 1958, initially as the Computing Laboratory at 9 South Parks Road, a Victorian building, now demolished to make way for the Experimental Psychology and Zoology departments. In 1963, due to space problems, the staff and computers moved to 19 Parks Road, the old Engineering Building. In 1970, the Computing Service occupied 17 and 19 Banbury Road, having split from the Computing Laboratory, which became the university's Department of Computer Science. By 1975, the Computing Service had taken over all of 7 to 19 Banbury Road, as IT Services still does today. An outpost at 59 George Street in central Oxford closed in the mid 1990s.\n\n\n"}
{"id": "669746", "url": "https://en.wikipedia.org/wiki?curid=669746", "title": "Park and ride", "text": "Park and ride\n\nPark and ride (or incentive parking) facilities are parking lots with public transport connections that allow commuters and other people heading to city centres to leave their vehicles and transfer to a bus, rail system (rapid transit, light rail, or commuter rail), or carpool for the remainder of the journey. The vehicle is left in the car park during the day and retrieved when the owner returns. Park and rides are generally located in the suburbs of metropolitan areas or on the outer edges of large cities. A park and ride that only offers parking for meeting a carpool and not connections to public transport may be called a park and pool.\n\nPark and ride is abbreviated as \"P+R\" on road signs in the UK, and is often styled as \"Park & Ride\" in marketing.\n\nIn Sweden, a tax has been introduced on the benefit of free or cheap parking paid by an employer, if workers would otherwise have to pay. The tax has reduced the number of workers driving into the inner city, and increased the usage of park and ride areas, especially in Stockholm. The introduction of a congestion tax in Stockholm has further increased the usage of park and ride.\n\nIn Prague, park and ride car parks are established near some metro and railway stations (about 17 parks near 12 metro stations and 3 train stations, in 2011). These car parks offer low prices and all-day and return (2× 75 min) tickets including the public transport fare.\n\nPark and ride facilities allow commuters to avoid a stressful drive along congested roads and a search for scarce, expensive city-centre parking. They may well reduce congestion by assisting the use of public transport in congested urban areas.\n\nThere is not much research on the pros and cons of park and ride schemes. It has been suggested that there is \"a lack of clear-cut evidence\nfor park and ride's widely assumed impact in reducing congestion\".\n\nPark and ride facilities help commuters who live beyond practical walking distance from the railway station or bus stop. They may also suit commuters with alternative fuel vehicles, which often have reduced range, when the facility is closer to home than the ultimate destination. They also are useful as a fixed meeting place for those carsharing or carpooling or using \"kiss and ride\" (see below). Also, some transit operators use park and ride facilities to encourage more efficient driving practices by reserving parking spaces for low emission designs, high-occupancy vehicles, or carsharing.\n\nMany park and rides have passenger waiting areas and/or toilets. Travel information, such as leaflets and posters, may be provided. At larger facilities, extra services such as a travel office, food shop, car wash, or cafeteria may be provided. These are often encouraged by municipal operators to encourage use of park and ride.\n\nPark and ride facilities, with dedicated car parks and bus services, began in the 1960s in the UK. Oxford operated the first such scheme, initially with an experimental service operating part-time from a motel on the A34 in the 1960s and then on a full-time basis from 1973. Better Choice Parking first offered an airport park and ride service at London Gatwick Airport in 1978. Oxford now operates park and ride from 5 dedicated car parks around the city. As of 2015, Oxford has the biggest urban park & ride network in the UK with a combined capacity of 5,031 car parking spaces.\n\nOne of the largest park and rides in Saudi Arabia is located at Kudai in Mecca. It helps people go the Masjid al-Haram. There is a Shuttle Service operated by SAPTCO that takes people during Ramadan from the Kudai Parking to the Masjid al-Haram.\n\nSome railway stations are promoted as a park and ride facility for a town some distance away, for instance for Looe and for St  Ives, both in Cornwall, England. Names of stations in the UK with large car parks outside the main urban area are often suffixed with \"Parkway\", such as , , and . At and , the stations are there to serve air as well as road passengers.\n\nIn the United States, it is common for outlying rail stations to include automobile parking, often with hundreds of spaces. Boston, for example, has built several large parking facilities at its commuter rail and metro stations near major highways and large arterial surface roads around the periphery of the city: Alewife, Braintree, Forest Hills, Hyde Park, Quincy Adams, Riverside, Route 128, Wellington, Woburn. The local transit operator, the MBTA, offers 46,000 park and ride spaces.\n\nB & R (B + R) is a name for using cycle boxes or racks near public transport terminals, mostly together with P & R car parks. This system can be promoted through integrated fare and tickets with public transport system.\n\nMany railway stations and airports feature a \"kiss-and-ride\" or \"kiss-and-fly\" area in which cars can stop briefly to discharge or, less commonly, pick up passengers. The term first appeared in a 20 January 1956 report in the \"Los Angeles Times\". It refers to the nominal scenario whereby a passenger is driven to the station by spouse or partner; they kiss each other goodbye before the passenger catches the train. \n\nDeutsche Bahn has announced that it will be changing the English expressions for Kiss and Ride, Service Points and Counters to German ones. In Italy the new Bologna Centrale railway station uses the \"kiss and ride\" signs. Some high-speed railway stations in Taiwan have signs outside stations reading \"Kiss and Ride\" in English, with Chinese characters above the words that read \"temporary pick-up and drop-off zone\". Kiss and Ride are getting popular in Poland. Cities with such areas include Kraków (since 15 November 2013), Warsaw (since 2016) or Toruń (since 2016). Locally they are known by its English name, i.e. \"Kiss and ride\" and while the sign is non-standardized, all of them contain the letters K+R.\n\nPark and ride schemes do not necessarily involve public transport. They can be provided to reduce the number of cars on the road by promoting carpooling, vanpooling, and carsharing. Partly because of the concentration of riders, and thus a reduced number of vehicles, these park and ride terminals often have express transit services into the urban area, such as a high-occupancy vehicle lane. The service may take passengers in only one direction in the morning (typically towards a central business district) and in the opposite direction in the evening, with no or a limited number of trips available in the middle of the day. It is often not allowed to park at these locations overnight. These attributes vary from region to region.\n\n"}
{"id": "7328483", "url": "https://en.wikipedia.org/wiki?curid=7328483", "title": "Path analysis (computing)", "text": "Path analysis (computing)\n\nPath analysis, is the analysis of a path, which is a portrayal of a chain of consecutive events that a given user or cohort performs during a set period of time while using a website, online game, or eCommerce platform. As a subset of behavioral analytics, path analysis is a way to understand user behavior in order to gain actionable insights into the data. Path analysis provides a visual portrayal of every event a user or cohort performs as part of a path during a set period of time.\n\nWhile it is possible to track a user’s path through the site, and even show that path as a visual representation, the real question is how to gain these actionable insights. If path analysis simply outputs a \"pretty\" graph, while it may look nice, it does not provide anything concrete to act upon.\n\nIn order to get the most out of path analysis the first step would be to determine what needs to be analyzed and what are the goals of the analysis. A company might be trying to figure out why their site is running slow, are certain types of users interested in certain pages or products, or if their user interface is set up in a logical way.\n\nNow that the goal has been set there are a few ways of performing the analysis. If a large percentage of a certain cohort, people between the ages of 18-25, logs into an online game, creates a profile and then spends the next 10 minutes wandering around the menu page, then it may be that the user interface is not logical. By seeing this group of users following the path that they did a developer will be able to analyze the data and realize that after creating a profile, the “play game” button does not appear. Thus, path analysis was able to provide actionable data for the company to act on and fix an error.\n\nIn eCommerce, path analysis can help customize a shopping experience to each user. By looking at what products other customers in a certain cohort looked at before buying one, a company can suggest “items you may also like” to the next customer and increase the chances of them making a purchase. Also, path analysis can help solve performance issues on a platform. For example, a company looks at a path and realizes that their site freezes up after a certain combinations of events. By analyzing the path and the progression of events that led to the error, the company can pinpoint the error and fix it.\n\nHistorically path analysis fell under the broad category of website analytics, and related only to the analysis of paths through websites. Path analysis in website analytics is a process of determining a sequence of pages visited in a visitor session prior to some desired event, such as the visitor purchasing an item or requesting a newsletter. The precise order of pages visited may or may not be important and may or may not be specified. In practice, this analysis is done in aggregate, ranking the paths (sequences of pages) visited prior to the desired event, by descending frequency of use. The idea is to determine what features of the website encourage the desired result. \"Fallout analysis,\" a subset of path analysis, looks at \"black holes\" on the site, or paths that lead to a dead end most frequently, paths or features that confuse or lose potential customers.\n\nWith the advent of big data along with web-based applications, online games, and eCommerce platforms, path analysis has come to include much more than just web path analysis. Understanding how users move through an app, game, or other web platform are all part of modern-day path analysis.\n\nIn the real world when you visit a shop the shelves and products are not placed in a random order. The shop owner carefully analyzes the visitors and path they walk through the shop, especially when they are selecting or buying products. Next the shop owner will reorder the shelves and products to optimize sales by putting everything in the most logical order for the visitors. In a supermarket this will typically result in the wine shelf next to a variety of cookies, chips, nuts, etc. Simply because people drink wine and eat nuts with it.\n\nIn most web sites there is a same logic that can be applied. Visitors who have questions about a product will go to the product information or support section of a web site. From there they make a logical step to the frequently asked questions page if they have a specific question. A web site owner also wants to analyze visitor behavior. For example, if a web site offers products for sale, the owner wants to convert as many visitors to a completed purchase. If there is a sign-up form with multiple pages, web site owners want to guide visitors to the final sign-up page.\n\nPath analysis answers typical questions like: \n\"Where do most visitors go after they enter my home page?\"\n\"Is there a strong visitor relation between product A and product B on my web site?\". \nQuestions that can't be answered by page hits and unique visitors statistics.\n\nGoogle Analytics provides a path function with funnels and goals. A predetermined path of web site pages is specified and every visitor walking the path is a goal. This approach is very helpful when analyzing how many visitors reach a certain destination page, called an end point analysis.\n\nThe paths visitors walk in a web site can lead to an endless number of unique paths. As a result, there is no point in analyzing each path, but to look for the strongest paths. These strongest paths are typically shown in a graphical map or in text like: Page A --> Page B --> Page D --> Exit.\n\n\n"}
{"id": "32457345", "url": "https://en.wikipedia.org/wiki?curid=32457345", "title": "Photonic curing", "text": "Photonic curing\n\nPhotonic curing is the high-temperature thermal processing of a thin film using pulsed light from a flashlamp. When this transient processing is done on a low-temperature substrate such as plastic or paper, it is possible to attain a significantly higher temperature than the substrate can ordinarily withstand under an equilibrium heating source such as an oven. Since the rate of most thermal curing processes (drying, sintering, reacting, annealing, etc.) generally increase exponentially with temperature (i.e. they obey the Arrhenius equation), this process allows materials to be cured much more rapidly than with an oven.\n\nIt has become a transformative process used in the manufacture of printed electronics as it allows inexpensive and flexible substrates to be substituted for traditional glass or ceramic substrates. Additionally, the higher temperature processing afforded by photonic curing reduces the processing time exponentially, often from minutes down to milliseconds, which increases throughput all while maintaining a small machine footprint.\n\nPhotonic curing is used as a thermal processing technique in the manufacturing of printed electronics as it allows the substitution of glass or ceramic substrate materials with inexpensive and flexible substrate materials such as polymers or paper. The effect can be demonstrated with an ordinary camera flash. Industrial photonic curing systems are typically water cooled and have controls and features similar to industrial lasers. The pulse rate can be fast enough to allow curing on the fly at speeds beyond 100 m/min making it suitable as a curing process for roll-to-roll processing. Material processing rates can exceed 1 m/s.\n\nThe maturing complexity of modern printed electronics for customer applications demands high throughput manufacturing and improved device function. The functionality of the printed electronics is critically important as customers demand more out of each device. Multiple layers are designed into each device, requiring ever more versatile processing techniques. Photonic curing is uniquely suited to complement the processing needs in the manufacture of modern printed electronics by providing a fast, reliable and transformative processing step. Photonic curing enables a lower thermal processing budget with current materials, and it can provide a path to incorporate more advanced materials and functionality into future printed electronics.\n\nPhotonic curing is similar to Pulse Thermal Processing, developed at Oak Ridge National Laboratory, in which a plasma arc lamp is used. In the case of photonic curing, the radiant power is higher and the pulse length is shorter. The total radiant exposure per pulse is less with photonic curing, but the pulse rate is much faster. \n"}
{"id": "25044237", "url": "https://en.wikipedia.org/wiki?curid=25044237", "title": "Positivo Tecnologia", "text": "Positivo Tecnologia\n\nPositivo Tecnologia (formerly known as Postivo Informática) is a Brazilian technology company headquartered in Curitiba, Paraná. It is the Information Technology arm of the larger organization Grupo Positivo. It is the largest computer manufacturer in Latin America and the tenth largest in the world.\n\nIn addition to computers, produces educational software, electronic games and set-top box for Brazilian digital television. The company also serves as an OEM/ODM/EMS.\n\nThe company invests in innovative solutions that integrate hardware, which results in products and services tailored to the consumer. It is present in more than 5000 Brazilian cities and has a strong performance in retail, public, and corporate markets.\n\nIt is listed on the Novo Mercado of B3.\n\n\nIn 2004, Positivo began actively targeting the retail market. Positivo was primarily focused on middle-class families who likely had never been able to purchase a home personal computer previously. Positivo creates lower-cost computers which are more accessible to the growing middle class in Brazil given their low cost: with financing, the cheapest of these PCs can be paid off for $30/month, making it possible to reach families making ~$270/month. These computers are designed to serve both as a TV and a personal computer.\n\nNow, Positivo makes a wider range of computers costing up to $2000 targeted toward higher-income customers.\n\nAccording to Grupo Positivo, Positivo Informática:\n\n"}
{"id": "43924657", "url": "https://en.wikipedia.org/wiki?curid=43924657", "title": "Quadraphonic open reel tape", "text": "Quadraphonic open reel tape\n\nQuadraphonic open reel tape or Q4 was the first medium for quadraphonic sound recording and playback, introduced to the American market by the Vanguard Recording Society in June 1969.\n\nIt was based on reel-to-reel tape, and was first used in European electronic-music studios by 1954.\n\nLike other quadraphonic formats it was unsuccessful and disappeared by the late 1970s.\n\nAll available four tracks were used in one direction on the ¼-inch tape, playing at a speed of 7½ inches per second (twice the speed of the regular 4-Track reel to reel tapes).\n\nThe four fully discrete tracks had full-bandwidth (unlike Q8 cartridges which had limited dynamic range).\n"}
{"id": "22845262", "url": "https://en.wikipedia.org/wiki?curid=22845262", "title": "Real-time locating system", "text": "Real-time locating system\n\nReal-time locating systems (RTLS) are used to automatically identify and track the location of objects or people in real time, usually within a building or other contained area. Wireless RTLS tags are attached to objects or worn by people, and in most RTLS, fixed reference points receive wireless signals from tags to determine their location. Examples of real-time locating systems include tracking automobiles through an assembly line, locating pallets of merchandise in a warehouse, or finding medical equipment in a hospital.\n\nThe physical layer of RTLS technology is usually some form of radio frequency (RF) communication, but some systems use optical (usually infrared) or acoustic (usually ultrasound) technology instead of or in addition to RF. Tags and fixed reference points can be transmitters, receivers, or both, resulting in numerous possible technology combinations.\n\nRTLS are a form of local positioning system, and do not usually refer to GPS or to mobile phone tracking. Location information usually does not include speed, direction, or spatial orientation.\n\nThe term RTLS was created (circa 1998) at the ID EXPO trade show by Tim Harrington (WhereNet), Jay Werb, (PinPoint), and Bert Moore, (Automatic Identification Manufacturers, Inc.(AIM)). It was created to describe and differentiate an emerging technology that not only provided the automatic identification capabilities of active RFID tags, but also added the ability to view the location on a computer screen. It was at this show that the first examples of a commercial radio based RTLS system were shown by PinPoint and WhereNet. Although this capability had been utilized previously by military and government agencies, the technology had been too expensive for commercial purposes. In the early 1990s, the first commercial RTLS were installed at three healthcare facilities in the United States, and were based on the transmission and decoding of infrared light signals from actively transmitting tags. Since then, new technology has emerged that also enables RTLS to be applied to passive tag applications.\n\nRTLS are generally used in indoor and/or confined areas, such as buildings, and do not provide global coverage like GPS. RTLS tags are affixed to mobile items to be tracked or managed. RTLS reference points, which can be either transmitters or receivers, are spaced throughout a building (or similar area of interest) to provide the desired tag coverage. In most cases, the more RTLS reference points that are installed, the better the location accuracy, until the technology limitations are reached.\n\nA number of disparate system designs are all referred to as \"real-time locating systems\", but there are two primary system design elements:\n\nThe simplest form of choke point locating is where short range ID signals from a moving tag are received by a single fixed reader in a sensory network, thus indicating the location coincidence of reader and tag. Alternately, a choke point identifier can be received by the moving tag, and then relayed, usually via a second wireless channel, to a location processor. Accuracy is usually defined by the sphere spanned with the reach of the choke point transmitter or receiver. The use of directional antennas, or technologies such as infrared or ultrasound that are blocked by room partitions, can support choke points of various geometries.\n\nID signals from a tag is received by a multiplicity of readers in a sensory network, and a position is estimated using one or more locating algorithms, such as trilateration, multilateration, or triangulation. Equivalently, ID signals from several RTLS reference points can be received by a tag, and relayed back to a location processor. Localization with multiple reference points requires that distances between reference points in the sensory network be known in order to precisely locate a tag, and the determination of distances is called ranging.\n\nAnother way to calculate relative location is if mobile tags communicate directly with each other, then relay this information to a location processor.\n\nRF trilateration uses estimated ranges from multiple receivers to estimate the location of a tag. RF triangulation uses the angles at which the RF signals arrive at multiple receivers to estimate the location of a tag. Many obstructions, such as walls or furniture, can distort the estimated range and angle readings leading to varied qualities of location estimate. Estimation-based locating is often measured in accuracy for a given distance, such as 90% accurate for 10 meter range.\n\nSystems that use locating technologies that do not go through walls, such as infrared or ultrasound, tend to be more accurate in an indoor environment because only tags and receivers that have line of sight (or near line of sight) can communicate.\n\nRTLS can be used numerous logistical or operational areas such as:\n\nRTLS may be seen as a threat to privacy when used to determine the location of people. The newly declared human right of informational self-determination gives the right to prevent one's identity and personal data from being disclosed to others, and also covers disclosure of locality, though this does not generally apply to the workplace.\n\nSeveral prominent labor unions have come out against the use of RTLS systems to track workers calling them \"the beginning of Big Brother\" and \"an invasion of privacy\". A common strategy to overcome opposition to these systems is often similar to the following. However, this loss of privacy may be outweighed by other benefits to staff. For example, Toronto General Hospital is looking at RTLS to reduce quarantine times after an infectious disease outbreak. After a recent SARS outbreak, 1% of all staff were quarantined, and more accurate data regarding who had been exposed to the virus could have reduced the need for quarantines.\n\nThere is a wide variety of systems concepts and designs to provide real-time locating.\n\nA general model for selection of the best solution for a locating problem has been constructed at the Radboud University of Nijmegen.\nMany of these references do not comply with the definitions given in international standardization with ISO/IEC 19762-5 and ISO/IEC 24730-1. However, some aspects of real-time performance are served and aspects of locating are addressed in context of absolute coordinates.\n\nDepending on the physical technology used, at least one and often some combination of ranging and/or angulating methods are used to determine location:\n\nReal-time locating is affected by a variety of errors. Many of the major reasons relate to the physics of the locating system, and may not be reduced by improving the technical equipment.\n\n\nMany RTLS systems require direct and clear line of sight visibility. For those systems, where there is no visibility from mobile tags to fixed nodes there will be no result or a non valid result from locating engine. This applies to satellite locating as well as other RTLS systems such as angle of arrival and time of arrival. Fingerprinting is a way to overcome the visibility issue: If the locations in the tracking area contain distinct measurement fingerprints, line of sight is not necessarily needed. For example, if each location contains a unique combination of signal strength readings from transmitters, the location system will function properly. This is true, for example, with some Wi-Fi based RTLS solutions. However, having distinct signal strength fingerprints in each location typically requires a fairly high saturation of transmitters.\n\n\nThe measured location may appear entirely faulty. This is a generally result of simple operational models to compensate for the plurality of error sources. It proves impossible to serve proper location after ignoring the errors.\n\n\n\"Real time\" is no registered branding and has no inherent quality. A variety of offers sails under this term. As motion causes location changes, inevitably the latency time to compute a new location may be dominant with regard to motion. Either an RTLS system that requires waiting for new results is not worth the money or the operational concept that asks for faster location updates does not comply with the chosen systems approach.\n\n\nLocation will never be reported \"exactly\", as the term \"real-time\" and the term \"precision\" directly contradict in aspects of measurement theory as well as the term \"precision\" and the term \"cost\" contradict in aspects of economy. That is no exclusion of precision, but the limitations with higher speed are inevitable.\n\n\nRecognizing a reported location steadily apart from physical presence generally indicates the problem of insufficient over-determination and missing of visibility along at least one link from resident anchors to mobile transponders. Such effect is caused also by insufficient concepts to compensate for calibration needs.\n\n\nNoise from various sources has an erratic influence on stability of results. The aim to provide a steady appearance increases the latency contradicting to real time requirements.\n\n\nAs objects containing mass have limitations to jump, such effects are mostly beyond physical reality. Jumps of reported location not visible with the object itself generally indicate improper modeling with the location engine. Such effect is caused by changing dominance of various secondary responses.\n\n\nLocation of residing objects gets reported moving, as soon as the measures taken are biased by secondary path reflections with increasing weight over time. Such effect is caused by simple averaging and the effect indicates insufficient discrimination of first echoes.\n\nThe basic issues of RTLS are standardized by the International Organization for Standardization and the International Electrotechnical Commission, under the ISO/IEC 24730 series. In this series of standards, the basic standard ISO/IEC 24730-1 identifies the terms describing a form of RTLS used by a set of vendors, but does not encompass the full scope of RTLS technology.\n\nCurrently several standards are published:\n\nThese standards do not stipulate any special method of computing locations, nor the method of measuring locations. This may be defined in specifications for trilateration, triangulation or any hybrid approaches to trigonometric computing for planar or spherical models of a terrestrial area.\n\n\nIn RTLS application in the Healthcare industry, various studies were issued discussing the limitations of the currently adopted RTLS. Currently used technologies RFID, Wi-fi, UWB, all RFID based are hazardous in the sense of interference with sensitive equipment. A study carried out by Dr Erik Jan van Lieshout of the Academic Medical Centre of the University of Amsterdam published in \"JAMA\" (\"Journal of the American Medical Equipment\") claimed \"RFID and UWB could shut down equipment patients rely on\" as \"RFID caused interference in 34 of the 123 tests they performed\". The first Bluetooth RTLS provider in the medical industry is supporting this in their article: \"The fact that RFID cannot be used near sensitive equipment should in itself be a red flag to the medical industry\". The RFID Journal responded to this study not negating it rather explaining real-case solution: \"The Purdue study showed no effect when ultrahigh-frequency (UHF) systems were kept at a reasonable distance from medical equipment. So placing readers in utility rooms, near elevators and above doors between hospital wings or departments to track assets is not a problem\". However the case of ”keeping at a reasonable distance” might be still an open question for the RTLS technology adopters and providers in medical facilities.\n\nIn many applications it is very difficult and at the same time important to make a proper choice among various communication technologies (e.g., RFID, WiFi, etc.) which RTLS may include. Wrong design decision made at early stages can lead to catastrophic results for the system and a significant loss of money for fixing and redesign. To solve this problem a special methodology for RTLS design space exploration was developed. It consists of such steps as modelling, requirements specification and verification into a single efficient process.\n\n\n"}
{"id": "42281221", "url": "https://en.wikipedia.org/wiki?curid=42281221", "title": "Scrumban", "text": "Scrumban\n\nScrumban is an Agile management methodology describing hybrids of Scrum and Kanban and was originally designed as a way to transition from Scrum to Kanban. Today, Scrumban is a management framework that emerges when teams employ Scrum as their chosen way of working and use the Kanban Method as a lens through which to view, understand and continuously improve how they work.\nAs the Kanban method was becoming more popular , Scrumban was developed as an attempt to make it easier for existing Scrum teams to begin exploring Lean and Kanban concepts .\n\nThe first article on Scrumban, which uses the spelling \"Scrum-ban\", describes several levels to transition from Scrum to Kanban.\n\nFundamentally, Scrumban is a management framework that emerges when teams employ Scrum as their chosen way of working and use the Kanban Method as a lens through which to view, understand and continuously improve how they work.\n\nScrumban is distinct from Scrum in the way it emphasizes certain principles and practices that are substantially different from Scrum's traditional foundation. Among these are:\n\nScrumban is distinct from the Kanban Method in that it:\n\nPerhaps most importantly, the principles and practices embedded within Scrumban are not unique to the software development process. They can be easily applied in many different contexts, providing a common language and shared experience across interrelated business functions. This, in turn, enhances the kind of organizational alignment that is an essential characteristic of success.\n\nWhen Corey Ladas introduced the world to Scrumban in his seminal book of that name, he defined it as a transition method for moving software development teams from Scrum to a “more evolved” software development framework. In actual practice, however, Scrumban has itself evolved to become a family of principles and practices that create complementary capabilities unique from both Scrum and the Kanban Method. These capabilities have led to three distinct manifestations:\n\n\nIn Scrumban, the teamwork is organized in small iterations and monitored with the help of a visual board, similar to Scrum and kanban boards. To illustrate each stage of work, teams working in the same space often use post-it notes or a large whiteboard. In the case of decentralized teams, visual management software such as Assembla, Targetprocess, Eylean Board, JIRA, Mingle or Agilo for Trac are often used. Planning meetings are held to determine what user stories to complete in the next iteration. The user stories are then added to the board and the team completes them, the team working on a few user stories at a time as practical (the work-in-progress, or WIP, limit). To keep iterations short, WIP limits are thus used, and a planning trigger is set in place for the team to know when to plan next - when WIP falls below a predetermined level. There are no predefined roles in Scrumban; the team keeps the roles they already have.\n\nWork iterations in Scrumban are kept short. This ensures that a team can easily adapt and change their course of action to a quickly changing environment. The length of the iteration is measured in weeks. The ideal length of an iteration depends on the work process of each team, and it is recommended not to have iterations exceeding two weeks. Velocity (a measure of productivity) is often used by the team to assess issues and trends in its throughput, in order to support continuous improvement.\n\nThe planning in Scrumban is based on demand and occurs only when the planning trigger goes off. The planning trigger is associated with the number of tasks left in the \"To Do\" section of the board - when it goes down to a certain number, the planning event is held. The number of tasks that should trigger a planning event is not predefined. It depends on a team's velocity (how quickly it can finish the remaining tasks) and on the time required to plan the next iteration. The tasks planned for the next iteration are added to the \"To Do\" section of the board.\n\nIt is recommended to prioritize tasks during the planning event. This means the tasks are added to the board with marked priorities. It helps the team members to know which tasks should be completed first and which can be completed later. The prioritization can be done by adding numbers to the tasks or by adding an additional priority column, where the most important tasks are put at the top and the less important tasks below.\n\nBucket size planning brings the possibility of long-term planning to Scrumban. It is based on the system of three buckets that the work items need to go through before making it on the Scrumban board. The three buckets represent three different stages of the plan and are usually called 1-year, 6-month and 3-month buckets. The 1-year bucket is dedicated for long-term goals that the company has, like penetrating a new market, releasing new product, etc. When the company decides to move forward with a plan, it is moved to the 6-month bucket, where the main requirements of this plan are crystallized. When a company is ready to start implementing the plan, the requirements are moved into the 3-month bucket and divided into clear tasks to be completed by the project team. It is from this bucket that the team draws tasks during their on-demand planning meeting and starts working on the tasks.\n\nThe basic Scrumban board is composed out of three columns: To Do, Doing and Done. After the planning meeting, the tasks are added to the To Do column, when a team member is ready to work on a task, he/she moves it to the Doing column and when he/she completes it, he/she moves it to the Done column. The Scrumban board visually represents the progress of the team. The task board columns are adapted and expanded based on the team's work progress. The most common add-ons include priority columns in the To Do section and columns like Design, Manufacturing, Testing in the Doing section.\n\nWIP limits -- \nTo ensure that the team is working effectively, Scrumban methodology states that a team member should be working on no more than one task at a time. To make sure this rule is followed Scrumban uses WIP (work in progress) limit. This limit is visualized on top of the Doing section of the board (also could be on each column of that section) and means that only that number of tasks can be in the corresponding column at one time. A WIP limit usually is equal to the number of people in the team but could be expanded based on the specifics of the team's work.\n\nTo Do limits --\nIn order to have more productive planning meetings, the number of tasks in the To Do section can be limited as well. The same as with WIP limits, it is written at the top of the To Do section or on top of the corresponding columns and limits the number of tasks in the To Do section or specific columns.\n\nScrumban does not require any specific number of team members or team roles. The roles a team has prior to adopting Scrumban are kept when implementing Scrumban. They are reinforced by team members having to choose the tasks to complete themselves. The team roles in Scrumban are more specialized and less cross-functional than what is expected in scrum teams.\n\nIn Scrumban tasks are not assigned to the team members by the team leader or project manager. Each team member chooses which task from the To Do section they are going to complete next. This guarantees a smooth process flow, where all the team members are equally busy at all times.\n\nFeature freeze is used in Scrumban when the project deadline is approaching. It means that only the features that the team already has for development can still be worked on and no additional features can be added.\n\nTriage usually happens right after feature freeze. With an approaching project deadline, the project manager decides which of the in-development features will be completed and which will stay unfinished. This guarantees that the team can focus on finishing important features before the project deadline and forget the less important ones.\n\n\nLike other methods, Scrumban can be implemented with a help of various tools. The most basic Scrumban implementation is a physical whiteboard with sticky notes. Electronic solutions, similar to scrum and kanban electronic boards are available as well. They offer a full automation of the board, where it only has to be updated by the team members. Electronic boards often also provide automatic reports, the possibility of attachments and discussions on tasks, time tracking, as well as integrations with other commonly used project management software.\n\n"}
{"id": "50255269", "url": "https://en.wikipedia.org/wiki?curid=50255269", "title": "Spotswood sewer tunnel", "text": "Spotswood sewer tunnel\n\nThe Spotswood sewer tunnel is a sanitary sewer tunnel in Melbourne, Victoria. It was constructed in 1895 to take sewerage under the Yarra River to the Spotswood Pumping Station, where it was pumped to the Werribee Sewage Farm.\n\nThe Melbourne and Metropolitan Board of Works (MMBW) was created in 1892 and appointed eminent British engineer James Mansergh to advise on a suitable system. However, local engineer William Thwaites was responsible for the design and construction. The system included sewer mains from the Melbourne CBD and southeastern suburbs extending across Fishermans Bend to a point opposite the pumping station. A tunneling shield was imported from Britain, to the design of British engineer James Henry Greathead. This was a 3.4 metres diameter cylindrical shield made of cast iron and steel plate. It was driven into the soft clays by hydraulic rams and maintained at two to three times atmospheric pressure to inhibit groundwater inflow with an airlock to allow miners to enter and exit. Cast-iron ring segments were bolted behind the shield as it advanced and then the tunnel was lined with 30 centimeters of concrete.\n\nOn the night 12 April 1895 (Good Friday), the shield failed and the river flooded into the tunnel drowning a young engineer and five workers. Three other men were waiting to enter through the air lock, and saw the incident through a small thick glass window, but were unable to do anything. Thousands of tons of clay were dumped in the riverbed to seal the hole and the tunnel pumped out. the bodies were recovered and work resumed under a new contractor. The river tunnel was finally completed 12 months later.\n\nA memorial for the six who died is located at the West Gate Bridge Memorial Park off Hyde Street.\n\n"}
{"id": "4196196", "url": "https://en.wikipedia.org/wiki?curid=4196196", "title": "Stainless steel soap", "text": "Stainless steel soap\n\nStainless steel soap is a piece of stainless steel, in the form of a soap bar or other hand-held shape. Its purported purpose is to neutralize or reduce strong odors such as those from handling garlic, onion, durian, guava, salami, or fish.\n\nScientific evidence of its efficacy appears lacking.\n\n"}
{"id": "970198", "url": "https://en.wikipedia.org/wiki?curid=970198", "title": "Technology demonstration", "text": "Technology demonstration\n\nA technology demonstration or demonstrator model, informally known as a tech demo, is a prototype, rough example or an otherwise incomplete version of a conceivable product or future system, put together as proof of concept with the primary purpose of showcasing the possible applications, feasibility, performance and method of an idea for a new technology. They can be used as demonstrations to the investors, partners, journalists or even to potential customers in order to convince them of the viability of the chosen approach, or to test them on ordinary users.\n\nTechnology demonstrations are often used in the computer industry, emerging as an important tool in response to short development cycles, in both software and hardware development.\n\n\nComputer technology demos should not be confused with demoscene-based demos, which, although often demonstrating new software techniques, are regarded as a stand-alone form of computer art.\n\nDemo Slam, a website from Google Inc., as launched with the slogan \"Bring your creativity. Bring your tech. Just bring it in general, fool! Demo Slam is here!\", is a large collection of technology demonstrations uploaded by users, and some of the Google executives as well, which will go to the 'Contender'.\n\nSales Engineering staff, often bearing the title Sales Engineer or Presales Consultant, will prepare technology demonstrations for business meetings or seminars to show capabilities of business products. This can include both software and hardware products, and can show multiple products integrating together. Usually a demonstration is less than a Proof of concept, but can come some of the way to showing how a business project may be justified. Large companies with tens or hundreds of Sales Engineers will often have a team who specialize in the production of demonstration systems and plans.\n\n\n"}
{"id": "22160435", "url": "https://en.wikipedia.org/wiki?curid=22160435", "title": "Teseq", "text": "Teseq\n\nTeseq AG, formerly Schaffner Test Systems is a supplier of Electromagnetic compatibility (EMC) test solutions. They develop and manufacture instruments for EMC emissions and immunity testing both for radiated and conducted emissions and immunity. Teseq operates ISO 17025 accredited calibration laboratories with EMC specialization.\n\nTeseq was twice nominated for Best in Test by Test & Measurement World Magazine and has been awarded A2LA certification. They delivered over 10,000 electrostatic discharge (ESD) simulators. Teseq equipment may be found in most EMC test laboratories.\n\nTeseq, formerly Schaffner Test Systems was the first company to recognize the threat of EMC emissions and interference and begin offering EMC instruments.\n\n1962 Schaffner Switzerland established by Dr. Hans Schaffner\n1971 First EMC test instrument launched\n1981 First electrostatic discharge (ESD) generator released\n1975–1990 Expansion to France, US, Singapore, Japan and China\n1998 Acquisition of Chase EMC Ltd., Capel, UK\n1999 Acquisition of MEB Messelektronik Berlin GmbH\n2006 Management buy-out and establishment of the new Teseq company as an AG (\"Aktiengesellschaft\")\n2012 Acquisition of MILMEGA and IFI \n"}
{"id": "1705347", "url": "https://en.wikipedia.org/wiki?curid=1705347", "title": "The Misadventures of Merlin Jones", "text": "The Misadventures of Merlin Jones\n\nThe Misadventures of Merlin Jones is a 1964 Walt Disney production starring Tommy Kirk and Annette Funicello. Kirk plays a college student who experiments with mind-reading and hypnotism, leading to run-ins with a local judge. Funicello plays his girlfriend (and sings the film's title song, accompanied by Disneyland's very own harmony quartet, The Yachtsmen, written by brothers Robert and Richard Sherman).\n\nThis film led to a 1965 sequel called \"The Monkey's Uncle\".\n\nMidvale College student Merlin Jones (Tommy Kirk), who is always involved with mind experiments, designs a helmet that connects to an electroencephalographic tape that records mental activity. He is brought before Judge Holmsby (Leon Ames) for wearing the helmet while driving and his license is suspended. Merlin returns to the lab and discovers accidentally that his new invention enables him to read minds.\n\nJudge Holmsby visits the diner where Merlin works part-time, and Merlin, through his newly found powers, learns that the judge is planning a crime. After informing the police, he is disregarded as a crackpot. Merlin and Jennifer (Annette Funicello), his girlfriend, break into Judge Holmsby's house looking for something to prove Holmsby's criminal intent but are arrested by the police. Holmsby then confesses that he is the crime book author, \"Lex Fortis,\" and asks that this identity be kept confidential. \n\nMerlin's next experiment uses hypnotism. After hypnotizing Stanley, Midvale's lab chimp, into standing up for himself against Norman (Norm Grabowski) - the bully student in charge of caring for Stanley, Merlin gets into a fight with Norman, and is brought before Judge Holmsby again. Intrigued by Merlin's experiments, the judge asks for Merlin's help in constructing a mystery plot for his next book. \n\nWorking on the premise that no honest person can be made to do anything they wouldn’t do otherwise – especially commit a crime – Merlin hypnotizes Holmsby and instructs him to kidnap Stanley. Shocked when the judge actually commits the crime, Merlin and Jennifer return the chimp, but are charged for the theft themselves. The judge sentences Merlin to jail, completely unaware of his own role in the crime. Livid at the injustice, Jennifer persuades Holmsby of his own guilt, and the good judge admits that there might be a little dishonesty in everybody.\n\nThe screen credit for writing reads, \"Screenplay by Tom and Helen August\", which were the pseudonyms for Alfred Lewis Levitt and Helen Levitt, two writers who were blacklisted.\n\nTo date Disney has not officially stated whether or not this film was actually two episodes of a planned television series, but this has long been suspected to be the case, with at least one critic, Eugene Archer, of \"The New York Times\", writing upon its release: \n\"Movies made for television are commonplace these days, but the idea of screening television shows in movie theaters is still farfetched. Who is expected to spend the $2? Strange as it sounds, this seems to be the explanation behind Walt Disney's latest hit, \"The Misadventures of Merlin Jones.\" It is a pastiche of two separate stories with the same set of characters, each running less than an hour (leaving time for commercials), stitched together in the middle and released yesterday in neighborhood theaters.\" \n\nFilming took place in early 1963. In March of that year it was reported NBC were so pleased with the results they wanted more Merlin Jones adventures. It appears that Disney then decided to release the movie theatrically.\n\nThe \"Chicago Tribune\" called it \"a kooky comedy of the type young people will enjoy thoroughly... good natured nonsense.\"\n\nAlthough critics were not impressed, audiences seemed to love it, as the film grossed over $4 million in North America, surprising even Disney. It made enough money to encourage a sequel in 1965.\n\n"}
{"id": "46469086", "url": "https://en.wikipedia.org/wiki?curid=46469086", "title": "The New Screen Savers", "text": "The New Screen Savers\n\nThe New Screen Savers is a weekly online TV show (podcast) hosted by Leo Laporte on the TWiT.tv network. The show is a spiritual successor of \"The Screen Savers\" on TechTV. On \"The New Screen Savers\" Laporte is typically joined by one of a rotating group of Twit.TV co-hosts including Megan Morrone, Mike Elgan (through December 8, 2015), Jason Howell, and Fr. Robert Ballecer SJ. The show occasionally is co-hosted by guest hosts such as Iain Thomson, Patrick Norton, Martin Sargent, Jason Calacanis or Jason Snell. The show is produced by Jerry Wagley (lead producer) and Anthony Nielsen (producer, technical director).\n\nThe program features tech news, interviews with makers, tech evangelists, scientists, photographers, musicians, call-in-help, and more. It is recorded live in front of a studio audience in the TWiT EastSide studio in Petaluma, California, United States. The show is streamed live on Saturdays at 3:00 P.M.PST and available on all popular podcatcher clients like Apple Podcasts, and Google Play. The show opener was filmed in San Francisco, CA and Petaluma, CA with the hosts riding Segways in-route to the studio.\n"}
{"id": "3827125", "url": "https://en.wikipedia.org/wiki?curid=3827125", "title": "Thomas B. Sheridan", "text": "Thomas B. Sheridan\n\nThomas B. Sheridan (born 23 December 1929, Cincinnati, OH) is American professor of mechanical engineering and Applied Psychology Emeritus at the Massachusetts Institute of Technology. He is a pioneer of robotics and remote control technology.\n\nSheridan was born Cincinnati, Ohio. In 1951 he received the B.S. degree in Mechanical Engineering from Purdue University in West Lafayette, IN, in 1954 the M.S.Eng. degree from the University of California, Los Angeles, the Sc.D. degree from the Massachusetts Institute of Technology (MIT) in 1959, Cambridge, and the Dr. (honorary) from Delft University of Technology, The Netherlands.\n\nFor most of his professional career he remained at MIT. He was assistant Professor of Mechanical Engineering from 1959 to 1964. Associate Professor of Mechanical Engineering from 1964 to 1970. Professor of Mechanical Engineering from 1970 to 1984. Professor of Engineering and Applied Psychology since 1984, and Professor of Aeronautics and Astronautics since 1993. In 1995-96 he was Ford Professor.\n\nHe is currently Professor Emeritus in the Departments of Mechanical Engineering and Department of Aeronautics and Astronautics. He has also served as a visiting professor at University of California, Berkeley, Stanford, Delft University, Kassel University, Germany, and Ben Gurion University, Israel.\n\nHe was co-editor of the MIT Press journal \"Presence: Teleoperators and Virtual Environments\" and served on several editorial boards; and was editor of \"IEEE Transactions on Man-Machine Systems\". \n\nSheridan chaired the National Research Council’s Committee on Human Factors, and has served on numerous government and industrial advisory committees. He is principal of Thomas B. Sheridan and Associates, a consulting firm. He was also President of the IEEE Systems, Man, and Cybernetics Society. He was President of HFES, and is a member of the National Academy of Engineering.\n\nSheridan received their Norbert Wiener and Joseph Wohl awards, the IEEE Centennial Medal (1984) and Third Millennium Medal. He is also a Fellow of the Human Factors and Ergonomics Society, recipient of their Paul M. Fitts Award, He received the 1997 National Engineering Award of the American Association of Engineering Societies and the 1997 Rufus Oldenburger Medal of the American Society of Mechanical Engineers.\n\nHis research interests are in experimentation, modeling, and design of human-machine systems in air, highway and rail transportation, space and undersea robotics, process control, arms control, telemedicine, and virtual reality. Working at MIT, Sheridan developed important concepts concerning human–robot interaction, particularly regarding supervisory control and telepresence.\n\nRobotics and telepresence is just one manifestation of his interest the boundary between human and automatic control. His book \"Humans and Automation\" is a concise summary of the history, issues, and progress in the role of the human and technology in automation.\n\nHe has published some books and over 200 technical papers. Books:\n\n\n"}
{"id": "9612222", "url": "https://en.wikipedia.org/wiki?curid=9612222", "title": "Travel plan", "text": "Travel plan\n\nA travel plan is a package of actions designed by a workplace, school or other organisation to encourage safe, healthy and sustainable travel options. By reducing car travel, travel plans can improve health and wellbeing, free up car parking space, and make a positive contribution to the community and the environment. Every travel plan is different, but most successful plans have followed a structured process in their development:\n\nThe term has now largely replaced green transport plan as the accepted UK term for a concept, which first emerged in the US in the 1970s (as site-based transportation demand management) and subsequently transferred to the Netherlands in 1989, where the terms company or commuter mobility management were applied.\n\nFrom the above and other definitions, these common features underpin the concept:\n\n\nThey can work well the 'package approach' allows complementary tools to be implemented in one go, which means effective but unpopular tools (such as parking restrictions) can be introduced alongside popular but expensive tools (like bus subsidies) to deliver the required benefits whilst cancelling out the negative impacts. Next, the use of the additional 'agent' such as a workplace, school or even a football club which means that travel plans replace the largely negative relationship between local authorities and citizens with a more positive relationship (such as between employer and employee or between school and parent/pupil). Finally, the site l-specific nature of travel plans means they are developed at the neighbourhood level and so focus directly on the transport needs of the users in that local area.\nThe concept works by developing balanced packages of user-focused transport tools in a partnership that seeks to provide meaningful benefits to each of the stakeholders involved: improved travel choices to the individuals; cost savings, happier and healthier staff and better company image to the implementing organisations; additional business opportunities to service providers and congestion reduction and improved air quality to the government.\n\nThe UK Department for Transport defines workplace travel plans as a package of measures produced by employers to encourage staff to use alternatives to single-occupancy car use. The first travel Ppans in the UK were adopted in Nottingham by Nottinghamshire County Council in 1995. Travel plans are now common in the UK, and are starting to become more common in many places throughout Europe as well as in Australia and New Zealand.\n\nA workplace can choose to develop a travel plan at any time be required to develop a travel plan as a condition of planning consent for an expansion or new development. Typical actions in a workplace travel plan include improving facilities for pedestrians and cyclists (showers, lockers and cycle parking), promotion and subsidy of public transport, and encouraging carpooling, working from home and teleconferencing.\n\nMaking it safer and easier for children to walk, cycle or catch public transport to school has long-term health benefits, reduces air pollution and traffic congestion, and helps children arrive at school awake, refreshed and ready to learn. \n\nBecause of the many benefits, local councils in the UK, Australia and New Zealand are actively involved in helping schools to develop and implement travel plans. In Canada, a national pilot project running from 2010 to 2012 is designed to bring stakeholders together to build school travel plans collaboratively. Typical actions in a school travel plan include promoting the health benefits of walking, providing more or better pedestrian crossings, tighter enforcement of parking and traffic rules around the school, providing cycle training, and setting up a walking school bus. School travel planning groups like Green Communities Canada also work on a policy level to encourage multi-tiered governmental policies that support active travel.\n\nA framework travel plan may be used for speculative development such as a business park where the occupiers of buildings are not known or where there will be multiple occupiers (such as a shared office block).\n\nThere are many examples of successful travel plans for tertiary campuses. Successful tertiary travel plans are usually prepared with the assistance of the local public transport agency. As well as the initiatives listed for school or workplace travel plans, tertiary travel plans can include a U-pass system for student travel on public transport. \n\nThe development of travel plans for hospitals is a relatively new and interesting field of travel planning.\n\nA real-estate developer may be required to provide a travel plan as a condition to gaining planning consent. A typical travel plan for a new development will provide for the promotion of sustainable transport through marketing initiatives and for contributions to public transport and to walking and cycling infrastructure. In the UK, a travel plan can form part of a Section 106 agreement, under the Town and Country Planning Act 1990.\n\n\n"}
{"id": "15470684", "url": "https://en.wikipedia.org/wiki?curid=15470684", "title": "U.S. Military connector specifications", "text": "U.S. Military connector specifications\n\nConnectors used by U.S. Department of Defense were originally developed in the 1930s for severe aeronautical and tactical service applications, and the Type \"AN\" (Army-Navy) series set the standard for modern military circular connectors. These connectors, and their evolutionary derivatives, are often called Military Standard, \"MIL-STD\", or (informally) \"MIL-SPEC\" or sometimes \"MS\" connectors. They are now used in aerospace, industrial, marine, and even automotive commercial applications.\n\nConnectors usually consist of (i) a mating pair (plug and receptacle) each equipped with male (\"pin\") or female (\"socket\") contacts; note that at least one of the connector halves, or its contacts, should be floating to minimize mechanical stresses.\n\n\n\n\n\n\n\n\n\nSelection of connector alternatives that are not defined by military specifications (MIL-C or MIL-DTL) can use either designated performance specifications (MIL-PRF) issued by the Department of Defense (DoD) or by using Commercial Item Descriptions (CID) issued by the General Services Administration (GSA) pursuant to DoD 4120.24-M, or by using standards developed by nationally and internationally recognized technical, professional, and industry associations and societies, collectively referred to as \"Non-Government Standards Bodies\" (NGSBs).\n\nPerformance Specifications: These connector specifications are intended to describe product that is essentially the same quality previously defined by familiar military specifications and built under the DoD's Qualified Manufacturer List (QML) product/supplier controlled system rather than the more-stringent Qualified Product Line (QPL) system.\n\n\nCommercial Item Descriptions (CID): CIDs are specifications describing products that are defined by the connector manufacturer’s specification versus a military specification. These products may not be suitable for environmentally severe or critical, communication or tactical military applications; however the items may be an acceptable cost effective choice in less demanding military or commercial applications.\n"}
{"id": "12845696", "url": "https://en.wikipedia.org/wiki?curid=12845696", "title": "Wheat lamp", "text": "Wheat lamp\n\nA wheat lamp is a type of incandescent light designed for use in underground mining, named for inventor Grant Wheat and manufactured by Koehler Lighting Products in Wilkes-Barre, Pennsylvania, United States, a region known for extensive mining activity.\n\nA safety lamp designed for use in potentially hazardous atmospheres such as firedamp and coal dust, the lamp is mounted on the front of the miner's helmet and powered by a wet cell battery worn on the miner's belt. The average wheat lamp uses a 3-5 watt bulb which will typically operate for 5 to 16 hours depending on the amp-hour capacity of the battery and the current draw of the bulb being used.\n\nA grain of wheat lamp is an unrelated, very small incandescent lamp used in medical and optical instruments, as well as for illuminating miniature railroad and similar models.\n"}
{"id": "5682069", "url": "https://en.wikipedia.org/wiki?curid=5682069", "title": "XvYCC", "text": "XvYCC\n\nxvYCC or Extended-gamut YCC (also x.v.Color) is a color space that can be used in the video electronics of television sets to support a gamut 1.8 times as large as that of the sRGB color space. xvYCC was proposed by Sony, specified by the IEC in October 2005 and published in January 2006 as IEC 61966-2-4.\n\nxvYCC was motivated by the fact that modern display and capture technologies often have underlying RGB primaries with significantly higher saturation than the traditional CRT displays, allowing them to handle a wider color gamut. But these devices have been unable to do this without upsetting basic calibration, as all existing video storage and transmission systems are based on CRT primaries, and are hence limited to the CRT gamut.\n\nxvYCC-encoded video retains the same color primaries and white point as BT.709, and uses either a BT.601 or BT.709 RGB-to-YCC conversion matrix and encoding. This allows it to travel through existing digital YCC data paths, and any colors within the normal gamut will be compatible.\n\nThe xvYCC color space permits YCC values that, while within the encoding range of YCC, have chroma values outside the range 16–240, or that correspond to negative RGB values, and hence would not have previously been valid. These are used to encode more saturated colors. For example, a cyan that lies outside the basic gamut of the primaries can be encoded as \"green plus blue minus red\".\n\nThese extra-gamut colors can then be displayed by a device whose underlying technology is not limited by the standard primaries.\n\nIn a paper published by Society for Information Display in 2006, the authors mapped the 769 colors in the Munsell Color Cascade to the BT.709 space and to the xvYCC space. 55% of the Munsell colors could be mapped to the sRGB gamut, but 100% of those colors could map to the xvYCC gamut. Deeper hues can be created – for example a deeper red by giving the opposing color (cyan) a negative coefficient.\n\nA mechanism for signaling xvYCC support and transmitting the gamut boundary definition for xvYCC has been defined in the HDMI 1.3 Specification. No new mechanism is required for transmitting the xvYCC data itself, as it is compatible with HDMI's existing YCbCr formats, but the display needs to signal its readiness to accept the extra-gamut xvYCC values, and the source needs to signal the actual gamut in use to help the display to intelligently adapt extreme colors to its own gamut limitations.\n\nThis should not be confused with HDMI 1.3's other new color feature, deep color. This is a separate feature that increases the precision of brightness and color information, and is independent of xvYCC.\n\nxvYCC is not supported by DVD-Video but is supported by the high-definition recording format AVCHD and PlayStation 3.\n\nOn January 7, 2013, Sony announced that it would release \"Mastered in 4K\" Blu-ray Disc titles which are sourced at 4K and encoded at 1080p. \"Mastered in 4K\" Blu-ray Disc titles can be played on existing Blu-ray Disc players and will support a larger color space using xvYCC.\n\nOn May 30, 2013, Eye IO announced that their encoding technology was licensed by Sony Pictures Entertainment to deliver 4K Ultra HD video. Eye IO encodes their video assets at 3840 x 2160 and includes support for the xvYCC color space.\n\nThe following graphics hardware support xvYCC color space when connected to a display device supporting xvYCC:\n\n"}
{"id": "1142475", "url": "https://en.wikipedia.org/wiki?curid=1142475", "title": "Yard (land)", "text": "Yard (land)\n\nA yard is an area of land immediately adjacent to a building or a group of buildings. It may be either enclosed or open. The word comes from the same linguistic root as the word \"garden\" and has many of the same meanings. \n\nA number of derived words exist, usually tied to a particular usage or building type. Some may be archaic or in lesser use now. Examples of such words are: courtyard, barnyard, hopyard, graveyard, churchyard, brickyard, prison yard, railyard, junkyard and stableyard.\n\nThe word \"yard\" came from the Anglo-Saxon \"geard\", compare \"garden\" (German \"Garten\"), Old Norse \"garðr\", Russian \"gorod\" = \"town\" (originally as an \"\"enclosed\" fortified area\"), Latin \"hortus\" = \"garden\" (hence horticulture and orchard), from Greek χορτος (chortos) = \"farm-yard\", \"feeding-place\", \"fodder\", (from which \"hay\" originally as grown in an enclosed field). \"Girdle,\" and \"court\" are other related words from the same root.\n\nIn areas where farming is an important part of life, a yard is also a piece of enclosed land for farm animals or other agricultural purpose, often referred to as a cattleyard, sheepyard, stockyard, etc. In Australia portable or mobile yards are sets of transportable steel panels used to build temporary stockyards.\n\nIn North America and Australasia today, a yard can be any part of a property surrounding or associated with a house or other residential structure, usually (although not necessarily) separate from a garden (where plant maintenance is more formalized). A yard will typically consist mostly of lawn or play area. The yard in front of a house is referred to as a front yard, the area at the rear is known as a backyard. Backyards are generally more private and are thus a more common location for recreation. Yard size varies with population density. In urban centres, many houses have very small or even no yards at all. In the suburbs, yards are generally much larger and have room for such amenities as a patio, a playplace for children, or a swimming pool. \n\nIn British English, these areas would usually be described as a \"garden\", similarly subdivided into a \"front garden\" and a \"back garden\", although paved areas may be called a yard, but more usually a patio. In modern Britain, the term yard is often used for depots and land adjacent to or among workplace buildings, as well as uncultivated land adjoining a building.\n\nIn North America, the term \"garden\" refers only to the area that contains plots of vegetables, herbs, flowers, and/or ornamental plants; and the term \"yard\" does not refer to the \"garden\", although the flower garden or vegetable garden may be within the yard.\n\n"}
{"id": "3514267", "url": "https://en.wikipedia.org/wiki?curid=3514267", "title": "Zinc telluride", "text": "Zinc telluride\n\nZinc telluride is a binary chemical compound with the formula ZnTe. This solid is a semiconductor material with a direct band gap of 2.26 eV. It is usually a p-type semiconductor. Its crystal structure is cubic, like that for sphalerite and diamond.\n\nZnTe has the appearance of grey or brownish-red powder, or ruby-red crystals when refined by sublimation. Zinc telluride typically had a cubic (sphalerite, or \"zincblende\") crystal structure, but can be also prepared as rocksalt crystals or in hexagonal crystals (wurtzite structure). Irradiated by a strong optical beam burns in presence of oxygen. Its lattice constant is 0.6101 nm, allowing it to be grown with or on aluminium antimonide, gallium antimonide, indium arsenide, and lead selenide. With some lattice mismatch, it can also be grown on other substrates such as GaAs, and it can be grown in thin-film polycrystalline (or nanocrystalline) form on substrates such as glass, for example, in the manufacture of thin-film solar cells. In the wurtzite (hexagonal) crystal structure, it has lattice parameters a = 0.427 and c = 0.699 nm.\n\nZinc telluride can be easily doped, and for this reason it is one of the more common semiconducting materials used in optoelectronics. ZnTe is important for development of various semiconductor devices, including blue LEDs, laser diodes, solar cells, and components of microwave generators. It can be used for solar cells, for example, as a back-surface field layer and p-type semiconductor material for a CdTe/ZnTe structure or in PIN diode structures.\n\nThe material can also be used as a component of ternary semiconductor compounds, such as CdZnTe (conceptually a mixture composed from the end-members ZnTe and CdTe), which can be made with a varying composition x to allow the optical bandgap to be tuned as desired.\n\nZinc telluride together with lithium niobate is often used for generation of pulsed terahertz radiation in time-domain terahertz spectroscopy and terahertz imaging. When a crystal of such material is subjected to a high-intensity light pulse of subpicosecond duration, it emits a pulse of terahertz frequency through a nonlinear optical process called optical rectification. Conversely, subjecting a zinc telluride crystal to terahertz radiation causes it to show optical birefringence and change the polarization of a transmitting light, making it an electro-optic detector.\n\nVanadium-doped zinc telluride, \"ZnTe:V\", is a non-linear optical photorefractive material of possible use in the protection of sensors at visible wavelengths. ZnTe:V optical limiters are light and compact, without complicated optics of conventional limiters. ZnTe:V can block a high-intensity jamming beam from a laser dazzler, while still passing the lower-intensity image of the observed scene. It can also be used in holographic interferometry, in reconfigurable optical interconnections, and in laser optical phase conjugation devices. It offers superior photorefractive performance at wavelengths between 600–1300 nm, in comparison with other III-V and II-VI compound semiconductors. By adding manganese as an additional dopant (ZnTe:V:Mn), its photorefractive yield can be significantly increased.\n\n"}
