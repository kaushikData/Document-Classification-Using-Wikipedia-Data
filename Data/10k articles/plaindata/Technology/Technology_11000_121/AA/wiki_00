{"id": "45339245", "url": "https://en.wikipedia.org/wiki?curid=45339245", "title": "Alice Perry", "text": "Alice Perry\n\nAlice Jacqueline Perry (24 October 1885 – 21 April 1969) was a poet and the first woman in Ireland to graduate with a degree in engineering.\n\nBorn in Wellpark, Galway in 1885, Alice was one of five daughters of James and Martha Perry (née Park). Her father was the County Surveyor in Galway West and co-founded the Galway Electric Light Company. Her uncle, John Perry, was a Fellow of the Royal Society and invented the navigational gyroscope.\n\nAfter graduating from the High School in Galway, she won a scholarship to study in Royal University, Galway in 1902. Having excelled in mathematics, she changed from studying for a degree in arts to an engineering degree. She graduated with first class honours in 1906. The family appear to have been academically gifted. Her sisters Molly and Nettie also went on to third level education; a third sister Agnes earned BA (1903) and MA (1905) in mathematics from Queen's College Galway (later UCG then NUIG), taught there in 1903–1904, was a Royal University of Ireland examiner in mathematics in 1906, and later became assistant headmistress at a secondary school in London.\n\nFollowing her graduation she was offered a senior postgraduate scholarship but owing to her father's death the following month, she did not take up this position. In December 1906 she succeeded her father temporarily as county surveyor for Galway County Council. She remained in this position for five or six months until a permanent appointment was made. She was an unsuccessful candidate for the permanent position and for a similar opportunity to be a surveyor in Galway East. She remains the only woman to have been a County Surveyor (County Engineer) in Ireland.\n\nIn 1908 she moved to London with her sisters, where she worked as a Lady Factory Inspector for the Home Office. From there she moved to Glasgow, at which point she converted from Presbyterianism to Christian Science in 1915. She met and married John (Bob) Shaw on 30 September 1916. Shaw was a soldier who died in 1917 on the Western Front.\n\nPerry retired from her inspector's position in 1921 and became interested in poetry, first publishing in 1922. In 1923 she moved to Boston, the headquarters of Christian Science. Until her death in 1969, Perry worked within the Christian Science movement as a poetry editor and practitioner, publishing seven books of poetry.\n\nAn All-Ireland medal has been named in her honour, The Alice Perry Medal, with the first prizes awarded in 2014.\n\nOn Monday 6 March 2017, NUI Galway held an official ceremony to mark the naming of the Alice Perry Engineering Building.\n\n\n\n\n"}
{"id": "343230", "url": "https://en.wikipedia.org/wiki?curid=343230", "title": "Autopilot", "text": "Autopilot\n\nAn autopilot is a system used to control the trajectory of an aircraft without constant 'hands-on' control by a human operator being required. Autopilots do not replace human operators, but instead they assist them in controlling the aircraft. This allows them to focus on broader aspects of operations such as monitoring the trajectory, weather and systems.\n\nThe autopilot is often used in conjunction with the autothrottle, when present, which is the analogous system controlling the power delivered by the engines.\nThe autopilot system on airplanes is sometimes colloquially referred to as \"George\".\n\nIn the early days of aviation, aircraft required the continuous attention of a pilot to fly safely. As aircraft range increased, allowing flights of many hours, the constant attention led to serious fatigue. An autopilot is designed to perform some of the tasks of the pilot.\n\nThe first aircraft autopilot was developed by Sperry Corporation in 1912. The autopilot connected a gyroscopic heading indicator and attitude indicator to hydraulically operated elevators and rudder. (Ailerons were not connected as wing dihedral was counted upon to produce the necessary roll stability.) It permitted the aircraft to fly straight and level on a compass course without a pilot's attention, greatly reducing the pilot's workload.\n\nLawrence Sperry (the son of famous inventor Elmer Sperry) demonstrated it in 1914 at an aviation safety contest held in Paris. At the contest, Sperry demonstrated the credibility of the invention by flying the aircraft with his hands away from the controls and visible to onlookers of the contest. Elmer Sperry Jr., the son of Lawrence Sperry, and Capt Shiras continued work after the war on the same autopilot, and in 1930 they tested a more compact and reliable autopilot which kept a US Army Air Corps aircraft on a true heading and altitude for three hours.\n\nIn 1930, the Royal Aircraft Establishment in the United Kingdom developed an autopilot called a pilots' assister that used a pneumatically-spun gyroscope to move the flight controls.\n\nFurther development of the autopilot was performed, such as improved control algorithms and hydraulic servomechanisms. Also, inclusion of additional instrumentation such as the radio-navigation aids made it possible to fly during night and in bad weather. In 1947 a US Air Force C-54 made a transatlantic flight, including takeoff and landing, completely under the control of an autopilot. \nBill Lear developed his F-5 automatic pilot and automatic approach control system, and was awarded the Collier Trophy for 1949.\n\nIn the early 1920s, the Standard Oil tanker \"J.A. Moffet\" became the first ship to use an autopilot.\n\nNot all of the passenger aircraft flying today have an autopilot system. Older and smaller general aviation aircraft especially are still hand-flown, and even small airliners with fewer than twenty seats may also be without an autopilot as they are used on short-duration flights with two pilots. The installation of autopilots in aircraft with more than twenty seats is generally made mandatory by international aviation regulations. There are three levels of control in autopilots for smaller aircraft. A single-axis autopilot controls an aircraft in the roll axis only; such autopilots are also known colloquially as \"wing levellers,\" reflecting their limitations. A two-axis autopilot controls an aircraft in the pitch axis as well as roll, and may be little more than a \"wing leveller\" with limited pitch oscillation-correcting ability; or it may receive inputs from on-board radio navigation systems to provide true automatic flight guidance once the aircraft has taken off until shortly before landing; or its capabilities may lie somewhere between these two extremes. A three-axis autopilot adds control in the yaw axis and is not required in many small aircraft.\n\nAutopilots in modern complex aircraft are three-axis and generally divide a flight into taxi, takeoff, climb, cruise (level flight), descent, approach, and landing phases. Autopilots exist that automate all of these flight phases except taxi and takeoff. An autopilot-controlled landing on a runway and controlling the aircraft on rollout (i.e. keeping it on the centre of the runway) is known as a CAT IIIb landing or Autoland, available on many major airports' runways today, especially at airports subject to adverse weather phenomena such as fog. Landing, rollout, and taxi control to the aircraft parking position is known as CAT IIIc. This is not used to date, but may be used in the future. An autopilot is often an integral component of a Flight Management System.\n\nModern autopilots use computer software to control the aircraft. The software reads the aircraft's current position, and then controls a flight control system to guide the aircraft. In such a system, besides classic flight controls, many autopilots incorporate thrust control capabilities that can control throttles to optimize the airspeed.\n\nThe autopilot in a modern large aircraft typically reads its position and the aircraft's attitude from an inertial guidance system. Inertial guidance systems accumulate errors over time. They will incorporate error reduction systems such as the carousel system that rotates once a minute so that any errors are dissipated in different directions and have an overall nulling effect. Error in gyroscopes is known as drift. This is due to physical properties within the system, be it mechanical or laser guided, that corrupt positional data. The disagreements between the two are resolved with digital signal processing, most often a six-dimensional Kalman filter. The six dimensions are usually roll, pitch, yaw, altitude, latitude, and longitude. Aircraft may fly routes that have a required performance factor, therefore the amount of error or actual performance factor must be monitored in order to fly those particular routes. The longer the flight, the more error accumulates within the system. Radio aids such as DME, DME updates, and GPS may be used to correct the aircraft position.\n\nA midway between fully automated flight and manual flying is Control Wheel Steering (CWS). Although it is becoming less observed as a stand-alone option in modern airliners, CWS is still a function on many aircraft today. Generally, an autopilot that is CWS equipped, has three positions being off, CWS, and CMD. In CMD (Command) mode the autopilot has full control of the aircraft, and receives its input from either the heading/altitude setting, radio and navaids, or the FMS (Flight Management System). In CWS mode, the pilot controls the autopilot through inputs on the yoke or the stick. These inputs are translated to a specific heading and attitude, which the autopilot will then hold until instructed to do otherwise. This provides stability in pitch and roll. Some aircraft employ a form of CWS even in manual mode, such as the MD-11 which uses a constant CWS in roll. In many ways, a modern Airbus fly-by-wire aircraft in Normal Law is always in CWS mode. The major difference is that in this system the limitations of the aircraft are guarded by the flight computer, and the pilot can not steer the aircraft past these limits.\n\nThe hardware of an autopilot varies from implementation to implementation, but is generally designed with redundancy and reliability as foremost considerations. For example, the Rockwell Collins AFDS-770 Autopilot Flight Director System used on the Boeing 777 uses triplicated FCP-2002 microprocessors which have been formally verified and are fabricated in a radiation-resistant process.\n\nSoftware and hardware in an autopilot is tightly controlled, and extensive test procedures are put in place.\n\nSome autopilots also use design diversity. In this safety feature, critical software processes will not only run on separate computers and possibly even using different architectures, but each computer will run software created by different engineering teams, often being programmed in different programming languages. It is generally considered unlikely that different engineering teams will make the same mistakes. As the software becomes more expensive and complex, design diversity is becoming less common because fewer engineering companies can afford it. The flight control computers on the Space Shuttle used this design: there were five computers, four of which redundantly ran identical software, and a fifth backup running software that was developed independently. The software on the fifth system provided only the basic functions needed to fly the Shuttle, further reducing any possible commonality with the software running on the four primary systems.\n\nA stability augmentation system (SAS) is another type of automatic flight control system; however, instead of maintaining the aircraft on a predetermined attitude or flight path, the SAS will actuate the aircraft flight controls to dampen out aircraft buffeting regardless of the attitude or flight path. SAS can automatically stabilize the aircraft in one or more axes. The most common type of SAS is the yaw damper which is used to eliminate the Dutch roll tendency of swept-wing aircraft. Some yaw dampers are integral to the autopilot system while others are stand-alone systems.\n\nYaw dampers usually consist of a yaw rate sensor (either a gyroscope or angular accelerometer), a computer/amplifier and a servo actuator. The yaw damper uses yaw rate sensor to sense when the aircraft begins a Dutch roll. A computer processes the signals from the yaw rate sensor to determine the amount of rudder movement that is required to dampen out the Dutch roll. The computer then commands the servo actuator to move the rudder that amount. The Dutch roll is dampened out and the aircraft becomes stable about the yaw axis. Because Dutch roll is an instability that is inherent to all swept-wing aircraft, most swept-wing aircraft have some sort of yaw damper system installed.\n\nThere are two types of yaw dampers: series yaw dampers and parallel yaw dampers. The servo actuator of a parallel yaw damper will actuate the rudder independently of the rudder pedals while the servo actuator of a series yaw damper is clutched to the rudder control quadrant and will result in pedal movement when the system commands the rudder to move.\n\nSome aircraft have stability augmentation systems that will stabilize the aircraft in more than a single axis. B-52s, for example, require both pitch and yaw SAS in order to provide a stable bombing platform. Many helicopters have pitch, roll and yaw SAS systems. Pitch and roll SAS systems operate much the same way as the yaw damper described above; however, instead of dampening out Dutch roll, they will dampen pitch and roll oscillations or buffeting to improve the overall stability of the aircraft.\n\nInstrument-aided landings are defined in categories by the International Civil Aviation Organization, or ICAO. These are dependent upon the required visibility level and the degree to which the landing can be conducted automatically without input by the pilot.\n\nCAT I - This category permits pilots to land with a decision height of and a forward visibility or Runway Visual Range (RVR) of . Autopilots are not required.\nCAT II - This category permits pilots to land with a decision height between and and a RVR of . Autopilots have a fail passive requirement.\n\nCAT IIIa -This category permits pilots to land with a decision height as low as and a RVR of . It needs a fail-passive autopilot. There must be only a 10 probability of landing outside the prescribed area.\n\nCAT IIIb - As IIIa but with the addition of automatic roll out after touchdown incorporated with the pilot taking control some distance along the runway. This category permits pilots to land with a decision height less than 50 feet or no decision height and a forward visibility of in Europe (76 metres, compare this to aircraft size, some of which are now over long) or in the United States. For a landing-without-decision aid, a fail-operational autopilot is needed. For this category some form of runway guidance system is needed: at least fail-passive but it needs to be fail-operational for landing without decision height or for RVR below .\n\nCAT IIIc - As IIIb but without decision height or visibility minimums, also known as \"zero-zero\". Not yet available on commercial airliners, but may be available in the near future.\n\nFail-passive autopilot: in case of failure, the aircraft stays in a controllable position and the pilot can take control of it to go around or finish landing. It is usually a dual-channel system.\n\nFail-operational autopilot: in case of a failure below alert height, the approach, flare and landing can still be completed automatically. It is usually a triple-channel system or dual-dual system.\n\nIn radio-controlled modelling, and especially RC aircraft and helicopters, an autopilot is usually a set of extra hardware and software that deals with pre-programming the model's flight.\n\n\n"}
{"id": "45719212", "url": "https://en.wikipedia.org/wiki?curid=45719212", "title": "Avi Yashchin", "text": "Avi Yashchin\n\nAvi Yashchin is an American businessman and entrepreneur. He founded CleanEdison, a New York-based green-jobs vocational education company, in 2008. One of the main purposes of the company is the provide clean innovation solutions to underserved areas. Yashchin is currently an Instructor with General Assembly, and has previously worked for Barclay's Capital and Lehman Brothers. He received his MBA from the Stern School of Business at New York University.\n\n"}
{"id": "12052371", "url": "https://en.wikipedia.org/wiki?curid=12052371", "title": "Bedrock mortar", "text": "Bedrock mortar\n\nA bedrock mortar (BRM) is an anthropogenic circular depression in a rock outcrop or naturally occurring slab, used by people in the past for grinding of grain, acorns or other food products. There are often a cluster of a considerable number of such holes in proximity indicating that people gathered in groups to conduct food grinding in prehistoric cultures. Correspondingly the alternative name gossip stone is sometimes applied, indicating the social context of the food grinding activity. Typical dimensions of the circular indentations are approximately 12 centimeters in diameter by 10 centimeters deep, although a considerable range of depths of the cavities have been documented . The bedrock mortar has been identified in a number of world regions, but has been particularly intensely documented in the Americas. An alternative term for the bedrock mortar site is bedrock milling station.\n\nA bedrock mortar should not be confused with a bedrock metate, which is a flat, trough-shaped depression often found with bedrock mortars.\n\n\n"}
{"id": "698505", "url": "https://en.wikipedia.org/wiki?curid=698505", "title": "Camera control unit", "text": "Camera control unit\n\nThe camera control unit (CCU) is typically part of a live television broadcast chain. It is responsible for powering the professional video camera, handling signals sent over the camera cable to and from the camera, and can be used to control various camera parameters remotely.\n\nBefore cameras became self contained units broadcast quality cameras required vast racks of control units just to generate a usable picture. In outside broadcast production, these racks took up an entire section of the OB truck and were operated by a small team of skilled engineers. These vision engineers had two roles. Firstly, they setup the CCUs at the start of a production and ensured that the picture created was of broadcast quality. This process included a lengthy 'lining up' process in which the vision engineer would work with the camera operator, to adjust the settings on both the actual camera and the CCU in tandem. During production, it was the vision engineers job to operate the CCUs and control both the focus and the colour balance. So skilled and labour intensive was this role that each CCU required its own dedicated vision engineer. \n\nModern cameras are more self-contained than their predecessors and most can operate without a CCU. Even when not required, a CCU can be of benefit for the following reasons.\n\nA CCU is often used in conjunction with a remote control panel (RCP), a waveform monitor and a vectorscope to rack and match many cameras together remotely.\n\nCommon adjustable parameters include:\n\nIn addition to these, there are usually options internally generating a test card for testing, return feeds or talkback. For more complex production, preset scene files can be recalled to quickly change the settings of the camera on the fly.\n\nBroadcast cameras typically carry several signals over the camera cable in addition to the camera output itself.\n\nTypically, RGB signals are transmitted over the camera cable. The CCU will usually convert these to SDI, YUV or composite for interfacing to other video equipment.\n\nTypical signals can be both digital signal and analog signals: \n\n\n"}
{"id": "2186375", "url": "https://en.wikipedia.org/wiki?curid=2186375", "title": "Carvery", "text": "Carvery\n\nA carvery is a meal served in a pub or a restaurant where cooked meat is freshly sliced to order for customers, sometimes offering unlimited servings in a buffet style for a fixed price. The term is most commonly used in the United Kingdom, Ireland, Cyprus, and Commonwealth countries like Canada and Australia, but it is also found in the United States.\n\nCarveries are often found in pubs and hotels, and are particularly commonly held at weekends, when they offer traditional Sunday roasts to a potentially large number of people. The meat is usually accompanied by a choice of potatoes (generally at least boiled, mashed and roasted) also stuffing followed by other vegetables (commonly including carrots, parsnips, peas and other traditional British vegetables), with gravy and a sauce considered a traditional accompaniment to the various meats (for example, mint sauce to accompany roast lamb, apple sauce to accompany roast pork and so on).\n\nCarveries existed as early as 1956 in London, in two Lyons Corner Houses. One of the restaurants, in each of the Strand and the Tottenham Court Road Lyons, was a carvery. They provided a three-course meal with beverage, but all but the carvery items were served by a Nippy (waitress). Even the carvery table had an employee to help those having difficulty in the actual carving. The price at this time was five shillings.\n\nIn the 1970s and later, many more carveries appeared in London. One well-known carvery was situated in the Regent Palace Hotel. The restaurant there was on the ground floor, the Art Deco ceiling of which has been reassembled in the new Air W1 building.\nLater they were operated by pub chains such as Harvester, Brewer's Fayre and Beefeater. The Toby Carvery brand took over many former Beefeater sites.\n\nThe chain of Fuzzy's Grub in London is a noted carvery, being voted \"Best Traditional British Restaurant, but all but the carv in London\" in Harden's 2007 guide.\n\nCarveries are also commonly offered by many local pubs.\n\nSome restaurants in the US use the term or concept, and it is a staple at some buffets. Examples include:\n\nMost California-style hofbrau restaurants may also be considered carveries.\n\n"}
{"id": "8004810", "url": "https://en.wikipedia.org/wiki?curid=8004810", "title": "Chopstick rest", "text": "Chopstick rest\n\nA chopstick rest is tableware, similar to a knife rest or a spoon rest, used to keep chopstick tips off the table and to prevent used chopsticks from contaminating or rolling off tables. Chopstick rests are found more commonly in restaurants than in homes. They come in various shapes and are made from clay, wood, plastic, metal, glass, porcelain or precious stones such as jade. If the chopsticks come in paper sleeves, some people fold the sleeves into chopstick rests.\n\nIn East Asia, chopstick rests are usually used at formal dinners. They are placed on the front-left side of the dishes, with the chopsticks parallel to the table edge and the points toward the left, or to the right side of the dishes, with the chopstick points towards to the front.\n\n\n"}
{"id": "9066481", "url": "https://en.wikipedia.org/wiki?curid=9066481", "title": "DF-21", "text": "DF-21\n\nThe Dong-Feng 21 (DF-21; NATO reporting name CSS-5 - Dong-Feng ()) is a two-stage, solid-fuel rocket, single-warhead medium-range ballistic missile (MRBM) in the Dong Feng series developed by China Changfeng Mechanics and Electronics Technology Academy. Development started in the late 1960s and was completed around 1985-86, but it was not deployed until 1991. It was developed from the submarine-launched JL-1 missile, and is China's first solid-fuel land-based missile. The U.S. Department of Defense in 2008 estimated that China had 60-80 missiles and 60 launchers; approximately 10-11 missiles can be built annually.\n\nOriginally developed as a strategic weapon, the DF-21's later variants were designed for both nuclear and conventional missions. It is thought able to carry a high explosive and submunition warheads, as well as a nuclear warhead of 300 kt. The latest DF-21D was said to be the world's first anti-ship ballistic missile (ASBM). The DF-21 has also been developed into a space-capable anti-satellite weapon/anti-missile weapon carrier.\n\nThough the launcher itself is mobile to reduce vulnerability, an actual launch unit requires support vehicles that can cover a 300×300-meter area, making it hard to move quickly and easier to detect. Also, the launcher is not made to travel off-road and requires solid ground when firing to prevent backblast and debris damage due to the hard launch, restricting its firing locations to roads and pre-made launch pads.\n\nThe basic variant DF-21 had a range of 1,770+ km, and a payload of consisting of a single 500 kt nuclear warhead, with an estimated circular error probable (CEP) of 300~400 m; this version did not enter operational service. The DF-21A was operational by 1996 and has improved accuracy with an estimated CEP of 100~300 m. This version is reported to have a similar 1,770+ km range, with a potential extended range of .\n\nRevealed in 2006, the DF-21C is a terminally guided version that has a maximum range believed to be about and accuracy estimated to be 50~10 m. The missile was the first dual-capable version, able to be armed with either a nuclear or conventional warhead. In 2010, the DF-21C was being deployed in central Western China.\n\nThis is an anti-ship ballistic missile that has a maximum range exceeding , according to the U.S. National Air and Space Intelligence Center. The Intelligence Center did not believe it was deployed in 2009. The guidance system is thought to be still in an evolutionary process as more UAVs and satellites are added.\n\nThe US Department of Defense stated in 2010 that China has developed and reached initial operating capability (IOC) of a conventionally armed high hypersonic land-based anti-ship ballistic missile based on the DF-21. This is the first ASBM and weapon system capable of targeting a moving aircraft carrier strike group from long-range, land-based mobile launchers. The DF-21D is thought to employ maneuverable reentry vehicles (MaRVs) with a terminal guidance system. It may have been tested in 2005-6, and the launch of the Jianbing-5/YaoGan-1 and Jianbing-6/YaoGan-2 satellites offering targeting information from synthetic aperture radar (SAR) and visual imaging respectively. The upgrades enhance China's ability to prevent US carriers from operating in the Taiwan Strait. Some have also suggested China could develop a \"MIRVd\" DF-21D with multiple independent missiles.\n\nUnited States Naval Institute in 2009 stated that such a warhead would be large enough to destroy an aircraft carrier in one hit and that there was \"currently ... no defense against it\" if it worked as theorized. The United States Navy has responded by switching its focus from a close blockade force of shallow water vessels to return to building deep water ballistic missile defense destroyers. The United States has also assigned most of its ballistic missile defense capable ships to the Pacific, extended the BMD program to all Aegis destroyers and increased procurement of SM-3 BMD missiles. The United States also has a large network optimized for tracking ballistic missile launches which may give carrier groups sufficient warning in order to move away from the target area while the missile is in flight. Kinetic defenses against the DF-21D would be difficult. The Navy's primary ballistic missile interceptor, the SM-3, would not be effective since it is designed to intercept missiles in the mid-course phase in space, so it would have to be launched almost immediately to hit before reentry or from an Aegis ship positioned under its flight path. The SM-2 Block 4 can intercept missiles reentering the atmosphere, but the warhead will be performing high-G maneuvers that may complicate interception.\n\nUse of such missile has been said by some experts to potentially lead to nuclear exchange, regional arms races with India and Japan, and the end of the INF Treaty between the United States and the Soviet Union, to which the People's Republic of China is not a party.\n\nChina has recently launched a series of satellites to support its ASBM efforts:\n\nChina is reported to be working on an Over-the-horizon radar to locate the targets for the ASBM. An apparent test of the missile was made against a target in the Gobi desert in January 2013.\n\nIn late 2013, a Russian Military Analysis report of the DF-21D concluded that the only way to successfully counter it would be through electronic countermeasures. Conventional interceptions of high-speed objectives have worked in the past, with the Russian report citing the 2008 interception of a malfunctioning satellite by a U.S. cruiser, but in that situation the warship had extensive knowledge of its location and trajectory. Against an attack from the Mach 10 DF-21D without knowing the missile's launch point, the U.S. Navy's only way to evade it would be through electronic countermeasures.\n\nThe emergence of the DF-21D has some analysts claiming that the \"carrier killer\" missiles have rendered the American use of aircraft carriers obsolete, as they are too vulnerable in the face of the new weapon and not worth the expense. Military leaders in the U.S. Navy and Air Force, however, do not see it as a \"game changer\" to completely count carriers out. First, the missile may not be able to single-handedly destroy its target, as the warhead is believed to be enough to only inflict a \"mission kill\" to make a carrier unable to conduct flight operations. Secondly, there is the problem of finding its target. The DF-21D has a range estimated between , so a carrier battle group would need to be located through other means before launching. Over-the-horizon radars cannot pinpoint their exact locations, and would have to be used in conjunction with Chinese recon satellites; recon aircraft and submarines could also look for them, but they are vulnerable to the carrier's defenses. Finally, although the DF-21D has radar and optical sensors for tracking, it has not yet been tested against a ship target moving at-sea at up to , let alone ones using clutter and countermeasures. The \"kill chain\" of the missile requires processing and constantly updating data of a carrier's location, preparing the launch, programming information, and then firing, a chain the U.S. military's AirSea Battle concept involves disrupting. Some U.S. analysts believe that the DF-21D doesn't fly any faster than Mach 5.\n\nThe DF-21D reentry vehicle appears to bear similarities to the American Pershing II missile's RV, which was withdrawn from service in 1988. The Pershing II's RV weighed and was fitted with four control fins to perform a 25-G pull-up after reentering the atmosphere, traveling at Mach 8 and then gliding to the target to pitch into a terminal dive. Army training manuals about the missile are available on the internet and public open-source literature extensively describes it; the DF-21 has a comparable range and payload. Though much is made of the DF-21D's damage infliction ability based solely on velocity and kinetic energy, the Australian Strategic Policy Institute has calculated that the energy of an inert RV impacting at Mach 6 had similar energy to the combined kinetic and explosive power of the American subsonic Harpoon anti-ship missile, which is one-quarter the energy of the Russian supersonic Kh-22 missile traveling at Mach 4 with a warhead.\n\nThe missile was shown to the public during the parade in Beijing celebrating 70 years since the end of World War II on September 3, 2015. A parade video shows missiles marked as DF-21D.\n\nThe DF-26 is a development of the DF-21 with range increased to to ; its existence was confirmed in the mid-2010s but it had already been in service for several years.\n\nAir-launched ballistic missile version of DF-21. carried by H-6K.The 3000 km range missile is scheduled for deployment in 2025.\n\nIn January 2014, Newsweek revealed that Saudi Arabia had secretly bought a number of DF-21 medium-range ballistic missiles in 2007. They also said that the American CIA had allowed the deal to go through as long as the missiles were modified to not be able to carry nuclear warheads. Saudi Arabia had previously secretly acquired Chinese DF-3A ballistic missiles in 1988, which was publicly revealed. While the DF-3 has a longer range, it was designed to carry a nuclear payload, and so had poor accuracy (0.6-2.4 miles (1000–4000 m) CEP) if used with a conventional warhead. It would only be useful against large area targets like cities and military bases. This made them useless during the Gulf War for retaliating against Iraqi Scud missile attacks, as they would cause mass civilian casualties and would not be as effective as the ongoing coalition air attacks. After the war, the Saudis and the CIA worked together to covertly allow the purchase of Chinese DF-21s. The DF-21 is solid-fueled instead of liquid-fueled like the DF-3, so it takes less time to prepare for launch. It is accurate to 30 meters CEP, allowing it to attack specific targets like compounds or palaces. The Saudis are not known to possess mobile launchers, but may use the same 12 launchers originally bought with the DF-3s. The number of DF-21 missiles that were bought is unknown. Newsweek speculates that details of the deal being made public is part of Saudi deterrence against Iran.\n\n"}
{"id": "47679455", "url": "https://en.wikipedia.org/wiki?curid=47679455", "title": "Darshan Singh Vohra", "text": "Darshan Singh Vohra\n\nDarshan Singh Vohra was an Indian Army officer, an engineer, one of the pioneers of prosthetics in Asia and the founder of the first prosthetic centre in Asia, \"Artificial Limb Centre\", Pune and \"Nevedac Prosthetic Centre\", another prosthetic facility in Chandigarh. After obtaining advanced training in prosthetics from Germany and England, he joined the Indian Army and was holding the rank of a colonel at the time of his superannuation. He founded the first prosthetic centre in Asia, \"Artificial Limb Centre\", immediately after World War II, in 1944, at Pune, for the rehabilitation of soldiers who lost their limbs in war and the facility has grown to become a 190 bedded healthcare centre, working attached to the Armed Forces Medical College, Pune.\n\nAfter retiring from the Army, the Government of Punjab invited him to set up a similar centre in Punjab and he established \"Nevedac Prosthetic Centre\", in the village of Zirakpur in the state in 1972. He served as an honorary adviser to the Government of Punjab, Government of Haryana and Chandigarh Administration and was a consultant to the National Industrial Corporation Limited, New Delhi for the setting up of the \"Artificial Limbs Manufacturing Corporation\" at Kanpur. Holder of a doctoral degree in Therapeutic Philosophy from The World University, Arizona, he was a fellow of the British Institute of Surgical Technologists, Artificial Limb Fitters, Great Britain and the Orthotics - Prosthetics Society of India and was a member of the Orthopaedic Works, West Germany and the International Society for Prosthetics and Orthotics, Copenhagen (ISPO). He also served as the president of the Orthotic - Prosthetic Society of India, from 1987 to 1991. The Government of India awarded him the fourth highest civilian honour of Padma Shri in 1988.\n\n"}
{"id": "18396206", "url": "https://en.wikipedia.org/wiki?curid=18396206", "title": "Drawdown card", "text": "Drawdown card\n\nDrawdown cards are used for testing paints and coatings through wet film preparation.\n\nBlack and white cards are used to measure both opacity and contrast ratio. By measuring reflectance values of both parts of the card one can gain a quantitative opacity value of a paint sample.\n\n"}
{"id": "36963916", "url": "https://en.wikipedia.org/wiki?curid=36963916", "title": "Edinburgh Science Triangle", "text": "Edinburgh Science Triangle\n\nThe Edinburgh Science Triangle (EST) is a multi-disciplinary partnership between universities, research institutes, the National Health Service, science parks, the national economic development agency Scottish Enterprise, and central and local government in Edinburgh and neighbouring council areas. The three points of the \"triangle\" are Livingston in West Lothian, Musselburgh in East Lothian, and the Easter Bush campus in Midlothian.\n\nThe collaborative project aims to attract new indigenous and inward investment, and to build a professional scientific community based on academic research and commercial enterprises. The target sectors for the project are the life sciences, informatics, micro- and optoelectronics and energy.\n\nThe Edinburgh Science Triangle was launched by Jim Wallace, the Deputy First Minister, in September 2004, at the Roslin BioCentre in Midlothian. It is a member of Edinburgh's Local Investment Partnership, which includes the City of Edinburgh Council, Edinburgh Chamber of Commerce, Scottish Development International and Scottish Enterprise. Scottish Development International promotes the Edinburgh Science Triangle abroad.\n\nFunding to promote and support the Edinburgh Science Triangle comes from Scottish Enterprise, the European Regional Development Fund, the City of Edinburgh, Midlothian and West Lothian councils, and the participating science parks.\n\n\n\n\n\n\n\n\n"}
{"id": "13784890", "url": "https://en.wikipedia.org/wiki?curid=13784890", "title": "Electromagnetic log", "text": "Electromagnetic log\n\nAn Electromagnetic Log, sometimes called an \"EM Log\", measures the speed of a vessel through water.\n\nIt operates on the principle that:\n1 when a conductor (such as water) passes through an electromagnetic field, a voltage is created and\n2 the amount of voltage created increases as the speed of the conductor increases.\n\nThe process is\n\n\n\nPit sword\n\n"}
{"id": "49107824", "url": "https://en.wikipedia.org/wiki?curid=49107824", "title": "Epos Now", "text": "Epos Now\n\nEpos Now is a cloud-based software provider, specialising in the design and manufacture of electronic point of sale (commonly referred to as EPOS), which encompasses features including but not limited to reporting, stock control, and CRM for retail and hospitality businesses. Epos Now software can be operated from any device or platform with a web-browser or by using Epos Now's IOS or Android app.\n\nEpos Now's UK headquarters are located in Norwich, England and their US headquarters are in Orlando, Florida. Founded in June 2011, the company was a pioneer in cloud-technology in the EPOS industry and is notable for being the first EPOS company to introduce an AppStore to their customers, which allows users to customise their system. The Epos Now AppStore was introduced in December 2014. Epos Now's innovation was recognised in 2016, when the company won a Queen's Award for Enterprise, the UK's highest accolade for business success.\n\nEpos Now is the UK's 13th fastest growing private technology company, and the fastest growing EPOS company in the UK.\n\nEpos Now was founded by Jacyn Heavens in 2011 with no external investment. CEO and founder Jacyn Heavens identified a gap in the market for an affordable EPOS system whilst managing a bar, and after searching for an EPOS system that would be suitable for his business, he started building software that would suit business owners like himself. The Epos Now Appstore was officially launched in September 2015, making it the first Appstore to be released within the EPOS industry. There are over 20,000 businesses using Epos Now.\n\nIn 2015, Epos Now was awarded 'Epos Innovation of the Year' by Retail Systems, and was shortlisted for EDP Business Awards and Tech Cities 2015.\n\nIn 2016 Epos Now was shortlisted for the UK Cloud Awards in the \"Most Innovative SMB Product of the Year\" category and the Engagement & Loyalty Awards in the \"Most Innovative Technology\" category. Epos Now was also a finalist in 2016 The Grocer Gold Awards, for the \"Technology Supplier of the Year\" category.\n\nOn 21 April 2016 it was announced the Epos Now had won a Queen's Award for Enterprise in the Innovation category.\n\nEpos Now were named Gold Stevie Award winners for The International Business Awards as \"The Most Innovative company of the Year 2016\".\n\nEpos Now won three Eastern Daily Press Business Awards in 2016 in the categories of; Employer of the Year, Tech Innovator of the year and Business of the Year sponsored by Barclays. \n\nThe company made the Tech Track 100 three years running, being named the 13th and 32nd and 53rd fastest growing tech company in the UK and named 30th fastest growing company in 2016's Deloitte Fast 50 with a growth of 597%. Epos Now were named 179th fastest growing tech company in the Deloitte 2016 Fast 500 EMEA. Epos Now were named Europes 46th fastest growing company in the Financial Times 1000 listing.\n\nEpos Now were recognised for their commitment to the professional development of their employees winning a Princess Royal Training Award in 2017.\n\nEpos Now were noted as 'Best SaaS Product for small business/SMBs' in the 2017 SaaS Awards.\n"}
{"id": "5024938", "url": "https://en.wikipedia.org/wiki?curid=5024938", "title": "Federation Against Copyright Theft", "text": "Federation Against Copyright Theft\n\nThe Federation Against Copyright Theft (informally FACT) is the UK's leading trade organisation established to protect and represent the interests of its members' Intellectual Property (IP).\n\nEstablished in 1983, FACT works closely with statutory law enforcement agencies to combat all forms of copyright infringement and members include global and UK film distributors, TV broadcasters and sport rights owners. Its sister-organisation the Australian Screen Association has a similar focus in the Oceania Region, taking over the responsibilities of the Australian Film and Video Security Office in the early 2000s.\n\nIn 2007 FACT reported seizing over 2.8m pirate DVDs and states it has \"enhanced its enforcement capabilities against those involved in the manufacture, distribution and sale of copyright material both online and in hard copy format\".\n\nAlso in 2007, FACT, in collaboration with UK police, took down well known hot-linking site Tv-links.co.uk. FACT makes the claim that the 26-year-old man from Cheltenham was arrested in connection with offences relating to the facilitation of copyright infringement on the internet whereas the arrest was over a matter of possible trademark infringement. While arrested under Section 92 of the Trade Marks Act 1994 he has now been released 'pending further investigation' with no charges filed against him as of 25 October. Section 92 of the Trade Marks Act 1994 deals with falsely applying signs to goods that may be mistaken for a registered trademark.\n\nIn June 2009, FACT brought a lawsuit against the company Scopelight and its founders for running a video search engine called Surfthechannel.com. The organisation accompanied a police raid on the Scopelights owner's homes, Anton and Kelly Vickerman, who collaborated with the police in the initial investigation and they allowed FACT employee's to inspect confiscated computers and the information on them. After a few months the police decided there was not currently sufficient evidence to prosecute the owners for criminal charges. Scopelight's owners requested their property back to which FACT refused claiming they were holding onto the equipment to be used for a civil case against the owners. The issue was brought to court and it was ruled that FACT's actions were improper and the equipment should have been returned as soon as police decided not to prosecute the owners of Scopelight.\n\nIn a subsequent appeal (Scopelight & Ors v Chief Constable of Northumbria Police & Federation Against Copyright Theft [2009] EWCA Civ 1156) The Court of Appeal overturned that judgment and instead ruled that the police did indeed have such a power. As a consequence of this successful appeal, the evidence against the directors of Scopelight Limited (Anton and Kelly Vickerman) was heard in a criminal trial at Newcastle Crown Court in June and July 2012. Both defendants were charged under Conspiracy to Defraud (two counts each) and the jury trial took place in front of His Honour Judge Evans.\n\nAfter a 7½-week trial, the jury found Kelly Vickerman not guilty on both counts and Anton Vickerman guilty on both counts of Conspiracy to Defraud.\n\nOn 14 August 2012 Anton Vickerman was sentenced to four years imprisonment on each charge of Conspiracy to Defraud, sentencing to run concurrently. FACT conducted an Intelligence gathering Operation against Vickerman and his family and associates to include an undercover operations.\n\nIn June 2013, FACT pressured the Usenet file indexing site called NZBsRus to close after issuing cease-and-desist letters to the owner and several staff members.\n\nFACT has produced several adverts which have appeared at the beginning of videos and DVDs released in the UK, as well as trailers shown before films in cinemas.\n\nDuring the 1990s, FACT created a 30-second to 1 minute anti-piracy warning called \"Beware of Illegal Video Cassettes\", reminding customers to check whether they have a genuine video and how to report questionable copies. They appeared on many different video cassettes by various home video companies. Versions for each studio depicting their respective security label (generally a hologram of the studio logo) were created, with several iterations for each as the FACT hotline number changed multiple times throughout the decade. The warning was placed at the beginning of practically every rental released VHS tape in the UK (as well as many retail tapes), similar to the FBI Warning found on tapes in the United States. CIC Video had a similar term, except the hologram carried CIC logo copies, and it was used internationally and it was translated, although UK used it from 1988 to 1991. Since late 1996, this warning was followed by a public information film featuring a man attempting to return a pirate video purchased from a market after discovering that the sound was garbled and the picture unwatchable, ending with the tagline \"Pirate Videos: Daylight Robbery.\" The \"Pirate Videos: Daylight Robbery\" ad was used until 2002. A precursor PIF, \"Video Piracy: It's Not Worth It!\" was released in 1995 that featured a young girl named Rebecca trying to watch a pirated VHS on a TV ending with a VHS player falling down with the words on top: \"VIDEO PIRACY. It's not worth it.\".\n\nIn 2002, FACT released a PIF called \"The Pirates are Out to Get You\". It featured a man destroying many items with an X-shaped branding iron, ending with the FACT logo and UK, Ireland, Australia & New Zealand hotlines. The warning was placed at the beginning of practically every rental VHS tape in the UK (as well as the majority of retail tapes), similar to the FBI Warning found on tapes in the United States. \n\nWith the advent of DVD, FACT borrowed the Motion Picture Association of America's anti piracy spot \"You Wouldn't Steal a Car\", which concentrated more on copyright infringement through peer-to-peer filesharing and less on counterfeit copies. The spot related the peer-to-peer file sharing of movies to stealing a handbag, a car, and other such items (similar to the US FAST \"Piracy is theft\" slogan of the 1990s).\n\nThe advert has been criticised by the general public and TV personalities alike: the most common complaint being that the advert only appears on genuinely purchased DVDs and cannot be skipped by fast forwarding or pressing the DVD menu button. More recent spots have included Knock-off Nigel, where a man is ridiculed by his friends and colleagues for buying counterfeit DVDs and downloading films from Bit Torrent along with ads that say \"Thank You\" to the British public for supporting the film industry by either buying a ticket and seeing a film in the cinema or purchasing a genuine DVD/Blu-ray.\n\n \n"}
{"id": "15833970", "url": "https://en.wikipedia.org/wiki?curid=15833970", "title": "Flip Video", "text": "Flip Video\n\nThe Flip Video cameras are an American series of tapeless camcorders for digital video created by Pure Digital Technologies, a company bought by Cisco Systems in March 2009; variants included the UltraHD, the MinoHD, and the SlideHD. Flip Video cameras were known for their simple interface with few buttons, minimal menus and built in USB plugs (from which they derived the flip name), and were marketed as making video \"simple to shoot, simple to share\" Production of the line of Flip video cameras ran from 2006 until April 2011, when Cisco Systems discontinued them as part of a move to \"...exit aspects of (their) consumer businesses.\" . Flip cameras contributed to an increase in the popularity of similar pocket camcorders, although the inclusion of HD video cameras in many smartphones has since made them a more niche product.\n\nFlip cameras can record videos at different resolutions. FlipHD camcorders digitally record high-definition video at 1280 x 720 resolution using H.264 video compression, Advanced Audio Coding (AAC) audio compression and the MP4 file format, while the older models used a 640 x 480 resolution. The MinoHD and SlideHD models had an internal lithium-ion rechargeable battery included, while the Ultra series included a removable battery that could be interchanged with standard AA or AAA batteries.\n\nAll models lacked memory card extension slots, though the Flip UltraHD(2 hr) could record to a storage device via FlipPort. Models could be connected to a computer with a flip-out USB connector, without the need for a USB cable. Flip Cameras recorded monaural sound, and used a simple clip-navigation interface with a D-pad and two control buttons which allowed for viewing of recorded videos, starting and stopping recording, and digital zoom. The third and final generation of Flip UltraHD cameras retailed for $149.99 and $199.99 for 4GB (1 hour) and 8GB (2 hour) models respectively, incorporated digital stabilization, and increased the frame rate from 30 to 60 frames per second. With FlipPort, users can plug in external accessories.\n\nAll Flip cameras included the required video player and 3ivx codec software, FlipShare, on the camera's internal storage. For all models after 2010, an HDMI cable could stream videos to TV screens.\n\nLater Flip Video models came in a variety of colors, and could be custom ordered with designs digitally painted on. Accessories for the Flip Video camera included an underwater case, a mini-tripod, a bicycle helmet attachment, and a wool case (Mino camcorders) or soft pouch (Flip UltraHD), rechargeable battery replacements for the UltraHD series, and an extension cable.\n\nFlip Video's accompanying software was called FlipShare, which facilitated downloads of videos, basic editing, and uploading to various websites. After the release of version 5.6, FlipShare no longer included a function to convert video to WMV format. While this has been acknowledged by Cisco as a defect, a fix has yet to be announced.\n\nThe first version was originally released as the \"Pure Digital Point & Shoot\" video camcorder on May 1, 2006 as a reusable follow-on to the popular CVS One-Time-Use Camcorder, a Pure Digital product sold through CVS/pharmacy stores that was designed for direct conversion to DVD media. This was in turn a line extension of previous digital disposable camera products (sold initially through Ritz Camera and associated brands under the Dakota Digital name). The camcorder was then renamed as the Flip Video a year later. On September 12, 2007, the Flip Ultra was released. The Flip Ultra was the best-selling camcorder on Amazon.com after its debut, capturing about 13% of the camcorder market. Flip products received an unusually large advertising campaign, including product placement, celebrity endorsements, and sponsoring of events such as concert tours during their introduction. From 2009, and through the Cisco takeover, the Flip range was sold in Europe by Widget UK\n\n\nA smaller version of the Flip, the Flip Video Mino, was released on June 4, 2008. The Mino captured video in 640x480 resolution at 30 frames per second. On launch it retailed for about US$180 in the United States, providing about 60 minutes of video recording capability with 2 GB flash memory capacity.\n\nThe third and final Flip MinoHD was released on September 20, 2010. It featured HD recording capabilities in the same dimensions as the second generation MinoHD (1280/720 at 30 fps), The only major change in the MinoHD third generation was Image Stabilization. Also released on September 20, 2010 was a 4 GB, MinoHD with one hour of recording capability. The one-hour version retailed for $179 and the two-hour version retailed for $229.\n\nFree Minos were made available to all audience members at YouTube Live due to Flip Video's sponsorship of the event. A station was even set up so people could upload the videos to YouTube.\n\nFlipShare TV was an accessory for the third generation Flip UltraHD camera, and allowed users to connect the TV base to their TV, plug in a USB transmitter key to their computer and view their Flipshare library.\n\nOn May 21, 2009, Cisco Systems acquired Pure Digital Technologies for US$ 590 million in stock.\n\nOn April 12, 2011, Cisco announced that it \"will exit aspects of its consumer business\" which includes shutting down the Flip Video division. \n\nSome observers suggested that the Flip was facing growing competition from camera phones, particularly smartphones (which disrupted consumer electronics trade such as point-and-shoot cameras, wristwatches, alarm clocks, portable music players and GPS devices) that had recently begun incorporating HD video cameras.\n\nContrary to popular perception, smartphones made up only a small fraction of overall worldwide sales of cell phones in 2011, and the Flip was still selling strongly when its discontinuation was announced. Other potential causes of the shutdown include the fact that consumer hardware was not part of Cisco's core businesses of services and software, and that their profit margins on consumer electronics at the time were narrow.\n\nAs Cisco shut down the Flip business instead of divesting of it, it has been suggested that the patents and other intellectual property from the acquisition could prove valuable to Cisco's videoconferencing business in the future.\n\n"}
{"id": "856622", "url": "https://en.wikipedia.org/wiki?curid=856622", "title": "Friden Flexowriter", "text": "Friden Flexowriter\n\nThe Friden Flexowriter was a teleprinter, a heavy-duty electric typewriter capable of being driven not only by a human typing, but also automatically by several methods, including direct attachment to a computer and by use of paper tape.\n\nElements of the design date to the 1920s, and variants of the machine were produced until the early 1970s; the machines found a variety of uses during the evolution of office equipment in the 20th century, including being among the first electric typewriters, computer input and output devices, forerunners of modern word processing, and also having roles in the machine tool and printing industries.\n\nThe Flexowriter can trace its roots to some of the earliest electric typewriters. In 1925, the Remington Typewriter Company wanted to expand their offerings to include electric typewriters. Having little expertise or manufacturing ability with electrical appliances, they partnered with Northeast Electric Company of Rochester and made a production run of 2500 electric typewriters. When the time came to make more units, Remington was suffering a management vacuum and could not complete contract negotiations, so Northeast began work on their own electric typewriter. In 1929, they started selling the Electromatic.\n\nIn 1931, Northeast was bought by Delco. Delco had no interest in a typewriter product line, so they spun the product off as a separate company called Electromatic. Around this time, Electromatic built a prototype automatic typewriter. This device used a wide roll of paper, similar to a player piano roll. For each key on the typewriter, there was a column on the roll of paper. If the key was to be pressed, then a hole was punched in the column for that key.\n\nThe Electromatic typewriter patents document the use of pivoted spiral cams operating against a hard rubber drive roller to drive the print mechanism. This was the foundation of essentially all later electric typewriters. The typewriter could be equipped with a \"remote control\" mechanism allowing one typewriter to control another or to record and play back typed data through a parallel data connection with one wire per typewriter key. The Electromatic tape perforator used a wide tape, with punch position per key on the keyboard.\n\nIn 1932, a code for the paper tape used to drive Linotype and other typesetting machines was standardized. This allowed use of a tape only five to seven holes wide to drive automatic typewriters, teleprinters and similar equipment.\n\nIn 1933, IBM wanted to enter the electric typewriter market, and purchased the Electromatic Corporation, renaming the typewriter the IBM Model 01, and continuing to use the Electromatic trademark.\n\nIBM experimented with several accessories and enhancements for its electric typewriter. In 1942, IBM filed a patent application for a typewriter that could print justified and proportionally spaced text. This required recording each line of text on a paper tape before it was printed. IBM experimented with a 12-hole paper tape compatible with their punched-card code.\nEventually, IBM settled on a 6-hole encoding, as documented in their automatic justifying typewriter patents filed in 1945. Equipping an electric typewriter with both a paper-tape reader and punch created the basic foundation for what would become the Flexowriter.\n\nBy the late 1930s, IBM had a nearly complete monopoly on unit record equipment and related punched card machinery, and expanding the product line into automatic typewriters equipped with paper tape raised antitrust issues. As a result, IBM sold the product line and factory to the Commercial Controls Corporation (CCC) of Rochester, New York, which also absorbed the National Postal Meter Corporation. CCC was formed by several former IBM employees.\n\nAround the time of World War II, CCC developed a proportional spacing model of the Flexowriter known as The Presidential (or sometimes the President). The model name was derived from the fact that these units were used to generate the White House letters informing families of the deaths of service personnel in the war. CCC also manufactured other complex mechanical devices for the war effort, including M1 carbines.\n\nIn 1944, the pioneering Harvard Mark I computer was constructed, using an Electromatic for output.\n\nIn 1950, Edwin O. Blodgett filed a patent on behalf of Commercial Controls Corporation for a \"tape controlled typewriter.\" This machine used a 6-level punched paper tape, and was the basis for the machines CCC and Friden built over the next 15 years. This improved machine was contemporaneous with the first generation of commercial computers. Applications for Flexowriters exploded in the 1950s, covering territory in commercial printing, machine tools, computers, and many forms of office automation. This versatility was helped by Friden's willingness to engineer and build many different configurations. In the late 1950s, CCC was purchased by Friden, a maker of electromechanical calculators, and it was under their name that the machines achieved their greatest diversity and success; applications are further detailed below.\n\nTwo tape stations allowed implementation of what was then called the form letter, the combination of standard text (one tape) with varying name and address information (the other tape).\n\nEdward (Edwin?) Blodgett, Chief Engineer of Friden R&D was replaced in 1964 while ill. It's unclear what effect this had on development, especially as Blodgett was apparently biased against electronics, favoring electromechnical solutions to design problems.\n\nFriden was acquired by the Singer Corporation in 1965. Singer had little or no understanding of the computer industry, and there was a clash of corporate culture with Friden employees.\nThere was a major redesign of the Flexowriter in the mid 1960s. The Model 2201 Programatic, introduced in 1965, had a sleek modern styling and 13 programmable function keys. This was the first major change in appearance of Flexowriters in nearly forty years. Programming was done using a 320-contact plugboard, and all of the logic was implemented using relays. The case, although modern looking, was entirely metal, giving the machine a shipping weight of 132 pounds (60 kg). The selling price was £2900 (British pounds). Although primarily sold as a stand-alone word processor (a term not yet in use at the time), Friden also sold it with a communications option allowing it to be used as a computer terminal. Members of the 2200 family operated at 135 words per minute (11.3 characters per second). The family also included the 2210 and 2211, on which the function keys were replaced with a numeric keypad, and the 2261, using ASCII instead of the proprietary 8-bit code used by other members of the 2200 family.\n\nThe 2300 series were cosmetically similar to the 2200 series, although without the function keys or numeric keypad, with a simplified plugboard, and operating at 145 words per minute (12 characters per second). In addition to the basic 2301, the 2302 supported the auxiliary tape readers and punches from the 2200 family. The 2304 offered proportional spacing and a carbon ribbon mechanism, making it suitable for preparing camera-ready copy. The base price for the 2300 family was £1400 (British pounds). This would be the last hurrah for the line, with production halting in the early 1970s.\n\nSales and innovation declined. In the late 1960s, the market for word processing equipment was shifting to magnetic media. IBM introduced the Magnetic Tape Selectric Typewriter (MT/ST) in 1964. In October, 1968, Information Control Systems introduced the Astrotype word processing system. Both of these used magnetic tape and Selectric print mechanisms. With its fixed type font and paper-tape recording medium, the Flexowriter had difficulty competing with these machines, although some Flexowriter documentation emphasized the fact that, unlike IBM's MT/ST tapes, Flexowriter users could cut and splice paper tapes, particularly if they could recognize some of the common codes such as carriage return. The Diablo daisy wheel printer, introduced in 1969, offered comparable print quality at twice the speed. Larger manufacturers such as IBM and DEC made their own console equipment, and video terminals began to appear, displacing paper-based systems. Eventually, even the CNC machine tool industry abandoned paper tape, although this was significantly slower because of the long working life of machine tools.\n\nFrom its earliest days through to at least the mid-1960s, Flexowriters were used as automatic letter writers.\n\nWhile the US White House was using them during the Second World War, in the 1960s, United States Members of Congress used Flexowriters extensively to handle enormous volumes of routine correspondence with constituents; an advantage of this method was that these letters appeared to have been individually typed by hand. These were complemented by autopen machines which could use a pen to place a signature on letters making them appear to have been hand-signed.\n\nAuxiliary paper-tape readers could be attached to a Flexowriter to create an early form of \"mail merge\", where a long custom-created tape containing individual addresses and salutations was merged with a closed-loop form-letter and printed on continuous-form letterhead; both tapes contained embedded \"control characters\" to switch between readers.\n\nAs the unit record equipment (tabulating machine) industry matured and became the computer industry, Flexowriters were commonly used as console terminals for computers. Because ASCII had not yet been standardized, each type of computer tended to use its own system for encoding characters; Flexowriters were capable of being configured with numerous encodings particular to the computer the machine was being used with.\n\nComputers that used Flexowriters as consoles include:\n\nFlexowriters could also be used as offline punches and printers. Programmers would type their programs on Flexowriters, which would punch the program onto paper tape. The tapes could then be loaded into computers to run the programs. Computers could then use their own punches to make paper tapes that could be used by the Flexowriters to print output. Among the computers which commonly used Flexowriters for this task was the DEC PDP-1.\n\nThe ability to support diverse encodings meant that adapting Flexowriters to generate the paper tapes used to drive CNC machine tool equipment was a relatively simple affair, and many Flexowriters found homes in machine shops into the 1970s, when magnetic media displaced paper tape in the industry.\n\nFriden manufactured equipment which could connect their calculators to Flexowriters, printing output and performing unit record tasks such as form letters for bills, and eventually manufactured their own computers to further enhance these capabilities. These variants were sold as the Friden Computyper. Computypers were electromechanical; they had no electronics, at least in their earlier models. The calculator mechanism, inside a desk-like enclosure, was much like a Friden model STW desktop calculator, except that it had electrical input (via solenoids) and output (low-torque rotary switches on the dial shafts) .\n\nA product known as the Justowriter (or Just-O-Writer) was developed for the printing industry. It allowed typists to produce justified text for use in typesetting. This worked by having the user type the document on a Recording unit, which placed extra codes for spacing on the paper tape. The tape was placed into a second specially adapted Flexowriter which had two paper tape reading heads; one would read the text while the other controlled the spacing of the print. Spacing codes were stored in relays inside the machine as a line of text progressed. At least some Justowriters used carbon (as opposed to ink-impregnated fabric) ribbons to produce cleaner type, suitable for mass photo-set reproduction, sometimes referred to as cold type composition.\n\nThe Line Casting Control or LCC product generated paper tapes for Linotype and Intertype automatic typesetters. In addition to punching a tape containing the text to be typeset, it turned on a lamp easily seen by the operator to show that the text on the line being typed could be typeset—it was within justifiable range. The LCC had a four-wheel rotary escapement, and a set of gears between the carriage rack and the escapement, to permit the smallest unit of spacing at the escapement to be quite small at the carriage. The spring-tensioned tape that moved the carriage had far more tension (possibly 20 lbs?) than did a standard Flexowriter.\n\nAmerican Type Founders produced a phototypesetter based on the Justowriter/Flexowriter platform.\n\nThere was an \"accounting\" model with an ultra-wide carriage and two-color ribbon for printing out wide financial reports. The Friden accounting model was called \"5010 COMPUTYPER\" and was capable of arithmetic functions (addition, subtraction, multiplication & division) at electronic speeds and to print the results automatically in a useful document.\n\nAs a cutting edge device of their time, Flexowriters, especially some of the specialized types like typesetters, were quite costly. They were made for extreme durability. There were porous bronze bearings, many hardened steel parts, very strong springs, and a substantial AC motor to move all the parts. Most parts are made of heavy gauge steel. The housing and most removable covers were die castings. While the final Singer models did make some use of plastics, even they are quite heavy compared to other electric typewriters of their time. As a result, the platen carriage is very heavy, and when the \"Carriage return\" key is pressed, the carriage moves with about 20 pounds of force and enough momentum to injure a careless operator. If used only as manual typewriters, and properly maintained, Flexowriters might last a century. When reproducing form letters from punched tape, the considerable speed and loud sound of the device made watching it a somewhat frightening experience. \n\nTowards the bottom of the unit there is a large rubber roller (\"power roll\") that rotates continuously at a few hundred rpm. It provides power for typing as well as power-operated backspace, type basket shift, and power for engaging (and probably disengaging) the carriage return clutch.\n\nReferring to the photo of the cam assembly (often simply called a cam; it was not meant to be disassembled), the holes in the side plates at the lower left are for the assembly's pivot rod, which is fixed to the frame. At the extreme upper left is part of a disconnectable pivot that pulls down on the typing linkage. When installed, down is to the right in the photo, so to speak.\n\nReferring again to the part at the upper left, the mating part has a threaded mounting for adjusting cam clearance from the power roll. The irregular \"roundish\" part, lower right center, is the cam, itself. It rotates in the frame while in contact with the power roll. The surface of the cam in contact with the power roll had grooves for better grip. As the radius at the contact patch increases, the frame rotates clockwise to pull down on the linkage to type the character.\n\nThis particular cam assembly has a cam that rotates a full turn for each operation; it might operate the backspace, basket shift, or carriage-return clutch disengage mechanism. Cams for typing characters rotated only half a turn, the halves of course being identical.\n\nBelow the cam in this photo (hidden) is a spring-loaded lever that pushes against a pin on the cam. On the upper edge of the cam, as shown, is a little projection that engages the release lever, which is at the lowest part of the image; it's an irregular shape.\n\nWhen a key is pressed down, it moves the release lever and unlatches the cam for that letter; the spring-loaded lever pressing on the pin rotates the cam until it engages the power roll. As the cam continues to turn, increasing radius rotates the cam's frame slightly (clockwise in the photo) to operate the typing linkage for that character.\n\nAs the cam continues to rotate, the spring-loaded lever pushes on the pin to move it toward home position, but if the key is still down, the cam (now out of contact with the power roll) stalls because the projection on the cam catches on another part of the release lever. The cam stalls until the key is released. When released, the lever catches the projection so the cam is now in home position. This is like a simple clock escapement, and prevents repeated typing. (The \"key-down\" anti-repeat stop can be removed, so that fast repetitive typing can be done, but this change is difficult to undo.)\n\nCarriage return was done by a non-stretch very durable textile tape attached to the platen advance mechanism at the left of the carriage. For a return, the tape wound up on a small reel operated from the drive system through a clutch.\nA cam engaged the clutch; it was disengaged by the left margin stop, perhaps directly, perhaps via another cam. (Info. needed here!) A light-torque spring kept the return tape wound on the reel.\n\nThe basic mechanism looks just like an IBM electric typewriter from the late 1940s. In fact, some Flexowriter parts are identical in fit and function to the early IBM electric typewriters (those with rotary carriage escapements, a gear-driven power roll, and a governor-controlled variable speed \"universal\" (wound-rotor/commutator) motor.)\n\nThe early IBM rotary-escapement proportional-spacing typewriters (three wheel rotary escapement, spur gear differentials) had code bars to control the amount of carriage movement for the current character. They were operated by the cams. However, the Flexowriter's mechanical encoder was a very different and far more rugged design, although still operated by the cams.\nFlexowriters (at least those prior to 1969) do not have transistors; electrical control operations were done with telephone-style (E-Class) relays, and troubleshooting often involved problems with the timing on the relays. Another reader has also found that the timing settings of the various leaf switches (such as in the tape reader) are also important.\n\nThe screws used in the Flexowriter were unique, having large flat heads with a very narrow screwdriver slot and a unique thread size and pitch. This may have been a conscious decision. Another reader found that standard 4-40UNC threads appear to fit some of the cover-attachments; internally, the headless set-screws require fluted Bristol keys, which are not commonly available in Great Britain.\n\nThere is a holder for a large roll of paper tape on the back of the unit, with tape feeding around to a punch on the left side, toward the rear. The tape reader is on the same side, toward the front, and is essentially identical to the reader shown on the front of the square housing in the photo of the auxiliary reader. The right side of a Flexowriter has a large (~1\") connector for hooking the unit up to computers and other equipment. Depending on the model, this connector may be wired in many different ways.\n\nAt various times and in various configurations, flexos came with 5, 6, 7 or 8 channel paper tape reader/punches, could have several auxiliary paper tape units attached, and could also attach to IBM punched card equipment.\n\n\n"}
{"id": "13443187", "url": "https://en.wikipedia.org/wiki?curid=13443187", "title": "Galileo GDS", "text": "Galileo GDS\n\nGalileo is a computer reservations system (CRS) owned by Travelport. As of 2000 it had a 26.4% share of worldwide CRS airline bookings.\n\nIn addition to airline reservations, the Galileo CRS is also used to book train travel, cruises, car rental, and hotel rooms.\n\nThe Galileo system was moved from Denver, Colorado, to the Worldspan datacenter in Atlanta, Georgia, on September 28, 2008, following the 2007 merger of Travelport and Worldspan (although they now share the same datacenter, they continue to be run as separate systems). \n\nGalileo is subject to the Capps II and its successor Secure Flight program for the selection of passengers with a risk profile.\n\nGalileo is a member of the International Air Transport Association, of the OpenTravel Alliance and of SITA.\n\nGalileo traces its roots back to 1971 when United Airlines created its first computerized central reservation system under the name Apollo. During the 1980s and early 1990s, a significant proportion of airline tickets were sold by travel agents. Flights by the airline owning the reservation system had preferential display on the computer screen. Due to the high market penetration of the Sabre and Apollo systems, owned by American Airlines and United Airlines, respectively, Worldspan and Galileo were created by other airline groups in an attempt to gain market share in the computer reservation system market and, by inference, the commercial airline market. Galileo was formed in 1987 by nine European carriers -- British Airways, KLM Royal Dutch Airlines, Alitalia, Swissair, Austrian Airlines, Olympic, Sabena, Air Portugal and Aer Lingus.\n\nIn response and to prevent possible government intervention, United Airlines spun off its Apollo reservation system, which was then controlled by Covia. Galileo International was born when Covia acquired Europe's Galileo and merged it with the Apollo system in 1992.\n\nThe Apollo reservation system was used by United Airlines until March 3, 2012, when it switched to SHARES, a system used by its former Continental Airlines subsidiary. Apollo is still used by Galileo International (now part of Travelport GDS) travel agency customers in the United States, Canada, Mexico, and Japan.\n\nGalileo UK was originally created from Travicom which was the world's first multi-access reservations system using the technology developed by Videcom. Travicom was a company launched by Videcom, British Airways, British Caledonian and CCL in 1976 which in 1988 became Galileo UK.\n\n\n\n"}
{"id": "18368901", "url": "https://en.wikipedia.org/wiki?curid=18368901", "title": "Heated roll laminator", "text": "Heated roll laminator\n\nA heated roll laminator uses heated rollers to melt glue extruded onto lamination film. This film is in turn applied to a substrate such as paper or card using pressure rollers. The primary purpose of laminating with such a machine is to embellish or protect printed documents or images. Heated roll laminators can vary in size from office based pouch laminators to industrial sized machines. Such industrial machines are primarily used for high quantity/quality output by printers or print finishers.\n\nWhether small office or industrial machines their primary function is to embellish or protect printed works. Such laminators are used to apply varying thicknesses of lamination film onto substrates such as paper or fabrics. The main advantage to the use of heated roll laminators is that of speed. Heated laminators use heated rollers or heated shoes to melt the glue which is applied to lamination film. The process of heating the glue prior to applying the film to a substrate allows for a faster application of the film. The laminates and adhesives used are generally cheaper to manufacture than cold roll laminates, often as much as half the cost depending on the comparison made. As the materials are non-adhesive until exposed to heat, they are much easier to handle. The glue is solid at room temperature, so lamination of this type is less likely to shift or warp after its application than pressure activated laminates, which rely on a highly viscous, adhesive fluid.\n\nRoll laminators typically use two rolls to complete the lamination process, with one roll being on top and the other roll on the bottom. These rolls slide onto metal bars, known as mandrels, which are then placed in the machine and feed through it. In the United States, the most common core size found on lamination film is one inch (25- to 27-inch-wide film). Larger format laminators use a larger core, often 2 to 3 inches in diameter. Film is usually available in 1.5, 3, 5, 7, and 10 mil thicknesses. The higher the number, the thicker the film. A mil is one thousandth of an inch (.001\").\n\nPrinters or print finishers often use industrial heated roll laminators to laminate such things as paperback book covers, magazine covers, posters, cards and postcards, in-shop displays as well as other applications.\n\n"}
{"id": "10977747", "url": "https://en.wikipedia.org/wiki?curid=10977747", "title": "Hinjawadi", "text": "Hinjawadi\n\nHinjawadi is a suburb located in Pune, Maharashtra, India. It is off the Dehu Road–Katraj bypass on the western side of Wakad in Pune. It is mainly known for the \"Rajiv Gandhi Infotech Park\" (which also extends to adjacent Maan and Marunji). The 2800-acre IT park in Hinjawadi, Maan and Marunji, houses hundreds of companies of different sizes.\n\nHinjawadi was a village until the construction of the Rajiv Gandhi Infotech Park. The park comprises three phases with further phases planned. Hinjawadi Industries Association (HIA) was laid out to provide a joint forum to all stakeholders based out of Hinjawadi and nearby places.\n\nHinjawadi is a proposed hub for integrated townships in Pune. The new \"Special Township Policy Act\" of the Maharashtra government has laid down an integrated approach to development of townships to de-congest Pune Municipal Corporation areas and encourage new settlements in its periphery though higher FSI (floor space index) than what is normally allowed in the municipal limits.\n\nDue to rapid development, Hinjawadi is facing serious traffic problems during peak hours. From 8.00 AM to 12:00 PM and 5.30 PM to 7.30 PM, HMVs (Heavy Motor Vehicles) are not allowed to enter the area due to high congestion on the roads and the number of vehicles are increasing due to a lot of career opportunities. Above 2.5 Lacs employees travel daily to and from the IT park. Recently, the traffic cops have designated several approach points as one-way to ease the traffic congestion which has helped to a certain extent. \n\nPune Metropolitan Region Development Authority (PMRDA) has suggested a metro route between Hinjawadi and Shivajinagar.\nThe line will travel through PMC, PCMC and MIDC areas.\n\nThe Maharashtra Industrial Development Corporation (MIDC) took special initiative to install closed circuit television camera project. The project, co-sponsored by the MIDC and the HIA, has 22 surveillance cameras monitoring the activity at six road junctions in the IT Park. It is integrated with the Rs 225-crore CCTV camera project (1,285 cameras at 441 junctions) coming up in the city. The output is monitored and recorded by the centralized unit at the Pune Police Commissionerate.\n\n\n"}
{"id": "803661", "url": "https://en.wikipedia.org/wiki?curid=803661", "title": "History of technology", "text": "History of technology\n\nThe history of technology is the history of the invention of tools and techniques and is one of the categories of the history of humanity. Technology can refer to methods ranging from as simple as stone tools to the complex genetic engineering and information technology that has emerged since the 1980s. The term technology comes from the Greek word techne, meaning art and craft, and the word logos, meaning word and speech. It was first used to describe applied arts, but it is now used to described advancements and changes which affect the environment around us.\n\nNew knowledge has enabled people to create new things, and conversely, many scientific endeavors are made possible by technologies which assist humans in traveling to places they could not previously reach, and by scientific instruments by which we study nature in more detail than our natural senses allow.\n\nSince much of technology is applied science, technical history is connected to the history of science. Since technology uses resources, technical history is tightly connected to economic history. From those resources, technology produces other resources, including \"technological artifacts\" used in everyday life.\n\nTechnological change affects and is affected by, a society's cultural traditions. It is a force for economic growth and a means to develop and project economic, political, military power and wealth.\n\nMany sociologists and anthropologists have created social theories dealing with social and cultural evolution. Some, like Lewis H. Morgan, Leslie White, and Gerhard Lenski have declared technological progress to be the primary factor driving the development of human civilization. Morgan's concept of three major stages of social evolution (savagery, barbarism, and civilization) can be divided by technological milestones, such as fire. White argued the measure by which to judge the evolution of culture was energy.\n\nFor White, \"the primary function of culture\" is to \"harness and control energy.\" White differentiates between five stages of human development: In the first, people use the energy of their own muscles. In the second, they use the energy of domesticated animals. In the third, they use the energy of plants (agricultural revolution). In the fourth, they learn to use the energy of natural resources: coal, oil, gas. In the fifth, they harness nuclear energy. White introduced a formula P=E*T, where E is a measure of energy consumed, and T is the measure of the efficiency of technical factors using the energy. In his own words, \"culture evolves as the amount of energy harnessed per capita per year is increased, or as the efficiency of the instrumental means of putting the energy to work is increased\". Nikolai Kardashev extrapolated his theory, creating the Kardashev scale, which categorizes the energy use of advanced civilizations.\n\nLenski's approach focuses on information. The more information and knowledge (especially allowing the shaping of natural environment) a given society has, the more advanced it is. He identifies four stages of human development, based on advances in the history of communication. In the first stage, information is passed by genes. In the second, when humans gain sentience, they can learn and pass information through experience. In the third, the humans start using signs and develop logic. In the fourth, they can create symbols, develop language and writing. Advancements in communications technology translate into advancements in the economic system and political system, distribution of wealth, social inequality and other spheres of social life. He also differentiates societies based on their level of technology, communication, and economy:\n\nIn economics, productivity is a measure of technological progress. Productivity increases when fewer inputs (classically labor and capital but some measures include energy and materials) are used in the production of a unit of output. Another indicator of technological progress is the development of new products and services, which is necessary to offset unemployment that would otherwise result as labor inputs are reduced. In developed countries productivity growth has been slowing since the late 1970s; however, productivity growth was higher in some economic sectors, such as manufacturing. For example, employment in manufacturing in the United States declined from over 30% in the 1940s to just over 10% 70 years later. Similar changes occurred in other developed countries. This stage is referred to as \"post-industrial\".\n\nIn the late 1970s sociologists and anthropologists like Alvin Toffler (author of \"Future Shock\"), Daniel Bell and John Naisbitt have approached the theories of post-industrial societies, arguing that the current era of industrial society is coming to an end, and services and information are becoming more important than industry and goods. Some extreme visions of the post-industrial society, especially in fiction, are strikingly similar to the visions of near and post-Singularity societies.\n\nThe following is a summary of the history of technology by time period and geography:\n\nDuring most of the Paleolithic – the bulk of the Stone Age – all humans had a lifestyle which involved limited tools and few permanent settlements. The first major technologies were tied to survival, hunting, and food preparation. Stone tools and weapons, fire, and clothing were technological developments of major importance during this period.\n\nHuman ancestors have been using stone and other tools since long before the emergence of \"Homo sapiens\" approximately 200,000 years ago. The earliest methods of stone tool making, known as the Oldowan \"industry\", date back to at least 2.3 million years ago, with the earliest direct evidence of tool usage found in Ethiopia within the Great Rift Valley, dating back to 2.5 million years ago. This era of stone tool use is called the \"Paleolithic\", or \"Old stone age\", and spans all of human history up to the development of agriculture approximately 12,000 years ago.\n\nTo make a stone tool, a \"core\" of hard stone with specific flaking properties (such as flint) was struck with a hammerstone. This flaking produced sharp edges which could be used as tools, primarily in the form of choppers or scrapers. These tools greatly aided the early humans in their hunter-gatherer lifestyle to perform a variety of tasks including butchering carcasses (and breaking bones to get at the marrow); chopping wood; cracking open nuts; skinning an animal for its hide, and even forming other tools out of softer materials such as bone and wood.\n\nThe earliest stone tools were irrelevant, being little more than a fractured rock. In the Acheulian era, beginning approximately 1.65 million years ago, methods of working these stone into specific shapes, such as hand axes emerged. This early Stone Age is described as the Lower Paleolithic.\n\nThe Middle Paleolithic, approximately 300,000 years ago, saw the introduction of the prepared-core technique, where multiple blades could be rapidly formed from a single core stone. The Upper Paleolithic, beginning approximately 40,000 years ago, saw the introduction of pressure flaking, where a wood, bone, or antler punch could be used to shape a stone very finely.\n\nThe end of the last Ice Age about 10,000 years ago is taken as the end point of the Upper Paleolithic and the beginning of the Epipaleolithic / Mesolithic. The Mesolithic technology included the use of microliths as composite stone tools, along with wood, bone, and antler tools.\n\nThe later Stone Age, during which the rudiments of agricultural technology were developed, is called the Neolithic period. During this period, polished stone tools were made from a variety of hard rocks such as flint, jade, jadeite, and greenstone, largely by working exposures as quarries, but later the valuable rocks were pursued by tunneling underground, the first steps in mining technology. The polished axes were used for forest clearance and the establishment of crop farming and were so effective as to remain in use when bronze and iron appeared. These stone axes were used alongside a continued use of stone tools such as a range of projectiles, knives, and scrapers, as well as tools, made organic materials such as wood, bone, and antler.\n\nStone Age cultures developed music and engaged in organized warfare. Stone Age humans developed ocean-worthy outrigger canoe technology, leading to migration across the Malay archipelago, across the Indian Ocean to Madagascar and also across the Pacific Ocean, which required knowledge of the ocean currents, weather patterns, sailing, and celestial navigation.\n\nAlthough Paleolithic cultures left no written records, the shift from nomadic life to settlement and agriculture can be inferred from a range of archaeological evidence. Such evidence includes ancient tools, cave paintings, and other prehistoric art, such as the Venus of Willendorf. Human remains also provide direct evidence, both through the examination of bones, and the study of mummies. Scientists and historians have been able to form significant inferences about the lifestyle and culture of various prehistoric peoples, and especially their technology.\n\nMetallic copper occurs on the surface of weathered copper ore deposits and copper was used before copper smelting was known. Copper smelting is believed to have originated when the technology of pottery kilns allowed sufficiently high temperatures. The concentration of various elements such as arsenic increase with depth in copper ore deposits and smelting of these ores yields arsenical bronze, which can be sufficiently work hardened to be suitable for making tools. Bronze is an alloy of copper with tin; the latter being found in relatively few deposits globally caused a long time to elapse before true tin bronze to became widespread. (See: Tin sources and trade in ancient times) Bronze was a major advance over stone as a material for making tools, both because of its mechanical properties like strength and ductility and because it could be cast in molds to make intricately shaped objects. \n\nBronze significantly advanced shipbuilding technology with better tools and bronze nails. Bronze nails replaced the old method of attaching boards of the hull with cord woven through drilled holes. Better ships enabled long distance trade and the advance of civilization. \nThis technological trend apparently began in the Fertile Crescent and spread outward over time. These developments were not, and still are not, universal. The three-age system does not accurately describe the technology history of groups outside of Eurasia, and does not apply at all in the case of some isolated populations, such as the Spinifex People, the Sentinelese, and various Amazonian tribes, which still make use of Stone Age technology, and have not developed agricultural or metal technology.\n\nBefore iron smelting was developed the only iron was obtained from meteorites and is usually identified by having nickel content. Meteoric iron was rare and valuable, but was sometimes used to make tools and other implements, such as fish hooks.\n\nThe Iron age involved the adoption of iron smelting technology. It generally replaced bronze and made it possible to produce tools which were stronger, lighter and cheaper to make than bronze equivalents. The raw materials to make iron, such as ore and limestone, are far more abundant than copper and especially tin ores. Consequently, iron was produced in many areas. \n\nIt was not possible to mass manufacture steel or pure iron because of the high temperatures required. Furnaces could reach melting temperature but the crucibles and molds needed for melting and casting had not been developed. Steel could be produced by forging bloomery iron to reduce the carbon content in a somewhat controllable way, but steel produced by this method was not homogeneous.\n\nIn many Eurasian cultures, the Iron Age was the last major step before the development of written language, though again this was not universally the case.\n\nIn Europe, large hill forts were built either as a refuge in time of war or sometimes as permanent settlements. In some cases, existing forts from the Bronze Age were expanded and enlarged. The pace of land clearance using the more effective iron axes increased, providing more farmland to support the growing population.\n\nThe Egyptians invented and used many simple machines, such as the ramp to aid construction processes. Egyptian society made significant advances during dynastic periods in areas such as astronomy, mathematics, and medicine. They also made paper and monuments. The Egyptians made significant advances in shipbuilding. Astronomy was used by Egyptian leaders to govern people.\n\nThe Indus Valley Civilization, situated in a resource-rich area, is notable for its early application of city planning and sanitation technologies. Indus Valley construction and architecture, called 'Vaastu Shastra', suggests a thorough understanding of materials engineering, hydrology, and sanitation.\n\nThe peoples of Mesopotamia (Sumerians, Akkadians, Assyrians, and Babylonians) have been credited with the invention of the wheel, but this is no longer certain. They lived in cities from c. 4000 BC, and developed a sophisticated architecture in mud-brick and stone, including the use of the true arch. The walls of Babylon were so massive they were quoted as a Wonder of the World. They developed extensive water systems; canals for transport and irrigation in the alluvial south, and catchment systems stretching for tens of kilometers in the hilly north. Their palaces had sophisticated drainage systems.\n\nWriting was invented in Mesopotamia, using the cuneiform script. Many records on clay tablets and stone inscriptions have survived. These civilizations were early adopters of bronze technologies which they used for tools, weapons and monumental statuary. By 1200 BC they could cast objects 5 m long in a single piece. The Assyrian King Sennacherib (704–681 BC) claims to have invented automatic sluices and to have been the first to use water screws, of up to 30 tons weight, which were cast using two-part clay molds rather than by the 'lost wax' process. The Jerwan Aqueduct (c. 688 BC) is made with stone arches and lined with waterproof concrete.\n\nThe Babylonian astronomical diaries spanned 800 years. They enabled meticulous astronomers to plot the motions of the planets and to predict eclipses.\n\nThe Chinese made many first-known discoveries and developments. Major technological contributions from China include early seismological detectors, matches, paper, sliding calipers, the double-action piston pump, cast iron, the iron plough, the multi-tube seed drill, the wheelbarrow, the suspension bridge, the parachute, natural gas as fuel, the compass, the raised-relief map, the propeller, the crossbow, the South Pointing Chariot and gunpowder.\n\nOther Chinese discoveries and inventions from the Medieval period include block printing, movable type printing, phosphorescent paint, endless power chain drive and the clock escapement mechanism. The solid-fuel rocket was invented in China about 1150, nearly 200 years after the invention of gunpowder (which acted as the rocket's fuel). Decades before the West's age of exploration, the Chinese emperors of the Ming Dynasty also sent large fleets on maritime voyages, some reaching Africa.\n\nGreek and Hellenistic engineers were responsible for myriad inventions and improvements to existing technology. The Hellenistic period, in particular, saw a sharp increase in technological advancement, fostered by a climate of openness to new ideas, the blossoming of a mechanistic philosophy, and the establishment of the Library of Alexandria and its close association with the adjacent museion. In contrast to the typically anonymous inventors of earlier ages, ingenious minds such as Archimedes, Philo of Byzantium, Heron, Ctesibius, and Archytas remain known by name to posterity.\n\nAncient Greek innovations were particularly pronounced in mechanical technology, including the ground-breaking invention of the watermill which constituted the first human-devised motive force not to rely on muscle power (besides the sail). Apart from their pioneering use of waterpower, Greek inventors were also the first to experiment with wind power (see Heron's windwheel) and even created the earliest steam engine (the aeolipile), opening up entirely new possibilities in harnessing natural forces whose full potential would not be exploited until the Industrial Revolution. The newly devised right-angled gear and screw would become particularly important to the operation of mechanical devices. That is when the age of mechanical devices started.\n\nAncient agriculture, as in any period prior to the modern age the primary mode of production and subsistence, and its irrigation methods, were considerably advanced by the invention and widespread application of a number of previously unknown water-lifting devices, such as the vertical water-wheel, the compartmented wheel, the water turbine, Archimedes' screw, the bucket-chain and pot-garland, the force pump, the suction pump, the double-action piston pump and quite possibly the chain pump.\n\nIn music, the water organ, invented by Ctesibius and subsequently improved, constituted the earliest instance of a keyboard instrument. In time-keeping, the introduction of the inflow clepsydra and its mechanization by the dial and pointer, the application of a feedback system and the escapement mechanism far superseded the earlier outflow clepsydra.\n\nThe famous Antikythera mechanism, a kind of analogous computer working with a differential gear, and the astrolabe both show great refinement in astronomical science.\n\nGreek engineers were also the first to devise automata such as vending machines, suspended ink pots, automatic washstands, and doors, primarily as toys, which however featured many new useful mechanisms such as the cam and gimbals.\n\nIn other fields, ancient Greek inventions include the catapult and the gastraphetes crossbow in warfare, hollow bronze-casting in metallurgy, the dioptra for surveying, in infrastructure the lighthouse, central heating, the tunnel excavated from both ends by scientific calculations, the ship trackway, the dry dock, and plumbing. In horizontal, vertical and transport, great progress resulted from the invention of the crane, the winch, the wheelbarrow and the odometer.\n\nFurther newly created techniques and items were spiral staircases, the chain drive, sliding calipers and showers.\n\nThe Romans developed an intensive and sophisticated agriculture, expanded upon existing iron working technology, created laws providing for individual ownership, advanced stone masonry technology, advanced road-building (exceeded only in the 19th century), military engineering, civil engineering, spinning and weaving and several different machines like the Gallic reaper that helped to increase productivity in many sectors of the Roman economy. Roman engineers were the first to build monumental arches, amphitheatres, aqueducts, public baths, true arch bridges, harbours, reservoirs and dams, vaults and domes on a very large scale across their Empire. Notable Roman inventions include the book (Codex), glass blowing and concrete. Because Rome was located on a volcanic peninsula, with sand which contained suitable crystalline grains, the concrete which the Romans formulated was especially durable. Some of their buildings have lasted 2000 years, to the present day.\n\nThe engineering skills of the Inca and the Mayans were great, even by today's standards. An example of this great engineering is the use of pieces weighing upwards of one ton in their stonework placed together so that not even a blade can fit into the cracks. Inca villages used irrigation canals and drainage systems, making agriculture very efficient. While some claim that the Incas were the first inventors of hydroponics, their agricultural technology was still soil based, if advanced.\n\nThough the Maya civilization did not incorporate metallurgy or wheel technology in their architectural constructions, they developed complex writing and astronomical systems, and created beautiful sculptural works in stone and flint. Like the Inca, the Maya also had command of fairly advanced agricultural and construction technology. The Maya are also responsible for creating the first pressurized water system in Mesoamerica, located in the Maya site of Palenque.\n\nThe main contribution of the Aztec rule was a system of communications between the conquered cities and the ubiquity of the ingenious agricultural technology of chinampas. In Mesoamerica, without draft animals for transport (nor, as a result, wheeled vehicles), the roads were designed for travel on foot, just as in the Inca and Mayan civilizations. The Aztec, subsequently to the Maya, inhereted many of the technologies and intellectual advancements of their predecessors: the Olmec (see Native American inventions and innovations).\n\nOne of the most significant development of the Medieval era was the development of economies where water and wind power were more significant than animal and human muscle power. Most water and wind power was used for milling grain. Water power was also used for blowing air in blast furnace, pulping rags for paper making and for felting wool. The \"Domesday Book\" recorded 5,624 water mills in Great Britain in 1086, being about one per thirty families.\n\nAs earlier empires had done, the Muslim caliphates united in trade large areas that had previously traded little. The conquered sometimes paid lower taxes than in their earlier independence, and ideas spread even more easily than goods. Peace was more frequent than it had been. These conditions fostered improvements in agriculture and other technology as well as in sciences which largely adapted from earlier Greek, Roman and Persian empires, with improvements.\n\nWhile medieval technology has been long depicted as a step backwards in the evolution of Western technology, sometimes willfully so by modern authors intent on denouncing the church as antagonistic to scientific progress (see e.g. Myth of the Flat Earth), a generation of medievalists around the American historian of science Lynn White stressed from the 1940s onwards the innovative character of many medieval techniques. Genuine medieval contributions include for example mechanical clocks, spectacles and vertical windmills. Medieval ingenuity was also displayed in the invention of seemingly inconspicuous items like the watermark or the functional button. In navigation, the foundation to the subsequent age of exploration was laid by the introduction of pintle-and-gudgeon rudders, lateen sails, the dry compass, the horseshoe and the astrolabe.\n\nSignificant advances were also made in military technology with the development of plate armour, steel crossbows, counterweight trebuchets and cannon. The Middle Ages are perhaps best known for their architectural heritage: While the invention of the rib vault and pointed arch gave rise to the high rising Gothic style, the ubiquitous medieval fortifications gave the era the almost proverbial title of the 'age of castles'.\n\nPapermaking, a 2nd-century Chinese technology, was carried to the Middle East when a group of Chinese papermakers were captured in the 8th century. Papermaking technology was spread to Europe by the Umayyad conquest of Hispania. A paper mill was established in Sicily in the 12th century. In Europe the fiber to make pulp for making paper was obtained from linen and cotton rags. Lynn Townsend White Jr. credited the spinning wheel with increasing the supply of rags, which led to cheap paper, which was a factor in the development of printing.\n\nBefore the development of modern engineering, mathematics was used by artisans and craftsmen, such as millwrights, clock makers, instrument makers and surveyors. Aside from these professions, universities were not believed to have had much practical significance to technology.\n\nA standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise \"De re metallica\" (1556), which also contains sections on geology, mining and chemistry. \"De re metallica\" was the standard chemistry reference for the next 180 years.\n\nDue to the casting of cannon, the blast furnace came into widespread use in France in the mid 15th century. The blast furnace had been used in China since the 4th century BC.\n\nThe invention of the movable cast metal type printing press, whose pressing mechanism was adapted from an olive screw press, (c. 1441) lead to a tremendous increase in the number of books and the number of titles published. Movable ceramic type had been used in China for a few centuries and woodblock printing dated back even further.\n\nThe era is marked by such profound technical advancements like linear perceptivity, double shell domes or Bastion fortresses. Note books of the Renaissance artist-engineers such as Taccola and Leonardo da Vinci give a deep insight into the mechanical technology then known and applied. Architects and engineers were inspired by the structures of Ancient Rome, and men like Brunelleschi created the large dome of Florence Cathedral as a result. He was awarded one of the first patents ever issued in order to protect an ingenious crane he designed to raise the large masonry stones to the top of the structure. Military technology developed rapidly with the widespread use of the cross-bow and ever more powerful artillery, as the city-states of Italy were usually in conflict with one another. Powerful families like the Medici were strong patrons of the arts and sciences. Renaissance science spawned the Scientific Revolution; science and technology began a cycle of mutual advancement.\n\nAn improved sailing ship, the (nau or carrack), enabled the Age of Exploration with the European colonization of the Americas, epitomized by Francis Bacon's \"New Atlantis\". Pioneers like Vasco da Gama, Cabral, Magellan and Christopher Columbus explored the world in search of new trade routes for their goods and contacts with Africa, India and China to shorten the journey compared with traditional routes overland. They produced new maps and charts which enabled following mariners to explore further with greater confidence. Navigation was generally difficult, however, owing to the problem of longitude and the absence of accurate chronometers. European powers rediscovered the idea of the civil code, lost since the time of the Ancient Greeks.\n\nThe stocking frame, which was invented in 1598, increased a knitter's number of knots per minute from 100 to 1000.\n\nMines were becoming increasingly deep and were expensive to drain with horse powered bucket and chain pumps and wooden piston pumps. Some mines used as many as 500 horses. Horse-powered pumps were replaced by the Savery steam pump (1698) and the Newcomen steam engine (1712).\n\nThe British Industrial Revolution is characterized by developments in the areas of textile machinery, mining, metallurgy and transport the steam engine and the invention of machine tools.\n\nBefore invention of machinery to spin yarn and weave cloth, spinning was done the spinning wheel and weaving done on a hand and foot operated loom. It took from three to five spinners to supply one weaver. The invention of the flying shuttle in 1733 doubled the output of a weaver, creating a shortage of spinners. The spinning frame for wool was invented in 1738. The spinning jenny, invented in 1764, was a machine that used multiple spinning wheels; however, it produced low quality thread. The water frame patented by Richard Arkwright in 1767, produced a better quality thread than the spinning jenny. The spinning mule, patented in 1779 by Samuel Crompton, produced a high quality thread. The power loom was invented by Edmund Cartwright in 1787.\n\nIn the mid 1750s the steam engine was applied to the water power-constrained iron, copper and lead industries for powering blast bellows. These industries were located near the mines, some of which were using steam engines for mine pumping. Steam engines were too powerful for leather bellows, so cast iron blowing cylinders were developed in 1768. Steam powered blast furnaces achieved higher temperatures, allowing the use of more lime in iron blast furnace feed. (Lime rich slag was not free-flowing at the previously used temperatures.) With a sufficient lime ratio, sulfur from coal or coke fuel reacts with the slag so that the sulfur does not contaminate the iron. Coal and coke were cheaper and more abundant fuel. As a result, iron production rose significantly during the last decades of the 18th century.\n\nThe revolution was driven by cheap energy in the form of coal, produced in ever-increasing amounts from the abundant resources of Britain. Coal converted to coke fueled higher temperature blast furnaces and produced cast iron in much larger amounts than before, allowing the creation of a range of structures such as The Iron Bridge. Cheap coal meant that industry was no longer constrained by water resources driving the mills, although it continued as a valuable source of power. The steam engine helped drain the mines, so more coal reserves could be accessed, and the output of coal increased. The development of the high-pressure steam engine made locomotives possible, and a transport revolution followed. The steam engine which had existed since the early 18th century, was practically applied to both steamboat and railway transportation. The Liverpool and Manchester Railway, the first purpose built railway line, opened in 1830, the Rocket locomotive of Robert Stephenson being one of its first working locomotives used.\n\nManufacture of ships' pulley blocks by all-metal machines at the Portsmouth Block Mills in 1803 instigated the age of sustained mass production. Machine tools used by engineers to manufacture parts began in the first decade of the century, notably by Richard Roberts and Joseph Whitworth. The development of interchangeable parts through what is now called the American system of manufacturing began in the firearms industry at the U.S Federal arsenals in the early 19th century, and became widely used by the end of the century.\n\nThe 19th century saw astonishing developments in transportation, construction, manufacturing and communication technologies originating in Europe. After a recession at the end of the 1830s and a general slowdown in major inventions, the Second Industrial Revolution was a period of rapid innovation and industrialization that began in the 1860s or around 1870 and lasted until World War I. It included rapid development of chemical, electrical, petroleum, and steel technologies connected with highly structured technology research.\n\nTelegraphy developed into a practical technology in the 19th century to help run the railways safely. Along with the development of telegraphy was the patenting of the first telephone. March 1876 marks the date that Alexander Graham Bell officially patented his version of an \"electric telegraph\". Although Bell is noted with the creation of the telephone, it is still debated about who actually developed the first working model.\n\nBuilding on improvements in vacuum pumps and materials research, incandescent light bulbs became practical for general use in the late 1870s. This invention had a profound effect on the workplace because factories could now have second and third shift workers.\n\nShoe production was mechanized during the mid 19th century. Mass production of sewing machines and agricultural machinery such as reapers occurred in the mid to late 19th century. Bicycles were mass-produced beginning in the 1880s.\n\nSteam-powered factories became widespread, although the conversion from water power to steam occurred in England before in the U.S. Ironclad warships were found in battle starting in the 1860s, and played a role in the opening of Japan and China to trade with the West.\n\n20th-century technology developed rapidly. Broad teaching and implementation of the scientific method, and increased research spending contributed to the advancement of modern science and technology. New technology improved communication and transport, thus spreading technical understanding.\n\nMass production brought automobiles and other high-tech goods to masses of consumers. Military research and development sped advances including electronic computing and jet engines. Radio and telephony improved greatly and spread to larger populations of users, though near-universal access would not be possible until mobile phones became affordable to developing world residents in the late 2000s and early 2010s.\n\nEnergy and engine technology improvements included nuclear power, developed after the Manhattan project which heralded the new Atomic Age. Rocket development led to long range missiles and the first space age that lasted from the 1950s with the launch of Sputnik to the mid-1980s.\n\nElectrification spread rapidly in the 20th century. At the beginning of the century electric power was for the most part only available to wealthy people in a few major cities such as New York, London, Paris, and Newcastle upon Tyne, but by the time the World Wide Web was invented in 1990 an estimated 62 percent of homes worldwide had electric power, including about a third of households in the rural developing world.\n\nBirth control also became widespread during the 20th century. Electron microscopes were very powerful by the late 1970s and genetic theory and knowledge were expanding, leading to developments in genetic engineering.\n\nThe first \"test tube baby\" Louise Brown was born in 1978, which led to the first successful gestational surrogacy pregnancy in 1985 and the first pregnancy by ICSI in 1991, which is the implanting of a single sperm into an egg. Preimplantation genetic diagnosis was first performed in late 1989 and led to successful births in July 1990. These procedures have become relatively common.\n\nThe massive data analysis resources necessary for running transatlantic research programs such as the Human Genome Project and the Large Electron-Positron Collider led to a necessity for distributed communications, causing Internet protocols to be more widely adopted by researchers and also creating a justification for Tim Berners-Lee to create the World Wide Web.\n\nVaccination spread rapidly to the developing world from the 1980s onward due to many successful humanitarian initiatives, greatly reducing childhood mortality in many poor countries with limited medical resources.\n\nThe US National Academy of Engineering, by expert vote, established the following ranking of the most important technological developments of the 20th century:\n\n\nIn the early 21st century research is ongoing into quantum computers, gene therapy (introduced 1990), 3D printing (introduced 1981), nanotechnology (introduced 1985), bioengineering/biotechnology, nuclear technology, advanced materials (e.g., graphene), the scramjet and drones (along with railguns and high-energy laser beams for military uses), superconductivity, the memristor, and green technologies such as alternative fuels (e.g., fuel cells, self-driving electric and plug-in hybrid cars), augmented reality devices and wearable electronics, artificial intelligence, and more efficient and powerful LEDs, solar cells, integrated circuits, wireless power devices, engines, and batteries.\n\nPerhaps the greatest research tool built in the 21st century is the Large Hadron Collider, the largest single machine ever built. The understanding of particle physics is expected to expand with better instruments including larger particle accelerators such as the LHC and better neutrino detectors. Dark matter is sought via underground detectors and observatories like LIGO have started to detect gravitational waves.\n\nGenetic engineering technology continues to improve, and the importance of epigenetics on development and inheritance has also become increasingly recognized.\n\nNew spaceflight technology and spacecraft are also being developed, like the Orion and Dragon. New, more capable space telescopes, such as the James Webb Telescope, to be launched to orbit in early 2021, and the Colossus Telescope are being designed. The International Space Station was completed in the 2000s, and NASA and ESA plan a manned mission to Mars in the 2030s. The Variable Specific Impulse Magnetoplasma Rocket (VASIMR) is an electro-magnetic thruster for spacecraft propulsion and is expected to be tested in 2015.\n\nBreakthrough Initiatives, together with famed physicist Stephen Hawking, plan to send the first ever spacecraft to visit another star, which will consist of numerous super-light chips driven by Electric propulsion in the 2030s, and receive images of the Proxima Centauri system, along with, possibly, the potentially habitable planet Proxima Centauri b, by midcentury.\n\n2004 saw the first manned commercial spaceflight when Mike Melvill crossed the boundary of space on June 21, 2004.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "13052107", "url": "https://en.wikipedia.org/wiki?curid=13052107", "title": "ILIAS", "text": "ILIAS\n\nILIAS (Integriertes Lern-, Informations- und Arbeitskooperations-System [German for \"Integrated Learning, Information and Work Cooperation System\"]) is an open-source web-based learning management system (LMS). It supports learning content management (including SCORM 2004 compliance) and tools for collaboration, communication, evaluation and assessment. The software is published under the GNU General Public License and can be run on any server that supports PHP and MySQL.\n\nILIAS is one of the first Learning Management Systems that have been used in universities. A prototype has been developed since end of 1997 within the VIRTUS project at the Faculty of Management, Economics and Social Sciences of the University of Cologne, initiated and organized by Wolfgang Leidhold. On November 2, 1998 version 1 of the learning management system ILIAS was published and offered for learning at the Cologne faculty of business administration, economics and social sciences. Due to increasing interest of other universities, the project team decided to publish ILIAS as open source software under the GPL in 2000. Between 2002 and 2004, a new ILIAS version was developed from scratch and called \"ILIAS 3\". In 2004, it became the first open source LMS that reached full SCORM (Sharable Content Object Reference Model) 1.2 compliance. SCORM 2004 compliance has been reached with version 3.9 in November 2007.\n\nThe idea behind ILIAS is to offer a flexible environment for learning and working online with integrated tools. ILIAS goes far beyond the idea of learning being confined to courses as a lot of other LMS do. ILIAS can rather be seen as a type of library providing learning and working materials and contents at any location of the repository. This offers the possibility to run ILIAS not as a locked warehouse but as an open knowledge platform where content might be made available for non-registered users too.\n\nILIAS offers a lot of features to design and run online-courses, create learning content, offer assessments and exercises, run surveys and support communication and cooperation among users.\n\nA general characteristic of ILIAS is the concept of Personal Desktop and Repository. While the Repository contains all content, courses and other materials structured in categories and described by metadata, the Personal Desktop is the individual workspace of each learner, author, tutor and administrator. The Personal Desktop contains selected items from the repository (e.g. currently visited courses or an interesting forum) as well as certain tools like mail, tagging, a calendar and also e-portfolio and personal blogs.\n\n\nAnother important characteristic of ILIAS is the repository. All learning content but also forums or chat rooms, tests and surveys, as well as plugged in virtual classrooms or other external tools are created, offered and administrated in the repository and its categories. Therefore, it is not necessary to build up courses for offering learning content. ILIAS could also be used like a kind of knowledge base or website. Access to all repository item is granted by the role-based access control (RBAC) of ILIAS. The repository is structured as tree with a root node and multiple levels. Each repository item is assigned to one node in the RBAC tree.\n\nILIAS offers four kinds of container for delivering content:\n\nContainer objects can be extended by using the page editor for adding text, images or videos to the page.\n\nAll content objects are handled as references. They can be moved, copied or linked into other branches of the repository tree. A file that has already been uploaded can be linked multiple times in different courses and categories without being uploaded a second time.\n\n\n\n\n\n\n\n\n"}
{"id": "1769437", "url": "https://en.wikipedia.org/wiki?curid=1769437", "title": "Intergraph", "text": "Intergraph\n\nIntergraph Corporation is an American software development and services company. It provides enterprise engineering and geospatially powered software to businesses, governments, and organizations around the world. Intergraph operates through three divisions: Hexagon PPM, Hexagon Safety & Infrastructure, and Hexagon Geospatial. The company’s headquarters is in Huntsville, Alabama, United States. \n\nIn 2008, Intergraph was one of the one hundred largest software companies in the world. In July 2010, Intergraph was acquired by Hexagon AB.\n\nIntergraph was founded in 1969 as M&S Computing, Inc., by former IBM engineers Jim\nMeadlock, his wife Nancy, Terry Schansman (the S of M&S), Keith Schonrock, and\nRobert Thurber who had been working with NASA and the U.S. Army in developing systems that would apply digital computing to real time missile guidance. The company was later renamed to \"Intergraph Corporation\" in 1980.\n\nIn 2000, Intergraph exited the hardware business and became purely a software company. On July 21, 2000, it sold its Intense3D graphics accelerator division to 3Dlabs, and its workstation and server division to Silicon Graphics.\n\nThe companies incorporated SmartSketch, a drawing program used previously for the PenPoint OS and EO tablet computer. When Pen computing did not take off, SmartSketch was ported to the Windows and Macintosh platforms.\n\nOn November 29, 2006, Intergraph was acquired by an investor group led by Hellman & Friedman LLC, Texas Pacific Group and JMI Equity, making the company privately held. On October 28, 2010, Intergraph was acquired by Hexagon AB. The transaction marks the return of Intergraph as part of a publicly traded company. \n\nAs part of the Hexagon acquisition, Hexagon moved the management of ERDAS, Inc. from under Leica Geosystems to Intergraph, and Z/I Imaging airborne imaging sensors from under Intergraph to Leica Geosystems.\n\nOn December 2, 2013, the geospatial technology portfolio was split out from under the Intergraph Security, Government and Infrastructure division to form the Hexagon Geospatial division. On October 13, 2015, the Intergraph Security, Government & Infrastructure division was rebranded as Hexagon Safety & Infrastructure. On January 9, 2017, the Intergraph Government Solutions division was rebranded as Hexagon US Federal. \n\nOn June 5, 2017, the Intergraph Process, Power & Marine division was rebranded as Hexagon PPM.\n\n"}
{"id": "54484359", "url": "https://en.wikipedia.org/wiki?curid=54484359", "title": "International Academy for Production Engineering", "text": "International Academy for Production Engineering\n\nThe International Academy for Production Engineering is a professional body for research into production engineering. It was formed as the International Institution for Production Engineering Research (CIRP) in 1951.\n\n\n"}
{"id": "48204615", "url": "https://en.wikipedia.org/wiki?curid=48204615", "title": "Jobe Water Sports", "text": "Jobe Water Sports\n\nJobe is a water sports brand and manufacturer of products used for wakeboarding, boating and standup paddleboarding. The Jobe headquarters and warehouse are located in Heerewaarden, Netherlands.\n\nThe brand was created in 1974 by former professional skier Jeff Jobe. Jobe skis were shipped all over the world. In 2008 Sport and Recreation Den Bol bought the trademark licenses from Jobe, changed their name into Jobe Sports International and a new Jobe era started. Currently Jobe sells its products worldwide and manufactures products used for the boating, jet ski, cable wakeboarding and stand-up paddling scenes.\n\nJobe sponsors numerous water sports events throughout the year. Jobe also has a large team of sponsored athletes who represent the brand.\n\n\n\n\n\n"}
{"id": "3730328", "url": "https://en.wikipedia.org/wiki?curid=3730328", "title": "Laundry ball", "text": "Laundry ball\n\nA laundry ball or washing ball is a product that is promoted as a substitute for laundry detergent. Producers of laundry balls often make pseudoscientific claims about how these balls work and exaggerate the extent of their benefits.\n\nIn the US the product was often sold on home shopping channels or by participants in multilevel marketing, although this is not the case in all countries.\n\nWhile many individuals report that these balls work, most test results show them to be similar to or less effective than washing in water without any detergent. Most of the effect can be attributed to the mechanical effect of the ball or to using hot water instead of cold water.\n\nThe US Federal Trade Commission has taken action against some of the manufacturers of these products because of their misleading claims. Consumer organizations from several countries have recommended against buying this type of product.\n\nThere are several shapes of laundry balls: laundry disks, globes, spheres or doughnuts. Some of the balls carry components inside, like ceramic pieces, magnetic material or coloured liquid that is claimed to be \"activated water\". Manufacturers claim that these components have certain effects on washing efficacy, although studies don't show any difference between the different types of balls. Some balls can be refilled with pellets of special detergent, or other ingredients.\n\nLaundry balls are marketed as cheaper, environmentally friendly alternatives to ordinary washing powders or liquids. The manufacturers claim the following benefits, which are technically real:\n\n\nBut they also claim many benefits that laundry balls do not have, according to many studies:\n\n\nHowever, the real effects are comparable to those of washing without any detergent and are sometimes worse.\n\nThe laundry ball could break open during washing, and the ceramic pieces inside it could damage the machinery of the washing machine.\n\nManufacturers rarely agree on why their laundry balls work, hinting that these claims are just made up by each individual manufacturer. Some claims are not backed by science, while others are an exaggeration of benefits. Balls that contain detergents may offer more cleaning power than water alone because their ingredients are comparable to normal washing powder, but in smaller quantities. It is claimed that conventional washing powder manufacturers recommend using more powder than is necessary, and that these powders contain unnecessary fillers or fragrances.\n\nThe effect of the laundry balls may be explained by simple mechanical action and by the usage of hotter water. Some manufacturers claim that their products reduce energy consumption, but their pamphlets recommend using hot water. Hot water will clean some types of spots better than cold water, leading some people to conclude that the balls worked. The mechanical action of the laundry balls can help clean some types of spots, but a golf ball will achieve the same effect for much less money.\n\nSome manufacturers claim that the components inside their balls emit far infrared rays, which are claimed to reduce the surface tension of water and facilitate washing. The claim of emitting infrared is not false, as almost all materials emit \"far infrared waves\" at room temperature, in other words, heat radiation. It is also true that heating reduces the surface tension of water, but the effect of the radiation emitted by the balls is negligible compared to the radiation emitted by the internal walls of the washing machine or by the water, especially if the water is hot.\n\nMagnetic water softeners claim that their magnetic fields can help remove scale from the washing machine and pipes, and prevent new limescale from adhering. Some companies claim to remove hardness ions from hard water, or to precipitate the molecules in the water so they won't \"stick\" to the pipes, or to reduce the surface tension of water. The claims are dubious, the scientific basis is unclear, the working mechanism is vaguely defined and understudied, and high-quality studies report negative results. The reputation of these products is further damaged by the pseudoscientific explanations that promoters keep putting forward.\n\nSome magnetic products claim that they \"change the molecular structure of water\", a pseudoscientific claim with no real scientific basis. There is no such thing as \"magnetized water\". Water is not paramagnetic, so its water molecules do not align in the presence of a magnetic field. Water is weakly diamagnetic (so it is repelled by magnets), but only to an extent so small that it is undetectable to most instruments.\n\nSome balls are refillable with small pellets of detergent which are sold only by the manufacturer of the ball. Critics question whether the amount and type of detergent released by these balls is sufficient to generate significant cleaning effects.\n\nIn 1997, Amway offered a ceramic washing disk on its catalogue, but removed it after concluding that it had \"no measurable impact on overall cleaning.\"\n\nIn 1997 Trade-Net, sold a laundry ball product (the Blue Laundry Ball) in various US states. Trade-Net claimed that the blue liquid inside their balls was structured water \"that emits a negative charge through the walls of the container into your laundry water.\" \"This causes the water molecule cluster to disassociate, allowing much smaller individual water molecules to penetrate into the innermost part of the fabric.\" Dennis Barnum, a professor of inorganic chemistry at Portland State University, said that the liquid was just water with a blue dye and couldn't possibly have the effect claimed by the manufacturer. Barnum also said that the claims were \"gibberish\" and used scientific terms in ways that sounded educated to the layman but didn't make any real sense. The Oregonian tested the balls, and found they washed marginally better than hot water with no detergent, and worse than using detergent.\n\nAfter complaints, Trade-Net's claims were investigated by consumer protection departments in Utah, Oregon and Florida, amongst others, and the company was prohibited from making certain claims, including that \"such product cleans as well as conventional laundry detergent\". Trade-Net offered a 'new' laundry ball product after this, but were forced to pay fines, including $190,000 to Oregon's Department of Justice, $10,000 to Utah and then in April 1999, $155,000 to the states of New York, Arizona, Arkansas, Hawaii, Idaho, Illinois, Michigan, Missouri, Nebraska, Nevada, Oklahoma and the FTC. The company disappeared shortly thereafter. The Federal Trade Commission has levied fines against other companies for similar fraudulent claims. However, other companies kept selling similar products over the Internet.\n\nThe judge ruling against Trade-Net, issued in April 1999, said the manufacturers failed to substantiate their claims and hadn't informed consumers about reports showing that the claims were incorrect.\n\nThe Australian Consumers' Association published a report in the April 1998 issue of its magazine \"Choice\". It concluded that laundry balls were no better than cold water.\n\nThe US Federal Trade Commission published in 1999 about laundry balls, rings and discs: \"Tests show that these gadgets do little more than clean out your wallet. At best, they’re marginally better than washing clothes in hot water alone, and not as effective as washing them with laundry detergent. At worst, the products are completely useless.\"\n\nIn 2000 the magazine \"Good Housekeeping\" tested several laundry balls sold in the US and concluded that \"these gizmos do little more than clean out your wallet.\"\n\nIn April 2009 the Italian consumer association Altroconsumo carried a small test and concluded that laundry balls didn't wash better than plain water.\n\nIn 2009 the Spanish consumer organization OCU made a study of \"ecobolas\" (a type of laundry ball marketed in Spain). It compared the efficacy of the laundry ball, normal detergent, and no detergent at all. It concluded that laundry balls were no better than using just water, and it recommended that consumers simply use a minimum amount of detergent.\n\nIn November 2011, the Hong Kong Consumer Council published a report on the effect of using washing liquid, washing powder and washing balls. The former two were shown to be effective in removing stains, while the washing balls were not more effective than plain water.\n\nSome organizations recommending against their use are Consumers Union, International Fabricare Institute (now called Drycleaning and Laundry Institute), Maytag, Soap and Detergent Association and Spanish OCU.\n\nIn February 2011 the Spanish National Institute of Consume (Instituto Nacional del Consumo INC) ordered 14 manufacturers to cease their deceiving advertisement after testing the wash balls and concluding that they are as effective, or even \"less\" effective, than washing with water alone.\n\nIn August 2012 the Portuguese Consumer Rights Council requested a ban on the washing balls because none of the advantages advertised were proven in tests.\n\nThe Australian consumer advocacy group (Choice Australia) gave a 'Shonky Award' to Nanosmart Laundry Balls in October 2015, stating that they \"don't work\" and that they should be renamed \"Nano-not-so-smart\" after testing the balls against plain water and finding they had no effect and that their scientific claims were simply untrue. Choice Australia states that they will refer the product to the Australian Competition and Consumer Commission for investigation over Nanosmart's misleading claims.\n\nBy making very vague claims, marketers can continue to sell laundry balls without running afoul of consumer protection laws that require veracity in advertisement.\n\nThe product is often sold by participants in multilevel marketing. They can also be found in ecological retail stores.\n\nDuring the initial marketing boom, balls were manufactured for other market niches, like washing cars.\n\n\n"}
{"id": "4605309", "url": "https://en.wikipedia.org/wiki?curid=4605309", "title": "LifeStraw", "text": "LifeStraw\n\nThe LifeStraw is a water filter designed to be used by one person to filter water for drinking. It filters a maximum of 4000 litres of water, enough for one person for three years. It removes almost all of waterborne bacteria and parasites. In addition to the classic LifeStraw, the manufacturer also produces other styles on the LifeStraw product: the LifeStraw Family, a larger unit designed for family use, and the LifeStraw Go Water Bottle, which incorporates a LifeStraw filter into a plastic sports water bottle. \n\nThe LifeStraw water filters are designed by the Swiss-based Vestergaard Frandsen. While originally developed for people living in developing nations and for distribution in humanitarian crisis, the LifeStraw has gained popularity as a consumer product. The LifeStraw is now used as a tool for survivalists and packed in emergency preparedness kits in addition to being used to help combat clean water scarcity worldwide. LifeStraw personal filters can provide clean water for up to a year, and the LifeStraw Family filters a maximum of 18,000 litres of water, providing safe drinking water for a family of five for up to three years. \n\nLifeStraw and LifeStraw Family were distributed in the 2010 Haiti earthquake, 2010 Pakistan floods, 2011 Thailand floods, and 2016 Ecuador earthquake, among other crises and initiatives. In the Mutomo District in Kenya which has suffered from long term drought, the Kenya Red Cross supplied filters to 3,750 school children and 6,750 households. In 2015, LifeStraw filters were deployed in Rwanda.\n\nThe LifeStraw is a plastic tube 22 cm long and 3 cm in diameter. Water that is drawn up through the straw first passes through hollow fibres that filter water particles down to 0.2 µm across, using only physical filtration methods and no chemicals. The entire process is powered by suction, similar to using a conventional drinking straw, and filters up to 4000 litres of water. While the initial model of the filter did not remove \"Giardia lamblia\", LifeStraw now removes a minimum of 98.9% of waterborne protozoan parasites including \"Giardia\" and \"Cryptosporidium\". Note that the standard LifeStraw is ineffective in filtering viruses, chemicals, salt water, and heavy metals.. However, the newer LifeStraw products (like LifeStraw Flex ) are capable of removing chemicals and heavy metals including lead.\n\nLifeStraw has been generally praised for its effective and instant method of bacteria and protozoa removal and consumer acceptability.\n\nAlthough LifeStraw is available for retail sale in the developed world, the majority of LifeStraw are distributed as part of public health campaigns or in response to complex emergencies by NGOs and organizations that give them away for free in the developing world.\n\nLifeStraw has been praised in the international media and won several awards including the 2008 Saatchi & Saatchi Award for World Changing Ideas, the ‘IDEX: 2005’ International Design Award and \"Best Invention of 2005\" by \"Time Magazine\". The LifeStraw was featured in the Museum of Modern Art in New York.\n\n\n"}
{"id": "45314797", "url": "https://en.wikipedia.org/wiki?curid=45314797", "title": "List of Stuff You Should Know episodes", "text": "List of Stuff You Should Know episodes\n\n<onlyinclude>\"Stuff You Should Know\" is a free podcast and video series published by the HowStuffWorks website and hosted by Josh Clark and Charles W. \"Chuck\" Bryant, both writers at HowStuffWorks. The podcast educates listeners on a wide variety of topics, often using popular culture as a reference giving the podcast comedic value. Episodes are normally around 45 minutes in length, although for more in-depth topics the show has run longer than an hour.\n\nThe winner of the 2014 and the 2016 People's Voice Webby Award, the show is downloaded more than 1 million times per week and is consistently on iTunes’ Top 10 podcast rankings. Stuff You Should Know's \"beautifully, beautifully done\" production has set \"the audio standard,\" according to podcast reviewers Pod on Pod. They added that the audio quality \"could not be improved\" on the NPR-level production. One reviewer said of it: \"It is never not fun to listen to.\"</onlyinclude>\n\n<nowiki>*</nowiki>\n<nowiki>**</nowiki>\n\n"}
{"id": "620534", "url": "https://en.wikipedia.org/wiki?curid=620534", "title": "List of agricultural machinery", "text": "List of agricultural machinery\n\nAgricultural equipment is any kind of machinery used on a farm to help with farming. The best-known example of this kind is the tractor.\n\n\n\n\n\n\n\nSteam-powered:\n\nOther:\n"}
{"id": "35990233", "url": "https://en.wikipedia.org/wiki?curid=35990233", "title": "Liter of Light", "text": "Liter of Light\n\nLiter of Light is an open source design for a low-cost light tube that refracts solar light to provide daytime interior lighting for dwellings with thin roofs. Daylighting is cheaper than using indoor electric lights during the day. The device is simple: a transparent two-liter bottle is filled with water plus a little bleach to inhibit algal growth and fitted into a hole in a roof. The device functions like a deck prism: during daytime the water inside the bottle refracts sunlight, delivering about as much light as a 40–60 watt incandescent bulb to the interior. A properly installed solar bottle can last up to 5 years.\n\nThe use of plastic bottles in this way to provide indoor lighting from daylight was developed by Alfredo Moser of Brazil. Using the technology as a social enterprise was first launched in the Philippines by Illac Diaz under the My Shelter Foundation in April 2011. In order to help the idea to grow sustainably, Diaz implemented a “local entrepreneur” business model whereby bottle bulbs are assembled and installed by local people, who can earn a small income for their work. Within months, one carpenter and one set of tools in one community in San Pedro, Laguna, expanded the organization to 15,000 solar bottle bulb installations in 20 cities and provinces around the Philippines, and began to inspire local initiatives around the world. MyShelter Foundation also established a training center that conducts workshops with youth, business companies, and other groups who are interested in volunteering their time to build lights in their communities.\n\nIn less than a year since inception, over 200,000 bottle bulbs were installed in communities around the world. Liter of Light has a goal to light up 1 million homes by the end of 2015.\n\nMyShelter Foundation was established by Illac Diaz in 2006 to create a system of sustainability and reliability through its capability-building and employment-generating projects. Based on the concepts of social enterprise, appropriate low-cost technologies, and alternative construction in the Philippines, MyShelter Foundation has pioneered projects such as the Pier One Seafarer’s Dormitory, the Design Against the Elements (DAtE) competition, and the Bottle School Project.\n\nThe Solar Bottle Bulb, as it has also been called, is installed in the roof of homes with the purpose of refracting sunlight in order to light up a room. The project’s innovation lies in its utilization of cheap, durable and readily available materials to produce high quality natural lighting enabling the urban poor to have access to an affordable, environmentally friendly long-term alternative to electric light for use during the day.\n\nMost of the plastic bottles used are recycled 1.5 liter bottles. After being filled with water and a little bleach, the bottle is pushed through a steel sheet that serves as a metal lock to prevent it from slipping. It is then embedded into a corrugated iron roof. A small part of the bottle is left outside while the rest of it protrudes into the house. Sealant is put around the hole made in the roof to keep it weather proof. The refractive properties of water ensures that the light from the sun that reaches the inside of the bottle becomes omni-directional mimicking an electric light bulb and emitting the same amount of light as a 40–60 W incandescent bulb depending on the amount of solar insolation available. Adding bleach to the water prevents it from turning green with algae and ensures a high quality light keeping the water clear for a longer time. In order to facilitate use of the invention through open source mechanisms, step-by-step guides on materials and installation are available online.\n\nUsing an appropriate durable, leak-proof, space-filling glue, that does not melt because of the sun, is one of the main challenges for the solution. Many local groups are experimenting with different glues to find the best solution for both costs and quality. It was usually found that silicone-based or polyurethane glues work best. The inventor, Alfredo Moser, used a polyester resin.\n\nA local project is underway.\n\nLights Foundation, an youth social venture founded by local community worker Shanjidul Alam Seban Shaan working on bottle light project in Chittagong. They made the local version of bottle light named 'Botol Bati' which costs around 2-2.5 USD and sustains for 4–5 years. Their R&D department working on the local night version of bottle light which can light up remote areas like Sundarbans, Cox's Bazar Kutubdia Upazila, Khagrachari District etc. where electricity is not available sufficiently, which will cost less than 6 dollars to make and will sustain for 4–5 years. They aim to spread the awareness about bottle light to the remote areas by training the local school students. Around 44.80% of Bangladeshi people have no access to electricity, around 24% of the total population are slum dwellers where people use illegal electricity, millions of people live in hilltract areas, where electricity is not available sufficiently; Lights Foundation's aim is to serve those people. Lights Foundation became the country partner of Liter of Light by launching Liter of Light Bangladesh officially, and Shanjidul Alam Seban Shaan became the Executive Director. Liter of Light Bangladesh started spreading the idea in New York and other states. \n\nLiter Of Light China was created by three passionate international students from China in 2016, and the first project in Rudong will be conducted in the early summer, 2017.\n\nThe movement in Colombia was started by Camilo Herrera in a small town called Duitama in the Boyaca Department, approximately 200 kilometers from Bogota. After launching a pilot in Bogota in collaboration with Liter of Light Switzerland in February 2012, a local Bogota organization was created. The first bottles in Bogota were installed in the areas of Divino Niño and La Colina of Ciudad Bolíva. The volunteers in Bogota are also working closely with Un Techo Para Mi Pais to identify areas in need.\n\nLiter of Light was started up in the Dominican Republic by the German volunteer Nicolai Rapp in 2015 who then distributed the project with other German and local volunteers in over 7 provinces. \nThey achieved the installation of bottle and solar nightlights in more than 250 houses with global fundraising. In the focus of their movement are the rural areas without or with limited access to electricity. \nThey also presented the idea of Liter of Light to several local NGO´s in seminars and workshops to reach all the off-grid communities of the country. Recently, their efforts also focused on bringing \nthe project to Haiti, where around 75% of the population has to live without access to electricity.\n\nA group of seven students from the French University in Egypt (UFE) have begun Liter of Light locally in the context of a social and environmental development project.\nIn November 2014, PepsiCo announced that they will implement Liter of Light in 3 villages in upper Egypt, with two partners : SunUtions Company for solar solutions and Masr ElKheir organization.\n\nIn 2015, Liter of Light France was created. The French branch is developing an important link between pedagogical project in Europe and the international cooperation around the world. The NGO wants in Europe to make the new generation aware of the energy poverty reality and show that they can have, with a small activity, a huge social and environmental impact. The NGO wants, as well, to deliver the message to developing countries that a new generation all around the world are caring about the World Citizenship. During International cooperation, Liter of Light France is underlining the main philosophies of the movement as technological innovation accessible to everybody, capacity building, and initiatives South-South. The movement in France was started by Olivier Lasbouygues, Paola Sierra and Isabel Rico.\n\nInspired by the project of Isang Litrong Liwanag, Liter of Light was introduced to the people of India by Pradeep Chanti. The first trial was done in Vikarabad, a rural village in Telangana in 2011. This bulb was successfully installed and is still working. With this experience, the team gradually expanded the concept. Later, Mr. Ranjeet Gakhare, an alumnus of Indian Institute of Technology Bombay and Mr. Chaitanya Reddy from Cornell University joined Pradeep and expanded the concept to all over India. With the generous help of several NGOs and various organizations, awareness, campaigns and workshops were conducted in various cities like Hyderabad, Mumbai, Chennai, Kolkata, Jalpaiguri, Delhi, etc. Starting with the one bottle in Vikarabad, the team is not only installing several models in houses of the unprivileged, but also mentoring various other organizations, NGOs, student bodies and the people in need. The demonstration of Liter of Light was conducted at TEDxChristUniversity in November 2012. In 2014 it was carried out by Desire Foundation, Bhubaneswar on the coruscating day of Diwali to celebrate the festival of light in a unique style with the underprivileged.\n\nLiter of Light India was nominated for Change Maker's Award by Uttarakhand Sustainable Development Festival and for 'Young Achiever's Award' by prestigious society magazine. \n\nLiter of Light India also takes initiatives like Woman Empowerment, City Cleanliness and Education.\n\nA youth group called Koch Hope has started to install 2L PET bottles in the Korogocho slums. At first the people were skeptical that the bottle installations would let in water, so the first bottles were installed in a local school as a test. The locals thus bought into the idea and the first 100 bulbs were installed in April 2011 for free in hopes to attract interest and donors. Next, the local initiative would like to expand to other areas around Nairobi.\n\nLiter of Light Malaysia ran its first project in June, 2015, and is run by a Malaysia-based social business called Incitement, founded by Zikry Kholil and Daniel de Gruijter, which runs several social initiatives. Liter of Light Malaysia has thus far completed 36 projects, predominantly for marginalized communities in Pahang, Malaysia and the Orang Asli communities living in Cameron Highlands, but also in Sabah and Sarawak in East Malaysia (Borneo). After Incitement launched Liter of Light Malaysia, it has also pushed Liter of Light into the Italian market under the umbrella of Incitement Italy. To date, Liter of Light Malaysia has installed 1850+ solar powered lamps in the rural areas of Malaysia, effectively providing light at night for more than 6,000 underprivileged Malaysians.\n\nLiter of Light began in Mexico in early 2013, sponsored by Qohélet A.C. Founded by Tere Gonzalez, who had previously worked with Liter of Light in India & Spain, the group began operations with a pilot program in the state of Chihuahua México, where they were able to benefit 114 people. As of September 2013, they are working to complete the next phase of installations in Ajusco, Mexico City, to benefit a further 500 families.\n\n(PROJECT UJYALO) “Ujyalo” means “the light” in Nepali. Ujyalo is one of the initiatives started by Ujyalo Foundation in Nepal.\n\nUjyalo creates lighting systems by using wasted plastic bottles that amplify a little amount of solar light with the help of water by refracting it into various directions to light up a room. Ujyalo is inspired by “A Liter of Light” initiative.\n\nThis natural bulb aims to create an environmental friendly, energy efficient and cost effective system that can be used to light up homes in Nepal that either do not have access to electricity or cannot afford it. Moreover, this will not only help provide light, but will also help us recycle wasted plastic bottles from the community.\n\nFurthermore, given that major cities in Nepal, like Kathmandu, have to endure almost 20 hours of load shedding in the winter on a daily basis, this project has the potential and need in both rural and urban Nepal.\n\nLiter of Light Netherlands aims at realizing projects in suitable countries with the purpose of establishing a local Liter of Light sub-organization and its work is threefold. Prior to the project, the work focuses mainly on gathering information, preparing the projects, promoting and fundraising. During the projects abroad, the project focuses on spreading the idea and inspiring people to get involved with Liter of Light and even starting their own project. Therefore, empowering the partner organization is a major goal and includes holding bottle-building workshops on site and setting up an operational framework. After the project, the partner organizations will be assisted and continuously supported by Liter of Light Netherlands.\n\nA partnership has been created with Ace Welfare Foundation, Pakistan. According to both of the organizations objectives, and considering the situation that more than 11% of the Pakistani population lives without access to the electricity of which 70% of the individuals live in the rural areas in 50,000 villages that are completely detached from the national grid, it has been mutually decided to form a partnership in order to implement A LITER OF LIGHT project in the territory of PAKISTAN, under the representation of ACE WELFARE FOUNDATION, founded by Mr. Vaqas Attaullah Butt. ACE WELFARE FOUNDATION\n\nA local project is underway.\n\nA local project is underway.\n\nThe solution was first launched in the Philippines by Illac Diaz under the MyShelter Foundation. As of July 2011, the organization had installed 10,000 bottles in the Philippines and shortly thereafter reached 15,000 installations and their goal for 2012 is to reach 1 million homes. In order to help the idea to grow sustainably, they have implemented a “local entrepreneur” business model, whereby bottles are put together and installed by locals who can in turn earn a small income for their work. Additionally a Liter of Light office has been established that conducts volunteer workshops.\n\nA local project is underway.\n\nLiter of Light Switzerland began as a project of the SIMagination Challenge at the University of St. Gallen. The project grew and was established as a student club at the University of St. Gallen and as a non-profit organization in Switzerland in November 2011. The organization’s first project was to plan and implement a pilot in Bogota together with Litro de Luz Colombia. The pilot took place in February 2012 and together they installed bottles in Ciudad Bolivar, Bogota. Throughout 2012, Liter of Light Switzerland undertook additional projects in Spain, India and Bangladesh.\n\nThe Swiss NGO plans to continue sharing lessons learned and spreading the concept via a global platform as well as launching more pilots around the world.\n\nWorld Unite! in cooperation with local NGOs has started A Liter of Light in Tanzania in April 2013. Project locations are Dar es Salaam, Moshi/Kilimanjaro and Zanzibar.\n\nA local project is underway.\n\nLiter of Light USA is committed to educating and raising awareness of Liter of Light and its global outreach. The company is headquartered in New York and frequently holds events and workshops in Manhattan. It maintains its own social media pages on Facebook, Twitter, LinkedIn and Google +\n\nA local project is underway.\n\n"}
{"id": "207379", "url": "https://en.wikipedia.org/wiki?curid=207379", "title": "Magnetic stripe card", "text": "Magnetic stripe card\n\nA magnetic stripe card is a type of card capable of storing data by modifying the magnetism of tiny iron-based magnetic particles on a band of magnetic material on the card. The magnetic stripe, sometimes called swipe card or magstripe, is read by swiping past a magnetic reading head. Magnetic stripe cards are commonly used in credit cards, identity cards, and transportation tickets. They may also contain an RFID tag, a transponder device and/or a microchip mostly used for business premises access control or electronic payment.\n\nMagnetic recording on steel tape and wire was invented in Denmark around 1900 for recording audio. In the 1950s, magnetic recording of digital computer data on plastic tape coated with iron oxide was invented. In 1960, IBM used the magnetic tape idea to develop a reliable way of securing magnetic stripes to plastic cards, under a contract with the US government for a security system. A number of International Organization for Standardization standards, ISO/IEC 7810, ISO/IEC 7811, ISO/IEC 7812, ISO/IEC 7813, ISO 8583, and ISO/IEC 4909, now define the physical properties of the card, including size, flexibility, location of the magstripe, magnetic characteristics, and data formats. They also provide the standards for financial cards, including the allocation of card number ranges to different card issuing institutions.\n\nMagnetic storage was known from World War II and computer data storage in the 1950s.\n\nIn 1969 Forrest Parry, an IBM engineer, had the idea of securing a piece of magnetic tape, the predominant storage medium at the time, to a plastic card base. He became frustrated because every adhesive he tried produced unacceptable results. The tape strip either warped or its characteristics were affected by the adhesive, rendering the tape strip unusable. After a frustrating day in the laboratory, trying to get the right adhesive, he came home with several pieces of magnetic tape and several plastic cards. As he walked in the door at home, his wife Dorothea was ironing clothing. When he explained the source of his frustration: inability to get the tape to \"stick\" to the plastic in a way that would work, she suggested that he use the iron to melt the stripe on. He tried it and it worked. The heat of the iron was just high enough to bond the tape to the card.\n\nThe major development of the magnetic striped plastic card began in 1969 at the IBM Information Records Division (IRD) headquartered in Dayton N.J. In 1970, the marketing organization was transferred by IBM DPD back to the Information Records Division in order to begin sales and marketing strategies for the magnetically striped and encoded cards being developed. It took almost two years for IBM IRD engineers to not only develop the process for reliably applying the magnetic stripe to plastic cards via a hot stamping method, but also develop the process for encoding the magnetic stripe utilizing the IBM Delta Distance C Optical Bar Code format. This engineering effort resulted in IBM IRD producing the first magnetic striped plastic credit and ID cards used by banks, insurance companies, hospitals and many others. Another result of this project was that IBM IRD and IBM Data Processing Division announced on February 24, 1971 the first Magnetic Credit Card Service Center and the IBM 2730-1 Transaction Validation Terminal.\nArthur E. Hahn Jr. was hired by IBM IRD in Dayton, N.J. on Aug 12, 1969 to head up this engineering effort. Other members of the group were David Morgan (Manager), Billy House (Software Developer), William Creeden (Programmer), and E. J. Gillen (Mechanical Engineering/Machining). They were given a recently announced IBM 360 Model 30 computer with 50k of RAM for control of the encoding/embossing of the Magnetic Stripe Cards. The IBM 360 computer was for scientific/business applications so the IRD engineers first had to convert the 360 into a \"process control computer\" and then develop software and hardware around it. Due to the limited RAM, the software was developed in 360 Assembler Language. This conversion enabled the 360 computer to monitor and control the entire production process the IRD engineers designed and built. The engineering design/build effort was carried out in a raised floor secured area of IBM IRD in Dayton, N.J. which was built specifically for the project. This tightly secured area with limited access was required because of the sensitivity of the data that would ultimately be used to encode and emboss the credit and ID cards.\n\nThe IRD engineers first had to develop a reliable process of hot stamping the magnetic stripe to the plastic cards. This was necessary in order to meet the close tolerances required to reliably encode and read the data on the Magnetic Stripe Cards by magnetic write/read heads. The magnetic stripe was encoded with a single track of data utilizing the IBM Delta Distance C Optical Bar Code format. The Delta Distance C Optical Bar Code was developed by the IBM Systems Development Division working at Research Triangle Park in Raleigh North Carolina headed up by George J. Laurer. Other members of the group were N. Joseph Woodland, Paul McEnroe, Dr. Robert Evans, Bernard Silver, Art Hamburgen, Heard Baumeister and Bill Crouse. The IBM group in Raleigh was competing with RCA, Litton-Zellweger and other companies who were working with the National Retail Merchants Association NRMA to develop a standard optical bar code to be used in the retail industry. NRMA wanted an optically readable code that could be printed on products allowing purchasers to rapidly \"check out\" at the new electronic cash register/checkout counters being developed. The code would also be used for production and inventory control of products. Of the many optical bar codes submitted to NRMA by IBM and other companies, NRMA finally selected the later version of the IBM bar code known as the Delta Distance D Optical Bar Code format. The Delta Distance C Code was an earlier version of the Universal Product Code (UPC). The UPC code was selected in 1973 by NRMA as their standard and has become the World Wide Standard that we all know today as the UPC Uniform Product Code.\n\nIn 1971, after the IBM IRD engineers completed the development and building phase of the project they began in 1969, they released the equipment to the IRD manufacturing group in Dayton N.J. to begin producing the plastic magnetic striped credit and ID cards. Because of the sensitivity of the customer data and the security requirements of banks, insurance companies and others, the manufacturing group decided to leave the entire line in the secured area where it was developed.\n\nBanks, insurance companies, hospitals etc., supplied IBM IRD with \"raw plastic cards\" preprinted with their logos, contact information etc. They also supplied the data information which was to be encoded and embossed on the cards. This data was supplied to IRD on large 0.5 inch wide, 10.5 inch diameter IBM Magnetic Tape Reels which was the standard for computers at that time.\nThe manufacturing process started by first applying the magnetic stripe to the preprinted plastic cards via the hot stamping process developed by the IBM IRD engineers. This operation of applying the magnetic stripe to the plastic cards was done off line in another area of IBM IRD and not in the secured area. The cards were then brought into the secured area and placed in \"hoppers\" at the beginning of the production line.\nThe tape reels containing the data were then installed on the modified IBM 360 computer prior to beginning the encoding, embossing and verification of the cards. After the 360 performed a check to verify that all systems and stations were loaded and ready to go, the computer began feeding the Magnetic Striped Plastic Cards from the hoppers at the front end of the production line down a motorized track. The entire operation was fully automated and controlled by the modified IBM 360 business computer. The line consisted of the following stations and operations:\n\n\nThis completed the manufacturing line for the magnetic striped encoded and embossed plastic credit and badge access cards. The envelopes were then taken to be posted and mailed directly to the customers of the companies who had ordered the cards from IRD.\n\nWhat this small engineering group at IBM IRD and the IBM Bar Code development group in Raleigh accomplished in developing the first magnetic stripe credit and ID cards cannot be overstated. They laid the foundation for the entire magnetic stripe card industry that we know and use today through our use of credit cards, ATM cards, ID cards, hotel room and access cards, transportation tickets, and all the terminals and card readers that read the cards and enter the data into computers. Their developments resulted in every person having the ability to easily carry a card that connects them directly to computers with all the ramifications thereof.\n\nNeither IBM nor anyone else applied for or received any patents pertaining to the magnetic stripe card, the delta-distance barcodes or even the Uniform Product Code (UPC). IBM felt that with an open architecture, it would enhance the growth of the media thereby resulting in more IBM computers and associated hardware being sold.\nAs with all new technologies, the magnetic stripe card developed and produced by IBM IRD with one track of encoded data using the Delta Distance C Bar Code format was quickly obsolete. Because of the electronic ATM/reservation/check out/and access systems that were rapidly developing, the banks, airlines and other industries required more encoded data. A wider magnetic stripe enabling multiple tracks of encoding along with new encoding standards was required.\n\nThe first US Patents for the ATM were granted in 1972 and 1973.\n\nOther groups within IBM and other companies continued on with expanding the work done by this small group of engineers at IBM IRD, however, the contributions that these IBM IRD engineers made to the development of the magnetic stripe card is analogous to the Wright Brothers' contribution to the airline industry of today.\n\nThere were a number of steps required to convert the magnetic striped media into an industry acceptable device. These steps included:\nThese steps were initially managed by Jerome Svigals of the Advanced Systems Division of IBM, Los Gatos, California from 1966 to 1975.\n\nIn most magnetic stripe cards, the magnetic stripe is contained in a plastic-like film. The magnetic stripe is located 0.223 inches (5.66 mm) from the edge of the card, and is 0.375 inches (9.52 mm) wide. The magnetic stripe contains three tracks, each 0.110 inches (2.79 mm) wide. Tracks one and three are typically recorded at 210 bits per inch (8.27 bits per mm), while track two typically has a recording density of 75 bits per inch (2.95 bits per mm). Each track can either contain 7-bit alphanumeric characters, or 5-bit numeric characters. Track 1 standards were created by the airlines industry (IATA). Track 2 standards were created by the banking industry (ABA). Track 3 standards were created by the thrift-savings industry.\n\nMagstripes following these specifications can typically be read by most point-of-sale hardware, which are simply general-purpose computers that can be programmed to perform specific tasks. Examples of cards adhering to these standards include ATM cards, bank cards (credit and debit cards including Visa and MasterCard), gift cards, loyalty cards, driver's licenses, telephone cards, membership cards, electronic benefit transfer cards (e.g. food stamps), and nearly any application in which value or secure information is \"not\" stored on the card itself. Many video game and amusement centers now use debit card systems based on magnetic stripe cards.\n\nMagnetic stripe cloning can be detected by the implementation of magnetic card reader heads and firmware that can read a signature of magnetic noise permanently embedded in all magnetic stripes during the card production process. This signature can be used in conjunction with common two-factor authentication schemes utilized in ATM, debit/retail point-of-sale and prepaid card applications.\n\nCounterexamples of cards which intentionally ignore ISO standards include hotel key cards, most subway and bus cards, and some national prepaid calling cards (such as for the country of Cyprus) in which the balance is stored and maintained directly on the stripe and not retrieved from a remote database.\n\nMagstripes come in two main varieties: high-coercivity (HiCo) at 4000 Oe and low-coercivity (LoCo) at 300 Oe, but it is not infrequent to have intermediate values at 2750 Oe. High-coercivity magstripes require higher amount of magnetic energy to encode, and therefore are harder to erase. HiCo stripes are appropriate for cards that are frequently used, such as a credit card. Other card uses include time and attendance tracking, access control, library cards, employee ID cards and gift cards. Low-coercivity magstripes require a lower amount of magnetic energy to record, and hence the card writers are much cheaper than machines which are capable of recording high-coercivity magstripes. However, LoCo cards are much easier to erase and have a shorter lifespan. Typical LoCo applications include hotel room keys, time and attendance tracking, bus/transit tickets and season passes for theme parks. A card reader can read either type of magstripe, and a high-coercivity card writer may write both high and low-coercivity cards (most have two settings, but writing a LoCo card in HiCo may sometimes work), while a low-coercivity card writer may write only low-coercivity cards.\n\nIn practical terms, usually low coercivity magnetic stripes are a light brown color, and high coercivity stripes are nearly black; exceptions include a proprietary silver-colored formulation on transparent American Express cards. High coercivity stripes are resistant to damage from most magnets likely to be owned by consumers. Low coercivity stripes are easily damaged by even a brief contact with a magnetic purse strap or fastener. Because of this, virtually all bank cards today are encoded on high coercivity stripes despite a slightly higher per-unit cost.\n\nMagnetic stripe cards are used in very high volumes in the mass transit sector, replacing paper based tickets with either a directly applied magnetic slurry or hot foil stripe. Slurry applied stripes are generally less expensive to produce and are less resilient but are suitable for cards meant to be disposed after a few uses.\n\nThere are up to three tracks on magnetic cards known as tracks 1, 2, and 3. Track 3 is virtually unused by the major worldwide networks , and often isn't even physically present on the card by virtue of a narrower magnetic stripe. Point-of-sale card readers almost always read track 1, or track 2, and sometimes both, in case one track is unreadable. The minimum cardholder account information needed to complete a transaction is present on both tracks. Track 1 has a higher bit density (210 bits per inch vs. 75), is the only track that may contain alphabetic text, and hence is the only track that contains the cardholder's name.\n\nTrack 1 is written with code known as DEC\nSIXBIT plus odd parity. The information on track 1 on financial cards is contained in several formats: A, which is reserved for proprietary use of the card issuer, B, which is described below, C-M, which are reserved for use by ANSI Subcommittee X3B10 and N-Z, which are available for use by individual card issuers:\n\nFormat B:\n\n\nThis format was developed by the banking industry (ABA). This track is written with a 5-bit scheme (4 data bits + 1 parity), which allows for sixteen possible characters, which are the numbers 0-9, plus the six characters codice_1. The selection of six punctuation symbols may seem odd, but in fact the sixteen codes simply map to the ASCII range 0x30 through 0x3f, which defines ten digit characters plus those six symbols. The data format is as follows:\n\n\nService code values common in financial cards:\n\nFirst digit\n\nSecond digit\n\nThird digit\n\nThe data stored on magnetic stripes on American and Canadian driver's licenses is specified by the American Association of Motor Vehicle Administrators. Not all states and provinces use a magnetic stripe on their driver's licenses. For a list of those that do, see the AAMVA list.\n\nThe following data is stored on track 1:\n\nThe following data is stored on track 2:\n\n\nThe following data is stored on track 3:\n\nNote: Each state has a different selection of information they encode, not all states are the same.\nNote: Some states, such as Texas, have laws restricting the access and use of electronically readable information encoded on driver's licenses or identification cards under certain circumstances.\n\nSmart cards are a newer generation of card that contain an integrated circuit. Some smart cards have metal contacts to electrically connect the card to the reader, and contactless cards use a magnetic field or radio frequency (RFID) for proximity reading.\n\nHybrid smart cards include a magnetic stripe in addition to the chip — this is most commonly found in a payment card, so that the cards are also compatible with payment terminals that do not include a smart card reader.\n\nCards with all three features: magnetic stripe, smart card chip, and RFID chip are also becoming common as more activities require the use of such cards.\n\nDuring DEF CON 24, Weston Hecker presented \"Hacking Hotel Keys, and Point Of Sales Systems.\" In the talk, Hecker described the way magnetic strip cards function and utilised spoofing software, and an Arduino to obtain administrative access from hotel keys, via service staff walking past him. Hecker claims he used administrative keys from POS systems on other systems, effectively providing access to any system with a magnetic stripe reader, providing access to run privileged commands.\n\n\n"}
{"id": "143357", "url": "https://en.wikipedia.org/wiki?curid=143357", "title": "Medical ultrasound", "text": "Medical ultrasound\n\nMedical ultrasound (also known as diagnostic sonography or ultrasonography) is a diagnostic imaging technique based on the application of ultrasound. It is used to see internal body structures such as tendons, muscles, joints, blood vessels, and internal organs. Its aim is often to find a source of a disease or to exclude any pathology. The practice of examining pregnant women using ultrasound is called obstetric ultrasound, and is widely used.\n\nUltrasound is sound waves with frequencies which are higher than those audible to humans (>20,000 Hz). Ultrasonic images, also known as sonograms, are made by sending pulses of ultrasound into tissue using a probe. The sound echoes off the tissue; with different tissues reflecting varying degrees of sound. These echoes are recorded and displayed as an image to the operator.\n\nMany different types of images can be formed using sonographic instruments. The most well-known type is a B-mode image, which displays the acoustic impedance of a two-dimensional cross-section of tissue. Other types of image can display blood flow, motion of tissue over time, the location of blood, the presence of specific molecules, the stiffness of tissue, or the anatomy of a three-dimensional region.\n\nCompared to other prominent methods of medical imaging, ultrasound has several advantages. It provides images in real-time, it is portable and can be brought to the bedside, it is substantially lower in cost, and it does not use harmful ionizing radiation. Drawbacks of ultrasonography include various limits on its field of view, such as the need for patient cooperation, dependence on physique, difficulty imaging structures behind bone and air, and the necessity of a skilled operator, usually a trained professional.\n\nSonography (ultrasonography) is widely used in medicine. It is possible to perform both diagnosis and therapeutic procedures, using ultrasound to guide interventional procedures (for instance biopsies or drainage of fluid collections). Sonographers are medical professionals who perform scans which are then typically interpreted by themselves or the radiologists, physicians who specialize in the application and interpretation of a wide variety of medical imaging modalities, or by cardiologists in the case of cardiac ultrasonography (echocardiography). Sonographers typically use a hand-held probe (called a transducer) that is placed directly on and moved over the patient. Increasingly, clinicians (physicians and other healthcare professionals who provide direct patient care) are using ultrasound in their office and hospital practices.\n\nSonography is effective for imaging soft tissues of the body. Superficial structures such as muscles, tendons, testes, breast, thyroid and parathyroid glands, and the neonatal brain are imaged at a higher frequency (7–18 MHz), which provides better axial and lateral resolution. Deeper structures such as liver and kidney are imaged at a lower frequency 1–6 MHz with lower axial and lateral resolution but greater penetration.\n\nA general-purpose ultrasound scanner may be used for most imaging purposes. Usually specialty applications may be served only by use of a specialty transducer. Most ultrasound procedures are done using a transducer on the surface of the body, but improved diagnostic confidence is often possible if a transducer can be placed inside the body. For this purpose, specialty transducers, including endovaginal, endorectal, and transesophageal transducers are commonly employed. At the extreme of this, very small transducers can be mounted on small diameter catheters and placed into blood vessels to image the walls and disease of those vessels.\n\nIn anesthesiology, Ultrasound is commonly used by anesthesiologists to guide injecting needles when placing local anaesthetic solutions near nerves. It is also used for gaining vascular access such as central venous cannulation and difficult arterial cannulation. Transcranial Doppler is frequently used by neuro-anesthesiologists for obtaining information about flow-velocity in the basal cerebral vessels.\n\nIn angiology or vascular medicine, duplex ultrasound (B Mode vessels imaging combined with Doppler flow measurement) is daily used to diagnose arterial and venous disease all over the body. This is particularly important in neurology, where ultrasound is used for assessing blood flow and stenoses in the carotid arteries (Carotid ultrasonography) and the big intracerebral arteries (Transcranial Doppler).\n\nIntravascular ultrasound (\"IVUS\") is a methodology using a specially designed catheter with a miniaturized ultrasound probe attached to the distal end of the catheter. The proximal end of the catheter is attached to computerized ultrasound equipment. It allows the application of ultrasound technology, such as piezoelectric transducer or CMUT, to see from inside blood vessels out through the surrounding blood column, visualizing the endothelium (inner wall) of blood vessels in living individuals.\n\nOn the legs, ultrasonography of deep venous thrombosis focuses on the deep veins, while ultrasonography of chronic venous insufficiency of the legs focuses on more superficial veins.\n\nEchocardiography is an essential tool in cardiology, to diagnose e.g. dilatation of parts of the heart and function of heart ventricles and valves\n\nPoint of care emergency ultrasound has many applications in emergency medicine, including the Focused Assessment with Sonography for Trauma (FAST) exam for assessing significant hemoperitoneum or pericardial tamponade after trauma. Ultrasound can be used for many other applications in the Emergency Department including evaluation for gallstones, kidney stones, fluid in the lungs or decreased cardiac output.\n\nAbdominal and endoanal ultrasound are frequently used in gastroenterology and colorectal surgery. In abdominal sonography, the solid organs of the abdomen such as the pancreas, aorta, inferior vena cava, liver, gall bladder, bile ducts, kidneys, and spleen are imaged. Sound waves are blocked by gas in the bowel and attenuated in different degree by fat, therefore there are limited diagnostic capabilities in this area. The appendix can sometimes be seen when inflamed (as in e.g.: appendicitis). Endoanal ultrasound is used particularly in the investigation of anorectal symptoms such as fecal incontinence or obstructed defecation. It images the immediate perianal anatomy and is able to detect occult defects such as tearing of the anal sphincter. Ultrasonography of liver tumors allows for both detection and characterization.\n\nGynecologic ultrasonography examines female pelvic organs (specifically the uterus, the ovaries, and the Fallopian tubes) as well as the bladder, the adnexa, and the Pouch of Douglas. It commonly uses vaginal ultrasonography. \n\nObstetrical sonography is commonly used during pregnancy to check on the development of the fetus. It can be used to identify many conditions that would be harmful to the mother and the baby, many health care professionals consider the risk of leaving these conditions undiagnosed to be much greater than the very small risk, if any, associated with undergoing an ultrasound scan. \n\nEven where sonography is used routinely in obstetric appointments during pregnancy, authorities discourage its use for non-medical purposes such as fetal \"keepsake\" videos and photos.\n\nObstetric ultrasound is primarily used to:\n\n\nAccording to the European Committee of Medical Ultrasound Safety (ECMUS) \nUltrasonic examinations should only be performed by competent personnel who are trained and updated in safety matters. Ultrasound produces heating, pressure changes and mechanical disturbances in tissue. Diagnostic levels of ultrasound can produce temperature rises that are hazardous to sensitive organs and the embryo/fetus. Biological effects of non-thermal origin have been reported in animals but, to date, no such effects have been demonstrated in humans, except when a microbubble contrast agent is present.Nonetheless, care should be taken to use low power settings and avoid pulsed wave scanning of the fetal brain unless specifically indicated in high risk pregnancies.\n\nUltrasound scanners have different Doppler-techniques to visualize arteries and veins. The most common is colour doppler or power doppler, but also other techniques like b-flow are used to show bloodflow in an organ. By using pulsed wave doppler or continuous wave doppler bloodflow velocities can be calculated.\n\nFigures released for the period 2005–2006 by the UK Government (Department of Health) show that non-obstetric ultrasound examinations constituted more than 65% of the total number of ultrasound scans conducted.\n\nBlood velocity can be measured in various blood vessels, such as middle cerebral artery or descending aorta, by relatively inexpensive and low risk ultrasound Doppler probes attached to portable monitors. These provides non-invasive or transcutaneous (non-piecing) minimal invasive blood flow assessment. Common examples are, Transcranial Doppler, Esophogeal Doppler and Suprasternal Doppler.\n\nMost structures of the neck, including the thyroid and parathryoid glands, lymph nodes, and salivary glands, are well-visualized by high-frequency ultrasound with exceptional anatomic detail. Ultrasound is the preferred imaging modality for thyroid tumors and lesions, and ultrasonography is critical in the evaluation, preoperative planning, and postoperative surveillance of patients with thyroid cancer. Many other benign and malignant conditions in the head and neck can be evaluated and managed with the help of diagnostic ultrasound and ultrasound-guided procedures.\n\nIn neonatology, transcranial Doppler can be used for basic assessment of intracerebral structural abnormalities, bleeds, ventriculomegaly or hydrocephalus and anoxic insults (Periventricular leukomalacia). The ultrasound can be performed through the soft spots in the skull of a newborn infant (Fontanelle) until these completely close at about 1 year of age and form a virtually impenetrable acoustic barrier for the ultrasound. The most common site for cranial ultrasound is the anterior fontanelle. The smaller the fontanelle, the poorer the quality of the picture.\n\nIn ophthalmology and optometry, there are two major forms of eye exam using ultrasound:\n\nIn pulmonology, endobronchial Ultrasound (EBUS) probes are applied to standard flexible endoscopic probes and used by pulmonologists to allow for direct visualization of endobronchial lesions and lymph nodes prior to transbronchial needle aspiration. Among its many uses, EBUS aids in lung cancer staging by allowing for lymph node sampling without the need for major surgery.\n\nUltrasound is routinely used in urology to determine, for example, the amount of fluid retained in a patient's bladder. In a pelvic sonogram, organs of the pelvic region are imaged. This includes the uterus and ovaries or urinary bladder. Males are sometimes given a pelvic sonogram to check on the health of their bladder, the prostate, or their testicles (for example to distinguish epididymitis from testicular torsion). In young males, it is used to distinguish more benign testicular masses (varicocele or hydrocele) from testicular cancer, which is highly curable but which must be treated to preserve health and fertility. There are two methods of performing a pelvic sonography – externally or internally. The internal pelvic sonogram is performed either transvaginally (in a woman) or transrectally (in a man). Sonographic imaging of the pelvic floor can produce important diagnostic information regarding the precise relationship of abnormal structures with other pelvic organs and it represents a useful hint to treat patients with symptoms related to pelvic prolapse, double incontinence and obstructed defecation. It is used to diagnose and, at higher frequencies, to treat (break up) kidney stones or kidney crystals (nephrolithiasis).\n\nScrotal ultrasonography is used in the evaluation of testicular pain, and can help identify solid masses.\n\nMusculoskeletal ultrasound in used to examine tendons, muscles, nerves, ligaments, soft tissue masses, and bone surfaces. Ultrasound is an alternative to x-ray imaging in detecting fractures of the wrist, elbow and shoulder for patients up to 12 years (Fracture sonography).\n\nQuantitative ultrasound is an adjunct musculoskeletal test for myopathic disease in children; estimates of lean body mass in adults; proxy measures of muscle quality (i.e., tissue composition) in older adults with sarcopenia\n\nIn nephrology, ultrasonography of the kidneys is essential in the diagnosis and management of kidney-related diseases. The kidneys are easily examined, and most pathological changes in the kidneys are distinguishable with ultrasound. US is an accessible, versatile, inexpensive, and fast aid for decision-making in patients with renal symptoms and for guidance in renal intervention. Renal ultrasound (US) is a common examination, which has been performed for decades. Using B-mode imaging, assessment of renal anatomy is easily performed, and US is often used as image guidance for renal interventions. Furthermore, novel applications in renal US have been introduced with contrast-enhanced ultrasound (CEUS), elastography and fusion imaging. However, renal US has certain limitations, and other modalities, such as CT and MRI, should always be considered as supplementary imaging modalities in the assessment of renal disease.\n\nThe creation of an image from sound is done in three steps – producing a sound wave, receiving echoes, and interpreting those echoes.\n\nA sound wave is typically produced by a piezoelectric transducer encased in a plastic housing. Strong, short electrical pulses from the ultrasound machine drive the transducer at the desired frequency. The frequencies can be anywhere between 1 and 18 MHz, though frequencies up to 50–100 megahertz have been used experimentally in a technique known as biomicroscopy in special regions, such as the anterior chamber of the eye. Older technology transducers focused their beam with physical lenses. Newer technology transducers use phased array techniques to enable the ultrasound machine to change the direction and depth of focus.\n\nThe sound is focused either by the shape of the transducer, a lens in front of the transducer, or a complex set of control pulses from the ultrasound scanner, in the (beamforming) technique. This focusing produces an arc-shaped sound wave from the face of the transducer. The wave travels into the body and comes into focus at a desired depth.\n\nMaterials on the face of the transducer enable the sound to be transmitted efficiently into the body (often a rubbery coating, a form of impedance matching). In addition, a water-based gel is placed between the patient's skin and the probe.\n\nThe sound wave is partially reflected from the layers between different tissues or scattered from smaller structures. Specifically, sound is reflected anywhere where there are acoustic impedance changes in the body: e.g. blood cells in blood plasma, small structures in organs, etc. Some of the reflections return to the transducer.\n\nThe return of the sound wave to the transducer results in the same process as sending the sound wave, except in reverse. The returned sound wave vibrates the transducer and the transducer turns the vibrations into electrical pulses that travel to the ultrasonic scanner where they are processed and transformed into a digital image.\n\nTo make an image, the ultrasound scanner must determine two things from each received echo:\n\nOnce the ultrasonic scanner determines these two things, it can locate which pixel in the image to light up and to what intensity.\n\nTransforming the received signal into a digital image may be explained by using a blank spreadsheet as an analogy. First picture a long, flat transducer at the top of the sheet. Send pulses down the 'columns' of the spreadsheet (A, B, C, etc.). Listen at each column for any return echoes. When an echo is heard, note how long it took for the echo to return. The longer the wait, the deeper the row (1,2,3, etc.). The strength of the echo determines the brightness setting for that cell (white for a strong echo, black for a weak echo, and varying shades of grey for everything in between.) When all the echoes are recorded on the sheet, we have a greyscale image.\n\nImages from the ultrasound scanner are transferred and displayed using the DICOM standard. Normally, very little post processing is applied to ultrasound images.\n\nUltrasonography (sonography) uses a probe containing multiple acoustic transducers to send pulses of sound into a material. Whenever a sound wave encounters a material with a different density (acoustical impedance), part of the sound wave is reflected back to the probe and is detected as an echo. The time it takes for the echo to travel back to the probe is measured and used to calculate the depth of the tissue interface causing the echo. The greater the difference between acoustic impedances, the larger the echo is. If the pulse hits gases or solids, the density difference is so great that most of the acoustic energy is reflected and it becomes impossible to see deeper.\n\nThe frequencies used for medical imaging are generally in the range of 1 to 18 MHz. Higher frequencies have a correspondingly smaller wavelength, and can be used to make sonograms with smaller details. However, the attenuation of the sound wave is increased at higher frequencies, so in order to have better penetration of deeper tissues, a lower frequency (3–5 MHz) is used.\n\nSeeing deep into the body with sonography is very difficult. Some acoustic energy is lost every time an echo is formed, but most of it (approximately formula_1) is lost from acoustic absorption. (See also Acoustic attenuation for further details on modeling of acoustic attenuation and absorption.)\n\nThe speed of sound varies as it travels through different materials, and is dependent on the acoustical impedance of the material. However, the sonographic instrument assumes that the acoustic velocity is constant at 1540 m/s. An effect of this assumption is that in a real body with non-uniform tissues, the beam becomes somewhat de-focused and image resolution is reduced.\n\nTo generate a 2D-image, the ultrasonic beam is swept. A transducer may be swept mechanically by rotating or swinging. Or a 1D phased array transducer may be used to sweep the beam electronically. The received data is processed and used to construct the image. The image is then a 2D representation of the slice into the body.\n\n3D images can be generated by acquiring a series of adjacent 2D images. Commonly a specialised probe that mechanically scans a conventional 2D-image transducer is used. However, since the mechanical scanning is slow, it is difficult to make 3D images of moving tissues. Recently, 2D phased array transducers that can sweep the beam in 3D have been developed. These can image faster and can even be used to make live 3D images of a beating heart.\n\nDoppler ultrasonography is used to study blood flow and muscle motion. The different detected speeds are represented in color for ease of interpretation, for example leaky heart valves: the leak shows up as a flash of unique color. Colors may alternatively be used to represent the amplitudes of the received echoes.\n\nSeveral modes of ultrasound are used in medical imaging. These are:\n\n\nAn additional expansion or additional technique of ultrasound is biplanar ultrasound, in which the probe has two 2D planes that are perpendicular to each other, providing more efficient localization and detection. Furthermore, an omniplane probe is one that can rotate 180° to obtain multiple images. In 3D ultrasound, many 2D planes are digitally added together to create a 3-dimensional image of the object.\n\nDoppler ultrasonography employs the Doppler effect to assess whether structures (usually blood) are moving towards or away from the probe, and its relative velocity. By calculating the frequency shift of a particular sample volume, for example flow in an artery or a jet of blood flow over a heart valve, its speed and direction can be determined and visualized. \"Color Doppler\" is the measurement of velocity by color scale. Color Doppler images are generally combined with grayscale (B-mode) images to display \"duplex ultrasonography\" images. Uses include:\n\nA contrast medium for medical ultrasonography is a formulation of encapsulated gaseous microbubbles to increase echogenicity of blood, discovered by Dr Raymond Gramiak in 1968 and named contrast-enhanced ultrasound. This contrast medical imaging modality is clinically used throughout the world, in particular for echocardiography in the United States and for ultrasound radiology in Europe and Asia.\n\nMicrobubbles-based contrast media is administrated intravenously in patient blood stream during the medical ultrasonography examination. Thanks to their size, the microbubbles remain confined in blood vessels without extravasating towards the interstitial fluid. An ultrasound contrast media is therefore purely intravascular, making it an ideal agent to image organ microvascularization for diagnostic purposes. A typical clinical use of contrast ultrasonography is detection of a hypervascular metastatic tumor, which exhibits a contrast uptake (kinetics of microbubbles concentration in blood circulation) faster than healthy biological tissue surrounding the tumor. Other clinical applications using contrast exist, such as in echocardiography to improve delineation of left ventricle for visually checking contractibility of heart after a myocardial infarction. Finally, applications in quantitative perfusion (relative measurement of blood flow ) emerge for identifying early patient response to an anti-cancerous drug treatment (methodology and clinical study by Dr Nathalie Lassau in 2011), enabling to determine the best oncological therapeutic options.\n\nIn oncological practice of medical contrast ultrasonography, clinicians use the method of parametric imaging of vascular signatures invented by Dr Nicolas Rognin in 2010. This method is conceived as a cancer aided diagnostic tool, facilitating characterization of a suspicious tumor (malignant versus benign) in an organ. This method is based on medical computational science to analyze a time sequence of ultrasound contrast images, a digital video recorded in real-time during patient examination. Two consecutive signal processing steps are applied to each pixel of the tumor:\n\nOnce signal processing in each pixel completed, a color spatial map of the parameter is displayed on a computer monitor, summarizing all vascular information of the tumor in a single image called parametric image (see last figure of press article as clinical examples). This parametric image is interpreted by clinicians based on predominant colorization of the tumor: red indicates a suspicion of malignancy (risk of cancer), green or yellow – a high probability of benignity. In the first case (suspicion of malignant tumor), the clinician typically prescribes a biopsy to confirm the diagnostic or a CT scan examination as a second opinion. In the second case (quasi-certain of benign tumor), only a follow-up is needed with a contrast ultrasonography examination a few months later. The main clinical benefits are to avoid a systematic biopsy (risky invasive procedure) of benign tumors or a CT scan examination exposing the patient to X-ray radiation. The parametric imaging of vascular signatures method proved to be effective in humans for characterization of tumors in the liver. In a cancer screening context, this method might be potentially applicable to other organs such as breast or prostate.\n\nThe future of contrast ultrasonography is in molecular imaging with potential clinical applications expected in cancer screening to detect malignant tumors at their earliest stage of appearance. Molecular ultrasonography (or ultrasound molecular imaging) uses targeted microbubbles originally designed by Dr Alexander Klibanov in 1997; such targeted microbubbles specifically bind or adhere to tumoral microvessels by targeting biomolecular cancer expression (overexpression of certain biomolecules occurs during neo-angiogenesis or inflammation processes in malignant tumors). As a result, a few minutes after their injection in blood circulation, the targeted microbubbles accumulate in the malignant tumor; facilitating its localization in a unique ultrasound contrast image. In 2013, the very first exploratory clinical trial in humans for prostate cancer was completed at Amsterdam in the Netherlands by Dr Hessel Wijkstra.\n\nIn molecular ultrasonography, the technique of acoustic radiation force (also used for shear wave elastography) is applied in order to literally push the targeted microbubbles towards microvessels wall; firstly demonstrated by Dr Paul Dayton in 1999. This allows maximization of binding to the malignant tumor; the targeted microbubbles being in more direct contact with cancerous biomolecules expressed at the inner surface of tumoral microvessels. At the stage of scientific preclinical research, the technique of acoustic radiation force was implemented as a prototype in clinical ultrasound systems and validated \"in vivo\" in 2D and 3D imaging modes.\n\nUltrasound is also used for elastography, which is a relatively new imaging modality that maps the elastic properties of soft tissue. This modality emerged in the last two decades. Elastography is useful in medical diagnoses as it can discern healthy from unhealthy tissue for specific organs/growths. For example, cancerous tumors will often be harder than the surrounding tissue, and diseased livers are stiffer than healthy ones.\n\nThere are many ultrasound elastography techniques.\n\nInterventional ultrasonography involves biopsy, emptying fluids, intrauterine Blood transfusion (Hemolytic disease of the newborn).\n\nCompression ultrasonography is when the probe is pressed against the skin. This can bring the target structure closer to the probe, increasing spatial resolution of it. Comparison of the shape of the target structure before and after compression can aid in diagnosis.\n\nIt used in ultrasonography of deep venous thrombosis, wherein absence of vein compressibility is a strong indicator of thrombosis. Compression ultrasonography has both high sensitivity and specificity for detecting proximal deep vein thrombosis only in symptomatic patients. Results are not reliable when the patient is symptomless and must be checked, for example in high risk postoperative patients mainly in orthopedic patients.\n\nAs with all imaging modalities, ultrasonography has its list of positive and negative attributes.\n\n\n\nUltrasonography is generally considered safe imaging, with the World Health Organizations saying: \n\nDiagnostic ultrasound studies of the fetus are generally considered to be safe during pregnancy. This diagnostic procedure should be performed only when there is a valid medical indication, and the lowest possible ultrasonic exposure setting should be used to gain the necessary diagnostic information under the \"as low as reasonably practicable\" or ALARP principle.\n\nHowever, medical ultrasonography should not be performed without a medical indication to perform it. To do otherwise would be to perform unnecessary health care to patients, which bring unwarranted costs and may lead to other testing. Overuse of ultrasonography is sometimes as routine as screening for deep vein thrombosis after orthopedic surgeries in patients who are not at heightened risk for having that condition.\n\nSimilarly, although there is no evidence ultrasound could be harmful for the fetus, medical authorities typically strongly discourage the promotion, selling, or leasing of ultrasound equipment for making \"keepsake fetal videos\".\n\n\nDiagnostic and therapeutic ultrasound equipment is regulated in the USA by the Food and Drug Administration, and worldwide by other national regulatory agencies. The FDA limits acoustic output using several metrics; generally, other agencies accept the FDA-established guidelines.\n\nCurrently, New Mexico, Oregon, and North Dakota are the only US states that regulate diagnostic medical sonographers. Certification examinations for sonographers are available in the US from three organizations: the American Registry for Diagnostic Medical Sonography, Cardiovascular Credentialing International and the American Registry of Radiologic Technologists. \n\nThe primary regulated metrics are Mechanical Index (MI), a metric associated with the cavitation bio-effect, and Thermal Index (TI) a metric associated with the tissue heating bio-effect. The FDA requires that the machine not exceed established limits, which are reasonably conservative so as to maintain diagnostic ultrasound as a safe imaging modality. This requires self-regulation on the part of the manufacturer in terms of the machine's calibration.\n\nUltrasound-based pre-natal care and sex screening technologies were launched in India in the 1980s. With concerns about its misuse for sex-selective abortion, the Government of India passed the Pre-natal Diagnostic Techniques Act (PNDT) in 1994 to regulate legal and illegal uses of ultrasound equipment. The law was further amended into the Pre-Conception and Pre-natal Diagnostic Techniques (Regulation and Prevention of Misuse) (PCPNDT) Act in 2004 to deter and punish prenatal sex screening and sex selective abortion. It is currently illegal and a punishable crime in India to determine or disclose the sex of a fetus using ultrasound equipment.\n\nAfter the French physicist Pierre Curie’s discovery of piezoelectricity in 1880, ultrasonic waves could be deliberately generated for industry. Thereafter, in 1940, the American acoustical physicist Floyd Firestone devised the first ultrasonic echo imaging device, the Supersonic Reflectoscope, to detect internal flaws in metal castings. In 1941, the Austrian neurologist Karl Theo Dussik was in collaboration with his brother, Friedreich, a physicist, likely, the first person to ultrasonically echo image the human body, outlining thereby the ventricles of a human brain. Ultrasonic energy was first applied to the human body for medical purposes by Dr George Ludwig at the Naval Medical Research Institute, Bethesda, Maryland in the late 1940s. English-born physicist John Wild (1914–2009) first used ultrasound to assess the thickness of bowel tissue as early as 1949; he has been described as the \"father of medical ultrasound\". Subsequent advances in the field took place concurrently in several countries. But it was not until 1963 when Meyerdirk & Wright launched production of the first commercial hand-held articulated arm compound contact B-mode scanner that ultrasound became generally available for medical use.\n\nLéandre Pourcelot, who was a researcher and teacher at INSA (Institut National des Sciences Appliquées) Lyon copublished in 1965 a report at \"Académie des sciences\" « Effet Doppler et mesure du débit sanguin » (Doppler effect and measure of the blood flow), the basis of his design of a Doppler flow meter in 1967.\n\nIn his book \"L'investigation vasculaire par ultrasonographie Doppler\" (Ed Masson, 1977) Dr Claude Franceschi laid down the Doppler Ultrasound fundamentals of the hemodynamics semiotics, which are still in use in current Doppler arterial and venous Duplex Ultrasound investigations.\n\nParallel developments in Glasgow, Scotland by Professor Ian Donald and colleagues at the Glasgow Royal Maternity Hospital (GRMH) led to the first diagnostic applications of the technique. Donald was an obstetrician with a self-confessed \"childish interest in machines, electronic and otherwise\", who, having treated the wife of one of the company's directors, was invited to visit the Research Department of boilermakers Babcock & Wilcox at Renfrew, where he used their industrial ultrasound equipment to conduct experiments on various morbid anatomical specimens and assess their ultrasonic characteristics. Together with the medical physicist . and fellow obstetrician Dr John MacVicar, Donald refined the equipment to enable differentiation of pathology in live volunteer patients. These findings were reported in \"The Lancet\" on 7 June 1958 as \"Investigation of Abdominal Masses by Pulsed Ultrasound\" – possibly one of the most important papers ever published in the field of diagnostic medical imaging.\n\nAt GRMH, Professor Donald and Dr James Willocks then refined their techniques to obstetric applications including fetal head measurement to assess the size and growth of the fetus. With the opening of the new Queen Mother's Hospital in Yorkhill in 1964, it became possible to improve these methods even further. Dr Stuart Campbell's pioneering work on fetal cephalometry led to it acquiring long-term status as the definitive method of study of foetal growth. As the technical quality of the scans was further developed, it soon became possible to study pregnancy from start to finish and diagnose its many complications such as multiple pregnancy, fetal abnormality and \"placenta praevia\". Diagnostic ultrasound has since been imported into practically every other area of medicine.\n\nMedical ultrasonography was used in 1953 at Lund University by cardiologist Inge Edler and Gustav Ludwig Hertz's son Carl Hellmuth Hertz, who was then a graduate student at the University's department of nuclear physics.\n\nEdler had asked Hertz if it was possible to use radar to look into the body, but Hertz said this was impossible. However, he said, it might be possible to use ultrasonography. Hertz was familiar with using ultrasonic reflectoscopes of the American acoustical physicist Floyd Firestone's invention for nondestructive materials testing, and together Edler and Hertz developed the idea of using this method in medicine.\n\nThe first successful measurement of heart activity was made on October 29, 1953 using a device borrowed from the ship construction company Kockums in Malmö. On December 16 the same year, the method was used to generate an echo-encephalogram (ultrasonic probe of the brain). Edler and Hertz published their findings in 1954.\n\nIn 1962, after about two years of work, Joseph Holmes, William Wright, and Ralph Meyerdirk developed the first compound contact B-mode scanner. Their work had been supported by U.S. Public Health Services and the University of Colorado. Wright and Meyerdirk left the University to form Physionic Engineering Inc., which launched the first commercial hand-held articulated arm compound contact B-mode scanner in 1963. This was the start of the most popular design in the history of ultrasound scanners.\n\nIn the late 1960s Dr Gene Strandness and the bio-engineering group at the University of Washington conducted research on Doppler ultrasound as a diagnostic tool for vascular disease. Eventually, they developed technologies to use duplex imaging, or Doppler in conjunction with B-mode scanning, to view vascular structures in real-time, while also providing hemodynamic information.\n\nThe first demonstration of color Doppler was by Geoff Stevenson, who was involved in the early developments and medical use of Doppler shifted ultrasonic energy.\n\nThe leading manufacturers of Ultrasound Equipment are Siemens Healthineers, GE Healthcare, and Philips.\n\n"}
{"id": "7581994", "url": "https://en.wikipedia.org/wiki?curid=7581994", "title": "Ministry of Science and Technology (Taiwan)", "text": "Ministry of Science and Technology (Taiwan)\n\nThe Ministry of Science and Technology (MOST; ) is the government ministry of the Republic of China (Taiwan) for the promotion and funding of academic research, development of science and technology and science parks. MOST is a member of Belmont Forum.\n\nThe MOST was originally established as the National Council on Science Development on 1 February 1959. In 1967, it was renamed to National Science Council (NSC; ). The NSC became the Ministry of Science and Technology on 3 February 2014.\n\n\n\n\nPolitical Party:\n\nThe 2014 budget for MOST is NT$44.043 billion, in which 79.6% is dedicated for support for academic research, 12.5% for promotion of national science and technology development and 7.9% for development of science parks.\n\nThe MOST building is accessible within walking distance South West from Technology Building Station of the Taipei Metro.\n\n\n"}
{"id": "39370689", "url": "https://en.wikipedia.org/wiki?curid=39370689", "title": "Mohamed Yousef Soliman", "text": "Mohamed Yousef Soliman\n\nMohamed Yousef Soliman is a professor and the former chairperson of the department of Petroleum Engineering at Texas Tech University. After working for Halliburton for 32 years, he joined Texas Tech in January 2011. He obtained his bachelor's degree in petroleum engineering from Cairo University in 1971. Having completed his bachelor's degree he came to the United States to continue higher education. He received his masters and doctorate degrees, both in Petroleum engineering, from Stanford University in 1975 and 1978 His M. S. Thesis was \"Rheological Properties of Emulsion Flowing Through Capillary Tubes Under Turbulent Conditions,\"; his Ph.D. thesis, \"Numerical Modeling of Thermal Recovery Processes.\"\n\nHe is the author or co-author of over 180 technical papers, almost all of which are in the field of Petroleum engineering and Oil industry. Additionally he is credited with 20 inventions.\n\nSoliman is a specialist in hydraulic fracturing and production engineering. He holds 21 patents on Hydraulic fracturing operations and analysis, testing and conformance applications, and is an author or co-author of over 170 technical papers and articles in areas of fracturing, reservoir engineering, well test analysis, conformance, and numerical simulation.\n\nHe has written chapters in \"World Oil's Handbook of Horizontal Drilling and Completion Technology\", the text \"Well Construction\", and the SPE monograph \"Well Test Analysis of Hydraulically Fractured Wells\". He has authored several books for internal use at Halliburton, including Stimulation and Reservoir Engineering Aspects of Horizontal Wells, Well Test Analysis, Hydraulic Fracturing, and chapters in Conformance, Stimulation, and FracPac. He is a distinguished member of the Society of Petroleum Engineers.\n\n"}
{"id": "17051391", "url": "https://en.wikipedia.org/wiki?curid=17051391", "title": "Mxim", "text": "Mxim\n\nMxim (Method to serve text and images) is an application development framework for hand held devices. It is a benchmark architecture for serving browseable content on cross platform devices.\n\nMxim Framework is application development framework for mobile phones. It works with Java enabled devices. It allows web developers can work using mxml to port web sites to the mobile applications.\n\nMxim Content Server is content serving solution for mxim applications.\n\nMxim applications can develop using mxml and Mxim server renders mxml application user interface, and behaves nearly like a web solutions including device level controls.\n\n"}
{"id": "17641506", "url": "https://en.wikipedia.org/wiki?curid=17641506", "title": "Nicolas Appert (study association)", "text": "Nicolas Appert (study association)\n\nNicolas Appert is a Dutch study association for Food Technology students at Wageningen University. The association is named after the French inventor Nicolas Appert, who can be seen as the first food technologist. The chef Nicolas Appert was researching ways to preserve food for a longer period of time and discovered that heating food products in a closed glass jar would greatly increase its shelflife. A process, nowadays called appertisation.\n\nOn September 6, 1962, during a foreign study trip, the study association Nicolas Appert was founded. Nowadays the association is still very active, having good relations with food companies and organising a lot of activities for its members.\n\nExamples of these activities are lectures and excursions from and to food companies, but also social activities such as a sporting or drinks with the departments of Food Processing, Food Chemistry, Food Microbiology, Food Physics and Product Design and Quality Management. All these activities are made possible by the participation of a large group of active, enthusiastic students in committees and the board.\n\n"}
{"id": "2367846", "url": "https://en.wikipedia.org/wiki?curid=2367846", "title": "Oracle Financial Services Software", "text": "Oracle Financial Services Software\n\nOracle Financial Services Software Limited(ofss) is a subsidiary of Oracle Corporation. It is an IT solution provider to the banking industry. It claims to have more than 900 customers in over 145 countries. Oracle Financial Services Software Limited is ranked No. 9 in IT companies of India and overall ranked No. 253 in Fortune India 500 list in 2011.\n\nThere was a company of CITIBANK in 1990 named as CitiCorp Oversea Software Ltd., (later known as i-flex in world market). Over a period of time, another(?) company was merged into it and new company was started named as Citicorp Information Technologies Industries Ltd was formed. (CITIL) out of COSL and named Rajesh Hukku to head CITIL. While COSL's mandate was to serve Citicorp's internal needs globally and be a cost center, CITIL's mandate was to be profitable by serving not only Citicorp but the whole global financial software market. Largely known as i-flex and then eventually renamed to Oracle Financial Services Software\n\nCITIL was started with universal banking product named as MicroBanker (which became successful in some English speaking parts of Africa and other developing regions over the next 3–4 years) and the retail banking product Finware. In the mid-90s, the firm developed Flexcube (stylized FLEXCUBE) at its Bangalore Centre. After the launch of Flexcube, all of CITIL's transnational banking products were brought under a common brand umbrella. Subsequently, company's name was changed to i-flex Solutions India Ltd.,\n\nOracle purchased Citigroup's 41% stake in i-flex solutions for US$593 million in August 2005, a further 7.52% in March and April 2006, and 3.2 per cent in an open-market purchase in mid-April 2006.\n\nOn 14 August 2006, oracle financial services announced it would acquire Mantas, a US-based anti-money laundering and compliance software company for US$122.6 million. The company part-funded the transaction through a preferential share allotment to majority shareholder Oracle Corporation.\n\nOn 12 January 2007, after an open offer price to minority shareholders, Oracle increased its stake in i-flex to around 83%.\n\nOn 4 April 2008, Oracle changed the name of the company to Oracle Financial Services Limited.\n\nOn 24 October 2010, Oracle announced the appointment of Chaitanya M Kamat as Managing Director and CEO of Oracle Financial Services Software Limited. . The outgoing CEO and MD, N.R.K.Raman retired from these posts after 25 years of service.\n\nNow Oracle Financial Services Software Limited is a major part of Oracle Financial Services Global Business Unit (FSGBU) under Sonny Singh who is the Vice President & Group Head of Oracle FSGBU World Wide.\n\nOracle Financial Services Software Limited has two main streams of business. The products division (formerly called BPD – Banking products Division) and PrimeSourcing. The company's offerings cover retail, corporate and investment banking, funds, cash management, trade, treasury, payments, lending, private wealth management, asset management and business analytics. The company undertook a rebranding exercise in the latter half of 2008. As part of this, the corporate website was integrated with Oracle's website and various divisions, services and products renamed to reflect the new identity post alignment with Oracle.\n\nRecently, Oracle Financial Services launched products for Internal Capital Adequacy Assessment Process, exposure management, enterprise performance management and energy and commodity trading compliance.\n\nThe company promotes its BPO business process outsourcing business via its subsidiary Equinox Corporation which is based in Irvine, California.\n\n"}
{"id": "2741241", "url": "https://en.wikipedia.org/wiki?curid=2741241", "title": "Phosphorescent organic light-emitting diode", "text": "Phosphorescent organic light-emitting diode\n\nPhosphorescent organic light-emitting diodes (PHOLED) are a type of organic light-emitting diode (OLED) that use the principle of phosphorescence to obtain higher internal efficiencies than fluorescent OLEDs. This technology is currently under development by many industrial and academic research groups.\n\nLike all types of OLED, phosphorescent OLEDs emit light due to the electroluminescence of an organic semiconductor layer in an electric current. Electrons and holes are injected into the organic layer at the electrodes and form excitons, a bound state of the electron and hole. \n\nElectrons and holes are both fermions with half integer spin. An exciton is formed by the coulombic attraction between the electron and the hole, and it may either be in a singlet state or a triplet state, depending on the spin states of these two bound species. Statistically, there is a 25% probability of forming a singlet state and 75% probability of forming a triplet state. Decay of the excitons results in the production of light through spontaneous emission.\n\nIn OLEDs using fluorescent organic molecules only, the decay of triplet excitons is quantum mechanically forbidden by selection rules, meaning that the lifetime of triplet excitons is long and phosphorescence is not readily observed. Hence it would be expected that in fluorescent OLEDs only the formation of singlet excitons results in the emission of useful radiation, placing a theoretical limit on the internal quantum efficiency (the percentage of excitons formed that result in emission of a photon) of 25%.\n\nHowever, phosphorescent OLEDs generate light from both triplet and singlet excitons, allowing the internal quantum efficiency of such devices to reach nearly 100%.\n\nThis is commonly achieved by doping a host molecule with an organometallic complex. These contain a heavy metal atom at the centre of the molecule, for example platinum or iridium, of which the green emitting complex Ir(mppy) is just one of many examples. The large spin-orbit interaction experienced by the molecule due to this heavy metal atom facilitates intersystem crossing, a process which mixes the singlet and triplet character of excited states. This reduces the lifetime of the triplet state, therefore phosphorescence is readily observed.\n\nDue to their potentially high level of energy efficiency, even when compared to other OLEDs, PHOLEDs are being studied for potential use in large-screen displays such as computer monitors or television screens, as well as general lighting needs. One potential use of PHOLEDs as lighting devices is to cover walls with large area PHOLED light panels. This would allow entire rooms to glow uniformly, rather than require the use of light bulbs which distribute light unequally throughout a room. The United States Department of Energy has recognized the potential for massive energy savings via the use of this technology and therefore has awarded $200,000 USD in contracts to develop PHOLED products for general lighting applications.\n\nOne problem that currently hampers the widespread adoption of this highly energy efficient technology is that the average lifetimes of red and green PHOLEDs are often tens of thousands of hours longer than those of blue PHOLEDs. This may cause displays to become visually distorted much sooner than would be acceptable for a commercially viable device.\n"}
{"id": "43298724", "url": "https://en.wikipedia.org/wiki?curid=43298724", "title": "PlayCanvas", "text": "PlayCanvas\n\nPlayCanvas is an open-source 3D game engine/interactive 3D application engine alongside a proprietary cloud-hosted creation platform that allows for simultaneous editing from multiple computers via a browser-based interface. It runs in modern browsers that support WebGL, including Mozilla Firefox and Google Chrome. The engine is capable of rigid-body physics simulation, handling three-dimensional audio and 3D animations.\n\nPlayCanvas has gained the support of ARM, Activision and Mozilla.\n\nThe PlayCanvas engine was open-sourced on June 4, 2014.\n\nThe PlayCanvas platform has collaborative real-time Editor that allows editing project by multiple developers simultaneously. The engine supports the WebGL 1.0 and 2.0 standard to produce GPU accelerated 3D graphics and allows for scripting via the JavaScript programming language.\nProjects can be distributed via an URL web link or packaged in native wrappers, e.g. for Android, using CocoonJS or for Steam using Electron, and many other options and platforms.\n\nVarious companies use PlayCanvas in projects of different disciplines of interactive 3D content in the web.\n\nDisney created an educational game for Hour of Code based on its Moana film.\n\nKing published Shuffle Cats Mini as a launch title for Facebook Instant Games.\n\nTANX - massively multiplayer online game of cartoon styled tanks.\n\nMiniclip published number of games on their platform with increase of HTML5 games popularity on the web.\n\nMozilla collaborated with PlayCanvas team creating After the Flood demo for presenting cutting edge features of WebGL 2.0.\n\n\n"}
{"id": "10194891", "url": "https://en.wikipedia.org/wiki?curid=10194891", "title": "Raleigh Bicycle Company", "text": "Raleigh Bicycle Company\n\nThe Raleigh Bicycle Company is a bicycle manufacturer based in Nottingham, England. Founded by Woodhead and Angois in 1885, who used Raleigh as their brand name, it is one of the oldest bicycle companies in the world. After being acquired by Frank Bowden, it became The Raleigh Cycle Company in December 1888, which was registered as a limited liability company in January 1889. By 1913, it was the biggest bicycle manufacturing company in the world. From 1921 to 1935, Raleigh also produced motorcycles and three-wheel cars, leading to the formation of the Reliant Company. The Raleigh division of bicycles is currently owned by the Dutch corporation Accell.\n\nIn 2006, the Raleigh Chopper was named in the list of British design icons in the Great British Design Quest organised by the BBC and the Design Museum.\n\nThe history of Raleigh bicycles started in 1885, when Richard Morriss Woodhead from Sherwood Forest, and Paul Eugene Louis Angois, a French citizen, set up a small bicycle workshop in Raleigh Street, Nottingham, England. In the spring of that year, they started advertising in the local press. The \"Nottinghamshire Guardian\" of 15 May 1885 printed what was possibly the first Woodhead and Angois classified advertisement.\nNearly two years later, the 11 April 1887 issue of \"The Nottingham Evening Post\" contained a display advertisement for the Raleigh ‘Safety’ model under the new banner ‘Woodhead, Angois, and Ellis. Russell Street Cycle Works.’ William Ellis had recently joined the partnership and provided much-needed financial investment. Like Woodhead and Angois, Ellis’s background was in the lace industry. He was a lace gasser, a service provider involved in the bleaching and treating of lace, with premises in nearby Clare Street and Glasshouse Street. Thanks to Ellis, the bicycle works had now expanded round the corner from Raleigh Street into former lace works on the adjoining road, Russell Street. By 1888, the company was making about three cycles a week and employed around half a dozen men. It was one of 15 bicycle manufacturers based in Nottingham at that time.\n\nFrank Bowden, a recent convert to cycling who on medical advice had toured extensively on a tricycle, first saw a Raleigh bicycle in a shop window in Queen Victoria Street, London, about the time that William Ellis’s investment in the cycle workshop was beginning to take effect. Bowden described how this led to him visiting the Raleigh works:\n\nIt is clear from Frank Bowden’s own account that, although he bought a Raleigh ‘Safety’ in 1887, he did not visit the Raleigh workshop until autumn 1888. That visit led to Bowden replacing Ellis as the partnership’s principal investor, though Bowden did not become the outright owner of the firm. He concluded that the company had a profitable future if it promoted its innovative features, increased its output, cut its overhead costs and tailored its products to the individual tastes and preferences of its customers. He bought out William Ellis’s share in the firm and was allotted 5,000 £1 shares, while Woodhead and Angois between them held another 5,000 shares.\n\nIn Frank Bowden's own lifetime, Raleigh publicity material stated that the firm was founded in 1888, which was when Bowden, as he himself confirmed, first bought into the enterprise. Thus, Raleigh's 30th anniversary was celebrated in 1918. The 1888 foundation date is confirmed by Bowden's great-grandson, Gregory Houston Bowden, who states that Frank Bowden \"began to negotiate with Woodhead and Angois and in December 1888 founded 'The Raleigh Cycle Company'.\" The December 1888 foundation date is also confirmed by Nottinghamshire Archives. In recent years, the Raleigh company has cited 1887 as a foundation date but, whilst this pre-dates Bowden's involvement, the Raleigh brand name was created by Woodhead and Angois and the enterprise can, as demonstrated above, be traced back to 1885.\n\nThe company established by Bowden in December 1888 was still privately owned with unlimited public liability. In January 1889, it became the first of a series of limited liability companies with Raleigh in its name. It had a nominal capital of £20,000, half of which was provided by Frank Bowden. Paul Angois was appointed director responsible for product design, Richard Woodhead was made director responsible for factory management, and Frank Bowden became chairman and managing director. Some shares were made available to small investors and local businessmen, but take-up was minimal, and Bowden ended up buying most of the public shares. He subsequently supplied virtually all the capital needed to expand the firm.\n\nWhen Frank Bowden got involved with the enterprise, the works comprised three small workshops and a greenhouse. As Woodhead, Angois and Ellis, the firm had expanded round the corner from Raleigh Street into Russell Street, where also stood Clarke’s five-storey former lace factory. To enable further expansion of the business, Bowden financed the renting of this property and installation of new machinery.\n\nUnder Bowden's guidance, Raleigh expanded rapidly. By 1891, the company occupied not only Clarke's factory but also Woodroffe’s Factory and Russell Street Mills. In November 1892, Raleigh signed a tenancy agreement for rooms in Butler’s factory on the other side of Russell Street. Shortly after this, the company also occupied Forest Road Mill. (Forest Road junctions with Russell Street at the opposite end from Raleigh Street.)\n\nBowden created a business which, by 1913, was the biggest bicycle manufacturing company in the world, occupying seven and a half acres in purpose-built premises completed in 1897 at Faraday Road, Lenton, Nottingham. It subsequently became very much bigger.\n\nSir Frank Bowden died in 1921 and his son Sir Harold Bowden, 2nd Baronet took over as\nchairman and chief executive, guiding the company through the next 17 years of expansion.\nThere was a resurgence in domestic and export demand for pedal bicycles and by February 1932 Raleigh had acquired all the Humber Limited trade marks. Manufacture was transferred to Raleigh's Nottingham works. Raleigh-made Humbers differed from Raleighs only in chainwheels, fork crowns and some brakework.\n\nDuring the Second World War, the Raleigh factory in Nottingham was used for the production of fuzes. Bicycle production was reduced to approximately 5% of its peacetime capacity.\n\nIn 1939, Raleigh opened a bicycle factory at 6 Hanover Quay, Dublin, Ireland and commenced bicycle production there. The Raleigh (Ireland) business expanded and moved to 8–11 Hanover Quay, Dublin in 1943. The plant produced complete bicycles and Sturmey-Archer hubs, and remained in production until 1976, when the factory burned down. Models produced there latterly were the Chopper and Triumph 20. The head badges changed in the late 1960s, possibly after the passing of the Trade Descriptions Act in the UK. Dublin-made machines no longer had \"Nottingham England\" on the Heron or Triumph head badge, the panel being left blank instead.\nIn 1899, Raleigh started to build motorcycles and in 1903, introduced the Raleighette, a belt-driven three-wheel motorcycle with the driver in the back and a wicker seat for the passenger between the two front wheels. Financial losses meant production lasted only until 1908.\n\nIn 1930, the company acquired the rights to the Ivy Karryall, a motorcycle fitted with a cabin for cargo and a hood for the driver. Raleigh's version was called the Light Delivery Van and had a chain drive. A two-passenger version was followed by Raleigh's first three-wheel car, the Safety Seven. It was a four-seat convertible with shaft drive and a maximum of . A saloon version was planned, but Raleigh shut its motor department to concentrate on bicycles again. Chief designer T. L. Williams took the equipment and remaining parts and moved to Tamworth, where his company produced three-wheelers for 65 years. The leftover parts from Raleigh carried an \"R\", so Williams chose a matching name: Reliant.\nRaleigh also made mopeds in the late 1950s and 1960s as the bicycle market declined. The most popular of which was the RM6 Runabout. This model featured unsprung front forks and a cycle type calliper front brake which made it a very affordable mode of transport. Because of its success, production continued until February 1971; 17 months after Raleigh had stopped manufacturing all other mopeds.\n\nAfter World War II, Raleigh became known for its lightweight \"sports roadster\" bicycles, often using Sturmey-Archer three and five-speed transmissions. These cycles were considerably lighter and quicker than either the old heavy English utility roadster or the American \"balloon-tire\" cruiser bikes. In 1946, Raleigh and other English bicycle manufacturers accounted for 95% of the bicycles imported into the United States.\n\nRaleigh's \"sports roadster\", or \"British racer\" bicycles were exported around the world, including the United States. The company continued to increase imports to the United States until 1955, when a rate increase in foreign bicycle tariffs caused a shift in imports in favour of bicycles from West Germany and the Netherlands. However, this proved only a temporary setback, and by 1964, Raleigh was again a major selling brand in the US bicycle market.\n\nIn 1965, Raleigh introduced the RSW 16, its long-awaited competitor to the hugely successful Moulton Bicycle. The new Raleigh shared several important features with the Moulton, including small wheels, an open frame and built-in luggage carrying capacity.\n\nHowever, the RSW lacked the Moulton's suspension, which compensated for the bumpy ride that comes with small wheels. Instead, Raleigh fitted the RSW with balloon tyres, which effectively smoothed the ride but at the cost of increased rolling resistance. Nevertheless, the RSW was pleasant to ride, and Raleigh's extensive retail network ensured its success.\n\nThe success of the RSW took sales away from the Moulton and put the maker into financial difficulties. Raleigh then bought out Moulton and produced both bikes until 1974. Raleigh also produced a sister model to the RSW, the 'Twenty', which was more successful and remained in production well into the 1980s.\n\nWhile bicycle production had steadily risen through the mid-1950s, the British market began to decline with the increasing affordability and popularity of the motor car. For much of the postwar era, British bicycle manufacturers had largely competed with each other in both the home and export markets, but 1956 saw the formation of the British Cycle Corporation by the Tube Investments Group which already owned Phillips, Hercules, Armstrong, and Norman. In 1957, Raleigh bought the BSA Cycles Ltd., BSA's bicycle division, which gave them exclusive use of the former brand names New Hudson and Sunbeam. Raleigh also already owned the Robin Hood brand, and Three Spires with Triumph (cycles) also at their disposal.\n\nBSA had itself acquired Triumph Cycle Co. Ltd. only five years previously. Ti added the Sun bicycle company to their stable in 1958, and with two \"super groups\" now controlling a large portion of the market, it was perhaps inevitable that in 1960, Tube Investments acquired Raleigh and merged the British Cycle Corporation with Raleigh to form TI–Raleigh, which now had 75% of the UK market. TI–Raleigh then acquired Carlton Cycles in Worksop, England that same year, at the time one of the largest semi-custom lightweight makers in the UK. Ti Raleigh gave total control of its cycle division to Raleigh and soon set about marketing many of the acquired names as budget ranges, though with Raleigh frames. The old British Cycle Corporation factory at Handsworth continued to produce non Raleigh branded product well into the 1970s, with Raleigh branded models built in the main plant at Nottingham. However, the Sun branded bicycles were made in the Carlton factory at Worksop, England.\n\nAs a vertically integrated manufacturer in the mid-1960s, TI–Raleigh owned Brooks (one of the oldest saddle makers in the world), Sturmey-Archer (pioneer of 3-speed hubs), and Reynolds (maker of 531 tubing). Carlton, which had been unable to make inroads in the USA market after a failed rebranding deal with Huffy, found success in the late 1960s by recasting itself as \"Raleigh-Carlton\", a Raleigh-logo'd bike with some Carlton badging, and using the US dealer network to import and distribute bikes.\n\nThe Raleigh Chopper was designed by Nottingham native Alan Oakley, though this has been disputed by Cambridge designer Tom Karen. The Chopper was patented in the UK in 1967 and patented in the US in 1968. The bike was the \"must have\" item and signifier of \"coolness\" for many children at the time. The Chopper was first available for sale in June 1969 in North America. It went on sale in the UK in 1970 and sold well, and was a key factor in reviving the company's fortunes. The \"Chopper\" featured a 3-speed Sturmey-Archer gear hub, shifted using a top-tube mounted gear lever reminiscent of the early Harley-Davidson suicide shifter — one of its \"cool\" features. Other differences were the unusual frame, long padded seat with backrest, sprung suspension at the back, high-rise handlebars, and differently sized front (16\") and rear (20\") wheels. Tyres were wider than usual for the time, with a chunky tread on the rear wheel, featuring red highlights on the sidewall. The price was from approximately £32 for a standard \"Chopper\" to £55 for the deluxe. Two smaller versions, the \"Chipper\" and \"Tomahawk\", also sold well.\n\nThe Mk 2 \"Chopper\" was an improved version from 1972. It had the option of five-speed derailleur gears in the United States, but all UK bikes had the 3 speed hub, with the exception of a model introduced in 1973 and only available in a bizarre shade of pink. This model was discontinued in 1976. The Mk 2 had a shorter seat and the frame modified to move the rear of the seat forward, this helped prevent the bike tipping up. The shorter seat also made it harder to ride '2 up' (2 people on the bike at a time). The \"Chopper\" remained in production until 1982, when the rising popularity of the BMX bicycle caused sales to drop off.\n\nRaleigh revisited the chopper design in recent times, with great success although the new version has had some changes to conform to modern safety laws. Gone is the top tube shifter and long integrated seat, but the look and feel of the bike remain.\n\nIn 1979, production of Raleigh 531 butted-tube bicycles reached 10,000 units a year. In 1980, the former Carlton factory at Worksop closed and production was moved to a Lightweights facility at Nottingham. However, all bicycles made there afterward still carried the W for Worksop frame number designation. In 1982, rights to the \"Raleigh USA\" name were purchased by the Huffy Corporation. Under the terms of the agreement, \"Raleigh of England\" licensed Huffy to design and distribute Raleigh bicycles in the USA, and Huffy was given instant access to a nationwide network of bike shops. The renamed \"Raleigh Cycle Company of America\" sold bikes in the US while the rest of the world, including Canada, received \"Raleigh of England\" bikes. At that time, production of some U.S. Raleigh models were shifted to Japan, with Bridgestone manufacturing most of these bikes. By 1984, all Raleighs for the American market, except the top-of-the range Team Professional (made in Ilkeston) and Prestige road bikes (made in Nottingham), were produced in the Far East. Meanwhile, in the home market, Raleigh had broken into the new UK BMX market with their Burner range, which was very successful.\n\nIn 1987, the leading German bicycle manufacturer Derby Cycle bought Raleigh from Ti and Raleigh USA from Huffy. In 1988, Derby opened a factory in Kent, Washington manufacturing two Raleigh lines, the bimetallic \"Technium\" road bike line, which used heat-treated aluminum main frame tubes, thermally bonded and heat-cured to internal steel lugs using a Boeing-developed proprietary epoxy — along with chromoly steel head tube and rear stays. Kent also manufactured the off-road chromoly steel \"Altimetric\" line (Tangent CX, Traverse CX, Tactic CX and Talon CX 1991-1992). The factory closed in 1994. All \"Raleigh Cycle Company of America\" parts and frames from 1995 on were then mass-produced in China and Taiwan and assembled in other plants.\n\nThe high-end framesets offered for sale in Raleigh catalogues together with the frames built for Team riders were produced in Ilkeston by the Special Bicycle Developments Unit (SBDU) from 1974 to 1989 under the guidance of Gerald V O'Donovan; this production was moved to a new \"Raleigh Special Products\" division in Nottingham on closure of the Kent factory.\n\n\"Raleigh Canada\" had a factory in Waterloo, Quebec from 1972 to 2013. Derby Cycle acquired Diamondback Bicycles in 1999. In the same year, Raleigh ceased volume production of frames in the UK and its frame-making equipment were sold by auction.\n\nIn 2000, Derby Cycle controlled Raleigh USA, Raleigh UK, Raleigh Canada, and Raleigh Ireland. In the latter three markets, Raleigh was the number-one manufacturer of bicycles. Derby Cycle began a series of divestitures, because of financial pressure and sold Sturmey-Archer's factory site to the University of Nottingham and Sturmey-Archer and saddle manufacturer Brooks to a small company called Lenark. Lenark promised to build a new factory in Calverton but failed to pay the first instalment and the company entered liquidation. It was reported that the reason for selling the business, after extracting the cash for the factory site, was to have Lenark declare it insolvent so that neither Derby nor Lenark would have to pay the redundancy costs. Sturmey-Archer's assets were acquired by SunRace of Taiwan who relocated the factory to Taiwan and sales to the Netherlands. Sister company Brooks was sold to Selle Royal of Italy.\n\nIn 2001, following continuing financial problems at Derby Cycle, there was a management buy-out of all the remaining Raleigh companies led by Alan Finden-Crofts.\n\nBy 2003, assembly of bicycles had ended in the UK with 280 assembly and factory staff made redundant, and bicycles were to come \"from Vietnam and other centres of 'low-cost, high-quality' production.\" with final assembly takes place in Cloppenburg, Germany.\n\nIn 2012, Derby was acquired by Pon, a Dutch company, as part of their new bicycle group, which also owns Gazelle and Cervélo. Pon now sell Raleigh under licence throughout Germany.\n\nIn April 2012, Raleigh UK, Canada and USA were acquired by a separate Dutch group Accell for £62m (US$100m), whose portfolio includes the Lapierre and Ghost bicycle brands.\n\nRaleigh had a long association with cycle sport. Most notable is the TI–Raleigh team of the 1970s and 1980s. In 1980 Joop Zoetemelk won the Tour de France on a Raleigh. In the mid-1980s the Raleigh team was co-sponsored by Panasonic. In 1984, riding Raleigh-badged bicycles, Team USA scored several impressive victories at the Olympic Games in Los Angeles. The company also supplied bicycles to the French Système U team in the late 1980s where Laurent Fignon lost the 1989 Tour de France to Greg LeMond by 8 seconds. The company's special products division made race frames, including those used by the Raleigh professional team of the 1970s. Presently Raleigh as a company owns the Diamondback Bike brand as well. During the 1980s Raleigh also supported British professional teams, including \"Raleigh Banana\" and \"Raleigh Weinmann\". Raleigh's most notable riders were Paul Sherwin, Malcolm Elliott, Mark Bell, Paul Watson, Jon Clay and Jeff Williams. It also sponsored a mountain bike team in the early 1990s that also raced in road events.\n\nIn 2009 it was announced that the company would be creating a new Continental-level cycling team called Team Raleigh. The Team were co-sponsored by the global shipping and logistics firm GAC in 2012 and were known as Team Raleigh-GAC. The season was notable for Team Raleigh's first victory in the Tour Series Round 6 and a succession of Premier Calendar wins, which resulted in team rider Graham Briggs finishing the season at the top of British Cycling's UK Elite Men's standings. Raleigh once again became the sole headline sponsor of the team in 2013 and the team re-paid the investment with high-profile wins in the Tour de Normandie, Tour of the Reservoir and Tour Series Rounds 1 and 2.\n\nThe Raleigh archives, including the Sturmey-Archer papers, are at Nottinghamshire Record Office.\n\n\n\"Saturday Night and Sunday Morning\", the 1958 debut novel by Alan Sillitoe, is partly set in Raleigh's Nottingham factory, Sillitoe himself being an ex-employee of the firm. Several scenes for the 1960 film adaptation starring Albert Finney were filmed on location at the factory itself. In the 1985 movie \"American Flyers\", David Sommers played by David Marshall Grant, is seen riding through St. Louis, Missouri, on a Raleigh bicycle from that same era. Later in the film, specialized bicycles are used for the race scenes in Colorado and training. In the 1986 bike messenger film \"Quicksilver\" a variety of Raleigh USA bicycles are used. 1984–85 road bikes are used throughout by notable players in the movie. Kevin Bacon's bicycle is a singlespeed '84 Raleigh Competition. While no differentiation is made in the film, at least three different configurations are seen on Bacon's bike during the movie: fixed-gear, singlespeed, and outfitted with 0-degree trick forks during various scenes in Bacon's apartment. A possible freewheel is suggested early in the film when Bacon dismounts while in motion and a distinct clicking sound is heard until the bike stops moving. A 1984/5 Raleigh Grand Prix is used for the opening chase sequence, and a 1984 or '85 Super Course makes a brief appearance in the opening credits.\n\n\nA much expanded version of the text of this book, with full academic referencing, is held by the National Cycle Archive at Warwick University for the benefit of serious researchers.\n\n"}
{"id": "59184147", "url": "https://en.wikipedia.org/wiki?curid=59184147", "title": "Residue-to-product ratio", "text": "Residue-to-product ratio\n\nIn climate engineering, the residue-to-product ratio (RPR) is used to calculate how much unused crop residue might be left after harvesting a particular crop. Also called the residue yield or straw/grain ratio, the equation takes the mass of residue divided by the mass of crop produced, and the result is dimensionless. \n\nThe RPR can be used to project costs and benefits of bio-energy projects, and is crucial in determining financial sustainability. The RPR is particularly important for estimating the production of biochar, a beneficial farm input obtained from crop residues through pyrolysis. However, it is important to note that RPR values are rough estimates taken from broad production statistics, and can vary greatly depending on crop variety, climate, processing, and residual moisture content. \n\n"}
{"id": "27158615", "url": "https://en.wikipedia.org/wiki?curid=27158615", "title": "Ring oiler", "text": "Ring oiler\n\nA ring oiler or oil ring is a form of oil-lubrication system for bearings.\n\nRing oilers were used for medium-speed applications with moderate loads, during the first half of the 20th century. These represented the later years of the stationary steam engine, and the beginnings of the high-speed steam engine, the internal combustion oil engine and electrical generating equipment. Before this time plain bearings were lubricated by drip-feed oil cups or manually by an engine tender with an oil can. As speeds or bearing loads later increased, forced pressure lubrication became more prevalent and the ring oiler fell from use.\n\nA ring oiler is a simple device, consisting of a large metal ring placed around a horizontal shaft, adjacent to a bearing. An oil sump is underneath this shaft and the ring is large enough to dip into the oil. As the shaft rotates, the ring is carried round with it. The rotating ring in turn picks up some oil and deposits it onto the shaft, from where it flows sideways and lubricates the bearings. The oil ring is effectively a simple lubrication pump, with only one moving part and no complex or high-precision components. The device is crude, but automatic, effective and reliable. Unlike a drip oiler, there is also no need to close off the oiler or remove oil wicks when the machine is stopped.\n\nRing oilers were used for speeds up to around 1,000 rpm. Above this, the oil tended to be thrown centrifugally from the ring, rather than carried by it (although it is still currently applied on steam turbines with speeds around 3200 rpm). The bearing must also remain horizontal and stable, so although suitable for crankshaft main bearings, they could not be used on connecting rod big end bearings. They were not used on vehicles for similar reasons, although the engines concerned at this time were anyway too large and heavy for practical mobile use. Automatic ring oilers were particularly useful for large engines with multiple horizontally opposed cylinders, where it was otherwise difficult to access the central main bearings. Ring oilers were most suited where bearing side-loads were relatively light, but the bearing capacity required more lubrication than could be supplied by a drip feed oiler. For this reason they were widely used on larger electric motors and generators.\n"}
{"id": "6785464", "url": "https://en.wikipedia.org/wiki?curid=6785464", "title": "Small wind turbine", "text": "Small wind turbine\n\nA small wind turbine is a wind turbine used for microgeneration, as opposed to large commercial wind turbines, such as those found in wind farms, with greater individual power output. The Canadian Wind Energy Association (CanWEA) defines \"small wind\" as ranging from less than 1000 Watt (1 kW) turbines up to 300 kW turbines. The smaller turbines may be as small as a 50 Watt auxiliary power generator for a boat, caravan, or miniature refrigeration unit. The IEC-61400-2:2006 Standard defines small wind turbines as wind turbines with a rotor swept area smaller than 200 m2, generating at a voltage below 1000 Va.c. or 1500 Vd.c.\n\nSmaller scale turbines for residential scale use are available. Their blades are usually in diameter and produce 1-10 kW of electricity at their optimal wind speed. Some units have been designed to be very lightweight in their construction, e.g. 16 kilograms (35 lb), allowing sensitivity to minor wind movements and a rapid response to wind gusts typically found in urban settings and easy mounting much like a television antenna. It is claimed, and a few are certified, as being inaudible even a few feet (about a metre) under the turbine.\n\nThe majority of small wind turbines are traditional horizontal axis wind turbines, but vertical axis wind turbines are a growing type of wind turbine in the small-wind market. Makers of vertical axis wind turbines such as WePower, Urban Green Energy, Helix Wind, and Windspire Energy, have reported increasing sales over the previous years.\n\nThe generators for small wind turbines usually are three-phase alternating current generators and the trend is to use the induction type. They are options for direct current output for battery charging and power inverters to convert the power back to AC but at constant frequency for grid connectivity. Some models utilize single-phase generators.\n\nSome small wind turbines can be designed to work at low wind speeds, but in general small wind turbines require a minimum wind speed of .\n\nDynamic braking regulates the speed by dumping excess energy, so that the turbine continues to produce electricity even in high winds. The dynamic braking resistor may be installed inside the building to provide heat (during high winds when more heat is lost by the building, while more heat is also produced by the braking resistor). The location makes low voltage (around 12 volt) distribution practical.\n\nSmall units often have direct drive generators, direct current output, lifetime bearings and use a vane to point into the wind. Larger, more costly turbines generally have geared power trains, alternating current output and are actively pointed into the wind. Direct drive generators are also used on some large wind turbines.\n\nTurbines are often mounted on a tower to raise them above any nearby obstacles. One rule of thumb is that turbines should be at least higher than anything within . Better locations for wind turbines are far away from large upwind obstacles. Measurements made in a boundary layer wind tunnel have indicated that significant detrimental effects associated with nearby obstacles can extend up to 80 times the obstacle's height downwind. However, this is an extreme case. Another approach to siting a small turbine is to use a shelter model to predict how nearby obstacles will affect local wind conditions. Models of this type are general and can be applied to any site. They are often developed based on actual wind measurements, and can estimate flow properties such as mean wind speed and turbulence levels at a potential turbine location, taking into account the size, shape, and distance to any nearby obstacles.\n\nA small wind turbine can be installed on a roof. Installation issues then include the strength of the roof, vibration, and the turbulence caused by the roof ledge. Small-scale rooftop turbines suffer from turbulence and rarely generate significant amounts of power, especially in towns and cities.\n\nIn July 2012, a new feed-in tariff approved by Japanese Industry Minister Yukio Edano went into effect, promising to boost the country's production of wind and solar energy production. The country is aiming to increase renewable energy investment in part as a response to the Fukushima radiation crisis in March 2011. The feed-in tariff applies to solar panels and small wind turbines and requires utilities to buy back electricity generated from renewable energy sources at government-established rates.\n\nSmall-scale wind power (turbines of less than 20 kW capacity) will be subsidized at least 57.75 JPY (about 0.74 USD per kwh).\n\nProperties in rural or suburban parts of the UK can opt for a wind turbine with inverter to supplement local grid power. The UK's Microgeneration Certification Scheme (MCS) provides feed-in tariffs to owners of qualified small wind turbines.\n\nSmall wind turbines added a total of 17.3 MW of generating capacity throughout the United States in 2008, according to the American Wind Energy Association (AWEA). That growth equaled a 78% increase in the domestic market for small wind turbines, which are defined as wind turbines with capacities of 100 kW or less. AWEA's \"2009 Small Wind Global Market Study\", published in late 2009 May, credited the increase in part to greater manufacturing volumes, as the industry was able to attract enough private investment to finance manufacturing plant expansions. It also credited rising electricity prices and greater public awareness of wind technologies for an increase in residential sale. But a poll of small wind manufacturers found that the growth in 2008 might be only a glimmer of things to come, as the companies projected a 30-fold growth in the U.S. small wind market within as little as five years, despite the global recession. The U.S. small wind industry also benefits from the global market, as it controls about half of the global market share. U.S. manufacturers garnered $77 million of the $156 million that was spent throughout the world on small wind turbine installations. A total of 38.7 MW of small wind power capacity was installed globally in 2008.\n\nIn the United States, residential wind turbines with outputs of 2–10 kW, typically cost between and installed ( per watt), although there are incentives and rebates available in 19 states that can reduce the purchase price for homeowners by up to 50 percent, to $3 per watt. The US manufacturer Southwest Windpower estimates a turbine to pay for itself in energy savings in 5 to 12 years.\n\nThe dominant models on the market, especially in the United States, are horizontal-axis wind turbines.\n\nTo enable consumers to make an informed decision when purchasing a small wind turbine, a method for consumer labeling has been developed by IEA Wind Task 27 in collaboration with IEC TC88 MT2. In 2011 IEA Wind published a Recommended Practice, which describes the tests and procedures required to apply the label.\n\nCroatia is an ideal market for small wind turbines due to Mediterranean climate and numerous islands with no access to the electric grid. In winter months when there is less sun, but more wind, small wind turbines are a great addition to isolated renewable energy sites (GSM, stations, marinas etc.). That way solar and wind power provide consistent energy throughout the year.\n\nIn Germany the feed-in tariff for small wind turbines has always been the same as for large turbines. This is the main reason the small wind turbine sector in Germany developed slowly. In contrast, small photovoltaic systems in Germany benefited from a high feed-in tariff, at times above 50 Euro-Cent per kilowatt hour.\n\nIn August 2014 the German renewable energy law was adjusted, also affecting the feed-in tariffs for wind turbines. For the operation of a small wind turbine with a capacity below 50 kilowatt the tariff amounts to 8.5 Euro-Cent for a period of 20 years.\n\nDue to the low feed-in tariff and high electricity prices in Germany, the economic operation of a small wind turbine depends on a large self-consumption rate of the electricity produced by the small wind turbine. Private households pay on average 28 cent per kilowatt hour for electricity (19% VAT included).\n\nAs part of the German renewable energy law 2014 a fee on self-consumed electricity was introduced in August 2014. The regulation does not apply to small power plants with a capacity below 10 kilowatt. With an amount of 1.87 Euro-Cents the fee is low.\n\nSome hobbyists have built wind turbines from kits, sourced components, or from scratch. DIY wind turbines are usually smaller (rooftop) turbines of approximately 1 kW or less. These small wind turbines are usually tilt-up or fixed / guyed towers.\n\nDo it yourself or DIY-wind turbine construction has been made popular by magazines such as OtherPower and Home Power.\n\nOrganizations as Practical Action have designed DIY wind turbines that can be easily built by communities in developing nations and are supplying concrete documents on how to do so.\n\n\n"}
{"id": "1028435", "url": "https://en.wikipedia.org/wiki?curid=1028435", "title": "Smart Tag", "text": "Smart Tag\n\nSmart Tag is the former name of a transponder-based electronic toll collection system implemented by the Virginia Department of Transportation (VDOT). It was launched as Fastoll on April 15, 1996. Fastoll was rebranded as Smart Tag in 1998, and was placed under the umbrella of Smart Travel. In November 2007, the Smart Tag brand name was retired in favor of E-ZPass Virginia, several years after the Smart Tag system became a part of the E-ZPass network.\n\nOriginally, Smart Tag only operated at certain toll roads and crossings in Virginia. The system became interoperable with the E-ZPass toll collection system on October 27, 2004, although Richmond Metropolitan Authority owned toll roads—Boulevard Bridge, the Downtown Expressway, and the Powhite Parkway (excluding the extension)—did not begin accepting E-ZPass until August 3, 2005; E-ZPass integration had been delayed due to damages from Tropical Storm Gaston. Smart Tag branded transponders operate throughout the E-ZPass network, and E-ZPass branded transponders operate at all E-ZPass Virginia (formerly Smart Tag) toll collection points.\n\nRoads and crossings that accept Smart Tag/E-ZPass Virginia/E-ZPass:\n\n\n"}
{"id": "6715646", "url": "https://en.wikipedia.org/wiki?curid=6715646", "title": "Smithfield Foods", "text": "Smithfield Foods\n\nSmithfield Foods, Inc., is a meat-processing company and wholly owned subsidiary of WH Group of China. Founded in 1936 as the Smithfield Packing Company in Smithfield, Virginia, by Joseph W. Luter and his son, the company is the largest pig and pork producer in the world. In addition to owning over 500 farms in the United States, Smithfield contracts with another 2,000 independent farms around the country to grow Smithfield's pigs. Outside the U.S., the company has facilities in Mexico, Poland, Romania, Germany and the UK. Globally the company employed 50,200 in 2016 and reported an annual revenue of $14 billion. Its 973,000-square-foot meat-processing plant in Tar Heel, North Carolina, was reported in 2000 to be the world's largest, processing 32,000 pigs a day.\n\nThen known as Shuanghui Group, WH Group purchased Smithfield Foods in 2013 for $4.72 billion, more than its market value. It was the largest Chinese acquisition of an American company to date. The acquisition of Smithfield's 146,000 acres of land made WH Group, headquartered in Luohe, Henan province, one of the largest overseas leasers of American farmland.\n\nSmithfield Foods began its growth in 1981 with the purchase of Gwaltney of Smithfield, followed by the acquisition of nearly 40 companies between then and 2008, including Eckrich; Farmland Foods of Kansas; John Morrell; Murphy Family Farms of North Carolina; Circle Four Farms of Utah; and Premium Standard Farms. The company was able to grow as a result of its highly industrialized pig production, confining thousands of pigs in large barns known as concentrated animal feeding operations, and controlling the animals' development from conception to packing.\n\nAs of 2006 Smithfield raised 15 million pigs a year and processed 27 million, producing over six billion pounds of pork and, in 2012, 4.7 billion gallons of manure. Killing 114,300 pigs a day, it was the top pig-slaughter operation in the United States in 2007; along with three other companies, it also slaughtered 56 percent of the cattle processed there until it sold its beef group in 2008. The company sells its products under several brand names, including Cook's, Eckrich, Gwaltney, John Morrell, Krakus, and Smithfield. Kenneth M. Sullivan became the president and chief executive officer in 2015.\n\nThe company traces its history to 1936, when Joseph W. Luter Sr. and his son, Joseph W. Luter Jr., opened the Smithfield Packing Company in Smithfield, Virginia. The men were working for P. D. Gwaltney when they set up the company; Joseph W. Luter Sr. was a salesman and Joseph W. Luter Jr. the general manager. Financing for the new company came from Peter Pruden of Suffolk and John S. Martin of Richmond. In an interview in 2009, Joseph W. Luter III described how the Luters would buy 15 hog carcasses a day, cut them up, box them, and sell them to small stores in Newport News and Norfolk. They built the Smithfield Packing Company plant in 1946 on Highway 10.\n\nJoseph W. Luter Jr. served as Smithfield's chief executive officer (CEO) until his death in 1962. He owned 42 percent of the company when he died. His son, Joseph W. Luter III, was at Wake Forest University at the time and joined Smithfield that year. Working in sales, he borrowed enough to buy a further eight-and-a-half percent of the shares, and in 1966 he became chairman and CEO. He told \"Virginia Living\" that when he took over Smithfield, the company was killing around 3,000 hogs a day, and when he left in January 1970, the figure was 5,000; the number of employees had risen from 800 to 1,400. In July 1969 he sold the company to Liberty Equities for $20 million; they had asked him to stay on, but in January 1970 they fired him. From then until 1975 he developed a ski resort, Bryce Mountain, in Virginia.\n\nAt the recommendation of the banks, Smithfield hired Luter as CEO again in April 1975, when it found itself in financial difficulties. At the time, according to Luter, the company had a net worth of under $1 million, debt of $17 million, and losses of $2 million a year. He said it even lost money in December 1974, which, considering it was holiday-ham season, \"was like Budweiser losing money in July\". Luter's restructuring of the company is credited with its improved performance. His son, Joseph W. Luter IV, became an executive vice-president of Smithfield Foods in 2008 and president of the Smithfield Packing Company, by then the parent company's largest subsidiary. Joseph W. Luter III remained as CEO until 2006 and was chairman until the company was sold to WH Group in 2013. Joseph W. Luter IV resigned in October 2013. His stock was valued at $21.1 million and Joseph W. Luter III's at $30 million.\n\nJoseph W. Luter III began his expansion of Smithfield in 1981 with the purchase of its main competitor, Gwaltney of Smithfield, for $42 million. This was followed by the acquisition of almost 40 companies in the pork, beef, and livestock industries between 1981 and around 2008, including Esskay Meats/Schluderberg-Kurdle in Baltimore, Valley Dale in Roanoke, and Patrick Cudahy in Milwaukee in 1984. In 1992 Smithfield opened the world's largest processing plant, a 973,000-square-foot facility in Tar Heel, North Carolina, which by 2000 could process 32,000 pigs a day.\n\nSmithfield purchased John Morrell & Co in Sioux Falls, SD, in 1995 and Circle Four Farms in 1998. In 1999 it bought two of the largest pig producers in the United States: Carroll's Foods and Murphy Family Farms of North Carolina, at that point the largest producer. According to agricultural researchers Jill Hobbs and Linda Young, Smithfield's purchase of these companies constituted a \"major structural change\" in the hog industry in the United States, leaving Smithfield in control of 10–15 percent of the country's hog production.\n\nFarmland Foods of Kansas City was added in 2003, as were Sara Lee's European Meats, ConAgra Foods Refrigerated Meats, Butterball (the poultry producer), and Premium Standard Farms in 2007. Smithfield sold its 49 percent share in Butterball in 2008 for an estimated $175 million. The acquisitions caused concern among regulators in the United States regarding the company's control of the food supply. After Smithfield's purchase of Murphy Family Farms, the Agriculture Department described it as \"absurdly big\". As of 2006 four companies—Smithfield, Tyson Foods, Swift & Company, and Cargill—were responsible for the production of 70 percent of pork in the United States.\n\nOn May 29, 2013, WH Group Ltd., then known as Shuanghui Group or Shineway Group, the largest meat producer in China, announced the purchase of Smithfield Foods for $4.72 billion. Shuanghui announced that it would list Smithfield on the Hong Kong Stock Exchange after completing the takeover. On September 6, 2013, the U.S. government approved Shuanghui International Holding’s purchase of Smithfield Food, Inc. The deal was valued at approximately $7.1 billion, which included debt. It was the largest stock acquisition by a Chinese company of an American company. At the time of the deal, China was one of the U.S's largest pork importers, although it had 475 million pigs of its own, roughly 60 percent of the global total. Smithfield's CEO, Ken Sullivan, said in 2017 that he sees the company's future as a \"consumer-packaged goods business\".\n\nFor decades Smithfield had run its acquisitions as independent operating companies, but in 2015, after the purchase by WH Group, it set up the \"One Smithfield\" initiative to unify them. Circle Four Farms in Milford, Utah, for example, became Smithfield Hog Production-Rocky Mountain Region. In 2016 Smithfield purchased the Californian pork processor Clougherty Packing PLC for $145 million, along with its Farmer John and Saag's Specialty Meats brands. Smithfield also acquired PFFJ (Pigs for Farmer John) LLC and three of its farms from Hormel Foods Corporation. In September 2017 it announced that it would purchase two Romanian packaged-meat suppliers, Elit and Vericom.\n\nIn 2016 Smithfield had 50,200 employees in the United States, Mexico and Europe, and an annual revenue of $14 billion. As of July 2017, the company's brands included Armour, Berlinki, Carando, Cook's, Curly's, Eckrich, Farmland, Gwaltney, Healthy Ones, John Morrell, Krakus, Kretschmar, Margherita, Morliny, Nathan's Famous, and Smithfield. In 2012 it opened a restaurant, Taste of Smithfield, in Smithfield, Virginia, located in the same Main Street building as its retail store, The Genuine Smithfield Ham Shoppe.\n\nAmongst those involved in the meat processing and products industry, Smithfield has so far made the largest contribution towards lobbying the US government during 2018..\n\nIn 1990 Smithfield began buying hog-farming operations, making it a vertically integrated company. As a result, it was able to expand by over 1,000 percent between 1990 and 2005. Vertical integration allows Smithfield to control every stage of pig production, from conception and birth, to slaughter, processing and packing, a system known as \"from squeal to meal\" or \"from birth to bacon\".\n\nThe company contracted farmers who had moved out of tobacco farming, and sent them piglets between eight and ten weeks old to be brought to market weights on diets controlled by Smithfield. Smithfield retained ownership of the pigs. Only farmers able to handle thousands of pigs were contracted, which meant that smaller farms went out of business. In North Carolina, Smithfield's expansion mirrored hog farmers' decline; there were 667,000 hog farms there in 1980 and 67,000 in 2005. When the U.S. government placed restrictions on the company, it moved into Eastern Europe. As a result, in Romania there were 477,030 hog farms in 2003 and 52,100 in 2007. There was a similar decline, by 56 percent between 1996 and 2008, in Poland.\n\nJoseph W. Luter III said that vertical integration produces \"high quality, consistent products with consistent genetics\". The company obtained 2,000 pigs and the rights to their genetic lines from Britain's National Pig Development Company in 1990, and used them to create Smithfield Lean Generation Pork, which the American Heart Association certified for its low fat, salt, and cholesterol content. According to Luter, it was vertical integration that enabled this.\n\nThe pigs are housed together in their thousands in identical barns with metal roofs, known as concentrated animal feeding operations (CAFOs). The floors of the buildings are slatted, allowing waste to be flushed into 30-feet-deep \"open-air pits the size of two football fields\", according to the \"Washington Post\". These are referred to within the industry as anaerobic lagoons. They dispose of effluent at a low cost, but they require large areas and release odors and methane, a major contributor to greenhouse gas.\n\nSmithfield Foods states that the lagoons contain an impervious liner made to withstand leakage. According to Jeff Tietz in \"Rolling Stone\", the waste—a mixture of excrement, urine, blood, afterbirths, stillborn pigs, drugs and other chemicals—overflows when it rains, and the liners can be punctured by rocks. Smithfield attributes the pink color of the waste to the health of the lagoons, and states that the color is \"a sign of bacteria doing what it should be doing. It's indicative of lower odor and lower nutrient content.\"\n\nSmithfield said in 2007 that it would phase out its use of gestation crates by 2017. Pregnant sows spend most of their lives in these stalls, which are too small to allow them to turn around. Pregnancies last about 115 days; the average life span of a sow in the United States is 4.2 litters. When they give birth, they are moved to a farrowing crate for three weeks, then artificially inseminated again and moved back to a gestation crate. The practice has been criticized by animal-welfare groups, supermarket chains and McDonald's. Smithfield did not commit to requiring its contract farms to phase out the crates. Almost half the company's sows in the United States live on its c. 2,000 contract farms.\n\nIn 2009 Smithfield said it would not meet the deadline because of the recession, but in 2011 it returned to its commitment, and to doing the same in Europe and Mexico by 2022. In January 2015 it said that 71.3 percent of pregnant sows on company-owned farms had been moved into a group-housing system. In January 2017 the company announced that 87 percent of sows on company-owned farms were no longer in crates, and that it would require its contract farms to phase out crates by 2022. As of January 2018, on company-owned farms in the United States, Smithfield confines pregnant sows in gestation crates for six weeks during the impregnation process. When pregnancy is confirmed, they are moved to pens within a group-housing system for about 10 weeks, then to a farrowing crate, then back to a gestation crate to be impregnated again. The company said it is recommending that its contract farms in the United States move to group housing by 2022. It uses two forms of group housing: in one system, 30–40 sows are kept in a pen with access to the individual gestation crates; in the other system, five or six sows are housed together in a pen. In July 2017 Direct Action Everywhere filmed the gestation crates at Smithfield's Circle Four Farms in Milford, Utah. The FBI subsequently raided two animal sanctuaries searching for two piglets removed by the activists. In January 2018 Smithfield released a video of the gestation and farrowing areas on one of its farms.\n\nSmithfield has come under criticism for the millions of gallons of untreated fecal matter it produces and stores in the lagoons. In 2012 it produced at least 4.7 billion gallons of manure in the United States; during their lifetimes, every pig will produce 1,100–1,300 liters. In a four-year period in North Carolina in the 1990s, 4.7 million gallons of hog fecal matter were released into the state's rivers. Workers and residents near Smithfield plants reported health problems and complained about the stench. The company was fined $12.6 million in 1997 by the Environmental Protection Agency (EPA) for 6,900 violations of the Clean Water Act after discharging illegal levels of slaughterhouse waste into the Pagan River in Virginia. Its facilities in North Carolina came under scrutiny in 1999 when Hurricane Floyd flooded lagoons holding fecal matter; many of Smithfield's contract farms were accused of polluting the rivers. Smithfield reached a settlement in 2000 with the state of North Carolina, agreeing to pay the state $50 million over 25 years.\n\nAccording to Ralph Deptolla of Smithfield Foods, the company created new executive positions to monitor the environmental issues. In 2001 it created an environmental management system and the following year hired Dennis Treacy, director of the Virginia Department of Environmental Quality since 1998, as executive vice president and chief sustainability officer. Treacy had previously been involved in the enforcement efforts against Smithfield. In 2005 the company received ISO 14001 certification for its hog production and processing facilities in the U.S., with the exception of new acquisitions, and, in 2009, 14 plants in the U.S. and 21 in Romania received certification. By 2011, 578 Smithfield facilities were ISO 14001-certified. In 2006 its subsidiary Murphy-Brown reached an agreement with the Waterkeeper Alliance, once one of Smithfield's biggest critics, to enhance environmental protection at the former's facilities in North Carolina. In 2009 the company said it had reduced its emissions since 2007, including its greenhouse-gas emissions by four percent; it attributed this to the divestiture of the beef group. In 2010 it released its ninth annual Corporate Social Responsibility report, and announced the creation of two sustainability committees.\n\nThe earliest confirmed case of the H1N1 virus (swine flu) during the 2009 flu pandemic was in a five-year-old boy in La Gloria, Mexico, near several facilities operated by Granjas Carroll de Mexico, a Smithfield Foods subsidiary that processes 1.2 million pigs a year and employs 907 people. This, together with tension between the company and local community over Smithfield's environmental record, prompted several newspapers to link the outbreak to Smithfield's farming practices. According to \"The Washington Post\", over 600 other residents of La Gloria became ill from a respiratory disease in March that year (later thought to be seasonal flu). The \"Post\" writes that health officials found no link between the farms and the H1N1 outbreak. Smithfield said that it had found no clinical signs of swine flu in its pigs or employees in Mexico, and had no reason to believe that the outbreak was connected to its Mexican facilities. The company said it routinely administers flu virus vaccine to its swine herds in Mexico and conducts monthly tests to detect the virus.\n\nResidents alleged that the company regularly violates local environmental regulations. According to the \"Washington Post\", local farmers had complained for years about headaches from the smell of the pig farms and said that wild dogs had been eating discarded pig carcasses. Smithfield was using biodigesters to convert dead pigs into renewable energy, but residents alleged that they regularly overflowed. Residents also feared that the waste stored in the lagoons would leak into the groundwater.\n\nIn 2009 Armour-Eckrich introduced smaller crescent-style packaging for its smoked sausages, which reduced the plastic film and corrugated cardboard the company used by over 840,000 pounds per year. In 2010 the John Morrell plant in Sioux Falls, SD, reduced its use of plastic by 40,600 pounds a year, and Farmland Foods reduced the corrugated packaging entering waste streams by over five million pounds a year. Smithfield Packing used 17 percent less plastic for deli meat. The company also eliminated 20,000 pounds of corrugated material a year by using smaller boxes to transport chicken frankfurters to its largest customer.\n\nConcerns have been raised about Smithfield's use of low doses of antibiotics to promote the pigs' growth, in addition to using antibiotics as part of a treatment regime. The concern was that the antibiotics were harmful to the animals and were contributing to the rise of antibiotic-resistant strains of bacteria. Smithfield said in 2005 that it would administer antibiotics only to animals who were sick themselves, or who were in close proximity to sick animals; however, in CAFOs all pigs are in close proximity to each other. The company introduced an antibiotic-free Pure Farms brand in 2017; it promoted the brand as free of antibiotics, artificial ingredients, hormones, and steroids.\n\nIn Poland, Smithfield Foods purchased former state farms for what its CEO said were \"small dollars\" and turned them into CAFOs using grants from the European Bank for Reconstruction and Development. Compassion in World Farming (CIWF) conducted an undercover investigation into Smithfield CAFOs there in 2006, and found sick and injured animals in the barns, and dead animals rotting. The CAFOs were run by Animex, a Smithfield subsidiary. In one barn, 26 pigs were reported to have died in a five-week period. The CIWF report said of a Smithfield lagoon in Boszkowo: \"Everywhere is the detritus of industrial factory farming—plastic syringe casings, intravenous needles and white clinical gloves—floating in the rancid cesspit and discarded on adjacent farmland.\"\n\nIn December 2010 the Humane Society of the United States (HSUS) released an undercover video taken by one of its investigators inside a Smithfield Foods facility. The investigator had worked for a month at Murphy-Brown, a Smithfield subsidiary in Waverly, Virginia. The Associated Press (AP) reported that the investigator videotaped 1,000 sows living in gestation crates. According to the AP, the material shows a pig being pulled by the snout, shot in the head with a stun gun, and thrown into a bin while trying to wriggle free. The investigator said he saw sows biting their crates and bleeding; staff jabbing them to make them move; staff tossing piglets into carts; and piglets born prematurely in gestation crates falling through the slats into the manure pits. The video won a 2012 Webby Award in the \"Public Service and Activism\" category.\n\nIn response, Smithfield told the AP that it has \"zero tolerance for any behavior that does not conform to our established animal well-being procedures\". The company asked Temple Grandin, a professor of animal husbandry, to review the footage; she recommended an inspection by animal welfare expert Jennifer Woods. Smithfield announced on December 21 that it had fired two workers and their supervisor. At the company's invitation, the Virginia state veterinarian Richard Wilkes visited the facility on December 22. He told \"The Virginian-Pilot\" that Smithfield had been \"very responsive and very responsible in how they've addressed the issues\", and that he had not seen \"any indication of abuse\" of the pigs and was impressed by their demeanor. A Humane Society spokesman said that Smithfield had provided the vet \"with a pre-announced, white glove tour\".\n\nIn 2010 a jury in Jackson County, Missouri, awarded 13 plaintiffs $825,000 each against a Smithfield subsidiary, Premium Standard. Two other plaintiffs were awarded $250,000 and $75,000. The plaintiffs argued that they were unable to enjoy their property because of the smell coming from the Smithfield facilities.\n\nIn 2017 in Wake County, North Carolina, nearly 500 residents sued a Smithfield subsidiary, Murphy-Brown, in 26 lawsuits, alleging nuisance and ill health caused by smells, open-air lagoons, and pig carcasses. Residents said their outdoor activities were limited as a consequence, and that they were unable to invite visitors to their homes. Smithfield said the complaints were without merit. On August 3, 2018, a federal jury awarded six North Carolina residents $470 million in damages against Murphy-Brown LLC. The verdict included $75 million each in punitive damages, plus $3–$5 million in compensatory damages for loss of enjoyment in their properties. A recently enacted state law capping punitive damages will lower that amount to $94 million. The plaintiffs had filed suit for \"stench odor, truck noise and flies generated near their homes on Kinlaw Farm in Bladen County.\"\n\nState representatives of agriculture in North Carolina accused lawyers and their plaintiffs of attempting to put farmers out of business. Steve Troxler, North Carolina's agricultural commissioner, said the litigation could harm farm production across the country. Troxler said, “One big problem is the use of the term ‘nuisance.’ I’d say just about anything could be a nuisance to someone at any point in time.\" Troxler said legal abuse of the word nuisance is a mounting concern.\n\nHuman Rights Watch (HRW) issued a 175-page report in 2005 documenting what it said were unsafe work conditions in the U.S. meat and poultry industry, citing working conditions at Smithfield Foods as an example. In particular, the report said, workers make thousands of repetitive motions with knives during each shift, leading to lacerations and repetitive strain injuries. It also alleged that the workers' immigrant status may be exploited to prevent them from making complaints or forming unions. According to the report, the speed at which the pigs are killed and processed makes the job inherently dangerous for workers. A Smithfield manager testified in 1998, during an unfair labor practices trial, that at the Tar Heel plant in North Carolina it takes 5–10 minutes to slaughter and complete the process of \"disassembly\" of an animal, including draining, cleaning, and cleaving. One worker told HRW that the disassembly line moves so fast that there is no time to sharpen the knives, which means harder cuts have to be made, with the resultant injuries to workers. Similar criticism was made by other groups about Smithfield facilities in Poland and Romania.\n\nThe Smithfield Packing plant in Tar Heel, North Carolina, was the site of a 15-year dispute between the company and the United Food and Commercial Workers Union (UFCW), which had tried since the early 1990s to organize the plant's roughly 5,000 hourly workers. Workers voted against the union in 1994 and 1997, but the National Labor Relations Board (NLRB) alleged that unfair election conduct had occurred and ordered a new election. During the 1997 election the company is alleged to have fired workers who supported the union, stationed police at the plant gates, and threatened plant closures. In 2000, according to Human Rights Watch, Smithfield set up its own security force, with \"special police agency\" status under North Carolina law, and in 2003 arrested workers who supported the union.\n\nSmithfield appealed the NLRB's ruling that the 1997 election was invalid, and, in 2006, the U.S. Circuit Court of Appeals found in favor of the NLRB. After demonstrations, lockouts, and a shareholder meeting that was disrupted by shareholders supporting the union, the union called for a boycott of Smithfield products. In 2007 Smithfield countered by filing a federal RICO Act lawsuit against the union. The following year Smithfield and the union reached an agreement, under which the union agreed to suspend its boycott in return for the company dropping its RICO lawsuit and allowing another election. In December 2008, workers voted 2,041 to 1,879 in favor of joining the union.\n\nIn 2009 Smithfield was assessed a $900,000 penalty by the U.S. Justice Department to settle charges that the company had engaged in illegal merger activity during its takeover of Premium Standard Farms LLC in 2006.\n\nSmithfield is a supplier of heparin, which is extracted from pigs' intestines and used as a blood thinner, to the pharmaceutical industry. In 2017 the company opened a bioscience unit and joined a tissue engineering group funded by the United States Department of Defense to the tune of $80 million. According to Reuters, the group included Abbott Laboratories, Medtronic and United Therapeutics.\n\nThe Smithfield Foundation, established in 2002, is a non-profit organization that acts as the philanthropic wing of Smithfield Foods, dedicated primarily to providing scholarships to the children and grandchildren of Smithfield employees. The foundation gave $5 million to Christopher Newport University in Newport News, Virginia, to establish the Luter School of Business, and in 2006 gave $5 million to the University of Virginia Cancer Center in Charlottesville, Virginia. It has also supported its \"learners to leaders\" programs, begun in 2006, in Sioux Falls, South Dakota; Green Bay, Wisconsin; Denison, Iowa; and Norfolk, Virginia.\n\nIn 2012 Smithfield announced a 15-race sponsorship with Richard Petty Motorsports (RPM) and driver Aric Almirola driving the No. 43 Ford Fusion in the NASCAR Sprint Cup Series. The sponsorship was increased to 30 races beginning in 2014. Smithfield rotates its brands on the car, featuring Smithfield, Eckrich, Farmland, Gwaltney, and Nathan's Famous. It is also the official food of Richmond International Raceway in Henrico County, Virginia. Smithfield and RPM parted ways in September 2017, allowing Smithfield to sponsor Stewart-Haas Racing in 2018.\n\n\nExternal links\n\nBooks\n\nArticles\n\n"}
{"id": "1536216", "url": "https://en.wikipedia.org/wiki?curid=1536216", "title": "Teletraffic engineering", "text": "Teletraffic engineering\n\nTelecommunications traffic engineering, teletraffic engineering, or traffic engineering is the application of traffic engineering theory to telecommunications. Teletraffic engineers use their knowledge of statistics including queuing theory, the nature of traffic, their practical models, their measurements and simulations to make predictions and to plan telecommunication networks such as a telephone network or the Internet. These tools and knowledge help provide reliable service at lower cost.\n\nThe field was created by the work of A. K. Erlang for circuit-switched networks but is applicable to packet-switched networks, as they both exhibit Markovian properties, and can hence be modeled by e.g. a Poisson arrival process.\n\nThe crucial observation in traffic engineering is that in large systems the law of large numbers can be used to make the aggregate properties of a system over a long period of time much more predictable than the behaviour of individual parts of the system.\n\nThe measurement of traffic in a public switched telephone network (PSTN) allows network operators to determine and maintain the quality of service (QoS) and in particular the grade of service (GoS) that they promise their subscribers. The performance of a network depends on whether all origin-destination pairs are receiving a satisfactory service.\n\nNetworks are handled as:\n\nCongestion is defined as the situation when exchanges or circuit groups are inundated with calls and are unable to serve all the subscribers. Special attention must be given to ensure that such high loss situations do not arise. To help determine the probability of congestion occurring, operators should use the Erlang formulas or the Engset calculation.\n\nExchanges in the PSTN make use of trunking concepts to help minimize the cost of the equipment to the operator. Modern switches generally have full availability and do not make use of grading concepts.\n\nOverflow systems make use of alternative routing circuit groups or paths to transfer excess traffic and thereby reduce the possibility of congestion.\n\nA very important component in PSTNs is the SS7 network used to route signalling traffic. As a supporting network, it carries all the signalling messages necessary to set up, break down or provide extra services. The signalling enables the PSTN to control the manner in which traffic is routed from one location to another.\n\nTransmission and switching of calls is performed using the principle of time-division multiplexing (TDM). TDM allows multiple calls to be transmitted along the same physical path, reducing the cost of infrastructure.\n\nA good example of the use of teletraffic theory in practice is in the design and management of a call center. Call centers use teletraffic theory to increase the efficiency of their services and overall profitability through calculating how many operators are really needed at each time of the day.\n\nQueueing systems used in call centers have been studied as a science. For example, completed calls are put on hold and queued until they can be served by an operator. If callers are made to wait too long, they may lose patience and default from the queue (hang up), resulting in no service being provided.\n\nTeletraffic Engineering is a well-understood discipline in the traditional voice network, where traffic patterns are established, growth rates can be predicted, and vast amounts of detailed historical data are available for analysis. However, in modern broadband networks, the teletraffic engineering methodologies used for voice networks are inappropriate.\n\nOf great importance is the possibility that extremely infrequent occurrences are more likely than anticipated. This situation is known as long-tail traffic. In some designs, the network might be required to withstand the unanticipated traffic.\n\nAs mentioned in the introduction, the purpose of teletraffic theory is to reduce cost in telecommunications networks. An important tool in achieving this goal is forecasting. Forecasting allows network operators to calculate the potential cost of a new network / service for a given QoS during the planning and design stage, thereby ensuring that costs are kept to a minimum.\n\nAn important method used in forecasting is simulation, which is described as the most common quantitative modelling technique in use today. An important reason for this is that computing power has become far more accessible, making simulation the preferred analytical method for problems that are not easily solved mathematically.\n\nAs in any business environment, network operators must charge tariffs for their services. These charges must be balanced with the supplied QoS. When operators supply services internationally, this is described as trade in services and is governed by the General Agreement on Trade in Services (GATS).\n\n\n"}
{"id": "17002524", "url": "https://en.wikipedia.org/wiki?curid=17002524", "title": "Temperature-responsive polymer", "text": "Temperature-responsive polymer\n\nTemperature-responsive polymers or thermoresponsive polymers are polymers that exhibit a drastic and discontinuous change of their physical properties with temperature. The term is commonly used when the property concerned is solubility in a given solvent, but it may also be used when other properties are affected. Thermoresponsive polymers belong to the class of stimuli-responsive materials, in contrast to temperature-sensitive (for short, thermosensitive) materials, which change their properties continuously with environmental conditions.\nIn a stricter sense, thermoresponsive polymers display a miscibility gap in their temperature-composition diagram. Depending on whether the miscibility gap is found at high or low temperatures, an upper or lower critical solution temperature exists, respectively (abbreviated UCST or LCST).\nResearch mainly focuses on polymers that show thermoresponsivity in aqueous solution. Promising areas of application are tissue engineering, liquid chromatography, drug delivery and bioseparation. Only a few commercial applications exist, for example, cell culture plates coated with an LCST-polymer.\n\nThe theory of thermoresponsive polymer (similarly, microgels) begins in the 1940's with work from Flory and Huggins who both independently produced similar theoretical expectations for polymer in solution with varying temperature. \n\nThe effects of external stimuli on particular polymers were investigated in the 1960s by Heskins and Guillet. They established 32 °C as the lower critical solution temperature (LCST) for poly(N-isopropyl arylamide).\n\nThermoresponsive polymer chains in solution adopt an expanded coil conformation. At the phase separation temperature they collapse to form compact globuli. This process can be observed directly by methods of static and dynamic light scattering. The drop in viscosity can be indirectly observed. When mechanisms which reduce surface tension are absent, the globules aggregate, subsequently causing turbidity and the formation of visible particles.\n\nThe phase separation temperature (and hence, the cloud point) is dependent on polymer concentration. Therefore, temperature-composition diagrams are used to display thermoresponsive behavior over a wide range of concentrations. Phases separate into a polymer-poor and a polymer-rich phase. In strictly binary mixtures the composition of the coexisting phases can be determined by drawing tie-lines. However, since polymers display a molar mass distribution this straightforward approach may be insufficient.\nDuring the process of phase separation the polymer-rich phase can vitrify before equilibrium is reached. This depends on the glass transition temperature for each individual composition. It is convenient to add the glass transition curve to the phase diagram, although it is no real equilibrium. The intersection of the glass transition curve with the cloud point curve is called Berghmans point. In the case of UCST polymers, above the Berghmans point the phases separate into two liquid phases, below this point into a liquid polymer-poor phase and a vitrified polymer-rich phase. For LCST polymers the inverse behavior is observed.\n\nPolymers dissolve in a solvent when the Gibbs energy of the system decreases, i.e., the change of Gibbs energy (ΔG) is negative. From the known Legendre transformation of the Gibbs–Helmholtz equation it follows that ΔG is determined by the enthalpy of mixing (ΔH) and entropy of mixing (ΔS).\n\nformula_1\n\nWithout interactions between the compounds there would be no enthalpy of mixing and the entropy of mixing would be ideal. The ideal entropy of mixing of multiple pure compounds is always positive (the term -T∙ΔS is negative) and ΔG would be negative for all compositions, causing complete miscibility. Therefore, the fact that miscibility gaps are observed can only be explained by interaction. In the case of polymer solutions, polymer-polymer, solvent-solvent and polymer-solvent interactions have to be taken into account. A model for the phenomenological description of polymer phase diagrams was developed by Flory and Huggins (see Flory–Huggins solution theory). The resulting equation for the change of Gibbs energy consists of a term for the entropy of mixing for polymers and an interaction parameter that describes the sum of all interactions.\n\nformula_2\n\nwhere\n\nA consequence of the Flory-Huggins theory is, for instance, that the UCST (if it exists) increases and shifts into the solvent-rich region when the molar mass of the polymer increases. Whether a polymer shows LCST and/or UCST behavior can be derived from the temperature-dependence of the interaction parameter (see figure). It has to be noted that the interaction parameter not only comprises enthalpic contributions but also the non-ideal entropy of mixing, which again consists of many individual contributions (e.g., the strong hydrophobic effect in aqueous solutions). For these reasons, classical Flory-Huggins theory cannot provide much insight into the molecular origin of miscibility gaps.\n\nThermoresponsive polymers can be functionalized with moieties that bind to specific biomolecules. The polymer-biomolecule conjugate can be precipitated from solution by a small change of temperature. Isolation may be achieved by filtration or centrifugation.\n\nFor some polymers it was demonstrated that thermoresponsive behavior can be transferred to surfaces. The surface is either coated with a polymer film or the polymer chains are bound covalently to the surface.\nThis provides a way to control the wetting properties of a surface by small temperature changes. The described behavior can be exploited in tissue engineering since the adhesion of cells is strongly dependent on the hydrophilicity/hydrophobicity. This way, it is possible to detach cells from a cell culture dish by only small changes in temperature, without the need to additionally use enzymes (see figure). Respective commercial products are already available.\n\nThermoresponsive polymers can be used as the stationary phase in liquid chromatography. Here, the polarity of the stationary phase can be varied by temperature changes, altering the power of separation without changing the column or solvent composition. Thermally related benefits of gas chromatography can now be applied to classes of compounds that are restricted to liquid chromatography due to their thermolability. In place of solvent gradient elution, thermoresponsive polymers allow the use of temperature gradients under purely aqueous isocratic conditions. The versatility of the system is controlled not only by changing temperature, but also by adding modifying moieties that allow for a choice of enhanced hydrophobic interaction, or by introducing the prospect of electrostatic interaction. These developments have already brought major improvements to the fields of hydrophobic interaction chromatography, size exclusion chromatography, ion exchange chromatography, and affinity chromatography separations, as well as pseudo-solid phase extractions (\"pseudo\" because of phase transitions).\n\nThree-dimensional covalently linked polymer networks are insoluble in all solvents, they merely swell in good solvents. Thermoresponsive polymer gels show a discontineous change of the degree of swelling with temperature. At the volume phase transition temperature (VPTT) the degree of swelling changes drastically. Researchers try to exploit this behavior for temperature-induced drug delivery. In the swollen state, previously incorporated drugs are released easily by diffusion. More sophisticated \"catch and release\" techniques have been elaborated in combination with lithography and molecular imprinting.\n\nIn physical gels unlike covalently linked gels the polymers chains are not covalently linked together. That means that the gel could re-dissolve in a good solvent under some conditions. Thermoresponsive physical gels, also sometimes called thermoresponsive injectable gels have been used in Tissue Engineering. This involves mixing at room temperature the thermoresponsive polymer in solution with the cells and then inject the solution to the body. Due to the temperature increase (to body temperature) the polymer creates a physical gel. Within this physical gel the cells are encapsulated. Tailoring the temperature that the polymer solution gels can be challenging because this depend by many factors like the polymer composition, architecture as well as the molar mass.\n\nExperimentally, the phase separation can be followed by turbidimetry. There is no universal approach for determining the cloud point suitable for all systems. It is often defined as the temperature at the onset of cloudiness, the temperature at the inflection point of the transmittance curve, or the temperature at a defined transmittance (e.g., 50%). The cloud point can be affected by many structural parameters of the polymer like the hydrophobic content, architecture and even the molar mass.\n\nThe cloud points upon cooling and heating of a thermoresponsive polymer solution do not coincide because the process of equilibration takes time. The temperature interval between the cloud points upon cooling and heating is called hysteresis. The cloud points are dependent on the cooling and heating rates, and hysteresis decreases with lower rates. There are indications that hysteresis is influenced by the temperature, viscosity, glass transition temperature and the ability to form additional intra- and inter-molecular hydrogen bonds in the phase separated state.\n\nAnother important property for potential applications is the extent of phase separation, represented by the difference in polymer content in the two phases after phase separation. For most applications, phase separation in pure polymer and pure solvent would be desirable although it is practically impossible. The extent of phase separation in a given temperature interval depends on the particular polymer-solvent phase diagram.\n\n\"Example\": From the phase diagram of polystyrene (molar mass 43,600 g/mol) in the solvent cyclohexane it follows that at a total polymer concentration of 10%, cooling from 25 to 20 °C causes phase separation into a polymer-poor phase with 1% polymer and a polymer-rich phase with 30% polymer content.\n\nAlso desirable for many applications is a sharp phase transition, which is reflected by a sudden drop in transmittance. The sharpness of the phase transition is related to the extent of phase separation but additionally relies on whether all present polymer chains exhibit the same cloud point. This depends on the polymer endgroups, dispersity, or—in the case of copolymers—varying copolymer compositions.\n\nDue to the low entropy of mixing, miscibility gaps are often observed for polymer solutions. Many polymers are known that show UCST or LCST behavior in organic solvents. Examples for organic polymer solutions with UCST are polystyrene in cyclohexane, polyethylene in diphenylether or polymethylmethacrylate in acetonitrile. An LCST is observed for, e.g., polypropylene in n-hexane, polystyrene in butylacetate or polymethylmethacrylate in 2-propanone.\n\nPolymer solutions that show thermoresponsivity in water are especially important since water as a solvent is cheap, safe and biologically relevant. Current research efforts focus on water-based applications like drug delivey systems, tissue engineering, bioseparation (see the section Applications). Numerous polymers with LCST in water are known. The most studied polymer is poly(\"N\"-isopropylacrylamide). Further examples are poly[2-(dimethylamino)ethyl methacrylate] (pDMAEMA) hydroxypropylcellulose, poly(vinylcaprolactame) and polyvinyl methyl ether.\n\nSome industrially relevant polymers show LCST as well as UCST behavior whereas the UCST is found outside the 0-to-100 °C region and can only be observed under extreme experimental conditions. Examples are polyethylene oxide, polyvinylmethylether and polyhydroxyethylmethacrylate. There are also polymers that exhibit UCST behavior between 0 and 100 °C. However, there are large differences concerning the ionic strength at which UCST behavior is detected. Some zwitterionic polymers show UCST behavior in pure water and also in salt-containing water or even at higher salt concentration. By contrast, polyacrylic acid displays UCST behavior solely at high ionic strength. Examples for polymer that show UCST behavior in pure water as well as under physiological conditions are poly(\"N\"-acryloylglycinamide), ureido-functionalized polymers, copolymers from \"N\"-vinylimidazole and 1-vinyl-2-(hydroxylmethyl)imidazole or copolymers from acrylamide and acrylonitrile. Polymers for which UCST relies on non-ionic interactions are very sensitive to ionic contamination. Small amounts of ionic groups may suppress phase separation in pure water.\n\nIt should be noted that the UCST is dependent on the molecular mass of the polymer. For the LCST this is not necessarily the case, as shown for poly(\"N\"-isopropylacrylamide).\n"}
{"id": "39338684", "url": "https://en.wikipedia.org/wiki?curid=39338684", "title": "Transparency in Armaments", "text": "Transparency in Armaments\n\nTransparency in Armaments (TIA) is an arms control reporting program established by the United Nations General Assembly on December 9, 1991 under UN resolution 46/36L. It calls for annual reporting by UN member states on imports, exports, and holdings of weapons in seven categories: battle tanks; armored combat vehicles; large caliber artillery systems; attack helicopters; combat aircraft; warships; and missiles and missile launchers. Reporting is not required but is strongly encouraged. Reports are sent to the Secretary General of the United Nations and are maintained in the United Nations Conventional Arms Register (UNCAR).\n\nReporting has not been consistent. At least 170 member states and three non-member states have reported at least once since reporting began. However, in 2010, only 72 national reports were received. The highest rate of compliance is by nations that are members of the Organization for Security and Co-operation in Europe (OSCE), because the data required by TIA is comparable to that required by other OSCE arms control initiatives.\n"}
{"id": "14449786", "url": "https://en.wikipedia.org/wiki?curid=14449786", "title": "Transport standards organisations", "text": "Transport standards organisations\n\nTransport standards organisations is an article transport Standards organisations, consortia and groups that are involved in producing and maintaining standards that are relevant to the global transport technology, transport journey planning and transport ticket/retailing industry. Transport systems are inherently distributed systems with complex information requirements. Robust modern standards for transport data are important for the safe and efficient operation of transport systems. These include:\n\n\nThe formal development of international standards is organised in three tiers of Standards Development Organisations, recognised by international agreements :\n\n\nThe SDOs conduct their work through a system of working groups, responsible for different areas of expertise. These evolve over time to accommodate changes in technology. key current working groups for transport standards are outlined below.\n\nCEN Allocates responsibility for different areas of transport standardisation to working groups\n\n\nISO Technical Committee 204 is responsible for \"Transport Information and Control Systems\". It has a number of standing Working Groups, which set up Subgroups from time to time.\n\nCurrent ISO TC204 Working Groups, Work program & Countries that provide Secretariat are as follows\n\n\nFor an up-to-date schedule of the remit of TC204, its current Working Groups and their points of contact please refer to:\nThe U.S. standards developing organization which is tasked with the domestic implementation of ISO TC204 Transport Standards, is the Telecommunications Industry Association.\n\nAs well as the formal SDOs, a number of other international bodies undertake work that is important for Transport and Transport Information standards\n\n\n\n\n\n\n\n\n"}
{"id": "12836798", "url": "https://en.wikipedia.org/wiki?curid=12836798", "title": "Waterworks Museum", "text": "Waterworks Museum\n\nThe Waterworks Museum is located on the northern side of Table Mountain, between the Woodhead and Hely-Hutchinson Reservoirs, in Cape Town, South Africa. The museum was founded in 1972 by Terence Timoney, a retired waterworks engineer. The museum houses a display of memorabilia from the construction of the dams on Table Mountain which include a beautifully restored narrow gauge steam engine. It has an interesting display of original equipment, hand tools, instruments and photographs and includes the original well-preserved steam locomotive used to haul equipment from the old cableway at Kasteelspoort. \n"}
{"id": "32326974", "url": "https://en.wikipedia.org/wiki?curid=32326974", "title": "Zafin", "text": "Zafin\n\nZafin is a banking software enterprise platform company that provides relationship-based pricing to banks and financial institutions.\n\nThe company has offices in Canada, USA, UK, Germany, Dubai, Malaysia, South Africa, and India. The company is founded and led by Karim Somji.\n\nZafin's main product, miRevenue, is used by banks to enable relationship-based pricing and enterprise billing functionalities.\n\nThe company's clients include banks and financial institutions such as Standard Chartered Bank, Bank of the West, CIMB, ZKB, Nedbank, HDFC Bank, National Bank of Abu Dhabi and SEB.\n\nZafin sells with IT partners including IBM, Silverlake, CGI and Dell Services.\n\nZafin won a Technology Award in 2007, courtesy of The Banker magazine for Best Implementation in the Retail Banking Project category. The award represented the first implementation of miRevenue in a bank and was awarded for the retail banking implementation at HDFC Bank.\n\nZafin was recognized as one of the \"Top 10 FinTech Companies to Watch\" by American Banker in 2013.\n\nZafin was listed on the Deloitte Fast 50 and Deloitte Fast 500 lists in 2014. The company experienced 865% revenue growth over the previous five year period \n\nZafin completed a customized pricing system for ZKB in 2008. The complexity of the integration, which resulted in allowing ZKB to perform relationship-based pricing for large customers in real time, was featured in an academic text, \"Management von Integrationsprojekten: Konzeptionelle Grundlagen und Fallstudien aus fachlicher und IT-Sicht\", edited by Dr. Robert Winter of the University of St. Gallen.\n\nIn October 2017, the company launched new fintech partner ecosystem to assist the banks with a one-stop origination platform.\n"}
