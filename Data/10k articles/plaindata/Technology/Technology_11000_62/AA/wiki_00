{"id": "3443954", "url": "https://en.wikipedia.org/wiki?curid=3443954", "title": "1,3,5-Triazido-2,4,6-trinitrobenzene", "text": "1,3,5-Triazido-2,4,6-trinitrobenzene\n\n1,3,5-Triazido-2,4,6-trinitrobenzene, also known as TATNB (triazidotrinitrobenzene) and TNTAZB (trinitrotriazidobenzene), is an aromatic high explosive composed of a benzene ring with three azido groups (N) and three nitro groups (NO) alternating around the ring. It has chemical formula C(N)(NO). Its velocity of detonation is 7,350 meters per second, comparable to TATB.\n"}
{"id": "27252213", "url": "https://en.wikipedia.org/wiki?curid=27252213", "title": "1970s in furniture", "text": "1970s in furniture\n\nFurniture of the 1970s refers to the style of furniture popular in the 1970s. Often, the furniture would be laid with bold fabric patterns and colors. Bold designs and prints were also used profusely in other decor. Other design elements found in 1970s furniture and interior decorating included the use of the colors brown, purple, orange, and yellow (sometimes all in the same piece of fabric), shag-pile carpet, textured walls, lacquered furniture, gaudy lampshades, lava lamps, and molded plastic furniture.\n\nAnother major aspect of 1970s furniture is the use of teak wood. The use of teak in fashionable furniture and panelling regained popularity in the 1960s and items became chunkier as it progressed into the 1970s. Because of the popularity of wood in homes, dark color palettes also became more widely used as the 1970s progressed. In the mid-to-late 1970s, pine wood began to replace teak wood, and color palettes became even darker.\n\n"}
{"id": "5657545", "url": "https://en.wikipedia.org/wiki?curid=5657545", "title": "Advanced Message Queuing Protocol", "text": "Advanced Message Queuing Protocol\n\nThe Advanced Message Queuing Protocol (AMQP) is an open standard application layer protocol for message-oriented middleware. The defining features of AMQP are message orientation, queuing, routing (including point-to-point and publish-and-subscribe), reliability and security.\n\nAMQP mandates the behavior of the messaging provider and client to the extent that implementations from different vendors are interoperable, in the same way as SMTP, HTTP, FTP, etc. have created interoperable systems. Previous standardizations of middleware have happened at the API level (e.g. JMS) and were focused on standardizing programmer interaction with different middleware implementations, rather than on providing interoperability between multiple implementations. Unlike JMS, which defines an API and a set of behaviors that a messaging implementation must provide, AMQP is a wire-level protocol. A wire-level protocol is a description of the format of the data that is sent across the network as a stream of bytes. Consequently, any tool that can create and interpret messages that conform to this data format can interoperate with any other compliant tool irrespective of implementation language.\n\nAMQP is a binary, application layer protocol, designed to efficiently support a wide variety of messaging applications and communication patterns. It provides flow controlled, message-oriented communication with message-delivery guarantees such as \"at-most-once\" (where each message is delivered once or never), \"at-least-once\" (where each message is certain to be delivered, but may do so multiple times) and \"exactly-once\" (where the message will always certainly arrive and do so only once), and authentication and/or encryption based on SASL and/or TLS. It assumes an underlying reliable transport layer protocol such as Transmission Control Protocol (TCP).\n\nThe AMQP specification is defined in several layers: (i) a type system, (ii) a symmetric, asynchronous protocol for the transfer of messages from one process to another, (iii) a standard, extensible message format and (iv) a set of standardised but extensible 'messaging capabilities.'\n\nAMQP was originated in 2003 by John O'Hara at JPMorgan Chase in London. AMQP was conceived as a co-operative open effort. The initial design was by JPMorgan Chase from mid-2004 to mid-2006 and it contracted iMatix Corporation to develop a C broker and protocol documentation. In 2005 JPMorgan Chase approached other firms to form a working group that included Cisco Systems, IONA Technologies, iMatix, Red Hat, and Transaction Workflow Innovation Standards Team (TWIST). In the same year JPMorgan Chase partnered with Red Hat to create Apache Qpid, initially in Java and soon after C++. Independently, RabbitMQ was developed in Erlang by Rabbit Technologies, followed later by the Microsoft and StormMQ implementations.\n\nThe working group grew to 23 companies including Bank of America, Barclays, Cisco Systems, Credit Suisse, Deutsche Börse, Goldman Sachs, HCL Technologies Ltd, Progress Software, IIT Software, INETCO Systems Limited, Informatica (including 29 West), JPMorgan Chase, Microsoft Corporation, my-Channels, Novell, Red Hat, Software AG, Solace Systems, StormMQ, Tervela Inc., TWIST Process Innovations ltd, VMware (which acquired Rabbit Technologies) and WSO2.\n\nIn August 2011, the AMQP working group announced its reorganization into an OASIS member section.\n\nAMQP 1.0 was released by the AMQP working group on 30 October 2011, at a conference in New York. At the event Microsoft, Red Hat, VMware, Apache, INETCO and IIT Software demonstrated software running the protocol in an interoperability demonstration. The next day, on 1 November 2011, the formation of an OASIS Technical Committee was announced to advance this contributed AMQP version 1.0 through the international open standards process. The first draft from OASIS was released in February 2012, the changes as compared to that published by the Working Group being restricted to edits for improved clarity (no functional changes). The second draft was released for public review on 20 June (again with no functional changes), and AMQP was approved as an OASIS standard on 31 October 2012.\n\nOASIS AMQP was approved for release as an ISO and IEC International Standard in April 2014. AMQP 1.0 was balloted through the Joint Technical Committee on Information Technology (JTC1) of the International Standards Organization (ISO) and the International Electrotechnical Commission (IEC). The approved OASIS AMQP submission has been given the designation, ISO/IEC 19464.\n\nPrevious versions of AMQP were 0-8, published in June 2006, 0-9, published in December 2006, 0-10 published in February 2008 and 0-9-1, published in November 2008. These earlier releases are significantly different from the 1.0 specification.\n\nWhilst AMQP originated in the financial services industry, it has general applicability to a broad range of middleware problems.\n\nAMQP defines a self-describing encoding scheme allowing interoperable representation of a wide range of commonly used types. It also allows typed data to be annotated with additional meaning, for example a particular string value might be annotated so that it could be understood as a URL. Likewise a map value containing key-value pairs for 'name', 'address' etc., might be annotated as being a representation of a 'customer' type.\n\nThe type-system is used to define a message format allowing standard and extended meta-data to be expressed and understood by processing entities. It is also used to define the communication primitives through which messages are exchanged between such entities, i.e. the AMQP \"frame bodies\".\n\nThe basic unit of data in AMQP is a \"frame\". There are nine AMQP frame bodies defined that are used to initiate, control and tear down the transfer of messages between two peers. These are:\n\n\nThe \"link protocol\" is at the heart of AMQP.\n\nAn \"attach\" frame body is sent to initiate a new link; a \"detach\" to tear down a link. Links may be established in order to receive or send messages.\n\nMessages are sent over an established \"link\" using the \"transfer\" frame. Messages on a link flow in only one direction.\n\nTransfers are subject to a credit based flow control scheme, managed using \"flow\" frames. This allows a process to protect itself from being overwhelmed by too large a volume of messages or more\nsimply to allow a subscribing link to pull messages as and when desired.\n\nEach transferred message must eventually be \"settled\". Settlement ensures that the sender and receiver agree on the state of the transfer, providing reliability guarantees. Changes in state and settlement for a transfer (or set of transfers) are communicated between the peers using the \"disposition\" frame. Various reliability guarantees can be enforced this way: at-most-once, at-least-once and exactly-once.\n\nMultiple links, in both directions, can be grouped together in a \"session\". A session is a bidirectional, sequential conversation between two peers that is initiated with a \"begin\" frame and\nterminated with an \"end\" frame. A connection between two peers can have multiple sessions multiplexed over it, each logically independent. Connections are initiated with an \"open\" frame in which\nthe sending peer's capabilities are expressed, and terminated with a \"close\" frame.\n\nAMQP defines as the \"bare message\", that part of the message that is created by the sending application. This is considered immutable as the message is transferred between one or more processes.\n\nEnsuring the message as sent by the application is immutable allows for end-to-end message signing and/or encryption and ensures that any integrity checks (e.g. hashes or digests) remain valid. The message can be annotated by intermediaries during transit, but any such annotations are kept distinct from the immutable \"bare message\". Annotations may be added before or after the bare message.\n\nThe \"header\" is a standard set of delivery-related annotations that can be requested or indicated for a message and includes time to live, durability, priority.\n\nThe bare message itself is structured as an optional list of standard properties (message id, user id, creation time, reply to, subject, correlation id, group id etc.), an optional list of\napplication-specific properties (i.e., extended properties) and a body, which AMQP refers to as application data.\n\nProperties are specified in the AMQP type system, as are annotations. The application data can be of any form, and in any encoding the application chooses. One option is to use the AMQP type system to send structured, self-describing data.\n\nThe link protocol transfers messages between two \"nodes\" but assumes very little as to what those nodes are or how they are implemented.\n\nA key category is those nodes used as a \"rendezvous point\" between senders and receivers of messages (e.g. \"queues\" or \"topics\"). The AMQP specification calls such nodes \"distribution nodes\" and codifies some common behaviors.\n\nThis includes:\n\n\nThough AMQP can be used in simple peer-to-peer systems, defining this framework for messaging capabilities additionally enables interoperability with messaging intermediaries (brokers, bridges etc.)\nin larger, richer messaging networks. The framework specified covers basic behaviors but allows for extensions to evolve that can be further codified and standardised.\n\n\n\nAMQP protocol version 1.0 is the current specification version. It focuses on core features which are necessary for interoperability at Internet scale. It contains less explicit routing than previous versions because core functionality is the first to be rigorously standardized. AMQP 1.0 interoperability has been more extensively tested with more implementors than prior versions.\n\nThe AMQP website contains the OASIS specification for version 1.0.\n\nEarlier versions of AMQP, published prior to the release of 1.0 (see History above) and significantly different from it, include:\n\nThese are the known open protocol specifications that cover the same or similar space as AMQP:\n\nJava Message Service (JMS), is often compared to AMQP. However, JMS is an API specification (part of the Java EE specification) that defines how message producers and consumers are implemented. JMS does not guarantee interoperability between implementations, and the JMS-compliant messaging system in use may need to be deployed on both client and server. On the other hand, AMQP is a wire-level protocol specification. In theory AMQP provides interoperability as different AMQP-compliant software can be deployed on the client and server sides. Note that, like HTTP and XMPP, AMQP does not have a standard API.\n\n\n"}
{"id": "50859529", "url": "https://en.wikipedia.org/wiki?curid=50859529", "title": "Alexander Duckham", "text": "Alexander Duckham\n\nAlexander Duckham (11 March 1877 – 1 February 1945) was an English chemist and businessman, best known for the development of machine lubricants. The son of an engineer, after university he specialised in lubrication, working briefly for Fleming's Oil Company before founding his own company, Alexander Duckham & Co, in Millwall in 1899.\n\nBy the outbreak of World War I, he was an authority on technological problems relating to lubrication, and the company went public in about 1920, relocating from Millwall to Hammersmith. By the time he died in 1945, Duckhams had assumed a dominant position for the supply of lubricants and corrosion inhibitors to the motor industry in Britain and other markets. A new manufacturing plant was opened in Staffordshire in 1968, and soon thereafter the company was taken over by BP.\n\nDuckham was born in Blackheath, London, the eldest son of a Falmouth-born mechanical and civil engineer, Frederic Eliot Duckham (1841 - died 13 January 1918 in Blackheath), who had patented improvements in governors for marine engines and invented a 'Hydrostatic Weighing Machine'. His mother was Maud Mary McDougall (1849-1921), sister of John McDougall of the flour-making family, which had a mill at Millwall Dock. His younger brother, Arthur Duckham, became one of the founders of the Institution of Chemical Engineers, and its first President.\n\nUpon leaving university in 1899, Alexander Duckham, who had worked briefly for Fleming's Oil Company, was encouraged by engineer Sir Alfred Yarrow, who lived nearby (Yarrow occupied Woodlands House in Mycenae Road, Westcombe Park for some years from 1896, close to the Duckham family home in Dartmouth Grove, Blackheath) to specialise in the study of lubrication, and was introduced to engineering firms with lubrication problems. Duckham established Alexander Duckham & Co in Millwall in 1899, and gradually assembled a team of engineers and chemists to whom he could delegate research work, freeing him to focus on lubricant production. Early customers included car dealer and racing driver Selwyn Edge who called weekly at Duckham's Millwall works for an oil change; Duckham, who bought his first car in 1899, also used to accompany Edge to Brooklands.\n\nYarrow and Lord Fisher subsequently encouraged Duckham to focus on sourcing raw materials for lubricants. From 1905 he helped pioneer the development of the Trinidad oil fields, including a deposit near Tabaquite of high-class crude oil suitable as a base for the preparation of lubricants, establishing a private company, Trinidad Central Oilfields, in 1911. The discovery and development of such lubricants was timely, coinciding with the evolution of internal combustion engines which demanded more advanced lubrication.\n\nAs well as being a successful businessman, Duckham was an early aviation pioneer and close friend of cross-channel aviator Louis Blériot – he paid for the stone memorial in Dover marking the place where Blériot landed in 1909 to complete the first flight across the English Channel in a heavier-than-air aircraft, and 25 years later hosted a dinner at London's Savoy Hotel marking the anniversary of the flight.\n\nThe outbreak of World War I in 1914 heightened the focus on mechanical efficiency, and the Duckham company was already established as the highest authority on technological problems in matters of lubrication. The company went public (c. 1920) soon after the war finished, and relocated from Millwall to Hammersmith in 1921.\n\nBy the time, Alexander Duckham died in 1945 (being succeeded as company chairman by his son Jack), Duckhams had assumed a dominant position in supply of lubricants and corrosion inhibitors to the motor industry and other markets. Behind Castrol, by 1967, it was regarded as the largest independent lubricating oil company in the UK and the third largest supplier of engine oil to motorists, producing the first multigrade oil for motorists. To cope with demand, a new manufacturing plant was opened in Aldridge, Staffordshire in 1968, shortly before the company was acquired by BP in 1969. Duckhams' Hammersmith site closed in 1979, was acquired by Richard Rogers' architects practice (today Rogers Stirk Harbour + Partners) in 1983, and was redeveloped to become the Thames Wharf Studios and the River Café.\n\nHe married Violet Ethel Narraway in 1902, and they had five children, all born in Greenwich: Alec Narraway Duckham (born c. 1904); Millicent A. M. Duckham (c. 1905); Joan Ethel Duckham (c. 1906); Jack Eliot Duckham (c. 1908); and Ruth Edith\nDuckham (born 1918).\n\nThe family lived for some years from 1907 in Vanbrugh Castle, close to Greenwich Park. In 1920, Duckham donated the house (and another property, Rooks Hill House in Sevenoaks) to the RAF Benevolent Fund to be used as a school for the children of RAF personnel killed in service. Vanbrugh Castle was later sold after the number of pupils declined; sale proceeds were used to educate RAF children, with funds later (1997) transferred to a charitable trust, the Alexander Duckham Memorial Schools Trust.\n"}
{"id": "48585266", "url": "https://en.wikipedia.org/wiki?curid=48585266", "title": "Biological Resource Center", "text": "Biological Resource Center\n\nA Biological Resource Centre (BRC) is considered to be one of the key elements for sustainable international scientific infrastructure, which is necessary to underpin successful delivery of the benefits of biotechnology, whether within the health sector, the industrial sector or other sectors, and in turn ensure that these advances help drive growth.\n\nThe OECD defines BRCs as follows:\n\n\"Biological Resource Centres are an essential part of the infrastructure underpinning biotechnology. They consist of service providers and repositories of the living cells, genomes of organisms, and information relating to heredity and the functions of biological systems. BRCs contain collections of culturable organisms (e.g. micro-organisms, plant, animal and human cells), replicable parts of these (e.g. genomes, plasmids, viruses, cDNAs), viable but not yet culturable organisms, cells and tissues, as well as databases containing molecular, physiological and structural information relevant to these collections and related bioinformatics.\"\n\nBRCs retain collections of biological material and associated information to facilitate access to ex situ biological resources and to ensure that they remain available for sustainable use.\n\nBiological resource collections are entities compliant with appropriate national law, regulations, and policies and have been constituted to fulfil many crucial roles, which include:\n\n\n"}
{"id": "16460488", "url": "https://en.wikipedia.org/wiki?curid=16460488", "title": "Butane torch", "text": "Butane torch\n\nA butane torch is a tool which creates an intensely hot flame using butane, a flammable gas.\n\nConsumer air butane torches are often claimed to develop flame temperatures up to approximately . This temperature is high enough to melt many common metals, such as aluminum and copper, and hot enough to vaporize many organic compounds as well.\n\nBrazing, soldering, plumbing\nOften used as daily task tools, butane torches work great for home improvement and work to solve problems with plumbing, soldering and brazing. Most of the times copper, silver and other metals are used for home repairs of tubes and other house things.\n\nButane torches are frequently employed as kitchen gadgets to caramelize sugar in cooking, such as when making crème brûlée. They may be marketed as kitchen torches, cooking torches, or culinary torches. Use of the butane torch in the kitchen is not limited to caramelizing sugar; it can be used to melt or brown toppings on casseroles or soups, to melt cheese, and to roast or char vegetables such as peppers.\n\nPocket butane torches are commonly used as lighters for cigars, capitalizing on the intensity of the flame to light quickly and evenly the large, relatively damp, burning surface of a cigar.\n\nMany bartenders and mixologists use butane torches in their recipes. Smoked and flaming cocktails are now a trend.\n\nButane torches are sometimes used in vaporizing cocaine free base (crack), methamphetamine or hash oil for inhalation.\n\n"}
{"id": "47573732", "url": "https://en.wikipedia.org/wiki?curid=47573732", "title": "Casa montañesa", "text": "Casa montañesa\n\nThe type of architecture referred to as Casa montañesa is a form of traditional construction of La Montaña and the communities of Cantabria, east of Asturias and northern Castile and León in northern Spain.\n\nThis housing style developed from the sixteenth through the eighteenth centuries and becoming widely seen by the end of the nineteenth century, when it was popularized by the Spanish architect Leonardo Rucabado. It was widely seen across Cantabria.\n\nThis type of traditional country house is the most characteristic of Cantabria. It highlights the south façade, open to the sun, with the other walls made of thick rough masonry. The corners are usually of ashlars with re-enclosure at all the spans. The entrance is through a gate of one or two arcs, sufficient to allow passage of a cart into an entryway, leading to the kitchen (although in some houses the kitchen is on the upper level), stables, wine cellar, pantry and stairs to the upper floor. In some areas, there are passageways to enter the upper floor from the outside.\n\nThe upper floor contains the bedrooms, two of which face the balcony. The interior walls originally are usually made of wood, although in recent times have been replaced by brick.\n\nThe sun area is the most typical element of this construction. It is a balcony run with wood rail protected from wind and rain by the overhanging roof and the projecting sidewalls.\n\nThe walls are usually topped by wooden trim pieces rough but of classic profiles, serving to support beams of the edge as securing the canes that form the overhang. The wood is usually painted dark brown.\n\n"}
{"id": "14495121", "url": "https://en.wikipedia.org/wiki?curid=14495121", "title": "Closed user group", "text": "Closed user group\n\nClosed User group (CUG) is a supplementary service provided by the mobile operators to mobile subscriber’s who can make and receive calls from any member associated within the group. This service is applicable for SMS also. There will be administrative owner who will be responsible for invoicing. Irrespective of this a CUG member can make and receive calls to and from other networks outside the CUG group too; although calls outside a CUG group may not be invoiced by the administrative owner.\n\nA subscriber may:\n\n\nIf the user is a member of multiple closed user groups there will be a preferred CUG assigned by the network that will be used by default. However, it is possible on a per-call basis to specify a different closed user group (of which the user is a member) for the call. It is also possible on a per-call basis to suppress the use of the preferred CUG, i.e. act as if the user is not a member of the closed user group, and to suppress the outgoing access permission, i.e. to insist that the call only go through if the destination is a member of the CUG.\n\nWhen an incoming call is received it is possible for the network to indicate the closed user group that is being applied to the call to the called user.\n\nFor example:\n\nMr Rao, a senior member at a pizza delivery outlet, could be a member of two closed user groups:\n\n\nMr Rao's preferred CUG would be that of his team. But based on whom Mr. Rao is calling, he can either suppress or enable the preferred CUG. Also, when Mr. Rao receives a call, the network would indicate which user group the call originated from.\n\nAs can be seen, this supplementary service is restricted in use only by organizations, and is not for use by the general public. However, there are handsets that support closed user group applications.\n\n"}
{"id": "26336225", "url": "https://en.wikipedia.org/wiki?curid=26336225", "title": "Cognitive engineering", "text": "Cognitive engineering\n\nCognitive engineering is a method of study using cognitive psychology to design & develop engineering systems to support the cognitive processes of users.\n\nIt was an engineering method used in the 1970s at Bell Labs, focused on how people form a cognitive model of a system based upon common metaphors. As explained, by Joseph Henry Condon:\n\nAccording to Condon, the ideas of cognitive engineering were developed later than, and independent from, the early work on the Unix operating system.\n\nDon Norman cited principles of cognitive engineering in his 1981 article, \"The truth about Unix: The user interface is horrid.\" Norman criticized the user interface of Unix as being \"a disaster for the casual user.\"\n\n"}
{"id": "8723369", "url": "https://en.wikipedia.org/wiki?curid=8723369", "title": "Computer architecture simulator", "text": "Computer architecture simulator\n\nA computer architecture simulator is a program that simulates the execution of computer architecture.\n\nComputer architecture simulators are used for the following purposes:\n\nComputer architecture simulators can be classified into many different categories depending on the context.\n\nA full-system simulator is execution-driven architecture simulation at such a level of detail that complete software stacks from real systems can run on the simulator without any modification. A full system simulator provides virtual hardware that is independent of the nature of the host computer. The full-system model typically includes processor cores, peripheral devices, memories, interconnection buses, and network connections. Emulators are full system simulators that imitate obsolete hardware instead of under development hardware.\n\nThe defining property of full-system simulation compared to an instruction set simulator is that the model allows real device drivers and operating systems to be run, not just single programs. Thus, full-system simulation makes it possible to simulate individual computers and networked computer nodes with all their software, from network device drivers to operating systems, network stacks, middleware, servers, and application programs.\n\nFull system simulation can speed the system development process by making it easier to detect, recreate and repair flaws. The use of multi-core processors is driving the need for full system simulation, because it can be extremely difficult and time-consuming to recreate and debug errors without the controlled environment provided by virtual hardware. This also allows the software development to take place before the hardware is ready, thus helping to validate design decisions.\n\nA cycle-accurate simulator is a computer program that simulates a microarchitecture on a cycle-by-cycle basis. In contrast an instruction set simulator simulates an instruction set architecture usually faster but not cycle-accurate to a specific implementation of this architecture; they are often used when emulating older hardware, where time precisions are very important from legacy reasons. Often, a cycle-accurate simulator is used when designing new microprocessorsthey can be tested, and benchmarked accurately (including running full operating system, or compilers) without actually building a physical chip, and easily change design many times to meet expected plan.\n\nCycle-accurate simulators must ensure that all operations are executed in the proper virtual (or real if it is possible) timebranch prediction, cache misses, fetches, pipeline stalls, thread context switching, and many other subtle aspects of microprocessors.\n\n\n"}
{"id": "243812", "url": "https://en.wikipedia.org/wiki?curid=243812", "title": "Crossing the Chasm", "text": "Crossing the Chasm\n\nCrossing the Chasm: Marketing and Selling High-Tech Products to Mainstream Customers or simply Crossing the Chasm (1991, revised 1999 and 2014), is a marketing book by Geoffrey A. Moore that focuses on the specifics of marketing high tech products during the early start up period. Moore's exploration and expansion of the diffusions of innovations model has had a significant and lasting impact on high tech entrepreneurship. In 2006, Tom Byers, director of the Stanford Technology Ventures Program, described it as \"still the bible for entrepreneurial marketing 15 years later\". The book's success has led to a series of follow-up books and a consulting company, The Chasm Group.\n\nIn \"Crossing the Chasm\", Moore begins with the diffusion of innovations theory from Everett Rogers, and argues there is a chasm between the early adopters of the product (the technology enthusiasts and visionaries) and the early majority (the pragmatists). Moore believes visionaries and pragmatists have very different expectations, and he attempts to explore those differences and suggest techniques to successfully cross the \"chasm,\" including choosing a target market, understanding the whole product concept, positioning the product, building a marketing strategy, choosing the most appropriate distribution channel and pricing.\n\n\"Crossing the Chasm\" is closely related to the technology adoption lifecycle where five main segments are recognized: innovators, early adopters, early majority, late majority and laggards. According to Moore, the marketer should focus on one group of customers at a time, using each group as a base for marketing to the next group. The most difficult step is making the transition between visionaries (early adopters) and pragmatists (early majority). This is the chasm that he refers to. If a successful firm can create a bandwagon effect in which enough momentum builds, then the product becomes a de facto standard. However, Moore's theories are only applicable for disruptive or discontinuous innovations. Adoption of continuous innovations (that do not force a significant change of behavior by the customer) are still best described by the original technology adoption lifecycle.\n\nPre-chasm, in technology entrepreneurship, describes the phase prior to \"Crossing the Chasm\" that focuses on the specifics of marketing high-tech products during the early start-up period. Pre-chasm was suggested as an extension to Moore's model, arguing that the phase prior to the \"chasm\" is left unintended and that it, driven by technology commoditization and lean startup principles, requires an ambidextrous approach to product development alongside marketing to achieve product-market fit.\n\nMoore and his publisher originally thought that the book would sell around 5,000 copies. By 2002, ten years after the first publication, more than 300,000 copies had been sold. Moore attributes this to word-of-mouth marketing, resonating initially with high-tech managers, then to engineers, venture capitalists and finally business schools. The book's success led to a number of sequels including \"Inside the Tornado\", \"Living on the Fault Line\" and \"The Chasm Companion\". \"Crossing the Chasm\" is available in several prints, one is .\n\n"}
{"id": "2821141", "url": "https://en.wikipedia.org/wiki?curid=2821141", "title": "Digital Chart of the World", "text": "Digital Chart of the World\n\nThe Digital Chart of the World (DCW) is a comprehensive digital map of Earth. It is the most comprehensive geographical information system (GIS) global database that is freely available as of 2006, although it has not been updated since 1992.\n\nThe primary source for this database is the United States Defense Mapping Agency's (DMA) operational navigation chart (ONC) 1:1,000,000 scale paper map series produced by the US, Australia, Canada, and the United Kingdom. These charts were designed to meet the needs of pilots and air crews in medium-and low-altitude \"en route\" navigation and to support military operational planning, intelligence briefings, and other needs.\n\nThe data is divided into 2,094 tiles that represent 5 × 5-degree areas of the globe, except data obtained from Penn State which is broken up by pre-1992 national boundaries, and data from the National Imagery and Mapping Agency (NIMA) which is broken into just five tiles. The data currency varies from place to place, ranging from the mid-1960s to the early 1990s.\n\nThe thematic layers of the Digital Chart of the World are:\n\n\n"}
{"id": "7647457", "url": "https://en.wikipedia.org/wiki?curid=7647457", "title": "Dokorder", "text": "Dokorder\n\nDokorder was a brand of tape recorder from Japanese electronics company , located in Ōta, Tokyo (not related to the Onkyo audio company of Osaka, neither to Denon) that included a four-reel transport system called \"Dub-A-Tape\" capable of feeding two different tapes through the same tape head assembly and, in the process, recording a duplicate of a tape. (The master and blank tapes passed through the same capstan, but through different tape head areas, yielding a Y-shaped tape path reflected in the slots built into the head assembly. Denki Onkyo also supplied a consumer and semi-pro line of reel to reel recorders (including 4 track multitracks) that competed with Tascam/Teac and Sony but ultimately its products were found to be less durable than the competition.\nIn 1982, Murata Manufacturing acquired 55% shares of Denki Onkyo, and in 1999, the company has been fully merged into Murata.\n"}
{"id": "1110298", "url": "https://en.wikipedia.org/wiki?curid=1110298", "title": "Enfleurage", "text": "Enfleurage\n\nEnfleurage is a process that uses odorless fats that are solid at room temperature to capture the fragrant compounds exuded by plants. The process can be \"cold\" enfleurage or \"hot\" enfleurage.\n\nThere are two types of processes:\n\nIn both instances, once the fat is saturated with fragrance, it is then called the \"enfleurage pomade\". The enfleurage pomade was either sold as it was, or it could be further washed or soaked in ethyl alcohol to draw the fragrant molecules into the alcohol. The alcohol is then separated from the fat and allowed to evaporate, leaving behind the absolute of the botanical matter. The spent fat is usually used to make soaps since it is still relatively fragrant.\n\nThe enfleurage fragrance extraction method is one of the oldest. It is also highly inefficient and costly but was the sole method of extracting the fragrant compounds in delicate flowers such as jasmine and tuberose, which would be destroyed or denatured by the high temperatures required by methods of fragrance extraction such as steam distillation. The method is now superseded by more efficient techniques such as solvent extraction or supercritical fluid extraction using liquid carbon dioxide (CO) or similar compressed gases.\n"}
{"id": "21821058", "url": "https://en.wikipedia.org/wiki?curid=21821058", "title": "English Electric Canberra (book)", "text": "English Electric Canberra (book)\n\nThe English Electric Canberra subtitled \"The History and Development of a Classic Jet\" () is a book by British military historian and author Bruce Barrymore Halpenny about the English Electric Canberra. Illustrated throughout, the book includes interviews with Wing Commander K H Wallis, the man Halpenny attributes as having \"saved the Canberra\".\n\nThe book looks at the development of the aircraft during the early days of jet power and beyond. Each of the many marks and variants are described. The type's record of service with RAF Squadrons is given together with descriptions of the many experimental models.\n\nIn the introduction, the author states, “it was a matter of producing either a technical book, or one that would appeal to a wider readership, setting out the true Canberra story: marks, variants, overseas orders, squadrons, records, experimental Canberras, camouflage, markings, and most importantly, the truth about bombing up the aircraft; also serious problems with which the Canberra was sent out to operational R.A.F. stations.\" He chose the latter and the finished article is an \"outstanding\" tribute to a remarkable aeroplane, though those that were deep aviation fans were unhappy that it was not a technical book. A case that the author could not satisfy everyone.\n\nThe book took 18 years to complete and the acknowledgments cover two full pages - a testimony to the thoroughness of Halpenny's research. Among the many firms and names mentioned, one in particular comes in for special mention. W/Cdr. K H Wallis, who saved the life of the Canberra by inventing the system of loading bombs for as late at 1951 at R.A.F. Binbrook, not a single aircraft was capable of delivering bombs, simply because the fuselage was too low to the ground! \n\nWith Canberra's introduction came the early Rolls Royce Axial Flow Avon engine, a full description of which, its history and development is given in Chapter 2 - the author even tells us how a jet engine delivers its thrust. \n\nThe book displays a collection of photographs - some never seen before, also the history and deployment of no less than 81 Squadrons each of which flies the Canberra in its various marks and roles. \n"}
{"id": "23621133", "url": "https://en.wikipedia.org/wiki?curid=23621133", "title": "Freedom Machines", "text": "Freedom Machines\n\nFreedom Machines is a 2004 PBS/P.O.V. documentary that looks at disability in the age of technology, presenting intimate stories of people ages 8–93, whose talents and independence are being unleashed by access to modern, enabling technologies. Nearly twenty years after the Americans with Disabilities Act of 1990, the film reflects on the gaps between its promise and the realities for our largest minority group – 54,000,000 Americans with disabilities. Whether mainstream tools or inventions such as a stair climbing wheelchair (iBOT), \"Freedom Machines\" examines the power of technology to change lives.\n\n\"Freedom Machines\" a public television program and national outreach campaign, looks at our beliefs about disability through the lens of assistive technology. The program \nexplores how human experience and technological innovations are outpacing social policies and the perceptions that have guided them.\n\nIn \"Freedom Machines\", viewers will meet a cross-section of America's population a few of the 54 million Americans with disabilities whose lives are being transformed with the help of new technologies. Despite its promise, statistics indicate that fewer than 25% of people with disabilities who could be helped by assistive technology are using it. A 1999 study commissioned by The California Endowment and conducted by the Alliance for Technology Access found that people with disabilities \"make do\" without vital technology because they are not aware that it is available to them and don't know how to obtain it. Furthermore, the people they most often turn to for information and referrals medical care providers, educators, and community technology centers have inadequate or outdated knowledge themselves. The hardest hit are the poor, those who speak English as a second language, minorities, and the rural poor.\n\nWhat makes this situation unusual is the existence of tangible solutions and the possibility that they can be widely applied. \"Freedom Machines\" weaves together the stories of a group of people in a dozen locations around the United States. It shows what is now possible, what will soon be possible, and why those who could and should benefit are not doing so.\n\n\"Freedom Machines\" studies the concept of diversity through the intimate stories of adults and children with disabilities who are using modern technologies to change their lives. Among them are Susanna who is beginning her college career, 38-year-old Floyd Stewart who was paralyzed in mid-life while raising four children; 92-year-old Gladys who is determined to overcome a hearing loss; and high school student Latoya Nesmith who dreams of becoming a translator at the United Nations. Fifteen years after passage of the Americans with Disabilities Act of 1990, \"Freedom Machines\" is a reflection on the status of life of America's largest minority group: 54 million people with disabilities.\n\nwith running times (min/sec)\n\nwith running times (min/sec)\n\n\n"}
{"id": "48870236", "url": "https://en.wikipedia.org/wiki?curid=48870236", "title": "GeoPackage", "text": "GeoPackage\n\nA GeoPackage (GPKG) is an open, non-proprietary, platform-independent and standards-based data format for geographic information system implemented as a SQLite database container. Defined by the Open Geospatial Consortium with the backing of the US military and published in 2014, GeoPackage has seen wide widespread support from various government, commercial, and open source organizations.\n\nDespite dozens of file formats and services for exchanging geospatial data, there was not an open format which could support both raster and vector data, while being efficiently decodable by software, particularly in mobile devices. This need was formally expressed at the OGC in 2012. The candidate standard was approved by OGC in February 2014.\n\nA GeoPackage is built up as an extended SQLite 3 database file (*.gpkg) containing data & metadata tables with specified definitions, integrity assertions, format limitations and content constraints.\nThe GeoPackage standard describes a set of conventions (requirements) for storing vector features, tile matrix sets of imagery and raster maps at various scales, schema and metadata. \nA GeoPackage can be extended by using the extension rules as defined in clause 2.3 of the standard. The OGC GeoPackage standard specifies a set of OGC member approved extensions in Annex F. While additional (vendor specific) extensions may also be added by following the rules for GeoPackage extensions, beware that doing so can impact interoperability.\n\nGeoPackage was designed to be as lightweight as possible and be contained in one ready-to-use single file. This makes it suitable for mobile applications in disconnected mode and rapid sharing on cloud storage, USB drives, etc. The GeoPackage extension F.3 RTree Spatial Indexes specifies how to use SQLite spatial indexes in order to speed up performance on spatial queries compared to traditional geospatial files formats.\n\n\n\n"}
{"id": "7421532", "url": "https://en.wikipedia.org/wiki?curid=7421532", "title": "Grasscycling", "text": "Grasscycling\n\nGrasscycling refers to an aerobic (requires air) method of handling grass clippings by leaving them on the lawn when mowing.\n\nThe term is a portmanteau combining \"grass\" and \"recycling\", and had come into use by at least 1990 as part of the push to reduce the huge quantities of clippings going into landfills, up to half of some cities' summertime waste flow, as 1,000 square feet (93 m) of lawn can produce 200 to 500 pounds (90 to 225 kg) of clippings a year.\n\nBecause grass consists largely of water (80% or more), contains little lignin, and has high nitrogen content, grass clippings easily break down during an aerobic process (comparable to composting) and returns the decomposed clippings to the soil within one to two weeks, acting primarily as a fertilizer supplement and, to a much smaller degree, a mulch. Grasscycling can provide 15 to 20% or more of a lawn's yearly nitrogen requirements. Proponents also note that grasscycling reduces the use of plastic bags for collecting yard waste and reduces trips to the curb or landfill to haul waste.\n\nOptimal grasscycle techniques include:\n\nAlthough a mulching mower can make grass clippings smaller, one is not necessary for grasscycling.\n\n\n"}
{"id": "45262035", "url": "https://en.wikipedia.org/wiki?curid=45262035", "title": "Hwb (Digital Learning for Wales)", "text": "Hwb (Digital Learning for Wales)\n\nHwb is a website and collection of online tools provided to all schools in Wales by the Welsh Government. It was created in response to the 'Find it, Make it, Use it, Share it' report into Digital Learning in Wales.\n\nThe main site (http://hwb.wales.gov.uk) contains over 88,000 bilingual resources that were transferred from NGfL Cymru. In addition teachers and learners with accounts can sign in and access a range of other online tools and resources. Included in this is a school specific Learning Platform (Hwb+).\n\n"}
{"id": "14295614", "url": "https://en.wikipedia.org/wiki?curid=14295614", "title": "IP-XACT", "text": "IP-XACT\n\nIP-XACT is an XML format that defines and describes individual, re-usable electronic circuit designs (individual pieces of intellectual property, or IPs) to facilitate their use in creating integrated circuits (i.e. \"microchips\"). IP-XACT was created by the SPIRIT Consortium as a standard to enable automated configuration and integration through tools.\n\nThe goals of the standard are\n\nApproved as IEEE 1685-2009 on December 9, 2009, published on February 18, 2010.\nSuperseded by IEEE 1685-2014. IEEE 1685-2009 was adopted as IEC 62014-4:2015.\n\nAll documents will have the following basic titular attributes spirit:vendor, spirit:library, spirit:name, spirit:version.\n\nA document typically represents one of:\n\nFor each port of a component there will be a spirit:busInterface element in the document. This may have a spirit:signalMap\nthat gives the mapping of the formal net names in the interface to the names used in a corresponding formal specification of the port.\nA simple wiring tool will use the signal map to know which net on one interface to connect to which net on another instance\nof the same formal port on another component.\n\nThere may be various versions of a component referenced in the document, each as a spirit:view element, relating to different versions of a design: typical levels are gate-level, RTL and TLM.\nEach view typically contains a list of filenames as a spirit:fileSet that implement the design at that level of abstraction in appropriate language, like Verilog,\nC++ or PSL.\n\nNon-functional data present includes the programmer's view with a list of spirit:register declarations inside a spirit:memoryMap or spirit:addressBlock.\n\n\n\n"}
{"id": "30722081", "url": "https://en.wikipedia.org/wiki?curid=30722081", "title": "Indian Electrical and Electronics Manufacturers Association", "text": "Indian Electrical and Electronics Manufacturers Association\n\nIndian Electrical and Electronics Manufacturers' Association (IEEMA) is a national representative organization of manufacturers of electrical, industrial electronics and allied equipment in India. As a representative of the electrical industry, IEEMA maintains the dialogue with the government of India, its departments, electric utilities, users, standardization bodies, educational institutions, research, development and testing agencies. Mr. Shreegopal Kabra is the president of IEEMA for the year 2017 -18, while Mr. Harish Agarwal and Mr. R K Chugh are the two vice presidents. Mr. Sunil Misra is the Director General of IEEMA.\n\nIndian Electrical & Electronics Manufacturers' Association (IEEMA) was established in 1948 when eight Indian companies decided to create a platform for the promotion of Indian Electrical Manufacturers’ Association (IEMA). The original signatories to the Memorandum of Articles of Association were:\nThe establishment of the Association synchronized with attaining of independence by India reflecting the faith of Indian industrialists in the growth of various individual sectors of the economy in the atmosphere of freedom. In those years, universally, electronics had not attained the status it enjoys today and was mainly restricted to audio, broadcasting, telephonic and telegraphic communication. Telephones were still considered a luxury, affordable by elite class only. Others areas of electronics were defence oriented. Development of civilian industries in this sector was restricted. In General, electronics was considered as a part of the electrical industry.\n\nIn 1965, Mr. PR Deshpande, the dynamic doyen of the industry was elected as the President of IEMA (the then Indian Electrical Manufacturers Association). In the year 1969, the Head Office and active work were shifted to Mumbai in view of the fact that electrical industry was situated prominently in western and southern regions of the Country. In 1986, \"Electronics\" was added to IEMA and since then it is known as IEEMA.\n\nIn the year 2013, the president and the executive committee of IEEMA realized the need for strengthening the New Delhi office for effective advocacy and influencing public policy. To effectively meet this objective, the newly appointed Director General of IEEMA was located in New Delhi. In the past, many national industry associations, have at some point in time moved from the traditional centres of industry concentration, such as Mumbai and Kolkata, to the country’s capital, New Delhi to meet similar objectives. The registered office of IEEMA continues to be in Mumbai, with a strengthened secretariat at New Delhi.\n\nIEEMA's major activities are disseminating information about government policy changes and statistics, representing views of the industry to the government, evolving price variation clauses covering a wide range of products and circulating price indices for the same, and formulating industry standards.\n\nIEEMA conducts technical activities where members meet and discuss issues concerning the industry and its growth opportunities.\n\nIEEMA organizes Elecrama exhibitions dedicated to the electrical T&D sector, international seminars, workshops and training programs, and publishes a monthly IEEMA Journal.\n\nAs the representative organization of the electrical and industrial electronics industry in India, IEEMA is a part of councils and committees constituted by the government of India.\n\nIEEMA as a voice of Indian electrical industry plays a crucial policy advocacy role with central and state government and its agencies. IEEMA is invited by various central and state ministries, central PSU’s, State Utilities to provide its inputs and to play an increasing role as a \"Partner of Choice\" in policy formulation.\n\nIEEMA works closely with standardisation bodies, R&D organisations, testing institutes, other apex industry bodies, autonomous institutions, trade bodies and sectoral & regional associations.\n\nIEEMA organizes delegation visits and participation in exhibitions abroad for the promotion of export of its members.\nActivities of IEEMA\nInteractive session of IEEMA\nInternational Events participation of IEEMA\nIEEMA IBD Meetings\nMoU Partners of IEEMA\nParticipation at International Event\nParticipation on Hannover Messe, Germany\nParticipation in Africa Utility Week, Cape Town S Africa\nParticipation in 5th International Istanbul Smart Grids and Cities Congress and Fair, Istanbul, Turkey\nParticipation in Middle East Electricity Dubai\n\nThrough its circulars, it circulates information about procedural and policy changes made by the Government in taxation, import-export policy, industrial regulations as well as tender information, business opportunities, production and import-export statistics, standards and other matters of interest to the industry.\n\nIEEMA Journal is a publication of IEEMA which covers technical and techno-commercial articles, industry information, statistics, business opportunities, IEEMA activities etc. IEEMA Journal is the leading electrical & electronics monthly published by IEEMA since June 1981, covering articles of techno-commercial importance, national and international news related to power sector. It is the only trade journal certified by Audit Bureau of Circulation (ABC), with the circulation of 10,000 copies every month.\n\nA directory of members, ‘IEEMA Directory’ containing information about its members and the industry.\n\nELECRAMA is a biennial exhibition for power, electrical, and industrial electronics and allied products organized by IEEMA. It is one of the largest international exhibition of electrical and industrial electronics industry in the world. \n\nIEEMA organizes training programs catering to the needs of the Indian industry.\n\n"}
{"id": "50666329", "url": "https://en.wikipedia.org/wiki?curid=50666329", "title": "Jenny's Journeys", "text": "Jenny's Journeys\n\nJenny's Journeys is a single-player educational video game by Minnesota Educational Computing Consortium (MECC) which was released on the Apple II in 1984. The game is intended for users age 10 and up. In the game, players use a map and a compass to navigate from Jenny's car to her destination.\n\nThis game was part of a group of video game titles with which MECC entered the \"home software market\" in the mid-1980s. The purpose of \"Jenny's Journeys\" is to provide a real-world context for children to apply their map-reading skills. In InCider, it is explained that this type of \"role-play simulation\" video game specializes in teaching kids how to follow instructions. \n\nThe main character is a woman named Aunt Jenny. She needs help on a trip to Lake City. Players control a car and direct where the car will travel to next, allowing them to reach the destination successfully. The game has three levels of difficulty.\n\nThe \"Jenny's Journeys\" video game received generally favorable reviews from users, with praise given for the game's educational content in terms of geography and orientation, as well as for the realistic-for-the-time computer simulations on screen.\n\nRegarding the educational content, \"Curriculum Review\" said the game has a \"sound educational purpose and efficient method of operation.\" \"Creative Computing\" wrote that the game \"reinforces map reading and cognitive skills.\" The game was used in schools to teach children about maps and compasses. In \"Computers, Thinking, and Social Studies,\" Gene Rooze wrote that the game is appropriate for social studies classes. Meanwhile, \"Educational Resources for Microcomputers\" found that the game \"makes good use of the computer's ability to create simulations.\"\n\n\"Jenny's Journeys\" ranked 7th in “Top Ten” Software Products for 1984–85 School Year (units sold) by that specific publisher.\n\n\n"}
{"id": "7380112", "url": "https://en.wikipedia.org/wiki?curid=7380112", "title": "Laser video display", "text": "Laser video display\n\nLaser color television ( laser TV), or laser color video display utilizes two or more individually modulated optical (laser) rays of different colors to produce a combined spot that is scanned and projected across the image plane by a polygon-mirror system or less effectively by optoelectronic means to produce a color-television display. The systems work either by scanning the entire picture a dot at a time and modulating the laser directly at high frequency, much like the electron beams in a cathode ray tube, or by optically spreading and then modulating the laser and scanning a line at a time, the line itself being modulated in much the same way as with digital light processing (DLP).\n\nThe special case of one ray reduces the system to a monochromatic display as, for example, in black-and-white television. This principle applies to a display as well as to a (front or rear) projection technique with lasers (a laser video projector).\n\nThe laser source for television or video display was originally proposed by Helmut K.V. Lotsch in the German Patent 1 193 844. In December 1977 H.K.V. Lotsch and F. Schroeter explained laser color television for conventional as well as projection-type systems and gave examples of potential applications. 18 years later the German-based company Schneider AG presented a functional laser-TV prototype at IFA'95 in Berlin/Germany. Due to bankruptcy of Schneider AG, however, the prototype was never developed further to a market-ready product.\n\nProposed in 1966, laser illumination technology remained too costly to be used in commercially viable consumer products.\nAt the Las Vegas Consumer Electronics Show in 2006, Novalux Inc., developer of Necsel semiconductor laser technology, demonstrated their laser illumination source for projection displays and a prototype rear-projection \"laser\" TV.\nFirst reports on the development of a commercial Laser TV were published as early as February 16, 2006 with a decision on the large-scale availability of laser televisions expected by early 2008.\nOn January 7, 2008, at an event associated with the Consumer Electronics Show 2008, Mitsubishi Digital Electronics America, a key player in high-performance red-laser\nand large-screen HDTV markets, unveiled their first commercial Laser TV, a 65\" 1080p model.\nA \"Popular Science\" writer was impressed by the color rendering of a Mitsubishi laser video display at CES 2008.\nSome even described it as being too intense to the point of seeming artificial.\nThislLaser TV, branded \"Mitsubishi LaserVue TV\", went on sale, November 16, 2008 for $6,999, but Mitsubishi's entire laser TV project was killed in 2012.\n\nLG introduced a front projected laser TV in 2013\nas a consumer product that displays images and videos measuring 100 inches (254 centimeters) with a full high-definition resolution of 1920 x 1080 pixels. It can project images onto the screen at a distance of 22 inches (56 centimeters).\n\nLasers may become an ideal replacement for the UHP lamps which are currently in use in projection display devices such as rear projection TV and front projectors. LG claims a lifetime of 25,000 hours for their laser projector, compared to 10,000 hours for a UHP.\nCurrent televisions are capable of displaying only 40% of the color gamut that humans can potentially perceive.\n\nA Laser TV requires lasers in three distinct wavelengths—red, green, and blue. While red laser diodes are commercially available, there are no commercially available green laser diodes which can provide the required power at room temperature with an adequate lifetime. Instead, frequency doubling can be used to provide the green wavelengths. Several types of lasers can be used as the frequency doubled sources: fibre lasers, inter-cavity doubled lasers, external cavity doubled lasers, eVCSELs, and OPSLs (Optically Pumped Semiconductor Lasers). Among the inter-cavity doubled lasers, VCSELs have shown much promise and potential to be the basis for a mass-produced frequency doubled laser.\n\nThe blue laser diodes became openly available around 2010.\n\nA VECSEL is a vertical cavity, and is composed of two mirrors. On top of one of them is a diode as the active medium. These lasers combine high overall efficiency with good beam quality. The light from the high power IR-laser diodes is converted into visible light by means of extra-cavity waveguided second harmonic generation. Laser-pulses with about 10 kHz repetition rate and various lengths are sent to a digital micromirror device where each mirror directs the pulse either onto screen or into the dump. Because the wavelengths are known all coatings can be optimized to reduce reflections and therefore speckle.\n\nThe video signal is introduced to the laser beam by an acousto-optic modulator (AOM) that uses a photorefractive crystal to separate the beam at distinct diffraction angles. The beam must enter the crystal at the specific Bragg angle of that AOM crystal. A piezoelectric element transforms the video signal into vibrations in the crystal to create an image.\n\nA rapidly rotating polygonal mirror gives the laser beam the horizontal refresh modulation. It reflects off of a curved mirror onto a galvanometer-mounted mirror which provides the vertical refresh. Another way is to optically spread the beam and modulate each entire line at once, much like in a DLP, reducing the peak power needed in the laser and keeping power consumption constant.\n\n\nThere are several realizations of laser projectors, one example being based on the principle of a flying light spot writing the image directly onto a screen. A laser projector of this type consists of three main components — a laser source uses the video signal to provide modulated light composed of the three sharp spectral colors — red, green, and blue — which a flexible, fiber-optic waveguide then transports to a relatively small projection head. The projection head deflects the beam according to the pixel clock and emits it onto a screen at an arbitrary distance. Such laser projection techniques are used in handheld projectors, planetariums, and for flight simulators and other virtual reality applications.\n\nDue to the special features of laser projectors, such as a high depth of field, it is possible to project images or data onto any kind of projection surface, even non-flat. Typically, the sharpness, color space, and contrast ratio are higher than those of other projection technologies. For example, the on-off contrast of a laser projector is typically 50,000:1 and higher, while modern DLP and LCD projectors range from 1000:1 to 40,000:1. In comparison to conventional projectors, laser projectors provide a lower luminous flux output, but because of the extremely high contrast the brightness actually appears to be greater.\n"}
{"id": "18294711", "url": "https://en.wikipedia.org/wiki?curid=18294711", "title": "Leah Jamieson", "text": "Leah Jamieson\n\nLeah H. Jamieson (born August 27, 1949, in Trenton, NJ, USA) is an American engineering educator serving at present as the John A. Edwardson Dean of Engineering and Ransburg Distinguished Professor of Electrical and Computer Engineering at Purdue University. She is a member of the US National Academy of Engineering and served as the 2007 President and CEO of the Institute of Electrical and Electronics Engineers (IEEE).\n\nJamieson was a founder of the Engineering Projects in Community Service program (EPICS), a multi-university engineering design program that operates in a service-learning context. She is a recipient of the Gordon Prize.\n\nJamieson was born in 1949 and grew up in New Jersey, USA. She received the B.S. degree in mathematics in 1972 from the Massachusetts Institute of Technology (MIT). She received M.A. and M.S.E. degrees in 1974 and a Ph.D. in 1977, all three from Princeton University.\n\nJamieson worked as Professor of Engineering at Purdue University since 1976. Her research interests include speech analysis and recognition; the design and analysis of parallel processing algorithms; and the application of parallel processing to the areas of digital speech, image, and signal processing. She has authored over 160 journal and conference papers in these areas and has co-edited books on algorithmically specialized parallel computers (Academic Press, 1985) and the characteristics of parallel algorithms (MIT Press,1987). She served Purdue as Director of the Graduate Program in Electrical Engineering (1990–94), Director of Graduate Admissions (1994–96), Interim Head of the School of Electrical and Computer Engineering (2002), and Associate Dean of Engineering for Undergraduate Education (2004–06). At present she is serving as the John A. Edwardson Dean of Engineering and Ransburg Distinguished Professor of Electrical and Computer Engineering.\n\nIn Fall 1995 Jamieson and her Purdue colleague Edward J. Coyle founded \"Engineering Projects in Community Service\", an academic engineering design program that operates in a service-learning context.\nThe program, which was initially offered only at Purdue, is available at present in 18 universities. It offers students from multiple disciplines with the opportunity to be part of engineering project design teams that work with nonprofit community organizations. The teams provide technological solutions to challenges faced by the community organizations and their target audiences. Examples of EPICS projects include the \"Spanish In Action\" Project at Butler University, which provides students with a web-based computer game that helps them learn Spanish vocabulary; and the \"Sensor Network Air Pollution Monitoring\" project at Drexel University that allows measurement of diesel particulate concentration in Philadelphia’s neighborhoods. For founding and administering EPICS, Jamieson has received in 2005 the Bernard M. Gordon Prize for Innovation in Engineering and Technology (with colleagues Edward J. Coyle and William C. Oakes). \nIn 2008 the EPICS program has announced \"EPICS High\", an extension of the program’s scope to integrate high school students in the design teams. \n\nIn November 2005 Jamieson was elected 2006 President-elect by members of the Institute of Electrical and Electronics Engineers (IEEE). IEEE the world's largest technical professional society, focused on electrical engineering, computer engineering, computer science and the related arts and sciences. The other candidates in the 2005 elections were Gerald Peterson and James M. Tien.\n\nJamieson served as President and CEO of IEEE in 2007. Her presidency was characterized by a strong effort to redefine and expand the strategic planning process within IEEE, and to start an IEEE public visibility program. Other notable developments during her presidency included expansion of IEEE’s pre-university engineering education programs and reorganization of the IEEE Regional Activities Board (renamed Member and Geographical Activities Board). \n\n\n"}
{"id": "33452337", "url": "https://en.wikipedia.org/wiki?curid=33452337", "title": "Levelling stone", "text": "Levelling stone\n\nA Levelling stone is an extremely hard, flat and abrasive stone used to flatten or level the surface of a whetstone which has become rounded, due to sharpening of blades.\n"}
{"id": "12692092", "url": "https://en.wikipedia.org/wiki?curid=12692092", "title": "List of UMTS networks", "text": "List of UMTS networks\n\nThe following is a list of mobile telecommunications networks using third-generation Universal Mobile Telecommunications System (UMTS) technology. This list does not aim to cover all networks, but instead focuses on networks deployed on frequencies other than 2100 MHz which is commonly deployed around the globe and on Multiband deployments.\n\n\n"}
{"id": "8663236", "url": "https://en.wikipedia.org/wiki?curid=8663236", "title": "Low Flight", "text": "Low Flight\n\nAs \"High Flight\" is for fixed-wing pilots, Low Flight is the poem for helicopter pilots. Written in 1972, it is the unofficial poem of Army Aviation and USMC helicopter pilots. \"Low Flight\" reverses the perspective from the soaring airplane pilot to the 50 foot AGL helicopter pilot. Its author is unknown.\n\n\nOh, I've slipped the surly bonds of earth<br>\nAnd hovered out of ground effect on semi-rigid blades;<br>\nEarthward I've auto'ed and met the rising brush of\nnon-paved terrain<br>\nAnd done a thousand things you would never care to<br>\nSkidded and dropped and flared<br>\nLow in the heat soaked roar.<br>\nConfined there, I've chased the earthbound traffic<br>\nAnd lost the race to insignificant\nheadwinds;<br>\nForward and up a little in ground effect<br>\nI've topped the General's hedge with drooping turns<br>\nWhere never Skyhawk or even Phantom flew.<br>\nShaking and pulling collective,<br> I've lumbered\nThe low untresspassed halls of victor airways,<br>\nPut out my hand and touched a tree.\n\n—Anonymous\n\nAnother version, dedicated to those who fly close air support for ground forces - REALLY close air, the following is from the prospective of flying 50 feet AGL at 400 KIAS (knots indicated airspeed).\n\n\nOh, I've slipped the swirling clouds of dust<br>\nA few feet from the dirt.<br>\nI have flown my plane low enough<br>\nTo make my bottom hurt.<br>\nI've raced over desert, hills, through valleys,<br>\nAnd mountain passes too.<br>\nFrolicked in the trees<br>\nWhere only gray squirrels flew.\n\nChasing cows along the way,<br>\nDisturbing ram and ewe.<br>\nI've done a hundred other things<br>\nYou damned well shouldn't do.<br>\nI have smacked the tiny sparrow,<br>\nBluebird, robin, and the rest.<br>\nDragged vorticies through branches<br>\nThrowing eggs out of their nests.\n\nI've hurled through total darkness<br>\nJust as blind as I could be,<br>\nAnd spent the night in terror<br>\nOf things I could not see.<br>\nI've turned my eyes to heaven<br>\nSweating bullets through the flight,<br>\nReached out my hand and pressed-to-test<br>\n—the Master Caution light.\n\n\"Low Flight\" \n\nOh, I have slipped through<br>\nswirling clouds of dust, <br>\nA few feet from the dirt,<br>\nI've flown the C-130 low enough,<br>\nTo make my bottom hurt.\n\nI've flown in the desert, hills and valleys,<br>\nMountains too, <br>\nFrolicked in the trees, <br>\nWhere only flying squirrels flew. \n\nChased the frightened cows along, <br>\nDisturbed the ram and ewe, <br>\nAnd done a hundred other things <br>\nThat you'd not care to do. \n\nI've smacked the tiny sparrow, <br>\nBluebird, robin, all the rest, <br>\nI've ingested baby eagles, <br>\nSimply sucked them from their nest. \n\nI've streaked through total darkness, <br>\nJust the other guys and me, <br>\nAnd spent the night in terror of <br>\nThings I could not see. \n\nI turned my eyes to heaven, <br>\nAs I sweated through the flight, <br>\nPut out my hand and touched, <br>\nThe Fire Warning Light. \n\nAuthor unknown\n"}
{"id": "21468426", "url": "https://en.wikipedia.org/wiki?curid=21468426", "title": "Mapping controversies", "text": "Mapping controversies\n\nMapping controversies (MC) is an academic course taught in science studies, stemming from the writings of the French sociologist and philosopher Bruno Latour. MC focuses exclusively on the controversies surrounding scientific knowledge rather than the established scientific facts or outcomes. Thus, it helps sociologists, anthropologists and other social scientists get insights not into scientific knowledge \"per se\", but rather into \"the process of gaining knowledge\". Thus, MC sheds light on those intermediate stages corresponding to the actual research process and pinpoints the connections between scientific work and other types of activities.\n\nThe term \"mapping controversies\" was first suggested in relation to analysis of scientific and technological controversies, and then lately re-affirmed as a widely applicable methodological approach going beyond the boundaries of Science Studies. It is usually used for the methodology that identifies and tracks down the polemics or debate surrounding a scientific fact, and utilises various visualisation tools to present the problem in its complexity.\n\nRecently Latour initiated the project \"Mapping Controversies on Science for Politics (MACOSPOL)\". The showcase website is mappingcontroversies.net \n\nIn 2008-2009 several universities in Europe and USA started teaching \"Mapping Controversies\" courses for students in political sciences, engineering, and architecture.\n\nAn earlier attempt to stage controversies in museum settings took place at the Gallery of Research in Vienna in 2005.\n"}
{"id": "2833400", "url": "https://en.wikipedia.org/wiki?curid=2833400", "title": "Markman v. Westview Instruments, Inc.", "text": "Markman v. Westview Instruments, Inc.\n\nMarkman v. Westview Instruments, Inc., 517 U.S. 370 (1996), is a United States Supreme Court case on whether the interpretation of patent claims is a matter of law or a question of fact. An issue designated as a matter of law is resolved by the judge, and an issue construed as a question of fact is determined by the jury. \n\nHerbert Markman patented a system to track clothes through the dry cleaning process using barcode to generate receipts and track inventory.\n\nThe 7th Amendment guarantees the right to a jury trial in patent infringement cases. The 7th Amendment preserves the right to jury trial as it existed in 1791. There is no dispute that infringement cases today must be tried by a jury as their predecessors were in 1791. However, the court held that the construction of the patent, including the terms of art within its claim, is exclusively within the province of the court. \n\nIn general, the effectiveness of a particular patent depends on its potential at blocking competitors. The key for a patent holder is getting the proper definition of words used in the patent to allow blocking of the particular troublesome competitive product. Prior to this decision, juries had the responsibility of deciding what the words used in patent claims meant. Opposing results in cases with similar facts were common, and a perception arose that the outcome of such trials was somewhat arbitrary. In \"Markman\", the Court held that judges, not juries, would evaluate and decide the meaning of the words used in patent claims. Judges were to look at four sources for definitions in order of priority: \n\nThis case has had a significant impact on the patent litigation process in the United States. Many jurisdictions now hold Markman hearings to construe patent claims prior to the start of the actual trial. Patent infringement suits now often settle after this stage of the litigation process.\n\nIn a unanimous ruling written by Justice David Souter, the court affirmed the judgment of the circuit court, holding that:\n\nMarkman was represented in the original trial by the law firm of Duane Morris, and by the law firm of Eckert Seamans on appeal. Defendants were represented by the law firm of Gollatz, Griffin, Ewing & McCarthy (now Flaster Greenberg) on appeal.\n\n"}
{"id": "45395473", "url": "https://en.wikipedia.org/wiki?curid=45395473", "title": "Media intelligence", "text": "Media intelligence\n\nMedia Intelligence uses data mining and data science to analyze public social and editorial media content. It refers to marketing systems that synthesize billions of online conversations into relevant information that allow organizations to measure and manage content performance, understand trends, and drive communications and business strategy.\n\nMedia intelligence can include software as a service using big data terminology. This includes questions about messaging efficiency, share of voice, audience geographical distribution, message amplification, influencer strategy, journalist outreach, creative resonance, and competitor performance in all these areas.\n\nMedia intelligence differs from business intelligence in that it uses and analyzes data outside company firewalls. Examples of that data are user-generated content on social media sites, blogs, comment fields, and wikis etc. It may also include other public data sources like press releases, news, blogs, legal filings, reviews and job postings.\n\nMedia Intelligence may also include competitive intelligence, wherein information that is gathered from publicly available sources such as social media, press releases, and news announcements are used to better understand the strategies and tactics being deployed by competing businesses. \n\nMedia Intelligence is enhanced by means of emerging technologies like semantic tagging, Natural Language Processing, sentiment analysis and machine translation. \n\nDifferent media intelligence platforms use different technologies for monitoring, curating content, engaging with content, data analysis and measurement of communications and marketing campaign success. These technology providers, such as BuzzCovery Meltwater, Synoptos, Radian 6, or Sysomos may obtain content by scraping content directly from websites or by connecting to the API provided by social media or other content platforms that are created for 3rd party developers to develop their own applications and services that access data. Facebook's Graph API is one such API that social media monitoring solution products would connect to pull data from. Technology companies may also get data from a data reseller, such as DataSift (acquired by Meltwater), Gnip (acquired by Twitter), LexisNexis, or Dow Jones/Factiva.\n\nSome social media monitoring and analytics companies use calls to data providers each time an end-user develops a query. Others archive and index social media posts to provide end users with on-demand access to historical data and enable methodologies and technologies leveraging network and relational data. Additional monitoring companies use crawlers and spidering technology to find keyword references, known as semantic analysis or natural language processing. Basic implementation involves curating data from social media on a large scale and analyzing the results to make sense out of it.\n\n"}
{"id": "39520963", "url": "https://en.wikipedia.org/wiki?curid=39520963", "title": "National Interagency Confederation for Biological Research", "text": "National Interagency Confederation for Biological Research\n\nThe National Interagency Confederation for Biological Research (NICBR, pronounced \"Nick Burr\") is a biotechnology and biodefense partnership and collaborative environment of eight U.S. Federal government agencies at Fort Detrick, Maryland, USA. The goals of the NICBR, created in 200?, are to enhance mission effectiveness of mission partners, to optimize NICBR as a sustainable community of scientific excellence while enhancing public trust and support, and to optimize governance to enhance performance. The NICBR Vision is \"Federal partners working in synergy to achieve a healthier and more secure nation.\" In support of these goals, subcommittees and working groups foster interagency collaboration, maximize safety and productivity of biological research and technology development, and minimize duplication of effort, technology, and facilities among the signatories.\n\nFour federal cabinet level departments are represented in the NICBR: The DoD (2 agencies), the DHHS (4), the DHS (1) and the USDA (1). The NICBR agencies are:\n\nSome NICBR laboratories are physically consolidated on the National Interagency Biodefense Campus (NIBC) which includes all NICBR partners except NCI, which maintains its own campus on the Rosemont Avenue side of Fort Detrick. The Fort Detrick Interagency Coordinating Committee (FDICC) is the central hub of the NICBR governance structure, which is chaired by the Fort Detrick U.S. Army garrison commander. It is composed of all NICBR partner representatives.\n\nThe FDICC meets twice a month and reports to the Executive Steering Committee (ESC), which is chaired by the commander, U. S. Army Medical Research and Materiel Command and is composed of equivalents across the partner agencies. The ESC reports to the NICBR Board of Directors (BOD), consisting of the chair, currently Army Surgeon General, and her equivalents across the partnership. Collectively, the NICBR governance bodies provide strategic direction and oversight to ensure that the NICBR Mission, \"to develop unique knowledge, tools, and products by leveraging advanced technologies and innovative discoveries to secure and defend the health of the American people,\" is carried out. Members of the NICBR embrace the core philosophy of \"Trust and Teamwork”.\n\nReporting to the FDICC are seven subcommittees and three working groups:\n\n\nThe NICBR Partnership Office (NPO) provides a staffing function and coordinating center for and under the direction of the FDICC. The NPO works closely with the subcommittees and working groups to facilitate execution of their individual charters and action items handed down by the governance bodies.\n"}
{"id": "27141128", "url": "https://en.wikipedia.org/wiki?curid=27141128", "title": "Noise-figure meter", "text": "Noise-figure meter\n\nA noise figure meter is an instrument for measuring the noise figure of an amplifier, mixer, or similar device. An example instrument is the 1983-era Agilent 8970A.\n\nOne way to perform the measurement is described on the Y-factor page. A noise-figure meter could automate that procedure as follows: A gated broadband noise source (such as an avalanche diode) drives the device under test. A measurement is made with the noise source on; another measurement with the noise source off. From those measurements and the characteristics of the noise source, the noise figure can be calculated.\n\nSome noise figure meters need a calibrated broadband noise source -- a noise generator. Several methods are used to generate broadband noise. Some methods require two sources: a \"hot\" and \"cold\" source. For high frequency measurements, the noise source will be embedded in a transmission line.\n\nNoise (electronics)#Thermal noise\n\nThermal noise in a resistor. Resistor in liquid nitrogen. Resistor in boiling water.\n\nNoise (electronics)#Shot noise\n\nElectrons crossing a gap make discrete arrivals. Impulse. White noise. Compare to thermal electrons.\n\nRandom noise generators can be made from temperature-limited vacuum tube diodes. The vacuum tube's anode (plate) is high enough to collect all the electrons emitted from the hot cathode. The operating conditions are set to avoid a space charge around the filament/cathode that would affect the electron emission. The anode current exhibits shot noise.\n\nThe noise current is set by the filament temperature. The current is an exponential function of filament temperature.\n\nAt low frequencies, there is 1/\"f\" noise. At high frequencies, the transit time of the electron becomes an issue.\n\nVoltage breakdown diodes are often used as noise generators. There are two breakdown mechanisms: Zener and avalanche. Diodes with the corresponding effects are known as Zener diodes and avalanche diodes. The two mechanisms have different noise behaviors.\n\nThe Zener effect (or internal field emission effect) dominates below 7 volts. The junction is thin, and the electric field is large enough that electrons jump the energy gap. The primary noise is shot noise. There is little other noise (excess noise).\n\nAvalanche breakdown is noisier. A carrier traversing the semiconductor junction is accelerated by the reverse-bias field, and it can generate new electron-hole pairs in a collision. Those new carriers can also generate more carriers in a subsequent collisions. The carriers don't arrive singly but rather in bunches. The result is avalanche multiplication of what would have been just shot noise. The spectrum, like shot noise, is white.\n\nAvalanche breakdown can also exhibit multi-state noise. The generated output noise appears to switch between two or more distinct levels. This noise has a 1/\"f\" characteristic. The effect can be minimized.\n\nSome commercial microwave noise generators use avalanche diodes to create a large excess noise figure that can be turned off and on. The impedance of the diode is different during the two states, so an output attenuator is used. The attenuator reduces the noise source output, but it minimizes mismatch loss. \n\n\n\nAlso\n"}
{"id": "30037240", "url": "https://en.wikipedia.org/wiki?curid=30037240", "title": "OnGreen", "text": "OnGreen\n\nOnGreen is an online social and business platform committed to cleantech technologies. Entrepreneurs post plans to get connections, feedback and investment. Companies and schools can post intellectual property on the site in the form of patents that can be licensed or sold.\n\nOnGreen, originally called Clean Green Guy, was founded in 2009 by Nikhil Jain. It was officially launched in March 2010 by Nikhil Jain and received majority seed funding in an amount of $1.4 million (US) in November 2010 from Shanghai-based Southern Hong Kong Investment Ltd. Shortly after the funding announcement, OnGreen launched its Patent Exchange.\n\nOnGreen was originally based in West Los Angeles, California but has since relocated to Pasadena, California. Entrepreneurs post their deals on the Deal Marketplace. OnGreen also allows governmental laboratories, companies, and schools to post intellectual property on the site in the form of patents that can be licensed or sold. These patents are posted on the Patent Exchange. There is also an Expert Panel, which consists of confirmed PhDs available to advise entrepreneurs or investors on the site. The site also includes a green job board and online green technology forums.\n\n\nMembership is free. Approximately 90% of the members are in the United States with the remainder of members stemming from 145 countries.\n"}
{"id": "11234274", "url": "https://en.wikipedia.org/wiki?curid=11234274", "title": "Optical fiber cable", "text": "Optical fiber cable\n\nAn optical fiber cable, also known as a fiber optic cable, is an assembly similar to an electrical cable, but containing one or more optical fibers that are used to carry light. The optical fiber elements are typically individually coated with plastic layers and contained in a protective tube suitable for the environment where the cable will be deployed. Different types of cable are used for different applications, for example long distance telecommunication, or providing a high-speed data connection between different parts of a building.\n\nOptical fiber consists of a core and a cladding layer, selected for total internal reflection due to the difference in the refractive index between the two. In practical fibers, the cladding is usually coated with a layer of acrylate polymer or polyimide. This coating protects the fiber from damage but does not contribute to its optical waveguide properties. Individual coated fibers (or fibers formed into ribbons or bundles) then have a tough resin buffer layer or core tube(s) extruded around them to form the cable core. Several layers of protective sheathing, depending on the application, are added to form the cable. Rigid fiber assemblies sometimes put light-absorbing (\"dark\") glass between the fibers, to prevent light that leaks out of one fiber from entering another. This reduces cross-talk between the fibers, or reduces flare in fiber bundle imaging applications.\n\nFor indoor applications, the jacketed fiber is generally enclosed, with a bundle of flexible fibrous polymer \"strength members\" like aramid (e.g. Twaron or Kevlar), in a lightweight plastic cover to form a simple cable. Each end of the cable may be terminated with a specialized optical fiber connector to allow it to be easily connected and disconnected from transmitting and receiving equipment. \n\nFor use in more strenuous environments, a much more robust cable construction is required. In \"loose-tube construction\" the fiber is laid helically into semi-rigid tubes, allowing the cable to stretch without stretching the fiber itself. This protects the fiber from tension during laying and due to temperature changes. Loose-tube fiber may be \"dry block\" or gel-filled. Dry block offers less protection to the fibers than gel-filled, but costs considerably less. Instead of a loose tube, the fiber may be embedded in a heavy polymer jacket, commonly called \"tight buffer\" construction. Tight buffer cables are offered for a variety of applications, but the two most common are \"Breakout\" and \"Distribution\". Breakout cables normally contain a ripcord, two non-conductive dielectric strengthening members (normally a glass rod epoxy), an aramid yarn, and 3 mm buffer tubing with an additional layer of Kevlar surrounding each fiber. The ripcord is a parallel cord of strong yarn that is situated under the jacket(s) of the cable for jacket removal. Distribution cables have an overall Kevlar wrapping, a ripcord, and a 900 micrometer buffer coating surrounding each fiber. These \"fiber units\" are commonly bundled with additional steel strength members, again with a helical twist to allow for stretching.\n\nA critical concern in outdoor cabling is to protect the fiber from contamination by water. This is accomplished by use of solid barriers such as copper tubes, and water-repellent jelly or water-absorbing powder surrounding the fiber.\n\nFinally, the cable may be armored to protect it from environmental hazards, such as construction work or gnawing animals. Undersea cables are more heavily armored in their near-shore portions to protect them from boat anchors, fishing gear, and even sharks, which may be attracted to the electrical power that is carried to power amplifiers or repeaters in the cable.\n\nModern cables come in a wide variety of sheathings and armor, designed for applications such as direct burial in trenches, dual use as power lines, installation in conduit, lashing to aerial telephone poles, submarine installation, and insertion in paved streets.\n\nIn September 2012, NTT Japan demonstrated a single fiber cable that was able to transfer 1 petabit per second () over a distance of 50 kilometers.\n\nModern fiber cables can contain up to a thousand fibers in a single cable, with potential bandwidth in the terabytes per second. In some cases, only a small fraction of the fibers in a cable may be actually \"lit\". Companies can lease or sell the unused fiber to other providers who are looking for service in or through an area. Companies may \"overbuild\" their networks for the specific purpose of having a large network of dark fiber for sale, reducing the overall need for trenching and municipal permitting. They may also deliberately under-invest to prevent their rivals from profiting from their investment.\n\nThe highest strand-count singlemode fiber cable commonly manufactured is the 864-count, consisting of 36 ribbons each containing 24 strands of fiber.\n\nOptical fibers are very strong, but the strength is drastically reduced by unavoidable microscopic surface flaws inherent in the manufacturing process. The initial fiber strength, as well as its change with time, must be considered relative to the stress imposed on the fiber during handling, cabling, and installation for a given set of environmental conditions. There are three basic scenarios that can lead to strength degradation and failure by inducing flaw growth: dynamic fatigue, static fatigues, and zero-stress aging.\n\nTelcordia GR-20, \"Generic Requirements for Optical Fiber and Optical Fiber Cable\", contains reliability and quality criteria to protect optical fiber in all operating conditions. The criteria concentrate on conditions in an outside plant (OSP) environment. For the indoor plant, similar criteria are in Telcordia GR-409, \"Generic Requirements for Indoor Fiber Optic Cable\".\n\n\nThe jacket material is application specific. The material determines the mechanical robustness, chemical and UV radiation resistance, and so on. Some common jacket materials are LSZH, polyvinyl chloride, polyethylene, polyurethane, polybutylene terephthalate, and polyamide.\n\nThere are two main types of material used for optical fibers: glass and plastic. They offer widely different characteristics and find uses in very different applications. Generally, plastic fiber is used for very short range and consumer applications, glass fiber is used for short/medium range (multi-mode) and long range (single-mode) telecommunications.\n\nThe buffer or jacket on patchcords is often color-coded to indicate the type of fiber used. The strain relief \"boot\" that protects the fiber from bending at a connector is color-coded to indicate the type of connection. Connectors with a plastic shell (such as SC connectors) typically use a color-coded shell. Standard color codings for jackets (or buffers) and boots (or connector shells) are shown below:\n\nRemark: It is also possible that a small part of a connector is additionally color-coded, e.g. the lever of an E-2000 connector or a frame of an adapter. This additional colour coding indicates the correct port for a patchcord, if many patchcords are installed at one point.\n\nIndividual fibers in a multi-fiber cable are often distinguished from one another by color-coded jackets or buffers on each fiber. The identification scheme used by Corning Cable Systems is based on EIA/TIA-598, \"Optical Fiber Cable Color Coding.\" EIA/TIA-598 defines identification schemes for fibers, buffered fibers, fiber units, and groups of fiber units within outside plant and premises optical fiber cables. This standard allows for fiber units to be identified by means of a printed legend. This method can be used for identification of fiber ribbons and fiber subunits. The legend will contain a corresponding printed numerical position number or color for use in identification.\nThe colour code used above resembles PE copper cables used in standard telephone wiring.\n\nIn the UK the colour codes for COF200 and 201 are different. Each 12 fibre bundle or element within a Cable Optical Fibre 200/201 cable is coloured as follows:\nEach element is in a tube within the cable (not a blown fibre tube) The cable elements start with the red tube and are counted around the cable to the green tube. Active elements are in white tubes and yellow fillers or dummies are laid in the cable to fill it out depending on how many fibres and units exists – can be up to 276 fibres or 23 elements for external cable and 144 fibres or 12 elements for internal. The cable has a central strength member normally made from fiberglass or plastic. There is also a copper conductor in external cables.\n\nOptical cables transfer data at the speed of light in glass. This is the speed of light in vacuum divided by the refractive index of the glass used, typically around 180,000 to 200,000 km/s, resulting in 5.0 to 5.5 microseconds of latency per km. Thus the round-trip delay time for 1000 km is around 11 milliseconds.\n\nTypical modern multimode graded-index fibers have 3 dB/km of attenuation loss (50% loss per km) at a wavelength of 850 nm, and 1 dB/km at 1300 nm. Singlemode 9/125 loses 0.4 dB/km at 1310 nm and 0.25 dB/km at 1550 nm. Very high quality singlemode fiber intended for long distance applications is specified at a loss of 0.19 dB/km at 1550 nm. POF (plastic optical fiber) loses much more: 1 dB/m at 650 nm. Plastic optical fiber is large core (about 1 mm) fiber suitable only for short, low speed networks such as within cars.\n\nEach connection made adds about 0.6 dB of average loss, and each joint (splice) adds about 0.1 dB. Depending on the transmitter power and the sensitivity of the receiver, if the total loss is too large the link will not function reliably.\n\nInvisible infrared light is used in commercial glass fiber communications because it has lower attenuation in such materials than visible light. However, the glass fibers will transmit visible light somewhat, which is convenient for simple testing of the fibers without requiring expensive equipment. Splices can be inspected visually, and adjusted for minimal light leakage at the joint, which maximizes light transmission between the ends of the fibers being joined.\n\nThe charts at \"Understanding wavelengths In fiber optics\" and \"Optical power loss (attenuation) in fiber\" illustrate the relationship of visible light to the infrared frequencies used, and show the absorption water bands between 850, 1300 and 1550 nm.\n\nThe infrared light used in telecommunications cannot be seen, so there is a potential laser safety hazard to technicians. The eye's natural defense against sudden exposure to bright light is the blink reflex, which is not triggered by infrared sources. In some cases the power levels are high enough to damage eyes, particularly when lenses or microscopes are used to inspect fibers that are emitting invisible infrared light. Inspection microscopes with optical safety filters are available to guard against this. More recently indirect viewing aids are used, which can comprise a camera mounted within a handheld device, which has an opening for the connectorized fiber and a USB output for connection to a display device such as a laptop. This makes the activity of looking for damage or dirt on the connector face much safer.\n\nSmall glass fragments can also be a problem if they get under someone's skin, so care is needed to ensure that fragments produced when cleaving fiber are properly collected and disposed of appropriately.\n\nThere are hybrid optical and electrical cables that are used in wireless outdoor Fiber To The Antenna (FTTA) applications. In these cables, the optical fibers carry information, and the electrical conductors are used to transmit power. These cables can be placed in several environments to serve antennas mounted on poles, towers, and other structures.\n\nAccording to Telcordia GR-3173, \"Generic Requirements for Hybrid Optical and Electrical Cables for Use in Wireless Outdoor Fiber To The Antenna (FTTA) Applications,\" these hybrid cables have optical fibers, twisted pair/quad elements, coaxial cables or current-carrying electrical conductors under a common outer jacket. The power conductors used in these hybrid cables are for directly powering an antenna or for powering tower-mounted electronics exclusively serving an antenna. They have a nominal voltage normally less than 60 VDC or 108/120 VAC. Other voltages may be present depending on the application and the relevant National Electrical Code (NEC).\n\nThese types of hybrid cables may also be useful in other environments such as Distributed Antenna System (DAS) plants where they will serve antennas in indoor, outdoor, and roof-top locations. Considerations such as fire resistance, Nationally Recognized Testing Laboratory (NRTL) Listings, placement in vertical shafts, and other performance-related issues need to be fully addressed for these environments.\n\nSince the voltage levels and power levels used within these hybrid cables vary, electrical safety codes consider the hybrid cable to be a power cable, which needs to comply with rules on clearance, separation, etc.\n\nInnerducts are installed in existing underground conduit systems to provide clean, continuous, low-friction paths for placing optical cables that have relatively low pulling tension limits. They provide a means for subdividing conventional conduit that was originally designed for single, large-diameter metallic conductor cables into multiple channels for smaller optical cables.\n\nInnerducts are typically small-diameter, semi-flexible subducts. According to Telcordia GR-356, there are three basic types of innerduct: smoothwall, corrugated, and ribbed. These various designs are based on the profile of the inside and outside diameters of the innerduct. The need for a specific characteristic or combination of characteristics, such as pulling strength, flexibility, or the lowest coefficient of friction, dictates the type of innerduct required.\n\nBeyond the basic profiles or contours (smoothwall, corrugated, or ribbed), innerduct is also available in an increasing variety of multiduct designs. Multiduct may be either a composite unit consisting of up to four or six individual innerducts that are held together by some mechanical means, or a single extruded product having multiple channels through which to pull several cables. In either case, the multiduct is coilable, and can be pulled into existing conduit in a manner similar to that of conventional innerduct.\n\nInnerducts are primarily installed in underground conduit systems that provide connecting paths between manhole locations. In addition to placement in conduit, innerduct can be directly buried, or aerially installed by lashing the innerduct to a steel suspension strand.\n\nAs stated in GR-356, cable is typically placed into innerduct in one of three ways. It may be\n\n\n"}
{"id": "33319325", "url": "https://en.wikipedia.org/wiki?curid=33319325", "title": "PC/104 Consortium", "text": "PC/104 Consortium\n\nThe PC/104 Consortium was established in February 1992 by 12 companies with a common vision of adapting desktop computer technology for embedded applications. The consortium has since had a tremendous, positive effect on the embedded computer marketplace and now includes over 50 member companies. The PC/104 Consortium's technological philosophy is to support legacy technology while developing new solutions for the future. Longevity is a requirement for embedded systems and one of the hallmarks of PC/104 technology.\n\n"}
{"id": "1650372", "url": "https://en.wikipedia.org/wiki?curid=1650372", "title": "Palm kernel oil", "text": "Palm kernel oil\n\nPalm kernel oil is an edible plant oil derived from the kernel of the oil palm \"Elaeis guineensis\". It should not be confused with the other two edible oils derived from palm fruits: palm oil, extracted from the pulp of the oil palm fruit, and coconut oil, extracted from the kernel of the coconut.\n\nPalm kernel oil, palm oil, and coconut oil are three of the few highly saturated vegetable fats; these oils give the name to the 16-carbon saturated fatty acid palmitic acid that they contain.\n\nPalm kernel oil, which is semi-solid at room temperature, is more saturated than palm oil and comparable to coconut oil.\n\nOil from the African oil palm \"Elaeis guineensis\" has long been recognized in West African countries. European merchants trading with West Africa occasionally purchased palm oil for use in Europe, but palm kernel oil remained rare outside West Africa.\n\nThe USDA has published historical production figures for palm kernel oil for years beginning October 1 and ending September 30:\nIn the 1960s, research and development (R&D) in oil palm breeding began to expand after Malaysia's Department of Agriculture established an exchange program with West African economies and four private plantations formed the Oil Palm Genetics Laboratory. The Malaysian government also established Kolej Serdang, which became the Universiti Pertanian Malaysia (UPM) in the 1970s to train agricultural and agroindustrial engineers and agribusiness graduates to conduct research in the field.\n\nIn 1979 with support from the Malaysian Agricultural Research and Development Institute (MARDI) and UPM, the government set up the Palm Oil Research Institute of Malaysia (Porim), a public-and-private-coordinated institution. B.C. Sekhar was appointed founder and chairman. Porim's scientists work in oil palm tree breeding, palm oil nutrition and potential oleochemical use. Porim was renamed Malaysian Palm Oil Board in 2000.\n\nPalm kernel oil, similarly to coconut oil, is high in saturated fats and is more saturated than palm oil. Palm kernel oil is high in lauric acid which has been shown to raise blood cholesterol levels, both as LDL-C (cholesterol contained in low-density lipoprotein) and HDL-C (cholesterol contained in high-density lipoprotein). However, the raise in total cholesterol concentration is partly due to more HDL-C than LDL-C. Palm kernel oil does not contain cholesterol or trans fatty acids.\n\nPalm kernel oil is commonly used in commercial cooking because it is lower in cost than other oils and remains stable at high cooking temperatures. The oil can also be stored longer than other vegetable oils.\n\nThe approximate concentration of fatty acids (FAs) in palm kernel oil is as follows:\n\nSplitting of oils and fats by hydrolysis, or under basic conditions saponification, yields fatty acids, with glycerin (glycerol) as a byproduct. The split-off fatty acids are a mixture ranging from C4 to C18, depending on the type of oil or fat.\n\nResembling coconut oil, palm kernel oil is packed with myristic and lauric fatty acids and therefore suitable for the manufacture of soaps, washing powders and personal care products. Lauric acid is important in soap making: a good soap must contain at least 15 per cent laurate for quick lathering, while soap made for use in sea water is based on virtually 100 per cent laurate.\n\nDerivatives of palmitic acid were used in combination with naphtha during World War II to produce napalm (aluminum naphthenate and aluminum palmitate).\n\n\n"}
{"id": "26528143", "url": "https://en.wikipedia.org/wiki?curid=26528143", "title": "Parametron", "text": "Parametron\n\nParametron is a logic circuit element invented by Eiichi Goto in 1954. The parametron is essentially a resonant circuit with a nonlinear reactive element which oscillates at half the driving frequency. The oscillation can be made to represent a binary digit by the choice between two stationary phases π radians (180 degrees) apart.\n\nParametrons were used in early Japanese computers from 1954 through the early 1960s. A prototype parametron-based computer, the PC-1, was built at the University of Tokyo in 1958. Parametrons were used in early Japanese computers due to being reliable and inexpensive but were ultimately surpassed by transistors due to differences in speed.\n\n"}
{"id": "67742", "url": "https://en.wikipedia.org/wiki?curid=67742", "title": "Percussion cap", "text": "Percussion cap\n\nThe percussion cap, introduced circa 1820, is a type of single-use ignition device used on muzzleloading firearms that enabled them to fire reliably in any weather conditions. This crucial invention gave rise to the caplock or percussion lock system.\n\nBefore this development, firearms used flintlock ignition systems that produced flint-on-steel sparks to ignite a pan of priming powder and thereby fire the gun's main powder charge (the flintlock mechanism replaced older ignition systems such as the matchlock and wheellock). Flintlocks were prone to misfire in wet weather, and many flintlock firearms were later converted to the more reliable percussion system.\n\nThe percussion cap is a small cylinder of copper or brass with one closed end. Inside the closed end is a small amount of a shock-sensitive explosive material such as fulminate of mercury. The percussion cap is placed over a hollow metal \"nipple\" at the rear end of the gun barrel. Pulling the trigger releases a hammer that strikes the percussion cap and ignites the explosive primer. The flame travels through the hollow nipple to ignite the main powder charge. Percussion caps were, and still are, made in small sizes for pistols and larger sizes for rifles and muskets.\n\nWhile the metal percussion cap was the most popular and widely used type of primer, their small size made them difficult to handle under the stress of combat or while riding a horse. Accordingly, several manufacturers developed alternative, \"auto-priming\" systems. The \"Maynard tape primer\", for example, used a roll of paper \"caps\" much like today's toy cap gun. The Maynard tape primer was fitted to some firearms used in the mid-nineteenth century and a few saw brief use in the American Civil War. Other disc or pellet-type primers held a supply of tiny fulminate detonator discs in a small magazine. Cocking the hammer automatically advanced a disc into position. However, these automatic feed systems were difficult to make with the manufacturing systems in the early and mid-nineteenth century and generated more problems than they solved. They were quickly shelved in favor of a single percussion cap that, while awkward to handle in some conditions, could be carried in sufficient quantities to make up for occasionally dropping one while a jammed tape primer system reduced the rifle to an awkward club.\n\nThe first practical solution for the problem of handling percussion caps in battle was the Prussian 1841 (Dreyse needle gun), which used a long needle to penetrate a paper cartridge filled with black powder and strike the percussion cap that was fastened to the base of the bullet. While it had a number of problems, it was widely used by the Prussians and other German states in the mid-nineteenth century and was a major factor in the 1866 Austro-Prussian War.\n\nIn the 1850s, the percussion cap was first integrated into a metallic cartridge, which contained the bullet, powder charge and primer. By the late 1 use shotshell primers instead of caps). Most percussion caps now use non-corrosive compounds such as lead styphnate.\n\nThe percussion cap replaced the flint, the steel \"frizzen\", and the powder pan of the flint-lock mechanism. It was only generally applied to the British military musket (the Brown Bess) in 1842, a quarter of a century after the invention of percussion powder and after an elaborate government test at Woolwich in 1834. The first percussion firearm produced for the US military was the percussion carbine version (c.1833) of the M1819 Hall rifle.\n\nThe discovery of fulminates was made by Edward Charles Howard (1774–1816) in 1800. The invention that made the percussion cap possible using the recently discovered fulminates was patented by the Rev. Alexander John Forsyth of Belhelvie, Aberdeenshire, Scotland in 1807.\n\nThis early system coined \"Percussion Lock\" operated in a near identical fashion to flintlock firearms and utilized fulminating primer made of fulminate of mercury, chlorate of potash, sulphur, and charcoal, which was ignited by concussion. It was an invention born of necessity: Rev. Forsyth had noticed that sitting birds would startle when smoke puffed from the powder pan of his flintlock shotgun, giving them sufficient warning to escape the shot. His invention of a fulminate-primed firing mechanism deprived the birds of their early warning system, both by avoiding the initial puff of smoke from the flintlock powder pan, as well as shortening the interval between the trigger pull and the shot leaving the muzzle.\n\nFulminate-primed guns were also less likely to misfire than flintlock guns. However, it was not until after Forsyth's patents expired that the conventional percussion cap system was developed. The percussion cap helped lead to the self-contained cartridge, where the bullet is held in by the casing, the casing is filled with gunpowder, and a primer is at the end.\n\nJoshua Shaw, an English-born American, is sometimes credited with the development of the first metallic percussion cap in 1814, but his claim remains clouded with controversy as he did not patent the idea until 1822. Furthermore, according to Lewis Winant, the US government's decision to award Shaw $25,000 as compensation for his invention was actually a mistake. Congress believed Shaw's patent was the earliest and awarded him a large sum of money based on this belief. However, a claim for the percussion cap was filed in 1819 and granted in 1820 as an addition to an 1818 patent by Francois Prelat, two years before Shaw's patent. This doesn't necessarily make Prelat the inventor however, as he made a habit of copying English patents and inventions at the time and the mode of operation he describes is flawed. According to historian Sidney James Gooding, the most likely inventor of the percussion cap is Joseph Egg, around 1817. Shaw's percussion caps used a mixture of fulminate of mercury, chlorate of potash, and ground glass contained in a small metallic cup. Other possible claimants include Joseph Manton and Col. Peter Hawker.\n\nThis invention was gradually improved, and came to be used, first in a steel cap, and then in a copper cap, by various gunmakers and private individuals before coming into general military use nearly thirty years later.\n\nThe alteration of the military flintlock to the percussion musket was easily accomplished by replacing the powder pan and steel \"frizzen\" with a nipple, and by replacing the cock or hammer that held the flint by a smaller hammer formed with a hollow made to fit around the nipple when released by the trigger. On the nipple was placed the copper cap containing the detonating composition, now made of three parts of chlorate of potash, two of fulminate of mercury and one of powdered glass. The hollow in the hammer contained the fragments of the cap if it fragmented, reducing the risk of injury to the firer's eyes.\n\nThe detonating cap, thus invented and adopted, brought about the invention of the modern cartridge case, and rendered possible the general adoption of the breech-loading principle for all varieties of rifles, shotguns and pistols.\n\nCaps are used in cartridges, grenades, rocket-propelled grenades, and rescue flares. Percussion caps are also used in land mine fuzes, boobytrap firing devices and anti-handling devices.\n\nAs a rule, most purpose-made military booby-trap firing devices contain some form of spring-loaded firing pin designed to strike a percussion cap connected to a detonator at one end. The detonator is inserted into an explosive charge—e.g., C4 or a block of TNT. Triggering the booby-trap (e.g., by pulling on a trip-wire) releases the cocked firing pin that flips forward to strike the percussion cap, firing both it and the attached detonator. The resulting shock-wave from the detonator sets off the main explosive charge.\n\n"}
{"id": "530988", "url": "https://en.wikipedia.org/wiki?curid=530988", "title": "Programmed Airline Reservations System", "text": "Programmed Airline Reservations System\n\nProgrammed Airline Reservations System (PARS) is an IBM proprietary large scale airline reservation application, a computer reservations system, executing under the control of IBM Airline Control Program (ACP) (and later its successor, Transaction Processing Facility (TPF)). Its international version was known as \"IPARS\".\n\nBy the 1960s, with the American Airlines SABRE reservations system up and running, IBM offered its expertise to other airlines, and soon developed Deltamatic for Delta Air Lines on the IBM 7074, and PANAMAC for Pan American World Airways using an IBM 7080. By 1967/8 IBM generalized its airline reservations work into the PARS system, which ran on the larger members of the IBM System/360 family and which could support the largest airlines' needs at that time (e.g. United Airlines ran about 3000 reservations terminals online in the 1972 timeframe). In the early 1970s IBM modified its PARS reservations system so it could accommodate the smaller regional airlines on smaller members of the 370 systems family. The high performance PARS operating system evolved from ACP (\"Airlines Control Program\") to TPF (\"Transaction Processing Facility\").\n\nIn the early days of automated reservations systems in the 1960s and 1970s the combination of ACP and PARS provided unprecedented scale and performance from an on-line real-time system, and for a considerable period ranked among the largest networks and systems of the era. In the early 1970s major US banks were developing major on-line teleprocessing applications systems and were in urgent need of ACP's high performance capabilities. ACP was made available by IBM to the banking industry in the mid-1970s. This system was used by the great majority of large airlines in the US and internationally; and its smaller 1970's version was used by many smaller regional airlines. PARS (and IPARS) was extremely successful, and it massively improved and revolutionized the efficiency of airlines passenger operations and their profitability.\n\nAlong with many other major and regional US airlines, the PARS system was later used by TWA and Northwest Airlines. In this context PARS was also used as a marketing name by TWA when selling their system to travel agencies.\nSwiss International Airlines and Brussels Airlines discontinued using PARS beginning of 2016\n\nCPARS (Compact Programmed Airlines Reservations) was used by smaller airlines (e.g. Icelandair). Among other limitations (compared to PARS) was a shorter booking horizon of 90 days.\n\n"}
{"id": "4705609", "url": "https://en.wikipedia.org/wiki?curid=4705609", "title": "Repulsion motor", "text": "Repulsion motor\n\nA repulsion motor is a type of electric motor for using on alternating current (AC). It was formerly used as a traction motor for electric trains (e.g. SR Class CP and SR Class SL electric multiple units) but has been superseded by other types of motors. Repulsion motors are classified under single phase motors. In repulsion motors the stator windings are connected directly to the AC power supply and the rotor is connected to a commutator and brush assembly, similar to that of a direct current (DC) motor.\n\nThe motor has a stator and a rotor but there is no electrical connection between the two and the rotor current is generated by induction. The rotor winding is connected to a commutator which is in contact with a pair of short-circuited brushes which can be moved to change their angular position relative to an imaginary line drawn through the axis of the stator. The motor can be started, stopped and reversed, and the speed can be varied, simply by changing the angular position of the brushes.\n\nMost commutator motors are limited to about 1,500 volts because higher voltages give rise to a risk of arcing across the commutator. Repulsion motors can be used at higher voltages because the rotor circuit is not electrically connected to the supply.\n\nRepulsion motors are based on the principle of repulsion between two magnetic fields. Consider a 2-pole salient pole motor with a vertical magnetic axis. The armature is connected to a commutator and brushes. The brushes are short circuited using a low-resistance jumper.\nWhen alternating current is supplied to the field (stator) winding, it induces an electromotive force (emf) in the armature. The direction of alternating current is such that it creates a north pole at the top and a south pole at the bottom. The direction of induced emf is given by Lenz's law, according to which the direction of induced emf opposes the cause producing it. The induced emf induces current in the armature conductors and the direction of the induced current depends on the position of the brushes.\n\nIf the brush axis is along the direction of the magnetic field, the armature behaves like an electromagnet and a N-pole is formed directly below the N-pole of the stator and a S-pole is formed directly above the S-pole of the stator. The net torque in this condition is zero. Both the N-poles repel each other and both the S-poles repel each other. The two repulsion forces are in direct opposition to each other and hence no torque will be developed. This is very fast process in this repulsion motor.\n\nIf the brushes are shifted through 90 degrees, so that the magnetic axis is perpendicular to the brush axis, the coils undergoing short circuit change. Apart from the coils undergoing short circuit, the voltage induced in the other coils between the brush terminals is neutralized and the net voltage is zero. Since there is no induced emf, there is no current in the circuit and the net torque developed is, again, zero.\n\nIf the brush axis is displaced at an angle to the magnetic axis, a net voltage is induced at the brush terminals which will produce current in the armature. The current in the armature circuit will produce its own magnetic field, with North and South poles, but in this condition, the North Pole is not directly under the North pole of the magnetic axis and the South Pole is not directly above the South Pole of the magnetic axis. The poles of the armature are slightly displaced from those of the stator. In this condition, the N-pole of the stator field will repel the N-pole of the rotor field and the S-pole of stator field will repel the S-pole of the rotor field, so the rotor starts rotating.\n\nThe direction of rotation is determined by the position of the brushes with respect to the magnetic field of the stator. If the brushes are shifted clockwise from the main magnetic axis, the motor will rotate in a clockwise direction. If the brushes are shifted counter-clockwise from the main magnetic axis, the motor will rotate in a counter-clockwise direction.\n\nThe starting torque of a repulsion motor is determined by the angle of brush shift from the main magnetic axis. The maximum torque is obtained from a brush shift of 45 degrees. Brush shift can also be used to control the speed of a repulsion motor.\n\nTypes of repulsion motor are listed below. It is likely that the different types were developed to match the torque/speed characteristics of the motor as closely as possible to the service it was required to provide.\n\nThe Elihu Thomson motor is the original repulsion motor and is described in \"Construction\" above.\n\nThe Deri motor is similar to the Elihu Thomson type but has two pairs of short-circuited brushes—one fixed and one moveable. This allows very fine control of speed.\n\nThis is the \"compensated\" repulsion motor devised independently by Latour and by Winter-Eichberg. There are, again, two pairs of brushes but they are fixed at right angles to each other. One pair is short-circuited while the other pair is fed with variable-voltage alternating current from tappings on the secondary winding of a small transformer. The primary winding of the transformer is in series with the stator winding of the motor. This motor has the same torque/speed characteristics as an ordinary series-wound motor.\n\nThe Atkinson motor has two stator coils at right angles to each other. Speed control (by brush-shifting) is possible from 75% below synchronous speed to 10% above. Starting torque is about 2.5 times full-load torque with twice full-load current.\n\nThese were used where high starting torque was required. They started as repulsion motors, but once they were running at a sizable fraction of full speed, the brushes were lifted mechanically and all commutator bars were short-circuited together to create the equivalent of a squirrel-cage induction motor.\n\nRepulsion motor applications included:\n\n"}
{"id": "6047014", "url": "https://en.wikipedia.org/wiki?curid=6047014", "title": "Selective door operation", "text": "Selective door operation\n\nSelective Door Operation, also called Selective Door Opening (or SDO) is a mechanism employed primarily on trains (although buses with multiple doors also generally have this feature) that allows the driver or conductor/guard to open the doors of a train separately.\n\nSelective Door Operation enables trains to call at a station where the platform is shorter than the train. Some doors can be prevented from opening to ensure that passengers do not disembark from any carriages not standing at the platform. The term Selective Door Operation is used mainly in the United Kingdom; some train operating companies used the term ‘Door De-Select’. A version of this is used in other countries and on other rail systems such as the London Underground.\n\nIn the UK various trains, either multiple units or coaches, have variations of the Selective Door Operating system. This usually depends on what the specific train operating company and/or train leasing company required, either at time of purchase or a later modification to an existing train to keep up to date with regulations. Examples of these variations are as follows:\n\n\nSelective door operation is implemented at certain railway stations in the United States. In the New York City Subway, the -car-long platforms at 145th Street (and formerly the 5-car-long loop platforms at South Ferry) are too short to accommodate full-length trains of ten cars, so only the first five cars of the train opened their doors at these stations. Also at South Ferry, the inner platform's curves were so tight that only the inner doors of the cars were able to be opened. At Times Square on track 4 of the 42nd Street Shuttle, the platform is so short that only the first door of the third car may open at the station.\n\nIn Boston, the boarding platform at the Bowdoin terminus of the MBTA Blue Line accommodates only four of the train's six cars; passengers must press buttons to open the doors. Similar selective door operation protocols are used on many commuter rail lines within the Northeast megalopolis since some commuter rail stations have platforms that are too short to accommodate longer trains.\n\nIn Seattle, the Satellite Transit System at Seattle-Tacoma International Airport uses selective door operation on the loop connecting the South Satellite (the airport's international concourse) with the airport's main terminal. Passengers on most international flights arrive at the South Satellite, where they are inspected by U.S. Customs and Border Protection officials; after clearing inspection, passengers have the option of waiting in line to be inspected by the Transportation Security Administration so that they may access the rear two cars and secure area of the airport (to catch a connecting flight) or directly boarding the first train car which transports them to the airport exit and the baggage claim area. The platform screen doors that provide access to the first car of the train do not open at the station serving Concourse B of the airport to prevent unscreened passengers from accessing the concourse. \n\nIn New Zealand, “selective door opening” is used on the Wairarapa Connection commuter train, as the Maymorn railway station platform is not long enough to accommodate all the carriages, and Maymorn passengers are restricted to the first three carriages.\n\nMost modern Selective Door Opening (SDO) systems receive their positioning data from the Global Positioning System (GPS). As the train arrives at a station, the GPS determines the location for the train's SDO control, which contains a database with a Unique Location Identifier (ULI) for each station. This then enables the correct number of coaches to be opened to suit the length of the platform. However this system is reliant on the train stopping in the correct position, since the SDO will authorise the doors to open as long as the train is within 300 metres of the station. Depending on which system is in use, SDO may not prevent the doors from being opened where there is no platform if;\n\nSelective Door Operation is different from Local Door Operation (LDO), which is used on many trains by train crew and other staff.\n\n"}
{"id": "1041698", "url": "https://en.wikipedia.org/wiki?curid=1041698", "title": "Sergey Lebedev (scientist)", "text": "Sergey Lebedev (scientist)\n\nSergey Alexeyevich Lebedev (; 2 November 1902, n.s. – 3 July 1974) was a Russian-born Ukrainian Soviet scientist in the fields of electrical engineering and computer science, and designer of the first Soviet computers.\n\nLebedev was born in Nizhny Novgorod, Russia. He graduated from Moscow Highest Technical School in 1928. From then until 1946 he worked at All-Union Electrotechnical Institute (formerly a division of MSTU) in Moscow and Kiev. In 1939 he was awarded the degree of Doctor of Sciences for the development of the theory of \"artificial stability\" of electrical systems.\n\nDuring World War II, Lebedev worked in the field of control automation of complex systems. His group designed a weapon-aiming stabilization system for tanks and an automatic guidance system for airborne missiles. To perform these tasks Lebedev developed an analog computer system to solve ordinary differential equations.\n\nFrom 1946 to 1951 he headed the Kiev Electrotechnical Institute of the Ukrainian Academy of Sciences, working on improving the stability of electrical systems. For this work he received the Stalin (State) prize in 1950.\n\nIn 1948 Lebedev learned from foreign magazines that scientists in western countries were working on the design of electronic computers, although the details were secret. In the autumn of the same year he decided to focus the work of his laboratory on computer design. Lebedev's first computer, BESM-1, was completed by the end of 1951. In April 1953 the State commission accepted the BESM-1 as operational, but it did not go into series production because of opposition from the Ministry of Machine and Instrument Building, which had developed its own weaker and less reliable machine.\n\nLebedev then began development of a new, more powerful computer, the M-20, the number denoting its expected processing speed of twenty thousand operations per second. In 1958 the machine was accepted as operational and put into series production. Simultaneously the BESM-2, a development of the BESM-1, went into series production. Though the BESM-2 was slower than the M-20, it was more reliable. It was used to calculate satellite orbits and the trajectory of the first rocket to reach the surface of the Moon. Lebedev and his team developed several more computers, notably the BESM-6, which was in production for 17 years.\n\nIn 1952, Lebedev became a professor at the Moscow Institute of Physics and Technology. From 1953 until his death he was the director of what is now called the Institute of Precision Mechanics and Computer Engineering.\n\nLebedev died in Moscow and is interred at Novodevichy Cemetery.\n\nIn 1996 the IEEE Computer Society recognized Sergey Lebedev with a Computer Pioneer Award for his work in the field of computer design and his founding of the Soviet computer industry.\n\n"}
{"id": "41705", "url": "https://en.wikipedia.org/wiki?curid=41705", "title": "Signal-to-crosstalk ratio", "text": "Signal-to-crosstalk ratio\n\nThe signal-to-crosstalk ratio at a specified point in a circuit is the ratio of the power of the wanted signal to the power of the unwanted signal from another channel. \n\nThe signals are adjusted in each channel so that they are of equal power at the zero transmission level point in their respective channels. \n\nThe signal-to-crosstalk ratio is usually expressed in dB. \n"}
{"id": "12402490", "url": "https://en.wikipedia.org/wiki?curid=12402490", "title": "Sleipner A", "text": "Sleipner A\n\nSleipner A is a combined accommodations, production and processing offshore platform at the Sleipner East gas field in the Norwegian sector of the North Sea. For those in huds it collapsed. It is a Condeep-type oil platform, built in Norway by the company Norwegian Contractors for Statoil.\n\nIt is known for its catastrophic failure on 23 August 1991, due to a design flaw, that resulted from an error caused by unconservative concrete codes and inaccurate finite element analysis modelling of the tricell, which formed part of the ballasting/flotation system.\n\nSleipner A is located on the Sleipner East gas field on the North Sea. Also six satellite fields–Gungne, Loke, Alpha North, Sigyn, Volve and Volve South–are tied-back to Sleipner A. In addition to its own operations, the platform is used as a remote operation center for the Sleipner B wellhead platform. The Sleipner B is operated from the Sleipner A via an umbilical cable. In addition, the Sleipner T carbon dioxide treatment platform is linked physically to the Sleipner A platform by a bridge.\n\nThe platform is designed to accommodate roughly 160 people. The platform deck is with height of .\n\nThe original hull was a gravity base made up of support pilings and concrete ballast chambers from which three or four shafts rise and upon which the deck sits. Once fully ballasted, the hull was to sit on the sea floor. There were 24 chambers, of which four formed the 'legs' supporting the facility on top in the case of the Sleipner A oil rig.\n\nThe hull was redesigned after the accident and the Sleipner A Platform was successfully completed in June 1993.\n\nThe original hull collapsed during the final construction because of a design flaw. It was towed into Gandsfjord where it was to be lowered in the water in a controlled ballasting operation at a rate of 1 meter per 20 minutes. This was necessary for the fitment of the deck platform to the hull. As the hull was lowered to the mark, rumbling noises were heard followed by the sound of water pouring into the unit. A cell wall had failed and a serious crack had developed, and sea water poured in at a rate that was too great for the deballasting pumps to deal with. Within a few minutes the hull began sinking at a rate of 1 meter per minute. As the structure sank deeper into the fjord, the buoyancy chambers imploded and the rubble struck the floor of the fjord, creating a Richter magnitude scale 3 earthquake.\n\nLater analysis showed that the failure would occur at 62 meters (203 feet).\n\nNo one was injured during the accident.\n\nThe post-accident investigation by SINTEF in Norway discovered that the root cause of the failure resulted from inaccurate NASTRAN calculations in the design of the structure. Stresses on the ballast chambers were underestimated by 47% and some concrete walls were designed too thin to resist foreseeable hydrostatic pressure when submerged. As the pressure increased, the walls failed and cracked, allowing sea water to enter the tank at an uncontrolled rate, eventually sinking the hull.\n\nAfter the accident, the project leaders from Norwegian Contractors were brought before the Statoil board, and were expecting severe repercussions. But the director instead asked the famous question \"Can you make a new one before schedule?\" to which the contractors replied \"Yes we can\". The new hull was completed before schedule.\n\nComputer-Aided Catastrophes, or CAC for short, such as the Sleipner Incident presented in this article, provide extremely valuable lessons for practising engineers working with numerical simulation tools such as the finite element method. The reason for the poor finite element result that led to the Sleipner Incident have been studied in more detail in NAFEMS Benchmark Challenge Number 6.\n"}
{"id": "58002148", "url": "https://en.wikipedia.org/wiki?curid=58002148", "title": "Sup'R'Mod", "text": "Sup'R'Mod\n\nThe Sup 'R' Mod II was an RF modulator sold by M&R Enterprises in the late 1970s and early 1980s. It allowed the connection of computers and other devices with composite video outputs to a television.\n\nApple Inc. wanted to provide users of their Apple II computers with a way to view color output on a television, but they had trouble getting FCC approval, because the RF modulation solution they were using was too noisy. Apple made an arrangement with a small nearby company, called M&R Enterprises, to manufacture and sell the devices. While Apple could not sell the modulator and computer as a package, retail computer dealers could sell both devices to the end user.\n\nMarty Spergel, who ran M&R Enterprises, was told by Steve Jobs that it might sell as many as 50 units a month. Spergel later estimated that he had sold about 400,000 units.\n\nThe Sup 'R' Mod II began selling in April 1978, for $29.95.\n\nThe Sup 'R' Mod II kit came with a small printed circuit board, an antenna switch, and a coaxial cable with a ferrite core and RCA connectors. Composite video was received by the circuit board through a short cable terminating in a molex connector, which plugged into a header on the Apple II motherboard. Input could also be provided through an RCA connector. The output of the RF modulator went out through a coaxial cable to the antenna switch.\n\nThe antenna switch allowed the user to select between television broadcasts and computer output. The television antenna was connected to inputs on the switch, and the switch output was connected to the back of the television. The connections used screw terminals with spade lugs. Moving the switch from \"TV\" to \"GAME PLAY\" selected the computer output.\n\nThe board and antenna switch came with a bit of double-sided tape to allow the user to attach them to the inside of the computer case and the side of the television, respectively.\n\nThe modulator presented a color signal on UHF channel 33.\n"}
{"id": "41764", "url": "https://en.wikipedia.org/wiki?curid=41764", "title": "Survivability", "text": "Survivability\n\nSurvivability is the ability to remain alive or continue to exist. The term has more specific meaning in certain contexts.\nFollowing disruptive forces such as flood, fire, disease, war, or climate change some species of flora, fauna, and local life forms are likely to survive more successfully than others because of consequent changes to their surrounding biophysical conditions.\n\nIn engineering, survivability is the quantified ability of a system, subsystem, equipment, process, or procedure to continue to function during and after a natural or man-made disturbance; for example a nuclear electromagnetic pulse from the detonation of a nuclear weapon.\n\nFor a given application, survivability must be qualified by specifying the range of conditions over which the entity will survive, the minimum acceptable level or post-disturbance functionality, and the maximum acceptable downtime.\n\nIn the military environment, survivability is defined as the ability to remain mission capable after a single engagement. Engineers working in survivability are often responsible for improving four main system elements: \n\n\nThe European Survivability Workshop introduced the concept of \"Mission Survivability\" whilst retaining the three core areas above, either pertaining to the \"survivability\" of a platform through a complete mission, or the \"survivability\" of the mission itself (i.e. probability of mission success). Recent studies have also introduced the concept of \"Force Survivability\" which relates to the ability of a force rather than an individual platform to remain \"mission capable\".\n\nThere is no clear prioritisation of the three elements; this will depend on the characteristics and role of the platform. Some platform types, such as submarines and airplanes, minimise their susceptibility and may, to some extent, compromise in the other areas. Main Battle Tanks minimise vulnerability through the use of heavy armours. Present day surface warship designs tend to aim for a balanced combination of all three areas.\n\nSurvivability denotes the ability of a ship and its on-board systems to remain functional and continue designated mission in a man-made hostile environment. The naval vessels are designed to operate in a man-made hostile environment, and therefore the survivability is a vital feature required from them. The naval vessel’s survivability is a complicated subject affecting the whole life cycle of the vessel, and should be considered from the initial design phase of every war ship.\n\nThe classical definition of naval survivability includes three main aspects, which are susceptibility, vulnerability, and recoverability; although, recoverability is often subsumed within vulnerability.\nSusceptibility consists of all the factors that expose the ship to the weapons effects in a combat environment. These factors in general are the operating conditions, the threat, and the features of the ship itself. The operating conditions, such as sea state, weather and atmospheric conditions, vary considerably, and their influence is difficult to address (hence they are often not accounted for in survivability assessment). The threat is dependent on the weapons directed against the ship and weapon’s performance, such as the range. The features of the ship in this sense include platform signatures (radar, infrared, acoustic, magnetic), the defensive systems on board, such as surface-to-air missiles, EW and decoys, and also the tactics employed by the platform in countering the attack (aspects such as speed, maneuverability, chosen aspect presented to the threat).\nVulnerability refers to the ability of the vessel to withstand the short-term effects of the threat weapon. Vulnerability is an attribute typical to the vessel and therefore heavily\naffected by the vessel’s basic characteristics such as size, subdivision, armouring, and other hardening features, and also the design of the ship's systems, in particular the location of equipment, degrees of redundancy and separation, and the presence within a system of single point failures. Recoverability refers to vessel’s ability to restore and maintain its functionality after sustaining damage. Thus, recoverability is dependent on the actions aimed to neutralize the effects of the damage. These actions include firefighting, limiting the extent of flooding, and dewatering. Besides the equipment, the crew also has a vital role in recoverability.\n\nThe crews of military combat vehicles face numerous lethal hazards which are both diverse and constantly evolving. Improvised Explosive Devices (IEDs), mines, and enemy fire are examples of such persistent and variable threats. Historically, measures taken to mitigate these hazards were concerned with protecting the vehicle itself, but due to this achieving only limited protection, the focus has now shifted to safeguarding the crew within from an ever-broadening range of threats, including Radio Controlled IEDs (RCIEDs), blast, fragmentation, heat stress, and dehydration.\n\nThe expressed goal of \"crew survivability\" is to ensure vehicle occupants are best protected. It goes beyond simply ensuring crew have the appropriate protective equipment and has expanded to include measuring the overpressure and blunt impact forces experienced by a vehicle from real blast incidents in order to develop medical treatment and improve overall crew survivability. Sustainable crew survivability is dependent on the effective integration of knowledge, training, and equipment:\n\nThreat intelligence identifying trends, emerging technologies, and attack tactics used by enemy forces enables crews to implement procedures that will reduce their exposure to unnecessary risks. Such intelligence also allows for more effective pre-deployment training programs where personnel can be taught the most up-to-date developments in IED concealment, for example, or undertake tailored training that will enable them to identify the likely attack strategy of enemy forces. In addition, with expert, current threat intelligence, the most effective equipment can be procured or rapidly developed in support of operations.\n\n\"The capability of a system to fulfill its mission, in a timely manner, in the presence of such as attacks or large-scale natural disasters. Survivability is a subset of resilience.\"\n\n“The capability of a system to fulfill its mission, in a timely manner, in the presence of attacks, failures, or accidents.”\n\n\n"}
{"id": "17563862", "url": "https://en.wikipedia.org/wiki?curid=17563862", "title": "Tech Council of Maryland", "text": "Tech Council of Maryland\n\nThe Tech Council of Maryland (TCM) is a technology trade association for companies with operations in Maryland, Washington, D.C. and Virginia. TCM has two divisions: The Tech Alliance, which serves the advanced technology industry, and MdBio, which serves the Maryland biotech industry.\n\nNotes \n\n"}
{"id": "11772928", "url": "https://en.wikipedia.org/wiki?curid=11772928", "title": "Weibull modulus", "text": "Weibull modulus\n\nThe Weibull modulus is a dimensionless parameter of the Weibull distribution which is used to describe variability in measured material strength of brittle materials. \n\nFor ceramics and other brittle materials, the maximum stress that a sample can be measured to withstand before failure may vary from specimen to specimen, even under identical testing conditions. This is related to the distribution of physical flaws present in the surface or body of the brittle specimen, since brittle failure processes originate at these weak points. When flaws are consistent and evenly distributed, samples will behave more uniformly than when flaws are clustered inconsistently. This must be taken into account when describing the strength of the material, so strength is best represented as a distribution of values rather than as one specific value. The Weibull modulus is a shape parameter for the Weibull distribution model which, in this case, maps the probability of failure of a component at varying stresses.\n\nConsider strength measurements made on many small samples of a brittle ceramic material. If the measurements show little variation from sample to sample, the calculated Weibull modulus will be high and a single strength value would serve as a good description of the sample-to-sample performance. It may be concluded that its physical flaws, whether inherent to the material itself or resulting from the manufacturing process, are distributed uniformly throughout the material. If the measurements show high variation, the calculated Weibull modulus will be low; this reveals that flaws are clustered inconsistently and the measured strength will be generally weak and variable. Products made from components of low Weibull modulus will exhibit low reliability and their strengths will be broadly distributed.\n\nTest procedures for determining the Weibull modulus are specified in DIN EN 843-5 and DIN 51 110-3.\n\nA further method to determine the strength of brittle materials has been described by the Wikibook contribution .\n\nIf the probability distribution of the strength, \"X\", is a Weibull distribution with its density given by \n\nthen \"k\" is the Weibull modulus.\n\nThe value of k shows the kind of failure being experienced. If k<1, then the failure rate decreases with time. This implies that the weak and defective parts fail in the beginning, with the harder sections surviving. If k=1, the rate of failure remains constant. This implies that there is random failure occurring. Thus, there should be some external factor strong enough to cause random failure irrespective of whether the section is strong or weak. If the value of k>1, the rate of failure increases over time. This points to some kind of ageing process, the weakening of the material with the passage of time.\n\n\n"}
{"id": "50387925", "url": "https://en.wikipedia.org/wiki?curid=50387925", "title": "Werner Dilger", "text": "Werner Dilger\n\nDilger studied Protestant theology and attended a basic study of mathematics. Then he turned to the computer science, the study of which he finished in Karlsruhe in 1974 as a computer scientist. At the University of Kaiserslautern received his doctorate and he qualified.\n\nFrom 1989 to 1993 he was professor of applied computing at the EBS University of Business and Law Castle Reichartshausen. Between 1993 and 1997 he worked the Technical University of Chemnitz as professor of Artificial Intelligence.\n\nHe was involved in setting up data mining company Prudsys AG in 1998. Dilger chaired the supervisory board at Prudsys AG between 2002 and 2006.\n\nHe died in a swimming accident in June 2007.\n\n\n"}
{"id": "292758", "url": "https://en.wikipedia.org/wiki?curid=292758", "title": "William Higinbotham", "text": "William Higinbotham\n\nWilliam Higinbotham (October 25, 1910 – November 10, 1994) was an American physicist. A member of the team that developed the first nuclear bomb, he later became a leader in the nonproliferation movement. He also has a place in the history of video games for his 1958 creation of \"Tennis for Two\", the first interactive analog computer game and one of the first electronic games to use a graphical display.\n\nHiginbotham was born in Bridgeport, Connecticut, and grew up in Caledonia, New York. His father was a minister in the Presbyterian Church. He earned his undergraduate degree from Williams College in 1932 and continued his studies at Cornell University. He worked on the radar system at MIT from 1941 to 1943.\n\nDuring World War II, he worked at Los Alamos National Laboratory and headed the lab's electronics group in the later years of the war, where his team developed electronics for the first nuclear bomb. His team created the bomb's ignition mechanism as well as measuring instruments for the device. Higinbotham also created the radar display for the experimental B-28 bomber. Following his experience with nuclear weapons, Higinbotham helped found the nuclear nonproliferation group Federation of American Scientists, serving as its first chairman and executive secretary. From 1974 until his death in 1994, Higinbotham served as the technical editor of the \"Journal of Nuclear Materials Management\", published by the Institute of Nuclear Materials Management.\n\nIn 1947 Higinbotham took a position at Brookhaven National Laboratory, where he worked until his retirement in 1984.\nIn 1958, as Head of the Instrumentation Division at Brookhaven, he created a computer game called \"Tennis for Two\" for the laboratory's annual exposition. A tennis simulator displayed on an oscilloscope, the game is credited with being one of the first video games. The game took Higinbotham a few weeks to complete, and was a popular attraction at the show. It was such a hit that Higinbotham created an expanded version for the 1959 exposition; this version allowed the gravity level to be changed so players could simulate tennis on Jupiter and the Moon. Higinbotham never patented \"Tennis for Two\", though he obtained over 20 other patents during his career.\n\nHe recalled in 1983,\nLargely forgotten for years, critics began to recognize \"Tennis for Two\"'s significance to the history of video games in the 1980s. In 1983, David Ahl, who had played the game at the Brookhaven exhibition as a teenager, wrote a cover story for \"Creative Computing\" in which he dubbed Higinbotham the \"Grandfather of Video Games\". Independently, Frank Lovece interviewed Higinbotham for a story on the history of video games in the June 1983 issue of \"Video Review\".\n\nIn 2011, Stony Brook University founded the William A. Higinbotham Game Studies Collection, managed by Head of Special Collections and University Archives Kristen Nyitray and Associate Professor of Digital Cultural Studies Raiford Guins. The Collection is explicitly dedicated to \"documenting the material culture of screen-based game media\", and in specific relation to Higinbotham: \"collecting and preserving the texts, ephemera, and artifacts that document the history and work of early game innovator and Brookhaven National Laboratory scientist William A. Higinbotham, who in 1958 invented the first interactive analog computer game, Tennis for Two.\" As part of preserving the history of \"Tennis for Two\", the Collection is producing a documentary on the history of the game and its reconstruction by Peter Takacs, physicist at Brookhaven National Laboratory.\n\nHiginbotham remained little interested in video games, preferring to be remembered for his work in nuclear nonproliferation. After his death, as requests for information on \"Tennis for Two\" increased, his son William B. Higinbotham told Brookhaven: \"It is imperative that you include information on his nuclear nonproliferation work. That was what he wanted to be remembered for.\" For this work the Federation of American Scientists named their headquarters Higinbotham Hall in 1994.\n\n"}
