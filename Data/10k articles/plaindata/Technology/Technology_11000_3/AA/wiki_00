{"id": "3148479", "url": "https://en.wikipedia.org/wiki?curid=3148479", "title": "ACCS", "text": "ACCS\n\nACCS is the NATO Air Command and Control System project, planned to replace the NATO Air Command and Control Systems of the nineties. At the highest level it comprised the Combined Air Operations Centre (CAOC) from which the air battle is run. Beneath this level of command is the Air Control Centre (ACC), Recognized Air Picture (RAP) Production Centre (RPC) and Sensor Fusion Post (SFP) combined in one entity called ARS. The ARS is the equivalent to the Control and Reporting Centers (CRCs) operated in the nineties. The ACCS project comprised both static and deployable elements. Under separate funding, NATO intended to procure deployable sensors for the deployable ACCS component (DAC).\n\nOversight of the project is provided by the NATO Communications and Information Agency (NCIA) in Brussels, Belgium (until 2012 executed by the NATO ACCS Management Organisation (NACMA) Board of Directors, senior representatives of the Nations engaged in the NATO ACCS project. The Board is responsible to the Secretary General of NATO for the delivery of the project. The NCIA AIRC2 PO&S is responsible for the day-to-day management of the project scientific support from former NC3A (now part of the NCIA), system and software engineering support from Systems Support Center (SSC) (as well part of the NCIA), logistic support from former NAMSA (again part of the NCIA) and operational support from SHAPE. \n\nThe contract to build ACCS was based on nineties specifications and awarded to the Air Command Systems International (ACSI) consortium in November 1999. Since 2000 ACSI has been a part of ThalesRaytheonSystems (TRS). The contract provided the development and testing of the ACCS system core software is now completed. The hardware and software system has been accepted by NATO. Italy is the first nation using ACCS for its Military Air Operations since April 2015. Germany, France and Belgium has not yet transited to ACCS, as validation nations, leaving 17 other nations still pending.\n\nFundamental issues with the system design has such delayed the implementation more than 10 years. \nAmazingly, NATO principal procurement agency asked TRS to provide additional capability related to missile defence. First capability is said to be operational in Ramstein (Germany).\n\nThe Integrated System Support of the ACCS system, if operational, will be provided by the NATO Communications and Information Agency (NCIA) supported by in-house System Support Center (SSC).\n\n"}
{"id": "43335396", "url": "https://en.wikipedia.org/wiki?curid=43335396", "title": "AXIOM (camera)", "text": "AXIOM (camera)\n\nAXIOM is an open hardware and free software digital cinema camera family of devices being developed by a DIY community around the apertus° project.\n\nThe community’s second generation camera, AXIOM Beta Compact, is presently in development.\n\nIn 2006 Oscar Spierenburg, a Dutch film director, noticed a discussion taking place on DVInfo.net entitled “3 channel 36 bit 1280 X 720 low $ camera”, inside which Elphel cameras, which are typically used in scientific applications, had been mentioned.\n\nIn the same year a discussion thread entitled \"High Definition with Elphel model 333 camera\" was posted on the DVInfo.net forum, whereupon the forum’s members discussed how best to adapt Elphel open hardware camera devices for use in film production. Sebastian Pichelhofer discovered this thread in 2008 and assisted with the project by developing an Elphel camera internal hard-disk recorder user interface.\n\nBy early 2009, and because over the course of three years upwards of 1000 posts had been submitted to this thread, the community realised that it was going to be difficult to maintain a full overview of the project in this way, and consequently a dedicated website was established. All of the decision making and naming/logo design was decided upon by the community.\n\nAfter having done some contracting work with Elphel, Pichelhofer focused full-time on the project across 2011 and in July 2012 the plan to create an AXIOM camera hardware prototype from scratch, and thereby overcome some of the limitations that were found to be inherent with Elphel hardware at the time (mainly due to Elphel Inc. having shifted the company's core business focus towards development of a panoramic camera solution), was announced at the Libre Software Meeting in Geneva. This prototype became known as AXIOM Alpha and was intended to gather feedback from typical shooting scenarios with a view to incorporating ideas into a future, more modular, kit version of the camera aimed at developers and early adopters (AXIOM Beta I Developer Kit).\n\nShortly after development on the AXIOM Alpha began, a nonprofit organization was established to provide legal shelter for the community, and an apertus° company was registered by Spierenburg in order to facilitate responsibilities that had been neglected up to this point, e.g. signing contracts with electronic part/service providers, paying for prototype manufacturing etc. Because of his drive and enthusiasm for the project Pichelhofer was elected apertus° Association chairman.\n\nAfter reading a local hackerspace forum post in May 2013 Herbert Pötzl became aware of the community's efforts and met with Pichelhofer shortly thereafter. Pötzl had an extensive background in electronics engineering and software development and was appointed AXIOM Technical Lead. After Pötzl helped to develop critical aspects of hardware and software the AXIOM Alpha prototype was showcased at the Vienna Hackerlab in March 2014 whilst rough planning for a more modular and powerful camera was well underway.\n\nIn 2014, after crowdfunding the research and development through Indiegogo, work began on creating AXIOM Beta - a five printed circuit board stack, FOSS and open-source hardware, digital cinema camera incorporating the ams Sensors Belgium CMV12000 CMOS image sensor.\n\nIn recent times, and because existing manufacturers have proved reluctant to open their protocols up to the wider world, user groups have accepted responsibility and contributed to what became known as the ‘DSLR revolution’ first-hand e.g. the Magic Lantern (firmware) community. Magic Lantern is a free and open source software add-on that runs from a camera’s SD/CF card. It added a host of new features to Canon’s DSLRs that weren't included from the factory by Canon. Because the AXIOM Beta concept is essentially the hardware equivalent of the software they originally pioneered, Magic Lantern partnered with the apertus° Association in September 2014. Since then the Magic Lantern community has been involved with various colour science experiments using the camera.\n\nResearch on an AXIOM Gamma started in March 2015 after the project was awarded funding from the EU's Horizon2020 programme.\n\nAXIOM Alpha was a digital cinema camera proof of concept prototype. Only two units were built in 2013 and the second revision model was presented at the Metalab (a Vienna-based Hackerspace) in spring 2014. The main components included a Zedboard using a Xilinx Zynq-7020 System on a chip (SoC) and a 4K Super35mm image sensor designed by the Belgian company CMOSIS (later renamed to \"ams Sensors Belgium\" after being acquired by ams ).\n\nThe ams Sensors Belgium CMV12000 Super35/APS-C image sensor has a resolution of 4096×3072 pixels, a color depth of 12 bits/pixel and is able to capture at a maximal frame rate of 300 frames/sec in 10-bit mode. It is connected to the ZedBoard over FMC.\nThe Xilinx Zynq Z-7020 combines a Cortex-A9 dual-core with an FPGA.\nThe ZedBoard contains the Zynq Z-7020 and all the necessary interfaces.\n\nThe operating system is Linux kernel-based (Arch Linux) and composed entirely of free and open-source software.\n\nSuccessor of the AXIOM Alpha and the first 4K open hardware camera - the development of which was financed through an Indiegogo crowdfunding campaign that exceeded its funding goal.\n\nAXIOM Beta utilizes the same 4K Super35 image sensor as its predecessor the AXIOM Alpha. The lens mount is a passive e-mount with Canon EF or Nikon F mount adapter. The first prototype of the AXIOM Beta was presented at the National Association of Broadcasters convention in Las Vegas.\n\nPresently there are three enclosure versions for the camera expected:\n\nAXIOM Beta Developer Kit (DK) - apertus° offers AXIOM Beta Developer Kit without an enclosure either in kit form or readily assembled. The AXIOM Beta DK provides easy access to the camera’s printed circuit boards and is aimed at those who want to work on software related development. With this in mind all associated design files, BOMs, and STL files (for CAD or 3D printing components such as a lightweight enclosure), software source code repositories, etc. are made freely available. First AXIOM Beta DK's began shipping in October 2016.\n\nAXIOM Beta Compact (CP) - A more user-friendly version of the camera suited to photo and video production environments is intended to ship with a CNC-milled aluminium enclosure and a web-based GUI (graphical user interface) for controlling camera functions through a Wi-Fi connection. AXIOM Beta CP is in development.\n\nAXIOM Beta Extended (EX) - A Shoulder mounted rig incorporating AXIOM Beta, larger cooling fans, data storage facilities and an on-board PC. At time of writing (November 2017) AXIOM Beta EX is presently in concept phase.\n\nResearch into developing the AXIOM Gamma started in March 2015 after the project was awarded Horizon 2020 funding from the European Union. The project was coordinated by the University of Applied Arts Vienna and involved a consortium of European partners including apertus°, af inventions, Antmicro and DENZ. The AXIOM Gamma utilizes a modular hardware concept that allows the camera to be a fully extendible and repairable camera system.\n\n\n"}
{"id": "1333905", "url": "https://en.wikipedia.org/wiki?curid=1333905", "title": "Applied arts", "text": "Applied arts\n\nThe applied arts are all the arts that apply design and decoration to everyday objects in order to make them aesthetically pleasing. The term is used in distinction to the fine arts that produce objects solely to be beautiful or stimulate the intellect. In practice, the two often overlap.\n\n\n\n\n"}
{"id": "24115554", "url": "https://en.wikipedia.org/wiki?curid=24115554", "title": "Associació d'Usuaris de Java de Catalunya", "text": "Associació d'Usuaris de Java de Catalunya\n\nAujac is the acronym of \"Associació d'Usuaris de Java de Catalunya\", non-profit association that intended to group all users of the Java language created by James Gosling at Sun MicroSystems.\n\nAujac (Associació d'Usuaris de Java de Catalunya) born with the initial proposal to bring near the new information technologies to the people. An effort made through one of the most active Java Users Groups in Europe.\n\nAUJAC starts to forge as an idea during the celebration of a Java course and after the initial creation of the JUG of Sabadell by Jordi Pujol Ulied, founder and president of the Aujac, in April of year 2000.\n\nThe interest was very high and soon summoned a first meeting for establishing the statutes and the first Board of Directors.\n\nThe meetings occurs often and with an important collective effort that it is it could make patent in the several encounters that they were made in the current headquarters of the entity, in Sabadell.\n\nThe members founding of AUJAC was:\n\nAt the end of year 2000, was constituted and recognized by the Generalitat de Catalunya, and being inscribed in the \"Registre General d'Entitats Jurídiques\" with number of inscription: 23889 / B\n\nOn the other hand, the AUJAC was recognized by Sun MicroSystems as official JUG of Catalonia.\n\nAnd in year 2003 the AUJAC becomes the most important 16é JUG of the world: Top25 JUG Program.\n\nAccording to the num article. 2 of its statutes:\n\nThe purposes of the association:\n\nAlong its existence, the Aujac is it composed of several boards of directors.\n\nThe members who compose the Board of Directors, as they marked the statutes, were chosen every two years in the General Assembly of Members, and the former members can be presented to the charge without any harm again.\nDuring the first biennium of existence of the AUJAC (2000-2002) the Board of Directors has been composed by the following persons:\n\nIn the 2002 elections, the Board of Directors for the period 2002-2004 was:\n\nIn the periode 2004-2006, the board of directors comes off composed in the following way:\n\nThe main activities of the Aujac it is they centered in three main axes: formation, broadcast and collaboration projects.\n\nIn the program of formation it is intended to:\n\nFacilitate the technology access to all the persons concerned to the Java technologies.\nEstablish a shared plan of formation in all the Catalan area, which they are governed by the same criterion of evaluation and formation, with a unitary program of courses.\n\nThe plan had a remarkable success, and the Barcelona Football club, \"Fundación Once\" and the University Autonòma of Barcelona (UAB) was one of their customers.\n\nThe plan of formation also allowed the creation of a Java Master Course at the \"Fundació Indústries de la Informació\", entity promoted by Mr. Antoni Farrés .\n\nOtherwise, it is also they imparted several courses in the EUIS, Escola Universitària d'Informàtica de Sabadell, ascribed to the UAB.\n\nWith respect to the diffusion of the Java technology, Aujac organized the \"Sun Tech Day\" in Sabadell 2004, and three editions of the \"Jornades Java Catalanes\", and became the first non-enterprise group to organize an event of this kind in Spain, copied afterwards by JavaHispano, which made two editions of a similar event, after having attended the first as a viewer.\n\nAs explained in the definition of the \"Jornades Java Catalanes\" these were:\n\nAujac was pioneer in the Spanish state, the year 2003, in being the first non-profit organization that achieved to gather in a same event about Java, the university world, the enterprise sector and the general society, acting as link of union among them.\n\nThis joint position of initiative and leadership between Aujac and all the Catalan universities, makes the present model have been adopted by other similar entities all over the Spanish state when carrying out similar events certifying the success of the same ones.\n\nThe first \"Jornades Java Catalanes\" was celebrated at the ETSE, \"Escola Técnica Superior d'Enginyeria\" from the \"Universitat Autonòma de Barcelona\", on 25 February 2003.\n\nThe sponsors of this event were:\n\nand the collaborator companies :\n\nWe can find press releases in Vilaweb:\n\nThe second \"Jornades Java Catalanes\" was celebrated at the Universitat Internacional de Catalunya, the 15th and 16 April in 2004.\n\nThe sponsors of this event were:\n\nThe participant companies were::\n\nAnd the collaborator companies were:\n\nThey receive support from:\n\nSome press notes can be consulted to several newspapers like :\n\n The second \"Jornades Java Catalanes\" was celebrated at the \"Sala de Congressos del Parc d’Innovació La Salle\", located at the University Campus of the given university, in 14th and 15 April in 2005.\n\nThe sponsors of this event were:\n\nThe participant companies were::\n\nThey receive support from:\n\nYou can see several press notes on a different sources:\n\n- Press note at COEIC:\n\n- Press note at LaFarga.cat\n\nBelong of the collaboration among the EUIS and AUJAC, a Sun Tech Day was organized by them.\n\nAngela Caicedo, Technology Evangelist of Sun Microsystems, was the speaker of the presentation of the Java2 Standard Edition 5.0 (Tiger).\n\nThat explained the new features and the technical aspects of this last version of the Java platform.\n\nAlso it made demonstrations of how to develop applications helping for us of the new possibilities.\n\nThe session took place on the 20th of December 2004 from 17.30 to 20.00 hours. The assistants received original CDs of Sun and a T-shirt, and raffled books and other gifts.\n\nAlmost 100 persons attended the act.\n\nDuring the act, AUJAC gives a commemorative diploma of gratitude to Mr. Reginald Hutcherson (boss of the Java Technology Evangelist).\n\nYou can you find references of this event in the web page of the EUIS:\n\nThe Aujac promotes several collaboration projects with entities from all over the territory. Basicament spread on two main axes: those activities that offered a series of services to the community, and of another the impulse of the technology in educational areas.\n\nIt is necessary to highlight that in the area of the teaching, the Aujac signs with the UAB University, several agreements of collaboration among those that highlight the teaching courses as well as the direction of career projects under the direction of Jordi Pujol Ulied, president of Aujac.\n\nIt is necessary to highlight that was signed two final university projects:\n\n\nAujac gives every April, the Honour Member to a personality or Catalan entity.\n\nWith this distinction they want to recognize those personalities or entities that with their disposition, implication, dedication and effort, have contributed to the growth and development of the technology society in our country.\n\n2001 Honour Member\n\nIn 2001, Mr. was elected as Honour Member of Aujac.\n\nPolitical icon of the democratic movement in the seventies in Sabadell, was the first democratic mayor of the quoted population, and one of the big social instigators of the post Franco's regime.\n\nInstigator on the other hand, of projects of the technology society, like the creation of the \"Fundació Indústries de la Informació\", with headquarters in Sabadell, or instigator of the projects of electronic administrations in Catalonia.\n\nIt is necessary to highlight that the open speech of the Aujac social presentation was offered by him in the headquarters of the ancient F2I, \"Fundació Indústries de la Informació\", in Sabadell, that today is the Chamber of Commerce of Sabadell.\n\n2002 Honour Member\n\nJoan Daví i Ferrer, became Member of Honor Aujac the year 2002.\n\nJoan Daví i Ferrer, from Terrassa, director of projects in \"Caixa Terrassa\", led in the bank world an opening towards the non-proprietary systems that it led to conceiving unedited bank architectures until then it, and who broke outlines, and especially with the traditional systems established in the financial environments.\n\nNowadays, its designs in computer architectures devoted to the bank services are a referent.\n\nFor all of this reasons, Aujac considered to distinguish it with the Honour member.\n\n2003 Honour Member\n\nConstant fighter towards the own difficulties has always known how to win the battle of the adversities, demonstrating that many times, there are not barriers that can be through with effort, dedication and constancy.\n\nTheir spirit of improvement, together with its professional profile, they act as him a person who has known how to transmit its pupils the illusion about the new technologies.\n\nHim for this qualities and many others, that Dr. Jordi Roig de Zarate was called \"Honour Member\" in year 2003.\n\n2004 Honour Member\n\nThe Mr. Eduard Elias (right), dean of the \"Col.legi Official d'Enginyeria en Informàtica de Catalunya\", was elected \"2004 Honour Member\".\nPromoter and instigator of the Catalan Association of Engineers in Computer Science (ACEI), he was some of the craftsmen of the creation of the Official School of Engineering in Computer Science of Catalonia.\n\nThis Honour Member recognizes years of dedication of the technology association and the effort to attain a regularization of the profession.\n\nHis forces and dedication has allowed the creation of the \"Col.legi Official d'Enginyeria en Informàtica de Catalunya \". Arrange of this, Aujac want to give him this honour member, in 16 of April 2004, in the UNICA University.\n\n2005 Honour Member\n\nAssociació per a Joves TEB, receives on 15 April 2005, during the closing ceremony of \"III Jornades Java Catalanes\", the \"2005 Honour Member\".\n\nWith this distinction it is wants to recognize the effort and dedication carried out, for this entity, that have demonstrated its contribution to the growth and development of information society in zones where the youngsters live in situations of exclusion, and where the new technologies could not be another barrier.\n\nIt is from this project, and other important project like Xarxa-Òmnia, RavalNet and a long list of projects that they have acted how authentic dynamic social agents and of bridges of cohesion there where bags of social exclusion could be produced.\n\nAccording to the agreements taken in the last General Assembly of Members, on Tuesday, 24 October 2006 at 20:00 hours, was approved by over ot the two third parts of the votes, the dissolution of the association.\n\nThe reasons that brought to the dissolution were mainly two, as they were explained at the meeting:\n\nThis close a cycle where AUJAC was the most important non-enterprise entity that promotes the diffusion of Java.\n"}
{"id": "58053955", "url": "https://en.wikipedia.org/wiki?curid=58053955", "title": "Augmented Reality Sandtable", "text": "Augmented Reality Sandtable\n\nThe Augmented Reality Sandtable (ARES) is an interactive, digital sand table that uses augmented reality (AR) technology to create a 3D battlespace map. It was developed by the Human Research and Engineering Directorate (HRED) at the Army Research Laboratory (ARL) to combine the positive aspects of traditional military sand tables with the latest digital technologies to better support soldier training and offer new possibilities of learning. It uses a projector to display a topographical map on top of the sand in a regular sandbox as well as a motion sensor that keeps track of changes in the layout of the sand to appropriately adjust the computer-generated terrain display.\n\nAn ARL study conducted in 2017 with 52 active duty military personnel (36 males and 16 females) found that the participants who used ARES spent less time setting up the table compared to participants who used a traditional sand table. In addition, ARES demonstrated a lower perceived workload score, as measured using the NASA Task Load Index (NASA-TLX) ratings, compared to the traditional sand table. However, there was no significant difference in post-knowledge test scores in recreating the visual map.\n\nThe ARES project was one of the 25 ARL initiatives in development from 1995 to 2015 that focused on visualizing spatial data on virtual or sand table interfaces. It was developed by HRED’s Simulation and Training Technology Center (STTC) with Charles Aumburn as the principal investigator. Collaborations involved with ARES included Dignitas Technologies, Design Interactive (DI), the University of Central Florida’s Institute for Simulation and Training, and the U.S. Military Academy at West Point.\n\nARES was largely designed to be a tangible user interface (TUI), in which digital information can be manipulated using physical objects such as a person’s hand. It was constructed using commercial off-the-shelf components, including a projector, a laptop, an LCD monitor, Microsoft’s Xbox Kinect sensor, and government-developed ARES software. With the projector and Kinect sensor both facing down on the surface of the sandbox, the projector provides a digital overlay over the sand and the Kinect sensor scans the surface of the map to detect any user gestures inside the boundaries of the sandbox.\n\nDuring development, researchers explored the possibility of incorporating ideas such as multi-touch surfaces, 3D holographic displays, and virtual environments. However, budget restrictions limited the implementation of such ideas.\n\nOn September 2014 during the Modern Day Marine exhibition in Quantico, Virginia, researchers from ARL showcased ARES for the first time.\n\nAccording to a 2015 technical report by ARL scientists, ARES is reported to have the following capabilities.\n\n"}
{"id": "53113973", "url": "https://en.wikipedia.org/wiki?curid=53113973", "title": "Avaloq", "text": "Avaloq\n\nAvaloq is a Swiss company that develops and provides software for core banking. The software system, Avaloq Banking Suite, is used by more than 140 banks worldwide.\n\nCore banking is the activity, at the heart of any bank, that deals with information about customers, transactions and account balances. Banks have traditionally bought in software for these purposes rather than developing it internally. More and more functions have had to be added over the years as requirements have changed, for example for online banking. The result has been very complex computer programs that can be difficult to manage. Also, coding changes for compliance to new regulations and for accommodating new products can be problematic.\n\nAs a result, banks have tended to buy in modern comprehensive software from large, specialist firms. This may be run by the bank itself or by an agency running the computing service on behalf of the bank.\n\nAvaloq was founded in Zurich in 1985, under the name \"BZ Informatik Aktiengesellschaft\", functioning as the information technology subsidiary of . In 1991, 30% of the share capital was taken by the employees as a result of a partial management buyout. At that time there were five staff and the cost of the buyout was $200,000.\n\nThe first customer was Swiss National Bank, rapidly followed by five other commercial banks in Switzerland. The system then spread worldwide although it has had little impact in America. Initially the firm was called BZ Informatik, and its product “AdvAntAge”.\n\nIn 1996 the current name was adopted. In 2001 the company split from BZ Bank, as the employees acquired 100% of the share capital.\n\nAvaloq Banking Group AG employs 2,500 staff, 500 of which are programmers. It has annual revenues of over $500 million and its software accounts for bank deposits to the sum of $4 trillion. The founder and chairman is Francisco Fernandez. \n\nThe Avaloq Banking Suite software is used by over 450 customers including HSBC, Barclays, Royal Bank of Scotland, UBS, Deutsche Bank, Nomura and Societe Generale. Some of the customers have performed business process outsourcing to Avaloq itself for the actual running of the core computing service which uses cloud computing.\nApart from a 10% holding by a Swiss bank, Avaloq is owned by its employees and it has major offices in Zurich, Edinburgh and Manila. In 2016 the firm was rumoured to be seeking investment from private equity investors.\n\nThe security of the data stored by the system is of the utmost importance. To guard against cyber attack the firm engages a number of companies based in Israel to attempt to break through the security systems and then help plug the weaknesses uncovered.\n"}
{"id": "13064631", "url": "https://en.wikipedia.org/wiki?curid=13064631", "title": "Baggage allowance", "text": "Baggage allowance\n\nOn the commercial transportation, mostly with airlines, the baggage allowance is the amount of checked baggage or hand/carry-on luggage the company will allow per passenger. There may be limits on the amount that is allowed free of charge, and hard limits on the amount that is allowed.\n\nThe limits vary per airline and depend on the class, elite status, type of ticket, flight origin and destination. If a flight is booked together with another flight it may also have different limits (e.g. if another flight on the same ticket is a long-haul flight). The exact baggage conditions are mentioned in the ticket information online.\n\nOn aircraft, there are two types of baggage, which are treated differently: checked baggage and hand/carry-on luggage. For both types, transportation companies have rules on the weight and size.\n\nFor checked baggage, stored in the aircraft hold, usually the weight is the limiting factor. All checked items are generally weighed by the airline during check-in, and if they exceed the limit, the passenger is informed by the airline. To avoid any fees, the passenger often must switch some of the items found in the suitcase to another suitcase, or else carry it on.\n\nCarry-on luggage is judged primarily by size. Bags are measured by dimension or in total linear measurement (length + width + height). However, there may also be other restrictions on the types of belongings that can be carried on the plane.\n\nThe International Air Transport Association (IATA) has released recommendations for limits on checked baggage and carry-on luggage. Some companies adhere to these recommendations, some adhere partially and some don't adhere at all to them.\n\nThe recommendations for checked baggage are: advised maximum weight 23 kg (50.71 lbs), weight limit 32 kg (70.55 lbs), advised maximum size 158 cm (62.2 in) length + width + height, limit 203 cm (nearly 80 in). The limit of 23 kg is present because of similar limits in health and safety regulations.\n\nBecause of the wide variation in hand/carry-on luggage limits, in 2015 IATA released a size recommendation for suitcases meant as hand/carry-on luggage. These state that suitcases should have a maximum size of 55 cm (21.65 in) long, 35 cm (13.78 in) wide and 20 cm (7.87 in) deep. If they meet these requirements, the bag may carry the logo \"IATA cabin OK\". This limit is tighter than most current airline limits, so bags with this logo are practically allowed everywhere.\n\nTwo concepts for baggage weight limits are in use.\n\nUnder the Piece Concept, passengers are permitted to check in a certain number of suitcases with a per-bag weight of up to 23 kilograms for Economy Class, and up to 32 kilograms for Business or First Class. The allowed weight per suitcase and the number of suitcases varies per airline and depends on the class, elite status, type of ticket, flight origin and destination.\n\nUnder the Weight Concept, each passenger is permitted to check in a total weight regardless of the number of suitcases. Often passengers traveling together can also combine their allowed weights. The total weight varies per airline and depends on the class, elite status, type of ticket, flight origin and destination.\n\nBaggage fees in the United States have been the norm for many airlines and the trend is predicted to continue with many trans-Atlantic flights starting to collect fees for the lowest cost tickets. IdeaWorks, a travel consulting firm, predicted fees will become the norm by the end of 2019 and globally thereafter.\n\nThe 23 largest airlines in the United States reported earning $4.6 billion in baggage fees in 2017.\n\n\nNew IATA link for reference: http://www.iata.org/whatwedo/passenger/baggage/Pages/check-bag.aspx\n"}
{"id": "3233153", "url": "https://en.wikipedia.org/wiki?curid=3233153", "title": "Black Friday Sale", "text": "Black Friday Sale\n\nBlack Friday Sale is a joint sales initiative by hundreds of online vendors in Germany, Austria and Switzerland, inspired by the American Black Friday. The event was first launched on 28 November 2013: Over its 24-hour run, more than 1.2 million people visited the site, making it the single largest online shopping event in German speaking countries. Many leading online retailers such as Zalando, Disney Store, Saturn, Galeria Kaufhof, Deichmann, The Body Shop, Sony, Yves Rocher and Dell participated in the event in 2013.\nOnce the Black Friday Sale started at 7 pm on 28 November 2013, more than 50.000 users simultaneously accessed the site, temporarily causing delays and server overloads. The overwhelming numbers of visitors also caused the servers of several online shops, such as the electronics store Saturn, to suffer significant downtime. Despite the technical problems, the first Black Friday Sale generated a significant turnover and was considered a notable success by online retailers and organizers alike.\n\nAfter successfully launching in 2013, and having a successful 2014, 2015 and 2016, the Black Friday Sale is set to return on 24 November 2017. Online shoppers are expected to spend billions on this Black Friday sale. Once more, several hundred leading online vendors will be involved, among them Douglas Holding, HP, Sony, Tally Weijl and Weltbild. eBay and PayPal are also featured as official partners of the event.\n"}
{"id": "2113262", "url": "https://en.wikipedia.org/wiki?curid=2113262", "title": "Body powder", "text": "Body powder\n\nBody powder is the generic name for alternatives to talcum powder. It is usually made from a combination of tapioca flour, rice flour, cornstarch, kaolin, arrowroot powder, and/or orrisroot powder, but also other powders may be used. In addition, water absorbing and water binding agents may be added such as polyacrylamide.\n"}
{"id": "2382041", "url": "https://en.wikipedia.org/wiki?curid=2382041", "title": "Camlock (electrical)", "text": "Camlock (electrical)\n\nA camlock, or also cam lock and Cam-Lok, is an interchangeable single-pole electrical connector often used in temporary electrical power production and distribution predominantly used in the North America. Originally a trade name as Cam-Lok, it is now a generic term.\n\nThe most common form is the \"16\" series, rated at 400 amperes with 105 °C terminations. Also in common use is the \"15\" series (\"mini-cam\"), rated at 150 amperes. A larger version is made denoted as the \"17\" series with ratings up to 760 A. A ball nose version and a longer nose standard version exist-the latter is the most common. Another version is the \"Posi-lok\" which has controlled interconnection sequencing to a panel and a shrouded connector body. The early version original connector was hot-vulcanized to the cable body; later versions use dimensional pressure to exclude foreign material from the connector pin area;the tail of the connector insulator body is trimmable to fit the cable outer diameter.\n\nThey are generally used where more than 50 A is required or 3 phase temporary connections are needed, and to connect large generators or building disconnects to distribution panels,or for test connections to load bank equipment for generator load certification; special events and the interconnection of entertainment lighting and sound equipment to power sources. They are usually found only in professional environments, where connections are performed by qualified personnel.\n\nThe colors correspond to the different functions of the terminals. Extra caution should be used to verify correct phase in relation to color coordination with foreign companies and equipment in traveling productions. The common American colors are: \n\nIt is very often assumed that these colors are dictated by the National Electric Code (NEC), however they are not. The NEC only requires that green must only be used for the equipment grounding conductor (NEC Article 250.119) and only white or grey for the neutral (grounded) conductor (NEC Article 200.6). These colors may not be used for any other purpose, nor may their purpose use a different color. No other colors are specified by the NEC for general power distribution.\n\nThe UK system uses the following colour codes, but as the use of Camlocks has been declining, it is very unlikely to find any matching the new colour codes:\nExtra care should be taken when connecting together systems using old and new colour codes, especially as black was originally used to indicate neutral and is now a phase colour, and blue which used to denote a phase is used to denote neutral.\n\n"}
{"id": "5911373", "url": "https://en.wikipedia.org/wiki?curid=5911373", "title": "Chemical computer", "text": "Chemical computer\n\nA chemical computer, also called reaction-diffusion computer, BZ computer (stands for Belousov–Zhabotinsky computer) or gooware computer is an unconventional computer based on a semi-solid chemical \"soup\" where data are represented by varying concentrations of chemicals. The computations are performed by naturally occurring chemical reactions.\n\nOriginally chemical reactions were seen as a simple move towards a stable equilibrium which was not very promising for computation. This was changed by a discovery made by Boris Belousov, a Soviet scientist, in the 1950s. He created a chemical reaction between different salts and acids that swing back and forth between being yellow and clear because the concentration of the different components changes up and down in a cyclic way. At the time this was considered impossible because it seemed to go against the second law of thermodynamics, which says that in a closed system the entropy will only increase over time, causing the components in the mixture to distribute themselves till equilibrium is gained and making any changes in the concentration impossible. But modern theoretical analyses shows sufficiently complicated reactions can indeed comprise wave phenomena without breaking the laws of nature. (A convincing directly visible demonstration was achieved by Anatol Zhabotinsky with the Belousov–Zhabotinsky reaction showing spiraling colored waves.)\n\nThe wave properties of the BZ reaction means it can move information in the same way as all other waves. This still leaves the need for computation, performed by conventional microchips using the binary code transmitting and changing ones and zeros through a complicated system of logic gates. To perform any conceivable computation it is sufficient to have NAND gates. (A NAND gate has two bits input. Its output is 0 if both bits are 1, otherwise it's 1). In the chemical computer version logic gates are implemented by concentration waves blocking or amplifying each other in different ways.\n\nIn 1989 it was demonstrated how light-sensitive chemical reactions could perform image processing. This led to an upsurge in the field of chemical computing.\nAndrew Adamatzky at the University of the West of England has demonstrated simple logic gates using reaction–diffusion processes. Furthermore, he has theoretically shown how a hypothetical \"2 medium\" modelled as a cellular automaton can perform computation. Adamatzky was inspired by a theoretical article on computation by using balls on a billiard table to transfer this principle to the BZ-chemicals and replace the billiard balls with waves: if two waves meet in the solution, they create a third wave which is registered as a 1. He has tested the theory in practice and is working to produce some thousand chemical versions of logic gates to create a chemical pocket calculator.\nOne of the problems with the present version of this technology is the speed of the waves; they only spread at a rate of a few millimeters per minute. According to Adamatzky, this problem can be eliminated by placing the gates very close to each other, to make sure the signals are transferred quickly. Another possibility could be new chemical reactions where waves propagate much faster.\n\nIn 2014, a chemical computing system was developed by an international team headed by the Swiss Federal Laboratories for Materials Science and Technology (Empa). The chemical computer used surface tension calculations derived from the Marangoni effect using an acidic gel to find the most efficient route between points A and B, outpacing a conventional satellite navigation system attempting to calculate the same route.\n\nIn 2015, Stanford University graduate students created a computer using magnetic fields and water droplets infused with magnetic nanoparticles, illustrating some of the basic principles behind a chemical computer.\n\nIn 2015, University of Washington students created a programming language for chemical reactions (originally developed for DNA analysis).\n\n\n"}
{"id": "5645363", "url": "https://en.wikipedia.org/wiki?curid=5645363", "title": "Clean-in-place", "text": "Clean-in-place\n\nClean-in-place (CIP) is a method of cleaning the interior surfaces of pipes, vessels, process equipment, filters and associated fittings, without disassembly.\n\nUp to the 1950s, closed systems were disassembled and cleaned manually. The advent of CIP was a boon to industries that needed frequent internal cleaning of their processes. Industries that rely heavily on CIP are those requiring high levels of hygiene, and include: dairy, beverage, brewing, processed foods, pharmaceutical, and cosmetics.\n\nThe benefit to industries that use CIP is that the cleaning is faster, less labor-intensive and more repeatable, and poses less of a chemical exposure risk. CIP started as a manual practice involving a balance tank, centrifugal pump, and connection to the system being cleaned. Since the 1950s, CIP has evolved to include fully automated systems with programmable logic controllers, multiple balance tanks, sensors, valves, heat exchangers, data acquisition and specially designed spray nozzle systems. Simple, manually operated CIP systems can still be found in use today.\n\nDepending on soil load and process geometry, the CIP design principle is one of the following:\nElevated temperature and chemical detergents are often employed to enhance cleaning effectiveness.\n\nTemperature of the cleaning solution. Elevating the temperature of a cleaning solution increases its dirt removal efficiency. Molecules with high kinetic energy dislodge dirt faster than slow moving molecules of a cold solution.\n\nConcentration of the cleaning agent. A concentrated cleaning solution will clean a dirty surface much better than a dilute one due to the increased surface binding capacity.\n\nContact time of the cleaning solution. The longer the detergent contact period, the higher the cleaning efficiency. After some time, the detergent eventually dissolves the hard stains/soil from the dirty surface.\n\nPressure exerted by the cleaning solution (or turbulence). The turbulence creates an abrasive force that dislodges stubborn soil from the dirty surface.\n\nOriginally developed for cleaning closed systems as described above, CIP has more recently been applied to groundwater source boreholes used for high end-uses such as natural mineral / spring waters, food production and carbonated soft drinks (CSD).\n\nBoreholes that are open to the atmosphere are prone to a number of chemical and microbiological problems, so sources for high end-use are often sealed at the surface (headworks). An air filter is built into the headworks to permit the borehole to inhale and exhale when the water level rises and falls quickly (usually due to the pump being turned on and off) without drawing in airborne particles or contaminants (spores, molds, fungi, bacteria, etc.).\n\nIn addition, CIP systems can be built into the borehole headworks to permit the injection of cleaning solutions (such as sodium hypochlorite or other sanitizers) and the subsequent recirculation of the mix of these chemicals and the groundwater. This process cleans the borehole interior and equipment without any invasive maintenance being required.\n\nCIP is commonly used for cleaning bioreactors, fermenters, mix vessels, and other equipment used in biotech manufacturing, pharmaceutical manufacturing and food and beverage manufacturing. CIP is performed to remove or obliterate previous cell culture batch components. It is used to remove in-process residues, control bioburden, and reduce endotoxin levels within processing equipment and systems. Residue removal is accomplished during CIP with a combination of heat, chemical action, and turbulent flow.\n\nThe U.S. Food and Drug Administration published a CIP regulation in 1978 applicable to pharmaceutical manufacturing. The regulation states, “Equipment and utensils shall be cleaned, maintained, and sanitized at appropriate intervals to prevent malfunctions or contamination that would alter the safety, identity, strength, quality or purity of the drug product beyond the official or other established requirements.”\n\nRepeatable, reliable, and effective cleaning is of the utmost importance in a manufacturing facility. Cleaning procedures are validated to demonstrate that they are effective, reproducible, and under control. In order to adequately clean processing equipment, the equipment must be designed with smooth stainless steel surfaces and interconnecting piping that has cleanable joints. The chemical properties of the cleaning agents must properly interact with the chemical and physical properties of the residues being removed.\n\nA typical CIP cycle consists of many steps which often include (in order):\n\nCritical parameters must be met and remain within the specification for the duration of the cycle. If the specification is not reached or maintained, cleaning will not be ensured and will have to be repeated. Critical parameters include temperature, flow rate/supply pressure, chemical concentration, chemical contact time, and final rinse conductivity (which shows that all cleaning chemicals have been removed).\n\n\n"}
{"id": "2466187", "url": "https://en.wikipedia.org/wiki?curid=2466187", "title": "Cold chain", "text": "Cold chain\n\nA cold chain or cool chain is a temperature-controlled supply chain. An unbroken cold chain is an uninterrupted series of refrigerated production, storage and distribution activities, along with associated equipment and logistics, which maintain a desired low-temperature range. It is used to preserve and to extend and ensure the shelf life of products, such as fresh agricultural produce, seafood, frozen food, photographic film, chemicals, and pharmaceutical drugs. Such products, during transport and when in transient storage, are sometimes called cool cargo. Unlike other goods or merchandise, cold chain goods are perishable and always en route towards end use or destination, even when held temporarily in cold stores and hence commonly referred to as cargo during its entire logistics cycle.\n\nCold chain logistics includes all of the means used to ensure a constant temperature for a product that is not heat stable, from the time it is manufactured until the time it is used. Moreover, cold chain is considered as a science, a technology and a process. It is a science as it requires the understanding of the chemical and biological processes associated with product perishability. It is a technology as it relies on physical means to ensure desirable temperature conditions along the supply chain. It is a process as a series of tasks must be performed to manufacture, store, transport and monitor temperature sensitive products.\n\nMobile refrigeration was innovated in 1940 by Frederick McKinley Jones, who founded Thermo King.\n\nCold chains are common in the food and pharmaceutical industries and also in some chemical shipments. One common temperature range for a cold chain in pharmaceutical industries is , but the specific temperature (and time at temperature) tolerances depend on the actual product being shipped. Unique to fresh produce cargoes, the cold chain requires to additionally maintain product specific environment parameters which include air quality levels (carbon dioxide, oxygen, humidity and others), which makes this the most complicated cold chain to operate.\n\nThis is important in the supply of vaccines to distant clinics in hot climates served by poorly developed transport networks. Disruption of a cold chain due to war may produce consequences similar to the smallpox outbreaks in the Philippines during the Spanish–American War.\n\nThere have been numerous events where vaccines have been shipped to third world countries with little to no cold chain infrastructure (Sub-Sahara Africa) where the vaccines were inactivated due to excess exposure to heat. Patients that thought they were being immunized, in reality were put at greater risk due to the inactivated vaccines they received. Thus great attention is now being paid to the entire cold chain distribution process to ensure that simple diseases can eventually be eradicated from society.\n\nTraditionally all historical stability data developed for vaccines was based on the temperature range of . With recent development of biological products by former vaccine developers, biologics has fallen into the same category of storage at due to the nature of the products and the lack of testing these products at wider storage conditions.\n\nThe cold chain distribution process is an extension of the good manufacturing practice (GMP) environment that all drugs and biological products are required to adhere to, enforced by the various health regulatory bodies. As such, the distribution process must be validated to ensure that there is no negative impact to the safety, efficacy or quality of the drug substance. The GMP environment requires that all processes that might impact the safety, efficacy or quality of the drug substance must be validated, including storage and distribution of the drug substance.\n\nA cold chain can be managed by a quality management system. It should be analyzed, measured, controlled, documented, and validated.\n\nThe overall approach to validation of a distribution process is by building more and more qualifications on top of each other to get to a validated state. This is done by executing a component qualification on the packaging components, an operational qualification to demonstrate that the process performs at the operational extremes and finally a performance qualification that demonstrates that what happens in the real world is within the limits of what was demonstrated in the operational qualification limits.\n\nPerforming thermal testing can also help with validating the cold chain. Certified test labs use environmental chambers to simulate ambient profiles that a package may encounter in the distribution cycle. Thermocouple probes and separate temperature dataloggers measure temperatures within the product load to determine the response of the package to the test conditions. Replicate testing based on a qualification protocols is used to create a final qualification report that can be used to defend the configuration when audited by regulators. It is normally best to have an individual that understands the principles of validation, when defending such processes to a federal regulatory body of any nation.\n\nCold chains need to be evaluated and controlled: \n\nDuring the distribution process one should monitor that process until one builds a sufficient data set that clearly demonstrates the process is in compliance and in a state of control. Each time the process does not conform to the process, the event should be properly documented, investigated and corrected so that the temperature excursion do not occur on future shipments. Any anomaly is thus considered to be a Non Conformance and should be assigned as a trackable event. The event must be reported immediately when it is identified and it is the expectation of the FDA that all adverse events be documented and investigated. The investigation should be completed in a timely manner and must come to some form of a \"root cause\" and also some form of \"corrective action\". The system may potentially stay in a Validated state if the root cause identifies that a Standard Operating Procedure (SOP) was not followed or followed incorrectly. If however a SOP needs to be changed or modified, then the system must be re-validated to demonstrate that the change to the SOP maintains the integrity of the process/system. A Non-Conformance may also generate a Corrective Action Preventative Action (CAPA), again, a documented process to make corrective or preventative actions to SOP's and other documents.\n\nNon Conformances and CAPA's are an essential part of the overall Quality System in the cGMP environment. Tracking and trending of these events will also allow businesses to monitor the overall \"health\" of the systems in place. Excessive Non Conformances can quickly identify areas of concern for management and allow for corrective actions to be taken. During regulatory inspections of quality systems, inspectors will frequently ask to review a list of all \"open\" Non Conformances\" so that they can quickly assess how an organization is processing these events and ensuring they are dealt with in a timely manner.\n\nThus the process is continually evolving and correcting for anomalies that occur in the process. Eventually the process can evolve into periodic monitoring once sufficient data demonstrates that the process is in a state of control. Any anomaly that occurs once a process is in a state of control may result in the process being invalidated and not in control and could potentially result in product withdraw from the market to ensure patient safety. A formal product withdraw is only done when the quality, safety or efficacy of a product is questionable. A single anomaly would not necessarily require a product withdraw if there is sufficient stability data that demonstrates that excursions will not affect product quality.\n\nIt is necessary to develop an internal documentation system as well as multi-party communication standards and protocols to transfer or create a central repository or hub to track information across the supply chain. These systems would monitor equipment status, product temperature history, and custody chain, etc. These help ensure that a food, pharmaceutical, or vaccine is safe and effective when reaching its intended consumer. It is also important to have a complete chain of custody for the entire life cycle of a product, so there is documented evidence as to whom had control of the product throughout the lifecycle of the product, up to the final users consumption of the product.\n\n"}
{"id": "4579112", "url": "https://en.wikipedia.org/wiki?curid=4579112", "title": "Contactless smart card", "text": "Contactless smart card\n\nA contactless smart card is a contactless credential whose dimensions are credit-card size. Its embedded integrated circuits can store (and sometimes process) data and communicate with a terminal via NFC. Commonplace uses include transit tickets, bank cards and passports.\n\nThere are two broad categories of contactless smart cards. Memory cards contain non-volatile memory storage components, and perhaps some specific security logic. Contactless smart cards contain read-only RFID called CSN (Card Serial Number) or UID, and a re-writeable smart card microchip that can be transcribed via radio waves.\n\nA contactless smart card is characterized as follows:\n\nContactless smart cards can be used for identification, authentication, and data storage. They also provide a means of effecting business transactions in a flexible, secure, standard way with minimal human intervention.\n\nContactless smart cards were first used for electronic ticketing in 1995 in Seoul, South Korea.\n\nSince then, smart cards with contactless interfaces have been increasingly popular for payment and ticketing applications such as mass transit. Globally, contactless fare collection is being employed for efficiencies in public transit. The various standards emerging are local in focus and are not compatible, though the MIFARE Classic card from Philips has a large market share in the United States and Europe.\n\nIn more recent times, Visa and MasterCard have agreed to standards for general \"open loop\" payments on their networks, with millions of cards deployed in the U.S., in Europe and around the world.\n\nSmart cards are being introduced in personal identification and entitlement schemes at regional, national, and international levels. Citizen cards, drivers’ licenses, and patient card schemes are becoming more prevalent. In Malaysia, the compulsory national ID scheme MyKad includes 8 different applications and is rolled out for 18 million users. Contactless smart cards are being integrated into ICAO biometric passports to enhance security for international travel.\n\nContactless smart card readers use radio waves to communicate with, and both read and write data on a smart card. When used for electronic payment, they are commonly located near PIN pads, cash registers and other places of payment. When the readers are used for public transit they are commonly located on fare boxes, ticket machines, turnstiles, and station platforms as a standalone unit. When used for security, readers are usually located to the side of an entry door.\n\nA contactless smart card is a card in which the chip communicates with the card reader through an induction technology similar to that of an RFID (at data rates of 106 to 848 kbit/s). These cards require only close proximity to an antenna to complete a transaction. They are often used when transactions must be processed quickly or hands-free, such as on mass transit systems, where a smart card can be used without even removing it from a wallet.\n\nThe standard for contactless smart card communications is ISO/IEC 14443. It defines two types of contactless cards (\"A\" and \"B\") and allows for communications at distances up to . There had been proposals for ISO/IEC 14443 types C, D, E, F and G that have been rejected by the International Organization for Standardization. An alternative standard for contactless smart cards is ISO/IEC 15693, which allows communications at distances up to .\n\nExamples of widely used contactless smart cards are Taiwan's EasyCard, Hong Kong's Octopus card, Shanghai's Public Transportation Card, South Korea's T-money (bus, subway, taxi), London's Oyster card, Beijing's Municipal Administration and Communications Card, Southern Ontario's Presto card, Japan Rail's Suica Card, the San Francisco Bay Area's Clipper Card, Melbourne's Myki Card, Sydney's Opal Card and India's More Card which predate the ISO/IEC 14443 standard. The following tables list smart cards used for public transportation and other electronic purse applications.\n\nA related contactless technology is RFID (radio frequency identification). In certain cases, it can be used for applications similar to those of contactless smart cards, such as for electronic toll collection. RFID devices usually do not include writeable memory or microcontroller processing capability as contactless smart cards often do.\n\nThere are dual-interface cards that implement contactless and contact interfaces on a single card with some shared storage and processing. An example is Porto's multi-application transport card, called Andante, that uses a chip in contact and contactless (ISO/IEC 14443 type B) mode.\n\nLike smart cards with contacts, contactless cards do not have a battery. Instead, they use a built-in inductor, using the principle of resonant inductive coupling, to capture some of the incident electromagnetic signal, rectify it, and use it to power the card's electronics.\n\nSince the start of using the Seoul Transportation Card, numerous cities have moved to the introduction of contactless smart cards as the fare media in an automated fare collection system.\n\nIn a number of cases these cards carry an electronic wallet as well as fare products, and can be used for low-value payments.\n\nStarting around 2005, a major application of the technology has been contactless payment credit and debit cards. Some major examples include:\n\n\nRoll-outs started in 2005 in the United States, and in 2006 in some parts of Europe and Asia (Singapore). In the U.S., contactless (non PIN) transactions cover a payment range of ~$5–$100.\n\nIn general there are two classes of contactless bank cards: magnetic stripe data (MSD) and contactless EMV.\n\nContactless MSD cards are similar to magnetic stripe cards in terms of the data they share across the contactless interface. They are only distributed in the U.S. Payment occurs in a similar fashion to mag-stripe, without a PIN and often in off-line mode (depending on parameters of the terminal). The security level of such a transaction is better than a mag-stripe card, as the chip cryptographically generates a code which can be verified by the card issuer's systems.\n\nContactless EMV cards have two interfaces (contact and contactless) and work as a normal EMV card via their contact interface. The contactless interface provides similar data to a contact EMV transaction, but usually a subset of the capabilities (e.g. usually issuers will not allow balances to be increased via the contactless interface, instead requiring the card to be inserted into a device which uses the contact interface). EMV cards may carry an \"offline balance\" stored in their chip, similar to the electronic wallet or \"purse\" that users of transit smart cards are used to.\n\nA quickly growing application is in digital identification cards. In this application, the cards are used for authentication of identity. The most common example is in conjunction with a PKI. The smart card will store an encrypted digital certificate issued from the PKI along with any other relevant or needed information about the card holder. Examples include the U.S. Department of Defense (DoD) Common Access Card (CAC), and the use of various smart cards by many governments as identification cards for their citizens. When combined with biometrics, smart cards can provide two- or three-factor authentication. Smart cards are not always a privacy-enhancing technology, for the subject carries possibly incriminating information about him all the time. By employing contactless smart cards, that can be read without having to remove the card from the wallet or even the garment it is in, one can add even more authentication value to the human carrier of the cards.\n\nThe Malaysian government uses smart card technology in the identity cards carried by all Malaysian citizens and resident non-citizens. The personal information inside the smart card (called MyKad) can be read using special APDU commands.\n\nSmart cards have been advertised as suitable for personal identification tasks, because they are engineered to be tamper resistant. The embedded chip of a smart card usually implements some cryptographic algorithm. There are, however, several methods of recovering some of the algorithm's internal state.\n\nDifferential power analysis\ninvolves measuring the precise time and electric current required for certain encryption or decryption operations. This is most often used against public key algorithms such as RSA in order to deduce the on-chip private key, although some implementations of symmetric ciphers can be vulnerable to timing or power attacks as well.\n\nSmart cards can be physically disassembled by using acid, abrasives, or some other technique to obtain direct, unrestricted access to the on-board microprocessor. Although such techniques obviously involve a fairly high risk of permanent damage to the chip, they permit much more detailed information (e.g. photomicrographs of encryption hardware) to be extracted.\n\n\n"}
{"id": "14092549", "url": "https://en.wikipedia.org/wiki?curid=14092549", "title": "Coolpower", "text": "Coolpower\n\nA coolpower setup in a truck engine can use an aftercooler mounted on top of the engine with a tip turbine fan, or an air-to-air cooler in front of the radiator. In either case, the goal is to cool the hot, compressed air going from the turbocharger into the engine. With the tip turbine, a small quantity of compressed air is bled off to spin a small fan that draws air through the aftercooler tubes. In some later models, a water cooler was also used, in addition to the tip turbine setup. Mack called the tip-turbine setup an intercooler and used a pyrometer to alert the operator if the exhaust gas exceeded a limit. The tip turbine setup was compact and the shorter plumbing runs allowed boost to build faster. Later models with the front mounted air-to-air cooler required a larger turbocharger to fill the tubes faster but with the benefit of additional power. Modern trucks use the air-to-air cooler in nearly all cases. \n\nLarge Mack trucks from the 1970s such as the Mack R-600 used coolpower systems. Some coolpower systems used vertical bar shutters that could be opened and shut in front of the radiator to maintain proper operating temperature. \n\nThe Mack Cruise Liner model built in 1984, a 6X4 Primemover, used a \n\nThe term is also used to refer to “Cool Power” air intake systems for turbocharged engines. These systems provide cooler air to the turbocharger and engine, instead of the potentially too-warm air from the engine compartment. Cooler intake air is denser, which means that the engine can produce the same power with less fuel. Cooler air also gives the engine more power for applications such as towing heavy loads up driving up steep grades during the summer. Cooler air drawn into the engine compartment lowers the temperature under the hood, which allows plastic, rubber and electronic parts to last longer. Lastly, cool power systems will supports larger turbochargers by creating additional air flow to the engine.\n\nThe US military uses the expression \"cool power\" to describe its \"regenerative drive unit\", a \"light-weight hybrid hydraulic drive system\" that weighs 330 pounds. The \"system can generate nearly of torque and power equivalent to a engine.\" It operates by storing \"energy normally lost as heat during the braking process in a high-pressure oil tank called an accumulator.\" The system use two hydraulic-fluid storage devices controlled by a central processor. One of the reasons the US military is interested in the system is that its \"cool\" power allows vehicles to move \"... without generating a \"thermal footprint\" that can be identified by enemy tracking systems.\"\n"}
{"id": "1553120", "url": "https://en.wikipedia.org/wiki?curid=1553120", "title": "Delphi (online service)", "text": "Delphi (online service)\n\nDelphi Forums is a U.S. online service provider and since the mid 1990s has been a community internet forum site. It started as a nationwide dialup service in 1983. Delphi Forums remains active as of 2016, claiming 4 million registered members and \"more than 8,000 active Forums\" in 2013.\n\nThe company that became Delphi was founded by Wes Kussmaul as Kussmaul Encyclopedia in 1981, featured ASCII-based encyclopedia, E-mail, and a primitive chat. Newswires, bulletin boards and better chat were added in early 1982.\n\nKussmaul recalled:\n\nDelphi was actually launched in October 1981, at Jerry Milden's Northeast Computer Show, as the Kussmaul Encyclopedia--the world's first commercially available computerized encyclopedia. (Frank Greenagle's Arête Encyclopedia was announced at about the same time, but you couldn't buy it until much later.) The Kussmaul Encyclopedia was actually a complete home computer system (your choice of Tandy Color Computer or Apple II) with a 300-bps modem that dialed up to a VAX computer hosting our online encyclopedia database. We sold the system for about the same price and terms as Britannica. People wandered around in it and were impressed with the ease with which they could find information. We had a wonderful cross-referencing system that turned every occurrence of a word that was the name of an entry in the encyclopedia into a hypertext link—in 1981...\n\nIn November 1982 Wes hired Glenn McIntyre as a software engineer primarily doing internal systems. Glenn brought in colleagues Kip Bryan and Dan Bruns. Kip wrote the software that became Delphi Conference and Delphi Forums. Dan, (another very strong software person) upon finishing his MBA at Harvard, become President and subsequently CEO when Wes moved on to form Global Villages.\n\nOn March 15, 1983, the Delphi name was first used by General Videotex Corporation. Forums were text-based, and accessed via Telenet, Sprintnet, Tymnet, Uninet, and Datapac (Canada).\n\nDelphi was extended to Argentina in 1985, through a partnership with the Argentine IT company Siscotel S.A.\n\nDelphi partnered with ASCII Corp. of Japan to open online services in 1991.\n\nDelphi provided national consumer access to the Internet in 1992. Features included E-mail (July 1992), FTP, Telnet, Usenet, text-based Web access (November 1992), MUDs, Finger, and Gopher. \"To a lot of people at the time, we seemed to be in an enviable position\" says Dan Bruns, Delphi's CEO. \"But we didn't have a lot of financing to fuel our growth...\"\n\nIn 1993 Delphi was sold to Rupert Murdoch's News Corporation. News Corporation recognized that there would be growth in consumer use of the internet and attempted to use Delphi as its vehicle. It had 125,000 text-based customers in 1995 and had 150 employees. Murdoch hired away IBM's director of high-performance computing and communications, Alan Baratz, in 1994 to run Delphi. Bruns and General Manager Rusty Williams stayed on. Delphi peaked with 500,000 paid subscribers and about 600 employees.\n\nBy 1995, Delphi had lost many of its subscribers, and Bruns left Delphi. In 1996, NewsCorp decided to exit the online business, was laying off almost half of Delphi's employees and wanted to sell or close Delphi. Dan Bruns and some of Delphi's original investors bought Delphi from NewsCorp for an undisclosed amount. With only 50,000 paying subscribers left, Delphi was back to its pre-NewsCorp size. \"We were on the same growth slope, but this time we were going down instead of up,\" he says. \"It felt a little poetic.\"\n\nIn 1996, Delphi launched a free, ad-supported managed-content website with associated message boards and chat rooms, under the management of a team led by Dan Bruns and which included Bill Louden, who had headed GEnie during its heyday. For a period of time, both text-based and web-based community services were available. After a year as a managed content site, Delphi reinvented itself as a community-driven service that allowed anyone to create an online community.\n\nProspero Technologies was formed in January 2000 as the merger of Delphi Forums and Wellengaged. Webpages for forums were discontinued.\n\nIn 2001, Rob Brazell purchased Delphi Forums, merged it with eHow and Idea Exchange, and formed Blue Frogg Enterprises. The Delphi.com domain was sold to Delphi Corporation, the auto parts manufacturer. Prospero partnered with Inforonics.\n\nIn 2002, Prospero reacquired Delphi Forums, joining it with Talk City to form Delphi Forums LLC.\n\nIn 2008, online community developer Mzinga acquired Littleton-based Prospero Technologies LLC, which was then owned by Bruce Buckland, chairman and CEO of Mallory Ventures. In March 2009, a Forrester Research analyst reported on Twitter that Mzinga was having financial difficulties after it had completed a second round of layoffs. On September 1, 2011, Mzinga sold Delphiforums back to early owner Dan Bruns.\nIn February 2013, Delphi Forums celebrated its 30th anniversary. Delphi owner Dan Bruns said \"It's true that the Delphi that launched in 1983 was very different from today's internet,\" Bruns said, \"but one thing remains the same: places like Delphi Forums provide a friendly, comfortable setting for people to share common interests and passions and to build lasting friendships. If we keep that simple truth in mind, we have a terrific legacy to build on going forward.\"\n\nDuring 2014, Delphi Forums began a beta test of a new forum software, called Zeta Delphi. The current long-time format, now called Classic Delphi, also remains, and hosts may use either software.\n\nIn the late 1980s and early 1990s, Delphi had a regular Wednesday night chat group (long before IRC and other chat programs became mainstream). Frequent attendees in this group included several professional science fiction writers, including Susan Casper, Pat Cadigan, Mike Resnick, Michael A. Banks, Jack L. Chalker, Lawrence Watt-Evans, Gardner Dozois, Janet Kagen, Lawrence Person, Martha Soukup, Barbara Delaplace and Orson Scott Card among others.\n\nDelphi Forums are divided into categories:\n\nSome of Delphi's present forums date back to the early to mid 1990s and remain popular today. Amongst those forums:\n\n\n"}
{"id": "1994913", "url": "https://en.wikipedia.org/wiki?curid=1994913", "title": "Diffractometer", "text": "Diffractometer\n\nA diffractometer (pronunciation: di-\"frak-'tä-m&-t&r) is a measuring instrument for analyzing the structure of a material from the scattering pattern produced when a beam of radiation or particles (such as X-rays or neutrons) interacts with it.\n\nBecause it is relatively easy to use electrons or neutrons having wavelengths smaller than a nanometer, electrons and neutrons may be used to study crystal structure in a manner very similar to X-ray diffraction. Electrons do not penetrate as deeply into matter as X-rays, hence electron diffraction reveals structure near the surface; neutrons do penetrate easily and have an advantage that they possess an intrinsic magnetic moment that causes them to interact differently with atoms having different alignments of their magnetic moments.\n\nA typical diffractometer consists of a source of radiation, a monochromator to choose the wavelength, slits to adjust the shape of the beam, a sample and a detector. In a more complicated apparatus, a goniometer can also be used for fine adjustment of the sample and the detector positions. When an area detector is used to monitor the diffracted radiation, a beamstop is usually needed to stop the intense primary beam that has not been diffracted by the sample, otherwise the detector might be damaged. Usually the beamstop can be completely impenetrable to the X-rays or it may be semitransparent. The use of a semitransparent beamstop allows the possibility to determine how much the sample absorbs the radiation using the intensity observed through the beamstop.\n\nThere are several types of X-ray diffractometer, depending of the research field (material sciences, powder diffraction, life sciences, structural biology, etc.) and the experimental environment, if it is a laboratory with its home X-ray source or a Synchrotron.\nIn laboratory, diffractometers are usually a \"all in one\" equipment, including the diffractometer, the video microscope and the X-ray source. Plenty of companies manufacture \"all in one\" equipment for X-ray home laboratory, such as Rigaku, PANalytical, Thermo Fisher Scientific, Bruker, and many others.\n\nThere are fewer diffractometer manufacturers for synchrotron, owing to few numbers of x-ray beamlines to equip and the need of solid expertise of the manufacturer.\nFor material sciences, Huber diffractometers are widely known and, for structural biology, Arinax diffractometers are the reference.\n\nNonetheless, due to few numbers of manufacturers, a large amount of synchrotron diffractometers are \"Homemade\" diffractometers, realized by synchrotron engineers team.\n\n"}
{"id": "2798525", "url": "https://en.wikipedia.org/wiki?curid=2798525", "title": "EmperorLinux", "text": "EmperorLinux\n\nEmperorLinux, Inc. is a reseller who, according to \"PC Magazine\", \"specialize in the sales of pre-configured Linux laptops for companies and individuals that want stable, easy-to use laptops\". EmperorLinux was founded in 1999 by Lincoln Durey, an EE Ph.D. from Tulane University. The company's first product was the BlackPerl Linux laptop, based on a Sony VAIO 505TR with a highly modified Linux kernel. Since 1999, the company has added a range of IBM ThinkPads, Dell Latitudes, and Sharp laptops to its lineup.\n\nThese laptops are available with most major Linux distributions, including Fedora, RHEL, Debian, Ubuntu, and SuSE. Significant improvements to stock Linux distributions come from the empkernel and a carefully configured /etc directory. Supported features include APM and ACPI suspend and hibernate support, CPU throttling, LCD backlight brightness control, wireless, and generally full support of the hardware under Linux.\n\nThe company is privately held and based in Atlanta, Georgia, US.\n\n"}
{"id": "3647027", "url": "https://en.wikipedia.org/wiki?curid=3647027", "title": "Flashback arrestor", "text": "Flashback arrestor\n\nA flashback arrestor or flash arrestor is a gas safety device most commonly used in oxy-fuel welding and cutting to stop the flame or reverse flow of gas back up into the equipment or supply line. It protects the user and equipment from damage or explosions. These devices are mainly used in industrial processes where oxy-fuel gas mixtures are handled and used. Flashback arrestors as safety products are essential to secure the workplaces and working environment. In former times wet flashback arrestors were also used. Today the industry standard is to use dry flashback arrestors with at least two safety elements.\n\nDry flashback arrestors typically use a combination of safety elements to stop a flashback or reverse flow of gas. This type is typically found in cutting and welding applications all over the world. They work equally effectively in all orientations, and need very little maintenance. In many countries or regions they are mandatory to be installed at the gas regulator or gas outlet/ tapping point. Depending on the application they are also often used at the torch side as an additional safety device.\nFlashback arrestors help prevent:\n\n\nAccording to the norm EN 730-1 / ISO 5175 they include a minimum of two safety elements:\n\nA gas non-return valve (NV), which:\n\nand a flame arrestor (FA), which:\n\nIn addition to these two basic safety functions a flashback arrestor can also have a:\n\nThermal cut-off valve (TV), which:\n\nand a pressure-sensitive gas cut-off valve (PV), which stops the gas flow in the event of pressure shocks\n\nThe flashback arrestors are suitable for most technical gases (fuel gases) such as acetylene, hydrogen, methane, propane, propylene and butane as well as oxygen and compressed air.\n\nFlashback arrestors have to be tested for gas non-return, for tightness and for gas flow by a qualified person depending on the country specific regulations.\n\nLiquid seal flame arrestors are liquid barriers following the principle of a siphon where the liquid stops the entering deflagration and/or detonation and extinguishes the flame, they work by bubbling the gas through a non-flammable and ideally non-gas-absorbing liquid, which is typically water. They stop the flame by preventing it from reaching the submerged intake.\n\nThese devices are normally very effective at stopping flashbacks from reaching the protected side of the system. They have the disadvantages of only working in one orientation and tend to be much larger than dry type arrestors. This makes them mainly only suitable for large or fixed installations and the liquid level needs to be constantly checked.\n\nOn smaller units having a pressure release valve to prevent the unit from bursting under a severe flashback, the fluid level should be monitored to keep it always above the intake and not so high that the liquid could splash or overflow into the outlet.\n\n"}
{"id": "19287205", "url": "https://en.wikipedia.org/wiki?curid=19287205", "title": "George Berkeley Ross", "text": "George Berkeley Ross\n\nGeorge Berkeley Ross (January 24, 1918 – September 1, 2006) was an early pioneer of information technology in the American petroleum industry who spearheaded the digitization of the exploration for petroleum.\n\nIn 1936, Ross started his career working as a mail boy for Humble Oil in Midland, Texas, the epicenter of the oil boom. Ross married Virginia Louise Elkin of the well-established Midland family in 1941. Fifty years later after serving on the team that had developed the software to computerize the geological exploration for oil, Ross ended his long and successful career as a Senior Analyst for Exploration Systems in Computer Geology at Exxon.\n\nRoss’s mathematical aptitude took him up the corporate ladder. After short stints as a gas station attendant in Odessa and the mailroom in Midland, he was tapped for the assignment of Field Lab Assistant plotting well logs in the Geologic Lease and Scouting Department, a post that permitted him to work alongside the Scouts in West Texas.\n\nWorld War II intervened, and Ross served as a first lieutenant in the artillery corps of the Ozark Division of the 102nd Infantry in the Allied invasion of Europe.\n\nA member of the US Army Reserve since his high school days in San Antonio, Ross’s mathematical background made him a natural for field artillery. He served with distinction in the European theatre. After landing at Normandy, Ross saw action as the Ozark Division advanced across the border between the Netherlands and Germany. They crossed the Wurm and Roer Rivers and occupied the sector from Homburg to Düsseldorf. In April 1945, Ross and the Ozark Division crossed the Rhine where they encountered stiff opposition in the Wesergebirge, then pushed on to the Elbe, where they halted on orders and established an outpost from Berlin, occupying their position until V-E Day.\n\nDuring the post-war period, Ross remained in the US Army Reserve. Shortly after his return to Texas, Humble’s Chief Scout, Dave Fransen, urged Ross to accept the position of Junior Scout in West Texas. In his new capacity, Ross worked with wildcats drilling in the Kelly-Snyder Field and the Sprayberry Trend Area Field, an area fifty-five miles long and twenty miles wide that now contains two thousand oil wells.\n\nIn 1953, the Rosses moved their growing family to Roswell, New Mexico, which had become a hive of wildcatting that kept Ross busy monitoring leases, expiration dates and drilling schedules.\n\nThe Korean War intervened, and Ross was recalled to active military duty. Serving as a major in Field Artillery Operations with the 1st Cavalry Division in Korea and Japan, Ross supervised supply chain management for forces engaged on the peninsula. After he completed two years of service, Ross came home to Roswell now as a Senior Scout, but soon he and his family returned to corporate headquarters in Midland where he continued to contribute to Humble’s drilling success.\n\nAt the same time, Ross maintained his military commission and served annual tours at the Pentagon under a succession of Secretaries of Defence: Charles E. Wilson; Neil H. McElroy and Thomas S. Gates. With twenty years of military service behind him, Ross retired from the Army, but he remained an active veteran attending annual reunions into this century. It was at the Pentagon in the late fifties where Ross first learned about the potential of computerization and information technology and became privy to the work of the Short Range Committee that led to the origin of COBOL (Common Business-Oriented Language).\n\nThe need to control and manage the torrent of information and geological data flooding into the petroleum corporations inspired Ross and a small group of perceptive industry savants to pursue the adaptation of computing technology and harness it to the geological exploration for oil. In order to contribute to the design of a viable corporate information technology strategy, Ross taught himself COBOL and the IBM Mathematical Formula Translating System (FORTRAN). Very esoteric subjects in 1962, computer programming and software development would have mystified the average person, but for Ross, a naturally gifted mathematician, they were a piece of cake. He was to become a self-taught expert in many fields. In addition to his studies of computer programming, Ross studied law, geology, economics, mathematics and government.\n\nIn the early 1960s, the major companies set up the Permian Basin Well Data System, and Ross was selected for work on three of its key committees. In addition to developing computer codes, Ross helped design formats for reporting data relevant to geological exploration. In 1966, Humble began to organize its own internal Information Systems project, and Ross was appointed a member of the first group in Midland. By this time, Ross had been named Scouting Supervisor, but with the pace of technological change his duties increasingly focused on the analysis of records that could best be done by computerization.\n\nIn 1970, Ross was promoted to his new post as a Senior Analyst in Computer Geology for the Exploration Information Systems based at the Humble corporate headquarters in Houston. In his new capacity, Ross managed the computerization of an important portion of the oil industry, geological exploration. Ross had started working in geological analysis when recordkeeping was completely manual, and he had been an integral player in the digitization of the most crucial part of the industry, exploration for new sources of petroleum.\n\nIn 1986 on his fiftieth anniversary with Humble-Exxon, Ross was the subject of a lengthy article in Exploration Update that highlighted his pivotal contribution to the industry. He retired later that year to enter an active retirement filled with travel, cultural pursuits and charitable activities.\n\nA lifelong enthusiast of museums, libraries, the theatre and cultural institutions, Ross launched into a whirlwind of activities to support worthy causes. The Alley Theater, the Commemorative Air Force, the Pioneers of the Permian Basin, American Indian College, Lindenwood University and Mount Holyoke College all benefited from his patronage.\n\nRoss was keenly interested in the conservation of natural resources, especially petroleum, and he was an authority on public transportation. A great advocate of rail travel, he supported many railroad organizations in Texas and in several other states. Ross firmly believed that government could and should do much more to invest in rebuilding America’s railroads and adapting new light rail systems for urban transport.\n\nIn Europe, Ross admired the much more advanced public transportation systems. While travelling on Eurostar, a high speed train that runs through the Channel Tunnel between Paris and London, Ross experienced a vision of the future of swift, clean, efficient and ecologically sound mass transit that inspired him to carry his message to the powers that be back home in America. Many public officials benefited from Ross’s analyses of the need for a more serious approach to mass transportation in America in general and Texas in particular.\n\nIn addition to his many public and official pursuits, Ross was a highly accomplished ballroom dancer who cut a dashing figure at the Petroleum Club and Lechner’s, a favourite restaurant in Houston. Ross performed elegant versions of the Waltz and the Polka, and he did a masterful and energetic version of the Texas Two-Step.\n\n1. \"Ross,\" Houston Daily Chronicle, September 10, 2006.\n\n2. \"George Berkeley Ross,\" Infinityplus\n\n\"This article incorporates text from George Berkeley Ross / Infinityplus, released under GFDL.\"\n"}
{"id": "32946833", "url": "https://en.wikipedia.org/wiki?curid=32946833", "title": "George Bretz (photographer)", "text": "George Bretz (photographer)\n\nGeorge M. Bretz (1842-1895) was an American photographer who is best known for his photographs of the Northeastern Pennsylvania Coal Region and its coal miners.\n\nA collection of Bretz's original glass plate negatives from the Kohinoor Mine at the Shenandoah Colliery were recently rediscovered at the National Museum of American History. Taken circa 1884, this was one of the earliest fully illuminated photo shoots in an underground mine. These photographs were displayed at the 1884 World Cotton Centennial in New Orleans, and again at the 1893 World’s Columbian Exposition in Chicago. Bretz is also known for his photos of alleged Molly Maguires, radical coal miners who fought against unfair labor practices in the coal fields. For the rest of his life, Bretz was considered an authority on coal mining, and articles about his photography were widely published.\n\n\n"}
{"id": "1410972", "url": "https://en.wikipedia.org/wiki?curid=1410972", "title": "Gigabyte Technology", "text": "Gigabyte Technology\n\nGiga-Byte Technology Co., Ltd. (branded as GIGABYTE) (), is a Taiwanese manufacturer and distributor of computer hardware.\n\nGigabyte is a motherboard vendor, with shipments of 4.8 million motherboards in Q1 2015, while ASUS shipped around 4.5 million motherboards in the same quarter. In 2010, Gigabyte was ranked 17th in \"Taiwan's Top 20 Global Brands\" by the Taiwan External Trade Development Council.\n\nThe company is publicly held and traded on the Taiwan Stock Exchange, stock id number .\n\nGigabyte Technology was established in 1986 by Pei-Cheng Yeh. Gigabyte's components are used in Alienware, Falcon Northwest, CybertronPC, Origin PC and exclusively in Technology Direct (Australia) Desktops with up to a 5 year warranty.\n\nA key feature of the Gigabyte dating back to 2006 were its ultra-durable motherboards, advertised with \"all solid capacitors\". On 8 August 2006 Gigabyte announced a joint venture with Asus. Gigabyte developed the world's first software-controlled power supply in July 2007.\n\nAn innovative method to charge the iPad and iPhone on the computer was introduced by Gigabyte in April 2010. Gigabyte launched the world’s first Z68 motherboard on 31 May 2011, with an on-board mSATA connection for Intel SSD and Smart Response Technology. On 2 April 2012, Gigabyte developed the world's first motherboard with 60A ICs from International Rectifier.\n\nGigabyte designs and manufactures motherboards for both AMD and Intel platforms, and also produces graphics cards and notebooks, with AMD and Nvidia GPUs, including the GeForce GTX 1080 and the Radeon R9 290X.\n\nOther products of Gigabyte also have included desktop computers, tablet computers, ultrabooks, mobile phones, personal digital assistants, server motherboards, server racks, networking equipment, optical drives, computer monitors, mice, keyboards, cooling components, power supplies, and computer cases.\n\nAorus is a registered sub-brand trademark of Gigabyte belonging to Aorus Pte. Ltd. which is a company registered in Singapore. Aorus specializes in gaming related products such as, motherboards, graphics cards, notebooks, Mice, Keyboards, Headsets, Cases and Cpu Coolers.\n\n\n"}
{"id": "45288828", "url": "https://en.wikipedia.org/wiki?curid=45288828", "title": "High-performance instrumented airborne platform for environmental research", "text": "High-performance instrumented airborne platform for environmental research\n\nThe high-performance instrumented airborne platform for environmental research (HIAPER) is a modified Gulfstream V aircraft operated by the Earth Observing Laboratory at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado. The aircraft was purchased brand-new from Gulfstream Aerospace in 2002 and then modified by Lockheed Martin in Greenville, South Carolina over a period of two years, for a total cost of $80 million.\n\nThe aircraft includes a wing mounted cloud radar which allows researchers a high resolution view into snow producing storms. The aircraft is designed and instrumented to observe and measure clouds from the stratosphere.\n\nThe HIAPER cloud radar (HCR) is an airborne, polarimetric, millimeter-wavelength radar capable of cloud remote sensing. Whole air samplers also collect air samples for later analysis on the ground.\n\nData collected by the 2013 HIAPER Pole-to-Pole Observations (HIPPO) campaign is publicly available.\n\n"}
{"id": "13624", "url": "https://en.wikipedia.org/wiki?curid=13624", "title": "High fidelity", "text": "High fidelity\n\nHigh fidelity (often shortened to hi-fi or hifi) is a term used by listeners, audiophiles and home audio enthusiasts to refer to high-quality reproduction of sound. This is in contrast to the lower quality sound produced by inexpensive audio equipment, or the inferior quality of sound reproduction that can be heard in recordings made until the late 1940s.\n\nIdeally, high-fidelity equipment has inaudible noise and distortion, and a flat (neutral, uncolored) frequency response within the human hearing range.\n\nBell Laboratories began experimenting with a range of recording techniques in the early 1930s. Performances by Leopold Stokowski and the Philadelphia Orchestra were recorded in 1931 and 1932 using telephone lines between the Academy of Music in Philadelphia and the Bell labs in New Jersey. Some multitrack recordings were made on optical sound film, which led to new advances used primarily by MGM (as early as 1937) and 20th Century-Fox Film Corporation (as early as 1941). RCA Victor began recording performances by several orchestras using optical sound around 1941, resulting in higher-fidelity masters for 78-rpm discs. During the 1930s, Avery Fisher, an amateur violinist, began experimenting with audio design and acoustics. He wanted to make a radio that would sound like he was listening to a live orchestra—that would achieve high fidelity to the original sound. After World War II, Harry F. Olson conducted an experiment whereby test subjects listened to a live orchestra through a hidden variable acoustic filter. The results proved that listeners preferred high fidelity reproduction, once the noise and distortion introduced by early sound equipment was removed.\n\nBeginning in 1948, several innovations created the conditions that made for major improvements of home-audio quality possible:\n\n\nIn the 1950s, audio manufacturers employed the phrase \"high fidelity\" as a marketing term to describe records and equipment intended to provide faithful sound reproduction. While some consumers simply interpreted \"high fidelity\" as fancy and expensive equipment, many found the difference in quality between \"hi-fi\" and the then standard AM radios and 78 rpm records readily apparent and bought 33⅓ LPs such as RCA's New Orthophonics and London's ffrr (Full Frequency Range Recording, a UK Decca system); and high-fidelity phonographs. Audiophiles paid attention to technical characteristics and bought individual components, such as separate turntables, radio tuners, preamplifiers, power amplifiers and loudspeakers. Some enthusiasts even assembled their own loudspeaker systems. In the 1950s, \"hi-fi\" became a generic term for home sound equipment, to some extent displacing \"phonograph\" and \"record player\".\n\nIn the late 1950s and early 1960s, the development of the Westrex single-groove stereophonic record cutterhead led to the next wave of home-audio improvement, and in common parlance \"stereo\" displaced \"hi-fi\". Records were now played on \"a stereo\". In the world of the audiophile, however, the concept of \"high fidelity\" continued to refer to the goal of highly accurate sound reproduction and to the technological resources available for approaching that goal. This period is regarded as the \"Golden Age of Hi-Fi\", when vacuum tube equipment manufacturers of the time produced many models considered endearing by modern audiophiles, and just before solid state (transistorized) equipment was introduced to the market, subsequently replacing tube equipment as the mainstream technology.\n\nA popular type of system for reproducing music beginning in the 1970s was the integrated music centre—which combined a phonograph turntable, AM-FM radio tuner, tape player, preamplifier, and power amplifier in one package, often sold with its own separate, detachable or integrated speakers. These systems advertised their simplicity. The consumer did not have to select and assemble individual components, or be familiar with impedance and power ratings. Purists generally avoid referring to these systems as high fidelity, though some are capable of very good quality sound reproduction. Audiophiles in the 1970s and 1980s preferred to buy each component separately. That way, they could choose models of each component with the specifications that they desired. In the 1980s, a number of audiophile magazines became available, offering reviews of components and articles on how to choose and test speakers, amplifiers and other components.\n\nListening tests are used by hi-fi manufacturers, audiophile magazines and audio engineering researchers and scientists. If a listening test is done in such a way that the listener who is assessing the sound quality of a component or recording can see the components that are being used for the test (e.g., the same musical piece listened to through a tube power amplifier and a solid state amplifier), then it is possible that the listener's pre-existing biases towards or against certain components or brands could affect their judgment. To respond to this issue, researchers began to use blind tests, in which the researchers can see the components being tested, but the listeners undergoing the experiments can't. In a double-blind experiment, neither the listeners nor the researchers know who belongs to the control group and the experimental group, or which type of audio component is being used for which listening sample. Only after all the data has been recorded (and in some cases, analyzed) do the researchers learn which components or recordings were preferred by the listeners. A commonly used variant of this test is the ABX test. A subject is presented with two known samples (sample \"A\", the reference, and sample \"B\", an alternative), and one unknown sample \"X,\" for three samples total. \"X\" is randomly selected from \"A\" and \"B\", and the subject identifies \"X\" as being either \"A\" or \"B\". Although there is no way to prove that a certain methodology is transparent, a properly conducted double-blind test can prove that a method is \"not\" transparent.\n\nScientific double-blind tests are sometimes used as part of attempts to ascertain whether certain audio components (such as expensive, exotic cables) have any subjectively perceivable effect on sound quality. Data gleaned from these double-blind tests is not accepted by some \"audiophile\" magazines such as \"Stereophile\" and \"The Absolute Sound\" in their evaluations of audio equipment. John Atkinson, current editor of \"Stereophile\", stated (in a 2005 July editorial named \"Blind Tests & Bus Stops\") that he once purchased a solid-state amplifier, the Quad 405, in 1978 after seeing the results from blind tests, but came to realize months later that \"the magic was gone\" until he replaced it with a tube amp. Robert Harley of \"The Absolute Sound\" wrote, in a 2008 editorial (on Issue 183), that: \"...blind listening tests fundamentally distort the listening process and are worthless in determining the audibility of a certain phenomenon.\"\n\nDoug Schneider, editor of the online Soundstage network, refuted this position with two editorials in 2009. He stated: \"Blind tests are at the core of the decades' worth of research into loudspeaker design done at Canada's National Research Council (NRC). The NRC researchers knew that for their result to be credible within the scientific community and to have the most meaningful results, they had to eliminate bias, and blind testing was the only way to do so.\" Many Canadian companies such as Axiom, Energy, Mirage, Paradigm, PSB and Revel use blind testing extensively in designing their loudspeakers. Audio professional Dr. Sean Olive of Harman International shares this view.\n\nStereophonic sound provided a partial solution to the problem of creating the illusion of live orchestral performers by creating a phantom middle channel when the listener sits exactly in the middle of the two front loudspeakers. When the listener moves slightly to the side, however, this phantom channel disappears or is greatly reduced. An attempt to provide for the reproduction of the reverberation was tried in the 1970s through quadraphonic sound but, again, the technology at that time was insufficient for the task. Consumers did not want to pay the additional costs and space required for the marginal improvements in realism. With the rise in popularity of home theater, however, multi-channel playback systems became affordable, and many consumers were willing to tolerate the six to eight channels required in a home theater. The advances made in signal processors to synthesize an approximation of a good concert hall can now provide a somewhat more realistic illusion of listening in a concert hall.\n\nIn addition to spatial realism, the playback of music must be subjectively free from noise, such as hiss or hum, to achieve realism. The compact disc (CD) provides about 90 decibels of dynamic range, which exceeds the 80 dB dynamic range of music as normally perceived in a concert hall. Audio equipment must be able to reproduce frequencies high enough and low enough to be realistic. The human hearing range, for healthy young persons, is 20 Hz to 20,000 Hz.\n\n\"Integrated\", \"mini\", or \"lifestyle\" systems (also known by the older terms \"music centre\" or \"midi system\") contain one or more sources such as a CD player, a tuner, or a cassette deck together with a preamplifier and a power amplifier in one box. Although some high-end manufacturers do produce integrated systems, such products are generally disparaged by audiophiles, who prefer to build a system from \"separates\" (or \"components\"), often with each item from a different manufacturer specialising in a particular component. This provides the most flexibility for piece-by-piece upgrades and repairs.\n\nFor slightly less flexibility in upgrades, a preamplifier and a power amplifier in one box is called an \"integrated amplifier\"; with a tuner, it is a \"receiver\". A monophonic power amplifier, which is called a \"monoblock\", is often used for powering a subwoofer. Other modules in the system may include components like cartridges, tonearms, hi-fi turntables, Digital Media Players, digital audio players, DVD players that play a wide variety of discs including CDs, CD recorders, MiniDisc recorders, hi-fi videocassette recorders (VCRs) and reel-to-reel tape recorders. Signal modification equipment can include equalizers and signal processors.\n\nThis modularity allows the enthusiast to spend as little or as much as they want on a component that suits their specific needs. In a system built from separates, sometimes a failure on one component still allows partial use of the rest of the system. A repair of an integrated system, though, means complete lack of use of the system. Another advantage of modularity is the ability to spend money on only a few core components at first and then later add additional components to the system. Some of the disadvantages of this approach are increased cost, complexity, and space required for the components.\n\nIn the 2000s, modern hi-fi equipment can include signal sources such as digital audio tape (DAT), digital audio broadcasting (DAB) or HD Radio tuners. Some modern hi-fi equipment can be digitally connected using fibre optic TOSLINK cables, universal serial bus (USB) ports (including one to play digital audio files), or Wi-Fi support. Another modern component is the \"music server\" consisting of one or more computer hard drives that hold music in the form of computer files. When the music is stored in an audio file format that is lossless such as FLAC, Monkey's Audio or WMA Lossless, the computer playback of recorded audio can serve as an audiophile-quality source for a hi-fi system.\n\n\n"}
{"id": "11816081", "url": "https://en.wikipedia.org/wiki?curid=11816081", "title": "Hotel Technology Next Generation", "text": "Hotel Technology Next Generation\n\nHospitality Technology Next Generation, formerly Hotel Technology Next Generation, commonly referred to as HTNG, is a global, non-profit trade association serving hotel companies and technology providers. It was founded in 2002 and is governed by a board of directors consisting of senior technology executives from hotel companies. Membership is open to companies and individuals involved with hospitality technology.\n\nThe organization's stated objective is to promote interoperability of the many technology systems used in the hotel industry, such as property management systems, point-of-sale systems, telephone systems, building automation systems, guestroom entertainment systems such as video on demand, security and access control systems, and many others. The organization's members meet regularly in small workgroups, where hotel companies and vendors work together to design interface standards (often using XML), reference architectures, network designs, and hospitality-specific network devices. HTNG holds annual members' meetings in North America, Europe, and Asia.\n\nHTNG operates a certification program for selected specifications, administered by The Open Group. \n\n\n\n"}
{"id": "19834860", "url": "https://en.wikipedia.org/wiki?curid=19834860", "title": "International Accreditation Forum", "text": "International Accreditation Forum\n\nThe International Accreditation Forum, Inc. (IAF) is the world association of Conformity Assessment Accreditation bodies and other bodies interested in conformity assessment in the fields of management systems, products, services, personnel and other similar programs of conformity assessment. Its primary function is to develop a single worldwide program of conformity assessment which reduces risk for business and its customers by assuring them that accredited certificates may be relied upon. Accreditation assures users of the competence and impartiality of the body accredited.\n\nIAF members accredit certification or registration bodies that issue certificates attesting that an organization's management, products or personnel comply with a specified standard (called conformity assessment).\n\nIAF has prepared an informative brochure which provides general information about IAF, its activities, membership and programs.\n\nThe primary purpose of IAF is two-fold. Firstly, to ensure that its accreditation body members only accredit bodies that are competent to do the work they undertake and are not subject to conflicts of interest. The second purpose of the IAF is to establish mutual recognition arrangements, known as Multilateral Recognition Arrangements (MLA), between its accreditation body members which reduces risk to business and its customers by ensuring that an accredited certificate may be relied upon anywhere in the world. The MLA contributes to the freedom of world trade by eliminating technical barriers to trade. IAF works to find the most effective way of achieving a single system that will allow companies with an accredited conformity assessment certificate in one part of the world, to have that certificate recognized else where in the world. The objective of the MLA is that it will cover all accreditation bodies in all countries in the world, thus eliminating the need for suppliers of products or services to be certified in each country where they sell their products or services.\n\nThe highest level of authority in IAF is the Members in a General Meeting. General\nMeetings make decisions and lay down policy in the name of the members. The Board\nis responsible for legal actions to be carried out on behalf of the members, for\ndeveloping broad policy directions for IAF and for ensuring that the day-to-day work of\nIAF is carried out in accordance with policies approved by members.\nThe terms of reference, tasks and duties as defined by the Bylaws and the\nMemorandum of Understanding (MoU) remain unchanged for the members at a\nGeneral Meeting, the Board of Directors and the Secretary.\nThe Executive Committee is responsible to the Board of Directors for the day-to-day\nwork of IAF on the basis of decisions made by the Members and directions by the Board\nof Directors.\nThe operations of all IAF Committees and Subordinate Groups, including the Executive\nCommittee, are subject to the IAF General Procedures\n\nMembership of the IAF consists of Accreditation Bodies (which accredit Certification Bodies, which certify or register conforming organisations). An official list is available online.\n\nMembers recognise other members as \"equivalent\", through accession to the Multilateral Recognition Agreements. These attest that Certificates traced to one member is equivalent to that of another.\n"}
{"id": "14050996", "url": "https://en.wikipedia.org/wiki?curid=14050996", "title": "Jetta (company)", "text": "Jetta (company)\n\nJetta International is an American original equipment manufacturer (OEM) and designer of computer laptops, mainly operating in the East Coast. The company was established in 1991, and is based in Monmouth Junction, New Jersey, (close to Princeton), where its only manufacturing plant is located.\n\n\"Jetbook\", the laptop series manufactured by Jetta, has Intel microprocessors and is supplied with a customizable selection of software. Jetbooks are known in the open source community because they can be purchased without a pre-installed operating system.\n\n"}
{"id": "22048327", "url": "https://en.wikipedia.org/wiki?curid=22048327", "title": "LabelTag", "text": "LabelTag\n\nLabelTag can create a circular label on the data side of any DVD+R or DVD-R disc containing basic information visible to the eye. When burning the data, the label is printed directly behind that data in the same recording session, and on normal recording speed on the same recording layer side. LabelTag works on any disc and does not require a special disc like LightScribe. Currently, LabelTag is an exclusive technology of Lite-On for its DVD writer drives.\n\nAn older alternative disc labeling technique was DiscT@2. To record the label on the opposite or back side of the disc by the drive itself LightScribe and LabelFlash are current standards. Besides that, printing using an inkjet printer is currently widely used.\n\nIn the image below an overview is presented of the system-functions of LabelTag. The PC host application controls the data location of the regular user data, including the structure of the file system. In the host application the user label input is transformed into a bit-map representation. The host sends print commands with the pixel information of the image to the drive. The drive interface, records the regular data, including lead in, session intros and closures, and finally lead out. The drive Interface (IF) part takes care of the interpretation of the pixel information and location of the image on the disc. The drive Servo part finally records the pixel information on the correct location including the encoded line numbering. The drive Servo part controls the record power, motor frequency, pixel frequency and channel bit frequency.\n\nThe label can be added or appended to the disc at any time if disc space is available. The disc status has to be “appendable” prior to the image recording. The label is recorded adjacent to the last recorded user data. It is up to the user to finalize the disc after the label is added. In case the disc remains \"appendable\" more user data or more labels can be added. The first section of the disc (26 mm) is used for all drives to do the start-up calibrations. Therefore, no label can be recorded at the inner diameter of the disc. If there is a label in this area, the risk of poor performance is big.\n\nIf you add a label on the data side of the disc, then this space is no longer available for data. The amount of space it takes depends on the size (width) of the label and the location of the label on the disc. The wider the label and the more it is located on the outside of the disc, the more space it takes. In other words: the bigger the label, the more disc surface is occupied and thus the more data-space you sacrifice. For example: a 5 mm label at the inside requires 10% capacity, and at the outside of the disc about 20% of the disc capacity.\n\nFor LabelTag the Eight-to-Fourteen Modulation (EFM+) channel code properties are modulated to create the visual label. In the EFM+ code table, the user data bytes are transformed into specific sequences of pits and lands, ranging from 3T up to 14T channel bits. To create a pit, a high power pulse from the laser is focused into a small spot on the dye layer. This heats the dye material such that an irreversible transformation of the dye material creates the pit. To create a land, the laser is turned to a low power, such that no transformation in the dye material occurs. In general for DVD+/-R media, the recorded marks have a different visual reflection. Therefore, by recording long sequences of reflective marks or reflective spaces one can create the visible label. To maintain maximum backwards compatibility, LabelTag uses a majority of long marks in the recording layer for a dark pixel, while the bright image pixels are created by a majority of long spaces in the recording layer. Only in that case a sufficient large Differential Phase Detection (DPD) tracking signal is generated. This DPD signal is required for DVD players to jump over the Label area to reach beyond the label appended user data. \n\nDuring the recording the normal servo processes are used. The beam follows the tracks (grooves). The spot is in focus on the recording layer. Each image line is recorded by repeating the same recording pattern several tracks. In case of DVD with a track pitch of 740 nm, a 100μm image line width is obtained by repeating the pattern 132 consecutive tracks. Because every label pixel is constructed by 132 consecutive tracks, higher image sharpness and contrast is obtained compared to DiscT@2.\nEach visible pixel is about 40μm × 100μm (w × l) at the inner radius up to 100μm × 100μm (w × l) at the outer radius. This leads to a picture quality above 250DPI. \n\nAs the label and the user data are recorded in the same dye layer of the DVD+/-R media, and label pattern uses run-lengths within the EFM+ range, the same (castle) write pulse form is used as for the regular user data recordings. To enhance the contrast a higher recording power is optional.\n\nAfter the visual tag is written to the disc, this disc remains accessible, readable and appendable for any legacy drives. For that, the label is recorded within the user data zone of the disc in its own Image Session. Then any regular DVD drive capable to read multi session DVDs is able to access the user data zone beyond the label. Very old DVD drives, supporting only single session DVD, might only retrieve data stored in the first session.\n\nAccess of the LabelTag area itself is prevented by two protection mechanisms:\n\nBesides, directly around the label a buffer zone is present that enhances the seek performance for the adjacent user data zones. Finally, a sufficient DPD tracking signal is required at the image location. For that reason, the bright and dark pixels are created by recording marks and spaces within the EFM+-runlength range.\n\nThe LabelTag format is flexible. It allows as many labels on a disc as space is available. A multisession discs allows burning more data and labels at a later stage. Once the disc is finalized, no more data or labels can be added. But, the more labels you add, the less space you have left for data.\n\nTo record a label on the disc, the application has to send the pixel information with a SCSI command to the drive. \n\nThe application is responsible for:\n\nFor LabelTag no new commands are introduced; the changes are made to the already existing commands. Pixel data describing the pixels for one or more image lines is sent to the drive via the WRITE12 SCSI Write Commands. The line width and number of pixels in each line are set in the LabelTag SCSI mode pages. The codice_1 command is called codice_2 command to emphasize the dedicated LabelTag SCSI mode pages settings. After this command the image data is located in the drive buffer. The drive starts writing after the buffer is full or a codice_3 command is sent.\n\nThe image data is formatted by the application as bitmap into bright pixels and dark pixels. The bright pixel corresponds to bitmap 0 and the dark pixel to bitmap 1. Therefore, LabelTag allows both text and logos to be printed.\n\nIn the LabelTag mode page the following properties are exchanged:\n\nThe application confirms if the drive is capable of making visual images using the LabelTag technology by sending the GETCONFIGURATION command. In case the LabelTag feature is current, a supported disc with an appendable session is inserted into the drive. The printing of the Label can begin. \n\nFirst the FS-Zone1 data is recorded on the next writable address. To write the image following the FS-Zone1, the mode bit is set in the LabelTag mode page. The drive is now in LabelTag mode. The data in TAGWRITE commands contains the Label pixel data. The label starts on the Next Writable Address of the inserted disc. The application sends a SYNCHRONIZE CACHE to the drive to let the drive start the image burning. The application unsets the mode bit in the LabelTag mode page to let the drive go back in data write mode. Before the drive can add the FS-Zone2, the application has to inquire the Next Writable Address from the drive to know where the end address is of the Image Window. The application finishes the LabelTag writing by closing the Image Session. \n\n\n"}
{"id": "15715221", "url": "https://en.wikipedia.org/wiki?curid=15715221", "title": "Latigent", "text": "Latigent\n\nLatigent LLC was an international provider of Business Intelligence, Enterprise RSS, and Call Center Reporting and Analytics software.\n\nLatigent, LLC of Chicago, IL, was founded in 2002, as a privately held and funded company, by Chris Crosby and Jason Kolb, two veterans of the contact center industry. Both founders believed that there was a more efficient way of using the information typically gathered in the contact center to better serve the customer and more profitably serve the enterprise. As a result, the Latigent BlueVue performance management product was born and introduced to the market in 2003.\n\nIn December 2005, Latigent released BlueVue II, a Business Information (BI) suite. Besides reporting functionality and complete data warehousing capabilities, BlueVue II also offered data integration capabilities for performance management.\n\nOn September 27, 2007 Cisco Systems announced a definitive agreement to acquire Latigent LLC.\n\n"}
{"id": "54202632", "url": "https://en.wikipedia.org/wiki?curid=54202632", "title": "Leyland L60", "text": "Leyland L60\n\nThe Leyland L60 was a British vertical six-cylinder opposed-piston two-stroke diesel engine designed by Leyland Motors Ltd in the late 1950's/early 1960's for the Chieftain Main Battle Tank (MBT). The engine was also used in the Vickers MBT and Vijayanta.\n\nThe initial engine choice for the FV4201, the vehicle that would later bear the name 'Chieftain', had in 1954 been a Rolls-Royce diesel V8, however during the Chieftain's design phase NATO changed to requiring a multi-fuel capability in 1957 making the intended Rolls-Royce engine unsuitable, and so a new engine with this capability was required. \n\nLeyland Motors, under the direction of the Fighting Vehicles Research and Development Establishment (FVRDE) at Chertsey, was asked to develop such an engine as an opposed-piston two-stroke diesel of similar design to those previously produced by Napier and Tilling-Stevens, the latter's Commer TS3 engine being particularly highly regarded. \n\nThis chosen configuration apart from being well-suited to multi-fuel use also had the advantages of being of simple design with a low parts count, had low bearing loads, and possessed good cold-starting characteristics. Some technical assistance was provided to Leyland by Rolls-Royce, who by that time had taken over the Napier company. Both Tilling-Stevens and Leyland produced single-cylinder prototype engines for the tank engine project and by 1959 the resulting complete engine design had become the Leyland 60, or L60, with the first engine running that same year.\n\nOne of the reasons the L60's unusual configuration was chosen was so as to obtain as compact a power plant as possible so allowing the height of the vehicle to be kept as low as was practicable, a requirement for the Chieftain's design philosophy. \n\nThe use of the two-stroke cycle allowed for a greater power for a given displacement, the 19 litres of the projected diesel engine being expected to be capable of around the same power as the previous, larger displacement, 27 litre, 600 hp Rover Meteor petrol engine, whilst taking up less room in the engine compartment. Scavenging was performed by a Roots blower.\n\nThe Chieftain's L60 engine and cooling system were designed into an integrated engine-pack which could be changed in-the-field using the crane of an FV434 Armoured Repair Vehicle, which had been designed for this purpose, and a complete engine change took around one-and-a-half, to 2 hours. \n\nThe requirement for an easily changeable engine pack was the result of a British Army analysis of previous tank battles that concluded that the longest likely future tank battle would last no longer than two hours, and so the most demanding requirement expected for any tank engine during wartime would be for it to be run at full power for this total amount of time only, and so it would then be advantageous for it to be removed from the vehicle after the battle and exchanged for another, fresh engine, all within a minimum of time. \n\nThis practice would also allow the engines to be worked-on in properly-equipped REME workshops rather than 'in the field', the engines being exchanged between vehicles and workshops as-and-when required. This philosophy was also applied to the contemporary FV430 series of vehicles.\n\nThe initial production L60 units were, at 585 bhp at 2,100 rpm, down on the designed initial power of 600 bhp, and were plagued with reliability problems such as burnt pistons and internal coolant leaks in to the cylinders, and fan drive belts overstressing fan bearing housings, however reliability was to improve over time with modifications and improvement programmes, such as the \"Sundance\" programme, which also included increased engine power.\n\nFinal production engines were of 750 bhp.\n\nInitially, due to unfamiliarity with the two-stroke engine's different exhaust note and power band compared to a four-stroke engine, and with the resulting difficulty in choosing the correct gear required for the particular driving task, trainee drivers tended to under-rev the engines and use inappropriate gear selections, leading to great difficulty climbing gradients, and when the Chieftain Mk 1 was first introduced some drivers had difficulty climbing the vehicle onto the trailers of Thornycroft Antar tank transporters.\n\n\nIn 1975 all British Chieftains were brought up to Chieftain Mark 5 standard as part of the \"Totem Pole\" programme which included the fitting of all vehicles with the 750 bhp L60 Mark 8A.\n\nThe engine was mated with a Merritt-Brown TN12 triple-differential epicyclic gearbox providing \"regenerative\" steering, a derivative of the system first used on the Churchill tank. The gearbox was semi-automatic foot-operated and had six forward, and two reverse gears. Like the engine, it was designed to be quickly replaceable. The TN12 had originally been developed for the cancelled FV300 light tank series.\n\n\n"}
{"id": "42686695", "url": "https://en.wikipedia.org/wiki?curid=42686695", "title": "Library stack", "text": "Library stack\n\nIn library science and architecture, a stack or bookstack (often referred to as a library building's \"stacks\") is a book storage area, as opposed to a reading area. More specifically, this term refers to a narrow-aisled, multilevel system of iron or steel shelving that evolved in the nineteenth century to meet increasing demands for storage space. An \"open-stack\" library allows its patrons to enter the stacks to browse for themselves; \"closed stacks\" means library staff retrieve books for patrons on request.\n\nFrench architect Henri Labrouste, shortly after making pioneering use of iron in the Bibliotheque Sainte-Genevieve of 1850, created a four story iron stack for the Bibliothèque nationale de France. In 1857, multilevel stacks with grated iron floors were installed in the British Library. In 1876, William R. Ware designed a stack for Gore Hall at Harvard University. In contrast to the structural relationship found in most buildings, the floors of these bookstacks did not support the shelving, but rather the reverse, the floors being attached to, and supported by, the shelving framework. Even the load of the building's roof, and of any non-shelving spaces above the stacks (such as offices), may be transmitted to the building's foundation through the shelving system itself. The building's external walls act as an envelope but provide no significant structural support.\n\nFor the Thomas Jefferson Building of the Library of Congress, completed in 1897, Bernard Richardson Green made a number of alterations to the Gore Hall design, including the use of all-metal shelving. The contract was won by the Snead and Company Ironworks, which went on to install its standardized design in libraries around the country. Notable examples include the Widener Library at Harvard and the seven level stack supporting the Rose Reading Room of the New York Public Library.\n\nThe Library of Congress bookstacks were designed and patented by Bernard R. Green (1843-1914), the engineer in charge of the Library of Congress construction, where this type of book storage was first used. Although the structure was of cast iron, the shelves were made from strips of thin U section steel, designed to be as light as an equivalent pine shelf. The top surface of the U section was ground, polished and 'lacquered' (the constituents of the lacquer are not known). Green designed the stacks to be modular, able to be erected several stories high as a single freestanding structural entity incorporating staircases and floors, and even capable of supporting a roof structure. He designed the shelves so that they could adjust to book sizes using a simple lug system without the need for any bolts or fixings. Although the bookstacks were decorated and very simply embellished, they are of machine-age industrial design.\n\nStacks were typically envisioned for access by library staff fetching books for patrons waiting elsewhere, and so were often built in ways making them unsuitable for public access. Increasing concern with opening stacks to the public, the desire to construct buildings adaptable to changing uses, and concerns over the feasibility of storing truly comprehensive collections of books contributed to the decline of the Snead stack. Angus Snead Macdonald, president of the Snead Company from 1915 to 1952, himself advocated for the transition to modular, open plan libraries in the mid twentieth century.\n\nFor closed stacks, mobile shelving may be used to minimize floor space. With computerized indexing, filing by book size saves up to 40% space as well.\n\n"}
{"id": "18596053", "url": "https://en.wikipedia.org/wiki?curid=18596053", "title": "MiWi", "text": "MiWi\n\nMiWi and MiWi P2P are proprietary wireless protocols designed by Microchip Technology that use small, low-power digital radios based on the IEEE 802.15.4 standard for wireless personal area networks (WPANs). It is designed for low data transmission rates and short distance, cost constrained networks, such as industrial monitoring and control, home and building automation, remote control, low-power wireless sensors, lighting control and automated meter reading.\n\nThe MiWi protocols are supported on certain Microchip PIC and dsPIC microcontrollers. When developing for these platforms, proprietary SDKs and hardware development tools, such as the ZENA wireless packet sniffer, may be used.\n\nMicrochip has released two application notes which present technical information on MiWi. These are not primarily protocol specifications and more than half of these documents are focused on implementing the MiWi protocol on Microchip microcontrollers.\n\nAs of 2009, interoperable third party implementations have not appeared. Unless they do, it will not be clear if those specifications are complete or accurate enough to serve roles other than supporting Microchip's code or being one more proprietary example of a lightweight WPAN stack. Many developers trying to use WPAN technologies have observed that Zigbee seems undesirably complex. Accordingly, there exists a technical niche for a simpler protocols, of which MiWi is a proprietary example.\n\nOn the PIC platform, the MiWi protocol stacks are small foot-print alternatives (3K-17K) to ZigBee (40K-100K), which makes them useful for cost-sensitive applications with limited memory. Although the MiWi software can all be downloaded for free from its official website, there exists a unique restriction and obligation to use it only with Microchip microcontrollers.\nThe MiWi P2P protocol stack supports star and peer-to-peer wireless-network topologies, useful for simple, short-range, wireless node-to-node communication. Additionally, the stack provides sleeping-node, active-scan and energy-detect features while supporting the low-power requirements of battery-operated devices.\n\nIn 2008, Microchip released a 2.4 GHz wireless transceiver module that is compatible with certain Microchip PIC and dsPIC microcontrollers (the Microchip MRF24J40MB), and can be used in production devices. Being ZigBee compliant, and capable of communicating using MiWi wireless protocols, it is based on the IEEE 802.15.4 Wireless PAN standard. Designed only for low-data rates and being low-cost, it has an integrated PCB antenna. The module is regulatory-agency certified for the USA (Federal Communications Commission (FCC)), Canada (Industry Canada) and Europe (ETSI), eliminating the need to receive independent FCC certification for their wireless products.\n\nThe Microchip ZENA (or formerly, Zigbee Enhanced Network Analyzer) is a wireless packet sniffer and network analyzer following the IEEE 802.15.4 specification on the 2.4 GHz band. The ZENA analyzer supports both the ZigBee and MiWi protocols. Accompanying software can analyze network traffic and graphically display decoded packets. It can also display the network topology and the messages as they flow through the network. With the provided key of the network, data on encrypted MiWi networks can be sniffed and viewed as well.\n"}
{"id": "21432806", "url": "https://en.wikipedia.org/wiki?curid=21432806", "title": "Ministry of Energy (Russia)", "text": "Ministry of Energy (Russia)\n\nThe Ministry of Energy of the Russian Federation is, since 2008, the Russian federal ministry responsible for energy policy.\n\nThis ministry was created in May 2008 as part of a reorganization by the incoming government of President Dmitry Medvedev. It is headquartered in Moscow. The former Ministry of Industry and Energy was turned into the Ministry of Industry, whose present Minister is Viktor Khristenko, gaining responsibility for trade policy from the former Ministry of Economic Development and Trade, but losing responsibility for energy policy, which was split off into the new Ministry of Energy. The former Federal Agency for Energy (Rosenergo) was also merged into the new Ministry of Energy.\n\n\n"}
{"id": "23950388", "url": "https://en.wikipedia.org/wiki?curid=23950388", "title": "N-slit interferometer", "text": "N-slit interferometer\n\nThe \"N\"-slit interferometer is an extension of the double-slit interferometer also known as Young's double-slit interferometer. One of the first known uses of \"N\"-slit arrays in optics was illustrated by Newton. In the first part of last century, Michelson described various cases of \"N\"-slit diffraction.\n\nFeynman described thought experiments, of two-slit quantum interference, of electrons using Dirac's notation. This approach was extended to \"N\"-slit interferometers, by Duarte and colleagues in 1989, using narrow-linewidth laser illumination, that is, illumination by indistinguishable photons. The first application of the \"N\"-slit interferometer was the generation and measurement of complex interference patterns. These interferograms are accurately reproduced, or predicted, by the \"N\"-slit interferometric equation for either even (\"N\" = 2, 4, 6,…), or odd (\"N\" = 3, 5, 7,…), numbers of slits.\n\nThe \"N\"-slit laser interferometer, introduced by Duarte, uses prismatic beam expansion to illuminate a transmission grating, or \"N\"-slit array, and a photoelectric detector array (such as a CCD or CMOS) at the interference plane to register the interferometric signal. The expanded laser beam illuminating the \"N\"-slit array is single-transverse-mode and narrow-linewidth. This beam can also take the shape, via the introduction of a convex lens prior to the prismatic expander, of a beam extremely elongated in the propagation plane and extremely thin in the orthogonal plane. This use of one-dimensional (or line) illumination eliminates the need of point-by-point scanning in microscopy and microdensitometry. Thus, these instruments can be used as straight forward \"N\"-slit interferometers or as interferometric microscopes (see section on microscopy).\n\nThe disclosure of this interferometric configuration introduced the use of digital detectors to \"N\"-slit interferometry.\n\n \nThese interferometers, originally introduced for applications in imaging, are also useful in optical metrology and have been proposed for secure optical communications in free space, between spacecraft. This is due to the fact that propagating \"N\"-slit interferograms suffer catastrophic collapse from interception attempts using macroscopic optical methods such as beam splitting. Recent experimental developments include terrestrial intra-interferometric path lengths of 35 meters and 527 meters. \nThese large, and very large, \"N\"-slit interferometers are also being used to study various propagation effects including microscopic disturbances on propagating interferometric signals. This work has yielded the first observation of diffraction patterns superimposed over propagating interferograms.\n\nThese diffraction patterns (as shown in the first photograph) are generated by inserting a spider web fiber (or spider silk thread) into the propagation path of the N-slit interferogram. The position of the spider web fiber is perpendicular to the plane of propagation.\n\n\"N\"-slit interferometers, using large intra interferometric distances, have been found to be effective detectors of clear air turbulence. Here, it should be mentioned that the distortions induced by clear air turbulence upon the interferometric signal are different, in both character and magnitude, from the catastrophic collapse resulting from attempted interception of optical signals using macroscopic optical elements such as beam splitters.\n\nAs previously mentioned the original application of the \"N\"-slit laser interferometer was \"interferometric imaging\". In particular, the one dimensionally expanded laser beam (with a cross section 25-50 mm wide by 10-25 μm high) was used to illuminate imaging surfaces (such as silver-halide films) to measure the microscopic density of the illuminated surface. Hence the use of the description \"interferometric microdensitometer\". Resolution down to the nano regime can be provided via the use of interinterferometric calculations. When used as a microdensitometer the \"N\"-slit interferometer is also known as a \"laser microdensitometer\".\n\nThe multiple-prism expanded laser beam is also described as an extremely elongated laser beam. The elongated dimension of the beam (25-50 mm) is in the plane of propagation while the very thin dimension (in the μm regime) of the beam is in the orthogonal plane. This was demonstrated, for imaging and microscopy applications, in 1993. Alternative descriptions of this type of extremely elongated illumination include the terms line illumination, linear illumination, thin light sheet illumination (in light sheet microscopy), and plane illumination (in selective plane illumination microscopy).\n\n\"N\"-slit interferometers are also of interest to researchers working in atom optics, Fourier imaging, optical computing, and quantum computing.\n\n"}
{"id": "241563", "url": "https://en.wikipedia.org/wiki?curid=241563", "title": "Obsolescence", "text": "Obsolescence\n\nObsolescence is the state of being which occurs when an object, service, or practice is no longer wanted even though it may still be in good working order; however, the international standard EN62402 Obsolescence Management - Application Guide defines Obsolescence as being the \"transition from availability of products by the original manufacturer or supplier to unavailability\". Obsolescence frequently occurs because a replacement has become available that has, in sum, more advantages compared to the disadvantages incurred by maintaining or repairing the original. Obsolete also refers to something that is already disused or discarded, or antiquated. Typically, obsolescence is preceded by a gradual decline in popularity.\n\nDriven by rapid technological changes, new components are developed and launched on the market with increasing speed. The result is a dramatic change in production methods of all components and their market availability. A growing industry sector is facing issues where life cycles of products no longer fit together with life cycles of required components. This issue is known as obsolescence, the status given to a part when it is no longer available from its original manufacturer. The problem of obsolescence is most prevalent for electronics technology, wherein the procurement lifetimes for microelectronic parts are often significantly shorter than the manufacturing and support life cycles for the products that use the parts. However, obsolescence extends beyond electronic components to other items, such as materials, textiles, and mechanical parts. In addition, obsolescence has been shown to appear for software, specifications, standards, processes, and soft resources, such as human skills. It is highly important to implement and operate an active management of obsolescence to mitigate and avoid extreme costs.\n\nTechnical obsolescence usually occurs when a new product or technology supersedes the old one, and it is preferred to use the new technology instead. Historical examples of new technologies superseding old ones include bronze replacing flint in hand-tools, DVDs replacing videocassettes, and the telephone replacing the telegraph. On a smaller scale, a particular product may become obsolete when a newer version replaces it. Many products in the computer industry become obsolete in this manner. For example, central processing units (CPUs) frequently become obsolete in favor of newer, faster units. Singularly, rapid obsolescence of data formats along with their supporting hardware and software can lead to loss of critical information, a process known as digital obsolescence.\n\nIn many cases, a new technology does not totally replace the old technology because the old technology is still useful in certain applications. For example, transistors replaced vacuum tubes in TV and radio receivers in the 1960s, but vacuum tubes were still used for powerful transmitters because transistors for these power levels were not available. Even today, one has to use multiple transistors for a purpose that used to require just one tube.\n\nProducts may also become obsolete when supporting technologies are no longer available to produce or even repair a product. For example, many integrated circuits, including CPUs, memory and even some relatively simple logic chips may no longer be produced because the technology has been superseded, their original developer has gone out of business or a competitor has bought them out and effectively killed off their products to remove competition. It is rarely worth redeveloping a product to get around these issues since its overall functionality and price/performance ratio has usually been superseded by that time as well.\n\nSome products become technologically obsolete due to changes in complementary products which results in the function of the first product being made unnecessary. For example, buggy whips became obsolete when people started to travel in cars rather than in horse-drawn buggies.\n\nItems become functionally obsolete when they can no longer adequately perform the function for which they were created. For example, while one could theoretically adapt an Avro Lancaster to deploy modern JDAM bombs, the situations in which it could actually succeed at doing so against modern air defenses would be so few that it would be essentially useless.\n\nManufacturers and repair companies will typically cease support for products once they become obsolete as keeping production lines in place and parts in storage for a shrinking user base becomes unprofitable. This causes scarcity of spare parts and skilled technicians for repairs and thus escalates maintenance costs for obsolete products. This ultimately leads to prohibitive expense in keeping old technology functioning.\n\nThe term \"obsolescence\" was first applied to the built environment in 1910 in an attempt to explain American skyscrapers' sudden loss of value. New York engineer Reginald P. Bolton attributed this phenomenon to \"something new and better out-competing the old\" and calculated the average architectural lifespan of varying building types in order to formulate a rough estimate for their impending obsolescence. For example, he suggested that hotel's obsolescence will occur faster than banks due to their ever-changing functions and tastes.\n\nSometimes marketers deliberately introduce obsolescence into their product strategy, with the objective of generating long-term sales volume by reducing the time between repeat purchases. One example might be producing an appliance which is deliberately designed to wear out within five years of its purchase, pushing consumers to replace it within five years.\n\nWhen a product is no longer desirable because it has gone out of the popular fashion, its style is obsolete. One example is flared leg jeans; although this article of clothing may still be perfectly functional, it is no longer desirable because style trends have moved away from the flared leg cut.\n\nBecause of the \"fashion cycle\", stylistically obsolete products may eventually regain popularity and cease to be obsolete. A current example is \"acid-wash\" jeans, which were popular in the 1980s, became stylistically obsolete in the mid to late 1990s, and returned to popularity in the 2000s.\n\nObsolescence management, also referred to as \"Diminishing Manufacturing Sources and Material Shortages\" (DMSMS), is defined as to the activities that are undertaken to mitigate the effects of obsolescence. Activities can include last-time buys, lifetime buys, and obsolescence monitoring.\n\n"}
{"id": "500338", "url": "https://en.wikipedia.org/wiki?curid=500338", "title": "Outrigger", "text": "Outrigger\n\nAn outrigger is a projecting structure on a boat, with specific meaning depending on types of vessel. Outriggers may also refer to legs on a wheeled vehicle which are folded out when it needs stabilization, for example on a crane that lifts heavy loads.\n\nAn outrigger describes any contraposing float rigging beyond the side (gunwale) of a boat to improve the vessel's stability. If a single outrigger is used it is usually but not always windward. The technology was originally developed by the Austronesian people. There are two main types of boats with outriggers: double outriggers (prevalent in maritime Southeast Asia and Madagascar) and single outriggers (prevalent in Melanesia, Micronesia and Polynesia). Multihull ships are also derived from outrigger boats.\n\nIn an outrigger canoe and in sailboats such as the proa, an outrigger is a thin, long, solid, hull used to stabilise an inherently unstable main hull. The outrigger is positioned rigidly and parallel to the main hull so that the main hull is less likely to capsize. If only one outrigger is used on a vessel, its weight reduces the tendency to capsize in one direction and its buoyancy reduces the tendency in the other direction.\n\nOn a keelboat, \"outrigger\" refers to a variety of structures by which the running rigging (such as a sheet) may be attached outboard (outside the lateral limits) of the boat's hull. The Racing Rules of Sailing generally prohibit such outriggers, though they are explicitly permitted on specific classes, such as the IMOCA Open 60 used in several major offshore races.\n\nIn fishing from vessels, an outrigger is a pole or series of poles that allow boats to trawl more lines in the water without tangling and simulates a school of fish.\n\nIn a rowing boat or galley, an outrigger (or just rigger) is a triangular frame that holds the rowlock (into which the oar is slotted) away from the saxboard (gunwale for gig rowing) to optimize leverage. Wooden outriggers appear on the new trireme around the 7th or 6th centuries BC and later on Italian galleys around AD 1300 while Harry Clasper (1812–1870), a British professional rower, popularised the use of the modern metal version and the top rowing events accepted the physiological and ergonomic advantages so acceded to its use in competitions. Wing-riggers are made by some manufacturers of racing shells which are reinforced arcs or form a single projection akin to aircraft wings instead of conventional thin metal triangular structures.\n\n"}
{"id": "14523288", "url": "https://en.wikipedia.org/wiki?curid=14523288", "title": "Pyrheliometer", "text": "Pyrheliometer\n\nA pyrheliometer is an instrument for measurement of direct beam solar irradiance. Sunlight enters the instrument through a \nwindow and is directed onto a thermopile which converts heat to an electrical signal that can be recorded. The signal voltage is converted via a formula to measure watts per square metre. It is used with a solar tracking system to keep the instrument aimed at the sun. A pyrheliometer is often used in the same setup with a pyranometer.\n\nPyrheliometer measurement specifications are subject to International Organization for Standardization (ISO) and World Meteorological Organization (WMO) standards.\nComparisons between pyrheliometers for intercalibration are carried out regularly to measure the amount of solar energy received.\nThe aim of the International Pyrheliometer Comparisons, which take place every 5 years at the World Radiation Centre in Davos, is to ensure the world-wide transfer of the World Radiometric Reference.\nDuring this event, all participants bring their instruments, solar-tracking and data acquisition systems to Davos to conduct simultaneous solar radiation measurements with the World Standard Group.\n\nTypical pyrheliometer measurement applications include scientific meteorological and climate observations, material testing research, and assessment of the efficiency of solar collectors and photovoltaic devices.\n\nPyrheliometers are typically mounted on a solar tracker. As the pyrheliometer only 'sees' the solar disk, it needs to be placed on a device that follows the path of the sun.\n\n"}
{"id": "51555217", "url": "https://en.wikipedia.org/wiki?curid=51555217", "title": "R J Mitchell Wind Tunnel", "text": "R J Mitchell Wind Tunnel\n\nThe R. J. Mitchell Wind Tunnel is a low-speed wind tunnel which is part of the Faculty of Engineering and the Environment at the University of Southampton. It is the largest wind tunnel in University ownership in the UK. It is named after famed British aircraft designer R.J. Mitchell.\n\nThe tunnel was built in the 1930s as part of the Farnborough Royal Aircraft Establishment cluster of tunnels. The tunnel was purchased by the University of Southampton for a minimal fee in 1979 on the understanding that they would cover the cost of transport and recommissioning at its new site on the Highfield Campus. This purchase was in response to a business case put forward at the time which focused around the need for more capacity to cope with increasing motorsport demand. Previously the 7x5 wind tunnel had been heavily utilised by many F1 teams and a larger facility was deemed desirable to increase model scales. The tunnel was first run in Southampton in 1981 and was formally opened by R. J. Mitchell’s son, Gordon Mitchell.\n\nIn the 1980s and 90s the tunnel was used extensively by Formula 1 teams as well as Team Penske who developed many Indy Cars in the facility. In the early 00s the tunnel was able to be utilised by University students and researchers as well as a variety of customers from various industries.\n\nIn 2013 the existing building was extended to provide a new entrance area to update the look of the building while providing new facilities for users as well as wheelchair access. This extension was formally opened by Chris Boardman in January 2014.\n\nIn January 2014 the R. J. Mitchell Wind Tunnel became a ‘National Wind Tunnel Facility’ under a government initiative announced by minister for Science and Universities David Willetts. The initiative gave 17 facilities access to a total of £13.3 million with funding coming from both EPSRC and the UK Aerodynamic Centre. The funding allocated to the tunnel was used to design and build a traverse system, install a combined chiller/heat pump into the circuit and also funded both a high resolution and high speed Particle Image Velocimetry (PIV) systems.\n\nThe R. J. Mitchell Wind Tunnel is a closed test section, closed return type wind tunnel powered by a electric motor. The test section is 3.5m wide by 2.4m high (11 ft by 8 ft) and the tunnel is capable of creating wind speeds of up to 40 m/s (90 mph). The tunnel has a moving ground installed along with dual stage boundary layer suction for ground effect or vehicle testing.\n\nThe facility has a range of possible configurations including an overhead 6-component balance, mechanised strut with in-car 6-component balance for vehicle work as well as a 2-component underfloor dynamometer that is used primarily for performance sports work. This sports work was initiated before the 2008 Beijing Olympic Games when the tunnel was an integral part of the 'Secret Squirrel Club' used to help improve the performance of the Team GB cycle team. The success in cycling was also continued in the Winter Games when the Team GB Skeleton Bob team used the tunnel as part of a research project which resulted in Amy Williams winning Gold in Vancouver.\n\nThe facility has various data acquisition systems available including pressure scanning and PIV systems.\n\n"}
{"id": "28307156", "url": "https://en.wikipedia.org/wiki?curid=28307156", "title": "Roller reamer", "text": "Roller reamer\n\nRoller reamers are employed in boring operations for the oil & gas industry.\nThe main function of roller reamers cut earth formations to enlarge the borehole to the desirable size during well drilling operation, which may be the original size of the drill bit in the case where the drill bit wears to be under-gauged. However, even for new drill bits, roller reamers are employed to cut formations because the bit does not always drill a true bore hole and because of slight lateral shifting which is inherent in the drilling operation, which shifting leaves ledges and other distortions.\nAdditionally, the second function of a roller reamer is to keep the drill stem in the center of the hole at the specific position of placement of the roller-reamer. In providing such a function, a reamer is often referred to as a stabilizer. Maintaining the drill stem centered has many beneficial effects, its primary one being minimizing unintentional hole-angle directional drilling.\n\nPopular hole sizes for well bores range from 5\" to 26\" in diameter. Size of circulation passage through the center of the body of the reamer typically range from 1 1/2\" to 3 1/4\" in diameter. The body size on the necks beyond the area where the rollers are mounted typically range from 4 1/8\" to 11\" in diameter. This latter size will normally be the same size as the drill collar. Body links normally range from 4' to 8'.\n\nMost roller-reamers have three rollers equally spaced in a single transverse section. Such roller-reamers are referred to as \"3-point reamer\". When two sets of three rollers spaced apart longitudinally are used, the roller-reamer is called a \"6-point reamer\". However, large diameter roller-reamers may have more than three rollers in one transverse section.\n\n\n"}
{"id": "15703762", "url": "https://en.wikipedia.org/wiki?curid=15703762", "title": "SeeReal Technologies", "text": "SeeReal Technologies\n\nSeeReal Technologies GmbH is a Dresden-based company focusing on the development of 3D display solutions. It is owned by its Luxembourg parent company SeeReal Technologies S.A., which is responsible for marketing, partnering and IP licensing.\n\nThe firm was founded in 2002 by Dr. Armin Schwerdtner, who had previously headed an optics research group at the Dresden University of Technology (TU Dresden).\n\nSeeReal develops technology that is licensed to display manufacturers, including several variants of a tracked autostereoscopic display.\n\nThe firm has presented a real-time desktop holographic display on the SID 2007 Display Week in Long Beach (California) and the FPD 2007 in Yokohama (Japan). The firm owns more than 100 patents in the field of holographic and autostereoscopic 3D displays .\n\nSeeReal has won several awards, including the “European Information Society Technologies Prize (IST)” and the “Innovation Prize of the Free State of Saxony”.\n"}
{"id": "16182677", "url": "https://en.wikipedia.org/wiki?curid=16182677", "title": "Side-stick", "text": "Side-stick\n\nA side-stick or sidestick controller is an aircraft control column (or joystick) that is located on the side console of the pilot, usually on the righthand side, or outboard on a two-seat flightdeck. Typically this is found in aircraft that are equipped with fly-by-wire control systems.\n\nThe throttle controls are typically located to the left of the pilot (or centrally on a two-seat flightdeck). Only one hand may thus be used for the stick, and both-hands operation is neither possible nor required.\n\nThe side-stick is used in many modern military fighter aircraft, such as the F-16 Fighting Falcon, Mitsubishi F-2, Dassault Rafale, and F-22 Raptor, and also on civil aircraft, such as the Sukhoi Superjet 100, Airbus A320 and later Airbus aircraft, including the largest passenger jet in service, the Airbus A380.\n\nThis arrangement contrasts with the more conventional design where the stick is located in the centre of the cockpit between the pilot's legs, called a \"centre stick\".\n\nIn the centre stick design, both the pilot's and co-pilot's controls are mechanically connected together so each pilot has a sense of the control inputs of the other. In typical Airbus side-stick implementations, the sticks are independent. The plane's computer either aggregates multiple inputs or a pilot can press a \"priority button\" to lock out inputs from the other side-stick. However, if both side sticks are moved in different directions (regardless of which pilot has priority), then both inputs are cancelled out. Examples of this occurrence include the 2009 crash of Air France Flight 447 (an Airbus A330 flying from Rio de Janeiro to Paris), and the 2015 crash of Indonesia AirAsia Flight 8501 (an Airbus A320 flying from Surabaya to Singapore).\n\n\n"}
{"id": "5358268", "url": "https://en.wikipedia.org/wiki?curid=5358268", "title": "Supervisory circuit", "text": "Supervisory circuit\n\nSupervisory circuits are electronic circuits that monitor one or more parameters of systems such as power supplies and microprocessors which must be maintained within certain limits,and take appropriate action if a parameter goes out of bounds, creating an unacceptable or dangerous situation. \n\nSupervisory circuits are known by a variety of names, including battery monitors, power supply monitors, supply supervisory circuits, and reset circuits.\n\nA thermal protection circuit consists of a temperature-monitoring circuit and a control circuit. The control circuit may either shut down the circuitry it is protecting, reduce the power available in order to avoid overheating, or notify the system (software or user). These circuits may be quite complex, programmable and software-run, or simple with predefined limits.\n\nVoltage protection circuits protect circuitry from either overvoltage or undervoltage; either of these situations can have detrimental effects. Supervisory circuits that specifically focus on voltage regulation are often sold as supply voltage supervisors and will reset the protected circuit when the voltage returns to operating range.\n\nTwo types of overvoltage protection devices are currently used: clamping, which passes through voltages up to a certain level, and foldback, which shunts voltage away from the load. The shunting creates a short circuit which removes power from the protected circuitry. In certain applications this circuitry can reset itself after the dangerous condition has passed.\n\n"}
{"id": "233487", "url": "https://en.wikipedia.org/wiki?curid=233487", "title": "Tachometer", "text": "Tachometer\n\nA tachometer (revolution-counter, tach, rev-counter, RPM gauge) is an instrument measuring the rotation speed of a shaft or disk, as in a motor or other machine. The device usually displays the revolutions per minute (RPM) on a calibrated analogue dial, but digital displays are increasingly common. \nThe word comes from Greek \"ταχος\" (\"tachos\" \"speed\") and \"metron\" (\"measure\"). Essentially the words tachometer and speedometer have identical meaning: a device that measures speed. It is by arbitrary convention that in the automotive world one is used for engine and the other for vehicle speed. In formal engineering nomenclature, more precise terms are used to distinguish the two.\n\nThe first mechanical tachometers were based on measuring the centrifugal force, similar to the operation of a centrifugal governor. The inventor is assumed to be the German engineer Dietrich Uhlhorn; he used it for measuring the speed of machines in 1817. Since 1840, it has been used to measure the speed of locomotives.\n\nTachometers or revolution counters on cars, aircraft, and other vehicles show the rate of rotation of the engine's crankshaft, and typically have markings indicating a safe range of rotation speeds. This can assist the driver in selecting appropriate throttle and gear settings for the driving conditions. Prolonged use at high speeds may cause inadequate lubrication, overheating (exceeding capability of the cooling system), exceeding speed capability of sub-parts of the engine (for example spring retracted valves) thus causing excessive wear or permanent damage or failure of engines. This is more applicable to manual transmissions than to automatics. On analogue tachometers, speeds above maximum safe operating speed are typically indicated by an area of the gauge marked in red, giving rise to the expression of \"redlining\" an engine — revving the engine up to the maximum safe limit. The red zone is superfluous on most modern cars, since their engines typically have a revolution limiter which electronically limits engine speed to prevent damage. Diesel engines with traditional mechanical injector systems have an integral governor which prevents over-speeding the engine, so the tachometers in vehicles and machinery fitted with such engines sometimes lack a redline.\n\nIn vehicles such as tractors and trucks, the tachometer often has other markings, usually a green arc showing the speed range in which the engine produces maximum torque, which is of prime interest to operators of such vehicles. Tractors fitted with a power take-off (PTO) system have tachometers showing the engine speed needed to rotate the PTO at the standardized speed required by most PTO-driven implements. In many countries, tractors are required to have a speedometer for use on a road. To save fitting a second dial, the vehicle's tachometer is often marked with a second scale in units of speed. This scale is only accurate in a certain gear, but since many tractors only have one gear that is practical for use on-road, this is sufficient. Tractors with multiple 'road gears' often have tachometers with more than one speed scale. Aircraft tachometers have a green arc showing the engine's designed cruising speed range.\n\nIn older vehicles, the tachometer is driven by the RMS voltage waves from the low tension (LT) side of the ignition coil, while on others (and nearly all diesel engines, which have no ignition system) engine speed is determined by the frequency from the alternator tachometer output. This is from a special connection called an \"AC tap\" which is a connection to one of the stator's coil output, before the rectifier. Tachometers driven by a rotating cable from a drive unit fitted to the engine (usually on the camshaft) exist - usually on simple diesel-engined machinery with basic or no electrical systems. On recent EMS found on modern vehicles, the signal for the tachometer is usually generated from an ECU which derives the information from either the crankshaft or camshaft speed sensor.\n\nTachometers are used to estimate traffic speed and volume (flow). A vehicle is equipped with the sensor and conducts \"tach runs\" which record the traffic data. These data are a substitute or complement to loop detector data. To get statistically significant results requires a high number of runs, and bias is introduced by the time of day, day of week, and the season. However, because of the expense, spacing (a lower density of loop detectors diminishes data accuracy), and relatively low reliability of loop detectors (often 30% or more are out of service at any given time), tach runs remain a common practice.\n\nSpeed sensing devices, termed variously \"wheel impulse generators\" (WIG), speed probes, or tachometers are used extensively in rail vehicles. Common types include opto-isolator slotted disk sensors and Hall effect sensors.\n\nHall effect sensors typically use a rotating target attached to a wheel, gearbox or motor. This target may contain magnets, or it may be a toothed wheel. The teeth on the wheel vary the flux density of a magnet inside the sensor head. The probe is mounted with its head a precise distance from the target wheel and detects the teeth or magnets passing its face. One problem with this system is that the necessary air gap between the target wheel and the sensor allows ferrous dust from the vehicle's underframe to build up on the probe or target, inhibiting its function.\n\nOpto-isolator sensors are completely encased to prevent ingress from the outside environment. The only exposed parts are a sealed plug connector and a drive fork, which is attached to a slotted disk internally through a bearing and seal. The slotted disk is typically sandwiched between two circuit boards containing a photo-diode, photo-transistor, amplifier, and filtering circuits which produce a square wave pulse train output customized to the customers voltage and pulses per revolution requirements. These types of sensors typically provide 2 to 8 independent channels of output that can be sampled by other systems in the vehicle such as automatic train control systems and propulsion/braking controllers.\n\nThe sensors mounted around the circumference of the disk provide quadrature encoded outputs and thus allow the vehicle's computer to determine the direction of rotation of the wheel. This is a legal requirement in Switzerland to prevent \"rollback\" when starting from standstill. Strictly, such devices are not tachometers since they do not provide a direct reading of the rotational speed of the disk. The speed has to be derived externally by counting the number of pulses in a time period. It is difficult to prove conclusively that the vehicle is stationary, other than by waiting a certain time to ensure that no further pulses occur. This is one reason why there is often a time delay between the train stopping, as perceived by a passenger, and the doors being released. Slotted-disk devices are typical sensors used in odometer systems for rail vehicles, such as are required for train protection systems — notably the European Train Control System.\n\nAs well as speed sensing, these probes are often used to calculate distance travelled by multiplying wheel rotations by wheel circumference.\n\nThey can be used to automatically calibrate wheel diameter by comparing the number of rotations of each axle against a master wheel that has been measured manually. Since all wheels travel the same distance, the diameter of each wheel is proportional to its number of rotations compared to the master wheel. This calibration must be done while coasting at a fixed speed to eliminate the possibility of wheel slip/slide introducing errors into the calculation. Automatic calibration of this type is used to generate more accurate traction and braking signals, and to improve wheel slip detection.\n\nA weakness of systems that rely on wheel rotation for tachometry and odometry is that the train wheels and the rails are very smooth and the friction between them is low, leading to high error rates if the wheels slip or slide. To compensate for this, secondary odometry inputs employ Doppler radar units beneath the train to measure speed independently.\n\nIn analogue audio recording, a tachometer is a device that measures the speed of audiotape as it passes across the head. On most audio tape recorders the tachometer (or simply \"tach\") is a relatively large spindle near the ERP head stack, isolated from the feed and take-up spindles by tension idlers.\n\nOn many recorders the tachometer spindle is connected by an axle to a rotating magnet that induces a changing magnetic field upon a Hall effect transistor. Other systems connect the spindle to a stroboscope, which alternates light and dark upon a photodiode.\n\nThe tape recorder's drive electronics use signals from the tachometer to ensure that the tape is played at the proper speed. The signal is compared to a reference signal (either a quartz crystal or alternating current from the mains). The comparison of the two frequencies drives the speed of the tape transport. When the tach signal and the reference signal match, the tape transport is said to be \"at speed.\" (To this day on film sets, the director calls \"Roll sound!\" and the sound man replies \"Sound speed!\" This is a vestige of the days when recording devices required several seconds to reach a regulated speed.)\n\nHaving perfectly regulated tape speed is important because the human ear is very sensitive to changes in pitch, particularly sudden ones, and without a self-regulating system to control the speed of tape across the head, the pitch could drift several percent. This effect is called a wow-and-flutter, and a modern, tachometer-regulated cassette deck has a wow-and-flutter of 0.07%.\n\nTachometers are acceptable for high-fidelity sound playback, but not for recording in synchronization with a movie camera. For such purposes, special recorders that record pilottone must be used.\n\nTachometer signals can be used to synchronize several tape machines together, but only if in addition to the tach signal, a directional signal is transmitted, to tell slave machines in which direction the master is moving.\n\n"}
{"id": "39976104", "url": "https://en.wikipedia.org/wiki?curid=39976104", "title": "Vinyl cutter", "text": "Vinyl cutter\n\nA vinyl cutter is a type of computer-controlled machine. Small vinyl cutters look like a desktop printer. Like a printer controls a nozzle, the computer controls the movement of a sharp blade over the surface of the material. This blade is used to cut out shapes and letters from sheets of thin self-adhesive plastic (vinyl). The vinyl can then be stuck to a variety of surfaces depending on the adhesive and type of material.\n\nTo cut out a design a vector-based image must be created in a software program (usually Adobe Illustrator or Corel Draw). It is then sent to the cutter where it cuts along the vector paths laid out in the design. The cutter is capable of moving the blade on an X and Y axis over the material, cutting it into any shape imaginable. Since the vinyl material comes in long rolls, projects with significant length like banners or billboards can be easily cut as well.\n\nThe one major limitation with vinyl cutters is that they can only cut shapes from solid colours of vinyl. A design with multiple colours must have each colour cut separately and then layered on top of each other as it is applied to the substrate. Also, since the shapes are cut out of solid colours, photographs and gradients cannot be reproduced with a stand alone cutter.\nIn addition to the capabilities of the cutter itself, the adhesive vinyl comes in a wide variety of colours and materials including gold and silver foil, vinyl that simulates frosted glass, holographic vinyl, reflective vinyl, thermal transfer material, and even clear vinyl imbedded with gold leaf. (Often used in the lettering on fire trucks and rescue vehicles.)\n\nAs vinyl film is supplied by the manufacturer, it comes attached to a release liner.\n\nComputer designed images are loaded onto the vinyl cutter via cords or over wifi depending on the model. Then the vinyl is loaded into the machine where it is automatically fed through and cut to follow the set design. \n\nThe vinyl cutter uses a small knife to precisely cut the outline of figures into a sheet or piece of vinyl, but not the release liner. The knife moves side to side and turns, while the vinyl is moved beneath the knife. The results from the cut process is an image cut into the material. \n\nThe material is then 'weeded' where the excess parts of the figures are removed from the release liner. It is possible to remove the positive parts, which would give a negative sticker, or remove the negative parts, giving a positive sticker. Removing the figure would be like removing the positive, giving a negative image of the figures.\n\nA sheet of transfer tape with an adhesive backing is laid on the weeded vinyl. A roller is applied to the tape, causing it to adhere to the vinyl. The transfer tape and the weeded vinyl is pulled off the release liner, and applied to a substrate, such as a sheet of aluminium. This results in an aluminium sign with vinyl figures.\n\nThe vinyl cutter is an entry level machine for making signs. Computer designed vector files with patterns and letters are directly cut on the roll of vinyl which is mounted and fed into the vinyl cutter through USB or serial cable. Vinyl cutters are mainly used to make signs, banners and advertisements. Advertisements seen on automobiles and vans are often made with vinyl cut letters. While these machines were designed for cutting vinyl, they can also cut through computer and specialty papers, as well as thicker items like thin sheets of .\n\nIn addition to sign business, vinyl cutters are commonly used for apparel decoration. To decorate apparel, a vector design needs to be cut in mirror image, weeded, and then heat applied using a commercial heat press or a hand iron for home use.\n\nSome businesses use their vinyl cutter to produce both signs and custom apparel. Many crafters also have vinyl cutters for home use. These require little maintenance and the vinyl can be bought in bulk relatively cheaply.\n"}
{"id": "478091", "url": "https://en.wikipedia.org/wiki?curid=478091", "title": "Von Neumann architecture", "text": "Von Neumann architecture\n\nThe von Neumann architecture—also known as the von Neumann model or Princeton architecture—is a computer architecture based on a 1945 description by the mathematician and physicist John von Neumann and others in the \"First Draft of a Report on the EDVAC\". That document describes a design architecture for an electronic digital computer with\n\nThe word has evolved to mean any stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus. This is referred to as the von Neumann bottleneck and often limits the performance of the system.\n\nThe design of a von Neumann architecture machine is simpler than a Harvard architecture machine—which is also a stored-program system but has one dedicated set of address and data buses for reading and writing to memory, and another set of address and data buses to fetch instructions.\n\nA stored-program digital computer keeps both program instructions and data in read-write, random-access memory (RAM). Stored-program computers were an advancement over the program-controlled computers of the 1940s, such as the Colossus and the ENIAC. Those were programmed by setting switches and inserting patch cables to route data and control signals between various functional units. The vast majority of modern computers use the same memory for both data and program instructions. The von Neumann vs. Harvard distinction applies to the cache architecture, not the main memory (split cache architecture).\n\nThe earliest computing machines had fixed programs. Some very simple computers still use this design, either for simplicity or training purposes. For example, a desk calculator (in principle) is a fixed program computer. It can do basic mathematics, but it cannot run a word processor or games. Changing the program of a fixed-program machine requires rewiring, restructuring, or redesigning the machine. The earliest computers were not so much \"programmed\" as \"designed\" for a particular task. \"Reprogramming\"—when possible at all—was a laborious process that started with flowcharts and paper notes, followed by detailed engineering designs, and then the often-arduous process of physically rewiring and rebuilding the machine. It could take three weeks to set up and debug a program on ENIAC.\n\nWith the proposal of the stored-program computer, this changed. A stored-program computer includes, by design, an instruction set, and can store in memory a set of instructions (a program) that details the computation.\n\nA stored-program design also allows for self-modifying code. One early motivation for such a facility was the need for a program to increment or otherwise modify the address portion of instructions, which operators had to do manually in early designs. This became less important when index registers and indirect addressing became usual features of machine architecture. Another use was to embed frequently used data in the instruction stream using immediate addressing. Self-modifying code has largely fallen out of favor, since it is usually hard to understand and debug, as well as being inefficient under modern processor pipelining and caching schemes.\n\nOn a large scale, the ability to treat instructions as data is what makes assemblers, compilers, linkers, loaders, and other automated programming tools possible. It makes \"programs that write programs\" possible. This has made a sophisticated self-hosting computing ecosystem flourish around von Neumann architecture machines.\n\nSome high level languages such as LISP leverage the von Neumann architecture by providing an abstract, machine-independent way to manipulate executable code at runtime, or by using runtime information to tune just-in-time compilation (e.g. in the case of languages hosted on the Java virtual machine, or languages embedded in web browsers).\n\nOn a smaller scale, some repetitive operations such as BITBLT or pixel and vertex shaders can be accelerated on general purpose processors with just-in-time compilation techniques. This is one use of self-modifying code that has remained popular.\n\nThe mathematician Alan Turing, who had been alerted to a problem of mathematical logic by the lectures of Max Newman at the University of Cambridge, wrote a paper in 1936 entitled \"On Computable Numbers, with an Application to the Entscheidungsproblem\", which was published in the \"Proceedings of the London Mathematical Society\". In it he described a hypothetical machine he called a \"universal computing machine,\" now known as the \"Universal Turing machine\". The hypothetical machine had an infinite store (memory in today's terminology) that contained both instructions and data. John von Neumann became acquainted with Turing while he was a visiting professor at Cambridge in 1935, and also during Turing's PhD year at the Institute for Advanced Study in Princeton, New Jersey during 1936 – 1937. Whether he knew of Turing's paper of 1936 at that time is not clear.\n\nIn 1936, Konrad Zuse also anticipated in two patent applications that machine instructions could be stored in the same storage used for data.\n\nIndependently, J. Presper Eckert and John Mauchly, who were developing the ENIAC at the Moore School of Electrical Engineering, at the University of Pennsylvania, wrote about the stored-program concept in December 1943.\n\nVon Neumann was involved in the Manhattan Project at the Los Alamos National Laboratory, which required huge amounts of calculation. This drew him to the ENIAC project, during the summer of 1944. There he joined the ongoing discussions on the design of this stored-program computer, the EDVAC. As part of that group, he wrote up a description titled \"First Draft of a Report on the EDVAC\" based on the work of Eckert and Mauchly. It was unfinished when his colleague Herman Goldstine circulated it with only von Neumann's name on it, to the consternation of Eckert and Mauchly. The paper was read by dozens of von Neumann's colleagues in America and Europe, and influenced the next round of computer designs.\n\nJack Copeland considers that it is \"historically inappropriate, to refer to electronic stored-program digital computers as 'von Neumann machines'\". His Los Alamos colleague Stan Frankel said of von Neumann's regard for Turing's ideas:\nAt the time that the \"First Draft\" report was circulated, Turing was producing a report entitled \"Proposed Electronic Calculator\". It described in engineering and programming detail, his idea of a machine he called the \"Automatic Computing Engine (ACE)\". He presented this to the Executive Committee of the British National Physical Laboratory on February 19, 1946. Although Turing knew from his wartime experience at Bletchley Park that what he proposed was feasible, the secrecy surrounding Colossus, that was subsequently maintained for several decades, prevented him from saying so. Various successful implementations of the ACE design were produced.\n\nBoth von Neumann's and Turing's papers described stored-program computers, but von Neumann's earlier paper achieved greater circulation and the computer architecture it outlined became known as the \"von Neumann architecture\". In the 1953 publication \"Faster than Thought: A Symposium on Digital Computing Machines\" (edited by B. V. Bowden), a section in the chapter on \"Computers in America\" reads as follows:\n\nThe Machine of the Institute For Advanced Studies, Princeton\nIn 1945, Professor J. von Neumann, who was then working at the Moore School of Engineering in Philadelphia, where the E.N.I.A.C. had been built, issued on behalf of a group of his co-workers, a report on the logical design of digital computers. The report contained a detailed proposal for the design of the machine that has since become known as the E.D.V.A.C. (electronic discrete variable automatic computer). This machine has only recently been completed in America, but the von Neumann report inspired the construction of the E.D.S.A.C. (electronic delay-storage automatic calculator) in Cambridge (see page 130).\nIn 1947, Burks, Goldstine and von Neumann published another report that outlined the design of another type of machine (a parallel machine this time) that would be exceedingly fast, capable perhaps of 20,000 operations per second. They pointed out that the outstanding problem in constructing such a machine was the development of suitable memory with instantaneously accessible contents. At first they suggested using a special vacuum tube—called the \"Selectron\"—which the Princeton Laboratories of RCA had invented. These tubes were expensive and difficult to make, so von Neumann subsequently decided to build a machine based on the Williams memory. This machine—completed in June, 1952 in Princeton—has become popularly known as the Maniac. The design of this machine inspired at least half a dozen machines now being built in America, all known affectionately as \"Johniacs.\"\nIn the same book, the first two paragraphs of a chapter on ACE read as follows:\n\nAutomatic Computation at the National Physical Laboratory\nOne of the most modern digital computers which embodies developments and improvements in the technique of automatic electronic computing was recently demonstrated at the National Physical Laboratory, Teddington, where it has been designed and built by a small team of mathematicians and electronics research engineers on the staff of the Laboratory, assisted by a number of production engineers from the English Electric Company, Limited. The equipment so far erected at the Laboratory is only the pilot model of a much larger installation which will be known as the Automatic Computing Engine, but although comparatively small in bulk and containing only about 800 thermionic valves, as can be judged from Plates XII, XIII and XIV, it is an extremely rapid and versatile calculating machine.\nThe basic concepts and abstract principles of computation by a machine were formulated by Dr. A. M. Turing, F.R.S., in a paper. read before the London Mathematical Society in 1936, but work on such machines in Britain was delayed by the war. In 1945, however, an examination of the problems was made at the National Physical Laboratory by Mr. J. R. Womersley, then superintendent of the Mathematics Division of the Laboratory. He was joined by Dr. Turing and a small staff of specialists, and, by 1947, the preliminary planning was sufficiently advanced to warrant the establishment of the special group already mentioned. In April, 1948, the latter became the Electronics Section of the Laboratory, under the charge of Mr. F. M. Colebrook.\nThe \"First Draft\" described a design that was used by many universities and corporations to construct their computers. Among these various computers, only ILLIAC and ORDVAC had compatible instruction sets.\n\nThe date information in the following chronology is difficult to put into proper order. Some dates are for first running a test program, some dates are the first time the computer was demonstrated or completed, and some dates are for the first delivery or installation.\n\n\nThrough the decades of the 1960s and 1970s computers generally became both smaller and faster, which led to evolutions in their architecture. For example, memory-mapped I/O lets input and output devices be treated the same as memory. A single system bus could be used to provide a modular system with lower cost. This is sometimes called a \"streamlining\" of the architecture.\nIn subsequent decades, simple microcontrollers would sometimes omit features of the model to lower cost and size.\nLarger computers added features for higher performance.\n\nThe shared bus between the program memory and data memory leads to the \"von Neumann bottleneck\", the limited throughput (data transfer rate) between the central processing unit (CPU) and memory compared to the amount of memory. Because the single bus can only access one of the two classes of memory at a time, throughput is lower than the rate at which the CPU can work. This seriously limits the effective processing speed when the CPU is required to perform minimal processing on large amounts of data. The CPU is continually forced to wait for needed data to move to or from memory. Since CPU speed and memory size have increased much faster than the throughput between them, the bottleneck has become more of a problem, a problem whose severity increases with every new generation of CPU.\n\nThe von Neumann bottleneck was described by John Backus in his 1977 ACM Turing Award lecture. According to Backus:\n\nSurely there must be a less primitive way of making big changes in the store than by pushing vast numbers of words back and forth through the von Neumann bottleneck. Not only is this tube a literal bottleneck for the data traffic of a problem, but, more importantly, it is an intellectual bottleneck that has kept us tied to word-at-a-time thinking instead of encouraging us to think in terms of the larger conceptual units of the task at hand. Thus programming is basically planning and detailing the enormous traffic of words through the von Neumann bottleneck, and much of that traffic concerns not significant data itself, but where to find it.\n\nThere are several known methods for mitigating the Von Neumann performance bottleneck. For example, the following all can improve performance:\n\nThe problem can also be sidestepped somewhat by using parallel computing, using for example the non-uniform memory access (NUMA) architecture—this approach is commonly employed by supercomputers. It is less clear whether the \"intellectual bottleneck\" that Backus criticized has changed much since 1977. Backus's proposed solution has not had a major influence. Modern functional programming and object-oriented programming are much less geared towards \"pushing vast numbers of words back and forth\" than earlier languages like FORTRAN were, but internally, that is still what computers spend much of their time doing, even highly parallel supercomputers.\n\nAs of 1996, a database benchmark study found that three out of four CPU cycles were spent waiting for memory. Researchers expect that increasing the number of simultaneous instruction streams with multithreading or single-chip multiprocessing will make this bottleneck even worse. In the context of multi-core processors, additional overhead is required to maintain cache coherence between processors and threads. \n\nAside from the von Neumann bottleneck, program modifications can be quite harmful, either by accident or design. In some simple stored-program computer designs, a malfunctioning program can damage itself, other programs, or the operating system, possibly leading to a computer crash. Memory protection and other forms of access control can usually protect against both accidental and malicious program modification.\n\n\n\n"}
{"id": "40302986", "url": "https://en.wikipedia.org/wiki?curid=40302986", "title": "Warp-field experiments", "text": "Warp-field experiments\n\nWarp-field experiments are a series of current and proposed experiments to create and detect instances of spacetime warping. The ultimate goal is to prove or disprove the possibility of spacetime metric engineering with reasonable amounts of energy.\n\nSpacetime metric engineering is a requirement for physically recreating solutions of general relativity such as Einstein–Rosen bridges or the Alcubierre drive. Current experiments focus on the Alcubierre metric and its modifications. Alcubierre's work from 1994 implies that even if the required exotic matter with negative energy densities can be created, the total mass–energy demand for his proposed warp drive would exceed anything that could be realistically attained by human technology. Other researchers aimed to improve the energy efficiency (see Alcubierre drive: Difficulties), however the propositions remain mostly speculative. Research groups at NASA's Johnsons Space Center and Dakota State University currently aim to experimentally evaluate several new approaches, especially a redesigned energy-density topology as well as an implication of brane cosmology theory. If space actually were to be embedded in higher dimensions, the energy requirements could be decreased dramatically and a comparatively small energy density could already lead to a spacetime curvature measurable using an interferometer. The theoretical framework for the experiments dates back to work by Harold G. White from 2003 as well as work by White and Eric W. Davis from 2006 that was published in the AIP, where they also consider how baryonic matter could, at least mathematically, adopt characteristics of dark energy (see section below). In the process, they described how a toroidal positive energy density may result in a spherical negative-pressure region, possibly eliminating the need for actual exotic matter.\n\nThe metric derived by Alcubierre was mathematically motivated by cosmological inflation. The original \"warp-drive\" spacetime metric can be written in (t,x,y,z) coordinates as:\n\nIt uses the curve (world line) formula_2 where formula_3 expresses the x-coordinate position of the moving spaceship frame.\n\nThe radius formula_4 is the euclidean distance from the curve. Furthermore, formula_5 is the speed of light and formula_6 is equivalent to formula_7, the velocity associated with the curve.\n\nThe shaping function formula_8 is any smooth function that satisfies formula_9 and decreases from the origin, vanishing at formula_10 for some point formula_11.\n\nThe driving phenomenon behind the apparently arbitrary velocity (including formula_12) could be (and was) postulated to be the York extrinsic time, formula_13, defined as:\nIt provides for the contraction of space in front, and expansion in the back of the warp bubble. The idea can be seen in some way as an applied extension of the hypothesis that the early universe also saw a rapid inflationary expansion that possibly exceeded the speed of light for some time. However, according to research, it appears that the York time behaviour is merely a side effect of another underlying mechanism. The problem that led to the assumption that York time is only one part in the larger picture is the unusual symmetry in the required energy density. Using the Einstein field equations, the stress energy tensor formula_15 can be derived from the Alcubierre metric, resulting in the necessary energy density:\n\nwhere formula_17 is the gravitational constant and formula_18.\n\nThis corresponds to a negative toroidal energy density symmetric to the x-axis. It is notable here, that during a sensitivity analysis for the DARPA-funded 100 Year Starship symposium, Harold White discovered that changing the energy-density distribution from a thin ring as originally proposed to more of a doughnut shape (effectively increasing the warp-bubble wall thickness) can decrease the required total negative energy by several orders of magnitude.\n\nThe symmetry in the energy distribution leads to the scenario in which the choice of positive x-axis is in fact arbitrary. The warp-drive mechanism would not know whether to go forward or backward along the x-axis. This paradox can be resolved by putting the Alcubierre metric into canonical form using Rindler's method and extracting the potential, formula_19. With the potential it is possible to derive the field equation for the spacetime expansion \"boost\", formula_20:\n\nThe boost can roughly be seen as a scalar multiplier acting on an initial velocity leading to the actual warp-velocity:\nThis boost is also an important analogy when considering higher-dimensional models (as seen below).\nThe expansion and contraction of space as measured by York time are now more of a secondary effect and can be considered equivalent to a pressure gradient of a moving sphere in a fluid. The scalar nature of the effect is an important clue when considering higher dimensions.\nConsidering the null geodesics for formula_23 inside the warp field, it can be seen that the world lines are space-like for external observers, but the moving frame never travels outside its local comoving light cone and thus does not violate special relativity.\n\nFrom there on, White and Davis showed similarities of the spacetime boost when considered in a higher-dimensional spacetime, such as in the Chung-Freese model. In this particular model our space exists on a brane and the space surrounding the brane is called the \"bulk\". The size of each extra dimension is considered to be at least finite and the latest research at CERN also constrains any large extra dimension theories. However, the actual size and total number of extra dimensions is not important when considering the implications on the boost field. The modified Robertson–Walker metric representing the model by Chung and Freese is:\nwhere the formula_25 term specifies our normal space and formula_26 the higher-dimensional bulk with our space located at the formula_27 plane.\n\nformula_28 is a typical cosmological expansion parameter (see accelerated expansion) and formula_29 is an arbitrary compactification factor for the extra dimensions. Considering the null geodesic solutions ( ds²=0 ) allows to develop the following relationship:\nFor zero formula_31 the expected speed of the photon is c, as expected. For large off-bulk coordinates formula_32 the speed formula_33 can be made arbitrarily large.\n\nThat means that light rays could have a spacelike appearance, an obvious parallel to the Alcubierre model. However, in the Alcubierre model the spacetime expansion boost is the driving phenomenon, whereas in the Chung–Freese model the off-brane bulk location U serves this purpose. It is thus theoretically possible that the boost of a 3+1 spacetime model is a scalar correction factor for higher-dimensional geometric effects on our brane, leading the following analogies to Alcubierre's model:\n\nIf a particle like an electron gains a high spacetime boost relative to an observer, it might actually leave the 3+1 brane (i.e. it gains non-zero U bulk coordinates) and its ability to interact electromagnetically diminishes. To illustrate this, White and Davis stated that in a 2D lab located at the x,y plane, a 2D electron that gets accelerated (obtains a high boost) gains a non-zero z coordinate. Thus, if a photon were to interact with it, it would need to be at the same (t,x,y,z) coordinate.\n\nConsidering the null geodesic equation again, one can see that if dU/dt=c, dX/dt=0 meaning that light comes to a standstill. This implies that a high hyperspace velocity reduces spacetime's \"stiffness\" or ability to resist being curved by energy, effectively reducing the energy requirements to warp it. This observation, together with the modified energy density distribution, first led researchers at NASA to begin thinking of testbeds to verify the new theoretical approach. Using the analogy between U and formula_19, it is obvious that a high velocity (dU/dt) with U=0 requires a field oscillation.\n\nBack in the Alcubierre model, it is notable that an outside observer would perceive a uniform potential (from the uniform boost within the sphere) that represents the warp-field region even though it originates from a toroidal energy density. It has similar characteristics as a Gaussian spherical surface held at constant electrostatic potential. To the outside observer, the warp-field sphere has a uniform energy density. By expanding the spherical region while maintaining the same relative boost value for the Gaussian surface, when considering the first law of thermodynamics, the following can be concluded (limited to the 3+1 brane):\nformula_39 can be replaced by formula_40, which is the total energy for the warp sphere with the same volume change formula_41 as on the right side of the equation.\n\nThus, the equation of state relating the pressure of the warp sphere formula_42 to its energy density formula_43 is\nwhich noticeably resembles the equation of state of cosmological vacuum energy (moreover, it is the equation of state of dark energy). If formula_43 of the original warp sphere is negative, formula_42 would be positive. However the last equation shows that the opposite is true, formula_43 is positive and formula_42 is negative. Considering that the spacetime expansion boost for the Alcubierre model can be made arbitrarily high depending on the choice of input variables, a high boost is thereby clearly not an exclusive feature common only to negative energy densities and could be obtained in the lab, provided powerful enough equipment.\n\nEinstein's field equations show that comparatively high amounts of energy are required for any significant curvature of spacetime under ordinary conditions. With the energy-requirement-decreasing concepts only being partially implemented yet, the available measurement methods are reaching the limits of what is technically possible. That is why current results remain mostly inconclusive until measurements can be further refined or the effect can be increased. New experimental setups have been proposed to boost sensitivity, and using the higher-dimensional yet still purely theoretical approach may increase any effect sufficiently to obtain significant results to prove or disprove the theory.\n\nThe only reported warp-field experiment currently being conducted is on a modified Michelson–Morley interferometer as proposed by Harold White and Eric Davis in 2003. This setup includes a ring-shaped energy device using high-voltage barium-titanate ceramic capacitors to attempt to warp space as shown in the diagram. White announced at a 2013 space conference that the first experimental results from this device were inconclusive.\n\nA time-of-flight experiment was proposed in 2013 using a modified Fabry–Pérot interferometer.\n"}
{"id": "30627910", "url": "https://en.wikipedia.org/wiki?curid=30627910", "title": "Washing mitt", "text": "Washing mitt\n\nA washing mitt is a piece of terry cloth, shaped like a pouch that the hand fits in. It is used as an aid in washing the body, for example, to apply soap to the body, and to remove the soap with a rinsed out washing mitt. It can also be used to freshen up the face. Usually, a towel is used to dry off the body afterwards.\n\nA washing mitt is especially useful for people that like to wash themselves at a sink, and do not wish to spill or waste much water.\nThe washing mitt is also a good tool for nurses and caregivers, who wash their patients in their beds.\nAn expansion on the washing mitt is the showercloth, with which also the back can be washed.\nThere are also special sponges which can be used in the shower, or in the bathtub, in combination with a shower gel. \n\nIn the case of bruises or injuries, a washing mitt can be filled with ice, so that it can be used as an ice bag.\n\nWashing mitts are mostly used in Belgium, France, Germany, Iran, Korea and The Netherlands. Wash cloths are commonly used in Korea both at home and within their public bathhouses. These are famous for being frequently used in public baths in Iran and being the nickname of second football team in there, Esteghlal, as both are common in blue color. In the rest of the world, wash cloths are in wider use.\n\nAn alternative name for the washing mitt is \"washglove\".\n\n"}
{"id": "41810270", "url": "https://en.wikipedia.org/wiki?curid=41810270", "title": "We are fed up", "text": "We are fed up\n\nWe are fed up! (Wir haben es satt!) is the theme of a series of demonstrations in Germany against industrial livestock production and for more sustainable farming. The biggest demonstrations take place every year in Berlin since 2011 and attract up to 30,000 people. Around 120 different groups, which represent farmers, companies, and environmental rights and animal rights activists organize and sustain the demonstrations. The protests take place parallel to the Berlin International Green Week.\n\nIndependent from the protests in Berlin, demonstrations under a similar theme occurred in Amsterdam in 2012 and 2013. The event in the Netherlands was called \"we zijn het MEGA zat\", which translates to \"we are MEGA fed up\" and is a reference to the scale of industrial livestock farming.\n\n\n"}
{"id": "869868", "url": "https://en.wikipedia.org/wiki?curid=869868", "title": "White spirit", "text": "White spirit\n\nWhite spirit (UK) or mineral spirits (US, Canada), also known as mineral turpentine (AU/NZ), turpentine substitute, petroleum spirits, solvent naphtha (petroleum), Varsol, Stoddard solvent, or, generically, \"paint thinner\", is a petroleum-derived clear liquid used as a common organic solvent in painting.\n\nA mixture of aliphatic, open-chain or alicyclic C7 to C12 hydrocarbons, white spirit is insoluble in water and is used as an extraction solvent, as a cleaning solvent, as a degreasing solvent and as a solvent in aerosols, paints, wood preservatives, lacquers, varnishes, and asphalt products. In western Europe about 60% of the total white spirit consumption is used in paints, lacquers and varnishes. White spirit is the most widely used solvent in the paint industry. In households, white spirit is commonly used to clean paint brushes after use, to clean auto parts and tools, as a starter fluid for charcoal grills, to remove adhesive residue from non-porous surfaces, and many other common tasks.\n\nThe word \"mineral\" in \"mineral spirits\" or \"mineral turpentine\" is meant to distinguish it from distilled spirits (distilled directly from fermented grains and fruit) or from true turpentine (distilled tree resin).\n\nThree different types and three different grades of white spirit exist. The \"type\" refers to whether the solvent has been subjected to hydrodesulfurization (removal of sulfur) alone (type 1), solvent extraction (type 2) or hydrogenation (type 3).\n\nEach type comprises three grades: low flash grade, regular grade, and high flash grade (\"flash\" refers to flash point). The grade is determined by the crude oil used as the starting material and the conditions of distillation.\n\nIn addition there is type 0, which is defined as distillation fraction with no further treatment, consisting predominantly of saturated C9 to C12 hydrocarbons with a boiling range of 140–200 °C.\n\nStoddard solvent is a specific mixture of hydrocarbons, typically > 65% C10 or higher hydrocarbons, developed in 1924 by Atlanta dry cleaner W. J. Stoddard and Lloyd E. Jackson of the Mellon Institute of Industrial Research as a less flammable petroleum-based dry cleaning solvent than the petroleum solvents then in use. Dry cleaners began using the result of their work in 1928 and it soon became the predominant dry cleaning solvent in the United States, until the late 1950s.\n\nTurpentine substitute is generally not made to a standard and can have a wider range of components than products marketed as white spirit, which is made to a standard (in the UK, British Standard BS 245, in Germany, DIN 51632). Turpentine substitute can be used for general cleaning but is not recommended for paint thinning as it may adversely affect drying times due to the less volatile components; while it may be used for brush cleaning its heavier components may leave an oily residue.\n\nIn Australia, white spirit is normally sold under the generic name of Shellite (etymology unknown), and is composed of C6 to C10 straight alkanes, classing it as light pure naptha. It is used for fuel and cleaning.\n\nType 1 white spirit is mainly used in most of the Europe and Stoddard solvent is used in the US, both of which correspond to each other.\nWhite Spirit is a petroleum distillate used as a paint thinner and mild solvent. In industry, mineral spirits are used for cleaning and degreasing machine tools and parts, and in conjunction with cutting oil as a thread cutting and reaming lubricant.\n\nMineral spirits are an inexpensive petroleum-based replacement for the vegetable-based turpentine. It is commonly used as a paint thinner for oil-based paint and cleaning brushes, and as an organic solvent in other applications. Mineral turpentine is chemically very different from turpentine, which mainly consists of pinene, and it has inferior solvent properties. Artists use mineral spirits as an alternative to turpentine since it is less flammable and less toxic. Because of interactions with pigments in oil paints, artists require a higher grade of mineral spirits than many industrial users, including the complete absence of residual sulfur.\n\nMineral spirits were formerly an active ingredient in the laundry soap \"Fels Naptha\", used to dissolve oils and grease in laundry stains, and as a popular remedy for eliminating the contagious oil urushiol in poison ivy. It was removed as a potential health risk.\n\nMineral spirits have a characteristic unpleasant kerosene-like odor. Chemical manufacturers have developed a low odor version of mineral turpentine which contains less of the highly volatile shorter hydrocarbons. Odorless mineral spirits are mineral spirits that have been further refined to remove the more toxic aromatic compounds, and are recommended for applications such as oil painting, where humans have close contact with the solvent.\n\nIn screen printing (also referred to as silk-screening), mineral spirits are often used to clean and unclog screens after printing with oil-based textile and plastisol inks. They are also used to thin inks used in making monoprints.\n\nMineral spirits are often used inside liquid-filled compasses and gauges.\n\nMineral spirits are also used for re-gripping golf clubs. After the old grip is removed, the mineral spirits are poured into the new grip and shaken. After the mineral spirits are poured on, the new underlying tape and the new grip are slid on. After an hour of drying out, the new grip and club are ready to use.\n\nAlthough not normally marketed as a fuel, white spirit can be used as an alternative to kerosene in portable stoves, since it is merely a light grade of kerosene. It cannot be used as an alternative to white gas, which is a much more volatile gasoline-like fuel.\n\nWhite spirits are also a major ingredient in some popular automotive fuel/oil additives, such as Marvel Mystery Oil, as they are capable of dissolving varnish and sludge buildup.\n\nMineral spirits are also commonly used for cutting fluid in ultraprecision lathes (commonly referred to as diamond turning machines).\n\nWhite spirit is mainly classed as an irritant. It has a fairly low acute toxicity by inhalation of the vapour, dermal (touching the skin) and oral routes (ingestion). However, acute exposure can lead to central nervous system depression resulting in lack of coordination and slowed reactions. Exposure to very high concentrations in enclosed spaces can lead to general narcotic effects (drowsiness, dizziness, nausea etc...) and can eventually lead to unconsciousness. Oral ingestion presents a high aspiration hazard. Prolonged or repeated skin exposure over a long period of time can result in severe irritant dermatitis, also called contact dermatitis. Exposure to white spirit in direct contact with the skin for several hours can cause severe chemical burns.\n\nExposure to an average white spirit concentration of 240 mg/m (40 ppm) for more than 13 years (of continuous exposure time) can lead to chronic central nervous system effects. Similar long-term studies have been made in which some of the observed effects included memory impairment, poor concentration, increased irritability etc. White spirit is implicated in the development of chronic toxic encephalopathy (CTE) among house painters. In severe cases CTE may lead to disability and personality changes. These effects in painters were first studied in 1970's in Nordic studies.\n\nOwing to the volatility and low bioavailability of its constituents, white spirit, although it is moderately toxic to aquatic organisms, is unlikely to present significant hazards to the environment. It should not however, be purposely poured down the sink or freshwater drain.\n\nPeople can be exposed to Stoddard solvent in the workplace by breathing it in, swallowing it, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (Permissible exposure limit) for Stoddard solvent exposure in the workplace as 500 ppm (2900 mg/m) over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 350 mg/m over an 8-hour workday and 1800 mg/m over 15 minutes. At levels of 20,000 mg/m, Stoddard solvent is immediately dangerous to life and health.\n\n\n"}
{"id": "51229208", "url": "https://en.wikipedia.org/wiki?curid=51229208", "title": "Wink (platform)", "text": "Wink (platform)\n\nWink is a brand of software and hardware products that connects with and controls smart home devices from a consolidated user interface. Wink, Labs Inc., which develops and markets Wink, was founded in 2014 as a spin-off from invention incubator Quirky. After Quirky went through bankruptcy proceedings, it sold Wink to Flex in 2015. As of 2016, the Wink software is connected to 1.3 million devices. In July 2017, Flex sold Wink to i.am+ for $38.7M.\n\nWink, Labs Inc. was founded at Quirky, an incubator program for inventions that relies on crowd-sourced product ideas. Wink, Labs was originally created as part of a collaboration with General Electric to control co-branded smart home products like air-conditioners. It was founded by current CTO Nathan Smith and received about $20 million in funding. The company spent twelve months working with fifteen electronics manufacturing companies to offer about 60 Wink-compatible products by July 2014. Wink was spun-off from Quirky in June 2014.\n\nAccording to Quirky, Wink products were in 300,000 homes by 2015. In April 2015 Wink experienced a security problem that made many of its hubs go offline or break, forcing the company to issue a recall. The recall caused a several-month inventory backlog and subsequent shortage of the Wink hub. Due to financial difficulties, due in part to the recall, Quirky began looking for buyers to sell Wink to in 2015. That November, after Quirky went through bankruptcy proceedings, it sold Wink for $15 million to Flextronics (now called Flex), to whom Quirky owed $18.7 million. Flex was Wink's primary supplier of firmware and hardware. As of 2016, 1.3 million devices are connected to Wink.\n\nOn 27 July 2017, in its First Quarter Report, Flex announced that it has sold its interest in Wink for $59 million, representing a $38.7 million gain on the balance sheet. Although the Report described the purchaser as \"an unrelated third-party venture backed company\", stories circulated in the technology press identifying the purchaser as i.am+, the technology firm founded by the performer Will.i.am.\n\nWink connects with third-party smart home devices associated with the Internet of Things, such as thermostats, door locks, ceiling fans, and Wi-Fi-enabled lights, to provide a single user interface on a mobile app or via a wall-mounted screen, called Relay. The allows the user to remotely control those devices. The mobile app is free, while consumers pay for a Wink Hub, or Wink Relay, which connects with smart devices in the home. The hubs integrate with competing software standards used by different manufacturers. Wink integrates with software from automated home device brands, such as Canary, which markets an app-controlled home system. In February 2016, new features were introduced to allow Wink to operate on the local network, in case a user's internet connection is down. In June 2016, compatibility with Uber, Fitbit, and IFTTT, was added to the Relay product. A second generation version of the Wink Hub was released in November 2016.\n\nThe second generation Wink Hub supports most smart home devices with Zigbee, ZWave, Lutron Clear Connect, and Kidde protocols. Wink 2 also added Bluetooth Low Energy, 5 GHz Wi-Fi radio, an Ethernet port, and 512MB of memory.\n\nIn October 2017, the Wink Lookout home security system was announced, consisting of open/close sensors, motion sensors, a siren, and the Wink hub.\n\nIn a 2014 competitive review comparing Wink to SmartThings, CNET said Wink was cheaper and supported more wireless standards, but had fewer and less reliable sensors to support automation. The article recommended SmartThings for tech-savvy users and Wink for general consumers. In an August 2014 review, CNET gave Wink a 7.7 score out of 10. It complimented the product for being close to the \"ideal\" whole-home security and automation service, but lamented that it wasn't \"a perfect replacement for some of the more sophisticated standalone smart home device apps.\" A January 2015 review of Wink by Tom's Guide rated the product a 7/10, \"very good\". The reviewer criticized the application for not giving as much control over individual smart home electronics as their own apps, but praised Wink for providing \"an easy way for people to dip their toes into smart home systems.\"\n\nA review in \"PCMAG\" of the Wink Hub 2 said it was easy to use and compatible with many devices, but had no battery backup or USB ports. Under \"Bottom Line\" the review said, \"Works with virtually every wireless protocol out there and supports dual-band Wi-Fi. Installation and device pairing is quick and easy.\" It gave the Hub 2 4.5 out of 5 stars and named it its new Editors' Choice for home automation hubs. In contrast, CNET gave the device three stars. The reviewer said the device is easy to set up and compatible with many devices, but gave the reviewer error messages. The reviewer was never able to successfully set it up the way she wanted. Tom's Guide gave the Wink Hub 2 7 out of 10. It also said the device was easy to use and compatible with many devices, but missing some advanced features. Tom's Guide said it was good for \"basic\" smart homes.\n\n"}
