{"id": "11375114", "url": "https://en.wikipedia.org/wiki?curid=11375114", "title": "A-IX-2", "text": "A-IX-2\n\nA-IX-2 (or hexal) is a Russian explosive used in modern Russian military shells. It consists of 73% RDX with 23% aluminum powder, phlegmatized with 4% wax and It has a Relative effectiveness factor of 1.54.\n\nIt has been in use since WWII by the Red Army.\n"}
{"id": "52084493", "url": "https://en.wikipedia.org/wiki?curid=52084493", "title": "Alfred Inselberg", "text": "Alfred Inselberg\n\nAlfred Inselberg is an American-Israeli mathematician and computer scientist based at Tel Aviv University.\n\nInselberg started his career at the Biological Computer Laboratory based at University of Illinois at Urbana-Champaign. There he was part of a cybernetics group working on biomathematics developing mathematical models of the ear, neural networks, and computer models for Vision and Non-linear Analysis, gaining a PhD in Mathematics and Physics. During this period he participated in the Symposium on Principles of Self-Organization. He is particularly noted for his work on parallel coordinates\nproposed in 1959, for the visualization of multidimensional geometries (as in Linear Algebra) and multivariate problems.\n\nAlfred was born in 1936 in Athens, Greece. Later he attended a great school, Whittingehame College in Brighton England, where he fell in love with \"Geometry\" and especially the ability to gain insight about geometrical problems by visually interacting with drawings depicting them. He attended the University of Illinois in Champaign-Urbana (UICU) receiving a B.Sc. in Aeronautical Engineering. Together with Gary van Sant, and two other students under the guidance of Professor Paul Torda, they founded the University of Illinois Rocket Society in 1953; four years prior to Sputnik. Continuing his studies at UICU he obtained in 1965 a Ph.D. in Applied Mathematics and Physics under the joint guidance of Professors Ray Langebartel and Heinz von Foerster.\n\nHe then held senior research positions at IBM where he developed a Mathematical Model of the Ear(Cochlea) (TIME Nov. 74) and later Collision-Avoidance Algorithms for Air Traffic Control (3 USA patents). Concurrently he had joint appointments at UCLA, USC, Technion and Ben Gurion University. Since 1995 he is Professor at the School of Mathematical Sciences of Tel Aviv University. He was elected Senior Fellow at the San Diego Supercomputing Center in 1996. \nHe is a sought after speaker presenting keynotes at major conferences and events. His textbook on \"Parallel Coordinates: VISUAL Multidimensional Geometry\", was published by Springer and was praised by Stephen Hawking among many others.\n"}
{"id": "15498298", "url": "https://en.wikipedia.org/wiki?curid=15498298", "title": "Allowable Strength Design", "text": "Allowable Strength Design\n\nAllowable Strength Design (ASD) is a term used by the American Institute of Steel Construction (AISC) in the 14th Edition of the Manual of Steel Construction.\n\nAllowable Stress Design philosophy was left unsupported by AISC after the 9th edition of the manual which remained an acceptable reference design standard in evolving building codes (e.g. International Building Code by the International Code Council). This presented problems since new research, engineering concepts and design philosophy were ignored in the minimum requirements and references in the aging 9th edition. As a result, structures that were code compliant based on design using the Allowable Stress Design methods may not have been code compliant if reviewed with the Load and Resistance Factor Design (LRFD) requirements - particularly where the LRFD procedures explicitly defined additional analysis which was not explicitly defined in the Allowable Stress Design procedures.\n\nAISC's Allowable Strength Design applies a quasi-safety factor approach to evaluating allowable strength. Ultimate strength of an element or member is determined in the same manner regardless of the load combination method considered (e.g. ASD or LRFD). Design load combination effects are determined in a manner appropriate to the intended form of the analysis results. ASD load combinations are compared to the ultimate strength reduced by a factor (omega) which provides a mathematical form similar to Allowable Stress Design resolved with a safety factor.\n\nThis AISC Allowable Strength Design does not attempt to relate capacity to elastic stress levels. Therefore, it is inappropriate to refer to the procedure or philosophy as either Allowable Stress or Permissible Stress Design.\n"}
{"id": "2667794", "url": "https://en.wikipedia.org/wiki?curid=2667794", "title": "Ambrosia artemisiifolia", "text": "Ambrosia artemisiifolia\n\nAmbrosia artemisiifolia, with the common names common ragweed, annual ragweed, and low ragweed, is a species of the genus \"Ambrosia\" native to regions of the Americas.\n\nThe species name, \"artemisiifolia\", is given because the leaves were thought to bear a resemblance to the leaves of Artemisia, the true wormwoods.\n\nIt has also been called the common names: American wormwood, bitterweed, blackweed, carrot weed, hay fever weed, Roman wormwood, short ragweed, stammerwort, stickweed, tassel weed.\n\nThe plant is native to: North America across Canada, the eastern and central United States, the Great Plains, and in Alaska; the Caribbean on Cuba, Hispaniola, and Jamaica; and South America in the southern bioregion (Argentina, Chile, Paraguay, Uruguay), the western bioregion (Bolivia, Peru), and Brazil. The distribution of common ragweed in Europe is expected to expand northwards in the future\n\nIt is the most widespread species of the genus in North America, which most of the other species of \"Ambrosia\" are endemic to.\n\n\"Ambrosia artemisiifolia\" is an annual plant that emerges in late spring. It propagates mainly by rhizomes, but also by seed.\n\nIt is much-branched, and grows up to in height. The pinnately divided soft and hairy leaves are long.\n\nIts bloom period is July to October in North America. Its pollen is wind-dispersed, and can be a strong allergen to people with hay fever.\n\nIt produces 2–4 mm obconic green to brown fruit. It sets seed in later summer or autumn. Since the seeds persist into winter and are numerous and rich in oil, they are relished by songbirds and upland game birds.\n\nCommon ragweed, \"Ambrosia artemisiifolia\", is a widespread invasive species, and can become a noxious weed, that has naturalized in: Europe; temperate Asia and the Indian subcontinent; temperate northern and southern Africa and Macronesia; Oceania in Australia, New Zealand, and Hawaii; and Southwestern North America in California and the Southwestern United States.\n\nCommon ragweed is a very competitive weed and can produce yield losses in soybeans as high as 30 percent. Control with night tillage reduces emergence by around 45 percent. Small grains in rotation will also suppress common ragweed if they are overseeded with clover. Otherwise, the ragweed will grow and mature and produce seeds in the small grain stubble.\n\nIts wind-blown pollen is highly allergenic.\n\n several herbicides were effective against common ragweed, although resistant populations were known to exist. In 2007 several \"Ambrosia artemisiifolia\" populations were glyphosate resistant, exclusively in the USA.\n\nSMARTER is a European interdisciplinary network of experts involved in the control of ragweed, health care professionals, aerobiologists, ecologists, economists, and atmospheric and agricultural modellers.\n\n\"Ambrosia artemisiifolia\" was a traditional medicinal plant for Native American tribes, including the Cherokee, Lakota, Iroquois, Dakota, and Delaware.\n\n\"Ambrosia artemisiifolia\" is used in phytoremediation projects remediating soil pollution, for removing heavy metals such as Lead from contaminated soil.\n\n"}
{"id": "437887", "url": "https://en.wikipedia.org/wiki?curid=437887", "title": "Audio editing software", "text": "Audio editing software\n\nAudio editing software is software which allows editing and generating of audio data. Audio editing software can be implemented completely or partly as library, as computer application, as Web application or as a loadable kernel module. Wave Editors are digital audio editors and there are many sources of software available to perform this function. Most can edit music, apply effects and filters, adjust stereo channels etc.\n\nA digital audio workstation (DAW) consists of software to a great degree, and usually is composed of many distinct software suite components, giving access to them through a unified graphical user interface using GTK+, Qt or some other library for the GUI widgets.\n\nEditors designed for use with music typically allow the user to do the following: \nTypically these tasks can be performed in a manner that is non-linear. Audio editors may process the audio data non-destructively in real-time, or destructively as an \"off-line\" process, or a hybrid with some real-time effects and some off-line effects.\n\nDestructive editing modifies the data of the original audio file, as opposed to just editing its playback parameters. Destructive editors are also known as \"sample editors\".\n\nDestructive editing applies edits and processing directly to the audio data, changing the data immediately. If, for example, part of a track is deleted, the \"deleted\" audio data is immediately removed from that part of the track.\n\nReal-time editing does not apply changes immediately, but applies edits and processing on the fly during playback. If, for example, part of a track is deleted, the \"deleted\" audio data is not actually removed from the track, but is hidden and will be skipped on playback.\n\n\n\n\n\nEditors designed for use in speech research add the ability to make measurements and perform acoustic analyses such as extracting and displaying a fundamental frequency contour or spectrogram. They typically lacks most or all of the effects that interest musicians.\n\n"}
{"id": "15023595", "url": "https://en.wikipedia.org/wiki?curid=15023595", "title": "C166 family", "text": "C166 family\n\nThe C166 family is a 16-bit microcontroller architecture from Infineon (formerly the semiconductor division of Siemens) in cooperation with STMicroelectronics. It was first released in 1993 and is a controller for measurement and control tasks. It uses the well-established RISC architecture, but features some microcontroller-specific extensions such as bit-addressable memory and an interrupt system optimized for low-latency. When this architecture was introduced the main focus was to replace 8051 controllers (from Intel).\n\nOpcode compatible successors of the C166 family are the C167 family, XC167 family, the XE2000 family and the XE166 family.\n\nAs of 2017, microcontrollers using the C166 architecture are still being manufactured by NIIET in Voronesh, Russia, as part of the 1887 series of integrated circuits. This includes a radiation-hardened device under the designation 1887VE6T ().\n\nThe Siemens/Infineon \"C167 family\" or STMicroelectronics \"ST10 family\" is a further development of the \"C166 family\". It has improved addressing modes and support for \"atomic\" instructions. Variants include, for example, Controller Area Network (CAN bus).\n"}
{"id": "43297496", "url": "https://en.wikipedia.org/wiki?curid=43297496", "title": "Ceramic valve", "text": "Ceramic valve\n\nA ceramic valve is a valve with ceramic trim, ball, seat, disc or lining. A carbon or stainless steel body is used to protect the ceramic trim from being damaged by sudden thermal or mechanical shock. Advanced ceramics are used in the manufacture including alumina, zirconia and silicon nitride. Significant benefits of the use of ceramic in valves (when compared to steel or other traditional materials) include resistance to wear and their lower mass. Thanks to the excellent corrosion resistance, abrasive resistance and wear resistance, ceramic valves are often used in severe corrosive and abrasive applications, such as FGD, and pneumatic refuse conveying systems.\n"}
{"id": "2847631", "url": "https://en.wikipedia.org/wiki?curid=2847631", "title": "Classé", "text": "Classé\n\nClassé is a Canadian manufacturer of high-performance music and theater components, such as amplifiers, pre-amplifiers and processors. In 2001 Classé became part of the Bowers & Wilkins Group. The distribution, financial and organizational resources offered by Bowers & Wilkins have propelled Classé to the forefront of contemporary high-end audio design.\n\nClassé amplifiers are used in both homes and professional recording studios like London's Abbey Road, and mastering facilities like Sterling Sound in New York City.\n\nClassé produced its first power amplifier in 1980. In 2001, Classé became part of the Bowers & Wilkins Group.\nSound United have purchased Classé 08-01-2018\n"}
{"id": "2026112", "url": "https://en.wikipedia.org/wiki?curid=2026112", "title": "Construction management", "text": "Construction management\n\nConstruction management (CM) is a professional service that uses specialized, project management techniques to oversee the planning, design, and construction of a project, from its beginning to its end. The purpose of CM is to control a project's time, cost and quality—sometimes referred to as a project's \"triple constraint.\" CM is compatible with all project delivery systems, including design-bid-build, design-build, CM At-Risk and Public Private Partnerships. Professional construction managers may be reserved for lengthy, large-scale, high budget undertakings (commercial real estate, transportation infrastructure, industrial facilities, and military infrastructure), called capital projects.\n\nContractors are assigned to a construction project during the design or once the design has been completed by a licensed architect or a licensed civil engineer. This is done by going through a bidding process with different contractors. The contractor is selected by using one of three common selection methods: low-bid selection, best-value selection, or qualifications-based selection.\n\nA construction manager should have the ability to handle public safety, time management, cost management, quality management, decision making, mathematics, working drawings, and human resources.\n\nThe functions of construction management typically include the following: \n\nThe Construction Management Association of America (CMAA) states the most common responsibilities of a Construction Manager fall into the following 7 categories: Project Management Planning, Cost Management, Time Management, Quality Management, Contract Administration, Safety Management, and CM Professional Practice. CM professional practice includes specific activities, such as defining the responsibilities and management structure of the project management team, organizing and leading by implementing project controls, defining roles and responsibilities, developing communication protocols, and identifying elements of project design and construction likely to give rise to disputes and claims.\n\n\nA bid is given to the owner by construction managers that are willing to complete their construction project. A bid tells the owner how much money they should expect to pay the construction management company in order for them to complete the project.\n\n\n\nThe stages of a typical construction project have been defined as feasibility, design, construction and operation, each stage relating to the project life cycle.\nFeasibility and design involves four steps: programming and feasibility, schematic design, design development, and contract documents. It is the responsibility of the design team to ensure that the design meets all building codes and regulations. It is during the design stage that the bidding process takes place.\n\nThe pre-construction stage begins when the owner gives a notice to proceed to the contractor that they have chosen through the bidding process. A notice to proceed is when the owner gives permission to the contractor to begin their work on the project. The first step is to assign the project team which includes the project manager (PM), contract administrator, superintendent, and field engineer.\nDuring the pre-construction stage, a site investigation must take place. A site investigation takes place to discover if any steps need to be implemented on the job site. This is in order to get the site ready before the actual construction begins. This also includes any unforeseen conditions such as historical artifacts or environment problems. A soil test must be done to determine if the soil is in good condition to be built upon.\n\nThe procurement stage is when labor, materials and equipment needed to complete the project are purchased. This can be done by the general contractor if the company does all their own construction work. If the contractor does not do their own work, they obtain it through subcontractors. Subcontractors are contractors who specialize in one particular aspect of the construction work such as concrete, welding, glass, or carpentry. Subcontractors are hired the same way a general contractor would be, which is through the bidding process. Purchase orders are also part of the procurement stage.\n\nThe construction stage begins with a pre-construction meeting brought together by the superintendent (on an American project). The pre-construction meeting is meant to make decisions dealing with work hours, material storage, quality control, and site access. The next step is to move everything onto the construction site and set it all up.\n\nA Contractor progress payment schedule is a schedule of when (according to project milestones or specified dates) contractors and suppliers will be paid for the current progress of installed work.\n\nProgress payments are partial payments for work completed during a portion, usually a month, during a construction period. Progress payments are made to general contractors, subcontractors, and suppliers as construction projects progress. Payments are typically made on a monthly basis but could be modified to meet certain milestones. Progress payments are an important part of contract administration for the contractor. Proper preparation of the information necessary for payment processing can help the contractor financially complete the project.\nOnce the owner moves into the building, a warranty period begins. This is to ensure that all materials, equipment, and quality meet the expectations of the owner that are included within the contract.\n\nWhen construction vehicles are driving around a site or moving earth, a lot of dust is created, especially during the dryer months. This may cause disruption for surrounding businesses or homes. A popular method of dust control is to have a water truck driving through the site spraying water on the dry dirt to minimize the movement of dust within and out of the construction site. When water is introduced mud is created. This mud sticks to the tires of the construction vehicles and is often lead out to the surrounding roads. A street sweeper may clean the roads to reduce dirty road conditions.\n\n\nProject meetings take place at scheduled intervals to discuss the progress on the construction site and any concerns or issues. The discussion and any decisions made at the meeting must be documented.\n\nDiaries, logs, and daily field reports keep track of the daily activities on a job site each day.\n\nLabor statements are required on a daily basis. Also list of Labor, PERT CPM are needed for labor planning to complete a project in time.\n\n\nConstruction Management education comes in a variety of formats: formal degree programs (Two-year associate degree; four-year baccalaureate degree, masters degree, project management, operations management engineer degree, doctor of philosophy degree, postdoctoral researcher); on-the-job-training; and continuing education and professional development. Information on degree programs is available from ABET, the American Council for Construction Education (ACCE), the Construction Management Association of America (CMAA) or the Associated Schools of Construction (ASC).\nAccording to the American Council for Construction Education (one of the academic accreditation agencies responsible for accrediting construction management programs in the U.S.), the academic field of construction management encompasses a wide range of topics. These range from general management skills, through management skills specifically related to construction, to technical knowledge of construction methods and practices. There are many schools offering Construction Management programs, including some offering a master's degree.\n\nCapital project management software (CPMS) refers to the systems that are currently available that help capital project owner/operators, program managers, and construction managers, control and manage the vast amount of information that capital construction projects create. A collection, or portfolio of projects only makes this a bigger challenge. These systems go by different names: capital project management software, computer construction software, construction management software, project management information systems. Usually Construction Management can be referred as subset of CPMS where the scope of CPMS is not limited to construction phases of project. Among main construction management software can be mentioned Procore and PlanGrid.\n\n\nThe construction industry typically includes three parties: an owner, a licensed designer (architect or engineer) and a builder (usually known as a general contractor). There are traditionally two contracts between these parties as they work together to plan, design and construct the project. The first contract is the owner-designer contract, which involves planning, design, and construction contract administration. The second contract is the owner-contractor contract, which involves construction. An indirect third-party relationship exists between the designer and the contractor, due to these two contracts.\n\nAn owner may also contract with a construction project management company as an adviser, creating a third contract relationship in the project. The construction manager's role is to provide construction advice to the designer, design advice to the constructor on the owner's behalf and other advice as necessary.\n\nThe phrase \"design, bid, build\" describes the prevailing model of construction management, in which the general contractor is engaged through a tender process after designs have been completed by the architect or engineer.\n\nMany owners – particularly government agencies – let out contracts known as \"design-build contracts\". In this type of contract, the construction team (known as the design-builder) is responsible for taking the owner's concept and completing a detailed design before (following the owner's approval of the design) proceeding with construction. Virtual design and construction technology may be used by contractors to maintain a tight construction time.\n\nThere are three main advantages to a design-build contract. First, the construction team is motivated to work with the architect to develop a practical design. The team can find creative ways to reduce construction costs without reducing the function of the final product. The second major advantage involves the schedule. Many projects are commissioned within a tight time frame. Under a traditional contract, construction cannot begin until after the design is finished and the project has been awarded to a bidder. In a design-build contract the contractor is established at the outset, and construction activities can proceed concurrently with the design. The third major advantage is that the design-build contractor has an incentive to keep the combined design and construction costs within the owner's budget. If speed is important, design and construction contracts can be awarded separately; bidding takes place on preliminary plans in a not-to-exceed contract instead of a single firm design-build contract.\n\nThe major problem with design-build contracts is an inherent conflict of interest. In a standard contract the architect works for the owner and is directly responsible to the owner. In design-build the architect works for the design-builder, not the owner, therefor the design-builder may make design and construction decisions that benefit the design-builder, but that do not benefit the owner. During construction, the architect normally acts as the owner's representative. This includes reviewing the builder's work and ensuring that the products and methods meet specifications and codes. The architect's role is compromised when the architect works for the design-builder and not for the owner directly. Thus, the owner may get a building that is over-designed to increase profits for the design-builder, or a building built with lesser-quality products to maximize profits.\n\nProject-management methodology is as follows:\n\n\nConstruction cost management is a fee-based service in which the construction manager (CM) is responsible exclusively to the owner, acting in the owner's interests at every stage of the project. The construction manager offers impartial advice on matters such as:\n\nComprehensive management of every stage of the project, beginning with the original concept and project definition, yields the greatest benefit to owners. As time progresses beyond the pre-design phase, the CM's ability to effect cost savings diminishes. The agency CM can represent the owner by helping select the design and construction teams and managing the design (preventing scope creep), helping the owner stay within a predetermined budget with value engineering, cost-benefit analysis and best-value comparisons. The software-application field of construction collaboration technology has been developed to apply information technology to construction management.\n\nCM at-risk is a delivery method which entails a commitment by the construction manager to deliver the project within a Guaranteed Maximum Price (GMP). The construction manager acts as a consultant to the owner in the development and design phases (preconstruction services), and as a general contractor during construction. When a construction manager is bound to a GMP, the fundamental character of the relationship is changed. In addition to acting in the owner's interest, the construction manager must control construction costs to stay within the GMP.\n\nCM at-risk is a global term referring to the business relationship of a construction contractor, owner and architect (or designer). Typically, a CM at-risk arrangement eliminates a \"low-bid\" construction project. A GMP agreement is a typical part of the CM-and-owner agreement (comparable to a \"low-bid\" contract), but with adjustments in responsibility for the CM. The advantage of a CM at-risk arrangement is budget management. Before a project's design is completed (six to eighteen months of coordination between designer and owner), the CM is involved with estimating the cost of constructing a project based on the goals of the designer and owner (design concept) and the project's scope. In balancing the costs, schedule, quality and scope of the project, the design may be modified instead of redesigned; if the owner decides to expand the project, adjustments can be made before pricing. To manage the budget before design is complete and construction crews mobilized, the CM conducts site management and purchases major items to efficiently manage time and cost.\n\n\n\nAn at-risk delivery method is best for large projects—both complete construction and renovation—that are not easy to define, have a possibility of changing in scope, or have strict schedule deadlines. Additionally, it is an efficient method in projects containing technical complexity, multi-trade coordination, or multiple phases.\n\nStarting with its Accelerated Bridge Program in the late 2000s, the Massachusetts Department of Transportation began employing accelerated construction techniques, in which it signs contracts with incentives for early completion and penalties for late completion, and uses intense construction during longer periods of complete closure to shorten the overall project duration and reduce cost.\n\n\n"}
{"id": "3584524", "url": "https://en.wikipedia.org/wiki?curid=3584524", "title": "Design load", "text": "Design load\n\nIn a general sense, the design load is the maximum amount of something a system is designed to handle or the maximum amount of something that the system can produce, which are very different meanings. For example, a crane with a design load of 20 tons is designed to be able to lift loads that weigh 20 tons or less. However, when a failure could be catastrophic, such as a crane dropping its load or collapsing entirely, a factor of safety is necessary. As a result, the crane should lift about 2 to 5 tons at the most. \n\nIn structural design, a design load is greater than the load which the system is expected to support. This is because engineers incorporate a safety factor in their design, in order to ensure that the system will be able to support at least the expected loads (called specified loads, despite any problems with construction, materials, etc. that go unnoticed during construction.\n\nA heater would have a general design load, meaning the maximum amount of heat it can produce. A bridge would have a specified load, with the design load being determined by engineers and applied as a theoretical load intended to ensure the actual real-world capacity of the specified load.\n\n"}
{"id": "30377575", "url": "https://en.wikipedia.org/wiki?curid=30377575", "title": "Dissolved gas flotation", "text": "Dissolved gas flotation\n\nDissolved gas flotation (DGF) systems are used for a variety of applications throughout the world. The process floats solids, oils and other contaminants to the surface of liquids. Once on the surface these contaminants are skimmed off and removed from the liquids. Oil and gas production facilities have used flotation systems to remove oil and solids from their produced and processed water (wastewater) for many years.The relative density of candle wax is 0.93, hence objects made of wax float on water.\n\nThe keys to good separation are both gravity and the creation of millions of very small bubbles. Based on Stokes' law, the size of the oil droplet and density of the droplet will affect the rate of rise to the surface. The larger and lighter the droplet, the faster it will rise to the surface. By attaching a small gas bubble to an oil droplet, the density of the droplet decreases, which increases the rate at which it will rise to the surface. Therefore, the smaller the gas bubbles created the smaller the oil droplet floated to the surface. Efficient flotation systems need to create as many small bubbles as possible. \n\nThe method in which the bubbles are introduced into the water stream and retention time are also important factors. The average retention time for a vertical unit is typically 4 to 5 minutes and 5 to 6 minutes for a horizontal unit.\n\nThe impeller in a DGF pump is designed with dual sides. One side is designed to drive the liquid like a normal centrifugal pump and the other side is designed to draw vapor into the pump and mix it with the liquid. In addition to the new impeller, a special seal was invented to extend the life of the pump. With these innovations the pump creates a sub-atmospheric pressure region within the pump's seal chamber. As the impeller draws in the vapor it is mixed with the liquid being pumped and compressed into micro-fine bubbles. Because of the close tolerance between the backvanes of the impeller and the backplate of the pump the vapor is sheared into fine bubbles and then they are compressed in the sub-atmospheric pressure region of the pump. These fine bubbles become dissolved into the liquid within the volute of the pump.\n\nThe result of this process provides similar size bubbles to a dissolved air flotation system. The backpressure valve on the discharge piping can regulate the bubble size in a DGF pump. The bubble size ranges from 50 down to 1 micrometer or less.\n\n\n"}
{"id": "9594112", "url": "https://en.wikipedia.org/wiki?curid=9594112", "title": "Emios", "text": "Emios\n\nEmios (an acronym for Environmental Memory Interoperable Open Service) is an MDD / MDE platform that aims to provide a range of services for storing and sharing information about environmental research activities.\n\nEmios, initiated by C. Faucher in June 2006, is based on the Environmental Memory concepts developed by the \"Motive\" CNRS committee and specifically by Franck Guarnieri in 2003.\n\nEmios is a set of Eclipse plugins based on EMF, distributed under the terms of the EPL License.\nThe current version is the 0.0.1 and mainly contains the Geographic Information Standards Manager: GISM. GISM is the first part of Emios and implements the ISO 19100 series of International Standards from ISO TC211.\n\nFaucher C., Gourmelon F., Lafaye J.Y., Rouan M., \"Mise en place d’une mémoire environnementale adaptée aux besoins d’un observatoire du domaine cotier : MEnIr\", Revue Internationale de Géomatique, Hermès/Lavoisier, vol 19/1, , pp. 7-26, 2009, http://geo.e-revues.com/\n\nFaucher C., Lafaye J.Y., 2007. Model-Driven Engineering for implementing the ISO 19100 series of international standards, \"CoastGIS 07, the 8th International Symposium on GIS and Computer Mapping for Coastal Zone Management\", vol. 2, p. 424-433, 7-10 October, Santander, Spain.\n\nEmios web site\n"}
{"id": "17388115", "url": "https://en.wikipedia.org/wiki?curid=17388115", "title": "Floating ground", "text": "Floating ground\n\nMost electrical circuits have a ground which is electrically connected to the Earth, hence the name \"ground\". The ground is said to be \"floating\" when this connection does not exist.\n\nConductors are also described as having a floating voltage if they are not connected electrically to another non-floating conductor. Without such a connection, voltages and current flows are induced by electromagnetic fields or charge accumulation within the conductor rather than being due to the usual external potential difference of a power source.\n\nElectrical equipment may be designed with a floating ground for one of several reasons. One is safety. For example, a low voltage DC power supply, such as a mobile phone charger is connected to the mains through a transformer of one type or another, and there is no direct electrical connection between the current return path on the low-voltage side and physical ground (earth). Ensuring that there is no electrical connection between mains voltage and the low-voltage plug makes it much easier to guarantee safety of the supply. It also allows the charger to safely only connect to live and neutral, which allows a two-prong plug in countries where this is relevant. Indeed, any home appliance with a two-prong plug must have a floating ground.\n\nAnother application is in electronic test equipment. Suppose you wish to measure a 0.5V potential difference between two wires that are both approximtely 100V above Earth ground. If your measuring device has to connect to Earth, some of its electronic components will have to deal with a 100V potential difference across their terminals. If the whole device floats, then its electronics will only see the 0.5V difference, allowing more delicate components to be used which can make more precise measurements. Such devices are often battery powered.\n\nThirdly, a floating ground can help eliminate ground loops, which reduces the noise coupled to the system. An example of such a configuration is shown in the image on the right. Systems isolated in this manner can and do drift in potential and if the transformer is capable of supplying much power, they can be dangerous. This is particularly likely if the floated system is near high voltage power lines. To reduce the danger of electric shocks, the chassis of the instruments are usually connected separately to Earth ground.\n\nFloating grounds can be dangerous if they are caused by failure to properly ground equipment that was designed to require grounding, because the chassis can be at a very different potential from that of any nearby organisms, who then get an electric shock upon touching it. Live chassis TVs, where the set's ground is derived by rectifying live mains, were common until the 1990s.\n\nExposed live grounds are dangerous. They are live, and can electrocute end users if touched. Headphone sockets fitted by end users to live chassis TVs are especially dangerous, as not only are they often live, but any electrical shock will pass through the user’s head. Sets that have both a headphone socket and a live chassis use an audio isolation transformer to make the arrangement safe.\n\nFloating grounds can cause problems with audio equipment using RCA connectors (also called phono connectors). With these common connectors, the signal pin connects before the ground, and 2 pieces of equipment can have a greater difference between their grounds than it takes to saturate the audio input. As a result, plugging or unplugging while powered up can result in very loud noises in speakers. If the ground voltage difference is small, it tends to only cause hum and clicks.\n\nA residual current device can be incorporated into a system to reduce but not eliminate the risks caused by a floating ground.\n\n"}
{"id": "5767604", "url": "https://en.wikipedia.org/wiki?curid=5767604", "title": "Frequency divider", "text": "Frequency divider\n\nA frequency divider, also called a clock divider or scaler or prescaler, is a circuit that takes an input signal of a frequency, formula_1, and generates an output signal of a frequency:\n\nwhere formula_3 is an integer. Phase-locked loop frequency synthesizers make use of frequency dividers to generate a frequency that is a multiple of a reference frequency. Frequency dividers can be implemented for both analog and digital applications.\n\nAnalog frequency dividers are less common and used only at very high frequencies. Digital dividers implemented in modern IC technologies can work up to tens of GHz.\n\nA regenerative frequency divider, also known as a Miller frequency divider, mixes the input signal with the feedback signal from the mixer.\n\nThe feedback signal is formula_4. This produces sum and difference frequencies formula_4, formula_6 at the output of the mixer. A low pass filter removes the higher frequency and the formula_4 frequency is amplified and fed back into mixer.\n\nA free-running oscillator which has a small amount of a higher-frequency signal fed to it will tend to oscillate in step with the input signal. Such frequency dividers were essential in the development of television.\n\nIt operates similarly to an injection locked oscillator. In an injection locked frequency divider, the frequency of the input signal is a multiple (or fraction) of the free-running frequency of the oscillator. While these frequency dividers tend to be lower power than broadband static (or flip-flop based) frequency dividers, the drawback is their low locking range. The ILFD locking range is inversely proportional to the quality factor (Q) of the oscillator tank. In integrated circuit designs, this makes an ILFD sensitive to process variations. Care must be taken to ensure the tuning range of the driving circuit (for example, a voltage-controlled oscillator) must fall within the input locking range of the ILFD.\n\nFor power-of-2 integer division, a simple binary counter can be used, clocked by the input signal. The least-significant output bit alternates at 1/2 the rate of the input clock, the next bit at 1/4 the rate, the third bit at 1/8 the rate, etc. An arrangement of flipflops is a classic method for integer-n division. Such division is frequency and phase coherent to the source over environmental variations including temperature. The easiest configuration is a series where each flip-flop is a divide-by-2. For a series of three of these, such system would be a divide-by-8. By adding additional logic gates to the chain of flip flops, other division ratios can be obtained. Integrated circuit logic families can provide a single chip solution for some common division ratios.\n\nAnother popular circuit to divide a digital signal by an even integer multiple is a Johnson counter. This is a type of shift register network that is clocked by the input signal. The last register's complemented output is fed back to the first register's input. The output signal is derived from one or more of the register outputs. For example, a divide-by-6 divider can be constructed with a 3-register Johnson counter. The six valid values of the counter are 000, 100, 110, 111, 011, and 001. This pattern repeats each time the network is clocked by the input signal. The output of each register is a f/6 square wave with 60° of phase shift between registers. Additional registers can be added to provide additional integer divisors.\n\n(\"Classification:\" \"asynchronous sequential logic\")<br>\nAn arrangement of D flip-flops are a classic method for integer-n division. Such division is frequency and phase coherent to the source over environmental variations including temperature. The easiest configuration is a series where each D flip-flop is a divide-by-2. For a series of three of these, such system would be a divide-by-8. More complicated configurations have been found that generate odd factors such as a divide-by-5. Standard, classic logic chips that implement this or similar frequency division functions include the 7456, 7457, 74292, and 74294. (see List of 7400 series integrated circuits)\n\nA fractional-n frequency synthesizer can be constructed using two integer dividers, a divide-by-n and a divide-by-(n + 1) frequency divider. With a modulus controller, n is toggled between the two values so that the VCO alternates between one locked frequency and the other. The VCO stabilizes at a frequency that is the time average of the two locked frequencies. By varying the percentage of time the frequency divider spends at the two divider values, the frequency of the locked VCO can be selected with very fine granularity.\n\nIf the sequence of divide by n and divide by (n + 1) is periodic, spurious signals appear at the VCO output in addition to the desired frequency. Delta-sigma fractional-n dividers overcome this problem by randomizing the selection of n and (n + 1), while maintaining the time-averaged ratios.\n\n\n"}
{"id": "82245", "url": "https://en.wikipedia.org/wiki?curid=82245", "title": "Geiger–Müller tube", "text": "Geiger–Müller tube\n\nThe Geiger–Müller tube or G–M tube is the sensing element of the Geiger counter instrument used for the detection of ionizing radiation. It was named after Hans Geiger, who invented the principle in 1908, and Walther Müller, who collaborated with Geiger in developing the technique further in 1928 to produce a practical tube that could detect a number of different radiation types.\n\nIt is a gaseous ionization detector and uses the Townsend avalanche phenomenon to produce an easily detectable electronic pulse from as little as a single ionising event due to a radiation particle. It is used for the detection of gamma radiation, X-rays, and alpha and beta particles. It can also be adapted to detect neutrons. The tube operates in the \"Geiger\" region of ion pair generation. This is shown on the accompanying plot for gaseous detectors showing ion current against applied voltage.\n\nWhile it is a robust and inexpensive detector, the G–M is unable to measure high radiation rates efficiently, has a finite life in high radiation areas and cannot measure incident radiation energy, so no spectral information can be generated and there is no discrimination between radiation types; such as between alpha and beta particles.\n\nA G-M tube consists of a chamber filled with a gas mixture at a low pressure of about 0.1 atmosphere. The chamber contains two electrodes, between which there is a potential difference of several hundred volts. The walls of the tube are either metal or have their inside surface coated with a conducting material or a spiral wire to form the cathode, while the anode is a wire mounted axially in the centre of the chamber.\n\nWhen ionizing radiation strikes the tube, some molecules of the fill gas are ionized directly by the incident radiation, and if the tube cathode is an electrical conductor, such as stainless steel, indirectly by means of secondary electrons produced in the walls of the tube, which migrate into the gas. This creates positively charged ions and free electrons, known as ion pairs, in the gas. The strong electric field created by the voltage across the tube's electrodes accelerates the positive ions towards the cathode and the electrons towards the anode. Close to the anode in the \"avalanche region\" where the electric field strength rises exponentially as the anode is approached, free electrons gain sufficient energy to ionize additional gas molecules by collision and create a large number of electron avalanches. These spread along the anode and effectively throughout the avalanche region. This is the \"gas multiplication\" effect which gives the tube its key characteristic of being able to produce a significant output pulse from a single original ionising event.\nIf there were to be only one avalanche per original ionising event, then the number of excited molecules would be in the order of 10 to 10. However the production of \"multiple avalanches\" results in an increased multiplication factor which can produce 10 to 10 ion pairs. The creation of multiple avalanches is due to the production of UV photons in the original avalanche, which are not affected by the electric field and move laterally to the axis of the anode to instigate further ionising events by collision with gas molecules. These collisions produce further avalanches, which in turn produce more photons, and thereby more avalanches in a chain reaction which spreads laterally through the fill gas, and envelops the anode wire. The accompanying diagram shows this graphically. The speed of propagation of the avalanches is typically 2–4 cm per microsecond, so that for common sizes of tubes the complete ionisation of the gas around the anode takes just a few microseconds.\nThis short, intense pulse of current can be measured as a \"count event\" in the form of a voltage pulse developed across an external electrical resistor. This can be in the order of volts, thus making further electronic processing simple.\n\nThe discharge is terminated by the collective effect of the positive ions created by the avalanches. These ions have lower mobility than the free electrons due to their higher mass and move slowly from the vicinity of the anode wire. This creates a \"space charge\" which counteracts the electric field that is necessary for continued avalanche generation. For a particular tube geometry and operating voltage this termination always occurs when a certain number of avalanches have been created, therefore the pulses from the tube are always of the same magnitude regardless of the energy of the initiating particle. Consequently, there is no radiation energy information in the pulses which means the Geiger–Muller tube cannot be used to generate spectral information about the incident radiation. In practice the termination of the avalanche is improved by the use of \"quenching\" techniques (see later).\nPressure of the fill gas is important in the generation of avalanches. Too low a pressure and the efficiency of interaction with incident radiation is reduced. Too high a pressure, and the “mean free path” for collisions between accelerated electrons and the fill gas is too small, and the electrons cannot gather enough energy between each collision to cause ionisation of the gas. The energy gained by electrons is proportional to the ratio “e/p”, where “e” is the electric field strength at that point in the gas, and “p” is the gas pressure.\n\nBroadly, there are two main types of Geiger tube construction.\n\nFor alpha particles, low energy beta particles, and low energy X-rays, the usual form is a cylindrical end-window tube. This type has a window at one end covered in a thin material through which low-penetrating radiation can easily pass. Mica is a commonly used material due to its low mass per unit area. The other end houses the electrical connection to the anode.\n\nThe pancake tube is a variant of the end window tube, but which is designed for use for beta and gamma contamination monitoring. It has roughly the same sensitivity to particles as the end window type, but has a flat annular shape so the largest window area can be utilised with a minimum of gas space. Like the cylindrical end window tube, mica is a commonly used window material due to its low mass per unit area. The anode is normally multi-wired in concentric circles so it extends fully throughout the gas space.\n\nThis general type is distinct from the dedicated end window type, but has two main sub-types, which use different radiation interaction mechanisms to obtain a count.\n\nUsed for gamma radiation detection above energies of about 25 KeV, this type generally has an overall wall thickness of about 1-2mm of chrome steel. Because most high energy gamma photons will pass through the low density fill gas without interacting, the tube uses the interaction of photons on the molecules of the wall material to produce high energy secondary electrons within the wall. Some of these electrons are produced close enough to the inner wall of the tube to escape into the fill gas. As soon as this happens the electron drifts to the anode and an electron avalanche occurs as though the free electron had been created within the gas. The avalanche is a secondary effect of a process that starts within the tube wall with the production of electrons that migrate to the inner surface of the tube wall, and then enter the fill gase. This effect is considerably attentuated at low energies below about 20 KeV \n\nThin walled tubes are used for:\n\nG–M tubes will not detect neutrons since these do not ionise the gas. However, neutron-sensitive tubes can be produced which either have the inside of the tube coated with boron, or the tube contains boron trifluoride or helium-3 as the fill gas. The neutrons interact with the boron nuclei, producing alpha particles, or directly with the helium-3 nuclei producing hydrogen and tritium ions and electrons. These charged particles then trigger the normal avalanche process.\n\nThe components of the gas fill mixture are an inert gas such as helium, argon or neon which is ionised by incident radiation, and a \"quench\" gas of 5–10% of an organic vapor or a halogen gas to prevent spurious pulsing by quenching the electron avalanches. This combination of gases is known as a Penning mixture and makes use of the Penning ionization effect.\n\nThe modern halogen-filled G–M tube was invented by Sidney H. Liebson in 1947 and has several advantages over the older tubes with organic mixtures. The halogen tube discharge takes advantage of a metastable state of the inert gas atom to more-readily ionize a halogen molecule than an organic vapor, enabling the tube to operate at much lower voltages, typically 400–600 volts instead of 900–1200 volts. While halogen-quenched tubes have greater plateau voltage slopes compared to organic-quenched tubes (an undesirable quality), they have a vastly longer life than tubes quenched with organic compounds. This is because the organic vapor is gradually destroyed by the discharge process (giving organic-quenched tubes a useful life of around 10 events), while the halogen ions can recombine over time (giving halogen-quenched tubes an effectively unlimited lifetime for most uses, although they will still eventually fail at some point due to other ionization-initiated processes that limit the lifetime of all Geiger tubes). For these reasons, the halogen-quenched tube is now the most common.\n\nThe \"Geiger plateau\" is the voltage range in which the G-M tube operates in its correct mode, where ionisation occurs along the length of the anode. If a G–M tube is exposed to a steady radiation source and the applied voltage is increased from zero, it follows the plot of current shown in the \"Geiger region\" where the gradient flattens; this is the Geiger plateau.\n\nThis is shown in more detail in the accompanying Geiger Plateau Curve diagram. If the tube voltage is progressively increased from zero the efficiency of detection will rise until the most energetic radiation starts to produce pulses which can be detected by the electronics. This is the \"starting voltage\". Increasing the voltage still further results in rapidly rising counts until the \"knee\" or threshold of the plateau is reached, where the rate of increase of counts falls off. This is where the tube voltage is sufficient to allow a complete discharge along the anode for each detected radiation count, and the effect of different radiation energies are equal. However, the plateau has a slight slope mainly due to the lower electric fields at the ends of the anode because of tube geometry. As the tube voltage is increased, these fields strengthen to produce avalanches. At the end of the plateau the count rate begins to increase rapidly again, until the onset of continuous discharge where the tube cannot detect radiation, and may be damaged..\n\nDepending on the characteristics of the specific tube (manufacturer, size, gas type, etc.) the voltage range of the plateau will vary. The slope is usually expressed as percentage change of counts per 100V. To prevent overall efficiency changes due to variation of tube voltage, a regulated voltage supply is used, and it is normal practice to operate in the middle of the plateau to reduce the effect of any voltage variations. \n\nThe ideal G–M tube should produce a single pulse for every single ionising event due to radiation. It should not give spurious pulses, and should recover quickly to the passive state, ready for the next radiation event. However, when positive argon ions reach the cathode and become neutral atoms by gaining electrons, the atoms can be elevated to enhanced energy levels. These atoms then return to their ground state by emitting photons which in turn produce further ionisation and thereby spurious secondary discharges. If nothing were done to counteract this, ionisation would be prolonged and could even escalate. The prolonged avalanche would increase the \"dead time\" when new events cannot be detected, and could become continuous and damage the tube. Some form of quenching of the ionisation is therefore essential to reduce the dead time and protect the tube, and a number of quenching techniques are used. \n\nSelf-quenching or internal-quenching tubes stop the discharge without external assistance, originally by means of the addition of a small amount of a polyatomic organic vapor originally such as butane or ethanol, but for modern tubes is a halogen such as bromine or chlorine.\n\nIf a poor gas quencher is introduced to the tube, the positive argon ions, during their motion toward the cathode, would have multiple collisions with the quencher gas molecules and transfer their charge and some energy to them. Thus, neutral argon atoms would be produced and the quencher gas ions in their turn would reach the cathode, gain electrons therefrom, and move into excited states which would decay by photon emission, producing tube discharge. However, effective quencher molecules, when excited, lose their energy not by photon emission, but by dissociation into neutral quencher molecules. No spurious pulses are thus produced.\n\nEven with chemical quenching, for a short time after a discharge pulse there is a period during which the tube is rendered insensitive and is thus temporarily unable to detect the arrival of any new ionizing particle (the so-called \"dead time\"; typically 50–100 microseconds). This causes a loss of counts at sufficiently high count rates and limits the G–M tube to an effective (accurate) count rate of approximately 10 counts per second even with external quenching. While a G-M tube is technically capable of reading higher count rates before it truly saturates, the level of uncertainty involved and the risk of saturation makes it extremely dangerous to rely upon higher count rate readings when attempting to calculate an equivalent radiation dose rate from the count rate. A consequence of this is that ion chamber instruments are usually preferred for higher count rates, however a modern external quenching technique can extend this upper limit considerably.\n\nExternal quenching, sometimes called \"active quenching\" or \"electronic quenching\", uses simplistic high speed control electronics to rapidly remove and re-apply the high voltage between the electrodes for a fixed time after each discharge peak in order to increase the maximum count rate and lifetime of the tube. Although this can be used instead of a quench gas, it is much more commonly used in conjunction with a quench gas.\n\nThe \"time-to-first-count method\" is a sophisticated modern implementation of external quenching that allows for dramatically increased maximum count rates via the use of statistical signal processing techniques and much more complex control electronics. Due to uncertainty in the count rate introduced by the simplistic implementation of external quenching, the count rate of a Geiger tube becomes extremely unreliable above approximately 10 counts per second. With the time-to-first-count method, effective count rates of 10 counts per second are achievable, two orders of magnitude larger than the normal effective limit. The time-to-first-count method is significantly more complicated to implement than traditional external quenching methods, and as a result of this it has not seen widespread use.\n\nOne consequence of the dead time effect is the possibility of a high count rate continually triggering the tube before the recovery time has elapsed. This may produce pulses too small for the counting electronics to detect and lead to the very undesirable situation whereby a G–M counter in a very high radiation field is falsely indicating a low level. This phenomenon is known as \"fold-back\". An industry rule of thumb is that the discriminator circuit receiving the output from the tube should detect down to 1/10 of the magnitude of a normal pulse to guard against this. Additionally the circuit should detect when \"pulse pile-up \" has occurred, where the apparent anode voltage has moved to a new dc level through the combination of high pulse count and noise. The electronic design of Geiger–Muller counters must be able to detect this situation and give an alarm; it is normally done by setting a threshold for excessive tube current.\n\nThe efficiency of detection of a G–M tube varies with the type of incident radiation. Tubes with thin end windows have very high efficiencies (can be nearly 100%) for high energy beta, though this drops off as the beta energy decreases due to attenuation by the window material. Alpha particles are also attenuated by the window. As alpha particles have a maximum range of less than 50 mm in air, the detection window should be as close as possible to the source of radiation. The attenuation of the window adds to the attenuation of air, so the window should have a density as low as 1.5 to 2.0 mg/cm to give an acceptable level of detection efficiency. The article on stopping power explains in more detail the ranges for particles types of various energies.\nThe counting efficiency of photon radiation (gamma and X-rays above 25 keV) depends on the efficiency of radiation interaction in the tube wall, which increases with the atomic number of the wall material. Chromium iron is a commonly used material, which gives an efficiency of about 1% over a wide range of energies.\n\nIf a G–M tube is to be used for gamma or X-ray dosimetry measurements the energy of incident radiation, which affects the ionising effect, must be taken into account. However pulses from a G–M tube do not carry any energy information, and attribute equal dose to each count event. Consequently the count rate response of a “bare” GM-tube to photons at different energy levels is non-linear with the effect of over-reading at low energies. The variation in dose response can be a factor between 5 to 15, according to individual tube construction; the very small tubes having the highest values.\n\nTo correct this a technique known as “Energy Compensation” is applied, which consists of adding a shield of absorbing material round the tube. This filter preferentially absorbs the low energy photons and the dose response is “flattened“. The aim is that sensitivity/energy characteristic of the tube should be matched by the absorption/energy characteristic of the filter. This cannot be exactly achieved, but the result is a more uniform response over the stated range of detection energies for the tube. \n\nLead and tin are commonly used materials, and a simple filter effective above can be made using a continuous collar along the length of the tube. However, at lower energy levels this attenuation can become too great, so air gaps are left in the collar to allow low energy radiation to have a greater effect. In practice, compensation filter design is an empirical compromise to produce an acceptably uniform response, and a number of different materials and geometries are used to obtain the required correction.\n\n\n"}
{"id": "46855758", "url": "https://en.wikipedia.org/wiki?curid=46855758", "title": "Goods wagons of welded construction", "text": "Goods wagons of welded construction\n\nGoods wagons of welded construction () were developed and built by the Deutsche Reichsbahn in Germany from 1933 to about 1945. With the introduction of welding technology in 1933 almost all wagon components were joined by welding and no longer by rivetting. This enabled goods wagons to be designed, for example, for higher speeds or for higher payloads through the use of different types of steel and other engineering changes, but their further development was so heavily influenced by the exigencies of the Second World War that, as early as 1939, the Deutsche Reichsbahn had to temper the design of goods wagons to the new economic circumstances. Because there were overlaps in the change from the \"Austauschbauart\" - goods wagons made with interchangeable components - to the new welded classes, the period of the changeover cannot be exactly defined. Several standard goods wagons and their classes are covered in other articles. Goods wagons built during the Second World War that were purely intended for military transport use, are covered under the article on \"Kriegsbauart\" - wartime classes.\n\nAs early as 1921 the development of goods wagons in the German states began to distinguish between those without special features, that represented the standard or norm, and those with certain characteristics i.e. that had a special equipment or properties.\n\nLikewise the welded class of goods wagons were divided into: \n\nGermany's goods wagons of \"Austauschbauart\" design, with their interchangeable components, were too expensively designed to continue to be economically manufactured. Consequently, the Deutsche Reichsbahn began to modify existing models or bring out new designs for all goods wagon classes, both from an economic standpoint and with regard to the increasing competition they faced in the transportation market from lorries. This led to a series of trials of various wagon types that got under way in 1932. The aim was to develop a series production in which the covered vans could be permitted to run at speeds higher than 65 km/h and open wagons would have a higher payload than 15 tonnes. The increase in maximum speed was necessary to enable goods wagons to be inserted in fast through trains as well as enabling goods wagons to be hauled within passenger trains. Through the introduction of various materials and production technologies, wagons could be produced more efficiently and the newly developed goods vans and trucks were continually developed and their designs optimised.\n\nThis wagon class was built in several variants. For example, the first series of these vans were delivered as \"Grs\" i.e. able to be transferred onto the Russian broad gauge network. There were also variants with steam heating (Ghs) and with both steam and electrical heating (Gehs). The first trial wagons were built in 1934 in order to find an alternative to the Gr Kassel class. These goods vans were also known colloquially as \"short Oppelns\".\n\n\n\n\n\n"}
{"id": "51607642", "url": "https://en.wikipedia.org/wiki?curid=51607642", "title": "Grecia (toucan)", "text": "Grecia (toucan)\n\nGrecia (hatched around March, 2014) is a chestnut-mandibled toucan widely known as the first toucan to receive a prosthetic 3D printed beak.\n\nGrecia was born in the wild in or around Grecia, one of the Cantons of Costa Rica. The toucan was beaten by youth and its top beak broke off. Government officials transported the bird to animal rescue center Zoo Ave, west of the city of Alajuela. A few 3D printing companies joined efforts to create a prosthetic beak which was successfully attached to Grecia. The bird received its name from the town of Grecia where it was picked up by city officials.\n\n"}
{"id": "44733676", "url": "https://en.wikipedia.org/wiki?curid=44733676", "title": "HackerNest", "text": "HackerNest\n\nHackerNest is a not-for-profit organization and global movement founded on January 11, 2011. The organization unites local technology communities around the world through community events and socially beneficial hackathons to further its mission of economic development through technological proliferation. It is Canada's largest, most prolific technology community with growing international popularity.\n\nHackerNest was founded on the belief that the fastest, most permanent way to improve the world is to build supportive local technology and innovation communities characterized by trust, sharing, and respect - everywhere. The rationale is that the technology community is the cornerstone of economic development enabling collaboration, innovation, knowledge-sharing, recruiting, and scientific progress. Growing and strengthening the community lets businesses hire better, perform better, and create more jobs, which ultimately increases economic prosperity.\n\nThe organization's ideology is deeply rooted in chaos theory, the idea that minor tweaks at the start of a process in a dynamic system can have a major impact on the end result. Similar to how the seemingly-insignificant act of handing a child a pencil culminated in the artistic legacy of Pablo Picasso decades later, making a new friend at a Tech Social could result in a partnership that one day cures cancer. HackerNest \"splinter cells\" (chapters) regularly host \"Tech Socials\" that are open to anyone interested in technology. The events vary slightly by city, but maintain the same core tenets: all are friendly and down-to-earth.\n\nThe first Tech Social was held in Toronto on Monday, January 31, 2011. HackerNest Toronto is currently the world's largest Meetup group for programmers and Canada's largest technologist community.\n\nAs of July 2017, HackerNest splinter cells have run over 550+ events in 34 cities across 14 countries on 5 continents.\n\nHackerNest offers hackathon production and innovation consulting services to companies, organizations, and governments.\n\nIn 2014, HackerNest produced Construct, Canada's largest hardware hackathon and DementiaHack for the British government, the world's first hackathon dedicated to helping people with dementia and their caregivers.\n\nIn 2015, the organization produced Deloitte's first internal innovation hackathon as well as DementiaHack with Facebook as the lead sponsor and support from the UK government, the Public Health Agency of Canada, and the Ontario Ministry of Health and Long-Term Care.\n\nIn 2016, HackerNest produced CourtHack with the US National Center for State Courts in Salt Lake City at the Utah Supreme Court (featuring Supreme Court Justice Constandinos Himonas as a judge) and the Hack4Equality LGBTQ hackathon with Grindr in Los Angeles which heavily featured White House Promise Zone and Opportunity Project data.\n\nHackerNest refers to its chapters in different cities as \"splinter cells\", a tongue-in-cheek reference to the eponymous popular video game franchise. Splinter cells are independently managed by volunteers and produce regular Tech Social events.\n\n\n\n\n\n\nPast HackerNest sponsors include Amazon, Microsoft, and Facebook. Notable companies that have donated office space as venues for Tech Socials include Google (Kitchener-Waterloo), Facebook (Seattle), Microsoft (Kuala Lumpur), Techstars (New York), and Twitter (New York).\n\nHackerNest actively participates on the City of Toronto's Innovation & Technology Advisory Committee and the Young Entrepreneur Council Advisory Body established by former Deputy Mayor Norm Kelly to help shape the city's interaction with the technology community.\n\n\n"}
{"id": "22574083", "url": "https://en.wikipedia.org/wiki?curid=22574083", "title": "I-City", "text": "I-City\n\ni-City is a 72 acre ICT-based urban development beside the Federal Highway in Section 7, Shah Alam, Selangor, Malaysia. Planned by architect Jon A. Jerde, i-City was designed as a fully integrated intelligent city, comprising corporate, leisure and residential components such as a 1 million sq. ft. regional shopping mall, office towers, Cybercentre office suites, hotels, apartments, data centers, and an innovation center.\n\ni-City is an MSC Malaysia Cybercentre where knowledge-based companies with MSC Malaysia status can exploit the various incentives offered under the MSC Malaysia Bills of Guarantee. It has also been endorsed as a tourism destination by the Ministry of Tourism and declared an International Park by the Selangor State Government. Such venues allow for entertainment and other cosmopolitan lifestyle outlets to operate on a 24-7 basis.\n\ni-City gross development value has increased to MYR 9 billion project from MYR 1.5 billion in 2005.\n\nIn 2017, i-City was named named one of the world’s top 25 brightest and most colourful places by CNN Travel. \n\ni-City plans to be a business location for 6 industry clusters, namely shared services & outsourcing, biotechnology, software development, as a media hub, an Islamic financial hub, and a data centre.\n\nWithin i-City, CityPark is the designated building to house MSC Malaysia-status companies. CityPark consists of 6 blocks and has a total office space of more than 300,000 sqft. The entire i-City development will feature eight corporate office towers, two luxury hotels, two blocks of 24-story residences, three and five story shop-offices, and retail suites.\n\ni-City is working with Best Western International for its 3-star hotel and Hilton International to build four-star DoubleTree i-City Hotel.\n\ni-City is in a joint-venture with Central Pattana of Thailand to develop a MYR 800 million CentralPlaza@i-City shopping mall. The mall will boasts gross floor area of 1.5 million square feet. Upon completion the mall will be largest shopping centre in Shah Alam. The shopping mall is expected to be opened in the second quarter of 2017.\n\ni-City has launched their Data Centre Park, which aims to be one of the largest data centre space providers in a single location in Malaysia. The Data Centre Park is designed for data centre service providers and large companies who plan to build their own data centre buildings. The park has the capacity to host up to four centres with a combined floor space of 200,000 sq ft.\n\nSnoWalk is a 50,000 sq ft Arctic environment attraction with 100 tons of ice sculptures that were shaped by a team of 30 ice sculptors from Harbin, China. With temperatures below 5 degrees Celsius and 100mm of snow on the ground, SnoWalk will be a family leisure attraction.\n\nWaterWorld at i-City features water rides and games ranging from tame to wild in their propensity to provide thrills for family. The main attraction consists of vortex water ride from a steep 160 ft launch tower and water slides.\n\nRed Carpet 2 is Malaysia’s first home-grown interactive wax museum. Measuring up to 30,000 square foot, Red Carpet 2 allows visitors to have fun with their favorite Superheroes like the Avengers, Royals, Head of States, Sci-fi, Tech Giants like Jack Ma and Mark Zuckerberg, Corporate Movers, MTV, Outer Space, Magical World, K-pop (Hallyu), Canto pop, Oscars, A-List artistes and more.\n\ni-City has hosted various large-scale events and programmes, such as Malaysia’s Independence Day Celebration on 31 August and the Mid-Autumn Festival. During the 2010 World Cup, i-City also showed the matches live from Johannesburg on its 6.5m x 8.5m video wall, the largest video screen in Southeast Asia. It has also hosted MTV World Stage Live in Malaysia, in July 2011.\n\n\"True Discovery\" is a reptile exhibition located behind Old Town White Coffee Shop. It houses a reptile collection as well as exotic animals like snakes and tarantulas. Among its attractions is a two headed terrapin.\n\ni-City will be connected by the Bandar Utama-Klang LRT Line via the I-City LRT Station once the LRT line starts operation in 2020. Currently, the nearest KTM Komuter station to i-City is Padang Jawa.\n\n"}
{"id": "51683865", "url": "https://en.wikipedia.org/wiki?curid=51683865", "title": "IIMBx", "text": "IIMBx\n\nIIMBx is a MOOC platform where free online courses are offered by the Indian Institute of Management, Bangalore (also known as IIMB). Operating on the edX platform, it offers free MOOCs and MicroMasters programs in a wide range of disciplines to a worldwide student body. IIMBx is also the national coordinator for SWAYAM (Study Webs of Active–Learning for Young Aspiring Mind), a MOOC-based initiative by the Government of India.\n\nIIMBx follows a weekly learning sequence which consists of lecture videos, online quizzes, multimedia cases, references, readings, and live sessions (via webinar). There are also discussion forums where students post queries and interact with their peers and the course team. These courses are offered in English and transcribed in Hindi to help more learners across India. Courses offered by IIMBx have no admission pre-requisite. Most of the courses run over the duration of 4–6 weeks. The passing grade differs from course to course, and learners can access this information in the ‘grading policy’ section. Learners can audit the course free of cost or receive a certificate of completion for a fee.\n\nIIMB is the first and only business school in India to offer MOOCs on edX. The initiative of launching MOOCs was taken up by former Director of IIMB, Sushil Vachani. IIMBx started its journey in August 2014 with five professors on board and has reached out to over 800,000 learners in 190 countries since then. In May 2017, IIM-B partnered with TCS iON to facilitate proctored exams for IIMBx MOOC courses.\n\nOnline courses at IIMBx are designed to give students, working professionals, and lifelong learners the opportunity to upgrade their skills for academic and professional careers. MOOCs are available in a wide range of management-related disciplines like marketing, strategy, accounting, operations, information systems, people management, finance, statistics, analytics, information technology management, international business, and sustainability.\n\nIn addition to its standalone MOOCs, IIMBx offers different MicroMasters programs for an enhanced, in-depth learning experience. These programs run for a longer time than MOOCs and are recognized by potential employers. In addition to the MicroMasters, IIMBx also offers credit-eligible programs: Fundamentals of Business Management, Specialized Marketing Courses, MBA Preparatory Programme, Certificate in Technology and Management (CTM), Professional Development Programme, and the General Management Programme for Young Leaders (YLP). The recent one being CTM, which is a joint programme from IIT Madras and IIM Bangalore. This is a 10-month programme comprising online courses offered by IIM Bangalore's IIMBx Programme and IIT Madras' CCE and RBCDSAI.\n\nThe IIMBx programme is founded on the philosophy that everyone – irrespective of financial or regional constraints – should have access to quality education. Led by IIMB faculty, IIMBx uses digital learning tools to enable anytime, anywhere learning in a global classroom. The vision of the IIMBx programme is to use digital learning to enable widespread access to management education.\n\nIIMBx conducts faculty development programs every quarter where teachers from across India learn to implement blended learning methods in the classroom. Through its FDPs, IIMBx has created a community of more than 700 educators who are keen to include MOOCs in the classroom.\n\nIIMBx organizes the Future of Learning conference every year. This event brings together leaders of higher educational institutions, policy makers, practitioners and L&D Technology providers to provoke thought, showcase innovation and share knowledge.\n"}
{"id": "2422001", "url": "https://en.wikipedia.org/wiki?curid=2422001", "title": "ISACA", "text": "ISACA\n\nISACA is an international professional association focused on IT governance. On its IRS filings, it is known as the Information Systems Audit and Control Association, although ISACA now goes by its acronym only.\n\nISACA originated in United States in 1967, when a group of individuals working on auditing controls in computer systems started to become increasingly critical of the operations of their organizations. They identified a need for a centralized source of information and guidance in the field. In 1969, Stuart Tyrnauer, an employee of the (then) Douglas Aircraft Company, incorporated the group as the EDP Auditors Association (EDPAA). Tyrnauer served as the body's founding chairman for the first three years. In 1976 the association formed an education foundation to undertake large-scale research efforts to expand the knowledge of and value accorded to the fields of governance and control of information technology.\n\nThe association became the \"Information Systems Audit and Control Association\" in 1994.\n\nIn March 2016, ISACA bought the CMMI Institute who is behind the Capability Maturity Model Integration.\n\nISACA currently serves more than 110,000 constituents (members and professionals holding ISACA certifications) in more than 180 countries. The job titles of members are such as IS auditor, consultant, educator, IS security professional, regulator, chief information officer, chief information security officer and internal auditor. They work in nearly all industry categories. There is a network of ISACA chapters with more than 200 chapters established in over 80 countries. Chapters provide education, resource sharing, advocacy, networking and other benefits.\n\n\n\nThe CSX-P, ISACA's first cybersecurity certification, was introduced in the summer of 2015. It is one of the few certifications that require the individual to work in a live environment, with real problems, to obtain a certification. Specifically, the exam puts test takers in a live network with a real incident taking place. The student's efforts to respond to the incident and fix the problem results in the type of score awarded.\n\n\n"}
{"id": "4162918", "url": "https://en.wikipedia.org/wiki?curid=4162918", "title": "In-flight entertainment", "text": "In-flight entertainment\n\nIn-flight entertainment (IFE) refers to the entertainment available to aircraft passengers during a flight. In 1936, the airship \"Hindenburg\" offered passengers a piano, lounge, dining room, smoking room, and bar during the 2 1/2 day flight between Europe and America. After the Second World War, IFE was delivered in the form of food and drink services, along with an occasional projector movie during lengthy flights. In 1985 the first personal audio player was offered to passengers, along with noise cancelling headphones in 1989. During the 1990s, the demand for better IFE was a major factor in the design of aircraft cabins. Before then, the most a passenger could expect was a movie projected on a screen at the front of a cabin, which could be heard via a headphone socket at his or her seat. Now, in most aircraft, private IFE TV screens are offered on most airlines.\n\nThe current European trend is to implement bring your own device systems that provide internet connectivity, allowing the user to stream a predefined range of multimedia content. Following this trend, companies such as Immfly are advancing at a fast pace to deliver on-board entertainment on short-haul commercial flights.\n\nDesign issues for IFE include system safety, cost efficiency, software reliability, hardware maintenance, and user compatibility.\n\nThe in-flight entertainment onboard airlines is frequently managed by content service providers.\n\nThe first in-flight movie was in 1921 on Aeromarine Airways showing a film called \"Howdy Chicago\" to its passengers as the amphibious airplane flew around Chicago. \nThe film \"The Lost World\" was shown to passengers of an Imperial Airways flight in April 1925 between London (Croydon Airport) and Paris.\n\nEleven years later in 1932, the first in-flight television called 'media event' was shown on a Western Air Express Fokker F.10 aircraft.\n\nThe post-WWII British Bristol Brabazon airliner was initially specified with a 37-seat cinema within its huge fuselage; this was later reduced to a 23-seat cinema sharing the rear of the aircraft with a lounge and cocktail bar. The aircraft never entered service.\n\nHowever, it was not until the 1960s that in-flight entertainment (other than reading, sitting in a lounge and talking, or looking out the window) was becoming mainstream and popular. In 1961, David Flexer of Inflight Motion Pictures developed the 16mm film system using a 25-inch reel for a wide variety of commercial aircraft. Capable of holding the entire film, and mounted horizontally to maximize space, this replaced the previous 30-inch-diameter film reels. In 1961, TWA committed to Flexer's technology and was first to debut a feature film in flight. Interviewed by the New Yorker in 1962, Mr Flexner said, \"an awful lot of ingenuity has gone into this thing, which started from my simply thinking one day, in flight, that air travel is both the most advanced form of transportation and the most boring.” Amerlon Productions, a subsidiary of Inflight, produced at least one film, \"Deadlier Than the Male\", specifically for use on airplanes.\n\nIn 1963, AVID Airline Products developed and manufactured the first pneumatic headset used on board the airlines and provided these early headsets to TWA. These early systems consisted of in-seat audio that could be heard with hollow tube headphones. In 1979 pneumatic headsets were replaced by electronic headsets. The electronic headsets were initially available only on selected flights and premium cabins whereas economy class still had to make do with the old pneumatic headsets. In the United States, the last airline to offer pneumatic headphones was Delta Air Lines, which switched to electronic headphones in 2003, despite the fact that all Delta aircraft equipped with in-flight entertainment since the Boeing 767-200 have included jacks for electronic headphones.\n\nThroughout the early to mid-1960s, some in-flight movies were played back from videotape, using early compact transistorized videotape recorders made by Sony (such as the SV-201 and PV-201) and Ampex (such as the VR-660 and VR-1500), and played back on CRT monitors mounted on the upper sides in the cabin above the passenger seats with several monitors placed a few seats apart from each other. The audio was played back through the headsets.\n\nIn 1971, TRANSCOM developed the 8mm film cassette. Flight attendants could now change movies in-flight and add short subject programming.\n\nIn the late 1970s and early 1980s, CRT-based projectors began to appear on newer widebody aircraft, such as the Boeing 767. These used LaserDiscs or video cassettes for playback. Some airlines upgraded the old film IFE systems to the CRT-based systems in the late 1980s and early 1990s on some of their older widebodies. In 1985, Avicom introduced the first audio player system, based on the Philips Tape Cassette technology. In 1988, the Airvision company introduced the first in-seat audio/video on-demand systems using LCD technology for Northwest Airlines. The trials, which were run by Northwest Airlines on its Boeing 747 fleet, received overwhelmingly positive passenger reaction. As a result, this completely replaced the CRT technology.\n\nToday, in-flight entertainment is offered as an option on almost all wide body aircraft, while some narrow body aircraft are not equipped with any form of In-flight entertainment at all. This is mainly due to the aircraft storage and weight limits. The Boeing 757 was the first narrow body aircraft to widely feature both audio and video In-flight entertainment and today it is rare to find a Boeing 757 without an In-flight entertainment system. Most Boeing 757s feature ceiling-mounted CRT screens, although some newer 757s may feature drop-down LCDs or audio-video on demand systems in the back of each seat. Many Airbus A320 series and Boeing 737 Next Generation aircraft are also equipped with drop-down LCD screens. Some airlines, such as WestJet, United Airlines, and Delta Air Lines, have equipped some narrow body aircraft with personal video screens at every seat. Others, such as Air Canada and JetBlue, have even equipped some regional jets with AVOD.\n\nFor the introduction of personal TVs onboard jetBlue, company management tracked that lavatory queuing went far down. They originally had two planes, one with functioning IFE and one with none, the functioning one later was called \"the happy plane\".\n\nOne major obstacle in creating an in-flight entertainment system is system safety. With the sometimes miles of wiring involved, voltage leaks and arcing become a problem. This is of more than theoretical concern. The IFE system was implicated in the crash of Swissair Flight 111 in 1998. To contain any possible issues, the in-flight entertainment system is typically isolated from the main systems of the aircraft. In the United States, for a product to be considered safe and reliable, it must be certified by the FAA and pass all of the applicable requirements found in the Federal Aviation Regulations. The concerning section, or title, dealing with the aviation industry and the electronic systems embedded in the aircraft, is CFR title 14 part 25. Contained inside Part 25 are rules relating to the aircraft's electronic system.\n\nThere are two major sections of the FAA's airworthiness regulations that regulate flight entertainment systems and their safety in transport category aircraft: 14 CFR 25.1301 which approves the electronic equipment for installation and use, by assuring that the system in question is properly labeled, and that its design is appropriate to its intended function. 14 CFR 25.1309 states that the electrical equipment must not alter the safety or functionality of the aircraft upon the result of a failure. One way for the intended IFE system to meet this regulatory requirement is for it to be independent from the aircraft's main power source and processor. By separating the power supplies and data links from that of the aircraft's performance processor, in the event of a failure the system is self-sustained, and can not alter the functionality of the aircraft. Upon a showing of compliance to all of the applicable U.S. regulations the in-flight entertainment system is capable of being approved in the United States. Certain U.S. design approvals for IFE may be directly accepted in other countries, or may be capable of being validated, under existing bilateral airworthiness safety agreements.\n\nThe companies involved are in a constant battle to cut costs of production, without cutting the system's quality and compatibility. Cutting production costs may be achieved by anything from altering the housing for personal televisions, to reducing the amount of embedded software in the in-flight entertainment processor. Difficulties with cost are also present with the customers, or airlines, looking to purchase in-flight entertainment systems. Most in-flight entertainment systems are purchased by existing airlines as an upgrade package to an existing fleet of aircraft. This cost can be anywhere from $2 million to $5 million for a plane to be equipped with a set of seat back LCD monitors and an embedded IFE system. Some of the IFE systems are being purchased already installed in a new aircraft, such as the Airbus A320, which eliminates the possibility of having upgrade difficulties. Some airlines are passing the cost directly into the customers ticket price, while some are charging a user fee based on an individual customers use. Some are also attempting to get a majority of the cost paid for by advertisements on, around, and in their IFE.\n\nThe largest international airlines sometimes pay more than $90,000 for a licence to show one movie over a period of two or three months. These airlines usually feature up to 100 movies at once, whereas 20 years ago they would have only 10 or 12. In the United States, airlines pay a flat fee every time the movie is watched by a passenger. Some airlines spend up to $20 million per year on content.\n\nSoftware for In-flight entertainment systems should be aesthetically pleasing, reliable, compatible, and also must be user friendly. These restrictions account for expensive engineering of individually specific software. In-flight entertainment equipment is often touch screen sensitive, allowing interaction between each seat in the aircraft and the flight attendants, which is wireless in some systems. Along with a complete aircraft intranet to deal with, the software of the in-flight entertainment system must be reliable when communicating to and from the main In-flight entertainment processor. These additional requirements not only place an additional strain on the software engineers, but also on the price. Programming errors can slip through the testing phases of the software and cause problems.\n\nA moving-map system is a real-time flight information video channel broadcast through to cabin project/video screens and personal televisions (PTVs). In addition to displaying a map that illustrates the position and direction of the plane, the system gives the altitude, airspeed, outside air temperature, distance to the destination, distance from the origination point, and local time. The moving-map system information is derived in real time from the aircraft's flight computer systems.\n\nThe first moving-map system designed for passengers was named Airshow and introduced in 1982. It was invented by Airshow Inc (ASINC), a small southern California corporation, which later became part of Rockwell Collins. KLM and Swissair were the first airlines to offer the moving map systems to their passengers.\n\nThe latest versions of moving-maps offered by IFE manufacturers include AdonisOne IFE, ICARUS Moving Map Systems, Airshow 4200 by Rockwell Collins, iXlor2 by Panasonic Avionics and JetMap HD by Honeywell Aerospace. In 2013, Betria Interactive unveiled FlightPath3D, a fully interactive moving-map that enables passengers to zoom and pan around a 3D world map using touch gestures, similar to Google Earth. FlightPath3D was chosen by Norwegian as the moving-map on their new fleet of Boeing 787 Dreamliners, running on Panasonic's Android based touch-screen IFE system.\n\nAfter the attempted Christmas Day bombing of 2009, the United States Transportation Security Administration (TSA) briefly ordered the live-map shut-off on international flights landing in the United States. Some airlines complained that doing so may compel the entire IFE system to remain shut. After complaints from airlines and passengers alike, these restrictions were eased.\n\nAudio entertainment covers music, as well as news, information, and comedy. Most music channels are pre-recorded and feature their own DJs to provide chatter, song introductions, and interviews with artists. In addition, there is sometimes a channel devoted to the plane's radio communications, allowing passengers to listen in on the pilot's in-flight conversations with other planes and ground stations.\n\nIn audio-video on demand (AVOD) systems, software such as MusicMatch is used to select music off the music server. Phillips Music Server is one of the most widely used servers running under Windows Media Center used to control AVOD systems.\n\nThis form of in-flight entertainment is experienced through headphones that are distributed to the passengers. The headphone plugs are usually only compatible with the audio socket on the passenger's armrest (and vice versa), and some airlines may charge a small fee to obtain a pair. The headphones provided can also be used for the viewing of personal televisions.\n\nIn-flight entertainment systems have been made compatible with XM Satellite Radio and with iPods, allowing passengers to access their accounts or bring their own music, along with offering libraries of full audio CDs from an assortment of artists.\n\nVideo entertainment is provided via a large video screen at the front of a cabin section, as well as smaller monitors situated every few rows above the aisles. Sound is supplied via the same headphones as those distributed for audio entertainment.\n\nHowever, personal televisions (PTVs) for every passenger provide passengers with channels broadcasting new and classic films, as well as comedies, news, sports programming, documentaries, children's shows, and drama series. Some airlines also present news and current affairs programming, which are often pre-recorded and delivered in the early morning before flights commence.\n\nPTVs are operated via an In flight Management System which stores pre-recorded channels on a central server and streams them to PTV equipped seats during flight. AVOD systems store individual programs separately, allowing a passenger to have a specific program streamed to them privately, and be able to control the playback.\n\nSome airlines also provide video games as part of the video entertainment system. For example, Singapore Airlines passengers on some flights have access to a number of Super Nintendo games as part of its \"KrisWorld\" entertainment system. Also Virgin America's and V Australia's new \"RED\" Entertainment System offers passengers internet gaming over a Linux-based operating system.\n\nMost airlines have now installed personal televisions (otherwise known as PTVs) for every passenger on most long-haul routes. These televisions are usually located in the seat-backs or tucked away in the armrests for front row seats and first class. Some show direct broadcast satellite television which enables passengers to view live TV broadcasts. Some airlines also offer video games using PTV equipment.Many are now providing closed captioning for deaf and hard-of-hearing passengers.\n\nAudio-video on demand (AVOD) entertainment has also been introduced. This enables passengers to pause, rewind, fast-forward, or stop a program that they have been watching. This is in contrast to older entertainment systems where no interactivity is provided for. AVOD also allows the passengers to choose among movies stored in the aircraft computer system.\n\nIn addition to the personal televisions that are installed in the seatbacks, a new portable media player (PMP) revolution is under way. There are two types available: commercial off the shelf (COTS) based players and proprietary players. PMPs can be handed out and collected by the cabin crew, or can be \"semi-embedded\" into the seatback or seat arm. In both of these scenarios, the PMP can pop in and out of an enclosure built into the seat, or an arm enclosure. An advantage of PMPs is that, unlike seatback PTVs, equipment boxes for the inflight entertainment system do not need to be installed under the seats, since those boxes increase the weight of the aircraft and impede legroom.\n\nPersonal on-demand videos are stored in an aircraft's main in-flight entertainment system, whence they can be viewed on demand by a passenger over the aircraft's built in media server and wireless broadcast system. Along with the on-demand concept comes the ability for the user to pause, rewind, fast forward, or jump to any point in the movie. There are also movies that are shown throughout the aircraft at one time, often on shared overhead screens or a screen in the front of the cabin. More modern aircraft are now allowing Personal Electronic Devices (PEDs) to be used to connect to the on board in-flight entertainment systems.\n\nRegularly scheduled in flight movies began to premiere in 1961 on flights from New York to Los Angeles.\n\nClosed captioning technology for deaf and hard-of-hearing passengers started in 2008 with Emirates Airlines. The captions are text streamed along with video and spoken audio and enables passengers to either enable or disable the subtitle/caption language. Closed captioning is capable of streaming various text languages, including Arabic, Chinese, English, French, German, Hindi, Spanish, and Russian. The technology is currently based on Scenarist file multiplexing so far; however, portable media players tend to use alternative technologies. A WAEA technical committee is trying to standardize the closed caption specification. In 2009, the US Department of Transportation ruled a compulsory use of captions of all videos, DVDs, and other audio-visual displays played for safety and/or informational purposes in aircraft should be high-contrast captioned (e.g., white letters on a consistent black background [14 CFR Part 382/ RIN 2105–AD41 /OST Docket No. 2006–23999]). As of 2013, several airlines, including \nhave closed-captioning provided on their AVOD systems.\n\nVideo games are another emerging facet of in-flight entertainment. Some game systems are networked to allow interactive playing by multiple passengers. Later generations of IFE games began to shift focus from pure entertainment to learning. The best examples of this changing trend are the popular trivia game series and the Berlitz Word Traveler that allows passengers to learn a new language in their own language. Appearing as a mixture of lessons and mini games, passengers can learn the basics of a new language while being entertained. Many more learning applications continue to appear in the IFE market.\n\nIn several airlines from the Muslim world the AVOD systems provide Qibla directions to allow Muslims to pray toward Mecca (e.g. Emirates, Iran Air, Etihad, Malaysia Airlines, Qatar Airways, Royal Jordanian and Saudia); Malaysia Airlines has built-in Qur'an e-books and Garuda Indonesia has a unique Qur'an channel.\n\nIn recent years, IFE has been expanded to include in-flight connectivity—services such as Internet browsing, text messaging, cell phone usage (where permitted), and emailing. In fact, some in the airline industry have begun referring to the entire in-flight-entertainment category as \"IFEC\" (In-Flight Entertainment and Connectivity or In-Flight Entertainment and Communication).\n\nThe airline manufacturer Boeing entered into the in-flight-connectivity industry in 2000 and 2001 with an offshoot called Connexion by Boeing. The service was designed to provide in-flight broadband service to commercial airlines; Boeing built partnerships with United Airlines, Delta, and American. By 2006, however, the company announced it was closing down its Connexion operation. Industry analysts cited technology, weight, and cost issues as making the service unfeasible at the time. The Connexion hardware that needed to be installed on an aircraft, for example, weighed nearly , which added more \"drag\" (a force working against the forward movement of the plane) and weight than was tolerable for the airlines.\n\nSince the shuttering of Connexion by Boeing, several new providers have emerged to deliver in-flight broadband to airlines—notably Row 44, OnAir and AeroMobile (who offer satellite-based solutions), and Aircell (which offers air-to-ground connectivity via a cellular signal).\n\nIn the past few years, many US commercial airlines have begun testing and deploying in-flight connectivity for their passengers, such as Alaska Airlines, American, Delta, and United. Industry expectations were that by the end of 2011, thousands of planes flying in the US will offer some form of in-flight broadband to passengers. Airlines around the world are also beginning to test in-flight-broadband offerings as well.\n\nNow, airlines provide satellite telephones integrated into their system. These are either found at strategic locations in the aircraft or integrated into the passenger remote control used for the individual in-flight entertainment. Passengers can use their credit card to make phone calls anywhere on the ground. A rate close to US$10.00/minute is usually charged regardless of where the recipient is located and a connection fee may be applied even if the recipient does not answer. These systems are usually not capable of receiving incoming calls. There are also some aircraft that allow faxes to be sent and the rate is usually the same as the call rate, but at a per page rate. Some systems also allow the transmission of SMS.\n\nMore modern systems allow passengers to call fellow passengers located in another seat by simply keying in the recipient's seat number.\n\nIFE producers have begun to introduce Intranet type systems. Virgin America's and V Australia's \"RED\" Entertainment System allows for passengers to chat amongst one another, compete against each other in the provided games, talk to the flight attendants and request, and pay for in advance, food or drinks, and have full access to the internet and email.\n\nSeveral airlines are testing in-cabin wi-fi systems. In-flight internet service is provided either through a satellite network or an air-to-ground network. In the Airbus A380 aircraft, data communication via satellite system allows passengers to connect to live Internet from the individual IFE units or their laptops via the in-flight Wi-Fi access.\n\nBoeing's cancellation of the Connexion by Boeing system in 2006 caused concerns that inflight internet would not be available on next-generation aircraft such as Qantas's fleet of Airbus A380s and Boeing Dreamliner 787s. However, Qantas announced in July 2007 that all service classes in its fleet of A380s would have wireless internet access as well as seat-back access to email and cached web browsing when the Airbuses started operations in October 2008. Certain elements were also retrofitted into existing Boeing 747-400s.\n\nSixteen major U.S. airlines now offer Wi-Fi connectivity service on their aircraft. The majority of these airlines use the service provided by Gogo Wi-Fi service. The service allows for Wi-Fi enabled devices to connect to the Internet. Delta currently has the most Wi-Fi equipped fleet with 500 aircraft that now offer in-flight Wi-Fi.\n\nAs a general rule, mobile phone use while airborne is usually not just prohibited by the carrier but also by regulatory agencies in the relevant jurisdiction (e.g. FAA and FCC in the US). However, with added technology, some carriers nonetheless allow the use of mobile phones on selected routes.\n\nEmirates became the first airline to allow mobile phones to be used during flight. Using the systems supplied by telecom company AeroMobile, Emirates launched the service commercially on 20 March 2008. \nInstalled first on an Airbus A340-300, AeroMobile is presently operating on Emirates A340, A330, and B777 aircraft.\nEmirates planned to roll out the system over their entire fleet by 2010.\n\nRyanair had previously aimed to become the first airline to enable mobile phone usage in the air, but instead ended up launching its system commercially in February 2009. The system is set up on 22 737-800 jets based at Dublin Airport and was fitted on Ryanair's 200+ fleet off 737-800 jets by 2010.\n\nOnAir offers inflight mobile connectivity to a range of airlines through its GSM network. The GSM network connects to the ground infrastructure via an Inmarsat SwiftBroadband satellite which provides consistent global coverage.\n\nMost airlines have their own brand for its in-flight entertainment system to differentiate themselves. Amongst them are:\n\n\n"}
{"id": "12086235", "url": "https://en.wikipedia.org/wiki?curid=12086235", "title": "Instituto de Medicina Molecular", "text": "Instituto de Medicina Molecular\n\nThe Instituto de Medicina Molecular (Institute of Molecular Medicine), or IMM for short, is an associated research institution of the University of Lisbon, in Lisbon, Portugal.\n\nIMM is devoted to human genome research with the aim of contributing to a better understanding of disease mechanisms, developing novel predictive tests, improving diagnostics tools, and developing new therapeutic approaches.\n\nIMM was created in November 2001, as a result from the association of 5 research centres from the University of Lisbon Medical School: the Biology and Molecular Pathology Centre (CEBIP), the Lisbon Neurosciences Centre (CNL), the Microcirculation and Vascular Pathobiology Centre (CMBV), the Gastroenterology Centre (CG), and the Nutrition and Metabolism Centre (CNB). In 2003, the Molecular Pathobiology Research Centre (CIPM) of the Portuguese Institute of Oncology Francisco Gentil (IPOFG) became an associate member of IMM.\n\nHistorically, IMM benefited from the full integration of academic researchers into the Lisbon Medical School who initiated their academic training and scientific careers at Instituto Gulbenkian de Ciência (IGC), in Oeiras, one of the first national institutions to introduce and make use of state-of-the-art cell and molecular biology techniques.\n\nThe IMM is now known as Instituto de Medicina Molecular João Lobo Antunes, to honour one of its founders and president (2001-2014), Professor João Lobo Antunes. Maria do Carmo-Fonseca is the current president of IMM, having served before as IMM Executive Director since its creation. The current Executive Director is the malaria researcher Maria Mota.\n\n"}
{"id": "16659679", "url": "https://en.wikipedia.org/wiki?curid=16659679", "title": "Kaldor's growth laws", "text": "Kaldor's growth laws\n\nKaldor's growth laws are a series of three \"laws\" relating to the causation of economic growth.\n\nLooking at the countries of the world now and through time Nicholas Kaldor noted a high correlation between living standards and the share of resources devoted to industrial activity, at least up to some level of income. Only New Zealand, Australia and Canada have become rich whilst relying mainly on agriculture. He proposed three laws on these empirical regularities:\n\nThirlwall (2003, p123–124) also reports Kaldor's highlighting of three subsidiary propositions which are also important to take into account. They are:\n"}
{"id": "3041515", "url": "https://en.wikipedia.org/wiki?curid=3041515", "title": "Kick scooter", "text": "Kick scooter\n\nA kick scooter, push scooter or scooter is a human-powered land vehicle with a handlebar, deck and wheels that is propelled by a rider pushing off the ground. The most common scooters today are made of aluminum, titanium and steel. Some kick scooters that are made for younger children have 3 or 4 wheels and are made of plastic or do not fold. High-performance racing scooters made for adults resemble the old \"penny-farthing\".\n\nMotorized scooters, historically powered by gas engines, and more recently electric motors, are self-propelled kick scooters capable of speeds of around .\n\nKick scooters have been handmade in industrial urban areas in Europe and the U.S. for at least 100 years, often as play items made for children to roam the streets. One common home-made version is made by attaching roller skate wheel sets to a board with some kind of handle, usually an old box. One can lean to turn, or by a second board connected by a crude pivot. The construction was all wood, with 3–4 inch (75–100 mm) wheels with steel ball bearings. An additional advantage of this construction was loud noise, just like from a \"real\" vehicle. An alternative construction consists of one steel clamp–on roller skate divided into front and rear parts and attached to a wood beam.\n\nA search of the German \"Bundesarchiv\" for \"roller\" reveals both homemade and manufactured childrens' scooters used and even raced in Paris, Berlin and Leipzig in 1930, 1948 and 1951. They are similar to modern designs.\n\nIn 1974, the Honda company made the Kick 'n Go, a scooter driven by a pedal on a lever. While it seemed to be as much effort to \"kick\" as a regular scooter, the novelty of it caught on and it became popular nevertheless.\n\nBefore bicycles became popular among children, steel scooters with two small bicycle wheels had been the most useful vehicles for them. Around 1987, many BMX manufacturers produced BMX-like scooters as Scoot. Those manufacturers discontinued their scooters, but some scooter manufacturers were established after years, and still develop similar scooters today; Some are used in dense urban areas for utility purposes, being faster than a folding scooter and more convenient than a utility bicycle. Some are made for off-road use and are described as Mountain Scooters. Besides commuting, sports competition and off-road use, large wheel scooters are a favorite for dog scootering where single or team dogs such as huskies pull a scooter and rider in the same way that a sled is pulled across snow. Some Amish are not allowed to ride bicycles, so they ride scooters instead. Today variations on the kicksled with scooter design features are also available, such as the Kickspark.\n\nThe development of the kickbike in Finland in 1994 changed the way scooters are viewed. The Kickbike has a large standard size bicycle front wheel and a much smaller rear wheel, allowing a much faster ride. The Footbike Eurocup has been held since 2001. Kickbike America is bringing the sport of dryland mushing to America. The Kickbike is regarded as the leading dog scooter in the world.\n\nIn 1996, a foldable aluminium scooter with inline skates wheels was created by Wim Ouboter of Micro Mobility Systems in Switzerland. The scooter was sold as \"Micro Skate Scooter\", \"Razor\" and \"JDBUG/JDRAZOR MS-130A\". After the Razor was introduced to Japan in 1999, many young people in Tokyo began to use it as a portable transporter then it became a fad around the world, and such small scooters also became popular toys for children.\n\nKick scooters used for extreme sport stunts and tricks, and made to withstand these kinds of stresses, are called pro scooters. Numerous brands specialize in stunt scooters and accessories such as helmets, pegs, grind wax, griptape clamps, and clothing.\n\nFolding kick scooters optimized for adults generally have more durable parts and are designed with wider decks, hand brake, and larger wheels, for smoother transportation instead of less weight and portability. The Xootr Street, which incorporates 180 mm (7.1 in) wheels with a maximum load of 300 lbs (136 kg). Go-Ped Know-Ped scooters have 6 inch wheels with solid-rubber tires with a maximum load of 400 lbs (181 kg). and its variant KickPed from NYCeWheels which is stripped of all sensitive parts, such as a handbrake which is replaced with a rear spoon brake in order to make the kick scooter long-lasting and durable.\n\nThree-wheeled scooters like tricycles have been produced for little children.\n\nIn 1999, Micro Mobility Systems and K2 Sports produced a reverse-three-wheeled scooter as \"Kickboard\". Micro also produced the Kickboard-like children's scooters as \"Mini Micro\" and \"Maxi Micro\". The reverse design inherently provides greater stability than the standard: a standing person will tend to stand at the front of a scooter rather than at the back. However, the steering geometry is inherently weak and requires design adaptation to improve its response. An example is the Mini Micro, which uses a spring-loaded system to translate lateral force on the handbars (child leaning) into turning motion on the wheels, referred by its makers as \"lean and steer\".\n\nThe early scooters, which were made with roller skates, were four-wheeled like skateboards.\n\nAround 2000, A Swiss company produced a four-wheeled scooter as \"Wetzer Stickboard\". The Stickboard was a narrow skateboard with a foldable pole on the nose.\n\nIn 2006, a company called Nextsport started producing a line of four-wheeled scooters, known as Fuzions. Fuzion scooters are typically bigger and heavier than Razor and Micro models. The early Fuzion models come with large, wide wheels, and an oversized deck for carving stability. Later scooters, such as the Fuzion NX, include smaller, harder wheels, and 360 degree handlebar spinning capabilities, unlike its predecessors. \n\nUnlike a kick scooter, a bicycle has a seat and drive train, which add speed, cost, weight and bulk. A folding scooter can be more easily carried than a folding bicycle or even a portable bicycle. Even a non-folding scooter is easier to manoeuvre between obstacles, as there are no protruding pedals. Thus a cyclist has advantages in longer journeys and open spaces, and a kick scooter in shorter and more crowded ones. Kick scooters seldom have a luggage rack, so the rider usually carries any cargo on their back.\n\nAt low speeds a bicycle is difficult to control while pedalling, which is why cyclists occasionally kick their way through dense traffic or in other conditions where they cannot take advantage of the speed of their machine. Thanks to the superior low-speed stability of a kick scooter, it is allowed on many footpaths where riding a bicycle is forbidden.\n\nSince the feet are nearer the ground on a scooter, it is easier to step on and off than even a step-through frame bicycle, hence the rider can alternate walking and pushing as energy and route dictate. Large wheel scooters afford a more effective cross training workout than standard bicycles as the whole body is engaged in the effort of kicking. Although the bicycle is a much more effective and efficient long distance machine, in 2001 Jim Delzer propelled a kick scooter across the United States.\n\n"}
{"id": "8941842", "url": "https://en.wikipedia.org/wiki?curid=8941842", "title": "Low-carbon economy", "text": "Low-carbon economy\n\nA low-carbon economy (LCE), low-fossil-fuel economy (LFFE), or decarbonised economy is an economy based on low carbon power sources that therefore has a minimal output of greenhouse gas (GHG) emissions into the biosphere, but specifically refers to the greenhouse gas carbon dioxide. GHG emissions due to anthropogenic (human) activity are the dominant cause of observed global warming (climate change) since the mid-20th century. Continued emission of greenhouse gases may cause further warming and long-lasting changes around the world, increasing the likelihood of severe, pervasive and irreversible impacts for people and ecosystems. \n\nShifting to low-carbon economy on a global scale could bring substantial benefits both for developed and developing countries. Many countries around the world are designing and implementing low emission development strategies (LEDS). These strategies seek to achieve social, economic and environmental development goals while reducing long-term greenhouse gas emissions and increasing resilience to climate change impacts.\n\nGlobally implemented low-carbon economies are therefore proposed by those having drawn this conclusion, as a means to avoid catastrophic climate change, and as a precursor to the more advanced, zero-carbon economy.\n\nNations may seek to become low-carbon or decarbonised economies as a part of a national climate change mitigation strategy. A comprehensive strategy to mitigate climate change is through carbon neutrality.\n\nThe aim of a LCE is to integrate all aspects of itself from its manufacturing, agriculture, transportation, and power-generation, etc. around technologies that produce energy and materials with little GHG emission, and, thus, around populations, buildings, machines, and devices that use those energies and materials efficiently, and, dispose of or recycle its wastes so as to have a minimal output of GHGs. Furthermore, it has been proposed that to make the transition to an LCE economically viable we would have to attribute a cost (per unit output) to GHGs through means such as emissions trading and/or a carbon tax.\n\nSome nations are presently low carbon: societies that are not heavily industrialised or populated. In order to avoid climate change on a global level, all nations considered carbon intensive societies, and societies that are heavily populated might have to become zero-carbon societies and economies. Several of these countries have pledged to cut their emissions by 100% via offsetting emissions rather than ceasing all emissions (carbon neutrality); in other words, emitting will not cease but will continue and will be \"offset\" to a different geographical area. EU emission trading system allows companies to buy international carbon credits, thus the companies can channel clean technologies to promote other countries to adopt low-carbon developments.\n\nLow-carbon economies present multiple benefits to ecosystem resilience, trade, employment, health, energy security, and industrial competitiveness.\n\nLow emission development strategies for the land use sector can prioritize the protection of carbon rich ecosystems to not only reduce emissions, but also to protect biodiversity and safeguard local livelihoods to reduce rural poverty - all of which can lead to more climate resilient systems, according to a report by the Low Emission Development Strategies Global Partnership (LEDS GP). REDD+ and blue carbon initiatives are among the measures available to conserve, sustainably manage, and restore these carbon rich ecosystems, which are crucial for natural carbon storage and sequestration, and for building climate resilient communities.\n\nTransitioning to a low-carbon, environmentally and socially sustainable economies can become a strong driver of job creation, job upgrading, social justice, and poverty eradication if properly managed with the full engagement of governments, workers, and employers’ organizations.\n\nEstimates from the International Labour Organization’s Global Economic Linkages model suggest that unmitigated climate change, with associated negative impacts on enterprises and workers, will have negative effects on output in many industries, with drops in output of 2.4% by 2030 and 7.2% by 2050.\n\nTransitioning to a low-carbon economy will cause shifts in the volume, composition, and quality of employment across sectors and will affect the level and distribution of income. Research indicates that eight sectors employing around 1.5 billion workers, approximately half the global workforce, will undergo major changes: agriculture, forestry, fishing, energy, resource intensive manufacturing, recycling, buildings, and transport.\n\nLow emission industrial development and resource efficiency can offer many opportunities to increase the competitiveness of economies and companies. According to the Low Emission Development Strategies Global Partnership (LEDS GP), there is often a clear business case for switching to lower emission technologies, with payback periods ranging largely from 0.5–5 years, leveraging financial investment.\n\nTrade and trade policies can contribute to low-carbon economies by enabling more efficient use of resources and international exchange of climate friendly goods and services. Removing tariffs and nontariff barriers to trade in clean energy and energy efficiency technologies is one such measure. In a sector where finished products consist of many components that cross borders numerous times - a typical wind turbine, for example, contains up to 8,000 components - even small tariff cuts would reduce costs. This would make the technologies more affordable and competitive in the global market, particularly when combined with a phasing out of fossil fuel subsidies.\n\nRecent advances in technology and policy will allow renewable energy and energy efficiency to play major roles in displacing fossil fuels, meeting global energy demand while reducing carbon dioxide emissions. Renewable energy technologies are being rapidly commercialized and, in conjunction with efficiency gains, can achieve far greater emissions reductions than either could independently.\n\nRenewable energy is energy that comes from natural resources such as sunlight, wind, rain, tides, and geothermal heat, which are renewable (naturally replenished). In 2008, about 19% of global final energy consumption came from renewables. During the five years from the end of 2004 through 2009, worldwide renewable energy capacity grew at rates of 10–60 percent annually for many technologies. For wind power and many other renewable technologies, growth accelerated in 2009 relative to the previous four years. More wind power capacity was added during 2009 than any other renewable technology. However, grid-connected photovoltaics increased the fastest of all renewables technologies, with a 60 percent annual average growth rate for the five-year period.\n\nEnergy for power, heat, cooling, and mobility is the key ingredient for development and growth, with energy security a prerequisite economic growth, making it arguably the most important driver for energy policy. Scaling up renewable energy as part of a low emission development strategy can diversify a country's energy mixes and reduces dependence on imports. It can also lower geopolitical risks and exposure to fuel price volatility, and improve the balance of trade for importing countries (noting that only a handful of countries export oil and gas). Renewable energy offers lower financial and economic risk for businesses through a more stable and predictable cost base for energy supply.\n\nEnergy efficiency gains in recent decades have been significant, but there is still much more that can be achieved. With a concerted effort and strong policies in place, future energy efficiency improvements are likely to be very large. Heat is one of many forms of \"energy wastage\" that could be captured to significantly increase useful energy without burning more fossil fuels.\n\nBiofuels, in the form of liquid fuels derived from plant materials, are entering the market, driven by factors such as oil price spikes and the need for increased energy security. However, many of the biofuels that are currently being supplied have been criticised for their adverse impacts on the natural environment, food security, and land use.\n\nThe challenge is to support biofuel development, including the development of new cellulosic technologies, with responsible policies and economic instruments to help ensure that biofuel commercialization is sustainable. Responsible commercialization of biofuels represents an opportunity to enhance sustainable economic prospects in Africa, Latin America and Asia.\n\nBiofuels have a limited ability to replace fossil fuels and should not be regarded as a ‘silver bullet’ to deal with transport emissions. However, they offer the prospect of increased market competition and oil price moderation. A healthy supply of alternative energy sources will help to combat gasoline price spikes and reduce dependency on fossil fuels, especially in the transport sector. Using transportation fuels more efficiently is also an integral part of a sustainable transport strategy.\n\nNuclear power has been offered as the primary means to achieve a LCE. In terms of large industrialized nations, mainland France, due primarily to 75% of its electricity being produced by nuclear power, has the lowest carbon dioxide production per unit of GDP in the world and it is the largest exporter of electricity in the world, earning it approximately €3 billion annually in sales.\n\nConcern is often expressed with the matter of spent nuclear fuel storage and security; although the physical issues are not large, the political difficulties are significant. The liquid fluoride thorium reactor (LFTR) has been suggested as a solution to the concerns posed by conventional nuclear.\n\nFrance reprocesses their spent nuclear fuel at the La Hague site since 1976 and has also treated spent nuclear fuel from France, Japan, Germany, Belgium, Switzerland, Italy, Spain and the Netherlands.\n\nOne proposal from Karlsruhe University developed as a virtual power station is the use of solar and wind energy for base load with hydro and biogas for make up or peak load. Hydro and biogas are used as grid energy storage. This requires the development of a smart intelligent grid hopefully including local power networks than use energy near the site of production, thereby reducing the existing 5% grid loss.\n\nA further development of this is the use of the carbon capture, hydrogen and its conversion into methane (SNG synthetic natural gas) to act as a storage for intermittent renewables.\n\nCO + 4H → CH + 2HO Sabatier reaction\n\nThis involves the use of the existing natural gas (methane) grid as the store. In this case, the carbon dioxide is given economic value as a component of energy carrier. This \"solar fuel\" cycle uses the excess electrical renewable energy that cannot be used instantaneously in the grid, which otherwise would be wasted to create hydrogen via electrolysis of water. The hydrogen is then combined with CO to create synthetic or substitute natural gas SNG and stored in the natural gas network. The natural gas is used to create electrical energy (and the heat used as well in CHP) on demand when there is not enough sun (photovoltaic, CSP...) or wind (turbines) or water (hydro, ocean current, waves...). The German natural gas grid, for example, has two months of storage, more than enough to outlast renewable energy low production points.\n\nThe concentration of CO in the upper layer of the world's oceans is higher than is found in air, and thus it is the most concentrated \"mine\" from which zero-net carbon fuels can be produced. The U.S. Navy estimates that a typical nuclear propelled aircraft carrier which generates 100 megawatts of electricity can produce 41,000 US gallons(155,202 litres) of jet fuel per day and production from the onboard nuclear reactor would cost about $6 per gallon($1.58 per liter). While that was about twice the petroleum fuel cost in 2010, it is expected to be much less than the market price in less than five years if recent trends continue. Moreover, since the delivery of fuel to a carrier battle group costs about $8 per gallon, shipboard production is already much less expensive. Heather Willauer of the United States Naval Research Laboratory proof-tested the technology in 2013, fueling an internal combustion engine equipped model airplane with the synthetic fuel.\n\nThe proposed strategy of carbon capture and storage (CCS) - continued use of non-renewable fossil fuels but without allowing carbon dioxide to reach the atmosphere - has also been considered as a means to achieve a LCE, either in a primary or supporting role. Major concerns include the uncertainty of costs and time needed to successfully implement CCS worldwide and with guarantees that stored emissions will not leak into the biosphere.\n\nCombined Heat and Power (CHP) is a technology which by allowing the more efficient use of fuel will at least reduce carbon emissions; should the fuel be biomass or biogas or hydrogen used as an energy store then in principle it can be a zero carbon option. CHP can also be used with a nuclear reactor as the energy source; there are examples of such installations in the far North of the Russian Federation.\n\nMost of the agricultural facilities in the developed world are mechanized due to rural electrification. Rural electrification has produced significant productivity gains, but it also uses a lot of energy. For this and other reasons (such as transport costs) in a low-carbon society, rural areas would need available supplies of renewably produced electricity.\n\nIrrigation can be one of the main components of an agricultural facility's energy consumption. In parts of California, it can be up to 90%. In the low carbon economy, irrigation equipment will be maintained and continuously updated and farms will use less irrigation water.\n\nDifferent crops require different amounts of energy input. For example, glasshouse crops, irrigated crops, and orchards require a lot of energy to maintain, while row crops and field crops do not need as much maintenance. Those glasshouse and irrigated crops that do exist will incorporate the following improvements:\n\nGlasshouse crops\nIrrigated arable crops\n\nLivestock operations can also use a lot of energy depending on how they are run. Feed lots use animal feed made from corn, soybeans, and other crops. Energy must be expended to produce these crops, process, and transport them. Free-range animals find their own vegetation to feed on. The farmer may expend energy to take care of that vegetation, but not nearly as much as the farmer growing cereal and oil-seed crops.\n\nMany livestock operations currently use a lot of energy to water their livestock. In the low-carbon economy, such operations will use more water conservation methods such as rainwater collection, water cisterns, etc., and they will also pump/distribute that water with on-site renewable energy sources (most likely wind and solar).\n\nDue to rural electrification, most agricultural facilities in the developed world use a lot of electricity. In a low-carbon economy, farms will be run and equipped to allow for greater energy efficiency. The dairy industry, for example, will incorporate the following changes:\n\nIrrigated Dairy\n\nFishing is quite energy intensive. Improvements such as heat recovery on refrigeration and trawl net technology will be common in the low-carbon economy.\n\nProtecting forests provides integrated benefits to all, ranging from increased food production, safeguarded local livelihoods, protected biodiversity and ecosystems provided by forests, and reduced rural poverty. Adopting low emission strategies for both agricultural and forest production also mitigates some of the effects of climate change.\n\nIn the low-carbon economy, forestry operations will be focused on low-impact practices and regrowth. Forest managers will make sure that they do not disturb soil-based carbon reserves too much. Specialized tree farms will be the main source of material for many products. Quick maturing tree varieties will be grown on short rotations in order to maximize output.\n\nFlaring and venting of natural gas in oil wells is a significant source of greenhouse gas emissions. Its contribution to greenhouse gases has declined by three-quarters in absolute terms since a peak in the 1970s of approximately 110 million metric tons/year, and in 2004 accounted for about 1/2 of one percent of all anthropogenic carbon dioxide emissions.\n\nThe World Bank estimates that 134 billion cubic meters of natural gas are flared or vented annually (2010 datum), an amount equivalent to the combined annual gas consumption of Germany and France or enough to supply the entire world with gas for 16 days.\nThis flaring is highly concentrated: 10 countries account for 70% of emissions, and twenty for 85%.\n\nThe top-ten leading contributors to world gas flaring in 2010, were (in declining order): Russia (26%), Nigeria (11%), Iran (8%), Iraq (7%), Algeria (4%), Angola (3%), Kazakhstan (3%), Libya (3%), Saudi Arabia (3%), and Venezuela (2%).\n\n\n\n\n\n\nRetail operations in the low-carbon economy will have several new features. One will be high-efficiency lighting such as compact fluorescent, halogen, and eventually LED light sources. Many retail stores will also feature roof-top solar panel arrays. These make sense because solar panels produce the most energy during the daytime and during the summer. These are the same times that electricity is the most expensive and also the same times that stores use the most electricity.\n\nSustainable, low-carbon transport systems are based on minimizing travel and shifting to more environmentally (as well as socially and economically) sustainable mobility, improving transport technologies, fuels and institutions. Decarbonisation of (urban) mobility by means of:\n\nSustainable transport has many co-benefits that can accelerate local sustainable development. According to a series of reports by the Low Emission Development Strategies Global Partnership (LEDS GP), low carbon transport can help create jobs, improve commuter safety through investment in bicycle lanes and pedestrian pathways, make access to employment and social opportunities more affordable and efficient. It also offers a practical opportunity to save people’s time and household income as well as government budgets, making investment in sustainable transport a 'win-win' opportunity.\n\nThere have been some moves to investigate the ways and extent to which health systems contribute to greenhouse gas emissions and how they may need to change to become part of a low-carbon world. The Sustainable Development Unit of the NHS in the UK is one of the first official bodies to have been set up in this area, whilst organisations such as the Campaign for Greener Healthcare are also producing influential changes at a clinical level. This work includes\nSome of the suggested changes needed are:\n\nLow-carbon tourism includes travels with low energy consumption, and low CO and pollution emissions. Change of personal behavior to more low-carbon oriented activities is mostly influenced by both individual awareness and attitudes, as well as external social aspect, such as culture and environment. Studies indicate that educational level and occupation influence an individual perception of low-carbon tourism.\n\nA good overview of the history of international efforts towards a low-carbon economy, from its initial seed at the inaugural UN Conference on the Human Environment in Stockholm in 1972, has been given by David Runnals.\nOn the international scene, the most prominent early step in the direction of a low-carbon economy was the signing of the Kyoto Protocol, which came into force on February 16, 2005, under which most industrialized countries committed to reduce their carbon emissions. Importantly, all member nations of the Organisation for Economic Co-operation and Development except the United States have ratified the protocol.\nEurope is the leading geopolitical continent in defining and mobilising decarbonisation policies. For instance, the UITP - an organisation advocating sustainable mobility and public transport - has an EU office, but less well developed contacts with, for example, the US. The European Union Committee of the UITP wants to promote decarbonisation of urban mobility in Europe. Although Europe is nowadays the leading geopolitical continent with regard to lowering emissions, Europe is quickly losing ground to Asia, with countries such as China and South Korea. However, the 2014 \"Global Green Economy Index™ (GGEI)\" ranks 60 nations on their green economic performance, finding that the Nordic countries and Switzerland have the best combined performance around climate change and green economy.\n\nAustralia has implemented schemes to start the transition to a low-carbon economy but carbon neutrality has not been mentioned and since the introduction of the schemes, emissions have increased. The Second Rudd Government pledged to lower emissions by 5-15%. In 2001, The Howard Government introduced a Mandatory Renewable Energy Target (MRET) scheme. In 2007, the Government revised the MRET - 20 percent of Australia's electricity supply to come from renewable energy sources by 2020. Renewable energy sources provide 8-10% of the nation's energy, and this figure will increase significantly in the coming years. However coal dependence and exporting conflicts with the concept of Australia as a low-carbon economy. Carbon-neutral businesses have received no incentive; they have voluntarily done so. Carbon-offset companies offer assessments based on lifecycle impacts to businesses that seek carbon neutrality. In Australia the only true certified carbon neutral scheme is the Australian government's National Carbon Offset Standard (NCOS) which includes a mandatory independent audit. Three of the four of Australia's top banks are now certified under this scheme and full list of compliant companies can be seen here http://www.environment.gov.au/climate-change/carbon-neutral/carbon-neutral-program/accredited-businesses#Certified_organisations . Businesses are now moving from unaccredited schemes such as noco2 and transitioning to NCOS as the only one that is externally audited. Most of leading carbon management companies have also aligned with NCOS such as Net Balance https://web.archive.org/web/20140819125415/http://www.netbalance.com/ , Pangolin Associates (who themselves are independently certified under NCOS) http://pangolinassociates.com/sustainability-services/ncos-carbon-neutrality/, Energetics http://energetics.com.au/home and the big four accounting firms.\n\nIn 2011 the Gillard Government introduced a price on carbon dioxide emissions for businesses. Although often characterised as a tax, it lacked the revenue-raising nature of a true tax. In 2013, on the election of the Abbott government, immediate legislative steps were taken to repeal the so-called carbon tax. The price on carbon was repealed on the 17th July 2014 by an act of parliament. As it stands Australia currently has no mechanism to deal with climate change.\n\nIn China, the city of Dongtan is to be built to produce zero net greenhouse gas emissions.\n\nThe Chinese State Council announced in 2009 it aimed to cut China's carbon dioxide emissions per unit of GDP by 40%-45% in 2020 from 2005 levels. However carbon dioxide emissions were still increasing by 10% a year by 2013 and China was emitting more carbon dioxide than the next two biggest countries combined (U.S.A. and India). Total carbon dioxide emissions were projected to increase until 2030.\n\nCosta Rica sources much of its energy needs from renewables and is undertaking reforestation projects. In 2007, the Costa Rican government announced the commitment for Costa Rica to become the first carbon neutral country by 2021.\n\nIceland began utilising renewable energy early in the 20th century and so since has been a low-carbon economy. However, since dramatic economic growth, Iceland's emissions have increased significantly per capita. As of 2009, Iceland energy is sourced from mostly geothermal energy and hydropower, renewable energy in Iceland and, since 1999, has provided over 70% of the nation's primary energy and 99.9% of Iceland's electricity. As a result of this, Iceland's carbon emissions per capita are 62% lower than those of the United States despite using more primary energy per capita, due to the fact that it is renewable and low-cost. Iceland seeks carbon neutrality and expects to use 100% renewable energy by 2050 by generating hydrogen fuel from renewable energy sources.\n\nLow carbon strategies for inclusive growth - An interim report (India), May 2011\n\nThe Economic Commission for Latin America and the Caribbean (ECLAC) estimates that economic losses related to climate change for Peru could reach over 15% of national gross domestic product (GDP) by 2100. Being a large country with a long coastline, snow-capped mountains and sizeable forests, Peru's varying ecosystems are extremely vulnerable to climate change. Several mountain glaciers have already begun to retreat, leading to water scarcity in some areas. In the period between 1990 and 2015, Peru experienced a 99% increase in per capita carbon emissions from fossil fuel and cement production, marking one of the largest increases amongst South American countries.\n\nPeru brought in a National Strategy on Climate Change in 2003. It is a detailed accounting of 11 strategic focuses that prioritize scientific research, mitigation of climate change effects on the poor, and creating Clean Development Mechanism (CDM) mitigation and adaptation policies.\n\nIn 2010, the Peruvian Ministry of Environment published a Plan of Action for Adaptation and Mitigation of Climate Change. The Plan categorises existing and future programmes into seven action groups, including: reporting mechanisms on GHG emissions, mitigation, adaptation, research and development of technology of systems, financing and management, and public education. It also contains detailed budget information and analysis relating to climate change.\n\nIn 2014, Peru hosted the Twentieth Conference of the Parties of the United Nations Framework Convention on Climate Change (UNFCCC COP20) negotiations. At the same time, Peru enacted a new climate law which provides for the creation of a national greenhouse gas inventory system called INFOCARBONO. According to the Low Emission Development Strategies Global Partnership (LEDS GP), INFOCARBONO is a major transformation of the country's greenhouse gas management system. Previously, the system was under the sole control of the Peruvian Ministry of the Environment. The new framework makes each relevant ministry responsible for their own share of greenhouse gas management.\n\nIn the United Kingdom, the Climate Change Act 2008 outlining a framework for the transition to a low-carbon economy became law on November 26, 2008. This legislation requires an 80% cut in the UK's carbon emissions by 2050 (compared to 1990 levels), with an intermediate target of between 26% and 32% by 2020. Thus, the UK became the first country to set such a long-range and significant carbon reduction target into law.\n\nA meeting at the Royal Society on 17–18 November 2008 concluded that an integrated approach, making best use of all available technologies, is required to move toward a low-carbon future. It was suggested by participants that it would be possible to move to a low-carbon economy within a few decades, but that 'urgent and sustained action is needed on several fronts'.\n\nIn June 2012, the UK coalition government announced the introduction of mandatory carbon reporting, requiring around 1,100 of the UK’s largest listed companies to report their greenhouse gas emissions every year. Deputy Prime Minister Nick Clegg confirmed that emission reporting rules would come into effect from April 2013 in his piece for The Guardian.\n\nIn July 2014, the UK Energy Savings Opportunity Scheme (ESOS) came into force. This requires all large businesses in the UK to undertake mandatory assessments looking at energy use and energy efficiency opportunities at least once every four years.\n\nThe low carbon economy has been described as a \"UK success story\", accounting for more than £120 billion in annual sales and employing almost 1 million people. A 2013 report suggests that over a third of the UK's economic growth in 2011/12 was likely to have come from green business.\n\nCompanies are planning large scale developments without using fossil fuels. Development plans such as those by World Wide Assets LLC for entire cities using only geothermal energy for electricity, geothermal desalination, and employing full recycling systems for water and waste are under development (2006) in Mexico and Australia.\n\nThe University of Reading has a Renewable Energy inc. a carbon management module MSc\n\nThe University of Edinburgh has both an on-campus Carbon Management MSc and an online Masters in Carbon Management. As well as a Carbon Finance MSc.\n\nThe University of East Anglia has a Strategic Carbon Management MBA.\n\nThe myclimate climate education offers capacity building tools like exhibitions, games, schoolbooks and courses for young people, adults and businesses.\n\nThe London School of Business and Finance has an MBA specialisation in Carbon Management.\n"}
{"id": "1512695", "url": "https://en.wikipedia.org/wiki?curid=1512695", "title": "Malemute", "text": "Malemute\n\nMalemute is the designation of an American sounding rocket. The Malemute has a maximum flight altitude of 165 km, a liftoff thrust of 57.00 kN, a total mass of 100 kg, a diameter of 0.41 m and a total length of 2.40 m.\n\n"}
{"id": "7153781", "url": "https://en.wikipedia.org/wiki?curid=7153781", "title": "Michael Coey", "text": "Michael Coey\n\nJohn Michael David Coey (born 24 February 1945), known as Michael Coey, is a Belfast-born experimental physicist working in the fields of magnetism and spintronics.\nAfter Tonbridge Mike Coey developed his interest in teaching in India before reading Physics at Jesus College, Cambridge, he subsequently gained a PhD from University of Manitoba, and Dip. d'Habilitation from University of Grenoble & ScD from Trinity College, Dublin.\n\nMike Coey has been a Professor of Physics at Trinity College, Dublin, for over 25 years, where he is currently Professor Emeritus Erasmus Smith's Professor of Natural and Experimental Philosophy (2007–2012), a chair that dates from 1724. Recognised as a distinguished European specialist in magnetic materials; internationally he continues to be a leader in the field of magnetism.\n\nIn 1994 Mike Coey founded Magnetic Solutions and went on to be the cofounder of CRANN Ireland's Nanoscience Research institute (2002) and conceived Dublin's unique Science Gallery (2006). He has published over 700 scientific articles on diverse aspects of magnetism, many of which have had significant impact on the scientific community. As Ireland's most highly cited scientist, with an h-index of 66 Mike Coey continues to make an impact at both the cutting edge of his chosen areas of specialisation and to the wider scientific community. His recent textbook \"Magnetism and Magnetic Materials\" has met the need for a general, tangible text about modern magnetism.\n\nMike Coey is a member of the Royal Irish Academy (1987), a Fellow of the Royal Society (2003) and a Foreign Associate of the US National Academy of Sciences (2005). He is also a fellow of the Institute of Physics, the American Mineralogical Society and the American Physical Society. Mike's numerous awards include a Fulbright Fellowship, the Charles Cree Medal of the Institute of Physics (1997), the Gold Medal of the Royal Irish Academy (2005) the RDS INTEL Prize Lecture on Nanoscience (2012) in addition to being the recipient of the Humboldt (2013) & Gutenberg (2015) prizes.\n\nMike Coey has an honorary doctorate from the Institute National Polytechnique Grenoble and has been a Distinguished Lecturer, IEEE Magnetics Study (2006) and the Albert Einstein Professor of the Chinese Academy of Sciences (2010). He delivered a public lecture on the History of Magnetism in Paris in 2010. Currently Mike holds positions at National University Singapore and the Max Planck Institute for Chemical Physics of Solids. His belief in advancement through collaboration demonstrated through postings as a visiting scientist/professor that include: IBM Yorktown Heights (1979), Institute of Physics Peking (1980), McGill University (1982), University of Bordeaux (1984), CEN-Grenoble (1985), Johns Hopkins APL (1986), Universite de Paris IV (1992), University of California, San Diego (1997), Florida State University (1998), University of Paris XI (1998), Leman University Geneva (2001/3), University of Strasbourg (2006). Mike Coey pioneered co-operation between academic and industrial laboratories in the groundbreaking Concerted European Action on Magnets (1985–95). Throughout his career he has strongly identified himself with the European spirit and tradition of collaboration.\n\nMike has been married to Wong May Coey for 40 years and has two sons, James and Dominic.\n"}
{"id": "52716", "url": "https://en.wikipedia.org/wiki?curid=52716", "title": "Microsoft SharePoint Workspace", "text": "Microsoft SharePoint Workspace\n\nMicrosoft SharePoint Workspace, previously known as Microsoft Office Groove, is a discontinued desktop application designed for document collaboration in teams with members who are regularly off-line or who do not share the same network security clearance. It is no longer included with Microsoft Office 2013. It has been replaced by a web-based service called SharePoint.\n\nGroove's uses have included coordination between emergency relief agencies where different organizations do not share a common security infrastructure and where offline access is important, and amongst teams of knowledge workers, such as consultants who need to work securely on client sites.\nIt is also used as a staging system for documents in development, where content can be developed then transferred to a portal when complete.\n\nGroove was initially developed by Lotus Notes creator Ray Ozzie, and developed by Groove Networks of Beverly, Massachusetts, until Microsoft's acquisition of Groove Networks in March 2005.\n\nGroove's basic set of services (including always-on security, persistent chat, store-and-forward messaging delivery, firewall/NAT transparency, \"ad-hoc\" group formation, and change notification) may be customized with tools.\n\n\"Tools\" are mini-applications that rely on Groove's underlying functionality to disseminate and synchronize their contents with other members' copies of the workspace. Groove provides various tools that can be added to (and removed from) a workspace to customize the functionality of each space (for example a calendar, discussion, file sharing, an outliner, pictures, notepad, sketchpad, web browser, etc.).\nTools that members use in a workspace often drive the nature of the person-to-person collaboration that ensues. In Groove 2007, the SharePoint Files tools can be used to take Sharepoint 2007 document libraries offline.\n\nGroove 2007 includes a presence subsystem, which keeps track of which users in the contact store are online, and presents the information in the launchbar. If Groove server is used, a user is considered online when they log on to the server. In absence of a server the \"Device Presence Protocol\" (which comes in different variants for LANs and WANs) is used. Groove also allows sending instant messages to peers. All session and user information is stored by the Groove client at client side.\n\nGroove Virtual Office 3.1 was the last version before Microsoft's acquisition of Groove Networks. The following versions have been released since:\n\nMicrosoft claims the name change is a natural progression since Groove is to SharePoint what Outlook is to Exchange. Microsoft asserts that features have been added to make it easier to deploy and manage, and claims that SharePoint Workspace will make it easier to access SharePoint content (or content from any server that implements the publicly documented protocols).\n\nMicrosoft Groove Server is a tool for centrally managing all deployments of Microsoft SharePoint Workspace in an enterprise. It enables using Active Directory for Groove user accounts, and create Groove Domains, with individual policy settings.\n\n"}
{"id": "42659851", "url": "https://en.wikipedia.org/wiki?curid=42659851", "title": "Mike Boich", "text": "Mike Boich\n\nMike Boich was a major figure at Apple Computer who was in charge of demonstrating the first Macintosh to software developers and potential customers. He is notable as a technology evangelist who persuaded developers to write computer software. He was instrumental in hiring Apple entrepreneur Guy Kawasaki. His name is listed — as credited — inside the original Macintosh 128k.\n"}
{"id": "48005845", "url": "https://en.wikipedia.org/wiki?curid=48005845", "title": "OMEMO", "text": "OMEMO\n\nOMEMO is an extension to the Extensible Messaging and Presence Protocol (XMPP, \"Jabber\") for multi-client end-to-end encryption developed by Andreas Straub. According to Straub, OMEMO uses the Double Ratchet Algorithm \"to provide multi-end to multi-end encryption, allowing messages to be synchronized securely across multiple clients, even if some of them are offline\". The name \"OMEMO\" is a recursive acronym for \"OMEMO Multi-End Message and Object Encryption\".\nIt is an open standard based on the Double Ratchet Algorithm and the Personal Eventing Protocol (PEP, XEP-0163).\nOMEMO offers future and forward secrecy and deniability with message synchronization and offline delivery.\n\nThe protocol was developed and first implemented by Andreas Straub as a Google Summer of Code project in 2015. The project's goal was to implement a double-ratchet-based multi-end to multi-end encryption scheme into an Android XMPP-based instant messaging client called Conversations.\nIt was introduced in Conversations and submitted to the XMPP Standards Foundation (XSF) as a proposed XMPP Extension Protocol (XEP) in the autumn of 2015 and got accepted as XEP-0384 in December 2016.\n\nIn July 2016, the ChatSecure project announced that they would implement OMEMO in the next releases. ChatSecure v4.0 supports OMEMO and was released on January 17, 2017.\n\nA first experimental release of an OMEMO plugin for the cross-platform XMPP client Gajim was made available on December 26, 2015.\n\nIn June 2016, the non-profit computer security consultancy firm Radically Open Security published an analysis of the OMEMO protocol.\n\n\n\n"}
{"id": "3851478", "url": "https://en.wikipedia.org/wiki?curid=3851478", "title": "Optical IP Switching", "text": "Optical IP Switching\n\nOptical IP Switching (OIS), is a novel method of creating transparent optical connections between network nodes using a flow-based approach. \nAn IP flow is a collection of IP packets going from the same source to the same destination: the exchange of IP packets is the mechanism that allows the transport of information over the Internet.\n\nRecent studies have shown that Internet traffic presents a heavy tail distribution, where a small number of flows carries a huge amount of data. This suggests the possibility of dynamically adapting the optical connections to carry these heavy flows.\n\nCurrently a packet has to traverse a certain number of routers, before reaching its destination and the network routers must analyze each packet and forward it towards the direction of the destination node. \nHowever, since a flow is defined as a sequence of packets going from the same source to the same destination, if the router recognises the flow it could create a short-cut by creating a “switched” connection allowing all the packets belonging to the same IP flow to proceed directly towards the correct direction without being analyzed one after the other. This general idea is known as IP switching.\n\nIf the shortcut however occurs at an optical level, the process becomes Optical IP Switching. The advantage of OIS comes from the fact that today packets are transmitted optically between two points but at each routing station they have to be converted into electrical signal, routed and converted back into optical to continue their travel over the optical fiber. If instead the router is able to recognise a flow, it could create a shortcut (“cut-through connection”) directly at the optical level, and all the packets belonging to the same flow could be directed to the right destination without the optical-to-electrical conversion process. This would save time, energy, memory and processing resources on the router.\n\nA basic implementation of the OIS concept sees an optical router that monitors IP traffic and if a flow appears with specific characteristics the router establishes an optical cut-through path between its upstream and downstream neighbours, requesting the upstream node to place all the packets belonging to the flow into the new path. The newly generated trail bypasses the IP layer of the router, as the packets transparently flow from the upstream to the downstream neighbour. Following a similar procedure the path can then be extended to more than three nodes, but this decision is always autonomously taken by each router and depends on the traffic encountered and on the resources locally available.\nSince an optical link however can carry several gigabits of data per second, it may be difficult to find a flow that alone can exploit the bandwidth offered by an optical trail.\nFor this reason, aggregating more IP flows into the same dedicated path is essential for the performance of an OIS network.\nThe aggregation introduces a trade-off between the number of IP flows that can be aggregated together and the length of the optical trail that accommodates them.\nIn order to achieve good performance only optical flows sharing a significant amount of network hops should be aggregated into the same path.\nA core node implementing optical IP switching must be endowed with electrical processing and memory resources (as a standard IP router), a variable number of optical transceivers and an optical switching element (usually a MEMS based device).\nAn edge node instead does not need an optical switching device because it could only function as source or destination of the optical flow.\n\nThe control protocol nearest to OIS is probably GMPLS, which is being standardized by the IETF. GMPLS aims at creating end-to-end connections after an explicit request from a customer or a network engineering service. This constitutes the main difference with OIS where the optical trials are automatically triggered by the encountered traffic; they are initially generated between three adjacent nodes, and then extended following a distributed decision.\n\n"}
{"id": "15342540", "url": "https://en.wikipedia.org/wiki?curid=15342540", "title": "Peshtemal", "text": "Peshtemal\n\nA peshtemal (also spelled peshtamal, pestamal or pestemal; from ) is a traditional Turkish towel used in Turkish baths. A staple of the Ottoman hamam culture, dating back hundreds of years, the pestemal was originally designed to help individual bathers maintain their privacy. Today, this light weight garment lends itself perfectly to quotidian use. In addition to being a wonderful bath towel due to its high absorbency, the pestemal dries faster than its thicker counterparts. It is also used to indicate which region people are from and is a part of a deep cultural tradition. There are many kinds of peshtemal, with different styles and colors in different areas of Turkey.\n\nThe peshtemal absorbs water as fast as a traditional towel, dries very quickly, takes up less space, is easy to carry and is therefore used as an alternative to the towel in bathrooms, pools, spas, beaches, sport facilities and for baby care. The peshtemal fabric is made of 100% cotton produced in manually operated looms in Turkey.\n\n"}
{"id": "10158889", "url": "https://en.wikipedia.org/wiki?curid=10158889", "title": "Philosophy of engineering", "text": "Philosophy of engineering\n\nThe philosophy of engineering is an emerging discipline that considers what engineering is, what engineers do, and how their work affects society, and thus includes aspects of ethics and aesthetics, as well as the ontology, epistemology, etc. that might be studied in, for example, the philosophy of science.\n\nEngineering is the profession aimed at modifying the natural environment, through the design, manufacture and maintenance of artifacts and technological systems. It might then be contrasted with science, the aim of which is to \"understand\" nature. Engineering at its core is about causing change, and therefore management of change is central to engineering practice. The philosophy of engineering is then the consideration of philosophical issues as they apply to engineering. Such issues might include the objectivity of experiments, the ethics of engineering activity in the workplace and in society, the aesthetics of engineered artifacts, etc.\n\nWhile engineering seems historically to have meant \"devising\", the distinction between art, craft and technology isn't clearcut. The Latin root \"ars\", the Germanic root \"kraft\" and the Greek root \"techne\" all originally meant the skill or ability to produce something, as opposed to, say, athletic ability. The something might be tangible, like a sculpture or a building, or less tangible, like a work of literature. Nowadays, \"art\" is commonly applied to the visual, performing or literary fields, especially the so-called fine arts ('the art of writing'), \"craft\" usually applies to the manual skill involved in the manufacture of an object, whether embroidery or aircraft ('the craft of typesetting') and \"technology\" tends to mean the products and processes currently used in an industry ('the technology of printing'). In contrast, \"engineering\" is the activity of effecting change through the design and manufacture of artifacts ('the engineering of print technology').\n\nWhat distinguishes engineering design from artistic design is the requirement for the engineer to make quantitative predictions of the behavior and effect of the artifact prior to its manufacture. Such predictions may be more or less accurate but usually includes the effects on individuals and/or society. In this sense, engineering can be considered a social as well a technological discipline and judged not just by whether its artifacts work, in a narrow sense, but also by how they influence and serve social values. What engineers do is subject to moral evaluation.\n\nSocio-technical systems, such as transport, utilities and their related infrastructures comprise human elements as well as artifacts. Traditional mathematical and physical modeling techniques may not take adequate account of the effects of engineering on people, and culture. The Civil Engineering discipline makes elaborate attempts to ensure that a structure meets its specifications and other requirements prior to its actual construction. The methods employed are well known as Analysis and Design. Systems Modelling and Description makes an effort to extract the generic unstated principles behind the engineering approach.\n\nThe traditional engineering disciplines seem discrete but the engineering of artifacts has implications that extend beyond such disciplines into areas that might include psychology, finance and sociology. The design of any artifact will then take account of the conditions under which it will be manufactured, the conditions under which it will be used, and the conditions under which it will be disposed. Engineers can consider such \"life cycle\" issues without losing the precision and rigor necessary to design functional systems.\n\n\n\n\n"}
{"id": "4759830", "url": "https://en.wikipedia.org/wiki?curid=4759830", "title": "Photochromic lens", "text": "Photochromic lens\n\nPhotochromic lenses are optical lenses that darken on exposure to specific types of light of sufficient intensity, most commonly ultraviolet (UV) radiation. In the absence of activating light the lenses return to their clear state. Photochromic lenses may be made of glass, polycarbonate, or another plastic. They are principally used in eyeglasses that are dark in bright sunlight, but clear in low ambient light conditions. They darken significantly within about a minute of exposure to bright light, and take somewhat longer to clear. A range of clear and dark transmittances are available.\n\nIn one sort of technology, molecules of silver chloride or another silver halide are embedded in photochromatic lenses. They are transparent to visible light without significant ultraviolet component, which is normal for artificial lighting. In another sort of technology, organic photochromic molecules, when exposed to ultraviolet (UV) rays as in direct sunlight, undergo a chemical process that causes them to change shape and absorb a significant percentage of the visible light, i.e., they darken. These processes are reversible; once the lens is removed from strong sources of UV rays the photochromic compounds return to their transparent state.\n\nPhotochromic lenses were developed by Roger Araujo at the Corning Glass Works Inc. in the 1960s, and the process was used in the first mass-produced variable tint lenses.\n\nThe glass version of these lenses achieve their photochromic properties through the embedding of microcrystalline silver halides (usually silver chloride) in a glass substrate. Plastic photochromic lenses use organic photochromic molecules (for example oxazines and naphthopyrans) to achieve the reversible darkening effect. These lenses darken when exposed to ultraviolet light of the intensity present in sunlight, but not in artificial light. In the presence of UV-A light (wavelengths of 320–400 nm), electrons from the glass combine with the colorless silver cations to form elemental silver. Because elemental silver is visible, the lenses appear darker. Back in the shade, this reaction is reversed. The silver returns to its original ionic state, and the lenses become clear.\n\nWith the photochromic material dispersed in the glass substrate, the degree of darkening depends on the thickness of glass, which poses problems with variable-thickness lenses in prescription glasses. With plastic lenses, the material is typically embedded into the surface layer of the plastic in a uniform thickness of up to 150 µm.\n\nTypically, photochromic lenses darken substantially in response to UV light in less than one minute, and continue to darken a little more over the next fifteen minutes. The lenses begin to clear in the absence of UV light, and will be noticeably lighter within two minutes, mostly clear within five minutes, and fully back to their non-exposed state in about fifteen minutes. A report by the Institute of Ophthalmology at the University College London suggested that at their clearest photochromic lenses can absorb up to 20% of ambient light.\n\nBecause photochromic compounds fade back to their clear state by a thermal process, the higher the temperature, the less dark photochromic lenses will be. This thermal effect is called \"temperature dependency\" and prevents these devices from achieving true sunglass darkness in very hot weather. Conversely, photochromic lenses will get very dark in cold weather conditions. Once inside, away from the triggering UV light, the cold lenses take longer to regain their transparency than warm lenses.\n\nPhotochromic lenses filter 100% UVA as well as UVB. UVB light is more energetic and causes sunburn as well as skin damage including cancers, UVA light causes skincancers but not usually sunburn. UVB is blocked by all glass, UVA light is not blocked by ordinary windows or lenses glass.\n\nA number of sunglass manufacturers and suppliers including Tifosi, Intercast, Oakley, Serengeti Eyewear, and Persol provide tinted lenses that use photochromism to go from a dark to a darker state. They are typically used for outdoor sunglasses rather than as general-purpose lenses.\n\n"}
{"id": "5498670", "url": "https://en.wikipedia.org/wiki?curid=5498670", "title": "Power dividers and directional couplers", "text": "Power dividers and directional couplers\n\nPower dividers (also power splitters and, when used in reverse, power combiners) and directional couplers are passive devices used mostly in the field of radio technology. They couple a defined amount of the electromagnetic power in a transmission line to a port enabling the signal to be used in another circuit. An essential feature of directional couplers is that they only couple power flowing in one direction. Power entering the output port is coupled to the isolated port but not to the coupled port. A directional coupler designed to split power equally between two ports is called a hybrid coupler.\n\nDirectional couplers are most frequently constructed from two coupled transmission lines set close enough together such that energy passing through one is coupled to the other. This technique is favoured at the microwave frequencies where transmission line designs are commonly used to implement many circuit elements. However, lumped component devices are also possible at lower frequencies, such as the audio frequencies encountered in telephony. Also at microwave frequencies, particularly the higher bands, waveguide designs can be used. Many of these waveguide couplers correspond to one of the conducting transmission line designs, but there are also types that are unique to waveguide.\n\nDirectional couplers and power dividers have many applications. These include providing a signal sample for measurement or monitoring, feedback, combining feeds to and from antennas, antenna beam forming, providing taps for cable distributed systems such as cable TV, and separating transmitted and received signals on telephone lines.\n\nThe symbols most often used for directional couplers are shown in figure 1. The symbol may have the coupling factor in dB marked on it. Directional couplers have four ports. Port 1 is the input port where power is applied. Port 3 is the coupled port where a portion of the power applied to port 1 appears. Port 2 is the transmitted port where the power from port 1 is outputted, less the portion that went to port 3. Directional couplers are frequently symmetrical so there also exists port 4, the isolated port. A portion of the power applied to port 2 will be coupled to port 4. However, the device is not normally used in this mode and port 4 is usually terminated with a matched load (typically 50 ohms). This termination can be internal to the device and port 4 is not accessible to the user. Effectively, this results in a 3-port device, hence the utility of the second symbol for directional couplers in figure 1.\n\nSymbols of the form;\nin this article have the meaning \"parameter \"P\" at port \"a\" due to an input at port \"b\"\".\n\nA symbol for power dividers is shown in figure 2. Power dividers and directional couplers are in all essentials the same class of device. \"Directional coupler\" tends to be used for 4-port devices that are only loosely coupled – that is, only a small fraction of the input power appears at the coupled port. \"Power divider\" is used for devices with tight coupling (commonly, a power divider will provide half the input power at each of its output ports – a divider) and is usually considered a 3-port device.\n\nCommon properties desired for all directional couplers are wide operational bandwidth, high directivity, and a good impedance match at all ports when the other ports are terminated in\nmatched loads. Some of these, and other, general characteristics are discussed below.\n\nThe coupling factor is defined as: formula_2\n\nwhere P is the input power at port 1 and P is the output power from the coupled port (see figure 1).\n\nThe coupling factor represents the primary property of a directional coupler. Coupling factor is a negative quantity, it cannot exceed for a passive device, and in practice does not exceed since more than this would result in more power output from the coupled port than power from the transmitted port – in effect their roles would be reversed. Although a negative quantity, the minus sign is frequently dropped (but still implied) in running text and diagrams and a few authors go so far as to define it as a \"positive\" quantity. Coupling is not constant, but varies with frequency. While different designs may reduce the variance, a perfectly flat coupler theoretically cannot be built. Directional couplers are specified in terms of the coupling accuracy at the frequency band center.\n\nThe main line insertion loss from port 1 to port 2 (P – P) is:\n\nInsertion loss: formula_3\n\nPart of this loss is due to some power going to the coupled port and is called coupling loss and is given by:\n\nCoupling loss: formula_4\n\nThe insertion loss of an ideal directional coupler will consist entirely of the coupling loss. In a real directional coupler, however, the insertion loss consists of a combination of coupling loss, dielectric loss, conductor loss, and VSWR loss. Depending on the frequency range, coupling loss becomes less significant above coupling where the other losses constitute the majority of the total loss. The theoretical insertion loss (dB) vs coupling (dB) for a dissipationless coupler is shown in the graph of figure 3 and the table below.\n\nIsolation of a directional coupler can be defined as the difference in signal levels in dB between the input port and the isolated port when the two other ports are terminated by matched loads, or:\n\nIsolation: formula_5\n\nIsolation can also be defined between the two output ports. In this case, one of the output ports is used as the input; the other is considered the output port while the other two ports (input and isolated) are terminated by matched loads.\n\nConsequently: formula_6\n\nThe isolation between the input and the isolated ports may be different from the isolation between the two output ports. For example, the isolation between ports 1 and 4 can be while the isolation between ports 2 and 3 can be a different value such as . Isolation can be estimated from the coupling plus return loss. The isolation should be as high as possible. In actual couplers the isolated port is never completely isolated. Some RF power will always be present. Waveguide directional couplers will have the best isolation.\n\nDirectivity is directly related to isolation. It is defined as:\n\nDirectivity: formula_7\n\nwhere: P is the output power from the coupled port and P is the power output from the isolated port.\n\nThe directivity should be as high as possible. The directivity is very high at the design frequency and is a more sensitive function of frequency because it depends on the cancellation of two wave components. Waveguide directional couplers will have the best directivity. Directivity is not directly measurable, and is calculated from the addition of the isolation and (negative) coupling measurements as:\n\nNote that if the positive definition of coupling is used, the formula results in:\n\nThe S-matrix for an ideal (infinite isolation and perfectly matched) symmetrical directional coupler is given by,\n\nIn general, formula_11 and formula_12 are complex, frequency dependent, numbers. The zeroes on the matrix main diagonal are a consequence of perfect matching – power input to any port is not reflected back to that same port. The zeroes on the matrix antidiagonal are a consequence of perfect isolation between the input and isolated port.\n\nFor a passive lossless directional coupler, we must in addition have,\n\nsince the power entering the input port must all leave by one of the other two ports.\n\nInsertion loss is related to formula_11 by;\n\nCoupling factor is related to formula_12 by;\n\nNon-zero main diagonal entries are related to return loss, and non-zero antidiagonal entries are related to isolation by similar expressions.\n\nSome authors define the port numbers with ports 3 and 4 interchanged. This results in a scattering matrix that is no longer all-zeroes on the antidiagonal.\n\nThis terminology defines the power difference in dB between the two output ports of a hybrid. In an ideal hybrid circuit, the difference should be . However, in a practical device the amplitude balance is frequency dependent and departs from the ideal difference.\n\nThe phase difference between the two output ports of a hybrid coupler should be 0°, 90°, or 180° depending on the type used. However, like amplitude balance, the phase difference is sensitive to the input frequency and typically will vary a few degrees.\n\nThe most common form of directional coupler is a pair of coupled transmission lines. They can be realised in a number of technologies including coaxial and the planar technologies (stripline and microstrip). An implementation in stripline is shown in figure 4 of a quarter-wavelength (λ/4) directional coupler. The power on the coupled line flows in the opposite direction to the power on the main line, hence the port arrangement is not the same as shown in figure 1, but the numbering remains the same. For this reason it is sometimes called a \"backward coupler\".\n\nThe \"main line\" is the section between ports 1 and 2 and the \"coupled line\" is the section between ports 3 and 4. Since the directional coupler is a linear device, the notations on figure 1 are arbitrary. Any port can be the input, (an example is seen in figure 20) which will result in the directly connected port being the transmitted port, the adjacent port being the coupled port, and the diagonal port being the isolated port. On some directional couplers, the main line is designed for high power operation (large connectors), while the coupled port may use a small connector, such as an SMA connector. The internal load power rating may also limit operation on the coupled line.\n\nAccuracy of coupling factor depends on the dimensional tolerances for the spacing of the two coupled lines. For planar printed technologies this comes down to the resolution of the printing process which determines the minimum track width that can be produced and also puts a limit on how close the lines can be placed to each other. This becomes a problem when very tight coupling is required and couplers often use a different design. However, tightly coupled lines can be produced in air stripline which also permits manufacture by printed planar technology. In this design the two lines are printed on \"opposite\" sides of the dielectric rather than side by side. The coupling of the two lines across their width is much greater than the coupling when they are edge-on to each other.\n\nThe λ/4 coupled line design is good for coaxial and stripline implementations but does not work so well in the now popular microstrip format, although designs do exist. The reason for this is that microstrip is not a homogeneous medium – there are two different mediums above and below the transmission strip. This leads to transmission modes other than the usual TEM mode found in conductive circuits. The propagation velocities of even and odd modes are different leading to signal dispersion. A better solution for microstrip is a coupled line much shorter than λ/4, shown in figure 5, but this has the disadvantage of a coupling factor which rises noticeably with frequency. A variation of this design sometimes encountered has the coupled line a higher impedance than the main line such as shown in figure 6. This design is advantageous where the coupler is being fed to a detector for power monitoring. The higher impedance line results in a higher RF voltage for a given main line power making the work of the detector diode easier.\n\nThe frequency range specified by manufacturers is that of the coupled line. The main line response is much wider: for instance a coupler specified as might have a main line which could operate at . As with all distributed element circuits, the coupled response is periodic with frequency. For example, a λ/4 coupled line coupler will have responses at \"n\"λ/4 where \"n\" is an odd integer.\n\nA single λ/4 coupled section is good for bandwidths of less than an octave. To achieve greater bandwidths multiple λ/4 coupling sections are used. The design of such couplers proceeds in much the same way as the design of distributed element filters. The sections of the coupler are treated as being sections of a filter, and by adjusting the coupling factor of each section the coupled port can be made to have any of the classic filter responses such as maximally flat (Butterworth filter), equal-ripple (Cauer filter), or a specified-ripple (Chebychev filter) response. Ripple is the maximum variation in output of the coupled port in its passband, usually quoted as plus or minus a value in dB from the nominal coupling factor.\nIt can be shown that coupled line directional couplers have formula_11 purely real and formula_12 purely imaginary at all frequencies. This leads to a simplification of the S-matrix and the result that the coupled port is always in quadrature phase (90°) with the output port. Some applications make use of this phase difference. Letting formula_22, the ideal case of lossless operation simplifies to,\n\nThe branch-line coupler consists of two parallel transmission lines physically coupled together with two or more branch lines between them. The branch lines are spaced λ/4 apart and represent sections of a multi-section filter design in the same way as the multiple sections of a coupled line coupler except that here the coupling of each section is controlled with the impedance of the branch lines. The main and coupled line are formula_24 of the system impedance. The more sections there are in the coupler, the higher is the ratio of impedances of the branch lines. High impedance lines have narrow tracks and this usually limits the design to three sections in planar formats due to manufacturing limitations. A similar limitation applies for coupling factors looser than ; low coupling also requires narrow tracks. Coupled lines are a better choice when loose coupling is required, but branch-line couplers are good for tight coupling and can be used for hybrids. Branch-line couplers usually do not have such a wide bandwidth as coupled lines. This style of coupler is good for implementing in high-power, air dielectric, solid bar formats as the rigid structure is easy to mechanically support.\n\nThe construction of the Lange coupler is similar to the interdigital filter with paralleled lines interleaved to achieve the coupling. It is used for strong couplings in the range to .\n\nThe earliest transmission line power dividers were simple T-junctions. These suffer from very poor isolation between the output ports – a large part of the power reflected back from port 2 finds its way into port 3. It can be shown that it is not theoretically possible to simultaneously match all three ports of a passive, lossless three-port and poor isolation is unavoidable. It is, however, possible with four-ports and this is the fundamental reason why four-port devices are used to implement three-port power dividers: four-port devices can be designed so that power arriving at port 2 is split between port 1 and port 4 (which is terminated with a matching load) and none (in the ideal case) goes to port 3.\n\nThe term \"hybrid coupler\" originally applied to coupled line directional couplers, that is, directional couplers in which the two outputs are each half the input power. This synonymously meant a quadrature coupler with outputs 90° out of phase. Now any matched 4-port with isolated arms and equal power division is called a hybrid or hybrid coupler. Other types can have different phase relationships. If 90°, it is a 90° hybrid, if 180°, a 180° hybrid and so on. In this article \"hybrid coupler\" without qualification means a coupled line hybrid.\n\nThe Wilkinson power divider consists of two parallel \"uncoupled\" λ/4 transmission lines. The input is fed to both lines in parallel and the outputs are terminated with twice the system impedance bridged between them. The design can be realised in planar format but it has a more natural implementation in coax – in planar, the two lines have to be kept apart so that they do not couple but have to be brought together at their outputs so they can be terminated whereas in coax the lines can be run side-by-side relying on the coax outer conductors for screening. The Wilkinson power divider solves the matching problem of the simple T-junction: it has low VSWR at all ports and high isolation between output ports. The input and output impedances at each port are designed to be equal to the characteristic impedance of the microwave system. This is achieved by making the line impedance formula_24 of the system impedance – for a system the Wilkinson lines are approximately \n\nCoupled line directional couplers are described above. When the coupling is designed to be it is called a hybrid coupler. The S-matrix for an ideal, symmetric hybrid coupler reduces to;\n\nThe two output ports have a 90° phase difference (-\"i\" to −1) and so this is a 90° hybrid.\n\nThe hybrid ring coupler, also called the rat-race coupler, is a four-port directional coupler consisting of a 3λ/2 ring of transmission line with four lines at the intervals shown in figure 12. Power input at port 1 splits and travels both ways round the ring. At ports 2 and 3 the signal arrives in phase and adds whereas at port 4 it is out of phase and cancels. Ports 2 and 3 are in phase with each other, hence this is an example of a 0° hybrid. Figure 12 shows a planar implementation but this design can also be implemented in coax or waveguide. It is possible to produce a coupler with a coupling factor different from by making each λ/4 section of the ring alternately low and high impedance but for a coupler the entire ring is made formula_24 of the port impedances – for a design the ring would be approximately .\n\nThe S-matrix for this hybrid is given by;\n\nThe hybrid ring is not symmetric on its ports; choosing a different port as the input does not necessarily produce the same results. With port 1 or port 3 as the input the hybrid ring is a 0° hybrid as stated. However using port 2 or port 4 as the input results in a 180° hybrid. This fact leads to another useful application of the hybrid ring: it can be used to produce sum (Σ) and difference (Δ) signals from two input signals as shown in figure 12. With inputs to ports 2 and 3, the Σ signal appears at port 1 and the Δ signal appears at port 4.\n\nA typical power divider is shown in figure 13. Ideally, input power would be divided equally between the output ports. Dividers are made up of multiple couplers and, like couplers, may be reversed and used as multiplexers. The drawback is that for a four channel multiplexer, the output consists of only 1/4 the power from each, and is relatively inefficient. The reason for this is that at each combiner half the input power goes to port 4 and is dissipated in the termination load. If the two inputs were coherent the phases could be so arranged that cancellation occurred at port 4 and then all the power would go to port 1. However, multiplexer inputs are usually from entirely independent sources and therefore not coherent. Lossless multiplexing can only be done with filter networks.\n\nThe branch-line coupler described above can also be implemented in waveguide.\n\nOne of the most common, and simplest, waveguide directional couplers is the Bethe-hole directional coupler. This consists of two parallel waveguides, one stacked on top of the other, with a hole between them. Some of the power from one guide is launched through the hole into the other. The Bethe-hole coupler is another example of a backward coupler.\n\nThe concept of the Bethe-hole coupler can be extended by providing multiple holes. The holes are spaced λ/4 apart. The design of such couplers has parallels with the multiple section coupled transmission lines. Using multiple holes allows the bandwidth to be extended by designing the sections as a Butterworth, Chebyshev, or some other filter class. The hole size is chosen to give the desired coupling for each section of the filter. Design criteria are to achieve a substantially flat coupling together with high directivity over the desired band.\n\nThe Riblet short-slot coupler is two waveguides side-by-side with the side-wall in common instead of the long side as in the Bethe-hole coupler. A slot is cut in the sidewall to allow coupling. This design is frequently used to produce a coupler.\n\nThe Schwinger reversed-phase coupler is another design using parallel waveguides, this time the long side of one is common with the short side-wall of the other. Two off-centre slots are cut between the waveguides spaced λ/4 apart. The Schwinger is a backward coupler. This design has the advantage of a substantially flat directivity response and the disadvantage of a strongly frequency-dependent coupling compared to the Bethe-hole coupler, which has little variation in coupling factor.\n\nThe Moreno crossed-guide coupler has two waveguides stacked one on top of the other like the Bethe-hole coupler but at right angles to each other instead of parallel. Two off-centre holes, usually cross-shaped are cut on the diagonal between the waveguides a distance formula_29 apart. The Moreno coupler is good for tight coupling applications. It is a compromise between the properties of the Bethe-hole and Schwinger couplers with both coupling and directivity varying with frequency.\n\nThe hybrid ring discussed above can also be implemented in waveguide.\n\nCoherent power division was first accomplished by means of simple Tee junctions. At microwave frequencies, waveguide tees have two possible forms – the E-plane and H-plane. These two junctions split power equally, but because of the different field configurations at the junction, the electric fields at the output arms are in phase for the H-plane tee and are 180° out of phase for the E-plane tee. The combination of these two tees to form a hybrid tee is known as the magic tee. The magic tee is a four-port component which can perform the vector sum (Σ) and difference (Δ) of two coherent microwave signals.\n\nThe standard 3 dB hybrid transformer is shown in figure 16. Power at port 1 is split equally between ports 2 and 3 but in antiphase to each other. The hybrid transformer is therefore a 180° hybrid. The centre-tap is usually terminated internally but it is possible to bring it out as port 4; in which case the hybrid can be used as a sum and difference hybrid. However, port 4 presents as a different impedance to the other ports and will require an additional transformer for impedance conversion if it is required to use this port at the same system impedance.\n\nHybrid transformers are commonly used in telecommunications for 2 to 4 wire conversion. Telephone handsets include such a converter to convert the 2-wire line to the 4 wires from the earpiece and mouthpiece.\n\nFor lower frequencies (less than ) a compact broadband implementation by means of RF transformers is possible. In figure 17 a circuit is shown which is meant for weak coupling and can be understood along these lines: A signal is coming in one line pair. One transformer reduces the voltage of the signal the other reduces the current. Therefore, the impedance is matched. The same argument holds for every other direction of a signal through the coupler. The relative sign of the induced voltage and current determines the direction of the outgoing signal.\n\nThe coupling is given by;\n\nFor a coupling, that is equal splitting of the signal between the transmitted port and the coupled port, formula_31 and the isolated port is terminated in twice the characteristic impedance – for a system. A power divider based on this circuit has the two outputs in 180° phase to each other, compared to λ/4 coupled lines which have a 90° phase relationship.\n\nA simple tee circuit of resistors can be used as a power divider as shown in figure 18. This circuit can also be implemented as a delta circuit by applying the Y-Δ transform. The delta form uses resistors that are equal to the system impedance. This can be advantageous because precision resistors of the value of the system impedance are always available for most system nominal impedances. The tee circuit has the benefits of simplicity, low cost, and intrinsically wide bandwidth. It has two major drawbacks; first, the circuit will dissipate power since it is resistive: an equal split will result in insertion loss instead of . The second problem is that there is directivity leading to very poor isolation between the output ports.\n\nThe insertion loss is not such a problem for an unequal split of power: for instance at port 3 has an insertion loss less than at port 2. Isolation can be improved at the expense of insertion loss at both output ports by replacing the output resistors with T pads. The isolation improvement is greater than the insertion loss added.\n\nA true hybrid divider/coupler with, theoretically, infinite isolation and directivity can be made from a resistive bridge circuit. Like the tee circuit, the bridge has insertion loss. It has the disadvantage that it cannot be used with unbalanced circuits without the addition of transformers; however, it is ideal for balanced telecommunication lines if the insertion loss is not an issue. The resistors in the bridge which represent ports are not usually part of the device (with the exception of port 4 which may well be left permanently terminated internally) these being provided by the line terminations. The device thus consists essentially of two resistors (plus the port 4 termination).\n\nThe coupled output from the directional coupler can be used to monitor frequency and power level on the signal without interrupting the main power flow in the system (except for a power reduction – see figure 3).\n\nIf isolation is high, directional couplers are good for combining signals to feed a single line to a receiver for two-tone receiver tests. In figure 20, one signal enters port P and one enters port P, while both exit port P. The signal from port P to port P will experience of loss, and the signal from port P to port P will have loss. The internal load on the isolated port will dissipate the signal losses from port P and port P. If the isolators in figure 20 are neglected, the isolation measurement (port P to port P) determines the amount of power from the signal generator F that will be injected into the signal generator F. As the injection level increases, it may cause modulation of signal generator F, or even injection phase locking. Because of the symmetry of the directional coupler, the reverse injection will happen with the same possible modulation problems of signal generator F by F. Therefore, the isolators are used in figure 20 to effectively increase the isolation (or directivity) of the directional coupler. Consequently, the injection loss will be the isolation of the directional coupler plus the reverse isolation of the isolator.\n\nApplications of the hybrid include monopulse comparators, mixers, power combiners, dividers, modulators, and phased array radar antenna systems. Both in-phase devices (such as the Wilkinson divider) and quadrature (90°) hybrid couplers may be used for coherent power divider applications. An example of quadrature hybrids being used in a coherent power combiner application is given in the next section.\n\nAn inexpensive version of the power divider is used in the home to divide cable TV or over-the-air TV signals to multiple TV sets and other devices. Multiport splitters with more than two output ports usually consist internally of a number of cascaded couplers. Domestic broadband internet service can be provided by cable TV companies (cable internet). The domestic user's internet cable modem is connected to one port of the splitter.\n\nSince hybrid circuits are bi-directional, they can be used to coherently combine power as well as splitting it. In figure 21, an example is shown of a signal split up to feed multiple low power amplifiers, then recombined to feed a single antenna with high power.\nThe phases of the inputs to each power combiner are arranged such that the two inputs are 90° out of phase with each other. Since the coupled port of a hybrid combiner is 90° out of phase with the transmitted port, this causes the powers to add at the output of the combiner and to cancel at the isolated port: a representative example from figure 21 is shown in figure 22. Note that there is an additional fixed 90° phase shift to both ports at each combiner/divider which is not shown in the diagrams for simplicity. Applying in-phase power to both input ports would not get the desired result: the quadrature sum of the two inputs would appear at both output ports – that is half the total power out of each. This approach allows the use of numerous less expensive and lower-power amplifiers in the circuitry instead of a single high-power TWT. Yet another approach is to have each solid state amplifier (SSA) feed an antenna and let the power be combined in space or be used to feed a lens attached to an antenna.\n\nThe phase properties of a 90° hybrid coupler can be used to great advantage in microwave circuits. For example, in a balanced microwave amplifier the two input stages are fed through a hybrid coupler. The FET device normally has a very poor match and reflects much of the incident energy. However, since the devices are essentially identical the reflection coefficients from each device are equal. The reflected voltage from the FETs are in phase at the isolated port and are 180° different at the input port. Therefore, all of the reflected power from the FETs goes to the load at the isolated port and no power goes to the input port. This results in a good input match (low VSWR).\n\nIf phase-matched lines are used for an antenna input to a 180° hybrid coupler as shown in figure 23, a null will occur directly between the antennas. To receive a signal in that position, one would have to either change the hybrid type or line length. To reject a signal from a given direction, or create the difference pattern for a monopulse radar, this is a good approach.\n\nPhase-difference couplers can be used to create beam tilt in a VHF FM radio station, by delaying the phase to the lower elements of an antenna array. More generally, phase-difference couplers, together with fixed phase delays and antenna arrays, are used in beam-forming networks such as the Butler matrix, to create a radio beam in any prescribed direction.\n\n\n"}
{"id": "23172782", "url": "https://en.wikipedia.org/wiki?curid=23172782", "title": "Program on Vehicle and Mobility Innovation", "text": "Program on Vehicle and Mobility Innovation\n\nThe Program on Vehicle and Mobility Innovation (PVMI) is the oldest and largest international research consortium aimed at understanding the challenges facing the global automotive industry.\n\nPVMI, founded as the International Motor Vehicle Program (IMVP) at the Massachusetts Institute of Technology in 1979, has mapped lean methodologies, established benchmarking standards, and probed the automotive value chain. The program's data-driven methods set the standard for industry research. In 2013, program was restructured and renamed as part of its incorporation into the Mack Institute for Innovation Management at the Wharton School.\n\nPVMI/IMVP has had a major impact on the global automobile industry and the economy that surrounds it since it was launched in 1979. More than 50 senior scientists, management experts, social scientists, and engineers have conducted interdisciplinary automotive research at more than 25 universities on six continents.\n\nThe program has gone through several phases since its conception in 1980:\n\n\n"}
{"id": "52471935", "url": "https://en.wikipedia.org/wiki?curid=52471935", "title": "Quick Charge", "text": "Quick Charge\n\nQuick Charge is a technology found in Qualcomm SoCs, used in devices such as mobiles, for managing power delivered over USB. It offers more power and thus charges batteries in devices faster than standard USB rates allow.\n\nQuick Charge is a proprietary technology which allows for the charging of battery powered devices, primarily mobile phones, at levels above and beyond the typical 5 volts and 2 amps for which most USB standards allow. Numerous other companies have their own competing technologies; these include Mediatek's Pump Express, OPPO's VOOC (licensed to OnePlus as Dash Charge). To take advantage of Qualcomm Quick Charge, both the host providing power and the device must support it. In 2012 the USB Implementers Forum (USB IF) announced that their USB Power Delivery (USB PD) standard had been finalized which allows for devices to transfer up to 100 watts of power over capable USB ports. \n\nBeginning with version 3, the driving technology behind Quick Charge was named \"INOV\" (\"Intelligent Negotiation for Optimal Voltage\"), with ensuing versions successively improving upon allowed charging voltage levels.\n\nQuick Charge version 4 was announced in December 2016 alongside the Snapdragon 835. Version 4 implements additional safety measures to protect against overcharging and overheating, and is compliant with both USB⁠-⁠C and USB Power Delivery (USB-PD) specifications.\n\n"}
{"id": "18541730", "url": "https://en.wikipedia.org/wiki?curid=18541730", "title": "Revolution Money", "text": "Revolution Money\n\nRevolution Money is a financial services company based in St. Petersburg, Florida. The company's products include a PIN based credit card, online person to person payments service, with a linked stored value card, and gift card. Revolution Money is the only credit card that does not charge retailers interchange fees. The company partnered with Yahoo! Sports and Fifth Third Bank.\n\nRevolution Money has three products: RevolutionCard is a credit card, Revolution MoneyExchange which provides free online money transfers between members, and RevolutionGift, the gift card. Revolution MoneyExchange accounts are issued by First Bank and Trust.\n\nRevolution MoneyExchange was an online bank intended as an alternative to PayPal and its chief competitor, Google Checkout. It was founded as GratisCard in April 2007. Ted Leonsis and Steve Case were on its board of directors. Revolution MoneyExchange was backed by Citi, Morgan Stanley, and Deutsche Bank AG, as well as its parent company, Revolution LLC.\n\nThere were no fees charged for online transfers between accounts. The Revolution MoneyExchange Card was a stored-value card that allowed account holders to access their funds for purchases at merchant locations on the RevolutionCard network and for cash withdrawals at ATMs nationwide.\n\nRevolutionGift is a prepaid PIN based gift card with no account number printed on the card. Other features included the capacity to send money via AOL Instant Messenger. \n\nOn November 18, 2009, American Express announced that it would acquire Revolution Money for US$500 million, and finally did for US$300 million. Revolution MoneyExchange was purchased by American Express in January 2010, and was renamed Serve Virtual Enterprises, Inc. Serve Enterprises then launched Serve and discontinued Revolution MoneyExchange on March 28, 2011.\n\n"}
{"id": "38574731", "url": "https://en.wikipedia.org/wiki?curid=38574731", "title": "Ring (company)", "text": "Ring (company)\n\nRing (formerly Doorbot) is a global home security company owned by Amazon. Ring manufactures a range of home security products that incorporate outdoor motion based cameras and doorbells, such as the Ring Video Doorbell, that was featured on ABC's \"Shark Tank\" in 2013. Ring Video Doorbell is a smart doorbell device that allows users to monitor and operate their front, back and garage doors remotely. When the doorbell is pressed, the Ring application begins a VOIP video call to a connected device, so that the owner can see and speak to visitors.\n\nRing's secondary office, called Ring Ukraine, is located in Kyiv. It includes a research lab and a client support center, and has its own website.\n\nFrom an early age, Ring founder Jamie Siminoff was interested in engineering and entrepreneurship: his father co-owned a plant forging steel pipes for oil refineries and, using his father’s tools, Siminoff manufactured humane mouse traps, heated blankets and remote-controlled cars after school. After attending Babson College and launching and selling two companies, Siminoff conceptualized a company called Doorbot, which would eventually become Ring, understanding his home safety was compromised because he could not hear the front doorbell from his garage. Siminoff modernized his doorbell by adding Internet connectivity, a high-definition camera and a motion detector to enable him to control the sensor’s direction. In 2012, Siminoff founded Ring and in 2013 released the first version of his product. Ring was bought by Amazon in February 2018.\n\nIn 2013, Siminoff went on ABC’s Shark Tank, asking for $700,000 for his company, at the time called DoorBot, that he estimated was worth $7 million. Kevin O’Leary made an offer as a potential investor that Siminoff declined. The experience being on Shark Tank helped Siminoff raise the profile of the company. He rebranded and got $5 million in additional sales.\n\nDoorbot was crowdfunded via Christie Street, and raised 364k, more than the $250k requested.\n\nAmazon agreed to acquire Ring, for somewhere between $1.2 billion and $1.8 billion in February 2018. Reuters values the deal to be worth over $1 billion.\n\nIn 2015, Ring released the Stick-Up Cam which incorporated the same features as the doorbell but without the actual doorbell button. In 2016, the Ring Floodlight Cam was introduced which incorporated two 1100 lumen LED floodlights to optimize security. In 2017, Ring launched a DIY home security system, Ring Protect, including an alarm keypad and motion sensors. That same year, Ring released its second version of the device, Ring Doorbell 2, with a rechargeable battery pack, night vision, a motion detector, speaker and microphone. Ring Neighborhoods is an additional security feature that helps non-video doorbell users monitor suspicious activity on their app. In July 2018, Ring launched an alarm security kit, which includes a keypad, siren, and motion sensors.\n\nIn 2016, Ring partnered with Los Angeles Police Department and installed Ring Video Doorbells on 10 percent of homes in Wilshire Park, California. The LAPD’s data showed that there was a 55 percent decrease in break-ins within the first six months of the doorbells being installed. In addition to break-in prevention, Ring products have been cited as aiding the police in identifying perpetrators.\n\nThe app for Ring doorbell is currently available on iOS, Android,Mac, and Windows 10 Mobile. An Alexa skill for the Amazon Echo Show and Fire TV also allow users to view video feeds from Ring devices. Ring is sold in over 16,000 big box retailers, including: Home Depot, Target and Best Buy.\n"}
{"id": "9060625", "url": "https://en.wikipedia.org/wiki?curid=9060625", "title": "SXML", "text": "SXML\n\nSXML is an alternative syntax for writing XML data (more precisely, XML Infosets) as S-expressions, to facilitate working with XML data in Lisp and Scheme. An associated suite of tools implements XPath, SAX and XSLT for SXML in Scheme and are available in the GNU Guile implementation of that language.\n\nTextual correspondence between SXML and XML for a sample XML snippet is shown below:\n\nCompared to other alternative representations for XML and its associated languages, SXML has the benefit of being directly parsable by existing Scheme implementations. The associated tools and documentation were praised in many respects by David Mertz in his IBM developerWorks column, though he also criticized the preliminary nature of its documentation and system.\n\nTake the following simple XHTML page:\nAfter translating it to SXML, the same page now looks like this:\n\nEach element's tag pair is replaced by a set of parentheses. The tag's name is not repeated at the end, it is simply the first symbol in the list. The element's contents follow, which are either elements themselves or strings. There is no special syntax required for XML attributes. In SXML they are simply represented as just another node, which has the special name of <nowiki>@</nowiki>. This can't cause a name clash with an actual <nowiki>\"@\"</nowiki> tag, because <nowiki>@</nowiki> is not allowed as a tag name in XML. This is a common pattern in SXML: anytime a tag is used to indicate a special status or something that is not possible in XML, a name is used that does not constitute a valid XML identifier.\n\nWe can also see that there's no need to \"escape\" otherwise meaningful characters like & and > as &amp; and &gt; entities. All string content is automatically escaped because it is considered to be pure content, and has no tags or entities in it. This also means it is much easier to insert autogenerated content and that there is no danger that we might forget to escape user input when we display it to other users (which could lead to all kinds of cross-site scripting attacks or other development annoyances).\n\n"}
{"id": "2377830", "url": "https://en.wikipedia.org/wiki?curid=2377830", "title": "Society of Women Engineers", "text": "Society of Women Engineers\n\nThe Society of Women Engineers (SWE), founded in 1950, is a not-for-profit educational and service organization in the United States. SWE has over 37,000 members in nearly 100 professional sections and 300 student sections throughout the United States.\n\nThe SWE Archives contain a series of letters from the Elsie Eaves Papers (bequeathed to the Society), which document how in 1919, a group of women at the University of Colorado attempted to organize a women's engineering society. This group included Lou Alta Melton, Hilda Counts and Elsie Eaves. These young women wrote letters to engineering schools across the nation, asking for information on women engineering students and graduates.\n\nThey found 63 women engineering students at 20 universities, 43 of those at the University of Michigan alone. From a letter that Hazel Quick wrote to Hilda Counts, we know that the Michigan women had organized a group in 1914, called the T-Square Society, although no one was sure (even then) if it was a business, honorary, or social organization.\n\nMany negative responses were received from schools that did not admit women into their engineering programs. From the University of North Carolina, Thorndike Saville, Associate Professor of Sanitary Engineering wrote: \"I would state that we have not now, have never had, and do not expect to have in the near future, any women students registered in our engineering department.\"\n\nSome responses were supportive of women in engineering, but not of a separate society. Many of the women contacted as a result of the inquiries wrote about their support for such an organization. Besides the Hazel Quick letter from Michigan, there was a reply from Alice Goff, expressing her support of the idea of a society for women in engineering and architecture, \"Undoubtedly an organization of such a nature would be of great benefit to all members, especially to those just entering the profession.\"\n\nThough the Society of Women Engineers did not become a formal organization until 1950, its origins are in the late 1940s when shortages of men due to World War II provided the new opportunities for women to pursue employment in engineering. Female student groups at Drexel Institute of Technology in Philadelphia, Cooper Union and City College of New York in New York City, began forming local meetings and networking activities.\n\nOn the weekend of May 27–28, 1950, about fifty women representing the four original districts of the Society of Women Engineers; New York City, Philadelphia, Washington, D.C., and Boston, met for the first national meeting at The Cooper Union's Green Engineering Camp in northern New Jersey. During this first meeting, the society elected the first president of SWE, Dr. Beatrice A. Hicks. The first official annual meeting was held in 1951, in New York City.\n\nIt wasn't until the 1960s after Russia launched Sputnik and interest in technological research and development intensified that many engineering schools began admitting women. Membership in SWE doubled to 1,200 and SWE moved its headquarters to the United Engineering Center in New York City.\n\nAfter WWII women were discouraged from entering into engineering. During the war, their efforts were seen as a patriotic duty. After the war, women in engineering were seen as an abnormality.\n\nOver the next decade, an increasing number of young women chose engineering as a profession, but few were able to rise to management-level positions. SWE inaugurated a series of conferences (dubbed the Henniker Conferences after the meeting site in New Hampshire) on the status of women in engineering and in 1973, signed an agreement with the National Society of Professional Engineers in hopes of recruiting a larger percentage of working women and students to its ranks.\n\nAt the same time, SWE increasingly became involved in the spirit and activities of the larger women's movement. In 1972, a number of representatives from women's scientific and technical committees and societies (including SWE) met to form an alliance and discuss equity for women in science and engineering. This inaugural meeting eventually led to the formation of the Federation of Organizations of Professional Women (FOPW). In addition, SWE's Council resolved in 1973 to endorse ratification of the Equal Rights Amendment, and a few years later, resolved not to hold national conventions in non-ERA-ratified states. The Equal Rights Amendment was first proposed by Alice Paul in the 1920s, after women gained the right to vote, and still has not been ratified to this day. In 1973, SWE signed an agreement with the National Society of Professional Engineers to recruit more women engineers and students as members.\n\nBy 1982, the Society had swelled to 13,000 graduate and student members spread out in 250 sections across the country. The Council of Section Representatives, which in partnership with an Executive Committee had governed the Society since 1959, had become so large SWE adopted a regionalization plan designed to bring the leadership closer to the membership. Today, SWE has over 17,000 student, graduate, and corporate members, and continues its mission as a 501(c)(3) non-profit educational service organization.\n\nThe Society of Women Engineers organization exists today largely because the gender balance in Engineering does not proportionally reflect population breakdowns of men and women in the United States. Encouragement of female students and promotion of Engineering as a field of study for women is a necessary and fundamental function of the organization. Engineering and related fields are heavily male-dominated, in part because of gender socialization and artificially reinforced gender norms. Theories such as the seek to understand and balance how different science, math, and engineering fields tend to over- or under-represent different groups of people in this country.\n\nIts mission statement, adopted in 1986, is \"Stimulate women to achieve full potential in careers as engineers and leaders, expand the image of the engineering profession as a positive force in improving the quality of life, demonstrate the value of diversity.\" Anyone is welcome to join, male or female, of all backgrounds in effort to support diversity within the organization. SWE includes all STEM majors even though it is predominately engineering based.\n\nThe Society of Women Engineers, a not-for-profit organization, awards multiple scholarships each year to women in undergrad and graduate STEM degree programs across the world. About 233 scholarships were awarded for the 2017 school year amounting to $715,000. SWE's CEO and executive director Karen Horting stated that SWE \"could not have such a successful program without our corporate and foundation partners and generous individuals who support our scholarships, and our hope is to continue to grow the program and provide financial resources to those studying for a career in engineering and technology.\"\n\nSWE is also working to provide opportunities to help promote change for women in STEM fields. SWE organizes opportunities to network, further professional development, and recognize all of the accomplishments that women contribute to STEM Fields.\n\nEthical behavior shown by a member in both personal and professional lives. In SWE, integrity is seen as the \"Key to Success\"; someone who displays integrity is generally able to bring a group, team, or challenge together even if the odds are stacked against them.\n\nThe SWE environment works to create a welcoming environment for all members. This allows the members and organization as a whole to benefit from a diversity. Sarah Watzman the FY17 Collegiate Director attributes staying in engineering to her undergraduate chapter of SWE. During her first year of undergrad, senior members of the chapter encouraged her to stay involved. She ran for a student position for her sophomore year. Watzman explains; \"Having that encouragement and knowing that I had a leadership position in a really inclusive society of strong, determined engineering women that helped me stay in engineering.\"\n\nMutual Support focuses on creating networks both personally and professionally. An example of this is Kate Van Dellen, a SWE member since 2004, moved to a different state after graduating college and was able to make connections through the local SWE professional section.\n\nAll SWE members are expected to demonstrate professionalism in their school work and business. Elia Zanella, a SWE chapter president, says attending professional conference is a physical example of professional excellence. Members are always smiling and dressed \"Inspiring is what I think of when I think of SWE.\" \n\nTrust is important in creating a belief that someone is honest and reliable. Without trust between leaders and members it will be difficult to gain confidence in a groups abilities.\n\nSWE offers support at all levels, from K-12 outreach programs and collegiate sections to professional development in the workplace. Programs are in place to help collegiate and professional members interact with their local communities. \nThe Society of Women Engineers is organized at the local, regional, national, and international levels. SWE hosts annual We Local regional events across the world. These events connect members in all stages of their careers and hosts similar events to the larger annual conference. SWE hosts one annual conference in a different location each year. Over 11,000 members attend the three-day conference making it the largest event of its kind. The annual conference allows members to network with peers from around the world. This conference includes professional development workshops, inspirational speakers, and a career fair. If an individual or chapter cannot attend a conference in person they have the opportunity to participate virtually by watching a keynote speaker or workshop online. \n\nEvery year, SWE holds GEARS Day event in universities such as University of Pennsylvania to help high school girls understand and explore more about engineering possibly in their future careers. Different workshops are offered, such as Engineering as an Environment Consultant, Bio-pharmaceutical Manufacturing, etc. Students will spend a day with members of the SWE community to explore more about the engineering world.\n\n\n\n\n\nIn 1951 and only a year after the society was first established, the SWE began publishing the \"Journal of the Society of Women Engineers\" which included both technical articles and society news. In 1954, the journal was superseded by the \"SWE Newsletter\", a magazine format which focused primarily on SWE and industry news. In 1980, it was again renamed, this time to \"US Woman Engineer\". In 1993, the title was changed yet again to \"SWE\" and this remains their current periodical title with the subtitle 'magazine of the Society of Women Engineers'. The fifth volume of \"SWE\" was published in 2011 to celebrate the society’s 60th anniversary and to explore SWE's history in more depth using its archives.\n\nLocated at Wayne State University’s Walter P. Reuther Library in Detroit, Michigan, USA, the Society's archives were established in 1957 by the Archives Committee, who voluntarily collected and maintained the Society's records. In 1993, SWE designated the Walter P. Reuther Library as the official repository of its historical materials.\n\nLocated within the Carey C. Shuart Women's Archive and Research Collection, the Houston Area Section of the Society of Women Engineers contains correspondence, business and financial records, photographs, and publications of the organization.\n\n\n\n"}
{"id": "42896288", "url": "https://en.wikipedia.org/wiki?curid=42896288", "title": "Steam juicer", "text": "Steam juicer\n\nA steam juicer (steam extractor) is a household kitchen utensil for separating juice from berries, fruits, and some types of vegetables in a process called steam juice extraction that is primarily used for preserving harvests faster than they can be consumed when fresh.\n\nIt makes use of steam to transfer gentle heat directly to the berries that are supported above a boiling water pot in a perforated basket. The juice is collected in the middle section and let out with a small hose while still hot and sterile directly into clean bottles for long term unrefrigerated storage. The quality of the juice is high with little solids due to the non-mechanical extraction process.\n\nThe units are generally made in a three-tier configuration that somewhat resembles a huge double boiler. The bottom section is a simple pot placed on a kitchen hot plate or gas range to boil water to generate the steam for the process. The next section is the juice collection container with an opening in the middle to pass the steam through. It has a hose fitting that is used to drain it directly into storage bottles. The top section is a perforated basket (with a loose-fitting lid) to hold the berries while allowing the steam to reach the complete batch but holding the berries (pulp, skins and pips) out of the extracted juice.\n\nThe process is convenient and as safe as any other stove-top cooking. The steam is not held under pressure and the juice is delivered with a rubber hose that requires minimal extra operations, occasionally a funnel with a strainer may be used to make the bottling more convenient. As usual one needs to be careful of scalding hot juice and the possibility of pulling over the tall juicer.\n\nWhen loading the charge of berries into the strainer section sugar may be added (in alternating layers) the amount depending on the projected use, sugar also acts as a preservative. Other additives may be pectin to prepare jellies, ascorbic acid to improve shelf life and other ingredients that will dissolve and mix with the juice as it is extracted. To get a consistent batch of juice the whole charge is allowed to extract and then bottled when complete; draining of the juice as it extracts will result in different compositions from start to end of the batch. Long dwell times in juicers made from aluminium when preparing acid juices is not recommended.\n\nIncreasing the steam flow-rate may improve the juice extraction rate.\n\nThis was a popular type of harvest juicer as it prepared the juice for immediate bottling in good sized batches. There is generally no need to strain or boil the juice after mechanical juicing and it could be operated over a wood stove in the rural country sides.\n\nThey are still popular in Finland and elsewhere and have been marketed under various names (Mehu Maija, MehuLiisa, MehuMatti, Krona) by various manufacturers (OY Alu AB, Hackmann, Opa, Victorio, Lehmann, Norpro) over the years. The same style of juicer has also been historically made in Germany and Portugal and now in China. Some of the modern units incorporate an electrical heating element in the steam generating pot that makes for very convenient operation away from the kitchen where small spills and hot stoves are not a problem.\n\nThe first units were probably made from aluminum due to the ease of fabrication of the complex shapes by metal spinning. Some have been made from enamelled steel (a German trend) and the modern bare metal units have moved to stainless steel alloys to prevent aluminium from leaching into the juice at the time of preparation.\n\nThe steam juicer is eminently well suited to preparing berry juices at the time of harvest for long term preservation due to the pasteurization that results from the extraction process.\n\nThey are also used to make jellies and cook vegetables in the tradition of a normal steamer or blanching for freezer storage.\n\nThey have been used as emergency distillation units for preparing clean water and medicinal alcohol.\n\n"}
{"id": "37652061", "url": "https://en.wikipedia.org/wiki?curid=37652061", "title": "Stochastic neural analog reinforcement calculator", "text": "Stochastic neural analog reinforcement calculator\n\nSNARC (Stochastic Neural Analog Reinforcement Calculator) is a neural net machine designed by Marvin Lee Minsky. George Miller gathered the funding for the project from the Air Force Office of Scientific Research in the summer of 1951. At the time, one of Minsky's graduate students at Princeton, Dean Edmonds, volunteered that he was good with electronics and therefore Minsky brought him onto the project.\n\nThe machine itself is a randomly connected network of approximately 40 Hebb synapses. These synapses each have a memory that holds the probability that signal comes in one input and another signal will come out of the output. There is a probability knob that goes from 0 to 1 that shows this probability of the signals propagating. If the probability signal gets through, a capacitor remembers this function and engages a \"clutch\". At this point, the operator will press a button to give reward to the machine. At this point, a large motor starts and there is a chain that goes to all 40 synapse machines, checking if the clutch is engaged or not. As the capacitor can only \"remember\" for a certain amount of time, the chain only catches the most recent updates of the probabilities.\n\nThis machine is considered one of the first pioneering attempts at the field of artificial intelligence. Minsky went on to be a founding member of MIT's Project MAC, which split to become the MIT Laboratory for Computer Science and the MIT Artificial Intelligence Lab, and is now the MIT Computer Science and Artificial Intelligence Laboratory. In 1985 Minsky became a founding member of the MIT Media Laboratory. \n\n"}
{"id": "32845520", "url": "https://en.wikipedia.org/wiki?curid=32845520", "title": "Stripe (company)", "text": "Stripe (company)\n\nStripe is a technology company. Its software allows individuals and businesses to receive payments over the Internet. Stripe provides the technical, fraud prevention, and banking infrastructure required to operate on-line payment systems.\n\nIrish entrepreneurs John and Patrick Collison founded Stripe in 2010. In June 2010, Stripe received seed funding from Y Combinator, a start-up accelerator. In May 2011, Stripe received a $2 million investment from venture capitalists Peter Thiel, Sequoia Capital, and Andreessen Horowitz. In February 2012, Stripe received an $18 million Series A investment led by Sequoia Capital at a $100 million valuation. Stripe launched publicly in September 2011 after an extensive private beta. Less than a year after its public launch, Stripe received a $20 million Series B investment. In March 2013, Stripe acquired chat and task-management application Kick-off.\n\nIn 2016, Stripe was valued at over $9 billion when it raised a $150 million round. In its next funding round in September 2018, Stripe received a $20 billion valuation while raising $245 million.\n\nStripe provides APIs that web developers can use to integrate payment processing into their websites and mobile applications. In April 2018, the company released anti-fraud tools that run alongside payment APIs to block fraudulent transactions.\n\nThe company expanded its services to include a billing product for online businesses. The new service operates within the Stripe platform, allowing businesses to manage subscription recurring revenue and invoicing.\n\nOn February 24, 2016 the company launched the Atlas platform that allows startups to incorporate more easily in the U.S. The platform originally launched as invite-only. In March 2016, Cuba was added to the list of countries covered under the program. Atlas was relaunched with improvements the following year. As of April 2017, Atlas had signed more than 200 international startups.\n\nOn April 30, 2018 the company announced an expansion to Atlas. The upgrades to Atlas included the ability to be used to set up Delaware-based limited liability companies.\n\nIn July 2018, Stripe began a platform for companies to issue Mastercard and Visa credit cards, available in private beta.\n\nOn September 17th, 2018 Stripe announced a new point of sale solution called Terminal, initially launched as an invite-only beta. The service offers physical credit card readers designed to work with Stripe. Terminal also includes detailed documentation along with SDKs for iOS, Javascript, and one for Android is in development.\n\n"}
{"id": "1264197", "url": "https://en.wikipedia.org/wiki?curid=1264197", "title": "Swather", "text": "Swather\n\nA swather, or windrower, is a farm implement that cuts hay or small grain crops and forms them into a windrow. \"Swather\" is predominantly the North American term for these machines. In Australia and other parts of the world, they are called \"windrowers\". They aid harvesting by speeding up the process of drying the crop down to a moisture content suitable for harvesting and storage. \n\nA swather may be self-propelled via an internal combustion engine, or may be drawn by a tractor and powered through a power take-off shaft. A swather uses a sickle bar (see mower) or cutting discs to cut the stems of the crop. A reel helps the cut crop fall neatly onto a canvas or auger conveyor which moves it and deposits it into a windrow, with all stems oriented in the same direction. Horizontal rollers behind the cutters may be used to crimp the stems of the crop to decrease drying time. The mown strip left behind is the swathe. For hay crops, this is the same basic sequence as also done in other ways, such as hand scything, cradling, and swathing; mowing with a mower and then raking with a hay rake; or mowing and conditioning with a mower-conditioner (the latter two sometimes also with additional tedding). But for grain crops, as combines replaced threshing machines, the swather introduced a new step in the harvesting process to provide for the drying time that binding formerly provided for. Binding allowed subsequent temporary storage of the cut plants before threshing (either in stooks or in a barn), during which time they dried out. Combining removes the binding step and the temporary pre-threshing storage step from the harvesting process, but in many regions of the world, combining without a preceding swathing step, that is, reaping and threshing a standing plant all at once, produces threshed grain that is too high in moisture for reliable storage. Getting around that problem drove the development both of swathing practice for grain crops (1920s–1930s) and of widespread industrial-scale grain drying (especially after World War II). Swathing (windrowing) is more common in the northern United States and Canada because the curing time for grain crops is reduced by cutting the plant stems. In regions with longer growing seasons, grain crops are usually left standing and harvested directly by combines. The difference is that in the latter regions the grain can reliably reach low enough moisture while still on the standing plant, whereas in the regions less conducive to such drying, swathing provides the extra help that is needed to reach low enough moisture. \n\nRegardless of which harvesting methods are used, threshed grain can receive additional drying, or not, as needed for storage; the ideal in efficiency (that is, in energy efficiency, person-hour and machine-hour productivity, and thus cost-effectiveness) is to produce grain that is dry enough as-is right out of the combine. But reality does not always match this ideal, in which case a damp harvest followed by drying is much better than no harvest. For farmers who do not have their own grain drying equipment, usually the grain elevator company that buys the grain will do the drying and pay its expense by docking some percentage points from the price paid for the grain. \n\nThe Swather is the mascot of sports teams at Hesston High School in Hesston, Kansas, USA. Hesston is the home to AGCO Corporation's large swather and combine harvester manufacturing plants.\n\n"}
{"id": "5881683", "url": "https://en.wikipedia.org/wiki?curid=5881683", "title": "Tap converter", "text": "Tap converter\n\nThe tap converter is a variation on the cycloconverter, invented in 1981 by New York City electrical engineer Melvin Sandler and significantly functionally enhanced in 1982 through 1984 by graduate students Mariusz Wrzesniewski, Bruce David Wilner, and Eddie Fung. Whereas the cycloconverter switches among a variety of staggered input phases to piece together an extremely jagged output signal, the tap converter synthesizes a much smoother signal by switching among a variety of (obviously synchronized) transformer output taps. \n\nBoth linear spacing and power-of-two-style Vernier spacing can be employed in establishing the tap positions, \"e.g.\", a four-tap transformer can provide taps at 0.25, 0.5, 0.75, 1.0 (linear) or 0.0625, 0.125, 0.25, and 0.5 (Vernier). (The limitations of the Vernier—in this case, that the maximum obtainable amplitude is 0.9375—are less discernible as more taps are added.)\n\nBy employing a Scott transformer input connection, in order to provide a quadrature phase, an even smoother output waveform can be obtained.\n\nPrototypes of the device were constructed and field-tested under a variety of conditions—nominally as a variable-speed constant-frequency (VSCF) power source for military aircraft—and ornate computer models were constructed for exploring more ornery considerations, such as flux leakage, hysteresis, and practical thyristor characteristics. All of this work was performed at New York's Cooper Union for the Advancement of Science and Art.\n\nAs of 2007, the tap converter remains uncommercialized but is used in several military applications due to the minimal output harmonics.\n\n"}
{"id": "36688638", "url": "https://en.wikipedia.org/wiki?curid=36688638", "title": "Thin walled beams", "text": "Thin walled beams\n\nA thin walled beam is a very useful type of beam (structure). The cross section of \"thin walled beams\" is made up from thin panels connected among themselves to create closed or open cross sections of a beam (structure). Typical closed sections include round, square, and rectangular tubes. Open sections include I-beams, T-beams, L-beams, and so on. Thin walled beams exist because their bending stiffness per unit cross sectional area is much higher than that for solid cross sections such a rod or bar. In this way, stiff beams can be achieved with minimum weight. Thin walled beams are particularly useful when the material is a composite laminate. For metallic structures, the theory is fully developed in. Pioneer work on composite laminates thin walled beams was done by Librescu.\n"}
{"id": "2657250", "url": "https://en.wikipedia.org/wiki?curid=2657250", "title": "Wrecking ball", "text": "Wrecking ball\n\nA wrecking ball is a heavy steel ball, usually hung from a crane, that is used for demolishing large buildings. It was most commonly in use during the 1950s and 1960s. Several wrecking companies claim to have invented the wrecking ball. An early documented use was in the breaking up of the in 1888–1889, by Henry Bath and Co, at Rock Ferry on the River Mersey.\n\nIn 1999, the wrecking ball was described as \"one of the most common forms of large-scale coarse demolition.\" Although the wrecking ball is still the most efficient way to raze a concrete frame structure, its use is decreasing. With the invention of hydraulic excavators and other machinery, the wrecking ball has become less common at demolition sites as its working efficiency is less than that of high reach excavators. \n\nModern wrecking balls have had a slight re-shaping, with the metal sphere changed into a pear shape with a portion of the top cut off. This shape allows the ball to be more easily pulled back through a roof or concrete slab after it has broken through.\n\nWrecking balls range from about to around . The ball is made from forged steel, which means the steel is not cast into a mold in a molten state. It is formed under very high pressure while the steel is red hot (soft but not molten) to compress and to strengthen it.\n\nTo demolish roofs and other horizontal spans, the ball is typically suspended by a length of steel chain attached to the lifting hook of a crane boom above the structure, the rope drum clutch is released and the ball is allowed to free-fall onto the structure. To demolish walls the ball is suspended at the desired height from a crane boom and a secondary steel rope pulls the ball toward the crane cab. The lateral rope drum clutch is then released and the ball swings as a pendulum to strike the structure. Another method for lateral demolition is to pivot the crane boom to accelerate the ball toward the target. This is repeated as needed until the structure is broken down into debris that can easily be loaded and hauled away. The demolition action is carried out entirely through the kinetic energy of the ball.\n\nDemolition work has been carried out using a wrecking ball suspended from a Kaman K-MAX helicopter.\n\nThe same mechanism is applied to quarrying rock where an excavator lifts and releases a loose ball (called a drop ball) onto large rocks to reduce them to manageable size.\n\nThe advancement of technology led to the development and use of blasting charges, safer than dynamite and more efficient or practical than wrecking balls, to destroy buildings. The most common use of blasting charges is to implode a building, thus limiting collateral damage; see demolition. Wrecking balls are more likely to cause collateral damage, because it is difficult to completely control the swing of the ball.\n\nHowever, wrecking balls are still used when other demolition methods may not be possible due to local environmental issues or asbestos/lead building content.\n\n"}
