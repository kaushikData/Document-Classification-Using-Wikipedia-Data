{"id": "12503315", "url": "https://en.wikipedia.org/wiki?curid=12503315", "title": "API oil–water separator", "text": "API oil–water separator\n\nAn API oil–water separator is a device designed to separate gross amounts of oil and suspended solids from the wastewater effluents of oil refineries, petrochemical plants, chemical plants, natural gas processing plants and other industrial oily water sources. The name is derived from the fact that such separators are designed according to standards published by the American Petroleum Institute (API).\n\nThe API separator is a gravity separation device designed using Stokes' law principles that define the rise velocity of oil droplets based on their density, size and water properties. The design of the separator is based on the specific gravity difference between the oil and the wastewater because that difference is much smaller than the specific gravity difference between the suspended solids and water. Based on that design criterion, most of the suspended solids will settle to the bottom of the separator as a sediment layer, the oil will rise to top of the separator, and the wastewater will be the middle layer between the oil on top and the solids on the bottom. The API Design Standards, when correctly applied, make adjustments to the geometry, design and size of the separator beyond simple Stokes Law principles. This includes allowances for water flow entrance and exit turbulence losses as well as other factors. API Specification 421 requires a minimum length to width ratio of 5:1 and minimum depth-to-width ratio of 0.3:0.5.\n\nTypically in operation of API separators the oil layer, which may contain entrained water and attached suspended solids, is continually skimmed off. This removed oily layer may be re-processing to recover valuable products, or disposed of. The heavier bottom sediment layer is removed by a chain and flight scraper (or similar device) and a sludge pump.\n\nAPI design separators, and similar gravity tanks, are not intended to be effective when any of the following conditions apply to the feed conditions: \nAccording to Stokes' Law, heavier oils require more retention time. In many cases where refineries have switched to heavier crude slates, the API separator’s efficiency has declined.\n\nBecause of performance limitations the water discharged from API type separators usually requires several further processing stages before the treated water can be discharged or reused. Further water treatment is designed to remove oil droplets smaller than 150 micron, dissolved materials and hydrocarbons, heavier oils or other contaminants not removed by the API. Secondary treatment technologies include dissolved air flotation (DAF), Anaerobic and Aerobic biological treatment, Parallel Plate Separators, Hydrocyclone, Walnut Shell Filters and Media filters.\n\nPlate separators, or Coalescing Plate Separators are similar to API separators, in that they are based on Stokes Law principles, but include inclined plate assemblies (also known as parallel packs). The underside of each parallel plate provides more surface for suspended oil droplets to coalesce into larger globules. Coalescing plate separators may not be effective in situation where water chemicals or suspended solids restrict or prevent oil droplets coalesce. In operation it is intended that sediment will slide down the topside of each parallel plate, however in many practical situations the sediment can adhere to the plates requiring periodic removal and cleaning. Such separators still depend upon the specific gravity between the suspended oil and the water. However, the parallel plates can enhance the degree of oil-water separation for oil droplets above 50 micron in size. Alternatively parallel plate separators are added to the design of API Separators and require less space than a conventional API separator to achieve a similar degree of separation.\n\nThe API separator was developed by the API and the Rex Chain Belt Company (now Evoqua). The first API separator was installed in 1933 at the Atlantic Refining Company (ARCO) refinery in Philadelphia. Since that time, virtually all of the refineries worldwide have installed API separators as a first primary stage of their oily wastewater treatment plants. The majority of those refineries installed the API separators using the original design based on the specific gravity difference between oil and water. However, many refineries now use plastic parallel plate packing to enhance the gravity separation. Today regulations often require API separators with fixed or floating covers for volatile organic compound (VOC) control. Also, most API separators must be above ground for spill detection.\n\nThere are other applications requiring oil-water separation. For example:\n\n\n\n"}
{"id": "47630505", "url": "https://en.wikipedia.org/wiki?curid=47630505", "title": "Abbot's Kitchen, Glastonbury", "text": "Abbot's Kitchen, Glastonbury\n\nThe Abbot's Kitchen is a mediaeval octagonal building that served as the kitchen at Glastonbury Abbey in Glastonbury, Somerset, England. It is a Grade I listed building. The abbot's kitchen has been described as \"one of the best preserved medieval kitchens in Europe\".\n\nThe stone-built construction dates from the 14th century and is one of a very few surviving mediaeval kitchens in the world. \n\nHistorically, the Abbot of Glastonbury lived well, as demonstrated by the abbot's kitchen, with four large fireplaces at its corners. The kitchen was part of the opulent abbot's house, begun under Abbot John de Breynton (1334–42). It is one of the best preserved medieval kitchens in Europe and the only substantial monastic building surviving at Glastonbury Abbey. The abbot's kitchen has been the only building at Glastonbury Abbey to survive intact. Later it was used as a Quaker meeting house.\n\nThe building is supported by curved buttresses on each side leading up to a cornice with grotesque gargoyles. Inside are four large arched fireplaces with smoke outlets above them, with another outlet in the centre of the pyramidal roof. The building is designed so that hot air from the cooking fires would have risen up to the top of the building and escaped, whilst cooler air came from openings lower down and sunk into the kitchen, cooling it.\n\nThe kitchen was attached to the high abbot's hall, although only one small section of its wall remains.\n\nThe architect Augustus Pugin surveyed and recorded the building in the 1830s. The Abbot's Kitchen was again surveyed and conserved in 2013, reopening in 2014.\n\n"}
{"id": "2279599", "url": "https://en.wikipedia.org/wiki?curid=2279599", "title": "Almadraba", "text": "Almadraba\n\nAlmadraba in Spanish () is a word of Al-Andalus Arabic origin \"almaḍraba\" : 'a place to strike' < Arabic root 'to strike, hit'. It is an elaborate and age-old Phoenician technique for trapping and catching Atlantic bluefin tuna that was learned and taken to areas such as Iberia during Iberia's Islamic period.\n\nThe technique is to trap and catch the tuna when they are crossing between the Atlantic Ocean to the Mediterranean during February to July, on their way to spawn and\nuntil recently, on its return journey, (\"al revés\") when they come back into the Atlantic Ocean, they also Bycatch: bullet tuna (Auxis rochei), little tunny (Euthynnus\nalletteratus), Atlantic bonito (Sarda sarda), bigeye tuna (Thunnus obesus) and swordfish\n(Xiphias gladius).\n\nIt is a traditional form of tuna fishing or netting fence to catch tuna that is carried out in Italy, Morocco,\nPortugal (Mainly in the Algarve) and Spain (Mainly Andalusia, Murcia and Valencia).\n\nA similar technique exists in Sicily known as \"mattanza\" (a borrowing from the Spanish word \"matanza\", meaning slaughter), introduced either by the Moors during Sicily's own Islamic period or by the Spanish afterwards.\n\n\n"}
{"id": "9227330", "url": "https://en.wikipedia.org/wiki?curid=9227330", "title": "American Society of Animal Science", "text": "American Society of Animal Science\n\nThe American Society of Animal Science (ASAS) is a non-profit professional organization for the advancement of livestock, companion animals, exotic animals and meat science. Founded in 1908, ASAS is headquartered in Champaign, Illinois.\n\nASAS members are involved in university research, education, and extension as well as in the feed, pharmaceutical, and other animal-related industries. Disciplines include nutrition, reproductive physiology, genetics, and behavior of food-producing animals and processing of meat-based products, including beef, pork, and veal.\n\nOfficial ASAS Mission: \"The American Society of Animal Science is a membership society that supports the careers of scientists and animal producers in the United States and internationally. The American Society of Animal Science fosters the discovery, sharing and application of scientific knowledge concerning the responsible use of animals to enhance human life and well-being.\"\n\nOrganizing ASAS (originally called the American Society of Animal Nutrition) began on July 28, 1908, at Cornell University in Ithaca, New York. A committee of animal nutritionists decided to present a plan for the new society during the International Livestock Exposition in Chicago that fall. When the society first officially gathered on November 26, 1908, 33 charter members represented 17 state experiment stations, the U.S. Department of Agriculture and Canada. The goals of the new society were: \"(1) to improve the quality of investigation in animal nutrition, (2) to promote more systematic and better correlated study of feeding problems, and (3) to facilitate personal interaction between investigators in this field.\" During the first year, the society had 100 members join.\n\nAt the society business meeting in 1912, the members made plans to broaden the membership base. On November 30, 1915, members changed the society name from the American Society of Animal Nutrition to the American Society of Animal Production. Members passed an amendment to the constitution to include members interested in teaching, breeding, and management investigations as well as nutritionists.\n\nBy the 50th anniversary year of the society, in 1958, there were 1829 members. A second name change was approved in 1961, when the official name became the American Society of Animal Science.\n\nIn 2008, ASAS celebrated its centennial. As of 2012, the society has more than 5,000 members. Members include animal scientists in academia, the animal industry, and state and federal agencies. The society also has student members and members working in animal and food production.\n\nSince 1998, ASAS been holding their annual meetings in convention centers jointly with the American Dairy Science Association (ASDA). ASAS holds yearly meetings for its Western Midwest, Southern and Northeast sections.\n\nASAS has also held joint meetings with the Asociación Argentina de Producción Animal (AAPA), the Asociación Mexicana de Producción Animal (AMPA), the Canadian Society of Animal Science (CSAS), the Chinese Association of Animal Science and Veterinary Medicine (CAAV), and the Poultry Science Association (PSA). ASAS regularly supports speaker exchanges with the European Federation of Animal Science (EAAP).\n\nIn 2012, ASAS began hosting a National Academic Quadrathlon competition at its annual meeting. The Academic Quadrathlon is an undergraduate student competition featuring winning teams from each of the four ASAS sections.\n\nASAS's scientific journal is the \"Journal of Animal Science\" (\"JAS\"), a monthly publication established in 1923 that includes supplemental information regarding abstracts and electronic data.. ASAS also co-publishes a quarterly review magazine called Animal Frontiers. Animal Frontiers features invited articles focused on a certain theme for each issue. In 2012, themes included \"Animal Selection,\" \"Animal Production and Water,\" and \"The Science of Animal Welfare.\"\n\nASAS also has an online presence. The society publishes an online newsletter called Taking Stock. Graduate student members contribute to a blog called the Graduate BULLetin (\"No bull, just the latest society news and career information\").\n\nIn 2012, ASAS launched a site called AnimalSmart.org. This site presents articles and videos about animal science to the public.\n\nASAS also supports the USDA National Agricultural Library's Animal Science Image Gallery.\n"}
{"id": "1060354", "url": "https://en.wikipedia.org/wiki?curid=1060354", "title": "American Thermoplastic Company", "text": "American Thermoplastic Company\n\nAmerican Thermoplastic Company, or ATC, is an American manufacturer of binders and other plastics products.\n\nATC was founded in 1954 in Pittsburgh, PA.\n\n"}
{"id": "35981916", "url": "https://en.wikipedia.org/wiki?curid=35981916", "title": "Architectural technology", "text": "Architectural technology\n\nArchitectural technology, or building technology, is the application of technology to the design of buildings. It is a component of architecture and building engineering and is sometimes viewed as a distinct discipline or sub-category. New materials and technologies generated new design challenges and construction methods throughout the evolution of building, especially since the advent of industrialisation in the 19th century. Architectural technology is related to the different elements of a building and their interactions, and is closely aligned with advances in building science.\n\nArchitectural technology can be summarised as the \"technical design and expertise used in the application and integration of construction technologies in the building design process.\" or as \"The ability to analyse, synthesise and evaluate building design factors in order to produce efficient and effective technical design solutions which satisfy performance, production and procurement criteria.\"\n\nMany specialists and professionals, consider Vitruvius's theories as the foundations of architectural technology. Vitruvius's attempt to classify building types, styles, materials and construction methods influenced the creation of many disciplines such as civil engineering, structural engineering, architectural technology and other practices which, now and since the 19th century, form a conceptual framework for architectural design.\n\nIn his published research, Stephen Emmitt explains that in our modern society, \"The relationship between building technology and design can be traced back to the Enlightenment and the industrial revolution, a period when advances in technology and science were seen as the way forward, and times of solid faith in progress... As technologies multiply in number and complexity the building profession started to fragment\".\n\nArchitectural technology is a discipline that spans architecture, building science and engineering. It is practiced by architects, architectural technologists, structural engineers, architectural / building engineers and others who develop the design / concept into a buildable reality. Specialist manufacturers who develop products used to construct buildings, are also involved in the discipline.\n\nPaul Nuttall director at ARUP compares the role of the building engineer to the role of the technical architect and architect. The practice of architectural technology cannot be limited to technology itself; it also relates to the space, the building, the light, the environment served and created by the design. The professional in charge of the technical design leads specialist engineers to provide the best technical and engineered solutions for the project.\n\nIn practice, architectural technology is developed, understood and integrated into a building by producing architectural drawings and schedules. Computer technology is now used on all but the simplest building types: in the twentieth century the use of computer aided design (CAD) became mainstream, allowing for highly accurate drawings that can be shared electronically, so that for example the architectural plans can be used as the basis for designing electrical and air handling services. As the design develops, that information can be shared with the whole design team. That process is currently taken to a logical conclusion with the widespread use of Building Information Modeling (BIM), which uses a three dimensional model of the building, created with input from all the disciplines to build up an integrated design.\n\nArchitectural technology is informed by both practical constraints, and building regulations, as well as standards relating to safety, environmental performance, fire resistance, etc.\n\nUntil the twentieth century, the materials used for building were limited to brick, stone, timber and steel to form structures, slate and tiles for roof coverings, lead and sometimes copper for waterproofing details and decorative roofing effects. The Romans used concrete, but it was virtually unknown as a building material until the invention of reinforced concrete in 1849. Modern construction is much more complex, with walls, floors and roofs all built up from many elements to include structure, insulation and waterproofing often as separate layers or elements.\n\n"}
{"id": "26892981", "url": "https://en.wikipedia.org/wiki?curid=26892981", "title": "Arri Alexa", "text": "Arri Alexa\n\nThe Arri Alexa (stylised as ΛLEXΛ) is a digital motion picture camera system made by Arri first introduced in April 2010. The camera was Arri's first major transition into digital cinematography after smaller previous efforts such as the Arriflex D-20 and D-21. \n\nAlexa cameras are designed for use in high budget feature films, television shows, and commercials, and compete with Red Digital Cinema's line of high end cameras such as the Epic Dragon and Weapon Dragon. Alexa uses the ALEV series of image sensors manufactured by ON Semiconductor.\n\nThe camera has several methods of recording, including SxS cards, CFast 2.0 cards and SXR Capture Drives at resolutions up to 2880 x 2160 pixels in either Rec. 709 or Log-C to ProRes or ARRIRAW codecs. Alexa camera owners can purchase additional software licenses that will unlock different capabilities of the Alexa Camera including High Speed 120fps recording, DNxHD codec and 4:3 \"Open Gate\" Mode for anamorphic lenses.\n\nIt features modularity, PL mount lenses, a Super 35 sized CMOS sensor shooting up to 2880×2160 resolution and supports uncompressed video or proprietary raw (ARRIRAW) data.\n\nThe price of the camera depends on model and accessories; as an example, in 2015 an Arri Alexa XT cost approximately $66,000-100,000, depending on accessories included.\n\nThe range of Alexa models has expanded over time:\n\nThe first camera of the Alexa product family. The ARRI ALEXA’s CMOS Super-35mm sensor is rated at 2.8K and ISO 800. That sensitivity allows the camera to see a full seven stops of over exposure and another seven stops of underexposure. To take advantage of this, ARRI offers both industry-standard REC709 HD video output as well as the Log-C mode that shows the entire range of the chip’s sensitivity, allowing for an extreme range of color correction options in post.\n\nThe ALEXA Plus added integrated wireless remote control, the ARRI Lens Data System (LDS), additional outputs, lens synchronization for 3D, and built-in position and motion sensors.\n\nThe ALEXA Plus added integrated wireless remote control, the ARRI Lens Data System (LDS), additional outputs, lens synchronization for 3D, and built-in position and motion sensors and a 4:3 sensor making it ideal for anamorphic cinematography.\n\nThe Alexa M had its imaging and processing unit broken down in two parts to be small, compact and lightweight for 3D rigs and other uses where size is a concern.\n\nThe Alexa Studio features an optical viewfinder, mechanical shutter, and a 4:3 sensor for anamorphic cinematography.\n\nIn February 2013, the range was renewed as Alexa XT (XT standing for extended technology). This range is upgraded versions of the original Alexa cameras, which are equipped with a so-called XR module, which replaces the SxS module on the cameras, and allows direct raw recording without the need for an external recorder. This module records on dedicated SSD drives. Further improvements are an internal ND filter unit, a 4:3 sensor and a quieter cooling fan. The range accordingly comprises the Alexa, the Alexa XT, the Alexa XT M, the Alexa XT Plus, the Alexa XT Studio, and the Alexa Fiber Remote. Existing cameras can be upgraded with the XR module for internal raw recording.\n\nOn 21 September 2014 at the Cinec convention in Munich, Arri announced the Alexa 65, a 6k 65mm digital cinema camera. As with cameras from competitor Panavision, the Alexa 65 camera was available by rental only, provided through the ARRI Rental Group. The Alexa 65 uses the A3X sensor, which has a maximum recordable resolution of 6560x3100.\n\nThe first production to use the camera was \"\", which was used to shoot the underwater sequence, and around forty percent of \"The Revenant\".\n\nIn May 2015, Marvel Studios announced that \"\" and its untitled sequel will be shot entirely with the brand new camera, marking the first time a narrative feature film was shot entirely with a customized version of Alexa 65 camera, Alexa IMAX.\n\nOn 24 February 2015, Arri announced the Alexa Mini. It has the same sensor as the other Alexa cameras. It features in-camera recording to CFast 2.0 cards, 200 FPS and 4K UHD in-camera upscaling.\n\nOn 18 March 2015, Arri announced the SXT line of Arri Alexa cameras which will support in-camera upscaling of Apple ProRes to 4K resolution and Rec. 2020 color space. Arri also announced the SXR module which can upgrade XT, XT Plus, and XT Studio cameras with the SXT features.\n\nOn 2 February 2018, Arri announced the Alexa LF at the BSC Expo. It's a true 4K Large Format Camera. The ALEXA LF’s A2X sensor is based on two vertical ALEV-III sensors, which are stitched together to create a seamless large format image. This is the same principle of how they created the ALEXA 65, which uses three Alexa sensors that are arranged vertically. The ALEXA LF can record in Open Gate in a resolution of 4448 x 3096.\n\nThe Alexa's ALEV III image sensor has 3392×2200 effective pixels used for generating an image, 2880×2160 pixels are generally used for recording on the Alexa Studio and M in 4:3 mode, and 2880×1620 pixels are used for recording on the regular Alexa and other models in 16:9 mode, the rest of the sensor is used for lookaround in the viewfinder. Alternately the full sensor resolution may be employed in 'Open Gate' mode for resolution demanding situations.\n\nThe Alexa 65 uses the A3X sensor, which has a 54.12 mm x 25.59 mm active imaging area. It provides up to 6560x3102 'Open Gate' maximum recordable resolution.\n\nThe Arri Alexa can record to 1920×1080 ProRes 422 or ProRes 4444 on SxS Cards or 2880×1620 ARRIRAW to external recording devices. The Arri Alexa Firmware 7 increases the resolution on the SxS cards to 2k ProRes 4444 (previously 1080p)\n\nArriRaw is a raw codec similar to CinemaDNG that contains unaltered Bayer sensor information, the data stream from the camera can be recorded via T-link with certified recorders like those from Codex Digital or Cineflow.\n\nThe ArriRaw format (along with the other recordable formats) contains static and dynamic metadata. These are stored in the header of the file and can be extracted with the free web tool metavisor or with the application Meta Extract provided by Arri. Of particular importance for visual effects are the lens metadata, which are stored only when Arri's lens data system (LDS) is supported by the lens used.\n\nAccording to cinematographer Roger Deakins, the Alexa's tonal range, color space and latitude exceed the capabilities of film. \"This camera has brought us to a point where digital is simply better\", says Deakins. Deakins used the camera to shoot the James Bond film \"Skyfall\", \"Unbroken\", \"Sicario\", \"Prisoners\" and the Academy Award for Best Cinematography winner \"Blade Runner 2049\".\n\nDue to the camera's simplicity of use and high image quality, several network television shows have been shot with the Alexa.\n\nSince its introduction, five movies shot on Alexa (\"Argo\", \"Birdman\", \"Spotlight\", \"Moonlight\", and \"The Shape of Water\") won an Academy Award for Best Picture. Also, movies shot on Alexa won Academy Award for Best Cinematography six times, including five in a row between 2011 and 2015, for \"Hugo\", \"Life of Pi\", \"Gravity\", \"Birdman\", \"The Revenant\", and \"Blade Runner 2049\".\n\n\n"}
{"id": "37447215", "url": "https://en.wikipedia.org/wiki?curid=37447215", "title": "Assaad W. Razzouk", "text": "Assaad W. Razzouk\n\nAssaad Wajdi Razzouk (born in 1964 in Beirut, Lebanon) is a Lebanese-British clean energy entrepreneur, investor and art gallery owner.\n\nAssaad Razzouk is the Group Chief Executive Officer of Sindicatum Sustainable Resources, which he co-founded in London in 2005. Sindicatum is an award-winning developer, owner and operator of clean energy projects worldwide and a producer of sustainable resources from natural products and waste.\n\nAssaad Razzouk is also the founder of South East Asia's first Middle Eastern contemporary art gallery, Sana Gallery in Singapore.; is affiliated with Washington, D.C.’s Middle East Institute as an Expert at the Middle East – Asia Project; serves on the Advisory Board of the Hong Kong-based Association for Sustainable & Responsible Investment in Asia (ASrIA), is a Board Member of the Climate Markets & Investment Association and is an independent Board Member of Cedrus Invest Bank s.a.l.\n\nFrom 1993 to 2002, Assaad Razzouk was an investment banker at Nomura International plc in London, where he was successively Head of the Middle East Group (1993-1997), Head of Corporate Finance – Emerging Markets (1997-1999), Head of Corporate Finance – Financial Institutions, Communications and Technology (1999-2001) and Deputy Head, Global Corporate Finance (2001-2002).\n\nRazzouk is a graduate of Syracuse University (\"summa cum laude\") and holds an MBA from Columbia University in New York.\n\nAssaad Razzouk won the Association for Sustainable & Responsible Investment in Asia (ASrIA)'s \"Most Progressive Corporate Leader\" award in 2011. \nRazzouk was also named among the Top 50 Low-Carbon Pioneers by CNBC Business in June 2007 and among the Top 600 Most Powerful People in Finance by Global Finance in September 1998. Assaad Razzouk was chosen among the world's 20 most influential CEOs on Twitter by INSEAD in May 2016, a list also including leaders such as Tim Cook, Bill Gates, Elon Musk and Richard Branson.\n\nUnder Razzouk's leadership, Sindicatum Sustainable Resources Group won the 2013 Commodity Business Awards for Excellence in Emission Markets and a Special Commendation for Excellence in Market Policy and Advisory. In 2012, Sindicatum won the Energy Institute ‘Energy Excellence’ Award for its Duerping Project in China in 2012. Sindicatum's Duerping Project was designed to optimise energy recovery from waste gas extracted from a coal mine that was previously vented to the atmosphere. Sindicatum Sustainable Resources Group also won the 2012 Commodity Business Award for Excellence in Renewable Energy Markets, as well as special commendations for Excellence in Emission Markets and Excellence in Policy and Advisory.\n\nIn 2011, Sindicatum Sustainable Resources Group won the Commodity Business Award for Excellence in Renewable Energy Markets, as well as a Special Commendation in the Commodity Market Policy & Advisory category. Sindicatum Sustainable Resources Group also won two Green Business Awards, for excellence in the Renewable Energy and Carbon Reduction categories in 2011.\n\n"}
{"id": "31015207", "url": "https://en.wikipedia.org/wiki?curid=31015207", "title": "Audioprosthology", "text": "Audioprosthology\n\nAudioprosthology (ACA) is a term used to refer to the profession of the fitting of a hearing aid, or auditory prosthesis. An audioprosthologist is defined as “an aid-fitting specialist who has completed a course in audioprosthology.” This term was adopted by a group of hearing instrument specialists and an organization named the International Hearing Society (IHS) in 1976.\n\nThe roots of the term make its definition self-explanatory: audio for hearing, prosthetic for device, and ology for science. “Audio” as used in both “audiology” and “audioprosthology,” is clearly a derivation from the Latin term “audire,” which means “to hear,” and is commonly used in numerous other English words that are related in varying ways to hearing and sound. Audioprosthology was originally formed by the International Hearing Society (IHS).\n\nAudioprosthologists and Hearing Instrument Specialists provide services and testing for hearing aids and hearing loss. Not to be confused with Audiologists, hearing instrument specialists' primary purpose is to test and properly fit a hearing device. All are required from each individual state to pass certain requirements and regulations, most of which the same practical exam must be taken by Audioprosthologists, Audiologists and Hearing Instrument Specialists. Only Doctors of Audiology can diagnose but still cannot prescribe any type of medication because their Doctorate is not a medical degree.Audiologist in most states have at minimum a Doctorate degree which is clinical. The Doctorate degree is 4 years post a bachelor's degree as well as at least 3 rotations as well as one year clinical training. Audiologist gave training in cochlear implants, intraoperative monitoring, hearing aids, as well as vestibular and balance disorders. Audiologist can also work in private practice, hospitals or doctors offices. There is a vast difference between a hearing aid dispenser and Audiologist.\n\nFounded in 1976 by Harold Williams, EdD, and Robert Briskey, the ACA program was developed in response to a need for advanced training for hearing instrument specialists. The name was believed to be an accurate description of the course of study – the application of prostheses, that is hearing aids, to ameliorate auditory impairments. The first graduates of the program in 1978 realized the benefits of the coursework immediately and recommended that the ACA program be offered nationwide.\nIn 1993 the International Hearing Society (IHS) assumed control of the curriculum and applied to the American Council on Education (ACE) for an assessment of the program’s university credit equivalence. ACE determined that completion of the ACA course of study was the equivalent of 15 upper level baccalaureate semester credits. That equivalency meant that colleges and universities that recognized the ACE credit-equivalence paradigm would accept those 15 credits toward an undergraduate degree. The ACA program was launched at sites across the U.S. and continues to be offered today.\n\nHearing aid specialists have typically learned their profession through an apprenticeship in that they’re trained and supervised by another licensed individual. When qualified they take their state’s examination, and upon passing, they’re granted a license to practice. This apprenticeship provides them with the skill set necessary for entry level, safe practice. The original intent of the ACA Program was to provide current practitioners with the scientific foundation for their vocation, thus taking them to an advanced practice designation through formal coursework, laboratory exercises, and summative examinations.\n\nThe ACA Program is an opportunity for adult learners to supplement their skill set with the knowledge and theoretical background that could move them to a higher level of proficiency and professionalism, that is, to an advanced practice status. The use of the term audioprosthologist is a privilege of successful completion of the course of study. An individual must complete and pass a 13-month course and a subsequent practicum prior to being granted this privilege.\n\nThe intent of the American Council on Education (ACE) process was to provide an opportunity for practitioners to gain access to a college degree through lifelong learning and workplace skills. The ACA Program has been determined to be equivalent to 15 semester hours of upper level baccalaureate credit by the ACE College Credit Recommendation Service. ACE will only evaluate courses of study that are comparable to the learning offered at the college level in terms of course content, learning methods, and assessment procedures. Over 1800 academic institutions accept the ACE credit recommendations. In the field of hearing instrument sciences, Spokane Falls Community College has provided advanced standing for ACA graduates, have accepted all ACA credits, and have granted an automatic one-third fulfillment toward the requirements for the two-year associate degree in hearing instrument sciences.\n\nThe ACA Program embraces the concept that working adults should have access to academic credit for formal courses and examinations taken outside traditional degree programs with 100% relevance to their chosen careers. Institutions of higher learning embrace these non-traditional approaches because they facilitate adult learners in earning undergraduate degrees. The ACA Program has been embraced by hearing instrument specialists because it provides an opportunity for them to achieve an advanced practice status, and a few have gone on to use this experience for college credit.\n\nThe ACA educational program contains five courses structured to conform to a semester-hour format common to universities. Each of the five courses is held over three two-day sessions (weekends) for a total of 42 classroom hours per course. The core faculty consists of individuals with extensive knowledge and experience in the academic and/or business world. It is the core faculty’s responsibility to teach the courses in the ACA program, evaluate student performance and attainment of learning objectives, make suggestions about additional faculty, periodically review curriculum, and make recommendations for curriculum revisions in light of new knowledge, methodologies, and advancements in hearing aid engineering.\n\nStudents are required to attend all classes and complete all class assignments with a grade of 70% or better. Failure requires that the course be repeated. Official transcripts are available to each student who completes the ACA through the ACE Transcript Services in Washington, DC.\n\n\n\n"}
{"id": "19677261", "url": "https://en.wikipedia.org/wiki?curid=19677261", "title": "Basic utility vehicle", "text": "Basic utility vehicle\n\nA Basic Utility Vehicle (BUV) is a simple rugged vehicle designed for use in the developing world. A slew of such vehicles were developed in the late 1960s and early 1970s; most only reached limited production and market penetration as used Western vehicles often proved cheaper. In Southeast Asia, these are often referred to as \"AUVs\", or \"Asian Utility Vehicles\". They have also been called \"Basic Transportation Vehicles\" (BTV). The Institute for Affordable Transportation (IAT) currently holds annual competitions aimed at developing new such vehicles.\n\nGreek financier Peter Kondorgouris began producing a small utility vehicle called the Farmobil, based on a 1957 design by Wilfried Fahr of Switzerland. Using BMW engines, about 1000 were built between 1962 and 1966. The Farco company was taken over by Chrysler in the mid-sixties and they took over distribution in many markets.\n\nBasic utility vehicles were developed by Ford and GM specifically for sale in East Asia, with the intent of opening new markets there and in Africa. Volkswagen also developed a vehicle, the EA489 Basistransporter which was built from 1975 until 1979. Dutch DAF developed a stillborn competitor for this field, the 1972 BATU (Basic Automotive Transport Unit) General Motors' BTV effort was sold under a variety of different names and used the Vauxhall Viva's underpinnings and 1256 cc inline-four engine. It was built in Ecuador, Malaysia, Costa Rica, Paraguay, Portugal, and the Philippines. It was marketed as the GM Amigo, Andino (Ecuador), Mitaí (Paraguay), and Bedford Harimau (\"Tiger\", Malaysia).\nCitroën's FAF and Baby Brousse also fit this mold, with the acronym FAF standing for \"Easy to Finance, Easy to Build.\" The FAF and related versions entered production in ten countries, most of them in the developing world. Over 30,000 were built from 1973 until the 1980s. Nissan called their Datsun 1200 AX a BTV, it was introduced in June 1977 and received their 1171 cc A12 engine. Nissan/Datsun built it in Thailand (Siam Motors) and in Portugal. In Portugal it was called the Datsun Sado. The most successful BUV is arguably the Toyota Kijang, which entered production in Indonesia and the Philippines (as the Tamaraw) in 1977 and 1976. The Kijang/Tamaraw has, over five generations, morphed into a fairly luxurious Compact MPV called the Toyota Innova.\n\nOne thing that unites the various Basic Utility Vehicles of this era is that they used almost exclusively flat, welded sheetmetal, giving them a certain uniformity of appearance. Later yet, the independent Africar project again tried to target developing markets, in particular, Africa. Only six were built from 1986 until the company folded in 1988.\n\nThe Institute for Affordable Transportation (IAT) is the promoter of this type of vehicle and the main sponsor. IAT runs the annual BUV Design Competition where engineering students bring their vehicles for judging and performance evaluation (including cost) and contribute to IAT’s research and development efforts. The competition harnesses the creative energy of college students from across the United States in an effort to develop a simple, low-cost utility vehicle that can benefit low-income people in rural areas of developing countries. \n\nStudent teams design and build these vehicles to compete in a series of tests and events to determine the best design. Each team also plans an oral report aimed towards the judges and spectators. In the reports, students discuss the planning, building, and testing processes that each BUV went through prior to the competition. Because the BUV must account for a lack of infrastructure, the vehicles must pass a variety of tests during the competition including an obstacle course, mud pit, mogul field, and endurance track. Many of the students work on the BUV as a senior capstone project at the end of their engineering degree.\n\nBUVs are designed around these specifications:\n\n\n\n"}
{"id": "13497493", "url": "https://en.wikipedia.org/wiki?curid=13497493", "title": "CDMA mobile test set", "text": "CDMA mobile test set\n\nA CDMA Mobile Test Set is a call simulating device that is used to test CDMA cell phones. It provides a network-like environment forming a platform to test the cell phone. This reduces cost of manufacturing and testing the cell phone in a real environment. It can be used to test all major 2G, 2.5G, 3G and 3.5G wireless technologies.\nIn a lab, high-precision measurement correction over the entire frequency and dynamic range as well as compensation for temperature effects in realtime are critical factors for achieving accuracy. A good quality mobile test set helps in achieving excellent accuracy, which is a major concern for mobile manufacturers.\n\nA mobile test set should ideally support the following technologies:\n\n\n\nAgilent 8960 <br>\nAgilent 8924C (Older model) <br>\nR&S CMU200 Universal Radio Communication Tester<br>\nAnritsu MT8820C <br>\nAnritsu MT8870A <br>\nAnritsu MD8475A <br>\n\nAgilent Technologies, http://www.home.agilent.com/agilent/product.jspx?nid=-536900143.0.00&lc=eng&cc=US\n<br>\nRohde & Schwarz International, http://www2.rohde-schwarz.com/en/products/test_and_measurement/product_categories/mobile_radio/\n<br>\nAnritsu Corporation, http://www.anritsu.com/en-US/Products-Solutions/Test-Measurement/Mobile-Wireless-Communications/Handset-One-Box-Testers/index.aspx\n"}
{"id": "13295184", "url": "https://en.wikipedia.org/wiki?curid=13295184", "title": "China–Brazil Earth Resources Satellite program", "text": "China–Brazil Earth Resources Satellite program\n\nThe China–Brazil Earth Resources Satellite program (CBERS) is a technological cooperation program between Brazil and China which develops and operates Earth observation satellites.\n\nThe basis for the space cooperation between China and Brazil was established in May 1984, when both countries signed a complementary agreement to the cooperation framework agreement in science and technology. In July 1988, China and Brazil signed the protocol establishing the joint research and production of the China-Brazil Earth Resources Satellites (CBERS). Brazil, emerging from a long military regime, sought to abandon the Cold War logic and establish new international partnerships. China was dedicated to its great internal reform, but was also seeking international partnerships to develop advanced technologies. The agreement was advantageous for both countries. Brazil had the chance to develop medium-size satellites at a time when it was only capable of building small ones (100 kg size). China had an international partner that posed no military threats and that was receptive of foreigners.\n\nBrazil and China negotiated the CBERS project during two years (1986–1988), exchanging important technical information and visiting each other’s facilities, and they concluded that both sides had all the human, technical and material conditions to jointly develop an Earth resource observation satellite program. The Complementary Protocol on Cooperation on Space Technology was renewed in 1994 and again in 2004.\n\nIn Brazil, the Instituto Nacional de Pesquisas Espaciais (INPE or National Institute of Space Research) and the Brazilian Space Agency (; AEB) are involved with the program, as is the Brazilian industrial sector. In China, organizations involved include the China Academy of Space Technology (a sub-entity of the China Aerospace Science and Technology Corporation), the China National Space Administration and various other organizations.\n\nInitially the program included development and deployment of two satellites, CBERS-1 and CBERS-2. Subsequently, agreement was reached to include three additional satellites, CBERS-3, 4 and 4B.\n\nThe first satellite of the series, CBERS-1, was successfully launched on October 14, 1999 on a Long March 4B. It is sometimes also called ZY 1. It remained functional until August 2003.\n\nThe second satellite, CBERS-2, was successfully launched on October 21, 2003 by a Long March 4B rocket from China. It was retired from service in January 2009, after the launch of CBERS-2B.\n\nCBERS-1 and 2 are identical satellites. They have three remote sensing multispectral cameras:\n\n\nCBERS-2B was launched on 19 September 2007 by a Long-March 4B rocket from the Taiyuan base in China. The satellite operated until June 2010. Sample images from CBERS-2B were made available on January 10, 2007.\n\nCBERS-2B is also similar to the two previous members of the series, but a new camera was added to the last satellite: High Resolution Panchromatic Camera (HRC). This camera records images in one single panchromatic band 0,50 – 0,80 µm which comprises part of the visible and of the near infrared portion of electromagnetic spectrum. The images recorded by this camera are 27 km width and have 2.7m spatial resolution. 130 days are required to obtain a full coverage of the Earth by this camera.\n\nCBERS-3 was launched in December 2013, but was lost after the Chang Zheng 4B rocket carrying it malfunctioned. The identical CBERS-4 satellite was successfully launched in December 2014. Both satellites carry four cameras:\n\n\nCBERS-4A is expected to be launched in 2019.\n\n"}
{"id": "9408163", "url": "https://en.wikipedia.org/wiki?curid=9408163", "title": "Clean process oven", "text": "Clean process oven\n\nA clean process oven is a type of industrial batch oven that is ideal for high-temperature applications, such as curing Polyimide, and annealing thin and film waters. Clean process ovens may be for air atmospheres, or inert atmospheres for oxidation-sensitive materials. Temperatures can be over 525 degrees Celsius. \n\nOther types of industrial batch ovens include laboratory, burn-in, reach-in, and walk-in/drive-in.\n"}
{"id": "30456374", "url": "https://en.wikipedia.org/wiki?curid=30456374", "title": "Consumer adoption of technological innovations", "text": "Consumer adoption of technological innovations\n\nConsumer adoption of technological innovations is the process consumers use to determine whether or not to adopt an innovation. This process is influenced by consumer characteristics, such as personality traits and demographic or socioeconomic factors, the characteristics of the new product, such as its relative advantage and complexity, and social influences, such as opinion leaders.\n\nIn the context of technological innovations, the adoption process is also influenced by one or several new technologies that are incorporated in the new product. New technologies are likely to significantly affect the innovation's functionality or interface. Functionality refers to the set of potential benefits that a product can provide the consumer. Interface refers here to the specific means by which a consumer interacts with a product to obtain a particular functionality. Specifically, new technologies suggest four types of innovations with unique characteristics that are likely to affect the adoption process. Alternatively it can be looked at as a Paradox of Technology.\n\nDonald Norman in his book, The Design of Everyday Things, outlines the idea of \"Paradox of Technology\". Norman's paradox states that when a new functionality is added to a technology, it also increases its complexity. Thus, a technology intended to make life easier by providing more functionality, also makes it more complex by making things harder to learn. A good design must reduce the difficulties in use of the ever-growing technology.\n\nInformation and communications technologies such as Facebook experienced this phenomenon when they released the News Feed functionality to all users. The new groundbreaking feature was met with mass upheaval with only one in 100 messages about News Feed being positive. Now, News Feed is an essential feature of Facebook which users today would be outraged if removed.\n\nSymbolic interactionism, the concept that people act toward things based on the meaning they have for them; and these meanings are derived from social interaction and modified though interpretation), plays a key in role in the consumer adoption of technological innovations. People have personal meanings for certain aspects of the technology; when these technologies are changed or modified it can greatly affect how the user interacts with the technology.\n\nA good example of this concept is the controversial removal of the Start Menu from Microsoft's Windows 8. A major reason this was so controversial is this concept of symbolic internationalism. Critique Mark Wilson writes, \"I grew up on Windows 3.1 and the introduction of the Start button/menu in Windows 95 was a revelation. Windows 8 was a step in the wrong direction.\" Similar critiques and reactions was a major issue and Microsoft even brought it back the next release of Windows. Many including Microsoft will argue the start menu was dated and that the newer metro start screen is an improvement upon the start menu but that is still up to debate and preference.\n\nWhen a new technology is introduced a user evaluates if the perceived benefits (functionality, aesthetics etc.) outweigh any negative social nuances it may introduce. New technology not only changes the way that the user interacts with it, but often also asks users to embrace new behaviors. However, as our technologies are increasingly becoming more mobile, these new behaviors frequently take place in a public location and become an integral part of a user’s social appearance.\n\nIt is often the case that every new technology introduces public discomfort. While the first handheld cellular phone was developed in 1973, it was not until the early 2000s that they technology became truly ubiquitous. While a part of the slow growth of cellular phones can be attributed to its design, another big part was the technology being considered esoteric by many.\n\nSandra Vannoy and Prashant Palvia developed a theoretical model called the \"Social Influence Model\" that investigates technology adoption at a societal or communal level. The postulate that social influence constitutes of four overlapping phenomenon:\n\n\nTechnology adoption is typically measured on two factors: embedment and embracement in daily life. Social influence deals with the embedment of technology. Embedment in daily life is evaluated by examining how other members of the society present in the same environment utilize the technology, and how the technology is perceived by these members.\n\nThese innovations are incremental in nature since they offer an existing functionality and an existing interface; however, they are usually characterized by esthetic changes that affect the product's appearance. Smart phones, for example, are usually black or silver when first introduced into the market but are available in multiple colors several months later.\n\nThese innovations provide benefits available by existing products but result in a new set of actions for the consumer. Voice recognition software is one example of this type of innovation. Consumers create documents or emails, for example, by dictating (instead of typing) to a computer.\n\nThese innovations do not change consumer interaction with a device; they offer, however, a new functionality. Multi-mode cellphones, for example, operate in more than one frequency and enable roaming between different countries.\n\nCar GPS navigation systems, for example, fall under this category. These products provide the consumer with novel functionality, such as door-to-door navigation and real-time traffic information. The novel interface implies a new set of actions for using the device, such as using a touch-screen and voice recognition interfaces.\nInnovations that incorporate a novel interface require significant learning cost from the consumer since they imply learning a new set of tasks. High learning cost is likely to hinder the adoption of such innovations, unless the functionality provided is new and provides significant benefits to the consumer. Furthermore, innovations incorporating a novel interface often result in fear of technological complexity leading to feelings of ineptitude and frustration. Conversely, innovations that provide the consumer with a new functionality are characterized by a high relative advantage, which is likely to facilitate adoption.\n\nAs technologies have improved in the past years privacy has become a major concern among consumers because the data revolution and Big Data. Technological innovations more recently have seriously been affected by these concerns and changes how people interact with these new technologies. Privacy is a very broad concept, it is very hard to define in simple manners and is still a controversial subject, and because of this confusion, consumers reject many innovations or unknowingly give their personal data to third parties. Daniel J. Solove is an expert in the topic of Privacy and in his recent book \"Understanding Privacy\" he lays out the problems and frameworks of privacy in the era of technology and the data revolution. Another good example is Eli Pariser's concept of The Filter Bubble that he lays out in his book, \"The Filter Bubble: What the Internet Is Hiding from You.\" Innovations like personalized search from Google are very controversial mainly because most consumers have no idea that it is even occurring.\n"}
{"id": "52177827", "url": "https://en.wikipedia.org/wiki?curid=52177827", "title": "Cosmetic packaging", "text": "Cosmetic packaging\n\nThe term cosmetic packaging is used for cosmetic containers (primary packaging) and secondary packaging of fragrances and cosmetic products. Cosmetic products are substances intended for human cleansing, beautifying and promoting an enhanced appearance without altering the body's structure or functions.\n\nCosmetic packaging is standardized by an international norm set by the International Organization for Standardization and regulated by national or regional regulations such as those issued by the EU or the FDA. Marketers and manufacturers of cosmetic products must be compliant to these regulations to be able to market their cosmetic products in the corresponding areas of jurisdiction.\n\nThe term cosmetic packaging includes primary and the secondary packaging. Primary packaging, also called cosmetic container, is housing the cosmetic product. It is in direct contact with the cosmetic product. Secondary packaging is the outer wrapping of one or several cosmetic containers. An important difference between primary and secondary packaging is that any information that is necessary to clarify the safety of the product must appear on the primary package. Otherwise, much of the required information can appear on just the secondary packaging. \nThe cosmetic container shall carry the name of the distributor, the ingredients, define storage, nominal content, product identification (e.g., batch number), warning notices and directions for use.\nThe secondary packaging shall in addition carry the address of the distributor and information on the cosmetic's mode of action. The secondary packaging does not need to carry any product identification notice.\nIn cases where the cosmetic product is only wrapped by one single container, this container needs to carry all the information.\n\nThere are multiple reasons why care must be put into cosmetic containers. Not only must they protect the product, they need to provide conveniences for vendors and ultimately consumers.\n\nThe main purpose of a container is to store the product so that it is not degraded through storage, shipping and handling. Degradation and damage can be caused by various causes. These causes can be categorized into biological, chemical, thermal causes, damage caused by radiation and damage caused by human interaction, by electric sources or by pressure.\n\nIn addition to protecting the product, packaging also play a big role marketing cosmetic products. While product quality is a major factor in the product's success, its packaging must be attractive since that's the essence of beauty marketing. Package design must capture the imagination and be associated with enhancing appearance. One of the keys to attractive packaging is artistic use of colors. Most relevant for the marketer is the outer secondary packaging. However, there are cosmetics which are distributed in one single cosmetic container.\n\nCosmetic packages must not only convey beauty, they must equate to brand awareness. Since the package is what the consumer initially sees, it is very influential in shaping perceptions about the product. Part of building brand awareness for a cosmetic product is associating it with emotion. Since it's not a survival product it's marketed to appeal to the desire to enhance appearance. The packaging must stimulate this emotion.\n\nLabels tell consumers what they need to know about the product, as far as how to use it and where it comes from. Companies must list the ingredients and the function of the product, especially when it is unclear. The label must contain contact information of the entity responsible for putting the product on the market. Labels also provide product tracking information.\n\nThe label must be easy to read, particularly for a customer where the product is being displayed. Certain compositions, such as perfumes, can be listed as one ingredient. Secondary packages are what the consumer sees as the outermost package. Primary packages are within the secondary package. Certain information can appear just on secondary packages. The most important information, particularly if the product is prone to misuse, must be displayed on both the primary and secondary packaging.\n\nOne of the most important aspects of regulations on labelling is that the information is accurate. Although the FDA does not have the resources to inspect all cosmetic products on the market, it can issue penalties for various violations involving packaging and labelling. It is the manufacturer's responsibility to make sure that its product is safe for public consumption.\n\nNone of the information, including name and address, may be misleading. Words can be abbreviated only if it is clear what they represent. All text must be printed clearly on the packaging. Smaller packages in which text is too difficult to read, should include tags with legible text.\n\nIngredients must be listed in a certain order with priority given to ingredients that represent 1% or more of the volume. These ingredients must be listed in descending order, based on weight. This group of ingredients is then followed by those that represent 1% or less of the product and listed in any order. Colorants may also be listed in any order.\n\nMany times cosmetic products are packaged in multiple layers. Whenever it is difficult to detect for the consumer, the number of units should be listed on the outer package, which should contain details about how to use the product and warnings on what to do if it is misused. It is essential that the product is protected from environmental elements such as mould and bacteria.\n\nThe packaging must be sufficient enough to protect the mechanical, thermal, biological and chemical properties of the product. It should also be strong enough to withstand human tampering and radiation damage.\n\nThe FDA overseas cosmetic packaging but does not test products. It leaves testing for safety up to manufacturers. It still provides regulations and can issue recalls when a product is associated with safety hazards. While the FDA does not have many restrictions on ingredients for cosmetic products, it does require that certain chemicals and colorants be listed.\n\nAs far as EU regulations regarding packaging, manufacturers must be compliant with EC No. 1223/2009. One of these requirements involves the manufacturer issuing a safety report before putting the product on the market. The manufacturer must also disclose any serious undesirable effects (SUE) to the EU. Marketers are required to list nano-materials. \nThe EU's definition of \"ingredients\" does not include raw or technical materials used in production that do not end up in the final product. In some cases when durability is an issue, the manufacturer must list an expiration date after the product has been opened. The words \"best used before\" are common for identifying the product expiration date.\n\nStandard ISO 22715 provides specifications for the packaging and labeling of all cosmetic products that are sold or distributed at no charge; i.e. free samples. National regulations dictate what products are to be regarded as cosmetics. While ISO 22715 is not legally binding, national regulations regarding cosmetic products can be even stricter than those laid out in ISO 22715. The link between standards and regulations is that a standard often represents the common denominator of national law, as the standardization committee consists of members of most countries.\n\n"}
{"id": "42609775", "url": "https://en.wikipedia.org/wiki?curid=42609775", "title": "Counterparty (technology)", "text": "Counterparty (technology)\n\nCounterparty is a financial platform for creating peer-to-peer financial applications on the bitcoin blockchain. The protocol specification and all Counterparty software is open source. The reference client is counterparty and a web wallet called Counterwallet showcases all protocol features. The protocol’s native currency, XCP, is the fuel that powers Counterparty. It is slightly deflationary, with approximately 2.6 million XCP having been created by burning Bitcoins in January 2014. Counterparty provides users with the world's first functioning decentralized digital currency exchange, as well as the ability to create their own virtual assets, issue dividends, create price feeds, bets and contracts for difference.\n\nCounterparty has a native currency called XCP. It was originally issued using a provable method called \"proof of burn\". This method involves sending bitcoins to a special address that renders the coins permanently unspendable. By avoiding funding during its launch, Counterparty has ensured that developers and users have equal financial opportunities. During January 2014, 2125.63 bitcoins were sent to 1CounterpartyXXXXXXXXXXXXXXXUWLpVr (worth roughly US$1.8 million at the time). XCP is used in the Counterparty protocol to create new assets, make bets, and perform callbacks on callable assets. Counterparty used proof of burn to issue XCP, instead of a more traditional fund-raising technique for altcoin launches, to keep the initial distribution of funds as fair and decentralized as possible, and to avoid potential legal issues.\n\n\ncounterpartyd is the reference implementation of the Counterparty protocol, and Counterwallet is a deterministic web-wallet frontend to counterpartyd, in which all cryptography is handled client-side. Both are open source and hosted on GitHub. Counterparty offers support for Solidity.\n\n\nSince Counterparty Assets are created within the bitcoin blockchain, the assets actually exist in any bitcoin block explorer, however, to decode the information on what bitcoin address has what asset an additional layer of block exploration needs to be applied. Currently, there are two block explorers you can use to locate Counterparty assets in bitcoin wallets:\n"}
{"id": "517668", "url": "https://en.wikipedia.org/wiki?curid=517668", "title": "Currah", "text": "Currah\n\nCurrah was a British computer peripheral manufacturer, famous mainly for the speech synthesis ROM cartridges it designed for the ZX Spectrum, Commodore 64, and other 8-bit home computers of the 1980s.\n\nCurrah μSource from Quadhouse. In a self-contained ROM cartridge it has a full-function-two-pass macro assembler, Forth and a debugger, all of which can interact with Basic. It is also compatible with Interface 1.\n\nThe Currah μSpeech, commonly referred to as the \"Microspeech\" plugged into the expansion port on the back of the ZX Spectrum. Additional leads were provided to feed the sound and UHF signal from the computer into the unit. The TV aerial lead plugged into the unit and speech sounds were added into the UHF signal generated by computer.\n\nBy default, the unit \"spoke\" every key-press the user made, even the direction keys which came out as \"CURSOR\". This could be controlled by a reserved variable codice_1. Typing\nwould turn this feature off.\n\nSpecific words and phrases could be spoken by assigning a value to the reserved string variable codice_2. This was interpreted letter-by-letter unless brackets were used to denote other allophones. A simple example would be \"(dth)is\", (dth) representing the voiced dental fricative /ð/. Sixty-three allophones were provided. Rudimentary pitch modulation could be achieved by altering the case of the letters—upper case letters being pronounced at a slightly higher pitch.\n\nA more complex example:\n\nThe unit contained a ULA which worked on a WRITE command from the microprocessor, a ROM containing the keyword speech patterns, and an SP0256-AL2 speech processor. It also contained a clock for clear speech and an audio modulator to transfer the sound to the TV lead. A small adjustment screw was provided, to allow fine tuning of the audio output.\n\nThe unit allocated itself the top 256 bytes of memory at switch-on and moved down the USR graphics and RAMTOP. This made it incompatible with some programs, particularly games, which use that space for machine code.\n\nFor cost reasons, the unit did not provide for daisy-chaining of further devices on the computer's expansion port. Many joystick interface manufacturers took the same approach, meaning that you could not have a joystick and the MicroSpeech unit plugged in at the same time.\n\n\"Booty\" (Firebird Software Ltd) detected the presence of a MicroSpeech unit and presented the user with a completely different game to that which would be played if the MicroSpeech unit was not present.\n\nCurrah was acquired by DK'Tronics in 1985. DK'Tronics continued to manufacture the MicroSpeech unit, and many of their software titles (such as \"Maziacs\" and \"Zig Zag\") supported it.\n\n"}
{"id": "32918865", "url": "https://en.wikipedia.org/wiki?curid=32918865", "title": "Demand articulation", "text": "Demand articulation\n\nDemand articulation is a concept developed within the scientific field of innovation studies which serves to explain learning processes about needs for new and emerging technologies. Emerging technologies are technologies in their early phase of development, which have not resulted in concrete products yet. Many characteristics of these technologies, such as the technological aspects but also the needs of users concerning the technology, have not been specified yet. Demand articulation can be defined as ‘iterative, inherently creative processes in which stakeholders try to address what they perceive as important characteristics of, and attempt to unravel preferences for an emerging innovation’.\n\nThe approach may be applied to describing the processes by which needs for emerging technologies become more concrete over time. At the same time, demand articulation can also be perceived as learning processes that can be evaluated.\n\nThe concept of demand articulation originates from the theoretical school that explains innovations as a result of the co-evolution of technological developments and societal pressures. The central idea behind this school is that innovations are not only deterministically formed following technological considerations and possibilities, but in interaction with societal aspects, such as ethical questions, user demands, implementation issues.\n\nIn the emergent phase, several aspects of the technology remain rather ‘fluid’ and can be formed by stakeholders involved. In this period, co-evolution of society and technology takes place and ‘societal entrenchment of a technology’ is carried by different processes in which several aspects of the technology become articulated over time. In the 1990s, the term articulation processes was introduced, including articulation of technology specifications, of product and maintenance networks, of cultural and political acceptability, and of demands.\n\nDemand articulation processes owe much to the work of Teubal, in which he stated that in existing markets users have defined their needs quite precisely, and prices (also of competitors) play a major role in sales decisions. With breakthrough, radical, emerging technologies there are no markets and needs in place. Producers can only offer blueprints. However, users might not have thought of the direction of solutions the new product offers, and the regime as a whole might even change as a result of the innovation, which both can lead to different preferences of users.\n\nIn order to deal with the determination of user preferences in the context of emerging technologies, Teubal introduced the term market determinateness as “the degree of specificity of the market signals received by the innovating firm and consequently to the extent to which it anticipates demand. In order to explain the concept we introduce four types of market signals, in ascending order of specificity: (1) signals about a need; (2) signals about a product class; (3) signals about basic functions; (4) signals about product specifications.” Ideally, as a technology emerges and several of its aspects become clear, users are also becoming specific about their ‘market signals’ or demands.\n\nKodama later picked up market determinateness or demand articulation, which he defined as “a dynamic interaction of technological activities that involves integrating potential demands into a product concept and decomposing this product concept into development agendas for its individual component technologies”. Building on this, the term ‘latent demand’ was introduced, which means that most stakeholders will not have an evident idea of what they want from the start. An actor might have a certain need that is ill-defined or latent, but which, in a sense, cannot be denied. For example, there have been an evident need for communication over long distances and even a then-farfetched idea of mobile communication devices, but the precise need for mobile phones (or SMS services for that matter) could not be foreseen before the introduction of these devices. The demand articulation process, therefore, is the start of a consciousness-raising exercise in which demands become increasingly concrete.\n\nThe definition of demand merits special attention in the context of demand articulation. A clear distinction should be made between two types of demands:\nFollowing the distinction made by Teubal above, substantive demand are more relevant in the emergent phase of technological development. Mowery and Rosenberg underline this by criticising the use of “the rather shapeless and elusive notion of [market] ‘needs’”.\n\nDemands should be regarded as a broad concept that includes a range of concepts varying in level of determinedness and varying in content areas. Concerning the varying degree of concreteness, demands include (ranging from less to more concrete):\n\nConcerning the content range, demands may include cultural, political, ethical, social issues, because in early stages of technology development it is unclear which issues would become important in steering, and because user preferences are diverse and partially dependent on these issues.\n\nUsers of products and services have the potential to contribute to innovation processes and to the success of eventual innovations. In the 1970s the Science Policy Research Unit (SPRU) conducted the SAPPHO-study in which resembling successful and unsuccessful innovations were compared. “The single measure that discriminated most clearly between success and failure was ‘user needs understood’”. Building on this finding, Von Hippel and his colleagues found that users were major sources or “loci” of innovations in several sectors. Not only do users point to directions of future needs, they could also have first-hand information on new research directions, ideas, problems, and solutions.\n\nThe role of demand-side actors has also been studied in a more holistic way in the innovation system literature. Here, users – and also intermediary organisations – appear as major actors that are engaged in demand articulation processes. Innovation occurs at the intersection of needs and opportunities, both of which show a large degree of variability and unpredictability. This requires not only exchange of information on qualities and costs of innovations, but also of information on the (technological and user-related) contents of these innovations. This content needs to be communicated. The major role of knowledge in user-producer interactions calls for an emphasis on interactive learning; to innovate successfully, producers constantly need to learn about the technological possibilities as well as about user needs.\n\nThese interactive learning processes in which demands for (characteristics of) innovations are increasingly better understood can be regarded as demand articulation.\n\nIn the light of the benefits of understanding user needs, poorly articulated demand is regarded as one of the systemic failures that innovation systems can face. Demand articulation, strategy and vision development should be stimulated. Public innovation policy can contribute to this by contemplating the use of demand-oriented policy instruments, besides supply-, diffusion- and infrastructure-oriented instruments. Examples of demand-oriented innovation policy instruments are public procurement, visioning exercises to produce R&D agendas and facilitate user-producer interactions in local, experimental transdisciplinary settings.\n\nIn several countries, including Germany and the Netherlands, demand-oriented innovation policy gradually gained momentum.\n"}
{"id": "42119079", "url": "https://en.wikipedia.org/wiki?curid=42119079", "title": "Electric tug", "text": "Electric tug\n\nAn electric tug is a battery-powered and pedestrian-operated machine used to move heavy loads on wheels.\n\nThe machines form part of the material handling equipment industry that, amongst others, also covers fork lift trucks, overhead cranes and pallet jacks.\nAlthough ‘electric tug’ is perhaps the most commonly used term, suppliers and customers regularly use a range of other names that include towing tractors, battery-powered tugs, electric hand trucks, electric tuggers and pedestrian operated tugs.\nThe tugs move loads across a single level. They do not lift the load clear of the ground which is why the load must be on wheels. If the load itself does not have wheels, it would be placed on a wheeled platform often referred to as a trolley, bogie or skate. The tug connects to this wheeled platform just as a fork lift truck links to a pallet in order to move a load sat on it.\nIn most cases a steel coupling (male) attached to the machine itself connects to a corresponding coupling (female) bolted to the load’s bogie. A second bogie or multiple bogies will each have identical female couplings attached to them so that a single male coupling attached to the machine can move them all without alterations.\n\nAn electric tug relies on the principal of tractive effort. The machine, once secured to the bogie, will lift a portion of the load ensuring the load’s wheels remain on the ground. This is achieved via the machine’s hydraulic mast which is designed to create downforce on the drive wheel immediately beneath it. It is the traction generated from this process that allows the tug to move very large and heavy objects. As a tug does not lift its load clear of the ground it does not have to conform to the Lifting Operations and Lifting Equipment Regulations 1998 (LOLER), therefore an operator does not need a licence to operate it.\n\nElectric tugs are used in many industry sectors. Some common applications include;\n\n\n"}
{"id": "37344121", "url": "https://en.wikipedia.org/wiki?curid=37344121", "title": "Evelyn Berezin", "text": "Evelyn Berezin\n\nEvelyn Berezin (born April 12, 1925) is an American computer designer best known for designing the first computer-driven word processor. She was also responsible for the first computer-controlled systems for airline reservations.\n\nBerezin started university at Hunter College in January 1941, studying economics instead of the physics she preferred because it was preferred as a subject for women at that time. After WWII started, new opportunities made the study of physics possible with a scholarship at New York University, plus free classes at both Hunter and Brooklyn Polytech during the war years. At the same time, she worked full-time during the day as an assistant in the Rheology Department of the Research Division of the International Printing Company (IPI). Going to university at night, she received her B.S. in physics in 1946.\n\nShe began graduate work at New York University, holding a fellowship from the United States Atomic Energy Commission. In 1951 she accepted a job with the Electronic Computer Corporation and began there as head of the Logic Design Department. Berezin was the only person doing the logic design for computers being developed by ECC. In 1957 ECC was purchased by Underwood Corporation (originally known as the Underwood Typewriter Company) . Here, she designed a number of computers which were very general in structure but individual in specific application. Among them was a system for the US Army for range calculations, a system for controlling the distribution of magazines, and also developed what is now considered the first office computer.\n\nThe Underwood Typewriter Company was not able to continue the development beyond 1957, and Berezin went to a company called Teleregister where she developed the first computerized banking system and also an airline reservation system which controlled 60 cities in a communication system that provided 1 second response time.\n\nIn 1968, Berezin had the idea for a word processor to simplify the work of secretaries, and in 1969 she founded a word processor company, Redactron Corporation, which became a public company and delivered thousands of systems to customers throughout its international marketing organization. In the 1970s, although the market continued strong the economy suffered a serious inflation, increasing interest rates to a level (16%) which was untenable for a business like Redactron which operated in a world in which equipment was rented. The Company was sold to the Burroughs Corporation, and integrated into its office equipment division. Berezin stayed on until 1979.\n\nIn 1980, Berezin served as President of Greenhouse Management Company, General Partner of a venture capital group dedicated to early stage high technology companies.\n\nThroughout her career she has received honorary doctorates from Adelphi University and Eastern Michigan University. Berezin has also served on the Boards of CIGNA, Standard Microsystems, Koppers, and Datapoint.\n\nBerezin is now retired. She has been on the Board of the Stony Brook Foundation at Stony Brook University, the Brookhaven National Laboratory and the Boyce Thompson Institute.\n\nBerezin established the Berezin-Wilenitz Endowment which will give the value of her estate to fund either a chair, professorship, or research fund at Stony Brook in any field of science as stated in her will and testament. In addition to the endowment, Berezin and her late husband funded the Sam and Rose Berezin Endowed Scholarship, a full-tuition scholarship that is awarded to an undergraduate student who plans to study in the field of science, engineering or mathematics, in honor of her parents. Berezin and Wilenitz also established the Israel Wilenitz Endowment, this provides discretionary funds to the Linguistics Department at Stony Brook University, where Wilenitz received a Master's Degree.\n\nBerezin was married for 51 years to Israel Wilenitz, born in 1922 in London. Wilenitz died on February 20, 2003. She currently resides in Setauket and has an apartment in New York City.\n\n\n"}
{"id": "52972673", "url": "https://en.wikipedia.org/wiki?curid=52972673", "title": "Feel Train", "text": "Feel Train\n\nFeel Train is a technology collaborative co-founded by Courtney Stanton and Darius Kazemi and based in Portland, Oregon.\n\nFeel Train is a worker-owned cooperative. Stanton and Kazemi are its first two worker-owners, and the organization is chartered to allow a maximum of eight employees, each with equal salary, equal share in the company and equal firing power over others, including the founders. \n\nFeel Train projects have included the Stay Woke Bot, a Twitter bot developed in collaboration with activists DeRay Mckesson and Samuel Sinyangwe, and Shortcut, an app developed with radio program \"This American Life\" to facilitate sharing audio clips across social media, similar to the way gifs allow video clips to be shared. Feel Train is also developing a Twitter bot based on the Obama Social Media Archive called Relive 44, which beginning in May 2017 will repost, eight years later, every tweet from President Barack Obama (whose first tweet came in May 2009.)\n\nFeel Train website\n"}
{"id": "3571544", "url": "https://en.wikipedia.org/wiki?curid=3571544", "title": "Ferring Pharmaceuticals", "text": "Ferring Pharmaceuticals\n\nFerring Holding SA, better known as Ferring Pharmaceuticals, is a multinational pharmaceutical company that specializes in the development and marketing of drugs for use in human medicine.\n\nThe company was founded by Frederik Paulsen Sr in Malmö, Sweden, in 1950, initially as the Nordiska Hormon Laboratoriet, renamed Ferring in 1954. A \"ferring\" in Frisian is a person from the island Föhr off the western coast of Germany. Mr. Paulsen's family originates from that island.\n\nThe first major breakthrough was the synthetic production of oxytocin and vasopressin in 1961. With further growth and expansion, the headquarters moved to Copenhagen, Denmark, and subsequently to Switzerland.\n\nIn August 2016, the company announced it would acquire the phase III sciatica candidate drug - Condoliase (SI-6603) from Seikagaku for approximately $95 million.\n\nFerring Holding SA headquarters are located in Saint-Prex, Switzerland. The company holds over 5,000 employees with manufacturing facilities in Europe, South America, China and has built two new facilities in the US and in India.\n\nIn Copenhagen, Ferring has a R&D centre in the Ferring Tower in Ørestad. The company has commissioned Foster and Partners to design a new building in the Scanport development which is expected to be completed in 2019. It will have area of 30,000 square metres and have room for 750 employees.\n\nDutch subsidiary Ferring B.V. expanded to the US in 1980. \nFerring established a research group at the Ferring Research Institute in San Diego, CA (USA) in 1996. Ferring assembled a peptide research group that collaborates with academic scientists. The Ferring Research Institute moved to its new home in San Diego in February 2009 with the opening of a 38,000-square-foot facility in Sorrento Valley. The new facility houses expanded research laboratories for peptide medicinal chemistry, biochemistry, bioanalytical, and pharmacology.\nIn 2012, Ferring Pharmaceuticals funded the Ferring Research Infertility and Gynaecology Grant.\n\nProducts are used in reproductive health, gastroenterology, urology, orthopaedics and endocrinology.\n\nFerring's chairman is Frederik Paulsen Jr, son of founder Frederik Paulsen Sr. The chief operating officer (COO) of the company is Michel L. Pettigrew. In 2015, Pascal Danglas and Per Falk took the posts of chief medical officer (CMO) and chief science officer (CSO), respectively.\n\n\n\n"}
{"id": "25180898", "url": "https://en.wikipedia.org/wiki?curid=25180898", "title": "George Philippidis", "text": "George Philippidis\n\nDr. George Philippidis is a renewable energy leader who has published and spoken extensively about the global need for renewable energy over the last 20 years. He advocates the development of renewable power and fuels to enhance energy security, combat climate change, and secure sustainable economic growth. He has authored 11 cleantech patents, written numerous articles, and spoken nationally and internationally emphasizing that renewable energy can initially supplement and augment current resources and progressively replace fossil energy based on its own merits rather than on government policy.\n\nEnergy diversity is essential to long-term socio-economic prosperity. A diverse energy portfolio, depending on the local availability of natural resources, includes (1) solar, wind, biogas, biomass, ocean, and geothermal energy for power generation; (2) biofuels and renewable hydrocarbons, such as diesel, biodiesel, jet biofuel, and ethanol from biomass, algae, oil-seed plants, and used oil; and (3) electric vehicles powered by renewable energy for transportation. The private sector and the markets should determine which of those forms of low- and no-carbon energy are most appropriate and cost-effective locally in different parts of the country and the world.\n\nDr. Philippidis is a Fulbright Scholar. He received a B.S. in Chemical Engineering from the Aristotle University of Thessaloniki in Macedonia, Greece and a Ph.D. in Chemical Engineering from the University of Minnesota in Minneapolis-St. Paul. He also studied Business Administration at the University of Denver obtaining an MBA. He led strategic business units at the National Renewable Energy Laboratory (NREL) of the US Department of Energy in Denver and at a subsidiary of Thermo Fisher Scientific in Boston before becoming Energy Director of the Applied Research Center, the business arm of Florida International University in Miami. Presently, he is Director of the Biofuels & Bioproducts Lab and Associate Professor at the Patel College of Global Sustainability at the University of South Florida in the Tampa Bay area, where he partners with companies and venture capital firms to commercialize renewable energy technologies and educates students and professionals in energy sustainability.\n\nHe has been advising the federal and state governments on energy policy and transition to a bioeconomy, venture capital and private equity firms on investment in cleantech, and the private sector in the United States, Latin America, and Europe on the establishment of renewable power and fuel industries.\n"}
{"id": "46885416", "url": "https://en.wikipedia.org/wiki?curid=46885416", "title": "Grazebrook beam engine", "text": "Grazebrook beam engine\n\nThe Grazebrook Engine is an 1817 beam engine that was used for blowing air over the hot coals of a blast furnace to increase the heat. It is now found as sentinel sculpture on the Dartmouth Circus roundabout at the entrance of the A38(M) in Birmingham, England. It is believed to be the largest steam engine used in Birmingham.\n\nIt was built in 1817 to the designs of Watt, who had a foundry in Soho, Birmingham, by Grazebrook & Whitehouse for their Dudley foundry in Dudley, Worcestershire. It provided air for two blast furnaces. In 1912, it was retired, remaining on site as a standby engine. It was dismantled in 1964, and is now displayed on the Dartmouth Circus roundabout at the entrance of the A38(M) (Aston Expressway) and the A4540 (Dartmouth Middleway).\n\nThis is a typical example of an early nineteenth century engine, it comprises a vertical double acting steam cylinder coupled via an rocking beam to a double acting air cylinder.\n\nThe beam is made of cast iron; it is long and weighs . The steam cylinder is in diameter and has a stroke of . It was designed to run at between 12 and 16 strokes per minute. Steam was provided by a bank of six Lancashire boilers. A pressure regulator vessel was fitted to smooth out the air flow. It provided air at to two blast furnaces.\n\nThe engine house was specially built using lime mortar so that the structure could 'flex' with the movement of the engine.\n\nGrazebrook had a works in Peartree Lane, Netherton, possibly during 1952/53.\n\nM. & W. Grazebrook's history can be traced to 1641, when Michael Grazebrook went into business in Stourbridge. The company had a glassworks in Stourbridge, a forge at Halesowen in the 1700s and their own colliery in Coseley. It then focused on iron production and fabrications moving to Netherton in 1800. It was served by the Grazebrook arm of the Dudley Canal.\n\n\n\n"}
{"id": "15275887", "url": "https://en.wikipedia.org/wiki?curid=15275887", "title": "Heat flux sensor", "text": "Heat flux sensor\n\nA heat flux sensor is a transducer that generates an electrical signal proportional to the total heat rate applied to the surface of the sensor. The measured heat rate is divided by the surface area of the sensor to determine the heat flux.\n\nThe heat flux can have different origins; in principle convective, radiative as well as conductive heat can be measured. Heat flux sensors are known under different names, such as heat flux transducers, heat flux gauges, heat flux plates. Some instruments that actually are single-purpose heat flux sensors like pyranometers for solar radiation measurement. Other heat flux sensors include Gardon gauges (also known as a circular-foil gauge), thin-film thermopiles, and Schmidt-Boelter gauges. In SI units, the heat rate is measured in Watts, and the heat flux is computed in Watts per meter squared.\n\nHeat flux sensors are used for a variety of applications. Common applications are studies of building envelope thermal resistance, studies of the effect of fire and flames or laser power measurements. More exotic applications include estimation of fouling on boiler surfaces, temperature measurement of moving foil material, etc.\n\nThe total heat flux is composed of a conductive, convective and radiative part. Depending on the application, one might want to measure all three of these quantities or single one out.\n\nAn example of measurement of conductive heat flux is a heat flux plate incorporated into a wall.\n\nAn example of measurement of radiative heat flux density is a pyranometer for measurement of solar radiation.\n\nAn example of a sensor sensitive to radiative as well as convective heat flux is a Gardon or Schmidt–Boelter gauge, used for studies of fire and flames. The Gardon must measure convection perpendicular to the face of the sensor to be accurate due to the circular-foil construction, while the wire-wound geometry of the Schmidt-Boelter gauge can measure both perpendicular and parallel flows.In this case the sensor is mounted on a water-cooled body. Such sensors are used in fire resistance testing to put the fire to which samples are exposed to the right intensity level.\n\nThere are various examples of sensors that internally use heat flux sensors examples are laser power meters, pyranometers, etc.\n\nWe will discuss three large fields of application in what follows.\n\nSoil heat flux is a most important parameter in agro-meteorological studies, since it allows one to study the amount of energy stored in the soil as a function of time.\n\nTypically two or three sensors are buried in the ground around a meteorological station at a depth of around 4 cm below the surface. The problems that are encountered in soil are threefold:\nThe result of all this is the quality of the data in soil heat flux measurement is not under control; the measurement of soil heat flux is considered to be extremely difficult.\n\nIn a world ever more concerned with saving energy, studying the thermal properties of buildings has become a growing field of interest. One of the starting points in these studies is the mounting of heat flux sensors on walls in existing buildings or structures built especially for this type of research. Heat flux sensors mounted to building walls or envelope component can monitor the amount of heat energy loss/gain through that component and/or can be used to measure the envelope thermal resistance, R-value, or thermal transmittance, U-value.\n\nThe measurement of heat flux in walls is comparable to that in soil in many respects. Two major differences however are the fact that the thermal properties of a wall generally do not change (provided its moisture content does not change) and that it is not always possible to insert the heat flux sensor in the wall, so that it has to be mounted on its inner or outer surface.\nWhen the heat flux sensor has to be mounted on the surface of the wall, one has to take care that the added thermal resistance is not too large. Also the spectral properties should be matching those of the wall as closely as possible. If the sensor is exposed to solar radiation, this is especially important. In this case one should consider painting the sensor in the same color as the wall. Also in walls the use of self-calibrating heat flux sensors should be considered.\n\nThe measurement of the heat exchange of human beings is of importance for medical studies, and when designing clothing, immersion suits and sleeping bags.\n\nA difficulty during this measurement is that the human skin is not particularly suitable for the mounting of heat flux sensors. Also the sensor has to be thin: the skin essentially is a constant temperature heat sink, so added thermal resistance has to be avoided. Another problem is that test persons might be moving. The contact between the test person and the sensor can be lost. For this reason, whenever a high level of quality assurance of the measurement is required, it can be recommended to use a self-calibrating sensor.\n\nHeat flux sensors are also used in industrial environments, where temperature and heat flux may be much higher. Examples of these environments are aluminium smelting, solar concentrators, coal fired boilers, blast furnaces, flare systems, fluidized beds, cokers...\n\nA heat flux sensor should measure the local heat flux density in one direction. The result is expressed in watts per square meter. The calculation is done according to:\n\nformula_1\n\nWhere formula_2 is the sensor output and formula_3 is the calibration constant, specific for the sensor.\n\nAs shown before in the figure to the left, heat flux sensors generally have the shape of a flat plate and a sensitivity in the direction perpendicular to the sensor surface.\n\nUsually a number of thermocouples connected in series called thermopiles are used. General advantages of thermopiles are their stability, low ohmic value (which implies little pickup of electromagnetic disturbances), good signal-noise ratio and the fact that zero input gives zero output. Disadvantageous is the low sensitivity.\n\nFor better understanding of heat flux sensor behavior, it can be modeled as a simple electrical circuit consisting of a resistance, formula_4, and a capacitor, formula_5. In this way it can be seen that one can attribute a thermal resistance formula_6, a thermal capacity formula_7 and also a response time formula_8 to the sensor.\n\nUsually, the thermal resistance and the thermal capacity of the entire heat flux sensor are equal to those of the filling material. Stretching the analogy with the electric circuit further, one arrives at the following expression for the response time:\n\nformula_9\n\nIn which formula_10 is the sensor thickness, formula_11 the density, formula_12 the specific heat capacity and formula_13 the thermal conductivity. From this formula one can conclude that material properties of the filling material and dimensions are determining the response time.\nAs a rule of thumb, the response time is proportional to the thickness to the power of two.\n\nOther parameters that are determining sensor properties are the electrical characteristics of the thermocouple. The temperature dependence of the thermocouple causes the temperature dependence and the non-linearity of the heat flux sensor. The non linearity at a certain temperature is in fact the derivative of the temperature dependence at that temperature.\n\nHowever, a well designed sensor may have a lower temperature dependence and better linearity than expected. There are two ways of achieving this:\n\nAnother factor that determines heat flux sensor behavior, is the construction of the sensor. In particular some designs have a strongly nonuniform sensitivity. Others even exhibit a sensitivity to lateral fluxes. The sensor schematically given in the above figure would for example also be sensitive to heat flows from left to right. This type of behavior will not cause problems as long as fluxes are uniform and in one direction only.\n\nTo promote uniformity of sensitivity, a so-called sandwich construction as shown in the figure to the left can be used. The purpose of the plates, which have a high conductivity, is to promote the transport of heat across the whole sensitive surface.\n\nIt is difficult to quantify non-uniformity and sensitivity to lateral fluxes. Some sensors are equipped with an extra electrical lead, splitting the sensor into two parts. If during application, there is non-uniform behavior of the sensor or the flux, this will result in different outputs of the two parts.\n\nSummarizing: The intrinsic specifications that can be attributed to heat flux sensors are thermal conductivity, total thermal resistance, heat capacity, response time, non linearity, stability, temperature dependence of sensitivity, uniformity of sensitivity and sensitivity to lateral fluxes. For the latter two specifications, a good method for quantification is not known.\n\nIn order to do in-situ measurements, the user must be provided with the correct calibration constant formula_14. This constant is also called \"sensitivity\". The sensitivity is primarily determined by the sensor construction and operation temperatures, but also by the geometry and material properties of the object that is measured. Therefore the sensor should be calibrated under conditions that are close to the conditions of the intended application. The calibration set-up should also be properly shielded to limit external influences. \n\nTo do a calibration measurement, one needs a voltmeter or datalogger with resolution of ±2μV or better. One should avoid air gaps between layers in the test stack. These can be filled with filling materials, like toothpaste, caulk or putty. If need be, thermally conductive gel can be used to improve contact between layers. A temperature sensor should be placed on or near the sensor, and connected to a readout device.\n\nThe calibration is done by applying a controlled heat flux through the sensor. By varying the hot and cold sides of the stack, and measuring the voltages of the heat flux sensor and temperature sensor, the correct sensitivity can be determined with:\n\nformula_15\n\nwhere formula_16 is the sensor output and formula_17 is the known heat flux through the sensor. \n\nIf the sensor is mounted onto a surface and is exposed to convection and radiation during the expected applications, the same conditions should be taken into account during calibration. \n\nDoing measurements at different temperatures allows for determining sensitivity as a function of the temperature. \n\nWhile heat flux sensors are typically supplied with a sensitivity by the manufacturer, there are times and situations that call for a re-calibration of the sensor. Especially in building walls or envelopes the heat flux sensors can not be removed after the initial installation or may be very difficult te reach. In order to calibrate the sensor, some come with an integrated heater with specified characteristics. By applying a known voltage on and current through the heater, a controlled heat flux is provided which can be used to calculate the new sensitivity.\n\nThe interpretation of measurement results of heat flux sensors is often done assuming that the phenomenon that is studied, is quasi-static and taking place in a direction transversal to the sensor surface.\nDynamic effects and lateral fluxes are possible error sources.\n\nThe assumption that conditions are quasi-static should be related to the response time of the detector.\n\nThe case that the heat flux sensor is used as a radiation detector (see figure to the left) will serve to illustrate the effect of changing fluxes. Assuming that the cold joints of the sensor are at a constant temperature, and an energy flows from formula_18, the sensor response is:\nformula_19\n\nThis shows that one should expect a false reading during a period that equals several response times, formula_8. Generally heat flux sensors are quite slow, and will need several minutes to reach 95% response. This is the reason why one prefers to work with values that are integrated over a long period; during this period the sensor signal will go up and down. The assumption is that errors due to long response times will cancel. The upgoing signal will give an error, the downgoing signal will produce an equally large error with a different sign. This will be valid only if periods with stable heat flow prevail.\n\nIn order to avoid errors caused by long response times, one should use sensors with low value of formula_21, since this product determines the response time. In other words: sensors with low mass or small thickness.\n\nThe sensor response time equation above holds as long as the cold joints are at a constant temperature. An unexpected result shows when the temperature of the sensor changes.\n\nAssuming that the sensor temperature starts changing at the cold joints, at a rate of formula_22, starting at formula_23, formula_8 is the sensor response time, the reaction to this is:\n\n"}
{"id": "35726772", "url": "https://en.wikipedia.org/wiki?curid=35726772", "title": "Hoffmann's Stärkefabriken", "text": "Hoffmann's Stärkefabriken\n\nHoffmann's Stärkefabriken (\"English: Hoffman's Starch Factories\") was a German company that produced starch and food chemicals. It was founded in 1850 and ceased operations in 1990.\n\nIt was the oldest industrial company in Bad Salzuflen, North Rhine-Westphalia-area of Germany.\n\n\n"}
{"id": "55951", "url": "https://en.wikipedia.org/wiki?curid=55951", "title": "Instant messaging", "text": "Instant messaging\n\nInstant messaging (IM) technology is a type of online chat that offers real-time text transmission over the Internet. A LAN messenger operates in a similar way over a local area network. Short messages are typically transmitted between two parties, when each user chooses to complete a thought and select \"send\". Some IM applications can use push technology to provide real-time text, which transmits messages character by character, as they are composed. More advanced instant messaging can add file transfer, clickable hyperlinks, Voice over IP, or video chat.\n\nNon-IM types of chat include multicast transmission, usually referred to as \"chat rooms\", where participants might be anonymous or might be previously known to each other (for example collaborators on a project that is using chat to facilitate communication). Instant messaging systems tend to facilitate connections between specified known users (often using a contact list also known as a \"buddy list\" or \"friend list\"). Depending on the IM protocol, the technical architecture can be peer-to-peer (direct point-to-point transmission) or client-server (an Instant message service center retransmits messages from the sender to the communication device).\n\nBy 2010, instant messaging over the Web was already in sharp decline, in favor of messaging features on social networks such as Facebook and Twitter. The most popular IM platforms, such as AIM, closed in 2017, and Windows Live Messenger was merged into Skype. Today, most instant messaging takes place on messaging apps (such as WhatsApp, Facebook Messenger, WeChat, and Viber), which by 2014 had more users than social networks.\n\nInstant messaging is a set of communication technologies used for text-based communication between two or more participants over the Internet or other types of networks. IM–chat happens in real-time. Of importance is that online chat and instant messaging differ from other technologies such as email due to the perceived quasi-synchrony of the communications by the users. Some systems permit messages to be sent to users not then 'logged on' (\"offline messages\"), thus removing some differences between IM and email (often done by sending the message to the associated email account).\n\nIM allows effective and efficient communication, allowing immediate receipt of acknowledgment or reply. However IM is basically not necessarily supported by transaction control. In many cases, instant messaging includes added features which can make it even more popular. For example, users may see each other via webcams, or talk directly for free over the Internet using a microphone and headphones or loudspeakers. Many applications allow file transfers, although they are usually limited in the permissible file-size.\n\nIt is usually possible to save a text conversation for later reference. Instant messages are often logged in a local message history, making it similar to the persistent nature of emails.\n\nThough the term dates from the 1990s, instant messaging predates the Internet, first appearing on multi-user operating systems like Compatible Time-Sharing System (CTSS) and Multiplexed Information and Computing Service (Multics) in the mid-1960s. Initially, some of these systems were used as notification systems for services like printing, but quickly were used to facilitate communication with other users logged into the same machine. As networks developed, the protocols spread with the networks. Some of these used a peer-to-peer protocol (e.g. talk, ntalk and ytalk), while others required peers to connect to a server (see talker and IRC). The Zephyr Notification Service (still in use at some institutions) was invented at MIT's Project Athena in the 1980s to allow service providers to locate and send messages to users.\n\nParallel to instant messaging were early online chat facilities, the earliest of which was Talkomatic (1973) on the PLATO system, which allowed 5 people to chat simultaneously on a 512x512 plasma display (5 lines of text + 1 status line per person). During the bulletin board system (BBS) phenomenon that peaked during the 1980s, some systems incorporated chat features which were similar to instant messaging; Freelancin' Roundtable was one prime example. The first such general-availability commercial online chat service (as opposed to PLATO, which was educational) was the CompuServe CB Simulator in 1980, created by CompuServe executive Alexander \"Sandy\" Trevor in Columbus, Ohio.\n\nEarly instant messaging programs were primarily real-time text, where characters appeared as they were typed. This includes the Unix \"talk\" command line program, which was popular in the 1980s and early 1990s. Some BBS chat programs (i.e. Celerity BBS) also used a similar interface. Modern implementations of real-time text also exist in instant messengers, such as AOL's Real-Time IM as an optional feature.\n\nIn the latter half of the 1980s and into the early 1990s, the Quantum Link online service for Commodore 64 computers offered user-to-user messages between concurrently connected customers, which they called \"On-Line Messages\" (or OLM for short), and later \"FlashMail.\" (Quantum Link later became America Online and made AOL Instant Messenger (AIM, discussed later). While the Quantum Link client software ran on a Commodore 64, using only the Commodore's PETSCII text-graphics, the screen was visually divided into sections and OLMs would appear as a yellow bar saying \"Message From:\" and the name of the sender along with the message across the top of whatever the user was already doing, and presented a list of options for responding. As such, it could be considered a type of graphical user interface (GUI), albeit much more primitive than the later Unix, Windows and Macintosh based GUI IM software. OLMs were what Q-Link called \"Plus Services\" meaning they charged an extra per-minute fee on top of the monthly Q-Link access costs.\n\nModern, Internet-wide, GUI-based messaging clients as they are known today, began to take off in the mid-1990s with PowWow, ICQ, and AOL Instant Messenger. Similar functionality was offered by CU-SeeMe in 1992; though primarily an audio/video chat link, users could also send textual messages to each other. AOL later acquired Mirabilis, the authors of ICQ; a few years later ICQ (then owned by AOL) was awarded two patents for instant messaging by the U.S. patent office. Meanwhile, other companies developed their own software; (Excite, MSN, Ubique, and Yahoo!), each with its own proprietary protocol and client; users therefore had to run multiple client applications if they wished to use more than one of these networks. In 1998, IBM released IBM Lotus Sametime, a product based on technology acquired when IBM bought Haifa-based Ubique and Lexington-based Databeam.\n\nIn 2000, an open-source application and open standards-based protocol called Jabber was launched. The protocol was standardized under the name Extensible Messaging and Presence Protocol (XMPP). XMPP servers could act as gateways to other IM protocols, reducing the need to run multiple clients. Multi-protocol clients can use any of the popular IM protocols by using additional local libraries for each protocol. IBM Lotus Sametime's November 2007 release added IBM Lotus Sametime Gateway support for XMPP.\n\nAs of 2010, social networking providers often offer IM abilities. Facebook Chat is a form of instant messaging, and Twitter can be thought of as a Web 2.0 instant messaging system. Similar server-side chat features are part of most dating websites, such as OKCupid or PlentyofFish. The spread of smartphones and similar devices in the late 2000s also caused increased competition with conventional instant messaging, by making text messaging services still more ubiquitous.\n\nMany instant messaging services offer video calling features, voice over IP and web conferencing services. Web conferencing services can integrate both video calling and instant messaging abilities. Some instant messaging companies are also offering desktop sharing, IP radio, and IPTV to the voice and video features.\n\nThe term \"Instant Messenger\" is a service mark of Time Warner and may not be used in software not affiliated with AOL in the United States. For this reason, in April 2007, the instant messaging client formerly named Gaim (or gaim) announced that they would be renamed \"Pidgin\".\n\nIn the 2010s, more people started to use messaging apps like WhatsApp, WeChat, Facebook Messenger and Line than instant messaging like AIM. For example, WhatsApp was founded in 2009, and Facebook acquired in 2014, by which time it already had half a billion users.\n\nEach modern IM service generally provides its own client, either a separately installed piece of software, or a browser-based client. These usually only work within the same IM network, although some allow limited function with other services. Third party client software applications exist that will connect with most of the major IM services.\n\nStandard complementary instant messaging applications offer functions like file transfer, contact list(s), the ability to hold several simultaneous conversations, etc. These may be all the functions that a small business needs, but larger organizations will require more sophisticated applications that can work together. The solution to finding applications capable of this is to use enterprise versions of instant messaging applications. These include titles like XMPP, Lotus Sametime, Microsoft Office Communicator, etc., which are often integrated with other enterprise applications such as workflow systems. These enterprise applications, or enterprise application integration (EAI), are built to certain constraints, namely storing data in a common format.\n\nThere have been several attempts to create a unified standard for instant messaging: IETF's Session Initiation Protocol (SIP) and SIP for Instant Messaging and Presence Leveraging Extensions (SIMPLE), Application Exchange (APEX), Instant Messaging and Presence Protocol (IMPP), the open XML-based Extensible Messaging and Presence Protocol (XMPP), and Open Mobile Alliance's Instant Messaging and Presence Service developed specifically for mobile devices.\n\nMost attempts at producing a unified standard for the major IM providers (AOL, Yahoo! and Microsoft) have failed, and each continues to use its own proprietary protocol.\n\nHowever, while discussions at IETF were stalled, Reuters signed the first inter-service provider connectivity agreement in September 2003. This agreement enabled AIM, ICQ and MSN Messenger users to talk with Reuters Messaging counterparts and vice versa. Following this, Microsoft, Yahoo! and AOL agreed to a deal in which Microsoft's Live Communications Server 2005 users would also have the possibility to talk to public instant messaging users. This deal established SIP/SIMPLE as a standard for protocol interoperability and established a connectivity fee for accessing public instant messaging groups or services. Separately, on October 13, 2005, Microsoft and Yahoo! announced that by the 3rd quarter of 2006 they would interoperate using SIP/SIMPLE, which was followed, in December 2005, by the AOL and Google strategic partnership deal in which Google Talk users would be able to communicate with AIM and ICQ users provided they have an AIM account.\n\nThere are two ways to combine the many disparate protocols:\n\nSome approaches allow organizations to deploy their own, private instant messaging network by enabling them to restrict access to the server (often with the IM network entirely behind their firewall) and administer user permissions. Other corporate messaging systems allow registered users to also connect from outside the corporation LAN, by using an encrypted, firewall-friendly, HTTPS-based protocol. Usually, a dedicated corporate IM server has several advantages, such as pre-populated contact lists, integrated authentication, and better security and privacy.\n\nCertain networks have made changes to prevent them from being used by such multi-network IM clients. For example, Trillian had to release several revisions and patches to allow its users to access the MSN, AOL, and Yahoo! networks, after changes were made to these networks. The major IM providers usually cite the need for formal agreements, and security concerns as reasons for making these changes.\n\nThe use of proprietary protocols has meant that many instant messaging networks have been incompatible and users have been unable to reach users on other networks. This may have allowed social networking with IM-like features and text messaging an opportunity to gain market share at the expense of IM.\n\nUsers sometimes make use of internet slang or text speak to abbreviate common words or expressions to quicken conversations or reduce keystrokes. The language has become widespread, with well-known expressions such as 'lol' translated over to face-to-face language.\n\nEmotions are often expressed in shorthand, such as the abbreviation LOL, BRB and TTYL; respectively laugh(ing) out loud, be right back, and talk to you later.\n\nSome, however, attempt to be more accurate with emotional expression over IM. Real time reactions such as (\"chortle\") (\"snort\") (\"guffaw\") or (\"eye-roll\") are becoming more popular. Also there are certain standards that are being introduced into mainstream conversations including, '#' indicates the use of sarcasm in a statement and '*' which indicates a spelling mistake and/or grammatical error in the prior message, followed by a correction.\n\nInstant messaging has proven to be similar to personal computers, email, and the World Wide Web, in that its adoption for use as a business communications medium was driven primarily by individual employees using consumer software at work, rather than by formal mandate or provisioning by corporate information technology departments. Tens of millions of the consumer IM accounts in use are being used for business purposes by employees of companies and other organizations.\n\nIn response to the demand for business-grade IM and the need to ensure security and legal compliance, a new type of instant messaging, called \"Enterprise Instant Messaging\" (\"EIM\") was created when Lotus Software launched IBM Lotus Sametime in 1998. Microsoft followed suit shortly thereafter with Microsoft Exchange Instant Messaging, later created a new platform called Microsoft Office Live Communications Server, and released Office Communications Server 2007 in October 2007. Oracle Corporation has also jumped into the market recently with its Oracle Beehive unified collaboration software. Both IBM Lotus and Microsoft have introduced federation between their EIM systems and some of the public IM networks so that employees may use one interface to both their internal EIM system and their contacts on AOL, MSN, and Yahoo. As of 2010, leading EIM platforms include IBM Lotus Sametime, Microsoft Office Communications Server, Jabber XCP and Cisco Unified Presence. Industry-focused EIM platforms such as Reuters Messaging and Bloomberg Messaging also provide IM abilities to financial services companies.\n\nThe adoption of IM across corporate networks outside of the control of IT organizations creates risks and liabilities for companies who do not effectively manage and support IM use. Companies implement specialized IM archiving and security products and services to mitigate these risks and provide safe, secure, productive instant messaging abilities to their employees. IM is increasingly becoming a feature of enterprise software rather than a stand-alone application.\n\nIM products can usually be categorised into two types: Enterprise Instant Messaging (EIM) and Consumer Instant Messaging (CIM). Enterprise solutions use an internal IM server, however this isn't always feasible, particularly for smaller businesses with limited budgets. The second option, using a CIM provides the advantage of being inexpensive to implement and has little need for investing in new hardware or server software.\n\nFor corporate use, encryption and conversation archiving are usually regarded as important features due to security concerns. There are also a bunch of open source encrypting messengers. Sometimes the use of different operating systems in organizations requires use of software that supports more than one platform. For example, many software companies use Windows in administration departments but have software developers who use Linux.\n\nAn Instant Message Service Center (IMSC) is a network element in the mobile telephone network which delivers instant messages. When a user sends an IM message to another user, the phone sends the message to the IMSC. The IMSC stores the message and delivers it to the destination user when they are available. The IMSC usually has a configurable time limit for how long it will store the message. Few companies who make many of the IMSCs in use in the GSM world are Miyowa, Followap and OZ. Other players include Acision, Colibria, Ericsson, Nokia, Comverse Technology, Now Wireless, Jinny Software, Miyowa, Feelingk and few others.\n\nMajor IM services are controlled by their corresponding companies. They usually follow the client-server model when all clients have to first connect to the central server. This requires users to trust this server because messages can generally be accessed by the company. Companies can be compelled to reveal their user's communication. Companies can also suspend user accounts for any reason. There is the class of instant messengers that uses the serverless model, which doesn't require servers, and the IM network consists only of clients. There are several serverless messengers: RetroShare, Tox, Bitmessage, Ricochet, Ring. Serverless messengers are generally more secure because they involve fewer parties.\n\nConversational commerce is e-commerce via various means of messaging:\n\n\nCrackers (malicious or black hat hackers) have consistently used IM networks as vectors for delivering phishing attempts, \"poison URLs\", and virus-laden file attachments from 2004 to the present, with over 1100 discrete attacks listed by the IM Security Center in 2004–2007. Hackers use two methods of delivering malicious code through IM: delivery of viruses, trojan horses, or spyware within an infected file, and the use of \"socially engineered\" text with a web address that entices the recipient to click on a URL connecting him or her to a website that then downloads malicious code.\n\nViruses, computer worms, and trojans usually propagate by sending themselves rapidly through the infected user's contact list. An effective attack using a poisoned URL may reach tens of thousands of users in a short period when each user's contact list receives messages appearing to be from a trusted friend. The recipients click on the web address, and the entire cycle starts again. Infections may range from nuisance to criminal, and are becoming more sophisticated each year.\n\nIM connections sometimes occur in plain text, making them vulnerable to eavesdropping. Also, IM client software often requires the user to expose open UDP ports to the world, raising the threat posed by potential security vulnerabilities.\n\nIn addition to the malicious code threat, the use of instant messaging at work also creates a risk of non-compliance to laws and regulations governing use of electronic communications in businesses.\n\nIn the United States alone there are over 10,000 laws and regulations related to electronic messaging and records retention. The better-known of these include the Sarbanes–Oxley Act, HIPAA, and SEC 17a-3.\n\nClarification from the Financial Industry Regulatory Authority (FINRA) was issued to member firms in the financial services industry in December, 2007, noting that \"electronic communications\", \"email\", and \"electronic correspondence\" may be used interchangeably and can include such forms of electronic messaging as \"instant messaging\" and text messaging. Changes to Federal Rules of Civil Procedure, effective December 1, 2006, created a new category for electronic records which may be requested during discovery in legal proceedings.\n\nMost nations also regulate use of electronic messaging and electronic records retention in similar fashion as the United States. The most common regulations related to IM at work involve the need to produce archived business communications to satisfy government or judicial requests under law. Many instant messaging communications fall into the category of business communications that must be archived and retrievable.\n\nIn the early 2000s, a new class of IT security provider emerged to provide remedies for the risks and liabilities faced by corporations who chose to use IM for business communications. The IM security providers created new products to be installed in corporate networks for the purpose of archiving, content-scanning, and security-scanning IM traffic moving in and out of the corporation. Similar to the e-mail filtering vendors, the IM security providers focus on the risks and liabilities described above.\n\nWith rapid adoption of IM in the workplace, demand for IM security products began to grow in the mid-2000s. By 2007, the preferred platform for the purchase of security software had become the \"computer appliance\", according to IDC, who estimate that by 2008, 80% of network security products will be delivered via an appliance.\n\nBy 2014 however, the level of safety offered by instant messengers was still extremely poor. According to a scorecard made by the Electronic Frontier Foundation, only 7 out of 39 instant messengers received a perfect score, whereas the most popular instant messengers at the time only attained a score of 2 out of 7. A number of studies have shown that IM services are quite vulnerable for providing user privacy.\n\nMore than 100 million users\nOther platforms\n\n\n\n"}
{"id": "43052203", "url": "https://en.wikipedia.org/wiki?curid=43052203", "title": "KDS (company)", "text": "KDS (company)\n\nKDS is a SaaS technology provider for online business travel booking and expense management. Yves Weisselberger founded the company in 1994 and served as its CEO until 2011. Dean Forbes took the position of CEO after Weisselberger. The company is based in Issy-les-Moulineaux, France with additional locations in Germany, United Kingdom, United States of America.\n\nKDS develops Neo, a cloud-based tool which enables business users to search and book complete door-to-door itineraries, instead of the traditional way of booking individual components. Using Neo, employees also manage business expenses through a calendar-based view instead of the common spreadsheet or line-item report approach.\n\nYves Weisselberger founded KDS in 1994. In 2011, Weisselberger stepped down as CEO and chose Dean Forbes as his replacement. Weisselberger chose Forbes to take his position because of Forbes's outside perspective.\n\nKDS launched Neo, a self-booking tool in January 2013. The tool was developed on the concept of booking an entire trip from door to door with a departure location, destination, and required time of arrival. The company also partnered with various payment partners to expand payment methods for transactions completed in the booking tool and a variety of travel inventory suppliers beyond air, rail and hotel to connect such as Groundscope, Snapcar, Addison Lee and Booking.com. In September 2013, KDS signed a five-year contract with Orange, a mobile network operator and internet provider in France, allowing Orange to use Neo for its business.\n\nOn January 29, 2014, the company's booking tool gained access to Renfe Operadora's content. KDS gained access after it partnered with SilverRail Technologies. The partnership also gave KDS access to North American rail content via Amtrak and Italy with Trenitalia.\n\nOn July 16, 2016, KDS released its latest version of Neo which for the first time combined the functionality of its legacy corporate booking tool, the door-to-door search and calendar-view introduced in Neo under a completely redesigned consumer-grade user interface.\n\nKDS released the results of a survey the company took in 2013 asking business travelers about their company's travel safety. The survey found that 15% of travelers believed that their employers were unaware of their whereabouts and that an additional 23% regularly booked outside of the company defined booking processes.\n\nLater that year, KDS and EPSA online surveyed 200 employees of companies of all sizes on expense fraud. Results found that 60% of employees thought cheating mileage expenses was justifiable because the system is poorly organized. 85% of participants stated that they had never cheated in their request for the reimbursement of business expenses. The survey also found that employee reimbursement time on average is 17 days and could take up to month.\n\nIn 2016, KDS launched a survey devised to investigate the attitudes, actions and behaviors of UK and U.S. business professionals when booking and managing their work-related travel and reporting travel expenses. The survey questioned over 1,000 participants and revealed that almost one quarter (22%) embellish their business travel expenses by regularly rounding up business mileage claims by 1 and 10 miles per journey.\n\nKDS developed door-to-door reservations tool Neo to manage travel booking and business travellers. Describing the concept of open booking as \"failure dressed up as innovation,\" Dean Forbes (KDS CEO) said KDS will emphasize Neo because he is confident it can sufficiently deter travelers from using alternative, unofficial booking channels. Open booking aims to collect data from bookings that take place on public websites.\n\nKDS launched Neo, a door-to-door corporate booking tool, in January 2013. The tool provides consumers live street view and map overlays while displaying door-to-door itinerary time lines with steps including walking, driving, biking, rail, hotel, flights and dining. In November 2013, Neo had 11 active clients with over 300,000 profiles.\n\nIn 2014, KDS developed the Neo expense solution including an optical character recognition tool, that captures data and turns it into text. The tool aims to get rid of the collection of receipts by uploading them to a user's expense system as a line item. The expense system is calendar based to provide users with context on when and how they incurred an expense.\n\nIn 2015, the company rolled out the Neo Move mobile app for the iPhone and Apple Watch, branded as an ‘on-trip companion and guide’ by providing the traveler with real-time, location-based information and the ability to scan expense receipts on the go.\n"}
{"id": "17870639", "url": "https://en.wikipedia.org/wiki?curid=17870639", "title": "List of traditional armaments", "text": "List of traditional armaments\n\nThis list of traditional armaments tries to include all \"traditional\" armaments. Essentially anything that is wieldable, excluding \"modern\" (post American civil war) firearms. It lists everything by typology in easy-to-reference tables for your searching convenience.\n\nThis catalog does not however presume to list every dialectal variant for the word spear, sword, etc. It only attempts to provide a broad range of designs or unique characteristics of various arms of historical or regional significance, i.e. items that stand out from the standard norm. This catalog may include some modern examples but it tries to lean towards weapons of the past.\n\n\"For mythological or fictional arms see:\"\n\nMartial uses\n\nFirst column header Prime example for comparison (weapon style/usage notation)\n\nEra\n\n\n\nA pole with a weaponized tip; often used to counter mounted cavalry or to aid in infantry charges. Their benefit is their reach. Their hindrance is they are often hard to wield and transport, and tie up both hands; this usually leads to the need of a backup weapon such as a sword.\n\"conical(spiked) or triangular (dagger) tipped – used primarily for impaling \"\n\n\n—Modern rifle pikes\n\n\n\n\n\n\n\n\n\n\n\n Socketed heads, fitted on a short staff or stick carved into a handle \"HELVE\" \n\n\" Used to break armor, shields,and bone by virtue of kinetic impact \"\n\n\"wedge shaped or crescent edged blade used to cleave\"\nCan be thrown or wielded, hafted weapons consist of a heavy head inset on a short handle and are used to deliver powerful blows\n\n\n\nLonger helve to accommodate two hands and a heavier tip\n\n\n\n\nTang fitted into a handle, can often have hand guards\n\n\n\nThe iffy category between dagger and knife and sword. Most chopping backswords are here as well as daggers over 12\"\n\n\n\n\n\n\n\n\nThe two-handed sword's hilt is long enough to accommodate both hands; to increase thrust or chop power. Many can be held with just one hand but are more effective with two\n\n\n\n\n\nEnhances punching power by increasing mass of the hand or concentrating force onto a smaller area\n\n\n\nblunt (but not always) weapons. traditionally fabricated from naturally originating material, often using only stoneage technology. Primarily used to cause deep tissue damage (contusions) rather than cuts.\n\n\nClubs tend to be hefty and sturdy; typically graduating from a handle to a weighted tip of larger size. Some have an edge that can split open a head. Normally fabricated from a single piece of wood. \n\n\nTend to be heavier and less balanced then the quarter staff, usually made from a single piece of hardened wood or other material. Typically too short to be considered a polearm and too off-balance to be effectively used with only one hand \n\n\n \"soft\" as opposed to \"rigid\" weapons. Typically they consist of a handle and a weaponized tip attached to each other by a flexible body of rope, chain, etc. This gives them a longer reach or incorporates mechanical advantage to increase momentum and thus striking power. The trade-off is that they are more difficult to control and may harm nearby allies or even the wielder himself. they also require open space so not to be snagged on or impeded by nearby obstacles and thus rendered ineffective. \n\n\n\n\n\nJust too fantastic to believe, but there is truth in it\n\n\nUses nothing but old fashioned muscle power, but may utilize a tool for extra leverage\n\n\n\n\nObjects launched by an engine/device held in hand that impart a velocity unattainable if thrown \n\n\n\nProjectiles often formed from lead and in various shapes; launched by a device using the force of expanding gas created from the burning of black powder (gunpowder) contained in a semi-sealed tube\nPistols/short barrel\n\n\nShouldered guns/long barrel\n\n\n\n\n Anything worn for protection but not considered as typical clothing \n\n\n\n\n\"Help to ensure heirs and to attach sidearms\"\n\n\n\n\n\n\"broaches/clasps/baldrics/banners/trumps\"\n\n\n\nweapon play\n\ntop\n"}
{"id": "45317362", "url": "https://en.wikipedia.org/wiki?curid=45317362", "title": "Live online tutoring", "text": "Live online tutoring\n\nLive online tutoring is the process of tutoring in an online environment, with teacher and student interacting in real-time without necessarily being in the same place. This real-time element, whilst presenting a significant technical challenge, sets live online tutoring apart from traditional online tutoring as it attempts to mimic in-person interaction as closely as possible rather than simply facilitating knowledge transfer.\n\nLive online tutoring is a relatively recent concept, originally pioneered by NetTutor in 1996 and popularized by the live online tutoring platform Tutor.com from 1998 in the US. It has developed alongside far more widespread asynchronous online tutoring and learning experiences, in turn popularized by organizations such as Khan Academy, universities and other educational institutions with the introduction of Massive Open Online Courses (MOOCS). These efforts were aimed primarily at people without local access to appropriate teachers and tutors, in an attempt to remove geographical barriers from education.\n\nHowever, lack of certain key technology such as reliable remote video and audio communication, and a lack of widespread stable high-bandwidth Internet access, prevented these early efforts from including a significant live component in their offering. Early innovations came from remote language learning services such as italki, where a stable audio connection was sufficient to deliver a reasonable service. In most contexts, online tutoring was thus fundamentally different from face-to-face tutoring, more closely resembling correspondence teaching than sitting in a classroom. Today, there are hundreds of companies and academic institutions based around the world offering live remote learning in a huge range of subjects, both in the context of academic learning and industrial training. With current technology and increasing Internet penetration in the developing world, there is a renewed interest in live offerings as exemplified by Chegg's recent acquisition of InstaEDU for $30 million.\n\nExisting live online tutoring services frequently include the following features to connect students and tutors:\n\n\nSome services also provide\n\n\nThe overall environment aims to reproduce the level of interaction present in a face-to-face session, and is fundamentally different from software packages and services for screen-casting and videoconferencing such as WebEx, where the focus is on one-way transmission of information.\n\nLive online tutoring has several advantages over face-to-face tutoring, although suffers from shortcomings in ease of non-verbal communication.\n\nWith the development of wearable technology, in particular smart headsets, richer interaction will become possible as companies work to mimic face-to-face interaction more closely. The language learning service Duolingo, for example, is already fully compatible with Google Glass.\n\n"}
{"id": "1836048", "url": "https://en.wikipedia.org/wiki?curid=1836048", "title": "Louis Couffignal", "text": "Louis Couffignal\n\nLouis Pierre Couffignal (16 March 1902 – 4 July 1966) was a French mathematician and cybernetics pioneer, born in Monflanquin. He taught in schools in the southwest of Brittany, then at the naval academy and, eventually, at the Buffon School.\n\nAfter joining the school, Couffignal hesitated to write a thesis on Symbolic Logic but after his meetings with Philbert Maurice d'Ocagne, he decided to focus on machines and on Mechanical Logic.\n\nHe published a variety of notes at the Academy of Sciences, with a focus on using binary computation by machines to solve new problems. Following Leibniz, he promoted binary numbers as the basis of calculating machines. Couffignal received his Doctorate of Sciences in 1938 with his thesis on Mechanical Analysis, demonstrating applications for machines to calculate celestial mechanics. Couffignal took on an interest in Cybernetics, influenced by his meetings with Louis Lapicque in 1941 and the cyberneticist Norbert Wiener in 1946.\n\nWith Lapicque, Couffignal compared the functioning of the nervous system and that of machines, as Wiener prepared his book \"Cybernetics\", the book that established the foundations for the subject.\n\nBetween 1938 and 1960, Couffignal was the director of the Blaise Pascal Calculation Center. In 1945, he was named Inspector General of Public Teaching. In 1951, Couffignal prepared an international conference on thinking machines to bring together the greatest specialists in this new science, including Norbert Wiener, W. Ross Ashby, Warren McCulloch, etc. As Inspector General, he created the first BTS teaching degree in France.\n\nCouffignal wrote several books and articles. A selection:\n\n\n"}
{"id": "43143940", "url": "https://en.wikipedia.org/wiki?curid=43143940", "title": "Maths Doctor", "text": "Maths Doctor\n\nMaths Doctor is a UK-based online tuition agency, located at Sussex University Innovation Centre.\n\nMaths Doctor was founded in Brighton, Sussex in 2008 by Simon Walsh, a Chartered Financial Analyst and former teacher. Walsh initially worked as an investment banker before re-training as a secondary maths teacher. After teaching in both independent and state schools, Walsh generated the concept of individually tailored online tuition sessions, culminating in his creation of Maths Doctor and its online tuition services.\n\nIn 2012, the agency was acquired by the educational organisation Macmillan Digital Education. Maths Doctor founder Simon Walsh was assigned as Managing Director of Maths Doctor following its acquisition.\n\nThe agency has a network of over 500 tutors across the United Kingdom, delivering live online lessons. All Maths Doctor tutors are DBS checked and receive training according to Local Safeguarding Children Board procedures and Department of Education guidelines.\n\nMaths Doctor tutors facilitate live one-to-one home tutoring via the internet, as well as worksheets and tutoring videos. To use these services, students receive an electronic pen and pad which connects to their computer via USB allowing students to access a shared white board with their tutor. Tutors offer preparation for various academic levels including Primary, Secondary and Further Education. Regular curriculum support and examination preparation is offered for 11+ and Common Entrance exams, KS2, KS3, GCSE, AS & A-Level, STEP and University examinations.\n\nIn 2010, Maths Doctor won the Midas Touch Award for Innovation at the London Business Start Up exhibition. In 2014, the company was announced as winner of the Private Tutoring Company category at the \"Education Investor\" Awards. Maths Doctor's online learning practices have also been covered by press agencies and online educational forums including \"The Huffington Post\", \"Forbes\", \"Mumsnet\" and \"The Good Schools Guide\".\n\n"}
{"id": "20757", "url": "https://en.wikipedia.org/wiki?curid=20757", "title": "Miguel de Icaza", "text": "Miguel de Icaza\n\nMiguel de Icaza (born November 23, 1972) is a Mexican-American programmer, best known for starting the GNOME, Mono, and Xamarin projects.\n\nDe Icaza was born in Mexico City and studied Mathematics at the Universidad Nacional Autonoma de México (UNAM), but never received a degree. He came from a family of scientists in which his father is a physicist and his mother a biologist. He started writing free software in 1992.\n\nOne of the earliest pieces of software he wrote for Linux was the Midnight Commander file manager, a text-mode file manager. He was also one of the early contributors to the Wine project.\n\nHe worked with David S. Miller on the Linux SPARC port and wrote several of the video and network drivers in the port, as well as the libc ports to the platform. They both later worked on extending Linux for MIPS to run on SGI's Indy computers and wrote the original X drivers for the system. With Ingo Molnar he wrote the original software implementation of RAID-1 and RAID-5 drivers of the Linux kernel.\n\nIn summer of 1997, he was interviewed by Microsoft for a job in the Internet Explorer Unix team (to work on a SPARC port), but lacked the university degree required to obtain a work H-1B visa. He said in an interview that he tried to persuade his interviewers to free the IE code even before Netscape did so with their own browser.\n\nDe Icaza started the GNOME project with Federico Mena in August 1997 to create a completely free desktop environment and component model for Linux and other Unix-like operating systems. He also created the GNOME spreadsheet program, Gnumeric.\n\nIn 1999, de Icaza, along with Nat Friedman, co-founded Helix Code, a GNOME-oriented free software company that employed a large number of other GNOME hackers. In 2001, Helix Code, later renamed Ximian, announced the Mono Project, to be led by de Icaza, with the goal to implement Microsoft's new .NET development platform on Linux and Unix-like platforms. In August 2003, Ximian was acquired by Novell. There, de Icaza was Vice President of Developer Platform.\n\nIn May 2011, de Icaza started Xamarin to replace MonoTouch and Mono for Android after Novell was bought by Attachmate and the projects were abandoned. Shortly afterwards, Xamarin and Novell reached an agreement where Xamarin took over the development and sales of these products.\n\nIn February 2016, Xamarin announced being acquired by Microsoft. One month later in Microsoft Build conference, it was announced that the Mono Project would be relicensed to MIT, Visual Studio would include Xamarin (even the free versions) without restrictions, and Xamarin SDKs would be opensourced.\n\nDe Icaza endorsed Microsoft's Office Open XML (OOXML) document standard, disagreeing with a lot of the widespread criticism in the open source and free-software community.\n\nHe also developed Mono – a free and open-source alternative to Microsoft's .NET Framework – for GNOME. This has raised much disagreement due to the patents that Microsoft holds on the .NET Framework.\n\nDe Icaza was criticized by Richard Stallman on the Software Freedom Day 2009, who labeled him as \"Traitor to the Free Software Community\". Icaza responded on his blog to Stallman with the remark that he believes in a \"world of possibility\" and that he is open for discussions on ways to improve the pool of open source and free software.\n\nIn August 2012, de Icaza criticized the Linux desktop as \"killed by Apple\". De Icaza specifically criticized a generally developer-focused culture, lack of backward compatibility and fragmentation among the various Linux distributions. In March 2013, de Icaza announced on his personal blog that he regularly used macOS instead of Linux for desktop computing.\n\nIn 2014 he joined Anders Hejlsberg on stage during the announcements of the .NET Foundation and the open sourcing of Microsoft's C# Compiler. He serves on the board of directors of the .NET Foundation.\n\nMiguel de Icaza has received the Free Software Foundation 1999 Award for the Advancement of Free Software, the MIT Technology Review Innovator of the Year Award 1999, and was named one of \"Time\" magazine's 100 innovators for the new century in September 2000.\n\nIn early 2010 he received a Microsoft MVP Award.\n\nIn March 2010, he was named as the fifth in the \"Most Powerful Voices in Open Source\" by MindTouch.\n\nDe Icaza has had cameo appearances in the 2001 motion pictures \"Antitrust\" and \"The Code.\"\n\nHe married Brazilian Maria Laura Soares da Silva (now Maria Laura de Icaza) in 2003.\n\nDe Icaza is critical of the actions of the state of Israel towards the Palestinians in the Middle East and has blogged about the subject.\n\nDe Icaza was granted US citizenship in January 2015.\n\n"}
{"id": "6078697", "url": "https://en.wikipedia.org/wiki?curid=6078697", "title": "Music instrument technology", "text": "Music instrument technology\n\nMusic instrument technology refers to the construction of instruments and the way they have changed over time. Such change has produced modern instruments that are considerably different from their historical antecedents.\n\nAn example is the way in which many instruments commonly associated with a modern symphony orchestra are markedly different from the same instruments for which European composers were composing music centuries ago.\n\nSuch changes include the addition of piston valves to brass instruments, the design of more complex fingering systems for woodwind instruments such as the flute, and the standardization of the family of orchestral string instruments.\n\nMany advancements were made in music instrument technology during the Middle Ages and 19th Century. The introduction of copper smelting allowed for trumpets, organ pipes, and slides to be constructed with sheet metal which had a smooth texture and consistency in thickness, allowing for more range of tones and sounds. Improvements in molding and casting during the 19th Century allowed for technological advancement to pianos. While originally constructed with wooden frames, limiting the amount of sound that could be produced, pianos began to be constructed of one-piece iron frames. This provided a more amplified volume from the instrument and allowed musicians to use less force when playing the instrument. Improvements in drum tuning were also established at this time. The \"Dresden\" model of tuning, involving steel technology and employing a foot petal with ratchet in order to attach the device to the timpani, was invented by Carl Pittrich. This technology allowed for timpani to be tuned much faster by the musician. The Dresden tuners could also be added onto existing timpani, allowing symphonies to continue using their existing instruments while still employing this new technology. Lastly, the 19th century also lead to the development of valves, and when added in to the construction of trumpets and horns, they allowed for the instruments to express a broader range to the harmonic series of notes being produced.\n\nSome of this technology represents patentable advancements in the musical instrument industry. See Musical Instrument Patent of Week\n\n\nBowles, Edmund A. \"The Impact of Technology on Musical Instruments.\" \"Cosmos Club.\" N.p., n.d. Web 16 Oct. 2013.\n"}
{"id": "1786529", "url": "https://en.wikipedia.org/wiki?curid=1786529", "title": "Network transparency", "text": "Network transparency\n\nNetwork transparency, in its most general sense, refers to the ability of a protocol to transmit data over the network in a manner which is transparent (invisible) to those using the applications that are using the protocol. In this way, users of a particular application may access remote resources in the same manner in which they would access their own local resources. An example of this is cloud storage, where remote files are presented as being locally accessible, and cloud computing where the resource in question is processing. \n\nThe term is often partially correctly applied in the context of the X Window System, which is able to transmit graphical data over the network and integrate it seamlessly with applications running and displaying locally; however, certain extensions of the X Window System are not capable of working over the network.\n\nIn a centralized database system, the only available resource that needs to be shielded from the user is the data (that is, the storage system). In a distributed DBMS, a second resource needs to be managed in much the same manner: the network. Preferably, the user should be protected from the network operational details. Then there would be no difference between database applications that would run on the centralized database and those that would run on a distributed one. This kind of transparency is referred to as network transparency or distribution transparency. From a database management system (DBMS) perspective, distribution transparency requires that users do not have to specify where data is located.\n\nSome have separated distribution transparency into location transparency and naming transparency.\n\nLocation transparency in commands used to perform a task is independent both of the locations of the data, and of the system on which an operation is carried out.\n\nNaming transparency means that a unique name is provided for each object in the database.\n\nTransparency in firewall technology can be defined at the networking (IP or Internet layer) or at the application layer.\n\nTransparency at the IP layer means the client targets the real IP address of the server. If a connection is non-transparent, then the client targets an intermediate host (address), which could be a proxy or a caching server. IP layer transparency could be also defined from the point of server's view. If the connection is transparent, the server sees the real client IP. If it is non-transparent, the server sees the IP of the intermediate host.\n\nTransparency at the application layer means the client application uses the protocol in a different way. An example of a transparent HTTP request for a server:\nGET / HTTP/1.1\nHost: example.org\nConnection: Keep-Alive\nAn example non-transparent HTTP request for a proxy (cache):\nGET http://foo.bar/ HTTP/1.1\nProxy-Connection: Keep-Alive\nApplication layer transparency is symmetric when the same working mode is used on both the sides. The transparency is asymmetric when the firewall (usually a proxy) converts server type requests to proxy type or vice versa.\n\nTransparency at the IP layer does not mean automatically application layer transparency.\n\n"}
{"id": "163806", "url": "https://en.wikipedia.org/wiki?curid=163806", "title": "Oil platform", "text": "Oil platform\n\nAn oil platform, offshore platform, or offshore drilling rig is a large structure with facilities for well drilling to explore, extract, store, process petroleum and natural gas which lies in rock formations beneath the seabed. In many cases, the platform contains facilities to house the workforce as well.\n\nMost commonly, oil platforms engage in activities on the continental shelf, though they can also be used in lakes, inshore waters and inland seas.\n\nDepending on the circumstances, the platform may be fixed to the ocean floor, may consist of an artificial island, or may float. Remote subsea wells may also be connected to a platform by flow lines and by umbilical connections. These sub-sea solutions may consist of one or more subsea wells, or of one or more manifold centres for multiple wells.\n\nOffshore drilling presents environmental challenges, both from the produced hydrocarbons and the materials used during the drilling operation. Controversies include the ongoing U.S. offshore drilling debate.\n\nThere are many different types of facilities from which offshore drilling operations take place. These include bottom founded drilling rigs (jackup barges and swamp barges), combined drilling and production facilities either bottom founded or floating platforms, and deepwater mobile offshore drilling units (MODU) including semi-submersibles and drillships. These are capable of operating in water depths up to . In shallower waters the mobile units are anchored to the seabed, however in deeper water (more than ) the semisubmersibles or drillships are maintained at the required drilling location using dynamic positioning.\n\nAround 1891, the first submerged oil wells were drilled from platforms built on piles in the fresh waters of the Grand Lake St. Marys (a.k.a. Mercer County Reservoir) in Ohio. The wide but shallow reservoir was built from 1837 to 1845 to provide water to the Miami and Erie Canal.\n\nAround 1896, the first submerged oil wells in salt water were drilled in the portion of the Summerland field extending under the Santa Barbara Channel in California. The wells were drilled from piers extending from land out into the channel.\n\nOther notable early submerged drilling activities occurred on the Canadian side of Lake Erie since 1913 and Caddo Lake in Louisiana in the 1910s. Shortly thereafter, wells were drilled in tidal zones along the Gulf Coast of Texas and Louisiana. The Goose Creek field near Baytown, Texas is one such example. In the 1920s, drilling was done from concrete platforms in Lake Maracaibo, Venezuela.\n\nThe oldest offshore well recorded in Infield's offshore database is the Bibi Eibat well which came on stream in 1923 in Azerbaijan. Landfill was used to raise shallow portions of the Caspian Sea.\n\nIn the early 1930s, the Texas Company developed the first mobile steel barges for drilling in the brackish coastal areas of the gulf.\n\nIn 1937, Pure Oil Company (now Chevron Corporation) and its partner Superior Oil Company (now part of ExxonMobil Corporation) used a fixed platform to develop a field in of water, one mile (1.6 km) offshore of Calcasieu Parish, Louisiana.\n\nIn 1938, Humble Oil built a mile-long wooden trestle with railway tracks into the sea at McFadden Beach on the Gulf of Mexico, placing a derrick at its end - this was later destroyed by a hurricane.\n\nIn 1945, concern for American control of its offshore oil reserves caused President Harry Truman to issue an Executive Order unilaterally extending American territory to the edge of its continental shelf, an act that effectively ended the 3-mile limit \"freedom of the seas\" regime.\n\nIn 1946, Magnolia Petroleum (now ExxonMobil) drilled at a site off the coast, erecting a platform in of water off St. Mary Parish, Louisiana.\n\nIn early 1947, Superior Oil erected a drilling/production platform in of water some 18 miles off Vermilion Parish, Louisiana. But it was Kerr-McGee Oil Industries (now Anadarko Petroleum Corporation), as operator for partners Phillips Petroleum (ConocoPhillips) and Stanolind Oil & Gas (BP), that completed its historic Ship Shoal Block 32 well in October 1947, months before Superior actually drilled a discovery from their Vermilion platform farther offshore. In any case, that made Kerr-McGee's well the first oil discovery drilled out of sight of land.\n\nThe British Maunsell Forts constructed during World War II are considered the direct predecessors of modern offshore platforms. Having been pre-constructed in a very short time, they were then floated to their location and placed on the shallow bottom of the Thames and the Mersey estuary.\n\nIn 1954, the first jackup oil rig was ordered by Zapata Oil. It was designed by R. G. LeTourneau and featured three electro-mechanically-operated lattice type legs. Built on the shores of the Mississippi river by the LeTourneau Company, it was launched in December 1955, and christened 'Scorpion'. The Scorpion was put into operation in May 1956 off Port Aransas, Texas. It was lost in 1969.\n\nWhen offshore drilling moved into deeper waters of up to , fixed platform rigs were built, until demands for drilling equipment was needed in the to depth of the Gulf of Mexico, the first jack-up rigs began appearing from specialized offshore drilling contractors such as forerunners of ENSCO International.\n\nThe first semi-submersible resulted from an unexpected observation in 1961. Blue Water Drilling Company owned and operated the four-column submersible Blue Water Rig No.1 in the Gulf of Mexico for Shell Oil Company. As the pontoons were not sufficiently buoyant to support the weight of the rig and its consumables, it was towed between locations at a draught midway between the top of the pontoons and the underside of the deck. It was noticed that the motions at this draught were very small, and Blue Water Drilling and Shell jointly decided to try operating the rig in the floating mode. The concept of an anchored, stable floating deep-sea platform had been designed and tested back in the 1920s by Edward Robert Armstrong for the purpose of operating aircraft with an invention known as the 'seadrome'. The first purpose-built drilling semi-submersible \"Ocean Driller\" was launched in 1963. Since then, many semi-submersibles have been purpose-designed for the drilling industry mobile offshore fleet.\n\nThe first offshore drillship was the \"CUSS 1\" developed for the Mohole project to drill into the Earth's crust.\n\nAs of June, 2010, there were over 620 mobile offshore drilling rigs (Jackups, semisubs, drillships, barges) available for service in the competitive rig fleet.\n\nOne of the world's deepest hubs is currently the Perdido in the Gulf of Mexico, floating in 2,438 meters of water. It is operated by Royal Dutch Shell and was built at a cost of $3 billion. The deepest operational platform is the Petrobras America Cascade FPSO in the Walker Ridge 249 field in 2,600 meters of water.\n\nNotable offshore fields include:\n\nLarger lake- and sea-based offshore platforms and drilling rig for oil. \n\n~fixed are those platforms which are fixed at a particular place and cannot be changed.\n\nThese platforms consist of slender, flexible towers and a pile foundation supporting a conventional deck for drilling and production operations. Compliant towers are designed to sustain significant lateral deflections and forces, and are typically used in water depths ranging from .\n\nThese platforms have hulls (columns and pontoons) of sufficient buoyancy to cause the structure to float, but of weight sufficient to keep the structure upright. Semi-submersible platforms can be moved from place to place and can be ballasted up or down by altering the amount of flooding in buoyancy tanks. They are generally anchored by combinations of chain, wire rope or polyester rope, or both, during drilling and/or production operations, though they can also be kept in place by the use of dynamic positioning. Semi-submersibles can be used in water depths from .\n\nJack-up Mobile Drilling Units (or jack-ups), as the name suggests, are rigs that can be jacked up above the sea using legs that can be lowered, much like jacks. These MODUs (Mobile Offshore Drilling Units) are typically used in water depths up to , although some designs can go to depth. They are designed to move from place to place, and then anchor themselves by deploying their legs to the ocean bottom using a rack and pinion gear system on each leg.\n\nA drillship is a maritime vessel that has been fitted with drilling apparatus. It is most often used for exploratory drilling of new oil or gas wells in deep water but can also be used for scientific drilling. Early versions were built on a modified tanker hull, but purpose-built designs are used today. Most drillships are outfitted with a dynamic positioning system to maintain position over the well. They can drill in water depths up to .\n\nThe main types of floating production systems are FPSO (floating production, storage, and offloading system). FPSOs consist of large monohull structures, generally (but not always) shipshaped, equipped with processing facilities. These platforms are moored to a location for extended periods, and do not actually drill for oil or gas. Some variants of these applications, called FSO (floating storage and offloading system) or FSU (floating storage unit), are used exclusively for storage purposes, and host very little process equipment. This is one of the best sources for having floating production.\n\nThe world's first floating liquefied natural gas (FLNG) facility is currently under development. See the section on particularly large examples below.\n\nTLPs are floating platforms tethered to the seabed in a manner that eliminates most vertical movement of the structure. TLPs are used in water depths up to about . The \"conventional\" TLP is a 4-column design which looks similar to a semisubmersible. Proprietary versions include the Seastar and MOSES mini TLPs; they are relatively low cost, used in water depths between . Mini TLPs can also be used as utility, satellite or early production platforms for larger deepwater discoveries.\n\nA GBS can either be steel or concrete and is usually anchored directly onto the seabed. Steel GBS are predominantly used when there is no or limited availability of crane barges to install a conventional fixed offshore platform, for example in the Caspian Sea. There are several steel GBS in the world today (e.g. offshore Turkmenistan Waters (Caspian Sea) and offshore New Zealand). Steel GBS do not usually provide hydrocarbon storage capability. It is mainly installed by pulling it off the yard, by either wet-tow or/and dry-tow, and self-installing by controlled ballasting of the compartments with sea water. To position the GBS during installation, the GBS may be connected to either a transportation barge or any other barge (provided it is large enough to support the GBS) using strand jacks. The jacks shall be released gradually whilst the GBS is ballasted to ensure that the GBS does not sway too much from target location.\n\nSpars are moored to the seabed like TLPs, but whereas a TLP has vertical tension tethers, a spar has more conventional mooring lines. Spars have to-date been designed in three configurations: the \"conventional\" one-piece cylindrical hull; the \"truss spar\", in which the midsection is composed of truss elements connecting the upper buoyant hull (called a hard tank) with the bottom soft tank containing permanent ballast; and the \"cell spar\", which is built from multiple vertical cylinders. The spar has more inherent stability than a TLP since it has a large counterweight at the bottom and does not depend on the mooring to hold it upright. It also has the ability, by adjusting the mooring line tensions (using chain-jacks attached to the mooring lines), to move horizontally and to position itself over wells at some distance from the main platform location. The first production spar was Kerr-McGee's Neptune, anchored in in the Gulf of Mexico; however, spars (such as Brent Spar) were previously used as FSOs.\n\nEni's Devil's Tower located in of water in the Gulf of Mexico, was the world's deepest spar until 2010. The world's deepest platform is currently the Perdido spar in the Gulf of Mexico, floating in 2,438 metres of water. It is operated by Royal Dutch Shell and was built at a cost of $3 billion.\n\nThe first truss spars were Kerr-McGee's Boomvang and Nansen.\nThe first (and only) cell spar is Kerr-McGee's Red Hawk.\n\nThese installations, sometimes called toadstools, are small platforms, consisting of little more than a well bay, helipad and emergency shelter. They are designed to be operated remotely under normal conditions, only to be visited occasionally for routine maintenance or well work.\n\nThese installations, also known as satellite platforms, are small unmanned platforms consisting of little more than a well bay and a small process plant. They are designed to operate in conjunction with a static production platform which is connected to the platform by flow lines or by umbilical cable, or both.\n\nThe Petronius Platform is a compliant tower in the Gulf of Mexico modeled after the Hess Baldpate platform, which stands above the ocean floor. It is one of the world's tallest structures.\n\nThe Hibernia platform in Canada is the world's largest (in terms of weight) offshore platform, located on the Jeanne D'Arc Basin, in the Atlantic Ocean off the coast of Newfoundland. This \"gravity base structure\" (GBS), which sits on the ocean floor, is high and has storage capacity for of crude oil in its high caisson. The platform acts as a small concrete island with serrated outer edges designed to withstand the impact of an iceberg. The GBS contains production storage tanks and the remainder of the void space is filled with ballast with the entire structure weighing in at 1.2 million tons.\n\nRoyal Dutch Shell is currently developing the first Floating Liquefied Natural Gas (FLNG) facility, which will be situated approximately 200 km off the coast of Western Australia and is due for completion around 2017. When finished, it will be the largest floating offshore facility. It is expected to be approximately 488m long and 74m wide with displacement of around 600,000t when fully ballasted.\n\nA typical oil production platform is self-sufficient in energy and water needs, housing electrical generation, water desalinators and all of the equipment necessary to process oil and gas such that it can be either delivered directly onshore by pipeline or to a floating platform or tanker loading facility, or both. Elements in the oil/gas production process include wellhead, production manifold, production separator, glycol process to dry gas, gas compressors, water injection pumps, oil/gas export metering and main oil line pumps.\n\nLarger platforms assisted by smaller ESVs (emergency support vessels) like the British Iolair that are summoned when something has gone wrong, \"e.g.\" when a search and rescue operation is required. During normal operations, PSVs (platform supply vessels) keep the platforms provisioned and supplied, and AHTS vessels can also supply them, as well as tow them to location and serve as standby rescue and firefighting vessels.\n\nNot all of the following personnel are present on every platform. On smaller platforms, one worker can perform a number of different jobs. The following also are not names officially recognized in the industry:\n\n\nDrill crew will be on board if the installation is performing drilling operations. A drill crew will normally comprise:\nWell services crew will be on board for well work. The crew will normally comprise:\n\nThe nature of their operation—extraction of volatile substances sometimes under extreme pressure in a hostile environment—means risk; accidents and tragedies occur regularly. The U.S. Minerals Management Service reported 69 offshore deaths, 1,349 injuries, and 858 fires and explosions on offshore rigs in the Gulf of Mexico from 2001 to 2010. On July 6, 1988, 167 people died when Occidental Petroleum's Piper Alpha offshore production platform, on the Piper field in the UK sector of the North Sea, exploded after a gas leak. The resulting investigation conducted by Lord Cullen and publicized in the first Cullen Report was highly critical of a number of areas, including, but not limited to, management within the company, the design of the structure, and the Permit to Work System. The report was commissioned in 1988, and was delivered November 1990. The accident greatly accelerated the practice of providing living accommodations on separate platforms, away from those used for extraction.\n\nThe offshore can be in itself a hazardous environment. In March 1980, the 'flotel' (floating hotel) platform \"Alexander L. Kielland\" capsized in a storm in the North Sea with the loss of 123 lives.\n\nIn 2001, \"Petrobras 36\" in Brazil exploded and sank five days later, killing 11 people.\n\nGiven the number of grievances and conspiracy theories that involve the oil business, and the importance of gas/oil platforms to the economy, platforms in the United States are believed to be potential terrorist targets. Agencies and military units responsible for maritime counter-terrorism in the US (Coast Guard, Navy SEALs, Marine Recon) often train for platform raids.\n\nOn April 21, 2010, the \"Deepwater Horizon\" platform, 52 miles off-shore of Venice, Louisiana, (property of Transocean and leased to BP) exploded, killing 11 people, and sank two days later. The resulting undersea gusher, conservatively estimated to exceed as of early June, 2010, became the worst oil spill in US history, eclipsing the Exxon Valdez oil spill.\n\nIn British waters, the cost of removing all platform rig structures entirely was estimated in 2013 at £30 billion.\n\nAquatic organisms invariably attach themselves to the undersea portions of oil platforms, turning them into artificial reefs. In the Gulf of Mexico and offshore California, the waters around oil platforms are popular destinations for sports and commercial fishermen, because of the greater numbers of fish near the platforms. The United States and Brunei have active Rigs-to-Reefs programs, in which former oil platforms are left in the sea, either in place or towed to new locations, as permanent artificial reefs. In the US Gulf of Mexico, as of September 2012, 420 former oil platforms, about 10 percent of decommissioned platforms, have been converted to permanent reefs.\n\nOn the US Pacific coast, marine biologist Milton Love has proposed that oil platforms off California be retained as artificial reefs, instead of being dismantled (at great cost), because he has found them to be havens for many of the species of fish which are otherwise declining in the region, in the course of 11 years of research. Love is funded mainly by government agencies, but also in small part by the California Artificial Reef Enhancement Program. Divers have been used to assess the fish populations surrounding the platforms.\n\nOffshore oil and gas production is more challenging than land-based installations due to the remote and harsher environment. Much of the innovation in the offshore petroleum sector concerns overcoming these challenges, including the need to provide very large production facilities. Production and drilling facilities may be very large and a large investment, such as the Troll A platform standing on a depth of 300 meters.\n\nAnother type of offshore platform may float with a mooring system to maintain it on location. While a floating system may be lower cost in deeper waters than a fixed platform, the dynamic nature of the platforms introduces many challenges for the drilling and production facilities.\n\nThe ocean can add several thousand meters or more to the fluid column. The addition increases the equivalent circulating density and downhole pressures in drilling wells, as well as the energy needed to lift produced fluids for separation on the platform.\n\nThe trend today is to conduct more of the production operations subsea, by separating water from oil and re-injecting it rather than pumping it up to a platform, or by flowing to onshore, with no installations visible above the sea. Subsea installations help to exploit resources at progressively deeper waters—locations which had been inaccessible—and overcome challenges posed by sea ice such as in the Barents Sea. One such challenge in shallower environments is seabed gouging by drifting ice features (means of protecting offshore installations against ice action includes burial in the seabed).\n\nOffshore manned facilities also present logistics and human resources challenges. An offshore oil platform is a small community in itself with cafeteria, sleeping quarters, management and other support functions. In the North Sea, staff members are transported by helicopter for a two-week shift. They usually receive higher salary than onshore workers do. Supplies and waste are transported by ship, and the supply deliveries need to be carefully planned because storage space on the platform is limited. Today, much effort goes into relocating as many of the personnel as possible onshore, where management and technical experts are in touch with the platform by video conferencing. An onshore job is also more attractive for the aging workforce in the petroleum industry, at least in the western world. These efforts among others are contained in the established term integrated operations. The increased use of subsea facilities helps achieve the objective of keeping more workers onshore. Subsea facilities are also easier to expand, with new separators or different modules for different oil types, and are not limited by the fixed floor space of an above-water installation.\n\nOffshore oil production involves environmental risks, most notably oil spills from oil tankers or pipelines transporting oil from the platform to onshore facilities, and from leaks and accidents on the platform. Produced water is also generated, which is water brought to the surface along with the oil and gas; it is usually highly saline and may include dissolved or unseparated hydrocarbons.\n\nThe world's deepest oil platform is the floating Perdido, which is a spar platform in the Gulf of Mexico in a water depth of .\n\nNon-floating compliant towers and fixed platforms, by water depth:\n\n\n"}
{"id": "12979395", "url": "https://en.wikipedia.org/wiki?curid=12979395", "title": "Photoinduced electron transfer", "text": "Photoinduced electron transfer\n\nPhotoinduced electron transfer (PET) is an excited state electron transfer process by which an excited electron is transferred from donor to acceptor. Due to PET a charge separation is generated, \"i.e.\", redox reaction takes place in excited state (this phenomenon is not observed in Dexter electron transfer).\n\nSuch materials include semiconductors that can be photoactivated like many solar cells, biological systems such as those used in photosynthesis, and small molecules with suitable absorptions and redox states.\n\nIt is common to describe where electrons reside as electron bands in bulk materials and electron orbitals in molecules. For the sake of expedience the following description will be described in molecular terms. When a photon excites a molecule, an electron in a ground state orbital can be excited to a higher energy orbital. This excited state leaves a vacancy in a ground state orbital that can be filled by an electron donor. It produces an electron in a high energy orbital which can be donated to an electron acceptor. In these respects a photoexcited molecule can act as a good oxidizing agent or a good reducing agent.\n\nThe end result of both reactions is that an electron is delivered to an orbital that is higher in energy than where it previously resided. This is often described as a charge separated electron-hole pair when working with semiconductors.\n\nIn the absence of a proper electron donor or acceptor it is possible for such molecules to undergo ordinary fluorescence emission. The electron transfer is one form of photoquenching.\n\nIn many photo-productive systems this charge separation is kinetically isolated by delivery of the electron to a lower energy conductor attached to the p/n junction or into an electron transport chain. In this case some of the energy can be captured to do work. If the electron is not kinetically isolated thermodynamics will take over and the products will react with each other to regenerate the ground state starting material. This process is called recombination and the photon's energy is released as heat.\n\nThe reverse process to photoinduced electron transfer is displayed by light emitting diodes (LED) and chemiluminescence. Where potential gradients are used to create excited states that decay by light emission.\n"}
{"id": "1004128", "url": "https://en.wikipedia.org/wiki?curid=1004128", "title": "Pool safety camera", "text": "Pool safety camera\n\nPool safety cameras are video monitoring or recording systems designed to improve safety, such as by reducing drowning deaths, injuries, or criminal activity, in public and private pools, waterparks, thermal baths, or spa facilities. Manufacturers include SwimEye, AngelEye, Argusmatik, Poseidon, and Zwembadcamera.\n\nAquatics video monitoring systems are broken into two categories:\n\nPassive systems provide lifeguards with views of below water swimmer activity and behaviour. The views are displayed at the lifeguard position/chair allowing them to incorporate them into their 10:20 scan to help with early identification of an incident developing or abnormal events occurring. They are primarily a means of addressing the physical limitations of viewing through glare and into blind spots in the swimming pool tank. They are designed to make the lifeguard's job easier.\nActive systems are designed to further help lifeguards in an attempt to address the physical limitations imposed by the human factor. \n\nMonitoring systems are further broken into three broad classes:\n\nViewing aids are typically underwater video cameras for lifeguards to see various views underwater simultaneously from a single location. They can be used for all types of swimming pools. Cameras can view areas which would otherwise be obstructed. These passive systems while providing additional vision for the lifeguards can also distract them from scanning the pool because of the moving images on the monitor next to them. All cameras are recorded in real-time and the below water cameras are normally paired with above water cameras providing face-to-body matching if there is an incident where a swimmer needs to be identified but may not show their face below the water, for example pedophilia , or other offenses , or to limit or safeguard from financial liability.\n"}
{"id": "23441092", "url": "https://en.wikipedia.org/wiki?curid=23441092", "title": "Rocket sled launch", "text": "Rocket sled launch\n\nA rocket sled launch, also known as \"ground based launch assist\", \"catapult launch assist\", and \"sky ramp launch\", is a proposed method for launching space vehicles. With this concept the launch vehicle is supported by an eastward pointing rail or maglev track that goes up the side of a mountain while an externally applied force is used to accelerate the launch vehicle to a given velocity. Using an externally applied force for the initial acceleration reduces the propellant the launch vehicle needs to carry to reach orbit. This allows the launch vehicle to carry a larger payload and reduces the cost of getting to orbit. When the amount of velocity added to the launch vehicle by the ground accelerator becomes great enough, single-stage-to-orbit flight with a reusable launch vehicle becomes possible.\n\nFor hypersonic research in general, tracks at Holloman Air Force Base have tested, as of 2011, small rocket sleds moving at up to (Mach 8.5).\n\nEffectively a 'sky ramp' would make the most expensive, first stage of a rocket fully reusable since the sled is returned to its starting position, to be refueled and may be reused in the order of hours after use. Present launch vehicles have performance-driven costs of thousands of dollars per kilogram of dry weight; sled launch would aim to reduce performance requirements and amortize hardware expenses over frequent, repeated launches. Designs for mountain based inclined rail 'rocket' sleds often use jet engines or rockets to accelerate the spacecraft mounted on it. Electromagnetic methods (such as Bantam, Maglifter, and StarTram) are another technique investigated to accelerate a rocket before launch, potentially scalable to greater rocket masses and velocities than air launch.\n\nNASA studies have shown that the Space Shuttle used more than a third of its fuel just to reach . If a rocket were already moving at launch, with corresponding reduced propellant needs, a greater fraction of liftoff mass could have been payload and hardware.\n\nDue to factors including the exponential nature of the rocket equation and higher propulsive efficiency than if a rocket takes off stationary, a NASA Maglifter study estimated that a launch of an ELV rocket from a 3000-meter altitude mountain peak could increase payload to LEO by 80% compared to the same rocket from a conventional launch pad. Mountains of such height are available within the mainland U.S. for the easiest logistics, or nearer to the Equator for a little more gain from Earth's rotation. Among other possibilities, a larger single-stage-to-orbit (SSTO) could be reduced in liftoff mass by 35% with such launch assist, dropping to 4 instead of 6 engines in one case considered.\n\nAt an anticipated efficiency close to 90%, electrical energy consumed per launch of a 500-ton rocket would be around 30 GJ, 8000 kilowatt hours (each kilowatt-hour costing a few cents at the current cost of electricity in the United States), aside from any additional losses in energy storage. It is a system with low marginal costs dominated by initial capital costs Although a fixed site, it was estimated to provide a substantial net payload increase for a high portion of the varying launch azimuths needed by different satellites, with rocket maneuvering during the early stage of post-launch ascent (an alternative to adding electric propulsion for later orbital inclination change). Maglev guideway costs were estimated as $10 – $20 million per mile in the 1994 study, which had anticipated annual maglev maintenance costs on the order of 1% of capital costs.\n\nRocket sled launch helps a vehicle gain altitude, and proposals commonly involve the track curving up a mountain. Advantages to any launch system that starts from high altitudes include reduce gravity drag (the cost of lifting fuel in a gravity well). The thinner air will reduce air resistance and allow more efficient engine geometries. Rocket nozzles have different shapes (expansion ratios) to maximize thrust at different air pressures. (Though NASA's aerospike engine for the Lockheed Martin X-33 was designed to change geometry to remain efficient at a variety of different pressures, the aerospike engine had added weight and complexity; X-33 funding was canceled in 2001; and other benefits from launch assist would remain even if aerospike engines reached flight testing).\n\nFor example, the air is 39% thinner at 2500 meters. The more efficient rocket plume geometry and the reduced air friction allows the engine to be 5% more efficient per amount of fuel burned.\n\nAnother advantage to high altitude launches is that it eliminates the need to throttle back the engine when the \"Max Q\" limit is attained. Rockets launched in thick atmosphere can go so fast that air resistance may cause structural damage. Engines are throttled back when Max Q is reached, until the rocket is high enough that they can resume full power. The Atlas V 551 gives an example of this. It reaches its Max Q at 30,000 feet. Its engine is throttled back to 60% thrust for 30 seconds. This reduced acceleration adds to the gravity drag the rocket must overcome. Additionally, space craft engines concerned with Max Q are more complex as they must be throttled during launch.\n\nA launch from high altitude need not throttle back at Max Q as it starts above the thickest portion of the Earth's atmosphere.\n\nDebora A. Grant and James L. Rand in: \"The Balloon Assisted Launch System - A Heavy Lift Balloon\" wrote: \"It was established some time ago that a ground launched rocket capable of reaching 20 km would be able to reach an altitude of almost 100km if it was launched from 20km.\" They suggest that small rockets are lifted above the majority of the atmosphere by balloon in order to avoid the problems discussed above.\n\nRocket sleds at China Lake testing ground have reached Mach 4 while carrying 60,000 kg masses. A sled track that gave a Mach 2 or greater launch assist could reduce the fuel to orbit by 40% or more, while helping counter the weight penalty when aiming to make a fully reusable launch vehicle. Angled at 55 degrees to vertical, a track on a tall mountain could allow a single stage to orbit reusable vehicle with no new technology.\n\n\n"}
{"id": "5690494", "url": "https://en.wikipedia.org/wiki?curid=5690494", "title": "Roy Ascott", "text": "Roy Ascott\n\nRoy Ascott (born 26 October 1934) is a British artist, who works with cybernetics and telematics, on an art which is technoetic, focusing on the impact of digital and telecommunications networks on consciousness.\n\nAscott exhibits internationally (including the Biennales of Venice and Shanghai), and is collected by Tate Britain and Arts Council England. He is recognised by Ars Electronica as the “visionary pioneer of media art”, and widely seen as a radical innovator in arts education and research, having occupied leading academic roles in England, Europe, North America, and China, and currently establishing his Technoetic Arts studio in Shanghai; directing a worldwide doctoral research network. In 2018 he became the sole subject of \"Cybernetics & Human Knowing: A Journal of Second Order Cybernetics, Autopoiesis and Cybersemiotics\" entitled \"A Tribute to the Messenger Shaman: Roy Ascott\".\n\nHe is President of the Planetary Collegium, Professor of Technoetic Arts Plymouth University, and the De Tao Master of Technoetic Arts at the DeTao Masters Academy in Shanghai. He is the founding editor of the research journal \"Technoetic Arts\", an honorary editor of \"Leonardo Journal\", and author of the book \"Telematic Embrace: Visionary Theories of Art, Technology and Consciousness.\" University of California Press\n\nHe is recipient of the Prix Ars Electronica Golden Nica award for Visionary Pioneer of Media Art 2014. The award is for “those men and women whose artistic, technological and social achievements have decisively influenced and advanced the development of new artistic directions.” He is a Doctor \"Honoris Causa\" of Ionian University, Corfu, Greece; Honorary Professor at Aalborg University Copenhagen; Honorary Professor at University of West London. \n\nRoy Ascott was born in Bath, England. He was educated at the City of Bath Boys' School. His National Service was spent as a Pilot Officer in RAF Fighter Command working with radar defence systems. From 1955-59 he studied Fine Art at King's College, University of Durham (now Newcastle University) under Victor Pasmore and Richard Hamilton, and Art History under Lawrence Gowing and Quentin Bell. He was awarded the degree of B.A. Hons Fine Art, Dunelm in 1959. On graduation he was appointed Studio Demonstrator (1959–61). He then moved to London, where he established the radical \"Groundcourse\" at Ealing Art College, which he subsequently established at Ipswich Civic College, in Suffolk, working with artist tutors such as Anthony Benjamin, Bernard Cohen (painter). R. B. Kitaj, Brian Wall, Harold Cohen, and Peter Startup. Notable alumni of the Groundcourse include Brian Eno, Pete Townshend, Stephen Willats, Roger Ruskin Spear, and Michael English.\n\nAscott taught in London Ealing, and was a visiting lecturer at other London art schools throughout the 1960s. Then briefly was President of Ontario College of Art, now OCAD University, Toronto, then Chair of Fine Art at Minneapolis College of Art and Design, before moving to California as Vice-President and Dean of San Francisco Art Institute, during the 1970s. He was Professor for Communications Theory at the University of Applied Arts Vienna during the 1980s, and Professor of Technoetic Arts at the University of Wales, Newport in the 1990s.Bot generated title -->. where he established the Centre for Advanced Inquiry in the Interactive Arts. He established the Planetary Collegium at Plymouth University in 2003.\n\nHe has advised new media arts organisations in Brazil, Japan, Korea, Europe and North America , as well as UNESCOBot generated title -->, and was Visiting Professor (VI), Design|Media Arts, University of California Los Angeles (2003–07) at the UCLA School of the Arts. Ascott was an International Commissioner for the XLII Venice Biennale of 1986 (Planetary Network and Laboratorio Ubiqua).\n\nHe is the founding president of the Planetary Collegium an advanced research center which he set up in 2003, with its Hub currently based in the University of Plymouth, UK, and nodes in China, Greece, Italy, and Switzerland. \nIn March 2012 he was appointed De Tao Master of Technoetic Arts at (DTMA), a high-level, multi-disciplined, creativity-oriented higher education institution in Shanghai, China.Bot generated title -->. In 2014, he established the Ascott Technoetic Arts Studio at DTMA creating the Technoetic Arts advanced degree programme, taught jointly with the Shanghai Institute of Visual Art. The DeTao-Node of the Planetary Collegium was established in 2015. He is a Doctor Honoris Causa of Ionian University, Corfu, Greece.\n\nSince the 1960s, Roy Ascott has been a practitioner of interactive computer art, electronic art, cybernetic art and telematic art.\nThe historian of art and technology Frank Popper writes of Roy Ascott:\n\nIn his first show (1964) at the Molton Gallery, London , he exhibited \"Analogue Structures and Diagram Boxes\", comprising change-paintings and other works in wood, perspex and glass. In 1964 Ascott published \"Behaviourist Art and the Cybernetic Vision\" in \"Cybernetica: journal of the International Association for Cybernetics\" (Namur). In 1968, he was elected Associate Member of the Institution of Computer Science, London (proposed by Gordon Pask). In 1972, he became a Fellow of the Royal Society of Arts.\n\nAscott has shown at the Venice Biennale, Shanghai Biennale, Electra Paris, Ars Electronica, V2 Institute for the Unstable Media , Milan Triennale, Biennale do Mercosul, Brazil, European Media Festival, and gr2000az at Graz, Austria. His first telematic project was \"La Plissure du Texte\" (1983), an online work of \"distributed authorship\" involving artists around the world. The second was his \"gesamtdatenwerk\" \"Aspects of Gaia: Digital Pathways across the Whole Earth\" (1989), an installation for the Ars Electronica Festival in Linz, discussed by (inter alia) Matthew Wilson Smith in \"The Total Work of Art: from Bayreuth to Cyberspace\", New York: Routledge, 2007. Retrospective exhibitions of his work were shown in May 2009 at Plymouth Arts Centre, England, then in the Incheon International Digital Arts Festival, Incheon, South Korea in September 2010, and at SPACE (studios) in Hackney, London in 2011. \"Syncretic Cybernetics\", a comprehensive exhibition of his work, was featured in the 9th Shanghai Biennale 2012. \"Roy Ascott: The Analogues\" (featuring his work of the 1960s) was shown at the Plug-in Institute ofContemporary Arts , Winnipeg, July-Sept 2013. In September 2014, a mini retrospective of his work was shown in Linz, at the time of his Ars Electronica Golden Nica award . He discussed his work on Geran TV .\nThe seminal work of 1962, \"Video-Roget\" was acquired in 2014, by the Tate Gallery, London for its permanent collection. Two key works were included in \"Electronic Superhighway\", at the Whitechapel Gallery, London in 2016 . \"Art in Europe 1945-68\" shown in ZKM, Karlsruhr, Germany Oct 2016/ Jan 2017, included his \"Change-painting 1966\" . His early work was the subject of the exhibition \"Roy Ascott: Form has Behaviour\", at the Henry Moore Institute, Leeds, Jan/Apr 2017 . \n\nSince the 1960s, Ascott has been a working with interactive computer art, telematic art. and systems art. Ascott built a theoretical framework for approaching interactive artworks, which brought together certain characteristics of Dada, Surrealism, Fluxus, Happenings, and Pop Art with the science of cybernetics. He was also influenced by the writings of Gordon Pask, Anthony Stafford Beer, William Ross Ashby, and F.H. George.\n\n\n\n"}
{"id": "27118137", "url": "https://en.wikipedia.org/wiki?curid=27118137", "title": "Segmentation Rules eXchange", "text": "Segmentation Rules eXchange\n\nSegmentation Rules eXchange or SRX is an XML-based standard that was maintained by Localization Industry Standards Association, until it became insolvent in 2011, and then by GALA.\n\nSRX provides a common way to describe how to segment text for translation and other language-related processes. It was created when it was realized that TMX was less useful than expected in certain instances due to differences in how tools segment text. SRX is intended to enhance the TMX standard so that translation memory (TM) data that is exchanged between applications can be used more effectively. Having the segmentation rules that were used when a TM was created increases the usefulness of the TM data.\n\nSRX make use of the ICU Regular Expression syntax, but not all programming languages support all ICU expressions, making implementing SRX in some languages difficult or impossible. Java is an example of this.\n\nSRX version 1.0 was officially accepted as an OSCAR standard in April 2004.\n\nSRX version 2.0 was officially accepted as an OSCAR standard in April 2008.\n\nSRX forms part of the Open Architecture for XML Authoring and Localization (OAXAL) reference architecture.\n\n"}
{"id": "48839543", "url": "https://en.wikipedia.org/wiki?curid=48839543", "title": "Social media in education", "text": "Social media in education\n\nSocial media in education refers to the practice of using social media platforms as a way to enhance the education of students. Social media is defined as \"a group of Internet-based applications that build on the ideological and technological foundations of Web 2.0, and that allow the creation and exchange of [user-generated content\".\n\nStudent use of computers as learning aids began in the early 1980s when drilling and practice programs were first developed and implemented into the classroom . Following the 1980s was a computer advancement boom that defined the 1990s and 2000s as CD-ROMS were first introduced and the internet became more user friendly. As of 2018, 95% of teenage students have access to a smartphone and 45% say they are online almost constantly . As the use of technology and social media has become more prevalent, some educators and parents argued that they were too distracting for the classroom environment. This led to many schools blocking Internet access, including access to social media sites, and even disallowing the use of cell phones to class. These policies proved to be ineffective in some cases, as students continue to bring their phones to class despite the no cell phone policy, and many even find ways to access social media sites regardless of precautions taken by school administrators. \n\nIn response to these challenges, many schools have adopted a \"Bring Your Own Device\" (BYOD) policy to school. This is a policy that allows students to bring their own internet accessing device, such as a phone or iPad, for the purpose of accessing the Internet for research and other in-class activities. While the BYOD concept was initially introduced as a way of reducing departmental technology costs, administrators and teachers are realizing other benefits from BYOD policies, such as increased student motivations and engagement and anywhere access to information. \n\nTechnology integration can be described as involving student needs rather than revolving around teacher needs. In a classroom with a whiteboard and a single computer, the learning will revolve round the teacher. With the use of technology, the learning environment can be expanded. \n\nThe use of technology in the classroom can be very positive. Technology can support and improve the learning environment. As technology is becoming more predominant in the world today, teachers believe that developing these technological skills among students can be very beneficial for entering the workforce. The technology creation of online textbooks has made tablets and laptops widely popular in the classroom. With this ability, students are able to have readily available access to this resource anywhere at any time. Laptops can be used to quickly access information in the classroom and gather knowledge. They can be used for interactive activities involving polls, note taking, recording data, and research. \n\nThe impact of using technology within classrooms can have a negative effect as well. A study shows that students who used laptops in class for nonacademic reasons had poorer class performance overall. These students spent most of their time on social media websites, online shopping, and other personal usage.\n\nThe recent developments in technology have changed how and what students learn in a classroom. The internet gives students access to more resources than ever before, in terms of both research and learning tools. Students are taught to be more critical in life overall when they have to decide which sources are credible or not when doing internet research . Students can also engage in active learning by using devices to participate in their field and service learning by working with organizations outside of the classroom to solve problems and create new projects . \n\nStudents can also use their devices to access learning management systems like Blackboard and Canvas. Students are able to complete their work anywhere as long as they have internet service, which grants them more freedom outside the classroom. \n\nOther apps have been developed that combine learning tasks with elements of social media. Quizlet is a new tool that helps students study in a new way. Quizlet allows users to create flashcard sets that are always available to study. It also takes these card sets and automatically generates practice tests and other activities to help students study, which they can share with other users. There is opposition to learning websites such as Quizlet because some people believe they make it easier for students to cheat, claiming that students can use their phone during the test to look up answers and can pass off other students work as their own . \n\nCollege institutions are adapting many social media platforms into their educational systems to improve communication with students and the overall quality of student life. It provides colleges with an easy fast method of communication and allows them to give and receive feedback to students. Social media usage has skyrocketed over the past decade as present research shows that almost all college students use some form of social networking website. Conducted studies show that 99% of college students who use social media use Facebook and 35% use Twitter. Facebook and Twitter have become the dominant forms of social media platforms that have successfully grown in popularity. Social media platforms such as Twitter, Facebook, and YouTube are widely used by educational institutions to make connecting with students and providing information convenient. Institutions also consider communicating information through the usage of technology a vital part in student success. In many classrooms across America, teachers have created social media pages for their classes on which they can post assignments as well as interact with their students. Schools have felt the need to make regulations for how students and faculty interact online. Many teachers stay away from \"friending\" of \"following\" their students online because it can become too personal.\n\nUse of social media has helped some educators mentor their students more effectively. \n\nRather than compete with, or deny access to social media sites, some schools have totally embraced them, and are using them to further students' educations.\n\nParents, students, and teachers are using social media to connect and communicate inside and outside the classroom. Programs like Black Board, School loop, Top Hat, and Moodle have created platforms to enhance the learning experience by increasing communication between all parties. These sites are not necessarily social media websites, but their added communication features such as fourms create an experience that is similar to that of social media. Studies have shown that 96% of students have internet access, as well as access to at least one social media site. Teachers are moving away from the no phones at school rule, and are implementing them into their courses to keep students attention in class . Teachers are at risk when using these platforms, especially when communicating with students outside of the classroom. Teachers use of social media outside of the classroom is not always protected by the teachers union . Teachers are taking risks when choosing to communicate with students outside of the classroom, especially when they are private conversations through social media. Transparency is the key to communicating with students. Teachers are choosing to use Twitter as a way to talk to their students in a safe and transparent way because it is a social media site where the posts and comments are open to the public. \n\n"}
{"id": "728753", "url": "https://en.wikipedia.org/wiki?curid=728753", "title": "Tandem-charge", "text": "Tandem-charge\n\nA tandem-charge or dual-charge weapon is an explosive device or projectile that has two or more stages of detonation.\n\nTandem charges are effective against reactive armour, which is designed to protect an armoured vehicle (mostly tanks) against anti-tank munitions. The first stage of the weapon is typically a weak charge that either pierces the reactive armour of the target without detonating it leaving a channel through the reactive armour so that the second warhead may pass unimpeded, or simply detonating the armourplate causing the timing of the counter-explosion to fail. The second detonation from the same projectile (which defines it as a tandem charge) attacks the same location as the first detonation where the reactive armour has been compromised. Since the regular armour plating is often the only defence remaining, the main charge (second detonation) has an increased likelihood of penetrating the armour.\n\nHowever, tandem-charges are useful only against ERA (explosive reactive armour) types of reactive armour, much less so against the non-explosive reactive armour (NxRA) types, since their inner liner is not explosive itself and thus not expended by the small forward warhead of tandem-charge attack.\n\nThe PG-7VR warhead for the RPG-7 rocket launcher and the PG-29V warhead for the more modern RPG-29 rocket launcher are examples of tandem charges, but the technology is employed worldwide because they were designed in the Cold War era to counter the reactive armour that was a common feature on Soviet tanks. Examples of missiles that use tandem charges include the BGM-71 TOW, FGM-148 Javelin, Brimstone and the MBT LAW.\n\nDual charges are also effective at increasing the effectiveness of warheads when used against structures (such as bunkers). Because the explosion of a unitary high explosive charge will follow the path of least resistance, much of the explosive power of a warhead will be lost to the air surrounding the target if detonated outside the structure. This effect can be countered by using heavily constructed gravity bombs with delay fuzes which penetrate the earth, concrete, etc. of the target before exploding—thus containing the explosion inside the structure and significantly increasing its effect.\n\nGravity bombs require aircraft to fly rather close to what may be a heavily-defended target which poses a significant risk to the launch aircraft. Cruise missiles equipped with large tandem-charge warheads can use the first charge to create a hole into which the missile flies before exploding the second charge, creating a similar effect of the delayed gravity bomb. An example of an anti-structure tandem-charge warhead is the BROACH warhead.\n\n"}
{"id": "175534", "url": "https://en.wikipedia.org/wiki?curid=175534", "title": "Techno-thriller", "text": "Techno-thriller\n\nA techno-thriller (also known as technothrillers) is a hybrid genre drawing from science fiction, thrillers, spy fiction, action, and war novels. They include a disproportionate amount (relative to other genres) of technical details on their subject matter (typically military technology); only hard science fiction tends towards a comparable level of supporting detail on the technical side. The inner workings of technology and the mechanics of various disciplines (espionage, martial arts, politics) are thoroughly explored, and the plot often turns on the particulars of that exploration.\n\nOne of the earliest techno-thrillers is thought to be \"The Satan Bug\" (1962) by Alistair MacLean. \nMichael Crichton and Tom Clancy are considered to be the fathers of the \"modern techno-thriller\"; Crichton's book \"The Andromeda Strain\" and Clancy's book \"The Hunt for Red October\" set out the type example which defined the genre, although many authors had been writing similar material earlier, such as Craig Thomas, whom BBC News also credits as an early innovator.\n\nTechno-thrillers focus strongly on details, especially on the technology, which is frequently of military origin. Techno-thrillers tend to have a broad scope in the narrative, and can often be regarded as contemporary speculative fiction; world wars are a common topic. Techno-thrillers often overlap, as far as the genre goes, with near-future science fiction, military fiction, and espionage fiction. To the extent that technology is now a dominant aspect of modern global culture, most modern thrillers are \"techno-thrillers\", and the genre is somewhat diffuse. Techno-thrillers blur smoothly into the category of hard science fiction; the defining characteristics of techno-thriller are an emphasis on real-world or plausible near-future technology. There is often a focus on military or military-political action. Techno-thrillers also overlap with conspiracy fiction and apocalyptic fiction.\n\n"}
{"id": "2545415", "url": "https://en.wikipedia.org/wiki?curid=2545415", "title": "Texas obscenity statute", "text": "Texas obscenity statute\n\nThe Obscene Device Law is a Texas statute prohibiting the sale of sex toys. The law was introduced in 1973, and was last updated in 2003. While the law technically remains in effect, in 2008 a U.S. District Judge released a report declaring it to be \"facially unconstitutional and unenforceable\".\n\nIn 1973, the Texas Legislature passed Section 43.21 of the Texas Penal Code, which, in part, prohibited the sale or promotion of \"obscene devices\"', being defined as \"a device including a dildo or artificial vagina, designed or marketed as useful primarily for the stimulation of human genital organs.\" The legislation was last updated in 2003, and Section 43.23 currently states \"A person commits an offense if, knowing its content and character, he wholesale promotes or possesses with intent to wholesale promote any obscene material or obscene device.\"\n\nProsecution under the statute is rare but has occasionally occurred. In Burleson in 2004, Joanne Webb, a mother of three and a former schoolteacher, faced up to one year in prison for selling a vibrator to two undercover police officers posing as a married couple at a private party. She was later acquitted, and the undercover officers were issued reprimands. In 2007, a lingerie shop in Lubbock was raided, and items \"deemed to be illegal by the Texas Penal Code\" were confiscated. The clerk on duty at the time was arrested and may have had to register as a sex offender. In 2001, attorneys Mary and Ted Roberts used the obscenity statute in an elaborate extortion scheme against a number of men who had engaged in extramarital relations with Mary Roberts.\n\nReliable Consultants, Inc., who operate four retail stores in Texas that carry a stock of sexual devices, and PHE, Inc., which is also engaged in the retail distribution of sexual devices through their website and catalogs, both filed lawsuits against the legislation, claiming that the statute is unconstitutional. In an appeal from the United States District Court for the Western District of Texas, a three-judge panel of the 5th Circuit Court of Appeals overturned the statute on February 12, 2008, by a vote of 2–1, holding that \"the statute has provisions that violate the Fourteenth Amendment of the U.S. Constitution\". The State of Texas filed a petition on February 22, 2008, for the Circuit Court to rehear the argument en banc.\n\nOn July 3, 2008, the Texas 13th District Court of Appeals in Corpus Christi in the case of Villareal vs. State, addressed the ruling of the federal 5th Circuit Court of Appeals. The 13th District Court of Appeals ruled that until the Texas Court of Criminal Appeals rules that 43.23 is unconstitutional, the promotion of obscene devices remains illegal. Therefore, despite the actions of the federal courts and the Texas Attorney General described elsewhere in this article, section 43.23 remains in effect in the twenty-county area of Texas covered by the jurisdiction of the 13th District Court of Appeals.\n\nOn August 1, 2008, the Fifth Circuit denied Texas's request to re-hear the case \"en banc\". The refusal created a split between federal circuits: the 5th Circuit overturned the Texas law and the 11th Circuit upheld a nearly identical Alabama law. That would usually mean that the United States Supreme Court would grant a writ of certiorari and rule in order to clear up the disagreement between the two circuits.\n\nOn November 4, 2008, U.S. District Judge Lee Yeakel released a two-page document dated October 29, 2008, in which he stated that the Texas Attorney General's Office notified him that they would not file a writ of certiorari with the Supreme Court. The next month, on November 13, Yeakel filed a \"joint status report\" that noted the parties had come to an agreement. \"Texas Penal Code §§ 43.23, to the extent that it applies to \"obscene devices\" as defined in Texas Penal Code § 43.21(a)(7), is declared to be facially unconstitutional and unenforceable throughout the State of Texas\".\n\n\n"}
{"id": "11030876", "url": "https://en.wikipedia.org/wiki?curid=11030876", "title": "Timeline of LiveJournal", "text": "Timeline of LiveJournal\n\nThis is a timeline of events in the history of the virtual community LiveJournal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "34295298", "url": "https://en.wikipedia.org/wiki?curid=34295298", "title": "TrafficDito", "text": "TrafficDito\n\nTrafficDito is a free traffic monitoring website and GPS-enabled iOS application for iPhones launched in the Philippines on October 21, 2011 by the TrafficDito Team, a local startup in the country. It works by sorting through Twitter users’ public tweets and its iPhone app-generated traffic reports describing road conditions along major roads in the cities of Makati, Pasig, Caloocan, Taguig, Parañaque, Manila, Mandaluyong, and Quezon City.\n\nTweets and reports not older than 3 hours are filtered by an algorithm that will determine the expressed sentiment as light, moderate, or heavy traffic. They are then labeled, color-coded and displayed in a stream that can be sorted via search. TrafficDito currently does not support cities outside Metro Manila, nor can it be used in countries outside the Philippines.\n\nThe TrafficDito iPhone application has features not available on its browser-based counterpart. Users on the iPhone app can send geo-tagged traffic reports labeled by users as light, moderate, or heavy, alongside any optional text details that may be added. App users may then view the crowdsourced reports and geo-tagged tweets as color-coded dots overlaid on a Google map of Metro Manila.\n\nIn its latest updates, the iPhone app was given additional reporting features such as automatic bearing-detection, Twitter integration, offline mode, and the ability to display geo-tagged reports sourced from tweets by the Metropolitan Manila Development Authority.\n\n"}
{"id": "845722", "url": "https://en.wikipedia.org/wiki?curid=845722", "title": "Tunnel magnetoresistance", "text": "Tunnel magnetoresistance\n\nTunnel magnetoresistance (TMR) is a magnetoresistive effect that occurs in a magnetic tunnel junction (MTJ), which is a component consisting of two ferromagnets separated by a thin insulator. If the insulating layer is thin enough (typically a few nanometres), electrons can tunnel from one ferromagnet into the other. Since this process is forbidden in classical physics, the tunnel magnetoresistance is a strictly quantum mechanical phenomenon.\n\nMagnetic tunnel junctions are manufactured in thin film technology. On an industrial scale the film deposition is done by magnetron sputter deposition; on a laboratory scale molecular beam epitaxy, pulsed laser deposition and electron beam physical vapor deposition are also utilized. The junctions are prepared by photolithography.\n\nThe direction of the two magnetizations of the ferromagnetic films can be switched individually by an external magnetic field. If the magnetizations are in a parallel orientation it is more likely that electrons will tunnel through the insulating film than if they are in the oppositional (antiparallel) orientation. Consequently, such a junction can be switched between two states of electrical resistance, one with low and one with very high resistance.\n\nThe effect was originally discovered in 1975 by Michel Jullière (University of Rennes, France) in Fe/Ge-O/Co-junctions at 4.2 K. The relative change of resistance was around 14%, and did not attract much attention. In 1991 Terunobu Miyazaki (Tohoku University, Japan) found a change of 2.7% at room temperature. Later, in 1994, Miyazaki found 18% in junctions of iron separated by an amorphous aluminum oxide insulator and Jagadeesh Moodera found 11.8% in junctions with electrodes of CoFe and Co. The highest effects observed to date with aluminum oxide insulators are around 70% at room temperature.\n\nSince the year 2000, tunnel barriers of crystalline magnesium oxide (MgO) have been under development. In 2001 Butler and Mathon independently made the theoretical prediction that using iron as the ferromagnet and MgO as the insulator, the tunnel magnetoresistance can reach several thousand percent. The same year, Bowen et al. were the first to report experiments showing a significant TMR in a MgO based magnetic tunnel junction [Fe/MgO/FeCo(001)]. \nIn 2004, Parkin and Yuasa were able to make Fe/MgO/Fe junctions that reach over 200% TMR at room temperature. In 2008, effects of up to 604% at room temperature and more than 1100% at 4.2 K were observed in junctions of CoFeB/MgO/CoFeB by S. Ikeda, H. Ohno group of Tohoku University in Japan.\n\nThe read-heads of modern hard disk drives work on the basis of magnetic tunnel junctions. TMR, or more specifically the magnetic tunnel junction, is also the basis of MRAM, a new type of non-volatile memory. The 1st generation technologies relied on creating cross-point magnetic fields on each bit to write the data on it, although this approach has a scaling limit at around 90–130 nm. There are two 2nd generation techniques currently being developed: Thermal Assisted Switching (TAS) and Spin Torque Transfer (STT). Magnetic tunnel junctions are also used for sensing applications. For example, a TMR-Sensor can measure angles in modern high precision wind vanes, used in the wind power industry.\n\nThe relative resistance change—or effect amplitude—is defined as\n\nwhere formula_2 is the electrical resistance in the anti-parallel state, whereas formula_3 is the resistance in the parallel state.\n\nThe TMR effect was explained by Jullière with the spin polarizations of the ferromagnetic electrodes. The spin polarization \"P\" is calculated from the spin dependent density of states (DOS) formula_4 at the Fermi energy:\n\nformula_5\n\nThe spin-up electrons are those with spin orientation parallel to the external magnetic field, whereas the spin-down electrons have anti-parallel alignment with the external field. The relative resistance change is now given by the spin polarizations of the two ferromagnets, \"P\" and \"P\":\n\nformula_6\n\nIf no voltage is applied to the junction, electrons tunnel in both directions with equal rates. With a bias voltage \"U\", electrons tunnel preferentially to the positive electrode. With the assumption that spin is conserved during tunneling, the current can be described in a two-current model. The total current is split in two partial currents, one for the spin-up electrons and another for the spin-down electrons. These vary depending on the magnetic state of the junctions.\n\nThere are two possibilities to obtain a defined anti-parallel state. First, one can use ferromagnets with different coercivities (by using different materials or different film thicknesses). And second, one of the ferromagnets can be coupled with an antiferromagnet (exchange bias). In this case the magnetization of the uncoupled electrode remains \"free\".\n\nThe TMR becomes infinite if \"P\" and \"P\" equal 1, i.e. if both electrodes have 100% spin polarization. In this case the magnetic tunnel junction becomes a switch, that switches magnetically between low resistance and infinite resistance. Materials that come into consideration for this are called \"ferromagnetic half-metals\". Their conduction electrons are fully spin-polarized. This property is theoretically predicted for a number of materials (e.g. CrO, various Heusler alloys) but its experimental confirmation has been the subject of subtle debate. Nevertheless, if one considers only those electrons that enter into transport, measurements by Bowen et al. of up to 99.6% spin polarization at the interface between LaSrMnO and SrTiO pragmatically amount to experimental proof of this property.\n\nThe TMR decreases with both increasing temperature and increasing bias voltage. Both can be understood in principle by magnon excitations and interactions with magnons, as well as due to tunnelling with respect to localized states induced by oxygen vacancies (see Symmetry Filtering section hereafter).\n\nPrior to the introduction of epitaxial magnesium oxide (MgO), amorphous aluminum oxide was used as the tunnel barrier of the MTJ, and typical room temperature TMR was in the range of tens of percent. MgO barriers increased TMR to hundreds of percent. This large increase reflects a synergetic combination of electrode and barrier electronic structures, which in turn reflects the achievement of structurally ordered junctions. Indeed, MgO filters the tunneling transmission of electrons with a particular symmetry that are fully spin-polarized within the current flowing across body-centered cubic Fe-based electrodes. Thus, in the MTJ's parallel (P) state of electrode magnetization, electrons of this symmetry dominate the junction current. In contrast, in the MTJ's antiparallel (AP) state, this channel is blocked, such that electrons with the next most favorable symmetry to transmit dominate the junction current. Since those electrons tunnel with respect to a larger barrier height, this results in the sizeable TMR.\n\nBeyond these large values of TMR across MgO-based MTJs, this impact of the barrier's electronic structure on tunnelling spintronics has been indirectly confirmed by engineering the junction's potential landscape for electrons of a given symmetry. This was first achieved by examining how the electrons of a LSMO half-metallic electrode with both full spin (P=+1 ) and symmetry polarization tunnel across an electrically biased SrTiO tunnel barrier. The conceptually simpler experiment of inserting an appropriate metal spacer at the junction interface during sample growth was also later demonstrated\n\nWhile theory, first formulated in 2001, predicts large TMR values associated with a 4eV barrier height in the MTJ's P state and 12eV in the MTJ's AP state, experiments reveal barrier heights as low as 0.4eV. This contradiction is lifted if one takes into account the localized states of oxygen vacancies in the MgO tunnel barrier. Extensive solid-state tunnelling spectroscopy experiments across MgO MTJs revealed in 2014 that the electronic retention on the ground and excited states of an oxygen vacancy, which is temperature-dependent, determines the tunnelling barrier height for electrons of a given symmetry, and thus crafts the effective TMR ratio and its temperature dependence. This low barrier height in turn enables the high current densities required for spin-transfer torque, discussed hereafter.\n\nThe effect of spin-transfer torque (STT) has been studied and applied widely in MTJs, where there is a tunnelling barrier sandwiched between a set of two ferromagnetic electrodes such that there is (free) magnetization of the right electrode, while assuming that the left electrode (with fixed magnetization) acts as spin-polarizer. This may then be pinned to some selecting transistor in an MRAM device, or connected to a preamplifier in an HDD application.\n\nThe STT vector, driven by the linear response voltage, can be computed from the expectation value of the torque operator:\n\nformula_7\n\nwhere formula_8 is the gauge-invariant nonequilibrium density matrix for the steady-state transport, in the zero-temperature limit, in the linear-response regime, and the torque operator formula_9 is obtained from the time derivative of the spin operator:\n\nformula_10\n\nUsing the general form of a 1D tight-binding Hamiltonian:\n\nformula_11\n\nwhere total magnetization (as macrospin) is along the unit vector formula_12 and the Pauli matrices properties involving arbitrary classical vectors formula_13, given by\n\nformula_14\n\nformula_15\n\nformula_16\n\nit is then possible to first obtain an analytical expression for formula_9 (which can be expressed in compact form using formula_18, and the vector of Pauli spin matrices formula_19).\n\nThe STT vector in general MTJs has two components: a parallel and perpendicular component:\n\nA parallel component:\nformula_20\n\nAnd a perpendicular component:\nformula_21\n\nIn symmetric MTJs (made of electrodes with the same geometry and exchange splitting), the STT vector has only one active component, as the perpendicular component disappears:\n\nformula_22.\n\nTherefore, only formula_23 vs. formula_24 needs to be plotted at the site of the right electrode to characterise tunnelling in symmetric MTJs, making them appealing for production and characterisation at an industrial scale.\n\nNote: \nIn these calculations the active region (for which it is necessary to calculate the retarded Green's function) should consist of the tunnel barrier + the right ferromagnetic layer of finite thickness (as in realistic devices). The active region is attached to the left ferromagnetic electrode (modeled as semi-infinite tight-binding chain with non-zero Zeeman splitting) and the right N electrode (semi-infinite tight-binding chain without any Zeeman splitting), as encoded by the corresponding self-energy terms.\n\nTheoretical tunnelling magneto-resistance ratios of 3400% have been predicted. However, the largest that have been observed are only 604%. One suggestion is that grain boundaries could be affecting the insulating properties of the MgO barrier; however, the structure of films in buried stack structures is difficult to determine. The grain boundaries may act as short circuit conduction paths through the material, reducing the resistance of the device. Recently, using new STEM techniques, the grain boundaries within FeCoB/MgO/FeCoB MTJs have been atomically resolved. This has allowed first principles calculations in the formalism of DFT to be performed on structural units that are present in real films. Such calculations have shown that the band gap can be reduced by as much as 45%.\n\n"}
{"id": "11903605", "url": "https://en.wikipedia.org/wiki?curid=11903605", "title": "Verdant Power", "text": "Verdant Power\n\nVerdant Power is a maker and installer of tidal power and hydroelectric systems. Their primary device is an underwater turbine, similar to a three-bladed wind turbine, that is designed to capture energy from tidal currents and (precipitation-driven) river currents. The company uses the trade term \"kinetic hydropower\" to distinguish their systems from those (tidal and hydroelectric) based on dam construction. The company's first project, the \"RITE Project\", is several turbines in New York City's East River. \n\nVerdant's home base is situated between Manhattan and Queens on Roosevelt Island in the middle of the East River, a tidal strait running from Long Island Sound to Upper New York Bay.\n\nThe Roosevelt Island Tidal Energy (RITE) Project, owned by Verdant Power, is the first tidal energy project to be issued a license from the Federal Energy Regulatory Commission (FERC). The first of three phases of the project was prototype testing from 2002 until 2006 when the phase 2 demonstration began. The six full-scale tidal turbines installed in the river bed constituted the \"world's first operation of a grid-connected tidal turbine array\". They provided power for a Gristedes supermarket and the adjacent Motorgate parking garage on Roosevelt Island.\n\nIn February 2012 the federal government announced an agreement with Verdant Power to install 30 tidal turbines in the channel, then projected to begin operations in 2015 and produce 1.05 MW of power. There were problems with turbine blade degradation. A September 2012 test of a newly designed 16-feet diameter turbine was a success, and the company estimated it will take about 5 years to complete the array of 30 turbines in the river. In August 2013, CBS News reported that Verdant planned to put \"two more turbines in the river over the next year or two and build up from there\".\n\n"}
{"id": "31068693", "url": "https://en.wikipedia.org/wiki?curid=31068693", "title": "Virtual Labs (India)", "text": "Virtual Labs (India)\n\nVirtual Labs is a project initiated by the Ministry of Human Resource Development, Government of India, under the National Mission on Education through Information and Communication Technology. The project aims to provide remote-access to Laboratories in various disciplines of science and engineering for students at all levels from under-graduate to research.\n\nIt also intends to develop a complete Learning Management System where the students can avail the various tools for learning, including additional web-resources, video-lectures, animated demonstrations and self-evaluation. There is also a component wherein costly equipment and resources are shared, which are otherwise available to only a limited number of users due to constraints on time and geographical distances.\n\nSeven IIT's (Delhi, Bombay, Kanpur, Kharagpur, Madras, Roorkee and Guwahati), IIIT Hyderabad, Amrita University, Dayalbagh University, NIT Karnataka, and College of Engineering, Pune, are the institutions participating in the project.\nThe Project intends to cover physical sciences, chemical science and various branches of engineering like electronics and communications, computer science and engineering, electrical engineering, mechanical engineering, chemical engineering, biotechnology engineering and civil engineering.\n\n\n\n"}
