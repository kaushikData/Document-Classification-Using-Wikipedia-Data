{"id": "27581577", "url": "https://en.wikipedia.org/wiki?curid=27581577", "title": "ATM Adaptation Layer 1", "text": "ATM Adaptation Layer 1\n\nAn ATM Adaptation layer 1 or AAL1 is used for transmitting Class A network traffic, that is, real-time, constant bit rate, connection oriented traffic (example- uncompressed audio and video). Bits are fed in by the application at constant rate and must be delivered to other end with minimum delay, jitter or overhead. The input is stream of bits without message boundaries. For this traffic, error detection protocols cannot be used since timeouts and retransmission causes delay but the missing cells are reported to the application, that must take its own action to recover from them.\n"}
{"id": "3485532", "url": "https://en.wikipedia.org/wiki?curid=3485532", "title": "Absorption refrigerator", "text": "Absorption refrigerator\n\nAn absorption refrigerator is a refrigerator that uses a heat source (e.g., solar energy, a fossil-fueled flame, waste heat from factories, or district heating systems) to provide the energy needed to drive the cooling process.\n\nThe principle can also be used to air-condition buildings using the waste heat from a gas turbine or water heater. Using waste heat from a gas turbine makes the turbine very efficient because it first produces electricity, then hot water, and finally, air-conditioning (called cogeneration/trigeneration).\n\nThe American National Standards Institute standard for the absorption refrigerator is given by the ANSI/AHRI standard 560–2000.\n\nIn the early years of the twentieth century, the vapor absorption cycle using water-ammonia systems was popular and widely used, but after the development of the vapor compression cycle it lost much of its importance because of its low coefficient of performance (about one fifth of that of the vapor compression cycle). Absorption refrigerators are a popular alternative to regular compressor refrigerators where electricity is unreliable, costly, or unavailable, where noise from the compressor is problematic, or where surplus heat is available (e.g., from turbine exhausts or industrial processes, or from solar plants).\n\nAbsorption cooling was invented by the French scientist Ferdinand Carré in 1858. The original design used water and sulphuric acid.\n\nIn 1922 Baltzar von Platen and Carl Munters, while they were still students at the Royal Institute of Technology in Stockholm, Sweden, enhanced the principle with a 3-fluid configuration. This \"Platen-Munters\" design can operate without a pump.\n\nCommercial production began in 1923 by the newly formed company \"AB Arctic\", which was bought by Electrolux in 1925. In the 1960s, absorption refrigeration saw a renaissance due to the substantial demand for refrigerators for caravans (travel trailers). AB Electrolux established a subsidiary in the United States, named Dometic Sales Corporation. The company marketed refrigerators for recreational vehicles (RVs) under the \"Dometic\" brand. In 2001, Electrolux sold most of its leisure products line to the venture-capital company EQT which created \"Dometic\" as a stand-alone company.\n\nIn 1926, Albert Einstein and his former student Leó Szilárd proposed an alternative design known as the Einstein refrigerator.\n\nAt the 2007 TED Conference, Adam Grosser presented his research of a new, very small, \"intermittent absorption\" vaccine refrigeration unit for use in third world countries. The refrigerator is a small unit placed over a campfire, that can later be used to cool 15 liters of water to just above freezing for 24 hours in a 30 °C environment.\n\nBoth absorption and compressor refrigerators use a refrigerant with a very low boiling point (less than ). In both types, when this refrigerant evaporates (boils), it takes some heat away with it, providing the cooling effect. The main difference between the two systems is the way the refrigerant is changed from a gas back into a liquid so that the cycle can repeat. An absorption refrigerator changes the gas back into a liquid using a method that needs only heat, and has no moving parts other than the refrigerant itself.\n\nThe absorption cooling cycle can be described in three phases:\n\nIn comparison, a compressor refrigerator uses a compressor, usually powered by either an electric or internal combustion motor, to increase the pressure on the gaseous refrigerant. The resulting hot, high-pressure gas is condensed to a liquid form by cooling in a heat exchanger (\"condenser\") that is exposed to the external environment (usually air in the room). The condensed refrigerant, now at a temperature near to that of the external environment, then passes through an orifice or a throttle valve into the evaporator section. The orifice or throttle valve creates a pressure drop between the high pressure condenser section and the low pressure evaporator section. The lower pressure in the evaporator section allows the liquid refrigerant to evaporate, which absorbs heat from the refrigerator food compartment. The now-vaporized refrigerant then goes back into the compressor to repeat the cycle.\n\nAnother difference between the two types is the refrigerant used. Compressor refrigerators typically use an HCFC or HFC, while absorption refrigerators typically use ammonia or water.\n\nA simple absorption refrigeration system common in large commercial plants uses a solution of lithium bromide and lithium chloride salt and water. Water under low pressure is evaporated from the coils that are being chilled. The water is absorbed by a lithium bromide/water solution. The system drives the water off the lithium bromide solution with heat.\n\nAnother variant, uses air, water, and a salt water solution. The intake of warm, moist air is passed through a sprayed solution of salt water. The spray lowers the humidity but does not significantly change the temperature. The less humid, warm air is then passed through an evaporative cooler, consisting of a spray of fresh water, which cools and re-humidifies the air. Humidity is removed from the cooled air with another spray of salt solution, providing the outlet of cool, dry air.\n\nThe salt solution is regenerated by heating it under low pressure, causing water to evaporate. The water evaporated from the salt solution is re-condensed, and rerouted back to the evaporative cooler.\n\nA single-pressure absorption refrigerator takes advantage of the fact that a liquid's evaporation rate depends upon the \"partial\" pressure of the vapor above the liquid and goes down with lower partial pressure. While having the same total pressure throughout the system, the refrigerator maintains a low partial pressure of the refrigerant (therefore high evaporation rate) in the part of the system that draws heat out of the low-temperature interior of the refrigerator, but maintains the refrigerant at high partial pressure (therefore low evaporation rate) in the part of the system that expels heat to the ambient-temperature air outside the refrigerator.\n\nThe refrigerator uses three substances: ammonia, hydrogen gas, and water. The cycle is closed, with all hydrogen, water and ammonia collected and endlessly reused. The system is pressurized to the pressure where the boiling point of ammonia is higher than the temperature of the condenser coil (the coil which transfers heat to the air outside the refrigerator, by being hotter than the outside air.) This pressure is typically 14–16 atm at which pressure the dew point of ammonia will be about 35 °C (95 °F).\n\nThe cooling cycle starts with liquid ammonia at room temperature entering the evaporator. The volume of the evaporator is greater than the volume of the liquid, with the excess space occupied by a mixture of gaseous ammonia and hydrogen. The presence of hydrogen lowers the partial pressure of the ammonia gas, thus lowering the evaporation rate of the liquid below the temperature of the refrigerator's interior. Ammonia evaporates, taking a small amount of heat from the liquid and lowering the liquid's temperature. It continues to evaporate, while the large enthalpy of vaporization (heat) flows from the warmer refrigerator interior to the cooler liquid ammonia and then to more ammonia gas.\n\nIn the next two steps, the ammonia gas is separated from the hydrogen so it can be reused.\n\n\nThe pure ammonia gas then enters the condenser. In this heat exchanger, the hot ammonia gas transfers its heat to the outside air, which is below the boiling point of the full-pressure ammonia, and therefore condenses. The condensed (liquid) ammonia flows down to be mixed with the hydrogen gas released from the absorption step, repeating the cycle.\n\n\n"}
{"id": "52526687", "url": "https://en.wikipedia.org/wiki?curid=52526687", "title": "AccelStor, Inc.", "text": "AccelStor, Inc.\n\nAccelStor Ltd. (formerly AccelStor, Inc.) Chinese: 英屬開曼群島商捷鼎創新股份有限公司 (formerly 捷鼎國際股份有限公司). AccelStor is an All Flash Storage OEM founded on November 27, 2014 in Taipei, Taiwan. In addition to providing Storage Appliance products, AccelStor also offers exclusive flash memory and storage acceleration software technology.\n\nAccelStor was founded by veterans of the storage, networking and IT markets in Taiwan in November 2014. The company’s product lineup consists of NVMe JBOF storage arrays (P310/P810), SATA JBOF All Flash Storage arrays (P710/P710-E/P710-SLED) and HA series Dual controller All Flash arrays (H510 & H710).\n\nJ series extension shelves are also available for P710 & HA models which can extend usable capacity to 1.1 PetaByte effective storage using Data Deduplication techniques and compression algorithms.\n\nThe distinguishing characteristic and feature of AccelStor AFA systems is their FlexiSuite™ software which includes an FMS 2016 award winning RAID replacement technology called FlexiRemap™ which won the best of show award at the 2016 FMS show in Santa Clara, California and which delivers stunning levels of sustained random WRITE IOPS performance (732,000 write IOPS @ 4K) with random reads delivering over 1.1 million IOPS @ 4K on their P710 SATA platforms.\n\nFlexiRemap™ was designed to solve the problems that traditional RAID brings to NAND Flash memory used in SSD disk which causes hotspots in the SSD used for parity and uneven load in remaining data carrying SSD's in a traditional RAID grouping. FlexiRemap™ uses two parity drives, two hot spares and evenly loads the remaining data SSD's by using an award winning algorithm that delivers stunning performance at a 28% Data Protection and serial write packing overhead, which when compared to RAID 5 (27% overhead) is a very reasonable overhead considering that it is delivering 3x extended SSD durability, 13x Performance boost over RAID 5 and very attractive latency features to boot.\n\nFlexiRemap™ outperforms Pure Storage RAID 3D by a wide margin and delivers maximum performance offered by current PCIe 3.0 server technology.\n\nAccelStor use three different hardware providers for their base systems, featuring SuperMicro platforms for the P710 and H series systems, AIC for the P310 NVMe platform and Dell for the new P810 NVMe AFA platform. \n\nThe P710-SLED (US) & P710-E (EU) was recently launched under pressure from their USA based Chief Solutions Architect to address the Education market gap for an AFA solution serving VMware VDI virtual machines for students at a price point lower than high end spinning Disk based storage arrays from EMC, Pure Storage and HPe.\n\nThe US arm of the company is called AccelStor Solutions and operates out of Fremont California.\n\nAll AccelStor SATA and SAS platforms have shelf extension capability and the latest code revision with all the bells and whistle features in code release 4.0. 6U for 1.1 PB storage is an attractive option for customers who require high IOPS storage platforms at pricing lower than spinning disk solutions from EMC and HPe.\n\nAccelStor FlexiSuite™ software offers full storage Data Services features such as FlexiDedupe™, FlexiSnap™, FlexiClone™ and FlexiRemap™ with Replication capability as well..\n\nAccelStor are also about to release NeoTopaz™ technology at the 2018 Flash memory Summit in Santa Clara, California which is an NVMe storage technology that utilizes RDMA over 100GB Ethernet (RoCE) and offers up virtual storage controllers in a virtual all flash array format called vFlexiArray™.\n\nvFlexiArrays™ can use the unique AccelStor share-nothing architecture for HA, or the more traditional two controllers with one set of storage HA solution. The Share nothing architecture has two separate and unique storage sets with synchronization taking place between the two virtual controllers over RoCE on 100GbE.\n\nThis architecture allows the virtual storage arrays to use the exact SSD type required for each storage use case and compute power can be scaled with memory and storage in separate silos. All data is isolated and fully protected and the virtual storage arrays use the full AccelStor award winning FlexiSuite™ technology pack on each vFlexiArray™ that includes the RAID free technology called FlexiRemap™.\n\nSpecifically designed for NAND flash memory operations, AccelStor’s FlexiRemap™ technology features a unique architectural re-design of RAID operation that considerably speeds up random I/O requests, a major cause of storage-access slowdowns. Unlike traditional RAID algorithms, FlexiRemap™ remaps data before passing it to the underlying flash memory layer, avoiding unnecessary processing overhead and extending the lifespan of SSDs by a large margin.\n\nFlexiRemap™ harnesses the power of processor architecture by managing the underlying flash memory directly from the kernel level of the operating system. This technology packs data intelligently during random writes, accelerating access and improving performance. Data is written onto each data carrying SSD evenly to prevent any section from wearing out prematurely as seen with RAID 5 and RAID 6. FlexiRemap™ also simplifies SSD management by integrating all the SSDs into a single store, combining capacity with data-access performance.\n\nThe power of AccelStor FlexiRemap™ technology is best demonstrated in AccelStor NeoSapphire all-flash arrays, which can deliver more than 732,000 sustained WRITE IOPS @ 4KB random writes in standalone rack-mount form factors, reaching the highest performance tier for enterprise and datacenter storage systems.\n\nAccelStor’s NeoSapphire™ series of all-flash array storage appliances come in NVMe or SATA based SSD formats and are powered by AccelStor FlexiRemap™ technology which delivers sustained high levels of performance. With up to 732K WRITE IOPS @ 4K, NeoSapphire™ arrays significantly speed up data access and processing performance while reducing power consumption and increasing available space utilization making them suitable for real world random read/write mixed load operations that traditional storage systems have difficulty processing and also for huge volumes of data that need to be processed very rapidly by AI or similar technology. NeoSapphire™ arrays come in standard 1U or 2U rack-mount form factors and feature web-based GUI management, multi-protocol support, front-accessible hot-swappable SSDs, and redundancy through virtual data groups.\n\nAccelStor spend a great deal of effort ensuring VMware, Openstack & Windows Server compatibility at extreme levels of performance as well as being one of the few storage devices that offer parallel cluster file system support such as BeeGFS as well as other HPC enhancements related to performance levels on a host of other file systems including standard CIFS and NFS support.\n\nAI platforms in particular see huge benefits from utilizing AccelStor AFA series appliances in their solution architecture as it offers the performance levels AI needs for real time problem solving using rapid data search capabilities. Genomic Sequencing operations also require ultra fast and low latency characteristics and NeoSapphire™ arrays are working globally with DNA sequencers successfully in this role.\n\n\nThe P310 is available with either four Fiber Channel or six 10 GbE SFP+ interfaces, 8 SSD drive capability and is usually armed with 4TB SSD drives. The P310 is typically used for HPC, AI and tasks such as DNA sequencing and facial recognition applications requiring low latency and high sustained IOPS.\n\nThe P810 is a new 24 SSD capable platform based on Dell hardware. Details and spec forthcoming as of June 2018. It was seen at Computex Taipei 2018 where the H710 was also showcased.\n\nAccelStor is also busy developing its NeoTopaz™ successor to NeoSapphire™, though they will realistically be complimentary but separate products and architectures which both use the FlexiSuite™. NeoTopaz™ focuses on NVMe-oF which uses RDMA over Converged Ethernet (RoCE) to overcome the data challenges design architects face with shared NVMe-oF access.\n\nNeoTopaz™ is expected Q3/Q4 2018.\n\nThese come in JBOF (P710 series) and HA (H510 & H710 Series)\n\nTransport connectivity is either via 6 x Fiber Channel or 6 x 10 GbE SFP+ ports.\n\nFibre Channel options are 8,16 or 32GB\n\n\n100GbE Mellanox transport is currently pending release to market and will be generally available Q1 2019.\n\nThese are available on the P710 and all H series platforms and are 12G SAS based using 2 Ports, with 8 lanes per port.\n\n\nIn July 2015, AccelStor was named among the Top 10 Coolest Storage Startups.\nIn June 2016, AccelStor was named one of the 20 Most Promising High-Performance Computing (HPC) Solution Providers of 2016 by CIOReview magazine, noting AccelStor’s commitment to unlocking the true performance of flash-based storage solutions with a software-defined approach.\nIn August 2016, the AccelStor NeoSapphire 3400 series was awarded the Storage Hardware Certification for VMware vSphere® as an Elite member of the VMware Technology Alliance Partner Program. This certification allows AccelStor’s customers to use 10GbE models of the NeoSapphire 3400 series with their VMware virtualized environment. Now, AccelStor’s complete NeoSapphire 3400 series of all-flash arrays (the NS3401, NS3405, NS3411 and NS3413) have all been awarded VMware’s Storage Hardware Certifications for vSphere status and are officially VMware Ready Storage.\nAlso in August 2016, AccelStor won both the Most Innovative Flash Memory Technology and the Best-of-Show Technology Innovation Award for its FlexiRemap software at Flash Memory Summit 2016 in Santa Clara, California. The Summit’s Best-of-Show awards reflect the highest honors in the flash memory and solid-state storage industry. The Most Innovative Flash Memory Technology award recognizes patented technologies that move flash technology forward, even becoming new industry standards.\nThe Silicon Review selected AccelStor as among the 50 Smartest Companies of the Year in September 2016.\nIn November 2016, AccelStor’s NeoSapphire 3605 and 3611 Fibre Channel all-flash arrays were officially certified by Tiger Technology, making AccelStor a partner for Fibre Channel storage and connectivity.\n"}
{"id": "3381318", "url": "https://en.wikipedia.org/wiki?curid=3381318", "title": "Airgo Networks", "text": "Airgo Networks\n\nAirgo Networks (formerly Woodside Networks), is a Palo Alto, California-based company specializing in the development of multiple-input multiple-output (MIMO) wireless technology. Airgo Networks was founded in 2001 by Gregory Raleigh, V.K. Jones and David Johnson.\n\nAirgo was a lead proponent of the 802.11n standard. The company began shipping the world’s first MIMO-OFDM chipsets in 2003.\n\nIn September 2005, Airgo Networks launched its third generation True MIMO chip set with support for data rates up to 240 Mbps.\n\nOn December 3, 2006, Qualcomm announced that it would acquire Airgo Networks for an undisclosed amount. Airgo is now owned by Qualcomm, although still located in Palo Alto, while Qualcomm's headquarters is in San Diego.\n"}
{"id": "17464907", "url": "https://en.wikipedia.org/wiki?curid=17464907", "title": "Allowance (engineering)", "text": "Allowance (engineering)\n\nIn engineering and machining, an allowance is a planned deviation between an exact dimension and a nominal or theoretical dimension, or between an intermediate-stage dimension and an intended final dimension. The unifying abstract concept is that a certain amount of difference \"allows for\" some known factor of compensation or interference. For example, an area of excess metal may be left because it is needed to complete subsequent machining. Common cases are listed below. An \"allowance,\" which is a \"planned\" deviation from an ideal, is contrasted with a \"tolerance,\" which accounts for expected but unplanned deviations.\n\nAllowance is basically the size difference between components that work together. Allowance between parts that are assembled is very important. For example, the axle of a car has to be supported in a bearing otherwise it will fall to the ground. If there was no gap between the axle and the bearing then there would be a lot of friction and it would be difficult to get the car to move. If there was too much of a gap then the axle would be jumping around in the bearing. It is important to get the allowance between the axle and the bearing correct so that the axle rotates smoothly and easily without juddering. \n\n\nOften the terms \"allowance\" and \"tolerance\" are used inaccurately and are improperly interchanged in engineering contexts. This is logical because both words generally can relate to the abstract concept of permission — that is, of a limit on what is acceptable. However, in engineering, separate meanings are enforced, as explained below.\n\nA tolerance is the limit of acceptable \"unintended\" deviation from a nominal or theoretical dimension. Therefore, a pair of tolerances, upper and lower, defines a range within which an actual dimension may fall while still being acceptable.\n\nIn contrast, an allowance is a \"planned\" deviation from the nominal or theoretical dimension.\n\nAn example of the concept of tolerance is a shaft for a machine is intended to be precisely 10 mm in diameter: 10 mm is the \"nominal\" dimension. The engineer designing the machine knows that in reality, the grinding operation that produces the final diameter may introduce a certain small-but-unavoidable amount of random error. Therefore, the engineer specifies a tolerance of ±0.01 mm (\"plus-or-minus\" 0.01 mm).\n\nAs long as the grinding machine operator can produce a shaft with actual diameter somewhere between 9.99 mm and 10.01 mm, the shaft is acceptable. Understanding how much error is predictable in a process and how much is easily avoidable; how much is unavoidable (or whose avoidance is possible but simply too expensive to justify); and how much is truly acceptable involves considerable judgment, intelligence, and experience.\n\nAn example of the concept of allowance can be shown in relation to the hole that this shaft must enter. It is evident that the above shaft cannot be certain to freely enter a hole that is also 10 mm with the same tolerance. It might, if the actual shaft diameter is 9.99 mm and the actual hole diameter is 10.01 mm, but it would not if conversely the actual shaft diameter is 10.01 mm and the actual hole diameter is 9.99 mm.\n\nTo be sure that there will be enough clearance between the shaft and its hole, \"taking account of the tolerance,\" an \"allowance\" is \"intentionally\" introduced in the dimensions specified. The hole diameter might be specified as 10.03 mm with a manufacturing tolerance of ±0.01 mm (\"plus-or-minus\" 0.01 mm). This means that the smallest acceptable hole diameter will be 10.02 mm while the largest acceptable shaft diameter will be 10.04 mm, leaving an \"allowance\" of 0.01 mm. The minimum clearance between the hole and the shaft will then be 0.01 mm. This will occur when both the shaft and the hole are at maximum material condition.\n"}
{"id": "13717593", "url": "https://en.wikipedia.org/wiki?curid=13717593", "title": "Apostolos Kaklamanis", "text": "Apostolos Kaklamanis\n\nApostolos Kaklamanis () (born September 7, 1936 in Lefkada) is a Greek politician and member of the Greek Parliament for the Panhellenic Socialist Movement (PASOK) for the Athens B constituency.\n\nHe has been elected as a PASOK MP in all the general elections since 1974.\n\nHe speaks English.\n\nHe has held the following government posts:\n\n\nHe was Speaker of the Hellenic Parliament from 22 October 1993 to 19 March 2004.\n\n"}
{"id": "9180624", "url": "https://en.wikipedia.org/wiki?curid=9180624", "title": "Arriflex 35", "text": "Arriflex 35\n\nThe Arriflex 35 (1937) was the first reflex 35mm production motion picture camera. It was built around the spinning reflex twin-bladed \"butterfly\" mirror shutter designed by Erich Kästner, chief engineer at Arnold & Richter Cine Technik (ARRI), Arri Group, set at 45 degrees horizontally to the lens axis. Modern standard models have a maximum shutter exposure opening of 165 degrees, (not 180 degrees as claimed in Arri manuals), the 35 IIC-BV model having a variable shutter. The mirror shutter allows the camera operator to see a viewfinder image equal to the recorded picture, without parallax, although there is noticeable image flicker in the viewfinder when the camera is running, caused by the two open exposure segments of the mirror shutter. It is still used extensively in motion pictures for sequences without synchronous sound - \"motor only sync\" - and unique camera movement, e.g. on Steadicam. It was widely used with 200 ft loads (the smaller 200 ft magazine was in production at that time) as a 'battlefield camera' for the German Wehrmacht during World War II for collecting battlefront intelligence, (e.g. for analyzing weapons effectiveness), for training films and for use in propaganda cinema films.\n\nThe camera utilizes a three lens turret with three aluminum Arri lens mounts (later 35 IIC/B with one stainless steel bayonet mount and two aluminum Arri mounts), and is capable of frame rates up to 80 frames per second with an accessory speed unit. Film magazines are for 200 ft or 400 ft loads. The DC motor is mounted downwards as a handgrip. Later flat base DC motor mount units were developed e.g. by the Cine 60 company, allowing the camera to have a lower profile, where the motor is mounted on the side of the camera body vertically upwards, allowing the camera to be mounted on standard tripod heads without a special head accommodating the handgrip motor, and providing a more compact profile for 'blimping'.\n\nNew models appeared over the years: the 35 II in 1946, the IIA in 1953, the IIB in 1960 and finally, the IIC in 1964. In 1982, the II-series was superseded by the Arriflex 35-3C model, having a IIC film transport movement, a swiveling viewfinder, a quartz-driven handgrip motor, a then-new Arri PL (positive locking) lens mount and a variable shutter. The 35-3C model was put into production to use up 150 sets of 35 IIC camera movement units still inventoried at the factory.\n\n"}
{"id": "30214168", "url": "https://en.wikipedia.org/wiki?curid=30214168", "title": "Bendix drive", "text": "Bendix drive\n\nA Bendix drive is a type of engagement mechanism used in starter motors of internal combustion engines. The device allows the pinion gear of the starter motor to engage or disengage the flywheel of the engine automatically when the starter is powered or when the engine fires, respectively. It is named after its inventor, Vincent Hugo Bendix.\n\nThe Bendix system places the starter drive pinion on a helical drive spring. When the starter motor begins turning, the inertia of the drive pinion assembly causes it to wind the spring forcing the length of the spring to change and engage with the ring gear. When the engine starts, backdrive from the ring gear causes the drive pinion to exceed the rotative speed of the starter, at which point the drive pinion is forced back and out of mesh with the ring gear.\n\nThe main drawback to the Bendix drive is that it relies on a certain amount of \"clash\" between the teeth of the pinion and the ring gears before they slip into place and mate completely; the teeth of the pinion are already spinning when they come into contact with the static ring gear, and unless they happen to align perfectly at the moment they engage, the pinion teeth will strike the teeth of the ring gear side-to-side rather than face-to-face, and continue to rotate until both align. This increases wear on both sets of teeth. For this reason the Bendix drive has been largely superseded in starter motor design by the pre-engagement system using a solenoid.\n\n"}
{"id": "9772947", "url": "https://en.wikipedia.org/wiki?curid=9772947", "title": "Boiler feedwater", "text": "Boiler feedwater\n\nBoiler feedwater is an essential part of boiler operations. The feed water is put into the steam drum from a feed pump. In the steam drum the feed water is then turned into steam from the heat. After the steam is used it is then dumped to the main condenser. From the condenser it is then pumped to the deaerated feed tank. From this tank it then goes back to the steam drum to complete its cycle. The feed water is never open to the atmosphere. This cycle is known as a closed system or Rankine cycle.\n\nDuring the early development of boilers, water treatment was not so much of an issue, as temperatures and pressures were so low that high amounts of scale and rust would not form to such a high amount, especially if the boiler was cleaned and/or “blown down”. It was general practice though, to install zinc plates and/or alkaline chemicals to reduce corrosion within the boiler. Many tests had been performed to try to determine the cause and possible protection from corrosion in boilers using distilled water, various chemicals, and sacrificial metals. Silver nitrate can be added to feedwater samples in order to detect contamination by seawater. Use of lime for alkalinity control had been mentioned as early as 1900, and was used by the French and British Navies up until about 1935. In modern boilers though, treatment of boiler feedwater is extremely critical, as many problems can result from the use of untreated water in extreme pressure and temperature environments; this includes lower efficiency in terms of heat transfer, overheating, damage, and high costs of cleaning.\n\nWater has higher heat capacity than most other substances. This quality makes it an ideal raw material for boiler operations. Boilers are part of a closed system as compared to open systems in a gas turbine. The closed system that is used is the Rankine cycle. This means that the water is recirculated throughout the system and is never in contact with the atmosphere. The water is reused and needs to be treated to continue efficient operations. Boiler water must be treated in order to be proficient in producing steam. Boiler water is treated to prevent scaling, corrosion, foaming, and priming. Chemicals are put into boiler water through the chemical feed tank to keep the water within chemical range. These chemicals are mostly oxygen scavengers and phosphates. The boiler water also has frequent blowdowns in order to keep the chloride content down. The boiler operations also include bottom blows in order to get rid of solids. Scale is precipitated impurities out of the water and then forms on heat transfer surfaces. This is a problem because scale does not transfer heat very well and causes the tubes to fail by getting too hot. Corrosion is caused by oxygen in the water. The oxygen causes the metal to oxidize which lowers the melting point of the metal. Foaming and priming is caused when the boiler water does not have the correct amount of chemicals and there are suspended solids in the water which carry over in the dry pipe. The dry pipe is where the steam and water mixture are separated.\n\nBoiler water treatment is used to control alkalinity, prevent scaling, correct pH, and to control conductivity. The boiler water needs to be alkaline and not acidic, so that it does not ruin the tubes. There can be too much conductivity in the feed water when there are too many dissolved solids. These correct treatments can be controlled by efficient operator and use of treatment chemicals. The main objectives to treat and condition boiler water is to exchange heat without scaling, protect against scaling, and produce high quality steam. The treatment of boiler water can be put into two parts. These are internal treatment and external treatment. (Sendelbach, p. 131) The internal treatment is for boiler feed water and external treatment is for make-up feed water and the condensate part of the system. Internal treatment protects against feed water hardness by preventing precipitating of scale on the boiler tubes. This treatment also protects against concentrations of dissolved and suspended solids in the feed water without priming or foaming. These treatment chemicals also help with the alkalinity of the feed water making it more of a base to help protect against boiler corrosion. The correct alkalinity is protected by adding phosphates. These phosphates precipitate the solids to the bottom of the boiler drum. At the bottom of the boiler drum there is a bottom blow to remove these solids. These chemicals also include anti-scaling agents, oxygen scavengers, and anti-foaming agents. Sludge can also be treated by two approaches. These are by coagulation and dispersion. When there is a high amount of sludge content it is better to coagulate the sludge to form large particles in order to just use the bottom blow to remove them from the feed water. When there is a low amount of sludge content it is better to use dispersants because it disperses the sludge throughout the feed water so sludge does not form.\n\nOxygen and Carbon Dioxide are removed from the feed water by deaeration. Deaeration can be accomplished by using deaerators heaters, vacuum deaerators, mechanical pumps, and steam-jet ejectors. In deaerating heaters steam sprays incoming feed water and carries away the dissolved gases. The deaerators also store hot feed water which is ready to be used in the boiler. This means of mechanical deaeration is also used with chemical oxygen scavenging agents to increase efficiency. (Sendelbach, p. 129) Deaerating heaters can be classified in to two groups. The two deaerating heaters are spray types and tray types. With tray type deaerating heaters the incoming water is sprayed into steam atmosphere in order to reach saturation temperature. When the saturation temperature is reached most of the oxygen and non-condensable gases are released. There are seals that prevent the recontamination of the water in the spray section. The water then falls to the storage tank below. The non-condensables and oxygen are then vented to the atmosphere. The components of the tray type deaerating heater are a shell, spray nozzles, direct contact vent condenser, tray stacks, and protective interchamber walls. The spray type deaerater is very similar to the tray type deaerater. The water is sprayed into a steam atmosphere and most of the oxygen and non-condensables are released to the steam. The water then falls to the steam scrubber where the slight pressure loss causes the water to flash a little bit which also helps with the removal of oxygen and non-condensables. The water then overflows to the storage tank. The gases are then vented to the atmosphere. With vacuum deaeration a vacuum is applied to the system and water is then brought to its saturation temperature. The water is sprayed in to the tank just like the spray and tray deaeraters. The oxygen and non-condensables are vented to the atmosphere.(Sendelbach, p. 130)\n\nThe feedwater must be specially treated to avoid problems in the boiler and downstream systems. Untreated boiler feed water can cause corrosion and fouling.\n\nCorrosive compounds, especially O and CO must be removed, usually by use of a deaerator. Residual amounts can be removed chemically, by use of oxygen scavengers. Additionally, feed water is typically alkalized to a pH of 9.0 or higher, to reduce oxidation and to support the formation of a stable layer of magnetite on the water-side surface of the boiler, protecting the material underneath from further corrosion. This is usually done by dosing alkaline agents into the feed water, such as sodium hydroxide (caustic soda) or ammonia. Corrosion in boilers is due to the presence of dissolved oxygen, dissolved carbon dioxide, or dissolved salts.\n\nDeposits reduce the heat transfer in the boiler, reduce the flow rate and eventually block boiler tubes. Any non-volatile salts and minerals that will remain when the feedwater is evaporated must be removed, because they will become concentrated in the liquid phase and require excessive \"blow-down\" (draining) to prevent the formation of solid precipitates. Even worse are minerals that form scale. Therefore, the make-up water added to replace any losses of feedwater must be demineralized/deionized water, unless a purge valve is used to remove dissolved minerals.\n\nSteam locomotives usually do not have condensers so the feedwater is not recycled and water consumption is high. The use of deionized water would be prohibitively expensive so other types of water treatment are used. Chemicals employed typically include sodium carbonate, sodium bisulfite, tannin, phosphate and an anti-foaming agent.\n\nTreatment systems have included:\n\n\n\n"}
{"id": "51063898", "url": "https://en.wikipedia.org/wiki?curid=51063898", "title": "Bond tester", "text": "Bond tester\n\nA bond tester is a scientific instrument used to measure the mechanical strength of bonds, evaluate bond strength distributions or determine compliance with specified bond strength requirements of the applicable acquisition document. Typically a load is applied to a bond by a hook or shear tool, whereafter a force measurement is taken and the failure mode of the tested sample is recorded. More often than not bond tests are destructive and samples are scrapped after testing. In aerospace and medical applications, non destructive testing is common, whereby the bond is loaded up to a point to reveal nonacceptable bonds while avoiding damage to acceptable bonds.\n\nElectrical and thermal bonds are such an integral part of electronic and semiconductor construction that they may often be taken for granted. Modern electronic assembly methods employ a myriad of bonding processes, each one a vital step in the manufacture of the final product. A typical consumer product such as a laptop computer may contain hundreds of thousands of bonds yet if one fails it will probably result in a system breakdown. \n\nFor an automatic test PC controlled moving table allows any number of bonds to be tested automatically from a stored program. \nResults can be analysed and output immediately or exported in a number of data base formats for subsequent analysis as desired.\nPowerful extended capabilities enable measurements such as force/time or force/distance curves to be made and deliver more data about the quality of the bond tested.\n\nThe most common test types performed on a bond tester are the wire pull test, which generally puts an upward force on a gold/aluminum/silver/copper wire, and the die shear test, which generally comprises loading a die from the side. When equipped with tweezers, bond testers may also perform cold bump pull tests. During such a test, a solder ball down to 50 µm in diameter is reform it to the shape something like a mushroom and then pulled off the surface. Modern bond testers can perform a wide variety of tests with high precision, because automation eliminates human influence on the measurement.\n\nMIL-STD-883, JEDEC and other standards are commonly followed in the industry.\n"}
{"id": "7402988", "url": "https://en.wikipedia.org/wiki?curid=7402988", "title": "C-MAC", "text": "C-MAC\n\nC-MAC is the variant approved by the European Broadcasting Union (EBU) for satellite transmissions. The digital information is modulated using 2-4PSK (phase-shift keying), a variation of quadrature PSK where only two of the phaser angles (±90°) are used.\n\nE-MAC (Extended MAC) is 16:9 version of C-MAC. Originally E-MAC was designed for 15:9 pictures, it later adopted the 16:9 aspect ratio.\n\nMAC transmits luminance and chrominance data separately in time rather than separately in frequency (as other analog television formats do, such as composite video).\n\nAudio and Scrambling (selective access)\n\nTV transmission systems\n"}
{"id": "21103136", "url": "https://en.wikipedia.org/wiki?curid=21103136", "title": "Corporate travel management", "text": "Corporate travel management\n\nCorporate travel management (CTM) is the function of managing a company’s strategic approach to travel (travel policy), the negotiations with all vendors, day-to-day operation of the corporate travel program, traveler safety and security, credit-card management and travel and expenses ('T&E') data management.\n\nCTM should not be confused with the work of a traditional Travel Agency. While agencies provide the day-to-day travel services to corporate clients, they are the implementing arm of what the corporation has negotiated and put forth in policy. In other words, CTM decides on the class of service which employees are allowed to fly, negotiates corporate fares/rates with airlines and hotels and determines how corporate credit cards are to be used. The agency on the other hand makes the actual reservation within the parameters given by the corporation.\n\nFor many companies T&E costs represent the second highest controllable annual expense, exceeded only by salary and benefits, and is commonly higher than IT or real estate costs. T&E costs are not only limited to travel (airline, rail, hotel, car rental, ferry/boat, etc.) but include all costs incurred during travel such as staff and client meals, taxi fares, gratuities, client gifts, supplies (office supplies and services), etc. Furthermore, this area often includes meeting management, traveler safety and security as well as credit card and overall travel data management.\n\nThe management of these costs are usually handled by the Corporate Travel Manager, a function which may be part of the Finance, HR, Procurement or Administrative Services Department.\n\nMany companies, especially large multinationals (MNC), opt for global consolidation of their travel procurement. In other words, they may choose to put their entire purchasing of travel arrangements in the hands of one Travel Management Company (TMC). This is almost always done with a global Request for Proposal (RFP), through which the company will invite major TMCs to participate in the RFP. The process and the selection of the TMC could take several months. Once the company has chosen its TMC, the handling of their travel arrangements will be handled by the selected TMC throughout the world. There could, of course, be exceptions in certain countries. \n\nThe advantages of a global consolidation lie in the game of numbers: the company will be able to bring to the table the advantage of global numbers when negotiating with suppliers. These negotiations could include airlines, hotel chains, individual hotels (for specific reasons), car-rental companies etc. The main goal of going the route of global consolidation is to create savings in the company's T&E budget.\n\nThe implementation of corporate travel management is often delegated to Travel Management Companies (TMC). A TMC will manage an organization's corporate or business travel program. They will often provide an online booking tool, mobile application, program management and consulting teams, executive travel services, meetings and events support, reporting functionality, and potentially others. These companies use Global Distribution Systems (GDS) to book flights for their clients. This allows the travel consultant to compare different itineraries and costs by displaying availability in real-time, allowing users to access fares for air tickets, hotel rooms and rental cars simultaneously.\n\nTravel Management Lite, is a lightweight version of a Corporate Travel Management solution, generally used by SMEs and growth companies who don't require extensive or bespoke solutions offered by TMCs. The main advantage to a Lite solution is to enhance real-time transparency in travel spending across the company, and provides access to mobile and web apps needed to book and manage a company's travel. This allows business travellers, assistants, and travel managers to book more efficiently, and have faster access to support, than were they to use different leisure booking sites or offline travel agents.\n"}
{"id": "27908366", "url": "https://en.wikipedia.org/wiki?curid=27908366", "title": "Cult of Carts", "text": "Cult of Carts\n\nCult of Carts is a term coined by the architectural historian A. K. Porter to describe various occasions in western Europe during the 12th and 13th centuries, when ordinary lay-people harnessed themselves to carts in the place of oxen in order to transport building materials to cathedral building sites.\n\nThroughout European history there have been several documentary accounts of occasions when the public spontaneously came together to labour on some important building project (the earliest being Suetonius' account of the rebuilding of the Temple of Jupiter Optimus Maximus in Rome after a fire in AD 70). In medieval Europe, perhaps the most widely known and influential of these events occurred during the building of the Benedictine Abbey at Montecassino (Italy) in 1066. The Abbey's chronicler, Peter the Deacon, described how a crowd of pious lay people spontaneously seized some heavy marble columns which had been delivered from Rome and carried them up the long steep hill to the building site, singing and praying as they went. \n\nA similar story was also told of the building of another Benedictine monastery at St Trond (now Sint-Truiden in Belgium), c.1155, which was included in an early 12th-century account of the Abbey's history by its Abbot, Adelhard II.\n\nThe first such account from the Gothic period was written by Abbot Suger of St Denis, who had visited Montecassino in 1123 and was familiar with the story of its construction. In his account of the building of the Abbey of St Denis (written c.1144) Suger described how, after finding some Roman marble columns in a disused quarry near Pontoise, he began to despair of ever retrieving them from the forest - until a crowd of local people of all social ranks came together of their own volition, tied ropes to the columns and dragged them to the road, accompanied by many spontaneous displays of pious devotion.\n\nIn 1145, a few years after the incident described by Suger, one of the most famous 'Cult of Cart' miracles occurred at Chartres, where Bishop Fulbert's cathedral was nearing completion. The event was described in a letter claiming to be an eye-witness account, written by Abbot Haymo of Saint-Pierre-sur-Dives to the monks of Tutbury Abbey in England. Haymo described how the citizens of Chartres, of all social classes, harnessed themselves to carts like oxen and dragged materials to the building site as an act of mass piety which involved the singing of hymns and the acceptance of chastisement from members of the clergy. \n\nIn the following years a number of similar events supposedly occurred in other towns around France the last recorded at Châlons-sur-Marne around 1171. However most of these events are known only from a single source, usually written by a member of the clergy from the relevant church. Several of these contemporary accounts are very similar in style and in details, which casts some doubt on their accuracy and also on the genuine spontaneity of these events, which may instead have been orchestrated by the local clergy.\n\nAn attempt was made to revive the practice in early 14th century Rome when material for the rebuilding of the Basilica of St. John Lateran was supposedly dragged in carts by local women, who would not allow the stones to be 'defiled by animals'. Generally however stories of the practice died out as opportunities for the expression of lay piety became more normalised through confraternities and other social structures.\n\nDuring the Gothic-revivals of the 19th and early 20th centuries, various writers used the supposedly spontaneous outbreaks of popular piety exemplified by the 'Cults of Carts' to evoke an over-romanticised view of medieval Europe as a religious golden-age. More modern scholarship has tended to view the stories more sceptically. As with all such foundation myths, evidence from documentary accounts must be tempered by an understanding of the role of such stories in promoting individual churches (and the Benedictine order in general) and also by the tendency of medieval chroniclers to adapt and copy stories from earlier texts (see topos).\n"}
{"id": "52514018", "url": "https://en.wikipedia.org/wiki?curid=52514018", "title": "CyberPowerPC", "text": "CyberPowerPC\n\nCyberPowerPC, also known as CyberPower, is an American computer manufacturer and retailer. It specializes in low-cost and high-performance hardware, particularly for computer gaming as well as custom PC builds.\n\nCyberPowerPC was founded and incorporated on February 17, 1998 in the City of Industry, California.\n\nIn 2003, the company was listed as the fastest growing privately owned business in the Los Angeles area by the Los Angeles Business Journal. \n\nFrom 2011-2016, CyberPowerPC has been consistently ranked within the top 150 largest privately owned companies headquartered in Los Angeles county by the Los Angeles Business Journal.\n\nCyberPowerPC produces and sells custom-built PCs primarily for use in computer gaming. Their products feature third-party components prepared into complete ready-to-use packages.\n\nWith the advent of VR gaming in 2014, CyberPowerPC, with support from Oculus VR, released the most inexpensive computer capable of running the Oculus Rift virtual reality system, according to Oculus CEO Brendan Iribe. Later, CyberPowerPC became notable for developing a PC build for the express purpose of streaming video games.\n\nAll desktops released by the company are built to user specifications with several pre-selected builds available. In general, these computers are not given a model name (contrary to laptops) due to the variable nature of each design.\n\nCyberPowerPC laptops fall into several gaming laptop model sets, called \"series\". As of 2016, all CyberPowerPC laptops use exclusively Intel Core i7 CPUs and Nvidia GeForce GPUs.\n\n\nIn 2014, CyberPowerPC released a Steam Machine designed to compete with the Xbox One and PlayStation 4 platforms.\n\nCyberPowerPC participates in the eSports form of competitive gameplay by sponsoring and hosting occasional tournaments, such as its Summer 2016 Pro CS:GO Series. Notable eSports teams Luminosity and Team SoloMid are sponsored by the business. Additionally, the company sells several lines of custom PC builds designed for eSports.\n\n"}
{"id": "51852004", "url": "https://en.wikipedia.org/wiki?curid=51852004", "title": "Digital Content Next", "text": "Digital Content Next\n\nDigital Content Next (DCN) is a nonprofit trade association that develops research, holds informational events and provides policy guidance for the digital content industry. It was known as the Online Publishers Association (OPA) until May 2014. The organization represents more than 75 media companies based in the United States and abroad, with activities focused mostly in the U.S.\n\nFounded in 2001 by Martin Nisenholtz as the Online Publisher's Association (OPA), DCN is based in New York City. Nisenholtz served as the president of the organization until June 2006. Pam Horan served as the organization's president until May 2014, when Jason Kint was named CEO.\n\nIn September 2014, the OPA rebranded as Digital Content Next. In September 2016, DCN launched TrustX, a not-for-profit, automated online ad marketplace comprising many member companies.\n\nDCN produces proprietary research for its members and the public, and creates public and private forums to explore and advance key issues that impact digital content brands.\n\nIn September 2016, DCN announced the creation of a not-for-profit cooperative digital advertising marketplace called TrustX. This marketplace is a subsidiary of DCN and operates as a public benefit corporation (B Corp) for the sole objective of creating a sustainable future for trusted advertising. Founding companies included DCN members CBS Interactive, Condé Nast, ESPN, Hearst and News Corp.\n\n"}
{"id": "54040969", "url": "https://en.wikipedia.org/wiki?curid=54040969", "title": "Educational technology in sub-Saharan Africa", "text": "Educational technology in sub-Saharan Africa\n\nEducational technology in sub-Saharan Africa refers to the promotion, development and use of information and communication technologies (ICT), m-learning, media, and other technological tools to improve aspects of education in sub-Saharan Africa. Since the 1960s, various information and communication technologies have aroused strong interest in sub-Saharan Africa as a way of increasing access to education, and enhancing its quality and fairness.\n\nThe first initiatives to introduce technologies into education were carried out by States directly, as they were at the time embarking on wide-ranging education reforms. During that period – from decolonization through to the 1980s – the dominant paradigm in education, common to African States and international organizations, was that of the interventionist State. Against that background, major large- scale programmes were developed, which were to some extent successful. In this way, the Bouaké schools radio in Côte d'Ivoire allowed more than 2,000 teachers per year to be trained in the 1970s.\n\nRadio was one of the first technologies to be put to work in the service of education in sub-Saharan Africa. In 1986, in Guinea, an experimental schools radio project was started at the national educational research and documentation institute (l’Institut national de documentation, de recherche et d’action pédagogique, the present-day Indrap), with the assistance of the cultural and technical cooperation agency that is now the Organisation internationale de la Francophonie (OIF). The radio broadcasts were therefore mainly focused on primary teachers’ needs so as to improve their teaching techniques, and on subjects that were considered as priorities for the pupils: French, arithmetic and science. In the early 1990s, radio was given a broader role in the promotion of basic education, with the support of UNICEF, especially with respect to girls’ access to education. While the results in terms of scholastic performance are still not properly understood, these programmes helped to train a large number of teachers.\n\nThroughout the 1960s, the concept of educational television took hold in Africa. One of the most iconic examples of the development of these programmes can be found in Côte d'Ivoire. After initial trials in Senegal and Niger (where educational television existed before national television) in 1965 and 1966, Côte d'Ivoire was chosen in 1971 as the testing ground, then for the large-scale roll-out, of a massive project for schooling via television. The Programme for Education by Television (PETV) was made the responsibility of UNESCO, assisted by cooperation from Belgium, France and the Ford Foundation. The PETV gave an illustration of how, from the 1970s, innovative programmes could be launched with the aid of the “new” ICT.\n\nIn the first five years, the rate of school enrolment in that country went from 20% to over 60%. While 300,000 pupils were receiving the programmes in 1975-1976, this had increased to 700,000 by 1980 (out of a total of one million pupils). Some evaluation reports indicate that the proportion of pupils who had had the benefit of television courses entering the sixth year of study was significantly higher than for the others, that the percentage having to retake a year had fallen from 30% to 10% during the lifetime of the project, and that pupils had acquired a better mastery of spoken French. The programme ran for 14 years and finally ended in 1982.\n\nBetween 1990 and 2000, multiple actions were started in order to turn technologies into a lever for improving education in sub-Saharan Africa. Many initiatives focused on equipping schools with computer hardware. A number of NGOs contributed, on varying scales, to bringing computer hardware into Africa, such as groups like Computer Aid International, Digital Links, SchoolNet Africa and World Computer Exchange. Sometimes with backing from cooperation agencies or development agencies like USAID, the African Bank or the French Ministry of Foreign Affairs, these individual initiatives grew without adequate coordination.\n\nA large part of the backbone of ICT4D was the action framework called the Africa Information Society Initiative (AISI). Seeking to install the ICT infrastructure in Africa, its goals were to were connect every single African village with the global information network by 2010 and spur growth of smaller ICT initiatives in different sectors. A decade after its enactment, there are still hundreds of villages without electricity and connectivity between disparate ICTs is lacking.\n\nIn the 2000s, various projects were begun, mainly aimed at giving each school child access to IT tools, individually or shared among a small group. Personalizing the practice of computer technology multiplies the potential uses of ICT in education, not just by familiarization with the technology tools themselves (learning technology), but also by the acquisition of cognitive skills (learning through technology). The American One Laptop per Child (OLPC) project, launched in several African countries in 2005, aimed to equip schools with laptop computers at low cost. While the average price of an inexpensive personal computer was between US$200 and US$500, OLPC offered its ultraportable XO-1 computer at the price of US$100. This technological breakthrough marked an important step in potential access to ICT. OLPC became an institutional system: the programme was “bought” by governments, which then took responsibility for distribution to the schools. The underlying logic of the initiative was one of centralization, thus enabling the large-scale distribution of the equipment. Almost 2 million teachers and pupils are now involved in the programme worldwide and more than 2.4 million computers have been delivered. Sugar is the free teaching platform installed on the XO computers from the OLPC Foundation. Sugar is both a graphic interface at the same time as containing applications, and was designed specifically for children, to support them in their learning with a variety of available content: reading courses, drawing tools, e-books, interactive applications and so on.\nUnlike more large scale efforts to integrate and enhance education, the One Laptop Per Child (OLPC) program faces criticism on the premise that targeting the poorest areas in Africa that may not have the financial viability to afford laptop computers and maintenance costs for the children. Despite its objectives of distributing 100 to 150 million laptops by the year 2008 to the developing countries with the most need, it is measured that as of August 2010, more than 80 percent of the 1.5 million laptops have been sent to high or upper middle income countries according to the World Bank’s classifications.\n\nIn comparing the distribution of money of the One Laptop Per Child (OLPC) program with other channels of education betterment, there are many low-cost programs that have more definitive impact than OPLC's distribution laptops to \"low-income\" countries. Some other proposed cost-benefit ratios for other aid programs seeking to improve educational and socioeconomic conditions include:\n\n\nMany of these low-cost programs have sought to specifically increase the access to education fo women in hopes of increase literacy, equality in pay, economic viability, better productivity, more democratic and responsive political institutions, and better overall public health.\n\nFollowing on from OLPC, the Intel group launched Classmate PC, a similar programme also intended for pupils in developing countries. Though it has a smaller presence in sub-Saharan Africa than the OLPC project, Classmate PC has enabled laptop computers to be delivered to primary schools in the Seychelles and Kenya, particularly in rural areas.\n\nThe CFSK (Computer for School in Kenya) project was started in 2002 with the aim of distributing computers to almost 9,000 schools.\n\nAccording to UNESCO in 2002, open educational resources (OERs) are open provisions of educational resources that are enabled by ICTs for use and adaptation by users for non-commercial users. In the context of teacher education in Sub-Saharan Africa, the most notable OER project is TESSA, Teacher Education in Sub-Saharan Africa, which constitutes 13 African institutions and five international organizations working to equip teachers with practical activities for classrooms and language specific modules.\n\nThere are three different modes of use for OERs across the 200,000 teachers in Ghana, Kenya, Nigeria, South Africa, Rwanda, Sudan, Uganda, Tanzania, and Zambia: very structured modules, loosely structured modules, and guided modules.\n\nSpecifically in Nigeria and Sudan, large-scale distance education is primarily used for teacher education and thus highly structured guides with TESSA study units are given to trainee teachers, who are often in rural areas with limited Internet access.\n\nSome criticisms of TESSA OERs cited by teachers include lack of relevance of curriculum materials and existing satisfaction with current resources.\n\nOne of the most prominent paradigms for technological change and innovation in Sub-Saharan African has been the information and communication technologies for development (ICT4D) initiative. Positioned as an avenue to move millions of individuals out of poverty in developing regions, few reports on experiments of such practices have demonstrated success of ICT4D on the standard of living in Africa. Regardless, ICT4D is still rendered as one of the most prominent technological interventions in the region. Every year, proposals for more sustainable ICT4D initiatives are suggested by researchers from around the world at the AFRICOMM Conference.\n\nWith the advent of new technologies and communication as a means for organization and development between public and private sectors in developing countries, in addition to the growing penetration of mobile devices due to their increasing affordability, the poor in Sub-Saharan African have largely been seen as both producers and consumers of information and communication technologies.\n\nAccording to a 2013 study conducted by the GSM Association, the number of mobile subscribers in Sub-Saharan Africa has increased 18 percent per year between the years 2007 and 2012. In 2012, the number of mobile subscriptions in Sub-Saharan African reached almost 650 million, greater than both that in the US and EU.\n\nAndroid penetration is the highest due to easy integration of social innovation tools in areas of e-health, e-learning, waste management, mobile banking, etc., and phones are the preferred way for many to access the internet.\n\nInitially, ICT in Sub-Saharan Africa was primarily restricted to private use, but today it has been introduced in formal and institutional spheres as a tool for development, social growth, economic expansion and population growth. A study conducted by the McKinsey Global Institute (MGC) released in November 2015 highlights that the internet only contributes 2.9 percent to Kenya's GDP and 1.2 percent to South Africa's GDP.\n\nThere is a huge push for mobile ICT in Sub-Saharan Africa for three key reasons:\n\n\nBeyond trying to increase GDP growth in the short-term, ICT is covered in four of the 17 different Sustainable Development Goals (SDGs), which were selected by the UN in September 2015. Seen and described as a catalyst for education, infrastructure development, sustainable industrialization and gender equality, the motivations for ICT's spread are outlined below:\n\n\n\"ICTs are truly transformational. With the power of technology, we can educate every African citizen, right across the continent. With the power of technology, we can open new opportunities and create new well-paid jobs for our people. With the power of technology we can deliver healthcare services to every African citizen, even in the remotest villages. And with the power of technology we can empower African women and leverage the fantastic energy and passion of young Africans. This is not just a pipe-dream: this is real.\" \n\n\nThe success or failure of ICTs in Sub-Saharan Africa is highly dependent on and challenged by regional problems such as food and water shortage, pandemic diseases, wars, or heritage loss. Some areas that have been identified for more immediate ICT4D action include health management, food and water, peace, and heritage. In terms of research initiatives that pertain to educational technology, one important use case is e-Learning, where the issue of penetration results from its targets. For example, in rural areas, e-Learning serves the purpose of advertisement and even propaganda, while in urban areas the purpose is more oriented to overcoming the shortage of teaching personnel.\n\nThere are many physical and cultural factors that inhibit the complete adoption and integration of ICT practices by teachers in Sub-Saharan including and not limited to: unreliable access to electricity, limited software and hardware provisions, language limitation, country size and terrain, and population dispersion. Educational limiting factors include teachers' literacy rates, access to professional development, and missing unified national policy on computer use in schools.\n\nThe main initiatives based on the use of ICT and the Internet in education originally focused on distance learning at university level. Thus, the African Virtual University (AVU), set up by the World Bank in 1997, was originally conceived as an alternative to traditional teaching. When it became an intergovernmental agency in 2003, it was training 40,000 people, mostly on short programmes. It shifted its focus to teacher training and to integrating technology into higher education. The AVU currently has ten e-learning centres.\n\nThe Agence universitaire de la Francophonie (AUF) has also, since 1999, set up around forty French-speaking digital campuses, more than half of them in Africa. In these infrastructures, dedicated to technology and set up within the universities, the AUF offers access to over 80 first and master's degrees entirely by distance learning, about 30 of which are awarded by African institutions and created with its support.\n\nMore recently, the MOOCs (Massive Open On- line Courses) phenomenon has grown up, first in the United States and then in Europe. The AUF is funding the development of the first MOOCs in higher education in Africa, in partnership with the French Ministry for Higher Education and with the support of UNESCO, and will use this form of remote learning to offer training and certification in ICTE skills for teachers. The African universities are taking a growing interest in this new method of learning, especially in view of the ever-increasing demand for higher education at a time when the continent is experiencing a deficit in qualified teachers.\n\nM-learning (or m-education), or the use of mobile technology in the service of education, is a recent practice, opening up fresh possibilities in the educational field. Given the shortage of books in many African schools, the digital tablet was soon seen as a solution to make up for the missing textbooks, and taken up both by governments and international organizations. In practice, this shortage is one that affects almost every African country. In Cameroon there is on average one textbook on reading per 11 pupils and one mathematics book per 13 children. The price of digital content on tablets is falling sharply compared to the traditional media (books, CD and DVD, etc.). One digital textbook, for instance, costs one-third to half the price of a paper textbook, with zero marginal cost.\n\nThe digital tablet today has potential uses that extend beyond the classroom. The American NGO WorldReader has set itself the goal of extending access to reading to the most underprivileged children by distributing readers designed by Amazon. With the financial support of USAID, Kindles have been made available to 600,000 children in nine States in sub-Saharan Africa. The NGO has stated that children using this system spend 50% more time on reading, and read up to 90 books per year. Moreover, the report published by USAID following the iRead impact study in Ghana of 337 pupils in six different schools in 2010-2011 shows a number of positive aspects of the use of readers. The pupils using Kindles proved to be more enthusiastic about reading, which allowed them to improve their technical skills and achieve better scores in the standardized tests.\n\nOld and outdated traditional computer hardware and the costs of maintenance have prompted those devising projects to turn to technologies that are cheaper to buy and easier to handle. Internet access is made easier by tablets and by the possibility of connecting via the mobile networks. Relatively cheap compared to the laptop computer and more flexible in its uses, tablets have the potential to provide an efficient response to part of the educational needs of sub-Saharan Africa.\n\n"}
{"id": "33712216", "url": "https://en.wikipedia.org/wiki?curid=33712216", "title": "Electronic markets", "text": "Electronic markets\n\nElectronic markets (or electronic marketplaces) are information systems (IS) which are used by multiple separate organizational entities within one or among multiple tiers in economic value chains. In analogy to the market concept which can be viewed from a macroeconomic (describing relationships among actors in an economic systems, e.g. a monopoly) as well as from a microeconomic (describing different allocation mechanisms, e.g. public auctions of telephone frequencies) perspective, electronic markets denote networked forms of business with many possible configurations: \n\nFirst, the topology of electronic markets may be centralized or decentralized in nature. Centralized electronic markets are hubs which often provide services to their participants. Decentralized settings involve sequential relationships within value chains which often are found when electronic messages are exchanged directly between businesses (electronic data interchange, EDI). \n\nSecond, the services provided by electronic markets may serve infrastructural or allocation purposes. Among the infrastructure services are routing, messaging, identification and partner directories whereas allocation services enable pricing process which in turn may be static or dynamic in nature. Typical implementations are catalogs, exchanges and auctions.\n\nThird, the relationships of actors involved in electronic markets may be stable or atomistic in nature. The former usually refers to classical supply chains where business collaborate during a longer period of time. In the latter case, the transaction partners are only stable for a single transaction. This is usually to be found in auction and other exchange settings. \n\nThis leads to two definitions: In a narrow sense Electronic Markets are mainly conceived as allocation platforms with dynamic price discovery mechanisms involving atomistic relationships. Popular examples originate from the financial and energy industries. In a broader sense, price discovery is not critical for electronic markets. This covers all forms of electronic collaboration between organizations and consumer as well as vice versa.\n\n\nElectronic markets are attributed important impacts on business efficiencies. From an industry perspective, transaction cost economics were used to illustrate the relationship between electronic markets and electronic hierarchies. While the former are in line with the narrow electronic markets definition, the latter are also included in the broader definition. This may be explained since in reality electronic markets have emerged as platforms which combine several modes of governance or types of coordination mechanisms. These “all-in-one-markets” link the possibility of competitive bidding for price discovery with the advantages of a predictable relationship to encourage relationship specific investments (non-contractible issues) and functionalities for closer collaboration. This perspective shows that it is important to distinguish between the market platform itself which creates an infrastructure between multiple parties and the coordination mechanisms operated on this platform which might be market-like or hierarchical in nature.\n\n"}
{"id": "30310296", "url": "https://en.wikipedia.org/wiki?curid=30310296", "title": "GSP Orizont", "text": "GSP Orizont\n\nGSP Orizont is a semi-submersible, jackup independent leg cantilever drilling rig operated by GSP Drilling, a Grup Servicii Petroliere subsidiary, and currently contracted by Iranian Offshore Engineering and Construction Company for drilling in the Iranian section of the Persian Gulf. The drilling unit is registered in Malta.\n\n\"GSP Orizont\" drilling rig was designed by Sonnat Offshore and was built by Petrom at the Galaţi Shipyard in 1982. The rig was completely reconstructed and refurbished in 2010 at a cost of US$50 million. The rig was owned and operated by Petrom from 1982 to 2005 when the company sold its six offshore platforms (including Atlas, Jupiter, Orizont, Prometeu and Saturn) to Grup Servicii Petroliere for US$100 million.\n\n\"GSP Orizont\" has a length of , breadth of , draft of , height of and depth of . She has a maximum drilling depth of and she could operate at a water depth of . As a drilling rig, \"GSP Orizont\" is equipped with advanced drilling equipment and has to meet strict levels of certification under international law. \"GSP Orizont\" is able to maneuver with its own engines (to counter drift and ocean currents), but for long-distance relocation it must be moved by specialist tugboats. The rig is capable of withstanding severe sea conditions including waves and winds.\n\nCurrently the GSP Orizont is operated by the Iranian company Iranian Offshore Engineering and Construction Company which uses the drilling rig at its Persian Gulf oil and natural gas prospects.\n\n"}
{"id": "31313922", "url": "https://en.wikipedia.org/wiki?curid=31313922", "title": "Gun F/X Tactical Development", "text": "Gun F/X Tactical Development\n\nGun f/x Tactical Development is an R&D firm serving the paintball industry. It was founded as Gun f/x in 1994 as a spin-off of Pro-Team Products, Inc.\n\nGun f/x Tactical Development developed a paintball-based force-on-force simulated M16 rifle training system. They are also involved in the design and development of Non-Lethal Weapons, Improvised Explosive Device training devices, and recoil simulators for firearms marksmanship training. The company is currently based in Maine and is associated with Armson USA, Pro-Team Products, and Pro-Team Manufacturing, a Florida-based CNC facility that produces the majority of their products. The company has been issued several patents for pneumatic devices that create air pressure.\n\nThe paintball-based M16 simulator is designated to the CAR 68. The CAR 68 has been used by military and law enforcement entities including the Israeli military, US Marine Corps, US Army, Secret Service, Latvian Special Forces, and Navy Seal teams. The CAR68 training weapon simulates the look, feel and performance characteristics of the standard duty weapon. The primary limitation of the CAR 68 is its effective range, which is restricted to a maximum of approximately 200 feet. The simulator is therefore mainly utilized in training scenarios in which range is not the factor such as Close Quarters Battle drills, urban combat (in which most fighting is done within structures), police encounter training (road stops, suspect take-down), and similar close-range situations. The CAR 68 was also adapted for use as a recoil simulator for marksmanship training with laser based target system such as Beamhit and others. The company has also developed weapons simulators for the Heckler & Koch MP5, M203 grenade launcher and LAWS anti-tank weapon, though none of these devices is in wide distribution.\n\nFollowing the introduction of the CAR68, Gun f/x was approached by the Monterey Bay Corporation to provide concept and development for a less-lethal projectile launcher for use by military and law enforcement. Working with Airgun Designs, this project produced the concept less-lethal weapon known as the UTPBS, which eventually became FN Herstal's FN 303. This design could be attached to an M16 rifle and was conceived as a less-lethal weapon coupled with a lethal weapon system, providing a wide range of response capabilities that were immediately available. A stand-alone version was also developed. The UTPBS also featured a rotating barrel magazine, allowing for a wide range of different projectiles to be available and selectable without the need to change magazines. The company is currently working on further development of the burst disc concept, as well as a variety of less-lethal weapons and simulators.\n\nIn conjunction with the UTPBS development project, Gun f/x Tactical Development was also tasked with developing a projectile for the system. The \"bismuth round\" (so-called due to the inclusion of bismuth for added mass that was required for extended range) and others including training and marking rounds were developed in conjunction with Perfect Circle Paintballs, which currently manufactures pepperballs and other specialty projectiles.\n\nIn 2002, the company began work on \"burst disc technologies\" - various methods for producing the rapid release of a high volume of pressurized gas. The technology allows for the simulation of weapon systems that produce high decibel reports and are capable of discharging projectiles (or other payloads such as smoke simulation powders, marking dyes, etc.) with a greater mass than a standard paintball.\n\nGun f/x has also developed Improvised Explosive Device and Mine Training Simulators The IED and mine simulators have been used to train troops in the deployment, detection, and disarming of mines and IEDs in Iraq and Afghanistan. The IED and mine simulators utilize a 12 gram CO2 cartridge and frangible burst cup. The devices are used with an inert talc powder that gives off a smoke effect. Both designs give a concussive report in the 100 decibel range, which simulates the sound made by real IEDs and mines. Both models can be configured for remote command detonation.\n\n"}
{"id": "2768223", "url": "https://en.wikipedia.org/wiki?curid=2768223", "title": "Halteres (ancient Greece)", "text": "Halteres (ancient Greece)\n\nHalteres (; , from \"ἅλλομαι\" - \"hallomai\", \"leap, spring\"; cf. \"ἅλμα\" - \"halma\", \"leaping\") were a type of dumbbells used in Ancient Greece. In ancient Greek sports, \"halteres\" were used as lifting weights, and also as weights in their version of the long jump, which was probably a set of three jumps. \"Halteres\" were held in both hands to allow an athlete to jump a greater distance; they may have been dropped after the first or second jump. According to archaeological evidence, the athlete would swing the weights backwards and forwards just before take-off, thrust them forwards during take-off, and swing them backwards just before releasing them and landing. \"Halteres\" were made of stone or metal, and weighed between . \n\nWriting in \"Nature\", biophysicist Alberto E. Minetti calculates that halteres added about to a long jump.\n"}
{"id": "211919", "url": "https://en.wikipedia.org/wiki?curid=211919", "title": "History of rail transport", "text": "History of rail transport\n\nThe history of rail transport began in 6th century BC in Ancient Greece. It can be divided up into several discrete periods defined by the principal means of track material and motive power used.\n\nEvidence indicates that there was 6 to 8.5 km long \"Diolkos\" paved trackway, which transported boats across the Isthmus of Corinth in Greece from around 600 BC. Wheeled vehicles pulled by men and animals ran in grooves in limestone, which provided the track element, preventing the wagons from leaving the intended route. The Diolkos was in use for over 650 years, until at least the 1st century AD. The paved trackways were also later built in Roman Egypt.\n\nIn 1515, Cardinal Matthäus Lang wrote a description of the Reisszug, a funicular railway at the Hohensalzburg Castle in Austria. The line originally used wooden rails and a hemp haulage rope and was operated by human or animal power, through a treadwheel. The line still exists and is operational, although in updated form and is possibly the oldest operational railway.\nWagonways (or tramways), with wooden rails and horse-drawn traffic, are known to have been used in the 1550s to facilitate transportation of ore tubs to and from mines. They soon became popular in Europe and an example of their operation was illustrated by Georgius Agricola (image left) in his 1556 work \"De re metallica\". This line used \"Hund\" carts with unflanged wheels running on wooden planks and a vertical pin on the truck fitting into the gap between the planks to keep it going the right way. The miners called the wagons \"Hunde\" (\"dogs\") from the noise they made on the tracks. There are many references to wagonways in central Europe in the 16th century.\n\nA wagonway was introduced to England by German miners at Caldbeck, Cumbria, possibly in the 1560s. A wagonway was built at Prescot, near Liverpool, sometime around 1600, possibly as early as 1594. Owned by Philip Layton, the line carried coal from a pit near Prescot Hall to a terminus about half a mile away. A funicular railway was made at Broseley in Shropshire some time before 1604. This carried coal for James Clifford from his mines down to the river Severn to be loaded onto barges and carried to riverside towns. The Wollaton Wagonway, completed in 1604 by Huntingdon Beaumont, has sometimes erroneously been cited as the earliest British railway. It ran from Strelley to Wollaton near Nottingham.\n\nThe Middleton Railway in Leeds, which was built in 1758, later became the world's oldest operational railway (other than funiculars), albeit now in an upgraded form. In 1764, the first railway in America was built in Lewiston, New York.\n\nThe introduction of steam engines for powering blast air to blast furnaces led to a large increase in British iron production after the mid 1750s.\n\nIn the late 1760s, the Coalbrookdale Company began to fix plates of cast iron to the upper surface of wooden rails, which increased their durability and load-bearing ability. At first only balloon loops could be used for turning wagons, but later, movable points were introduced that allowed passing loops to be created.\nA system was introduced in which unflanged wheels ran on L-shaped metal plates these became known as plateways. John Curr, a Sheffield colliery manager, invented this flanged rail in 1787, though the exact date of this is disputed. The plate rail was taken up by Benjamin Outram for wagonways serving his canals, manufacturing them at his Butterley ironworks. In 1803, William Jessop opened the Surrey Iron Railway, a double track plateway, sometimes erroneously cited as world's first public railway, in south London.\nIn 1789, William Jessop had introduced a form of all-iron edge rail and flanged wheels for an extension to the Charnwood Forest Canal at Nanpantan, Loughborough, Leicestershire. In 1790, Jessop and his partner Outram began to manufacture edge-rails. Jessop became a partner in the Butterley Company in 1790. The first public edgeway (thus also first public railway) built was the Lake Lock Rail Road in 1796. Although the primary purpose of the line was to carry coal, it also carried passengers.\n\nThese two systems of constructing iron railways, the \"L\" plate-rail and the smooth edge-rail, continued to exist side by side into the early 19th century. The flanged wheel and edge-rail eventually proved its superiority and became the standard for railways.\n\nCast iron was not a satisfactory material for rails because it was brittle and broke under heavy loads. The wrought iron invented by John Birkinshaw in 1820 replaced cast iron. Wrought iron (usually simply referred to as \"iron\") was a ductile material that could undergo considerable deformation before breaking, making it more suitable for iron rails. But wrought iron was expensive to produce until Henry Cort patented the puddling process in 1784. In 1783, Cort also patented the rolling process, which was 15 times faster at consolidating and shaping iron than hammering. These processes greatly lowered the cost of producing iron and iron rails. The next important development in iron production was hot blast developed by James Beaumont Neilson (patented 1828), which considerably reduced the amount of coke (fuel) or charcoal needed to produce pig iron. Wrought iron was a soft material that contained included slag or \"dross\". The softness and dross tended to make iron rails distort and delaminate and they typically lasted less than 10 years in use, and sometimes as little as one year under high traffic. All these developments in the production of iron eventually led to replacement of composite wood/iron rails with superior all-iron rails.\n\nThe introduction of the Bessemer process, enabling steel to be made inexpensively, led to the era of great expansion of railways that began in the late 1860s. Steel rails lasted several times longer than iron. Steel rails made heavier locomotives possible, allowing for longer trains and improving the productivity of railroads. The Bessemer process introduced nitrogen into the steel, which caused the steel to become brittle with age. The open hearth furnace began to replace the Bessemer process near the end of 19th century, improving the quality of steel and further reducing costs. Steel completely replaced the use of iron in rails, becoming standard for all railways.\n\nJames Watt, a Scottish inventor and mechanical engineer, greatly improved the steam engine of Thomas Newcomen, hitherto used to pump water out of mines. Watt developed a reciprocating engine in 1769, capable of powering a wheel. Although the Watt engine powered cotton mills and a variety of machinery, it was a large stationary engine. It could not be otherwise: the state of boiler technology necessitated the use of low pressure steam acting upon a vacuum in the cylinder; this required a separate condenser and an air pump. Nevertheless, as the construction of boilers improved, Watt investigated the use of high-pressure steam acting directly upon a piston. This raised the possibility of a smaller engine, that might be used to power a vehicle and he patented a design for a steam locomotive in 1784. His employee William Murdoch produced a working model of a self-propelled steam carriage in that year.\nThe first full-scale working railway steam locomotive was built in the United Kingdom in 1804 by Richard Trevithick, a British engineer born in Cornwall. This used high-pressure steam to drive the engine by one power stroke. The transmission system employed a large flywheel to even out the action of the piston rod. On 21 February 1804, the world's first steam-powered railway journey took place when Trevithick's unnamed steam locomotive hauled a train along the tramway of the Penydarren ironworks, near Merthyr Tydfil in South Wales. Trevithick later demonstrated a locomotive operating upon a piece of circular rail track in Bloomsbury, London, the \"Catch Me Who Can\", but never got beyond the experimental stage with railway locomotives, not least because his engines were too heavy for the cast-iron plateway track then in use.\nThe first commercially successful steam locomotive was Matthew Murray's rack locomotive \"Salamanca\" built for the Middleton Railway in Leeds in 1812. This twin-cylinder locomotive was not heavy enough to break the edge-rails track and solved the problem of adhesion by a cog-wheel using teeth cast on the side of one of the rails. Thus it was also the first rack railway.\n\nThis was followed in 1813 by the locomotive \"Puffing Billy\" built by Christopher Blackett and William Hedley for the Wylam Colliery Railway, the first successful locomotive running by adhesion only. This was accomplished by the distribution of weight between a number of wheels. \"Puffing Billy\" is now on display in the Science Museum in London, making it the oldest locomotive in existence.\nIn 1814 George Stephenson, inspired by the early locomotives of Trevithick, Murray and Hedley, persuaded the manager of the Killingworth colliery where he worked to allow him to build a steam-powered machine. Stephenson played a pivotal role in the development and widespread adoption of the steam locomotive. His designs considerably improved on the work of the earlier pioneers. He built the locomotive \"Blücher\", also a successful flanged-wheel adhesion locomotive. In 1825 he built the locomotive \"Locomotion\" for the Stockton and Darlington Railway in the north east of England, which became the first public steam railway in the world, although it used both horse power and steam power on different runs. In 1829, he built the locomotive \"Rocket\", which entered in and won the Rainhill Trials. This success led to Stephenson establishing his company as the pre-eminent builder of steam locomotives for railways in Great Britain and Ireland, the United States, and much of Europe. The first public railway which used only steam locomotives, all the time, was Liverpool and Manchester Railway, built in 1830.\n\nSteam power continued to be the dominant power system in railways around the world for more than a century.\n\nThe first known electric locomotive was built in 1837 by chemist Robert Davidson of Aberdeen in Scotland, and it was powered by galvanic cells (batteries). Thus it was also the earliest battery electric locomotive. Davidson later built a larger locomotive named \"Galvani\", exhibited at the Royal Scottish Society of Arts Exhibition in 1841. The seven-ton vehicle had two direct-drive reluctance motors, with fixed electromagnets acting on iron bars attached to a wooden cylinder on each axle, and simple commutators. It hauled a load of six tons at four miles per hour (6 kilometers per hour) for a distance of . It was tested on the Edinburgh and Glasgow Railway in September of the following year, but the limited power from batteries prevented its general use. It was destroyed by railway workers, who saw it as a threat to their job security.\nWerner von Siemens demonstrated an electric railway in 1879 in Berlin. The world's first electric tram line, Gross-Lichterfelde Tramway, opened in Lichterfelde near Berlin, Germany, in 1881. It was built by Siemens. The tram ran on 180 Volt DC, which was supplied by running rails. In 1891 the track was equipped with an overhead wire and the line was extended to Berlin-Lichterfelde West station. The Volk's Electric Railway opened in 1883 in Brighton, England. The railway is still operational, thus making it the oldest operational electric railway in the world. Also in 1883, Mödling and Hinterbrühl Tram opened near Vienna in Austria. It was the first tram line in the world in regular service powered from an overhead line. Five years later, in the US electric trolleys were pioneered in 1888 on the Richmond Union Passenger Railway, using equipment designed by Frank J. Sprague.\nThe first use of electrification on a main line was on a four-mile stretch of the Baltimore Belt Line of the Baltimore and Ohio Railroad (B&O) in 1895 connecting the main portion of the B&O to the new line to New York through a series of tunnels around the edges of Baltimore's downtown.\n\nElectricity quickly became the power supply of choice for subways, abetted by the Sprague's invention of multiple-unit train control in 1897. By early 1900s most street railways were electrified.\n\nThe first practical AC electric locomotive was designed by Charles Brown, then working for Oerlikon, Zürich. In 1891, Brown had demonstrated long-distance power transmission, using three-phase AC, between a hydro-electric plant at Lauffen am Neckar and Frankfurt am Main West, a distance of 280 km. Using experience he had gained while working for Jean Heilmann on steam-electric locomotive designs, Brown observed that three-phase motors had a higher power-to-weight ratio than DC motors and, because of the absence of a commutator, were simpler to manufacture and maintain. However, they were much larger than the DC motors of the time and could not be mounted in underfloor bogies: they could only be carried within locomotive bodies.\n\nIn 1894, Hungarian engineer Kálmán Kandó developed a new type 3-phase asynchronous electric drive motors and generators for electric locomotives. Kandó's early 1894 designs were first applied in a short three-phase AC tramway in Evian-les-Bains (France), which was constructed between 1896 and 1898.\n\nIn 1896, Oerlikon installed the first commercial example of the system on the Lugano Tramway. Each 30-tonne locomotive had two motors run by three-phase 750 V 40 Hz fed from double overhead lines. Three-phase motors run at constant speed and provide regenerative braking, and are well suited to steeply graded routes, and the first main-line three-phase locomotives were supplied by Brown (by then in partnership with Walter Boveri) in 1899 on the 40 km Burgdorf—Thun line, Switzerland.\nItalian railways were the first in the world to introduce electric traction for the entire length of a main line rather than just a short stretch. The 106 km Valtellina line was opened on 4 September 1902, designed by Kandó and a team from the Ganz works. The electrical system was three-phase at 3 kV 15 Hz. In 1918, Kandó invented and developed the rotary phase converter, enabling electric locomotives to use three-phase motors whilst supplied via a single overhead wire, carrying the simple industrial frequency (50 Hz) single phase AC of the high voltage national networks.\n\nAn important contribution to the wider adoption of AC traction came from SNCF of France after World War II. The company conducted trials at 50 Hz, and established it as a standard. Following SNCF's successful trials, 50 Hz (now also called industrial frequency) was adopted as standard for main lines across the world.\n\nEarliest recorded examples of an internal combustion engine for railway use included a prototype designed by William Dent Priestman, which was examined by Sir William Thomson in 1888 who described it as a \"[Priestman oil engine] mounted upon a truck which is worked on a temporary line of rails to show the adaptation of a petroleum engine for locomotive purposes.\". In 1894, a two axle machine built by Priestman Brothers was used on the Hull Docks.\n\nIn 1906, Rudolf Diesel, Adolf Klose and the steam and diesel engine manufacturer Gebrüder Sulzer founded Diesel-Sulzer-Klose GmbH to manufacture diesel-powered locomotives. Sulzer had been manufacturing diesel engines since 1898. The Prussian State Railways ordered a diesel locomotive from the company in 1909. The world's first diesel-powered locomotive was operated in the summer of 1912 on the Winterthur–Romanshorn railway in Switzerland, but was not a commercial success. The locomotive weight was 95 tonnes and the power was 883 kW with a maximum speed of 100 km/h. Small numbers of prototype diesel locomotives were produced in a number of countries through the mid-1920s.\nA significant breakthrough occurred in 1914, when Hermann Lemp, a General Electric electrical engineer, developed and patented a reliable direct current electrical control system (subsequent improvements were also patented by Lemp). Lemp's design used a single lever to control both engine and generator in a coordinated fashion, and was the prototype for all diesel–electric locomotive control systems. In 1914, world's first functional diesel–electric railcars were produced for the \"Königlich-Sächsische Staatseisenbahnen\" (Royal Saxon State Railways) by Waggonfabrik Rastatt with electric equipment from Brown, Boveri & Cie and diesel engines from Swiss Sulzer AG. They were classified as . The first regular use of diesel–electric locomotives was in switching (shunter) applications. General Electric produced several small switching locomotives in the 1930s (the famous \"44-tonner\" switcher was introduced in 1940) Westinghouse Electric and Baldwin collaborated to build switching locomotives starting in 1929.\n\nIn 1929, the Canadian National Railways became the first North American railway to use diesels in mainline service with two units, 9000 and 9001, from Westinghouse.\n\nAlthough high-speed steam and diesel services were started before 1960s in Europe, they were not very successful.\nThe first electrified high-speed rail Tōkaidō Shinkansen was introduced in 1964 between Tokyo and Osaka in Japan. Since then high-speed rail transport, functioning at speeds up and above 300 km/h, has been built in Japan, Spain, France, Germany, Italy, the People's Republic of China, Taiwan (Republic of China), the United Kingdom, South Korea, Scandinavia, Belgium and the Netherlands. The construction of many of these lines has resulted in the dramatic decline of short haul flights and automotive traffic between connected cities, such as the London–Paris–Brussels corridor, Madrid–Barcelona, Milan–Rome–Naples, as well as many other major lines.\n\nHigh-speed trains normally operate on standard gauge tracks of continuously welded rail on grade-separated right-of-way that incorporates a large turning radius in its design. While high-speed rail is most often designed for passenger travel, some high-speed systems also offer freight service.\n\nBelgium took the lead in the Industrial Revolution on the Continent starting in the 1820s. It provided an ideal model for showing the value of the railways for speeding the industrial revolution. After splitting from the Netherlands in 1830, the new country decided to stimulate industry. It planned and funded a simple cross-shaped system that connected the major cities, ports and mining areas and linked to neighboring countries. Unusually, the Belgian state became a major contributor to early rail development and championed the creation of a national network with no duplication of lines. Belgium thus became the railway center of the region.\n\nThe system was built along British lines, often with British engineers doing the planning. Profits were low but the infrastructure necessary for rapid industrial growth was put in place. The first railway in Belgium, running from northern Brussels to Mechelen, was completed in May 1835.\n\nThe earliest railway in Britain was a wagonway system, a horse drawn wooden rail system, used by German miners at Caldbeck, Cumbria, England, perhaps from the 1560s. A wagonway was built at Prescot, near Liverpool, sometime around 1600, possibly as early as 1594. Owned by Philip Layton, the line carried coal from a pit near Prescot Hall to a terminus about half a mile away. On 26 July 1803, Jessop opened the Surrey Iron Railway, south of London erroneously considered first railway in Britain, also a horse-drawn one. It was not a railway in the modern sense of the word, as it functioned like a turnpike road. There were no official services, as anyone could bring a vehicle on the railway by paying a toll.\n\nThe Middleton Railway in Leeds, which was built in 1758, later became the world's oldest operational railway (other than funiculars), albeit now in an upgraded form. In 1764, the first railway in the Americas was built in Lewiston, New York.\nThe first passenger Horsecar or tram, Swansea and Mumbles Railway was opened between Swansea and Mumbles in Wales in 1807. Horse remained preferable mode for tram transport even after arrival of steam engines, well till the end of 19th century. The major reason was that the horse-cars were clean as compared to steam driven trams which caused smoke in city streets.\n\nIn 1812, Oliver Evans, an American engineer and inventor, published his vision of what steam railways could become, with cities and towns linked by a network of long distance railways plied by speedy locomotives, greatly speeding up personal travel and goods transport. Evans specified that there should be separate sets of parallel tracks for trains going in different directions. However, conditions in the infant United States did not enable his vision to take hold. This vision had its counterpart in Britain, where it proved to be far more influential. William James, a rich and influential surveyor and land agent, was inspired by the development of the steam locomotive to suggest a national network of railways. It seems likely that in 1808 James attended the demonstration running of Richard Trevithick's steam locomotive \"Catch me who can\" in London; certainly at this time he began to consider the long-term development of this means of transport. He proposed a number of projects that later came to fruition and is credited with carrying out a survey of the Liverpool and Manchester Railway. Unfortunately he became bankrupt and his schemes were taken over by George Stephenson and others. However, he is credited by many historians with the title of \"Father of the Railway\".\n\nIt was not until 1825, that the success of the Stockton and Darlington Railway, world's first public railway, proved that the railways could be made as useful to the general shipping public as to the colliery owner. This railway broke new ground by using rails made of rolled wrought iron, produced at Bedlington Ironworks in Northumberland. Such rails were stronger. This railway linked the town of Darlington with the port of Stockton-on-Tees and was intended to enable local collieries (which were connected to the line by short branches) to transport their coal to the docks. As this would constitute the bulk of the traffic, the company took the important step of offering to haul the colliery wagons or chaldrons by locomotive power, something that required a scheduled or timetabled service of trains. However, the line also functioned as a toll railway, on which private horse-drawn wagons could be carried. This curious hybrid of a system (which also included, at one stage, a horse-drawn passenger wagon) could not last and within a few years, traffic was restricted to timetabled trains. (However, the tradition of private owned wagons continued on railways in Britain until the 1960s.)\nThe success of the Stockton and Darlington encouraged the rich investors in the rapidly industrialising North West of England to embark upon a project to link the rich cotton manufacturing town of Manchester with the thriving port of Liverpool. The Liverpool and Manchester Railway was the first modern railway, in that both the goods and passenger traffic were operated by scheduled or timetabled locomotive hauled trains. When it was built, there was serious doubt that locomotives could maintain a regular service over the distance involved. A widely reported competition was held in 1829 called the Rainhill Trials, to find the most suitable steam engine to haul the trains. A number of locomotives were entered, including \"Novelty\", \"Perseverance\" and \"Sans Pareil\". The winner was Stephenson's Rocket, which steamed better because of its multi-tubular boiler (suggested by Henry Booth, a director of the railway company).\n\nThe promoters were mainly interested in goods traffic, but after the line opened on 15 September 1830, they were surprised to find that passenger traffic was just as remunerative. The success of the Liverpool and Manchester railway influenced the development of railways elsewhere in Britain and abroad. The company hosted many visiting deputations from other railway projects and many railwaymen received their early training and experience upon this line. The Liverpool and Manchester line was, however, only long. The world's first trunk line can be said to be the Grand Junction Railway, opening in 1837 and linking a midpoint on the Liverpool and Manchester Railway with Birmingham, via Crewe, Stafford and Wolverhampton.\n\nThe earliest locomotives in revenue service were small four-wheeled ones similar to the Rocket. However, the inclined cylinders caused the engine to rock, so they first became horizontal and then, in his \"Planet\" design, were mounted inside the frames. While this improved stability, the \"crank axles\" were extremely prone to breakage. Greater speed was achieved by larger driving wheels at expense of a tendency for wheel slip when starting. Greater tractive effort was obtained by smaller wheels coupled together, but speed was limited by the fragility of the cast iron connecting rods. Hence, from the beginning, there was a distinction between the light fast passenger locomotive and the slower more powerful goods engine. Edward Bury, in particular, refined this design and the so-called \"Bury Pattern\" was popular for a number of years, particularly on the London and Birmingham.\n\nMeanwhile, by 1840, Stephenson had produced larger, more stable, engines in the form of the 2-2-2 \"Patentee\" and six-coupled goods engines. Locomotives were travelling longer distances and being worked more extensively. The North Midland Railway expressed their concern to Robert Stephenson who was, at that time, their general manager, about the effect of heat on their fireboxes. After some experiments, he patented his so-called Long Boiler design. These became a new standard and similar designs were produced by other manufacturers, particularly Sharp Brothers whose engines became known affectionately as \"Sharpies\".\n\nThe longer wheelbase for the longer boiler produced problems in cornering. For his six-coupled engines, Stephenson removed the flanges from the centre pair of wheels. For his express engines, he shifted the trailing wheel to the front in the 4-2-0 formation, as in his \"Great A\". There were other problems: the firebox was restricted in size or had to be mounted behind the wheels; and for improved stability most engineers believed that the centre of gravity should be kept low.\n\nThe most extreme outcome of this was the Crampton locomotive which mounted the driving wheels behind the firebox and could be made very large in diameter. These achieved the hitherto unheard of speed of but were very prone to wheelslip. With their long wheelbase, they were unsuccessful on Britain's winding tracks, but became popular in the USA and France, where the popular expression became \"prendre le Crampton\".\n\nJohn Gray of the London and Brighton Railway disbelieved the necessity for a low centre of gravity and produced a series of locomotives that were much admired by David Joy who developed the design at the firm of E. B. Wilson and Company to produce the 2-2-2 Jenny Lind locomotive, one of the most successful passenger locomotives of its day. Meanwhile, the Stephenson 0-6-0 Long Boiler locomotive with inside cylinders became the archetypal goods engine.\n\nRailways quickly became essential to the swift movement of goods and labour that was needed for industrialization. In the beginning, canals were in competition with the railways, but the railways quickly gained ground as steam and rail technology improved and railways were built in places where canals were not practical.\n\nBy the 1850s, many steam-powered railways had reached the fringes of built-up London. But the new companies were not permitted to demolish enough property to penetrate the City or the West End, so passengers had to disembark at Paddington, Euston, King's Cross, Fenchurch Street, Charing Cross, Waterloo or Victoria and then make their own way by hackney carriage or on foot into the centre, thereby massively increasing congestion in the city. A Metropolitan Railway was built underground to connect several of these separate railway terminals and was the world's first \"Metro\".\n\nThe railways changed British society in numerous and complex ways. Although recent attempts to measure the economic significance of the railways have suggested that their overall contribution to the growth of GDP was more modest than an earlier generation of historians sometimes assumed, it is nonetheless clear that the railways had a sizeable impact in many spheres of economic activity. The building of railways and locomotives, for example, called for large quantities of heavy materials and thus provided a significant stimulus or ‘backward linkage’, to the coal-mining, iron-production, engineering and construction industries.\n\nThey also helped to reduce transaction costs, which in turn lowered the costs of goods: the distribution and sale of perishable goods such as meat, milk, fish and vegetables were transformed by the emergence of the railways, giving rise not only to cheaper produce in the shops but also to far greater variety in people's diets.\n\nFinally, by improving personal mobility the railways were a significant force for social change. Rail transport had originally been conceived as a way of moving coal and industrial goods but the railway operators quickly realised the potential market for railway travel, leading to an extremely rapid expansion in passenger services. The number of railway passengers trebled in just eight years between 1842 and 1850: traffic volumes roughly doubled in the 1850s and then doubled again in the 1860s.\n\nAs the historian Derek Aldcroft has noted, ‘in terms of mobility and choice they added a new dimension to everyday life’.\n\nIn France, railways were first operated by private coal companies the first legal agreement to build a railway was given in 1823 and the line (from Saint-Étienne to Andrézieux) was operated in 1827. Much of the equipment was imported from Britain but this stimulated machinery makers, which soon created a national heavy industry. Trains became a national medium for the modernization of backward regions and a leading advocate of this approach was the poet-politician Alphonse de Lamartine. One writer hoped that railways might improve the lot of \"populations two or three centuries behind their fellows\" and eliminate \"the savage instincts born of isolation and misery.\" Consequently, France built a centralized system that radiated from Paris (plus lines that cut east to west in the south). This design was intended to achieve political and cultural goals rather than maximize efficiency.\n\nAfter some consolidation, six companies controlled monopolies of their regions, subject to close control by the government in terms of fares, finances and even minute technical details. The central government department of Ponts et Chaussées [bridges and roads] brought in British engineers and workers, handled much of the construction work, provided engineering expertise and planning, land acquisition and construction of permanent infrastructure such as the track bed, bridges and tunnels. It also subsidized militarily necessary lines along the German border, which was considered necessary for the national defense. Private operating companies provided management, hired labor, laid the tracks and built and operated stations. They purchased and maintained the rolling stock—6,000 locomotives were in operation in 1880, which averaged 51,600 passengers a year or 21,200 tons of freight. .\n\nAlthough starting the whole system at once was politically expedient, it delayed completion and forced even more reliance on temporary experts brought in from Britain. Financing was also a problem. The solution was a narrow base of funding through the Rothschilds and the closed circles of the Bourse in Paris, so France did not develop the same kind of national stock exchange that flourished in London and New York. The system did help modernize the parts of rural France it reached and help to develop many local industrial centers, mostly in the North (coal and iron mines) and in the East (textiles and heavy industry). Critics such as Émile Zola complained that it never overcame the corruption of the political system, but rather contributed to it.\n\nThe railways probably helped the industrial revolution in France by facilitating a national market for raw materials, wines, cheeses and imported and exported manufactured products. In \"The Rise of Rail-Power in War and Conquest, 1833–1914\", published in 1915, Edwin A. Pratt wrote, \"the French railways … attained a remarkable degree of success. … It was estimated that the 75,966 men and 4,469 horses transported by rail from Paris to the Mediterranean or to the frontiers of the Kingdom of Sardinia between April 20 and 30 April [during the 1859 Second Italian War of Independence] would have taken sixty days to make the journey by road. … This… was about twice as fast as the best achievement recorded up to that time on the German railways. \" Yet the goals set by the French for their railway system were moralistic, political and military rather than economic. As a result, the freight trains were shorter and less heavily loaded than those in such rapidly industrializing nations such as Britain, Belgium or Germany. Other infrastructure needs in rural France, such as better roads and canals, were neglected because of the expense of the railways, so it seems likely that there were net negative effects in areas not served by the trains.\n\nAn operation was illustrated in Germany in 1556 by Georgius Agricola in his work \"De re metallica\". This line used \"Hund\" carts with unflanged wheels running on wooden planks and a vertical pin on the truck fitting into the gap between the planks to keep it going the right way. The miners called the wagons \"Hunde\" (\"dogs\") from the noise they made on the tracks. This system became very popular across Europe.\nThe takeoff stage of economic development came with the railroad revolution in the 1840s, which opened up new markets for local products, created a pool of middle managers, increased the demand for engineers, architects and skilled machinists and stimulated investments in coal and iron. Political disunity of three dozen states and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines did link the major cities; each German state was responsible for the lines within its own borders. Economist Friedrich List summed up the advantages to be derived from the development of the railway system in 1841:\n\n\nLacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain’s. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies and further rapid growth. Unlike the situation in France, the goal was support of industrialisation and so heavy lines crisscrossed the Ruhr and other industrial districts and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight a day and forged ahead of France.\n\nRail transport in the Netherlands is generally considered to have begun on 20 September 1839 when the first train, drawn by the locomotive \"De Arend\", successfully made the 16 km trip from Amsterdam to Haarlem. However, the first plan for a railroad in the Netherlands was launched only shortly after the first railroad opened in Britain.\n\nThe history of rail transport in the Netherlands can be described in six eras:\n\n\nIn the early 1830s, the Russian father and son inventors the Cherepanovs built the first Russian steam locomotive. The first railway line was built in Russia in 1837 between Saint-Petersburg and Tsarskoye Selo. It was 27 km long and linked the Imperial Palaces at Tsarskoye Selo and Pavlovsk. The track gauge was . Russia was in need of big transportation systems and geographically suited to railroads, with long flat stretches of land and comparatively simple land acquisition. It was hampered, however, by its outmoded political situation and a shortage of capital. Foreign initiative and capital were required. It was the Americans who brought the technology of railway construction to Russia. In 1842, planning began for the building of Russia's first important railway; it linked Moscow and St Petersburg.\n\nCuba, then a Spanish colony, built its first rail line in 1837. The history of rail transport in peninsular Spain begins in 1848 with the construction of a railway line between Barcelona and Mataró. In 1852, the first narrow gauge line was built. In 1863 a line reached the Portuguese border. By 1864, the Madrid-Irun line had been opened and the French border was reached.\n\nThe first Canadian railway, the Champlain and St. Lawrence Railroad, was opened in 1836 outside of Montreal, a seasonal portage railway to connect river traffic. It was followed by the Albion Railway in Stellarton, Nova Scotia in 1840, a collier railway connecting coal mines to a seaport. In Canada, the national government strongly supported railway construction for political goals. First it wanted to knit the far-flung provinces together and second, it wanted to maximize trade inside Canada and minimize trade with the United States, to avoid becoming an economic satellite. The Grand Trunk Railway of Canada linked Toronto and Montreal in 1853, then opened a line to Portland, Maine (which was ice-free) and lines to Michigan and Chicago. By 1870 it was the longest railway in the world. The Intercolonial line, finished in 1876, linked the Maritimes to Quebec and Ontario, tying them to the new Confederation.\n\nAnglo entrepreneurs in Montreal sought direct lines into the US and shunned connections with the Maritimes, with a goal of competing with American railroad lines heading west to the Pacific. Joseph Howe, Charles Tupper and other Nova Scotia leaders used the rhetoric of a \"civilizing mission\" centered on their British heritage, because Atlantic-centered railway projects promised to make Halifax the eastern terminus of an intercolonial railway system tied to London. Leonard Tilley, New Brunswick's most ardent railway promoter, championed the cause of \"economic progress,\" stressing that Atlantic Canadians needed to pursue the most cost-effective transportation connections possible if they wanted to expand their influence beyond local markets. Advocating an intercolonial connection to Canada and a western extension into larger American markets in Maine and beyond, New Brunswick entrepreneurs promoted ties to the United States first, connections with Halifax second and routes into central Canada last. Thus metropolitan rivalries between Montreal, Halifax and Saint John led Canada to build more railway lines per capita than any other industrializing nation, even though it lacked capital resources and had too little freight and passenger traffic to allow the systems to turn a profit.\n\nDen Otter (1997) challenges popular assumptions that Canada built transcontinental railways because it feared the annexationist schemes of aggressive Americans. Instead Canada overbuilt railroads because it hoped to compete with, even overtake Americans in the race for continental riches. It downplayed the more realistic Maritimes-based London-oriented connections and turned to utopian prospects for the farmlands and minerals of the west. The result was closer ties between north and south, symbolized by the Grand Trunk's expansion into the American Midwest. These economic links promoted trade, commerce and the flow of ideas between the two countries, integrating Canada into a North American economy and culture by 1880. About 700,000 Canadians migrated to the US in the late 19th century. The Canadian Pacific, paralleling the American border, opened a vital link to British Canada and stimulated settlement of the Prairies. The CP was affiliated with James J. Hill's American railways and opened even more connections to the South. The connections were two-way, as thousands of American moved to the Prairies after their own frontier had closed.\n\nTwo additional transcontinental lines were built to the west coast—three in all—but that was far more than the traffic would bear, making the system simply too expensive. One after another, the federal government was forced to take over the lines and cover their deficits. In 1923, the government merged the Grand Trunk, Grand Trunk Pacific, Canadian Northern and National Transcontinental lines into the new the Canadian National Railways system. Since most of the equipment was imported from Britain or the US and most of the products carried were from farms, mines or forests, there was little stimulation to domestic manufacturing. On the other hand, the railways were essential to the growth of the wheat regions in the Prairies and to the expansion of coal mining, lumbering and paper making. Improvements to the St. Lawrence waterway system continued apace and many short lines were built to river ports.\n\nThe earliest railway in Canada is a wooden railroad reportedly used in the construction of the French fortress at Louisburg, Nova Scotia.\n\nRailroads played a large role in the development of the United States from the industrial revolution in the North-east 1810–50 to the settlement of the West 1850–1890. The American railroad mania began with the Baltimore and Ohio Railroad in 1828 and flourished until the Panic of 1873 bankrupted many companies and temporarily ended growth.\n\nAlthough the South started early to build railways, it concentrated on short lines linking cotton regions to oceanic or river ports and the absence of an interconnected network was a major handicap during the Civil War. The North and Midwest constructed networks that linked every city by 1860. In the heavily settled Midwestern Corn Belt, over 80 percent of farms were within 10 miles of a railway, facilitating the shipment of grain, hogs and cattle to national and international markets. A large number of short lines were built, but thanks to a fast developing financial system based on Wall Street and oriented to railway bonds, the majority were consolidated into 20 trunk lines by 1890. State and local governments often subsidized lines, but rarely owned them.\n\nThe system was largely built by 1910, but then trucks arrived to eat away the freight traffic and automobiles (and later airplanes) to devour the passenger traffic. The use of diesel electric locomotives (after 1940) made for much more efficient operations that needed fewer workers on the road and in repair shops.\n\nRoute mileage peaked at 254,000 in 1916 and fell to 140,000 in 2009.\n\nIn 1830, there were about of railroad track, in short lines linked to coal and granite mines.). After this, railroad lines grew rapidly. Ten years later, in 1840, the railways had grown to . By 1860, on the eve of civil war, the length had reached , mostly in the North. The South had much less trackage and it was geared to moving cotton short distances to river or ocean ports. The Southern railroads were destroyed during the war but were soon rebuilt. By 1890, the national system was virtually complete with .\nIn 1869, the symbolically important transcontinental railroad was completed in the United States with the driving of a golden spike (near the city of Ogden).\n\nIn Latin America in the late 19th and early 20th centuries railways were critical elements in the early stages of modernization of the Latin American economy, especially in linking agricultural regions to export-oriented seaports. After 1870 Latin American governments encouraged further rail development through generous concessions that included government subsidies for construction. Railway construction is the subject of considerable scholarship, examining the economic, political, and social impacts of railroads. Railways transformed many regions of Latin America beginning in the late nineteenth century. \"Increasing exports of primary commodities, rising imports of capital goods, the expansion of activities drawing directly and indirectly on overseas investment, the rising share of manufacturing in output, and a generalized increase in the pace and scope of economic activity were all tied closely to the timing and character of the region's infrastructural development.\n\nRates of railway line construction were not uniform, but by 1870 railway line construction was underway, with Cuba leading with the largest railway track in service (1,295 km), followed by Chile (797 km), Brazil (744 km), Argentina (732 km), Peru (669 km), and Mexico (417 km). By 1900, Argentina (16,563 km), Brazil (15,316 km) and Mexico (13,615 km) were the leaders in length of track in service, and Peru, which had been an early leader in railway construction, had stagnated (1,790 km). In Mexico, growing nationalistic fervor led the government to bring the bulk of the nation's railroads under national control in 1909, with a new government corporation, Ferrocarriles Nacionales de México (FNM), that exercised control of the main trunk rail lines through a majority of share ownership.\n\nThe first proposals for railways in India were made in Madras in 1832. The first train in India ran from Red Hills to Chintadripet bridge in Madras in 1837. It was called \"Red Hill Railway\". It was hauled by a rotary steam engine locomotive manufactured by William Avery. It was built by Sir Arthur Cotton. It was primarily used for transporting granite stones for road building work in Madras. In 1845, a railway was built at Dowleswaram in Rajahmundry. It was called \"Godavari Dam Construction Railway\". It was also built by Arthur Cotton. It was used to supply stones for construction of a dam over Godavari.\n\nOn 8 May 1845, Madras Railway was incorporated. In the same year, the East India Railway company was incorporated. On 1 August 1849, Great Indian Peninsular Railway (GIPR) was incorporated. In 1851, a railway was built in Roorkee. It was called \"Solani Aqueduct Railway\". It was hauled by steam locomotive Thomason, named after a British officer-in-charge. It was used for transporting construction materials for building of aqueduct over Solani river. In 1852, the \"Madras Guaranteed Railway Company\" was incorporated.\n\nThe first passenger train in India ran between Bombay (Bori Bunder) and Thane on 16 April 1853. The 14-carriage train was hauled by three steam locomotives: Sahib, Sindh and Sultan. It ran for about 34 kilometers between these two cities carrying 400 people. The line was built and operated by GIPR. This railway line was built in broad gauge, which became the standard for the railways in the country. The first passenger railway train in eastern India ran from Howrah, near Calcutta to Hoogly, for distance of 24 miles, on 15 August 1854. The line was built and operated by EIR. The first passenger train in South India ran from Royapuram / Veyasarapady (Madras) to Wallajah Road (Arcot) on 1 July 1856, for a distance of 60 miles. It was built and operated by Madras Railway. On 24 February 1873, the first tramway (a horse-drawn tramway) opened in Calcutta between Sealdah and Armenian Ghat Street, a distance of 3.8 km.\n\nIranian railway history goes back to 1887 when an approximately 20-km long railway between Tehran and Ray was established. After this time many short railways were constructed but the main railway, Trans-Iranian Railway, was started in 1927 and operated in 1938 by connecting the Persian Gulf to the Caspian Sea.\n\nJapan developed its first railway line in 1872 with technical and materiel assistance provided by several western nations such as Britain and America.\n\nIt was in 1847 when the first railway was imagined but it was not until 1861 when it came into existence in the form of the railway built from Karachi to Kotri. Since then rail transport is a popular mode of non-independent transport in Pakistan.\n\n\n"}
{"id": "3800886", "url": "https://en.wikipedia.org/wiki?curid=3800886", "title": "IMEC", "text": "IMEC\n\nImec is an international R&D and innovation hub, active in the fields of nanoelectronics and digital technologies. It is led since 2009 by Luc Van den Hove.\n\nIn September 2016, imec merged with the Flemish digital research center, iMinds.\n\nImec is one of the leading organizations for R&D in nanoelectronics and digital technologies. The institute employs around 3,500 researchers from more than 75 countries and has numerous facilities dedicated to research and development around the world, including 12,000 square meters of cleanroom capacity for semiconductor processing.\n\nHeadquartered in Leuven (Belgium), imec is present in seven other international locations in six other countries: the Netherlands, Taiwan, China, India, the United States and Japan.\n\nIn 1982 the Flemish Government set up a program in the field of microelectronics with the goal to strengthen the microelectronics industry in Flanders. This program included setting up a laboratory for advanced research in microelectronics (IMEC), a semiconductor foundry (former Alcatel Microelectronics, now STMicroelectronics and AMI Semiconductor), and a training program for VLSI design engineers. The latter is now fully integrated in the IMEC activities.\n\nIMEC was founded in 1984 as a non-profit organization led by Prof. Roger Baron Van Overstraeten. The name Imec is an acronym of the original full name: Interuniversitair Micro-Electronica Centrum VZW. It is supervised by a Board of Directors, which includes delegates from industry, Flemish universities and the Flemish Government. Since 1984, IMEC has been led by Roger Van Overstraeten, Gilbert Declerck (as of June 1999), and Luc Van den hove (as of July 2009).\n\nIn February 2016 it was announced that imec would be merging with the Flemish digital research center, iMinds. The goal was to strengthen Flanders’ international authority as a technology hub.\n\nPhilippe Muyters, Flemish Minister of Innovation, stated: “When [imec and iMinds] were founded, the line between hardware and software was still very clear. Today, and especially in the future, this line is increasingly blurring – with technology, systems and applications being developed in close conjunction. The merger anticipates this trend and creates a high-tech research center for the digital economy that keeps Flanders on the world map.”\n\nThe merger was finalized on September 21, 2016.\n\nThe IMEC campus in Leuven, Belgium includes 24,400m² of office space, laboratories, training facilities, and technical support rooms. At the heart of the campus are 2 cleanrooms which run a semi-industrial operation (24/7). There is a 300mm cleanroom (450mm ready) that focuses on R&D towards (sub-)10 nm process technology and a 200mm cleanroom for R&D, development-on-demand, prototyping and low volume manufacturing on more-than-Moore technologies (sensors, actuators, and MEMS, NEMS etc.). IMEC has, among others, a pilot line for silicon and organic solar cells, unique laboratories for bioelectronics research, and equipment for materials characterization and reliability testing. For research on technologies for the intuitive internet of things, IMEC has dedicated labs for sensor and imaging technologies, wireless connectivity.\n\nIMEC performs research on different application fields of nanoelectronics, applications related to the intuitive internet of things,\nhealthcare and energy. More specifically this includes wearable health monitoring (EEG, ECG sensors, ...), life sciences (lab-on-chip, cells-on-chip, neuroprobes); wireless communication (reconfigurable radios, radar, ...); image sensors and vision systems (hyperspectral imaging, lens free microscopy, ...); large-area flexible electronics; solar cells and batteries; GaN power electronics, ….\n\nIMEC has offices at the following locations:\n"}
{"id": "43656643", "url": "https://en.wikipedia.org/wiki?curid=43656643", "title": "Integrated gasification fuel cell cycle", "text": "Integrated gasification fuel cell cycle\n\nLower-temperature fuel cell types such as the proton exchange membrane fuel cell, phosphoric acid fuel cell, and alkaline fuel cell require pure hydrogen as fuel, typically produced from external reforming of natural gas. However, fuels cells operating at high temperature such as the solid oxide fuel cell (SOFC) are not poisoned by carbon monoxide and carbon dioxide, and in fact can accept hydrogen, carbon monoxide, carbon dioxide, steam, and methane mixtures as fuel directly, because of their internal shift and reforming capabilities. This opens up the possibility of efficient fuel cell-based power cycles consuming solid fuels such as coal and biomass, the gasification of which results in syngas containing mostly hydrogen, carbon monoxide and methane which can be cleaned and fed directly to the SOFCs without the added cost and complexity of methane reforming, water gas shifting and hydrogen separation operations which would otherwise be needed to isolate pure hydrogen as fuel.\nA power cycle based on gasification of solid fuel and SOFCs is called an Integrated Gasification Fuel Cell (IGFC) cycle; the IGFC power plant is analogous to an integrated gasification combined cycle power plant, but with the gas turbine power generation unit replaced with a fuel cell (high temperature type such as SOFC) power generation unit. By taking advantage of intrinsically high energy efficiency of SOFCs and process integration, exceptionally high power plant efficiencies are possible. Furthermore, SOFCs in the IGFC cycle can be operated so as to isolate a carbon dioxide-rich anodic exhaust stream, allowing efficient carbon capture to address greenhouse gas emissions concerns of coal-based power generation.\n\nThe IGFC system combines use of SOFCs as a topping cycle to the gas turbine or heat recovery steam generator-based bottoming cycle. Typical major components of the IGFC system, this one centered on a SOFC module running at atmospheric pressure, are identified in the simplified cycle diagram.\n\nThe system fuel as depicted is coal, converted to syngas by the gasifier, which is then supplied to the SOFC module after cleanup and pressure reduction. The syngas pressure reduction step is accomplished in this system concept by an expander/generator, which thereby produces part of the cycle's gross power generation. Oxygen for the coal gasification process is provided by a conventional air separation unit, and steam for the gasifier is raised by power system heat and recycled water. Note that the SOFC module is configured to maintain the anode and cathode off-gas streams separated, and the anode off-gas, which contains some electrochemically-unreacted hydrogen and carbon monoxide, is combusted to completion at the oxy-combustor. Maintaining separation of the off-gas streams restricts the large atmospheric nitrogen content to the cathode side, and simplifies the CO capture process to anode off-gas cooling, water-vapor condensation, CO drying, and CO compression. Compressed CO is suitable for carbon utilization or storage (CUS) as appropriate. Heat recovered from the anode-side process can be used by a power-generating bottoming cycle consisting of a heat recovery steam generator and steam turbine. On the cathode side, process air for the SOFC electrochemical process and for module cooling is provided by an air blower; heat can be recovered from the hot cathode off-gas stream to preheat the process air as needed, and for the generation of additional power.\nDue to the inherently efficient SOFC, and to using recovered SOFC exhaust heat to generate additional electric power, an IGFC system is capable of operating at a high electric efficiency that significantly exceeds those associated with conventional pulverized coal and integrated gasification combined cycle power systems. IGFC efficiency margins considered achievable, based upon the U.S. Department of Energy's National Energy Technology Laboratory comparative studies of advanced power systems, are apparent in the table provided in subsequent discussion.\n\nImprovement in the IGFC cycle efficiency is possible by pressurized operation of the SOFCs, as depicted in the diagram of IGFC with pressurized SOFC cycle. The process is basically similar to the atmospheric-pressure cycle, but it would run the SOFC module at elevated pressure, achieving an SOFC voltage boost, and would replace the cathode-side process-air blower with an air compressor. Also, an expander/generator would be installed in the cathode off-gas stream to reduce gas pressures and generate additional power (this tends to drop the temperature of the gases so much that steam generation to run a steam turbine is not a viable option). Optionally, an expander/generator set could also be placed in the anode off-gas stream, just downstream of the oxy-combustor, and ahead of off-gas heat recovery.\n\nMultiple types of solid fuel gasifiers are commercially available for coal, petcoke, and biomass gasification. Designs vary depending on fuel and intended application. As a result, they can differ in the composition of the syngas produced and the efficiency with which they convert coal energy content to syngas energy content - a performance parameter typically termed cold gas efficiency. The gasifiers also vary in their main operating parameters – e.g., process temperature, pressure, and demands for oxygen and steam. For power systems based upon the integration of coal gasification and SOFC technologies, these parameters, particularly cold gas efficiency and the oxygen and steam demands, will affect electricity production efficiency.\n\nGasifiers are of three main types – entrained flow, moving bed, and fluidized bed. \nEntrained flow gasifiers (e.g., GE Energy, Shell, E-Gas™, Siemens) could attract early interest for fuel cell power system applications because they are relatively well developed and are used in current integrated gasification combined cycle power system designs and applications. Entrained flow gasification typically proceeds at relatively high process temperatures, requires oxygen input at relatively high rates, steam input at low to moderate rates, and it produces a syngas product with very small methane content, typically less than 1% (vol). Cold gas efficiencies for entrained flow gasification are usually in the 80% range.\nThe moving-bed gasifier (e.g., Lurgi) operates at moderate temperature levels, and with moderate oxygen and steam supply requirements. The cold gas efficiency achieved by this gasifier is higher, circa 90%, and its syngas product stream will have methane content nominally in the 4-5% (vol) range. Fluidized-bed gasification (e.g., KBR Transport) proceeds with similar characteristics, but will exhibit somewhat lower syngas methane content, typically in the 2-3% (vol) range.\n\nOf particular interest to the SOFC-based IGFC power system is catalytic coal gasification, because of the characteristically high methane content of its resultant syngas. This process experienced development in the 1980s for synthetic natural gas production purposes. Compared to conventional gasification summarized above, a catalytic gasifier would require less oxygen input, run at a lower process temperature, and produce a syngas stream with a higher methane concentration [15-30% (vol)], in addition to hydrogen and carbon monoxide. With its lower operating temperature, a relatively high cold gas efficiency of at least 90% is projected for catalytic gasification, and this characteristic, as well as the process need for less oxygen input, would directly support high-efficiency IGFC power system operation. Furthermore, internal reformation of the substantial methane content of its syngas within the SOFC module can be used by design to assist module cooling, and can thereby lead to reductions in the parasitic power demand associated with cooling air supply.\nThe SOFC electric power generator module in an IGFC system could conceivably be fueled with syngas supplied by any of the available conventional coal gasifiers, and studies indicate that power system electric efficiencies in the 45-50% range are achievable, depending on whether the power system uses atmospheric-pressure or pressurized SOFC modules. However, from the efficiency viewpoint particularly, the preferred coal gasification approach for the application is catalytic. Using that technology, IGFC system efficiencies in 56-60% range are projected, again depending on SOFC module pressurization. Estimates and comparisons are tabulated below.\n\nIn addition to high power system efficiencies, studies also project significant IGFC system power plant capital cost, cost of electricity, and net water usage advantages: IGFC electric power systems integrating catalytic coal gasification with SOFC module designs that separate anode and cathode off-gas streams, and feature methane reformation-augmented SOFC cooling, would operate cleanly with very high electric efficiencies, while providing for high levels of carbon capture, and requiring low net water input.\n\nSOFC design and IGFC process configuration can greatly simplify the capture of carbon dioxide, which will increasingly be demanded for low greenhouse gas emissions of most fossil-fuel utilization processes. In conventional combustion, fuel is burned in air, resulting in exhaust gases containing large amount of nitrogen from which capture of a pure carbon dioxide stream (needed for storage of carbon in greenhouse gas emissions control scenarios) is inefficient. In oxy-combustion, oxygen is extracted from air and used for fuel combustion, resulting in exhaust gases uncontaminated with nitrogen from which capture of a pure carbon dioxide stream is efficient. However, a large energy penalty for doing the air separation required for isolating the oxygen stream in the first place is incurred. In contrast, for SOFC function neither inefficient carbon capture from exhaust gases nor air separation is required: the only required interaction of the anode and cathode reactant streams is the transfer of oxygen from cathode side (air) to anode side (fuel). All carbon, excepting the negligible amount in atmospheric air coming in at the cathode, will enter the module with fuel on the anode side, and it must exit the anode as carbon dioxide and carbon monoxide. By designing the SOFC module to keep anode and cathode off-gas streams separated, dilution of that carbon-rich stream with atmospheric nitrogen from the cathode side is avoided, allowing simple and inexpensive carbon dioxide separation and capture downstream.\n\n\n"}
{"id": "28000935", "url": "https://en.wikipedia.org/wiki?curid=28000935", "title": "Intubation", "text": "Intubation\n\n\"See\" Tracheal Intubation\n\nIntubation (sometimes entubation) is a medical procedure involving the insertion of a tube into the body. Patients are generally anesthetized beforehand. Examples include tracheal intubation, and the balloon tamponade with a Sengstaken-Blakemore tube (a tube into the gastrointestinal tract).\n"}
{"id": "36636042", "url": "https://en.wikipedia.org/wiki?curid=36636042", "title": "Law Enforcement Warning", "text": "Law Enforcement Warning\n\nLaw Enforcement Warning (SAME code: LEW) is a warning issued through the Emergency Alert System (EAS) in the United States to warn the public of criminal events that pose a threat to public safety. These include jailbreaks, riots, and bomb explosions. An authorized law enforcement agency may blockade roads, waterways, or facilities, evacuate or deny access to affected areas, and arrest violators or suspicious persons. The warning is usually issued by a law enforcement agency and is relayed by the National Weather Service.\n"}
{"id": "32059319", "url": "https://en.wikipedia.org/wiki?curid=32059319", "title": "List of satellite pass predictors", "text": "List of satellite pass predictors\n\nThe following is a list of tools on a variety of platforms that may be used to predict the pass of an orbiting artificial satellite over a given point on Earth. They are used to generate a list of dates, times and directions when and where objects such as the International Space Station, Genesis, or Tiangong 1 space stations will be visible to ground observers, as well as many man-made objects that can be seen with the unaided eye including the Hubble Space Telescope.\n\n\nThis section includes applications for the iPhone, iPad and iPod touch. Apps generally use coordinates provided by the device's built in GPS. Some require an active internet connection others update periodically\n\n\n\n\nAll websites and applications base their predictions on formula using two-line element sets which describe the satellites and their orbits.\n"}
{"id": "20857", "url": "https://en.wikipedia.org/wiki?curid=20857", "title": "Masonry", "text": "Masonry\n\nMasonry is the building of structures from individual units, which are often laid in and bound together by mortar; the term \"masonry\" can also refer to the units themselves. The common materials of masonry construction are brick, building stone such as marble, granite, travertine, and limestone, cast stone, concrete block, glass block, and adobe. Masonry is generally a highly durable form of construction. However, the materials used, the quality of the mortar and workmanship, and the pattern in which the units are assembled can substantially affect the durability of the overall masonry construction. A person who constructs masonry is called a mason or bricklayer. These are both classified as construction trades.\n\nMasonry is commonly used for walls and buildings. Brick and concrete block are the most common types of masonry in use in industrialized nations and may be either weight-bearing or a veneer. Concrete blocks, especially those with hollow cores, offer various possibilities in masonry construction. They generally provide great compressive strength, and are best suited to structures with light transverse loading when the cores remain unfilled. Filling some or all of the cores with concrete or concrete with steel reinforcement (typically rebar) offers much greater tensile and lateral strength to structures.\n\n\n\nMasonry has high compressive strength under vertical loads but has low tensile strength (against twisting or stretching) unless reinforced. The tensile strength of masonry walls can be increased by thickening the wall, or by building masonry \"piers\" (vertical columns or ribs) at intervals. Where practical, steel reinforcements such as windposts can be added.\n\nA masonry veneer wall consists of masonry units, usually clay-based bricks, installed on one or both sides of a structurally independent wall usually constructed of wood or masonry. In this context the brick masonry is primarily decorative, not structural. The brick veneer is generally connected to the structural wall by brick ties (metal strips that are attached to the structural wall, as well as the mortar joints of the brick veneer). There is typically an air gap between the brick veneer and the structural wall. As clay-based brick is usually not completely waterproof, the structural wall will often have a water-resistant surface (usually tar paper) and weep holes can be left at the base of the brick veneer to drain moisture that accumulates inside the air gap. Concrete blocks, real and cultured stones, and veneer adobe are sometimes used in a very similar veneer fashion.\n\nMost insulated buildings that utilize concrete block, brick, adobe, stone, veneers or some combination thereof feature interior insulation in the form of fiberglass batts between wooden wall studs or in the form of rigid insulation boards covered with plaster or drywall. In most climates this insulation is much more effective on the exterior of the wall, allowing the building interior to take advantage of the aforementioned thermal mass of the masonry. This technique does, however, require some sort of weather-resistant exterior surface over the insulation and, consequently, is generally more expensive.\n\nThe strength of a masonry wall is not entirely dependent on the bond between the building material and the mortar; the friction between the interlocking blocks of masonry is often strong enough to provide a great deal of strength on its own. The blocks sometimes have grooves or other surface features added to enhance this interlocking, and some \"dry set\" masonry structures forgo mortar altogether.\n\nSolid brickwork is made of two or more wythes of bricks with the units running horizontally (called \"stretcher\" bricks) bound together with bricks running transverse to the wall (called \"header\" bricks). Each row of bricks is known as a course. The pattern of headers and stretchers employed gives rise to different 'bonds' such as the common bond (with every sixth course composed of headers), the English bond, and the Flemish bond (with alternating stretcher and header bricks present on every course). Bonds can differ in strength and in insulating ability. Vertically staggered bonds tend to be somewhat stronger and less prone to major cracking than a non-staggered bond.\n\nThe wide selection of brick styles and types generally available in industrialized nations allow much variety in the appearance of the final product. In buildings built during the 1950s-1970s, a high degree of uniformity of brick and accuracy in masonry was typical. In the period since then this style was thought to be too sterile, so attempts were made to emulate older, rougher work. Some brick surfaces are made to look particularly rustic by including \"burnt\" bricks, which have a darker color or an irregular shape. Others may use antique salvage bricks, or new bricks may be artificially aged by applying various surface treatments, such as tumbling. The attempts at rusticity of the late 20th century have been carried forward by masons specializing in a free, artistic style, where the courses are intentionally \"not\" straight, instead weaving to form more organic impressions.\n\nA crinkle-crankle wall is a brick wall that follows a serpentine path, rather than a straight line. This type of wall is more resistant to toppling than a straight wall; so much so that it may be made of a single wythe of unreinforced brick and so despite its longer length may be more economical than a straight wall.\n\nBlocks of cinder concrete (\"cinder blocks\" or \"breezeblocks\"), ordinary concrete (\"concrete blocks\"), or hollow tile are generically known as Concrete Masonry Units (CMUs). They usually are much larger than ordinary bricks and so are much faster to lay for a wall of a given size. Furthermore, cinder and concrete blocks typically have much lower water absorption rates than brick. They often are used as the structural core for veneered brick masonry, or are used alone for the walls of factories, garages and other industrial-style buildings where such appearance is acceptable or desirable. Such blocks often receive a stucco surface for decoration. Surface-bonding cement, which contains synthetic fibers for reinforcement, is sometimes used in this application and can impart extra strength to a block wall. Surface-bonding cement is often pre-coloured and can be stained or painted thus resulting in a finished stucco-like surface.\n\nThe primary structural advantage of concrete blocks in comparison to smaller clay-based bricks is that a CMU wall can be reinforced by filling the block voids with concrete with or without steel rebar. Generally, certain voids are designated for filling and reinforcement, particularly at corners, wall-ends, and openings while other voids are left empty. This increases wall strength and stability more economically than filling and reinforcing all voids. Typically, structures made of CMUs will have the top course of blocks in the walls filled with concrete and tied together with steel reinforcement to form a bond beam. Bond beams are often a requirement of modern building codes and controls. Another type of steel reinforcement, referred to as ladder-reinforcement, can also be embedded in horizontal mortar joints of concrete block walls. The introduction of steel reinforcement generally results in a CMU wall having much greater lateral and tensile strength than unreinforced walls.\n\n\"Architectural masonry is the evolvement of standard concrete masonry blocks into aesthetically pleasing concrete masonry units (CMUs).\" CMUs can be manufactured to provide a variety of surface appearances. They can be colored during manufacturing or stained or painted after installation. They can be split as part of the manufacturing process, giving the blocks a rough face replicating the appearance of natural stone, such as brownstone. CMUs may also be scored, ribbed, sandblasted, polished, striated (raked or brushed), include decorative aggregates, be allowed to slump in a controlled fashion during curing, or include several of these techniques in their manufacture to provide a decorative appearance.\n\n\"Glazed concrete masonry units are manufactured by bonding a permanent colored facing (typically composed of polyester resins, silica sand and various other chemicals) to a concrete masonry unit, providing a smooth impervious surface.\"\n\nGlass block or glass brick are blocks made from glass and provide a translucent to clear vision through the block.\n\nStone blocks used in masonry can be dressed or rough, though in both examples: corners, door and window jambs, and similar areas are usually dressed.\nStone masonry utilizing dressed stones is known as ashlar masonry, whereas masonry using irregularly shaped stones is known as rubble masonry. Both rubble and ashlar masonry can be laid in coursed rows of even height through the careful selection or cutting of stones, but a great deal of stone masonry is uncoursed.\n\nGabions are baskets, usually now of zinc-protected steel (galvanized steel) that are filled with fractured stone of medium size. These will act as a single unit and are stacked with setbacks to form a revetment or retaining wall. They have the advantage of being both well drained and flexible, and so resistant to flood, water flow from above, frost damage, and soil flow. Their expected useful life is only as long as the wire they are composed of and if used in severe climates (such as shore-side in a salt water environment) must be made of appropriate corrosion-resistant wire. Most modern gabions are rectangular.\n\nEarlier gabions were often cylindrical wicker baskets, open at both ends, used usually for temporary, often military, construction.\n\nSimilar work can be done with finer aggregates using cellular confinement.\n\nA low grade concrete may be placed in woven plastic sacks similar to that used for sandbags and then emplaced. The sacks are then watered and the emplacement then becomes a series of artificial stones that conform to one another and to adjacent soil and structures. This conformation makes them resistant to displacement. The sack becomes non-functional and eventually disintegrates. This type of masonry is frequently used to protect the entrances and exits of water conduits where a road passes over a stream or dry wash. It is also used to protect stream banks from erosion, especially where a road passes close by.\n\nMasonry walls have an endothermic effect of its hydrates, as in chemically bound water, as well as unbound moisture from the concrete block, as well as the poured concrete if the hollow cores inside the blocks are filled.\n\nMasonry buildings can also be built to increase safety by reducing fire damage, such as the use of fire cuts during construction.\n\nFrom the point of view of material modelling, masonry is a special material of extreme mechanical properties (with a very high ratio between strength in compression and in tension), so that the applied loads do not diffuse as they do in elastic bodies, but tend to percolate along lines of high stiffness, see the figure on the right and watch a video for more details.\n\n\n"}
{"id": "21903992", "url": "https://en.wikipedia.org/wiki?curid=21903992", "title": "Medical encyclopedia", "text": "Medical encyclopedia\n\nA medical encyclopedia is a comprehensive written compendium that holds information about diseases, medical conditions, tests, symptoms, injuries, and surgeries. It may contain an extensive gallery of medicine-related photographs and illustrations. \nA medical encyclopedia provides information to readers about health questions. It may also contain some information about the history of diseases, the development of medical technology uses to detect diseases in its early phase.\nA licensed physician should be consulted for diagnosis and treatment of any and all medical conditions.\n\nFour major elements define a medical encyclopaedia: its subject matter, its scope, its method of organization, and its method of production:\n\nMedlinePlus is a free Web site that provides consumer health information for patients, families, and health care providers. MedlinePlus brings together quality information from the United States National Library of Medicine, the National Institutes of Health (NIH), other U.S. government agencies, and health-related organizations. The U.S. National Library of Medicine produces and maintains MedlinePlus.\n\nThe Centers for Disease Control and Prevention (or CDC) is an agency of the United States Department of Health and Human Services based in the Metro Atlanta area, adjacent to the campus of Emory University and northeast of downtown Atlanta. It works to protect public health and safety by providing information to enhance health decisions, and it promotes health through partnerships with state health departments and other organizations. The CDC focuses national attention on developing and applying disease prevention and control (especially infectious diseases), environmental health, occupational safety and health, health promotion, prevention and education activities designed to improve the health of the people of the United States.\n\nWebMD is an American provider of health information services. It is primarily known for its public Internet site, which has information regarding health and health care, including a symptom checklist, pharmacy information, blogs of physicians with specific topics and a place to store personal medical information. The site was reported to have received over 17.1 million average monthly unique visitors in Q1 2007 and is the leading health portal in the United States. The site receives information from accredited individuals and is reviewed by a medical review board consisting of four physicians to ensure accuracy.\n\nMedscape is a professional portal for physicians with 30 medical specialty areas and over 30 physician discussion boards. Recently WebMD has been acquired by the News Corporation.\n\nMedicineNet, Inc. is owned and Operated by WebMD and part of the WebMD Network emphasizing non-technical, in-depth medical peer-reviewed information for consumers. Founded in 1996, WebMD acquired MedicineNet in 2004. MedicineNet, Inc.'s main office is in San Clemente, Calif., and the corporate office is in New York City.\n\n\n"}
{"id": "28149133", "url": "https://en.wikipedia.org/wiki?curid=28149133", "title": "Mentec PDP-11", "text": "Mentec PDP-11\n\nMentec Limited was founded in 1978 and initially focused on the development of monitoring and control software and systems. It was a significant Digital reseller and OEM in Ireland. Mentec Computer Systems Limited was a subsidiary of Mentec Limited that developed PDP-11 processors. Mentec Inc. was a US-based subsidiary of Mentec Limited. In the early 1980s it had a range of remote terminal units based on the SBC/11-21 (Falcon).\n\nOnce the DEC J-11 PDP-11 processor chip set became available in 1982 Mentec commenced the design of its first PDP-11 single board computer the M70. In 1994 Digital transferred the PDP-11 operating systems to Mentec Inc.\n\nThe M70 was developed between 1982 and 1984. It was a quad Q-bus module based on the J-11 chipset incorporating onboard ECC DRAM, bootstrap EPROMs and 4 serial lines implemented using DEC DC319 DLART chips.\n\nThe M71 was a version of the M70 intended for process control. It provided for 1/4 or 1 M Byte of ECC DRAM, up to 1/2 MB of EPROM, lines (DC-319 DLARTs) and two parallel ports implemented using 8255 chips. It was initially designed by Mentec for use in its own Remote Terminal Units.\n\nThe M80 was a further development of the M70 but using parity memory and a slightly higher clock rate. It also introduced software configuration via the bootstrap which all but eliminated wire-wrap configuration.\n\nThis was effectively merely a clock tweaked version of the M80.\n\nThe M100 was the last of Mentec's J-11 based processor boards. It was a somewhat tidied up and faster re-design of the M90.\n\nSome late models incorporated a daughter card with a Xilinx part which replaced the DLARTs and implemented a FIFO to prevent overruns for OEM applications.\n\nA small number of late models incorporated an SRAM daughter card which replaced the on-board DRAM.\n\nThe M11 was a microcoded re-implementation from scratch of the M100.\nIt was based around two Texas Instruments TI8832 ALUs and a TI 8818 microsequencer.\nOne of the ALUs was used as the processor ALU while the second was used to implement the memory management unit. An Intel i960 processor was used to load the microcode, perform floating point (in IEEE format) and provide ODT.\nThe 4 DLARTs of the earlier M100 were emulated on a single Xilinx part.\nAll of the memory (both microcode and PDP-11 main memory) was implemented using SRAM.\nWhile not of any significant effect in the field it suffered from the fact that it used a large number of microcode controlled drivers onto tri-state buses, which made developing microcode somwehat hazardous.\n\nThe M11 design was implemented in VHDL and fully simulated using Mentor Graphics QuickSim II with behavioural language models for both the Q-Bus and console UART. It ran patched versions of the Digital PDP-11/23 CPU diagnostics on the simulator before any hardware was constructed.\n\nThe M1 was an ASIC re-implementation of the M11. Despite being an ASIC implementation it was also fully microcoded.\n"}
{"id": "1592956", "url": "https://en.wikipedia.org/wiki?curid=1592956", "title": "Millipede memory", "text": "Millipede memory\n\nMillipede memory is a non-volatile computer memory stored on nanoscopic pits burned into the surface of a thin polymer layer, read and written by a MEMS-based probe. It promised a data density of more than 1 terabit per square inch (1 gigabit per square millimeter), which is about the limit of the perpendicular recording hard drives. Millipede storage technology was pursued as a potential replacement for magnetic recording in hard drives, at the same time reducing the form-factor to that of flash media. IBM demonstrated a prototype millipede storage device at CeBIT 2005, and was trying to make the technology commercially available by the end of 2007. However, because of concurrent advances in competing storage technologies, no commercial product has been made available since then.\n\nThe main memory of modern computers is constructed from one of a number of DRAM-related devices. DRAM basically consists of a series of capacitors, which store data as the presence or absence of electrical charge. Each capacitor and its associated control circuitry, referred to as a \"cell\", holds one bit, and bits can be read or written in large blocks at the same time. In contrast, hard drives store data on a disk that is covered with a magnetic material; data is represented as local magnetisation of this material. Reading and writing are accomplished by a single head, which waits for the requested memory location to pass under the head while the disk spins. As a result, the drive's performance is limited by the mechanical speed of the motor, and is generally hundreds of thousands of times slower than DRAM. However, since the \"cells\" in a hard drive are much smaller, the storage density is much higher than DRAM.\n\nMillipede storage attempts to combine features of both. Like a hard drive, millipede stores data in a substrate or medium and accesses the data by moving the medium under the head as well. However, millipede uses many nanoscopic heads that can read and write in parallel, thereby increasing the throughput. Additionally, millipede's physical medium stores a bit in a small area, leading to high storage densities. Mechanically, millipede uses numerous \"atomic force probes\" each of which is responsible for reading and writing a large number of bits associated with it. Bits are stored as a pit, or the absence of one, in the surface of a thermo-active polymer deposited as a thin film on a carrier known as the \"sled.\" Any one probe can only read or write a fairly small area of the sled available to it, a \"storage field\". Normally the sled is moved to position the selected bits under the probe using electromechanical actuators similar to those that position the read/write head in a typical hard drive, although the actual distance moved is tiny. The sled is moved in a scanning pattern to bring the requested bits under the probe, a process known as \"x/y scan.\"\n\nThe amount of memory serviced by any one field/probe pair is fairly small, but so is its physical size. Many such field/probe pairs are used to make up a memory device. Data reads and writes can be spread across many fields in parallel, increasing the throughput and improving the access times. For instance, a single 32-bit value would normally be written as a set of single bits sent to 32 different fields. In the initial experimental devices, the probes were mounted in a 32x32 grid for a total of 1,024 probes. As the layout looked like the legs on a millipede (animal), the name stuck. The design of the cantilever array involves making numerous mechanical cantilevers, on which a probe has to be mounted. All the cantilevers are made entirely out of silicon, using surface micromachining at the wafer surface.\n\nNon-crosslinked polymers retain a low glass temperature, around 120 °C for PMMA and if the tip is heated to above the glass temperature, it leaves a small indentation. Indentations have been made at 3 nm lateral resolution. By heating the probe immediately next to an indentation, the polymer will re-melt and fill in the indentation, erasing it (see also: thermo-mechanical scanning probe lithography). After writing, the probe tip can be used to read the indentations. If each indentation is treated as one bit then a storage density of 0.9 Tb/in could theoretically be achieved.\nEach probe in the cantilever array stores and reads data thermo-mechanically, handling one bit at a time. To accomplish a read, the probe tip is heated to around 300 °C and moved in proximity to the data sled. If the probe is located over a pit the cantilever will push it into the hole, increasing the surface area in contact with the sled, and in turn increasing the cooling as heat leaks into the sled from the probe. In the case where there is no pit at that location, only the very tip of the probe remains in contact with the sled, and the heat leaks away more slowly. The electrical resistance of the probe is a function of its temperature, rising with increasing temperature. Thus when the probe drops into a pit and cools, this registers as a drop in resistance. A low resistance will be translated to a \"1\" bit, or a \"0\" bit otherwise. While reading an entire storage field, the tip is dragged over the entire surface and the resistance changes are constantly monitored. \n\nTo write a bit, the tip of the probe is heated to a temperature above the \"glass transition temperature\" of the polymer used to manufacture the data sled, which is generally acrylic glass. In this case the transition temperature is around 400 °C. To write a \"1\", the polymer in proximity to the tip is softened, and then the tip is gently touched to it, causing a dent. To erase the bit and return it to the zero state, the tip is instead pulled up from the surface, allowing surface tension to pull the surface flat again. Older experimental systems used a variety of erasure techniques that were generally more time consuming and less successful. These older systems offered around 100,000 erases, but the available references do not contain enough information to say if this has been improved with the newer techniques. \n\nAs one might expect, the need to heat the probes requires a fairly large amount of power for general operation. However, the exact amount is dependent on the speed that data is being accessed; at slower rates the cooling during read is smaller, as is the number of times the probe has to be heated to a higher temperature to write. When operated at data rates of a few megabits per second, Millipede is expected to consume about 100 milliwatts, which is in the range of flash memory technology and considerably below hard drives. However, one of the main advantages of the Millipede design is that it is highly parallel, allowing it to run at much higher speeds into the GB/s. At these sorts of speeds one might expect power requirements more closely matching current hard drives. Data transfer speed is limited to the kilobits-per-second range for an individual probe, which amounts to a few megabits for an entire array. Experiments done at IBM's Almaden Research Center showed that individual tips could support data rates as high as 1 - 2 megabits per second, potentially offering aggregate speeds in the GB/s range.\n\nThe earliest generation millipede devices used probes 10 nanometers in diameter and 70 nanometers in length, producing pits about 40 nm in diameter on fields 92 µm x 92 µm. Arranged in a 32 x 32 grid, the resulting 3 mm x 3 mm chip stores 500 megabits of data or 62.5 MB, resulting in an areal density, the number of bits per square inch, on the order of 200 Gbit/in². IBM initially demonstrated this device in 2003, planning to introduce it commercially in 2005. By that point hard drives were approaching 150 Gbit/in², and have since surpassed it.\n\nDevices demonstrated at CeBIT in 2005 have improved on the basic design, using a 64 x 64 cantilever chips with a 7 mm x 7 mm data sled, boosting the data storage capacity to 800 Gbit/in² using smaller pits. It appears the pit size can scale to about 10 nm, resulting in a theoretical areal density just over 1Tbit/in². IBM planned to introduce devices based on this sort of density in 2007. For comparison, as of late 2011, laptop hard drives are shipping with a density of 636 Gbit/in², and it is expected that heat-assisted magnetic recording and patterned media together could support densities of 10 Tbit/in². Flash reached almost 250 Gbit/in² in early 2010. As of 2015, because of concurrent advances in competing storage technologies, no commercial product has been made available so far.\n\n\n"}
{"id": "51682311", "url": "https://en.wikipedia.org/wiki?curid=51682311", "title": "Multefire", "text": "Multefire\n\nMulteFire™ is an LTE-based technology that operates standalone in unlicensed and shared spectrum, including the global 5 GHz band. Based on 3GPP Release 13 and 14, MulteFire technology supports Listen-Before-Talk for fair co-existence with Wi-Fi and other technologies operating in the same spectrum. It supports private LTE and neutral host deployment models. Target vertical markets include industrial IoT, enterprise, cable, and various other vertical markets.\n\nThe MulteFire Release 1.0 specification was developed by the MulteFire Alliance, an independent, diverse and international member-driven consortium. Release 1.0 was published to MulteFire Alliance members in January 2017 and was made publicly available in April 2017. The MulteFire Alliance is currently working on Release 1.1 which will add further optimizations for IoT and new spectrum bands.\n\nAccording to Harbor Research in its published white paper, the market opportunity for private LTE networks for industrial and commercial IoT will reach $118.5 billion in 2023. It also reported that the total addressable revenue for Enterprise markets deploying private and neutral host LTE with MulteFire will reach $5.7 billion by 2025.  \n\nThe MulteFire Alliance has grown to more than 40 members. Its board members include Boingo Wireless, CableLabs, Ericsson, Huawei, Intel, Nokia, Qualcomm and SoftBank. The organization is open to any company with an interest in advancing LTE and cellular technology in unlicensed and shared spectrum.  \n\n"}
{"id": "1929510", "url": "https://en.wikipedia.org/wiki?curid=1929510", "title": "National Safety Council", "text": "National Safety Council\n\nThe National Safety Council (NSC) is a 501(c)(3) nonprofit, public service organization promoting health and safety in the United States of America. Headquartered in Itasca, Illinois, NSC is a member organization, founded in 1913 and granted a congressional charter in 1953. Members include more than 55,000 businesses, labor organizations, schools, public agencies, private groups and individuals.\n\nThe group focuses on areas where the greatest number of preventable injuries and deaths occur, including workplace safety, prescription medication abuse, teen driving, cell phone use while driving and safety in homes and communities.\n\nIn 1912, the first Cooperative Safety Congress was held in Milwaukee, Wisconsin. The event was sponsored by the Association of Iron and Steel Electrical Engineers. The approximately 200 attendees, representing industry and government, resolved to “organize and create a permanent body for the promotion of the safety to human life in the industries of the United States.\n\nAt the Second Safety Congress in 1913, the National Council for Industrial Safety was established. It was headquartered in Chicago, Illinois and Robert W. Campbell served as first president and William H. Cameron served as secretary.\nThe name was changed to National Safety Council in 1914, to reflect the organization's expanded scope to include traffic and home safety. As membership increased, the NSC began producing posters, technical fact sheets, and other publications.\nIn 1953, the U.S. Congress and President Dwight D. Eisenhower recognized the importance of the NSC’s efforts with a Congressional charter to: “…arouse and maintain the interest of the people of the United States… in safety and in accident prevention, and to encourage the adoption and institution of safety methods by all persons, corporations, and other organizations.\"\n\n\nNSC is governed by a Board of Directors and a Board of Delegates. The Board of Directors manages fiduciary and strategic affairs. The Board of Delegates develops the mission agenda, creates public policies, and tracks safety, health and environmental trends. More than 2,000 volunteers, drawn from NSC industry volunteer divisions, assist the boards in determining policies, operating procedures and programs to be developed and implemented by the Council’s professional staff.\n\nThe National Safety Council’s network of 40 local chapters conducts safety, health and environmental efforts at the community level, providing training, conferences, workshops, consultation, newsletters, updates and safety support materials, as well as networking avenues.\n\nMembers of NSC are segmented into Divisions, also known as special interest groups. Division members plan and create programs for the annual NSC Congress & Expo, and participate in discussions of research findings, new concepts, trends, and ideas for safety challenges. Divisions meet twice a year. The divisions are Business & Industry, Construction, Highway Traffic Safety, Labor, Motor Transportation, and Utilities. Some divisions are further segmented into sections.\n\nHeld each fall, the NSC Congress & Expo attracts between 15,000-18,000 safety and health professionals, plus industry suppliers from several countries. The event promotes safety and health products and services, and new safety technologies and training methods. Members attending the annual Congress can also earn continuing education credits by participating in technical sessions and professional development seminars.\n\nIn 1996 NSC established June as National Safety Month, aiming to increase awareness of the leading safety and health risks and ultimately decrease the number of unintentional injuries and deaths. Each week focuses on a specific safety venue: workplace, traffic, home, and community.\n\n\n"}
{"id": "29954555", "url": "https://en.wikipedia.org/wiki?curid=29954555", "title": "NordBEC", "text": "NordBEC\n\nThe EBEC (European BEST Engineering Competition) Nordic is an engineering competition. It was first conceived at the regional meeting in Røros in autumn 2007. The Nordic engineering competition did not officially begin until the regional meeting in Copenhagen in spring 2009. Shortly after, a coordinator was selected, followed by the selection of a hosting BEST group. At the regional meeting in Uppsala in spring 2009, the Nordic engineering competition was established as nordBEC, standing for Nordic BEST Engineering Competition. The first nordBEC was held in Copenhagen, from 27–31 March 2010, and the second one in Trondheim, from 16–20 April 2011\n\nNordBEC gathers the winners of local rounds of engineering competitions organized in top Scandinavian universities. As a result, the best students from the entire region will be selected. During the competition, students work in teams of 4, using their creativity, innovative thinking, and teamwork skills in order to come up with solutions for challenges set for them.\n\nNordBEC has two categories:\n\nIn the Nordic region there are 10 different BEST groups. In 2010, eight of these groups have decided to have a local engineering competition. The winning teams in each local engineering competition attend the nordBEC. The following universities actively participate:\n\nnordBEC (Nordic BEST Engineering Competition) was organised for the first time by Local BEST Group Copenhagen between the 27th and 31 March 2010. 60 students representing 8 Nordic Universities of Technology obtained passes for nordBEC by winning Local BEST Engineering Competitions (LBECs) at their home universities. The winners won a chance to participate the European BEST Engineering Competition, which took place in Cluj-Napoca, August 2010.\n\nThe Second edition was organised by Local BEST Group Trondheim from the 16th till 20 April 2011.\n\n"}
{"id": "7062643", "url": "https://en.wikipedia.org/wiki?curid=7062643", "title": "Oil burner (engine)", "text": "Oil burner (engine)\n\nAn oil burner engine is a steam engine that uses oil as its fuel. The term is usually applied to a locomotive or ship engine that burns oil to heat water, to produce the steam which drives the pistons, or turbines, from which the power is derived.\n\nThis is mechanically very different from diesel engines, which use internal combustion, although they are sometimes colloquially referred to as oil burners.\n\nAn early pioneer of this form of engine was James Holden, of Britain's Great Eastern Railway. In James Holden's system, steam was raised by burning coal before the oil fuel was turned on.\n\nSome oil-burning engines were originally designed to be coal powered but were converted. When a coal-burning steam locomotive is converted to burn oil, various modifications are usual:\n\nThe latter two changes are needed because oil firing produces higher temperatures than coal firing, and can cause rapid erosion of metal. For a similar reason, the smokebox is sometimes painted with silver-coloured heat-resisting paint.\n\n\nNilgiri Mountain railway\n\nJames Holden's use of oil firing on the Great Eastern Railway is mentioned above and it was used sporadically on Britain's railways, usually because of coal shortages. A Parliamentary question was asked about it in 1919.\n\n\n\n\n\n\n"}
{"id": "722773", "url": "https://en.wikipedia.org/wiki?curid=722773", "title": "Phalaris arundinacea", "text": "Phalaris arundinacea\n\nPhalaris arundinacea, sometimes known as reed canary grass, is a tall, perennial bunchgrass that commonly forms extensive single-species stands along the margins of lakes and streams and in wet open areas, with a wide distribution in Europe, Asia, northern Africa and North America. Other common names for the plant include gardener's-garters in English, alpiste roseau in French, rohrglanzgras in German, kusa-yoshi in Japanese, caniço-malhado in Portuguese, and hierba cinta and pasto cinto in Spanish.\n\nThe stems can reach 2 meters in height. The leaf blades are usually green, but may be variegated. The panicles are up to 30 centimeters long. The spikelets are light green, often streaked with darker green or purple. This is a perennial grass which spreads underground by its thick rhizomes.\n\nA number of cultivars of \"P. arundinacea\" have been selected for use as ornamental plants, including variegated (striped) cultivars – sometimes called ribbon grass – such as 'Castor' and 'Feesey'. The latter has a pink tinge to the leaves. When grown, although drought-tolerant, it likes abundant water and can even be grown as an aquatic plant.\n\nFrom Plants for a Future (pfaf.org):\nEdible Uses\nEdible Parts: Leaves; Root; Seed; Stem.\nEdible Uses: Sweetener.\n\nRoot - raw or cooked like potatoes[2, 13, 74, 102, 106, 183]. It contains up to 5% sugar. The flavour and texture are best when the root is young and still growing[144]. It can be dried, ground coarsely and used as a porridge[12, 46, 62]. In Russia they are harvested and processed into starch[269]. Young shoots - raw or cooked[61, 62, 102, 179]. They are best if used before the leaves form, when they are really delicious[144]. They can be used like bamboo shoots[183]. The partly unfolded leaves can be used as a potherb and the Japanese dry young leaves, grind them into a powder and mix them with cereal flour when making dumplings[183]. The stems are reported to contain 4.8 g protein, 0.8 g fat, 90.0 g total carbohydrate, 41.2 g fiber, and 4.4 g ash[269]. Seed - raw or cooked[257]. It can be ground into a powder and used as a flour[57, 62, 102, 106]. The seed is rather small and difficult to remove from the husk but it is said to be very nutritious[183]. A sugar is extracted from the stalks or wounded stems[2, 5, 62, 95]. A sweet liquorice-like taste[95], it can be eaten raw or cooked[62]. The stems can be boiled in water and then the water boiled off in order to obtain the sugar[178]. A sugary gum that exudes from the stems can be rolled into balls and eaten as sweets[183]. A powder extracted from the dried stems can be moistened and roasted like marshmallow[62, 95, 102, 183]. \n\nReed canarygrass grows well on poor soils and contaminated industrial sites, and researchers at Teesside University's Contaminated Land & Water Centre have suggested it as the ideal candidate for phytoremediation in improving soil quality and biodiversity at brownfield sites.\n\nThe grass can also easily be turned into bricks or pellets for burning in biomass power stations. Furthermore, it provides fibers which find use in pulp and papermaking processes.\n\n\"P. arundinacea\" is also planted as a hay crop or for forage.\n\nThis species of \"Phalaris\" may also be used as a source for the psychedelic drugs DMT, 5-MeO-DMT and 5-OH-DMT (bufotenin), as well as Hordenine and 5-MeO-NMT; however, N,N-DMT is considered most desirable. Although the concentrations of these compounds is lower than in other potential sources, such as \"Psychotria viridis\" and \"Mimosa tenuiflora\", large enough quantities of the grass can be refined to make an ad hoc ayahuasca brew.\n\nIn many places, \"P. arundinacea\" is an invasive species in wetlands, particularly in disturbed areas. It has been reported as an invasive weed in floodplains, riverside meadows, and other wetland habitat types around the world. When \"P. arundinacea\" invades a wetland, it inhibits native vegetation and reduces biological diversity. It alters the entire ecosystem. The grass propagates by seed and rhizome, and once established, is difficult to eradicate.\n\nSome species contain gramine.\n\nLeaves of \"P. arundinacea\" contain DMT, 5-MeO-DMT and bufotenin. Levels of beta-carbolines and hordenine have also been reported.\n\n"}
{"id": "65910", "url": "https://en.wikipedia.org/wiki?curid=65910", "title": "Printed circuit board", "text": "Printed circuit board\n\nA printed circuit board (PCB) mechanically supports and electrically connects electronic components or electrical components using conductive tracks, pads and other features etched from one or more sheet layers of copper laminated onto and/or between sheet layers of a non-conductive substrate. Components are generally soldered onto the PCB to both electrically connect and mechanically fasten them to it.\n\nPrinted circuit boards are used in all but the simplest electronic products. They are also used in some electrical products, such as passive switch boxes.\n\nAlternatives to PCBs include wire wrap and point-to-point construction, both once popular but now rarely used. PCBs require additional design effort to lay out the circuit, but manufacturing and assembly can be automated. Specialized CAD software is available to do much of the work of layout. Mass-producing circuits with PCBs is cheaper and faster than with other wiring methods, as components are mounted and wired in one operation. Large numbers of PCBs can be fabricated at the same time, and the layout only has to be done once. PCBs can also be made manually in small quantities, with reduced benefits.\n\nPCBs can be single-sided (one copper layer), double-sided (two copper layers on both sides of one substrate layer), or multi-layer (outer and inner layers of copper, alternating with layers of substrate). Multi-layer PCBs allow for much higher component density, because circuit traces on the inner layers would otherwise take up surface space between components. The rise in popularity of multilayer PCBs with more than two, and especially with more than four, copper planes was concurrent with the adoption of surface mount technology. However, multilayer PCBs make repair, analysis, and field modification of circuits much more difficult and usually impractical.\n\nThe world market for bare PCBs exceeded $60.2 billion in 2014. In 2018, the Global Single Sided Printed Circuit Board Market Analysis Report estimated that the PCB market would reach $79 billion by 2024.\n\nBefore the development of printed circuit boards electrical and electronic circuits were wired point-to-point on a chassis. Typically, the chassis was a sheet metal frame or pan, sometimes with a wooden bottom. Components were attached to the chassis, usually by insulators when the connecting point on the chassis was metal, and then their leads were connected directly or with jumper wires by soldering, or sometimes using crimp connectors, wire connector lugs on screw terminals, or other methods. Circuits were large, bulky, heavy, and relatively fragile (even discounting the breakable glass envelopes of the vacuum tubes that were often included in the circuits), and production was labor-intensive, so the products were expensive.\n\nDevelopment of the methods used in modern printed circuit boards started early in the 20th century. In 1903, a German inventor, Albert Hanson, described flat foil conductors laminated to an insulating board, in multiple layers. Thomas Edison experimented with chemical methods of plating conductors onto linen paper in 1904. Arthur Berry in 1913 patented a print-and-etch method in the UK, and in the United States Max Schoop obtained a patent to flame-spray metal onto a board through a patterned mask. Charles Ducas in 1927 patented a method of electroplating circuit patterns.\n\nThe Austrian engineer Paul Eisler invented the printed circuit as part of a radio set while working in the UK around 1936. In 1941 a multi-layer printed circuit was used in German magnetic influence naval mines. Around 1943 the USA began to use the technology on a large scale to make proximity fuses for use in World War II. After the war, in 1948, the USA released the invention for commercial use. Printed circuits did not become commonplace in consumer electronics until the mid-1950s, after the \"Auto-Sembly\" process was developed by the United States Army. At around the same time in the UK work along similar lines was carried out by Geoffrey Dummer, then at the RRDE.\n\nEven as circuit boards became available, the point-to-point chassis construction method remained in common use in industry (such as TV and hi-fi sets) into at least the late 1960s. Printed circuit boards were introduced to reduce the size, weight, and cost of parts of the circuitry. In 1960, a small consumer radio receiver might be built with all its circuitry on one circuit board, but a TV set would probably contain one or more circuit boards.\n\nPredating the printed circuit invention, and similar in spirit, was John Sargrove's 1936–1947 Electronic Circuit Making Equipment (ECME) which sprayed metal onto a Bakelite plastic board. The ECME could produce three radio boards per minute.\n\nDuring World War II, the development of the anti-aircraft proximity fuse required an electronic circuit that could withstand being fired from a gun, and could be produced in quantity. The Centralab Division of Globe Union submitted a proposal which met the requirements: a ceramic plate would be screenprinted with metallic paint for conductors and carbon material for resistors, with ceramic disc capacitors and subminiature vacuum tubes soldered in place. The technique proved viable, and the resulting patent on the process, which was classified by the U.S. Army, was assigned to Globe Union. It was not until 1984 that the Institute of Electrical and Electronics Engineers (IEEE) awarded Mr. Harry W. Rubinstein the Cledo Brunetti Award for early key contributions to the development of printed components and conductors on a common insulating substrate. Mr. Rubinstein was honored in 1984 by his alma mater, the University of Wisconsin-Madison, for his innovations in the technology of printed electronic circuits and the fabrication of capacitors. This invention also represents a step in the development of integrated circuit technology, as not only wiring but also passive components were fabricated on the ceramic substrate.\n\nOriginally, every electronic component had wire leads, and a PCB had holes drilled for each wire of each component. The component leads were then inserted through the holes and soldered to the copper PCB traces. This method of assembly is called \"through-hole\" construction. In 1949, Moe Abramson and Stanislaus F. Danko of the United States Army Signal Corps developed the \"Auto-Sembly\" process in which component leads were inserted into a copper foil interconnection pattern and dip soldered. The patent they obtained in 1956 was assigned to the U.S. Army. With the development of board lamination and etching techniques, this concept evolved into the standard printed circuit board fabrication process in use today. Soldering could be done automatically by passing the board over a ripple, or wave, of molten solder in a wave-soldering machine. However, the wires and holes are inefficient since drilling holes is expensive and consumes drill bits and the protruding wires are cut off and discarded.\n\nFrom the 1980s onward, small surface mount parts have been used increasingly instead of through-hole components; this has led to smaller boards for a given functionality and lower production costs, but with some additional difficulty in servicing faulty boards.\n\nIn the 1990s the use of multilayer surface boards became more frequent. As a result, size was further minimized and both flexible and rigid PCBs were incorporated in different devices. In 1995 PCB manufacturers began using microvia technology to produce High-Density Interconnect (HDI) PCBs.\n\nHDI technology allows for a denser design on the PCB and significantly smaller components. As a result, components can be closer and the paths between them shorter. HDIs use blind/buried vias, or a combination that includes microvias. With multi-layer HDI PCBs the interconnection of stacked vias is even stronger, thus enhancing reliability in all conditions. The most common applications for HDI technology are computer and mobile phone components as well as medical equipment and military communication equipment. A 4-layer HDI microvia PCB Cost is equivalent in quality to an 8-layer through-hole PCB. However, the cost is much lower.\n\nRecent advances in 3D printing have meant that there are several new techniques in PCB creation. 3D printed electronics (PEs) can be utilized to print items layer by layer and subsequently the item can be printed with a liquid ink that contains electronic functionalities.\n\nManufacturers may not support component-level repair of printed circuit boards because of the relatively low cost to replace compared with the time and cost of troubleshooting to a component level. In board-level repair, the technician identifies the board (PCA) on which the fault resides and replaces it. This shift is economically efficient from a manufacturer's point of view but is also materially wasteful, as a circuit board with hundreds of good components may be discarded and replaced due to the failure of one minor and inexpensive part such as a resistor or capacitor. This practice is a significant contributor to the problem of e-waste.\n\nA basic PCB consists of a flat sheet of insulating material and a layer of copper foil, laminated to the substrate. Chemical etching divides the copper into separate conducting lines called tracks or \"circuit traces\", pads for connections, vias to pass connections between layers of copper, and features such as solid conductive areas for EM shielding or other purposes. The tracks function as wires fixed in place, and are insulated from each other by air and the board substrate material. The surface of a PCB may have a coating that protects the copper from corrosion and reduces the chances of solder shorts between traces or undesired electrical contact with stray bare wires. For its function in helping to prevent solder shorts, the coating is called solder resist.\n\nA printed circuit board can have multiple copper layers. A two-layer board has copper on both sides; multi layer boards sandwich additional copper layers between layers of insulating material. Conductors on different layers are connected with vias, which are copper-plated holes that function as electrical tunnels through the insulating substrate. Through-hole component leads sometimes also effectively function as vias. After two-layer PCBs, the next step up is usually four-layer. Often two layers are dedicated as power supply and ground planes, and the other two are used for signal wiring between components.\n\n\"Through hole\" components are mounted by their wire leads passing through the board and soldered to traces on the other side. \"Surface mount\" components are attached by their leads to copper traces on the same side of the board. A board may use both methods for mounting components. PCBs with only through-hole mounted components are now uncommon. Surface mounting is used for transistors, diodes, IC chips, resistors and capacitors. Through-hole mounting may be used for some large components such as electrolytic capacitors and connectors.\n\nThe pattern to be etched into each copper layer of a PCB is called the \"artwork\". The etching is usually done using photoresist which is coated onto the PCB, then exposed to light projected in the pattern of the artwork. The resist material protects the copper from dissolution into the etching solution. The etched board is then cleaned. A PCB design can be mass-reproduced in a way similar to the way photographs can be mass-duplicated from film negatives using a photographic printer.\n\nIn multi-layer boards, the layers of material are laminated together in an alternating sandwich: copper, substrate, copper, substrate, copper, etc.; each plane of copper is etched, and any internal vias (that will not extend to both outer surfaces of the finished multilayer board) are plated-through, before the layers are laminated together. Only the outer layers need be coated; the inner copper layers are protected by the adjacent substrate layers.\n\nFR-4 glass epoxy is the most common insulating substrate. Another substrate material is cotton paper impregnated with phenolic resin, often tan or brown.\n\nWhen a PCB has no components installed, it is less ambiguously called a \"printed wiring board\" (\"PWB\") or \"etched wiring board\". However, the term \"printed wiring board\" has fallen into disuse. A PCB populated with electronic components is called a \"printed circuit assembly\" (\"PCA\"), \"printed circuit board assembly\" or \"PCB assembly\" (\"PCBA\"). In informal usage, the term \"printed circuit board\" most commonly means \"printed circuit assembly\" (with components). The IPC preferred term for assembled boards is \"circuit card assembly\" (\"CCA\"), and for assembled backplanes it is \"backplane assemblies\". \"Card\" is another widely used informal term for a \"printed circuit assembly\".\n\nA PCB may be \"silkscreen\" printed with a legend identifying the components, test points, or identifying text. Originally, an actual silkscreen printing process was used for this purpose, but today other, finer quality printing methods are usually used instead. Normally the screen printing is not significant to the function of the PCBA.\n\nA minimal PCB for a single component, used for prototyping, is called a \"breakout board\". The purpose of a breakout board is to \"break out\" the leads of a component on separate terminals so that manual connections to them can be made easily. Breakout boards are especially used for surface-mount components or any components with fine lead pitch.\n\nAdvanced PCBs may contain components embedded in the substrate.\n\nThe first PCBs used through-hole technology, mounting electronic components by leads inserted through holes on one side of the board and soldered onto copper traces on the other side. Boards may be single-sided, with an unplated component side, or more compact double-sided boards, with components soldered on both sides. Horizontal installation of through-hole parts with two axial leads (such as resistors, capacitors, and diodes) is done by bending the leads 90 degrees in the same direction, inserting the part in the board (often bending leads located on the back of the board in opposite directions to improve the part's mechanical strength), soldering the leads, and trimming off the ends. Leads may be soldered either manually or by a wave soldering machine.\n\nThrough-hole manufacture adds to board cost by requiring many holes to be drilled accurately, and it limits the available routing area for signal traces on layers immediately below the top layer on multi-layer boards, since the holes must pass through all layers to the opposite side. Once surface-mounting came into use, small-sized SMD components were used where possible, with through-hole mounting only of components unsuitably large for surface-mounting due to power requirements or mechanical limitations, or subject to mechanical stress which might damage the PCB (e.g. by lifting the copper off the board surface).\n\nSurface-mount technology emerged in the 1960s, gained momentum in the early 1980s and became widely used by the mid-1990s.\nComponents were mechanically redesigned to have small metal tabs or end caps that could be soldered directly onto the PCB surface, instead of wire leads to pass through holes. Components became much smaller and component placement on both sides of the board became more common than with through-hole mounting, allowing much smaller PCB assemblies with much higher circuit densities.\nSurface mounting lends itself well to a high degree of automation, reducing labor costs and greatly increasing production rates. Components can be supplied mounted on carrier tapes. Surface mount components can be about one-quarter to one-tenth of the size and weight of through-hole components, and passive components much cheaper. However, prices of semiconductor surface mount devices (SMDs) are determined more by the chip itself than the package, with little price advantage over larger packages, and some wire-ended components, such as 1N4148 small-signal switch diodes, are actually significantly cheaper than SMD equivalents.\n\nEach trace consists of a flat, narrow part of the copper foil that remains after etching. Its resistance, determined by its width, thickness, and length, must be sufficiently low for the current the conductor will carry. Power and ground traces may need to be wider than signal traces. In a multi-layer board one entire layer may be mostly solid copper to act as a ground plane for shielding and power return. For microwave circuits, transmission lines can be laid out in a planar form such as stripline or microstrip with carefully controlled dimensions to assure a consistent impedance. In radio-frequency and fast switching circuits the inductance and capacitance of the printed circuit board conductors become significant circuit elements, usually undesired; conversely, they can be used as a deliberate part of the circuit design, as in Distributed element filters, obviating the need for additional discrete components.\n\nThe European Union bans the use of lead (among other heavy metals) in consumer items, a piece of legislature called the RoHS, for Restriction of Hazardous Substances, directive. PCBs to be sold in the EU must be RoHS-compliant, meaning that all manufacturing processes must not involve the use of lead, all solder used must be lead-free, and all components mounted on the board must be free of lead, mercury, cadmium, and other heavy metals.\n\nLaminates are manufactured by curing under pressure and temperature layers of cloth or paper with thermoset resin to form an integral final piece of uniform thickness. The size can be up to in width and length. Varying cloth weaves (threads per inch or cm), cloth thickness, and resin percentage are used to achieve the desired final thickness and dielectric characteristics. Available standard laminate thickness are listed in\nANSI/IPC-D-275.\n\nThe cloth or fiber material used, resin material, and the cloth to resin ratio determine the laminate's type designation (FR-4, CEM-1, G-10, etc.) and therefore the characteristics of the laminate produced. Important characteristics are the level to which the laminate is fire retardant, the dielectric constant (e), the loss factor (tδ), the tensile strength, the shear strength, the glass transition temperature (T), and the Z-axis expansion coefficient (how much the thickness changes with temperature).\n\nThere are quite a few different dielectrics that can be chosen to provide different insulating values depending on the requirements of the circuit. Some of these dielectrics are polytetrafluoroethylene (Teflon), FR-4, FR-1, CEM-1 or CEM-3. Well known pre-preg materials used in the PCB industry are FR-2 (phenolic cotton paper), FR-3 (cotton paper and epoxy), FR-4 (woven glass and epoxy), FR-5 (woven glass and epoxy), FR-6 (matte glass and polyester), G-10 (woven glass and epoxy), CEM-1 (cotton paper and epoxy), CEM-2 (cotton paper and epoxy), CEM-3 (non-woven glass and epoxy), CEM-4 (woven glass and epoxy), CEM-5 (woven glass and polyester). Thermal expansion is an important consideration especially with ball grid array (BGA) and naked die technologies, and glass fiber offers the best dimensional stability.\n\nFR-4 is by far the most common material used today. The board stock with unetched copper on it is called \"copper-clad laminate\".\n\nWith decreasing size of board features and increasing frequencies, small nonhomogeneities like uneven distribution of fiberglass or other filler, thickness variations, and bubbles in the resin matrix, and the associated local variations in the dielectric constant, are gaining importance.\n\nThe circuitboard substrates are usually dielectric composite materials. The composites contain a matrix (usually an epoxy resin) and a reinforcement (usually a woven, sometimes nonwoven, glass fibers, sometimes even paper), and in some cases a filler is added to the resin (e.g. ceramics; titanate ceramics can be used to increase the dielectric constant).\n\nThe reinforcement type defines two major classes of materials: woven and non-woven. Woven reinforcements are cheaper, but the high dielectric constant of glass may not be favorable for many higher-frequency applications. The spatially nonhomogeneous structure also introduces local variations in electrical parameters, due to different resin/glass ratio at different areas of the weave pattern. Nonwoven reinforcements, or materials with low or no reinforcement, are more expensive but more suitable for some RF/analog applications.\n\nThe substrates are characterized by several key parameters, chiefly thermomechanical (glass transition temperature, tensile strength, shear strength, thermal expansion), electrical (dielectric constant, loss tangent, dielectric breakdown voltage, leakage current, tracking resistance...), and others (e.g. moisture absorption).\n\nAt the glass transition temperature the resin in the composite softens and significantly increases thermal expansion; exceeding T then exerts mechanical overload on the board components - e.g. the joints and the vias. Below T the thermal expansion of the resin roughly matches copper and glass, above it gets significantly higher. As the reinforcement and copper confine the board along the plane, virtually all volume expansion projects to the thickness and stresses the plated-through holes. Repeated soldering or other exposition to higher temperatures can cause failure of the plating, especially with thicker boards; thick boards therefore require a matrix with a high T.\n\nThe materials used determine the substrate's dielectric constant. This constant is also dependent on frequency, usually decreasing with frequency. As this constant determines the signal propagation speed, frequency dependence introduces phase distortion in wideband applications; as flat a dielectric constant vs frequency characteristics as is achievable is important here. The impedance of transmission lines decreases with frequency, therefore faster edges of signals reflect more than slower ones.\n\nDielectric breakdown voltage determines the maximum voltage gradient the material can be subjected to before suffering a breakdown (conduction, or arcing, through the dielectric).\n\nTracking resistance determines how the material resists high voltage electrical discharges creeping over the board surface.\n\nLoss tangent determines how much of the electromagnetic energy from the signals in the conductors is absorbed in the board material. This factor is important for high frequencies. Low-loss materials are more expensive. Choosing unnecessarily low-loss material is a common engineering error in high-frequency digital design; it increases the cost of the boards without a corresponding benefit. Signal degradation by loss tangent and dielectric constant can be easily assessed by an eye pattern.\n\nMoisture absorption occurs when the material is exposed to high humidity or water. Both the resin and the reinforcement may absorb water; water also may be soaked by capillary forces through voids in the materials and along the reinforcement. Epoxies of the FR-4 materials aren't too susceptible, with absorption of only 0.15%. Teflon has very low absorption of 0.01%. Polyimides and cyanate esters, on the other side, suffer from high water absorption. Absorbed water can lead to significant degradation of key parameters; it impairs tracking resistance, breakdown voltage, and dielectric parameters. Relative dielectric constant of water is about 73, compared to about 4 for common circuit board materials. Absorbed moisture can also vaporize on heating, as during soldering, and cause cracking and delamination, the same effect responsible for \"popcorning\" damage on wet packaging of electronic parts. Careful baking of the substrates may be required to dry them prior to soldering.\n\nOften encountered materials:\n\nLess-often encountered materials:\n\nCopper thickness of PCBs can be specified directly or as the weight of copper per area (in ounce per square foot) which is easier to measure. One ounce per square foot is 1.344 mils or 34 micrometers thickness. \"Heavy copper\" is a layer exceeding three ounces of copper per ft, or approximately 0.0042 inches (4.2 mils, 105 μm) thick. Heavy copper layers are used for high current or to help dissipate heat.\n\nOn the common FR-4 substrates, 1 oz copper per ft (35 µm) is the most common thickness; 2 oz (70 µm) and 0.5 oz (18 µm) thickness is often an option. Less common are 12 and 105 µm, 9 µm is sometimes available on some substrates. Flexible substrates typically have thinner metalization. Metal-core boards for high power devices commonly use thicker copper; 35 µm is usual but also 140 and 400 µm can be encountered.\n\nSafety Standard UL 796 covers component safety requirements for printed wiring boards for use as components in devices or appliances. Testing analyzes characteristics such as flammability, maximum operating temperature, electrical tracking, heat deflection, and direct support of live electrical parts.\n\nInitially PCBs were designed manually by creating a photomask on a clear mylar sheet, usually at two or four times the true size. Starting from the schematic diagram the component pin pads were laid out on the mylar and then traces were routed to connect the pads. Rub-on dry transfers of common component footprints increased efficiency. Traces were made with self-adhesive tape. Pre-printed non-reproducing grids on the mylar assisted in layout. The finished photomask was photolithographically reproduced onto a photoresist coating on the blank copper-clad boards.\n\nModern PCBs are designed with dedicated layout software, generally in the following steps:\n\nPCB manufacturing consists of many steps.\n\nManufacturing starts from the fabrication data generated by computer aided design, and component information. The fabrication data is read into the CAM (Computer Aided Manufacturing) software. CAM performs the following functions:\n\n\nSeveral small printed circuit boards can be grouped together for processing as a panel. A panel consisting of a design duplicated \"n\"-times is also called an \"n\"-panel, whereas a \"multi-panel\" combines several different design onto a single panel. The outer tooling strip often includes tooling holes, a set of panel fiducials, a test coupon, and may include hatched copper pour or similar patterns for even copper distribution over the whole panel in order to avoid bending. The assemblers often mount components on panels rather than single PCBs because this is efficient.\n\nThe panel is eventually broken into individual PCBs along perforations or grooves in the panel. Today depaneling is often done by lasers which cut the board with no contact. Laser depaneling reduces stress on the fragile circuits, improving the yield of defect-free units.\n\nThe first step is to replicate the pattern in the fabricator's CAM system on a protective mask on the copper foil PCB layers. Subsequent etching removes the unwanted copper. (Alternatively, a conductive ink can be ink-jetted on a blank (non-conductive) board. This technique is also used in the manufacture of hybrid circuits.)\n\nThe method chosen depends on the number of boards to be produced and the required resolution.\n\n\n\n\nSubtractive methods remove copper from an entirely copper-coated board to leave only the desired copper pattern. In additive methods the pattern is electroplated onto a bare substrate using a complex process. The advantage of the additive method is that less material is needed and less waste is produced. In the full additive process the bare laminate is covered with a photosensitive film which is imaged (exposed to light through a mask and then developed which removes the unexposed film). The exposed areas are sensitized in a chemical bath, usually containing palladium and similar to that used for through hole plating which makes the exposed area capable of bonding metal ions. The laminate is then plated with copper in the sensitized areas. When the mask is stripped, the PCB is finished.\n\nSemi-additive is the most common process: The unpatterned board has a thin layer of copper already on it. A reverse mask is then applied. (Unlike a subtractive process mask, this mask exposes those parts of the substrate that will eventually become the traces.) Additional copper is then plated onto the board in the unmasked areas; copper may be plated to any desired weight. Tin-lead or other surface platings are then applied. The mask is stripped away and a brief etching step removes the now-exposed bare original copper laminate from the board, isolating the individual traces. Some single-sided boards which have plated-through holes are made in this way. General Electric made consumer radio sets in the late 1960s using additive boards.\n\nThe (semi-)additive process is commonly used for multi-layer boards as it facilitates the plating-through of the holes to produce conductive vias in the circuit board.\n\nChemical etching is usually done with ammonium persulfate or ferric chloride. For PTH (plated-through holes), additional steps of electroless deposition are done after the holes are drilled, then copper is electroplated to build up the thickness, the boards are screened, and plated with tin/lead. The tin/lead becomes the resist leaving the bare copper to be etched away.\n\nThe simplest method, used for small-scale production and often by hobbyists, is immersion etching, in which the board is submerged in etching solution such as ferric chloride. Compared with methods used for mass production, the etching time is long. Heat and agitation can be applied to the bath to speed the etching rate. In bubble etching, air is passed through the etchant bath to agitate the solution and speed up etching. Splash etching uses a motor-driven paddle to splash boards with etchant; the process has become commercially obsolete since it is not as fast as spray etching. In spray etching, the etchant solution is distributed over the boards by nozzles, and recirculated by pumps. Adjustment of the nozzle pattern, flow rate, temperature, and etchant composition gives predictable control of etching rates and high production rates.\n\nAs more copper is consumed from the boards, the etchant becomes saturated and less effective; different etchants have different capacities for copper, with some as high as 150 grams of copper per litre of solution. In commercial use, etchants can be regenerated to restore their activity, and the dissolved copper recovered and sold. Small-scale etching requires attention to disposal of used etchant, which is corrosive and toxic due to its metal content.\n\nThe etchant removes copper on all surfaces exposed by the resist. \"Undercut\" occurs when etchant attacks the thin edge of copper under the resist; this can reduce conductor widths and cause open-circuits. Careful control of etch time is required to prevent undercut. Where metallic plating is used as a resist, it can \"overhang\" which can cause short-circuits between adjacent traces when closely spaced. Overhang can be removed by wire-brushing the board after etching.\n\nMulti-layer printed circuit boards have trace layers inside the board. This is achieved by laminating a stack of materials in a press by applying pressure and heat for a period of time. This results in an inseparable one piece product. For example, a four-layer PCB can be fabricated by starting from a two-sided copper-clad laminate, etch the circuitry on both sides, then laminate to the top and bottom pre-preg and copper foil. It is then drilled, plated, and etched again to get traces on top and bottom layers.\n\nThe inner layers are given a complete machine inspection before lamination because afterwards mistakes cannot be corrected. The automatic optical inspection system compares an image of the board with the digital image generated from the original design data.\n\nHoles through a PCB are typically drilled with drill bits made of solid coated tungsten carbide. Coated tungsten carbide is used because board materials are abrasive. High-speed-steel bits would dull quickly, tearing the copper and ruining the board. Drilling is done by computer-controlled drilling machines, using a \"drill file\" or Excellon file that describes the location and size of each drilled hole.\n\nHoles may be made conductive, by electroplating or inserting hollow metal eyelets, to connect board layers. Some conductive holes are intended for the insertion of through-hole-component leads. Others used to connect board layers, are called vias.\n\nWhen very small vias are required, drilling with mechanical bits is costly because of high rates of wear and breakage. In this case, the vias may be laser drilled—evaporated by lasers. Laser-drilled vias typically have an inferior surface finish inside the hole. These holes are called \"micro vias\". It is also possible with \"controlled-depth\" drilling, laser drilling, or by pre-drilling the individual sheets of the PCB before lamination, to produce holes that connect only some of the copper layers, rather than passing through the entire board. These holes are called \"blind vias\" when they connect an internal copper layer to an outer layer, or \"buried vias\" when they connect two or more internal copper layers and no outer layers.\n\nThe hole walls for boards with two or more layers can be made conductive and then electroplated with copper to form \"plated-through holes\". These holes electrically connect the conducting layers of the PCB. For multi-layer boards, those with three layers or more, drilling typically produces a \"smear\" of the high temperature decomposition products of bonding agent in the laminate system. Before the holes can be plated through, this smear must be removed by a chemical \"de-smear\" process, or by \"plasma-etch\". The de-smear process ensures that a good connection is made to the copper layers when the hole is plated through. On high reliability boards a process called etch-back is performed chemically with a potassium permanganate based etchant or plasma. The etch-back removes resin and the glass fibers so that the copper layers extend into the hole and as the hole is plated become integral with the deposited copper.\n\nProper plating or surface finish selection can be critical to process yield, the amount of rework, field failure rate, and reliability.\n\nPCBs are plated with solder, tin, or gold over nickel and a resist for etching away the unneeded underlying copper.\n\nAfter PCBs are etched and then rinsed with water, the solder mask is applied, and then any exposed copper is coated with solder, nickel/gold, or some other anti-corrosion coating.\n\nMatte solder is usually fused to provide a better bonding surface for bare copper. Treatments, such as benzimidazolethiol, prevent surface oxidation of bare copper. The places to which components will be mounted are typically plated, because untreated bare copper oxidizes quickly, and therefore is not readily solderable. Traditionally, any exposed copper was coated with solder by hot air solder levelling (HASL). The HASL finish prevents oxidation from the underlying copper, thereby guaranteeing a solderable surface. This solder was a tin-lead alloy, however new solder compounds are now used to achieve compliance with the RoHS directive in the EU, which restricts the use of lead. One of these lead-free compounds is SN100CL, made up of 99.3% tin, 0.7% copper, 0.05% nickel, and a nominal of 60 ppm germanium.\n\nIt is important to use solder compatible with both the PCB and the parts used. An example is ball grid array (BGA) using tin-lead solder balls for connections losing their balls on bare copper traces or using lead-free solder paste.\n\nOther platings used are OSP (organic surface protectant), immersion silver (IAg), immersion tin, electroless nickel with immersion gold coating (ENIG), electroless nickel electroless palladium immersion gold (ENEPIG) and direct gold plating (over nickel). Edge connectors, placed along one edge of some boards, are often nickel-plated then gold-plated. Another coating consideration is rapid diffusion of coating metal into tin solder. Tin forms intermetallics such as CuSn and AgCu that dissolve into the Tin liquidus or solidus(@50C), stripping surface coating or leaving voids.\n\n\"Electrochemical migration\" (ECM) is the growth of conductive metal filaments on or in a printed circuit board (PCB) under the influence of a DC voltage bias. Silver, zinc, and aluminum are known to grow whiskers under the influence of an electric field. Silver also grows conducting surface paths in the presence of halide and other ions, making it a poor choice for electronics use. Tin will grow \"whiskers\" due to tension in the plated surface. Tin-lead or solder plating also grows whiskers, only reduced by reducing the percentage of tin. Reflow to melt solder or tin plate to relieve surface stress lowers whisker incidence. Another coating issue is tin pest, the transformation of tin to a powdery allotrope at low temperature.\n\nAreas that should not be soldered may be covered with solder resist (solder mask). One of the most common solder resists used today is called \"LPI\" (liquid photoimageable solder mask).  A photo-sensitive coating is applied to the surface of the PWB, then exposed to light through the solder mask image film, and finally developed where the unexposed areas are washed away. Dry film solder mask is similar to the dry film used to image the PWB for plating or etching. After being laminated to the PWB surface it is imaged and developed as LPI. Once but no longer commonly used, because of its low accuracy and resolution, is to screen print epoxy ink. In addition to repelling solder, solder resist also provides protection from the environment to the copper that would otherwise be exposed.\n\nA legend is often printed on one or both sides of the PCB. It contains the component designators, switch settings, test points and other indications helpful in assembling, testing, servicing, and sometimes using the circuit board.\n\nThere are three methods to print the legend.\n\nBoards with no components installed are usually \"bare-board tested\" for \"shorts\" and \"opens\". A short is a connection between two points that should not be connected. An open is a missing connection between points that should be connected. For high-volume production, a fixture or a rigid needle adapter makes contact with copper lands on the board. The fixture or adapter is a significant fixed cost and this method is only economical for high-volume or high-value production. For small or medium volume production \"flying probe\" testers are used where test probes are moved over the board by an XY drive to make contact with the copper lands. There is no need for a fixture and hence the fixed costs are much lower. The CAM system \"instructs\" the electrical tester to apply a voltage to each contact point as required and to check that this voltage appears on the appropriate contact points and only on these.\n\nIn assembly the bare board is populated (or \"stuffed\") with electronic components to form a functional \"printed circuit assembly\" (PCA), sometimes called a \"printed circuit board assembly\" (PCBA). In through-hole technology, the component leads are inserted in holes surrounded by conductive \"pads\"; the holes keep the components in place. In surface-mount technology (SMT), the component is placed on the PCB so that the pins line up with the conductive \"pads\" or \"lands\" on the surfaces of the PCB; solder paste, which was previously applied to the pads, holds the components in place temporarily; if surface-mount components are applied to both sides of the board, the bottom-side components are glued to the board. In both through hole and surface mount, the components are then soldered; once cooled and solidified, the solder holds the components in place permanently and electrically connects them to the board.\n\nThere are a variety of soldering techniques used to attach components to a PCB. High volume production is usually done with a \"Pick and place machine\" or SMT placement machine and bulk wave soldering or reflow ovens, but skilled technicians are able to hand-solder very tiny parts (for instance 0201 packages which are 0.02 in. by 0.01 in.) under a microscope, using tweezers and a fine-tip soldering iron, for small volume prototypes. Some SMT parts cannot be soldered by hand, such as BGA packages. All through-hole components can be hand soldered, making them favored for prototyping where size, weight, and the use of the exact components that would be used in high volume production are not concerns.\n\nOften, through-hole and surface-mount construction must be combined in a single assembly because some required components are available only in surface-mount packages, while others are available only in through-hole packages. Or, even if all components are available in through-hole packages, it might be desired to take advantage of the size, weight, and cost reductions obtainable by using some available surface-mount devices. Another reason to use both methods is that through-hole mounting can provide needed strength for components likely to endure physical stress (such as connectors that are frequently mated and demated or that connect to cables expected to impart substantial stress to the PCB-and-connector interface), while components that are expected to go untouched will take up less space using surface-mount techniques. \"For further comparison, see the SMT page.\"\n\nAfter the board has been populated it may be tested in a variety of ways:\nTo facilitate these tests, PCBs may be designed with extra pads to make temporary connections. Sometimes these pads must be isolated with resistors. The in-circuit test may also exercise boundary scan test features of some components. In-circuit test systems may also be used to program nonvolatile memory components on the board.\n\nIn boundary scan testing, test circuits integrated into various ICs on the board form temporary connections between the PCB traces to test that the ICs are mounted correctly. Boundary scan testing requires that all the ICs to be tested use a standard test configuration procedure, the most common one being the Joint Test Action Group (JTAG) standard. The JTAG test architecture provides a means to test interconnects between integrated circuits on a board without using physical test probes, by using circuitry in the ICs to employ the IC pins themselves as test probes. JTAG tool vendors provide various types of stimuli and sophisticated algorithms, not only to detect the failing nets, but also to isolate the faults to specific nets, devices, and pins.\n\nWhen boards fail the test, technicians may desolder and replace failed components, a task known as \"rework\".\n\nPCBs intended for extreme environments often have a conformal coating, which is applied by dipping or spraying after the components have been soldered. The coat prevents corrosion and leakage currents or shorting due to condensation. The earliest conformal coats were wax; modern conformal coats are usually dips of dilute solutions of silicone rubber, polyurethane, acrylic, or epoxy. Another technique for applying a conformal coating is for plastic to be sputtered onto the PCB in a vacuum chamber. The chief disadvantage of conformal coatings is that servicing of the board is rendered extremely difficult.\n\nMany assembled PCBs are static sensitive, and therefore they must be placed in antistatic bags during transport. When handling these boards, the user must be grounded (earthed). Improper handling techniques might transmit an accumulated static charge through the board, damaging or destroying components. The damage might not immediately affect function but might lead to early failure later on, cause intermittent operating faults, or cause a narrowing of the range of environmental and electrical conditions under which the board functions properly. Even bare boards are sometimes static sensitive: traces have become so fine that it's quite possible to blow an etch off the board (or change its characteristics) with a static charge. This is especially true on non-traditional PCBs such as MCMs and microwave PCBs.\n\nCordwood construction can save significant space and was often used with wire-ended components in applications where space was at a premium (such as fuzes, missile guidance, and telemetry systems) and in high-speed computers, where short traces were important. In cordwood construction, axial-leaded components were mounted between two parallel planes. The components were either soldered together with jumper wire, or they were connected to other components by thin nickel ribbon welded at right angles onto the component leads. To avoid shorting together different interconnection layers, thin insulating cards were placed between them. Perforations or holes in the cards allowed component leads to project through to the next interconnection layer. One disadvantage of this system was that special nickel-leaded components had to be used to allow the interconnecting welds to be made. Differential thermal expansion of the component could put pressure on the leads of the components and the PCB traces and cause mechanical damage (as was seen in several modules on the Apollo program). Additionally, components located in the interior are difficult to replace. Some versions of cordwood construction used soldered single-sided PCBs as the interconnection method (as pictured), allowing the use of normal-leaded components.\n\nBefore the advent of integrated circuits, this method allowed the highest possible component packing density; because of this, it was used by a number of computer vendors including Control Data Corporation. The cordwood method of construction was used only rarely once semiconductor electronics and PCBs became widespread.\n\nMultiwire is a patented technique of interconnection which uses machine-routed insulated wires embedded in a non-conducting matrix (often plastic resin). It was used during the 1980s and 1990s. (Kollmorgen Technologies Corp, filed 1978) As of 2010, Multiwire was still available through Hitachi.\n\nSince it was quite easy to stack interconnections (wires) inside the embedding matrix, the approach allowed designers to forget completely about the routing of wires (usually a time-consuming operation of PCB design): Anywhere the designer needs a connection, the machine will draw a wire in a straight line from one location/pin to another. This led to very short design times (no complex algorithms to use even for high density designs) as well as reduced crosstalk (which is worse when wires run parallel to each other—which almost never happens in Multiwire), though the cost is too high to compete with cheaper PCB technologies when large quantities are needed.\n\nCorrections can be made to a Multiwire board more easily than to a PCB.\n\nThere are other competitive discrete wiring technologies that have been developed.\n\n\nPCB materials\n\nPCB layout software\n\n"}
{"id": "1810990", "url": "https://en.wikipedia.org/wiki?curid=1810990", "title": "Project Sherwood", "text": "Project Sherwood\n\nProject Sherwood was the codename for a United States program in controlled nuclear fusion during the period it was classified. The prospect of turning cheap and inexhaustible hydrogen into inexpensive power captured the imagination of Lewis Strauss, the Atomic Energy Commission (AEC) chairman from 1953 to 1958, who believed fusion reactors could generate electricity \"too cheap to meter\".\n\nSherwood developed out of a number of \"ad hoc\" efforts dating back to about 1951. Primary among these was the stellarator program at Princeton University, itself code-named Project Matterhorn. Since then the weapons labs had clamoured to join the club, Los Alamos with its z-pinch efforts, Livermore's magnetic mirror program, and later, Oak Ridge's fuel injector efforts. By 1953 the combined budgets were increasing into the million dollar range, demanding some sort of oversight at the AEC level.\n\nThe name \"Sherwood\" was suggested by Paul McDaniel, Deputy Director of the AEC. He noted that funding for the wartime Hood Building was being dropped and moved to the new program, so they \"robbing Hood to pay Friar Tuck\", a reference to the British physicist and fusion researcher James L. Tuck. The connection to Robin Hood and Friar Tuck gave the project its name.\n\nStrauss strongly supported keeping the program secret until pressure from the United Kingdom led to a declassification effort at the 2nd Atoms for Peace meeting in the fall of 1958. After this time a number of purely civilian organizations also formed to organize meetings on the topic, with the American Physical Society organizing meetings under their Division of Plasma Physics. These meetings have been carried on to this day and were renamed International Sherwood Fusion Theory Conference. The original Project Sherwood became simply the Controlled Thermonuclear Research program within the AEC and it's follow-on organizations.\n\nResearch centered on three plasma confinement designs; the stellarator headed by Lyman Spitzer at the Princeton Plasma Physics Laboratory, the toroidal pinch or Perhapsatron led by James Tuck at the Los Alamos National Laboratory and the magnetic mirror devices at the Livermore National Laboratory led by Richard F. Post. By June, 1954 a preliminary study had been completed for a full scale \"Model D\" stellarator that would be over long and produce 5,000 MW of electricity at a capital cost of $209 per kilowatt. However, each concept encountered unanticipated problems, in the form of plasma instabilities that prevented the requisite temperatures and pressures from being achieved, and it eventually became clear that sustained hydrogen fusion would not be developed quickly. Strauss left AEC in 1958 and his successor did not share Strauss' enthusiasm for fusion research. Consequently, Project Sherwood was relegated from a crash program to one that concentrated on basic research.\n\nThe funding for Project Sherwood began with the closure of another program named Project Lincoln at the Hood Laboratory. As the number of people working on the projects grew, so did the budget. Under Strauss the program was reorganized, and its funding and staffing increased dramatically. From early 1954 to 1955, the number of people working on Project Sherwood grew from 45 to 110. By the next year, that number had doubled. The original budget from the shut down of Project Lincoln was $1 million. The breakdown of the year budget from 1951 to 1957 can be seen in the table below. At its peak, Project Sherwood had a budget of $23 million per year and retained more than 500 scientists.\nThe declassification of the program was a large topic of discussion between scientists at all of the laboratories involved with the project and at the Sherwood conferences. The reasoning for an initial high classification status was that if the research into controlled fusion were to be successful then it would be a significant advantage in regards to military aspects. However, it came to the point that the research needed to be industrialized. Some of this work could be conducted without access to the classified information; however, there were some instances where the classified information of the program was a necessity for those people working on projects such as the large-scale stellarator, the ultra-high vacua, and the problem of energy storage. In these instances, there was a contract with the Commission that the information that was being used would only be shared with the personnel that was directly working on the project. It soon became apparent that industrial companies were expected to become highly invested in the area of fission and because of this it began clear that these companies should have full access to the research information obtained by Project Sherwood. In June 1956, permits for the research information from Project Sherwood became available through the Commission for companies that were qualified. However, between 1955 and 1958, information became more and more available to the public with its gradual declassification beginning with the sharing of information with the United Kingdom. Huge supporters of declassification of the program included the director of the Division of Research, Thomas Johnson, and a member of his staff, Amasa Bishop. Some of their reasoning for wanting declassification was that the secrecy of the project could negatively impact their ability to enlist and employ experienced personnel to the program. The also argued that it would change the way their conferences could be held. The scientists working on the project would be able to freely discuss their findings with others in the scientific community rather than only the scientists working on the same project. By May 1958, basic information about the various projects within Project Sherwood including the stellarator, magnetic mirrors, and molecular ion beams had been released to the public.\n\nIn the early 1950s, Oak Ridge National Laboratory was composed of a small group of scientists that were mostly experienced with research in ion-source technology. However, research from Project Sherwood was a growing area of interest, and the researchers at Oak Ridge National Laboratory wanted to participate in the discovery of controlled fusion. They studied areas of controlled fusion such as the rate of plasma diffusion in a magnetic field and the charge-exchange process. However, their work with ion-source was still a large part of their research.\n\nAlthough there was already a main project (magnetic mirror) at the University of California, a scientist named W. R. Baker began his research into the pinch effect at UCRL, Berkeley in 1952. Two years later, S. Colgate began research on shock-heating at UCRL, Livermore.\n\nThere was another small group of scientists at Tufts College in Medford, Massachusetts that had become involved in the research of the pinch effect. Although their work was not an official part of the Atomic Energy Commission, some of their personnel did attended the Sherwood conferences.\n\nIn 1954, there was a program started at New York University called the Division of Research. It was a small program that included personnel from the Institute of Mathematical Sciences at New York University.\n\n\n"}
{"id": "954329", "url": "https://en.wikipedia.org/wiki?curid=954329", "title": "Schuler tuning", "text": "Schuler tuning\n\nSchuler tuning is a design principle for inertial navigation systems that accounts for the curvature of the Earth. An inertial navigation system, used in submarines, ships, aircraft, and other vehicles to keep track of position, determines directions with respect to three axes pointing \"north\", \"east\", and \"down\". To detect the vehicle's orientation, the system contains an \"inertial platform\" mounted on gimbals, with gyroscopes that detect motion connected to a servo system to keep it pointing in a fixed orientation in space. However, the directions \"north\", \"east\" and \"down\" change as the vehicle moves on the curved surface of the Earth. Schuler tuning describes the conditions necessary for an inertial navigation system to keep the inertial platform always pointing \"north\", \"east\" and \"down\", so it gives correct directions on the near-spherical Earth. It is widely used in electronic control systems.\n\nAs first explained by German engineer Maximilian Schuler in a 1923 paper, a pendulum whose period exactly equals the orbital period of a hypothetical satellite orbiting just above the surface of the Earth (about 84 minutes) will tend to remain pointing at the center of the Earth when its support is suddenly displaced. Such a pendulum would have a length equal to the radius of the Earth. Consider a simple gravity pendulum, whose length equals the radius of the Earth, suspended in a uniform gravitational field of the same strength as that experienced at the Earth's surface. If suspended from the surface of the Earth, the bob of the pendulum would be at the center of the Earth. If it is hanging motionless and its support is moved sideways, the bob tends to remain motionless, so the pendulum always points at the center of the Earth. If such a pendulum were attached to the inertial platform of an inertial navigation system, the platform would remain level, facing \"north\", \"east\" and \"down\", as it was moved about on the surface of the Earth.\n\nA rigid pendulum may also be made to have the required period, with a pivot near its center of gravity.\n\nThe Schuler period can be derived from the classic formula for the period of a pendulum:\nwhere L is the radius of the earth in meters and g is the local acceleration of gravity in metres per second per second.\n\nA pendulum the length of the Earth's radius is impractical, so Schuler tuning doesn't use physical pendulums. Instead, the electronic control system of the inertial navigation system is modified to make the platform behave as if it were attached to a pendulum. The inertial platform is mounted on gimbals, and an electronic control system keeps it pointed in a constant direction with respect to the three axes. As the vehicle moves, the gyroscopes detect changes in orientation, and a feedback loop applies signals to torquers to rotate the platform on its gimbals to keep it pointed along the axes. \n\nTo implement Schuler tuning, the feedback loop is modified to tilt the platform as the vehicle moves in the north-south and east-west directions, to keep the platform facing \"down\". To do this, the torquers that rotate the platform are fed a signal proportional to the vehicle's north-south and east-west velocity. The turning rate of the torquers is equal to the velocity divided by the radius of the Earth \"R\":\n\nSo:\n\nThe acceleration \"a\" is a combination of the actual vehicle acceleration and the acceleration due to gravity acting on the tilting inertial platform. It can be measured by an accelerometer mounted fixed on the platform, in either the north-south or east west direction, horizontally. So this equation can be seen as a version of the equation for a simple gravity pendulum with a length equal to the radius of the Earth. The inertial platform acts as if it were attached to such a pendulum.\n\nSchuler's time constant has other applications. Suppose a tunnel is dug from one end of the Earth to the other end straight through its center. A stone dropped in such a tunnel oscillates harmonically with Schuler's time constant. It can also be proved that the time is the same constant for a tunnel that is not through the center of Earth. Such a tunnel has to be an Earth-centered ellipse, the same shape as the path of the stone. These thought experiments (or rather the results of the corresponding calculations) rely on an assumption of uniform density throughout the Earth. Since the density is not actually uniform, the \"true\" periods would deviate from Schuler's time constant.\n"}
{"id": "4459459", "url": "https://en.wikipedia.org/wiki?curid=4459459", "title": "Soap dispenser", "text": "Soap dispenser\n\nA soap dispenser is a device that, when manipulated or triggered appropriately, dispenses soap (usually in small, single-use quantities). It can be manually operated by means of a handle, or can be automatic. Soap dispensers are often found in public toilets.\n\nThe design of a manual soap dispenser is generally determined by whether the soap comes in liquid, powder, or foam form.\n\nWhen soap is dispensed in liquid form, it is generally in a squeeze bottle or pump. The most popular soap dispensers of this type are plastic pump bottles, many of which are disposable.\n\nWilliam Shepphard patented liquid soap on August 22, 1865. Minnetonka Corporation introduced the first modern liquid soap and cornered the market by buying up the entire supply of the plastic pumps needed for the liquid soap dispensers.\n\nDispensers of powder soaps, such as borax, often take the form of a metal box with a weighted lever; when the lever is pressed, a handful of soap is released.\n\nManual dispensers of foam soap often consist of a large button which squeezes the foam out of a tube. Many liquid soap dispensers operate in this way as well. A few dispensers operate with a lever that pulls forward and squeezes the soap out.\n\nThe majority of manual foam soap dispensers have the soap in a bladder in the dispenser in liquid form, as the pump is pressed the liquid soap is pushed through a small foaming nozzle which foams the soap.\n\nAn automatic soap dispenser is specifically a hands-free dispenser of soap (both liquid soap and foaming soap), but generally can be used for other liquids such as hand sanitizers, shampoos, or hand lotions. Automatic dispensers are often battery-powered. The touch-free design dispenses the liquid when a sensor detects motion under the nozzle. The electronic components of an automatic soap dispenser allow for a timing device or signal (sound, lights, etc.) which can indicate to the user whether they have washed their hands for the correct amount of time or not.\n\nHands-free dispensers for water and soap/hand sanitizer have particular virtues for operating theatres and treatment rooms.\n\nFoam soap dispensers have dual pumps that when used move both air and soap, injecting both together through small openings to create a lather. They can be found in both manual and automatic varieties.\n\n"}
{"id": "31571635", "url": "https://en.wikipedia.org/wiki?curid=31571635", "title": "Sondor", "text": "Sondor\n\nSondor is a manufacturer of Audio Video equipment located in Zollikon, Switzerland until 2017. \nSondor was founded in 1952 by Willy Hungerbuehler. Sondor started as a manufacturer of 16 mm film and 35 mm film magnetic film equipment. They are noted as inventing the standard for bi-phase interlocking pulse signals to sync sound to film. Sondor added a film transport telecine to it line of film sound equipment. Sondor products are found in many in post-production studios for record and playback and in movie theater for sound playback. playback.\n\nSondor film transport telecines uses a spinning prism telecine, like the model NOVA and ALTRA. \n\nSome Sound Film followers player-recorder are the: OMA E and BASIC.\nSOUNDHOUSE is a product to add sound pick up to other telecines, like the Spirit DataCine.\n\nThe other major maker of sound followers is Magna Tech.\nDAT recorders and Direct to disk recording have replaced much of the work done on separate film sound followers.\n\nOn December 9, 2016 Digital Film Technology (dft), completed the acquisition of Sondor. DFT is the maker of the Scanity film scanner. \n\nCurrent Sondor products:\n\n\n"}
{"id": "45399146", "url": "https://en.wikipedia.org/wiki?curid=45399146", "title": "Spin disk", "text": "Spin disk\n\nA spin disk is an socket wrench accessory used to quickly turn nuts after they have been loosened with the wrench.\n\nSpin disks are usually a flat round disk, commonly of plastic, with a square hole in the middle to fit over the socket wrench's male adapter plug. The edges are grooved for a solid grip.\n\nUsing the spin disk will be much quicker than using the socket wrench's handle, especially if its range of motion is limited. If the nut is very loose, just using ones fingers may be the best option.\n\nSince this accessory can easily be lost or fall off during use, many manufacturers forgo the spinner accessory in favor of grooving placed directly on the sockets themselves to aid finger operation.\n"}
{"id": "1170995", "url": "https://en.wikipedia.org/wiki?curid=1170995", "title": "Steam car", "text": "Steam car\n\nA steam car is a car (automobile) powered by a steam engine. A steam engine is an external combustion engine (ECE) where the fuel is combusted away from the engine, as opposed to an internal combustion engine (ICE) where the fuel is combusted within the engine. ECEs have a lower thermal efficiency, but it is easier to regulate carbon monoxide production.\n\nThe first steam powered vehicle was supposedly built in 1672 by Ferdinand Verbiest, a Flemish Jesuit in China. The vehicle was a toy for the Chinese Emperor. While not intended to carry passengers, and therefore not exactly a \"car\", Verbiest's device is likely to be the first ever engine-powered vehicle. The first real experimental steam powered cars were built in the late 18th and 19th centuries, but it was not until after Richard Trevithick had developed the use of high-pressure steam, around 1800, that mobile steam engines became a practical proposition. By the 1850s it was viable to produce them commercially: steam road vehicles were used for many applications.\n\nDevelopment was hampered by adverse legislation from the 1830s and then the rapid development of internal combustion engine technology in the 1900s, leading to their commercial demise. Relatively few steam powered vehicles remained in use after the Second World War. Many of these vehicles were acquired by enthusiasts for preservation.\n\nThe search for renewable energy sources has led to an occasional resurgence of interest in using steam power for road vehicles.\n\nA steam engine is an external combustion engine (ECE: the fuel is combusted away from the engine), as opposed to an internal combustion engine (ICE: the fuel is combusted within the engine). While gasoline-powered ICE cars have an operational thermal efficiency of 15% to 30%, early automotive steam units were capable of only about half this efficiency. A significant benefit of the ECE is that the fuel burner can be configured for very low emissions of carbon monoxide, nitrogen oxides and unburned carbon in the exhaust, thus avoiding pollution.\n\nThe greatest technical challenges to the steam car have focused on its boiler. This represents much of the total mass of the vehicle, making the car heavy (an internal combustion-engined car requires no boiler), and requires careful attention from the driver, although even the cars of 1900 had considerable automation to manage this. The single largest restriction is the need to supply feedwater to the boiler. This must either be carried and frequently replenished, or the car must also be fitted with a condenser, a further weight and inconvenience.\n\nSteam-powered and electric cars outsold gasoline-powered ones in many US states prior to the invention of the electric starter, since internal combustion cars relied on a hand crank to start the engine, which was difficult and occasionally dangerous to use, as improper cranking could cause a backfire capable of breaking the arm of the operator. Electric cars were popular to some extent, but had a short range, and could not be charged on the road if the batteries ran low.\n\nEarly steam cars, once working pressure was attained, could be instantly driven off with high acceleration; but they typically take several minutes to start from cold, plus time to get the burner to operating temperature. To overcome this, development has been directed toward flash boilers, which heat a much smaller quantity of water to get the vehicle started, and, in the case of Doble cars, spark ignition diesel burners.\n\nThe steam car does have advantages over internal combustion-powered cars, although most of these are now less important than in the early 20th century. The engine (excluding the boiler) is smaller and lighter than an internal combustion engine. It is also better suited to the speed and torque characteristics of the axle, thus avoiding the need for the heavy and complex transmission required for an internal combustion engine. The car is also quieter, even without a silencer.\n\nA French inventor, Nicolas-Joseph Cugnot, built the first working self-propelled land-based mechanical vehicle. There is an unsubstantiated story that a pair of Yorkshiremen, engineer Robert Fourness and his cousin, physician James Ashworth had a steam carriage running in 1788, after being granted a British Patent, No.1674 of December 1788. An illustration of it even appeared in Hergé's book \"Tintin raconte l'Histoire de l'Automobile\" (Casterman, 1953). The first substantiated steam carriage for personal use was that of Josef Božek in 1815. He was followed by Thomas Blanchard of Massachusetts in 1825. Over thirty years passed before there was a flurry of steam cars from 1859 onwards with Dugeon, Roper and Spenser from the United States, Thomes Rickett, Austin, Catley and Ayres from England, and Innocenzo Manzetti from Italy being the earliest. Others followed with the first Canadian, Henry Taylor in 1867, Amédée Bollée and Louis Lejeune of France in 1878, and Rene Thury of Switzerland in 1879.\n\nThe 1880s saw the rise of the first larger scale manufacturers, particularly in France, the first being Bollée (1878) followed by De Dion-Bouton (1883), Whitney of East Boston (1885), Ransom E. Olds (1886), Serpollet (1887), and Peugeot (1889).\n\nThis early period also saw the first repossession of an automobile in 1867 and the first getaway car the same year - both by Francis Curtis of Newburyport, Massachusetts.\n\nThe 1890s were dominated by the formation of numerous car manufacturing companies. The internal combustion engine was in its infancy, whereas steam power was well established. Electric powered cars were becoming available but suffered from their inability to travel longer distances.\n\nThe majority of steam powered car manufacturers from this period were from the United States. The more notable of these were Clark from 1895 to 1909, Locomobile from 1899 to 1903 when it switched to gaosoline engines, and Stanley from 1897 to 1924. As well as England and France, other countries also made attempts to manufacture steam cars: Cederholm of Sweden (1892), Malevez of Belgium (1898-1905), Schöche of Germany (1895), and Herbert Thomson of Australia (1896-1901)\nOf all the new manufacturers from the 1890s, only four continued to make steam cars after 1910. They were Stanley (to 1924) and Waverley (to 1916) of the United States, Buard of France (to 1914), and Miesse of Belgium (to 1926).\n\nThere were a large number of new companies formed in the period from 1898 to 1905. Steam cars outnumbered other forms of propulsion among very early cars. In the U.S. in 1902, 485 of 909 new car registrations were steamers. From 1899, Mobile had ten branches and 58 dealers across the U.S. The center of U.S. steamer production was New England, where 38 of the 84 manufacturers were located. Examples include White (Cleveland), Eclipse (Easton, Massachusetts), Cotta (Lanark, Illinois), Crouch (New Brighton, Pennsylvania), Hood (Danvers, Massachusetts; lasted just one month), Kidder (New Haven, Connecticut), Century (Syracuse, New York), and Skene (Lewiston, Maine; the company built everything but the tires). By 1903, 43 of them were gone and by the end of 1910 of those companies that were started in the decade those left were White which lasted to 1911, Conrad which lasted to 1924, Turner-Miesse of England which lasted to 1913, Morriss to 1912, Doble to 1930, Rutherford to 1912, and Pearson-Cox to 1916. \n\nAssembly-line mass production by Henry Ford dramatically reduced the cost of owning a conventional automobile, was also a strong factor in the steam car's demise as the Model T was both cheap and reliable. Additionally, during the 'heyday' of steam cars, the internal combustion engine made steady gains in efficiency, matching and then surpassing the efficiency of a steam engine when the weight of a boiler is factored in.\n\nWith the introduction of the electric starter, the internal combustion engine became more popular than steam, but the internal combustion engine was not necessarily superior in performance, range, fuel economy and emissions. Some steam enthusiasts feel steam has not received its share of attention in the field of automobile efficiency.\n\nApart from Brooks of Canada, all the steam car manufacturers that commenced between 1916 and 1926 were in the United States. Endurance (1924-1925) were the last steam car manufacturer to commence operations. American/Derr continued retrofitting production cars of various makes with steam engines, and Doble was the last steam car manufacturer. They ceased business in 1930.\n\nFrom the 1940s onward, various steam cars were constructed, usually by enthusiasts. Among those mentioned were Charles Keen, Cal Williams' 1950 Ford Conversion, Forrest R Detrick's 1957 Detrick S-101 prototype, and Harry Peterson's Stanley powered Peterson. The Detrick was constructed by Detrick, William H Mehrling, and Lee Gaeke who designed the engine based on a Stanley.\n\nCharles Keen began constructing a steam car in 1940 with the intention of restarting steam car manufacturing. Keen's family had a long history of involvement with steam propulsion going back to his great-great-grandfather in the 1830s, who helped build early steam locomotives. His first car, a Plymouth Coupe, used a Stanley engine. In 1948 and 1949, Keen employed Abner Doble to create a more powerful steam engine, a v4. He used this in La Dawri Victress S4 bodied sports car. Both these cars are still in existence. Keen died in 1969 before completing a further car. His papers and patterns were destroyed at that time.\n\nIn the 1950s, the only manufacturer to investigate steam cars was Paxton. Abner Doble developed the Doble Ultimax engine for the Paxton Phoenix steam car, built by the Paxton Engineering Division of McCulloch Motors Corporation, Los Angeles. The engine's sustained maximum power was . A Ford Coupe was used as a test-bed for the engine. The project was eventually dropped in 1954.\n\nIn 1957, Williams Engine Company Incorporated of Ambler began offering steam engine conversions for existing production cars. When air pollution became a significant issue for California in the mid-1960s the state encouraged investigation into the use of steam-powered cars. The fuel crises of the early 1970s prompted further work. None of this resulted in renewed steam car manufacturing.\n\nSteam cars remain the domain of enthusiasts, occasional experimentation by manufacturers, and those wishing to establish steam-powered land speed records.\n\nIn 1967, California established the California Air Resources Board and began to implement legislation to dramatically reduce exhaust emissions. This prompted renewed interest in alternative fuels for motor vehicles and a resurgence of interest in steam-powered cars in the state.\n\nThe idea for having patrol cars fitted with steam engines stemmed from an informal meeting in March 1968 of members of the California Assembly Transportation Committee. In the discussion, Karsten Vieg, a lawyer attached to the Committee, suggested that six cars be fitted with steam engines for testing by California District Police Chiefs. A bill was passed by the legislature to fund the trial.\n\nIn 1969, the California Highway Patrol initiated the project under Inspector David S Luethje to investigate the feasibility of using steam engined cars. Initially General Motors had agreed to pay a selected vendor $20,000 toward the cost of developing an Rankine cycle engine, and up to $100,000 for outfitting six Oldsmobile Delmont 88s as operational patrol vehicles. This deal fell through because the Rankine engine manufacturers rejected the General Motors offer.\n\nThe plan was revised and two 1969 Dodge Polaras were to be retrofitted with steam engines for testing. One car was to be modified by Don Johnson of Thermodynamic Systems Inc. and the other by industrialist William P Lear's Lear Motors Incorporated. At the time, the California State Legislature was introducing strict pollution control regulations for automobiles and the Chair of the Assembly Transportation Committee, John Francis Foran, was supportive of the idea. The Committee also was proposing to test four steam-powered buses in the San Francisco Bay Area that year.\n\nInstead of a Polara, Thermodynamic Systems (later called General Steam Corp), was given a late-model Oldsmobile Delmont 88. Lear was given a Polara but it does not appear to have been built. Both firms were given 6 months to complete their projects with Lear's being due for completion on 1 August 1969. Neither car had been completed by the due date and in November 1969, Lear was reported as saying the car would be ready in 3 months. Lear's only known retrofit was a Chevrolet Monte Carlo unrelated to the project. As for the project, it seems to have never been completed, with Lear pulling out by December.\n\nIn 1969, the National Air Pollution Control Administration announced a competition for a contract to design a practical passenger-car steam engine. Five firms entered. They were the consortium of Planning Research Corporation and STP Corporation; Battelle Memorial Institute, Columbus, Ohio; Continental Motors Corporation, Detroit; Vought Aeronautical Division of Ling-Temco-Vought, Dallas; and Thermo Electron Corporation, Waltham, Massachusetts.\n\nGeneral Motors introduced two experimental steam-powered cars in 1969. One was the SE 124 based on a converted Chevrolet Chevelle and the other was designated SE 101 based on the Pontiac Grand Prix. The SE 124 had its standard gasoline engine replaced with a 50 hp power Besler steam engine V4, using the 1920 Doble patents; the SE 101 was fitted with a 160 hp steam engine developed by GM Engineering. Power was transferred via a Toric automatic gearbox. The results was disappointing. The steam engine was heavy and weighted 300 kg more than a standard V8 and gave about half the power.\n\nIn October 1969, the Massachusetts Institute of Technology and the California Institute of Technology put out a challenge for a race August 1970 from Cambridge, Massachusetts to Pasadena, California for any college that wanted to participate in. The race was open for electric, steam, turbine power, and internal combustion engines: liquid-fueled, gaseous-fueled engines, and hybrids. \nTwo steam-powered cars entered the race. University of California, San Diego's modified AMC Javelin and Worcester Polytechnic Institute's converted 1970 Chevrolet Chevelle called the \"tea kettle\". Both dropped out on the second day of the race.\n\nThe California Assembly passed legislation in 1972 to contract two companies to develop steam-powered cars. They were Aerojet Liquid Rocket Company of Sacramento and Steam Power Systems of San Diego. Aerojet installed a steam turbine into a Chevrolet Vega, while Steam Power Systems built the Dutcher, a car named after the company's founder, Cornelius Dutcher. Both cars were tested by 1974 but neither car went into production. The Dutcher is on display at the Petersen Automotive Museum in Los Angeles.\n\nBoth Johnson and Lear had contemplated constructing steam-powered cars for the Indy 500, Johnson first in the early 1960s when with Controlled Steam Dynamics and in 1968 with Thermodynamic Systems and Lear in 1969. A third steam racing car was contemplated by a consortium of Planning Research Corporation and Andy Granatelli of STP Corporation. Lear proceeded with the idea and constructed a car, but ran out of funds while trying to develop the engine. The car is thought to be at the National Automobile and Truck Museum of the United States in Auburn, Indiana. Johnson was also noted as working on a steam-powered helicopter.\n\nWilliam D Thompson, 69-year-old retired San Diego automotive engineer, also announced he planned to enter a steam-powered race car. Thompson was working on a $35,000 steam-powered luxury car and he intended to use the car's engine in the race car. He had claimed that he had almost 250 orders for his cars. By comparison, Rolls Royces cost about $17,000 at that time.\n\nWith Lear pulling out of attempting to make a steam car, Donald Healey decided to make a basic steam-car technology more in line with Stanley or Doble and aimed at enthusiasts. He planned to have the car in production by 1971.\n\nEdward Pritchard created a steam-powered 1963 model Ford Falcon in 1972. It was evaluated by the Australian Federal Government and was also taken to the United States for promotional purposes.\n\nAs a result of the 1973 oil crisis, SAAB started a project in 1974 codenamed ULF (short for utan luftföroreningar, Swedish for Without Air Pollution)) headed by Dr. Ove Platell which made a prototype steam-powered car. The engine used an electronically controlled 28-pound multi-parallel-circuit steam generator with 1-millimeter-bore tubing and 16 gallons per hour firing rate which was intended to produce of continuous power, and was about the same size as a standard car battery. Lengthy start-up times were avoided by using air compressed and stored when the car was running to power the car upon starting until adequate steam pressure was built up. The engine used a conical rotary valve made from pure boron nitride. To conserve water, a hermetically sealed water system was used.\n\nThe project was cancelled and the project engineer, Ove Platell, started a company, Ranotor, with his son Peter Platell to continue its development. Ranotor is developing a steam hybrid that uses the exhaust heat from an ordinary petrol engine to power a small steam engine, with the aim of reducing fuel consumption by 20%. In 2008, truck manufacturers Scania and Volvo were said to be interested in the project.\n\nIn 1974, the British designer Peter Pellandine produced the first Pelland Steamer for a contract with the South Australian Government. It had a fibreglass monocoque chassis (based on the internal combustion-engined Pelland Sports) and used a twin-cylinder double-acting compound engine. It has been preserved at the National Motor Museum at Birdwood, South Australia.\n\nIn 1977, the Pelland Mk II Steam Car was built, this time by Pelland Engineering in the UK. It had a three-cylinder double-acting engine in a 'broad-arrow' configuration, mounted in a tubular steel chassis with a Kevlar body, giving a gross weight of just . Uncomplicated and robust, the steam engine was claimed to give trouble-free, efficient performance. It had huge torque () at zero engine revs, and could accelerate from in under 8 seconds.\n\nPellandine made several attempts to break the land speed record for steam power, but was thwarted by technical issues. Pellandine moved back to Australia in the 1990s where he continued to develop the Steamer. The latest version is the Mark IV.\n\nFrom 1996, a R&D subsidiary of the Volkswagen group called Enginion AG was developing a system called ZEE (Zero Emissions Engine). It produced steam almost instantly without an open flame, and took 30 seconds to reach maximum power from a cold start. Their third prototype, EZEE03, was a three-cylinder unit meant to fit in a Škoda Fabia automobile. The EZEE03 was described as having a \"two-stroke\" (i.e. single-acting) engine of displacement, producing up to (). Exhaust emissions were said to be far below the SULEV standard. It had an oilless engine with ceramic cylinder linings using steam instead of oil as a lubricant. However, Enginion found that the market was not ready for steam cars, so they opted instead to develop the Steamcell power generator/heating system based on similar technology.\n\nIn 1892, painter Jöns Cederholm and his brother, André, a blacksmith, designed their first steam car, a two-seater, introducing a condenser in 1894. They planned to use it for transportation between their home in Ystad and their summer house outside town. Unfortunately the automobile was destroyed in Sweden's first automobile accident but the Cederholm brothers soon built a second, improved version of their steam car reusing many parts from the first one. The car is preserved in a museum in Skurup.\n\nWhat is considered by many to be the first marketable popular steam car appeared in 1899 from the Locomobile Company of America, located in Watertown, Massachusetts, and from 1900 in Bridgeport, Connecticut. Locomobile manufactured several thousand of its \"Runabout\" model in the period 1899-1903, designed around a motor design leased from the Stanley Steamer Company. The company ceased producing steam cars in 1903 and changed to limited-production, internal combustion powered luxury automobiles. In 1922, it was acquired by Durant Motors and discontinued with the failure of the parent company in 1929.\n\nPerhaps the best-known and best-selling steam car was the Stanley Steamer, produced from 1896 to 1924. Between 1899 and 1905, Stanley outsold all gasoline-powered cars, and was second only to the electric cars of the Columbia Automobile Company in the US. It used a compact fire-tube boiler to power a simple double-acting two-cylinder engine. Because of the phenomenal torque available at all engine speeds, the steam car's engine was typically geared directly to the rear axle, with no clutch or variable speed transmission required. Until 1914, Stanley steam cars vented their exhaust steam directly to the atmosphere, necessitating frequent refilling of the water tank; after 1914, all Stanleys were fitted with a condenser, which considerably reduced their water consumption.\n\nIn 1906, the Land Speed Record was broken by a Stanley steam car, piloted by Fred Marriott, which achieved at Ormond Beach, Florida. This annual week-long \"Speed Week\" was the forerunner of today's Daytona 500. This record was not exceeded by any car until 1910.\n\nAttempts were made to bring more advanced steam cars on the market, the most remarkable being the Doble Steam Car which shortened start-up time very noticeably by incorporating a highly efficient monotube steam generator to heat a much smaller quantity of water along with effective automation of burner and water feed control. By 1923, Doble's steam cars could be started from cold with the turn of a key and driven off in 40 seconds or less. When the boiler had achieved maximum working pressure, the burner would cut out until pressure had fallen to a minimum level, whereupon it would re-ignite; by this means the car could achieve around 15 miles per gallon (18.8 litres/100 km) of kerosene despite its weight in excess of . Ultimately, despite their undoubted qualities, Doble cars failed due to poor company organisation and high initial cost.\n\nIn 1900, the American Bicycle Co. of Toledo, Ohio, created a 6.25 hp Toledo Steam Carriage (a description from the \"Horseless Age\", December 1900). The American Bicycle Co was one of the enterprises within Col. Albert Pope's large conglomerate of bicycle and motor vehicles manufacturers. The Toledo Steam Carriage was a very well-made, high-quality machine where every component, bar the tires, bell, instruments and lights were made within the dedicated 245,000 sq ft factory in Toledo, Ohio. The Toledo is considered to be one of the best steam cars produced at the time. The engine was particularly robust and the 2, 3\" diameter x 4\" stoke pistons employed piston style valves instead of 'D' valves thus insuring better balance and reduced leakage of steam. In September 1901 two Toledo steamers, one model B (a model A machine but with the foul-weather gear designating it as a model B) and one class E (public delivery vehicle), were entered by the American Bicycle Co. into the \"New York to Buffalo Endurance Contest\" of mid-September 1901. There were 36 cars in class B and three in class E; the class B Toledo won the Grosse Point race. On 4 January 1902, a specially built Toledo steam carriage was the first automobile to forge a trail from Flagstaff, Arizona to the South Rim of The Grand Canyon, a distance of 67 miles. As a publicity exercise the trip was to assess the potential of starting a steam bus service but the anticipated afternoon journey took three days due to problems with supplies of the wrong fuel. Though the Toledo towed a trailer filled with additional water and fuel supplies, the four participants omitted to take any food; one, the journalist Winfield Hoggaboon, wrote up an amusing article in the Los Angeles Herald two weeks later. In December 1901, the company changed from the American Bicycle Company to the newly formed International Motor Car Company to concentrate on steam- and gasoline-driven models, with electric vehicles being made by the separate Waverly Electric Co. Both steam and gasoline models were manufactured, but, as the public favoured the gasoline models and steam carriage sales were slow, steam carriage production ceased in July 1902 and gasoline-driven models were then made under the name Pope-Toledo. Total production of the steamers was between 285 and 325 units, as confirmed by a letter from the International Motor Car Co bookkeeper to the firms' accountant in June 1902.\n\nThe White Steamer was manufactured in Cleveland, Ohio, from 1900 until 1910 by the White Motor Company.\n\nThe Endurance Steam Car was a steam car manufactured in the United States from 1922 until 1924. The company had its origins in the Coats Steam Car and began production on the East Coast before shifting operations to Los Angeles in 1924. There, one single touring car was made using a 1923 Elcar 6-60 body before the factory moved again, this time to Dayton, Ohio, where one more car was built, a sedan, before the company folded.\n\nThe land speed record for Steam powered cars stood from 1906 when a Stanley steam car, driven by Fred Marriott, which achieved at Ormond Beach, Florida. Despite several attempts to break the record it stood until 25 August 2009 when Team Inspiration of the British Steam Car Challenge set a new speed record of . A second attempt by Don Wales on 26 August achieved an average speed of . The Stanley steam car held the land speed record for all vehicles for four years before it was broken by an internal combustion engine powered land vehicle. The 2009 record was set by a steam turbine powered vehicle vs the reciprocating piston-engine design used by F.E. and F.O. Stanley in 1906. \n\nErnest Kanzler, the owner of Autocoast approached Ross (Skip) Hedrick began work in the fall of 1968 with the idea of installing a steam engine in Hedrick's 1964 Indy car for an attempt at the 1906 steam car record. Hedrick and Richard J Smith joined Autocoast to design the engine. The car was readied for the 1969 Bonneville speed week, but was unable to run because an unexpected rain-storm made the salt flats too soggy.\n\nIn 1985, Barber-Nichols Engineering of Denver used a steam turbine they had designed for Lear and the Los Angeles city bus program to attempt to gain the steam-powered land speed record. The car was run at Speed Week on the Bonneville Salt Flats over a period of several years, eventually reaching a measured speed of 145.607 mph one pass. A fire prevented the return run, and the speed was not recognized by the FIA. FIA land speed records are based on an average of two runs (called 'passes') in opposite directions, taken within an hour of each other.\n\nOn 25 August 2009, Team Inspiration of the British Steam Car Challenge broke the long-standing record for a steam vehicle set by a Stanley Steamer in 1906, setting a new speed record of at the Edwards Air Force Base, in the Mojave Desert of California. This was the longest standing automotive record in the world. It had been held for over 100 years. The car was driven by Charles Burnett III and reached a maximum speed of on the first run and on the second.\n\nOn 26 August 2009, the same car, driven this time by Don Wales, the grandson of Sir Malcolm Campbell, broke a second record by achieving an average speed of over two consecutive runs over a measured kilometre. This was also recorded and has since been ratified by the FIA.\n\nOn 6 September 2014, Chuk Williams of Steam Speed America attempted to break the current world record in their steam-powered streamliner. The car reached 147 mph on its first run, but flipped and crashed when its braking chutes failed to open; Williams was injured in the accident, and the car severely damaged.\n\n"}
{"id": "34565204", "url": "https://en.wikipedia.org/wiki?curid=34565204", "title": "TEst Mobile System", "text": "TEst Mobile System\n\nTest Mobile System (TEMS) is a technology used by telecom operators to measure, analyze and optimize their mobile networks. It is considered as the basic tool to perform wireless network drive testing, benchmarking, monitoring and analysis. The TEMS Products business was divested to Ascom on June 2, 2009. The TEMS Products business has been divested to InfoVista, effective October 3, 2016.\n"}
{"id": "11992094", "url": "https://en.wikipedia.org/wiki?curid=11992094", "title": "Thames Valley Park", "text": "Thames Valley Park\n\nThames Valley Park is a high-tech business park adjacent to the River Thames on the eastern outskirts of Reading in the English county of Berkshire.\n\nThe park partially lies within the civil parish of Sonning. Close by are the Ali's Pond Local Nature Reserve, with paths and large ponds, and Sonning Hill.\n\nCompanies based at the park include BBC Radio Berkshire, Steria, SGI, Regus, BG Group, Websense, Oracle, Microsoft, OpenText and ING Direct.\n\nThe park is at the northern terminus of the A3290 (formerly part of the A329(M)) giving good connections to the M4 and A4. Originally the A329(M) was supposed to follow the river west from the park towards a junction with Vastern Road.\n\nThere is a free bus service between Thames the park and Reading railway station, with good connections for London and Heathrow Airport.\n\nA railway station to serve the park was proposed in the 1998 Reading Borough Local Plan, on the site of the coal sidings of the closed Earley power station. As at April 2013, the station had not been built.\nThe project was commenced by Speyhawk who went into receivership in 1993. Argent Group acquired the site and went on to complete the development in phases. It is built on well over a million cubic metres of pulverised fuel ash, from the power station, to bury the ash, gravels were extracted, and used for the construction of the A329(M) extension, this funded a large part of both the Reading Cross Town Route (A329(M) ) (Thames valley Park Drive) and TVP development. The works were under two separate contracts, completed by Fairclough Civil Engineering - Egham Office. There is a deep pumping station for the development in the centre of the TVP roundabout installed by Trant Contractors. Access is through a tunnel under the Paddington to Exeter railway line from Sutton Seeds Roundabout, dig using the driven shoe method, then precast tunnel sections were installed as linings.\n\nThe green park is not a natural feature, it was built at the end of the developments, ensuring the flood plain was reinstated and biodiversity improved.\n\nEvidence of the old power station is now restricted to a V shaped ditch, running into the Thames near the TVP roundabout to the north west, this was shortened to the point where the outfall of TVP stormwater drainage, and A329(m) drainage meet.\n\nAccess west into the Reading town, was designed into the roundabout adjacent to the railway line, its levels and layout reflect this, which in some way make its current configuration notable.\n\nThe road was one of the first designed using ground probing radar, which identified some soft ground features prior to commencement of works. The dual carriageway is constructed on a geotextile, because of the soft nature of the floodplain, and raised above it to avoid disruption during flooding, the roadways is then built upon a layer of chalk to sub-base level.\n\n\n"}
{"id": "1041702", "url": "https://en.wikipedia.org/wiki?curid=1041702", "title": "The Gutenberg Galaxy", "text": "The Gutenberg Galaxy\n\nThe Gutenberg Galaxy: The Making of Typographic Man is a 1962 book by Marshall McLuhan, in which the author analyzes the effects of mass media, especially the printing press, on European culture and human consciousness. It popularized the term \"global village\", which refers to the idea that mass communication allows a village-like mindset to apply to the entire world; and \"Gutenberg Galaxy\", which we may regard today to refer to the accumulated body of recorded works of human art and knowledge, especially books.\n\nMcLuhan studies the emergence of what he calls Gutenberg Man, the subject produced by the change of consciousness wrought by the advent of the printed book. Apropos of his axiom, \"The medium is the message,\" McLuhan argues that technologies are not simply inventions which people employ but are the means by which people are re-invented. The invention of movable type was the decisive moment in the change from a culture in which all the senses partook of a common interplay to a tyranny of the visual. He also argued that the development of the printing press led to the creation of nationalism, dualism, domination of rationalism, automatisation of scientific research, uniformation and standardisation of culture and alienation of individuals.\n\nMovable type, with its ability to reproduce texts accurately and swiftly, extended the drive toward homogeneity and repeatability already in evidence in the emergence of perspectival art and the exigencies of the single \"point of view\". He writes:\n\nThe book is unusual in its design. McLuhan described it as one which \"develops a mosaic or field approach to its problems\". The mosaic image to be constructed from data and quotations would then reveal \"causal operations in history\".\n\nThe book consists of five parts:\n\n\nThe main body of the book, part 2, \"The Gutenberg Galaxy\", consists of 107 short \"chapters\",\nmany of which are just three, two, or even one page(s) in length.\nSuch a large collection of small chapters does fit the picture of a mosaic.\n\nApparently, McLuhan also had some ideas about how to browse a book. \"Marshall McLuhan, the guru of The Gutenberg Galaxy\" (1962), recommends that the browser turn to page 69 of any book and read it. If you like that page, buy the book.\" Such apparent arbitrariness fits with picking a particular piece (or part) of a mosaic and deciding if you like it. Certainly the McLuhan test can be applied to the Gutenberg Galaxy itself. Doing so will reveal a further insight into the purpose of his own book.\n\nMcLuhan declares his book to be \"complementary to \"The Singer of Tales\" by Albert B. Lord.\" The latter work follows on from the Homeric studies of Milman Parry who turned to \"the study of the Yugoslave epics\" to prove that the poems of Homer were oral compositions.\n\nThe book may also be regarded as a way of describing four epochs of history:\n\nFor the break between the time periods in each case the occurrence of a new medium is responsible, the \"hand-writing\" terminates the oral phase, the printing and the electricity revolutionizes afterwards culture and society.\n\nGiven the clue of \"hand-writing\" that terminates the \"oral phase\" one expects \"printing\" to terminate the manuscript phase and the \"electrifying\" to bring an end to the Gutenberg era. The strangeness of the use of \"electrifying\" is entirely appropriate in the McLuhan context of 1962. The Internet did not exist then.\n\nMcLuhan himself suggests that the last section of his book might play the major role of being the first section:\n\nThe last section of the book, \"The Galaxy Reconfigured,\" deals with the clash of electric and mechanical, or print, technologies, and the reader may find it the best prologue.\nThe oral tradition is not dead. In schools or at home or in the street, where children are taught to learn by heart, to memorize, nursery rhymes or poems or songs, then they can be said to participate in the oral tradition. The same is often true of the children belonging to religious groups who are taught to learn to say their prayers. In other words, childhood is one of the ages of man (in Shakespeare's sense) and is essentially an oral tribal culture. The transition from this oral culture takes place when the child is taught to read and write. Then the child enters the world of the manuscript culture.\n\nMcLuhan identifies James Joyce's \"Finnegans Wake\" as a key that unlocks something of the nature of the oral culture.\"\n\nOf particular importance to the Oral Culture is the Art of memory.\n\nIn commenting on the then Soviet Union, McLuhan puts \"the advertising and PR community\" on a par with them in so far that both \"are concerned about access to the media and about results.\" More remarkably he asserts that \"Soviet concern with media \"results\" is natural to any oral society where interdependence is the result of instant interplay of cause and effect in the total structure. Such is the character of a village, or since electric media, such is also the character of global village.\"\n\nThe culture of the manuscript (literally hand-writing) is often referred to by McLuhan as scribal culture.\n\nMedieval illumination, gloss, and sculpture alike were aspects of the art of memory, central to scribal culture.\nAssociated with this epoch is the Art of memory (in Latin Ars Memoriae).\n\nFinnegans Wake:\nJoyce's \"Finnegans Wake\" (like Shakespeare's \"King Lear\") is one of the texts which McLuhan frequently uses throughout the book in order to weave together the various strands of his argument.\n\nThroughout \"Finnegans Wake\" Joyce specifies the Tower of Babel as the tower of Sleep,\nthat is, the tower of the witless assumption, or what Bacon calls the reign of the Idols.\nHis episodic and often rambling history takes the reader from pre-alphabetic tribal humankind to the electronic age. According to McLuhan, the invention of movable type greatly accelerated, intensified, and ultimately enabled cultural and cognitive changes that had already been taking place since the invention and implementation of the alphabet, by which McLuhan means phonemic orthography. (McLuhan is careful to distinguish the phonetic alphabet from logographic/logogramic writing systems, like hieroglyphics or ideograms.)\n\nPrint culture, ushered in by the Gutenberg press in the middle of the fifteenth century, brought about the cultural predominance of the visual over the aural/oral. Quoting with approval an observation on the nature of the printed word from \"Prints and Visual Communication\" by William Ivins, McLuhan remarks:\nIn this passage [Ivins] not only notes the ingraining of lineal, sequential habits, but, even more important, points out the visual homogenizing of experience of print culture, and the relegation of auditory and other sensuous complexity to the background. [...] The technology and social effects of typography incline us to abstain from noting interplay and, as it were, \"formal\" causality, both in our inner and external lives. Print exists by virtue of the static separation of functions and fosters a mentality that gradually resists any but a separative and compartmentalizing or specialist outlook.\n\nThe main concept of McLuhan's argument (later elaborated upon in \"The Medium is the Massage\") is that new technologies (like alphabets, printing presses, and even speech itself) exert a gravitational effect on cognition, which in turn affects social organization: print technology changes our perceptual habits (\"visual homogenizing of experience\"), which in turn affects social interactions (\"fosters a mentality that gradually resists all but a... specialist outlook\"). According to McLuhan, the advent of print technology contributed to and made possible most of the salient trends in the Modern period in the Western world: individualism, democracy, Protestantism, capitalism and nationalism. For McLuhan, these trends all reverberate with print technology's principle of \"segmentation of actions and functions and principle of visual quantification.\"\n\nIn the early 1960s, McLuhan wrote that the visual, individualistic print culture would soon be brought to an end by what he called \"electronic interdependence\": when electronic media would replace visual culture with aural/oral culture. In this new age, humankind will move from individualism and fragmentation to a collective identity, with a \"tribal base.\" McLuhan's coinage for this new social organization is the \"global village\".\n\nThe term is sometimes described as having negative connotations in \"The Gutenberg Galaxy\", but McLuhan himself was interested in exploring effects, not making value judgments:\nInstead of tending towards a vast Alexandrian library the world has become a computer, an electronic brain, exactly as an infantile piece of science fiction. And as our senses have gone outside us, Big Brother goes inside. So, unless aware of this dynamic, we shall at once move into a phase of panic terrors, exactly befitting a small world of tribal drums, total interdependence, and superimposed co-existence. [...] Terror is the normal state of any oral society, for in it everything affects everything all the time. [...] In our long striving to recover for the Western world a unity of sensibility and of thought and feeling we have no more been prepared to accept the tribal consequences of such unity than we were ready for the fragmentation of the human psyche by print culture.\n\nKey to McLuhan's argument is the idea that technology has no \"per se\" moral bent—it is a tool that profoundly shapes an individual's and, by extension, a society's self-conception and realization:\nIs it not obvious that there are always enough moral problems without also taking a moral stand on technological grounds? [...] Print is the extreme phase of alphabet culture that detribalizes or decollectivizes man in the first instance. Print raises the visual features of alphabet to highest intensity of definition. Thus print carries the individuating power of the phonetic alphabet much further than manuscript culture could ever do. Print is the technology of individualism. If men decided to modify this visual technology by an electric technology, individualism would also be modified. To raise a moral complaint about this is like cussing a buzz-saw for lopping off fingers. \"But\", someone says, \"we didn't know it would happen.\" Yet even witlessness is not a moral issue. It is a problem, but not a moral problem; and it would be nice to clear away some of the moral fogs that surround our technologies. It would be good for morality.\n\nThe moral valence of technology's effects on cognition is, for McLuhan, a matter of perspective. For instance, McLuhan contrasts the considerable alarm and revulsion that the growing quantity of books aroused in the latter seventeenth century with the modern concern for the \"end of the book.\" If there can be no universal moral sentence passed on technology, McLuhan believes that \"there can only be disaster arising from unawareness of the causalities and effects inherent in our technologies.\"\n\nThough the World Wide Web was invented thirty years after \"The Gutenberg Galaxy\" was published, McLuhan may have coined and certainly popularized the usage of the term \"surfing\" to refer to rapid, irregular and multidirectional movement through a heterogeneous body of documents or knowledge, e.g., statements like \"Heidegger surf-boards along on the electronic wave as triumphantly as Descartes rode the mechanical wave.\" Paul Levinson's 1999 book \"Digital McLuhan\" explores the ways that McLuhan's work can be better understood through the lens of the digital revolution. Later, Bill Stewart's 2007 \"Living Internet\" website describes how McLuhan's \"insights made the concept of a global village, interconnected by an electronic nervous system, part of our popular culture well before it actually happened.\"\n\nMcLuhan frequently quoted Walter Ong's \"Ramus, Method, and the Decay of Dialogue\" (1958), which evidently had prompted McLuhan to write \"The Gutenberg Galaxy\". Ong wrote a highly favorable review of this new book in \"America\". However, Ong later tempered his praise, by describing McLuhan's \"The Gutenberg Galaxy\" as \"a racy survey, indifferent to some scholarly detail, but uniquely valuable in suggesting the sweep and depth of the cultural and psychological changes entailed in the passage from illiteracy to print and beyond.\" McLuhan himself said of the book, \"I'm not concerned to get any kudos out of [\"The Gutenberg Galaxy\"]. It seems to me a book that somebody should have written a century ago. I wish somebody else had written it. It will be a useful prelude to the rewrite of \"Understanding Media\" [the 1960 NAEB report] that I'm doing now.\" \n\nMcLuhan's \"The Gutenberg Galaxy\" won Canada's highest literary award, the Governor-General's Award for Non-Fiction, in 1962. The chairman of the selection committee was McLuhan's colleague at the University of Toronto and oftentime intellectual sparring partner, Northrop Frye.\n\n\n\n"}
{"id": "30993", "url": "https://en.wikipedia.org/wiki?curid=30993", "title": "Thermometer", "text": "Thermometer\n\nA thermometer is a device that measures temperature or a temperature gradient. A thermometer has two important elements: (1) a temperature sensor (e.g. the bulb of a mercury-in-glass thermometer or the digital sensor in an infrared thermometer) in which some change occurs with a change in temperature, and (2) some means of converting this change into a numerical value (e.g. the visible scale that is marked on a mercury-in-glass thermometer or the digital readout on an infrared model). Thermometers are widely used in industries\n\nto monitor processes, in meteorology, in medicine, and in scientific research.\n\nSome of the principles of the thermometer were known to Greek philosophers of two thousand years ago. The modern thermometer gradually evolved from the thermoscope with the addition of a scale in the early 17th century and standardisation through the 17th and 18th centuries.\n\nWhile an individual thermometer is able to measure degrees of hotness, the readings on two thermometers cannot be compared unless they conform to an agreed scale. Today there is an absolute thermodynamic temperature scale. Internationally agreed temperature scales are designed to approximate this closely, based on fixed points and interpolating thermometers. The most recent official temperature scale is the International Temperature Scale of 1990. It extends from to approximately .\n\nVarious authors have credited the invention of the thermometer to Hero of Alexandria. The thermometer was not a single invention, however, but a development.\nHero of Alexandria (10–70 AD) knew of the principle that certain substances, notably air, expand and contract and described a demonstration in which a closed tube partially filled with air had its end in a container of water. The expansion and contraction of the air caused the position of the water/air interface to move along the tube.\n\nSuch a mechanism was later used to show the hotness and coldness of the air with a tube in which the water level is controlled by the expansion and contraction of the gas. These devices were developed by several European scientists in the 16th and 17th centuries, notably Galileo Galilei. As a result, devices were shown to produce this effect reliably, and the term \"thermoscope\" was adopted because it reflected the changes in sensible heat (the concept of temperature was yet to arise). The difference between a thermoscope and a thermometer is that the latter has a scale. Though Galileo is often said to be the inventor of the thermometer, what he produced were thermoscopes.\n\nThe first clear diagram of a thermoscope was published in 1617 by Giuseppe Biancani (1566 – 1624): the first showing a scale and thus constituting a thermometer was by Robert Fludd in 1638. This was a vertical tube, closed by a bulb of air at the top, with the lower end opening into a vessel of water. The water level in the tube is controlled by the expansion and contraction of the air, so it is what we would now call an air thermometer.\n\nThe first person to put a scale on a thermoscope is variously said to be Francesco Sagredo (1571–1620) or Santorio Santorio in about 1611 to 1613.\n\nThe word thermometer (in its French form) first appeared in 1624 in \"La Récréation Mathématique\" by J. Leurechon, who describes one with a scale of 8 degrees. The word comes from the Greek words θερμός, \"thermos\", meaning \"hot\" and μέτρον, \"metron\", meaning \"measure\".\n\nThe above instruments suffered from the disadvantage that they were also barometers, i.e. sensitive to air pressure. In 1629, Joseph Solomon Delmedigo, a student of Galileo, published what is apparently the first description and illustration of a sealed liquid-in-glass thermometer. It is described as having a bulb at the bottom of a sealed tube partially filled with brandy. The tube has a numbered scale. Delmedigo does not claim to have invented this instrument, nor does he name anyone else as its inventor. In about 1654 Ferdinando II de' Medici, Grand Duke of Tuscany (1610–1670), actually produced such an instrument, the first modern-style thermometer, dependent on the expansion of a liquid, and independent of air pressure. Many other scientists experimented with various liquids and designs of thermometer.\n\nHowever, each inventor and each thermometer was unique—there was no standard scale. In 1665 Christiaan Huygens (1629–1695) suggested using the melting and boiling points of water as standards, and in 1694 Carlo Renaldini (1615–1698) proposed using them as fixed points on a universal scale. In 1701, Isaac Newton (1642–1726/27) proposed a scale of 12 degrees between the melting point of ice and body temperature.\n\nIn 1714 Dutch scientist and inventor Daniel Gabriel Fahrenheit invented the first reliable thermometer, using mercury instead of alcohol and water mixtures. In 1724 he proposed a temperature scale which now (slightly adjusted) bears his name. He could do this because he manufactured thermometers, using mercury (which has a high coefficient of expansion) for the first time and the quality of his production could provide a finer scale and greater reproducibility, leading to its general adoption. In 1742, Anders Celsius (1701–1744) proposed a scale with zero at the boiling point and 100 degrees at the freezing point of water, though the scale which now bears his name has them the other way around. French entomologist René Antoine Ferchault de Réaumur invented an alcohol thermometer and temperature scale in 1730 that ultimately proved to be less reliable than Fahrenheit's mercury thermometer.\n\nThe first physician that put thermometer measurements to clinical practice was Herman Boerhaave (1668–1738). In 1866, Sir Thomas Clifford Allbutt (1836–1925) invented a clinical thermometer that produced a body temperature reading in five minutes as opposed to twenty. In 1999, Dr. Francesco Pompei of the Exergen Corporation introduced the world's first temporal artery thermometer, a non-invasive temperature sensor which scans the forehead in about two seconds and provides a medically accurate body temperature.\n\nOld thermometers were all non-registering thermometers. That is, the thermometer did not hold the temperature reading after it was moved to a place with a different temperature. Determining the temperature of a pot of hot liquid required the user to leave the thermometer in the hot liquid until after reading it. If the non-registering thermometer was removed from the hot liquid, then the temperature indicated on the thermometer would immediately begin changing to reflect the temperature of its new conditions (in this case, the air temperature). Registering thermometers are designed to hold the temperature indefinitely, so that the thermometer can be removed and read at a later time or in a more convenient place. Mechanical registering thermometers hold either the highest or lowest temperature recorded, until manually re-set, e.g., by shaking down a mercury-in-glass thermometer, or until an even more extreme temperature is experienced. Electronic registering thermometers may be designed to remember the highest or lowest temperature, or to remember whatever temperature was present at a specified point in time.\n\nThermometers increasingly use electronic means to provide a digital display or input to a computer.\n\nThermometers may be described as empirical or absolute. Absolute thermometers are calibrated numerically by the thermodynamic absolute temperature scale. Empirical thermometers are not in general necessarily in exact agreement with absolute thermometers as to their numerical scale readings, but to qualify as thermometers at all they must agree with absolute thermometers and with each other in the following way: given any two bodies isolated in their separate respective thermodynamic equilibrium states, all thermometers agree as to which of the two has the higher temperature, or that the two have equal temperatures. For any two empirical thermometers, this does not require that the relation between their numerical scale readings be linear, but it does require that relation to be strictly monotonic. This is a fundamental character of temperature and thermometers.\n\nAs it is customarily stated in textbooks, taken alone, the so-called \"zeroth law of thermodynamics\" fails to deliver this information, but the statement of the zeroth law of thermodynamics by James Serrin in 1977, though rather mathematically abstract, is more informative for thermometry: \"Zeroth Law – There exists a topological line formula_1 which serves as a coordinate manifold of material behaviour. The points formula_2 of the manifold formula_1 are called 'hotness levels', and formula_1 is called the 'universal hotness manifold'.\" To this information there needs to be added a sense of greater hotness; this sense can be had, independently of calorimetry, of thermodynamics, and of properties of particular materials, from Wien's displacement law of thermal radiation: the temperature of a bath of thermal radiation is proportional, by a universal constant, to the frequency of the maximum of its frequency spectrum; this frequency is always positive, but can have values that tend to zero. Another way of identifying hotter as opposed to colder conditions is supplied by Planck's principle, that when a process of isochoric adiabatic work is the sole means of change of internal energy of a closed system, the final state of the system is never colder than the initial state; except for phase changes with latent heat, it is hotter than the initial state.\n\nThere are several principles on which empirical thermometers are built, as listed in the section of this article entitled \"Primary and secondary thermometers\". Several such principles are essentially based on the constitutive relation between the state of a suitably selected particular material and its temperature. Only some materials are suitable for this purpose, and they may be considered as \"thermometric materials\". Radiometric thermometry, in contrast, can be only slightly dependent on the constitutive relations of materials. In a sense then, radiometric thermometry might be thought of as \"universal\". This is because it rests mainly on a universality character of thermodynamic equilibrium, that it has the universal property of producing blackbody radiation.\n\nThere are various kinds of empirical thermometer based on material properties.\n\nMany empirical thermometers rely on the constitutive relation between pressure, volume and temperature of their thermometric material. For example, mercury expands when heated.\n\nIf it is used for its relation between pressure and volume and temperature, a thermometric material must have three properties:\n\n(1) Its heating and cooling must be rapid. That is to say, when a quantity of heat enters or leaves a body of the material, the material must expand or contract to its final volume or reach its final pressure and must reach its final temperature with practically no delay; some of the heat that enters can be considered to change the volume of the body at constant temperature, and is called the latent heat of expansion at constant temperature; and the rest of it can be considered to change the temperature of the body at constant volume, and is called the specific heat at constant volume. Some materials do not have this property, and take some time to distribute the heat between temperature and volume change.\n\n(2) Its heating and cooling must be reversible. That is to say, the material must be able to be heated and cooled indefinitely often by the same increment and decrement of heat, and still return to its original pressure, volume and temperature every time. Some plastics do not have this property;\n\n(3) Its heating and cooling must be monotonic. That is to say, throughout the range of temperatures for which it is intended to work,\n\nAt temperatures around about 4 °C, water does not have the property (3), and is said to behave anomalously in this respect; thus water cannot be used as a material for this kind of thermometry for temperature ranges near 4 °C.\n\nGases, on the other hand, all have the properties (1), (2), and (3)(a)(α) and (3)(b)(α). Consequently, they are suitable thermometric materials, and that is why they were important in the development of thermometry.\n\nAccording to Preston (1894/1904), Regnault found constant pressure air thermometers unsatisfactory, because they needed troublesome corrections. He therefore built a constant volume air thermometer. Constant volume thermometers do not provide a way to avoid the problem of anomalous behaviour like that of water at approximately 4 °C.\n\nPlanck's law very accurately quantitatively describes the power spectral density of electromagnetic radiation, inside a rigid walled cavity in a body made of material that is completely opaque and poorly reflective, when it has reached thermodynamic equilibrium, as a function of absolute thermodynamic temperature alone. A small enough hole in the wall of the cavity emits near enough blackbody radiation of which the spectral radiance can be precisely measured. The walls of the cavity, provided they are completely opaque and poorly reflective, can be of any material indifferently. This provides a well-reproducible absolute thermometer over a very wide range of temperatures, able to measure the absolute temperature of a body inside the cavity.\n\nA thermometer is called primary or secondary based on how the raw physical quantity it measures is mapped to a temperature. As summarized by Kauppinen et al., \"For primary thermometers the measured property of matter is known so well that temperature can be calculated without any unknown quantities. Examples of these are thermometers based on the equation of state of a gas, on the velocity of sound in a gas, on the thermal noise voltage or current of an electrical resistor, and on the angular anisotropy of gamma ray emission of certain radioactive nuclei in a magnetic field.\"\n\nIn contrast, \"Secondary thermometers are most widely used because of their convenience. Also, they are often much more sensitive than primary ones. For secondary thermometers knowledge of the measured property is not sufficient to allow direct calculation of temperature. They have to be calibrated against a primary thermometer at least at one temperature or at a number of fixed temperatures. Such fixed points, for example, triple points and superconducting transitions, occur reproducibly at the same temperature.\"\n\nThermometers can be calibrated either by comparing them with other calibrated thermometers or by checking them against known fixed points on the temperature scale. The best known of these fixed points are the melting and boiling points of pure water. (Note that the boiling point of water varies with pressure, so this must be controlled.)\n\nThe traditional way of putting a scale on a liquid-in-glass or liquid-in-metal thermometer was in three stages:\n\nOther fixed points used in the past are the body temperature (of a healthy adult male) which was originally used by Fahrenheit as his upper fixed point ( to be a number divisible by 12) and the lowest temperature given by a mixture of salt and ice, which was originally the definition of . (This is an example of a Frigorific mixture). As body temperature varies, the Fahrenheit scale was later changed to use an upper fixed point of boiling water at .\n\nThese have now been replaced by the defining points in the International Temperature Scale of 1990, though in practice the melting point of water is more commonly used than its triple point, the latter being more difficult to manage and thus restricted to critical standard measurement. Nowadays manufacturers will often use a thermostat bath or solid block where the temperature is held constant relative to a calibrated thermometer. Other thermometers to be calibrated are put into the same bath or block and allowed to come to equilibrium, then the scale marked, or any deviation from the instrument scale recorded. For many modern devices calibration will be stating some value to be used in processing an electronic signal to convert it to a temperature.\n\nThe precision or resolution of a thermometer is simply to what fraction of a degree it is possible to make a reading. For high temperature work it may only be possible to measure to the nearest 10 °C or more. Clinical thermometers and many electronic thermometers are usually readable to 0.1 °C. Special instruments can give readings to one thousandth of a degree. However, this precision does not mean the reading is true or accurate, it only means that very small changes can be observed.\n\nA thermometer calibrated to a known fixed point is accurate (i.e. gives a true reading) at that point. Most thermometers are originally calibrated to a constant-volume gas thermometer. In between fixed calibration points, interpolation is used, usually linear. This may give significant differences between different types of thermometer at points far away from the fixed points. For example, the expansion of mercury in a glass thermometer is slightly different from the change in resistance of a platinum resistance thermometer, so these two will disagree slightly at around 50 °C. There may be other causes due to imperfections in the instrument, e.g. in a liquid-in-glass thermometer if the capillary tube varies in diameter.\n\nFor many purposes reproducibility is important. That is, does the same thermometer give the same reading for the same temperature (or do replacement or multiple thermometers give the same reading)? Reproducible temperature measurement means that comparisons are valid in scientific experiments and industrial processes are consistent. Thus if the same type of thermometer is calibrated in the same way its readings will be valid even if it is slightly inaccurate compared to the absolute scale.\n\nAn example of a reference thermometer used to check others to industrial standards would be a platinum resistance thermometer with a digital display to 0.1 °C (its precision) which has been calibrated at 5 points against national standards (−18, 0, 40, 70, 100 °C) and which is certified to an accuracy of ±0.2 °C.\n\nAccording to British Standards, correctly calibrated, used and maintained liquid-in-glass thermometers can achieve a measurement uncertainty of ±0.01 °C in the range 0 to 100 °C, and a larger uncertainty outside this range: ±0.05 °C up to 200 or down to −40 °C, ±0.2 °C up to 450 or down to −80 °C.\n\nIn theory any physical phenomenon exhibiting a temperature dependence could be used as a thermometer, measuring temperature indirectly. Some of these properties have been exploited. For example, blackbody radiation allows one to measure the temperature in a blast furnace or kiln, or the temperature of a distant star\n\n\n\n\n\n\n\n\n\n\n\n\nThermometers utilize a range of physical effects to measure temperature. Temperature sensors are used in a wide variety of scientific and engineering applications, especially measurement systems. Temperature systems are primarily either electrical or mechanical, occasionally inseparable from the system which they control (as in the case of a mercury-in-glass thermometer). Thermometers are used in roadways in cold weather climates to help determine if icing conditions exist. Indoors, thermistors are used in climate control systems such as air conditioners, freezers, heaters, refrigerators, and water heaters. Galileo thermometers are used to measure indoor air temperature, due to their limited measurement range.\n\nSuch liquid crystal thermometers (which use thermochromic liquid crystals) are also used in mood rings and used to measure the temperature of water in fish tanks.\n\nFiber Bragg grating temperature sensors are used in nuclear power facilities to monitor reactor core temperatures and avoid the possibility of nuclear meltdowns.\n\nNanothermometry is an emergent research field dealing with the knowledge of temperature in the sub-micrometric scale. Conventional thermometers cannot measure the temperature of an object which is smaller than a micrometre, and new methods and materials have to be used. Nanothermometry is used in such cases. Nanothermometers are classified as luminescent thermometers (if they use light to measure temperature) and non-luminescent thermometers (systems where thermometric properties are not directly related to luminescence).\n\nThermometers used specifically for low temperatures.\n\n\nVarious thermometric techniques have been used throughout history such as the Galileo thermometer to thermal imaging.\nMedical thermometers such as mercury-in-glass thermometers, infrared thermometers, pill thermometers, and liquid crystal thermometers are used in health care settings to determine if individuals have a fever or are hypothermic.\n\nThermometers are important in food safety, where food at temperatures within can be prone to potentially harmful levels of bacterial growth after several hours which could lead to foodborne illness. This includes monitoring refrigeration temperatures and maintaining temperatures in foods being served under heat lamps or hot water baths.\nCooking thermometers are important for determining if a food is properly cooked. In particular meat thermometers are used to aid in cooking meat to a safe internal temperature while preventing over cooking. They are commonly found using either a bimetallic coil, or a thermocouple or thermistor with a digital readout.\nCandy thermometers are used to aid in achieving a specific water content in a sugar solution based on its boiling temperature.\n\nAlcohol thermometers, infrared thermometers, mercury-in-glass thermometers, recording thermometers, thermistors, and Six's thermometers are used in meteorology and climatology in various levels of the atmosphere and oceans. Aircraft use thermometers and hygrometers to determine if atmospheric icing conditions exist along their flight path. These measurements are used to initialize weather forecast models. Thermometers are used in roadways in cold weather climates to help determine if icing conditions exist and indoors in climate control systems.\n\n\n\n"}
{"id": "17527292", "url": "https://en.wikipedia.org/wiki?curid=17527292", "title": "This Mechanical Age", "text": "This Mechanical Age\n\nThis Mechanical Age is a 1954 American short documentary film about the early days of aviation, produced by Robert Youngson. In 1955, it won an Academy Award for Best Short Subject (One-Reel) at the 27th Academy Awards.\n"}
{"id": "33378812", "url": "https://en.wikipedia.org/wiki?curid=33378812", "title": "Transparent heating film", "text": "Transparent heating film\n\nTransparent heating film, also called transparent heating plastic or heating transparent polymer film is a thin and flexible polymer film with a conductive optical coating. Transparent heating films may be rated at 2.5kW/m at voltages below 48 volts direct current (VDC). This allows heating with secure transformers delivering voltages which will not hurt the human body. Transparent conductive polymer films may be used for heating transparent glasses. A combination with transparent SMD electronic for multipurpose applications, is also possible. It is also a variant of carbon heating film.\n\n"}
{"id": "3132886", "url": "https://en.wikipedia.org/wiki?curid=3132886", "title": "Universal remote", "text": "Universal remote\n\nA universal remote is a remote control that can be programmed to operate various brands of one or more types of consumer electronics devices. Low-end universal remotes can only control a set number of devices determined by their manufacturer, while mid- and high-end universal remotes allow the user to program in new control codes to the remote. Many remotes sold with various electronic include universal remote capabilities for other types of devices, which allows the remote to control other devices beyond the device it came with. For example, a VCR remote may be programmed to operate various brands of televisions.\n\nOn May 30, 1985, Philips introduced the first universal remote (U.S. Pat. #4774511) under the Magnavox brand name.\nIn 1985, Robin Rumbolt, William \"Russ\" McIntyre, and Larry Goodson with North American Philips Consumer Electronics (Magnavox, Sylvania, and Philco) developed the first universal remote control. Shortly after development was completed and patent applications filed, Magnavox initiated the \"Smart, Very Smart\" campaign, coining the \"smart\" axiom. McIntyre has claimed that the primary design challenge was fitting the well-crafted, tight code into an extremely limited memory space. At least two subsequent patents followed: US Pat. 4703359, on November 20, 1988 and US Pat. 4951131, in 1989.\n\nIn 1987, the first programmable universal remote control was released. It was called the \"CORE\" and was created by CL 9, a startup founded by Steve Wozniak, the inventor of the Apple I and Apple II computers.\n\nIn March 1987, Steve Ciarcia published an article in Byte Magazine entitled \"Build a Trainable Infrared Master Controller\", describing a universal remote with the ability to upload the settings to a computer. This device had macro capabilities.\n\nMost universal remotes share a number of basic design elements:\n\n\nCertain highly reduced designs such as the TV-B-Gone or keychain-sized remotes include only a few buttons, such as power and channel/volume selectors.\n\nHigher-end remotes have numerous other features:\n\n\nSome universal remotes allow the code lists programmed into the remote to be updated to support new brands or models of devices not currently supported by the remote. Some higher end universal learning remotes require a computer to be connected. The connection is typically done via USB from the computer to mini-USB on the remote or the remotes base station.\n\nIn 2000, a group of enthusiasts discovered that universal remotes made by UEI and sold under the One For All, RadioShack, and other brands can be reprogrammed by means of an interface called JP1.\n\nIR learning remotes can learn the code for any button on many other IR remote controls. This functionality allows the remote to learn functions not supported by default for a particular device, making it sometimes possible to control devices that the remote was not originally designed to control. A drawback of this approach is that the learning remote needs a functioning teaching remote. Also, some entertainment equipment manufacturers use pulse frequencies that are higher than what the learning remote can detect and store in its memory.\n\nThese remotes feature an LCD screen that can be either monochrome or full color. The \"buttons\" are actually images on the screen, which, when touched, will send IR signals to controlled devices. Some models have multiple screens that are accessed through virtual buttons on the touch-screen and other models have a combination of the touch-screen and physical buttons.\n\nSome models of the touch-screen remotes are programmed using a graphical interface program on a PC, which allows the user to customize the screens, backgrounds, buttons and even the \"actions\" the buttons perform. The \"project\" that is created is then downloaded into the remote through a USB cable or, in the most recent models, wirelessly by Bluetooth or Wi-Fi.\n\nThe newest touch-screen remotes, such as the Logitech 900 and 1100, include an RF transmitter to allow signals to reach locations much farther than the usual range of IR (approximately 6 meters). RF also does not require line of sight.\n\nSome touch-screen remote controls, such as the Ray Super Remote, now have content recommendations built directly in to the universal remote control. \n\nSmartphones and tablets such as those running Nokia's Maemo (N900), Apple's iOS and Google's Android operating system can also be used as universal remote controls.\n\nA number of devices from vendors such as Samsung, LG and Nokia include a built-in IR port that can be used as a remote, while others require a physical attachment, or 'dongle', be connected on to the phone when used as a remote. The dongle is required to convert the electrical control signals from the phone into infra red signals that are required by most home audio visual components for remote control. However it is also possible to implement a system that does not require a dongle. Such systems use a stand-alone piece of hardware called a 'gateway', which receives the electrical control signals from the smartphone in Bluetooth or wi-fi form and forward them on in infra red form to the components to be controlled.\n\n"}
{"id": "53612724", "url": "https://en.wikipedia.org/wiki?curid=53612724", "title": "Voleo", "text": "Voleo\n\nMobile-focused fintech company Voleo Inc. specializes in smartphone stock trading apps for investment clubs, enabling users to form teams with people they trust to democratically manage a portfolio in publicly traded securities.\n\nVoleo Inc. is privately held and is based in Vancouver, BC, Canada. Voleo USA Inc, is a FINRA registered broker-dealer, and is a wholly owned subsidiary of Voleo, Inc. Voleo USA is registered with the U.S. Securities and Exchange Commission and is a member of the Securities Investor Protection Corporation (SIPC).\n\nVoleo was founded in 2013 in Vancouver, BC. Founder Jay Sujir, Chairman, recruited co-founders Mark Morabito, Executive Chairman, and Thomas Beattie, CEO to bring the concept to market. The company’s senior technology team is led by Anthony Tsui, VP Technology, and Gordon Jones, VP Product.\n\nIn October 2015, Voleo launched a simulated university trading competition in collaboration with Toronto Stock Exchange (TSX) and TSX Venture Exchange (TSXV), with the results announced in 2016. Over two thousand students participated in this inaugural event. \nVoleo was part of Accenture’s FinTech Innovation Lab London 2016. In the ensuing months, Voleo prepared its apps for live trading and commenced the registration of subsidiary Voleo USA.\n\nIn March 2017, Voleo announced the completion of US State registrations for its subsidiary Voleo USA. Most recently, Voleo won Best of Show for its white-label platform at Finovate Fall 2017 in New York, was selected for PlugAndPlay's Fintech Accelerator. In December 2017, Voleo along with Nasdaq completed an inaugural equity trading competition, and the winning team was joined by host representatives for a tour of Nasdaq's Market Site in Times Square in February 2018.\n\nThe Voleo stock trading app for investment clubs is available in the United States through the Google Play Store and Apple App Store. The Voleo app allows users to create clubs with 3-100 people, propose, discuss, and vote on trades. \nAll customer accounts and assets are held by APEX Clearing Corporation, who are the fully disclosed carrying broker of introducing broker Voleo USA, Inc.\n\nVoleo SimuTrader is a free investment club simulator available internationally through the Google Play Store and Apple App Store. The Voleo SimuTrader allows users to form investment clubs, propose trades, discuss and vote, track performance, and manage decisions in an entirely simulated environment.\n\nBoth Voleo apps track decisions made by club members and assigns a score to each member, known as a Definitive Return on Investment Decisions (DROID) score. This enables users to learn while feeding a community where top performing individuals as well as clubs can be followed in real time for investment ideas. These ideas can then be vetted with their trusted peers.\n"}
