{"id": "50761717", "url": "https://en.wikipedia.org/wiki?curid=50761717", "title": "AOS and SOA", "text": "AOS and SOA\n\nIn computing, AoS and SoA refer to contrasting ways to arrange a sequence of records in memory, with regard to interleaving, and are of interest in SIMD programming.\n\nStructure of arrays (or SoA) is a layout separating elements of a record (or 'struct' in the C programming language) into one parallel array per field. The motivation is easier manipulation with packed SIMD instructions in most instruction set architectures, since a single SIMD register can load homogeneous data, possibly transferred by a wide internal datapath (e.g. 128-bit). If only a specific part of the record is needed, only those parts need to be iterated over, allowing more data to fit onto a single cache line. The downside is requiring more cache ways when traversing data, and inefficient indexed addressing. (see also: planar image format)\n\nArray of structures (or AoS) is the opposite (and more conventional) layout, in which data for different fields is interleaved. \nThis is often more intuitive, and supported directly by most programming languages.\n\nHere is an example of the two layouts written in C++:\nstruct Point {\n\n/* Array of structures approach */\n//Each struct contains information about an individual ball.\nstruct Ball {\n\nstd::vector<Ball> balls;\n\n//Updating is as simple as looping over the array of balls.\nvoid update_positions(std::vector<Ball>& bs) {\n\n//If we only care about a particular attribute (in this case, the name) we\n//have to load the entire ball into the function, which would be wasteful.\nvoid print_names(const std::vector<Ball>& bs) {\n\n/* Structure of arrays approach */\n//The struct consists of arrays of each particular attribute of the ball.\nstruct Balls {\n\n//To iterate over the structure of arrays, we have to iterate over each of the \n//relevant arrays.\nvoid update_positions(Balls& bs) {\n};\n\n//If we only care about a particular attribute, we only have to load the \n//relevant arrays, and nothing else.\nvoid print_names(const Balls& bs) {\nA hybrid approach is possible, e.g. de-interleaving a number of objects that correspond to the number of SIMD lanes.\n\nIt is also possible to split some subset of a structure (rather than each individual field) into a parallel array this can actually improve locality of reference if different pieces of fields are used at different times in the program (see data oriented design).\n\nSome SIMD architectures provide strided load/store instructions to load homogeneous data from SoA format. Yet another option used in some CELL libraries is to de-interleave data from AoS format when loading sources into registers, and interleave when writing out results (facilitated by superscalar issue of permutes). Some vector maths libraries align floating point 4D vectors with the SIMD register to leverage the associated data path and instructions, whilst still providing programmer convenience, although this does not scale to SIMD units wider than four lanes.\n\nIn the GPU stream processing model, SoA layout is generally not required, as they provide true vector addressing.\n\nAOS vs SOA presents a choice when considering 3D or 4D vector data on machines with 4-lane SIMD hardware. SIMD ISAs are usually designed for homogeneous data, however some provide a dot product instruction and additional permutes making the AOS case easier to handle. As of 2016, most GPUs have moved away from 4D instructions to scalar SIMT pipelines, for better performance for compute kernels.\n\nMost languages support the AOS format more naturally by combining records and various array abstract data types. The experimental JAI programming language is a recent attempt to provide language level SOA support. Julia language, supports multi-dimensional arrays, with AOS, or SOA (through a package).\n\n"}
{"id": "57707341", "url": "https://en.wikipedia.org/wiki?curid=57707341", "title": "Ace Stream", "text": "Ace Stream\n\nAce Stream is a peer-to-peer multimedia streaming protocol, built using BitTorrent technology. Ace Stream has been recognized by sources as a potential method for broadcasting and viewing bootlegged live video streams.\n\nAce Stream began under the name TorrentStream as a pilot project to use BitTorrent technology to stream live video. In 2013 TorrentStream was re-released under the name ACE Stream.\n\nAce Stream clients function as both a client and a server. When users stream a video feed using Ace Stream, they are simultaneously downloading from peers and uploading the same video to other peers.\n"}
{"id": "1717029", "url": "https://en.wikipedia.org/wiki?curid=1717029", "title": "Acorn Online Media Set Top Box", "text": "Acorn Online Media Set Top Box\n\nThe Acorn Online Media Set Top Box was produced by the \"Online Media\" division of Acorn Computers Ltd for the Cambridge Cable and Online Media Video on Demand trial and launched early 1996. Part of this trial involved a home-shopping system in partnership with Parcelforce.\n\nThe hardware was trialled by NatWest bank, as exhibited at the 1995 \"Acorn World\" trade show.\n\nThe STB1 was a customised Risc PC based system, with a Wild Vision Movie Magic expansion card in a podule slot, and a network card based on Asynchronous Transfer Mode.\n\n\nThe STB20 was a new PCB based around the ARM7500 System On Chip.\n\n\nBy this time Online Media had been restructured back into Acorn Computers, so the STB22 is branded as 'Acorn'.\n\n\n"}
{"id": "2927689", "url": "https://en.wikipedia.org/wiki?curid=2927689", "title": "Advance ship notice", "text": "Advance ship notice\n\nAn advance ship notice or advance shipping notice (ASN) is a notification of pending deliveries, similar to a packing list. It is usually sent in an electronic format and is a common EDI document. In the EDI X12 system, it is known as the EDI 856 document and the EDIFACT equivalent is the DESADV (Dispatch Advice) message. The ASN can be used to list the contents of a shipment of goods as well as additional information relating to the shipment, such as order information, product description, physical characteristics, type of packaging, markings, carrier information, and configuration of goods within the transportation equipment. The ASN enables the sender to describe the contents and configuration of a shipment in various levels of detail and provides an ordered flexibility to convey information.\n\nThe ASN is noteworthy in that it is a new concept in logistics, enabled by the advance of modern communication methods. Although it provides information similar to the Bill of lading, its function is very different. While the Bill of lading is meant to accompany a load on its path, the goal of the ASN is to provide information to the destination's receiving operations well in advance of delivery. This tends to impact the logistics stream in three areas: cost, accuracy, and flexibility. \n\nCost. Modern receiving operations rarely have time to break down a shipping unit (carton or pallet) and identify its components, depending instead on quick scans of barcodes on shipping labels. An ASN can provide a list of all of the barcoded ID numbers of the shipping units and the contents of each. Receiving costs are thought to be reduced by about 40%. \n\nAccuracy. Upon receipt of the ASN, the receiver is immediately informed of any difference between what was expected, and what has actually been shipped.\n\nFlexibility. Knowing the actual fill rates of the orders gives the receiver the opportunity to re-allocate goods in subsequent shipments.\n\nASN and finance \nThe ASN can be used to pay suppliers directly for goods received. This can be accomplished by receiving the ASN into the company computer system (ERP), printing company labels for each container received, affixing the labels on the containers, and then transmitting any discrepancies to the supplier via EDI.\n\n"}
{"id": "16507549", "url": "https://en.wikipedia.org/wiki?curid=16507549", "title": "Alzheimer's disease research", "text": "Alzheimer's disease research\n\nThere are different approaches in Alzheimer's disease research. One approach is to reduce amyloid beta, for example with bapineuzumab, an antibody in phase III studies for patients in mild to moderate stage; semagacestat, a Î³-secretase inhibitor, MPC-7869; and UB-311, ACC-001 or CAD106, vaccines against amyloid beta. Other approaches are neuroprotective agents, like AL-108 (phase II completed); or metal-protein interaction attenuation, as is the case of PBT2 (phase II completed). Yet another approach is to use general cognitive enhancers, as may be the case for memantine, a pharmaceutical approved in the United States and European Union to treat symptoms of moderate-to-severe AD. In animals, ultrasound has been used for penetrating the blood-brain barrier and activating microglial cells to eliminate amyloid beta and restored memory function. Finally, there are basic investigations on the origin and mechanisms of Alzheimer's disease.\n\nSeveral potential treatments for Alzheimer's disease are under investigation, including several compounds being studied in phase 3 clinical trials. The most important clinical research is focused on potentially treating the underlying disease pathology, for which reduction of amyloid beta is a common target of compounds under investigation.\n\nImmunotherapy or vaccination for Alzheimer's stimulates the immune system to attack beta-amyloid. One approach is active immunization, which would stimulate a permanent immune response. The vaccine AN-1792 showed promise in mouse and early human trials, but in a 2002 Phase II trial, 6% of subjects (18 of 300) developed serious brain inflammation resembling meningoencephalitis, and the trial was stopped. In long-term followups, 20% of subjects had developed high levels of antibodies to beta-amyloid. While placebo-patients and non-antibody responders worsened, these antibody-responders showed a degree of stability in cognitive levels as assessed by the neuropsychological test battery (although not by other measures), and had lower levels of the protein tau in their cerebrospinal fluid. These results may suggest reduced disease activity in the antibody-responder group. Autopsies found that immunization resulted in clearance of amyloid plaques, but did not prevent progressive neurodegeneration.\n\nA Phase IIA study of ACC-001, a modified version of AN-1792, is now recruiting subjects.\n\nOne AÎ² vaccine was found to be effective against inclusion body myositis in mouse models.\n\nAlso derived from the AN-1792 immunotherapy program, there is an infused antibody approach termed a passive vaccine in that it does not invoke the immune system and would require regular infusions to maintain the artificial antibody levels. Micro-cerebral hemorrhages may be a threat to this process.\n\nBapineuzumab, an antibody to amyloid-Î², was previously being developed; however, the drug failed in phase 3 clinical trials. The antibody was designed as essentially identical to the natural antibody triggered by the earlier AN-1792 vaccine.\n\nA recent study showed FDA-approved cancer drugs, PD-1 inhibitors, may benefit patients with Alzheimer's disease. The study used a mouse model of Alzheimer's disease and an antibody against PD-1 to demonstrate a statistically significant reduction in amyloid-Î² plaques and improved cognitive performance.\n\nGamma secretase is a protein complex thought to be a fundamental building block in the development of the amyloid beta peptide. A gamma secretase inhibitor, semagacestat, failed to show any benefit to Alzheimer's disease patients in clinical trials.\n\nTarenflurbil (MPC-7869, formerly R-flubiprofen) is a gamma secretase modulator sometimes called a selective amyloid beta 42 lowering agent. It is believed to reduce the production of the toxic amyloid beta in favor of shorter forms of the peptide. Negative results were announced regarding tarenflurbil in July 2008 and further development was canceled.\n\n, Amgen and Novartis had entered into a co-development agreement to progress the oral beta secretase inhibitor CNP520.\n\nPBT2 is an 8-hydroxy quinoline that removes copper and zinc from cerebrospinal fluid, which are held to be necessary catalysts for amyloid beta aggregation. This drug has been in a Phase II trial for early Alzheimers and which has reported preliminarily promising, but not detailed, results.\n\nSimvastatin, a statin, stimulates brain vascular endothelial cells to create a beta-amyloid ejector. The use of this statin may have a causal relationship to decreased development of the disease.\n\nThis approach is based on the prominent aspect of Alzheimer's disease, which is common for many other neurodegenerative diseases: energy deficit. It has first been noted for the case of insulin insufficiency in the brain of Alzheimer's patients. Because of that Alzheimer's disease has been called Type 3 diabetes and the insulin modification therapies are in pharmaceutical's pipelines.\n\nSeveral other pharmaceuticals are under investigation to treat Alzheimer's disease.\n\nAllopregnanolone has been identified as a potential drug agent. Levels of neurosteroids such as allopregnanolone decline in the brain in old age and AD. Allopregnanolone has been shown to aid the neurogenesis that reverses cognitive deficits in a mouse model of AD.\n\nA retrospective analysis of five million patient records with the US Department of Veterans Affairs system found that different types of commonly used anti-hypertensive medications had very different AD outcomes. Those patients taking angiotensin receptor blockers (ARBs) were 35â40% less likely to develop AD than those using other anti-hypertensives.\n\nSeveral studies using antibiotics in animal models of Alzheimer's Disease indicated that minocycline and doxycycline exerted a protective effect in\npreventing neuron death and slowing the onset of the disease.\n\nA preliminary trial of antibiotic therapy with doxycycline and rifampin at McMaster University had indicated that it was effective in delaying the progress of the disease: \"In conclusion, a 3-month course of doxycycline and rifampin reduced cognitive worsening at 6 months of follow-up in patients with mild to moderate AD.\" A re-examination of the same data using \"AUC analysis of the pooled index\nshowed significant treatment effect over the 12-month period\". However, when these preliminary findings were followed up with a subsequent larger trial, they found that \"doxycycline or rifampin, alone or in combination, has no beneficial effects on cognition or function in [Alzheimer's disease].\"\n\nThe possibility that AD could be treated with antiviral medication is suggested by a study showing colocation of herpes simplex virus with amyloid plaques.\n\nAs of 2018, there is no high-quality clinical research on the possible use of cannabis to deter cognitive decline and dementia in Alzheimer's disease.\n\nAlso in July 2008 results were announced of a study in which an antihistamine that was formerly available in Russia, Dimebon, was given to a group of AD patients. The group receiving Dimebon improved somewhat over the 6 months of the study (and this continued for the next six months), whereas those on placebo deteriorated.\nUnfortunately the consecutive phase-III trial failed to show significant positive effects in the primary and secondary endpoints. The sponsors acknowledged in March 2010 that initial results of the phase III trial showed that while the drug had been well tolerated, its outcomes did not significantly differ from the placebo control.\n\nAs a TNF inhibitor, etanercept is currently being studied for its potential use to help treat dementia. Several studies suggest that increased levels of tumor necrosis factor (TNF) correlates to worsening psychiatric symptoms and neurodegeneration.\n\nRecent studies suggest an association between insulin resistance and AD (fat cell sensitivity to insulin can decline with aging):\nIn clinical trials, a certain insulin sensitizer called \"rosiglitazone\" improved cognition in a subset of AD patients;\nin vitro, beneficial effects of Rosiglitazone on primary cortical rat neurons have been demonstrated.\nInitial research suggests intranasal insulin, increasing insulin levels in the brain with minimal insulin increase in the rest of the body, might also be utilized. Preclinical studies show that insulin clears soluble beta-amyloid from the brain within minutes after a systemic injection in diabetic transgenic mice modeling AD.\n\nIn July 2008, researchers announced positive results from methylthioninium chloride (MTC), (trade name: Rember) a drug that dissolved Tau polymers. Phase II results indicate that it is the first therapy that has success in modifying the course of disease in mild to moderate AD.\n\nOriginally considered an enigmatic protein, the sigma-1 receptor has been identified as a unique ligand-regulated molecular chaperone in the endoplasmic reticulum of cells. This discovery led to the review of many proposed roles of this receptor in many neurological diseases including Alzheimer's.\n\nA 2013 study showed that translocator protein can prevent and partially treat Alzheimer's disease in mice.\n\nR7 is a prodrug of 7,8-dihydroxyflavone, an agonist of TrkB, the main receptor of brain-derived neurotrophic factor (BDNF). R7 is currently in preclinical development for the treatment of Alzheimer's disease.\n\nThe most widely used biomarker is the peptide beta-amyloid 1-42 (AÎ²42), which is measured in cerebrospinal fluid. Levels of AÎ²42 has a high agreement with beta-amyloid positron emission tomography (PET) and both methods detect prodromal AD with equally high accuracy.\n\nStudies have also shown that people with AD had decreased glutamate (Glu) as well as decreased Glu/creatine (Cr), Glu/myo-inositol (mI), Glu/N-acetylaspartate (NAA), and NAA/Cr ratios compared to normal people. Both decreased NAA/Cr and decreased hippocampal glutamate may be an early indicator of AD.\n\nEarly research using a small cohort of Alzheimer's disease patients may have identified autoantibody markers for AD. The applicability of these markers is unknown.\n\nA small human study in 2011 found that monitoring blood dehydroepiandrosterone (DHEA) variations in response to an oxidative stress could be a useful proxy test: the subjects with MCI did not have a DHEA variation, while the healthy controls did.\n\nA 2013 study on 202 people at the Saarland University in Germany found 12 microRNAs in the blood were 93% accurate in diagnosing Alzheimer's disease.\n\nPositive preliminary results in rats with a non-invasive ultrasound technology aimed to clear the brain of amyloid plaques were reported in Science Translational Medicine. An Australian team describes the strategy as beaming ultrasound into the brain tissue. By oscillating at high frequencies, the sound waves combined with blood-borne microbubbles are able to open up the blood-brain barrier, so diminishing the brain defenses for some hours - an interval in which they stimulate the brainâs microglial cells into activation (and, also, give drugs or the immune system access to the brain). The team reports having observed an important clearing out in the beta-amyloid clumps, a change attributed to the microglial cells since their function is basically connected with waste-removal; and full restoration of the lost memory and cognitive functions in 75 percent of the mice they tested it on, without concomitant damage to the brain parenchyma (either in the tissue that was surrounding the beta-amyloid plates, or elsewhere). The treated mice are reported to have displayed improved performance in three memory tasks - a maze, a test to make them to recognise new objects, and one to make them to remember the places they should avoid. On these results, the team is planning on starting trials with higher animal models, such as sheep and monkeys, for eventually to have human trials underway in 2017.\n\nAlthough i\"n silico\" studies have advanced our understanding of Alzheimerâs disease (AD) in many different areas, there are still limitations to these methodologies because bioinformatics tools are biased toward known data. Nonetheless, the findings obtained from using publicly available bioinformatics tools and databases have provided a mean to discover new treatments and to spark new questions to facilitate the process of finding cures for AD.\n\nFrom gene expression patterns obtained in microarray datasets, correlation between cellular physiology and diseases can be revealed. Divergence studies (e.g. Jensen-Shannon divergence computations which interprets difference in gene expression and probability of distribution patterns) reveals gene expression distribution difference between AD and normal aging brains. That is, expressed genes that are negatively correlated with normal aging brain but are positively correlated with AD brains are possible biomarkers for AD diagnosis and treatment. Combining KEGG and PATHWAY studio, ATP5C1, COX6A1, NDUFV2, PLCB1, and PPP3CA are metabolism and mitochondrial-related genes that have been shown to be reduced in AD samples. Furthermore, metabolic dysregulations such as calcium homeostasis and insulin signaling have also been identified to contribute to the onset of AD. Genes that are associated with calcium and insulin signaling are found using GATHER (online bioinformatics tool for analyzing genomic signatures). In fact, insulin signaling impairment and AD has been considered to be related in many levels. Functional protein sequence alignments (e.g. ClustalW, MUSCLE) and phylogenetic analysis (e.g. Phylip, Mega) demonstrate that acetylcholinesterase (AChE) and butyrylcholinesterase (BChE) are highly linked in these two diseases. Increased BChE contributes altered lipoprotein metabolism and insulin insensitivity, and is positively correlated to hypertension and diabetes in correlation studies. AChE allows stabilization of neurotransmitter, acetylcholine (ACh), which is one of the main target for AD treatment. However, recent \"in\" silico pharmacological study examined drug-disease interaction showed that AChE inhibitors may not be the answer to AD treatment. PKC, ARG, HDAC, and GSK3 inhibitors that regulate calcium homeostasis and genetic modification of cell cycle and apoptosis may be the future targets of AD medication.\n\nNeuronal plasticity is a key player in cognitive function that cannot be ignored in study of AD progression. Microarray studies found that NEFM, NEFL, and SV2B are highly downregulated in samples obtained from severe AD patients. NEFL is a neurofilament gene that has been shown to be related to hypotrophy of axons in motorneurons when mutated. However, both neurofilaments (NEFL and NEFM) have been documented to be involved in neurological disease, Charcot-Marie-Tooth, instead of AD, which demonstrate possible unknown connections of AD to other neurological diseases. SV2B is another gene that is downregulated in AD and has been shown to be related to neurodegeneration, particularly synaptic calcium-regulated exocytosis. The downregulation of genes responsible for neural synapse and neuroplasticity is related to another family of protein that has been found to be related to AD pathogenesis, EGR (early growth response). This EGR is regulated by upregulated FOXO1 (Forkhead Box O1) through PI3K/Akt pathway, which is listed as one of the pathway for future in anti-AD medication. These findings using computational methods allow for the connection of different studies and facilitate the understanding of disease complexity as well as directing to new possible biomarkers of AD.\n\nThe current treatment for AD symptoms are acetylcholinesterase inhibitors and N-methyl-D-aspartate receptor (NMDA) antagonists. Based on the current literature on AD pharmacology research, analyzing differentially expressed genes in drug-drug, disease-disease, and drug-disease models allows the discovery of novel pharmaceutical agents that potentially treat more than AD symptoms. Analytical tools such as Connectivity Map (cMap) were used in drug-disease interaction from publicly available microarray data. Gene signatures from the cMap-based interpretation showed that common anti-AD drugs (tacrine, donepezil, galantamine, memantine, and rivastigmine) were not listed in the final drug list. Rather, other compounds that inhibit downstream effectors of cell proliferation, Wnt and insulin pathways, epigenetic modifications, and cell cycle regulation were among the top in the final anti-AD drug list. These findings further supported the fact that AD is a disease of degeneration and growth dysregulation. In fact, the final list of anti-AD drugs, obtained from analyzing microarray datasets and cMap drug-disease model contained the common effector of AD and diabetes â glycogen synthase kinase 3 (GSK3-an enzyme that has been found to be related to hyperphosphorylation of tau protein) â confirmed the link between the two diseases. Further pathway and network interpretation of genes obtained from AD microarray datasets using KEGG, WikiPathways, Reactome, Biocarta, and NetworkAnalyst showed that epidermal growth factor (EGF) and its receptors were strongly associated with pathogenesis of AD. EGFR is a transmembrane protein and a member of the HER/ErbB receptor family that share a common pathway with insulin receptors (Ras/Raf/Mak and PI3K/Akt). Furthermore, amyloid protein precursor (APP) was found to be indirectly related based on network analysis. AÎ² (one of the diagnostic findings of AD) activates EGFR and inhibition of the receptor improved memory disorders in AÎ²-overexpressed drosophila. Drugs that block GSK3 were found to be affecting PI3K/Akt pathway, demonstrating that EGFR could be a new target for pharmaceutical agent in treating AD.\n\n"}
{"id": "55782936", "url": "https://en.wikipedia.org/wiki?curid=55782936", "title": "Andy and Bill's law", "text": "Andy and Bill's law\n\nAndy and Bill's law is a statement regarding the relationship between hardware and software upgrades. The law originates from a humorous one-liner told in the 1990s during computing conferences: \"what Andy giveth, Bill taketh away.\" The phrase is a riff upon the business strategies of former Intel CEO Andy Grove, and former CEO of Microsoft, Bill Gates. Intel and Microsoft had entered into a lucrative partnership in the 1980s through to the 1990s, and the standard chipsets in Microsoft Windows were Intel brand. Despite the profit Intel gained from the deal, Grove felt that Gates wasn't making full use of the powerful capabilities of Intel chips, and that he was in fact refusing to upgrade his software to achieve optimum hardware performance. Grove's frustration with the dominance of Microsoft software over Intel hardware became public, which spawned the humorous catchphrase; and, later, Andy and Bill's law: that new software will always push ahead of hardware and get credit for the performance of the computer.\n"}
{"id": "50340277", "url": "https://en.wikipedia.org/wiki?curid=50340277", "title": "Asia Venture Group", "text": "Asia Venture Group\n\nAsia Venture Group Sdn Bhd (AVG) is a private holding company investing in digital businesses. AVG invests in early stage companies, and has since inception, co-founded 4 companies: iMoney, TrustedCompany, iprice and HappyFresh. The company focuses on digitally scalable business models in Southeast Asia.\n\nAVG was incorporated in Kuala Lumpur by German entrepreneur Tim Marbach in 2013. That year, AVG invested USD500,000 in imoney.my, a Malaysia-based financial products comparison site. In November 2013, AVG co-founded its second company, TrustedCompany, a review platform for e-commerce businesses in Malaysia, India and Brazil.\n\nIn 2014, Kai Kux joined AVG as its Managing Director. AVG co-founded its third and fourth companies in 2014. iPrice is a one stop platform for e-commerce discovery in Southeast Asia. HappyFresh is an online grocery platform with one hour delivery service, which commenced operation at the end of 2014.\n\nAs of 2016, AVG (together with its co-founded companies) had a presence in 8 cities and 13 offices, with more than 1,000 employees from more than 35 countries.\n\nAs of 2016 AVG had invested in more than 20 companies, predominantly early stage companies, such as: KFit, Scalable Capital or Movinga. AVG has already exited some investments such as Audibene, KaufDA and Returbo.\n"}
{"id": "307623", "url": "https://en.wikipedia.org/wiki?curid=307623", "title": "Asthma spacer", "text": "Asthma spacer\n\nA spacer is a device used to increase the ease of administering aerosolized medication from a metered-dose inhaler (MDI). It adds space in the form of a tube or âchamberâ between the mouth and canister of medication. Most spacers have a one-way valve that allows the person to inhale the medication while inhaling and exhaling normally; these are often referred to as valved holding chambers (VHC).\n\nSpacers help those unable to breathe deeply, as well as those unable to synchronize their breathing so that they inhale just as the MDI is actuated; the latter is known as poor \"hand-lung coordination\".\n\nThe term spacer is often used to refer to any tube-like MDI add-on device. Some spacers utilize a collapsing bag design to provide visual feedback that successful inspiration is taking place. Another type is transparent plastic in two vase-shaped parts that come together forming a barrel shape.\n\nTo use an inhaler without a spacer requires coordinating several actions in a set order (pressing down on the inhaler, breathing in deeply as soon as the medication is released, holding your breath, exhaling), and not everyone is able to master this sequence. Use of a spacer, particularly a valved holding chamber, avoids such timing issues. Valved holding chambers are particularly useful for children, people with severe shortness of breath, and those with cognitive impairment.\n\nAfter removing the MDI's cap, the MDI mouthpiece is inserted into the back of the spacer. The front part of the chamber is closed off by either a mouthpiece or a mask that covers the mouth and nose. To administer the medication, the MDI is depressed once, resulting in the release of one dose of medication. The medication from the MDI is then suspended in the spacer's chamber while the person inhales the aerosolized medication by breathing in and out. The exhaled breath exits the device through the valves rather than entering the chamber. Some spacers are equipped with a whistle, which sounds if the person is inhaling too quickly.\n\nSpacers slow down the speed of the aerosol coming from the inhaler, meaning that less of the asthma drug impacts on the back of the mouth and somewhat more may get into the lungs. In the case of corticosteroids, less residue in the mouth reduces the risk of developing oral candidiasis, a yeast infection. Rinsing the mouth after application of inhaled steroids has a similar effect.\n\nWhereas people with asthma can keep an MDI close-by at all times, the bulkiness of spacers can limit their use outside the home.\n"}
{"id": "16292798", "url": "https://en.wikipedia.org/wiki?curid=16292798", "title": "Automatic fire suppression", "text": "Automatic fire suppression\n\nAutomatic fire suppression systems control and extinguish fires without human intervention. Examples of automatic systems include fire sprinkler system, gaseous fire suppression, and condensed aerosol fire suppression. When fires are extinguished in the early stages loss of life is minimal since 93% of all fire-related deaths occur once the fire has progressed beyond the early stages.\n\nToday there are numerous types of Automatic Fire Suppression Systems and standards for each one. Systems are as diverse as the many applications. In general, however, Automatic Fire Suppression Systems fall into two categories: \"engineered\" and \"pre-engineered\" systems.\n\n\nBy definition, an automatic fire suppression system can operate without human intervention. To do so it must possess a means of detection, actuation and delivery. In many systems, detection is accomplished by mechanical or electrical means. Mechanical detection uses fusible-link or thermo-bulb detectors. These detectors are designed to separate at a specific temperature and release tension on a release mechanism. Electrical detection uses heat detectors equipped with self-restoring, normally-open contacts which close when a predetermined temperature is reached. Remote and local manual operation is also possible. Actuation usually involves either a pressurized fluid and a release valve, or in some cases an electric pump. Delivery is accomplished by means of piping and nozzles. Nozzle design is specific to the agent used and coverage desired.\n\nIn the early days, water was the exclusive fire suppression agent. Although still used today, water has limitations. Most notably, its liquid and conductive properties can cause as much property damage as fire itself.\n\nDespite their effectiveness, chemical fire extinguishing agents are not without disadvantages. In the early 20th century, carbon tetrachloride was extensively used as a dry cleaning solvent, a refrigerant and as a fire extinguishing agent. In time, it was found carbon tetrachloride could lead to severe health effects. From the mid-1960s Halon 1301 was the industry standard for protecting high-value assets from the threat of fire. Halon 1301 had many benefits as a fire suppression agent; it is fast-acting, safe for assets and required minimal storage space. Halon 1301's major drawbacks are that it depletes atmospheric ozone and is potentially harmful to humans. Since 1987, some 191 nations have signed The Montreal Protocol on Substances That Deplete the Ozone Layer. The Protocol is an international treaty designed to protect the ozone layer by phasing out the production of a number of substances believed to be responsible for ozone depletion. Among these were halogenated hydrocarbons often used in fire suppression. As a result, manufacturers have focused on alternatives to Halon 1301 and Halon 1211 (halogenated hydrocarbons). A number of countries have also taken steps to mandate the removal of installed Halon systems. Most notably these include Germany and Australia, the first two countries in the world to require this action. In both of these countries complete removal of installed Halon systems has been completed except for a very few essential-use applications. The European Union is currently undergoing a similar mandated removal of installed Halon systems.\n\nThe first fire extinguisher patent was issued to Alanson Crane of Virginia on Feb. 10, 1863. The first fire sprinkler system was patented by H.W. Pratt in 1872. But the first practical automatic sprinkler system was invented in 1874 by Henry S. Parmalee of New Haven, CT. He installed the system in a piano factory he owned.\n\nSince the early 1990s manufacturers have successfully developed safe and effective Halon alternatives. These include DuPont's FM-200, American Pacificâs Halotron, FirePro's FPC Compound and 3M's Novec 1230 Fire Protection Fluid. Generally, the Halon replacement agents available today fall into two broad categories, in-kind (gaseous extinguishing agents) or not in-kind (alternative technologies). In-kind gaseous agents generally fall into two further categories, halocarbons and inert gases. Not in-kind alternatives include such options as water mist or the use of early warning smoke detection systems.\n\nhttps://www.epa.gov/snap/substitutes-total-flooding-agents\n"}
{"id": "11064899", "url": "https://en.wikipedia.org/wiki?curid=11064899", "title": "Ballistic limit", "text": "Ballistic limit\n\nThe ballistic limit or limit velocity is the velocity required for a particular projectile to reliably (at least 50% of the time) penetrate a particular piece of material. In other words, a given projectile will generally not pierce a given target when the projectile velocity is lower than the ballistic limit. The term \"ballistic limit\" is used specifically in the context of armor; \"limit velocity\" is used in other contexts.\n\nThe ballistic limit equation for laminates, as derived by Reid and Wen is as follows:\n\nformula_1\nwhere\n\nAdditionally, the ballistic limit for small-caliber into homogeneous armor by TM5-855-1 is:\n\nformula_9<br>\nwhere\n\n"}
{"id": "44592416", "url": "https://en.wikipedia.org/wiki?curid=44592416", "title": "Band-gap engineering", "text": "Band-gap engineering\n\nBand-gap engineering is the process of controlling or altering the band gap of a material. This is typically done to semiconductors by controlling the composition of alloys or constructing layered materials with alternating compositions. A band gap is the range in a solid where no electron state can exist. The band gap of insulators is much larger than in semiconductors. Conductors or metals have a much smaller or nonexistent band gap than semiconductors since the valence and conduction bands overlap. Controlling the band gap allows for the creation of desirable electrical properties.\n\nMolecular-beam epitaxy is a technique used to construct thin epitaxial films of materials ranging from oxides to semiconductors to metals. Different beams of atoms and molecules in an ultra-high vacuum environment are shot onto a nearly atomically clean crystal, creating a layering effect. This is a type of thin-film deposition. Semiconductors are the most commonly used material due to their use in electronics. Technologies such as quantum well devices, super-lattices, and lasers are possible with MBE. Epitaxial films are useful due to their ability to be produced with electrical properties different from those of the substrate, either higher purity, or fewer defects or with a different concentration of electrically active impurities as desired. Varying the composition of the material alters the band gap due to bonding of different atoms with differing energy level gaps.\n\nSemiconducting materials are able to be altered with strain-inducing from tunable sizes and shapes due to quantum confinement effects. A larger tunable bandgap range is possible due to the high elastic limit of semiconducting nanostructures. Strain is the ratio of extension to original length, and can be used on the nanoscale.\n\nZnO Nanowires are used in nanogenerators, nanowire field effect transistors, piezo-electric diodes, and chemical sensors. Several studies have been conducted on the effect of strain on different physical properties. Sb-doped ZnO nanowires experience variation in resistance when exposed to strain. Bending strain can induce an increase in electrical conductance. Strain can also induce change of transport properties and band-gap variation. By correlating these two effects under experimentation the variation of transport properties as a function of band-gap can be generated. Electrical measurements are obtained using scanning tunnelling microscope-transmission electron microscope probing system.\n\nWhen lithographically generated graphene ribbons are laterally confined in charge it creates an energy gap near the charge neutrality point. The narrower the ribbons result in larger energy gap openings based on temperature dependent conductance. A narrow ribbon is considered a quasi one dimensional system in which an energy band gap opening is expected. Single sheets of graphene are mechanically extracted from bulk graphite crystals onto a silicon substrate and are contacted with Cr/Au metal electrodes. Hydrogen silsesquioxane is spun onto the samples to form an etch mask and then oxygen plasma is used to etch away the unprotected graphene.\n"}
{"id": "17828291", "url": "https://en.wikipedia.org/wiki?curid=17828291", "title": "Barkhausen stability criterion", "text": "Barkhausen stability criterion\n\nIn electronics, the Barkhausen stability criterion is a mathematical condition to determine when a linear electronic circuit will oscillate. It was put forth in 1921 by German physicist Heinrich Georg Barkhausen (1881â1956). It is widely used in the design of electronic oscillators, and also in the design of general negative feedback circuits such as op amps, to prevent them from oscillating.\n\nBarkhausen's criterion applies to linear circuits with a feedback loop. It cannot be applied directly to active elements with negative resistance like tunnel diode oscillators.\n\nThe kernel of the criterion is that a complex pole pair must be placed on the imaginary axis of the complex frequency plane if steady state oscillations should take place. In the real world, it is impossible to balance on the imaginary axis, so in practice a steady-state oscillator is a non-linear circuit:\n\n\nIt states that if \"A\" is the gain of the amplifying element in the circuit and Î²(\"j\"Ï) is the transfer function of the feedback path, so Î²\"A\" is the loop gain around the feedback loop of the circuit, the circuit will sustain steady-state oscillations only at frequencies for which:\n\nBarkhausen's criterion is a \"necessary\" condition for oscillation but not a \"sufficient\" condition: some circuits satisfy the criterion but do not oscillate. Similarly, the Nyquist stability criterion also indicates instability but is silent about oscillation. Apparently there is not a compact formulation of an oscillation criterion that is both necessary and sufficient.\n\nBarkhausen's original \"formula for self-excitation\", intended for determining the oscillation frequencies of the feedback loop, involved an equality sign: |Î²\"A\"| = 1. At the time conditionally-stable nonlinear systems were poorly understood; it was widely believed that this gave the boundary between stability (|Î²\"A\"| < 1) and instability (|Î²\"A\"| â¥ 1), and this erroneous version found its way into the literature. However, \"stable\" oscillations only occur at frequencies for which equality holds.\n\n"}
{"id": "3609568", "url": "https://en.wikipedia.org/wiki?curid=3609568", "title": "Bean bag chair", "text": "Bean bag chair\n\nA bean bag chair is a large fabric bag, filled with polystyrene beads (Sacco chair), dried beans, or a similar substance. The product is an example of an anatomic chair. The shape of the object is set by the user. Although designed to be a chair, due to its amorphous nature it is often confused with tuffets and ottomans.\n\nSacco, the first artifact of that kind, was introduced in 1968 by three Italian designers: Piero Gatti, Cesare Paolini and Franco Teodoro. The object was created in the Italian Modernism movement.\nBeing a post war era phenomenon, Italian modernismâs design was highly inspired with new available technology. Post war technology allowed an increase in the processes of production, by introducing new materials such as polystyrene. The idea of mass-produced goods made within an inexpensive price range appealed to consumers. It therefore created the need for a revolution in the creative and manufacturing process. âThe designer was an integral member of a process that included marketing as well as engineeringâ (Raizman 284). The inspiration left by Corradino DâAscanoâs Vespa design for the Piaggio Corporation in 1946, added value to the essence of the designer. With successful designs, brands could sell more products, and therefore the identity of the designer played an important advertising role.\nAnother important figure of the Italian modernism period was Gio Ponti. Inspired by modernism's art movements, Ponti created new forms of objects. His asymmetrically balanced designs freed the Italian objects form their classic representations. The designer promoted Italian designs on famous exhibitions called âMilan Triennaleâ : âThese exhibitions, organized as early as the 1920s â¦ were responsible for increasing the visibility of Italian design in an international setting â (Raizman 285). After becoming an editor of the Domus (magazine) in 1947, Ponti contributed to not only Italian design of that time, but also : âthe human and creative element in modern industrial design as well as its practical, economic and social benefits.â(Raizman 285)\n\nPiero Gatti, Cesare Paolini and Franco Teodoro, inspired by their designer predecessors, came up in 1968 with the design of Sacco the âshapeless chairâ. Although it was not the first design of an amorphous chair in Italian history, Sacco was the first successful product created in partnership with Zanotta. The predecessor of the product had a major design flaw of not being able to sustain its form and therefore never reached production. Sacco picked up that flaw and with the use of leather for exterior and right placed stitching. It is worth mentioning that the use of leather was not coincidental as at that time the textile was an Italian national pride product. The target user of the chair was the lax, hippie community and their non-conformist household. \"In an era characterized by the hippie culture, apartment sharing and student demonstrations, the thirty-something designers created a nonpoltrona (non-chair) and thus launched an attack on good bourgeois taste.\"\n\nOther designers have followed the âshapelessâ chair design, creating a range of inspired products that take after Sacco. Amongst many, the most successful contemporary model would be Jukka Setalaâs Fatboy. The product launched in 2002 brought the Finnish designer global recognition. The new form of the bean bag chair has less stitching and a more geometrical take in the means of shape. It also has (just like ALL bean bags) an EPS filling which is more durable than PVC that hardly anyone use.\n\n"}
{"id": "2359796", "url": "https://en.wikipedia.org/wiki?curid=2359796", "title": "Ceiling balloon", "text": "Ceiling balloon\n\nA ceiling balloon also called a pilot balloon or pibal, is used by meteorologists to determine the height of the base of clouds above ground level during daylight hours. In the past, and sometimes today, a theodolite was used to track the balloon in order to determine the speed and direction of winds aloft.\nThe principle behind the ceiling balloon is that timing of a balloon with a known ascent rate (how fast it climbs) from its release until it disappears into the clouds can be used to calculate the height of the bottom of the clouds.\n\nA ceiling balloon is a small, usually red, (fluted) rubber balloon commonly measuring 76 mm (3 in) across prior to inflation, inflated to ~40 cm (~15.75 in) diameter. After inflation the balloon is taken outside and released. By timing the balloon from release until it enters the cloud a ceiling height can be obtained. When correctly inflated the balloon will rise at rate of 140 m/min (460 ft/min). The bases of clouds are very rarely flat and solid, so the ceiling height is not when the balloon disappears but when the colour begins to fade. The balloon can also be used to measure the vertical visibility into a layer of fog or blowing snow. In this case the balloon will begin to fade as soon as it is released, so the vertical visibility is when the balloon disappears. If the balloon is visible for a considerable distance into the cloud layer the observer should make note of it as it is of importance to aircraft.\n\nThe ceiling balloon is a reliable, safe and simple way to get an indication of the height of clouds. However, it does suffer from some disadvantages that the observer must be aware of. Rain and wet snow may slow the ascent of the balloon, giving a falsely high ceiling and high winds and poor visibility may cause the balloon to appear to enter the cloud before it actually does. As the balloon rises at a rate of 140 m/min (460 ft/min) it will take over five minutes for the balloon to reach 700 m (2300 ft). Beyond this height the ability to follow the balloon, even with binoculars, is poor, as even the slightest movement of the eye off the balloon will almost certainly ensure that it vanishes.\n\nAt night when it is not practical to use a balloon the ceiling projector is used. However, during twilight it may be impossible to use the ceiling projector and then a pibal (pilot balloon) light may be used. This is a simple flashlight bulb attached to a battery. To charge the battery it is immersed in water for three minutes and then tied to the balloon prior to inflation. These are rarely used today.\n\nThe balloons and associated equipment are usually stored in a cabinet mounted on a wall close to the gas cylinders. The cabinet has three doors one of which opens down and to it the filler stand is attached. At the top of the filler stand is a \"L\" shaped pipe with two rings, a small one on the bottom and a larger one on the top called the inflation nozzle. The rings stop the tube from dropping through the stand or rising too far when the balloon is inflated. The top ring has several grooves cut into it, to help grip the balloon which is fitted to it.\n\nAt the bottom of the pipe is a weight that, when the precise amount of gas has been added, will lift to indicate the balloon is full. A rubber hose is attached to this pipe and passes through the filler stand twice. The first hole is larger than the tube to permit movement, while the second is used to hold the tubing in place.\n\nFrom there the tube runs to a needle valve that controls the amount of gas flowing to the balloon. A second tube will then run from the valve to a regulator valve that is attached to the gas cylinder. This valve has two pressure gauges attached. One showing the total pressure remaining in the gas cylinder and the second showing the amount of gas flowing through the tubing. Typically the cylinder, which is made of steel and weighs about 140 lb (65 kg). It contains the equivalent of about 200 ftÂ³ (5.7 mÂ³) of gas at standard pressure, stored at a pressure of 2000 psi (14 megapascals) and will inflate approximately 120 balloons (according to a nominal diameter of 45 cm).\n\nOn the opposite side of the cabinet is space to store balloons, string and pibal lights. The gas used to fill the balloon is helium or hydrogen. Because of its low cost ceiling balloons are often filled with hydrogen gas, but sometimes helium is used.\n\nThe balloon is attached to the inflation nozzle and a piece of string is wound around the neck. After donning safety glasses and hearing protection a check is made to ensure the needle valve is fully closed. The main valve on the cylinder is then opened, followed by the regulator valve. Next, the needle valve is opened and the balloon begins to inflate. As the balloon reaches the correct size the inflation nozzle will begin to lift. At this point the needle valve is closed along with the regulator valve and cylinder vale. The string is then used to tie off the balloon neck to ensure that no gas can escape.\n\nCaution must be used during inflation due to the occasional faulty balloon and its failure. If the person inflating the balloon is not wearing goggles or hearing protectors then eye or ear damage can result.\n\n\n"}
{"id": "4061565", "url": "https://en.wikipedia.org/wiki?curid=4061565", "title": "Charge sharing", "text": "Charge sharing\n\nIn digital electronics, charge sharing is an undesirable signal integrity phenomenon observed most commonly in the Domino logic family of digital circuits. The charge sharing problem occurs when the charge which is stored at the output node in the precharge phase is shared among the output or junction capacitances of transistors which are in the evaluation phase. Charge sharing may degrade the output voltage level or even cause erroneous output value\n"}
{"id": "43042840", "url": "https://en.wikipedia.org/wiki?curid=43042840", "title": "Claims to the first powered flight", "text": "Claims to the first powered flight\n\nSeveral aviators have been claimed as the first to fly a powered aeroplane. \n\nMuch controversy has surrounded these claims. It is most widely held today that the Wright Brothers were the first to fly successfully. Brazil regards Santos-Dumont as the first successful aviator because the Wright Flyer took off from a rail and, after 1903, used a catapult. An editorial in the influential \"Jane's All the World's Aircraft\" 2013 edition supported the claim of Gustave Whitehead.\n\nSeveral aviators and/or their supporters have laid claim to the first manned flight in a powered aeroplane. Claims that have received significant attention include:\n\nIn judging these claims, the generally accepted requirements are for sustained powered and controlled flight. In 1890 Ader had made a brief uncontrolled and unsustained \"hop\" in his Ãole, but such a hop is not regarded as true flight. The ability to take off unaided is also sometimes regarded as necessary. The air historian Charles Gibbs-Smith has said that, \"The criteria of powered flight must remain to some extent a matter of opinion.\"\n\nSome notable powered hops were made before the problem of powered flight was finally solved.\n\nIn 1874 FÃ©lix du Temple built a steam-powered aeroplane which took off from a ramp with a sailor on board and remained airborne for a short distance. This has sometimes been claimed as the first powered flight in history but the claim is generally rejected because takeoff was gravity-assisted and flight was not sustained. It is however credited as the first powered take-off in history.\n\nTen years later in 1884 the Russian Alexander Mozhaysky achieved similar success, launching his craft from a ramp and remaining airborne for 30Â m (98Â ft). The claim that this was a sustained flight has not been taken seriously outside Russia.\n\nClÃ©ment Ader's Ãole of 1890 was a bat-winged tractor monoplane which achieved a brief, uncontrolled hop, becoming the first heavier-than-air machine in history to take off from level ground under its own power. However his hop is not regarded as true flight because it was neither sustained nor controlled. Ader would later make a false claim of a more extended flight.\n\nIn 1896, Samuel Pierpont Langley, the Secretary of the Smithsonian Institution, had successful flights with several unpiloted models, and his \"Number 6\" model flew more than . From these successes, in 1898 Langley received grants from the War Department for $50,000 and an additional $20,000 from the Smithsonian, to attempt development of a piloted version.\n\nThe 1901 Drachenflieger of Wilhelm Kress did not even achieve a hop. A floatplane, it is notable as the first heavier-than-air craft to be powered by an internal combustion engine rather than steam. It demonstrated good control when taxiing, but was too underpowered to take off.\n\nFew of the claims to powered flight were widely accepted, or even made, at the time the events took place. Both the Wrights and Whitehead suffered in their early years from a lack of general recognition, while neither Ader nor Langley made any claim in the years immediately following their work.\n\nThe pioneer Octave Chanute promoted the Wrights' work, some of which he witnessed, in America and Europe. The brothers began to gain recognition in Britain, where Colonel John Edward Capper was taking charge of Army aeronautical work. On a visit to the U.S. in 1904 Capper befriended the Wrights and subsequently helped foster their early recognition. He also visited Langley, who openly discussed his failure with Capper.\n\nThe perspective changed in 1906. Early in the year Langley died, without ever making any claim. The U.S. Army rejected a proposal from the Wrights on the basis that their machine's ability to fly had not been demonstrated. Thus, when Alberto Santos-Dumont made a brief flight that year in his 14-Bis aeroplane, there was no acknowledged antecedent and he was acclaimed in France and elsewhere as the first to fly. Ader responded by claiming that he had flown in his Avion III back in 1897.\n\nSantos-Dumont's 1906 claim to flight has never been seriously disputed, although few authorities credit his as the first, while some have questioned the effectiveness of his controls.\n\nIn 1908 the Wrights embarked on a series of demonstrations with their by now much improved Flyers, Orville in America and Wilbur in Europe. The European demonstrations drew instant recognition of their technical achievements, although both the use of a launch trolley and lack of aerodynamic stability meant that acceptance of the first flight claim was not universal.\n\nAder's claim was debunked in 1910, when the official report on his work was finally published.\n\nIn America the Smithsonian Institution began to promulgate a claim for Langley, who had been its Secretary from 1887 until his death. In 1914 they loaned the Aerodrome to Glenn Curtiss who modified it heavily until it became capable of brief flights. They used this as the basis for a claim that the Aerodrome was the first aeroplane \"capable of flight\". Orville, the surviving Wright brother, began a long and bitter legal battle to force it to recognize their claim to primacy, his disgust reaching such a peak that in 1928 he sent the historic Flyer for display in the British Science Museum in London. He eventually succeeded in his battle when the Smithsonian admitted to Curtiss' modifications and withdrew its claim in 1942.\n\nMeanwhile, with the publication of a co-authored article and a book by the journalist Stella Randolph, Whitehead had also begun to gain vocal advocates. In 1945 Orville Wright issued a critique of the evidence for Whitehead.\n\nOrville died on January 30, 1948. As part of the Smithsonian's final deal with his executors, the Flyer was returned to its native America and put on display. A clause in the contract required the Smithsonian to claim primacy for the Wrights, on pain of losing the prize exhibit.\n\nWhitehead's advocates have fought an equally bitter battle with the Smithsonian in an attempt to gain recognition, in which the Smithsonian's contract with the Wright estate has been subject to heavy criticism. Although Whitehead's advocates have gained some following, they remain in the minority.\n\nIt is most widely held today that the Wright Brothers were the first to make controlled sustained heavier-than-air flights. Brazil still officially regards Santos-Dumont as the first successful aviator because his craft did not use an external launch system. \n\nFollowing his hop in the Ãole, ClÃ©ment Ader obtained funding from the French Ministry for War. He completed the similar but larger Avion III in 1897. In trials it failed to leave the ground at all, and remains notable only for its twin steam engines.\n\nSeveral years later in 1906, following BlÃ©riot's first successful flights Ader claimed publicly that his \"Ãole\" had flown for 330Â ft (100 m) in 1891, and that the \"Avion III\" had flown for 1,000Â ft on the second day of its trials.\n\nAder's claim for the \"Avion III\" was refuted four years later in 1910, when the French Ministry for War finally published its report on his work. His claim for the \"Ãole\" also collapsed through lack of credibility.\n\nWhen Gustav Weisskopf immigrated to the United States he changed his name to Gustave Whitehead. There he began an extensive series of experiments with gliders, aero engines and motorized flying machines. Whitehead and other sources have claimed he made successful powered airplane flights.\n\n\nStanley Beach was a friend of Whitehead, sharing a patent application with him in 1905. Beach's father was editor of Scientific American, which was initially sympathetic to Whitehead and included a number of reports of short flights by Whitehead. Later, Beach and Whitehead fell out and, now editor of the Scientific American in his turn, Beach denied that Whitehead ever flew.\n\nWhitehead's claims were not taken seriously until two journalists, Stella Randolph and Harvey Phillips, wrote an article in a 1935 edition of \"Popular Aviation\" journal. Harvard University economics professor John B. Crane responded with a rebuttal, published in \"National Aeronautic Magazine\" in December 1936. The next year Randolph expanded the article, together with additional research, into a book titled \"Lost flights of Gustave Whitehead.\" Crane changed his mind in 1938 and suggested that a Congressional investigation should consider the claims. By 1945 Orville Wright was sufficiently concerned about the Whitehead claims that he issued his own rebuttal in \"US Air Services\". Then in 1949 Crane published a new article in \"Air Affairs\" magazine that supported Whitehead.\n\nFollowing a chance discovery in 1963, reserve U.S. Air Force major William O'Dwyer was asked to research into Whitehead. He became convinced that Whitehead did fly and contributed research material to a second book by Stella Randolph, \"The Story of Gustave Whitehead, Before the Wrights Flew\", published in 1966.\n\nO'Dwyer and Randolph co-authored another book, \"History by Contract\", published in 1978. The book criticised the Smithsonian Institution for its contracted obligation to credit only the 1903 Wright Flyer for the first powered controlled flight, claiming that it created a conflict of interest and had been kept secret. The Smithsonian defended itself vigorously.\n\nOn 8 March 2013 the aviation annual \"Jane's All the World's Aircraft\" published an editorial by Paul Jackson endorsing the Whitehead claim. Jackson's editorial drew heavily upon information provided by aviation researcher John Brown, whom he complimented for his work. Brown had begun researching Whitehead's aircraft while working as a contractor for the Smithsonian. \"Jane's\" corporate owner later distanced itself from the editorial, stating it contained the views of the editor, not the publisher.\n\nBrown analysed in great detail a vintage photograph of an early indoor aeronautical exhibition, claiming that an even earlier photograph visible on the wall showed the Whitehead No. 21 aircraft in powered flight. Jackson's editorial did not mention the claim, and two weeks after its publication he stated to the press that the image was not significant to the claim of Whitehead's flight, remarking, \"And that entirely spurious 'Where's the photograph?' argument.\". Aviation historian Carroll Gray subsequently identified the photo seen on the wall \"beyond any reasonable doubt\" as a glider built and displayed in California by aviation pioneer John J. Montgomery.\n\nResponding to the renewed controversy, Tom Crouch, a senior curator at the Smithsonian's National Air and Space Museum, formally acknowledged the Wright contract, saying that it had never been a secret. He also stated;\n\n\"Scientific American\" published a rebuttal of the Whitehead claims written by its senior copy editor Daniel C. Schlenoff, who asserted, regarding the \"Bridgeport Herald\" report, that \"The consensus on the article is that it was an interesting work of fiction.\" \n\nThirty-eight air historians and journalists also responded to the revived controversy with a \"Statement Regarding The Gustave Whitehead Claims of Flight,\" rejecting the evidence supporting the claims and noting that, \"When it comes to the case of Gustave Whitehead, the decision must remain, not proven.\"\n\nSamuel Pierpont Langley was Secretary to the Smithsonian Institution from 1887 until the year of his death in 1906. During this period, and in due course supported by the United States War Department, he conducted aeronautical experiments, culminating in his manned Aerodrome A. Under Langley's instruction Charles M. Manly attempted to fly the craft from a catapult on the roof of a houseboat in 1903. Two attempts, on 7 October and 8 December, both failed with Manley receiving a soaking each time.\n\nSome ten years later in 1914 Glenn Curtiss modified the Aerodrome and flew it a few hundred feet, as part of his attempt to fight a patent owned by the Wright brothers, and as an effort by the Smithsonian to rescue Langley's aeronautical reputation. The Curtiss flights emboldened the Smithsonian to display the Aerodrome in its museum as \"the first man-carrying aeroplane in the history of the world capable of sustained free flight\". Fred Howard, extensively documenting the controversy, wrote: \"It was a lie pure and simple, but it bore the imprimatur of the venerable Smithsonian and over the years would find its way into magazines, history books, and encyclopedias, much to the annoyance of those familiar with the facts.\"\n\nThe Smithsonian's action triggered a decades-long feud with the surviving Wright brother, Orville. It was not until 1942 that the Smithsonian finally relented, publishing the Aerodrome modifications made by Curtiss and recanting misleading statements it had made about the 1914 tests.\n\nOn 17 December 1903 a few miles south of Kitty Hawk, North Carolina the Wright brothers launched their airplane from a dolly running along a short rail, which was laid on level ground. Taking turns, Orville and Wilbur made four brief flights at an altitude of about ten feet each time. The flight paths were all essentially straight; turns were not attempted. Each flight ended in a bumpy and unintended \"landing\" on the undercarriage skids, or runners as the craft did not have wheels. The last flight, by Wilbur, was 852 feet (260 m) in 59 seconds, much longer than each of the three previous flights of 120, 175 and 200 feet. The Flyer moved forward under its own engine power and was not assisted by catapult, a device the brothers did use during flight tests in the next two years and at public demonstrations in the U.S. and Europe in 1908-1909. A headwind averaging about 20Â mph gave the machine sufficient airspeed to become airborne; its speed over the ground was less than 10Â mph. Photographs were taken of the machine in flight.\n\nThe Wrights kept detailed logs and diaries about their work. Their correspondence with Octave Chanute provides a virtual history of their efforts to invent a flying machine. They also documented their work in photographs, although they did not make public any photos of their powered flights until 1908. Their written records also were not made available to the public at the time, though they were published in 1953 after the Wright estate donated them to the U.S. Library of Congress.\n\nThe Wrights' claim to a historic first flight was largely accepted by U.S. newspapers but inaccurately reported initially. In January 1904 they issued a statement to newspapers accurately describing the flights. After an announced public demonstration in May 1904 in Dayton, the Wrights made no further effort to publicize their work, and were advised by their patent attorney to keep details of their machine confidential. In 1905 a few dozen people witnessed flights by the Wright Flyer III. Pioneers such as Octave Chanute and the British Army officer Lt. Col. John Capper were among those who believed the Wrights' public and private statements about their flights.\n\nIn 1906, almost three years after the first flights, the U.S. Army rejected an approach from the Wrights on the basis that their proposed machine's ability to fly had not been demonstrated.\n\nBy 1907 the Wrights' claims were accepted widely enough for them to be in negotiations with Britain, France and Germany as well as their own government, and early in 1908 they concluded contracts with both the US War Department and a French syndicate. In May Wilbur sailed for Europe in order to carry out acceptance trials for the French contract. On 4 July 1908 Glenn Curtiss gave the first widely publicised public demonstration of flight in the USA. As a result, the Wright brothers' prestige fell. Subsequent flights of both brothers that year went on to astonish the world and their early claims gained almost universal public recognition as legitimate.\n\nSubsequent criticisms of the Wrights have included accusations of secrecy before coming to Europe in 1908, the use of a catapult-assisted launch and such a lack of aerodynamic stability as to make the machine almost unflyable. \n\nFollowing the Curtiss experiments with the Langley Aerodrome in 1914, surviving Wright brother Orville began a long and bitter campaign against the Smithsonian to gain recognition. His disgust reached such a peak in 1928 that he sent the historic Flyer for display in the British Science Museum in London. It was not until 1942 that the Smithsonian finally relented, at last retracting its claims for Langley and acknowledging the Wrights' place in history.\n\nOrville died on January 30, 1948. The Flyer - delayed by the war and an arrangement for a copy to be made - was returned to its native America and put on display in the Smithsonian. A clause in the contract required the Smithsonian to claim primacy for the Wrights, at risk of losing their newly acquired prize exhibit. The Smithsonian has honoured its contract ever since and continues to support the Wrights' claim.\n\nAlberto Santos-Dumont was a Brazilian aviation pioneer of French ancestry. Having emigrated to France for studies, he made his aeronautical name in that country with airships before turning to heavier-than-air flight. On 23 October 1906 he flew his 14-bis biplane for a distance of 60Â metres (197Â ft) at a height of about five meters or less (15Â ft). The flight was officially observed and verified by the AÃ©ro-Club (later renamed the AÃ©ro-Club de France). This won Santos-Dumont the Deutsch-Archdeacon Prize for the first officially-observed flight of more than 25Â meters. This flight is also recognised as the first powered flight in Europe. Then on 12 November a flight of 22.2 seconds carried the 14-bis some 220Â m (722Â ft), earning the AÃ©ro-Club prize of 1,500 francs for the first flight of more than 100 m. This flight was also observed by the newly formed FÃ©dÃ©ration AÃ©ronautique Internationale (FAI) and became the first record in their log book.\n\nThe lateral control system comprised ailerons mounted between the wings and attached to a harness worn by the pilot, who was intended to correct any rolling movement by leaning in the opposite direction. Both flights ended when the craft began to roll and Santos-Dumont landed because he was unable to correct it. This has led some critics to question the lateral control capability of the 14-bis, however it has not prevented the flight from being recognised. At that time the Wrights' claim had not yet been accepted in Europe and was questioned in America by the authoritative \"Scientific American\". Thus, Santos-Dumont was credited by many with the first powered flight.\n\nIn 1894 Sir Hiram Maxim tested a flying machine running on a track and held down by safety rails because it lacked adequate flight control. The machine lifted off the track and met the safety rails and this is sometimes claimed as a flight. Maxim himself never made such a claim.\n\nJohn Hall, of Springfield, Massachusetts was credited, alongside Whitehead, with flights prior to the Wrights by Crane in his 1947 \"Air Affairs\" magazine article.\n\nPreston A. Watson was subject of a claim made in 1953 by his brother J.Y. Watson, that he had flown before the Wrights in 1903. J.Y. Watson later admitted that this was in an unpowered glider.\n\nRichard Pearse of New Zealand is credited by some in his country with making the first powered airplane flight on 31 March 1903. He did not claim the feat himself and gave ambiguous information about the chronology of his work. Biographer Geoffrey Rodliffe wrote, \"no responsible researcher has ever claimed that he achieved fully controlled flight before the Wright brothers, or indeed at any time\".\n\nKarl Jatho of Germany is generally credited with making powered airborne hops in Hanover between August and November 1903. He claimed a hop of about high on August 18, 1903 and several more hops or short flights by November 1903 for distances up to at height. His aircraft made takeoffs from level ground. In Germany some enthusiasts credit him with making the first airplane flight. Sources differ whether his aircraft was controlled.\n\nJacob Ellehammer made a powered hop of about 138Â ft (42 m) on 12 September 1906. This has been claimed as a flight. Ellehammer's attempt was not officially observed, whereas Santos-Dumont's, only a few weeks later, was observed and has been given primacy.\n\n\n"}
{"id": "6034085", "url": "https://en.wikipedia.org/wiki?curid=6034085", "title": "Colloid thruster", "text": "Colloid thruster\n\nA colloid thruster (or \"electrospray thruster\") is a type of low thrust electric propulsion rocket engine that uses electrostatic acceleration of charged liquid droplets for propulsion. In a colloid thruster, charged liquid droplets are produced by an electrospray process and then accelerated by a static electric field. The liquid used for this application tends to be a low-volatility ionic liquid.\n\nLike other ion thrusters, its benefits include high efficiency, thrust density, and specific impulse; however it has very low total thrust, on the order of micronewtons. It provides very fine attitude control or efficient acceleration of small spacecraft over long periods of time.\n\nEight electrospray thrusters were first used in space on the NASA ST-7 ESA LISA Pathfinder mission, to demonstrate disturbance reduction. Having logged roughly 1,400 hours on-orbit, the Busek built thrusters system met 100% of their mission goals for the LISA Pathfinder mission.\nBy the end of April 2015, Busek had developed a smaller electrospray colloid thruster capable of generating 20Â mN in a 17.8 x 17.8 x 4.3Â cm (7\"Ã7\"Ã1.7\") package.\n\nIn July 2013, scientists from Michigan Technological University and the University of Maryland led by Kurt Terhune demonstrated an electrospray system within a transmission electron microscope (TEM). This led to the discovery that the TEM environment formed needle-like structures on the thruster disrupting the way the electrospray system works.\n\nThe SkyFire nanosatellite, to be launched in 2019 for a lunar flyby, will demonstrate the use of this propulsion system.\n\n\n"}
{"id": "39937914", "url": "https://en.wikipedia.org/wiki?curid=39937914", "title": "Convention on Early Notification of a Nuclear Accident", "text": "Convention on Early Notification of a Nuclear Accident\n\nThe Convention on Early Notification of a Nuclear Accident is a 1986 International Atomic Energy Agency (IAEA) treaty whereby states have agreed to provide notification of any nuclear accident that occur within its jurisdiction that could affect other states. It, along with the Convention on Assistance in the Case of a Nuclear Accident or Radiological Emergency, was adopted in direct response to the April 1986 Chernobyl disaster.\n\nBy agreeing to the Convention, a state acknowledges that when any nuclear or radiation accident occurs within its territory that has the potential of affecting another state, it will promptly notify the IAEA and the other states that could be affected. The information to be reported includes the incident's time, location, and the suspected amount of radioactivity release.\n\nThe Convention was concluded and signed at a special session of the IAEA general conference on 26 September 1986; the special session was called because of the Chernobyl disaster, which had occurred five months before. Significantly, the Soviet Union and the Ukrainian SSRâthe states that were responsible for the Chernobyl disasterâboth signed the treaty at the conference and quickly ratified it. It was signed by 69 states and the Convention entered into force on 27 October 1986 after the third ratification.\n\nAs of 2015, 119 state parties have ratified the Convention; the European Atomic Energy Community, the Food and Agriculture Organization, the World Health Organization, and the World Meteorological Organization have also entered into the Convention. The states that have ratified the Convention but have since denounced it and withdrawn from the agreement are Bulgaria, Hungary, Mongolia, and Poland. The states that have signed the Convention but not ratified it are Bahamas, CÃ´te d'Ivoire, Democratic Republic of the Congo, Holy See, Niger, North Korea, Sierra Leone, Sudan, Syria, and Zimbabwe. \n\n\n"}
{"id": "41540019", "url": "https://en.wikipedia.org/wiki?curid=41540019", "title": "Defense Treaty Ready Inspection Readiness Program", "text": "Defense Treaty Ready Inspection Readiness Program\n\nThe Defense Treaty Ready Inspection Readiness Program (DTIRP) is a United States Department of Defense (DoD) security preparedness and outreach program designed to provide security education and awareness concerning the operational activities associated with arms control implementation. DTIRP uses specially trained personnel, analyses, and educational activities to provide arms control implementation advice and assistance to facilities subject to on-site inspection activities and observation overflights.\n\nThe authority for the DTIRP outreach program is provided by Department of Defense Instruction (USDI) 5205.10, Department of Defense Treaty Inspection Readiness Program (DTIRP). This Directive states the purpose of the DTIRP outreach program as being to ensure the continued protection of Department of Defense (DoD) programs, operations, and activities by providing security preparedness education, assistance, and advice concerning the wide range of operational activities associated with implementing arms control treaties and agreements.\n\nThe precursor to the DTIRP outreach program was the Defense Contractor Inspection Readiness Program (DCIRP). DCIRP was established in August 1990 by the Deputy Under Secretary of Defense for Security Policy to assist the defense contractor community with developing and implementing effective security countermeasures at facilities subject to inspection under the Strategic Arms Reduction Treaty (START).\n\nIn the fall of 1991, DCIRP was reorganized, renamed \"DTIRP,\" and transferred to the Assistant Secretary of Defense for Command, Control, Communications, and Intelligence [ASD(C3I)]. The new name, Defense Treaty Inspection Readiness Program (DTIRP), reflected the expansion of DTIRP's mission to include DoD facilities, in addition to defense contractor facilities, and to include other arms control treaties and agreements in addition to START.\n\nAlthough the scope of DTIRP's mission expanded, its focus remained the same:\n\n\n - official website is offline\n"}
{"id": "10339", "url": "https://en.wikipedia.org/wiki?curid=10339", "title": "Electrochemical cell", "text": "Electrochemical cell\n\nAn electrochemical cell is a device capable of either generating electrical energy from chemical reactions or using electrical energy to cause chemical reactions. The electrochemical cells which generate an electric current are called voltaic cells or galvanic cells and the other ones are called electrolytic cells which are used to drive chemical reactions like electrolysis. A common example of an galvanic cells is a standard 1.5 volt cell meant for consumer use. A \"battery\" consists of one or more cells, connected either in parallel, series or series-and-parallel pattern.<r: A cell is a device which can convert chemical energy into electrical energy. A simple cell does this by using a combination of two metals (electrodes) and an electrolyte solution (which could be acid/alkali/salt solution).\nMetals can conduct electricity due to metallic bonding. Metallic bonds are the electrostatic force of attraction between negatively charged delocalised electrons and the positively charged metal ions. Metal atoms are surrounded by a âseaâ of delocalised electrons. This means they are free to move within the structure- this gives metals its conducting abilities as an electric current can flow through it. \nA reaction between metals where electrons are gained is called a reduction reaction\nA reaction where electrons are lost is called a oxidation reaction \nA reaction where both reduction and oxidation occur is called a redox reaction. A redox reaction occurs when electrons are transferred from a substance that is oxidized to one that is being reduced.\nef></ref>\n\nAn electrolytic cell is an electrochemical cell that drives a non-spontaneous redox reaction through the application of electrical energy. They are often used to decompose chemical compounds, in a process called electrolysisâthe Greek word lysis means \"to break up\".\n\nImportant examples of electrolysis are the decomposition of water into hydrogen and oxygen, and bauxite into aluminium and other chemicals. Electroplating (e.g. of copper, silver, nickel or chromium) is done using an electrolytic cell. Electrolysis is a technique that uses a direct electric current (DC).\n\nAn electrolytic cell has three component parts: an electrolyte and two electrodes (a cathode and an anode). The electrolyte is usually a solution of water or other solvents in which ions are dissolved. Molten salts such as sodium chloride are also electrolytes. When driven by an external voltage applied to the electrodes, the ions in the electrolyte are attracted to an electrode with the opposite charge, where charge-transferring (also called faradaic or redox) reactions can take place. Only with an external electrical potential (i.e. voltage) of correct polarity and sufficient magnitude can an electrolytic cell decompose a normally stable, or inert chemical compound in the solution. The electrical energy provided can produce a chemical reaction which would not occur spontaneously otherwise.\n\nA galvanic cell, or voltaic cell, named after Luigi Galvani, or Alessandro Volta respectively, is an electrochemical cell that derives electrical energy from spontaneous redox reactions taking place within the cell. It generally consists of two different metals connected by a salt bridge, or individual half-cells separated by a porous membrane.\n\nVolta was the inventor of the voltaic pile, the first electrical battery. In common usage, the word \"battery\" has come to include a single galvanic cell, but a battery properly consists of multiple cells.\n\nA primary cell is a Galvanic battery that is designed to be used once and discarded, and not recharged with electricity and reused like a secondary cell (rechargeable battery). In general, the electrochemical reaction occurring in the cell is not reversible, rendering the cell unrechargeable. As a primary cell is used, chemical reactions in the battery use up the chemicals that generate the power; when they are gone, the battery stops producing electricity and is useless. In contrast, in a secondary cell, the reaction can be reversed by running a current into the cell with a battery charger to recharge it, regenerating the chemical reactants. Primary cells are made in a range of standard sizes to power small household appliances such as flashlights and portable radios. \n\nPrimary batteries make up about 90% of the $50 billion battery market, but secondary batteries have been gaining market share. About 15 billion primary batteries are thrown away worldwide every year, virtually all ending up in landfills. Due to the toxic heavy metals and strong acids or alkalis they contain, batteries are hazardous waste. Most municipalities classify them as such and require separate disposal. The energy needed to manufacture a battery is about 50 times greater than the energy it contains. Due to their high pollutant content compared to their small energy content, the primary battery is considered a wasteful, environmentally unfriendly technology. Due mainly to increasing sales of wireless devices and cordless tools which cannot be economically powered by primary batteries and come with integral rechargeable batteries, the secondary battery industry has high growth and has slowly been replacing the primary battery in high end products. \n\nA secondary cell, commonly referred to as a \"rechargeable battery\" is an electrochemical cell that can be run as both a galvanic cell or as an electrolytic cell. This is used as a convenient way to store electricity, when current flows one way the levels of one or more chemicals build up (charging), while it is discharging they reduce and the resulting electromotive force can do work.\n\nA \"fuel cell\" is an electrochemical cell that converts the chemical energy from a fuel into electricity through an electrochemical reaction of hydrogen fuel with oxygen or another oxidizing agent. Fuel cells are different from batteries in requiring a continuous source of fuel and oxygen (usually from air) to sustain the chemical reaction, whereas in a battery the chemical energy comes from chemicals already present in the battery. Fuel cells can produce electricity continuously for as long as fuel and oxygen are supplied.\n\nThe first fuel cells were invented in 1838. The first commercial use of fuel cells came more than a century later in NASA space programmes to generate power for satellites and space capsules. Since then, fuel cells have been used in many other applications. Fuel cells are used for primary and backup power for commercial, industrial and residential buildings and in remote or inaccessible areas. They are also used to power fuel cell vehicles, including forklifts, automobiles, buses, boats, motorcycles and submarines.\n\nThere are many types of fuel cells, but they all consist of an anode, a cathode, and an electrolyte that allows positively charged hydrogen ions (protons) to move between the two sides of the fuel cell. At the anode a catalyst causes the fuel to undergo oxidation reactions that generate protons (positively charged hydrogen ions) and electrons. The protons flow from the anode to the cathode through the electrolyte after the reaction. At the same time, electrons are drawn from the anode to the cathode through an external circuit, producing direct current electricity. At the cathode, another catalyst causes hydrogen ions, electrons, and oxygen to react, forming water. Fuel cells are classified by the type of electrolyte they use and by the difference in startup time ranging from 1 second for proton exchange membrane fuel cells (PEM fuel cells, or PEMFC) to 10 minutes for solid oxide fuel cells (SOFC). A related technology is flow batteries, in which the fuel can be regenerated by recharging. Individual fuel cells produce relatively small electrical potentials, about 0.7 volts, so cells are \"stacked\", or placed in series, to create sufficient voltage to meet an application's requirements. In addition to electricity, fuel cells produce water, heat and, depending on the fuel source, very small amounts of nitrogen dioxide and other emissions. The energy efficiency of a fuel cell is generally between 40â60%; however, if waste heat is captured in a cogeneration scheme, efficiencies up to 85% can be obtained.\n\nThe fuel cell market is growing, and in 2013 Pike Research estimated that the stationary fuel cell market will reach 50 GW by 2020.\n\nAn electrochemical cell consists of two half-cells. Each \"half-cell\" consists of an electrode and an electrolyte. The two half-cells may use the same electrolyte, or they may use different electrolytes. The chemical reactions in the cell may involve the electrolyte, the electrodes, or an external substance (as in fuel cells that may use hydrogen gas as a reactant). In a full electrochemical cell, species from one half-cell lose electrons (oxidation) to their electrode while species from the other half-cell gain electrons (reduction) from their electrode.\n\nA \"salt bridge\" (e.g., filter paper soaked in KNO NaCl, or some other electrolyte) is often employed to provide ionic contact between two half-cells with different electrolytes, yet prevent the solutions from mixing and causing unwanted side reactions. An alternative to a salt bridge is to allow direct contact (and mixing) between the two half-cells, for example in simple electrolysis of water.\n\nAs electrons flow from one half-cell to the other through an external circuit, a difference in charge is established. If no ionic contact were provided, this charge difference would quickly prevent the further flow of electrons. A salt bridge allows the flow of negative or positive ions to maintain a steady-state charge distribution between the oxidation and reduction vessels, while keeping the contents otherwise separate. Other devices for achieving separation of solutions are porous pots and gelled solutions. A porous pot is used in the Bunsen cell (right).\n\nEach half-cell has a characteristic voltage. Various choices of substances for each half-cell give different potential differences. Each reaction is undergoing an equilibrium reaction between different oxidation states of the ions: When equilibrium is reached, the cell cannot provide further voltage. In the half-cell that is undergoing oxidation, the closer the equilibrium lies to the ion/atom with the more positive oxidation state the more potential this reaction will provide. Likewise, in the reduction reaction, the closer the equilibrium lies to the ion/atom with the more \"negative\" oxidation state the higher the potential.\n\nThe cell potential can be predicted through the use of electrode potentials (the voltages of each half-cell). These half-cell potentials are defined relative to the assignment of 0 volts to the standard hydrogen electrode (SHE). (See table of standard electrode potentials). The difference in voltage between electrode potentials gives a prediction for the potential measured. When calculating the difference in voltage, one must first rewrite the half-cell reaction equations to obtain a balanced oxidation-reduction equation.\n\n\nCell potentials have a possible range of roughly zero to 6 volts. Cells using water-based electrolytes are usually limited to cell potentials less than about 2.5 volts due to high reactivity of the powerful oxidizing and reducing agents with water that is needed to produce a higher voltage. Higher cell potentials are possible with cells using other solvents instead of water. For instance, lithium cells with a voltage of 3 volts are commonly available.\n\nThe cell potential depends on the concentration of the reactants, as well as their type. As the cell is discharged, the concentration of the reactants decreases and the cell potential also decreases.\n\n"}
{"id": "14331646", "url": "https://en.wikipedia.org/wiki?curid=14331646", "title": "European Competitive Telecommunications Association", "text": "European Competitive Telecommunications Association\n\nEstablished in 1998, ECTA is the leading pan-European telecoms association promoting market liberalisation and competition in the European communications sector, fostering âcompetition and open accessâ and developing policy by representing ânew entrantâ interests to European institutions and Government bodies. ECTA seeks to create confidence for investors through clear and consistent regulation to unlock the growth potential of Europeâs businesses.\n\nECTA represents more than 100 companies, including leaders in the following market segments throughout Europe:\n\n\n\nECTA members are some of the primary innovators in implementation of advanced telecommunications and broadband services\n\nThe European Competitive Telecommunications Association was formed in 1998 by Robert J. Dombkowski and Elizabeth J. Schumacher, founders of MCN, UK, who collectively brought together a group of telecommunications resellers and suppliers in Europe in order to push for deregulation of the telecommunications market across Western and Eastern Europe. The founding members included high ranking executives of the major telecom players in Europe including Nick Jeffries of Cable and Wireless, Michael Potter venture capitalist of Paradigm Ventures specialized in telecom start-ups and film director of Orphans of Apollo, Hans Gerber of SITA-Equant, Gustav Schaefer of Unisource Carrier Services, Ian Ashby of Lucent Technologies, Claude Simpson, the president of Immix Telecom (ECTA Member of the Year, 1999), Holland Taylor of USA GlobalLink, and Michael Rhodes of Coudert Brothers.\n\nPrior to 1987, the European telecommunications market consisted predominantly of monopolies or telecommunications organizations (TO's) such as BT, France Telecom and Deutsche Telekom. In 1987, the European Union adopted the 1987 Green Paper that essentially stated it was in Europe's best interests to overhaul the current system and liberalize telecommunications services. As a result, from 1988 to 1998 the European Commission adopted multiple directives that obligated member states to open markets for equipment, telecom services, value-added, data services, satellite, mobile and voice to competition.\n\nMember States set up national regulatory agencies (NRA's) that oversaw these directives and developed policies and procedures for telecommunications organizations to follow. The directives provided a framework for the EU commission to ensure compliance and take judicial action when necessary. In 1990 the Open Network Provision (ONP) Framework Directive was adopted by the EU that established regulations for open access to the existing infrastructure and networks, interconnection and 'fair pricing' for resellers.\n\nECTA was established as a result of restrictive practices by former monopolies to limit activities by resellers to access networks and obtain fair pricing. Committees were established within the organization to collectively respond to new EU directives and written and 'unwritten' policies that were carried out by NRA's. ECTA responded to numerous proposed directives and called for regulations to be developed to better protect reseller interests and encourage enforcement in the spirit of the original 1987 Green Paper. ECTA has contributed white papers and had input into many policy telecom policies since its inception in 1997.\n\nECTA also serves as a forum for carriers and resellers to meet and discuss openly issues that are in the interests of the parties. ECTA has also served to educate the telecommunications market, providing information and market research on European telecommunications as well as a market forum for new products and services.\n\n\n\n\nECTA holds two events a year to discuss the latest developments in electronic communications and digital issues as a whole. The list of past ECTA events and conferences can be found on their website.\n\nIn March 2015, ECTA sponsored a conference in presence of Vice-President of the European Commission Andrus Ansip. The title of this conference was \"Creating Europeâs Digital Highways: Competition, Innovation and Investment in High-speed Broadband\". The statements of the speakers as well as their video interviews including the speech from Marvin Ammori from the Centre for Internet and Society at Stanford Law School can be found on website.\n\nThe current priority of the European Commission is to foster investments in telecom infrastructures in order to build a Connected Digital Single Market. Vice-President Andrus Ansip, released on 6 May with Commissioner GÃ¼nther Oettinger a Digital Single Market Strategy which ECTA CEOs welcomed in an open letter. \n\nMore about ECTA and ECTA's role in moulding the European telecommunications market can be found on their website website.\n\n"}
{"id": "82926", "url": "https://en.wikipedia.org/wiki?curid=82926", "title": "Floor", "text": "Floor\n\nA floor is the bottom surface of a room or vehicle or even possibly the surface on which people dance, commonly referred to as a 'dance floor'. Floors vary from simple dirt in a cave to many-layered surfaces modern technology. Floors may be stone, wood, bamboo, metal or any other material that can support the expected load.\n\nThe levels of a building are often referred to as floors, although a more proper term is storey.\n\nFloors typically consist of a subfloor for support and a floor covering used to give a good walking surface. In modern buildings the subfloor often has electrical wiring, plumbing, and other services built in. As floors must meet many needs, some essential to safety, floors are built to strict building codes in some regions.\n\nWhere a special floor structure like a floating floor is laid upon another floor then both may be referred to as subfloors.\n\nSpecial floor structures are used for a number of purposes:\n\n\nFloor covering is a term to generically describe any finish material applied over a floor structure to provide a walking surface. Flooring is the general term for a permanent covering of a floor, or for the work of installing such a floor covering. Both terms are used interchangeably but floor covering refers more to loose-laid materials.\n\nMaterials almost always classified as floor covering include carpet, area rugs, and resilient flooring such as linoleum or vinyl flooring. Materials commonly called flooring include wood flooring, laminated wood, ceramic tile, stone, terrazzo, and various seamless chemical floor coatings.\n\nThe choice of material for floor covering is affected by factors such as cost, endurance, noise insulation, comfort and cleaning effort. Some types of flooring must not be installed below grade (lower than ground level), and laminate or hardwood should be avoided where there may be moisture or condensation.\n\nThe subfloor may be finished in a way that makes it usable without any extra work, see: \n\nThere are a number of special features that may be used to ornament a floor or perform a useful service. Examples include Floor medallions which provide a decorative centerpiece of a floor design, or Gratings used to drain water or to rub dirt off shoes.\n\nFloors may be built on beams or joists or use structures like prefabricated hollow core slabs. The subfloor builds on those and attaches by various means particular to the support structure but the support and subfloor together always provides the strength of a floor one can sense underfoot. Nowadays, subfloors are generally made from at least two layers of moisture resistant ('AC' grade, one side finished and sanded flat) plywood or composite sheeting, jointly also termed \"Underlayments\" on floor joists of 2x8, 2x10, or 2x12's (dimensional lumber) spaced generally on centers, in the United States and Canada. Some flooring components used solely on concrete slabs consist of a dimpled rubberized or plastic layer much like bubble wrap that provide little tiny pillars for the sheet material above. These are manufactured in squares and the edges fit together like a mortise and tenon joint. Like a floor on joists not on concrete, a second sheeting underlayment layer is added with staggered joints to disperse forces that would open a joint under the stress of live loads like a person walking.\n\nThree layers are common only in high end highest quality construction. The two layers in high quality construction will both be thick sheets (as will the third when present), but the two layers may achieve a combined thickness of only half-that in cheaper construction â panel overlaid by plywood subflooring. At the highest end, or in select rooms of the building there might well be three sheeting layers, and such stiff subflooring is necessary to prevent the cracking of large floor tiles of or more on a side, and the structure under such a floor will frequently also have extra 'bracing' and 'blocking' joist-to-joist intended spread the weight to have as little sagging on any joist as possible when there is a live load on the floor above.\n\nIn Europe and North America only a few rare floors will be seen to have no separate floor covering on top, and those are normally because of a temporary condition pending sales or occupancy; in semi-custom new construction and some rental markets, such floors are provided for the new home buyer (renter) to select their own preferred floor coverings usually a wall to wall carpet, or one piece vinyl floor covering. Wood clad ('Hardwood') and tile covered finished floors generally will require a stiffer higher quality subfloor, especially for the later class. Since the wall base and flooring interact forming a joint, such later added semi-custom floors will generally not be hardwood for that joint construction would be in the wrong order unless the wall base trim was also delayed pending the choosing.\n\nThe subfloor may also provide underfloor heating and if floor radiant heating is not used, will certainly suffer puncture openings to be put through for forced air ducts for both heating and air conditioning, or pipe holes for \"forced hot water\" or \"steam heating\" transport piping conveying the heat from furnace to the local room's heat exchangers (radiators).\n\nSome sub-floors are inset below the top surface level of surrounding flooring's joists and such subfloors and a normal height joist are joined to make a plywood box both molding and containing at least of concrete (A 'Mud Floor' in builders parlance). Alternatively, only a slightly inset floor topped by a fibrous mesh and concrete building composite floor cladding is used for smaller high quality tile floorsâthese 'concrete' subfloors have a good thermal match with ceramic tiles and so are popular with builders constructing kitchen, laundry and especially both common and high end bathrooms and any other room where large expanses of well supported ceramic tile will be used as a finished floor. Floors using small ( and smaller) ceramic tiles generally use only an additional layer of plywood (if that) and substitute adhesive and substrate materials making do with both a flexible joints and semi-flexible mounting compounds and so are designed to withstand the greater flexing which large tiles cannot tolerate without breaking.\n\nA ground-level floor can be an earthen floor made of soil, or be solid ground floors made of concrete slab.\n\nGround level slab floors are uncommon in northern latitudes where freezing provides significant structural problems, except in heated interior spaces such as basements or for outdoor unheated structures such as a gazebo or shed where unitary temperatures are not creating pockets of troublesome meltwaters. Ground-level slab floors are prepared for pouring by grading the site, which usually also involves removing topsoil and other organic materials well away from the slab site. Once the site has reached a suitable firm inorganic base material that is graded further so that it is flat and level, and then topped by spreading a layer-cake of force dispersing sand and gravel. Deeper channels may be dug, especially the slab ends and across the slab width at regular intervals in which a continuous run of rebar is bent and wired to sit at two heights within forming a sub-slab 'concrete girder'. Above the targeted bottom height (coplanar with the compacted sand and gravel topping) a separate grid of rebar or welded wire mesh is usually added to reinforce the concrete, and will be tied to the under slab 'girder' rebar at intervals. The under slab cast girders are used especially if it the slab be used structurally, i.e. to support part of the building.\n\nFloors in wood-frame homes are usually constructed with joists centered no more than apart, according to most building codes. Heavy floors, such as those made of stone, require more closely spaced joists. If the span between load-bearing walls is too long for joists to safely support, then a heavy crossbeam (thick or laminated wood, or a metal I-beam or H-beam) may be used. A \"subfloor\" of plywood or waferboard is then laid over the joists.\n\nIn modern buildings, there are numerous services provided via ducts or wires underneath the floor or above the ceiling. The floor of one level typically also holds the ceiling of the level below (if any).\n\nServices provided by subfloors include:\n\n\nIn floors supported by joists, utilities are run through the floor by drilling small holes through the joists to serve as conduits. Where the floor is over the basement or crawlspace, utilities may instead be run under the joists, making the installation less expensive. Also, ducts for air conditioning (central heating and cooling) are large and cannot cross through joists or beams; thus, ducts are typically at or near the plenum, or come directly from underneath (or from an attic).\n\nPipes for plumbing, sewerage, underfloor heating, and other utilities may be laid directly in slab floors, typically via cellular floor raceways. However, later maintenance of these systems can be expensive, requiring the opening of concrete or other fixed structures. Electrically heated floors are available, and both kinds of systems can also be used in wood floors as well.\nWood floors, particularly older ones, will tend to 'squeak' in certain places. This is caused by the wood rubbing against other wood, usually at a joint of the subfloor. Firmly securing the pieces to each other with screws or nails may reduce this problem.\n\nFloor vibration is a problem with floors. Wood floors tend to pass sound, particularly heavy footsteps and low bass frequencies. Floating floors can reduce this problem. Concrete floors are usually so massive they do not have this problem, but they are also much more expensive to construct and must meet more stringent building requirements due to their weight.\n\nFloors with a chemical sealer, like stained concrete or epoxy finishes, usually have a slick finish presenting a potential slip and fall hazard, however there are anti skid additives and coatings which can help mitigate this and provide increased traction. Reliable, science-backed floor slip resistance testing can help floor owners and designers determine if their floor is too slippery, or allow them to choose an appropriate flooring for the intended purpose before installation.\n\nThe flooring may need protection sometimes. A gym floor cover can be used to reduce the need to satisfy incompatible requirements.\n\nFloor cleaning is a major occupation throughout the world and has been since ancient times. Cleaning is essential to prevent injuries due to slips and to remove dirt. Floors are also treated to protect or beautify the surface. The correct method to clean one type of floor can often damage another, so it is important to use the correct treatment.\n"}
{"id": "1548510", "url": "https://en.wikipedia.org/wiki?curid=1548510", "title": "Fractional-order control", "text": "Fractional-order control\n\nFractional-order control (FOC) is a field of control theory that uses the fractional-order integrator as part of the control system design toolkit.\n\nThe fundamental advantage of FOC is that the fractional-order integrator weights history using a function that decays with a power-law tail. The effect is that the effects of all time are computed for each iteration of the control algorithm. This creates a 'distribution of time constants,' the upshot of which is there is no particular time constant, or resonance frequency, for the system.\n\nIn fact, the fractional integral operator formula_1 is different from any integer-order rational transfer function formula_2, in the sense that it is a non-local operator that possesses an infinite memory and takes into account the whole history of its input signal.\n\nFractional-order control shows promise in many controlled environments that suffer from the classical problems of overshoot and resonance, as well as time diffuse applications such as thermal dissipation and chemical mixing. Fractional-order control has also been demonstrated to be capable of suppressing chaotic behaviors in mathematical models of, for example, muscular blood vessels.\n\n\n"}
{"id": "18814341", "url": "https://en.wikipedia.org/wiki?curid=18814341", "title": "Frit compression", "text": "Frit compression\n\nFrit compression is the technique used to fabricate buckypaper and buckydiscs from a suspension of carbon nanotubes in a solvent. This is a quick, efficient method over surfactant-casting or acid oxidation filtration of carbon nanotubes.\n\nTraditional methods of buckypaper production involves the use of surfactants to disperse carbon nanotubes into aqueous solutions. It was found that filtering this suspension allowed the nanotubes to pack together in a paper-like mat, thus coining the term âbuckypaperâ (bucky being the reference to the buckminsterfullerene molecule). The problem was the difficulty in removing the surfactant afterwards, where the surfactant has been linked with cell lysis and tissue inflammation.\n\nAcid oxidation of carbon nanotubes can also be used in filtration to form buckypaper, but requires a high degree of surface acidic groups in order to obtain efficient dispersal in aqueous solution.\n\nAn alternative casting method was developed in 2008 to produce buckypaper that did not require the use of surfactants or the acid oxidation of carbon nanotubes in order to obtain high-purity buckypaper for biomedical applications.\n\nThe frit-compression system was adapted from a Solid phase extraction (SPE) column, where a suspension of carbon nanotubes is squeezed between two polypropylene frits (70 micrometre pore diameter) inside a syringe column. The pore structure of the frit allows a rapid exit of the solvent leaving the carbon nanotubes to be pressed together. The presence of the solvent controls the interaction between the tubes allowing the formation of tube-tube junctions; its surface tension directly affects the overlap of adjoining nanotubes thus gaining control over the porosity and pore diameter distribution of buckypaper. The distribution of carbon nanotubes in solvent does not have to be a stable suspension, rather a general dispersion serves much easier to keep the nanotubes between the frits rather than pass through them.\n\nOnce the system is compressed, the frit-carbon nanotube sandwich is removed from the syringe housing and allowed to dry. The frits can then be removed to leave intact buckypaper. This methodology rapidly speeds up the casting process, avoids use of surfactants and acid oxidation, and the solvent can be fully recovered.\n\nThe cross-sectional geometry of the syringe housing will determine the final structure of the buckypaper and the amount of carbon nanotubes added to the column will affect the height of the carbon nanotube mat. Although there is currently no formal classification for paper, discs and columns, it was deemed necessary to differentiate between the different structures obtained for research purposes.\n\nTypically, cylindrical columns are used with a few milligrams of carbon nanotubes in a solvent. This generates buckypaper with a circular cross-section and film heights of a few hundred micrometres. Buckypaper is usually a class of carbon nanotube mats with depths from 1 to 500 micrometres.\n\nBuckypaper with a height that is larger than 500 micrometres (0.5Â mm) is called a buckydisc, being thicker than buckypaper and not paper-like. Moreover, when casting in water, the edges of the film can lift due to surface tension effects of the remaining solvent in the system that can pull carbon nanotubes closer together.\n\nBuckydiscs with heights larger than 1Â mm can be referred to as buckycolumns. These carbon nanotube monoliths often exhibit hyperboloid geometries and are highly compressible \n\nIt is possible to use square housing to generate square cross sections, known as buckyprisms.\n\n"}
{"id": "23813381", "url": "https://en.wikipedia.org/wiki?curid=23813381", "title": "Future Tactical Truck System", "text": "Future Tactical Truck System\n\nThe Future Tactical Truck System (FTTS) was a United States Armed Forces program for which the Operational Requirements Document was drawn up during 2003. FTTS was a proposed two vehicle modular family that was to replace the AM General High Mobility Multipurpose Wheeled Vehicle (HMMWV (the Humvee)), Family of Medium Tactical Vehicles (FMTV), Oshkosh M977 Heavy Expanded Mobility Tactical Truck (HEMTT), Oshkosh Palletized Load System (PLS) (in certain echelons), and all remaining M35, M809 and M939 series of 2.5 and 5 ton trucks. The FTTS-UV (Utility Vehicle) was to replace the HMMWV, while the FTTS-MSV (Manoeuvre Sustainment Vehicle) was to replace all other types.\n\nBy 2006 work on FTTS was faltering and according to the Foreword of Jane's Military Vehicles and Logistics 2006-2007 by Shaun C Connors and Christopher F Foss (): \"The U.S. Armyâs original intention was that the FTTS (the logistic support for the Future Combat System (FCS)) with just two variants would eventually replace virtually all of the current tactical wheeled vehicle fleet. In a more realistic approach, the current FTTS-MSV/UV Advanced Concept Technology Demonstration (ACTD) efforts (with input from other efforts) will now be used âto define requirementsâ for future U.S. Army trucks.\"\n\nThe Association of the United States Army (July 2006) \"Army Tactical Wheeled Vehicle Strategy: Meeting Current and Future Needs\" pdf stated that the ACTD for FTTS was taking a two phase approach; the first phase involved modeling and simulation (M&S) efforts in which multiple vendors presented concepts for technologies to be incorporated into designs for the UV and MSV variants of FTTS. The second phase included awards to three of the M&S contractors to manufacture prototype designs: two contractors to produce their UV version with trailer, and one contractor to produce two MSVs with companion trailers. All vehicles and trailers were to be evaluated in a Military Utility Assessment (MUA) at Fort Lewis, Washington. The MUA was scheduled to occur during the first and second quarters of FY07, with the results to fed into the requirements development process, this to define future truck requirements.\n\nElements of FTTS-UV ACTD were fed in to the ongoing Joint Light Tactical Vehicle (JLTV) program, this the replacement for part of the HMMWV fleet. On 24 April 2006, establishment of a Joint Program Office to manage the JLTV effort was endorsed. The AUSA pdf file previously referenced stated that: \"Following approval, [of the JLTV Initial Capabilities Document by the Secretary of Defense],the Future Tactical Truck Systems Advanced Concept Technology Demonstration, the PSD [Platform Systems Demonstration], EMIP [Expedited Modernization Initiative Procedure] and various Office of Naval Research Science and Technology activities will inform the requirements process, thus ensuring requirements are realistic, achievable and relevant to all services.\n\nDeliveries of the medium and heavy category trucks in production when FTTS commenced, and to be replaced by FTTS, continue. The FMTV (now with Oshkosh) production contract has recently been extended to September 2016, and award of the Family of Heavy Tactical Vehicles IV (FHTV) contract, on a sole source basis to Oshkosh, is anticipated in the near future. FHTV contract awards include HEMTT and PLS.\n\nTo evaluate possible future truck systems and designs, the U.S. Army's Tank-automotive and Armaments Command refereed the Future Tactical Truck Systems - Advanced Concepts and Technology Demonstration (ACTD) program. Navistar International and Lockheed Martin's proposals, the International FTTS and Lockheed FTTS Utility Vehicles were selected, as well as the Armor Holdings proposal for the MSV. In August 2006 they were tested at the Aberdeen Proving Grounds. Following this evaluation they were parked in The Pentagon courtyard for evaluation by higher-ranking military officials.\"\n\nInternational Truck's proposal for FTTS utilized a Parallel Hybrid Drivetrain and modular armor. It has undergone testing by 5th Brigade, 2nd Infantry Division and 14th Engineer Battalion at the Aberdeen Proving Grounds and Fort Lewis, along with Lockheed Martin's design and Armor Holdings' Maneuver Sustainment Vehicle.\n\n\n\n"}
{"id": "1156742", "url": "https://en.wikipedia.org/wiki?curid=1156742", "title": "Gas cracker", "text": "Gas cracker\n\nA gas cracker is any device that splits the molecules in a gas or liquid, usually by electrolysis, into atoms. The end product is usually a gas. A hydrocracker is an example of a gas cracker. In nature, molecules are split often, such as in food digestion and microbial digestion activity. A gas cracker device splits the molecule at a rate much greater than that normally found in nature. In science and industry, gas crackers are used to separate two or more elements in a molecule. For example, liquid water, or , is separated into hydrogen and oxygen gases. This is not to be confused with the splitting of the nucleus (nuclear power).\n\nPetrochemicals are usually manufactured in large scale from petroleum feed stocks using fluid catalytic cracking. Naphtha, natural gas, refinery off-gas and gas from cokers and thermal crackers are good sources. Thus natural gas is one of the most wanted feed stocks for petrochemicals production. The thermal craking of natural gas proceeds at very high temperature resulting in olefins (Mostly ethylene/propylene). The temperature in a gas cracker exceeds 1000Â°C. For ultimate decomposition of gas into elements more than 1500 Â°C is required. Thus, acetylene/carbon black production encounters such high temperatures. Usually oxy-combustion methods are used for attaining such high temperatures. BASF burners/Kellog burners are available in the market. \n\nFurther electro cracking or plasma cracking methods are also available.\n\n\n"}
{"id": "6198973", "url": "https://en.wikipedia.org/wiki?curid=6198973", "title": "Green Wheel", "text": "Green Wheel\n\nThe Peterborough Millennium Green Wheel is an network of cycleways, footpaths and bridleways. Designed as part of a sustainable transport system for the city, it was created as part of a Millennium project around Peterborough, England.\n\nThe name \"Green Wheel\" alludes to the circular nature of the major part of the path, which encircles Peterborough, with cycle route \"spokes\" leading from this perimeter, which passes through several peripheral settlements around Peterborough, into the city centre, allowing easy transport around the network, much of which required no new construction, instead using or improving already existing cycle routes or roads. The only major new construction for the project was that of a curved cycle bridge over the River Nene near Whittlesey, from where the path can be accessed northwards towards Flag Fen, into the city centre or southwards towards the Ortons. The network is fully signposted. As well as this, three circular pipe tunnels were constructed near Etton village in order to allow the Green Wheel route to pass underneath the A15.\n\nThe project also encourages recreational use and has created a sculpture trail, which provides functional, landscape artworks along the Green Wheel route and a âLiving Landmarksâ project involving the local community in the creation of local landscape features such as mini woodlands, ponds and hedgerows.\n\nThe project cost Â£11 million and was 50% funded by the National Lottery through the Millennium Commission and has also been the winner of many awards including a RIBA award for Architecture in 2003.\n\nThe Bedford Green Wheel is also a project to build on the existing network of traffic free paths and quiet routes for cyclists and walkers. This network will run around Bedford, England, and includes 'spokes' linking into the town centre. The network will link parks, nature reserves, countryside and homes. This project is part of Bedford Borough Council's Green Infrastructure Plan 2009.\n\nThe Cycling Campaign for North Bedfordshire promotes and encourages the use of the Bedford Green Wheel.\n\n"}
{"id": "17081364", "url": "https://en.wikipedia.org/wiki?curid=17081364", "title": "Help (command)", "text": "Help (command)\n\nIn computing, codice_1 is a command in various command line shells such as codice_2, codice_3, Bash, 4DOS/4NT, Windows PowerShell, Singularity shell, Python and GNU Octave. It provides online information about available commands and the shell environment.\n\nThe command is available in operating systems such as DOS, OS/2, Microsoft Windows, ReactOS, OpenVOS, MPE/iX, KolibriOS and also in the DEC RT-11 and TOPS-20 operating systems. Furthermore it is available in the open source MS-DOS emulator DOSBox and in the EFI shell. The DEC OS/8 CCL codice_1 command prints information on specified OS/8 programs. On Unix, the command is part of the Source Code Control System and prints help information for the SCCS commands.\n\n help [command]\n\nArguments:\n\nThe help command is available in MS-DOS 5.x and later versions of the software.\nIf no arguments are provided, the command lists the contents of codice_6. In MS-DOS 6.x this command exists as FASTHELP. In DR-DOS, HELP is a batch file that launches DR-DOS' internal help program, DOSBOOK.\n\nThe MS-DOS 6.xx help command uses QBasic to view a quickhelp codice_7 file, which contains more extensive information on the commands, with some hyperlinking etc. The MS-DOS 6.22 help system is included on Windows 9x CD-ROM versions as well.\n\nPC DOS 7.xx help uses codice_8 to open OS/2 style INF files (codice_9, codice_10 and codice_11), opening these to the appropriate pages.\n\nThe FreeDOS version was developed by Joe Cosentino.\n\nThe 4DOS/4NT help command uses a text user interface to display the online help.\n\nUsed without parameters, help lists and briefly describes every system command.\nWindows NT-based versions use MS-DOS 5 style help. Versions before Windows Vista also have a Windows help file (codice_12 or codice_13) in a similar style to MS-DOS 6.\n\nIn Windows PowerShell, help is a short form (implemented as a PowerShell function) for access to the codice_14 Cmdlet.\n\nWindows PowerShell includes an extensive, console-based help system, reminiscent of man pages in Unix. The help topics include help for cmdlets, providers, and concepts in PowerShell.\n\nIn Bash, the builtin command help lists all Bash builtin commands if used without arguments. Otherwise, it prints a brief summary of a command. Its syntax is:\n\n"}
{"id": "295888", "url": "https://en.wikipedia.org/wiki?curid=295888", "title": "History of calendars", "text": "History of calendars\n\nThe history of calendars, means that people creating and using methods for keeping track of days and larger divisions of time, covers a practice with ancient roots.\n\nArcheologists have reconstructed methods of timekeeping that go back to prehistoric times at least as old as the Neolithic. The natural units for timekeeping used by most historical societies are the day, the solar year and the lunation.\nCalendars are explicit schemes used for timekeeping. The first historically attested and formulized calendars date to the Bronze Age, dependent on the development of writing in the Ancient Near East. The Sumerian calendar was the earliest, followed by the Egyptian, Assyrian and Elamite calendars.\n\nA larger number of calendar systems of the ancient Near East appear in the Iron Age archaeological record, based on the Assyrian and Babylonian calendars. This includes the calendar of the Persian Empire, which in turn gave rise to the Zoroastrian calendar as well as the Hebrew calendar.\n\nCalendars in antiquity were usually lunisolar, depending on the introduction of intercalary months to align the solar and the lunar years. This was mostly based on observation, but there may have been early attempts to model the pattern of intercalation algorithmically, as evidenced in the fragmentary 2nd-century Coligny calendar. Nevertheless, the Roman calendar contained very ancient remnants of a pre-Etruscan 10-month solar year.\n\nThe Roman calendar was reformed by Julius Caesar in 45 BCE. The Julian calendar was no longer dependent on the observation of the new moon but simply followed an algorithm of introducing a leap day every four years.\nThis created a dissociation of the calendar month from the lunation.\n\nIn the 11th century in Persia, a calendar reform led by Khayyam was announced in 1079, when the length of the year was measured as 365.24219858156 days. Given that the length of the year is changing in the sixth decimal place over a person's lifetime, this is outstandingly accurate. For comparison the length of the year at the end of the 19th century was 365.242196 days, while today it is 365.242190 days.\n\nThe Gregorian calendar was introduced as a refinement of the Julian calendar in 1582, and is today in worldwide use as the \"de facto\" calendar for secular purposes.\n\nThe term \"calendar\" itself is taken from the \"calends\", the term for the first day of the month in the Roman calendar, related to the verb \"calare\" \"to call out\", referring to the calling or the announcement that the new moon was just seen. Latin \"calendarium\" meant \"account book, register\", as accounts were settled and debts were collected on the calends of each month.\n\nThe Latin term was adopted in Old French as \"calendier\" and from there in Middle English as \"calender\" by the 13th century. The spelling \"calendar\" is from Early Modern English.\n\nAn alternative hypothesis connects \"calendar\" with Koledari in Slavic, pre-Christian tradition, which was later incorporated into Christmas. \"Kolo\" means \"circle, cycle\" and \"dar\" means \"a gift\".\nA number of prehistoric structures have been proposed as having had the purpose of timekeeping (typically keeping track of the course of the solar year).\nThis includes many megalithic structures, and reconstructed arrangements going back far into the Neolithic period.\n\nA ceramic artefact from Bulgaria, known as the Slatino furnace model, has been pronounced by local archeologists and media to be the oldest known calendar representation, a claim not endorsed in mainstream views.\n\nA mesolithic arrangement of twelve pits and an arc found in Warren Field, Aberdeenshire, Scotland, dated to roughly 10,000 years ago, has been described as a lunar calendar and was dubbed the \"world's oldest known calendar\" in 2013.\n\nThe Oldest European calendar is found near to Vukovar in modern-day Croatia. It is a ceramic vessel bearing inscribed ideograms of celestial objects.\n\nThe ancient Sumerian calendar divided a year into 12 lunar months of 29 or 30 days.\nEach month began with the sighting of a new moon. Sumerian months had no uniform name throughout Sumer because of the religious diversity. This resulted in scribes and scholars referring to them as \"the first month\", \"the fifth month\" etc. To keep the lunar year of 354 days in step with the solar year of 365.242 days an extra month was added periodically, much like a Gregorian leap year.\n<br>\nThere were no weeks in the Sumerian calendar. Holy days and time off from work were usually celebrated on the first, seventh and fifteenth of each month. In addition to these holy days, there were also feast days which varied from city to city.\n\nAlthough the earliest evidence of Iranian calendrical traditions is from the second millennium BCE, predating the appearance of the Iranian prophet Zoroaster, the first fully preserved calendar is that of the Achaemenids. Throughout recorded history, Persians have been keen on the idea and importance of having a calendar. They were among the first cultures to use a solar calendar and have long favoured a solar over lunar and lunisolar approaches. The sun has always been a symbol in Iranian culture and is closely related to the folklore regarding Cyrus the Great.\n\nOld Persian inscriptions and tablets indicate that early Iranians used a 360-day calendar based on the solar observation directly and modified for their beliefs. Days were not named. The months had two or three divisions depending on the phase of the moon. Twelve months of 30 days were named for festivals or activities of the pastoral year. A 13th month was added every six years to keep the calendar synchronized with the seasons.\n\nThe first calendars based on Zoroastrian cosmology appeared in the later Achaemenid period (650 to 330 BCE). They evolved over the centuries, but month names changed little until now.\n\nThe unified Achaemenid Empire required a distinctive Iranian calendar, and one was devised in Egyptian tradition, with 12 months of 30 days, each dedicated to a yazata (Eyzad), and four divisions resembling the Semitic week. Four days per month were dedicated to Ahura Mazda and seven were named after the six Amesha Spentas. Thirteen days were named after Fire, Water, Sun, Moon, Tiri and Geush Urvan (the soul of all animals), Mithra, Sraosha (Soroush, yazata of prayer), Rashnu (the Judge), Fravashi, Bahram (yazata of victory), Raman (Ramesh meaning peace), and Vata, the divinity of the wind. Three were dedicated to the female divinities, Daena (yazata of religion and personified conscious), Ashi (yazata of fortune) and Arshtat (justice). The remaining four were dedicated to Asman (lord of sky or Heaven), Zam (earth), Manthra Spenta (the Bounteous Sacred Word) and Anaghra Raocha (the 'Endless Light' of paradise).\n\nThe Parthians (Arsacid dynasty) adopted the same calendar system with minor modifications, and dated their era from 248 BCE, the date they succeeded the Seleucids. Their names for the months and days are Parthian equivalents of the Avestan ones used previously, differing slightly from the Middle Persian names used by the Sassanians. For example, in Achaemenid times the modern Persian month 'Day' was called Dadvah (Creator), in Parthian it was Datush and the Sassanians named it Dadv/Dai (Dadar in Pahlavi).\n\nWhen in April of AD 224 the Parthian dynasty fell and was replaced by the Sasanid, the new king, Ardashir I, abolished the official Babylonian calendar and replaced it with the Zoroastrian. This involved a correction to the places of the \"gahanbar\", which had slipped back in the seasons since they were fixed. These were placed eight months later, as were the \"epagemonai\", the 'Gatha' or 'Gah' days after the ancient Zoroastrian hymns of the same name. Other countries, such as the Armenians and Choresmians, did not accept the change.\n\nToghril Beg, the founder of the Seljuq dynasty, had made Esfahan the capital of his domains and his grandson Malik-Shah was the ruler of that city from 1073. An invitation was sent to Khayyam from Malik-Shah and from his vizier Nizam al-Mulk asking Khayyam to go to Esfahan to set up an Observatory there. Other leading astronomers were also brought to the Observatory in Esfahan and for 18 years Khayyam led the scientists and produced work of outstanding quality. During this time Khayyam led work on compiling astronomical tables and he also contributed to calendar reform in 1079.\n\nCowell quotes The Calcutta Review No 59:-\n\"When the Malik Shah determined to reform the calendar, Omar was one of the eight learned men employed to do it, the result was the Jalali era (so called from Jalal-ud-din, one of the king's names) - 'a computation of time,' says Gibbon, 'which surpasses the Julian, and approaches the accuracy of the Gregorian style.\"'\n\nKhayyam measured the length of the year as 365.24219858156 days. Two comments on this result. Firstly it shows an incredible confidence to attempt to give the result to this degree of accuracy. We know now that the length of the year is changing in the sixth decimal place over a person's lifetime. Secondly it is outstandingly accurate. For comparison the length of the year at the end of the 19th century was 365.242196 days, while today it is 365.242190 days.\n\nThe Greeks, as early as the time of Homer, appear to have been familiar with the division of the year into the twelve lunar months but no intercalary month \"Embolimos\" or day is then mentioned. Independent of the division of a month into days, it was divided into periods according to the increase and decrease of the moon. \nThus, the first day or new moon was called \"Noumenia\". The month in which the year began, as well as the names of the months, differed among the states, and in some parts even no names existed for the months, as they were distinguished only numerically, as the first, second, third, fourth month, etc.\n\nThe ancient Athenian calendar was a lunisolar calendar with 354-day years, consisting of twelve months of alternating length of 29 or 30 days. To keep the calendar in line with the solar year of 365.242 days, an extra, intercalary month was added in every other year. The Athenian months were called Hekatombion, Metageitnion, Boedromion, Pyanepsion, Maimakterion, Poseidon, Gamelion, Anthesterion, Elaphebolion, Munychion, Thargelion, and Skirophorion. The intercalary month usually came after Poseidon, and was called second Poseidon.\n\nIn addition to their regular, \"festival\" calendar, the Athenians maintained a second, political calendar . This \"conciliar\" calendar divided the year into \"prytanies\", one for each of the \"phylai\", the subdivisions of Athenian citizens. The number of phylai, and hence the number of prytanies, varied over time. Until 307 BC, there were 10 phylai. After that the number varies between 11 and 13 (usually 12). Even more confusing, while the conciliar and festival years were about the same length in the 4th century BC, such was not regularly the case earlier or later. Documents dated by prytany are frequently very difficult to assign to a particular equivalent in the Julian calendar.\n\nThe table of Greek Olympiads, following the four-year cycles between the Olympic Games from 1 July 776 BC, continued until the end of the 4th century AD. The Babylonian Era of Nabonassar, beginning on 26 February 747 BC, was used by the Greeks of Alexandria. It was later known in the Middle Ages from the works of Ptolemy.\n\nThe Greek calendars were greatly diversified by the Hellenistic period, with separate traditions in every Greek state.\nOf primary importance for the reconstruction of the regional Greek calendars is the calendar of Delphi, because of the numerous documents found there recording the manumission of slaves, many of which are dated both in the Delphian and in a regional calendar.\n\nThe Macedonian Era of the Seleucids, which began with the conquest of Babylon by Seleucus Nicator in 312 BC. It became widely used in the Levant. The Jews knew it as the \"era of contracts\", and used it in Europe until the 15th century.\n\nThe Roman Republican calendar numbered years based on the sitting consuls. References to the year of consulship were used in both conversation and official records. Romans from the same family often had the same \"praenomen\", which sometimes makes it difficult to distinguish them, and there were two consuls at any one time, each of whom might sometimes hold the appointment more than once, meaning that it was (and is) necessary to be well educated in history to understand the references. The Romans had an eight-day week, with the market-day falling every eight days. It was called a \"nundinum\" or 'nine-day' in inclusive counting.\n\nMost of the regional Hindu calendars are inherited from a system standardized in classical Hindu astronomy \nas adopted via Indo-Greek transmission in the final centuries BCE, and reformed by Gupta era astronomers such as Äryabhaá¹­a and VarÄhamihira.\n\nBefore the Spring and Autumn period (before 770 BC), the Chinese Calendars were solar calendars. In the so-called five-phase calendar, the year consists of 10 months and a transition, each month being 36 days long, and the transitions 5 or 6 days. \nDuring the Warring States period (~475-220 BC), the primitive lunisolar calendars were established under the Zhou Dynasty, known as the six ancient calendars (). The months of these calendars begin on the day with the new moon, with 12 or 13 months (lunations) in a year. The intercalary month is placed at the end of the year. In Qin China, the Qin calendar () was introduced. It follows the rules of Zhuanxu's calendar, but the months order follows the Xia's calendar.\n\nTime keeping was important to Vedic rituals, and \"Jyotisha\" was the Vedic era field of tracking and predicting the movements of astronomical bodies in order to keep time, in order to fix the day and time of these rituals. This study was one of the six ancient Vedangas, or ancillary science connected with the Vedas â the scriptures of Hinduism.\n\nHindu calendar, sometimes referred to as \"Panchanga\", is a collective term for the various lunisolar calendars traditionally used in Hinduism. They adopt a similar underlying concept for timekeeping, but differ in their relative emphasis to moon cycle or the sun cycle, the names of months and when they consider the New Year to start. The ancient Hindu calendar is similar in conceptual design to the Jewish calendar, but different from the Gregorian calendar. Unlike Gregorian calendar which adds additional days to lunar month to adjust for the mismatch between twelve lunar cycles (354 lunar days) and nearly 365 solar days, the Hindu calendar maintains the integrity of the lunar month, but insert an extra full month by complex rules, every few years, to ensure that the festivals and crop related rituals fall in the appropriate season.\n\nThe Hindu calendars have been in use in the Indian subcontinent since ancient times, and remains in use by the Hindus in India and Nepal particularly to set the Hindu festival dates. Early Buddhist and Jain communities of India adopted the ancient Hindu calendar, later Vikrami calendar and then local Buddhist calendars. Buddhist and Jain festivals continue to be scheduled according to a lunar system in the luni-solar calendar.\n\nThe old Roman year had 304 days divided into 10 months, beginning with March. However the ancient historian Livy gave credit to the second early Roman king Numa Pompilius for devising a calendar of 12 months. The extra months \"Ianuarius\" and \"Februarius\" had been invented, supposedly by Numa Pompilius, as stop-gaps. Julius Caesar realized that the system had become inoperable, so he effected drastic changes in the year of his third consulship. The New Year in 709 AUC began on 1 January and ran over 365 days until 31 December. Further adjustments were made under Augustus, who introduced the concept of the \"leap year\" in 757 AUC (AD 4). The resultant Julian calendar remained in almost universal use in Europe until 1582, and in some countries until as late as the twentieth century.\n\nMarcus Terentius Varro introduced the \"Ab urbe condita\" epoch, assuming a foundation of Rome in 753 BC. The system remained in use during the early Middle Ages until the widespread adoption of the Dionysian era in the Carolingian period.\n\nIn the Roman Empire, the AUC year could be used alongside the consular year, so that the consulship of Quintus Fufius Calenus and Publius Vatinius could be determined as 707 AUC (or 47 BC), the third consulship of Caius Julius Caesar, with Marcus Aemilius Lepidus, as 708 AUC (or 46 BC), and the fourth consulship of Gaius Julius Caesar as 709 AUC (or 45 BC).\n\nThe seven-day week has a tradition reaching back to the ancient Near East, but the introduction of the \"planetary week\" which remains in modern use dates to the Roman Empire period (see also names of the days of the week).\n\nFor the first six centuries since the birth of Jesus Christ, European countries used various local systems to count years, most usually regnal years, modeled on the Old Testament. In some cases, Creation dating was also used. In the 6th century, the Christian monk Dionysius Exiguus devised the \"Anno Domini\" system, dating from the Incarnation of Jesus. In the 8th century, the Anglo-Saxon historian Bede the Venerable used another Latin term, \"ante uero incarnationis dominicae tempus\" (\"the time before the Lord's true incarnation\", equivalent to the English \"before Christ\"), to identify years before the first year of this era.\n\nAccording to the Catholic Encyclopedia, even Popes continued to date documents according to regnal years, and usage of AD only gradually became common in Europe from the 11th to the 14th centuries. In 1422, Portugal became the last Western European country to adopt the \"Anno Domini\" system.\n\nIn 1267, the medieval scientist Roger Bacon stated the times of full moons as a number of hours, minutes, seconds, thirds, and fourths (\"horae\", \"minuta\", \"secunda\", \"tertia\", and \"quarta\") after noon on specified calendar dates. Although a third for of a second remains in some languages, for example Arabic Ø«Ø§ÙØ«Ø©, the modern second is further divided decimally.\n\nRival calendar eras to \"Anno Domini\" remained in use in Christian Europe. In Spain, the \"Era of the Caesars\" was dated from Octavian's conquest of Iberia in 39 BC. It was adopted by the Visigoths and remained in use in Catalonia until 1180, Castille until 1382 and Portugal until 1415.\n\nFor chronological purposes, the flaw of the Anno Domini system was that dates have to be reckoned backwards or forwards according as they are BC or AD. According to the \"Catholic Encyclopedia\", \"in an ideally perfect system all events would be reckoned in one sequence. The difficulty was to find a starting point whence to reckon, for the beginnings of history in which this should naturally be placed are those of which chronologically we know least.\" For both Christians and Jews, the prime historical date was the Year of Creation, or \"Annus Mundi\". The Eastern Orthodox Church fixed the date of Creation at 5509 BC. This remained the basis of the ecclesiastical calendar in the Greek and Russian Orthodox world until modern times. The Coptic Church fixed on 5500 BC. Later, the Church of England, under Archbishop Ussher in 1650, would pick 4004 BC.\n\nThe Bulgar calendar was a calendar system used by the Bulgars, a seminomadic people, originally from Central Asia, who from the 2nd century onwards dwelled in the Eurasian steppes north of the Caucasus and around the banks of river Volga. In 681 part of the Bulgars settled in the Balkan peninsula and established Bulgaria. The main source of information used for reconstruction of the Bulgar calendar is a short 15th century transcript, which contains 10 pairs of calendar terms, the same dating system being found in a marginal note of a manuscript from the 10th c. According to the reconstructed calendar, the Bulgars used a 12-year cyclic calendar similar to the one adopted by Turkic peoples from the Chinese calendar, with names and numbers that are deciphered as in Bulgar language.\n\nThe Islamic calendar is based on the prohibition of intercalation (\"nasi'\") by Muhammad, in Islamic tradition dated to a sermon held on 9 Dhu al-Hijjah AH 10 (Julian date: 6 March 632). This resulted in an observationally based lunar calendar shifting relative to the seasons of the solar year.\n\nIn medieval Iceland, a calendar was introduced in the 10th century. While the ancient Germanic calendars were based on lunar months, the new Icelandic calendar introduced a purely solar reckoning, with a year having a fixed number of weeks (52 weeks or 364 days). This necessitated the introduction of \"leap weeks\" instead of the Julian leap days.\n\nDuring the Mughal rule, land taxes were collected from Bengali people according to the Islamic Hijri calendar. This calendar was a lunar calendar, and its new year did not coincide with the solar agricultural cycles. According to some sources, Mughal Emperor Akbar asked his royal astronomer Fathullah Shirazi to create a new calendar by combining the lunar Islamic calendar and solar Hindu calendar already in use, and this was known as \"Fasholi shan\" (harvest calendar). According to Amartya Sen, Akbar's official calendar \"Tarikh-ilahi\" with the zero year of 1556 CE was a blend of pre-existing Hindu and Islamic calendars. It was not used much in India outside of Akbar's Mughal court, and after his death the calendar he launched was abandoned. However, adds Sen, there are traces of the \"Tarikh-ilahi\" that survive in the Bengali calendar. Some historians attribute the Bengali calendar to the 7th century Hindu king Shashanka.\n\nOf all the ancient calendar systems, the Maya and other Mesoamerican systems are the most complex. The Mayan calendar had 2 years, the 260-day Sacred Round, or \"tzolkin\", and the 365-day Vague Year, or \"haab\".\n\nThe Sacred Round of 260 days is composed of two smaller cycles: the numbers 1 through 13, coupled with 20 different day names: Imix, Ik, Akbal, Kan, Chicchan, Cimi, Manik, Lamat, Muluc, Oc, Chuen, Eb, Ben, Ix, Men, Cib, Caban, Eiznab, Cauac, and Ahau. The Sacred Round was used to determine important activities related to the gods and humans: name individuals, predict the future, decide on auspicious dates for battles, marriages, and so on.\n\nThe two cycles of 13 and 20 intermesh and are repeated without interruption: the cycle would begin with 1 Imix, then 2 Ik, then 3 Akbal and so on until the number 13 was reached, at which point the number cycle was restarted so 13 Ben would be followed by 1 Ix, 2 Men and so on. This time Imix would be numbered 8. The cycle ended after 260 days, with the last day being 13 Ahau.\n\nThe Vague Year of 365 days is similar to our modern calendar, consisting of 18 months of 20 days each, with an unlucky five-day period at the end. The Vague Year had to do primarily with the seasons and agriculture, and was based on the solar cycle. The 18 Maya months are known, in order, as: Pop, Uo, Zip, Zotz, Tzec, Xuc, Yaxkin, Mol, Chen, Yax, Zac, Ceh, Mac, Kankin, Maun, Pax, Kayab and Cumku. The unlucky five-day period was known as Uayeb, and was considered a time which could hold danger, death and bad luck.\n\nThe Vague Year began with the month of Pop. The Maya 20-day month always begins with the seating of the month, followed by days numbered 1 to 19, then the seating of the following month, and so on. This ties in with the Maya notion that each month influences the next. The Maya new year would start with 1 Pop, followed by 2 Pop, all the way through to 19 Pop, followed by the seating of the month of Uo, written as 0 Uo, then 1 Uo, 2 Uo and so on. These two cycles coincided every 52 years. The 52-year period of time was called a \"bundle\" and was similar to a modern-day century.\n\nWhile the Gregorian calendar is now in worldwide use for secular purposes, various medieval or ancient calendars remain in regional use for religious or social purposes, including the Julian calendar, the Hebrew calendar, the Islamic calendar, various Hindu calendars, the Zoroastrian calendar etc.\n\nThere are also various modern calendars that see limited use, either created for the use of new religious movements or reformed versions of older religious calendars, or calendars introduced by regionalist or nationalist movements.\n\n\n\n"}
{"id": "49100052", "url": "https://en.wikipedia.org/wiki?curid=49100052", "title": "Holobuilder", "text": "Holobuilder\n\nHoloBuilder Inc., founded in 2016, assists builders and engineers to create immersive progress views of construction sites.\n\nThe company is a German-American construction technology-startup, developed both in San Francisco, California, and Aachen, Germany. They offer tools to create and share 360Â° views of construction sites and buildings. Those digitized sites help at managing their real-world counterparts effortlessly from everywhere. In 2016, HoloBuilder has been used in 190 countries around the world, where more than 15,000 projects have been created, which have been viewed almost 800,000 times.\n\nThe cloud-based software www.HoloBuilder.com providing the leading cloud and mobile software for virtual reality capturing of construction sites.\nHoloBuilder for constructions focuses on the whole development process, from the planning phase up until maintenance management.\n\nHolobuilder.com is an online virtual reality capturing software. This means that users can easily and quickly create virtual tours using 360Â° photos through your construction site.\nWhen reliable and affordable 360Â° cameras hit the market, it became easier than ever to capture your surroundings with the tap of one single button.\n\nHoloBuilder projects support multiple sheets, making it easy to manage big construction sites with multiple floors in one single web-based project, which is easily accessible from anywhere. Coupled with the Time Travel feature, another functionality that makes it easy to track the progress over time on a construction site.\n\nUsers include construction companies, realtors. architects, designers and Industry 4.0 companies who use it to create Virtual Tours of real estate properties, construction sites or digital Augmented Reality user manuals.\n\nThese projects can be created and viewed in the browser and are embeddable into any website as an iFrame. HoloBuilder leverages HTML5 and WebGL to be usable in browsers without the need of Plugins like Adobe Flash and in mobile browsers, it uses the device sensors to support browser-based VR in Google Cardboard and similar VR headsets.\n\nUsers can import their 360 pictures taken with Cameras like the Ricoh Theta S and combine them with 3D models from editors like AutoCAD, SketchUp or Blender.\n\nThe JobWalk App enables users to create virtual 360Â° tours on the go. The user connects a 360Â° Camera with his mobile device via WiFi. After taking 360Â° images and linking the scenes, the 360Â° tour can be synchronized with the web-platform.\n\n\n\n\n"}
{"id": "50245739", "url": "https://en.wikipedia.org/wiki?curid=50245739", "title": "IKEA Museum", "text": "IKEA Museum\n\nThe IKEA Museum is a museum located in Ãlmhult, Sweden, that opened to the public on June 30, 2016. It presents the history of the Swedish furnishing company IKEA. It replaced IKEA Through the Ages, a smaller 800 m exhibition that showed 20 different room settings with IKEA furniture and objects.\n\nThe IKEA Museum is in the same building where the first IKEA store opened in 1958. The store moved to a new location in Ãlmhult in 2012. At this time, work began on converting the store into a museum, with an original planned opening date of 2015. The facade of the building was restored to recreate its original appearance. The museum has an area of 3500 m.\n\n"}
{"id": "300278", "url": "https://en.wikipedia.org/wiki?curid=300278", "title": "Integrated receiver/decoder", "text": "Integrated receiver/decoder\n\nAn integrated receiver/decoder (IRD) is an electronic device used to pick up a radio-frequency signal and convert digital information transmitted in it.\n\nConsumer IRDs, commonly called set-top boxes, are used by end users and are much cheaper compared to professional IRDs. To curb content piracy, they also lack many features and interfaces found in professional IRDs such as outputting uncompressed SDI video or ASI transport stream dumps. They are also designed to be more aesthetically pleasing.\n\nCommonly found in radio, television, Cable and satellite broadcasting facilities, the IRD is generally used for the reception of contribution feeds that are intended for re-broadcasting. The IRD is the interface between a receiving satellite dish or Telco networks and a broadcasting facility video/audio infrastructure. \n\nProfessional IRDs have various features that consumer IRDs lack such as:\n\n\n\n"}
{"id": "34640672", "url": "https://en.wikipedia.org/wiki?curid=34640672", "title": "KeiyÅ Industrial Zone", "text": "KeiyÅ Industrial Zone\n\n, also known as the KeiyÅ Industrial Region, the KeiyÅ Industrial Area, or the KeiyÅ Industrial Belt, is an industrial zone on the northeastern coast of Tokyo Bay that crosses 8 cities in Chiba Prefecture, Japan. The zone spans from the western part of Urayasu in the northeast to Futtsu in the southeast of the region. The zone has no political or administrative status.\n\nThe name of the industrial zone is formed from two kanji characters. The first, , means \"capital city\" and refers to Tokyo. The second, , meaning \"leaf\", is the second kanji in \"Chiba\" and refers to Chiba Prefecture, and the compound refers to the Tokyo-Chiba region.\n\nThe KeiyÅ Industrial Zone spans the coast of Tokyo Bay from Urayasu in the northeast, through Funabashi, Chiba City, Kisarazu, Kimitsu, Ichihara, Sodegaura, and ends in Futtsu to the southeast. Numerous small rivers empty into the industrial region, and provide a source of water to support industry. They include the Edo River, the YÅrÅ River, and the Koito River.\n\nBefore industrialization the KeiyÅ region was originally home to \"nori\" seaweed collection, the shellfish industry, mixed small-scale fishing and agricultural villages, and beach resorts. The Keihin region, spanning west from Tokyo to Yokohama, was developed after World War I. With the rapid development of the defense industry in Japan from the beginning of the ShÅwa period in 1926, a plan for the decentralization of industry from the immediate Tokyo area was planned in 1935.\nThe KeiyÅ Industrial Region was fully developed after World War II. Some land reclamation had been carried out in coastal areas of Tokyo Bay as part of the industrialization of Japan in the early 20th century. Reclaimed land areas replaced traditional fishing areas and supported small factories. The construction of the Kawasaki Steel Works in Chiba City in 1953 marked the beginning of the large-scale construction of heavy industry infrastructure in the industrial zone, and other industries soon followed. The deepwater ports of the KeiyÅ Industrial Zone were built starting in the 1950s. Thermal power generators were built, and large tracts of land were reclaimed from the bay for expansion of the KeiyÅ region. KeiyÅ was significantly expanded in the 1960s. Heavy metal and chemical production were among the highest in Japan by the 1970s.\n\nThe zone is a major base for the electric power generation, petrochemical, petroleum, shipbuilding, logistics, shipping, and steel industries. The Port of Chiba is a major component to the KeiyÅ Industrial Region.\n"}
{"id": "46406824", "url": "https://en.wikipedia.org/wiki?curid=46406824", "title": "Language workbench", "text": "Language workbench\n\nA language workbench is a software development tool designed to define, reuse and compose domain-specific languages together with their integrated development environment. Language workbenches support language-oriented programming. Language workbenches were introduced and popularized by Martin Fowler in 2005.\n\nLanguage workbenches usually support:\n\n\n\n\n\n"}
{"id": "52991251", "url": "https://en.wikipedia.org/wiki?curid=52991251", "title": "Nitro Zeus", "text": "Nitro Zeus\n\nNitro Zeus is a project name for a well funded comprehensive cyber attack plan created as a mitigation strategy after the Stuxnet malware campaign and its aftermath. Unlike Stuxnet, that was loaded onto a system after the design phase to affect its proper operation, Nitro Zeus's objectives are built into a system during the design phase unbeknownst to the system users. This built-in feature allows a more assured and effective cyber attack against the system's users.\n\nThe information about its existence was raised during research and interviews carried out by Alex Gibney for his \"Zero Days\" documentary film. The proposed long term widespread infiltration of major Iranian systems would disrupt and degrade communications, power grid, and other vital systems as desired by the cyber attackers. This was to be achieved by electronic implants in Iranian computer networks. The project was seen as one pathway in alternatives to full-scale war. There was no requirement for this type of plan after the Iran Nuclear Deal was signed, though any functioning implants in Iran's critical SCADA infrastructure systems are not likely to be removed.\n\n"}
{"id": "51873", "url": "https://en.wikipedia.org/wiki?curid=51873", "title": "Paper clip", "text": "Paper clip\n\nA paper clip (or sometimes paperclip) is a device used to hold sheets of paper together, usually made of steel wire bent to a looped shape. Most paper clips are variations of the \"Gem\" type introduced in the 1890s or earlier, characterized by the almost two full loops made by the wire. Common to paper clips proper is their utilization of torsion and elasticity in the wire, and friction between wire and paper. When a moderate number of sheets are inserted between the two \"tongues\" of the clip, the tongues will be forced apart and cause torsion in the bend of the wire to grip the sheets together.\n\nPaper clips usually have an oblong shape with straight sides, but may also be triangular or circular, or have more elaborate shapes. The most common material is steel or some other metal, but moulded plastic is also used.\nSome other kinds of paper clip use a two-piece clamping system. Recent innovations include multi-colored plastic-coated paper clips and spring-fastened binder clips. \n\nAccording to the Early Office Museum, the first patent for a bent wire paper clip was awarded in the United States to Samuel B. Fay, in 1867. This clip was originally intended primarily for attaching tickets to fabric, although the patent recognized that it could be used to attach papers together. Fay received U.S. patent 64,088 on April 23, 1867. Although functional and practical, Fay's design along with the 50 other designs patented prior to 1899 are not considered reminiscent of the modern paperclip design known today. Another notable paper clip design was also patented in the United States by Erlman J. Wright in 1877. This clip was advertised at that time for use in fastening newspapers.\nThe most common type of wire paper clip still in use, the Gem paper clip, was never patented, but it was most likely in production in Britain in the early 1870s by \"The Gem Manufacturing Company\", according to the American expert on technological innovations, Professor Henry J. Petroski. He refers to an 1883 article about \"Gem Paper-Fasteners\", praising them for being \"better than ordinary pins\" for \"binding together papers on the same subject, a bundle of letters, or pages of a manuscript\". Since the 1883 article had no illustration of this early \"Gem\", it may have been different from modern paper clips of that name. The earliest documentation of its existence is an 1894 advertisement for \"Gem Paper Clips\". In 1904 Cushman & Denison registered a trade mark for the \"Gem\" name in connection with paper clips. The announcement stated that it had been used since March 1, 1892, which may have been the time of its introduction in the United States. Paper clips are still sometimes called \"Gem clips\", and in Swedish the word for any paper clip is \"gem\".\n\nDefinite proof that the modern type of paper clip was well known in 1899 at the latest, is the patent granted to William Middlebrook of Waterbury, Connecticut on April 27 of that year for a \"Machine for making wire paper clips.\" The drawing clearly shows that the product is a perfect clip of the Gem type. The fact that Middlebrook did not mention it by name, suggests that it was already well known at the time. Since then countless variations on the same theme have been patented. Some have pointed instead of rounded ends, some have the end of one loop bent slightly to make it easier to insert sheets of paper, and some have wires with undulations or barbs to get a better grip. In addition, purely aesthetic variants have been patented, clips with triangular, star, or round shapes. But the original Gem type has for more than a hundred years proved to be the most practical, and consequently by far the most popular. Its qualitiesâease of use, gripping without tearing, and storing without tanglingâhave been difficult to improve upon.\n\nIt has been claimed, though apparently without evidence, that Herbert Spencer, the originator of the term \"survival of the fittest\", invented the paper clip. Spencer claimed in his autobiography to have invented a \"binding-pin\" that was distributed by Ackermann & Company, and he shows a drawing of the pin in his Appendix I (following Appendix H). This pin looked more like a modern cotter pin than a modern paper clip, but it was designed to hold sheets of paper together. It is approximately 15Â cm unfolded.\n\nA Norwegian, Johan Vaaler (1866â1910), has erroneously been identified as the inventor of the paper clip. He was granted patents in Germany and in the United States (1901) for a paper clip of similar design, but less functional and practical, because it lacked the last turn of the wire. Vaaler probably did not know that a better product was already on the market, although not yet in Norway. His version was never manufactured and never marketed, because the superior \"Gem\" was already available.\n\nLong after Vaaler's death his countrymen created a national myth based on the false assumption that the paper clip was invented by an unrecognised Norwegian genius. Norwegian dictionaries since the 1950s have mentioned Vaaler as the inventor of the paper clip, and that myth later found its way into international dictionaries and much of the international literature on paper clips.\n\nVaaler probably succeeded in having his design patented abroad, despite the previous existence of more useful paper clips, because patent authorities at that time were quite liberal and rewarded any marginal modification of existing inventions. Johan Vaaler began working for \"Alfred J. Bryns Patentkontor\" in Kristiania in 1892 and was later promoted to office manager, a position he held until his death. As the employee of a patent office, he could easily have obtained a patent in Norway. His reasons for applying abroad are not known; it is possible that he wanted to secure the commercial rights internationally. Also, he may have been aware that a Norwegian manufacturer would find it difficult to introduce a new invention abroad, starting from the small home market.\n\nVaaler's patents expired quietly, while the \"Gem\" was used worldwide, including his own country. The failure of his design was its impracticality. Without the two full loops of the fully developed paper clip, it was difficult to insert sheets of paper into his clip. One could manipulate the end of the inner wire so that it could receive the sheet, but the outer wire was a dead end because it could not exploit the torsion principle. The clip would instead stand out like a keel, perpendicular to the sheet of paper. The impracticality of Vaaler's design may easily be demonstrated by cutting off the last outer loop and one long side from a regular Gem clip.\n\nThe originator of the Norwegian paper clip myth was an engineer of the Norwegian national patent agency who visited Germany in the 1920s to register Norwegian patents in that country. He came across Vaaler's patent, but failed to detect that it was not the same as the then-common Gem-type clip. In the report of the first fifty years of the patent agency, he wrote an article in which he proclaimed Vaaler to be the inventor of the common paper clip. This piece of information found its way into some Norwegian encyclopedias after World War II.\n\nEvents of that war contributed greatly to the mythical status of the paper clip. Patriots wore them in their lapels as a symbol of resistance to the German occupiers and local Nazi authorities when other signs of resistance, such as flag pins or pins showing the cipher of the exiled King Haakon VII of Norway were forbidden. Those wearing them did not yet see them as national symbols, as the myth of their Norwegian origin was not commonly known at the time.\nThe clips were meant to denote solidarity and unity (\"we are bound together\"). The wearing of paper clips was soon prohibited, and people wearing them could risk severe punishment.\n\nThe leading Norwegian encyclopedia mentioned the role of the paper clip as a symbol of resistance in a supplementary volume in 1952, but did not yet proclaim it a Norwegian invention. That information was added in later editions. According to the 1974 edition, the idea of using the paper clip to denote resistance originated in France. A clip worn on a lapel or front pocket could be seen as \"deux gaules\" (two posts or poles) and be interpreted as a reference to the leader of the French Resistance, General Charles de Gaulle.\n\nThe post-war years saw a widespread consolidation of the paper clip as a national symbol. Authors of books and articles on the history of Norwegian technology eagerly seized it to make a thin story more substantial. They chose to overlook the fact that Vaaler's clip was not the same as the fully developed Gem-type clip. In 1989 a giant paper clip, almost high, was erected on the campus of a commercial college near Oslo in honour of Vaaler, ninety years after his invention was patented. But this monument shows a Gem-type clip, not the one patented by Vaaler. The celebration of the alleged Norwegian origin of the paper clip culminated in 1999, one hundred years after Vaaler submitted his application for a German patent. A commemorative stamp was issued that year, the first in a series to draw attention to Norwegian inventiveness. The background shows a facsimile of the German \"Patentschrift\". However, the figure in the foreground is not the paper clip depicted on that document, but the much better known \"Gem\". In 2005, the national biographical encyclopedia of Norway (\"Norsk biografisk leksikon\") published the biography of Johan Vaaler, stating he was the inventor of the paper clip.\n\nWire is versatile in its nature. Thus a paper clip is a useful accessory in many kinds of mechanical work including computer work: the metal wire can be unfolded with a little force. Several devices call for a very thin rod to push a recessed button which the user might only rarely need. This is seen on most CD-ROM drives as an \"emergency eject\" should the power fail; also on early floppy disk drives (including the early Macintosh). Various smartphones require the use of a long thin object such as a paper clip to eject the SIM card and some Palm PDAs advise the use of a paper clip to reset the device. The track ball can be removed from early Logitech pointing devices using a paper clip as the key to the bezel. A paper clip bent into a \"U\" can be used to start an ATX PSU without connecting it to a motherboard (connect the green to a black on the motherboard header). One or more paper clips can make a loopback device for a RS232 interface (or indeed many interfaces). A paper clip could be installed in a Commodore 1541 disk-drive as a flexible head-stop. A paper clip can be used (unsafely) to temporarily bridge a blown fuse. The steel wire from a paperclip can be used in dentistry to form a dental post.\n\nPaper clips can be bent into a crude but sometimes effective lock picking device. Some types of handcuffs can be unfastened using paper clips. There are two approaches. The first one is to unfold the clip in a line and then twist the end in a right angle, trying to imitate a key and using it to lift the lock fixator. The second approach, which is more feasible but needs some practice, is to use the semi-unfolded clip kink for lifting when the clip is inserted through the hole where the handcuffs are closed.\n\nA paper clip image is the standard image for an attachment in an email client.\n\nIn 1994, the US imposed anti-dumping tariffs against China, on paper clips.\n\n\n\n\n\"Patents\"\n"}
{"id": "3372103", "url": "https://en.wikipedia.org/wiki?curid=3372103", "title": "Pneudraulics", "text": "Pneudraulics\n\nDerived from the words hydraulics and pneumatics, pneudraulics is the term used when discussing systems on military aircraft that use either or some combination of hydraulic and pneumatic systems. Sometimes, land tractors will have pneumatic tires and hydraulic shovels.\n\nThe science of fluids made of both gas and liquid.\n\n"}
{"id": "41724303", "url": "https://en.wikipedia.org/wiki?curid=41724303", "title": "Posted write", "text": "Posted write\n\nA posted write is a computer bus write transaction that does not wait for a write completion response to indicate success or failure of the write transaction. For a posted write, the CPU assumes that the write cycle will complete with zero wait states, and so doesn't wait for the done. This speeds up writes considerably. For starters, it doesn't have to wait for the done response, but it also allows for better pipelining of the datapath without much performance penalty.\n\nA non-posted write requires that a bus transaction responds with a write completion response to indicate success or failure of the transaction, and is naturally much slower than a posted write since it requires a round trip delay similar to read bus transactions.\n\nIn reference to memory bus accesses, a posted write is referred to as a posted memory write (PMW).\n\n\n\n"}
{"id": "36197112", "url": "https://en.wikipedia.org/wiki?curid=36197112", "title": "PowerHUB", "text": "PowerHUB\n\nPowerHUB refers to the name of a series of Integrated Circuits (ICs) developed by ST-Ericsson, a 50/50 joint venture of Ericsson and STMicroelectronics established on February 3, 2009.\n\nThese ICs are designed for the energy management and the battery charging of mobile devices.\n\nThe first member of the PowerHUB family, the PM2300, has been announced by ST-Ericsson on February 9, 2011.\n\nOn February 28, 2012, ST-Ericsson introduced a new IC in this family, the PM2020, supporting the wireless charging technology standardized by the Wireless Power Consortium (WPC).\n"}
{"id": "7408536", "url": "https://en.wikipedia.org/wiki?curid=7408536", "title": "Programmable communicating thermostat", "text": "Programmable communicating thermostat\n\nThe term programmable communicating thermostat (PCT) is used by the California Energy Commission to describe programmable thermostats that can receive information wirelessly. \n\nThe first version of the PCT introduced in the 2008 building standards proceeding also required that PCTs allow temperature control during emergency events to avoid blackouts. This feature was removed after public input indicated a strong fear of the non-overrideable \"big brother\" feel of this feature.\n\nA talk at the S4 SCADA security conference in January 2008 indicated adding a public key encryption scheme to the specification, giving each thermostat a random 160-bit number. The installer or homeowner would call this number in to the utility or other service provider (operator), who would then send the Operator's public key to the thermostat over RDS. Using this method, the PCT would receive messages only from the operator(s) explicitly agreed to by the homeowner. \n\nThermostats can also communicate wirelessly through the Internet or via a home automation technology, such as Insteon. These advanced thermostats can be adjusted via computer or Internet capable phone to allow users to adjust the temperature in their home without being present.\n\n\n"}
{"id": "7665856", "url": "https://en.wikipedia.org/wiki?curid=7665856", "title": "Pyrolant", "text": "Pyrolant\n\nA pyrolant (from Greek \"pyros\", fire) is an energetic material that generates hot flames upon combustion. Pyrolants are metal-based pyrotechnic compositions containing virtually any oxidizer.\nThe term was originally coined by Kuwahara in 1992, in a paper on magnesium/Teflon/Viton, to distinguish between compositions that serve as propellants and those yielding hot flames which are not necessarily suitable for propellant purposes.\n\n\"Thermites\" constitute a subdivision of pyrolants referring to mixtures containing a narrow range of oxygen-based oxidizers only, Hence the term thermite cannot be used interchangeably with \"pyrolant\".\n\nA similar common term is \"propellant\", which describes either a homogeneous or composite material that generates thrust upon combustion, but which may contain fuels instead of or in addition to the metals contained in thermites.\n\nPyrolants are generally characterized by high combustion temperatures (> 2000 K) and high amounts of condensed reaction products at equilibrium conditions such as metal oxides, fluorides and soot.\nTypical pyrolants find use as pyrotechnic initiators (Zr/BaCrO or Zr/KClO), illuminating flare (Mg/NaNO) and decoy flare compositions (Mg/(CF))\n"}
{"id": "8532924", "url": "https://en.wikipedia.org/wiki?curid=8532924", "title": "Robert Trappl", "text": "Robert Trappl\n\nRobert Trappl (born 16 January 1939 in Vienna) is an Austrian scientist and head of the Austrian Research Institute for Artificial Intelligence in Vienna, which was founded in 1984. He is known for his work in the field of cybernetics and artificial intelligence. \n\nIn the 1960s, Trappl received in Vienna a degree in electrical engineering, a degree in sociology from the Institute of Advanced Studies (Vienna), and a PhD in psychology, with a minor in astronomy. \n\nSince the 1970s he has been working at the Medical University of Vienna, Austria. Since the 1980s he has been the director of the Austrian Research Institute for Artificial Intelligence and later until 2006 he also was head of the Institute of Medical Cybernetics and Artificial Intelligence at the Medical University of Vienna. Besides his work at the University he has advised several national and international companies and organizations.\n\nIn the 1970s he was President of the Austrian Society for Cybernetic Studies. In this position, he co-founded in 1980 the International Federation for Systems Research of which he became vice-president and from 1986 to 1988 president. \n\nTrappl is also editor-in-chief of the scientific journals \"Applied Artificial Intelligence\" and \"Cybernetics and Systems\", and a member of the editorial boards of several other journals.\n\nTrappl has published over 180 articles and cooperated in some 34 books. A selection:\n\n"}
{"id": "26466913", "url": "https://en.wikipedia.org/wiki?curid=26466913", "title": "Simulated Electronic Launch Peacekeeper", "text": "Simulated Electronic Launch Peacekeeper\n\nSimulated Electronic Launch Peacekeeper (SELP) was a method used by the United States Air Force to verify the reliability of the LGM-118A Peacekeeper intercontinental ballistic missile.\n\nSELM replaced key components at the Launch Control Center to allow a physical \"keyturn\" by missile combat crew members. This test allowed end-to-end verification of the ICBM launch process.\n\nSELP was phased out with the deactivation of the Peacekeeper ICBM in 2005.\n\nThe ICBM System Program Office at Hill AFB, Utah provided technical support to SELP tests The information obtained from tests provided a complete assessment of the weapon systems for Air Force Space Command (AFSPC).\n\n"}
{"id": "14571247", "url": "https://en.wikipedia.org/wiki?curid=14571247", "title": "Synthetically thinned aperture radar", "text": "Synthetically thinned aperture radar\n\nSynthetic thinned aperture radiometry (STAR) is a method of radar in which the coherent product (correlation) of the signal from pairs of antennas is measured at different antenna-pair spacings (baselines). These products yield sample points in the Fourier transform of the brightness temperature map of the scene, and the scene itself is reconstructed by inverting the sampled transform. The reconstructed image includes all of the pixels in the entire field-of-view of the antennas.\n\nThe main advantage of the STAR architecture is that it requires no mechanical scanning of an antenna. Using a static antenna simplifies the antenna system dynamics and improves the time-bandwidth product of the radiometer. Furthermore, aperture thinning reduces the overall volume and mass of the antenna system. A disadvantage is the reduction of radiometric sensitivity (or increase in rms noise) of the image due to a decrease in signal-to-noise ratio for each measurement compared to a filled aperture. Pixel averaging is required for good radiometric sensitivity.\n\n"}
{"id": "20467301", "url": "https://en.wikipedia.org/wiki?curid=20467301", "title": "TOSCO II process", "text": "TOSCO II process\n\nThe TOSCO II process is an above ground retorting technology for shale oil extraction, which uses fine particles of oil shale that are heated in a rotating kiln. The particularity of this process is that it use hot ceramic balls for the heat transfer between the retort and a heater. The process was tested in a 40Â tonnes per hour test facility near Parachute, Colorado.\n\nTOSCO II process is a refinement of the Swedish Aspeco process. The Tosco Corporation purchased its patent rights in 1952. In 1956, the Denver Research Institute performed research and development of this technology, including testing of a 24Â ton per day pilot plant, which operated until 1966. Later the technology development was continued under Tosco's own directions. In 1964 Tosco, Standard Oil of Ohio, and Cleveland Cliffs Iron Company formed Colony Development, a joint venture company to develop the Colony Shale Oil Project and to commercialize the TOSCO II technology. The project was ended in April 1972.\n\nThe TOSCO II process is classified as a hot recycled solids technology. It employs a horizontal rotating kiln-type retort. In this process, oil shale is crushed smaller than and enters the system through pneumatic lift pipes in which oil shale is elevated by hot gas streams and preheated to about . After entering into retort, oil shale is mixed with hot ceramic balls with temperature from to . This increase the oil shale temperature to between and , in which pyrolysis occurs. In the pyrolysis process, kerogen decomposes to oil shale gas and oil vapors, while the remainder of the oil shale forms spent shale. Vapors are transferred to a condensor (fractionator) for separation into various fractions. At the kiln passage, the spent shale and the ceramic balls are separated in a perforated rotating separation drum (trommel). The crushed spent shale falls through holes in the trommel, while ceramic balls are transferred to the ball heater. Combustible shale gas is burned in the ball heater to reheat the ceramic balls.\n\nThe overall thermal efficiency of TOSCO II process is low because the energy of spent shale is not recovered and much of the produced shale gas is consumed by the process itself. The efficiency could be increased by burning char (carbonaceous residue in the spent shale) instead of shale gas as a fuel of the ball heater. The process' other disadvantages are mechanical complexity and large number of moving parts. Also the lifetime of ceramic balls is limited. Disposal of spent shale includes environmental problems because it is very finely crushed and contains carbon residue.\n\n"}
{"id": "25601529", "url": "https://en.wikipedia.org/wiki?curid=25601529", "title": "The Dark Report", "text": "The Dark Report\n\nThe Dark Report is a website that provides business intelligence to executive leaders, pathologists, and managers in the laboratory testing industry. It is produced by the DARK Intelligence Group, Inc., a private company that was founded in 1995 with headquarters in Spicewood, Texas, United States.\n\nThe Dark Report delivers news, analysis, and commentary about the management and operation of medical laboratories, clinical laboratories, pathology laboratories, anatomic pathology groups, and pathology practices It covers developments in clinical diagnostics and molecular diagnostics. It tracks events involving in vitro diagnostics (IVD) companies, genetic testing companies, information technology companies serving the clinical lab profession, and new companies with genetic tests, companion diagnostics, and pharmacogenomics assays. Editor-In-Chief Robert L. Michel is an authority on clinical laboratory management and pathology laboratory operations.\n\nIssued every third Monday, The Dark Report has published since September 1995. It delivers assessments of the lab testing industry, for laboratory CEOs, COOs, CFOs, senior executives, and pathologists. It is a paid membership subscription service. It provides news and analysis of events in the laboratory testing industry. Members and clients of The Dark Report range from executives at laboratory companies around the world to leaders at IVD In vitro diagnostics and information technology companies.\n\nDark Daily is a national and international e-briefing service for people involved in the management of clinical laboratories and pathology laboratories. E-briefings are distributed electronically four or five times each week. It is free to any individual who subscribes at the web site http://www.darkdaily.com. Established in September 2006, the top 10 countries by number of subscribers are United States, Canada, United Kingdom, Australia, New Zealand, India, Ireland, Germany, Spain, and France.\n\nA two-day gathering on clinical laboratory and pathology management, every spring the Executive War College attracts five to six hundred senior-level lab executives, administrators, pathologists, Ph.D.s, and managers from around the world. The Executive War College was founded in 1996.\n\nThis is a conference devoted to the use of Lean Six Sigma, ISO 9001, ISO 15189 and similar quality management systems in clinical laboratories, pathology laboratories, and hospitals. Lab Quality Confab attracts an international audience of laboratory professionals and pathologists. More than 60 speakers and sessions conducted over a three-day period give participantâs insights and laboratory/hospital case studies of successes with Lean and Six Sigma techniques.\n\nAt the web site for Dark Daily is a library of clinical laboratory management information and resources. This includes white papers and reports authored by industry experts about clinical laboratory management and pathology management and operations. There are audio conference transcripts and recordings, as well as streaming video of experts in clinical laboratory management and pathology operations. An archive of Dark Daily e-briefings is available, dating back to the beginning of the service in September 2006.\n\nPrincipals of The Dark Report provided consulting services to clinical laboratories, pathology groups, hospitals, health systems, and vendors to the laboratory testing industry.\n\n-The Dark Report is a co-producer and participant in international conferences in pathology and clinical laboratory management.\n\nAn annual event since 2002, Frontiers in Laboratory Medicine (FiLM) has been conducted in the United Kingdom. It is co-founded and co-produced by the Association for Clinical Biochemistry and Laboratory Medicine (ACB) and The Dark Report . FiLM has been recognized as a forum for innovation in pathology testing by progressive laboratories in the National Health Service (NHS).\n\nFor laboratory executives, senior administrators, and pathologists in Canada, Executive Edge was established in 2005. It is co-produced by QSE Consulting and The Dark Report . Conducted biannually in the fall in Toronto, Executive Edge is a resource to the pathology and clinical laboratory management profession in Canada.\n\nFirst conducted in 2007 The Business of Pathology (TBOP) takes place in Australia. It is co-produced by the Australasian Association of Clinical Biochemists (AACB) and The Dark Report . It brings together leaders in pathology and clinical laboratory testing to address issues in the management and operation of pathology laboratories in Australia and Pacific Rim countries. It takes place biannually in the fall.\n\nThe DARK Report was founded in 1995 by Publisher and Editor-In-Chief Robert L. Michel , with R. Lewis Dark as Publisher Emeritus. It is a management intelligence service for executives and leaders in the laboratory testing industry. The DARK Report gives intelligence assessment of trends and market developments in clinical diagnostics.\n\n\n1) http://www.entrepreneur.com/tradejournals/article/172182789.html \n2) http://www.frontiersinlabmedicine.com/\n3) http://findarticles.com/p/articles/mi_m3230/is_11_39/ai_n27469265/\n4) http://www.dotmed.com/news/story/8151/\n5) http://www.phcconsulting.com/WordPress/2009/04/07/dark-report-staff-your-lab-with-top-performers-how-to-recruit-and-retain-the-best-med-techs/\n6) http://www.paml.com/News/The%20Dark%20Report%5E%5E%20PAML%20and%20MSCL%20Venture.aspx\n7) http://www.bio-medicine.org/biology-technology-1/The-Dark-Report-and-Siemens-Introduce-the-Worlds-First-Integrated-Molecular-Summit-2764-1/\n8) http://www.clpmag.com/clprime/2008-01-30_12.asp\n9) http://blog.aperio.com/2008/12/dark-report-momentum-continues-for-digital-pathology.html\n10) http://www.encyclopedia.com/doc/1G1-173921071.html\n11) http://www.medcompare.com/featuredarticle.asp?articleid=499\n12) http://www.huliq.com/48689/dark-report-and-siemens-introduce-world039s-first-integrated-molecular-summit\n13) http://www.ahti.net/dark%20interview.htm\n14) http://www.aruplab.com/AboutARUP/PressRoom/Articles_LandingPages/dark_report_weber.jsp\n15) http://www.accessmylibrary.com/coms2/summary_0286-33533919_ITM\n16) http://www.highbeam.com/doc/1G1-173921071.html\n17) http://www.biomedexperts.com/Abstract.bme/18072682/The_Dark_Report_s_Robert_Michel_extrapolates_industry_trends\n18) http://www.darkreport.com/PDF/Medical-Laboratory-Observer-Promotes-Executive-War-College-1031.pdf\n19) http://www.darkreport.com/PDF/MLO-Robert-Michel-Laboratory-Trends.pdf\n\n"}
{"id": "1555706", "url": "https://en.wikipedia.org/wiki?curid=1555706", "title": "Tipu's Tiger", "text": "Tipu's Tiger\n\nTipu's Tiger or Tippu's Tiger is an eighteenth-century automaton or mechanical toy created for Tipu Sultan, the ruler of the Kingdom of Mysore in India. The carved and painted wood casing represents a tiger savaging a near life-size European man. Mechanisms inside the tiger and man's bodies make one hand of the man move, emit a wailing sound from his mouth and grunts from the tiger. In addition a flap on the side of the tiger folds down to reveal the keyboard of a small pipe organ with 18 notes.\n\nThe tiger was created for Tipu and makes use of his personal emblem of the tiger and expresses his hatred of his enemy, the British of the East India Company. The tiger was discovered in his summer palace after East India Company troops stormed Tipu's capital in 1799. The Governor General, Lord Mornington sent the tiger to Britain initially intending it to be an exhibit in the Tower of London. First exhibited to the London public in 1808 in East India House, then the offices of the East India Company in London, it was later transferred to the Victoria and Albert Museum (V&A) in 1880 (accession number 2545(IS)). It now forms part of the permanent exhibit on the \"Imperial courts of South India\". From the moment it arrived in London to the present day, Tipu's Tiger has been a popular attraction to the public.\n\nTipu's Tiger was originally made for Tipu Sultan (also referred to as Tippoo Saib, Tippoo Sultan and other epithets in nineteenth-century literature) in the Kingdom of Mysore (today in the Indian state of Karnataka) around 1795. Tipu Sultan used the tiger systematically as his emblem, employing tiger motifs on his weapons, on the uniforms of his soldiers, and on the decoration of his palaces. His throne rested upon a probably similar life-size wooden tiger, covered in gold; like other valuable treasures it was broken up for the highly organised prize fund shared out between the British army.\n\nTipu had inherited power from his father Hyder Ali, a Muslim soldier who had risen to become \"dalwai\" or commander-in-chief under the ruling Hindu Wodeyar dynasty, but from 1760 was in effect the ruler of the kingdom. Hyder, after initially trying to ally with the British against the Marathas, had later become their firm enemy, as they represented the most effective obstacle to his expansion of his kingdom, and Tipu grew up with violently anti-British feelings.\n\nThe tiger formed part of a specific group of large caricature images commissioned by Tipu showing European, often specifically British, figures being attacked by tigers or elephants, or being executed, tortured and humiliated and attacked in other ways. Many of these were painted by Tipu's orders on the external walls of houses in the main streets of Tipu's capital, Seringapatam. Tipu was in \"close co-operation\" with the French, who were at war with Britain and still had a presence in South India, and some of the French craftsmen who visited Tipu's court probably contributed to the internal works of the tiger.\n\nIt has been proposed that the design was inspired by the death in 1792 of a son of General Sir Hector Munro, who had commanded a division during Sir Eyre Coote's victory at the Battle of Porto Novo (Parangipettai) in 1781 when Hyder Ali, Tipu Sultan's father, was defeated with a loss of 10,000 men during the Second Anglo-Mysore War. Hector Sutherland Munro, a 17-year-old East India Company Cadet on his way to Madras, was attacked and killed by a tiger on 22 December 1792 while hunting with several companions on Saugor Island in the Bay of Bengal (still one of the last refuges of the Bengal tiger). However a similar scene was depicted on a silver mount on a gun made for Tipu and dated 1787-88, five years before the incident.\n\nTipu's Tiger is notable as an example of early musical automata from India, and also for the fact that it was especially constructed for Tipu Sultan.\n\nWith overall dimensions for the object of high and long, the man at least is close to life-size. The painted wooden shell forming both figures likely draws upon South Indian traditions of Hindu religious sculpture. It is typically about half an inch thick, and now much reinforced on the inside following bomb damage in World War II. There are many openings at the head end, formed to match the pattern of the inner part of the painted tiger stripes, which allow the sounds from the pipes within to be heard better, and the tiger is \"obviously male\". The top part of the tiger's body can be lifted off to inspect the mechanics by removing four screws. The construction of the human figure is similar but the wood is much thicker. Examination and analysis by the V&A conservation department has determined that much of the current paint has been restored or overpainted.\n\nThe human figure is clearly in European costume, but authorities differ as to whether it represents a soldier or civilian; the current text on the V&A website avoids specifying, other than describing the figure as \"European\".\n\nThe operation of a crank handle powers several different mechanisms inside Tipu's Tiger. A set of bellows expels air through a pipe inside the man's throat, with its opening at his mouth. This produces a wailing sound, simulating the cries of distress of the victim. A mechanical link causes the man's left arm to rise and fall. This action alters the pitch of the 'wail pipe'. Another mechanism inside the tiger's head expels air through a single pipe with two tones. This produces a \"regular grunting sound\" simulating the roar of the tiger. Concealed behind a flap in the tiger's flank is the small ivory keyboard of a two-stop pipe organ in the tiger's body, allowing tunes to be played.\n\nThe style of both shell and workings, and analysis of the metal content of the original brass pipes of the organ (many have been replaced), indicates that the tiger was of local manufacture. The presence of French artisans and French army engineers within Tipu's court has led many historians to suggest there was French input into the mechanism of this automaton.\n\nTipu's Tiger was part of the extensive plunder from Tipu's palace captured in the fall of Seringapatam, in which Tipu died, on 4 May 1799, at the culmination of the Fourth Anglo-Mysore War. An \"aide-de-camp\" to the Governor General of the East India Company, Richard Wellesley, 1st Marquess Wellesley, wrote a memorandum describing the discovery of the object:\n\nThe earliest published drawing of Tippoo's Tyger was the frontispiece for the book \"A Review of the Origin, Progress and Result, of the Late Decisive War in Mysore with Notes\" by James Salmond, published in London in 1800. It preceded the move of the exhibit from India to England and had a separate preface titled \"Description of the Frontispiece\" which said:\n\nUnlike Tipu's throne, which also featured a large tiger, and many other treasures in the palace, the materials of Tipu's Tiger had no intrinsic value, which together with its striking iconography is what preserved it and brought it back to England essentially intact. The Governors of the East India Company had at first intended to present the tiger to the Crown, with a view to it being displayed in the Tower of London, but then decided but to keep it for the Company. After some time in store, during which period the first of many \"misguided and wholly unjustified endeavours at \"improving\" the piece\" from a musical point of view may have taken place, it was displayed in the reading-room of the East India Company Museum and Library at East India House in Leadenhall Street, London from July 1808.\n\nIt rapidly became a very popular exhibit, and the crank-handle controlling the wailing and grunting could apparently be freely turned by the public. The French author Gustave Flaubert visited London in 1851 to see the Great Exhibition, writes Julian Barnes, but finding nothing of interest in The Crystal Palace, visited the East India Company Museum where he was greatly enamoured by Tipu's Tiger. By 1843 it was reported that \"The machine or organ ... is getting much out of repair, and does not altogether realize the expectation of the visitor\". Eventually the crank-handle disappeared, to the great relief of students using the reading-room in which the tiger was displayed, and \"The Athenaeum\" later reported that\n\nWhen the East India Company was taken over by the Crown in 1858, the tiger was stored in Fife House, Whitehall until 1868, when it moved down the road to the new India Office, which occupied part of the building still used by today's Foreign and Commonwealth Office. In 1874 it was moved to the India Museum in South Kensington, which was in 1879 dissolved, with the collection distributed between other museums; the V&A records the tiger as acquired in 1880. During World War II the tiger was badly damaged by a German bomb which brought down the roof above it, breaking the wooden casing into several hundred pieces, which were carefully pieced together after the war, so that by 1947 it was back on display. In 1955 it was exhibited in New York at the Museum of Modern Art through the summer and spring.\n\nIn recent times, Tipu's Tiger has formed an essential part of museum exhibitions exploring the historical interface between Eastern and Western civilisation, colonialism, ethnic histories and other subjects, one such being held at the Victoria and Albert Museum itself in autumn 2004 titled \"Encounters:the meeting of Asia and Europe, 1500â1800\". In 1995, 'The Tiger and the Thistle' bi-centennial exhibition was held in Scotland on the topic of \"Tipu Sultan and the Scots\". The organ was considered too fragile to travel to Scotland for the exhibition. Instead, a full-sized replica made of fibre glass and painted by Derek Freeborn, was exhibited in its place. The replica itself also had an earlier Scottish association, having been made in 1986 for 'The Enterprising Scot' exhibition, which was held to commemorate the October 1985 merger of the Royal Scottish Museum and the National Museum of Antiquities of Scotland to form a new entity - the National Museum of Scotland.\n\nToday Tipu's Tiger is arguably the best-known single work in the Victoria and Albert Museum as far as the general public is concerned. It is a \"must-see\" highlight for school-children's visits to the Victoria and Albert Museum, and functions as an iconic representation of the museum, replicated in various forms of memorabilia in the museum shops including postcards, model kits and stuffed toys. Visitors can no longer operate the mechanism since the device is now kept in a glass case.\n\nA small model of this toy is exhibited in Tipu Sultan's wooden palace in Bangalore. Although other items associated with Tipu, including his sword, have recently been purchased and brought back to India by billionaire Vijay Mallya, Tipu's Tiger has not itself been the subject of an official repatriation request, presumably due to the ambiguity underlying Tipu's image in the eyes of Indians; his being an object of loathing in the eyes of some Indians while considered a hero by others.\n\nTipu Sultan identified himself with tigers; his personal epithet was 'The Tiger of Mysore,' his soldiers were dressed in 'tyger' jackets, his personal symbol invoked a tiger's face through clever use of calligraphy and the tiger motif is visible on his throne, and other objects in his personal possession, including Tipu's Tiger. Accordingly, as per Joseph Sramek, for Tipu the tiger striking down the European in the organ represented his symbolic triumph over the British.\n\nThe British hunted tigers, not just to emulate the Mughals and other local elites in this \"royal\" sport, but also as a symbolic defeat of Tipu Sultan and any other ruler who stood in the path of British domination. The tiger motif was used in the \"Seringapatam medal\" which was awarded to those who participated in the 1799 campaign, where the British lion was depicted as overcoming a prostrate tiger, the tiger being the dynastic symbol of Tipu's line. The Seringapatam medal was issued in gold for the highest dignitaries who were associated with the campaign as well as select officers on general duty, silver for other dignitaries, field officers and other staff officers, in copper-bronze for the non-commissioned officers and in tin for the privates. On the reverse it had a frieze of the storming of the fort while the obverse showed, in the words of a nineteenth-century tome on medals, \"the BRITISH LION subduing the TIGER, the emblem of the late Tippoo Sultan's government, with the period when it was effected and the following words 'ASSUD OTTA-UL GHAULIB', signifying the Lion of God is the conqueror, or the conquering Lion of God.\"\nIn this manner, the iconography of this automaton was adopted and overturned by the British. When Tipu's Tiger was displayed in London in the nineteenth century, British viewers of the time \"characterised the tiger as a trophy and symbolic justification of British colonial rule\". Tipu's Tiger along with other trophies such as Tipu's sword, the throne of Ranjit Singh, Tantya Tope's \"kurta\" and Nana Saheb's betel-box which was made of brass, were all displayed as \"memorabilia of the Mutiny\".\n\nIn one interpretation, the display of Tipu's Tiger in South Kensington, served to remind the visitor of the \"noblesse oblige\" of the British Empire to bring civilisation to the barbaric lands of which Tipu was king.\n\nTipu's Tiger is also notable as a literal image of a tiger killing a European, an important symbol in England at the time, and from about 1820 the \"Death of Munro\" became one of the scenes in the repertoire of Staffordshire pottery figurines. Tiger-hunting in the British Raj, is also considered to represent not just the political subjugation of India, but in addition, the triumph over India's environment. The iconography persisted and during the rebellion of 1857, Punch ran a political cartoon showing the Indian rebels as a tiger, attacking a victim in an identical pose to Tipu's Tiger, being defeated by the British forces shown by the larger figure of a lion.\nMotives for collection of articles, such as Tipu's Tiger, are seen by literary historian Barrett Kalter as having a social and cultural context. The collection of Western and Indian art by Tipu Sultan is seen by Kalter as motivated by the need to display his wealth and legitimise his authority over his subjects who were predominantly Hindu and did not share his religion, viz. Islam. In the case of the East India Company, collection of documents, artefacts and \"objet's d'art\"Â  from India helped develop the idea of a subjugated Indian populace in the minds of the British people, the thought being that the possession of such objects of a culture represented understanding of, dominance over, and mastery of that culture.\nIn a detailed study published in 1987 of the tiger's musical and noise-making functions, Arthur W.J.G. Ord-Hume concluded that since coming to Britain, \"the instrument has been ruthlessly reworked, and in doing so much of its original operating principles have been destroyed\". There are two ranks of pipes in the organ (as opposed to the wailing and grunting functions), each \"comprising eighteen notes, [which] are nominally of 4ft pitch and are unisons - i.e. corresponding pipes in each register make sounds of the same musical pitch. This is an unusual layout for a pipe organ although while selecting the two stops together results in more sound ... there is also detectable a slight beat between the pipes so creating a celeste effect. ... it is considered likely that as so much work has been done ... this characteristic may be more an accident of tuning than an intentional feature\". The tiger's grunt is made by a single pipe in the tiger's head and the man's wail by a single pipe emerging at his mouth and connected to separate bellows located in the man's chest, where they can be accessed by unbolting and lifting off the tiger. The grunt operates by cogs gradually raising the weighted \"grunt-pipe\" until it reaches a point where it slips down \"to fall against its fixed lower-board or reservoir, discharging the air to form the grunting sound\" Today all the sound-making functions rely on the crank-handle to power them, though Ord-Hume believes this was not originally the case.\n\nWorks on the noise-making functions included those made over several decades by the famous organ-building firm Henry Willis & Sons, and Henry Willis III, who worked on the tiger in the 1950s, contributed an account to a monograph by Mildred Archer of the V&A. Ord-Hume is generally ready to exempt Willis work from his scathing comments on other drastic restorations, which \"vandalism\" is assumed to be by unknown earlier organ-builders.\nThere was a detailed account of the sound-making functions in \"The Penny Magazine\" in 1835, whose anonymous author evidently understood \"things mechanical and organs in particular\". From this and Ord-Hume's own investigations, he concluded that the original operation of the man's \"wail\" had been intermittent, with a wail only being produced after every dozen or so grunts from the tiger above, but that at some date after 1835 the mechanism had been altered to make the wail continuous, and that the bellows for the wail had been replaced with smaller and weaker ones, and the operation of the moving arm altered.\n\nPuzzling features of the present instrument include the placing of the handle, which when turned is likely to obstruct a player of the keyboard. Ord-Hume, using the 1835 account, concludes that originally the handle (which is a nineteenth-century British replacement, probably of a French original) only operated the grunt and wail, while the organ was operated by pulling a string or cord to work the original bellows, now replaced. The keyboard, which is largely original, is \"unique in construction\", with \"square ivory buttons\" with round lathe-turned tops instead of conventional keys. Though the mechanical functioning of each button is \"practical and convenient\" they are spaced such that \"it is almost impossible to stretch the hand to play an octave\". The buttons are marked with small black spots, differently placed but forming no apparent pattern in relation to the notes produced and corresponding to no known system of marking keys. The two stop control knobs for the organ are located, \"rather confusingly\", a little below the tiger's testicles. The instrument is now rarely played, but there is a V&A video of a recent performance.\n\nTipu's Tiger has provided inspiration to poets, sculptors, artists and others from the nineteenth century to the present day. The poet John Keats saw Tipu's Tiger at the museum in Leadenhall Street and worked it into his satirical verse of 1819, \"The Cap and Bells\". In the poem, a soothsayer visits the court of the Emperor Elfinan. He hears a strange noise and thinks the Emperor is snoring.\n\nThe French poet, Auguste Barbier, described the tiger and its workings and meditated on its meaning in his poem, \"Le Joujou du Sultan\" (The Plaything of the Sultan) published in 1837. More recently, the American Modernist poet, Marianne Moore wrote in her 1967 poem \"Tippoo's Tiger\" about the workings of the automaton, though in fact the tail was never movable:\n\n\"Die Seele\" (The Souls), a work by painter Jan Balet (1913â2009), shows an angel trumpeting over a flower garden while a tiger devours a uniformed French soldier. The Indian painter M. F. Husain painted Tipu's Tiger in his characteristic style in 1986 titling the work as \"Tipu Sultan's Tiger\". The sculptor Dhruva Mistry, when a student at the Royal College of Art, adjacent to the Victoria and Albert Museum, frequently passed Tipu's Tiger in its glass case and was inspired to make a fibre-glass and plastic sculpture \"Tipu\" in 1986. The sculpture \"Rabbit eating Astronaut\" (2004) by the artist Bill Reid is a humorous \"homage\" to the tiger, the rabbit \"chomping\" when its tail is cranked round.\n\n\n\n\n"}
{"id": "2360931", "url": "https://en.wikipedia.org/wiki?curid=2360931", "title": "Valentino Braitenberg", "text": "Valentino Braitenberg\n\nValentino Braitenberg (or \"Valentin von Braitenberg\"; 18 June 1926 â 9 September 2011) was an Italian neuroscientist and cyberneticist. He was former director at the Max Planck Institute for Biological Cybernetics in TÃ¼bingen, Germany.\n\nHis book \"Vehicles: Experiments in Synthetic Psychology\" became famous in Robotics and among Psychologists, in which he described how hypothetical analog vehicles (a combination of sensors, actuators and their interconnections), though simple in design, can exhibit behaviors akin to aggression, love, foresight, and optimism. These have come to be known as Braitenberg vehicles. His pioneering scientific work was concerned with the relation between structures and functions of the brain.\n\nValentino Braitenberg grew up in the province of South Tyrol. Braitenberg's father was Senator , a member of the South Tyrolean nobility.\n\nSince the age of 6, Braitenberg grew up bilingual in the two languages Italian and German. German was spoken at home and all schooling was Italian, conform to the historic context. The humanistic Lyceum-Gymnasium (High school) in Bolzano gave him an excellent classic education including Italian literature. The German literary education was based on the classical writers he found in the extensive home library. In addition, he trained as a violinist at the in Bolzano and became a talented violinist and violist.\n\nBraitenberg studied Medicine and Psychiatry at the Universities of Innsbruck and Rome between 1945 and 1954. He accompanied his studies with chamber music performances with his viola and violin, where he developed a repertoire of violin-piano duos with a colleague. . He completed his medical training with an internship at the psychiatric clinic in Rome, where he decided to prefer a scientific career dedicated to the understanding of brain functions. He spent a few years at Yale University in New Haven (USA) when he was invited by Prof. Eduardo Caianiello in 1958 to set up a biocybernetics research group at the Physics Institute of the University of Naples Federico II, the âLaboratorio di Ciberneticaâ. Between 1958 and 1968 he was adjunct Professor of Cybernetics at the Physics Institute of the University of Naples. In 1963 Braitenberg earned the Libera Docenza in Cybernetics and Information Theory, the title that used to grant access to Professorship at Italian Universities. From 1968 until his retirement in 1994 he was co-founder and co-Director of the Max Planck Institute for Biological Cybernetics in TÃ¼bingen and Honorary Professor at the Universities of TÃ¼bingen and Freiburg. After 1994 he was appointed Professor at the Specialization School in Scienze Motorie (Motoric Sciences) at the Rovereto branch of the University of Rovereto. From 1998 to 2001 he was president of the Laboratorio di Scienze Cognitive at the University of Trento in Rovereto.\n\nBraitenberg received an honorary doctorate from the University of Salzburg in 1995.\n\nBraitenberg was married to the painter Elisabeth Hanna. They had three children, Margareta, Carla, and Zeno.\n\nAccording to Maier (2012), Braitenberg's interest in understanding the brain began in 1948, when he looked for the first time at some human brain tissue under a microscope. He said that although the connections seemed unbelievably complex, Braitenberg eventually realised that computers could serve as a useful model for understanding the brain. She said that he made seminal contributions to understanding the neuroanatomy of the cerebellum, the wiring of the eye of the fly, and the organisation of the human cerebrum.\n\nBraitenberg published more than 180 scientific works during his lifetime, not including abstracts, reprints, translations into different languages, and different editions of some of his works.\nAccording to a search of Google Scholar in September 2014, Braitenberg's book, \"Vehicles: Experiments in synthetic psychology\", had received at least 2622 citations.\n\nBooks published by Braitenberg include:\n\n\n\n\n"}
{"id": "48341518", "url": "https://en.wikipedia.org/wiki?curid=48341518", "title": "WT1190F", "text": "WT1190F\n\nWT1190F (9U01FF6, UDA34A3, or UW8551D) was a small temporary satellite of Earth that impacted Earth on 13 November 2015 at 06:18:34.3 (Â±1.3 seconds) UTC. It is thought to have been space debris from the trans-lunar injection stage of the 1998 Lunar Prospector mission. It was first discovered on 18 February 2013 by the Catalina Sky Survey. It was then lost, and reacquired on 29 November 2013. It was again discovered on 3 October 2015, and the object was soon identified to be the same as the two objects previously sighted by the team, who have been sharing their data through the International Astronomical Union's Minor Planet Center (MPC). An early orbit calculation showed that it was orbiting Earth in an extremely elliptical orbit, taking it from within the geosynchronous satellite ring to nearly twice the distance of the Moon. It was also probably the same object as 9U01FF6, another object on a similar orbit discovered on 26 October 2009.\n\nWT1190F had been orbiting Earth as a temporary satellite since mid 2009 (named as UWAIS), if not longer. While it has not been positively identified with any known artificial satellites, its estimated density of 0.1 g/cmÂ³ was much lower than would be expected of a natural object as even water has a density of 1 g/cmÂ³. Hence, European Space Agency astronomers have concluded that the object was likely a fuel tank of some sort.\n\nAfter more observations, astronomers determined that the object would impact the Earth on 13 November 2015 at 06:18 UTC (11:48 local time), south of Sri Lanka. Due to its small size, it was expected that most or all of the object would burn up in the atmosphere before impacting, but would be visible as a bright daytime fireball if the sky was not badly overcast.\n\nA ground-based observational campaign was organized as a possible test for future collision events involving also natural bodies.\n\nWT1190F was first discovered by the Mount Lemmon Survey, a participant in the Catalina Sky Survey Near-Earth Object surveying program. The object was identified with an apparent magnitude 19.5 on 18 February 2013, and given the temporary designation UDA34A3, but was lost soon after, with an observation arc of only 5 hours. However, it was again seen by the same survey on 29 November 2013 and given the designation UW8551D and lost again, only being observed for 1 hour 35 minutes.\n\nMost recently, it was recovered on 3 October 2015 and given the designation WT1190F. Its orbit was soon calculated and found to be orbiting Earth, but not with the orbit of any known artificial satellite. The object's orbit was soon connected, allowing more observations to be made, and several precovery observations have been found of the object, dating back to June 2009.\n\nThe type of orbit that WT1190F had was not stable long-term. An object in this type of orbit was likely to impact into Earth or the Moon, or acquire enough orbital speed to be ejected into orbit around the Sun. It was not likely that it had been orbiting Earth for decades. In 2011 the orbit had an eccentricity of 0.33 and perigee (closest approach to Earth) of . It passed about from the Moon on 24 May 2012. By 2013 the eccentricity had increased to 0.70 and the perigee decreased to .\n\nDuring WT1190F's orbit, it changed significantly in brightness, from an apparent magnitude 16 at perigee, to magnitude 23 at apogee. It spent most of its time dimmer than magnitude 20. This, combined with solar pressure acceleration, the Yarkovsky effect, and frequent orbital perturbations by the Moon, made it difficult to precisely predict its orbit and location. About one hour before atmospheric entry, the object had a R magnitude of 13.6, roughly the brightness of Pluto.\n\nWT1190F made atmospheric entry at . Whatever was left from the re-entry was calculated to have fallen into the ocean about 100Â km off of Galle, Sri Lanka. The closest approach to Galle occurred during atmospheric flight when the object had an altitude of 45km and a distance of 51km. For observers in Colombo, Sri Lanka, the object started out 30 degrees above the horizon coming in from slightly south of due west. Its mass was not sufficient to cause any risk to the area, but the event still produced a bright fireball. Scientists wanted to study WT1190F to better understand the trajectory and atmospheric entry of satellites, debris, and small asteroids from translunar orbit. The International Astronomical Center (IAC) and the United Arab Emirates Space Agency utilized a Gulfstream 450 jet to study the re-entry from above the clouds and haze. The airborne observation team successfully captured the re-entry on video.\n\nThe International Astronomical Center (IAC) and the United Arab Emirates Space Agency observed WT1190F as it fell towards the Earth. The IAC chartered a Gulfstream 450 jet to bring researchers such as Peter Jenniskens to the area of WT1190F's impact, at a high altitude, to view the event over clouds or haze. The Next TC3 Consortium Asteroid Detection and Early Warning team narrowed the atmospheric entry time to Â±1.3 seconds.\n\nObservers on the ground could not see the fireball because of rain, but the plane was able to find an opening in the clouds. The fireball was a bright naked eye object. Spectroscopic data was acquired that will help determine what the object was made of.\n\n\n\n"}
{"id": "46626433", "url": "https://en.wikipedia.org/wiki?curid=46626433", "title": "Willy Sachs", "text": "Willy Sachs\n\nWilly Sachs (23 July 1896 â 19 November 1958) was a German industrialist. He served the Third Reich as \"ObersturmbannfÃ¼hrer\" and \"WehrwirtschaftsfÃ¼hrer\" and was awarded the Federal Cross of Merit after the Second World War. Sachs was an honorary citizen of Schweinfurt, Mainberg and Oberaudorf.\n\nWilly Sachs was born in Schweinfurt, the only son of the industrialist Ernst Sachs. After internships with several international companies Sachs joined his father's company in 1923 as a board member, and upon the senior Sachs' death in 1932, became the sole owner of Fichtel & Sachs AG in Schweinfurt. Sachs was seen as a caring patriarch, often given to spontaneous generosity. He saw it as his mission in life to share his father's work with the next generation. However, he inherited little of his father's talent at management. Although he held the title of General Director, the company of 7,000 workers was, by 1939, actually run by its directors Heinz Kaiser, Rudolf Baier and Michael Schlegelmilch. Sachs turned to hunting, women, and alcohol as diversions. His lavish parties at Schloss Mainberg and on the Rechenau became legendary. It was said, \"wherever there was a party, the consul [Sachs] was there.\" (Sachs had inherited the title of Royal Swedish Consul from his father upon whom it had been bestowed for his work with SKF.)\n\nIn 1933, Sachs became a member of the SS and the Nazi Party. As the head of an important arms manufacturer, he was named \"WehrwirtschaftsfÃ¼hrer\" [\"War Industry Leader\"]. Heinrich Himmler awarded him medals and honorary titles (including \"ObersturmbannfÃ¼hrer\" in 1943) and helped with Sachs' divorce from Elinor von Opel and the ensuing custody battle for their children. Hermann GÃ¶ring was a guest of Sachs' hunting outings in Mainberg.\nIn 1936, as patron of 1. FC Schweinfurt 05, Sachs donated the stadium that bears his name, the Willy-Sachs-Stadion in Schweinfurt. This gift to the city secured his lasting popularity beyond death. As part of the Schweinfurt \"Lest we forget\" initiative, the local press (including \"SÃ¼ddeutsche Zeitung\", Gerhard Fischer and Werner Skrentny) initiated a campaign to rename the stadium due to Sachs' Nazi affiliation. The campaign met with low approval among the general public.\n\nIn May 1945, Sachs was arrested by the American military in Oberaudorf and held until February 1947. During the denazification process following World War II, he was twice labeled a \"follower\" (Category IV). Author Wilfried Rott has labeled this process a \"whitewashing\".\n\nAfter his release, at the age of 51, Sachs officially retired from active management and was relegated as Chairman of the Supervisory Board to ceremonial duties. In recognition of his philanthropy (including restoration of the Ernst Sachs Assistance organization as the Occupation Pensions Authority), Sachs was awarded the Order of Merit.\n\nSachs spent his last years mostly on the family estate (Sachs Rechenau) at Oberaudorf. On November 19, 1958, he committed suicide at the age of 62, driven by depression and fear of blackmail. Willy Sachs was laid to rest to the great sympathy of the populace.\n\nSachs was married to Elinor von Opel, daughter of Wilhelm von Opel, from 1925 to 1935. They had two sons: Ernst Wilhelm (1929-1977) and Gunter (1932-2011).\nFrom 1937 to 1947, he was married to Ursula Meyer, of Prey, Vosges.\nFollowing his 1947 divorce, Sachs lived with his partner Catherine HirnbÃ¶ck, with whom he had one child: Peter Sachs (born 1950). Sachs officially adopted Peter in 1957.\n"}
