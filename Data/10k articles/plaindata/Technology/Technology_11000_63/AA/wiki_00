{"id": "5965480", "url": "https://en.wikipedia.org/wiki?curid=5965480", "title": "Aperture (computer memory)", "text": "Aperture (computer memory)\n\nIn computing, an aperture is a portion of physical address space (i.e. physical memory) that is associated with a particular peripheral device or a memory unit. Apertures may reach external devices such as ROM or RAM chips, or internal memory on the CPU itself.\n\nTypically, a memory device attached to a computer accepts addresses starting at zero, and so a system with more than one such device would have ambiguous addressing. To resolve this, the memory logic will contain several aperture selectors, each containing a range selector and an interface to one of the memory devices. The set of selector address ranges of the apertures are disjoint. When the CPU presents a physical address within the range recognized by an aperture, the aperture unit routes the request (with the address remapped to a zero base) to the attached device. Thus, apertures form a layer of address translation below the level of the usual virtual-to-physical mapping.\n\n"}
{"id": "38946837", "url": "https://en.wikipedia.org/wiki?curid=38946837", "title": "Autapse", "text": "Autapse\n\nAn autapse is a chemical or electrical synapse from a neuron onto itself. It can also be described as a synapse formed by the axon of a neuron on its own dendrites, \"in vivo\" or \"in vitro\".\n\nThe term \"autapse\" was first coined in 1972 by Van der Loos and Glaser, who observed them in Golgi preparations of the rabbit occipital cortex while originally conducting a quantitative analysis of rabbit neocortex circuitry. Also in the 1970s, autapses have been described in dog and rat cerebral cortex, monkey neostriatum, and cat spinal cord.\n\nIn 2000, they were first modeled as supporting persistence in recurrent neural networks. In 2004, they were modeled as demonstrating oscillatory behavior, which was absent in the same model neuron without autapse. More specifically, the neuron oscillated between high firing rates and firing suppression, reflecting the spike bursting behavior typically found in cerebral neurons. In 2009, autapses were, for the first time, associated with sustained activation. This proposed a possible function for excitatory autapses within a neural circuit. In 2014, electrical autapses were shown to generate stable target and spiral waves in a neural model network. This indicated that they played a significant role in stimulating and regulating the collective behavior of neurons in the network. In 2016, a model of resonance was offered.\n\nAutapses have been used to simulate \"same cell\" conditions to help researchers make quantitative comparisons, such as studying how \"N\"-methyl-D-aspartate receptor (NMDAR) antagonists affect synaptic versus extrasynaptic NMDARs.\n\nRecently, it has been proposed that autapses could possibly form as a result of neuronal signal transmission blockage, such as in cases of axonal injury induced by poisoning or impeding ion channels. Dendrites from the soma in addition to an auxiliary axon may develop to form an autapse to help remediate the neuron's signal transmission.\n\nAutapses can be either glutamate-releasing (excitatory) or GABA-releasing (inhibitory), just like their traditional synapse counterparts. Similarly, autapses can be electrical or chemical by nature.\n\nBroadly speaking, negative feedback in autapses tends to inhibit excitable neurons whereas positive feedback can stimulate quiescent neurons.\n\nAlthough the stimulation of inhibitory autapses did not induce hyperpolarizing inhibitory post-synaptic potentials in interneurons of layer V of neocortical slices, they have been shown to impact excitability. Upon using a GABA-antagonist to block autapses, the likelihood of an immediate subsequent second depolarization step increased following a first depolarization step. This suggests that autapses act by suppressing the second of two closely timed depolarization steps and therefore, they may provide feedback inhibition onto these cells. This mechanism may also potentially explain shunting inhibition.\n\nIn cell culture, autapses have been shown to contribute to the prolonged activation of B31/B32 neurons, which significantly contribute food-response behavior in \"Aplysia\". This suggests that autapses may play a role in mediating positive feedback. It is important to note that the B31/B32 autapse was unable to play a role in initiating the neuron's activity, although it is believed to have helped sustain the neuron's depolarized state. The extent to which autapses maintain depolarization remains unclear, particularly since other components of the neural circuit (i.e. B63 neurons) are also capable of providing strong synaptic input throughout the depolarization. Additionally, it has been suggested that autapses provide B31/B32 neurons with the ability to quickly repolarize. Bekkers (2009) has proposed that specifically blocking the contribution of autapses and then assessing the differences with or without blocked autapses could better illuminate the function of autapses.\n\nHindmarsh–Rose (HR) model neurons have demonstrated chaotic, regular spiking, quiescent, and periodic patterns of burst firing without autapses. Upon the introduction of an electrical autapse, the periodic state switches to the chaotic state and displays an alternating behavior that increases in frequency with a greater autaptic intensity and time delay. On the other hand, excitatory chemical autapses enhanced the overall chaotic state. The chaotic state was reduced and suppressed in the neurons with inhibitory chemical autapses. In HR model neurons without autapses, the pattern of firing altered from quiescent to periodic and then to chaotic as DC current was increased. Generally, HR model neurons with autapses have the ability to swap into any firing pattern, regardless of the prior firing pattern.\n\nNeurons from several brain regions, such as the neocortex, substantia nigra, and hippocampus have been found to contain autapses.\n\nAutapses have been observed to be relatively more abundant in GABAergic basket and dendrite-targeting cells of the cat visual cortex compared to spiny stellate, double bouquet, and pyramidal cells, suggesting that the degree of neuron self-innervation is cell-specific. Additionally, dendrite-targeting cell autapses were, on average, further from the soma compared to basket cell autapses.\n\n80% of layer V pyramidal neurons in developing rat neocortices contained autaptic connections, which were located more so on basal dendrites and apical oblique dendrites rather than main apical dendrites. The dendritic positions of synaptic connections of the same cell type were similar to those of autapses, suggesting that autaptic and synaptic networks share a common mechanism of formation.\n\nIn the 1990s, paroxysmal depolarizing shift-type interictal epileptiform discharges has been suggested to be primarily dependent on autaptic activity for solitary excitatory hippocampal rat neurons grown in microculture.\n\nMore recently, in human neocortical tissues of patients with intractable epilepsy, the GABAergic output autapses of fast-spiking (FS) neurons have been shown to have stronger asynchronous release (AR) compared to both non-epileptic tissue and other types of synapses involving FS neurons. The study found similar results using a rat model as well. An increase in residual Ca2+ concentration in addition to the action potential amplitude in FS neurons was suggested to cause this increase in AR of epileptic tissue. Anti-epileptic drugs could potentially target this AR of GABA that seems to rampantly occur at FS neuron autapses.\n\nUsing a glia-conditioned medium to treat glia-free purified rat retinal ganglion microcultures has been shown to significantly increase the number of autapses per neuron compared to a control. This suggests that glia-derived soluble, proteinase K-sensitive factors induce autapse formation in rat retinal ganglion cells.\n"}
{"id": "995679", "url": "https://en.wikipedia.org/wiki?curid=995679", "title": "Autofocus", "text": "Autofocus\n\nAn autofocus (or AF) optical system uses a sensor, a control system and a motor to focus on an automatically or manually selected point or area. An electronic rangefinder has a display instead of the motor; the adjustment of the optical system has to be done manually until indication. Autofocus methods are distinguished by their type as being either active, passive or hybrid variants.\n\nAutofocus systems rely on one or more sensors to determine correct focus. Some AF systems rely on a single sensor, while others use an array of sensors. Most modern SLR cameras use through-the-lens optical sensors, with a separate sensor array providing light metering, although the latter can be programmed to prioritize its metering to the same area as one or more of the AF sensors.\n\nThrough-the-lens optical autofocusing is now often speedier and more precise than can be achieved manually with an ordinary viewfinder, although more precise manual focus can be achieved with special accessories such as focusing magnifiers. Autofocus accuracy within 1/3 of the depth of field (DOF) at the widest aperture of the lens is common in professional AF SLR cameras.\n\nMost multi-sensor AF cameras allow manual selection of the active sensor, and many offer automatic selection of the sensor using algorithms which attempt to discern the location of the subject. Some AF cameras are able to detect whether the subject is moving towards or away from the camera, including speed and acceleration data, and keep focus on the subject — a function used mainly in sports and other action photography; on Canon cameras this is known as AI servo, while on Nikon cameras it is known as \"continuous focus\".\n\nThe data collected from AF sensors is used to control an electromechanical system that adjusts the focus of the optical system. A variation of autofocus is an \"electronic rangefinder\", a system in which focus data are provided to the operator, but adjustment of the optical system is still performed manually.\n\nThe speed of the AF system is highly dependent on the widest aperture offered by the lens. F-stops of around 2 to 2.8 are generally considered optimal in terms of focusing speed and accuracy. Faster lenses than this (e.g.: 1.4 or 1.8) typically have very low depth of field, meaning that it takes longer to achieve correct focus, despite the increased amount of light.\n\nMost consumer camera systems will only autofocus reliably with lenses that have a widest aperture of at least 5.6, while professional models can often cope with lenses that have a widest aperture of 8, which is particularly useful for lenses used in conjunction with teleconverters. \n\nBetween 1960 and 1973, Leitz (Leica) patented an array of autofocus and corresponding sensor technologies. At photokina 1976, Leica had presented a camera based on their previous development, named Correfot, and in 1978 they displayed an SLR camera with fully operational autofocus. The first mass-produced autofocus camera was the Konica C35 AF, a simple point and shoot model released in 1977. The Polaroid SX-70 Sonar OneStep was the first autofocus single-lens reflex camera, released in 1978. The Pentax ME-F, which used focus sensors in the camera body coupled with a motorized lens, became the first autofocus 35 mm SLR in 1981. In 1983 Nikon released the F3AF, their first autofocus camera, which was based on a similar concept to the ME-F. The Minolta 7000, released in 1985, was the first SLR with an integrated autofocus system, meaning both the AF sensors and the drive motor were housed in the camera body, as well as an integrated film advance winder — which was to become the standard configuration for SLR cameras from this manufacturer, and also Nikon abandoned their F3AF system and integrated the autofocus-motor and sensors in the body. Canon, however, elected to develop their EOS system with motorised lenses instead. In 1992, Nikon changed back to lens integrated motors with their AF-I and AF-S range of lenses; today their entry-level DSLRs do not have a focus motor in the body due to a broad range of available lenses with internal focus motors.\n\nActive AF systems measure distance to the subject independently of the optical system, and subsequently adjust the optical system for correct focus.\n\nThere are various ways to measure distance, including ultrasonic sound waves and infrared light. In the first case, sound waves are emitted from the camera, and by measuring the delay in their reflection, distance to the subject is calculated. Polaroid cameras including the Spectra and SX-70 were known for successfully applying this system. In the latter case, infrared light is usually used to triangulate the distance to the subject. Compact cameras including the Nikon 35TiQD and 28TiQD, the Canon AF35M, and the Contax T2 and T3, as well as early video cameras, used this system. A newer approach included in some consumer electronic devices, like mobile phones, is based on the time-of-flight principle, which involves shining a laser or LED light to the subject and calculating the distance based on the time it takes for the light to travel to the subject and back. This technique is sometimes called \"laser autofocus\", and is present in many mobile phone models from several vendors.\n\nAn exception to the two-step approach is the mechanical autofocus provided in some enlargers, which adjust the lens directly.\n\nPassive AF systems determine correct focus by performing passive analysis of the image that is entering the optical system. They generally do not direct any energy, such as ultrasonic sound or infrared light waves, toward the subject. (However, an autofocus assist beam of usually infrared light is required when there is not enough light to take passive measurements.) Passive autofocusing can be achieved by phase detection or contrast measurement.\n\nPhase detection (PD) is achieved by dividing the incoming light into pairs of images and comparing them. Through-the-lens secondary image registration (TTL SIR) passive phase detection is often used in film and digital SLR cameras. The system uses a beam splitter (implemented as a small semi-transparent area of the main reflex mirror, coupled with a small secondary mirror) to direct light to an AF sensor at the bottom of the camera. Two micro-lenses capture the light rays coming from the opposite sides of the lens and divert it to the AF sensor, creating a simple rangefinder with a base within the lens's diameter. The two images are then analysed for similar light intensity patterns (peaks and valleys) and the separation error is calculated in order to find whether the object is in front focus or back focus position. This gives the direction and an estimate of the required amount of focus-ring movement.\n\nPD AF in a continuously focusing mode (e.g. \"AI Servo\" for Canon, \"AF-C\" for Nikon, Pentax and Sony) is a closed-loop control process. PD AF in a focus-locking mode (e.g. \"One-Shot\" for Canon, \"AF-S\" for Nikon and Sony) is widely believed to be a \"one measurement, one movement\" open-loop control process, but focus is confirmed only when the AF sensor sees an in-focus subject. The only apparent differences between the two modes are that a focus-locking mode halts on focus confirmation, and a continuously focusing mode has predictive elements to work with moving targets, which suggests they are the same closed-loop process.\n\nAlthough AF sensors are typically one-dimensional photosensitive strips (only a few pixels high and a few dozen wide), some modern cameras (Canon EOS-1V, Canon EOS-1D, Nikon D2X) feature TTL area SIR sensors that are rectangular in shape and provide two-dimensional intensity patterns for a finer-grain analysis. Cross-type focus points have a pair of sensors oriented at 90° to one another, although one sensor typically requires a larger aperture to operate than the other.\n\nSome cameras (Minolta 7, Canon EOS-1V, 1D, 30D/40D, Sony DSLR-A700, DSLR-A850, DSLR-A900) also have a few \"high-precision\" focus points with an additional set of prisms and sensors; they are only active with \"fast lenses\" with certain geometrical apertures (typically f-number 2.8 and faster). Extended precision comes from the wider effective measurement base of the \"range finder\".\n\nContrast-detection autofocus is achieved by measuring contrast within a sensor field through the lens. The intensity difference between adjacent pixels of the sensor naturally increases with correct image focus. The optical system can thereby be adjusted until the maximal contrast is detected. In this method, AF does not involve actual distance measurement at all. This creates significant challenges when tracking moving subjects, since a loss of contrast gives no indication of the direction of motion towards or away from the camera. \n\nContrast-detect autofocus is a common method in digital cameras that lack shutters and reflex mirrors. Most DSLRs use this method (or a hybrid of both contrast and phase-detection autofocus) when focusing in their live-view modes. A notable exception is Canon digital cameras with Dual Pixel CMOS AF. Mirrorless interchangeable-lens cameras typically used contrast-measurement autofocus, although phase detection has become the norm on most mirrorless cameras giving them significantly better AF tracking performance compared to contrast detection.\n\nContrast detection places different constraints on lens design when compared with phase detection. While phase detection requires the lens to move its focus point quickly and directly to a new position, contrast-detection autofocus instead employs lenses that can quickly sweep through the focal range, stopping precisely at the point where maximal contrast is detected. This means that lenses designed for phase detection often perform poorly on camera bodies that use contrast detection.\n\nThe assist light (also known as AF illuminator) \"activates\" passive autofocus systems in low-light and low-contrast situations in some cameras. The lamp projects visible or IR light onto the subject, which the camera's autofocus system uses to achieve focus. Many cameras that do not have a dedicated autofocus assist lamp instead use their built-in flash, illuminating the subject with stroboscopic bursts of light. The strobe bursts aid the autofocus system in the same fashion as a dedicated assist light, but have the disadvantage of startling or annoying living subjects. Another disadvantage is that if the camera uses flash focus assist and is set to an operation mode that overrides the flash, it may also disable the focus assist and autofocus may fail to acquire the subject. Similar stroboscopic flashing is sometime used to reduce the red-eye effect, but this method is only intended to constrict the subject's eye pupils prior to the actual shot being taken, and thus reduce retinal reflections.\n\nIn some cases, external flash guns have integrated autofocus assist lamps that replace the stroboscopic on-camera flash. Another way to assist contrast based AF systems in low light is to beam a laser pattern on to the subject. The laser method is commercially called Hologram AF Laser and was used in Sony Cybershot cameras around the year 2003, including Sony's F707, F717 and F828 models.\n\nIn a hybrid autofocus system, focus is achieved by combining two or more methods, such as:\n\n\nThe double effort is typically used to mutually compensate for the intrinsical weaknesses of the various methods in order to increase the overall reliability and accuracy or to speed up AF function.\n\nA rare example of an early hybrid system is the combination of an active IR or ultrasonic auto-focus system with a passive phase-detection system. An IR or ultrasonic system based on timed reflection will work regardless of the light conditions, but can be easily fooled by obstacles like window glasses, and the accuracy is typically restricted to a rather limited number of steps. Phase-detection autofocus \"sees\" through window glasses without problems and is much more accurate, but it does not work in low-light conditions or on surfaces without contrasts or with repeating patterns.\n\nA very common example of combined usage is the phase-detection auto-focus system used in single-lens reflex cameras since the 1985s. The passive phase-detection auto-focus needs some contrast to work with, making it difficult to use in low-light scenarios or on even surfaces. An AF illuminator will illuminate the scene and project contrast patterns onto even surfaces, so that phase-detection auto-focus can work under these conditions as well.\n\nA newer form of a hybrid system is the combination of passive phase-detection auto-focus and passive contrast auto-focus, sometimes assisted by active methods, as both methods need some visible contrast to work with. Under their operational conditions, phase-detection auto-focusing is very fast, since the measurement method provides both information, the amount of offset and the direction, so that the focusing motor can move the lens right into (or close to) focus without additional measurements. Additional measurements on the fly, however, can improve accuracy or help keep track of moving objects. However, the accuracy of phase-detection auto-focus depends on its effective measurement basis. If the measurement basis is large, measurements are very accurate, but can only work with lenses with a large geometrical aperture (e.g. 1:2.8 or larger). Even with high contrasty objects, phase-detection AF cannot work at all with lenses slower than its effective measurement basis. In order to work with most lenses, the effective measurement basis is typically set to between 1:5.6 and 1:6.7, so that AF continues to work with slow lenses (at least for as long as they are not stopped down). This, however, reduces the intrinsical accuracy of the autofocus system, even if fast lenses are used. Since the effective measurement basis is an optical property of the actual implementation, it cannot be changed easily. Very few cameras provide multi-PD-AF systems with several switchable measurement bases depending on the lens used in order to allow normal auto-focusing with most lenses, and more accurate focusing with fast lenses.\nContrast AF does not have this inherit design limitation on accuracy as it only needs a minimal object contrast to work with. Once this is available, it can work with high accuracy regardless of the speed of a lens; in fact, for as long as this condition is met, it can even work with the lens stopped down. Also, since contrast AF continues to work in stopped-down mode rather than only in open-aperture mode, it is immune to aperture-based focus shift errors phase-detection AF systems suffer since they cannot work in stopped-down mode. Thereby, contrast AF makes arbitrary fine-focus adjustments by the user unnecessary. Also, contrast AF is immune to focusing errors due to surfaces with repeating patterns and they can work over the whole frame, not just near the center of the frame, as phase-detection AF does. The down-side, however, is that contrast AF is a closed-loop iterative process of shifting the focus back and forth in rapid succession. Compared to phase-detection AF, contrast AF is slow, since the speed of the focus iteration process is mechanically limited and this measurement method does not provide any directional information. Combining both measurement methods, the phase-detection AF can assist a contrast AF system to be fast and accurate at the same time, to compensate aperture-based focus-shift errors, and to continue to work with lenses stopped down, as, for example, in stopped-down measuring or video mode.\n\nRecent developments towards mirrorless cameras seek to integrate the phase-detection AF sensors into the image sensor itself. Typically, these phase-detection sensors are not as accurate as the more sophisticated stand-alone sensors, but since the fine focusing is now carried out through contrast focusing, the phase-detection AF sensors are only need to provide coarse directional information in order to speed up the contrast auto-focusing process.\n\nIn July, 2010, Fujifilm announced a compact camera, the F300EXR, which included a hybrid autofocus system consisting of both phase-detection and contrast-based elements. The sensors implementing the phase-detection AF in this camera are integrated into the camera's Super CCD EXR. Currently it is used by Fujifilm FinePix Series, Fujifilm X100S, Ricoh, Nikon 1 series, Canon EOS 650D/Rebel T4i and Samsung NX300.\n\nActive systems will typically not focus through windows, since sound waves and infrared light are reflected by the glass. With passive systems this will generally not be a problem, unless the window is stained. Accuracy of active autofocus systems is often considerably less than that of passive systems.\n\nActive systems may also fail to focus a subject that is very close to the camera (e.g., macro photography).\n\nPassive systems may not find focus when the contrast is low, notably on large single-colored surfaces (walls, blue sky, etc.) or in low-light conditions. Passive systems are dependent on a certain degree of illumination to the subject (whether natural or otherwise), while active systems may focus correctly even in total darkness when necessary. Some cameras and external flash units have a special low-level illumination mode (usually orange/red light) which can be activated during auto-focus operation to allow the camera to focus.\n\nA method variously referred to as \"trap focus\", \"focus trap\", or \"catch-in-focus\" uses autofocus to take a shot when a subject moves into the focal plane (at the relevant focal point); this can be used to get a focused shot of a rapidly moving object, particularly in sports or wildlife photography, or alternatively to set a \"trap\" so that a shot can automatically be taken without a person present. This is done by using AF to \"detect\" but not \"set\" focus – using manual focus to set focus (or switching to manual after focus has been set) but then using \"focus priority\" to detect focus and only release the shutter when an object is in focus. The technique works by choosing the focus adjustment (turning AF off), then setting the shooting mode to \"Single\" (AF-S), or more specifically focus priority, then depressing the shutter – when the subject moves into focus, the AF detects this (though it does not change the focus), and a shot is taken.\n\nThe first SLR to implement trap focusing was the Yashica 230 AF. Trap focus is also possible on some Pentax (e.g. K-x and K-5), Nikon, and Canon EOS cameras. The EOS 1D can do it using software on an attached computer, whereas cameras like the EOS 40D and 7D have a custom function (III-1 and III-4 respectively) which can stop the camera trying to focus after it fails. On EOS cameras without genuine trap focus, a hack called \"almost trap focus\" can be used, which achieves some of the effects of trap focus. By using the custom firmware Magic Lantern, some Canon DSLRs can perform trap focus.\n\nAI Servo is an autofocus mode found on Canon SLR cameras. The same principle is used by other brands such as Nikon, Sony, and Pentax, called \"continuous focus\" (AF-C). Also referred to as \"focus tracking\", it is used to track a subject as it moves around the frame, or toward and away from the camera. When in use, the lens will constantly maintain its focus on the subject, hence it is commonly used for sports and action photography. AI refers to artificial intelligence: algorithms that constantly predict where a subject is about to be based on its speed and acceleration data from the autofocus sensor.\n\nModern autofocus is done through one of two mechanisms; either a motor in the camera body and gears in the lens (\"screw drive\") or through electronic transmission of the drive instruction through contacts in the mount plate to a motor in the lens. Lens-based motors can be of a number of different types, but are often ultrasonic motors or stepper motors.\n\nSome camera bodies, including all Canon EOS bodies and the more budget-oriented among Nikon's DX models, do not include an autofocus motor and therefore cannot autofocus with lenses that lack an inbuilt motor. Some lenses, such as Pentax' DA* designated models, although normally using an inbuilt motor, can fall back to screwdrive operation when the camera body does not support the necessary contact pins.\n\n\n\n"}
{"id": "4039699", "url": "https://en.wikipedia.org/wiki?curid=4039699", "title": "Baldwin Hills Dam disaster", "text": "Baldwin Hills Dam disaster\n\nThe Baldwin Hills Dam disaster occurred on December 14, 1963, when the dam containing the Baldwin Hills Reservoir suffered a catastrophic failure and flooded the residential neighborhoods surrounding it. It began with signs of lining failure, followed by increasingly serious leakage through the dam at its east abutment. After three hours the dam breached, with a total release of , resulting in five deaths and the destruction of 277 homes. Vigorous rescue efforts averted a greater loss of life.\n\nThe reservoir was located on a low hilltop in Baldwin Hills, Los Angeles, California. It was constructed between 1947 and 1951 by the Los Angeles Department of Water and Power directly on an active fault line, which was subsidiary to the well known nearby Newport–Inglewood Fault. The underlying geologic strata were considered unstable for a reservoir, and the design called for a compacted soil lining meant to prevent seepage into the foundation. The fault lines were considered during planning but were deemed by some, although not all, of the engineers and geologists involved as not significant.\n\nThe former reservoir is now part of the Kenneth Hahn State Recreation Area.\n\nThe failure of the Baldwin Hills Reservoir received an exceptional amount of attention from the civil engineering community and remains the subject of continuing interest. The reservoir had been conceived, designed, and built during and after World War II, a time when the pace of dam building was accelerating even as some disastrous dam failures were occurring, indicating a need for safer technologies. The builders of the Baldwin Hills dam, the Los Angeles Department of Water and Power, were aware of the difficult geologic conditions presented by the site and knew from past experiences, notably the catastrophic failure of the St. Francis Dam in 1928 in which over 400 people lost their lives, the serious consequences of a failure, even of a small reservoir in an urban setting. But this was also an era of new engineering ventures onland, sea, and space, with new technologies boldly advanced to meet what were seen as hostile challenges from both nature and communist ideologies. While dams were recognized as potentially dangerous, like nuclear technologies, they were also considered by Americans as a showcase technology—a means of fending off danger and spreading progressive American technologies and associated social benefits at home and abroad.\n\nThe Baldwin Hills dam designer, engineer Ralph Proctor, had also worked as an assistant civil engineer for the Los Angeles Department of Water and Power on the failed St. Francis Dam and had subsequently devised new methods of producing compacted earth fill in building its replacement. Proctor aggressively proceeded with the Baldwin Hills project even in the face of safety concerns and disagreements over important design details raised within his own department.\n\nLate 1963, when the failure occurred, was a time of another notable public disaster. Only two months before at the Vajont Dam in Italy, a massive landslide into the reservoir behind it, created a seiche which overtopped the dam flooding the valley below and causing the deaths of approximately 2000 people. The Baldwin Hills Reservoir had been built, as were others, to assure an ample supply of safe water for the people of Los Angeles in case of catastrophe such as earthquake, fire, or war, and its failure was a blow to engineering confidence and the subject of many writings and two professional conferences (1972 and 1987, see references). The failure occurred shortly after the death of the authoritative Harvard engineer Karl Terzaghi whose ideas had long dominated both earth dam engineering and the engineering science of soil mechanics; Terzaghi had also made significant contributions to understanding subsidence in oilfields. This left the assessment of the Baldwin Hills failure in the hands of a new generation of engineers, some who took on conflicting roles as experts in various lawsuits.\n\nThe design and construction of the dam had been inspected and approved by the California Department of Water Resources. A meticulously documented study published by that agency in 1964—while pointing out various connections between oilfield operations in the Inglewood Oil Field and ground disturbances in the area, including beneath the reservoir and at some distance from the reservoir—concluded rather vaguely that the failure was due to \"an unfortunate combination of physical factors\".\n\nThe monetary damages resulting from the failure were large, and some of the investigations which followed the state study were sponsored by litigants seeking more specific conclusions relevant to legal liability. This drew attention to oilfield operations in the area. From the outset it was clear that the ground faulting and fault creep which destroyed the reservoir were probably related to the many feet of ground subsidence which had occurred a half mile west of the reservoir over decades of oil extraction in the Inglewood field. The oilfield-related subsidence in the Inglewood field, though generally denied by the oil companies as a legal policy, was documented exhaustively by the US Geological Survey in 1969. Subsidence following oil extraction from shallow deposits in unconsolidated sediments had been understood by oil industry experts since the 1920s.\n\nFollowing the discovery in 1970 by geologist Douglas Hamilton of faulting and surface seepage of oilfield waste brines along the fault which traversed and extended south of the reservoir, Hamilton and Meehan concluded that oilfield injection for waste disposal and improved recovery of oil, a new technology at the time, was a significant cause of the failure, triggering hydraulic fracturing and aggravating movements on a fault traversing the reservoir even on the day of the failure. Subsequently, the US Geological Survey concluded in 1976 that displacements at the ground surface causing reservoir failure and also ground cracking in the Stocker-LaBrea area southeast of the reservoir were 90 percent or more attributable to exploitation of the Inglewood oil field, and that this faulting was likely aggravated by waterflooding with pressures exceeding hydraulic fracturing levels.\n\nBy 1972, nearly a decade after the failure, the immediate legal issues had been settled out of court and the matter was reopened as a topic of discussion among investigators in a published engineering conference at Purdue University.\n\nEngineer Thomas Leps, who had served as consultant on the 1964 state investigation, took on a role as neutral reviewer in this and most subsequent American studies of the failure. Leps concluded that there had been about 7 inches of offset on the fault beneath the reservoir during its life, about 2 inches of which had occurred in the months just before the failure. Leps associated the latter with repressurization of the oilfield. This, along with stretching of the ground due to subsidence of about 12 feet from oil extraction, had caused the lining failure which doomed the reservoir.\n\nSome prominent consultants including those on a team led by Arthur Casagrande, Harvard successor to Karl Terzaghi, held that oilfield operations were not a significant influence at all but that the failure was the result of defective siting and design with the heavy weight of the dam and reservoir being the significant cause of the fatal foundation movement. This view exonerated the oil companies, namely Standard Oil, which had sponsored the study. Casagrande refused to acknowledge any ground movements in the area as being related to oilfield operations and argued that ground movements that affected the dam were found only beneath the reservoir, not in adjoining areas.\n\nMost of these questions were examined once again in 1986 following investigations of a suspiciously similar major failure of the Bureau of Reclamation's Teton Dam in June, 1976, and a near failure of the Department of Water and Power's Lower Van Norman Dam in the 1971 San Fernando earthquake. Professor Ronald Scott of Caltech, who had participated in the Casagrande studies, noted at a follow-up 1987 conference on Baldwin Hills that Casagrande had ignored or been unaware of ground movements clearly unrelated to the reservoir (e.g. those at Stocker-LaBrea) in his analysis. Another engineer, Stanley Wilson—who had also worked with Casagrande on the 1972 studies and supported the claim that oilfield subsidence was an insignificant cause—now conceded that analogous ground offsets extended well outside the reservoir area, notably in the Stocker-LaBrea area, so that the reservoir and other fault movements could not be attributed to the reservoir itself—thus tacitly attributing responsibility for the failure to oilfield operations. Hence, there appeared to be convergence of opinion on the role of oilfield subsidence and repressurization.\n\nThe issue of oilfield causation was a central theme in most of these discussions, with little attention having been directed to the details of the failure. The absolute necessity of a lining for this site was generally taken for granted in these proceedings even as it had been by Proctor himself, regardless of the fact that almost all earth dams perform satisfactorily without linings. Some suggestions as to possible preventive design and construction techniques that might have made the dam safer were raised to engineering consensus and reached a state of textbook knowledge in the late 1980s. For example, the character of the compacted earth lining (which had been regularly referred to as clay but must have been substantially silt and sand, having been derived from the local Inglewood formation) was raised, if obliquely, in the suggestion made in the end that improved performance might have come from the use of a different lining material.\n\nIn 2001 a new angle on failure analysis was introduced by Mahunthan and Schofield, who concluded that overcompaction of the dam fill and lining was a significant aggravating factor in both the Baldwin Hills and Teton failures. This assertion was based on Schofield's concepts of critical-state soil mechanics, a corollary of which was that heavily compacted but lightly confined soils could be dangerously unstable where seepage forces were present. This issue had not been raised in the previous American-dominated discussions and remains in some degree contrary to American ideas in both theoretical soil mechanics and practical geotechnical engineering. In fact the 1964 DWR failure study implied that heavy compaction was a favored technique for earth dam construction, and this assumption appeared not to have been reexamined over the twenty five years of post-failure investigation and discussion.\n\nThe failure of the reservoir has been a subject of ongoing interest in the field of dam breach studies. A recent study examined the dam failure as a two-stage process and succeeded in modeling the flood in the urban area downstream.\n\nAlthough the Baldwin Hills Reservoir site has now been dedicated as a community park, and there is no further significant hazard associated with ground movements there, the associated faults to the southeast (Stocker-LaBrea and the Windsor School area) continue to move significantly as of 2012, causing damage to private and public facilities. The current oilfield operator, Plains Exploration and Production Company (PXP), which has intensified production and development efforts in the oilfield with the rising price of petroleum, does not, unlike its predecessor Standard Oil, acknowledge any causal connection between fault movements and oilfield activities, and has retained a team of consultants who support this position or conclude that the causes of the movements are unknown. The role of shallow hydraulic fracturing, which has recently been introduced as a means of stimulating production at depths of about 2000 feet in the southeast part of the Inglewood field, and at greater depths elsewhere in the field, has also generated public concern and controversy. However, oil operators, while admitting that fracture pressures are being exceeded, do not acknowledge a relationship between injection at fracturing pressure levels and fault movement. The PXP and PXP consultant conclusions, that adverse effects are either unknown or not present, are disputed by other reviewers.\n\nRecent discharges of oilfield gases in the Baldwin Hills may also be related to raised pressures resulting from injection, and may be of similar origin as the gas problems in the nearby Salt lake field.\n\nKTLA used a helicopter to cover the disaster. Common today, this was perhaps the first such live aerial coverage of a breaking news event. Richard N. Levine, a 17-year-old photography student, rushed to a higher viewpoint and made 35-mm-pictures of the evolving dam break.\n\n\nNotes\nBibliography\n\n"}
{"id": "2883178", "url": "https://en.wikipedia.org/wiki?curid=2883178", "title": "Black liquor", "text": "Black liquor\n\nIn industrial chemistry, black liquor is the waste product from the kraft process when digesting pulpwood into paper pulp removing lignin, hemicelluloses and other extractives from the wood to free the cellulose fibers.\n\nThe equivalent material in the sulfite process is usually called brown liquor, but the terms red liquor, thick liquor and sulfite liquor are also used.\n\nApproximately 7 tonnes of black liquor are produced in the manufacture of one tonne of pulp.\n\nThe black liquor is an aqueous solution of lignin residues, hemicellulose, and the inorganic chemicals used in the process. The black liquor comprises 15% solids by weight of which 10% are organic chemicals and 5% are inorganic chemicals. Normally the organics in black liquor are 40-45% soaps, 35-45% lignin and 10-15% other organics.\n\nThe organic matter in the black liquor is made up of water/alkali soluble degradation components from the wood. Lignin is degraded to shorter fragments with sulphur content at 1-2% and sodium content at about 6% of the dry solids. Cellulose and hemicellulose is degraded to aliphatic carboxylic acid soaps and hemicellulose fragments. The extractives gives tall oil soap and crude turpentine. The soaps contain about 20% sodium.\n\nThe residual lignin components currently serve for hydrolytic or pyrolytic conversion or just burning only. Hemicellulosis may undergo fermentation processes, alternatively.\n\nEarly kraft pulp mills discharged black liquor to watercourses. Black liquor is quite toxic to aquatic life, and causes a very dark caramel color in the water. The invention of the recovery boiler by G.H. Tomlinson in the early 1930s, was a milestone in the advancement of the kraft process.\n\nBy 2000, the better kraft mills recovered 99.5% or more of the black liquor, and purified the remainder in biological treatment plants, reducing the environmental effect of the waste waters below the level of scientific significance, except perhaps in very small streams. Even in the 21st century, some small kraft mills remained (producing at most a few tons of pulp per day) that discharged all black liquor. However, these are rapidly disappearing. Some kraft mills, particularly in North America, still recovered under 98% of the black liquor in 2007, which can cause some environmental contamination, even when biologically treated. The general trend is for such obsolete mills to modernize or shut down.\n\nThe black liquor contains more than half of the energy content of the wood fed into the digester of a kraft pulp mill. It is normally concentrated to 65 - 80% by multi-effect evaporators and burned in a recovery boiler to produce energy and recover the cooking chemicals. The viscosity increases as the concentration goes up. At about 50 - 55% solids the salt solubility limit is reached. Tall oil is an important byproduct separated from the black liquor with skimming before it goes to the evaporators or after the first evaporator stage.\n\nPulp mills have used black liquor as an energy source since at least the 1930s.\nMost kraft pulp mills use recovery boilers to recover and burn much of the black liquor they produce, generating steam and recovering the cooking chemicals (sodium hydroxide and sodium sulfide used to separate lignin from the cellulose fibres needed for papermaking). This has helped paper mills reduce problems with water emissions, reduce their use of chemicals by recovery and reuse, and become nearly energy self-sufficient by producing, on average, 66 percent of their own electricity needs on-site.\n\nIn the United States, paper companies have consumed nearly all of the black liquor they produce since the 1990s. As a result, the forest products industry has become one of the United States' leading generators of carbon-neutral renewable energy, producing approximately 28.5 terawatt hours of electricity annually.\n\nNew waste-to-energy methods to recover and utilize the energy in the black liquor have been developed. The use of black liquor gasification has the potential to achieve higher overall energy efficiency than the conventional recovery boiler, while generating an energy-rich syngas from the liquor. The syngas can be burnt in a gas turbine combined cycle to produce electricity (usually called \"BLGCC\" for Black Liquor Gasification Combined Cycle; similar to IGCC) or converted through catalytic processes into chemicals or fuels such as methanol, dimethyl ether (DME), or F-T diesel (usually called \"BLGMF\" for Black Liquor Gasification for Motor Fuels). This gasification technology is currently under operation in a 3 MW pilot plant at Chemrec’s test facility in Piteå, Sweden. The DME synthesis step will be added in 2011 in the \"BioDME\" project, supported by the European Commission's Seventh Framework Programme (FP7) and the Swedish Energy Agency.\n\nUsed for biofuels production, the black liquor gasification route has been shown to have very high conversion efficiency and greenhouse gas reduction potential.\n\nHydrothermal liquefaction is suitable for converting black liquor to advanced biofuels due to the process' ability to handle high moisture inputs.\n\nWhere recovery boiler capacity is limited and a bottleneck in the pulp mill the lignin in the black liquor may be extraordinary and exported or used as fuel in the mill's lime kiln, thereby often replacing fossil based fuel with biofuel.\n\nA tax credit created by the U.S. Congress in 2005 as part of the to reward and support the use of liquid alternative fuel derived from hydrocarbons in the transportation sector was expanded in 2007 to include non-mobile uses of liquid alternative fuel derived from biomass. This change meant that, in addition to fish processors, animal renderers and meat packers, kraft pulp producers became eligible for the tax credit as a result of their generation and use of black liquor to make energy. For one large company (International Paper) this could amount to as much as $3.7 billion in benefits. Weyerhaeuser announced in May 2009 that it was also pursuing the tax credit. While some have criticized the paper industry's eligibility for the alternative fuel mix tax credit on the grounds that it is increasing fossil fuel use, the industry has countered that adding a fossil fuel is actually a requirement of the law and that, regardless, this does not result in a net increase in fossil fuel use since companies are merely replacing the existing fossil fuel they already mix with black liquor—natural gas—with one of the three fuels specified by the law: gasoline, kerosene or diesel. The bio-fuel credit for Black Liquor ended on January 1, 2010.\n"}
{"id": "20620479", "url": "https://en.wikipedia.org/wiki?curid=20620479", "title": "Blood plasma fractionation", "text": "Blood plasma fractionation\n\nBlood plasma fractionation refers to the general processes of separating the various components of blood plasma, which in turn is a component of blood obtained through blood fractionation.\n\nBlood plasma is the liquid component of whole blood, and makes up approximately 55% of the total blood volume. It is composed primarily of water with small amounts of minerals, salts, ions, nutrients, and proteins in solution. In whole blood, red blood cells, leukocytes, and platelets are suspended within the plasma.\n\nPlasma contains a large variety of proteins including albumin, immunoglobulins, and clotting proteins such as fibrinogen. Albumin constitutes about 60% of the total protein in plasma and is present at concentrations between 35 and 55 mg/mL. It is the main contributor to osmotic pressure of the blood and it functions as a carrier molecule for molecules with low water solubility such as lipid-soluble hormones, enzymes, fatty acids, metal ions, and pharmaceutical compounds. Albumin is structurally stable due to its seventeen disulfide bonds and unique in that it has the highest water solubility and the lowest isoelectric point (pI) of the plasma proteins. Due to the structural integrity of albumin it remains stable under conditions where most other proteins denature.\n\nMany of the proteins in plasma have important therapeutic uses. Albumin is commonly used to replenish and maintain blood volume after traumatic injury, during surgery, and during plasma exchange. Since albumin is the most abundant protein in the plasma its use may be the most well known, but many other proteins, although present in low concentrations, can have important clinical uses. See table below.\nWhen the ultimate goal of plasma processing is a purified plasma component for injection or transfusion, the plasma component must be highly pure. The first practical large-scale method of blood plasma fractionation was developed by Edwin J. Cohn during World War II. It is known as the Cohn process (or Cohn method). This process is also known as cold ethanol fractionation as it involves gradually increasing the concentration of ethanol in the solution at 5C and 3C. The Cohn Process exploits differences in properties of the various plasma proteins, specifically, the high solubility and low pI of albumin. As the ethanol concentration is increased in stages from 0% to 40% the [pH] is lowered from neutral (pH ~ 7) to about 4.8, which is near the pI of albumin. At each stage certain proteins are precipitated out of the solution and removed. The final precipitate is purified albumin. Several variations to this process exist, including an adapted method by Nitschmann and Kistler that uses less steps, and replaces centrifugation and bulk freezing with filtration and diafiltration.\n\nSome newer methods of albumin purification add additional purification steps to the Cohn Process and its variations, while others incorporate chromatography, with some methods being purely chromatographic. Chromatographic albumin processing as an alternative to the Cohn Process emerged in the early 1980s, however, it was not widely adopted until later due to the inadequate availability of large scale chromatography equipment. Methods incorporating chromatography generally begin with cryodepleted plasma undergoing buffer exchange via either diafiltration or buffer exchange chromatography, to prepare the plasma for following ion exchange chromatography steps. After ion exchange there are generally further chromatographic purification steps and buffer exchange.\n\n\"For further information see chromatography in blood processing.\"\n\nIn addition to the clinical uses of a variety of plasma proteins, plasma has many analytical uses. Plasma contains many biomarkers that can play a role in clinical diagnosis of diseases, and separation of plasma is a necessary step in the expansion of the human plasma proteome.\n\nPlasma contains an abundance of proteins many of which can be used as biomarkers, indicating the presence of certain diseases in an individual. Currently, 2D Electrophoresis is the primary method for discovery and detection of biomarkers in plasma. This involves the separation of plasma proteins on a gel by exploiting differences in their size and pI. Potential disease biomarkers may be present in plasma at very low concentrations, so, plasma samples must undergo preparation procedures for accurate results to be obtained using 2D Electrophoresis. These preparation procedures aim to remove contaminants that may interfere with detection of biomarkers, solubilize the proteins so they are able to undergo 2D Electrophoresis analysis, and prepare plasma with minimal loss of low concentration proteins, but optimal removal of high abundance proteins.\n\nThe future of laboratory diagnostics are headed toward lab-on-a-chip technology, which will bring the laboratory to the point-of-care. This involves integration of all of the steps in the analytical process, from the initial removal of plasma from whole blood to the final analytical result, on a small microfluidic device. This is advantageous because it reduces turn around time, allows for the control of variables by automation, and removes the labor-intensive and sample wasting steps in current diagnostic processes.\n\nThe human plasma proteome may contain thousands of proteins, however, identifying them presents challenges due to the wide range of concentrations present. Some low abundance proteins may be present in picogram (pg/mL) quantities, while high abundance proteins can be present in milligram (mg/mL) quantities. Many efforts to expand the human plasma proteome overcome this difficulty by coupling some type of high performance liquid chromatography (HPLC) or reverse phase liquid chromatography (RPLC) with high efficiency cation exchange chromatography and subsequent tandem mass spectrometry for protein identification.\n\n"}
{"id": "6885750", "url": "https://en.wikipedia.org/wiki?curid=6885750", "title": "Bootstrapping (electronics)", "text": "Bootstrapping (electronics)\n\nIn the field of electronics, a bootstrap circuit is one where part of the output of an amplifier stage is applied to the input, so as to alter the input impedance of the amplifier. When applied deliberately, the intention is usually to increase rather than decrease the impedance. Generally, any technique where part of the output of a system is used at startup is described as bootstrapping. \n\nIn the domain of MOSFET circuits, \"bootstrapping\" is commonly used to mean pulling up the operating point of a transistor above the power supply rail. The same term has been used somewhat more generally for dynamically altering the operating point of an operational amplifier (by shifting both its positive and negative supply rail) in order to increase its output voltage swing (relative to the ground). In the sense used in this paragraph, bootstrapping an operational amplifier means \"using a signal to drive the reference point of the op-amp's power supplies\". A more sophisticated use of this rail bootstrapping technique is to alter the non-linear C/V characteristic of the inputs of a JFET op-amp in order to decrease its distortion.\n\nIn analog circuit designs, a bootstrap circuit is an arrangement of components deliberately intended to alter the input impedance of a circuit. Usually it is intended to increase the impedance, by using a small amount of positive feedback, usually over two stages. This was often necessary in the early days of bipolar transistors, which inherently have quite a low input impedance. Because the feedback is positive, such circuits can suffer from poor stability and noise performance compared to ones that don't bootstrap.\n\n\"Negative\" feedback may alternatively be used to bootstrap an input impedance, causing the apparent impedance to be reduced. This is seldom done deliberately, however, and is normally an unwanted result of a particular circuit design. A well-known example of this is the Miller effect, in which an unavoidable feedback capacitance appears increased (i.e. its impedance appears reduced) by negative feedback. One popular case where this \"is\" done deliberately is the Miller compensation technique for providing a low-frequency pole inside an integrated circuit. To minimize the size of the necessary capacitor, it is placed between the input and an output which swings in the opposite direction. This bootstrapping makes it act like a larger capacitor to ground.\n\nA N-MOSFET/IGBT needs a significantly positive charge (\"V > V\") applied to the gate in order to turn on. Using only N-channel MOSFET/IGBT devices is a common cost reduction method due largely to die size reduction (there are other benefits as well). However, using nMOS devices in place of pMOS devices means that a voltage higher than the power rail supply (V+) is needed in order to bias the transistor into linear operation (minimal current limiting) and thus avoid significant heat loss.\n\nA bootstrap capacitor is connected from the supply rail (V+) to the output voltage. Usually the source terminal of the N-MOSFET is connected to the cathode of a recirculation diode allowing for efficient management of stored energy in the typically inductive load (See Flyback diode). Due to the charge storage characteristics of a capacitor, the bootstrap voltage will rise above (V+) providing the needed gate drive voltage.\n\nA MOSFET/IGBT is a voltage-controlled device which, in theory, will not have any gate current. This makes it possible to utilize the charge inside the capacitor for control purposes. However, eventually the capacitor will lose its charge due to parasitic gate current and non-ideal (i.e. finite) internal resistance, so this scheme is only used where there is a steady pulse present. This is because the pulsing action allows for the capacitor to discharge (at least partially if not completely). Most control schemes that use a bootstrap capacitor force the high side driver (N-MOSFET) off for a minimum time to allow for the capacitor to refill. This means that the duty cycle will always need to be less than 100% to accommodate for the parasitic discharge unless the leakage is accommodated for in another manner.\n\nIn switch-mode power supplies, the regulation circuits are powered from the output. To start the power supply, a leakage resistance can be used to trickle-charge the supply rail for the control circuit to start it oscillating. This approach is less costly and more efficient than providing a separate linear power supply just to start the regulator circuit. \n\nAC amplifiers can use bootstrapping to increase output swing. A capacitor (usually referred as \"bootstrap capacitor\") is connected from the output of the amplifier to the bias circuit, providing bias voltages that exceed the power supply voltage. Emitter followers can provide rail-to-rail output in this way, which is a common technique in class AB audio amplifiers.\n\nWithin an integrated circuit a bootstrap method is used to allow internal address and clock distribution lines to have an increased voltage swing. The bootstrap circuit uses a coupling capacitor, formed from the gate/source capacitance of a transistor, to drive a signal line to slightly greater than the supply voltage. \n\n"}
{"id": "22418315", "url": "https://en.wikipedia.org/wiki?curid=22418315", "title": "British Approvals Service for Cables", "text": "British Approvals Service for Cables\n\nBritish Approvals Service for Cables (commonly known as BASEC) is an independent accredited certification body headquartered in Milton Keynes, United Kingdom. Here, the organization's dedicated testing laboratory also operates which is believed to be the largest of its type in Europe. BASEC was established in 1971 and principally provides product certification services for all types of cable and wire, ancillary products and management systems within the cable industry. The organization maintains operations throughout the world including Africa, America, Asia and Europe.\n\nBASEC also offers process capability assessment in cable making, certification of innovative and variant cable products, and provides independent investigation, testing and advice in connection with the manufacture or use of cables, or in the event of disputes.\n\nBASEC has been testing and certifying electrical cables for over forty years. The organization was formed in 1971 by a number of leading industry bodies, including BSI, BCA, The Institution of Electrical Engineers (IEE), and The Electrical Contractors Association (ECA), a group which has now been joined by The National Inspection Council for Electrical Installation Contracting (NICEIC). By detailed examination of manufacturers' production processes and controls, and rigorous testing, BASEC ensures that products meet appropriate British, European and International standards. The organization has grown to be a modern, accessible body servicing and representing its many client manufactures.\n\nThe organization's Board and Certification Committee provides independent governance. BASEC is Government-nominated and accredited by UKAS, demonstrating competence, impartiality and reliability in its ability to deliver results. BASEC is a member of the HAR agreement group of European Committee for Electrotechnical Standardization (CENELEC). This provides certification to harmonised cables within Europe in accordance with the harmonization standard. BASEC provides HAR scheme certification to manufacturers based in the UK and Ireland, where certified cables are marked with the approval legend \"BASEC <HAR>\". BASEC is affiliated with and supports the work of the Approved Cables Initiative (ACI) in seeking a cross market solution to the problem of sub-standard and counterfeit cable in the UK market.\n\nBASEC's main approval service is Product Certification which applies to individual cable types. All products are rigorously tested to meet necessary and appropriate standards through detailed examination of manufacturers production processes and controls. Product samples are retrieved from the client manufactures on which BASEC then carries out tests at its accredited laboratories in Milton Keynes. BASEC also verifies that products fully comply with the relevant specification by full type testing at independent laboratories or by witnessing tests at the client factory. Manufacturers may apply for a license to display the BASEC mark on their products only when BASEC is satisfied that their manufacturing systems are capable of consistently producing cable products. After having attained a BASEC Product Marking License they are subjected to regular factory audits and products surveillance testing. BASEC issues certificates to unique, specific products when they have passed thorough testing and the manufacturer can demonstrate ongoing conformity.\n\nCables are tested against the requirements of the relevant standards for characteristics such as: \n\nThe organization has extended its United Kingdom Accreditation Service (UKAS) accreditation to include ISO/IEC 17025.\n\nMost cable manufacturers, distributors and service providers now operate with formalised management systems. These give customers a degree of reassurance that the organization has procedures and processes in place that will assist in the delivery of a product, process or service that conforms to expectations, and that if things go wrong then the organization will respond appropriately. BASEC provides management system certification to cable manufacturers and other organizations involved within the cables sector. Assessment may be to an individual standard or to two or more in an integrated manner, such as:\n\n\nBASEC's own approach to management system certification is tailored specifically to cable making. It is designed to assess an organizations ability to produce goods and services consistently to specification and customer requirements in a safe manner with due regard to the environment. BASEC is accredited by UKAS against ISO/IEC 17021 for QMS certification and for EMS certification.\n\nAs of 1 July 2017 under the European Construction Products Regulation (CPR), it is now mandatory for cable manufacturers and suppliers to apply CE marking to any products covered by the harmonised European standard EN 50575. The CPR applies to all cables placed on the market for use in fixed installation in domestic, commercial and industrial premises and other civil engineering works anywhere in the European Union. It applies to power, communications and fibre optic cables irrespective of the place of manufacture.\nThese tests with a few exceptions need to be carried out by an independent Notified Body. BASEC is designated Notified Body No. 2662 for the CPR System 1+ and System 3.\n\nBASEC has the fire test equipment to undertake the following tests:\n\nThe BASEC mark found upon approved cable products is a recognized sign of assurance of independent cable testing and approval. Manufacturers may apply for a license to display the BASEC mark on their products only when BASEC is satisfied that their manufacturing systems are capable of consistently producing safe cable products, through assessment to the BASEC Product Certification Requirements. Manufacturers also have the option of displaying the BASEC name or roundel on reels and drums. BASEC approved manufacturers recognize quality and safety and this should be seen as a sign of assurance for contractors, customers and wholesalers.\n\nCable standards not only specify the dimensions and materials of a cable, they also require that a range of specific tests are undertaken to prove the construction and performance. Many non-approved cables have not been subject to the required tests. It is a common misunderstanding that a cable is compliant with standards or even BASEC approved just because the supplier claims that it has been produced to a particular standard. Cable marked with only a standard number should be treated with caution, it is probable that nobody independent of the manufacturer has examined that cable, and the claims made may be unreliable. Only cable marked with the \"BASEC\" name is BASEC approved, by demonstrating its compliance to the required standards.\n\nUpon the occurrence of a concern, problem or dispute in relation to use of cables, for example in the case of failure or performance problems, BASEC can conduct an independent investigation into performance or compliance to a specification. This may include testing and the assessment of specifications. BASEC will use appropriate experts, and commission tailored examination and testing programs from suitable laboratories, to address the problem. The organization will advise on the cases of the problem and provide assistance with the selection of specifications. As an official Notified Body under the European Low Voltage Directive, BASEC can perform assessments and prepare technical reports for manufacturers and importers of cable products. BASEC is also able to provide expert witness services in cases of legal dispute.\n\n\n"}
{"id": "2025130", "url": "https://en.wikipedia.org/wiki?curid=2025130", "title": "Chintz", "text": "Chintz\n\nChintz (from the plural of chint) was originally glazed calico textiles, initially specifically those imported from India,(derived from chhint-a hindi word) printed with designs featuring flowers and other patterns in different colours, typically on a light plain background. Since the 19th century the term has also been used for the style of floral decoration developed in those calico textiles, but then used more widely, for example on chintzware pottery and wallpaper. Chintz designs are mostly European patterns loosely derived from the style of Indian designs themselves reflecting, via Mughal art, decorative traditions in Islamic art such as the arabesque, and especially the Safavid art of Persia. \n\nUnglazed calico was traditionally called \"cretonne\". The word \"calico\" is derived from the name of the Indian city Calicut (Kozhikkode in native Malayalam) to which it had a manufacturing association. In contemporary language the word \"chintz\" and \"chintzy\" can be used to refer to clothing or furnishings which are vulgar or florid in appearance, and commonly in informal speech, to refer to cheap, low quality, or gaudy things, including personal behavior.\n\nChintz was originally a woodblock printed, painted or stained calico produced in India from 1600 to 1800 and popular for bed covers, quilts and draperies. Around 1600, Portuguese and Dutch traders were bringing examples of Indian chintz into Europe on a small scale, but the English and French merchants began sending large quantities. By 1680 more than a million pieces of chintz were being imported into England per year, and a similar quantity was going to France and the Dutch Republic. These early imports were probably mostly used for curtains, furnishing fabrics, and bed hangings and covers (Samuel Pepys bought a set for his wife). It has been suggested that wearing them as clothes began when these were replaced and given to maidservants, who made them into dresses, and also that they were first worn as linings. \n\nWith imported chintz becoming so popular with Europeans during the late 17th century, French and English mills grew concerned, as they could not make chintz. In 1686 the French declared a ban on all chintz imports. In 1720 England's Parliament enacted a law that forbade \"the Use and Warings in Apparel of imported chintz, and also its use or Wear in or about any Bed, Chair, Cushion or other Household furniture\".\n\nEven though chintz was outlawed, there were loopholes in the legislation. The Court of Versailles was outside the law and fashionable young courtiers continued wearing chintz. In 1734, French naval officer, M. de Beaulieu, who was stationed at Pondicherry, India, sent home letters along with actual samples of chintz fabric during each stage of the process to a chemist friend detailing the dyeing process of cotton chintz. His letters and samples can be seen today in the Muséum national d'Histoire naturelle in Paris.\n\nIn 1742, another Frenchman, Father Coeurdoux also supplied details of the chintz making process, while he was trying to convert the Indians to Catholicism. In 1759 the ban against chintz was lifted. By this time French and English mills were able to produce chintz.\n\nEuropeans at first produced reproductions of Indian designs, and later added original patterns. A well-known make was \"toile de Jouy\", which was manufactured in Jouy, France, between 1700 and 1843. Modern chintz usually consists of bright overall floral patterns printed on a light background but there are some popular patterns on black backgrounds as well.\n\n"}
{"id": "36727183", "url": "https://en.wikipedia.org/wiki?curid=36727183", "title": "Computer Braille Code", "text": "Computer Braille Code\n\nComputer Braille is an adaptation of braille for precise representation of computer-related materials such as programs, program lines, computer commands, and filenames. Unlike standard 6-dot braille scripts, but like Gardner–Salinas braille codes, this may employ the extended 8-dot braille patterns. The resulting 256 braille characters are assigned to the 256 characters of 8-bit computer encodings.\n\nThere are two standards of representation of computer code with braille:\n\n1) The Computer Braille Code as defined by the Braille Authority of North America. However, since January 2016 it is no longer official in the USA and replaced by Unified English Braille (UEB). It employs only the 6-dot braille patterns to represent all code points of ASCII as well as many technical characters and commands. It is virtually identical to the Braille ASCII, a system of representation of braille with ASCII characters, which goal is mirrored to the Computer Braille Code. To represent ASCII code points 0x60, 0x7B, 0x7C, 0x7D, 0x7E as well as capital letters the 4-5-6 () character is used as the shift indicator or modifier. Thus, (grave accent, 0x60) is represented by , where is assigned to (at sign, 0x40). In other words, either adds (for punctuation) or subtracts (for letters) 32 to or from the ASCII value of the following character. Unlike Braille ASCII (underscore, 0x5F) is represented by .\n\n2) The Braille Computer Notation as defined by the Braille Authority of the United Kingdom. In this notation both 6- and 8-dot patterns may be used. With the 6-dot code various combinations of braille characters can represent many technical, mathematical and logical symbols. The character is used as a universal modifier. The 8-dot code is designed that its 6-dot subset is identical to the 6-dot code. The remainder are assigned by the following rules:\n"}
{"id": "28689141", "url": "https://en.wikipedia.org/wiki?curid=28689141", "title": "Custom home", "text": "Custom home\n\nA custom home is a one-of-a-kind house that is designed for a specific client and for a particular location. The custom home builder may use plans created by an architect or by a professional home designer. Custom homes afford consumers the opportunity to control layout, lot size, and accessibility.\n\nIn most cases, custom home builders construct on land the home buyer already owns. Some developers sell fully serviced lots specifically for the construction of custom homes. This makes it easy to build a custom home since the lot is construction-ready and builders can focus purely on the design of the home.\n\n\n\nThe Construction Industry attributes to being the fourth largest contributor to GDP in Australian economy, employing 9.1% of the Australian workforce.\n\nDespite the economic recovery, house building is rapidly increasing. Housing construction grew for a 15th month in April, which is the longest period of growth 2006/07.\n\nIn the United States, the home building industry accounts for $82 billion in revenue with an annual growth of 5.8%. The industry currently attributes to 398,391 employees over 163,843 Businesses.\n\n"}
{"id": "16089380", "url": "https://en.wikipedia.org/wiki?curid=16089380", "title": "Cut-off factor", "text": "Cut-off factor\n\nCut-off factor (AKA \"cut-off length\") is a factor used to calculate the length of a hose cut to achieve the desired overall length of hose plus fittings. It is commonly seen in hydraulic hose and fitting specifications. The cut-off factor is specific to a particular hose fitting.\n\nThe formula used in calculating the optimum overall length is:\n\nformula_1.\n\nIn this formula, C1 represents the cut-off factor of the first hose end and C2 represents the cut-off factor of the second hose end.\n"}
{"id": "4432859", "url": "https://en.wikipedia.org/wiki?curid=4432859", "title": "Devolo", "text": "Devolo\n\ndevolo AG is a technology company founded on May 1, 2002 in Aachen, Germany, and specializes in the development of communications devices for private consumers and industrial applications. Its product range includes devices for DSL, ISDN and analogue Internet access as well as home-networking solutions using cabled Ethernet, Wireless LAN, or, mainly, existing electrical wiring (HomePlug with Ethernet, USB or WiFi).\n\n"}
{"id": "18101742", "url": "https://en.wikipedia.org/wiki?curid=18101742", "title": "Drill line", "text": "Drill line\n\nIn a drilling rig, the drill line is a multi-thread, twisted wire rope that is threaded or reeved through the traveling block and crown block to facilitate the lowering and lifting of the drill string into and out of the wellbore.\n\nOn larger diameter lines, traveling block loads of over a \"million\" pounds are possible.\n\nTo make a connection is to add another segment of drill pipe onto the top the drill string. A segment is added by pulling the kelly above the rotary table, stopping the mud pump, hanging off the drill string in the rotary table, unscrewing the kelly from the drill pipe below, swinging the kelly over to permit connecting it to the top of the new segment (which had been placed in the mousehole), and then screwing this assembly into the top of the existing drill string. Mud circulation is resumed, and the drill string is lowered into the hole until the bit takes weight at the bottom of the hole. Drilling then resumes.\n\n"}
{"id": "33925461", "url": "https://en.wikipedia.org/wiki?curid=33925461", "title": "Edsby", "text": "Edsby\n\nEdsby is a cloud-based software application that combines social networking with class and student management features. It is a learning management system with data aggregation and analytics features for K-12 students, teachers and parents.\n\nEdsby, originally named CoreFour, was founded in April 2010 by the three original founders and architects of the FirstClass software application, along with a longtime co-worker. It offers K-12 school districts various online social learning tools connected to their existing legacy systems, including gradebooks, attendance-taking, parent-teacher communication features, classroom management, content management and social classroom interaction.\n\nEdsby is sold only to K-12 school districts and not available as a free version. It sells its system directly to school districts and private schools and charges a traditional per-user fee annually. Edsby synchronizes bi-directionally with school districts' legacy student information systems and other systems and creates and manages classes, students, teachers and schedules by leveraging districts' existing databases, speeding deployment and district-wide adoption.\n\nEdsby has partnered with Microsoft and makes its service available on the Microsoft Azure cloud service worldwide. Microsoft has enhanced its OneNote application to support easy entry of grades into the Edsby gradebook.\n\nOne significant Edsby site is the school district of Hillsborough County Public Schools. Edsby is deployed to all of the 194,000 students and 15,000 teachers at 260 schools in the district. Edsby replaced a more expensive system, Edline, in September 2013 for grade books and parent engagement. The company claims to serve approximately 20 million requests every day. District educator reaction has been positive, with some end-user grousing.\n\nDistrict-wide deployments with other school systems include 34,000 students and 3,450 staff at Kawartha Pine Ridge District School Board. In August, 2017, the company announced it had added half a million more paid users.\n\n\n"}
{"id": "5624327", "url": "https://en.wikipedia.org/wiki?curid=5624327", "title": "Electric eye", "text": "Electric eye\n\nAn electric eye is a photodetector used for detecting obstruction of a light beam. An example is the door safety system used on garage door openers that use a light transmitter and receiver at the bottom of the door to prevent closing if there is any obstruction in the way that breaks the light beam. The device does not provide an image; only presence of light is detectable. Visible light may be used, but infrared radiation conceals the operation of the device and typically is used in modern systems. Originally, systems used lamps powered by direct current or the power line alternating current frequency, but modern photodetector systems use an infrared light-emitting diode modulated at a few kilohertz, which allows the detector to reject stray light and improves the range, sensitivity and security of the device.\n\nHighway vehicle counter\nFirst compact commercial unit\nAutomatic wrapping machines\nAutomatic door opener\nBusiness alarm system\nAutomatic cameras\n\n\n"}
{"id": "49361513", "url": "https://en.wikipedia.org/wiki?curid=49361513", "title": "Flowtite Technology", "text": "Flowtite Technology\n\nFlowtite Technology AS is a Norwegian technology company, owned by Amiantit Group. It develops GRP pipe manufacturing technology and designs tailor-made manufacturing equipment under the brand name \"Flowtite\". Its predecessor, Vera Fabrikker, was the first company in the world to utilize a continuous filament winding machine for production of glassfibre-reinforced plastic pipes (GRP pipes) and to invent the corresponding manufacturing process, commonly known as continuous filament winding process or Drostholm process.\n\nThe history of Flowtite Technology can be traced back to 1929 when the owners of Jotun established a manufacturing plant for vegetable oils, called Vera Fabrikker, in Sandefjord, Norway. In May 1966 engineers from Vera Fabrikker met with Frede Hilmar Drostholm at an exhibition in Copenhagen, Denmark. The company Drostholm Ltd presented a prototype of a continuous winding machine for production of GRP pipes — an invention of Drostholm’s partner, Peder Ulrik Poulsen. Vera Fabrikker purchased this machine with a goal to develop a process enabling the machine to produce GRP pipes and tank shells. After a year of unsuccessful experimenting, a second invention was made — a steel band was added to the machine and in 1968 Vera Fabrikker started producing GRP pipes and tanks. In 1971 the GRP technology was bought by Owens Corning, USA. Two years later, in 1977, Owens Corning together with Jotun established a new company — Veroc Technology AS (the last two letters in Veroc stand for Owens Corning). It was responsible for the development of GRP pipes and tanks, as well as manufacturing and installing production equipment for the licensed GRP pipe producers all over the world. In 1993 Owens Corning took over Veroc Technology 100%. Later, in 1998, the company name was changed to Flowtite Technology AS, as it is known today. In 2001 Flowtite Technology AS was acquired by the Saudi Arabian Amiantit Company (Amiantit Group) and became its main technology centre in 2006.\n\nThe brand Flowtite and its logo was created in the 1970s by Owens Corning, when the company produced GRP pipes under the trademark Flowtite. The trademark was first registered in the USA in 1975, then progressively across the world. It is composed of the words “flow” and “tite” (a dialect version of “tight”) and represents a steady movement of liquids through a compact and sealed medium. Two circles of the Flowtite logotype symbolize a pipe containing water flow.\n\nFlowtite pipes can only be produced with a custom-designed production equipment, which has a basic filament winding machine as a basis. It consists of a continuous steel band mandrel, supported by beams installed circumferentially to form a cylindrical shape. The mandrel moves continuously in a spiral path towards the exit assembly. As the mandrel rotates, all composite materials are metered onto it in precise amounts, forming one layer after another. As a pipe is being formed on the mandrel, it moves forward to the curing and cutting areas and the finished product comes out at the end of one continuous production process.\n\nFlowtite GRP pipes are used in various industries and multiple applications, such as:\n\n"}
{"id": "608228", "url": "https://en.wikipedia.org/wiki?curid=608228", "title": "Goma-2", "text": "Goma-2\n\nGoma-2 Eco was a type of high explosive manufactured for industrial use (chiefly mining) by Unión Española de Explosivos S.A.\n\nIt was a gelatinous, nitroglycol-based explosive widely used within Spain and exported abroad.\nIt was used by ETA in the 1980s and 1990s.\n\nThere were two variants of Goma-2: Goma-2 EC and Goma-2 ECO. As of 2017, the manufacturer MAXAM Corp. S. L. has reformulated the Goma-type ammonia gelatine dynamites which are marketed worldwide under the new Riodin trade name.\n\nGoma-2 explosive was a mixture of several chemicals:\n\nAs with other commercial blasting explosives, detonators were needed to initiate a detonation (usually a blasting cap # 8).\n\nGoma-2 ECO was the explosive that was used in the 2004 Madrid train bombings. Terrorist Jamal Ahmidan, also known as El Chino, bought the explosive illegally from a mine in northern Spain. It was also planned by the same cell that carried out the Madrid bombings to use the explosive to derail a high-speed train. About 80 kilograms of the explosive was used by ETA in Operation Ogro to assassinate Luis Carrero Blanco. The explosion was so powerful it threw Blanco's car over a five story building.\n\n"}
{"id": "3155420", "url": "https://en.wikipedia.org/wiki?curid=3155420", "title": "Grill (cryptology)", "text": "Grill (cryptology)\n\nThe grill method (), in cryptology, was a method used chiefly early on, before the advent of the cyclometer, by the mathematician-cryptologists of the Polish Cipher Bureau (\"Biuro Szyfrów\") in decrypting German Enigma machine ciphers. The Enigma rotor cipher machine changes plaintext characters into cipher text using a different permutation for each character, and so implements a polyalphabetic substitution cipher.\n\nThe German navy started using Enigma machines in 1926; it was called \"Funkschlüssel C\" (\"Radio cipher C\"). By 15 July 1928, the German Army (\"Reichswehr\") had introduced their own version of the Enigma—the \"Enigma G\"; a revised \"Enigma I\" (with plugboard) appeared in June 1930. The Enigma I used by the German military in the 1930s was a 3-rotor machine. Initially, there were only three rotors labeled \"I\", \"II\", and \"III\", but they could be arranged in any order when placed in the machine. Rejewski identified the rotor permutations by , , and ; the encipherment produced by the rotors altered as each character was encrypted. The rightmost permutation () changed with each character. In addition, there was a plugboard that did some additional scrambling.\n\nThe number of possible different rotor wirings is:\n\nThe number of possible different reflector wirings is:\n\nThe number of possible different plugboard wirings (for six cables) is:\n\nTo encrypt or decrypt, the operator made the following machine key settings:\n\nIn the early 1930s, the Germans distributed a secret monthly list of all the daily machine settings. The Germans knew that it would be foolish to encrypt the day's traffic using the same key, so each message had its own \"message key\". This message key was the sender-chosen initial rotor positions (e.g., YEK). The message key had to be conveyed to the recipient operator, so the Germans decided to encrypt it using the day's pre-specified daily ground setting (\"Grundstellung\"). The recipient would use the daily machine settings for all messages. He would set the Enigma's initial rotor position to the ground setting and decrypt the message key. The recipient would then set the initial rotor position to the message key and decrypt the body of the message.\n\nThe Enigma was used with radio communications, so letters were occasionally corrupted during transmission or reception. If the recipient did not have the correct message key, then the recipient could not decipher the message. The Germans decided to send the three-letter message key twice to guard against transmission errors. Instead of encrypting the message key \"YEK\" once and sending the encrypted key twice, the Germans doubled the message key to \"YEKYEK\" (\"doubled key\"), encrypted the doubled key with the ground setting, and sent the encrypted doubled key. The recipient could then recognize a garbled message key and still decrypt the message. For example, if the recipient received and decrypted the doubled key as \"YEKYEN\", then the recipient could try both message keys \"YEK\" and \"YEN\"; one would produce the desired message and the other would produce gibberish.\n\nThe encrypted doubled key was a huge cryptographic mistake because it allowed cryptanalysts to know two encipherments of the same letter, three places apart, for each of the three letters. The Polish codebreakers exploited this mistake in many ways. Marian Rejewski used the doubled key and some known daily keys obtained by a spy, to determine the wiring of the three rotors and the reflector. In addition, code clerks often did not choose secure random keys, but instead chose weak keys such as \"AAA\", \"ABC\", and \"SSS\". The Poles later used the doubled weak keys to find the unknown daily keys. The grill method was an early exploitation of the doubled key to recover part of the daily settings. The cyclometer and the \"bomba kryptologiczna\" were later exploitations of the doubled key.\n\nFrode Weierud provides the procedure, secret settings, and results that were used in a 1930 German technical manual.\n\nThe first line of the message is not encrypted. The \"1035\" is the time, \"90\" is number of characters encrypted under the message key, and \"341\" is a system indicator that tells the recipient how the message was encrypted (i.e., using Enigma with a certain daily key). The first six letters in the body (\"PKPJXI\") are the doubled key (\"ABLABL\") encrypted using the daily key settings and starting the encryption at the ground setting/Grundstellung \"FOL\". The recipient would decipher the first six letters to recover the message key (\"ABL\"); he would then set the machine's rotors to \"ABL\" and decipher the remaining 90 characters. Notice that the Enigma does not have numerals, punctuation, or umlauts. Numbers were spelled out. Most spaces were ignored; an \"X\" was used for a period. Umlauts used their alternative spelling with a trailing \"e\". Some abbreviations were used: a \"Q\" was used for \"CH\".\n\nWhen Rejewski started his attack in 1932, he found it obvious that the first six letters were the enciphered doubled key.\n\nThe daily key settings and ground setting will permute the message key characters in different ways. That can be shown by encrypting six of the same letter for all 26 letters:\n\nFrom this information, the permutations for each of the six message keys can be found. Label each permutation \"A B C D E F\". These permutations are secret: the enemy should not know them.\nNotice the permutations are disjoint transpositions. For the \"A\" permutation, it not only changes \"A\" into \"P\" but it also changes \"P\" into \"A\". That allows the machine to both encrypt and decrypt messages.\n\nAugustin-Louis Cauchy introduced \"two-line notation\" in 1815 and \"cycle notation\" in 1844.\n\nRejewski made an incredible discovery. Without knowing the plugboard settings, the rotor positions, the ring settings, or the ground setting, he could solve for all the daily message keys. All he needed were enough messages and some code clerks using non-random message keys.\n\nThe message key is three characters long, so the doubled key is six characters long. Rejewski labeled the permutations for the successive message-key characters \"A B C D E F\". He did not know what those permutations were, but he did know that \"A\" and \"D\" permutations encrypted the same message key letter, that \"B\" and \"E\" encrypted the same letter, and that \"C\" and \"F\" encrypted the same letter. If are the (unknown) plaintext letters of the message key and are the corresponding (known) ciphertext letters, then\n\nThe equations can be post multiplied by \"D\", \"E\", and \"F\" respectively to simplify the right hand sides:\n\nThe plaintext values are unknown, so those terms are just dropped to leave:\n\nThe above equations describe a path through the permutations. If is passed through the inverse of , then it produces . If that character passes through , then the result is .\n\nRejewski also knew that the Enigma permutations were self inverses: Enigma encryption and decryption were identical. That means that where is the identity permutation. Consequently, . Thus:\n\nThe above equations show the relationship between the doubled key characters. Although Rejewski did not know the individual permutations \"A B C D E F\", a single message told him how specific characters were permuted by the composed permutations \"AD\", \"BE\", and \"CF\".\n\nFrom many messages, Rejewski could determine the composed permutations completely. In practice, about 60 messages were needed to determine the permutations.\n\nRejewski recorded the three permutations with a cyclic notation he called the characteristic. gives an example:\n\nIn this notation, the first cycle of permutation would map d to v, v to p, p to f, ..., y to o, and o would wrap around to d.\n\nMarks and Weierud give and example from Alan Turing that shows these cycles can be completed when some information is incomplete.\n\nFurthermore, Enigma permutations were simple transpositions, which meant that each permutation \"A B C D E F\" only transposed pairs of characters. Those character pairs had to come from different cycles of the same length. Moreover, any one pairing between two cycles determined all the other pairs in those cycles. Consequently, permutations \"A\" and \"D\" both had to transpose a and s because (a) and (s) are the only cycles of length one and there is only one way to pair them. There are two ways to match (bc) and (rw) because b must pair with either r or w. Similarly, there are ten ways to match the remaining ten-character cycles. In other words, Rejewski now knew that there were only twenty possibilities for the permutations \"A\" and \"D\". Similarly, there were 27 candidates for \"B\" and \"E\", and 13 candidates for \"C\" and \"F\".\n\nAt this point, the Poles would exploit weaknesses in the code clerks' selection of message keys to determine which candidates were the correct ones. If the Poles could correctly guess the key for a particular message, then that guess would anchor two cycles in each of the three characteristics.\n\nThe Poles intercepted many messages; they would need about 60 messages in the same daily key to determine the characteristic, but they may have many more. Early on, Rejewski had identified the six characters that made up the message key. If the code clerks were choosing random message keys, then one would not expect to see much correlation in the encrypted six characters. However, some code clerks were lazy. What if, out of a hundred messages, there were five messages from five different stations (meaning five different code clerks) that all used the same message key \"PUUJJN\"? That they all came up with the same key suggests they used a very simple or very common key. The Poles kept track of different stations and how those stations would choose message keys. Early on, clerks often used simple keys such as \"AAA\" or \"BBB\".\n\nThe end result was that without knowing the Enigma's plugboard settings, the rotor positions, or the ring settings, Rejewski determined each of the permutations \"A B C D E F\", and hence all of the day's message keys.\n\nInitially, Rejewski used the knowledge of permutations \"A B C D E F\" (and a manual obtained by a French spy) to determine the rotor wirings. After learning the rotor wirings, the Poles used the permutations to determine the rotor order, plugboard connections, and ring settings through further steps of the grill method.\n\nUsing the daily key in the 1930 technical manual above, then (with enough messages) Rejewski could find the following characteristics:\n\nAlthough there are theoretically 7 trillion possibilities for each of the \"A B C D E F\" permutations, the characteristics above have narrowed the \"A\" and \"D\" permutations to just 13 possibilities, \"B\" and \"E\" to just 30 possibilities, and \"C\" and \"F\" to just 20 possibilities. The characteristic for \"CF\" has two singleton cycles, (e) and (z). Those singleton cycles must pair in the individual permutations, so the characteristic for \"CF\" implies that the \"E\" and \"Z\" exchange in both the \"C\" and \"F\" permutations.\nThe pairing of \"E\" and \"Z\" can be checked in the original (secret) permutations given above.\n\nRejewski would now know that indicators with the pattern \"..E..E\" were from a message key of \"..Z\"; similarly an indicator of \"..Z..Z\" were from a message key of \"..E\". In the day's traffic, he might find indicators such as \"PKZJXZ\" or \"RYZOLZ\"; might one of these indicators be the common (lazy) message key \"EEE\"? The characteristic limits the number of possible permutations to a small number, and that allows some simple checks. \"PKZJXZ\" cannot be \"EEE\" because it requires \"K\" and \"E\" to interchange in \"B\", but both \"K\" and \"E\" are part of the same cycle in \"BE\": (kxtcoigweh). Interchanging letters must come from distinct cycles of the same length. The repeating key could also be confirmed because it could uncover other repeating keys.\n\nThe indicator \"RYZOLZ\" is a good candidate for the message key \"EEE\", and it would immediately determine both permutations \"A\" and \"D\". For example, in \"AD\", the assumed message key \"EEE\" requires that \"E\" and \"R\" interchange in \"A\" and that \"E\" and \"O\" interchange in \"D\".\n\nIf \"E\" interchanges with \"R\" in \"A\" (notice one character came from the first cycle in \"AD\" and the other character came from the second cycle), then the letter following \"E\" (i.e. \"D\") will interchange with the letter preceding \"R\" (i.e. \"X\") .\n\nThat can be continued to get all the characters for both permutations.\n\nThis characteristic notation is equivalent to the expressions given for the 1930 permutations \"A\" and \"D\" given above by sorting the cycles so that the earliest letter is first.\n\nThe guessed message key of \"EEE\" producing indicator \"RYZOLZ\" would also determine the pairing of the 10-long cycles in permutation \"BE\".\n\nThat determines most of \"B\" and \"E\", and there would only be three possible variations left that pair (ujd) and (mqa). There are still 20 possible variations for \"C\" and \"F\". At this point, the Poles could decrypt all of the first and fourth letters of the daily keys; they could also decrypt 20 out 26 of the second and fifth letters. The Poles' belief in these permutations could be checked by looking at other keys and seeing if they were typical keys used by code clerks.\n\nWith that information, they could go looking for and find other likely weak message keys that would determine the rest of the \"A B C D E F\" permutations. For example, if the Poles had an indicator \"TKYWXV\", they could decrypt it as \"BB.BB.\"; checking the cycles for \"CF\" would reveal that the indicator is consistent with message key \"BBB\".\n\nRejewski modeled the machine as permutation made from permutations of plugboard (), the wiring from the keyboard/lamps to the rotors (), the three rotors (), and the reflector (). The permutation for each position of the doubled key was different, but they were related by a permutation that represented a single step of a rotor ( is known). Rejewski assumed that the left and middle rotors did not move while encrypting the doubled key. The six letters of the doubled key consequently see the permutations A B C D E F:\n\nRejewski simplied these equations by creating as a composite reflector made from the real reflector and two leftmost rotors:\n\nSubstitution produces:\n\nThe result is six equations in four unknowns (\"S H N Q\"). Rejewski had a commercial Enigma machine, and he initially thought that would be the same. In other words, Rejewski guessed that\nLater, Rejewski realized that guess was wrong. Rejewski then guessed (correctly) that was just the identity permutation:\nThat still left three unknowns. Rejewski comments:\n\nHaving the daily keys meant that was now known. The known permutations were moved to the left side in the equations by premultiplying and post multiplying.\n\nThe leftmost and rightmost permutations on the right-hand side (which were also known) were moved to the left; the results were given the variable names \"U V W X Y Z\":\n\nRejewski then multiplied each equation with the next:\nNext, Rejewski eliminated the common subexpression by substituting its value obtained from the previous product.\nThe result is a set of four equations in just one unknown: .\n\nFor the 1930 example above,\nare transformed to the permutations:\n\nand then multiplied to produce the five successive products:\nNow the goal is to find the single structure preserving map that transforms UV to VW, VW to WX, WX to XY, and XY to YZ. Found by subscription of cycle notation. When maps to , the map must mate cycles of the same length. That means that codice_1 in must map to one of codice_2 in . In other words, codice_3 must map to one of codice_4. These can be tried in turn.\n\nBut codice_3 must map the same to codice_6 in each pairing, so other character mappings are also determined:\n\nConsequently, the character maps for codice_7, codice_8, and codice_9 are discovered and consistent. Those mappings can be exploited:\nWhich determines the rest of the map and consistently subscribes:\nThe resulting map with successive subscriptions:\n\nThe map gives us , but that is also congugate (structure preserving). Consequently, the 26 possible values for are found by subscribing in 26 possible ways.\n\nThe model above ignored the right rotor's ring setting (22) and ground setting (12), both of which were known because Rejewski had the daily keys. The ring setting has the effect of counterrotating the drum by 21; the ground setting advances it by 11. Consequently, the rotor rotation is -10, which is also 16.\nThe physical grill was used to determine both the rightmost rotor, its initial position, and the plugboard settings.\n\nRejewsky observed that is close to the identity permutation (in the early 1930s, only 12 of 26 letters were affected by the plugboard). He moved everything but to the left side of the equations by premultiplying or postmultiplying. The resulting system of equations is:\n\nAt his point, is unknown, but it is the same for each equation. Rejewski does not know , but he knows it is one of the rotors (I, II, and III), and he knows the wiring for each of those rotors. There were only three rotors and 26 possible initial rotations. Consequently, there are only 84 possible values for . Rejewski can look at each possible value to see if the permutation is consistent. If there were no steckers ( were the identity), then each equation would produce the same .\n\nConsequently, he made one bottom sheet for each possible rotor (three sheets). Each bottom sheet consisted of 31 lines (26 + 5 to make six lines contiguous). Each line contained the stepped permutation of a known rotor. For example, a suitable bottom sheet for rotor III is,\n\nIn the early 1930s, the rotor order was the same for a month or more, so the Poles usually knew which rotor was in the rightmost position and only needed to use one bottom sheet. After 1 November 1936, the rotor order changed every day. The Poles could use the clock method to determine the rightmost rotor, so the grill would only need to examine that rotor's bottom sheet.\n\nFor the top sheet, Rejewski wrote the six permutations through .\nThere were six slits so the permutations on the bottom sheet would show through at the proper place.\n\nThe top sheet would then be slid through all possible positions of rotor , and the cryptanalyst would look for consistency with some unknown but constant permutation . If there is not a consistent , then the next position is tried.\n\nHere's what the grill would show for the above permutations at its consistent alignment:\nIn permutation , the cryptanalyst knows that codice_10 interchange. He can see how rotor III would scramble those letters by looking at the first line (the alphabet in order) and the line visible through the slit. The rotor maps codice_11 into codice_12 and it maps codice_13 into codice_14. If we ignore steckers for the moment, that means permutation would interchange codice_15. For to be consistent, it must be the same for all six permutations.\n\nLook at the grill near permutation to check if its also interchanges codice_15. Through the slit, find the letter codice_12 and look in the same column two lines above it to find codice_18. That tells us the rotor, when it has advanced three positions, now maps codice_18 into codice_12. Similarly, the advanced rotor will map codice_21 into codice_14. Looking at permutation , it interchanges codice_23, so the two tests are consistent.\n\nSimilarly, in permutation , the codice_24 interchange and imply that codice_25 interchange in . Looking at permutation , codice_26 interchange and also imply that codice_25 interchange in .\n\nAll such tests would be consistent if there were no steckers, but the steckers confuse the issue by hiding such matches. If any of the letters involved in the test is steckered, then it will not look like a match.\n\nThe effect of the rotor permutation can be removed to leave the implied by the permutations. The result (along with the actual value of ) is:\n\nMost of the letters in an implied permutation are incorrect. An exchange in an implied permutation is correct if two letters are not steckered. About one half the letters are steckered, so the expectation is only one fourth of the letters in an implied permutation are correct. Several columns show correlations; column codice_28 has three codice_29 characters, and codice_30 interchange in the actual ; column codice_31 has four codice_32 characters, and codice_33 interchange in .\n\nRejewski states that writing down all the possible \"would be too laborious\", so he developed the grill (grid) method. \"Next, the grid is moved along the paper on which the drum connections are written until it hits upon a position where some similarities show up among the several expression . ... In this way the setting of drum and the changes resulting from permutation are found simultaneously. This process requires considerable concentration since the similarities I mentioned do not always manifest themselves distinctly and can be very easily overlooked.\" The reference does not describe what techniques were used. Rejewski did state that the grill method required unsteckered pairs of letters.\n\nPermutation has the exchanges codice_34. If we assume the exchange codice_35 is unsteckered, that implies exchanges codice_36. The other five permutations can be quickly checked for an unsteckered pair that is consistent with interchanging codice_36 — essentially checking column codice_38 for other rows with codice_39 without computing the entire table. None are found, so codice_35 would have at least one stecker so the assumption it is unsteckered is abandoned. The next pair can be guessed as unsteckered. The exchange codice_41 implies exchanges codice_42; that is consistent with codice_43 in , but that guess fails to pan out because codice_44 and codice_45 are steckered.\n\nFollowing those guesses ultimately leads to a contradiction:\nThe third exchange codice_46 implies exchanges codice_47; this time permutation with an unsteckered codice_48 would be consistent with exchanging codice_47.\n\nAt this point, the guess is that the letters codice_50 are unsteckered. From that guess, all the steckers can be solved for this particular problem. The known (assumed) exchanges in are used to find exchanges in , and those exchanges are used to extend what is known about .\n\nUsing those unsteckered letters as seeds finds codice_48 interchange in and implies codice_52 is in ; similarly codice_53 interchange in and implies codice_54 is in . Examining codice_54 in the other permutations finds codice_56 is a stecker.\n\nThat adds letters codice_57 to the seeds. Those letters were also unknown above, so further information can be gleaned by revisiting: also has codice_58.\nRevisit codice_59 in gives more information:\nAnother revisit fully exploits codice_47:\nThat addition fills out even more:\nAll of is now known after examining 3 exchanges in . The rest of can be found easily.\n\nWhen a match is found, then the cryptanalyst would learn both the initial rotation of and the plugboard (\"Stecker\") permutation .\n\nAt this point, the rotor positions for the permutation is not known. That is, the initial positions (and possibly the order) of rotors and are not known. The Poles applied brute force by trying all possible initial positions () of the two rotors. With three rotors, knowing which rotor was at position meant there were only two possible ways to load the other two rotors.\nLater, the Poles developed a catalog of all the permutations. The catalog was not large: there were six possible combinations of two left rotors with initial settings, so the catalog had 4,056 entries. After using the grill, the Poles would look up in the catalog to learn the order and initial positions of the other two rotors.\n\nInitially, the Germans changed the rotor order infrequently, so the Poles would often know the rotor order before they began working. The rotor order changed every quarter until 1 February 1936. Then it changed every month until 1 November 1936, when it was changed daily.\n\nThe cryptanalyst now knew the plugboard, the rotor order, and the absolute setting of the rotors for the doubled key, but he did not know the ring setting. He also knew what the message key setting should be, but that setting was useless without knowing the ring setting. The ring setting could be anything, and that meant the Poles did know how to position the rotors for the message body. All the work up to this point had focussed on exploiting the doubled key. To determine the ring setting, the attention now shifted to the actual message.\n\nHere, the Germans had made another mistake. Each message usually started with the text \"ANX\", which was German \"an\" meaning \"to:\" with the \"X\" meaning space. The Poles applied brute force here, too. They would go through up to settings to find settings that produced \"ANX\". Once found, the cryptanalyst would use the absolute setting of the rotors to determine the ring setting. The entire daily key was thus recovered.\n\nLater, the Poles refined the brute force search technique. By examining some messages, they could determine the position of the rightmost rotor; consequently, only 676 rotor positions would have to be tried. Rejewski no longer remembers how this trick worked.\n\nThe grill method is described by Marian Rejewski as being \"manual and tedious\" and, like the later cryptologic bomb, as being \"based... on the fact that the plug connections [in the Enigma's commutator, or \"plugboard\"] did not change all the letters.\" Unlike the bomb, however, \"the grill method required unchanged \"pairs\" of letters [rather than] only unchanged letters.\"\n\nInitially, the plugboard only swapped six pairs of letters. That left more than half of the alphabet unaffected by permutation . The number of steckers changed 1 August 1936; then it could be from five to eight pairs of letters were swapped. The extra swapped characters reduced the effectiveness of the grid method, so the Poles started looking for other methods. The result was the cyclometer and corresponding card catalog; that method was immune to steckers.\n\nThe grill method found application as late as December 1938 in working out the wiring in two Enigma rotors newly introduced by the Germans. (This was made possible by the fact that a \"Sicherheitsdienst\" net, while it had introduced the new drums IV and V, continued using the old system for enciphering the individual message keys.)\n\nOn 15 September 1938, most German nets stopped encrypting the doubled key with a common setting (the ground setting). The Poles had been able to take advantage of all messages in a net using the same machine settings to encrypt the doubled key. Now most nets stopped doing that; instead, the operator would choose his own ground setting and send it in the clear to the recipient. This change frustrated the grill method and the cyclometer card catalog. One net, the \"Sicherheitsdienst\" (SD) net, continued to use a common ground setting, and that net was used to reverse engineer new rotors (IV and V) that were introduced. The SD net traffic was doubly encoded, so the ANX method would not work. The grill method would sometimes fail after the Germans increased the number of plugboard connections to ten on 1 January 1939. When the SD net switched to the new message-key protocol on 1 July 1939, the grill method (and the cyclometer method) were no longer useful.\n\nHere's an example of the new message procedure for a message on 21 September 1938.\n\nThe \"3 TLE\" (German \"Teile\", parts) says it is a 3-part message; the \"1TL\" (German \"Teil\", part) says this is the first part; the \"172\" says there are 172 characters in the message (including the message key). For this message, the ground setting \"FRX\" is transmitted twice in the clear; the ground setting would/should be different for every message on net. Consequently, the Poles could not find the needed sixty message keys encrypted under the same ground setting. Without the same-key message volume, they could not determine the characteristic, so they could not determine the permutations \"A B C D E F\" or use the grill. For this message, the daily settings (rotor order, plugboard, and ring settings) were used with \"FRX\" to decrypt the first six characters (\"HCALN U\") to obtain the doubled message key (\"AGIAGI\").\n\nTo decrypt these messages, the Poles used other techniques to exploit the doubled message key.\n\n\n\n"}
{"id": "31114554", "url": "https://en.wikipedia.org/wiki?curid=31114554", "title": "Harriet Fasenfest", "text": "Harriet Fasenfest\n\nHarriet Fasenfest (born 1953) is an American writer, urban gardener, and food preservation educator in Portland, Oregon. A former owner/operator of several restaurants and cafes, she uses the term \"householding\" when referring to the practice of home food growing, canning and storage. She published her first book, \"A Householder's Guide to the Universe\", in 2010.\n\nFasenfest was born and raised in Bronx, New York. She moved to the Northwest in 1978 where she owned and operated the Bertie Lou's and Harriet's Eat Now cafes. In 1990 she went back to traveling the country, married, moved back to California, and then moved to Georgia in 1992. While there, she began working with Habitat for Humanity International. In 1997 Fasenfest returned to Portland to work as director of the Performance Salon Series at North Portland's North Star Ballroom, an events arena which combined art and performance with social activism, and in 2000 opened her last cafe, Groundswell.\n\nIn 2004 she transformed her backyard in Northern Portland into a producing garden. She grows produce with an eye to home canning and storing, and teaches classes in food preservation and backyard gardening. She is the co-owner of Preserve, an educational center for home gardening and food preservation, and creator of the www.portlandpreserve.com website, on which she writes a blog called \"The Householder's Grab Bag\". She also blogs for Culinate.com.\n\nFasenfest regards home food preservation as a political and economic statement. She uses the term \"householding\" rather than \"homemaking\" or \"home economics\" to describe her work. She published her views on home gardening and food preservation in \"A Householder's Guide to the Universe: A Calendar of Basics for the Home and Beyond\". The book is organized by month and season.\n\nShe lives in Portland, Oregon \n\n\n\n"}
{"id": "11390850", "url": "https://en.wikipedia.org/wiki?curid=11390850", "title": "Hydraulic telegraph", "text": "Hydraulic telegraph\n\nA hydraulic telegraph () is either of two different hydraulic-telegraph telecommunication systems. The earliest one was developed in 4th-century BC Greece, while the other was developed in 19th-century AD Britain. The Greek system was deployed in combination with semaphoric fires, while the latter British system was operated purely by hydraulic fluid pressure.\n\nAlthough both systems employed water in their sending and receiver devices, their transmission media were completely different. The ancient Greek system transmitted its semaphoric information to the receiver visually, which limited its use to line-of-sight distances in good visibility weather conditions only. The 19th-century British system used water-filled pipes to effect changes to the water level in the receiver unit (similar to a transparent water-filled flexible tube used as a level indicator), thus limiting its range to the hydraulic pressure that could be generated at the transmitter's device. \n\nWhile the Greek device was extremely limited in the codes (and hence the information) it could convey, the British device was never deployed in operation other than for very short-distance demonstrations. The British device could, however, be used in any visibility within its range of operation so long as its conduits, if unheated, did not freeze in sub-zero temperatures —which contributed to its impracticality.\n\nThe ancient Greek design was described in the 4th century BC by Aeneas Tacticus and the 3rd century BC by the historian Polybius. According to Polybius, it was used during the First Punic War to send messages between Sicily and Carthage.\n\nThe system involved identical containers on separate hills, which are \"not\" connected to each other; each container would be filled with water, and a vertical rod floated within it. The rods were inscribed with various predetermined codes at various points along its height.\n\nTo send a message, the sending operator would use a torch to signal the receiving operator; once the two were synchronized, they would simultaneously open the spigots at the bottom of their containers. Water would drain out until the water level reached the desired code, at which point the sender would lower his torch, and the operators would simultaneously close their spigots. Thus the length of time the sender's torch was visible could be correlated with specific predetermined codes and messages.\n\nA contemporary description of the ancient telegraphic method was provided by Polybius. In \"The Histories\", Polybius wrote:\n\nThe British civil engineer Francis Whishaw, who later became a principal in the General Telegraph Company, publicized a hydraulic telegraph in 1838 but was unable to deploy it commercially. By applying pressure at a transmitter device connected to a water-filled pipe which travelled all the way to a similar receiver device, he was able to effect a change in the water level which would then indicate coded information to the receiver's operator.\n\nThe system was estimated to cost £200 per mile (1.6 km) and could convey a vocabulary of 12,000 words. The U.K.'s \"Mechanics Magazine\" in March 1838 described it as follows:\n\nThe article concluded speculatively that the \"... hydraulic telegraph may supersede the semaphore and the galvanic telegraph\".\n\n\n"}
{"id": "1066586", "url": "https://en.wikipedia.org/wiki?curid=1066586", "title": "Hydrilla", "text": "Hydrilla\n\nHydrilla('Waterthyme' ) or hydrilla) is a genus of aquatic plant, usually treated as containing just one species, Hydrilla verticillata, though some botanists divide it into several species. It is native to the cool and warm waters of the Old World in Asia, Africa and Australia, with a sparse, scattered distribution; in Australia from Northern Territory, Queensland, and New South Wales.\n\nThe stems grow up to 1–2m long. The leaves are arranged in whorls of two to eight around the stem, each leaf 5–20 mm long and 0.7–2 mm broad, with serrations or small spines along the leaf margins; the leaf midrib is often reddish when fresh. It is monoecious (sometimes dioecious), with male and female flowers produced separately on a single plant; the flowers are small, with three sepals and three petals, the petals 3–5 mm long, transparent with red streaks. It reproduces primarily vegetatively by fragmentation and by rhizomes and turions (overwintering), and flowers are rarely seen. They have air spaces to keep them upright.\n\n\"Hydrilla\" has a high resistance to salinity compared to many other freshwater associated aquatic plants.\n\n\"Hydrilla\" closely resembles some other related aquatic plants, including \"Egeria densa\" and \"Elodea canadensis\". Synonyms include \"H. asiatica\", \"H. japonica\", \"H. lithuanica\", and \"H. ovalifolica\".\n\n\"Hydrilla verticillata\" is allelopathic to the common hornwort (\"Ceratophyllum demersum\") and prickly hornwort (\"C. muricatum\"), that is, it produces compounds that inhibit growth of the latter two species.\n\n\"Hydrilla\" is naturalized and invasive in the United States following release in the 1950s and 1960s from aquariums into waterways in Florida, due to the aquarium trade. It is now established in Canada and the southeast from Connecticut to Texas, and also in California. By the 1990s control and management were costing millions of dollars each year.\n\nSpecifically, a Florida west coast aquarium dealer shipped live \"Hydrilla\" from Sri Lanka under the common name \"Indian star-vine.\" After these plants were considered unsatisfactory, they were dumped into a canal near Tampa Bay, where they flourished. By 1955, the plants found their way from Tampa to Miami as they were transported for cultivation and pet trade sale. It is believed that several undocumented cases of accidental or careless releases followed, as there was extensive spread of the \"Hydrilla\" throughout Florida and the southeastern U.S.\n\nAs an invasive species in Florida, \"Hydrilla\" has become the most serious aquatic weed problem for Florida and most of the U.S. Because it was such a threat as an invasive species, restrictions were placed to allow only a single type of chemical, fluridone, to be used as a herbicide. This was done to prevent the evolution of multiple mutants, and resulted in fluridone resistant \"Hydrilla\". “As \"Hydrilla\" spread rapidly to lakes across the southern United States in the past, the expansion of resistant biotypes is likely to pose significant environmental challenges in the future.”\n\n\"Hydrilla\" populations have caused economic, environmental, and ecological damage. \"Hydrilla\" is known to be an aggressive and competitive plant, even out-competing and displacing native species, such as pondweeds and eelgrass. Due to its competitive nature, \"Hydrilla\" has created monocultures, an area dominated by a singular species, rather than having a balance among many species, like in a normal ecosystem.\n\nIn Australia, \"Hydrilla\" can become invasive if the nutrient levels are raised in disturbed ecosystems, though is not generally known to be problematic.\n\n\"Hydrilla\" can be controlled by herbicides, as well as grass carp, itself an invasive species in North America. Insects used as biological pest control for this plant include weevils of the genus \"Bagous\" and the Asian hydrilla leaf-mining fly (\"Hydrellia pakistanae\"). Tubers pose a problem as they can lie dormant for a number of years, making it even more difficult to remove from waterways and estuaries. \"Hydrilla\" holds the advantage in that in can spread efficiently through both tubers and turions.\n\nIn 2011 the inlet of Cayuga Lake, one of the Finger Lakes in New York State, used the chemical herbicide endothall to try to head off a possible future disaster. The first year nearly $100,000 and numerous man-hours were spent trying to eradicate the \"Hydrilla\" infestation. Follow-up treatments were planned for at least five years. The City of Ithaca as well as other local officials are willing to pay the price because without quick action the plant could get into the lake and possibly spread to other Finger Lakes in the region.\n\nThis abundant source of biomass is a known bioremediation hyperaccumulator of Mercury, Cadmium, Chromium and Lead, and as such can be used in phytoremediation.\n"}
{"id": "40037692", "url": "https://en.wikipedia.org/wiki?curid=40037692", "title": "I.D. Systems", "text": "I.D. Systems\n\nI.D. Systems, Inc. is an American company headquartered in Woodcliff Lake, New Jersey, that produces wireless asset management systems for industrial trucks, rental vehicles, and transportation assets. The company utilizes wireless communication technologies—including radio frequency identification (RFID), Wi-Fi, ultrahigh-frequency (UHF), satellite, and cellular—as well as sensor technology and proprietary software to manage high-value corporate assets, such as forklifts, airport ground support equipment, rental vehicles, dry van trailers, chassis, refrigerated trailers, flatbeds, railcars, and intermodal containers.\n\nI.D. Systems was founded in 1993 by Kenneth S. Ehrman. The company introduced the use of radio frequency identification (RFID) technology for industrial asset tracking and management. In 1995, I.D. Systems was awarded a $6.6 million contract from U.S. Postal Service (USPS) to develop and implement a tracking system for test letters and packages. I.D. Systems went public on the NASDAQ in 1999.\n\nIn 2005, I.D. Systems received a three-year contract from the U.S. Postal Service to implement its Wireless Asset Net, a wireless industrial vehicle management system, at 460 USPS facilities nationwide. According to Jeffrey Jagid, the company's former chairman and CEO, this project is a key milestone both in the company's relationship with USPS and in mass commercialization of the company's wireless technology.\n\nIn 2009, I.D. Systems acquired didBOX Ltd., a privately held, United Kingdom-based manufacturer and marketer of driver identification systems for forklift fleets. The acquisition made didBOX a wholly owned subsidiary of I.D. Systems, broadened I.D. Systems' product line, and expanded sales of the company's vehicle management solutions in the European market. In 2010, I.D. Systems acquired GE Asset Intelligence, LLC, a business unit of the General Electric Company, which provides trailer and container tracking solutions for manufacturers, retailers, shippers, and freight transportation companies. The acquisition expanded the scope of I.D. Systems' product line and complemented its portfolio of wireless asset management patents.\n\nIn 2017, I.D. Systems acquired Keytroller, a privately held, Florida-based manufacturer and marketer of safety systems for industrial lift trucks. https://globenewswire.com/news-release/2017/07/11/1042701/0/en/I-D-Systems-to-Acquire-Keytroller-Strengthening-Its-Position-in-Industrial-Truck-Management-Market-and-Overall-Growth-Outlook.html\n\nI.D. Systems' solutions consist of hardware, software, maintenance, support, and consulting services. The solutions can be divided into three categories: industrial vehicle management, transportation asset management, and rental fleet management.\n\nI.D. Systems' industrial vehicle management systems are used by manufacturers, distributors, retailers, government entities, and other organizations with powered industrial trucks (forklifts, tow tractors, pallet jacks, etc.) to monitor and evaluate fleet operations, track vehicle movement, and manage vehicle operators. The system consists of four elements: a wireless computer installed on the vehicle, wireless communication nodes placed within the facility, system software hosted either on site or in a remote data center, and user software configured in either a client-server or thin-client (Internet browser-based) architecture. Data may be transmitted by any combination of Wi-Fi, cellular or UHF wireless technologies.\n\nThe company offers three systems for industrial vehicle management. didBox™ is a non-wireless system for driver identification. PowerBox™ and PowerFleet® are wireless systems with functions ranging from operator access control, electronic safety checklists, and impact management to location tracking, battery management, maintenance scheduling, and task dispatching.\n\nThe company's customers in the industrial vehicle management market include Ford Motor Company, Nestlé North America, Procter & Gamble, Toyota, and Walmart (which accounted for 15% of the company's 2012 revenues).\n\nI.D. Systems provides a variation of its industrial vehicle management system designed specifically for ground support equipment (pushback tractors, baggage tugs, cargo loaders, catering vans, fuel trucks, etc.) and other vehicles that operate in and around airports.\n\nThis system, branded AvRamp®, was developed with funding from the U.S. Transportation Security Administration (TSA) and tested by the TSA at Newark Liberty International Airport and the JAXPORT seaport in Jacksonville, Florida. According to a 2005 TSA report, \"The Newark Liberty International Airport (EWR) Vehicle Tracking Demonstration—Wireless Fleet Management System,\" I.D. Systems' technology accurately located vehicles and enhanced the security of aircraft fuel trucks and other maintenance vehicles.\n\nThe company's customers in the airport vehicle management market include Envoy Air and American Airlines.\n\nI.D. Systems' Asset Intelligence subsidiary provides asset management solutions for the freight transportation industry under the VeriWise™ brand. The VeriWise™ product line includes a range of telematics solutions for tracking the location and monitoring the condition and status of dry van trailers, refrigerated trailers (\"reefers\"), flatbeds, chassis, containers, and railcars.\n\nFor managing fleets of dry van trailers and containers, the company offers a product called VeriWise™ Track and Trace™ for location tracking, a cargo sensor to monitor load status, and a motion sensor to detect trailer movement. The company also offers a temperature sensor to monitor the condition of reefers and an impact sensor to track collisions of railcars.\n\nTo manage these transportation assets, VeriWise™ products leverage satellite and cellular communications and web-based data processing technologies. Users access the information about their assets through a remotely hosted website called VeriWise™ Internet Portal (VIP), which generates reports and displays tabular data and a Google map interface.\n\nThe company's customers in the transportation asset management market include FAB Express, Forward Air, Freymiller, National Retail Systems, and Royal Freight.\n\nIn 2011, I.D. Systems and Avis Budget Group signed an exclusive agreement to deploy I.D. Systems' wireless vehicle management technology on more than 25,000 Avis Budget vehicles, enabling Avis customers to self-manage their rentals by computer or smartphone. The technology can also automate and expedite the rental and return process, track vehicle mileage, measure fuel consumption, and remotely control a vehicle's door locks. In February 2013, the company was awarded its second patent (patent number 8370268) for an automated wireless rental car management system.\n\nIn 2012, the company launched I.D. Systems Analytics, a set of web-based data reporting software tools. Analytics provides visibility into the performance of industrial assets across multiple facilities and geographies. The data enables users to evaluate individual facility performance, compare facilities side-by-side, and assess performance against industry-wide benchmarks, built on the company's database.\n\nInitial adopters of this product include Walgreen Company.\n\n\n"}
{"id": "7461133", "url": "https://en.wikipedia.org/wiki?curid=7461133", "title": "IFT Industrial Scientist Award", "text": "IFT Industrial Scientist Award\n\nThe IFT Industrial Scientist Award has been awarded every year since 1994. It is awarded by the Institute of Food Technologists (IFT) for scientists who made technical contributions to advancing the food industry.\n\nAward winners receive a USD 3000 honorarium and a plaque from IFT.\n\n"}
{"id": "8008565", "url": "https://en.wikipedia.org/wiki?curid=8008565", "title": "Immunity-aware programming", "text": "Immunity-aware programming\n\nWhen writing firmware for an embedded system, immunity-aware programming refers to programming techniques which improve the tolerance of transient errors in the program counter or other modules of a program that would otherwise lead to failure. Transient errors are typically caused by single event upsets, insufficient power, or by strong electromagnetic signals transmitted by some other \"source\" device.\n\nImmunity-aware programming is an example of defensive programming and EMC-aware programming. Although most of these techniques apply to the software in the \"victim\" device to make it more reliable, a few of these techniques apply to software in the \"source\" device to make it emit less unwanted noise.\n\nMicrocontrollers' firmware can inexpensively improve the electromagnetic compatibility of an embedded system.\n\nEmbedded systems firmware is usually not considered to be a source of radio frequency interference. Radio emissions are often caused by harmonic frequencies of the system clock and switching currents. The pulses on these wires can have fast rise and fall times, causing their wires to act as radio transmitters. This effect is increased by badly-designed printed circuit boards. These effects are reduced by using microcontroller output drivers with slower rise times, or by turning off system components.\n\nThe microcontroller is easy to control. It is also susceptible to faults from radio frequency interference. Therefore, making the microcontroller's software resist such errors can cheaply improve the system's tolerance for electromagnetic interference by reducing the need for hardware alterations.\n\nCMOS microcontrollers have specific weak spots which can be strengthened by software that works against electromagnetic interference. Failure mode and effects analysis of a system and its requirements is often required. Electromagnetic compatibility issues can easily be added to such an analysis.\n\nSlow changes of power supply voltage do not cause significant disturbances, but rapid changes can make unpredictable trouble. If a voltage exceeds parameters in the controller's data sheet by 150 percent, it can cause the input port or the output port to get hung in one state, known as CMOS latch-up. Without internal current control, latch-up causes the microcontroller to burn out. The standard solution is a mix of software and hardware changes. Most embedded systems have a watchdog timer. This watchdog should be external to the microcontroller so that it is likely to be immune to any plausible electromagnetic interference. It should reset the power supply, briefly switching it off. The watchdog period should be half or less of the time and power required to burn out the microcontroller. The power supply design should be well-grounded and decoupled using capacitors and inductors close to the microcontroller; some typical values are 100uF and 0.1uF in parallel.\n\nLow power can cause serious malfunctions in most microcontrollers. For the CPU to successfully decode and execute instructions, the supplied voltage must not drop below the minimum voltage level. When the supplied voltage drops below this level, the CPU may start to execute some instructions incorrectly. The result is unexpected activity on the internal data and control lines. This activity may cause:\n\n\nBrownout detection solves most of those problems in most systems by causing the system to shut down when main power is unreliable. One typical system retriggers a timer each time that the AC main voltage exceeds 90% of its rated voltage. If the timer expires, it interrupts the microcontroller, which then shuts down its system. Many systems also measure the power supply voltages, to guard against slow power supply degradation.\n\nThe input ports of CMOS oscillators have high impedances, and are thus very susceptible to transient disturbances. According to Ohm's law, high impedance causes high voltage differences. They also are very sensitive to short circuit from moisture or dust.\n\nOne typical failure is when the oscillators' stability is affected. This can cause it to stop, or change its period. The normal system hedges are to have an auxiliary oscillator using some cheap, robust scheme such as a ring of inverters or a resistor-capacitor one-shot timer. After a reset (perhaps caused by a watchdog timer), the system may default to these, only switching in the sensitive crystal oscillator once timing measurements have proven it to be stable. It is also common in high-reliability systems to measure the clock frequency by comparing it to an external standard, usually a communications clock, the power line, or a resistor-capacitor timer.\n\nBursts of electromagnetic interference can shorten clock periods or cause runt pulses that lead to incorrect data access or command execution. The result is wrong memory content or program pointers. The standard method of overcoming this in hardware is to use an on-chip phase locked loop to generate the microcontroller's actual clock signal. Software can periodically verify data structures and read critical ports using voting, distributing the reads in time or space.\n\nInput/output ports—including address lines and data lines—connected by long lines or external peripherals are the antennae that permit disturbances to have effects. Electromagnetic interference can lead to incorrect data and addresses on these lines. Strong fluctuations can cause the computer to misread I/O registers or even stop communication with these ports. Electrostatic discharge can actually destroy ports or cause malfunctions.\n\nMost microcontrollers' pins are high impedance inputs or mixed inputs and outputs. High impedance input pins are sensitive to noise, and can register false levels if not properly terminated. Pins that are not terminated inside an IC need resistors attached. These have to be connected to ground or supply, ensuring a known logic state.\n\nAn analysis of possible errors before correction is very important. The cause must be determined so that the problem can be fixed.\n\nThe Motor Industry Software Reliability Association identifies the required steps in case of an error as follows:\n\n\nFundamentally one uses redundancy to counter faults. This includes running extra code (redundancy in time) as well as keeping extra bits (redundancy in space).\n\nA disturbed instruction pointer can lead to serious errors, such as an undefined jump to an arbitrary point in the memory, where illegal instructions are read. The state of the system will be undefined. IP errors can be handled by use of software based solutions such as function tokens and an NOP slide(s).\n\nMany processors, such as the Motorola 680x0, feature a hardware trap upon encountering an illegal instruction. A correct instruction, defined in the trap vector, is executed, rather than the random one. Traps can handle a larger range errors than function tokens and NOP slides. Supplementary to illegal instructions, hardware traps securely handle memory access violations, overflows, or a division by zero.\n\nImproved noise immunity can be achieved by execution flow control known as token passing. The figure to the right shows the functional principle schematically. This method deals with program flow errors caused by the instruction pointers.\n\nThe implementation is simple and efficient. Every function is tagged with a unique function ID. When the function is called, the function ID is saved in a global variable. The function is only executed if the function ID in the global variable and the ID of the function match. If the IDs do not match, an instruction pointer error has occurred, and specific corrective actions can be taken. A sample implementation of token passing using a global variable programmed in C is stated in the following source listing.\n\nThis is essentially an \"arm / fire\" sequencing, for every function call. Requiring such a sequence is part of safe programming techniques, as it generates tolerance for single bit (or in this case, stray instruction pointer) faults.\n\nThe implementation of function tokens increases the program code size by 10 to 20%, and slows down the performance. To improve the implementation, instead of global variables like above, the function ID can be passed as an argument within the functions header as shown in the code sample below.\n\nWith NOP-Fills, the reliability of a system in case of a disturbed instruction pointer can be improved in some cases. The entire program memory that is not used by the program code is filled with No-Operation (NOP) instructions. In machine code a NOP instruction is often represented by 0x00 (for example, Intel 8051, ATmega16, etc.). The system is kept in a defined state. At the end of the physical program memory, an instruction pointer error handling (IPEH IP-Error-Handler) has to be implemented. In some cases this can be a simple reset.\n\nIf an instruction pointer error occurs during the execution and a program points to a memory segment filled with NOP instructions, inevitably an error occurred and is recognized.\n\nThree methods of implementing NOP-Fills are applicable:\n\n\n\nWhen using the CodevisionAVR C compiler, NOP fills can be implemented easily. The chip programmer offers the feature of editing the program flash and EEPROM to fill it with a specific value. Using an Atmel ATmega16, no jump to reset address 0x00 needs to be implemented, as the overflow of the instruction pointer automatically sets its value to 0x00. Unfortunately, resets due to overflow are not equivalent to intentional reset. During the intended reset, all necessary MC registers are reset by hardware, which is not done by a jump to 0x00. So this method will not be applied in the following tests.\nMicrocontroller architecture requires the I/O leads to be placed at the outer edge of the silicon die. Thus I/O contacts are strongly affected by transient disturbances on their way to the silicon core, and I/O registers are one of the most vulnerable parts of the microcontroller. Wrongly-read I/O registers may lead to an incorrect system state. The most serious errors can occur at the reset port and interrupt input ports. Disturbed data direction registers (DDR) may inhibit writing to the bus.\n\nThese disturbances can be prevented as following:\n\n1. Cyclic update of the most important registers\n\n2. Multiple read of input registers\n\nIn systems without error detection and correction units, the reliability of the system can be improved by providing protection through software. Protecting the entire memory (code and data) may not be practical in software, as it causes an unacceptable amount of overhead, but it is a software implemented low-cost solution for code segments.\n\nAnother elementary requirement of digital systems is the faultless transmission of data. Communication with other components can be the weak point and a source of errors of a system. A well-thought-out transmission protocol is very important. The techniques described below can also be applied to data transmitted, hence increasing transmission reliability.\n\nA cyclic redundancy check is a type of hash function used to produce a checksum, which is a small integer from a large block of data, such as network traffic or computer files. CRCs are calculated before and after transmission or duplication, and compared to confirm that they are equal. A CRC detects all one- or two-bit errors, all odd errors, all burst errors if the burst is smaller than the CRC, and most of the wide-burst errors. Parity checks can be applied to single characters (VRC—vertical redundancy check), resulting in an additional parity bit or to a block of data (LRC—longitudinal redundancy check), issuing a block check character. Both methods can be implemented rather easily by using an XOR operation. A trade-off is that less errors can be detected than with the CRC. Parity Checks only detect odd numbers of flipped bits. The even numbers of bit errors stay undetected. A possible improvement is the usage of both VRC and LRC, called Double Parity or Optimal Rectangular Code (ORC).\n\nSome microcontrollers feature a hardware CRC unit.\n\nA specific method of data redundancy is duplication, which can be applied in several ways, as described in the following:\n\n\nWhen the data is read out, the two sets of data are compared. A disturbance is detected if the two data sets are not equal. An error can be reported. If both sets of data are corrupted, a significant error can be reported and the system can react accordingly.\n\nIn most cases, safety-critical applications have strict constraints in terms of memory occupation and system performance. The duplication of the whole set of variables and the introduction of a consistency check before every read operation represent the optimum choice from the fault coverage point of view. Duplication of the whole set of variables enables an extremely high percentage of faults to be covered by this software redundancy technique. On the other side, by duplicating a lower percentage of variables one can trade off the obtained fault coverage with the CPU time overhead.\n\nThe experimental result shows that duplicating only 50% of the variables is enough to cover 85% of faults with a CPU time overhead of just 28%.\n\nAttention should also be paid to the implementation of the consistency check, as it is usually carried out after each read operation or at the end of each variable's life period. Carefully implementing this check can minimize the CPU time and code size for this application.\n\n\nAs the detection of errors in data is achieved through duplicating all variables and adding consistency checks after every read operation, special considerations have to be applied according to the procedure interfaces. Parameters passed to procedures, as well as return values, are considered to be variables. Hence, every procedure parameter is duplicated, as well as the return values. A procedure is still called only once, but it returns two results, which must hold the same value. The source listing to the right shows a sample implementation of function parameter duplication.\n\n\nTo duplicate a test is one of the most robust methods that exists for generic soft error detection. A drawback is that no strict assumption on the cause of the errors (EMI, ESD etc.), nor on the type of errors to expect (errors affecting the control flow, errors affecting data etc.) can be made. Erroneous bit-changes in data-bytes while stored in memory, cache, register, or transmitted on a bus are known. These data-bytes could be operation codes (instructions), memory addresses, or data. Thus, this method is able to detect a wide range of faults, and is not limited to a specific fault model. Using this method, memory increases about four times, and execution time is about 2.5 times as long as the same program without test duplication. Source listing to the right shows a sample implementation of the duplication of test conditions.\n\nCompared to test duplication, where one condition is cross-checked, with branching duplication the condition is duplicated.\n\nFor every conditional test in the program, the condition and the resulting jump should be reevaluated, as shown in the figure. Only if the condition is met again, the jump is executed, else an error has occurred.\n\n\nWhat is the benefit of when data, tests, and branches are duplicated when the calculated result is incorrect? One solution is to duplicate an instruction entirely, but implement them differently. So two different programs with the same functionality, but with different sets of data and different implementations are executed. Their outputs are compared, and must be equal. This method covers not just bit-flips or processor faults but also programming errors (bugs). If it is intended to especially handle hardware (CPU) faults, the software can be implemented using different parts of the hardware; for example, one implementation uses a hardware multiply and the other implementation multiplies by shifting or adding. This causes a significant overhead (more than a factor of two for the size of the code). On the other hand, the results are outstandingly accurate.\n\nReset ports and interrupts are very important, as they can be triggered by rising/falling edges or high/low potential at the interrupt port. Transient disturbances can lead to unwanted resets or trigger interrupts, and thus cause the entire system to crash. For every triggered interrupt, the instruction pointer is saved on the stack, and the stack pointer is decremented.\n\nTry to reduce the amount of edge triggered interrupts. If interrupts can be triggered only with a level, then this helps to ensure that noise on an interrupt pin will not cause an undesired operation. It must be kept in mind that level-triggered interrupts can lead to repeated interrupts as long as the level stays high. In the implementation, this characteristic must be considered; repeated unwanted interrupts must be disabled in the ISR. If this is not possible, then on immediate entry of an edge-triggered interrupt, a software check on the pin to determine if the level is correct should suffice.\n\nFor all unused interrupts, an error-handling routine has to be implemented to keep the system in a defined state after an unintended interrupt.\n\nUnintentional resets disturb the correct program execution, and are not acceptable for extensive applications or safety-critical systems.\n\nA frequent system requirement is the automatic resumption of work after a disturbance/disruption. It can be useful to record the state of a system at shut down and to save the data in a non-volatile memory. At startup the system can evaluate if the system restarts due to disturbance or failure (warm start), and the system status can be restored or an error can be indicated. In case of a cold start, the saved data in the memory can be considered valid.\n\nThis method is a combination of hard- and software implementations. It proposes a simple circuit to detect an electromagnetic interference using the device's own resources. Most microcontrollers, like the ATmega16, integrate analog to digital converters (ADCs), which could be used to detect unusual power supply fluctuations caused by interferences.\n\nWhen an interference is detected by the software, the microcontroller could enter a safe state while waiting for the aggression to pass. During this safe state, no critical executions are allowed. The graphic presents how interference detection can be performed. This technique can easily be used with any microcontroller that features an AD-converter.\n\nA watchdog timer is an electronic timer that detects abnormal operation of other components and initiates corrective action to restore normal operation. It especially ensures that microcontroller controlled devices do not completely fail if a software error or momentary hardware error occurs. Watchdog timers are typically based on either a monostable timer or digital counter. The timer circuit may be integrated on the microcontroller chip or be implemented as an external circuit. Watchdog timers can significantly improve the reliability of a microcontroller in an electromagnetically-influenced environment.\n\nThe software informs the watchdog at regular intervals that it is still working properly. If the watchdog is not informed, it means that the software is not working as specified any more. Then the watchdog resets the system to a defined state. During the reset, the device is not able to process data and does not react to calls.\n\nAs the strategy to reset the watchdog timer is very important, two requirements have to be attended:\n\n\nSimple activation of the watchdog and regular resets of the timer do not make optimal use of a watchdog. For best results, the refresh cycle of the timer must be set as short as possible and called from the main function, so a reset can be performed before damage is caused or an error occurred. If a microcontroller does not have an internal watchdog, a similar functionality can be implemented by the use of a timer interrupt or an external device.\n\nA brown-out circuit monitors the VCC level during operation by comparing it to a fixed trigger level. When VCC drops below the trigger level, the brown-out reset is immediately activated. When VCC rises again, the MCU is restarted after a certain delay.\n\n\n"}
{"id": "38626463", "url": "https://en.wikipedia.org/wiki?curid=38626463", "title": "Information and communications technology in Kosovo", "text": "Information and communications technology in Kosovo\n\nInformation and communication technology (ICT) in Kosovo has experienced a remarkable development since 1999. From being almost non-existent 10 years ago, Kosovar companies in the information technology (IT) domain offer today wide range of ICT services to their customers both local as well as to foreign companies. Kosovo has the youngest population in Europe, with advanced knowledge in ICT.\n\nToday, public and private education institutions in the IT field, through certified learning curricula by companies such as CISCO and Microsoft, provide education to thousands of young Kosovars while the demand for this form of training is still rising.\n\nKosovo has two authorized mobile network operators and is the only country in the region not having awarded any UMTS license. Kosovo has neither awarded licenses for fixed wireless access, nor made the 900 and 1800 MHz bands technology neutral. Currently around 1,200,000 customers of \"Vala\" Post and Telecom of Kosovo (PTK). As of March 2007 the second GSM license granted to IPKO – Telekom Slovenije. Currently IPKO has ca. 300,000 users. Following the Brussels Agreement, Kosovo has its own telephone dialing code: 383. Before this assignment, network operators in Kosovo used either 387 (Monaco) or 386 (Slovenia). All other codes were to have been superseded by the new code on 15 January 2017, but some are still in use.\n\nThe infrastructure of ICT sector in Kosovo is mainly built of microwave network, optic and coaxial cable (DOCSIS). The telecom industry is liberalized and legislation is introduced adopting European union(EU) regulatory principles and promoting competition. Some of the main internet providers are PTK, IPKO, Kujtesa and Artmotion \n\nFirst ICT companies in Kosovo can be found as early 1984, these companies where mainly focused on radio telecommunication and audio-video systems, while in early and mid '90s more companies were created, mainly specializing in personal computer sales. ICT industry in Kosovo boomed after 1999 with a lot of new companies being created, among which IPKO which now one of the major telecommunication providers and one of the biggest foreign investments in Kosovo.\n\nAccording to Regulatory Authority of Electronics and Postal Communication 2011 report, 86 telecommunication licenses have been issued since 2004.\nIn 2010, 74 percent of the population was subscribed to mobile phone services, or a total number of 1,537,164 In 2007, PTK reported growth of subscribers from 300,000 to 800,000 in less than a year. In 2006, the number was 562,000.\n\nThere need for showing your ID to get a Sim card and they can be bought prepaid for 5 Euros with credit.\nThere are a large number of shops for buying used mobile phones, they are sold on the street. \nAlso SIM unlocking shops are numerous. \nYou can buy credit for your mobile phone for the nominal price of 5 Euros, but the handlers charge more for the credit, from 1 Euro more.\n\nTwo licensed Mobile network operators are offering their services in competition with two MVNOs. The market, however, remains concentrated with the incumbent‟s mobile subsidiary controlling over 65% of the market. Mobile broadband services are not available as no UMTS licenses have been awarded. So far, there are no plans to carry out the re-farming of 900/1800 MHz bands or assign frequency spectrum for mobile broadband.\n\nThere are three virtual operators :\n\nFixed telephony penetration rate is among the lowest in Europe about 5 lines per 100 inhabitants in 2011, in contrast with neighboring countries and EU countries where penetration rates are 25% and 40% respectfully.\n\nThere are currently three licensed Fixed Telephony providers in Kosovo:\n\n\nPTK is by far the leading provider with market share of 94.4%, IPKO has only 5.6%. Number of subscribers dropped in 2011 to 86,014 from 88,372 in 2010, marking a drop of 2,358 or 2.67% of subscriber base.\n\nAfter the breakup of Yugoslavia, with dialing code 38, Kosovo used the Serbian dialing code, 381 for new and existing landlines. As mobile networks were introduced, PTK adopted the code 377 (Monaco) and IPKO adopted the code 386 (Slovenia). This situation resulted in the highly unusual simultaneous use of three international dialing codes.\n\nIn September 2012 the Assembly of Kosovo approved a resolution on replacing the various dialing codes in use with the Albanian country code 355. While this initiative draw a lot of media attention, it never saw the light of day.\n\nIn January 2016 after ongoing political discussions between Kosovo and Serbia, it was agreed that Kosovo would get its own country code: 383. This code is available for all mobile and fixed-line operators, all of whom were using other international telecom country code. The code 383 has now been formally assigned, and is in the process of adoption. This code was to have replaced all former codes on 15 January 2017, but the transition has yet to be complete.\n\nTotal of 38 licensed companies provide internet services in Kosovo, 6 of them with direct peering towards international gateways. Number of technologies are used to provide internet to end users, most popular being the cable DOCSIS technology with 68.95% of the market, followed by 25.43% xDSL and 5.62% other technologies like FTTX and wireless.\n\nIn contrast with other countries, majority of market share is owned by private operators, with a total of 74.57%. The biggest operator being IPKO with 51.21% followed by Kujtesa with 19.08% and PTK with 25.43%.\n\nOthers include:\n\nInternet penetration in Kosovo is in the process of steady development due to healthy competition and mainly due to government policies and strategies adopted with the aim to reach the maturity of the neighboring countries and EU integration. Broadband internet access accounts for the majority of internet connections, with ADSL, cable and wireless available.\n\nInternet penetration in Kosovo is somewhere between 5.40% to 20.90% depending on which report you look at. This miss-balance report is attributed mainly to rural areas where internet access is not formally offered but is rather resold formally by smaller operators, though the latest study conducted by XYZ puts internet penetration in Kosovo at 72.1% which is very close to EU countries, which reported internet penetration of 73% in 2011. As of September 2010, the number of internet subscribers was 105,061 or 35.02% household penetration rate.\n\nInternet coverage is widespread but it experiences frequent outages.\n\nDigital television transition is still an ongoing process in Kosovo, limiting the analog television broadcast domain to only three national channels. Due to this conditions all four major telecommunications companies in Kosovo now broadcast digital TV on other mediums. While IPKO and Kujtesa have chosen to reuse the existing coaxial network, PTK which offers xDSL services went for IPTV, Artomotion offers digital TV to selected users in Pristina.\nIPKO has recently launched an IPTV solution, tailored towards mobile customers. Currently platform is available in iOS, Android and via a web browser.\n\nDue to missing 3G/LTE licenses in Kosovo, and a growing demand for mobile broadband services from subscribers, both telecommunication providers PTK and IPKO turned to Municipal wireless network (Muni Wi-Fi). Both operators cover the majority of cities around Kosovo and touristic destinations like Prekaz and Brezovica, with more cities to be covered later.\n\nThe Internet Exchange Point (KOSIX), the first of its kind in Kosovo, started operating on 23 June 2011, and it operates as a functional unit within Telecommunications Regulatory Authority (TRA). Its function is to provide the ISPs operating in Kosovo, an Internet exchange point for local traffic exchange.\nValuable and continuous contributions for the implementation of this project have given: United States Agency for International Development (USAID) offered throughout their program Kosovo Private Enterprise Program (KPEP), Norwegian government offered through Ministry of Foreign Affairs and Norwegian embassy respectively, Cisco Systems International BV, and University of Prishtina which have offered adequate space within the buildings of Electrical and Computer Engineering Faculty.\n\nCurrently four national ISPs are interconnected via KOSIX, there is no cost for peering.\n\nICT in Kosovo consists of relatively young companies (most of them incorporated after 1999) and with predominantly small companies with less than 20 employees. 53.8 percent of ICT companies are individual business while limited Liability Company (28.6 percent). Businesses specializing in maintenance and manufacturing are purely individual, while other sub-sectors are served by a mix of individual and LLC businesses. Other forms of incorporation are rare, with 4.4 percent being Limited Partnership, and 2.2 percent Joint-Stock Company. The rest (5.5) are either public companies or have the unusual status of NGOs.\nCurrently, the ICT companies are determined to grow and prosper within the Kosovo market, while very few companies seek expansion in markets outside Kosovo.\n\nAccording to TRA report, ICT industry generated €239,518,037.36, 83.19 of which is generated by Mobile network operator, 8.37% from Fix telephony operators, 7.77% from Internet service providers and 0.68% from leased lines.\n\nThe structure of the ICT market in Kosovo is diverse in the variety of activities, sales being the main activity 62 percent of the ICT companies have reported to import goods for retail, meanwhile their exports are minimal and their market share growth is seen to be within Kosovo, and could reach as far as Macedonia and Albania. The average annual turnover in the sector is 250,000 Euros, with an increasing number of companies reporting turnovers in millions of Euros.\n\nThe ICT sector is dominated by domestic firms. According to the survey, 80.2 percent of the respondents represented companies that are 100% domestically owned. Only 6.6 percent of the companies are entirely owned by foreigners. Mixed ownership is rare (3.3 percent). The other 3.3 percent that answered \"Other\" were either mostly owned by foreign companies or public/state-owned companies. Foreign investors are mostly present in the sub-sectors: consulting, information services, vendors, manufacturing/assembling, and retail.\n\nThe majority of ICT companies is small. More than three-quarters (77.1 percent) employ from one to twenty employees. Only a handful of the companies have more than 100 employees. Most of the employees working in the ICT sector are male, leaving the female employees in a tiny minority. Around 19 percent of the companies do not have any female employees at all. Over 93 per cent employ up to 10 women, 6 per cent of the firms employ between 11 and 20, and only 1.4 percent up to 30. There is one large company which employs 230 female workers.\n\nThis list is a result of a survey conducted by USAID, Kosovo Private Enterprise Program (KPEP) with 829 ICT companies.\n\nAccording to statute, STIKK is a non-profit association founded and registered in accordance with the Law on Freedom of Association and Non-Governmental Organization. STIKK represents the interests of the information and communications technology of Kosova, and the interests of professionals in ITC industry.\n\nThe increasing number of vacancies for ICT professionals in Kosovo is reflecting the increasing progress of the\nindustry, although thanks to the high quality of the university education of IT specialists, and the increasing interest\nof young people in modern technologies, there are no signs of systematic shortages in ICT employment, except a\nregistered under-supply of specialists in the field of software development and programming. The number of ICT\ngraduates grows each year and the leader in providing the needed skills to the industry is the Faculty of Electrical\nand Computer Engineering of the University of Pristina. ICT skilled professionals are also supplied by the\nUniversity for Business and Technology, the American University in Kosovo, as well as a few vocational education\nproviders.\n\nAccording to the Kosovo Accreditation Agency there are currently 13 higher education institutions, public and private, accredited to offer ICT related study programs in their curricula.\n\n\n\nAAB College https://aab-edu.net/ AAB College is the first non-public institution of higher education in Kosovo\n\n\n\nFor a short period Kosovo has managed to adopt few very important pieces of legislation and a strategic framework to support the government’s efforts to regulate, promote and improve the development of the ICT sector in Kosovo. Some of the most important legislative acts that have influenced the progress of the sector are:\n\n\n\nLaw on Telecommunications adopted by the Assembly and promulgated by UNMIK, Regulation 2003/16,recognizes the need to improve the telecommunications sector of Kosovo, by establishing an independent regulatory agency responsible for licensing and supervising the providers of telecommunications services in Kosovo, encouraging the private sector participation and competition in the provision of services; setting standards for all service providers in Kosovo, and, establishing provisions for consumer protection.\nTRA officially started operating in January 2004.\nDuring its young development TRA went through some important milestones that represented\na very important step towards a free, competitive market which promotes the development of the information society in Kosova.\n\nDue to the overly young population in Kosovo, digital divide is not very notable, this phenomenon is more notable with people in their fifties and above. This problem was evident with the educational system as well, to address this Government of Kosovo organized an ECDL course for about 27000 teachers across Kosovo.\n\nThe labor force in the ICT sector is dominated by men with women comprising a marginal portion (although more significant in larger companies).\n\nMost ICT firms are based in the Pristina, the economic, political, and social center of the country, where most businesses are located and where there is the highest concentration of customers,as much as 81 percent of all ICT companies have Prishtina as their head office location. The rest are fairly evenly spread out in the regional centers: Peja, Prizren, Gjilan, Gjakova, Podujeva, and Ferizaj.\n\nOver the years there have been a number of open source organizations including Albanian Linux user group (AlbaLinux). Due to lack of support, most of them are now \"passive\"; among the most active and successful open source groups is Free Libre Open Source Software Kosova (FLOSSK).\n\nFLOSSK began in March 2009 at the initiative of James Michael DuPont as a result of the desire to organize a conference on free and open software. After six difficult months and with the help of many supporters, FLOSSK organized the first conference of free and open software in Kosovo in August 2009.\nApart from the conference, FLOSSK continued to work in various activities such as organizing Software Freedom Days in different cities of Kosovo, lectures on free software throughout Kosovo, translating software, collaborating with the media to promote free software and creating local free software groups in various cities.\nFrom the beginning, FLOSSK members and the general public learned about GNU/Linux operating system, FLOSS programs for solving everyday problems, map creation using OpenStreetMap, and met free software movement figures from around the world.\n\nSoftware Freedom Kosova Conference is an annual conference on free and open source software and related developments in knowledge, culture and mapping held in Pristina, Kosovo. It is the largest conference of its kind in the region. The conference is organised by Free/Libre Open Source Software Kosova (FLOSSK), Kosovo Association of Information and Communication Technology, Ipko Foundation and Faculty of Electrical and Computer Engineering of the University of Prishtina.\n\nA case study on ICT training in Kosovo performed by CISCO Networking Academy (NetAcad) states that the educated and experienced workforce as a whole is searching higher salaries and better working conditions abroad. If graduates are not experienced, they stay for a while in Kosovo and when they have gained experience they start searching opportunities for migration.\n\nIt is exactly the findings that NetAcad case study revealed that make Kosovo a perfect ICT outsourcing country, and time difference with the USA makes it only more appealing for the U.S. market. Such is a story of 3CIS which provides highly specialized\nservices to major telecommunication carriers across the globe. This includes network architecture design, planning, consulting, implementation,integration and testing with a strong expertise on mobile backhauling. 3CIS also provides on-site consulting ser-\nvices as well as manages and coordinates the activities in a multi-vendor environment during the life-cycle of the complete project. On top of this, 3CIS also offers Project management services that are tailored to suit client needs from initial planning to project completions.\n\nKosovo has made it to other markets as well, both individually as well as established companies. Sprigs is the best example of a Kosovar start-up company established in Pristina.\nAnoniem is the highest-profile job to date for SPRIGS, which was founded in late 2010, by a Dutch entrepreneur, and this job was trusted to a half-dozen young Kosovar Albanian programmers, who work at computers at a repurposed apartment that now houses the technical brain trust of this IT outsourcing company.\n\n\n\n"}
{"id": "32484563", "url": "https://en.wikipedia.org/wiki?curid=32484563", "title": "International Institute of Refrigeration", "text": "International Institute of Refrigeration\n\nThe International Institute of Refrigeration (IIR) (also known, in French, as the Institut International du Froid (IIF)), is an independent intergovernmental science and technology based organization which promotes knowledge of refrigeration and associated technologies and applications on a global scale that improve quality of life in a cost effective and environmentally sustainable manner including:\n\nIts scientific and technology activities are coordinated by 10 commissions which are divided into five distinct sections. The Institute produces a monthly journal on refrigeration, the \"International Journal of Refrigeration\", that is published by Elsevier.\n\nThe staff of the International Institute of Refrigeration includes the Director General, Didier Coulomb, the Scientific and Technical Information Department (STID), the Communications and Conference Development Department, the Translation Department, and the Secretariat.\n\nIn the mid-19th century, demand for natural ice exploded during the summer months in Europe, North America and various colonies. Thanks to the advent of railways and steam ships, natural ice came onto the market. In order to meet demand, suppliers began looking for alternative ways of producing ice. Ferdinand Carré invented the absorption machine in 1859, then along came the vapour-compression refrigerator that is still used today.\n\nIn order to promote the development of refrigeration technologies in a developing environment with huge economic potential thanks to this new technology, the International Institute of Refrigeration was created.\nThe bylaws of the IIR as an intergovernmental organization were defined by an International Agreement signed on December 1, 1954 and an Application Protocol signed on November 20, 1956.\n\nWith the use of its international network, the IIR has established an Expertise Directory and a Laboratory Directory. The IIR is a bilingual organization that works in both English and French and operates thanks to:\n\nThe General Conference of the IIR defines the IIR's general policy and convenes once every four years. It includes representatives appointed by member countries. Prof. Philippe Lebrun (CERN) is currently President of the General Conference; he was elected in August 2015. The General Conference elects the President and Vice-Presidents of the Executive Committee.\n\nThe Executive Committee of the IIR handles the administrative and financial aspects of the daily running of the IIR, and meets once per year. It includes one delegate per member country, a president and three to six vice-presidents. Since 2015, the President of the Executive Committee is Prof. Felix Ziegler, Delegate of Germany.\n\nThe Management Committee is responsible for management of the IIR in between Executive Committee meetings . It includes:\n\nThe Science and Technology Council (STC) coordinates the scientific and technical activities of the IIR. The Science and Technology Council includes five distinct sections that are in turn divided into 10 commissions. The Science and Technology Council includes a president, three to four vice-presidents and commission presidents.\n\nThe President of the STC is Dr Piotr Domanski. (NIST).\n\nThe IIR's scientific activities are divided into five sections, each of which is divided into two commissions; there are thus 10 commissions:\n\nSection A Cryogenics and Liquefied Gases focuses on refrigeration science and technology at low temperatures: the cryogenic domain spans the lower part of the temperature scale, from absolute zero to 120 K, thus encompassing the normal boiling points of air gases as well as of liquid natural gas (LNG).\n\nSection A comprises two Commissions, A1 Cryophysics and Cryoengineering, and A2 Liquefaction and Separation of Gases. Commission A1 deals with research, development and industrial activities at the lowest temperatures, including low-temperature physics, applications of superconductivity and helium cryogenics. Commission A2 essentially covers the liquefied gas industry, including air separation and LNG technology, two mature domains with high economic stakes and ongoing developments addressing important societal issues such as energy efficiency and carbon sequestration.\n\nSection A also maintains and develops relations with other Sections of the IIR, mainly Commission B1 Thermodynamics and Transfer Processes in the field of thermodynamics and transfer processes, essential tools of the cryogenic engineer, and Commission C1 Cryobiology, Cryomedicine and Health Products for the cooling of biological specimens and living tissues for preservation or treatment which require implementing cryogenic processes. Section A consists of a panel of multidisciplinary professionals and experts in sciences and technologies such as thermodynamics, condensed matter physics, materials science, heat transfer, fluid dynamics, vacuum and leak-tightness, instrumentation and process control, applied to the low-temperature domain.\n\nThe vice-president of Section A is Dr Ralf Herzog (ILK Dresden).\n\nCommission A1 - Cryophysics, Cryoengineering deals with research, development and industrial activities at the lowest temperatures, including low-temperature physics, applications of superconductivity and helium cryogenics.\n\nThe president of Commission A1 is Dr Ales Srnka (Institute of Scientific Instruments of the CAS).\n\nThe work of Commission A2 Liquefaction and Separation of Gases reflects world-wide activities in the domain of separation of gases and liquefaction. Apart from the personal involvement of Commission members in various projects, the Commission is present at conferences, workshops and seminars: LNG International Exhibition and Conference, GASTECH, Cryogenics, Cryogen Expos, European Cryogenic Course and others.\n\nThe Commission is close to academia, industry and end users of separated and liquefied gases.\nCommission members work closely with Commission A1 Cryophysics, Cryoengineering and Commission C1 Cryobiology, Cryomedicine and Health Products.\n\nThe president of Commission A2 is Dr Maciej Chorowski (Wroclaw University of Technology).\n\nSection B Thermodynamics, Equipment and Systems of the IIR focuses on the technological and scientific fundamentals of classical refrigeration, excluding cryogenic temperatures. The fundamentals are represented by its Commission B1 Thermodynamics and Transfer Processes whereas Commission B2 Refrigerating Equipment covers all kinds of refrigeration technology. Section B is a key player in most of the IIR international conferences; except for the International Conference of Refrigeration (ICR) organised every four years for all 10 IIR Commissions, where approximately 50% of all presentations are related to Section B topics.\n\nIndependently, and together with other Sections, Section B hosts a multitude of conferences such as the Gustav Lorentzen Conference on Natural Working Fluids, the Ohrid Conference on Ammonia and CO2 Refrigeration Technologies, the Jordanian Conference on Refrigeration and Air Conditioning; or conferences on Thermodynamic Properties and Transfer Processes of Refrigerants, on Magnetic Refrigeration at Room Temperature, on Compressors and Coolants, and on Phase Change Materials and Slurries for Refrigeration and Air Conditioning.\n\nA number of Working Groups, where emerging topics in refrigeration are discussed by IIR experts with the aim of publishing results in handbooks or other forms publications, are organised within the scope of Section B. Main topics include mitigation of direct emissions of greenhouse gases in refrigeration, refrigerant charge reduction in refrigerating systems, magnetic cooling, life cycle climate performance evaluation, and refrigerant system safety.\n\nSection B has more than 120 members from across the globe.\n\nThe president of Section B is Prof. YongTae Kang (School of Mechanical Engineering Korea University).\n\nThe objectives of Commission B1 Thermodynamics and Transfer Processes are to provide academic and industrial information and data, and to propose any solutions on thermodynamics and transfer processes. The Commission B1 has been extremely active in IIR Working Groups, sub-commissions, IIR conferences and co-sponsored conferences and commission business meetings.\n\nAs well as being involved in IIR Working Groups on the mitigation of direct emissions of greenhouse gases in refrigeration, the Commission is equally involved in the Working Group on Life Cycle Climate Performance (LCCP) Evaluation.\n\nActive in IIR conferences and congresses, Commission B1 similarly organises workshops in various fields such as refrigerant charge reduction in refrigerating systems. Initiatives and opportunities, such as the phase-down of high-GWP refrigerants, energy-efficient buildings and cars, transport refrigeration, food preservation, the economic importance of the refrigeration sector, the involvement of the younger generation and identifying industrial needs are all at the heart of Commission B1.\n\nThe president of Commission B1 is Prof. Carlos Infante Ferreira (TU Delft).\n\nThe activities of Section C deal with the application of refrigeration technologies to life sciences and food sciences.\n\nCommission C1 Cryobiology, cryomedicine and health products is particularly focused on the application of refrigeration technologies on various branches of medicine: cryosurgery and oncology, cryotherapy, blood, organs and tissue preservation, health products (especially vaccines and thermosensitive preparations). On the one hand, the work focusses on the biological and biochemical aspects of the effects of refrigeration on organs, tissues and treated products, and on the other hand on the applied refrigeration techniques and technologies.\n\nCommission C2 food science and engineering is focused more particularly on the application of refrigeration technologies in the area of food sciences: preservation (refrigeration, freezing); hygiene and safety in its microbiological aspect; process (lyophilisation, cryoconcentration, cryoprecipitation, partial or total crystallisation). The work focuses on establishing a model for the transfer of heat and matter during refrigeration treatments, on the effects of refrigeration on food products, and on the evolution kinetics of products kept in cold storage. The work deals with the impact of the integrity of the cold chain on the quality of food, including in warm climate countries.\n\nThe president of Section C is Mr Jacques Guilpart (Maison du Froid Conseil).\nCommission C1 Cryobiology, Cryomedicine and Health Products have clearly defined objectives in cryobiology, cryomedicine and health products research; knowledge dissemination; technology transfer and education.\n\nThis Commission is truly active and participates in the various workshop series on cryoprocessing of biopharmaceuticals and biomaterials, as well as establishing innovative e-training actions concerning the Commission’s multidisciplinary needs as well as the interdisciplinary needs of the following commissions: A1 Cryophysics, Cryoengineering, A2 Liquefaction and Separation of Gases and finally C1 Cryobiology, Cyomedicine and Health Products.\n\nThe president of Commission C1 is Prof. Noboru Motomura (University of Tokyo).\nCommission C2 on Food Science and Engineering focuses on research and breakthrough technologies related to food science and engineering. The Commission is key in hosting the IIR Sustainability and the Cold Chain Conference (ICCC), held internationally since 2010. In addition to the Cold Chain conferences and the IIR Congress, Commission C2 has also co-sponsored four other conferences in Macedonia, Spain, Croatia and Germany, and continues to reinforce its leading role at the heart of developments in food science and engineering. The Commission was a very active member of the organization team for the 2011 IIR congress in Prague.\n\nIIR Commission C2 is composed of 38 experts from 20 IIR member countries. The Commission is involved in various IIR Working Groups and innovative projects linked to the development of the food chain across the globe.\n\nThe president of Commission C2 is Dr Judith Evans (London South Bank University).\n\nSection D Storage and Transport of the IIR is involved in the controlled-temperature logistics and distribution of temperature-sensitive products, from foodstuffs to health products (medicines, vaccines, blood products, organs ...) from artwork to chemicals.\n\nIt addresses all issues of equipment and solutions for a durable cold chain from the production or manufacture, to the consumption or use of these products.\n\nSection D thus covers the issues of storage, transportation by land, air or water, packaging, distribution and delivery of these products to the consumer, and the traceability of the cold chain.\n\nThe Section is involved in warehouse and platform equipment, devices for temperature-controlled transport, coolants or cool packs, small coolers and refrigerated containers, chillers, refrigerated furnishings, refrigerated cabinets, climate chambers, refrigerators and freezers, but also to thermometers and temperature recorders.\n\nThe cold chain involves many temperature ranges, both positive and negative, from -80 °C to + 63 °C.\n\nThe vice-president of Section D is Gérald Cavalier (Cemafroid SNC).\n\nCommission D1 Refrigerated Storage deals with the storage of all products which require temperature control, such as food and pharmaceuticals. Industrial, commercial and residential storage are also taken into account so that, in cooperation with Commission D2 Refrigerated Transport, the entire cold chain is treated, from raw materials to the final product at our home. Refrigeration plays an essential role for perishable products. While the estimated capacity of refrigerated warehouses was over 500 million cubic meters worldwide in 2014, in some countries global food losses due to the lack of a cold chain are still very important and can reach as much as 20% of the global food supply. At the same time, in heavily industrialised countries, the use of commercial and domestic refrigerators accounts for up to 6% of global electricity consumption.\n\nAs a result, the Commission faces important issues in order to promote widespread, energy efficient and environmentally friendly storage systems. New refrigerants, synergies to save or exchange energy with other systems and new technologies are the main focus of its activity. One of the most important themes in these days for this Commission is energy efficiency\n\nThe president of Commission D1 is Kuniaki Kawamura (Mayekawa Mfg. Co. Ltd.)\n\nThe IIR’s Commission D2 Refrigerated Transport is extremely active. In addition to the IIR’s four yearly congress, Commission D2 participates in the IIR Conference on Sustainability and the Cold Chain, held out of synchronisation to the congress.\n\nEvery year, Commission D2 CERTE test engineers meet in a European country to discuss refrigerated transport technology and testing issues. This group subsequently advises the United Nations working party on transport of perishable foodstuffs held each year in Geneva. Commission D2 is currently addressing the “Cold Chain for Pharmaceutical Products” and will add this to regular transport discussion and advisory topics. Commission D2 also helps to produce Informatory Notes to assist in areas of technical or regulatory difficulty.\n\nThe role of the IIR is well recognized, and in particular, the expertise of the members of Commission D2 makes an important contribution to refrigerated transport issues: reducing food wastage and minimizing emissions.\n\nThe president of Commission D2 is Richard Lawton (CRT Cambridge).\n\nIIR Section E co-ordinates the work of the both Commissions E1 Air-Conditioning and E2 Heat Pumps and Heat Recovery.\n\nThe core activities and interests of both Commissions are strongly connected resulting in tight collaborate and jointly organised conferences.\n\nAir-conditioning is a subject that is now more frequently addressed due to both better comfort in an increasing number of countries and the effects of global warming. Now, even countries where demand for air-conditioning during summer months was limited, due to a cooler climate, require the operation of an air-conditioning plant for longer periods. The demand of heating is nevertheless significant and the most efficient system to provide heating is undoubtedly the heat pump. No other technology can provide net primary energy savings, economic benefits to users and reduced climate impact at the same time.\n\nAlso providing a cooling effect, the heat pump is expected to be the most common solution in the future for all year round operations. The combination of these technologies, with heat recovery capable buildings or industrial plants, cooling and heating requirements can be meet in the most efficient, reliable, cost effective and environmentally friendly way.\nSection E with its Commissions is made up of 101 experts.\n\nThe vice-president of Section E is Prof. Renato Lazzarin (Universita di Padova DTG).\n\nCommission E1 Air Conditioning often collaborates with Commission E2 Heat pumps and Energy Recovery as they have at least one common aspect, the compressor. Both Commissions frequently work with the same equipment which is adapted according to the seasons, alternating between air conditioners and heat pumps.\n\nThe Commission is involved in various aspects of air conditioning from equipment to systems. In the last years it developed a particular focus on energy saving and sustainability, whilst maintaining good conditions of thermal comfort ranging from topics such as free cooling, solar cooling or long term energy storage. The general importance of the themes addressed by the Commission results in relevant International Conferences.\n\nThe expertise of the Commission members on the use of new refrigerants in air conditioning systems, annual comparative studies of innovative and renewable energy systems, opportunities of part load operation on air conditioning systems to limit penalties or even to gain efficiency, and on other up-to-date research fields, is valuable, not only to the scientific community but also to the multitude of air conditioning users.\n\nThe president of Commission E1 is Prof. Xianting Li (Tsinghua University).\n\nCommission members are proposed by member countries then appointed by the STC following proposals from Presidents of commissions. These commission members comprise industry, university, and research-centre specialists or refrigeration practitioners.\n\nThe aim of commission E2 Heat Pumps, Energy Recovery is to promote and enhance scientific and technological knowledge in heat pump and energy recovery fields thanks to various activities such as the organization or co-sponsoring of international conferences, or the publication of books and Informatory Notes.\n\nThe president of Commission E2 is Dr Alberto Coronas (University Rovira i Virgili).\n\nREAL Alternatives for LIFE, an extension of the previous REAL Alternatives project, aims to develop new training materials to help conform to safety standards and to introduce a range of practical exercises and assessments. \n\nFinanced by the “LIFE programme”, the EU funding instrument for the environment and climate action, this 3-year project begun in June 2017 and promotes innovative solutions and will raise awareness of the best practices in reducing emission and adapting to climate change worldwide.\n\nThe objective of the CryoHub project is to investigate and extend the potential of large-scale Cryogenic Energy Storage (CES) and explore the application of the stored energy for both cooling and energy generation.\nFunded by the European Union Horizon 2020 Research and Innovation Programme, the three and a half year project began in April 2016 and will end in September 2019.\n\nSuperSmart aimed to achieve both decisive environmental benefits through the implementation of efficient heating and cooling solutions, as well as economic benefits through reduced energy use in the supermarket sector across Europe.\n\nFunded by the European Union Horizon 2020 Research and Innovation Programme, the 3-year project began in February 2016 and will end in January 2019. \n\nThis ELICiT project focuses on the application of magnetic cooling technology in domestic refrigeration appliances.\n\nFunded by Framework Programme 7 (FP7), the 3-year project began in January 2014 and ended in December 2016. Its main objective is to further increase collaboration between SMEs and global appliance manufacturers, and to draw on the expertise found in universities and research centres).\n\nThe COOL-SAVE project carried out the development and dissemination of cost effective strategies to improve energy efficiency in cooling systems in the food and drink sector. More specifically, it aimed at optimising vapour-compression mechanical systems in the food and drink sector, which can be achieved through the dissemination and the implementation of energy efficient strategies which prove effective.\n\nFunded by Intelligent Energy Europe, the 3-year project began in 2012 and ended in April 2015.\n\nA 4-year Framework Programme 7 (FP7) funded project begun in 2010, FRISBEE ended on 31 August 2014. The objective of the FRISBEE project was to provide new tools, concepts and solutions for improving refrigeration technologies along the European food cold chain.\n\nFridoc is the most comprehensive refrigeration database in the world. It contains nearly 100,000 references to documents in all domains of refrigeration.\n\nMost of the documents referenced in Fridoc are scientific and technical. Fridoc also contains many review articles, documents on economic data and statistics, articles dealing with regulations and standardization, etc.\n\nThe IIR has over 160 publications available on refrigeration and refrigeration applications: reference documents, guides, technical books, conference and congress proceedings, tables and diagrams comprising the thermophysical properties of refrigerants.\nBooks in the refrigeration field published by other publishers are also available for purchase.\n\nRecent publications include the IIR technical guide on the application of \"CO2 as a Refrigerant \" by Dr Andy Pearson (2014), the \"Handbook on indirect refrigeration and heat pump systems\", editor, Åke Melinder (2015).\n\nThe IIR produces a quarterly newsletter. It features refrigeration-sector news: regulations, events, economic data, monitoring, technological progress, etc.\n\nIt provides a detailed overview of the general developments within the sector worldwide and as acts a regular information tool for readers.\n\nThe \"International Journal of Refrigeration\" is the reference journal in the refrigeration field. It is now ranked 19th out of 115 journals in the domain of mechanical engineering and 13th out of 49 in the field of thermodynamics.\n\nIt is practical for all those wanting to keep abreast of research and industrial news in the refrigeration, air-conditioning, heat-pump, refrigerated storage and transport fields.\n\nThe IIR holds international conferences and congresses on key themes which include:\n\nOnce every four years the IIR holds overarching International Congress of Refrigeration (ICR) encompassing all refrigeration fields which unites major stakeholders active in the refrigeration industry.\nThis event takes place in a different location worldwide each time and acts as a platform for experts, engineers, professors, students and other interested parties to exchange on current matters within the field.\n\nIIR Working Groups (WG) operate on a temporary basis, bringing together specialists, to work on projects arising from current issues.\n\nTheir aim is to promote development, provide knowledge and give recommendations in these spheres. In order to achieve these objectives, they hold conferences and workshops, write publications and provide recommendations.\n\nMembers of WGs are IIR members from industry, university and research-centre settings or are refrigeration practitioners.\n\nToday, the IIR has 58 member countries, 26 of which are active, paying members, representing over two-thirds of the global population. These countries are divided into six categories according to their contributions to the IIR. They take part in IIR activities via their delegates and their nominated commission members. The delegates and commission members determine IIR priorities and take part in the IIR scientific activities and Working Groups, and develop recommendations. Member countries are entitled to host several IIR conferences and meetings per year. Their category level (ranging from 1 to 6) determines the services that they receive.\n\nThe following countries are members of the IIR:\n\nBenefactor and corporate members can be companies, universities, national, regional or international organizations, laboratories, associations or any other structure active in or connected to the refrigeration industry or IIR activities.\n\nPrivate members can be individuals such as researchers, scientists, industrial practitioners or professors with extensive expertise, passion or active in fields related to the refrigeration sector.\n"}
{"id": "24341524", "url": "https://en.wikipedia.org/wiki?curid=24341524", "title": "KIU System", "text": "KIU System\n\nKIU is a Computer Reservations System and Global Distribution System used by airlines, travel agencies and hosted airlines GSAs worldwide. It current IATA code is C1 and it is certified by SITA / CUTE and Ultra Electronics UltraCUSE airport check-in platforms.\n\nKIU was formed in 2002 by a team of professionals of the Airline and IT industry. KIU has experienced a continuous growth until become the most important Latin American Computer Reservations System and Global Distribution System, Airline inventory, Departure Control System, Air Cargo & revenue accounting systems with strong presence in many regional, low cost airlines and domestic airlines of the following countries : Argentina, Colombia, Venezuela, Ecuador, Peru, Bolivia, Paraguay, Aruba, Panama, Mexico & Spain.\n\n\n\n"}
{"id": "11943080", "url": "https://en.wikipedia.org/wiki?curid=11943080", "title": "Kegel (bowling)", "text": "Kegel (bowling)\n\n, or , (German for \"skittle\", skittles) is a German bowling game played in Australia, in which a player rolls a wooden or plastic ball along a smooth, hard indoor lane (German: \"\", bowling alley). The object of the game is to knock down the nine kegels at the other end of the lane. Kegel is based on the traditional German game of nine-pin bowling and is therefore closely related to both skittles and ten-pin bowling. It was introduced to South Australia by German settlers in the 19th century and remains popular in areas in which many German people settled, such as the Barossa Valley.\n\n"}
{"id": "9733776", "url": "https://en.wikipedia.org/wiki?curid=9733776", "title": "Mandy Chessell", "text": "Mandy Chessell\n\nAmanda Elizabeth Chessell is a computer scientist and a Distinguished Engineer at IBM. She has been awarded the title of IBM Master Inventor. She is also a Member of the IBM Academy of Technology.\n\nOutside IBM, Chessell is the first woman to be awarded the Silver Medal of the Royal Academy of Engineering. In 2002 she was elected a Fellow of the Royal Academy of Engineering.\n\nChessell is a visiting professor at the University of Sheffield and at the Surrey Centre for the Digital Economy (CoDE) at the University of Surrey.\n\nMandy Chessell joined IBM in 1987. She is based at IBM's Hursley laboratory located near Winchester in Hampshire, UK.\n\nHer early work focused on distributed transaction processing, adding features to products such as CICS, Encina, Component Broker and WebSphere Application Server. She has also work on event management, business process modelling and outside-in design (OID).\n\nThen she focused on developing model-driven tools to simplify the analysis and design of large systems and then to automate their development. This work covers the development of user interfaces, services, information integration technology in the field of Master Data Management.\n\nHer work today is focused on data lake architectures, metadata management and information governance.\n\nChessell frequently lectures on topics related to Computer Science and, in particular, innovation. Such lectures take place at universities such as Queen Mary University of London.\n\nShe was also one of the 30 women identified in the BCS Women in IT Campaign in 2014. Who were then featured in the e-book \"Women in IT: Inspiring the next generation\" produced by the BCS, The Chartered Institute for IT.\n\nIn 2000, she was among the first group of \"MIT Technology Review\" magazine's TR100.\n\nIn 2001, she won the Royal Academy of Engineering Silver Medal for the invention and engineering of Reusable Software Component Architecture.\n\nIn 2002, she was elected a Fellow of The Royal Academy of Engineering.\n\nIn 2004, Chessell won the British Computer Society nomination for the Women's Engineering Society \"Karen Burt\" award.\n\nIn 2006, Chessell won a Female Inventor of the Year Award for building capacity for innovation.\n\nAlso in 2006, Chessell was awarded a prize for the Best Woman in the Corporate Sector at the Blackberry Women in Technology awards.\n\nIn 2011, Chessell was made an honorary fellow of the Institution of Engineering Designers (IED).\n\nIn 2012, Chessell received Innovator of the Year at the Cisco everywoman in Technology Awards.\n\nIn 2013, Chessell received an Honorary Doctor of Science from Plymouth University.\n\nIn 2015, Chessell received an Honorary Doctorate of Technology from University of Brighton.\n\nShe was appointed Commander of the Order of the British Empire (CBE) in the 2015 New Year Honours for services to engineering.\n\nIn 2016, Chessell was named in the Top 50 Influential Women in Engineering List 2016 and received an Honorary Doctorate of Technology from University of South Wales\n\nIn 2017, Chessell received an Honorary Doctor of Science from University of Bath.\n\nChessell studied Computer Science from an early age and has both an O-Level and an A-Level in the subject. She studied at Plymouth Polytechnic up to 1987 and obtained a Bachelors Honours Degree in Computing with Informatics.\n\nSubsequently, Chessell joined IBM in 1987 at Hursley Park, Winchester where she studied for a master's degree in software engineering at the University of Brighton (completed in 1997). Her studies at Brighton were sponsored by IBM.\n\n\n"}
{"id": "24616985", "url": "https://en.wikipedia.org/wiki?curid=24616985", "title": "Ministry of Environment and Energy (Greece)", "text": "Ministry of Environment and Energy (Greece)\n\nThe Ministry of Environment and Energy () is a government department of Greece responsible for industrial, environmental and energy policy. It replaced the narrower Ministry of the Environment, Energy and Climate Change, which was itself created on 7 October 2009 to succeed the Ministry for the Environment, Physical Planning and Public Works. The incumbent minister is Panos Skourletis. The Alternate Minister of Environment is Giannis Tsironis.\n\n"}
{"id": "43940392", "url": "https://en.wikipedia.org/wiki?curid=43940392", "title": "Nano flake", "text": "Nano flake\n\nIn a general meaning a nanoflake is a flake (that is, an uneven piece of material with one dimension substantially smaller than the other two) with at least one nanometric dimension (that is, between 1 and 100 nm). A flake is not necessarily perfectly flat but it is characterized by a plate-like form or structure. There are nanoflakes of all sorts of materials.\n\nIn a more restricted meaning, in the context of solar energy, Nano Flakes are a type of semiconductor that has potential for solar energy creation as the product itself is only in the prototype phase. With its crystalline structure the crystals are able to absorb light and harvest 30 percent of solar energy directed at its surface. These Nano Flakes can potentially also help out with economic and environmental problems associated with solar energy. Working on this Nano Flake Technology is Dr. Martin Aagesen at the Niels Bohr Institute at the University of Copenhagen with a PhD from the Nano Science Center. When Dr. Martin Aagesen discovered and published this new idea in 2007 there was much publicity about it and sparked many people to work on it. One major company that is working on these Nano structures is SunFlake which is a science company from the Nano Science Center with Dr. Martin Aagesen as Chief Executive Officer (CEO). The funding for the Nano flakes came from Danish Venture Capital fund SEED capital and University of Copenhagen.\n\nThe Nano flakes have a structure that contains tiny crystals in which millions of these crystals could fit into a single square centimeter. The tiny crystals absorb the sunlight and use the solar energy to convert it to electricity. This perfect crystalline structure is why this product can revolutionize solar energy. The large surface to volume ratio and the texture of the surface of this nano structure provides a larger absorption rate of the sun's light energy. Also researchers are working on trying to combine it with different semiconducting materials since the usual requirements of a need for a similar crystal structure for the carrier substrate is less stressed in the Nano flakes structure. The carrier substrate in the Nano flakes purpose is to permit growth of the nano structures and works as a contact for the Nano structures when they are actively absorbing the sun's energy.\n\nSolar energy obtained from the Nano flakes can help benefit in a couple of ways. Nano flakes can potentially help lower the cost of solar energy. Also since more solar energy can theoretically be obtained from Nano flakes, their use can potentially keep the earth's environment cleaner by reducing the need for fossil fuels.\n\nThe high cost of solar energy stems from the difficulty of converting the solar energy into electricity for use, and less than 1 percent of the world's electricity comes from the sun because of this process. Nano flakes can potentially help with the economic issues of solar energy by lowering the cost due to an easier process and a better outcome of energy. Nano flake technology can potentially make it easier to convert solar energy into electricity estimated at twice the amount that today's solar cells can harvest. This new technology can also potentially lower the cost of solar energy because it allows for a reduction in expensive semiconducting silicon. Energy loss is also potentially reduced with a shorter distance of the solar energy transportation across smaller Nano flakes.\n\nNano Flake technology can also help keep the environment cleaner as the sun as the source it produces clean pure sustainable energy that can be converted into electricity. While fossil fuel is the primary energy source for electricity, using solar energy obtained from Nano flakes will lower dependence on fossil fuels. When fossil fuels are burned for use they release a toxic gas which has a huge impact on earth's pollution. Also the process of obtaining these fossil fuels is not good for the environment, whether it be mining for coal, drilling for oil, or hydraulic fracturing of the earth's surface to reach the oil and gas.\n\n"}
{"id": "48064444", "url": "https://en.wikipedia.org/wiki?curid=48064444", "title": "Network documentation", "text": "Network documentation\n\nNetwork documentation is a form of technical documentation. It is the practice of maintaining records about networks of computers. The documentation is used to give administrators information about how the network should look, perform and where to troubleshoot problems as they occur.\n\nAs the purpose of network documentation is to keep networks running as smoothly as possible while minimizing downtime when repairs are necessary, essential parts of network documentation include:\n\n\nNotation that helps administrators remember key details are the basics of network documentation while visual representations assist in helping administrators understand how equipment and the notation relates to one another.\nA basic network diagram includes hardware and shows how it is connected. Basic diagrams include L1/L2 drawing of the physical connectivity and layout of the network.\n\nThough network documentation can be done by hand, for larger organizations network documentation software is utilized. Software applications can include diagrams, inventory management and circuit and cable traces. Examples include Graphical Networks' netTerrain, Microsoft Visio, Docusnap, Gliffy, Opnet's Netmapper, and CENTREL Solutions' XIA Configuration among others.\n"}
{"id": "49031124", "url": "https://en.wikipedia.org/wiki?curid=49031124", "title": "Nitrokey", "text": "Nitrokey\n\nNitrokey is an open source USB key to enable secure encryption and signing of data. The secret keys are always stored inside the Nitrokey which protects against malware (such as computer viruses) and attackers. A user-chosen PIN and a tamper-proof smart card protect the Nitrokey in case of loss and theft.The hardware and software of Nitrokey are available as open source, free software and open hardware which enables independent parties to verify the security of the device. Nitrokey is supported on Microsoft Windows, Linux, and macOS.\n\nIn 2008 Jan Suhr, Rudolf Böddeker and another friend were travelling and found themselves looking to use encrypted emails in internet cafés, which meant the secret keys had to remain secure against computer viruses. Some proprietary USB dongles existed at the time, but lacked in certain ways. Consequentially, they established as an open source project - Crypto Stick - in August 2008 which grew to become Nitrokey. It was a spare-time project of the founders to develop a hardware solution to enable the secure usage of email encryption. The first version of the Crypto Stick was released on 27 December 2009. In late 2014 the founders decided to professionalize the project, which was renamed Nitrokey. Nitrokey's firmware was audited by German cybersecurity firm Cure53 in May 2015, and its hardware was audited by the same company in August 2015. The first four Nitrokey models became available on 18 September 2015.\n\nSeveral Nitrokey models exist and the Nitrokey Pro is the flagship model. It contains the following features:\n\nThe upcoming Nitrokey Storage provides the same features as the Nitrokey Pro and additionally contains an encrypted mass storage.\n\n\n\nNitrokey's developers believes that proprietary systems can't provide strong security and that security systems need to be open source. For instance there have been cases in which NSA intercepts security devices being shipped and implanted backdoors into it. In 2011 RSA was hacked and secret keys of securID tokens been stolen which allowed hackers to circumvent their authentication. As revealed in 2010, many FIPS 140-2 Level 2 certified USB storage devices from various manufacturers could easily be cracked by using a default password. Nitrokey, because of being open source and because of its transparency, wants to provide high secure system and avoid security issues which its proprietary rivals were facing. Nitrokey's mission is to provide the best Open Source security key to protect the digital lives of its users.\n\n"}
{"id": "33928416", "url": "https://en.wikipedia.org/wiki?curid=33928416", "title": "Optophone", "text": "Optophone\n\nThe optophone is a device, used by the blind, that scans text and generates time-varying chords of tones to identify letters. It is one of the earliest known applications of sonification. Dr. Edmund Fournier d'Albe of Birmingham University invented the optophone in 1913, which used selenium photosensors to detect black print and convert it into an audible output which could be interpreted by a blind person. Barr and Stroud participated in improving the resolution and usability of the instrument.\n\nOnly a few units were built and reading was initially exceedingly slow; a demonstration at the 1918 Exhibition involved Mary Jameson reading at one word per minute. Later models of the Optophone allowed speeds of up to 60 words per minute, though only some subjects are able to achieve this rate.\n\nOptacon\n\n"}
{"id": "5799310", "url": "https://en.wikipedia.org/wiki?curid=5799310", "title": "Outdoor–indoor transmission class", "text": "Outdoor–indoor transmission class\n\nOutdoor–indoor transmission class (OITC) is a standard used for indicating the rate of transmission of sound between outdoor and indoor spaces in a structure. It is based on the ASTM E-1332 Standard Classification for the Determination of Outdoor–Indoor Transmission Class. An alternative similar standard for determining the rate of acoustic isolation of a separation between spaces is sound transmission class (STC).\nWhile STC is based on a noise spectrum targeting speech sounds, OITC utilizes a source noise spectrum that considers frequencies down to 80 Hz (aircraft/rail/truck traffic) and is weighted more to lower frequencies.\n\n"}
{"id": "18308626", "url": "https://en.wikipedia.org/wiki?curid=18308626", "title": "Radio science subsystem", "text": "Radio science subsystem\n\nA radio science subsystem (RSS) is a subsystem placed on board a spacecraft for radio science purposes. \n\nThe RSS uses radio signals to probe a medium such as a planetary atmosphere. The spacecraft transmits a highly stable signal to ground stations, receives such a signal from ground stations, or both. Since the transmitted signal parameters are accurately known to the receiver, any changes to these parameters are attributable to the propagation medium or to the relative motion of the spacecraft and ground station.\n\nThe RSS is usually not a separate instrument; its functions are usually \"piggybacked\" on the existing telecommunications subsystem. More advanced systems use multiple antennas with orthogonal polarizations.\n\nRadio science is commonly used to determine the gravity field of a moon or planet by observing Doppler shift. This requires a highly stable oscillator on the spacecraft, or more commonly a \"2-way coherent\" transponder that phase locks the transmitted signal frequency to a rational multiple of a received uplink signal that usually also carries spacecraft commands.\n\nAnother common radio science observation is performed as a spacecraft is occulted by a planetary body. As the spacecraft moves behind the planet, its radio signals cuts through successively deeper layers of the planetary atmosphere. Measurements of signal strength and polarization vs time can yield data on the composition and temperature of the atmosphere at different altitudes.\n\nIt is also common to use multiple radio frequencies coherently derived from a common source to measure the dispersion of the propagation medium. This is especially useful in determining the free electron content of a planetary ionosphere.\n\n\n\n"}
{"id": "344127", "url": "https://en.wikipedia.org/wiki?curid=344127", "title": "Signal trace", "text": "Signal trace\n\nIn electronics, a signal trace on a printed circuit board (PCB) is the equivalent of a wire for conducting signals. Each trace consists of a flat, narrow part of the copper foil that remains after etching. Signal traces are usually narrower than power or ground traces because the current carrying requirements are usually much less.\n\n"}
{"id": "47259185", "url": "https://en.wikipedia.org/wiki?curid=47259185", "title": "Soil-adjusted vegetation index", "text": "Soil-adjusted vegetation index\n\nEmpirically derived NDVI products have been shown to be unstable, varying with soil colour, soil moisture, and saturation effects from high density vegetation. In an attempt to improve NDVI, Huete developed a vegetation index that accounted for the differential red and near-infrared extinction through the vegetation canopy. The index is a transformation technique that minimizes soil brightness influences from spectral vegetation indices involving red and near-infrared (NIR) wavelengths.\n\nThe index is given as:\n\nwhere L is a canopy background adjustment factor. An L value of 0.5 in reflectance space was found to minimize soil brightness variations and eliminate the need for additional calibration for different soils.\nThe transformation was found to nearly eliminate soil-induced variations in vegetation indices.\n"}
{"id": "44560327", "url": "https://en.wikipedia.org/wiki?curid=44560327", "title": "SpotOption", "text": "SpotOption\n\nSpotOption was a privately held platform software provider based in Israel in the controversial binary option industry, which was banned in Israel starting in January 2018. The firm announced that it has left the binary options business and is exploring other possibilities. It had previously announced a downsizing of its operations in Israel and moving many functions to other locations. The firm claimed to have 70 percent share in the market for binary options platforms, and charged binary options firms up to 12.5% of their revenues.\n\nThe firm's office in Ramat Gan was raided by the FBI, accompanied by Israeli police, in January 2018. The FBI was searching for evidence in the case of Lee Elbaz. Elbaz, CEO of Yukom, a binary options broker, is accused of fraud. Pini Peter, owner of SpotOption said his company has left the binary options business and \"I’m considering closing the company down altogether.\"\n\nOn 30 January 2018 Facebook banned advertisements for binary options trading as well as well as for cryptocurrencies and initial coin offering (ICOs).\n\nSources disagree on the founding of the company. Bloomberg reports \"Spot Option\" at 7 Jabotinsky, Ramat Gan, Israel, was founded in 2011; a location consistent with information from Companies House in the UK which list a founding date of 2016.\n\nThe nonprofit public interest news organization \"Bureau of Investigative Journalism\" reports a unit in Israel appearing in 2010. They also report that the founder and primary shareholder is Pinchas Peterktzishvilly, also known as Pini Peter, who was convicted on money laundering charges in 2005. According to an advertisement for Banc De Binary, its CEO Oren Shabat Laurent was a founder of SpotOption. Oren himself was embroiled in significant controversy surrounding hiring experts to obfuscate the truth about Banc de Binary and SpotOption's affiliate funnel online; which effectively sees the latter create a range of affiliate funnel online products such as auto-traders, which in turn lure new depositors into placing funds at Spot Option's tied agents and brands. \n\nThe firm claims that two thirds of the binary option dealers around the world use SpotOption, and that the firm has $5 billion annual trading volume. In 2015, the company's former CEO, Ran Amiran, said \"SpotOption is the industry\". SpotOption and TechFinancials together supply trading technology to brokers with claimed $8 billion annual turnover. SpotOption supplied brokerages including the now defunct Banc De Binary, formerly SpotOption's biggest client, and smaller brokerages including itrader, BDSwiss, OptionRally, Webitrader, and FXDD. \n\nIn 2016, SpotOption claimed 300 brands or affiliates. among them are:\n\n\nAccording to the \"Bureau of Investigative Journalism\", \"PowerPoint presentations posted online by Hong Kong-based SpotOption sales manager Thomas Chang in 2016 and former Middle East sales manager Fakhri Husseini in 2013 told potential brokers that only 20% of people who invest in binary options ever get any money back after signing up.\" SpotOption exhibited at the IFX Expo International held in Cyprus in May 2016. Their director of marketing, Tammy Levy, when asked about crackdowns by law enforcement, was quoted as saying \n\"SpotOption is a technology company, okay? Everyone is responsible for checking regulation in the jurisdiction where they want to work. I am here to tell you what options you have technologically.\"\n\nAt the 2011 iGaming Super Show, the company showed its platform to traditional gaming vendors, claiming it was compatible with industry regulations and profitable for operators. Peterktzishvilly (Pini Peter) said they intended \"to bridge the forex and gaming industries\". The company appeared at numerous gambling industry trade shows including ICE Totally Gaming in London in 2014 and the American Gaming Association's Global Gaming Expo Asia 2015 in Macao, calling itself creator of \"the perfect financial game\".\n\nIn May 2015, SpotOption launched its new Spot5 platform that would include new instruments called \"digital contracts\". These were developed in order to meet new regulatory requirements. On 23 August 2017, the company announced their intention to launch a blockchain based trading platform for binary options called SpotChain, a new product which will raise money from the crowds to fund their operations.\n"}
{"id": "51771271", "url": "https://en.wikipedia.org/wiki?curid=51771271", "title": "Stanford Mobile Inquiry-based Learning Environment (SMILE)", "text": "Stanford Mobile Inquiry-based Learning Environment (SMILE)\n\nStanford Mobile Inquiry-based Learning Environment (SMILE) https://smile.stanford.edu is a mobile learning management server software designed to help students study school subject matter, develop higher order learning skills and generate transparent real-time learning analytics.\n\nSMILE combines a mobile-based question application for students with a management application for teachers. The technology allows students to create multiple-choice questions on mobile phones during class and share these questions with their classmates and teacher.\n\nThe main goal of SMILE is to develop students’ questioning skills, encourage greater student-centric activities and practices in classrooms, and enable a low-cost mobile wireless learning environment.\n\nSeeds of Empowerment (Seeds), is a global non-profit 501(c)(3) organization founded in 2009 by Dr. Paul Kim. The NGO helped develop SMILE and also pilot studies around the world using the software since 2009.\n\nA major contribution to the initial technical design of SMILE was originated from a research study led by Dr. Paul Kim along with his research assistants at Stanford University. The initial research study named PSILAN (PocketSchool Interactive Learning Network) investigated a portable ad-hoc network solution that can enable a multi-user interactive learning environment in areas where resources such as electricity or access to Internet is limited. This research was part of multiple projects affiliated with Stanford's major interdisciplinary research program named Programmable Open Mobile Internet backed by National Science Foundation.\n\nThe latest developments on SMILE have been made by GSE-IT at the Graduate School of Education of Stanford University and partnering organizations such as Edify.org. The license of the trademarks, software, hardware, and its technical design remains with the Office of Technology Licensing (OTL) at Stanford University.\n\nRecently, SMILE has been listed as one of innovative tools for the schools of tomorrow by United Nations's Education Commission, chaired by Gordon Brown, former Prime Minister of United Kingdom, in its 2016 report named \"Learning Generation Report\"\n\nThe software allows students to generate, share, and evaluate multimedia-rich questions. The data management software gathers these responses and the time students take to respond, and saves them for the teacher to analyze. Teachers can also enter questions to test information.\n\nFinally, the software also allows for both teamwork and competition which teachers can use to promote a classroom environment that is either collaborative or competitive, depending on what will motivate students.\n\nSMILE has five operating modes. The facilitator chooses the mode for each activity.\n\nThe instrument is designed to identify performance variations. It enables organizers to define five different levels of question quality. For example:\n\nIn lieu of test scores, the ratings of the questions can be used to assess learning outcomes. Analyzing a student's ability to rate other student's questions can be used to determine the student's level of critical thinking skills. The process of rating questions from their peers allows the students to think about the content in a deeper way.\n\nSMILE is intended for a wide range of educational settings. Not only is SMILE is content-agnostic, the pedagogical model behind SMILE is to encourage the student to make critical inquiries--the hard work, the investigation--instead of being told the answers by an educator. Many successful implementations of SMILE are implemented by teachers incorporating SMILE two to three times a week. To address the rapidly advancing technological age, SMILE effectively allows schools and teachers to flip the classroom.\n\nSMILE can be used by students in an elementary classroom. Students create questions which are then solved and rated by peers. The entire process is controlled and monitored by a teacher with an activity management application. \n\nIn the learning process, students proceed through stages of Make Your Question, where students create multiple choice questions. Solve Questions aggregates the questions and sends them back to students to solve and rate on a five-point scale. See Results allows students to see the their score. \n\nThe teacher has multiple features at his or her disposal. The Activity Flow window allows the teacher to activate the various stages of the activity. The Student Status window displays the current status of each student. The Scoreboard displays individual student’s responses. The Question Status window displays metadata about the question. The Question window displays the question itself and its predetermined correct answer. The Top Scorers window displays which student achieved the highest score and which question received the highest ratings. The Save Questions button allows the teacher to save the data from a given exercise to the server.\n\nSMILE takes advantage of mobile devices to increase participation, engagement, and collaboration from the students. Students are creating their own math questions, including using the device's camera to take pictures of their questions. All participants have access to the questions during and after class. \n\nIn a typical SMILE activity, students go through several phases of learning. They are: Introduction and device exploration, Prompt for problems, Student grouping and generating questions, Question generation, Question solving, Result review, Reflection, and Repetition and enrichment. Song, Kim, and Karimi describe this in their 2012 paper, Inquiry-based Learning Environment Using Mobile Devices in Math Classroom. \n\nSMILE can be used in participatory action research to engage participants in critical thinking and community engagement while eventually deriving solutions that are locally relevant. \n\nDuring the question generation process, participants can capture images from textbooks or take photos of environmental issues to address. Once the questions are generated, they are aggregated and redistributed to the participants. The participants then go through a period of reflection, response, evaluation, and verification. \n\nThe process enables the participants to think beyond simple health-relevant facts. Also, SMILE leverages the collective construction of inquiries, leading to more reliable and accurate depiction of the reality as a whole. They are able to distinguish facts from opinions, verify sources, analyze cause and effect, determine faulty generalizations, and avoid oversimplification. \n\nBy empowering the participants to co-evaluate and reflect their own phenomena, the external researcher's role is simply to facilitate conversation. Examining the questions and answer choices generated by participants help researchers uncover important insights. \n\nSMILE Plug creates an ad-hoc network which enables students to engage in SMILE activities and exchange inquiries with peers in their classrooms or their own school. Participants in the SMILE Plug model must be physically present and connected to the ad-hoc SMILE WiFi network. A SMILE Plug router contains the SMILE server software, KIWIX, Khan Academy Lite, various open education resources including open education textbooks, and four different coding language school programs.\n\nSMILE Global enables students around the world to exchange their inquiries regardless of their location. People who are interested in a particular topic (e.g., for example, 'health'), they can search the keyword and also create their own questions, respond to existing questions, or comment on questions and answer. The SMILE Global server is accessible in the cloud.\n\nBoth SMILE Plug and SMILE Global allow students to incorporate multimedia components in their questions: SMILE Plug uses images, and SMILE Global uses images, audio and video.\n\nThe cost of implementing a SMILE activity depends on the infrastructure available in the school, but at minimum costs: $80 per mobile phone (one for every 2-3 students), $300 for a notebook laptop computer, and $100 for a local router.\n\nBecause of the ubiquitous nature of mobile devices, the low barrier to entry, and the modular growth potential, mobile devices provide a low-cost, high-reward alternative to the traditional computer lab model.\n\nIn many cases, business leaders provided local telecom network infrastructure and equipment such as mobile devices and computers, while local educational administrators facilitated the participation of local schools and offered teacher workshops. University staff conducted research on strategies to enhance the model within the local context and NGO partners provided localized knowledge, program oversight, and project coordination. This approach brings together many stakeholders, without exhausting the limited resources of any one sector.\n\nIn 2012, the SMILE team partnered with Marvell to create SMILE Consortium.\n\nIn 2016, the SMILE Plug was implemented on a Raspberry Pi 3. SMILE will boot in one minute when plugged into USB power. In developing countries with limited access to electricity, a USB battery pack is required. The Pi, designed for use in areas of low internet connectivity, provides a local WiFi access point. Students and teachers with mobile devices, tablets, laptops, and computers can connect to the Plug.\n\nThe Plug requires a microSD card which acts as the hard drive and local repository of the offline resources. In order to update the SMILE Plug, one will swap out the previous microSD card with a newer microSD card with updated resources.\n\nThe total cost of a SMILE Plug varies depending on the material parts used to build the device. The base price is $85, including $35 for the Raspberry Pi motherboard; $10 for a case; $40 for a 128GB Class 10 microSD card. A battery pack for 12 hours of use costs an additional $25; the LED screen an additional $40.\n\nIn 2017, SMILE Global will interface with a natural language processing API. The SMILE team has prepared a databank of questions pre-categorized according to the question quality rubric. The API will return a rating for each question that is submitted on SMILE Global. The immediate response and feedback will give students a chance to make improvements to their questions in real time.\n\nThe question quality rubric will allow teachers to gauge the level of learning from their students. \n\nSMILE has reached over 25 countries, including North America, India, Argentina, Mexico, Costa Rica, Colombia, Nepal, China, Uruguay, Indonesia, South Korea, South Africa, Sri Lanka, Pakistan, and Tanzania.\n\nStudents used SMILE to think critically about what it means to be an engaged citizen in their community, generating questions for their peers about such potential moral dilemmas as homelessness, suicide, stealing, and school bullying and violence. One student addressed the increasing incidence of suicide locally and asked their peers for the major cause. This revealed additional benefits of SMILE—not only is it a tool for managing student learning and assessment, it also facilitates discussions of issues that students see as important.\n\nIn 2012, the Ministry of Education in Buenos Aires looked into modifying the cell phone prohibition use in the classroom, in effect since 2006. In addition to using SMILE, educators can now create executable programs on mobile devices to help facilitate learning in the classroom. \n\nSMILE workshops on Music, Language Arts, and Mathematics was implemented in Misiones and Talarin August 2011. By using an exploratory learning pedagogy, students were able to compose songs. The power of mobile devices to reach the last mile and the last school is most evident where electricity and internet access is not guaranteed. \n\nIn rural settings, desktop computers may be too cumbersome and have too high of an overhead. Mobile phones and tablets, on the other hand, are portable and battery-powered, making for a flexible mobile-learning environment. One of the keys in deploying mobile technology is a focus on bringing the desired content with a strong pedagogical foundation. \n\nDue to the centralized nature of Chile, it is harder for education to reach the poorest areas of the country. The concept of a mobile classroom, or \"pocket school,\" connected and tied together by a network of mobile phones, is an attempt to take advantage of the resources already available in the most underserved communities. \n\nStudents were asked to generate math questions covering a wide range of topics, from triangle angle sum theorem, to fractions, areas and diameters. Teachers were surprised at the students’ enthusiasm. They were also surprised at the students’ ability to adopt the technology by themselves and to train each other and even their teachers.\n\nSMILE Global was tested with medical students at Chungbuk National University. Criteria for high-quality questions, criteria rubrics, and examples of high- and low-quality questions were discussed with students first. This initial overview seemed to be important in promoting deep inquiry. As the students were already very experienced in using technology, they spent 60% of their time on the inquiry-making task.\n\nStudies suggest that SMILE could be implemented relatively easily in a wide range of classroom settings; it was adopted by students relatively quickly; and it increased the use of inquiry-based pedagogies.\n\nAs students evaluated each other’s questions and understood which ones got higher ratings, over time they developed questions that were more conceptually difficult and of higher quality.\n\nTeachers need an initial training period and some follow-up mentoring so they can facilitate questions. Tailoring the content of the trainings to the local environment is crucial. Without putting the benefits of SMILE into the local context, teachers and students will find no compelling reason to adopt the pedagogy. \n\nFinally, local education officials must be on board. SMILE worked best when officials, along with civil society organizations, universities, and local businesses, worked together to bring it to classrooms and supported different elements of its implementation.\n\nThe success rate of implementing SMILE is dependent on how cohesively inquiry-based pedagogy is tied to the curricula taught at a school. While SMILE can be implemented with the existing curricula (for example, with students asking simple recall math questions), it is most effective as an additional platform to foster critical thinking. This could come in the form of a regularly scheduled SMILE class each week to discuss questioning. Although many students shy away at first, repeated practice learning how to answer, rate, and create questions within a comfortable space will allow them to grow their higher-order thinking skills.\n\nHigher teacher motivation, better classroom integration, and higher frequency of use are three factors that increase SMILE retention. Teachers are encouraged to help students become knowledge-holders and questioners; teachers themselves are encouraged to challenge the status quo of rote memorization inside the classroom. Though question-and-answer sessions may create uncomfortable situations where the teachers sole authority is questioned, the additional transfer of knowledge and learning happens in this discomfort and is beneficial to the student's growth.\n\nFor online classrooms, SMILE helps drive student engagement with the content, the community, and the instructor. A study on pre-service teachers meeting remotely showed positive correlation in using SMILE and achieving higher levels of collaboration, higher sense of autonomy, and optimizing content relevance and authenticity.\n\nRelationships between students and teachers changed during the SMILE workshops. Teachers were not simply transmitting information to students; rather, students were drawing on written or digital resources to formulate their own questions. The teachers played important roles in guiding students through the solutions to difficult questions, correcting any mistakes, and elaborating on student-generated questions.\n\nSchool and country contexts highly influence students’ initial abilities to form deep questions. In the developing world, many teachers lack adequate teacher training. Their training is often limited to content, rather than pedagogical practices. In some cases, when teachers were asked to use new technology before they were comfortable with it, their discomfort led them to stifle students’ questions. Cultural norms governing relations between adults and students, and socialization into situational authority roles, may also inhibit student questioning.\n\nSMILE was more difficult to implement in areas where rote memorization pedagogies were typical. Some students found it hard to generate their own questions, given their previous classroom experiences with rote memorization activities.\n\nAdditionally, students with little experience manipulating smart phones took longer to understand and use the technology, but eventually adjusted after exploration.\n"}
{"id": "21281856", "url": "https://en.wikipedia.org/wiki?curid=21281856", "title": "Swedish Bar Systems", "text": "Swedish Bar Systems\n\nSwedish Bar Systems is a Stockholm based supplier and distributor of a bar inventory management system specializing in liquor logistics in the hospitality industry. The system comprises hardware, software and services which is used to improve business strategies and processes. The information collected is used for research and business development and also synchronized with the establishments accounting and inventory processes to increase time management and revenue. Swedish Bar Systems currently has installations throughout Scandinavia.\n\nFounded in 2005 in Stockholm, Sweden, the company released the first bar inventory management system to use free pour spouts, able to measure the volume of liquor poured and wirelessly transfer the encoded information to a database. The pourers contain a flow sensor that measure the volume dispensed in real time, which provides an overview of the establishment inventory.\n\nSwedish Bar Systems manufactures and sells hardware, software and services for the liquor market. The systems hardware and software is used in the hospitality industry, which includes touch screen computers for showing and monitoring logistics. The spout measures free poured volumes and sends this information wirelessly to a receiver. The receiver collects all signals from the spouts and enters it into the database that handles the inventory.\n\nSwedish Bar Systems provides numerous solutions for different establishments sizes, including single-entity, chains, franchises or multiple locations, including casinos, stadiums, and cruise ships. Swedish Bar Systems have the ability to interface with other POS systems such as Squirrel Systems and MICROS Systems. To connect bars or locations into one database a local server is set up, so the information can be shown on software modules.\n\nSwedish Bar Systems received several million SEK investments in 2006, 2007 and later 2008 they were purchased by the listed company DQE Designed Quality Entertainment on the Nordic MTF list, a subsidiary to Börse Stuttgart.\n\nThe company has been recognized by the Financial Times,\nand awarded several national and international awards for their concept, among others McKinsey & Company - Venture Cup, London school of Economics, Royal Melbourne Institute of Technology \nand Stockholm School of Entrepreneurship (SSES) \ninnovation award.\n\n"}
{"id": "36266318", "url": "https://en.wikipedia.org/wiki?curid=36266318", "title": "Teresa H. Meng", "text": "Teresa H. Meng\n\nTeresa H. Meng (; born 1961) is a Professor of Electrical Engineering at Stanford University and founder of Atheros Communications.\n\nMeng, born and raised in Taiwan, graduated from the National Taiwan University with a bachelor's degree. She received her Ph.D. in Electrical Engineering and Computer Science at the University of California, Berkeley in 1988.\n\nShe joined the Stanford faculty in 1988.\n\nIn 1999, Meng took leave from her post at Stanford University to found Atheros Communications, a company which focuses on technology for wireless communications. Meng was named one of the top 10 entrepreneurs by Red Herring in 2001.\n\nMeng returned to Stanford in 2000.\n"}
{"id": "4097515", "url": "https://en.wikipedia.org/wiki?curid=4097515", "title": "Thomas P. Hughes", "text": "Thomas P. Hughes\n\nThomas Parke Hughes (September 13, 1923 – February 3, 2014) was an American historian of technology. He was an emeritus professor of history at the University of Pennsylvania and a visiting professor at MIT and Stanford.\n\nHe received his Ph.D. from the University of Virginia in 1953.\n\nHe, along with John B. Rae, Carl W. Condit, and Melvin Kranzberg, were responsible for the establishment of the Society for the History of Technology and he was a recipient of its highest honor, the Leonardo da Vinci Medal in 1985.\n\nHe contributed to the concepts of technological momentum, technological determinism, large technical systems, social construction of technology, and introduced systems theory into the history of technology.\n\n\n"}
{"id": "4469365", "url": "https://en.wikipedia.org/wiki?curid=4469365", "title": "Transistor count", "text": "Transistor count\n\nThe transistor count is the number of transistors on an integrated circuit (IC). Transistor count is the most common measure of IC complexity, although there are caveats. For instance, the majority of transistors are contained in the cache memories in modern microprocessors, which consist mostly of the same memory cell circuits replicated many times. The rate at which transistor counts have increased generally follows Moore's law, which observed that the transistor count doubles approximately every two years. , the largest transistor count in a commercially available single-chip processor is 19.2 billion— AMD's Ryzen-based Epyc. In other types of ICs, such as field-programmable gate arrays (FPGAs), Intel's (previously Altera) Stratix 10 has the largest transistor count, containing over 30 billion transistors.\nA microprocessor incorporates the functions of a computer's central processing unit on a single integrated circuit. It is a multipurpose, programmable device that accepts digital data as input, processes it according to instructions stored in its memory, and provides results as output.\n\nThe \"second generation\" of computers (transistor computers) featured boards filled with discrete transistors and magnetic memory cores.\n\nA graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the building of images in a frame buffer intended for output to a display.\n\nA field-programmable gate array (FPGA) is an integrated circuit designed to be configured by a customer or a designer after manufacturing.\n\nTransistor count for generic logic functions is based on static CMOS implementation.\n\nSemiconductor memory is an electronic data storage device, often used as computer memory, implemented on integrated circuit.\n\nWe know that in order to store a single bit (which may be 1 or 0), one flip-flop is required, made of around eight transistors. Typical CMOS Static random-access memory (SRAM) consists of 6 transistors. For Dynamic random-access memory (DRAM), 1T1C, which means one transistor and one capacitor structure is common. Capacitor charged or not is used to store 1 or 0. For flash memory, the data is stored in floating gate, and the resistance of the transistor is sensed to interpret the data stored. Depending on how fine scale the resistance could be separated, one transistor could store up to 3-bits, meaning eight distinctive level of resistance possible per transistor. However, the fine the scale comes with cost of repeatability therefore reliability. Typically, low grade 2-bits MLC flash is used for flash drive, so a 16 GB flash drive contains roughly 64 billion transistors.\n\nHistorically, each processing element in earlier parallel systems—like all CPUs of that time—was a serial computer built out of multiple chips. As transistor counts per chip increases, each processing element could be built out of fewer chips, and then later each multi-core processor chip could contain more processing elements.\n\nGoodyear MPP :\n8 pixel processors per chip, 3,000 to 8,000 transistors per chip.\n\nBrunel University Scape (single-chip array-processing element):\n256 pixel processors per chip, 120,000 to 140,000 transistors per chip.\n\nCell Broadband Engine:\n9 cores per chip, 234 million transistors per chip.\n\n\n"}
{"id": "18674875", "url": "https://en.wikipedia.org/wiki?curid=18674875", "title": "Turbine blade", "text": "Turbine blade\n\nA turbine blade is the individual component which makes up the turbine section of a gas turbine or steam turbine. The blades are responsible for extracting energy from the high temperature, high pressure gas produced by the combustor. The turbine blades are often the limiting component of gas turbines. To survive in this difficult environment, turbine blades often use exotic materials like superalloys and many different methods of cooling, such as internal air channels, boundary layer cooling, and thermal barrier coatings. Blade fatigue is a major source of failure in steam turbines and gas turbines. Fatigue is caused by the stress induced by vibration and resonance within the operating range of machinery. To protect blades from these high dynamic stresses, friction dampers are used.\n\nBlades of wind turbines and water turbines are designed to operate in different conditions, which typically involve lower rotational speeds and temperatures.\n\nIn a gas turbine engine, a single turbine section is made up of a disk or hub that holds many turbine blades. That turbine section is connected to a compressor section via a shaft (or \"spool\"), and that compressor section can either be axial or centrifugal. Air is compressed, raising the pressure and temperature, through the compressor stages of the engine. The temperature is then greatly increased by combustion of fuel inside the combustor, which sits between the compressor stages and the turbine stages. The high-temperature and high-pressure exhaust gases then pass through the turbine stages. The turbine stages extract energy from this flow, lowering the pressure and temperature of the air and transfer the kinetic energy to the compressor stages along the spool. This process is very similar to how an axial compressor works, only in reverse.\n\nThe number of turbine stages varies in different types of engines, with high-bypass-ratio engines tending to have the most turbine stages. The number of turbine stages can have a great effect on how the turbine blades are designed for each stage. Many gas turbine engines are twin-spool designs, meaning that there is a high-pressure spool and a low-pressure spool. Other gas turbines use three spools, adding an intermediate-pressure spool between the high- and low-pressure spool. The high-pressure turbine is exposed to the hottest, highest-pressure air, and the low-pressure turbine is subjected to cooler, lower-pressure air. The difference in conditions leads to the design of high-pressure and low-pressure turbine blades that are significantly different in material and cooling choices even though the aerodynamic and thermodynamic principles are the same.\nUnder these severe operating conditions inside the gas and steam turbines, the blades face high temperature, high stresses, and potentially high vibrations. Steam turbine blades are critical components in power plants which convert the linear motion of high-temperature and high-pressure steam flowing down a pressure gradient into a rotary motion of the turbine shaft.\n\nTurbine blades are subjected to very strenuous environments inside a gas turbine. They face high temperatures, high stresses, and a potential environment of high vibration. All three of these factors can lead to blade failures, potentially destroying the engine, therefore turbine blades are carefully designed to resist these conditions.\n\nTurbine blades are subjected to stress from centrifugal force (turbine stages can rotate at tens of thousands of revolutions per minute (RPM)) and fluid forces that can cause fracture, yielding, or creep failures. Additionally, the first stage (the stage directly following the combustor) of a modern turbine faces temperatures around , up from temperatures around in early gas turbines. Modern military jet engines, like the Snecma M88, can see turbine temperatures of . Those high temperatures weaken the blades and make them more susceptible to creep failures. The high temperatures can also make the blades susceptible to corrosion failures. Finally, vibrations from the engine and the turbine itself (see blade pass frequency) can cause fatigue failures.\n\nA key limiting factor in early jet engines was the performance of the materials available for the hot section (combustor and turbine) of the engine. The need for better materials spurred much research in the field of alloys and manufacturing techniques, and that research resulted in a long list of new materials and methods that make modern gas turbines possible. One of the earliest of these was Nimonic, used in the British Whittle engines.\n\nThe development of superalloys in the 1940s and new processing methods such as vacuum induction melting in the 1950s greatly increased the temperature capability of turbine blades. Further processing methods like hot isostatic pressing improved the alloys used for turbine blades and increased turbine blade performance. Modern turbine blades often use nickel-based superalloys that incorporate chromium, cobalt, and rhenium.\n\nAside from alloy improvements, a major breakthrough was the development of directional solidification (DS) and single crystal (SC) production methods. These methods help greatly increase strength against fatigue and creep by aligning grain boundaries in one direction (DS) or by eliminating grain boundaries altogether (SC). SC research began in the 1960s with Pratt and Whitney and took about 10 years to be implemented. One of the first implementations of DS was with the J58 engines of the SR-71.\nAnother major improvement to turbine blade material technology was the development of thermal barrier coatings (TBC). Where DS and SC developments improved creep and fatigue resistance, TBCs improved corrosion and oxidation resistance, both of which became greater concerns as temperatures increased. The first TBCs, applied in the 1970s, were aluminide coatings. Improved ceramic coatings became available in the 1980s. These coatings increased turbine blade temperature capability by about 200 °F (90 °C). The coatings also improve blade life, almost doubling the life of turbine blades in some cases.\n\nMost turbine blades are manufactured by investment casting (or lost-wax processing). This process involves making a precise negative die of the blade shape that is filled with wax to form the blade shape. If the blade is hollow (i.e., it has internal cooling passages), a ceramic core in the shape of the passage is inserted into the middle. The wax blade is coated with a heat-resistant material to make a shell, and then that shell is filled with the blade alloy. This step can be more complicated for DS or SC materials, but the process is similar. If there is a ceramic core in the middle of the blade, it is dissolved in a solution that leaves the blade hollow. The blades are coated with a TBC, and then any cooling holes are machined.\n\nCeramic matrix composites (CMC), where fibers are embedded in a ceramic matrix, are being developed for use in turbine blades. The main advantage of CMCs over conventional superalloys is their light weight and high temperature capability. SiC/SiC composites consisting of silicon matrix reinforced by silicon carbide fibers have been shown to withstand operating temperatures 200°-300 °F higher than nickel superalloys. GE Aviation successfully demonstrated the use of such SiC/SiC composite blades for the low-pressure turbine of its F414 jet engine.\n\nNote: This list is not inclusive of all alloys used in turbine blades.\n\nAt a constant pressure ratio, thermal efficiency increases as the maximum temperature increases. But, high temperatures can damage the turbine, as the blades are under large centrifugal stresses and materials are weaker at high temperature. So, turbine blade cooling is essential.\n\nCooling of components can be achieved by air or liquid cooling. Liquid cooling seems to be more attractive because of high specific heat capacity and chances of evaporative cooling but there can be problem of leakage, corrosion, choking, etc. which works against this method. On the other hand, air cooling allows the discharged air into main flow without any problem. Quantity of air required for this purpose is 1–3% of main flow and blade temperature can be reduced by 200–300 °C. \nThere are many techniques of cooling used in gas turbine blades; convection, film, transpiration cooling, cooling effusion, pin fin cooling etc. which fall under the categories of internal and external cooling. While all methods have their differences, they all work by using cooler air (often bled from the compressor) to remove heat from the turbine blades.\n\nIt works by passing cooling air through passages internal to the blade. Heat is transferred by conduction through the blade, and then by convection into the air flowing inside of the blade. A large internal surface area is desirable for this method, so the cooling paths tend to be serpentine and full of small fins. The internal passages in the blade may be circular or elliptical in shape. Cooling is achieved by passing the air through these passages from hub towards the blade tip. This cooling air comes from an air compressor. In case of gas turbine the fluid outside is relatively hot which passes through the cooling passage and mixes with the main stream at the blade tip.\n\nA variation of convection cooling, impingement cooling, works by hitting the inner surface of the blade with high velocity air. This allows more heat to be transferred by convection than regular convection cooling does. Impingement cooling is used in the regions of greatest heat loads. In case of turbine blades, the leading edge has maximum temperature and thus heat load. Impingement cooling is also used in mid chord of the vane. Blades are hollow with a core. There are internal cooling passages. Cooling air enters from the leading edge region and turns towards the trailing edge.\n\nFilm cooling (also called \"thin\" film cooling), a widely used type, allows for higher heat transfer rates than either convection and impingement cooling. This technique consists of pumping the cooling air out of the blade through multiple small holes in the structure. A thin layer (the film) of cooling air is then created on the external surface of the blade, reducing the heat transfer from main flow, whose temperature (1300–1800 kelvins) can exceed the melting point of the blade material (1300–1400 kelvins). The air holes can be in many different blade locations, but they are most often along the leading edge. A United States Air Force program in the early 1970s funded the development of a turbine blade that was both film and convection cooled, and that method has become common in modern turbine blades.\nInjecting the cooler bleed into the flow reduces turbine isentropic efficiency; the compression of the cooling air (which does not contribute power to the engine) incurs an energetic penalty; and the cooling circuit adds considerable complexity to the engine. All of these factors have to be compensated by the increase in overall performance (power and efficiency) allowed by the increase in turbine temperature.\n\nThe blade surface is made of porous material which means having a large number of small orifices on the surface. Cooling air is forced through these porous holes which forms a film or cooler boundary layer. Besides this uniform cooling is caused by effusion of the coolant over the entire blade surface.\n\nIn the narrow trailing edge film cooling is used to enhance heat transfer from the blade. There is an array of pin fins on the blade surface. Heat transfer takes place from this array and through the side walls. As the coolant flows across the fins with high velocity, the flow separates and wakes are formed. Many factors contribute towards heat transfer rate among which the type of pin fin and the spacing between fins are the most significant.\n\nThis is similar to film cooling in that it creates a thin film of cooling air on the blade, but it is different in that air is \"leaked\" through a porous shell rather than injected through holes. This type of cooling is effective at high temperatures as it uniformly covers the entire blade with cool air. Transpiration-cooled blades generally consist of a rigid strut with a porous shell. Air flows through internal channels of the strut and then passes through the porous shell to cool the blade. As with film cooling, increased cooling air decreases turbine efficiency, therefore that decrease has to be balanced with improved temperature performance.\n\n\n\n"}
{"id": "9022198", "url": "https://en.wikipedia.org/wiki?curid=9022198", "title": "Turtle (robot)", "text": "Turtle (robot)\n\nTurtles are a class of educational robots designed originally in the late 1940s (largely under the auspices of researcher William Grey Walter) and used in computer science and mechanical engineering training. These devices are traditionally built low to the ground with a roughly hemispheric (sometimes transparent) shell and a power train capable of a very small turning radius. The robots are often equipped with sensor devices which aid in avoiding obstacles and, if the robot is sufficiently sophisticated, allow it some perception of its environment. Turtle robots are commercially available and are common projects for robotics hobbyists. \n\nTurtle robots are closely associated with the work of Seymour Papert and the common use of the Logo programming language in computer education of the 1980s. Turtles specifically designed for use with Logo systems often come with pen mechanisms allowing the programmer to create a design on a large sheet of paper. The original Logo turtle, built by Paul Wexelblat at BBN, was named \"Irving\" and was demonstrated at the former Muzzey Junior High in Lexington, Massachusetts. \"Irving\" contained bump sensors and could give audio feedback with a bell. The development of the robotic Logo turtle led to the use of the term to describe the cursor in video screen implementations of the language and its turtle graphics package.\n\n\n"}
{"id": "9300583", "url": "https://en.wikipedia.org/wiki?curid=9300583", "title": "Wax thermostatic element", "text": "Wax thermostatic element\n\nThe wax thermostatic element was invented in 1934 by Sergius Vernet (1899–1968). Its principal application is in automotive thermostats used in the engine cooling system. The first applications in the plumbing and heating industries were in Sweden (1970) and in Switzerland (1971).\n\nWax thermostatic elements transform heat energy into mechanical energy using the thermal expansion of waxes when they melt. This wax motor principle also finds applications besides engine cooling systems, including heating system thermostatic radiator valves, plumbing, industrial, and agriculture.\n\nThe internal combustion engine cooling thermostat maintains the temperature of the engine near its optimum operating temperature by regulating the flow of coolant to an air cooled radiator. This regulation is now carried out by an internal thermostat. Conveniently, both the sensing element of the thermostat and its control valve may be placed at the same location, allowing the use of a simple self-contained non-powered thermostat as the primary device for the precise control of engine temperature. Although most vehicles now have a temperature-controlled electric cooling fan, \"the unassisted air stream can provide sufficient cooling up to 95% of the time\" and so such a fan is not the mechanism for primary control of the internal temperature.\n\nResearch in the 1920s showed that cylinder wear was aggravated by condensation of fuel when it contacted a cool cylinder wall which removed the oil film. The development of the automatic thermostat in the 1930s solved this problem by ensuring fast engine warm-up.\n\nThe first thermostats used a sealed capsule of an organic liquid with a boiling point just below the desired opening temperature. These capsules were made in the form of a cylindrical bellows. As the liquid boiled inside the capsule, the capsule bellows expanded, opening a sheet brass plug valve within the thermostat. As these thermostats could fail in service, they were designed for easy replacement during servicing, usually by being mounted under the water outlet fitting at the top of the cylinder block. Conveniently this was also the hottest accessible part of the cooling circuit, giving a fast response when warming up.\n\nCooling circuits have a small bypass path even when the thermostat is closed, usually by a small hole in the thermostat. This allows enough flow of cooling water to heat the thermostat when warming up. It also provided an escape route for trapped air when first filling the system. A larger bypass is often provided, through the cylinder block and water pump, so as to keep the rising temperature distribution even.\n\nWork on cooling high-performance aircraft engines in the 1930s led to the adoption of pressurised cooling systems, which became common on cars post-war. As the boiling point of water increases with increasing pressure, these pressurised systems could run at a higher temperature without boiling. This increased both the working temperature of the engine, thus its efficiency, and also the heat capacity of the coolant by volume, allowing smaller cooling systems that required less pump power. A drawback to the bellows thermostat was that it was also sensitive to pressure changes, thus could sometimes be forced shut again by pressure, leading to overheating. The later wax pellet type has a negligible change in its external volume, thus is insensitive to pressure changes. It is otherwise identical in operation to the earlier type. Many cars of the 1950s, or earlier, that were originally built with bellows thermostats were later serviced with replacement wax capsule thermostats, without requiring any change or adaption.\n\nThis most common modern form of thermostat now uses a wax pellet inside a sealed chamber. Rather than a liquid-vapour transition, these use a solid-liquid transition, which for waxes is accompanied by a large increase in volume. The wax is solid at low temperatures, and as the engine heats up, the wax melts and expands. The sealed chamber operates a rod which opens a valve when the operating temperature is exceeded. The operating temperature is fixed, but is determined by the specific composition of the wax, so thermostats of this type are available to maintain different temperatures, typically in the range of 70 to 90°C (160 to 200°F). Modern engines run hot, that is, over 80 °C (180 °F), in order to run more efficiently and to reduce the emission of pollutants.\n\nWhile the thermostat is closed, there is no flow of coolant in the radiator loop, and coolant water is instead redirected through the engine, allowing it to warm up rapidly while also avoiding hot spots. The thermostat stays closed until the coolant temperature reaches the nominal thermostat opening temperature. The thermostat then progressively opens as the coolant temperature increases to the optimum operating temperature, increasing the coolant flow to the radiator. Once the optimum operating temperature is reached, the thermostat progressively increases or decreases its opening in response to temperature changes, dynamically balancing the coolant recirculation flow and coolant flow to the radiator to maintain the engine temperature in the optimum range as engine heat output, vehicle speed, and outside ambient temperature change. Under normal operating conditions the thermostat is open to about half of its stroke travel, so that it can open further or reduce its opening to react to changes in operating conditions. A correctly designed thermostat will never be fully open or fully closed while the engine is operating normally, or overheating or overcooling would occur.\nEngines which require a tighter control of temperature, as they are sensitive to \"Thermal shock\" caused by surges of coolant, may use a \"constant inlet temperature\" system. In this arrangement the inlet cooling to the engine is controlled by double-valve thermostat which mixes a re-circulating sensing flow with the radiator cooling flow. These employ a single capsule, but have two valve discs. Thus a very compact, and simple but effective, control function is achieved.\n\nThe wax used within the thermostat is specially manufactured for the purpose. Unlike a standard paraffin wax, which has a relatively wide range of carbon chain lengths, a wax used in the thermostat application has a very narrow range of carbon molecule chains. The extent of the chains is usually determined by the melting characteristics demanded by the specific end application. To manufacture a product in this manner requires very precise levels of distillation.\n\nThe temperature sensing material contained in the cup transfers pressure to the piston by means of the diaphragm and the plug, held tightly in position by the guide. On cooling, the initial position of the piston is obtained by means of a return spring.\nFlat diaphragm elements are particularly noted for their high level of accuracy, and therefore mainly used in sanitary installations and heating.\n\nSqueeze-Push elements contain a synthetic rubber sleeve-like component shaped like the 'finger of a glove' which surrounds the piston. As the temperature increases, pressure from the expansion of the thermostatic material moves the piston with a lateral squeeze and a vertical push. As with the flat diaphragm element, the piston returns to its initial position by means of a return spring. These elements are slightly less accurate but provide a longer stroke.\n\nThe stroke is the movement of the piston in relation to its starting point. The ideal stroke corresponds to the temperature range of the elements. According to the type of element, it can vary from 1.5 mm to 16 mm.\n\nThe temperature range lies between the minimum and maximum operating temperature of the element. Elements can cover temperatures ranging from -15 °C to +120 °C. Elements may move in proportion to the temperature change over some part of the range, or may open suddenly around a particular temperature depending on the composition of the waxes.\n\nHysteresis is the difference noted between the upstroke and down stroke curve on heating and cooling of the element. Hysteresis is caused by the thermal inertia of the element and by the friction between the parts in motion.\n\n\n"}
{"id": "21547842", "url": "https://en.wikipedia.org/wiki?curid=21547842", "title": "Zimbabwe Institution of Engineers", "text": "Zimbabwe Institution of Engineers\n\nThe Zimbabwe Institution of Engineers is the professional organization of engineers in Zimbabwe. It has graded membership, including student, technician, graduate and corporate membership as well as the status of fellow. \n\n"}
{"id": "57930428", "url": "https://en.wikipedia.org/wiki?curid=57930428", "title": "Đorđe Genčić", "text": "Đorđe Genčić\n\nĐorđe Genčić (1861 – 1938) was a Yugoslav industrialist and politician. He served as Mayor of Niš and was a leading figure in the May Coup.\n"}
