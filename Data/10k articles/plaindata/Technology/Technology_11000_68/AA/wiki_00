{"id": "38625337", "url": "https://en.wikipedia.org/wiki?curid=38625337", "title": "Abel Bliss Professorship", "text": "Abel Bliss Professorship\n\nAbel Bliss, Jr. was born on August 8, 1853. His father, Abel Bliss, Sr., was a native of Wilbraham, Mass, and was born on February 9, 1810. His mother was Lucinda Blake, a native of the Bay State and born in Springfield, October 14, 1816. He was the sixth of the seven children born into his family, and attended high school at Englewood, and afterward was a student of engineering at University of Illinois at Champaign. He was married, on February 21, 1877, to Nettie Lynk. He gave up engineering partway through his degree, due to circumstances that required him to take charge of the family farm, where he grew wealthy through his agricultural business and real estate investments. In June 1874, the University granted him a partial certificate in civil engineering. His business ventures included agriculture and real estate, and by 1929, he was a partner in the land development and oil production company of Bliss & Wetherbee. Mr. Bliss died in the mid-1930s.\n\nHelen Eva Bliss, the daughter of Abel Bliss Jr., graduated from the University of Illinois in 1911 with a degree in Liberal Arts and Sciences. After her retirement from working life, she established The Bliss Professor of Engineering in memory of her father. Part of the Bliss bequest was earmarked to support the Grainger Engineering Library and Information Center Endowment, and the remainder towards other projects for “advancing the scholastic activities of the School of Engineering.” The bequest was so generous that it currently sustains several professorships annually at the School of Engineering, University of Illinois. \n\nThe Abel Bliss Professorship has been awarded to individuals such as Jiawei Han (computer science), Rashid Bashir (semiconductors, MEMS, and Biosensors), William P. King (Materials Science), Stephen A. Boppart, (Bioengineering and Internal Medicine), Kent D. Choquette, (vertical-cavity surface-emitting lasers), Philippe H. Geubelle (Aerospace Engineering), David Ruzic (Plasma Engineering), Albert J. Valocchi (water resources engineering, groundwater hydrology and contaminant transport), Elyse Rosenbaum (Bliss Faculty Scholar Award), and Rob A. Rutenbar (computer science and computer engineering) all of whom are currently researching further technologies at the University of Illinois.\n"}
{"id": "1272412", "url": "https://en.wikipedia.org/wiki?curid=1272412", "title": "Accounting information system", "text": "Accounting information system\n\nAn accounting as an information system (AIS) is a system of collecting, storing and processing financial and accounting data that are used by decision makers. An accounting information system is generally a computer-based method for tracking accounting activity in conjunction with information technology resources. The resulting financial reports can be used internally by management or externally by other interested parties including investors, creditors and tax authorities. Accounting information systems are designed to support all accounting functions and activities including auditing, financial accounting & reporting, managerial/ management accounting and tax. The most widely adopted accounting information systems are auditing and financial reporting modules.\n\nTraditionally, accounting is purely based on manual approach. Experience and skilfulness of an individual accountant are critical in accounting processes. Even using the manual approach can be ineffective and inefficient. Accounting information systems resolve many of above issues. AISs can support an automation of processing large amount of data and produce timely and accuracy of information.\n\nEarly accounting information systems were designed for payroll functions in 1970s. Initially, accounting information systems were predominantly developed \"in-house\" as legacy systems. Such solutions were expensive to develop and difficult to maintain. Therefore, many accounting practitioners preferred the manual approach rather than computer-based. Today, accounting information systems are more commonly sold as prebuilt software packages from large vendors such as Microsoft, Sage Group, SAP AG|SAP and Oracle Corporation|Oracle where it is configured and customized to match the organization’s business processes. Small businesses often use accounting lower costs software packages such as MYOB and Quickbooks. Large organisations would often choose ERP systems. As the need for connectivity and consolidation between other business systems increased, accounting information systems were merged with larger, more centralized systems known as enterprise resource planning (ERP). Before, with separate applications to manage different business functions, organizations had to develop complex interfaces for the systems to communicate with each other. In ERP, a system such as accounting information system is built as a module integrated into a suite of applications that can include manufacturing, supply chain, human resources. These modules are integrated together and are able to access the same data and execute complex business processes. Today, Cloud-based accounting information systems are increasingly popular for both SMEs and large organisations for lower costs. With adoption of accounting information systems, many businesses have removed low skills, transactional and operational accounting roles.\n\nAn AIS typically follows a multitier architecture separating the presentation to the user, application processing and data management in distinct layers. The presentation layer manages how the information is displayed to and viewed by functional users of the system (through mobile devices, web browsers or client application). The entire system is backed by a centralized database that stores all of the data. This can include transactional data generated from the core business processes (purchasing, inventory, accounting) or static, master data that is referenced when processing data (employee and customer account records and configuration settings). As transactions occur, the data is collected from the business events and stored into the system’s database where it can be retrieved and processed into information that is useful for making decisions. The application layer retrieves the raw data held in the log database layer, processes it based on the configured business logic and passes it onto the presentation layer to display to the users. For example, consider the accounts payable department when processing an invoice. With an accounting information system, an accounts payable clerk enters the invoice, provided by a vendor, into the system where it is then stored in the database. When goods from the vendor are received, a receipt is created and also entered into the AIS. Before the accounts payable department pays the vendor, the system’s application processing tier performs a three-way matching where it automatically matches the amounts on the invoice against the amounts on the receipt and the initial purchase order. Once the match is complete, an email is sent to an accounts payable manager for approval. From here a voucher can be created and the vendor can ultimately be paid.\n\nA big advantage of computer-based accounting information systems is that they automate and streamline reporting, develop advanced modelling and support data mining. Reporting is major tool for organizations to accurately see summarized, timely information used for decision-making and financial reporting. The accounting information system pulls data from the centralized database, processes and transforms it and ultimately generates a summary of that data as information that can now be easily consumed and analyzed by business analysts, managers or other decision makers. These systems must ensure that the reports are timely so that decision-makers are not acting on old, irrelevant information and, rather, able to act quickly and effectively based on report results. Consolidation is one of the hallmarks of reporting as people do not have to look through an enormous number of transactions. For instance, at the end of the month, a financial accountant consolidates all the paid vouchers by running a report on the system. The system’s application layer provides a report with the total amount paid to its vendors for that particular month. With large corporations that generate large volumes of transactional data, running reports with even an AIS can take days or even weeks.\n\nAfter the wave of corporate scandals from large companies such as Tyco International, Enron and WorldCom, major emphasis was put on enforcing public companies to implement strong internal controls into their transaction-based systems. This was made into law with the passage of the Sarbanes–Oxley Act of 2002 which stipulated that companies must generate an internal control report stating who is responsible for an organization’s internal control structure and outlines the overall effectiveness of these controls. Since most of these scandals were rooted in the companies' accounting practices, much of the emphasis of Sarbanes Oxley was put on computer-based accounting information systems. Today, AIS vendors tout their governance, risk management, and compliance features to ensure business processes are robust and protected and the organization's assets (including data) are secured.\n\nMany large and SMEs are now adopting cost effective cloud-based accounting information system in recent years.\n\nLooking back years ago, most organizations, even larger ones, hire outside consultants, either from the software publisher or consultants who understand the organization and who work to help select and implement the ideal configuration, taking all components into consideration.\n\nThe steps to implement an accounting information system are as follows:\n\n\n\n\nMany AIS professionals work for consulting firms, large corporations, insurance companies, financial firms, government agencies and public accounting firms, among other types of companies. With technological advancement, traditional accounting practice will shift to accounting information systems practice. Both accounting and information technology professional bodies are working on the new directions of accounting programs and industry practices. System Auditors is one of the top choices in the past two decades, they look at the controls, data processing, data integrity, general operation, maintenance, security and other aspects of all types of information systems used by businesses. A lot of the companies will deal with software and finding a software that is right for the company, or maintaining a software for a company. \nIf you are interested in the career, you might have the choice of working in the financial department of any type of business, or of working with a financially oriented company or a programming-oriented company that specializes in AIS. Some job titles in this field of work include financial manager, financial examiner and chief financial officer. You could also become a computer systems analyst, a computer information systems manager or a computer software engineer or programmer specializing in financial software.\n\nIf you are working with a financially oriented company, your job duties could range from analyzing an AIS for data integrity to managing the entire AIS. In a programming-oriented company, your focus may be directed towards developing new software in AIS or fixing bugs in an AIS. In both cases, you may also have the option of consulting, which requires travelling to different companies to provide analysis and advice concerning the company's AIS.\n\nThere are industry associations offer certificates that related to AIS area include CISA, AIS, CISSP, CIA, AFE, CFE, and CITP.\n\n"}
{"id": "460192", "url": "https://en.wikipedia.org/wiki?curid=460192", "title": "American Empire style", "text": "American Empire style\n\nAmerican Empire is a French-inspired Neoclassical style of American furniture and decoration that takes its name and originates from the Empire style introduced during the First French Empire period under Napoleon's rule. It gained its greatest popularity in the U.S. after 1820 and is considered the second, more robust phase of the Neoclassical style, which earlier had been expressed in the Adam style in Britain and \"Louis Seize\", or Louis XVI, in France. As an early-19th-century design movement in the United States, it encompassed architecture, furniture and other decorative arts, as well as the visual arts.\n\nIn American furniture, the Empire style was most notably exemplified by the work of New York cabinetmakers Duncan Phyfe and Paris-trained Charles-Honoré Lannuier. Other major furniture centers renowned for regional interpretations of the American Empire style were Boston, Philadelphia, and Baltimore. Many examples of American Empire cabinetmaking are characterized by antiquities-inspired carving, gilt-brass furniture mounts, and decorative inlays such as stamped-brass banding with egg-and-dart, diamond, or Greek-key patterns, or individual shapes such as stars or circles.\n\nThe most elaborate furniture in this style was made around 1815-25, often incorporating columns with rope-twist carving, animal-paw feet, anthemion, stars, and acanthus-leaf ornamentation, sometimes in combination with gilding and \"vert antique\" (antique green, simulating aged bronze). The Red Room at the White House is a fine example of American Empire style. A simplified version of American Empire furniture, often referred to as the Grecian style, generally displayed plainer surfaces in curved forms, highly figured mahogany veneers, and sometimes gilt-stencilled decorations. Many examples of this style survive, exemplified by massive chests of drawers with scroll pillars and glass pulls, work tables with scroll feet and fiddleback chairs. Elements of the style enjoyed a brief revival in the 1890s with, particularly, chests of drawers and vanities or dressing tables, usually executed in oak and oak veneers.\n\nThis Americanized interpretation of the Empire style continued in popularity in conservative regions outside the major metropolitan centers well past the mid-nineteenth century.\n\n\n"}
{"id": "13655564", "url": "https://en.wikipedia.org/wiki?curid=13655564", "title": "Baptist well drilling", "text": "Baptist well drilling\n\nBaptist well drilling is a very simple, manual method to drill water wells. The Baptist drilling rig can be built in any ordinary arc welding workshop and materials for a basic version costs about 150 US dollars (2006 prices). In suitable conditions, boreholes over 100 m deep have been drilled with this method.\n\nThe method was developed by Terry Waller, a North American Baptist missionary in Africa and Bolivia. It applies some of the same principles used in mechanized commercial well drilling, but does so using the simplest, most available and cheapest possible materials.\n\nRural people in developing countries often cannot afford to have specialists drill or dig wells for them. This method was developed to provide poor people with a way to help themselves with their water supply.\n\nA Baptist drilling rig, fit to drill holes up to deep, can be built in Nicaragua for about US $150. This includes all essential non-common tools to operate it. Its core element, the drill bits, can be made in about any arc-welding workshop, using only scrap steel and materials that can be found in virtually any hardware store.\n\nOnce the well is drilled, it is cased with an inexpensive PVC tube. Fitting the well with a slab of concrete as a sanitary seal and a simple PVC piston pump (also built by the users themselves) will cost about 2.5 dollars per meter well depth.\n\nA hybrid between sludging and percussion drilling, this method permits to drill through all kinds of loose alluvial soils, sands, silts and clays, as well as “soft” rocks, like light conglomerates, consolidated volcanic ashes, some calcareous rocks and weathered materials. It will not penetrate hard igneous rock or boulders (e.g. in ancient river beds).\n\nLike in sludging, the drilling process is continuous: the drill bit is normally not removed from the borehole until it is finished and the broken-up material is pumped to the surface in the drilling liquid (mud). But instead of using a hand as a valve on top of the drill pipe (sludging), the drill bit itself doubles as a foot-valve. The operator’s hand does not have to reach the end of the drill pipe and drill stem extensions can be several meters long.\n\nPercussion action is performed by lifting the drill stem with a rope over a pulley, attached to a simple derrick, made with locally available materials, such as wood or bamboo poles.\n\nThe borehole diameter is kept as small as possible in order to remove a minimum of material and hence advance rapidly. The standard drill bit is based on water pipe fittings and unless a larger diameter is required, the borehole is cased with PVC pipe.\n\nThe main drill tool consists of a length of metal pipe with a bit/valve. Extensions are standard PVC potable water pipes.\n\nNo temporary casing is used. The borehole being kept full of mud and the “caking” of mud into any unstable sand layers, as a consequence of the percussion action and friction of the smooth lateral edges of the bit, is normally sufficient to stabilize it. The drilling mud is evacuated from the borehole after casing the well by pouring or injecting water into the casing, called backwashing.\n\nThis technique adapts best to sand, loam and light rock. The standard drill bits also work through sticky and even consolidated clays. Nevertheless, best results in varying conditions are obtained with an array of different bits:\n\nIf required, the upper part of the well can be reamed and cased with larger diameter pipe (3 or 5 inches), to accommodate larger pumps. A shallow (large diameter) rope pump, for example, may require a wider well and submersible pumps commonly need at least 4\". Note that there is no need to enlarge the entire depth of the borehole: reaming until slightly below the lowest expected water table (the pump's water intake) is sufficient.\n\n"}
{"id": "54061840", "url": "https://en.wikipedia.org/wiki?curid=54061840", "title": "Bitpit", "text": "Bitpit\n\nbitpit is an open source modular library for scientific computing. The goal of bitpit is to ease the burden of writing scientific programs providing the common building blocks needed by every scientific application.\n\nEach module of the bitpit library is developed to address a specific aspect of real-life application development. Modules can be used as building blocks to quickly develop a high-performance scientific application. The library consists of several modules ranging from low level functionalities like algebraic operators to high level functionalities like the evaluation of distance functions on computational meshes.\n\nFeatures and the modules of bitpit include:\n\n\n"}
{"id": "32107992", "url": "https://en.wikipedia.org/wiki?curid=32107992", "title": "Boleto", "text": "Boleto\n\nBoleto Bancário, simply referred to as Boleto (English: Ticket) is a payment method in Brazil regulated by FEBRABAN, short for Brazilian Federation of Banks.\n\nA boleto can be paid at ATMs, branch facilities and internet banking of any Bank, Post Office, Lottery Agent and some supermarkets until its due date. After the due date it can only be paid at the issuer bank facilities.\n\nBoleto can only be collected by an authorized Collector Agent in the Brazilian territory. All Brazilian banks, Post Offices (Correios) and Lottery Agencies plus some companies from the private sector have joined the system.\n\nOpen source projects like BoletoPHP allow the merchandisers to generate unregistered Boleto Bancários without communicating with the bank. \n\n"}
{"id": "4471163", "url": "https://en.wikipedia.org/wiki?curid=4471163", "title": "CEA-936-A", "text": "CEA-936-A\n\nCEA-936-A (\"USB Carkit Specification\") is a CEA standard to allow the use of a mini-USB connector for UART and analog audio signals. It is intended to allow connection of a mobile phone to analog hands-free car kits, chargers, and other RS-232 devices.\n\nFrom the mobile phone side, the USB D- wire is used as either the USB D- signal, the UART Transmit data signal, the left stereo speaker audio channel, or the mono speaker audio channel, and the USB D+ wire is used as either the USB D+ signal, the UART Receive data signal, the right stereo speaker audio channel, or the mono microphone audio channel.\n\ncarkit specification also finds optional support in ULPI specification for off chip USB Physical layer ICs.\n\n"}
{"id": "764781", "url": "https://en.wikipedia.org/wiki?curid=764781", "title": "Colander", "text": "Colander\n\nA colander (or cullender) is a bowl-shaped kitchen utensil with holes in it used for draining food such as pasta or rice. A colander is also used to rinse vegetables.\n\nThe perforated nature of the colander allows liquid to drain through while retaining the solids inside. It is sometimes also called a pasta strainer or kitchen sieve.\n\nConventionally, colanders are made of a light metal, such as aluminium or thinly rolled stainless steel. Colanders are also made of plastic, silicone, ceramic, and enamelware.\n\nThe word colander comes from the Latin \"colum\" meaning \"sieve\".\n\nThe colander in the form of a pasta strainer was adopted as the religious headgear of the religion (or pseudoreligion) Pastafarianism in deference to the Flying Spaghetti Monster.\n\n\n"}
{"id": "27511822", "url": "https://en.wikipedia.org/wiki?curid=27511822", "title": "Commodity production", "text": "Commodity production\n\nCommodity production may refer to:\n"}
{"id": "1769024", "url": "https://en.wikipedia.org/wiki?curid=1769024", "title": "Condeep", "text": "Condeep\n\nCondeep (abbr. \"concrete deep water structure\") refers to a make of gravity-based structure for oil platforms developed by engineer Olav Mo in Hoeyer-Ellefsen and fabricated by Norwegian Contractors in Stavanger, Norway. A Condeep usually consists of a base of concrete oil storage tanks from which one, three or four concrete shafts rise. The Condeep base always rests on the sea floor, and the shafts rise to about 30 meters above the sea level. The platform deck itself is not a part of the construction.\n\nThe Condeep is used for a series of production platforms introduced for crude oil and natural gas production in the North Sea and Norwegian continental shelf.\n\nFollowing the success of the concrete oil storage tank on the Ekofisk field, Norwegian Contractors introduced the Condeep production platform concept in 1973.\n\nThis gravity-based structure for a platform was unique in that it was built from reinforced concrete instead of steel, which was the norm up to that point. This platform type was designed for the heavy weather conditions and the great water depths often found in the North Sea.\n\nCondeep has the advantage that it allows for storage of oil at sea in its own construction. It further allows equipment installation in the hollow legs well protected from the sea. In contrast, one of the challenges with steel platforms is that they only allow for limited weight on the deck compared with a Condeep where the weight allowance for production equipment and living quarters is seldom a problem.\n\nThe Troll A platform is the largest Condeep to date. It was built over a period of four years, using a workforce of 2,000, and deployed in 1995 to produce gas from the enormous Troll oil field.\n\nWith a total height of 472 meters, Troll A was the tallest structure ever to be built and moved. Many sources erroneously state that it is the largest. The largest structure ever moved (also a Condeep) was the Gullfaks C platform in the Norwegian North Sea. The total weight of the Troll A Condeep when launching was 1.2 million tons. 245,000 m³ of concrete and 100,000 tons of steel for reinforcement were used. The amount of steel corresponds to 15 Eiffel towers. The platform is placed at a depth of 300 meters. For stability, it is dug 35 meters into the sea floor.\n\n\n"}
{"id": "16503997", "url": "https://en.wikipedia.org/wiki?curid=16503997", "title": "Daisy chain (electrical engineering)", "text": "Daisy chain (electrical engineering)\n\nIn electrical and electronic engineering a daisy chain is a wiring scheme in which multiple devices are wired together in sequence or in a ring. Other than a full, single loop, systems which contain internal loops cannot be called daisy chains.\n\nDaisy chains may be used for power, analog signals, digital data, or a combination thereof.\n\nThe term daisy chain may refer either to large scale devices connected in series, such as a series of power strips plugged into each other to form a single long line of strips, or to the wiring patterns embedded inside of devices. Other examples of devices which can be used to form daisy chains are those based on USB, FireWire, Thunderbolt and Ethernet cables.\n\nFor analog signals, connections usually consist of a simple electrical bus and, especially in the case of a chain of many devices, may require the use of one or more repeaters or amplifiers within the chain to counteract attenuation (the natural loss of energy in such a system). Digital signals between devices may also travel on a simple electrical bus, in which case a bus terminator may be needed on the last device in the chain. However, unlike analog signals, because digital signals are discrete, they may also be electrically regenerated, but not modified, by any device in the chain.\n\nSome hardware can be attached to a computing system in a daisy chain configuration by connecting each component to another similar component, rather than directly to the computing system that uses the component. Only the last component in the chain directly connects to the computing system. For example, chaining multiple components that each have a UART port to each other. The components must also behave cooperatively. e.g., only one seizes the communications bus at a time.\n\n\nAny particular daisy chain forms one of two network topologies:\n\n\nUsers can daisy chain computing sessions together. Using services such as Telnet or SSH, the user creates a session on a second computer via Telnet, and from the second session, Telnets to a third and so on. Another typical example is the \"terminal session inside a terminal session\" using RDP. Reasons to create daisy chains include connecting to a system on a non-routed network via a gateway system, preserving sessions on the initial computer while working on a second computer, to save bandwidth or improve connectivity on an unstable network by first connecting to a better connected machine. A less wholesome purpose is camouflaging activity while engaged in cybercrime.\n"}
{"id": "168753", "url": "https://en.wikipedia.org/wiki?curid=168753", "title": "Data haven", "text": "Data haven\n\nA data haven, like a corporate haven or tax haven, is a refuge for uninterrupted or unregulated data. Data havens are locations with legal environments that are friendly to the concept of a computer network freely holding data and even protecting its content and associated information. They tend to fit into three categories: a physical locality with weak information-system enforcement and extradition laws, a physical locality with intentionally strong protections of data, and virtual domains designed to secure data via technical means (such as encryption) regardless of any legal environment.\n\nTor's onion space (hidden service), HavenCo (centralized), and Freenet (decentralized) are three models of modern-day virtual data havens.\n\nReasons for establishing data havens include access to free political speech for users in countries where censorship of the Internet is practiced.\n\nOther reasons can include:\n\nThe 1978 report of the British government's Data Protection Committee expressed concern that different privacy standards in different countries would lead to the transfer of personal data to countries with weaker protections; it feared that Britain might become a \"data haven.\" Also in 1978, Adrian Norman published a mock consulting study on the feasibility of setting up a company providing a wide range of data haven services, called \"Project Goldfish.\" \n\nScience fiction novelist William Gibson used the term in his novels \"Count Zero\" and \"Mona Lisa Overdrive\", as did Bruce Sterling in \"Islands in the Net\". The 1990s segments of Neal Stephenson's 1999 novel \"Cryptonomicon\" concern a small group of entrepreneurs attempting to create a data haven.\n\n"}
{"id": "757279", "url": "https://en.wikipedia.org/wiki?curid=757279", "title": "Disc filter", "text": "Disc filter\n\nA disc filter is a type of water filter used primarily in irrigation, similar to a screen filter, except that the filter cartridge is made of a number of plastic discs stacked on top of each other like a pile of poker chips. Each disc is covered with small grooves or bumps. The discs (or rings) each have a hole in the middle, forming a hollow cylinder in the middle of the stack. The water passes through the small passages in between and the impurities are trapped behind.\nThe filtration quality is based on the quantity and size of particles that the filtering element is able to retain. Higher quality filtration simply means cleaner water. This depends on the geometry of the channels, including the size, length, angle, and number of generated intersection points. The discs are typically color coded to denote the level of filtration. Filtration quality is usually measured in microns, based on the smallest size particle filtered. The typical range is from 25 microns for the finest level of filtration to 400 microns for the coarsest. Sometimes the filtration quality is given as the equivalent mesh size of a comparable screen filter. Typical mesh sizes range from 40 to 600. When using mesh sizes, 40 is the coarsest and 600 is the finest or highest level of filtration.\n\nDisc filters range in size from small units with a 3/4\" inlet and outlet used for landscape drip irrigation systems to very large banks of multiple filters manifolded together used for filtering large volumes of water for agricultural and industrial applications.\n\nSome disc filters, especially the smaller ones, must be taken apart and cleaned by hand. Many of the larger ones can be backflushed in such a way that the discs are able to separate and spin during the cleaning cycle. In some cases, a booster pump may be required for backflushing. Disc filters can be used for many types of contaminants, including fine sand and organic matter. However, when used to filter organic matter, they will clog more quickly than a media filter and will have to be cleaned more often. One advantage that the disc filter has over the media filter is that it can backflush more quickly with less flush water.\n\nDisc filters used in agricultural irrigation are covered by the ISO 9912-2 standard.\n\nThe disc filter was originally developed in 1936 to filter hydraulic fluid in the B-17 bomber. In these filters, the discs were made of stainless steel and brass. This type of filter began to be used in Israel to filter irrigation water in the 1960s.\n"}
{"id": "42766111", "url": "https://en.wikipedia.org/wiki?curid=42766111", "title": "Education Networks of America", "text": "Education Networks of America\n\nEducation Networks of America (ENA) is a private company providing internet services to public schools and libraries. It is based in Nashville, Tennessee, in the United States.\n\nENA was founded in 1996, and is estimated to generate $100 million in revenue annually. It is now owned by Zelnick Media Capital, an investment company based in New York.\n\nOn January 18, 2018, TeleQuality Communications (TeleQuality) of San Antonio, Texas became a subsidiary of ENA On June 28, 2018, CatchOn became part of ENA family of companies\n"}
{"id": "2176826", "url": "https://en.wikipedia.org/wiki?curid=2176826", "title": "Elaboration likelihood model", "text": "Elaboration likelihood model\n\nThe elaboration likelihood model (ELM) of persuasion is a dual process theory describing the change of attitudes. The ELM was developed by Richard E. Petty and John Cacioppo in 1980. The model aims to explain different ways of processing stimuli, why they are used, and their outcomes on attitude change. The ELM proposes two major routes to persuasion: the central route and the peripheral route.\n\n\nElaboration likelihood model is a general theory of attitude change. According to the theory's developers Richard E. Petty and John T. Cacioppo, they intended to provide a general \"framework for organizing, categorizing, and understanding the basic processes underlying the effectiveness of persuasive communications\".\n\nThe study of attitudes and persuasion began as the central focus of social psychology, featured in the work of psychologists Gordon Allport (1935) and Edward Alsworth Ross (1908). Allport described attitudes as \"the most distinctive and indispensable concept in contemporary social psychology\". Considerable research was devoted to the study of attitudes and persuasion from the 1930s through the late 1970s. These studies embarked on various relevant issues regarding attitudes and persuasion, such as the consistency between attitudes and behaviors and the processes underlying attitude/behavior correspondence. However, Petty and Cacioppo noticed a major problem facing attitude and persuasion researchers to the effect that there was minimal agreement regarding \"if, when, and how the traditional source, message, recipient, and channel variables affected attitude change\". Noticing this problem, Petty and Cacioppo developed the elaboration likelihood model as their attempt to account for the differential persistence of communication-induced attitude change. Petty and Cacioppo suggested that different empirical findings and theories on attitude persistence could be viewed as stressing one of two routes to persuasion which they presented in their elaboration likelihood model.\n\nThe elaboration likelihood model proposes two distinct routes for information processing: a central route and a peripheral route. The ELM holds that there are numerous specific processes of change on the \"elaboration continuum\" ranging from low to high. When the operation processes at the low end of the continuum determine attitudes, persuasion follows the peripheral route. When the operation processes at the high end of the continuum determine attitudes, persuasion follows the central route.\n\nThe central route is used when the message recipient has the motivation as well as the ability to think about the message and its topic. When people process information centrally, the cognitive responses, or elaborations, will be much more relevant to the information, whereas when processing peripherally, the individual may rely on heuristics and other rules of thumb when elaborating on a message. Being at the high end of the elaboration continuum, people assess object-relevant information in relation to schemas that they already possess, and arrive at a reasoned attitude that is supported by information. It is important to consider two types of factors that influence how and how much one will elaborate on a persuasive message. The first are the factors that influence our motivation to elaborate, and the second are the factors that influence our ability to elaborate. Motivation to process the message may be determined by a personal interest in the subject of the message, or individual factors like the need for cognition. However, if the message recipient has a strong, negative attitude toward the position proposed by the message, a boomerang effect (an opposite effect) is likely to occur. That is, they will resist the message, and may move away from the proposed position. Two advantages of the central route are that attitude changes tend to last longer and are more predictive of behavior than the changes from the peripheral route. Overall, as people’s motivation and ability to process the message and develop elaborations decreases, the peripheral cues present in the situation become more important in their processing of the message.\n\nThe peripheral route is used when the message recipient has little or no interest in the subject and/or has a lesser ability to process the message. Being at the low end of the elaboration continuum, recipients do not examine the information as thoroughly. With the peripheral route, they are more likely to rely on general impressions (e.g. \"this feels right/good\"), early parts of the message, their own mood, positive and negative cues of the persuasion context, etc. Because people are \"cognitive misers,\" looking to reduce mental effort, they often use the peripheral route and thus rely on heuristics (mental shortcuts) when processing information. When an individual is not motivated to centrally process an issue because they lack interest in it, or if the individual does not have the cognitive ability to centrally process the issue, then these heuristics can be quite persuasive. Robert Cialdini's Principles of Social Influence (1984), which include commitment, social proof, scarcity, reciprocation, authority, as well as liking the person who is persuading you, are some examples of frequently used heuristics. In addition, credibility can also be used as a heuristic in peripheral thinking because when a speaker is seen as having a higher credibility, then the listener may be more likely to believe the message. Credibility is a low-effort and somewhat reliable way to give us an answer of what to decide and/or believe without having to put in much work to think it through.\n\nIf these peripheral influences go completely unnoticed, the message recipient is likely to maintain their previous attitude towards the message. Otherwise, the individual will temporarily change his attitude towards it. This attitude change can be long-lasting, although durable change is less likely to occur than it is with the central route.\n\nThe two most influential factors that affect which processing route an individual uses are motivation (the desire to process the message; see Petty and Cacioppo, 1979) and ability (the capability for critical evaluation; see Petty, Wells and Brock, 1976). The extent of motivation is in turn affected by attitude and personal relevance. Individuals' ability for elaboration is affected by distractions, their cognitive busyness (the extent to which their cognitive processes are engaged by multiple tasks), and their overall knowledge.\n\n\"Attitudes\" towards a message can affect motivation. Drawing from cognitive dissonance theory, when people are presented with new information (a message) that conflicts with existing beliefs, ideas, or values, they will be motivated to eliminate the dissonance, in order to remain at peace with their own thoughts. For instance, people who want to believe that they will be academically successful may recall more of their past academic successes than their failures. They may also use their world knowledge to construct new theories about how their particular personality traits may predispose them to academic success (Kunda, 1987). If they succeed in accessing and constructing appropriate beliefs, they may feel justified in concluding that they will be academically successful, not realizing that they also possess knowledge that could be used to support the opposite conclusion.\n\n\"Personal relevance\" can also affect an individual's degree of motivation. For instance, undergraduate students were told of a new exam policy that would take effect either one or ten years later. The proposal of the new exam policy was either supported by strong or weak arguments. Those students who were going to personally be affected by this change would think more about the issue than those students who were not going to be personally affected.\n\nAn additional factor that affects degree of motivation is an individual's need for cognition. Individuals who take greater pleasure in thinking than others tend to engage in more effortful thinking because of its intrinsic enjoyment for them, regardless of the importance of the issue to them or the need to be correct.\n\n\"Ability\" includes the availability of cognitive resources (for instance, the absence of time pressures or distractions) and the relevant knowledge needed to examine arguments. Distractions (for instance, noise in a library where a person is trying to read a journal article) can decrease a person's ability to process a message. Cognitive busyness, which can also serve as a distraction, limits the cognitive resources otherwise available for the task at hand (assessing a message). Another factor of ability is familiarity with the relevant subject. Though they might not be distracted nor cognitively busy, their insufficiency in knowledge can hinder people's engagement in deep thinking.\n\nSome psychologists lump opportunity in with Ability as it primarily relates to the time available to the individual to make a decision. The popular train of thought today is that this is a category of its own.\n\nThere are four core ideas to the ELM.\n\n\nOne of the main assumptions of the ELM is that the attitudes formed through the central route rather than the peripheral route are stronger and more difficult to change. This means that when the central route is taken (involving high-elaboration thought in which all information is being carefully analyzed), the attitudes formed become more stable and less susceptible to counter-persuasion, whereas when the peripheral route is taken (involving low-elaboration thought based on heuristics and shortcuts to establish an attitude) short-term attitude change is more likely to occur.\n\nA variable is essentially anything that can \"increase\" or \"decrease\" the persuasiveness of a message.\" Motivation\" (desire to process the message), \"ability\" (capability for critical evaluation), \"attractiveness\", \"mood\" and \"expertise\" are just a few examples of variables that can influence persuasiveness. Variables also have different roles, for example, they may have a positive effect as a cue, but a negative effect if it ends up decreasing thought about a strong message.\n\nUnder high elaboration, a given variable (e.g., expertise) can serve as an \"argument\" (e.g., \"If Einstein agrees with the theory of relativity, then this is a strong reason for me to as well\") or a \"biasing factor\" (e.g., \"If an expert agrees with this position it is probably good, so let me see who else agrees with this conclusion\"), at the expense of contradicting information. Under low-elaboration conditions, a variable may act as a \"peripheral cue\" (e.g., the belief that \"experts are always right\"). While this is similar to the Einstein example above, this is a shortcut which (unlike the Einstein example) does not require thought. Under moderate elaboration, a variable may direct the \"extent of information processing\" (e.g., \"If an expert agrees with this position, I should really listen to what (s)he has to say\").\n\nRecent adaptations of the ELM have added a role for variables: to affect the extent to which a person trusts their thoughts in response to a message (\"self-validation role\"). A person may think, \"If an expert presented this information, it is probably correct, and thus I can trust that my reactions to it are informative with respect to my attitude.\" This role, because of its metacognitive nature, only occurs in high-elaboration conditions.\n\nScholars have been studying different variables in this model in difference context. For example, in \"The Elaboration Likelihood Model: Limitations and Extensions in Marketing\", Bitner et al. proposed that motivation and ability are under an interlaced influence of situational variables, person variables, and product categories variables.\n\nFor an individual intent on forming long-lasting beliefs on topics, the central route is advantageous by the fact that arguments are scrutinized intensely and that information is unlikely to be overlooked. However, this route uses a considerable amount of energy, time, and mental effort.\n\nIt is not worthwhile to exert considerable mental effort to achieve correctness in all situations and people do not always have the requisite knowledge, time, or opportunity to thoughtfully assess the merits of a proposal. For those, the use of the peripheral route excels at saving energy, time, and mental effort. This is particularly advantageous in situations in which one must make a decision within a small time constraint. On the other hand, the peripheral route is prone to errors in judgment, at least in attributing reasons for behaviors.\n\nResearchers have applied the elaboration likelihood model to many fields, including advertising, marketing, consumer behavior and health care, just to name a few.\n\nThe elaboration likelihood model can be applied to advertising and marketing.\n\nIn 1983, Petty, Cacioppo and Schumann conducted a study to examine source effects in advertising. It was a product advertisement about a new disposable razor. The authors purposefully made one group of subjects highly involved with the product, by telling them the product would be test marketed soon in the local area and by the end of the experiment they would be given a chance to get a disposable razor. Whereas, the authors made another group of subjects have low involvement with the product by telling them that the product would be test marketed in a distant city and by the end of the experiment they would have the chance to get a toothpaste. In addition to varying involvement, the authors also varied source and message characteristics by showing a group of the subjects ads featuring popular athletes, whereas showing other subjects ads featuring average citizens; showing some subjects ads with strong arguments and others ads with weak arguments. This experiment shows that when the elaboration likelihood was low, featuring famous athletes in the advertisement would lead to more favorable product attitudes, regardless of the strength of the product attributes presented. Whereas when elaboration likelihood was high, only the argument strength would manipulate affected attitudes. Lee et al. supported the studies on that product involvement strengthens the effects of \"endorser–product congruence on consumer responses\" when the endorsers expertise is well related with product to create source credibility. Lee's finding also helps to understand celebrity endorsement as not only a peripheral cue but also a motivation for central route.\n\nLater in 1985, Bitner, Mary J., and Carl Obermiller expand this model theoretically in the field of marketing. They proposed in the marketing context, the determinant of routes is more complex, involving variables of situation, person, and product categories. Trampe et al. (2010) also discovered that product relevance is directional proportional to the attractiveness.\n\nIt is widely acknowledged that effects of ads are not only limited to the information contained in the ad alone but are also a function of the different appeals used in the ads (like use celebrities or non-celebrities as endorsers). In a study conducted by Rollins and Bhutada in 2013, ELM theory was the framework used to understand and evaluate the underlying mechanisms describing the relationships between endorser type, disease state involvement and consumer response to direct-to-consumer advertisements (DTCA). The finding showed while endorser type did not significantly affect consumer attitudes, behavioral intentions and information search behavior; level of disease state involvement, though, did. More highly involved consumers had more positive attitudes, behavioral intentions and greater information search behavior.\n\n\nRecent research has been conducted to apply the ELM to the healthcare field. In 2009, Angst and Agarwal published a research article, \"Adoption of Electronic Health Records in the Presence of Privacy Concerns: the Elaboration Likelihood Model and Individual Persuasion\". This research studies electronic health records (EHRs), (an individual's) concern for information privacy (CFIP) and the elaboration likelihood model (ELM). The two researchers aimed to investigate the question, \"Can individuals be persuaded to change their attitudes and opt-in behavioral intentions toward EHRs, and allow their medical information to be digitized even in the presence of significant privacy concerns?\"\n\nSince the ELM model provides an understanding how to influence attitudes, the said model could be leveraged to alter perceptions and attitudes regarding adoption and adaptation of change.\n\nFindings of the research included:\n\nChen and Lee conducted a study about online shopping persuasion by applying the elaboration likelihood model back to 2008. In this study, how online shopping influences consumers' beliefs and perceived values on attitude and approach behavior were examined. \"Twenty cosmetics and 20 hotel websites were selected for participants to randomly link to and read, and the students were then asked to fill in a 48-item questionnaire via the internet. It was found that when consumers have higher levels of agreeableness and conscientiousness, central route website contents would be more favorable for eliciting utilitarian shopping value; whereas when consumers have higher levels of emotional stability, openness, and extraversion, peripheral route website contents would be more critical in facilitating experiential and hedonic shopping value\", Chen explained.\n\nIn 2009, another study about the effects of consumer skepticism on online shopping was conducted by Sher and Lee. Data on young customers' attitudes about a product were acquired through an online experiment with 278 college students, and two findings emerged after analysis. First, highly skeptical consumers tend to stick with their original impression than been influenced by other factors (Central Route); which means, they are biased against certain types of information and indifferent to the message quality. Second, consumers with low skepticism tend to adopt the peripheral route in forming attitude; that is, they are more persuaded by online review quantity. Lee indicated, \"these findings contribute to the ELM research literature by considering a potentially important personality factor in the ELM framework\".\n\nOther studies applied ELM in e-commerce and internet related fields are listed below for your additional references:\n\nIn order to reduce youth smoking by developing improved methods to communicate with higher risk youth, Flynn and his colleagues conducted a study in 2013, exploring the potential of smoking prevention messages on TV based on the ELM. \"Structured evaluations of 12 smoking prevention messages based on three strategies derived from the ELM were conducted in classroom settings among a diverse sample of non-smoking middle school students in three states. Students categorized as likely to have higher involvement in a decision to initiate cigarette smoking, are reported relatively high ratings on a cognitive processing indicator for messages focused on factual arguments about negative consequences of smoking than for messages with fewer or no direct arguments. Message appeal ratings did not show greater preference for this message type among higher involved versus lower involved students. Ratings from students reporting lower academic achievement suggested difficulty processing factual information presented in these messages. The ELM may provides a useful strategy for reaching adolescents at risk for smoking initiation, but particular attention should be focused on lower academic achievers to ensure that messages are appropriate for them.\"\n\nAnother research directed by Boyce and Kuijer was focusing on media body ideal images triggers food intake among restrained eaters based on ELM. Their hypotheses were based on restraint theory and the ELM. From the research, they found participants' attention (advertent/inadvertent) toward the images was manipulated. Although restrained eaters' weight satisfaction was not significantly affected by either media exposure condition, advertent (but not inadvertent) media exposure triggered restrained eaters' eating. These results suggest that teaching restrained eaters how to pay less attention to media body ideal images might be an effective strategy in media–literary interventions.\n\nWith the development of the internet and the emerging new media, L. G. Pee (2012) has conduct interesting research on the influence of trust on social media using the ELM theory. The findings resulted that source credibility, the majority influence, and information quality has strong effect on the trust for users.\n\nHans-Joachim Mosler applied ELM to study if and how a minority can persuade the majority to change its opinion. The study used Agent-based social simulation. There were 5 agents. 3 (or 4) of whom held a neutral opinion on some abstract topic, while the other 2 (or 1) held a different opinion. In addition, there were differences between the agents regarding their \"argument quality\" and \"peripheral cues\". The simulation was done in rounds. In each round, one of the agents had an opportunity to influence the other agents. The level of influence was determined by either the argument strength (if the central route was taken) or the peripheral cues (if the peripheral route was taken). After 20 rounds of persuasion, the distance between the majority's original opinion to its new opinion was studied. It was found that, the peripheral cues of the minority were more important than the argument quality. I.e, a minority with strong arguments but negative cues (e.g., different skin-color or bad reputation) did not succeed in convincing the majority, while a minority with weak arguments and positive cues (e.g., appearance or reputation) did succeed. The results depend also on the level of \"personal relevance\" – how much the topic is important to the majority and to the minority.\n\nIn Mental Health Counseling\n\n\"Counseling and Stigma\"\n\nOne of the most common reasons why an individual does not attend counseling is because they are worried about the falling into a stigma (being considered crazy, or having serious “issues”). This stigma—which was prevalent 30 years ago, still exists today. Fortunately, an implementation of the ELM can help increase the positive perceptions of counseling amongst the undergraduate student population. Students that repeatedly watched a video that explained the function and positive outcomes of mental health counseling demonstrated a significant and lasting change in their perception to counseling. Students who watched the video once or not at all maintained a relatively negative view towards counseling. Thus, repeated exposure towards the positive elements of counseling lead towards a greater elaboration and implementation of the central route to combat negative social stigma of counseling. Most negative intuitions exist within the realm of the peripheral route, and therefore to work against stigmas the general public needs to engage their central route of processing.\n\n\"Counselor Credibility\"\n\nThe more credible a counselor is perceived as, the more likely that counseling clients are to perceive the counselor’s advice as impactful. However, counselor credibility is strongly mediated by the degree to which the client understands the information conveyed by the counselor. Therefore, it is extremely important that counseling clients feel that they understand their counselor. The use of metaphor is helpful for this. Metaphors require a deeper level of elaboration, thereby engaging the central route of processing. Kendall (2010) suggests using metaphor in counseling as a valid method towards helping clients understand the message/psychological knowledge conveyed by the client. When the client hears a metaphor that resonates with them, they are far more likely to trust and build positive rapport with the counselor.\n\nIn designing a test for the aforementioned model, it is necessary to determine the quality of an argument, i.e., whether it is viewed as strong or weak. If the argument is not seen as strong, then the results of persuasion will be inconsistent. A strong argument is defined by Petty and Cacioppo as \"one containing arguments such that when subjects are instructed to think about the message, the thoughts they generate are fundamentally favorable.\" An argument that is universally viewed as weak will elicit unfavorable results, especially if the subject considers it under high elaboration, thus being the central route. Test arguments must be rated by ease of understanding, complexity and familiarity. To study either route of the elaboration likelihood model, the arguments must be designed for consistent results. Also, when assessing persuasion of an argument, the influence of peripheral cues needs to be taken into consideration as cues can influence attitude even in the absence of argument processing. The extent or direction of message processing also needs to be taken into consideration when assessing persuasion, as variables can influence or bias thought by enabling or inhibiting the generation of a particular kind of thought in regard to the argument. \"While the ELM theory continues to be widely cited and taught as one of the major cornerstones of persuasion, questions are raised concerning its relevance and validity in 21st century communication contexts.\"\n\nSome researchers have been criticized for misinterpreting the ELM. One such instance is Kruglanski and Thompson, who write that the processing of central or peripheral routes is determined by the \"type\" of information that affects message persuasion. For example, message variables are only influential when the central route is used and information like source variables is only influential when the peripheral route is used. In fact, the ELM does not make statements about \"types\" of information being related to routes. Rather, the key to the ELM is \"how\" any type of information will be used depending on central or peripheral routes, regardless of what that information is. For example, the central route may permit source variables to influence preference for certain language usage in the message (e.g. \"beautiful\") or validate a related product (e.g. cosmetics), while the peripheral route may only lead individuals to associate the \"goodness\" of source variables with the message. Theoretically, all of these could occur simultaneously. Thus, the distinction between central and peripheral routes is not the type of information being processed as those types can be applied to both routes, but rather how that information is processed and ultimately whether processing information in one way or the other will result in different attitudes.\n\nA second instance of misinterpretation is that processing of the central route solely involves thinking about the message content and not thoughts about the issue. Petty and Cacioppo (1981) stated \"If the issue is very important to the person, but the person doesn't understand the arguments being presented in the message, or if no arguments are actually presented, then elaboration of arguments cannot occur.…Nevertheless, the person may still be able to think about the issue.\" Therefore, issue-relevant thinking is still a part of the central route and is necessary for one to think about the message content.\n\nLastly, a third instance of misinterpretation by Kruglanski and Thompson is the disregard for the quantitative dimension presented by the ELM and more focus on the qualitative dimension. This quantitative dimension is the peripheral route involves low-elaboration persuasion that is quantitatively different from the central route that involves high elaboration. With this difference the ELM also explains that low-elaboration persuasion processes are qualitatively different as well. It is seen as incorrect if the ELM focuses on a quantitative explanation over a qualitative one; however one of the ELM's key points is that elaboration can range from high to low which is not incorrect as data from experiments conducted by Petty (1997) as well as Petty and Wegener (1999) suggest that persuasion findings can be explained by a quantitative dimension without ever needing a qualitative one.\n\nIn 2014, J. Kitchen et al. scrutinized the literatures of the ELM for the past 30 years. They came up with four major research areas that have received most significant criticism:\n\nThe first critique lies on the basis of the model's initial development. For the fact that ELM was built on previous empirical researches and diverse literature base to unify disparate ideas, the model is inherently descriptive because of the intuitive and conceptual assumptions underlying. For example, Choi and Salmon criticized Petty and Cacioppo's assumption that correct recall of a products led directly to highly involvement. They proposed that the high involvement are likely to be the result of other variations, for example sample population; and the weak/strong arguments in one study are likely to result in different involvement characteristics in another study.\n\nThe elaboration likelihood continuum is ought to show that a human can undergo a natural progression from high involvement to low involvement with the corresponding effects. This continuum can account for the swift between the central and the peripheral routes, but has yet been lack of comprehensive and empirical testing since the beginning. However, researches has been done under three distinct conditions: high, low, and moderate.\n\nThis area of critique basically lands on the nature of ELM being a dual-process model, which indicates that the receivers will rely on one of the routes (central or peripheral) to process messages and possibly change attitude and behaviour. Stiff (1986) questioned the validity of ELM because the message should be able to be processed through two routes simultaneously. On top of Stiff's questioning, alternative models have been raised. Mackenzie et al (1986) advocated a Dual Mediation Hypothesis (DMH) that allow receivers to process the ad's content and its execution at the same time with reasonable vigilance. Lord et al. (1995) proposed a Combined Influence Hypothesis which argues that the central and peripheral cues worked in combination despite the variables of motivation and ability. Kruglanski et al. (1999) proposed a single cognitive process instead of the dual-process model. Although drawing on the fundamental conception from ELM, such as motivation, ability and continuum, the unimodel suggests a normative and heuristic rules for human to make judgement based on the evidence. The Heuristic Systematic Model (HSM) is another alternative model concerning this issue.\n\nMany studies have been expanding and/or refining the model by examining and testing the variables, particularly in advertising research. For example, Britner and Obermiller (1985) were among the first to expand the model to new variables under the peripheral processing. They proposed situation, person, and product categories as new variables under the context of marketing.\n\n\n\n"}
{"id": "5633026", "url": "https://en.wikipedia.org/wiki?curid=5633026", "title": "Fixture (tool)", "text": "Fixture (tool)\n\nA fixture is a work-holding or support device used in the manufacturing industry. Fixtures are used to securely locate (position in a specific location or orientation) and support the work, ensuring that all parts produced using the fixture will maintain conformity and interchangeability. Using a fixture improves the economy of production by allowing smooth operation and quick transition from part to part, reducing the requirement for skilled labor by simplifying how workpieces are mounted, and increasing conformity across a production run.\n\nA fixture differs from a jig in that when a fixture is used, the tool must move relative to the workpiece; a jig moves the piece while the tool remains stationary.\n\nA fixture's primary purpose is to create a secure mounting point for a workpiece, allowing for support during operation and increased accuracy, precision, reliability, and interchangeability in the finished parts. It also serves to reduce working time by allowing quick set-up, and by smoothing the transition from part to part. It frequently reduces the complexity of a process, allowing for unskilled workers to perform it and effectively transferring the skill of the tool maker to the unskilled worker. Fixtures also allow for a higher degree of operator safety by reducing the concentration and effort required to hold a piece steady.\n\nEconomically speaking the most valuable function of a fixture is to reduce labor costs. Without a fixture, operating a machine or process may require two or more operators; using a fixture can eliminate one of the operators by securing the workpiece.\n\nFixtures must always be designed with economics in mind; the purpose of these devices is to reduce costs, and so they must be designed in such a way that the cost reduction outweighs the cost of implementing the fixture. It is usually better, from an economic standpoint, for a fixture to result in a small cost reduction for a process in constant use, than for a large cost reduction for a process used only occasionally.\nMost fixtures have a solid component, affixed to the floor or to the body of the machine and considered immovable relative to the motion of the machining bit, and one or more movable components known as clamps. These clamps (which may be operated by many different mechanical means) allow workpieces to be easily placed in the machine or removed, and yet stay secure during operation. Many are also adjustable, allowing for workpieces of different sizes to be used for different operations. Fixtures must be designed such that the pressure or motion of the machining operation (usually known as the feed) is directed primarily against the solid component of the fixture. This reduces the likelihood that the fixture will fail, interrupting the operation and potentially causing damage to infrastructure, components, or operators.\n\nFixtures may also be designed for very general or simple uses. These multi-use fixtures tend to be very simple themselves, often relying on the precision and ingenuity of the operator, as well as surfaces and components already present in the workshop, to provide the same benefits of a specially-designed fixture. Examples include workshop vises, adjustable clamps, and improvised devices such as weights and furniture.\n\nEach component of a fixture is designed for one of two purposes: location or support.\n\nLocating components ensure the \"geometrical stability\" of the workpiece. They make sure that the workpiece rests in the correct position and orientation for the operation by addressing and impeding all the degrees of freedom the workpiece possesses.\n\nFor locating workpieces, fixtures employ pins (or \"buttons\"), clamps, and surfaces. These components ensure that the workpiece is positioned correctly, and remains in the same position throughout the operation. Surfaces provide support for the piece, pins allow for precise location at low surface area expense, and clamps allow for the workpiece to be removed or its position adjusted. Locating pieces tend to be designed and built to very tight specifications.\n\nIn designing the locating parts of a fixture, only the \"direction\" of forces applied by the operation are considered, and not their \"magnitude\". Locating parts technically support the workpiece, but do not take into account the strength of forces applied by the process and so are usually inadequate to actually secure the workpiece during operation. For this purpose, support components are used.\n\nTo secure workpieces and prevent motion during operation, support components primarily use two techniques: positive stops and friction. A positive stop is any immovable component (such as a solid surface or pin) that, by its placement, physically impedes the motion of the workpiece. Support components are more likely to be adjustable than locating components, and normally do not press tightly on the workpiece or provide absolute location.\n\nSupport components usually bear the brunt of the forces delivered during the operation. To reduce the chances of failure, support components are usually not also designed as clamps.\n\nFixtures are usually classified according to the machine for which they were designed. The most common two are \"milling fixtures\" and \"drill fixtures\".\n\nMilling operations tend to involve large, straight cuts that produce lots of chips and involve varying force. Locating and supporting areas must usually be large and very sturdy in order to accommodate milling operations; strong clamps are also a requirement. Due to the vibration of the machine, positive stops are preferred over friction for securing the workpiece. For high-volume automated processes, milling fixtures usually involve hydraulic or pneumatic clamps.\n\nDrilling fixtures cover a wider range of different designs and procedures than milling fixtures. Though workholding for drills is more often provided by jigs, fixtures are also used for drilling operations.\n\nTwo common elements of drilling fixtures are the hole and bushing. Holes are often designed into drilling fixtures, to allow space for the drill bit itself to continue through the workpiece without damaging the fixture or drill, or to guide the drill bit to the appropriate point on the workpiece. Bushings are simple bearing sleeves inserted into these holes to protect them and guide the drill bit.\n\nBecause drills tend to apply force in only one direction, support components for drilling fixtures may be simpler. If the drill is aligned pointing down, the same support components may compensate for the forces of both the drill and gravity at once. However, though monodirectional, the force applied by drills tends to be concentrated on a very small area. Drilling fixtures must be designed carefully to prevent the workpiece from bending under the force of the drill.\n\n\n"}
{"id": "398703", "url": "https://en.wikipedia.org/wiki?curid=398703", "title": "Foundation for Intelligent Physical Agents", "text": "Foundation for Intelligent Physical Agents\n\nThe Foundation for Intelligent Physical Agents (FIPA) is a body for developing and setting computer software standards for heterogeneous and interacting agents and agent-based systems.\n\nFIPA was founded as a Swiss not-for-profit organization in 1996 with the ambitious goal of defining a full set of standards for both implementing systems within which agents could execute (agent platforms) and specifying how agents themselves should communicate and interoperate in a standard way.\n\nWithin its lifetime the organization's membership included several academic institutions and a large number of companies including Hewlett Packard, IBM, BT (formerly British Telecom), Sun Microsystems, Fujitsu and many more. A number of standards were proposed, however, despite several agent platforms adopting the \"FIPA standard\" for agent communication it never succeeded in gaining the commercial support which was originally envisaged. The Swiss organization was dissolved in 2005 and an IEEE standards committee was set up in its place.\n\nThe most widely adopted of the FIPA standards are the Agent Management and Agent Communication Language (FIPA-ACL) specifications.\n\nThe name FIPA is somewhat of a misnomer as the \"physical agents\" with which the body is concerned exist solely in software (and hence have no physical aspect).\n\n\n\n"}
{"id": "22537939", "url": "https://en.wikipedia.org/wiki?curid=22537939", "title": "Funai", "text": "Funai\n\nFunai is the main supplier of electronics to Walmart and Sam's Club stores in the US, with production output in excess of 2 million flat-panel televisions during the summertime per year for Black Friday sale. Funai is the OEM providing assembled televisions and video players/recorders to major corporations such as Sharp, Toshiba, Denon, and others. Funai also manufactures printers for Dell and Lexmark and produces printers under the Kodak name.\n\nFunai was founded by Tetsuro Funai, the son of a sewing machine manufacturer. During the 1950s before the company was formed, Funai produced sewing machines and was one of the first Japanese makers to enter the United States retail market. Then, the introduction of transistor technology had begun to change the face of the electronics market. The Funai company was formed, Tetsuro Funai became CEO for 47 years and a US dollar billionaire, and the first actual products produced were the transistor radios.\n\nIn 1980, Funai launched a sales and manufacturing subsidiary in Germany. Funai also developed the Compact Video Cassette (CVC) format in the same year, a joint development with Technicolor, trying to compete with VHS and Betamax. Sales were poor and not well-received due to ongoing VHS vs. Beta war, and the CVC format was abandoned a few years later.\n\nFunai began to see rising sales of the VHS format, so in 1984, Funai released its first VHS video cassette player (VP-1000) for the worldwide market, while ordering all transport chassis mechanisms from Shintom for quick and efficient production. By the late 1980s, Funai quickly became the largest 2-head mono VHS video cassette recorder (VCR) manufacturer in Japan.\n\nIn 1991, a U.S. sales subsidiary was established in New Jersey, and it began to sell cathode ray tube (CRT) televisions. In 1992, Funai canceled its contract from Shintom due to the rising cost of VCR chassis mechanism and the expensive Japanese labor, and decided to build its own lower-cost chassis mechanism instead. This creative move dramatically boosted up profits and reduced VCR prices down fast. Funai developed a new, permanent strategy in 1993 by opening two new state-of-the-art factories in China, which transferred all VHS VCRs production out from Japan. By 1997, Funai became the first manufacturer to sell a new VHS VCR below $100 for the North American Market, while Philips Magnavox brand they produced for was the best-seller. Quickly, Tetsuro Funai, the founder, became Japan's first US dollar billionaire electronic CEO. Later, the DVD technology was formed, and by 2001, Funai sold its first DVD player for less than $100. By then, Funai's U.S. subsidiary had relocated to Torrance, California. Today, Funai is one of the world's largest producer of DVD players, and is now one of the major suppliers of electronics to Wal-Mart on Black Friday\n\nIn 2008, CEO and founder Tetsuro Funai retired and stepped down from CEO to become chairman. Philips signed a seven-year contract with Funai to license, sell, and distribute Philips- and Magnavox-branded televisions in North America. In 2013, Funai acquired the option to buy the rest of Philips' consumer electronics operations and a license to globally market Philips branded consumer electronics. But that purchase was terminated by Philips because of what Philips saw as breach of contract.\n\nFunai has made inkjet hardware for Lexmark International, Inc since 1997. In August 2012, Lexmark announced that it would be ceasing production of its inkjet printer line. In April 2013, Funai announced that it had signed an agreement to acquire Lexmark's inkjet-related technology and assets for approximately $100 million (approximately ¥ 9.5 billion).\n\nFunai acquired more than 1,500 inkjet patents, Lexmark's inkjet-related research and development assets and tools, all outstanding shares and the manufacturing facility of Lexmark International (Philippines), Inc., and other inkjet-related technologies and assets. Through this transaction, Funai acquired the capabilities to develop, manufacture and sell inkjet hardware as well as inkjet supplies.\n\nThe decline of VHS videotape began with the introduction to market of the DVD format in 1997. Funai continued to manufacture VHS tape recorders into the early part of 21st century, mostly under the Sanyo brand in China and North America. In July 2016, Funai ceased production of VHS equipment, the last known company in the world to do so, after poor sales of its last VCR/DVD player combos.\n\n\n"}
{"id": "43387223", "url": "https://en.wikipedia.org/wiki?curid=43387223", "title": "Geoffrey G. Eichholz", "text": "Geoffrey G. Eichholz\n\nGeoffrey Gunther Eichholz, (June 29, 1920 – January 8, 2018) an educational leader in health physics at the Georgia Institute of Technology. Dr. Eichholz played a key role in the successful establishment of the Department of Nuclear Engineering and Health Physics. The Department has been a constant source of well-educated and well trained graduates in the field of nuclear engineering, health physics and medical physics. Professor Eichholz was involved at all levels of the educational ladder including leadership roles and participation in doctoral and masters committees.\n\nEichholz was born on 29 June 1920 in Hamburg, Germany to Max Eichholz and Adele \"Daisy\" née Elias. Dr. Max Eichholz was a lawyer and senior member of the Hamburg City Parliament and a well-known opponent of the Nazi party. As a result, after 1933, his father was severely persecuted and arrested multiple times. In 1943, Dr. Max Eichholz died at Auschwitz.\n\nAt the age of 18 during Kristallnacht the Nazis expelled him from Berlin Technical University and he was forced to leave his home country. Relatives in England offered him a place to stay.\n\nIn 1938, Eichholz graduated from the Johanneum High School. He attended the Berlin Technical University for two partial semesters that ended on 9 November 1938 during Krystallnacht. He was expelled from the University and was fortunate to escape arrest after witnessing the destruction and burning of synagogues. Early in 1939 Eichholz was awarded a Refugee Scholarship from Harvard University effective in the following Fall semester. With this information, he was able to secure a transit visa to Great Britain where he could stay with relatives.\n\nIn March 1939, Eichholz arrived in England without any money to his name. He obtained a position as an unpaid research assistant in the physics department at Bristol University. \nA distant relative, David Eichholz, Professor in Classics at the University of Bristol set young Eichholz up with a position as an unpaid researcher for Professor Arthur Mannering Tyndall, Department of Physics. Eichholz worked for Cecil F. Powell scanning photographic emulsions. He worked alongside another refugee, Harald Rossi. In 1950, Powell would go on to be awarded the Nobel Prize in Physics for his discovery of pi mesons in these very same photographic emulsions.\n\nIn August 1939, war in Europe erupted and Eichholz was called into duty as a firewatch on top of the Physics tower. He was formally recognized as a Victim of Nazi Oppression. Due to the outbreak of war, the U.S. Consulate General refused to grant Eichholz a student visa. At this point Professor Tyndall arranged for Eichholz to be admitted to Bristol as a first-year honours student with tuition and fees waived. Eichholz was formally matriculated by Chancellor Winston Churchill. At Bristol, as an undergraduate student Eichholz had on occasion interacted with noteworthy scientists including: Walter Heitler, Herbert Fröhlich, Kurt Hoselitz and Hans Heitler. Finally in February 1940, Eichholz was reunited with his mother who had escaped from Germany one week before the War.\n\nIn May 1940, after the Battle of Dunkirk, all male refugee enemy aliens were interned. Eichholz spent the summer under canvas and was eventually transferred to the Isle of Man. In December 1940, he was released and rejoined his mother, now in Bradford, Yorkshire. A recommendation from Tyndall and an interview with Professor Edmund Clifton Stoner at the University of Leeds garnered Eichholz admittance to Leeds as a second year physics student in the midst of an accelerated wartime curriculum. In May 1942, he graduated with First-Class Honours in physics. James Chadwick was the external examiner. Eichholz was assigned to the Admiralty Signal Establishment, Witley, Surrey for work on radar development. The assignment involved work with microwaves and waveguides, state of the art technology at the time.\n\nAt the end of World War II, in January 1946, Eichholz returned to his studies at Leeds to complete his PhD. \nProfessor Stoner suggested the topic of Magnetic Resonances at Microwave Frequencies. The thesis in physics was completed in September 1947, passed the oral examination and scrutiny by the external examiner in Sheffield and the oral presentation was completed just prior to departing via ship to Canada.\n\nIn early 1947, Eichholz had replied to an advertisement in Nature for a faculty position at the University of British Columbia (UBC) and was offered the position of assistant professor, contingent upon completion of his PhD and becoming a British citizen. The conditions were met and so he began a long trip to Vancouver, with 5 days at sea, followed by 6 days by train across Canada.\n\nEichholz experienced culture shock as he encountered the change from wartime England to the immensity of Canada. At UBC Eichholz was assigned teaching duties with returning veterans as his primary student group. He had to establish laboratories and initiate research projects in nuclear physics. He became involved with ionizing radiation and radiation detection. A productive summer was spent at the AECL Chalk River Laboratories, and he witnessed the startup of the NRX Reactor. During his time off, he enjoyed sailing on the Ottawa River with W.B. Lewis. Lewis was known as the father of the CANDU nuclear reactor.\n\nIn 1951, Eichholz moved to Ottawa and accepted a position with the Canadian Bureau of Mines, as head of the Physics and Radiotracer Subdivision. The work focused on uranium assays of ores and minerals. Later projects involved development of novel radiation detectors and the utilization of radioisotopes in industry. Eichholz developed the first multiwire spark counter. Additional studies involved neutron activation analysis to determine the oxygen content in steel and the use of radiotracers in explosives and the steel industry.\n\nIn 1963, Eichholz was recruited by the Georgia Institute of Technology (Georgia Tech) for the position of Professor in the newly established graduate program in the School of Nuclear Engineering. He remained there for 25 years. At the time the program did not offer an undergraduate degree in nuclear engineering. By the end of the 1960s, a significant program in Health Physics was added. Eichholz developed additional courses for these programs that added to the breadth and depth of these specialties. He published or coauthored several books for these courses and they include: Radioisotope Engineering, Environmental Aspects of Nuclear Power, Nuclear Radiation Detection, and Radon. In 1975 Eichholz was named Regents’ Professor of Nuclear Engineering.\n\nWhile at Georgia Tech, Eichholz maintained an active research program that encompassed laboratories in 4 different building on campus. Research focus areas included radiotracer utilization, irradiation effects, radon, and dosimetry. He devoted a great deal of effort to studies on the migration of radioactive material and pollutants through unsaturated soils.\n\nIn a somewhat unrelated field, he also taught a course on Architectural Acoustics in the College of Architecture for over 20 years.\n\nPastimes include tennis, world affairs and historical genealogy. Lectured at Mercer Senior University on world affairs and historical genealogy. Dr. Eichholz enjoys world travel and has visited over 100 countries.\n\n\n\n\n\n\n"}
{"id": "319341", "url": "https://en.wikipedia.org/wiki?curid=319341", "title": "Guidance system", "text": "Guidance system\n\nA guidance system is a virtual or physical device, or a group of devices implementing a guidance process used for controlling the movement of a ship, aircraft, missile, rocket, satellite, or any other moving object. Guidance is the process of calculating the changes in position, velocity, attitude, and/or rotation rates of a moving object required to follow a certain trajectory and/or attitude profile based on information about the object's state of motion.\n\nA guidance system is usually part of a Guidance, navigation and control system, whereas navigation refers to the systems necessary to calculate the current position and orientation based on sensor data like those from compasses, GPS receivers, Loran-C, star trackers, inertial measurement units, altimeters, etc. The output of the navigation system, the navigation solution, is an input for the guidance system, among others like the environmental conditions (wind, water, temperature, etc.) and the vehicle's characteristics (i.e. mass, control system availability, control systems correlation to vector change, etc.). In general, the guidance system computes the instructions for the control system, which comprises the object's actuators (e.g., thrusters, reaction wheels, body flaps, etc.), which are able to manipulate the flight path and orientation of the object without direct or continuous human control.\n\nOne of the earliest examples of a true guidance system is that used in the German V-1 during World War II. The navigation system consisted of a simple gyroscope, an airspeed sensor, and an altimeter. The guidance instructions were target altitude, target velocity, cruise time, and engine cut off time.\n\nA guidance system has three major sub-sections: Inputs, Processing, and Outputs. The input section includes sensors, course data, radio and satellite links, and other information sources. The processing section, composed of one or more CPUs, integrates this data and determines what actions, if any, are necessary to maintain or achieve a proper heading. This is then fed to the outputs which can directly affect the system's course. The outputs may control speed by interacting with devices such as turbines, and fuel pumps, or they may more directly alter course by actuating ailerons, rudders, or other devices.\n\nInertial guidance systems were originally developed for rockets. American rocket pioneer Robert Goddard experimented with rudimentary gyroscopic systems. Dr. Goddard's systems were of great interest to contemporary German pioneers including Wernher von Braun. The systems entered more widespread use with the advent of spacecraft, guided missiles, and commercial airliners.\n\nUS guidance history centers around 2 distinct communities. One driven out of Caltech and NASA Jet Propulsion Laboratory, the other from the German scientists that developed the early V2 rocket guidance and MIT. The GN&C system for V2 provided many innovations and was the most sophisticated military weapon in 1942 using self-contained closed loop guidance. Early V2s leveraged 2 gyroscopes and lateral accelerometer with a simple analog computer to adjust the azimuth for the rocket in flight. Analog computer signals were used to drive 4 external rudders on the tail fins for flight control. Von Braun engineered the surrender of 500 of his top rocket scientists, along with plans and test vehicles, to the Americans. They arrived in Fort Bliss, Texas in 1945 and were subsequently moved to Huntsville, Al in 1950 (aka Redstone arsenal). Von Braun's passion was interplanetary space flight. However his tremendous leadership skills and experience with the V-2 program made him invaluable to the US military. In 1955 the Redstone team was selected to put America's first satellite into orbit putting this group at the center of both military and commercial space.\n\nThe Jet Propulsion Laboratory traces its history from the 1930s, when Caltech professor Theodore von Karman conducted pioneering work in rocket propulsion. Funded by Army Ordnance in 1942, JPL's early efforts would eventually involve technologies beyond those of aerodynamics and propellant chemistry. The result of the Army Ordnance effort was JPL's answer to the German V-2 missile, named MGM-5 Corporal, first launched in May 1947. On December 3, 1958, two months after the National Aeronautics and Space Administration (NASA) was created by Congress, JPL was transferred from Army jurisdiction to that of this new civilian space agency. This shift was due to the creation of a military focused group derived from the German V2 team. Hence, beginning in 1958, NASA JPL and the Caltech crew became focused primarily on unmanned flight and shifted away from military applications with a few exceptions. The community surrounding JPL drove tremendous innovation in telecommunication, interplanetary exploration and earth monitoring (among other areas).\n\nIn the early 1950s, the US government wanted to insulate itself against over dependency on the Germany team for military applications. Among the areas that were domestically \"developed\" was missile guidance. In the early 1950s the MIT Instrumentation Laboratory (later to become the Charles Stark Draper Laboratory, Inc.) was chosen by the Air Force Western Development Division to provide a self-contained guidance system backup to Convair in San Diego for the new Atlas intercontinental ballistic missile. The technical monitor for the MIT task was a young engineer named Jim Fletcher who later served as the NASA Administrator. The Atlas guidance system was to be a combination of an on-board autonomous system, and a ground-based tracking and command system. This was the beginning of a philosophic controversy, which, in some areas, remains unresolved. The self-contained system finally prevailed in ballistic missile applications for obvious reasons. In space exploration, a mixture of the two remains.\n\nIn the summer of 1952, Dr. Richard Battin and Dr. J. Halcombe (\"Hal\") Laning Jr., researched computational based solutions to guidance as computing began to step out of the analog approach. As computers of that time were very slow (and missiles very fast) it was extremely important to develop programs that were very efficient. Dr. J. Halcombe Laning, with the help of Phil Hankins and Charlie Werner, initiated work on MAC, an algebraic programming language for the IBM 650, which was completed by early spring of 1958. MAC became the work-horse of the MIT lab. MAC is an extremely readable language having a three-line format, vector-matrix notations and mnemonic and indexed subscripts. Today's Space Shuttle (STS) language called HAL, (developed by Intermetrics, Inc.) is a direct offshoot of MAC. Since the principal architect of HAL was Jim Miller, who co-authored with Hal Laning a report on the MAC system, it is a reasonable speculation that the space shuttle language is named for Jim's old mentor, and not, as some have suggested, for the electronic superstar of the Arthur Clarke movie \"2001-A Space Odyssey.\" (Richard Battin, AIAA 82-4075, April 1982)\n\nHal Laning and Richard Battin undertook the initial analytical work on the Atlas inertial guidance in 1954. Other key figures at Convair were Charlie Bossart, the Chief Engineer, and Walter Schweidetzky, head of the guidance group. Walter had worked with Wernher von Braun at Peenemuende during World War II.\n\nThe initial \"Delta\" guidance system assessed the difference in position from a reference trajectory. A velocity to be gained (VGO) calculation is made to correct the current trajectory with the objective of driving VGO to Zero. The mathematics of this approach were fundamentally valid, but dropped because of the challenges in accurate inertial navigation (e.g. IMU Accuracy) and analog computing power. The challenges faced by the \"Delta\" efforts were overcome by the \"Q system\" of guidance. The \"Q\" system's revolution was to bind the challenges of missile guidance (and associated equations of motion) in the matrix Q. The Q matrix represents the partial derivatives of the velocity with respect to the position vector. A key feature of this approach allowed for the components of the vector cross product (v, xdv,/dt) to be used as the basic autopilot rate signals-a technique that became known as \"cross-product steering.\" The Q-system was presented at the first Technical Symposium on Ballistic Missiles held at the Ramo-Wooldridge Corporation in Los Angeles on June 21 and 22, 1956. The \"Q System\" was classified information through the 1960s. Derivations of this guidance are used for today's military missiles. The CSDL team remains a leader in the military guidance and is involved in projects for most divisions of the US military.\n\nOn August 10 of 1961 NASA Awarded MIT a contract for preliminary design study of a guidance and navigation system for Apollo program. (see Apollo on-board guidance, navigation, and control system, Dave Hoag, International Space Hall of Fame Dedication Conference in Alamogordo, N.M., October 1976 ). Today's space shuttle guidance is named PEG4 (Powered Explicit Guidance). It takes into account both the Q system and the predictor-corrector attributes of the original \"Delta\" System (PEG Guidance). Although many updates to the shuttles navigation system have taken place over the last 30 years (ex. GPS in the OI-22 build), the guidance core of today's Shuttle GN&C system has evolved little. Within a manned system, there is a human interface needed for the guidance system. As Astronauts are the customer for the system, many new teams are formed that touch GN&C as it is a primary interface to \"fly\" the vehicle. For the Apollo and STS (Shuttle system) CSDL \"designed\" the guidance, McDonnell Douglas wrote the requirements and IBM programmed the requirements.\n\nMuch system complexity within manned systems is driven by \"redundancy management\" and the support of multiple \"abort\" scenarios that provide for crew safety. Manned US Lunar and Interplanetary guidance systems leverage many of the same guidance innovations (described above) developed in the 1950s. So while the core mathematical construct of guidance has remained fairly constant, the facilities surrounding GN&C continue to evolve to support new vehicles, new missions and new hardware. The center of excellence for the manned guidance remains at MIT (CSDL) as well as the former McDonnell Douglas Space Systems (in Houston).\n\nGuidance systems consist of 3 essential parts: navigation which tracks current location, guidance which leverages navigation data and target information to direct flight control \"where to go\", and control which accepts guidance commands to effect change in aerodynamic and/or engine controls.\n\nNavigation is the art of determining where you are, a science that has seen tremendous focus in 1711 with the Longitude prize. Navigation aids either measure position from a \"fixed\" point of reference (ex. landmark, north star, LORAN Beacon), \"relative\" position to a target (ex. radar, infra-red, ...) or track \"movement\" from a known position/starting point (e.g. IMU). Today's complex systems use multiple approaches to determine current position. For example, today's most advanced navigation systems are embodied within the Anti-ballistic missile, the RIM-161 Standard Missile 3 leverages GPS, IMU and ground segment data in the boost phase and relative position data for intercept targeting. Complex systems typically have multiple redundancy to address drift, improve accuracy (ex. relative to a target) and address isolated system failure. Navigation systems therefore take multiple inputs from many different sensors, both internal to the system and/or external (ex. ground based update). Kalman filter provides the most common approach to combining navigation data (from multiple sensors) to resolve current position. Example navigation approaches:\n\n\n\nGuidance is the \"driver\" of a vehicle. It takes input from the navigation system (where am I) and uses targeting information (where do I want to go) to send signals to the flight control system that will allow the vehicle to reach its destination (within the operating constraints of the vehicle). The \"targets\" for guidance systems are one or more state vectors (position and velocity) and can be inertial or relative. During powered flight, guidance is continually calculating steering directions for flight control. For example, the space shuttle targets an altitude, velocity vector, and gamma to drive main engine cut off. Similarly, an Intercontinental ballistic missile also targets a vector. The target vectors are developed to fulfill the mission and can be preplanned or dynamically created.\n\nControl. Flight control is accomplished either aerodynamically or through powered controls such as engines. Guidance sends signals to flight control. A Digital Autopilot (DAP) is the interface between guidance and control. Guidance and the DAP are responsible for calculating the precise instruction for each flight control. The DAP provides feedback to guidance on the state of flight controls.\n\n\n"}
{"id": "49011770", "url": "https://en.wikipedia.org/wiki?curid=49011770", "title": "HPUE", "text": "HPUE\n\nHigh Power User Equipment (HPUE) is a special class of user equipment for the LTE cellular network.\n\nIn Release 11 of the LTE standard, 3GPP proposed so called High Power User Equipment (HPUE) for band 14 (700 MHz). While current off-the-shelf UE is only allowed to transmit at a maximum output power of 23 dBm, the HPUE are allowed to transmit with an output power of up to 31 dBm. Since the transmission range depends on the transmit power, the cell range can be increased by nearly 80% and user devices can operate better in challenging RF environments, such as inside large concrete buildings.\n\nThe first HPUE has been presented by the Finnish company Bittium Oyj formerly known as Elektrobit, together with Rohde & Schwarz at Mobile World Congress 2014. They implemented the test cases of 3GPP, to show that the use of HPUE does indeed not interfere with adjacent commercial LTE networks.\n\nThe first portable, battery-operated device to operate with HPUE capabilities in actual field operation is the HPUE Band 14 Personal Gateway device by Assured Wireless Corporation in early 2017. This device is fully compliant with 3GPP specifications for HPUE and operates at the full 31dBm maximum transmit power levels. Tests on networks around the U.S. and, specifically, actual public safety user experience at the 2017 Houston Livestock Show and Rodeo showed the ability of this HPUE equipment to operate with any LTE Band 14 network with reliable performance that stays connected in areas where all existing standard-power LTE devices fail to connect, while also improving user data rates even in areas of good coverage and increasing overall cell capacity.\n"}
{"id": "2680215", "url": "https://en.wikipedia.org/wiki?curid=2680215", "title": "Independent hardware vendor", "text": "Independent hardware vendor\n\nAn independent hardware vendor (IHV) is a company specializing in making or selling computer hardware, usually for niche markets.\n\n"}
{"id": "2625019", "url": "https://en.wikipedia.org/wiki?curid=2625019", "title": "International Open Source Network", "text": "International Open Source Network\n\nThe International Open Source Network has as its slogan \"software freedom for all\". It is a Centre of Excellence for free software (also known as FLOSS, FOSS, or open-source software) in the Asia-Pacific region.\n\nIOSN says it \"shapes its activities around FOSS technologies and applications. It is \"tasked specifically to facilitate and network FOSS advocates and human resources in the region.\"\n\nIOSN's website says: \"FOSS presents itself as an access solution for developing countries. It represents an opportunity for these countries to adopt affordable software and solutions towards bridging the digital divide. Only the use of FOSS permits sustainable development of software; it is technology that is free to learn about, maintain, adapt and reapply\".\n\nIt explains its emphasis on Free and Open Source Software for the following reasons:\n\nBeginning 2008, IOSN is now managed from three centers of excellence: University of the Philippines Manila (ASEAN+3), CDAC in Chennai, India (South Asia), and a consortium composed of members from the academe and government in the Pacific Island Countries (PIC).\n\nIOSN's objectives include:\n\nIOSN is an initiative of the UNDP Asia-Pacific Development Information Programme and is supported by the International Development Research Centre (IDRC).\n\n"}
{"id": "22113839", "url": "https://en.wikipedia.org/wiki?curid=22113839", "title": "Johann Christian Ritter", "text": "Johann Christian Ritter\n\nJohann Christian Ritter (25 July 1755 – 9 September 1810) was a German in the service of the Dutch East India Company who came to South Africa in 1784. He was the first to print in the Cape, the earliest record is an almanac titled \"Almanach voor't jaar 1796\".\n\nJ.C. Ritter was born in 1755 either in Bayreuth or Hof an der Saale, these are cities less than 50 km distant from each other in Germany and may have referred to the same place. He was the Son of the book binder Georg Stephan Ritter and his wife Johanna Dorothea (née. Leidenforst). He married Barbara Fuhrmann of Danzig and they had no children and she died after him in 1813-6-9.\n\nHe was in the service of the Dutch East India Company when he arrived at the Cape Colony in the later half of 1784 and was appointed by them in 1794 to print government notices, forms and so forth.\n\nIn the 10 years before he started to print he bound books, he may have been a book binder by trade when he arrived at the Cape though it may be that he took on this work through familiarity, having been the son of a bookbinder.\n\nHe died at the age of 55.\n\nIt is not certain if he brought the small hand-press with him or if the press only arrived later when he prepared for printing or was appointed to the task.\n\nHe undertook to print official forms and handbills as well as almanacs. The surviving (1796) almanac was printed in 1795. As he also printed almanacs for at least the years 1795 and 1797, so he would likely have started his printing in 1794 or before but none of his earlier work has been discovered. His almanac did not bring him any great financial reward, having been used by many to copy out of by hand, notwithstanding some factual errors relating to dates of lunar eclipses. Only one copy of the first leaf is known to exist in the Sir George Grey Collection of the South African Library and this one may be a proof sheet.\n\nSoon after 1797 when the British took occupation of the Cape, permission to print was transferred to Messrs Walker and Robertson who had questionable ties with the briefly serving governor Sir George Yonge even though Ritter and Harry Harwood Smith, a printing contemporary of his, had petitioned for the printing rights.\n\nRitter's press may also have been used in 1799 by V.A. Schoonberg for printing the first book in South Africa which was also the first printed religious text. It was a translation, into Dutch, of a letter brought to the Cape, and published, by J.T. van der Kemp of the London Missionary Society. It was used to print a military proclamation in 1799 as it was the only available press.\n\nGlobal spread of the printing press – to see how printing spread to the European colonies, often political or religious limitations restricted the spread of printing faster than the public might have desired.\n\n\n"}
{"id": "29987638", "url": "https://en.wikipedia.org/wiki?curid=29987638", "title": "Lead picrate", "text": "Lead picrate\n\nLead picrate, Pb(CH(NO)O), is an organic chemical compound from the group of picrates, salt of picric acid and lead with a +2 oxidation state. It is an initiating explosive, and thus highly sensitive.\n\nFor demonstration purposes, it can be most easily and safely prepared \"in situ\". Picric acid is mixed with red lead (PbO), and then heated. It is not recommended that more than 50 mg is ever made at any one time. At around 130–160 °C, as the picric acid melts, the mixture detonates violently.\n"}
{"id": "41073456", "url": "https://en.wikipedia.org/wiki?curid=41073456", "title": "Lisa Brummel", "text": "Lisa Brummel\n\nLisa E. Brummel (born 1959/1960) was Executive Vice President, Human Resources for Microsoft Corporation and retired December 31, 2014. She previously served as corporate vice president of the company's Home & Retail Division.\n\nBrummel was raised in Connecticut. She earned a bachelor's degree in sociology from Yale University in 1981, followed by a master's degree in business administration from the University of California, Los Angeles. She joined Microsoft in 1989.\n\nShe is a co-owner of the Seattle Storm, a professional women's basketball team in the WNBA.\n"}
{"id": "8528827", "url": "https://en.wikipedia.org/wiki?curid=8528827", "title": "Llandarcy oil refinery", "text": "Llandarcy oil refinery\n\nThe Llandarcy Oil Refinery was the UK's first oil refinery in 1922, owned by BP. Previously to this refinery, the only oil refined in the UK came from Scottish shale. \n\nIt was known as the National Oil Refinery and BP Llandarcy. BP would later have its Kent Refinery and Grangemouth Refinery. \n\nConstruction began in February 1919, with a new railway line; it cost £3m. The refinery was formally opened on 29 June 1922 by Stanley Baldwin, President of the Board of Trade.\n\nIn 1961 a new oil terminal was built by BP at Angle Bay in Pembrokeshire.\n\nWhen opened it was making around 150,000 gallons of petrol a day. In 1960 it was refining 8 million tons of crude oil a year. It was the third biggest oil refinery in the UK after Fawley Refinery and Lindsey Oil Refinery.\n\nIt closed in 1998. The site was demolished in October 2009.\n\nThe site of the refinery was off the Llandarcy Interchange of the present-day M4, near the B4290 and Skewen. The area is known as Coedffranc. To the south, off the A483, is Crymlyn Burrows.\n\nThe site covered 650 acres.\n\n"}
{"id": "32595505", "url": "https://en.wikipedia.org/wiki?curid=32595505", "title": "Locale (computer hardware)", "text": "Locale (computer hardware)\n\nIn computer architecture a locale is an abstraction of the concept of a localized set of hardware resources which are close enough to enjoy uniform memory access.\n\nFor instance, on a computer cluster each node may be considered a locale given that there is one instance of the operating system and uniform access to memory for processes running on that node. Similarly, on an SMP system, each node may be defined as a locale. Parallel programming languages such as Chapel have specific constructs for declaring locales.\n\n"}
{"id": "43083432", "url": "https://en.wikipedia.org/wiki?curid=43083432", "title": "Magnetoquasistatic field", "text": "Magnetoquasistatic field\n\nA magnetoquasistatic field is a class of electromagnetic field in which a slowly oscillating magnetic field is dominant. A magnetoquasistatic field is typically generated by \"low-frequency\" induction from a magnetic dipole or a current loop. The magnetic near-field of such an emitter behaves differently from the more commonly used far-field electromagnetic radiation. At low frequencies the rate of change of the instantaneous field strength with each cycle is relatively slow, giving rise to the name \"magneto-quasistatic\". The near field or quasistatic region typically extends no more than a wavelength from the antenna, and within this region the electric and magnetic fields are approximately decoupled.\n\nWeakly conducting non-magnetic bodies, including the human body and many mineral rocks, are effectively transparent to magnetoquasistatic fields, allowing for the transmission and reception of signals through such obstacles. Also, long-wavelength (i.e. low-frequency) signals are better able to propagate round corners than shorter-wave signals. Communication therefore need not be line-of-sight.\n\nThe communication range of such signals depends on both the wavelength and the electromagnetic properties of the intervening medium at the chosen frequency, and is typically limited to a few tens of meters.\n\nThe laws of primary interest are Ampère's circuital law (with the displacement current density neglected) and the magnetic flux continuity law. These laws have associated with them continuity conditions at interfaces. In the absence of magnetizable materials, these laws determine the magnetic field intensity H given its source, the current density J. H is not everywhere irrotational. However, it is solenoidal everywhere.\n\nA typical antenna comprises a 50-turn coil around a polyoxymehtylene tube with diameter 16.5 cm, driven by a class E oscillator circuit. Such a device is readily portable when powered by batteries. Similarly, a typical receiver consist of an active receiving loop with diameter of one meter, an ultra-low-noise amplifier, and a band-pass filter.\n\nIn operation the oscillator drives current through the transmitting loop to create an oscillating magnetic field. This field induces a voltage in the receiving loop, which is then amplified.\n\nBecause the quasistatic region is defined within one wavelength of the electromagnetic source, emitters are limited to a frequency range between about 1 kHz and 1 MHz. Reducing the oscillating frequency increases the wavelength and hence the range of the quasistatic region, but reduces the induced voltage in the receiving loops which worsens the signal-to-noise ratio. In experiments carried out by the Carnegie Institute of Technology, the maximum range reported by was 50 meters.\n\nIn resonant coupling, the source and receiver are tuned to resonate at the same frequency and are given similar impedances. This allows power as well as information to flow from the source to the receiver. Such coupling via the magnetoquasistatic field is called resonant inductive coupling and can be used for wireless energy transfer.\n\nApplications include induction cooking, induction charging of batteries and some kinds of RFID tag.\n\nConventional electromagnetic communication signals cannot pass through the ground. Most mineral rock is neither electrically conducting nor magnetic, allowing magnetic fields to penetrate. Magnetoquasistatic systems have been successfully used for underground wireless communication, both surface-to-underground and between underground parties.\n\nAt extremely low frequencies, below about 1 kHz, the wavelength is long enough for long-distance communication, although at a slow data rate. Such systems have been installed in submarines, with the local antenna comprising a wire up to several kilometers in length and trailed behind the vessel when at or near the surface.\n\nWireless position tracking is being increasingly used in applications such as navigation, security, and asset tracking. Conventional position tracking devices use high frequencies or microwaves, including global positioning systems (GPS), ultra-wide band (UWB) systems, and radio frequency identification systems (RFID), but these systems can easily be blocked by obstacles in their path. Magnetoquasistatic positioning takes advantage of the fact that the fields are largely undisturbed when in the presence of human beings and physical structures, and can be used for both position and orientation tracking for ranges up to 50 meters.\n\nTo accurately determine the orientation and position of a dipole/emitter, allowance must be made not only for the field pattern generated by the emitter, but also for the eddy-currents they induce in the earth, which create secondary fields detectable by the receivers. By using complex image theory to correct this field generation from earth, and by using frequencies on the order of a few hundred kilohertz to obtain the required signal-to-noise ratio (SNR), it is possible to analyze the position of the dipole through azimuthal orientation, formula_1, and inclination orientation, formula_2.\n\nA Disney research team has used this technology to effectively determine the position and orientation of an American football, something not traceable through conventional wave propagation techniques due to human body obstruction. They inserted an oscillator-driven coil, around the diameter of the center of the ball, to generate the magnetoquasistatic field. The signal was able to pass undisturbed through multiple players.\n\n"}
{"id": "2296421", "url": "https://en.wikipedia.org/wiki?curid=2296421", "title": "Morris &amp; Co.", "text": "Morris &amp; Co.\n\nMorris, Marshall, Faulkner & Co. (1861–1875) was a furnishings and decorative arts manufacturer and retailer founded by the artist and designer William Morris with friends from the Pre-Raphaelites. With its successor Morris & Co. (1875–1940) the firm's medieval-inspired aesthetic and respect for hand-craftsmanship and traditional textile arts had a profound influence on the decoration of churches and houses into the early 20th century.\n\nAlthough its most influential period was during the flourishing of the Arts and Crafts Movement in the 1880s and 1890s, Morris & Co. remained in operation in a limited fashion from World War I until its closure in 1940. The firm's designs are still sold today under licences given to Sanderson and Sons part of the Walker Greenbank wallpaper and fabrics business (which owns the \"Morris & Co.\" brand) and Liberty of London.\n\nMorris, Marshall, Faulkner & Co., \"Fine Art Workmen in Painting, Carving, Furniture and the Metals,\" was jointly created by Morris, Ford Madox Brown, Edward Burne-Jones, Charles Faulkner, Dante Gabriel Rossetti, P. P. Marshall, and Philip Webb in 1861 to create and sell medieval-inspired, handcrafted items for the home. The prospectus set forth that the firm would undertake carving, stained glass, metal-work, paper-hangings, chintzes (printed fabrics), and carpets. The first headquarters of the firm were at 8 Red Lion Square in London.\n\nThe work shown by the firm at the 1862 International Exhibition attracted much notice, and within a few years it was flourishing. In the autumn of 1864 a severe illness obliged Morris to choose between giving up his home at Red House in Kent and giving up his work in London. With great reluctance he gave up Red House, and in 1865 established himself under the same roof with his workshops, now relocated to larger premises in Queen Square, Bloomsbury.\n\nThe decoration of churches was from the first an important part of the business. A great wave of church-building and remodelling by the Church of England in the 1840s and 1850s increased the demand for ecclesiastical decoration of all kinds, especially stained glass. But this market shrank in the general depression of the later 1860s, and the firm increasingly turned to secular commissions. On its non-ecclesiastical side, the product line was extended to include, besides painted windows and mural decoration, furniture, metal and glass wares, cloth and paper wall-hangings, embroideries, jewellery, woven and knotted carpets, silk damasks, and tapestries.\n\nMorris was producing repeating patterns for wallpaper as early as 1862, and some six years later he designed his first pattern specifically for fabric printing. As in so many other areas that interested him, Morris chose to work with the ancient technique of hand woodblock printing in preference to the roller printing which had almost completely replaced it for commercial uses.\n\nIn August 1874, Morris determined to restructure the partnership, generating a dispute with Marshall, Rossetti, and Madox Brown over the return on their shares. The company was dissolved and reorganized under Morris's sole ownership as Morris & Co. on 31 March 1875.\n\nDuring these years, Morris took up the practical art of dyeing as a necessary adjunct of his manufacturing business. He spent much of his time at the Staffordshire dye works of Thomas Wardle, mastering the processes of that art and making experiments in the revival of old or discovery of new methods. One result of these experiments was to reinstate indigo dyeing as a practical industry, and generally to renew the use of those vegetable dyes, like madder, which had been driven almost out of use by the anilines.\n\nDyeing of wools, silks, and cottons was the necessary preliminary to what he had much at heart, the production of woven and printed fabrics of the highest excellence; and the period of incessant work at the dye-vat (1875–76) was followed by a period during which he was absorbed in the production of textiles (1877–78), and more especially in the revival of carpet-weaving as a fine art.\nIn June 1881, Morris relocated his dyeworks from Queen Square to an early eighteenth-century silk-throwing works at Merton Abbey Mills, after determining that the water of the River Wandle was suitable for dyeing. The complex, on , included several buildings and a dyeworks, and the various buildings were soon adapted for stained-glass, textile printing, and fabric- and carpet-weaving.\n\nIn 1879, Morris had taught himself tapestry weaving in the medieval style and set up a tapestry workshop with his apprentice John Henry Dearle at Queen Square. Dearle executed Morris and Co.'s first figural tapestry from a design by Walter Crane in 1883. Dearle was soon responsible for the training of all tapestry apprentices in the expanded workshop at Merton Abbey, and partnered with Morris on designing details such as fabric patterns and floral backgrounds for tapestries based on figure drawings or \"cartoons\" by Burne-Jones (some of them repurposed from stained glass cartoons). and animal figures by Philip Webb. Suites of tapestries were made as part of whole-house decorating schemes, and tapestries of Burne-Jones angels and scenes from the Arthurian legends were a staple of Morris & Co. into the twentieth century.\n\nTwo significant secular commissions helped establish the firm's reputation in the late 1860s: a royal project at St. James's Palace and the \"green dining room\" at the South Kensington Museum (now the Victoria and Albert) of 1867. The green dining room (preserved as the Morris Room at the V&A) featured stained glass windows and panel figures by Burne-Jones, panels with branches of fruit or flowers by Morris, and olive branches and a frieze by Philip Webb. The St. James's commission comprised decorative schemes for the Armoury and the Tapestry Room, and included panels of stylized floral patterns painted on ceilings, cornices, dados, windows, and doors.\n\nIn 1871 Morris & Co. were responsible for the windows at All Saints church in the village of Wilden near to Stourport-on-Severn. They were designed by Edward Burne-Jones for Alfred Baldwin, his wife's brother-in-law.\n\nStanden near East Grinstead, West Sussex, was designed between 1892 and 1894 by Philip Webb for a prosperous London solicitor, James Beale, his wife Margaret, and their family. It is decorated with Morris carpets, fabrics and wallpapers.\n\nStanmore Hall was the last major decorating commission executed by Morris & Co. before Morris's death in 1896. It was also the most extensive commission undertaken by the firm, and included a series of tapestries based on the story of the Holy Grail for the dining room, to which Morris devoted his energies, the rest of the work being executed under the direction of Dearle.\n\nOther Morris & Co. commissions include the ceiling within the dining room of Charleville Forest Castle, Ireland, interiors of Bullers Wood House, now Bullers Wood School in Chislehurst, Kent, and stained glass windows at Adcote.\n\nAs Morris pursued other interests, notably socialism and the Kelmscott Press, day-to-day work at the firm was delegated. Morris's daughter May became the director of the embroidery department in 1885, when she was in her early twenties. Dearle, who had begun designing repeating patterns for wallpapers and textiles in the late 1880s, was head designer for the firm by 1890, handling interior design commissions and supervising the tapestry, weaving, and fabric-printing departments at Merton Abbey.\n\nDearle's contributions to textile design were long overshadowed by Morris. Dearle exhibited his designs under the Morris name rather than his own in the Arts and Crafts Exhibitions and the major Morris retrospective of 1899, and even today many Dearle designs are popularly offered as \"William Morris\" patterns.\n\nOn Morris's death in 1896, Dearle became Art Director of the firm, which changed its name again to Morris & Co. Decorators Ltd. in 1905. Dearle managed the company's textile works at Merton Abbey until his own death in 1932. The firm was finally dissolved in the early months of World War II.\n\nMorris & Co. repeating patterns were occasionally offered as both block-printed wallpapers and fabric during Morris's lifetime; many of the patterns still available are offered in both forms by their current manufacturers.\n\n\n"}
{"id": "37771398", "url": "https://en.wikipedia.org/wiki?curid=37771398", "title": "NRF51 series", "text": "NRF51 series\n\nThe nRF51 Series SoCs are a family of ultra low-power wireless SoCs from Nordic Semiconductor. The nRF51 series are designed to enable a wide range of wireless embedded systems and consumer electronic products in many different fields of wireless connectivity including wearable devices, computer peripherals, mobile phone accessories, security devices and sensor applications. The nRF51 series devices support a range of ultra low-power wireless communication protocols including: Bluetooth low energy, ANT, ANT+ and 2.4 GHz proprietary protocols.\n\nThe nRF51 series devices employ Nordic Semiconductor's 3rd generation 2.4 GHz radio architecture. This radio uses narrow-band GFSK modulation and is frequency adaptable across the 2.4 GHz ISM band.\n\n"}
{"id": "36733660", "url": "https://en.wikipedia.org/wiki?curid=36733660", "title": "Oil mist lubrication", "text": "Oil mist lubrication\n\nOil mist lubrication oils are applied to rolling element (antifriction) bearings as an oil mist. Neither oil rings nor constant level lubricators are used in pumps and drivers connected to plant-wide oil mist systems. Oil mist is an atomized amount of oil carried or suspended in a volume of pressurized dry air. The oil mist, actually a ratio of one volume of oil suspended or carried in 200,000 volumes of clean, dry air, moves in a piping system (header). The point of origin is usually a mixing valve (the oil mist generator), connected to this header. Branch lines often feed oil mist to hundreds of rolling elements in the many pumps and drivers connected to a plant-wide system. \n\nAt standstill, or while on standby, pump and driver bearings are preserved by the surrounding oil mist, which exists in the bearing housing space at a pressure just barely higher than ambient. These pump and driver bearings are lubricated from the time when atomized oil globules join (or wet out) to become larger oil droplets. This joining-into-large-droplets starts whenever the equipment shafts rotate, which is when small globules come into contact with each other and start coating the bearing elements. \n\nAs of April 2017 over 150,000 process centrifugal pumps are operating with oil mist as the sole bearing lubricant. The estimated number of electric motors on pure oil mist exceeds 27,000; several of these at a Texas Gulf Coast petrochemical facility have been in flawless service since 1978. <ref. Bloch & Shamim, \"Oil Mist Lubrication: Practical Applications,\" The Fairmont Press, Lilburn, GA, ></ref. Bloch, \"Pump Wisdom: Problem Solving for Operators and Specialists,\" John Wiley & Sons, Hoboken, NJ, >\n\nThere are also plant-wide oil distribution systems whereby liquid oil (not an oil/air mixture) is pressurized and injected, through spray nozzles, into the pump bearings. These oil spray systems are not to be confused with the more economical oil mist systems. However, both oil mist and oil spray applications can take credit for lower frictional losses and both should be taken into account while performing cost justification analyses.\n"}
{"id": "23079699", "url": "https://en.wikipedia.org/wiki?curid=23079699", "title": "Optical modulators using semiconductor nano-structures", "text": "Optical modulators using semiconductor nano-structures\n\nAn optical modulator is an optical device which is used to modulate a beam of light with a perturbation device. It is a kind of transmitter to convert information to optical binary signal through optical fiber (optical waveguide) or transmission medium of optical frequency in fiber optic communication. There are several methods to manipulate this device depending on the parameter of a light beam like amplitude modulator (majority), phase modulator, polarization modulator etc.\nThe easiest way to obtain modulation is modulation of intensity of a light by the current driving the light source (laser diode). This sort of modulation is called direct modulation, as opposed to the external modulation performed by a light modulator. For this reason, light modulators are called external light modulators.\nAccording to manipulation of the properties of material modulators are divided into two groups, absorptive modulators (absorption coefficient) and refractive modulators (refractive index of the material). Absorption coefficient can be manipulated by Franz-Keldysh effect, Quantum-Confined Stark Effect, excitonic absorption, or changes of free carrier concentration. Usually, if several such effects appear together, the modulator is called electro-absorptive modulator. Refractive modulators most often make use of electro-optic effect (amplitude & phase modulation), other modulators are made with acousto-optic effect, magneto-optic effect such as Faraday and Cotton-Mouton effects. The other case of modulators is spatial light modulator (SLM) which is modified two dimensional distribution of amplitude & phase of an optical wave.\n\nOptical modulators can be implemented using Semiconductor Nano-structures to increase the performance like high operation, high stability, high speed response, and highly compact system. Highly compact electro-optical modulators have been demonstrated in compound semiconductors. However, in silicon photonics, electro-optical modulation has been demonstrated only in large structures, and is therefore inappropriate for effective on-chip\nintegration. Electro-optical control of light on silicon is challenging owing to its weak electro-optical properties. The large dimensions of previously demonstrated structures were necessary to achieve a significant modulation of the transmission in spite of the small change of refractive index of silicon. Liu et al. have recently demonstrated a high-speed silicon optical modulator based on a metal–oxide–semiconductor (MOS) configuration. Their work showed a high-speed optical active device on silicon—a critical milestone towards optoelectronic integration on silicon.\n\nAn electro-optic modulator is a device which can be used for controlling the power, phase or polarization of a laser beam with an electrical control signal. It typically contains one or two Pockels cells, and possibly additional optical elements such as polarizers. The principle of operation is based on the linear electro-optic effect (the Pockels effect, the modification of the refractive index of a nonlinear crystal by an electric field in proportion to the field strength).\n\nThe crystal which is covered by electrode may be considered to be a voltage-variable wave-plate. When a voltage is applied, the retardation of laser polarization of the light would be changed while a beam passes through an ADP crystal. This variation in polarization results in intensity modulation downstream from the output polarizer. The output polarizer converts the phase shift into an amplitude modulation.\n\nMicrometre-scale silicon electro-optic modulator\n\nThis device was fabricated a shape of the p-i-n ring resonator on a silicon-on-insulator substrate with a 3-mm-thick buried oxide layer. Both the waveguide coupling to the ring and that forming the ring have awidth of 450 nm and a height of 250 nm. The diameter of the ring is 12 mm, and the spacing between the ring and the straight waveguide is 200 nm.\n\nAcousto-optic modulators are used to vary and control laser beam intensity. A Bragg configuration gives a single first order output beam, whose intensity is directly linked to the power of RF control signal. The rise time of the modulator is simply deduced by the necessary time for the acoustic wave to travel through the laser beam. For highest speeds the laser beam will be focused down, forming a beam waist as it passes through the modulator.\n\nIn an AOM a laser beam is caused to interact with a high frequency ultrasonic sound wave inside an optically polished block of crystal or glass (the interaction medium). By carefully orientating the laser with respect to the sound waves the beam can be made to reflect off the acoustic wave-fronts (Bragg diffraction). Therefore, when the sound field is present the beam is deflected and when it is absent the beam passes through undeviated. By switching the sound field on and off very rapidly the deflected beam appears and disappears in response (digital modulation). By varying the amplitude of the acoustic waves the intensity of the deflected beam can similarly be modulated (analogue modulation).\n\nAcoustic solitons in semiconductor nanostructures\n\nAcoustic solitons strongly influence the electron states in a semiconductor nanostructure. The amplitude of soliton pulses is so high that the electron states in a quantum well make temporal excursions in energy up to 10 meV. The subpicosecond duration of the solitons is less than the coherence time of the optical transition between the electron states and a frequency modulation of emitted light during the coherence time (chirping effect) is observed. This system is for an ultrafast control of electron states in semiconductor nanostructures.\n\nA dc magnetic field Hdc is applied perpendicular to the light propagation direction to produce a single domain, transverse directed 4~Ms. The rf modulation field Hrf, applied by means of a coil along the light propagation direction, wobbles 4~Ms through an angle of @ and produces a time varying magnetization component in the longitudinal direction. This component then produces an ac variation in the plane of polarization via the longitudinal Faraday effect. Conversion to amplitude modulation is accomplished by the indicated analyzer.\n\nWideband magneto-optic modulation in a bismuth-substituted yttrium iron garnet waveguide\n\nThe current transient creates a time-varying magnetic field that has a component along the direction of optical propagation. This component (underneath the microstrip line) acts to tip the magnetization, M, along the propagation direction of the optical beam. A static in-plane magnetic field, by, is applied perpendicular to the light propagation direction, thus ensuring the return of M to its initial orientation after the passage of the current transient. Depending on the component of the magnetization along the z-direction, Mz, the optical beam experiences a rotation of its polarization due to the Faraday effect. The polarization modulation is converted into an intensity modulation via a polarization analyzer, which is detected by a high-speed photodiode.\n\nMODULATION OF THz RADIATION BY SEMICONDUCTOR NANOSTRUCTURES\n\nAs a result of increased demand for bandwidth, wireless short-range communication systems are expected to extend into the THz frequency range. Therefore, the fundamental interactions between THz radiation and semiconductors are receiving increasing attention. This new quantum structure is based on the well-established technology for producing high electron mobility transistors where an electron gas is confined at a GaAs/AlxGa1 xAs interface. The electron density at the hetero-interface can be controlled by the application of an external gate voltage, which in turn will alter the transmission/reﬂection characteristics of the device to an incident THz beam.\n\n\n40 Gbit/s Phase Modulator\nThe 40 Gbit/s Phase Modulator is a high performance, low drive voltage External Optical Modulator designed for customers developing next generation 40G transmission systems. The increased bandwidth allows for chirp control in high-speed data communications.\nApplications ; Chirp Control for High-Speed Communications (SONET OC-768 Interfaces, SDH STM-256 Interfaces), Coherent communications, C & L Band Operation, Optical Sensing, All-optical frequency shifting.\n\n\nApplications ; acousto-optic modulators include laser printing, video disk recording, laser projection systems.\n\n"}
{"id": "6588369", "url": "https://en.wikipedia.org/wiki?curid=6588369", "title": "Outline of manufacturing", "text": "Outline of manufacturing\n\nThe following outline is provided as an overview of and topical guide to manufacturing:\n\nManufacturing – use of machines, tools and labor to produce goods for use or sale. Includes a range of human activity, from handicraft to high-tech, but most commonly refers to industrial production, where raw materials are transformed into finished goods on a large scale.\n\n\n\n\nIndustrial Revolution\n\nFactory\n\nIndustrial process\n\n\n\n\n\n\n\nTaxonomy of manufacturing processes\n\n\n\n\n"}
{"id": "50957870", "url": "https://en.wikipedia.org/wiki?curid=50957870", "title": "SD-WAN", "text": "SD-WAN\n\nSD-WAN is an acronym for software-defined networking in a wide area network (WAN). An SD-WAN simplifies the management and operation of a WAN by decoupling (separating) the networking hardware from its control mechanism. This concept is similar to how software-defined networking implements virtualization technology to improve data center management and operation.\n\nA key application of an SD-WAN is to allow companies to build higher-performance WANs using lower-cost and commercially available Internet access, enabling businesses to partially or wholly replace more expensive private WAN connection technologies such as MPLS.\n\nAmerican marketing research firm Gartner predicted in 2015 that by the end of 2019 30% of enterprises will deploy SD-WAN technology in their branches.\n\nWANs allow companies to extend their computer networks over large distances, to connect remote branch offices to data centers and each other, and deliver the applications and services required to perform business functions. When companies extend networks over greater distances and sometimes across multiple carriers' networks, they face operational challenges including network congestion, packet delay variation, packet loss, and even service outages. Modern applications such as VoIP calling, videoconferencing, streaming media, and virtualized applications and desktops require low latency. Bandwidth requirements are also increasing, especially for applications featuring high-definition video. It can be expensive and difficult to expand WAN capability, with corresponding difficulties related to network management and troubleshooting.\n\nSD-WAN products are designed to address these network problems. By enhancing or even replacing traditional branch routers with virtualization appliances that can control application-level policies and offer a network overlay, less expensive consumer-grade Internet links can act more like a dedicated circuit. This simplifies the setup process for branch personnel. SD-WAN products can be physical appliances or virtual appliances, and are placed in small remote and branch offices, larger offices, corporate data centers, and increasingly on cloud platforms.\n\nA centralized controller is used to set policies and prioritize traffic. The SD-WAN takes into account these policies and the availability of network bandwidth to route traffic. This helps ensure that application performance meets service level agreements (SLAs).\n\nSD-WAN consists of several technologies combined with newer enhancements. Redundant telecommunication links connecting remote sites date back to the 1970s with X.25 links used for remote mainframe terminal access. Central management of those links with a greater focus on application delivery across the WAN started to become popular in the mid-2000s. SD-WAN combines the two, and adds the ability to dynamically share network bandwidth across the connection points. Additional enhancements include central controllers, integrated analytics and on-demand circuit provisioning, with some network intelligence based in the cloud, allowing centralized policy management and security.\n\nNetworking publications started using the term SD-WAN to describe this new networking trend as early as 2014.\n\nResearch firm Gartner has defined an SD-WAN as having four required characteristics:\n\n\nSD-WAN products can be physical appliances or software based.\n\nFeatures of SD-WANs include resilience, security and quality of service (QoS), with flexible deployment options and simplified administration and troubleshooting.\n\nA resilient SD-WAN reduces network downtime. The technology must feature real time detection of outages and automatic switch over to working links.\n\nSD-WAN technology supports quality of service by having application level awareness, giving bandwidth priority to the most critical applications. This may include dynamic path selection, sending an application on a faster link, or even splitting an application between two paths to improve performance by delivering it faster.\n\nSD-WAN communication is usually secured using IPsec, a staple of WAN security.\n\nSD-WANs can improve application delivery using caching, storing recently accessed information in memory to speed future access.\n\nMost SD-WAN products are available as pre-configured appliances, placed at the network edge in data centers, branch offices and other remote locations. There are also virtual appliances that can work on existing network hardware, or the appliance can be deployed as a virtual appliance on the cloud in environments such as Amazon Web Services (AWS). This allows enterprises to benefit from SD-WAN services as they migrate application delivery from corporate servers to cloud based services such as Salesforce.com and Google apps.\n\nManagement simplicity is a key requirement for SD-WANs, per Gartner. As with network equipment in general, GUIs may be preferred to command line interface (CLI) methods of configuration and control. Other beneficial administrative features include automatic path selection, the ability to centrally configure each end appliance by pushing configuration changes out, and even a true software defined networking approach that lets all appliances and virtual appliances be configured centrally based on application needs rather than underlying hardware.\n\nWith a global view of network status, a controller that manages SD-WAN can perform careful and adaptive traffic engineering by assigning new transfer requests according to current usage of resources (links). For example, this can be achieved by performing central calculation of transmission rates at the controller and rate-limiting at the senders (end-points) according to such rates.\n\nThere are some similarities between SD-WAN and WAN optimization, the name given to the collection of techniques used to increase data-transfer efficiencies across WANs. The goal of each is to accelerate application delivery between branch offices and data centers, but SD-WAN technology focuses additionally on cost savings and efficiency, specifically by allowing lower cost network links to perform the work of more expensive leased lines, whereas WAN Optimization focuses squarely on improving packet delivery. An SD-WAN utilizing virtualization techniques assisted with WAN Optimization traffic control allows network bandwidth to dynamically grow or shrink as needed. SD-WAN technology and WAN optimization can be used separately or together, and some SD-WAN vendors are adding WAN optimization features to their products.\n\nA WAN edge router is a device that routes data packets between different WAN locations, giving an enterprise access to a carrier network. Also called a boundary router, it is unlike a core router, which only sends packets within a single network. SD-WANs can work as an overlay to simplify the management of existing WAN edge routers, by lowering dependence on routing protocols. SD-WAN can also potentially be an alternative to WAN Edge routers.\n\nSD-WANs are similar to hybrid WANs, and sometimes the terms are used interchangeably, but they are not identical. A hybrid WAN consists of different connection types, and may have a software defined network (SDN) component, but doesn't have to.\n\nAs there is no standard algorithm for SD-WAN controllers, device manufacturers each use their own proprietary algorithm in the transmission of data. These algorithms determine which traffic to direct over which link and when to switch traffic from one link to another. Given the breadth of options available in relation to both software and hardware SD-WAN control solutions, it's imperative they be tested and validated under real-world conditions within a lab setting prior to deployment.\n\nThere are multiple solutions available for testing purposes, ranging from purpose-built network emulation appliances which can apply specified network impairments to the network being tested in order to reliably validate performance, to software-based solutions.\n\nIT website Network World divides the SD-WAN vendor market into three groups: established networking vendors who are adding SD-WAN products to their offerings, WAN specialists who are starting to integrate SD-WAN functionality into their products, and startups focused specifically on the SD-WAN market.\n\nAlternatively, a market overview by Nemertes Research groups SD-WAN vendors into categories based on their original technology space, and which are \"Pure-play SD-WAN providers\", \"WAN optimization vendors\", \"Link-aggregation vendors\", and \"General network vendors\" While Network World's second category (startups focused specifically on the SD-WAN market), is generally equivalent to Nemertes' \"Pure-play SD-WAN providers\" category, Nemertes offers a more detailed view of the preexisting WAN and overall networking providers. \n\nAdditionally, Nemertes Research also describes the in-net side of the SD-WAN market, describing the go-to-market strategy of connectivity providers entering the SD-WAN market. These providers include \"Network-as-a-service vendors\", \"Carriers or telcos\", \"Content delivery networks\" and \"Secure WAN providers\".\n\nSeveral online resources, including the networking technology podcast \"Packet Pushers\", keep an updated list of existing SD-WAN vendors. In June 2018, Network World named 10 hot SD-WAN startups.\n"}
{"id": "27979868", "url": "https://en.wikipedia.org/wiki?curid=27979868", "title": "Saladero", "text": "Saladero\n\nSaladero is a basic industry that produces salted meat such as charqui. It was one of the earliest industries of Argentina and Uruguay after the Argentine War of Independence, benefiting from the availability of cattle in the Humid Pampas and the low technology and manpower requirements. Most of the production was sold to Cuba and Brazil to feed slaves. In time, it expanded into other areas, such as extracting the leather, horns and fat from cows (fat was useful for public lighting, soaps and candles). Saladero declined at the end of the 19th century, with the lowering numbers of foreign slaves (and thus the smaller demand for food for them) and the expansion of refrigeration techniques.\n\n"}
{"id": "11604807", "url": "https://en.wikipedia.org/wiki?curid=11604807", "title": "Screw terminal", "text": "Screw terminal\n\nA screw terminal is a type of electrical connector where a wire is held by the tightening of a screw. \n\nThe wire may be wrapped directly under the head of a screw, may be held by a metal plate forced against the wire by a screw, or may be held by what is, in effect, a set screw in the side of a metal tube. The wire may be directly stripped of insulation and inserted under the head of a screw or into the terminal. Otherwise, it may be either inserted first into a ferrule, which is then inserted into the terminal, or else attached to a connecting lug. which is then fixed under the screw head. \n\nDepending on the design, a flat-blade screwdriver, a cross-blade screwdriver, hex key, Torx key, or other tool may be required to properly tighten the connection for reliable operation.\n\nScrew terminals are used extensively in building wiring for the distribution of electricity - connecting electrical outlets, luminaires and switches to the mains, and for directly connecting major appliances such as clothes dryers and ovens drawing in excess of 15 amperes. \n\nScrew terminals are commonly used to connect a chassis ground, such as on a record player or surge protector. Most public address systems in buildings also use them for speakers, and sometimes for other outputs and inputs. Alarm systems and building sensor and control systems have traditionally used large numbers of screw terminations.\n\nGrounding screws are often color-coded green and, when used on consumer electronics, often have a washer with gripping \"teeth\". \n\nPrinted circuit board (PCB) terminal blocks are specially designed with a copper alloy pin of suitable size and length and can be inserted in printed circuit boards to be soldered to allow electrical signals and current to flow to and from PCBs and electrical equipment. Some designs provide features that allow the flow of molten solder to endure a better connection between the circuit traces of the board and the electrical equipment which is meant to be controlled or fed appropriate power. \n\nMultiple screw terminals can be arranged in the form of a barrier strip (as illustrated at the top right), with a number of short metal strips separated by a raised insulated \"barrier\" on an insulating \"block\" - each strip having a pair of screws with each screw connecting to a separate conductor, one at each end of the strip. These are known as connector strips or chocolate blocks (\"choc blocks\") in the UK. This nick-name arises from the first such connectors made in the UK by GEC, Witton in the 1950s. Moulded in brown plastic they were said to resemble a small bar of chocolate.\n\nA similar arrangement is common with paired screw terminals, where metal tubes are loosely encased in an insulating block with a set screw at each end of each tube to hold and thus connect a conductor. These are often used to connect light fixtures and are shown at the right. \n\nAlternatively, terminals can also be arranged as a terminal strip or terminal block, with several screws along (typically) two long strips. This creates a bus bar for power distribution, and so may also include a master input connector, usually binding posts or banana connectors.\n\nAssembly of a screw connection requires some care in workmanship to ensure proper removal of insulation, containment of all wire strands, and the adequate tightening of the screw. If the wire diameter is small in relation to the size of the screw, the wire may be cut through by the over-tightening of the screw. This is less likely to occur when a wire is clamped between two plates by the action of a screw. Since wire strands may not be contained by the screw head in a basic screw terminal, stranded wires may be crimped into a ferrule to prevent the bridging of terminals; this partly offsets the economy of a \"bare\" wire termination.\n\nWhile wires may be crimped, they should not be heavily tinned with solder prior to installation in a screw terminal, since the soft metal will cold flow, resulting in a loose connection and possible fire hazard. Screw connectors sometimes come loose if not done up tightly enough at fitting time. Verifying adequate tightening torque requires calibrated installation tools and proper training. In the UK, all screw connectors on fixed mains installations are required to be accessible for servicing, for this reason.\n\nScrew terminals are low in cost when compared to other types of connectors, and can be readily designed into products for circuits carrying currents of from a fraction of an ampere up to several hundred amperes at low to moderate frequencies. The terminals easily can be re-used in the field, allowing for the replacement of wires or equipment, generally with standard hand tools. Screw terminals usually avoid the requirement for a specialized mating connector to be applied to the ends of wires.\n\nWhen properly tightened, the connections are physically and electrically secure because they firmly contact a large section of wire. The terminals are relatively low cost compared with other types of connector, and a screw terminal can easily be integrated into the design of a building wiring device (such as a socket, switch, or lamp holder). \n\nDisadvantages include the time taken to strip a wire and, in basic terminals, properly wrap it around a screw head, since it is essential that any wire installed under a screw head be \"wound\" in the correct direction (usually clockwise,) so that the conductors are not forced outwards when the screw is tightened. This procedure is more time consuming than using a plug-in connector - thus making screw connections uncommon for portable equipment, where wires are repeatedly connected and disconnected.\n\nHowever, with the clamping plate type of screw terminal this time is reduced, since it is necessary only to insert the stripped wire between the terminal and the rear clamping plate then tighten the connection, using the screw to clamp the wire between the terminal and the clamping plate, without any need properly to wrap it around the screw head.\n\nThe screw mechanism limits the minimum physical size of a terminal, making screw terminals less useful where very many connections are required.\nIt is difficult to automate multiple terminations with screw connections. \n\nVibration or corrosion can cause a screw connection to deteriorate over time.\nThe use of screw terminal \"chocolate blocks\" in building wiring installations has sharply declined in favour of crimp, push and twist type connectors which are not prone to working loose and easier to fit. In the UK chocolate blocks are no longer approved for connections that are not accessible for inspection (ie. under floors).\n\n"}
{"id": "32270687", "url": "https://en.wikipedia.org/wiki?curid=32270687", "title": "Shenzhen NORCO Intelligent Technology", "text": "Shenzhen NORCO Intelligent Technology\n\nShenzhen NORCO Intelligent Technology Co., Ltd (NORCO, 华北工控), is a Chinese company with 30 branch offices, employing over 700 employees. Founded in 1991 and headquartered in Shenzhen, China, it is a manufacturer of industrial PCs and embedded computers. NORCO industrial PC products include CPU cards, embedded industrial motherboards, industrial computers, industrial workstations, Panel PCs, firewalls, storage arrays (inc. NAS), industrial power supplies, industrial chassis, passive backplanes, industrial computer accessories and IO adapters.\n\nNORCO products are used in computer-based applications, including military, communication, material, industrial automation, energy, traffic, aviation, health care, network, AI (artificial intelligence), security, vehicle, banking and entertainment etc.\n\nNORCO distributes its products to overseas markets in Asia, the Americas and Europe, and the domestic market, and has subsidiaries in Netherlands, Agent in Singapore, California USA, Ohio USA, in Germany. NORCO is a member of Intel Embedded Alliance (EA). In 2011 NORCO has elevated to associate level in Intel Embedded Alliance, and was the first PRC company to be elevated to Associate Level.\n\nDuring the 2010 Shanghai Expo, NORCO embedded products were used in the fire alarm systems of the Permanent Pavilion, Russia Pavilion, Future Pavilion and Baogang Large Stage in the Expo Park; NORCO signage PCs were used in the information distribution system of China Pavilion.\n\nDuring the celebrations of the 60th anniversary of the People's Republic of China, NORCO motherboards with Intel IVI system were applied to car PCs in the military review, which is the first applications of motherboards with Intel IVI system in vehicle mounted PC market.\n\nNORCO embedded industrial PCs were used in the security examination system, intelligent parking system and video surveillance system of the 2008 Summer Olympics Stadium.\n\n"}
{"id": "2352910", "url": "https://en.wikipedia.org/wiki?curid=2352910", "title": "Solar cell", "text": "Solar cell\n\nA solar cell, or photovoltaic cell, is an electrical device that converts the energy of light directly into electricity by the photovoltaic effect, which is a physical and chemical phenomenon. It is a form of photoelectric cell, defined as a device whose electrical characteristics, such as current, voltage, or resistance, vary when exposed to light. Individual solar cell devices can be combined to form modules, otherwise known as solar panels. In basic terms a single junction silicon solar cell can produce a maximum open-circuit voltage of approximately 0.5 to 0.6 volts.\n\nSolar cells are described as being photovoltaic, irrespective of whether the source is sunlight or an artificial light. They are used as a photodetector (for example infrared detectors), detecting light or other electromagnetic radiation near the visible range, or measuring light intensity.\n\nThe operation of a photovoltaic (PV) cell requires three basic attributes:\n\nIn contrast, a solar thermal collector supplies heat by absorbing sunlight, for the purpose of either direct heating or indirect electrical power generation from heat. A \"photoelectrolytic cell\" (photoelectrochemical cell), on the other hand, refers either to a type of photovoltaic cell (like that developed by Edmond Becquerel and modern dye-sensitized solar cells), or to a device that splits water directly into hydrogen and oxygen using only solar illumination.\nAssemblies of solar cells are used to make solar modules that generate electrical power from sunlight, as distinguished from a \"solar thermal module\" or \"solar hot water panel\". A solar array generates solar power using solar energy.\n\nMultiple solar cells in an integrated group, all oriented in one plane, constitute a solar photovoltaic panel or module. Photovoltaic modules often have a sheet of glass on the sun-facing side, allowing light to pass while protecting the semiconductor wafers. Solar cells are usually connected in series and parallel circuits or series in modules, creating an additive voltage. Connecting cells in parallel yields a higher current; however, problems such as shadow effects can shut down the weaker (less illuminated) parallel string (a number of series connected cells) causing substantial power loss and possible damage because of the reverse bias applied to the shadowed cells by their illuminated partners. Strings of series cells are usually handled independently and not connected in parallel, though as of 2014, individual power boxes are often supplied for each module, and are connected in parallel. Although modules can be interconnected to create an array with the desired peak DC voltage and loading current capacity, using independent MPPTs (maximum power point trackers) is preferable. Otherwise, shunt diodes can reduce shadowing power loss in arrays with series/parallel connected cells.\n\nThe photovoltaic effect was experimentally demonstrated first by French physicist Edmond Becquerel. In 1839, at age 19, he built the world's first photovoltaic cell in his father's laboratory. Willoughby Smith first described the \"Effect of Light on Selenium during the passage of an Electric Current\" in a 20 February 1873 issue of Nature. In 1883 Charles Fritts built the first solid state photovoltaic cell by coating the semiconductor selenium with a thin layer of gold to form the junctions; the device was only around 1% efficient.\n\nIn 1888 Russian physicist Aleksandr Stoletov built the first cell based on the outer photoelectric effect discovered by Heinrich Hertz in 1887.\n\nIn 1905 Albert Einstein proposed a new quantum theory of light and explained the photoelectric effect in a landmark paper, for which he received the Nobel Prize in Physics in 1921.\n\nVadim Lashkaryov discovered \"p\"-\"n\"-junctions in Cuformula_1O and silver sulphide protocells in 1941.\n\nRussell Ohl patented the modern junction semiconductor solar cell in 1946 while working on the series of advances that would lead to the transistor.\n\nThe first practical photovoltaic cell was publicly demonstrated on 25 April 1954 at Bell Laboratories. The inventors were Calvin Souther Fuller, Daryl Chapin and Gerald Pearson.\n\nSolar cells gained prominence with their incorporation onto the 1958 Vanguard I satellite.\n\nSolar cells were first used in a prominent application when they were proposed and flown on the Vanguard satellite in 1958, as an alternative power source to the primary battery power source. By adding cells to the outside of the body, the mission time could be extended with no major changes to the spacecraft or its power systems. In 1959 the United States launched Explorer 6, featuring large wing-shaped solar arrays, which became a common feature in satellites. These arrays consisted of 9600 Hoffman solar cells.\n\nBy the 1960s, solar cells were (and still are) the main power source for most Earth orbiting satellites and a number of probes into the solar system, since they offered the best power-to-weight ratio. However, this success was possible because in the space application, power system costs could be high, because space users had few other power options, and were willing to pay for the best possible cells. The space power market drove the development of higher efficiencies in solar cells up until the National Science Foundation \"Research Applied to National Needs\" program began to push development of solar cells for terrestrial applications.\n\nIn the early 1990s the technology used for space solar cells diverged from the silicon technology used for terrestrial panels, with the spacecraft application shifting to gallium arsenide-based III-V semiconductor materials, which then evolved into the modern III-V multijunction photovoltaic cell used on spacecraft.\n\nImprovements were gradual over the 1960s. This was also the reason that costs remained high, because space users were willing to pay for the best possible cells, leaving no reason to invest in lower-cost, less-efficient solutions. The price was determined largely by the semiconductor industry; their move to integrated circuits in the 1960s led to the availability of larger boules at lower relative prices. As their price fell, the price of the resulting cells did as well. These effects lowered 1971 cell costs to some $100 per watt.\n\nIn late 1969 Elliot Berman joined Exxon's task force which was looking for projects 30 years in the future and in April 1973 he founded Solar Power Corporation, a wholly owned subsidiary of Exxon at that time. The group had concluded that electrical power would be much more expensive by 2000, and felt that this increase in price would make alternative energy sources more attractive. He conducted a market study and concluded that a price per watt of about $20/watt would create significant demand. The team eliminated the steps of polishing the wafers and coating them with an anti-reflective layer, relying on the rough-sawn wafer surface. The team also replaced the expensive materials and hand wiring used in space applications with a printed circuit board on the back, acrylic plastic on the front, and silicone glue between the two, \"potting\" the cells. Solar cells could be made using cast-off material from the electronics market. By 1973 they announced a product, and SPC convinced Tideland Signal to use its panels to power navigational buoys, initially for the U.S. Coast Guard.\n\nResearch into solar power for terrestrial applications became prominent with the U.S. National Science Foundation's Advanced Solar Energy Research and Development Division within the \"Research Applied to National Needs\" program, which ran from 1969 to 1977, and funded research on developing solar power for ground electrical power systems. A 1973 conference, the \"Cherry Hill Conference\", set forth the technology goals required to achieve this goal and outlined an ambitious project for achieving them, kicking off an applied research program that would be ongoing for several decades. The program was eventually taken over by the Energy Research and Development Administration (ERDA), which was later merged into the U.S. Department of Energy.\n\nFollowing the 1973 oil crisis, oil companies used their higher profits to start (or buy) solar firms, and were for decades the largest producers. Exxon, ARCO, Shell, Amoco (later purchased by BP) and Mobil all had major solar divisions during the 1970s and 1980s. Technology companies also participated, including General Electric, Motorola, IBM, Tyco and RCA.\n\nAdjusting for inflation, it cost $96 per watt for a solar module in the mid-1970s. Process improvements and a very large boost in production have brought that figure down 99%, to 68¢ per watt in 2016, according to data from Bloomberg New Energy Finance.\nSwanson's law is an observation similar to Moore's Law that states that solar cell prices fall 20% for every doubling of industry capacity. It was featured in an article in the British weekly newspaper The Economist in late 2012.\n\nFurther improvements reduced production cost to under $1 per watt, with wholesale costs well under $2. Balance of system costs were then higher than those of the panels. Large commercial arrays could be built, as of 2010, at below $3.40 a watt, fully commissioned.\n\nAs the semiconductor industry moved to ever-larger boules, older equipment became inexpensive. Cell sizes grew as equipment became available on the surplus market; ARCO Solar's original panels used cells in diameter. Panels in the 1990s and early 2000s generally used 125 mm wafers; since 2008, almost all new panels use 156 mm cells. The widespread introduction of flat screen televisions in the late 1990s and early 2000s led to the wide availability of large, high-quality glass sheets to cover the panels.\n\nDuring the 1990s, polysilicon (\"poly\") cells became increasingly popular. These cells offer less efficiency than their monosilicon (\"mono\") counterparts, but they are grown in large vats that reduce cost. By the mid-2000s, poly was dominant in the low-cost panel market, but more recently the mono returned to widespread use.\n\nManufacturers of wafer-based cells responded to high silicon prices in 2004–2008 with rapid reductions in silicon consumption. In 2008, according to Jef Poortmans, director of IMEC's organic and solar department, current cells use of silicon per watt of power generation, with wafer thicknesses in the neighborhood of 200 microns. Crystalline silicon panels dominate worldwide markets and are mostly manufactured in China and Taiwan. By late 2011, a drop in European demand dropped prices for crystalline solar modules to about $1.09 per watt down sharply from 2010. Prices continued to fall in 2012, reaching $0.62/watt by 4Q2012.\n\nSolar PV is growing fastest in Asia, with China and Japan currently accounting for half of worldwide deployment.\nGlobal installed PV capacity reached at least 301 gigawatts in 2016, and grew to supply 1.3% of global power by 2016.\n\nIn fact, the harnessed energy of silicon solar cells at the cost of a dollar has surpassed its oil counterpart since 2004. It was anticipated that electricity from PV will be competitive with wholesale electricity costs all across Europe and the energy payback time of crystalline silicon modules can be reduced to below 0.5 years by 2020.\n\nSolar-specific feed-in tariffs vary by country and within countries. Such tariffs encourage the development of solar power projects.\nWidespread grid parity, the point at which photovoltaic electricity is equal to or cheaper than grid power without subsidies, likely requires advances on all three fronts. Proponents of solar hope to achieve grid parity first in areas with abundant sun and high electricity costs such as in California and Japan. In 2007 BP claimed grid parity for Hawaii and other islands that otherwise use diesel fuel to produce electricity. George W. Bush set 2015 as the date for grid parity in the US. The Photovoltaic Association reported in 2012 that Australia had reached grid parity (ignoring feed in tariffs).\n\nThe price of solar panels fell steadily for 40 years, interrupted in 2004 when high subsidies in Germany drastically increased demand there and greatly increased the price of purified silicon (which is used in computer chips as well as solar panels). The recession of 2008 and the onset of Chinese manufacturing caused prices to resume their decline. In the four years after January 2008 prices for solar modules in Germany dropped from €3 to €1 per peak watt. During that same time production capacity surged with an annual growth of more than 50%. China increased market share from 8% in 2008 to over 55% in the last quarter of 2010. In December 2012 the price of Chinese solar panels had dropped to $0.60/Wp (crystalline modules). (The abbreviation Wp stands for watt peak capacity, or the maximum capacity under optimal conditions.)\n\nAs of the end of 2016, it was reported that spot prices for assembled solar \"panels\" (not cells) had fallen to a record-low of US$0.36/Wp. It was estimated that the world's largest producer, Trina Solar Ltd. of China, was probably selling at a loss. The second largest supplier, Canadian Solar Inc., had reported costs of US$0.37/Wp in the third quarter of 2016, having dropped $0.02 from the previous quarter, and hence was probably still at least breaking even. Many producers expected costs would drop to the vicinity of $0.30 by the end of 2017. It was also reported that new solar installations were cheaper than coal-based thermal power plants in some regions of the world, and this was expected to be the case in most of the world within a decade.\n\nThe solar cell works in several steps:\n\nThe most commonly known solar cell is configured as a large-area p–n junction made from silicon. Other possible solar cell types are organic solar cells, dye sensitized solar cells, perovskite solar cells, quantum dot solar cells etc. The illuminated side of a solar cell generally has a transparent conducting film for allowing light to enter into active material and to collect the generated charge carriers. Typically, films with high transmittance and high electrical conductance such as indium tin oxide, conducting polymers or conducting nanowire networks are used for the purpose.\n\nSolar cell efficiency may be broken down into reflectance efficiency, thermodynamic efficiency, charge carrier separation efficiency and conductive efficiency. The overall efficiency is the product of these individual metrics.\n\nThe power conversion efficiency of a solar cell is a parameter which is defined by the fraction of incident power converted into electricity.\n\nA solar cell has a voltage dependent efficiency curve, temperature coefficients, and allowable shadow angles.\n\nDue to the difficulty in measuring these parameters directly, other parameters are substituted: thermodynamic efficiency, quantum efficiency, integrated quantum efficiency, V ratio, and fill factor. Reflectance losses are a portion of quantum efficiency under \"external quantum efficiency\". Recombination losses make up another portion of quantum efficiency, V ratio, and fill factor. Resistive losses are predominantly categorized under fill factor, but also make up minor portions of quantum efficiency, V ratio.\n\nThe fill factor is the ratio of the actual maximum obtainable power to the product of the open circuit voltage and short circuit current. This is a key parameter in evaluating performance. In 2009, typical commercial solar cells had a fill factor > 0.70. Grade B cells were usually between 0.4 and 0.7. Cells with a high fill factor have a low equivalent series resistance and a high equivalent shunt resistance, so less of the current produced by the cell is dissipated in internal losses.\n\nSingle p–n junction crystalline silicon devices are now approaching the theoretical limiting power efficiency of 33.16%, noted as the Shockley–Queisser limit in 1961. In the extreme, with an infinite number of layers, the corresponding limit is 86% using concentrated sunlight.\n\nIn 2014, three companies broke the record of 25.6% for a silicon solar cell. Panasonic's was the most efficient. The company moved the front contacts to the rear of the panel, eliminating shaded areas. In addition they applied thin silicon films to the (high quality silicon) wafer's front and back to eliminate defects at or near the wafer surface.\n\nIn 2015, a 4-junction GaInP/GaAs//GaInAsP/GaInAs solar cell achieved a new laboratory record efficiency of 46.1 percent (concentration ratio of sunlight = 312) in a French-German collaboration between the Fraunhofer Institute for Solar Energy Systems (Fraunhofer ISE), CEA-LETI and SOITEC.\n\nIn September 2015, Fraunhofer ISE announced the achievement of an efficiency above 20% for epitaxial wafer cells. The work on optimizing the atmospheric-pressure chemical vapor deposition (APCVD) in-line production chain was done in collaboration with NexWafe GmbH, a company spun off from Fraunhofer ISE to commercialize production.\n\nFor triple-junction thin-film solar cells, the world record is 13.6%, set in June 2015.\n\nIn 2016, researchers at Fraunhofer ISE announced a GaInP/GaAs/Si triple-junction solar cell with two terminals reaching 30.2% efficiency without concentration.\n\nIn 2017, a team of researchers at National Renewable Energy Laboratory (NREL), EPFL and CSEM (Switzerland) reported record one-sun efficiencies of 32.8% for dual-junction GaInP/GaAs solar cell devices. In addition, the dual-junction device was mechanically stacked with a Si solar cell, to achieve a record one-sun efficiency of 35.9% for triple-junction solar cells.\nSolar cells are typically named after the semiconducting material they are made of. These materials must have certain characteristics in order to absorb sunlight. Some cells are designed to handle sunlight that reaches the Earth's surface, while others are optimized for use in space. Solar cells can be made of only one single layer of light-absorbing material (single-junction) or use multiple physical configurations (multi-junctions) to take advantage of various absorption and charge separation mechanisms.\n\nSolar cells can be classified into first, second and third generation cells. The first generation cells—also called conventional, traditional or wafer-based cells—are made of crystalline silicon, the commercially predominant PV technology, that includes materials such as polysilicon and monocrystalline silicon. Second generation cells are thin film solar cells, that include amorphous silicon, CdTe and CIGS cells and are commercially significant in utility-scale photovoltaic power stations, building integrated photovoltaics or in small stand-alone power system. The third generation of solar cells includes a number of thin-film technologies often described as emerging photovoltaics—most of them have not yet been commercially applied and are still in the research or development phase. Many use organic materials, often organometallic compounds as well as inorganic substances. Despite the fact that their efficiencies had been low and the stability of the absorber material was often too short for commercial applications, there is a lot of research invested into these technologies as they promise to achieve the goal of producing low-cost, high-efficiency solar cells.\n\nBy far, the most prevalent bulk material for solar cells is crystalline silicon (c-Si), also known as \"solar grade silicon\". Bulk silicon is separated into multiple categories according to crystallinity and crystal size in the resulting ingot, ribbon or wafer. These cells are entirely based around the concept of a p-n junction.\nSolar cells made of c-Si are made from wafers between 160 and 240 micrometers thick.\n\nMonocrystalline silicon (mono-Si) solar cells are more efficient and more expensive than most other types of cells. The corners of the cells look clipped, like an octagon, because the wafer material is cut from cylindrical ingots, that are typically grown by the Czochralski process. Solar panels using mono-Si cells display a distinctive pattern of small white diamonds.\n\nEpitaxial wafers of crystalline silicon can be grown on a monocrystalline silicon \"seed\" wafer by chemical vapor deposition (CVD), and then detached as self-supporting wafers of some standard thickness (e.g., 250 µm) that can be manipulated by hand, and directly substituted for wafer cells cut from monocrystalline silicon ingots. Solar cells made with this \"kerfless\" technique can have efficiencies approaching those of wafer-cut cells, but at appreciably lower cost if the CVD can be done at atmospheric pressure in a high-throughput inline process. The surface of epitaxial wafers may be textured to enhance light absorption.\n\nIn June 2015, it was reported that heterojunction solar cells grown epitaxially on n-type monocrystalline silicon wafers had reached an efficiency of 22.5% over a total cell area of 243.4 cmformula_2.\n\nPolycrystalline silicon, or multicrystalline silicon (multi-Si) cells are made from cast square ingots—large blocks of molten silicon carefully cooled and solidified. They consist of small crystals giving the material its typical metal flake effect. Polysilicon cells are the most common type used in photovoltaics and are less expensive, but also less efficient, than those made from monocrystalline silicon.\n\nRibbon silicon is a type of polycrystalline silicon—it is formed by drawing flat thin films from molten silicon and results in a polycrystalline structure. These cells are cheaper to make than multi-Si, due to a great reduction in silicon waste, as this approach does not require sawing from ingots. However, they are also less efficient.\n\nThis form was developed in the 2000s and introduced commercially around 2009. Also called cast-mono, this design uses polycrystalline casting chambers with small \"seeds\" of mono material. The result is a bulk mono-like material that is polycrystalline around the outsides. When sliced for processing, the inner sections are high-efficiency mono-like cells (but square instead of \"clipped\"), while the outer edges are sold as conventional poly. This production method results in mono-like cells at poly-like prices.\n\nThin-film technologies reduce the amount of active material in a cell. Most designs sandwich active material between two panes of glass. Since silicon solar panels only use one pane of glass, thin film panels are approximately twice as heavy as crystalline silicon panels, although they have a smaller ecological impact (determined from life cycle analysis). \n\nCadmium telluride is the only thin film material so far to rival crystalline silicon in cost/watt. However cadmium is highly toxic and tellurium (anion: \"telluride\") supplies are limited. The cadmium present in the cells would be toxic if released. However, release is impossible during normal operation of the cells and is unlikely during fires in residential roofs. A square meter of CdTe contains approximately the same amount of Cd as a single C cell nickel-cadmium battery, in a more stable and less soluble form.\n\nCopper indium gallium selenide (CIGS) is a direct band gap material. It has the highest efficiency (~20%) among all commercially significant thin film materials (see CIGS solar cell). Traditional methods of fabrication involve vacuum processes including co-evaporation and sputtering. Recent developments at IBM and Nanosolar attempt to lower the cost by using non-vacuum solution processes.\n\nSilicon thin-film cells are mainly deposited by chemical vapor deposition (typically plasma-enhanced, PE-CVD) from silane gas and hydrogen gas. Depending on the deposition parameters, this can yield amorphous silicon (a-Si or a-Si:H), protocrystalline silicon or nanocrystalline silicon (nc-Si or nc-Si:H), also called microcrystalline silicon.\n\nAmorphous silicon is the most well-developed thin film technology to-date. An amorphous silicon (a-Si) solar cell is made of non-crystalline or microcrystalline silicon. Amorphous silicon has a higher bandgap (1.7 eV) than crystalline silicon (c-Si) (1.1 eV), which means it absorbs the visible part of the solar spectrum more strongly than the higher power density infrared portion of the spectrum. The production of a-Si thin film solar cells uses glass as a substrate and deposits a very thin layer of silicon by plasma-enhanced chemical vapor deposition (PECVD).\n\nProtocrystalline silicon with a low volume fraction of nanocrystalline silicon is optimal for high open circuit voltage. Nc-Si has about the same bandgap as c-Si and nc-Si and a-Si can advantageously be combined in thin layers, creating a layered cell called a tandem cell. The top cell in a-Si absorbs the visible light and leaves the infrared part of the spectrum for the bottom cell in nc-Si.\n\nThe semiconductor material Gallium arsenide (GaAs) is also used for single-crystalline thin film solar cells. Although GaAs cells are very expensive, they hold the world's record in efficiency for a single-junction solar cell at 28.8%. GaAs is more commonly used in multijunction photovoltaic cells for concentrated photovoltaics (CPV, HCPV) and for solar panels on spacecrafts, as the industry favours efficiency over cost for space-based solar power.\n\nMulti-junction cells consist of multiple thin films, each essentially a solar cell grown on top of another, typically using metalorganic vapour phase epitaxy. Each layer has a different band gap energy to allow it to absorb electromagnetic radiation over a different portion of the spectrum. Multi-junction cells were originally developed for special applications such as satellites and space exploration, but are now used increasingly in terrestrial concentrator photovoltaics (CPV), an emerging technology that uses lenses and curved mirrors to concentrate sunlight onto small, highly efficient multi-junction solar cells. By concentrating sunlight up to a thousand times, \"High concentrated photovoltaics (HCPV)\" has the potential to outcompete conventional solar PV in the future.\n\nTandem solar cells based on monolithic, series connected, gallium indium phosphide (GaInP), gallium arsenide (GaAs), and germanium (Ge) p–n junctions, are increasing sales, despite cost pressures. Between December 2006 and December 2007, the cost of 4N gallium metal rose from about $350 per kg to $680 per kg. Additionally, germanium metal prices have risen substantially to $1000–1200 per kg this year. Those materials include gallium (4N, 6N and 7N Ga), arsenic (4N, 6N and 7N) and germanium, pyrolitic boron nitride (pBN) crucibles for growing crystals, and boron oxide, these products are critical to the entire substrate manufacturing industry.\n\nA triple-junction cell, for example, may consist of the semiconductors: GaAs, Ge, and . Triple-junction GaAs solar cells were used as the power source of the Dutch four-time World Solar Challenge winners Nuna in 2003, 2005 and 2007 and by the Dutch solar cars Solutra (2005), Twente One (2007) and 21Revolution (2009). GaAs based multi-junction devices are the most efficient solar cells to date. On 15 October 2012, triple junction metamorphic cells reached a record high of 44%.\n\nIn 2016, a new approach was described for producing hybrid photovoltaic wafers combining the high efficiency of III-V multi-junction solar cells with the economies and wealth of experience associated with silicon. The technical complications involved in growing the III-V material on silicon at the required high temperatures, a subject of study for some 30 years, are avoided by epitaxial growth of silicon on GaAs at low temperature by plasma-enhanced chemical vapor deposition (PECVD)\n\nPerovskite solar cells are solar cells that include a perovskite-structured material as the active layer. Most commonly, this is a solution-processed hybrid organic-inorganic tin or lead halide based material. Efficiencies have increased from below 5% at their first usage in 2009 to over 20% in 2014, making them a very rapidly advancing technology and a hot topic in the solar cell field. Perovskite solar cells are also forecast to be extremely cheap to scale up, making them a very attractive option for commercialisation. So far most types of perovskite solar cells have not reached sufficient operational stability to be commercialised, although many research groups are investigating ways to solve this.\n\nWith a transparent rear side, bifacial solar cells can absorb light from both the front and rear sides. Hence, they can produce more electricity than conventional monofacial solar cells. The first patent of bifacial solar cells was filed by Japanese researcher Hiroshi Mori, in 1966. Later, it is said that Russia was the first to deploy bifacial solar cells in their space program in the 1970s. In 1976, the Institute for Solar Energy of the Technical University of Madrid, began a research program for the development of bifacial solar cells led by Prof. Antonio Luque. Based on 1977 US and Spanish patents by Luque, a practical bifacial cell was proposed with a front face as anode and a rear face as cathode; in previously reported proposals and attempts both faces were anodic and interconnection between cells was complicated and expensive. In 1980, Andrés Cuevas, a PhD student in Luque's team, demonstrated experimentally a 50% increase in output power of bifacial solar cells, relative to identically oriented and tilted monofacial ones, when a white background was provided. In 1981 the company Isofoton was founded in Málaga to produce the developed bifacial cells, thus becoming the first industrialization of this PV cell technology. With an initial production capacity of 300kW/yr. of bifacial solar cells, early landmarks of Isofoton's production were the 20kWp power plant in San Agustín de Guadalix, built in 1986 for Iberdrola, and an off grid installation by 1988 also of 20kWp in the village of Noto Gouye Diama (Senegal) funded by the Spanish international aid and cooperation programs. \n\nDue to the reduced manufacturing cost, companies have again started to produce commercial bifacial modules since 2010. By 2017, there were at least eight certified PV manufacturers providing bifacial modules in North America. It has been predicted by the International Technology Roadmap for Photovoltaics (ITRPV) that the global market share of bifacial technology will expand from less than 5% in 2016 to 30% in 2027.\n\nDue to the significant interest in the bifacial technology, a recent study has investigated the performance and optimization of bifacial solar modules worldwide. The results indicate that, across the globe, ground-mounted bifacial modules can only offer ~10% gain in annual electricity yields compared to the monofacial counterparts for a ground albedo coefficient of 25% (typical for concrete and vegetation groundcovers). However, the gain can be increased to ~30% by elevating the module 1 m above the ground and enhancing the ground albedo coefficient to 50%. Sun \"et al.\" also derived a set of empirical equations that can optimize bifacial solar modules analytically.\n\nAn online simulation tool is available to model the performance of bifacial modules in any arbitrary location across the entire world. It can also optimize bifacial modules as a function of tilt angle, azimuth angle, and elevation above the ground.\n\nIntermediate band photovoltaics in solar cell research provides methods for exceeding the Shockley–Queisser limit on the efficiency of a cell. It introduces an intermediate band (IB) energy level in between the valence and conduction bands. Theoretically, introducing an IB allows two photons with energy less than the bandgap to excite an electron from the valence band to the conduction band. This increases the induced photocurrent and thereby efficiency.\n\nLuque and Marti first derived a theoretical limit for an IB device with one midgap energy level using detailed balance. They assumed no carriers were collected at the IB and that the device was under full concentration. They found the maximum efficiency to be 63.2%, for a bandgap of 1.95eV with the IB 0.71eV from either the valence or conduction band.\nUnder one sun illumination the limiting efficiency is 47%.\n\nIn 2014, researchers at California NanoSystems Institute discovered using kesterite and perovskite improved electric power conversion efficiency for solar cells.\n\nPhoton upconversion is the process of using two low-energy (\"e.g.\", infrared) photons to produce one higher energy photon; downconversion is the process of using one high energy photon (\"e.g.,\", ultraviolet) to produce two lower energy photons. Either of these techniques could be used to produce higher efficiency solar cells by allowing solar photons to be more efficiently used. The difficulty, however, is that the conversion efficiency of existing phosphors exhibiting up- or down-conversion is low, and is typically narrow band.\n\nOne upconversion technique is to incorporate lanthanide-doped materials (, , or a combination), taking advantage of their luminescence to convert infrared radiation to visible light. Upconversion process occurs when two infrared photons are absorbed by rare-earth ions to generate a (high-energy) absorbable photon. As example, the energy transfer upconversion process (ETU), consists in successive transfer processes between excited ions in the near infrared. The upconverter material could be placed below the solar cell to absorb the infrared light that passes through the silicon. Useful ions are most commonly found in the trivalent state. ions have been the most used. ions absorb solar radiation around 1.54 µm. Two ions that have absorbed this radiation can interact with each other through an upconversion process. The excited ion emits light above the Si bandgap that is absorbed by the solar cell and creates an additional electron–hole pair that can generate current. However, the increased efficiency was small. In addition, fluoroindate glasses have low phonon energy and have been proposed as suitable matrix doped with ions.\n\nDye-sensitized solar cells (DSSCs) are made of low-cost materials and do not need elaborate manufacturing equipment, so they can be made in a DIY fashion. In bulk it should be significantly less expensive than older solid-state cell designs. DSSC's can be engineered into flexible sheets and although its conversion efficiency is less than the best thin film cells, its price/performance ratio may be high enough to allow them to compete with fossil fuel electrical generation.\n\nTypically a ruthenium metalorganic dye (Ru-centered) is used as a monolayer of light-absorbing material. The dye-sensitized solar cell depends on a mesoporous layer of nanoparticulate titanium dioxide to greatly amplify the surface area (200–300 m/g , as compared to approximately 10 m/g of flat single crystal). The photogenerated electrons from the light absorbing dye are passed on to the n-type and the holes are absorbed by an electrolyte on the other side of the dye. The circuit is completed by a redox couple in the electrolyte, which can be liquid or solid. This type of cell allows more flexible use of materials and is typically manufactured by screen printing or ultrasonic nozzles, with the potential for lower processing costs than those used for bulk solar cells. However, the dyes in these cells also suffer from degradation under heat and UV light and the cell casing is difficult to seal due to the solvents used in assembly. The first commercial shipment of DSSC solar modules occurred in July 2009 from G24i Innovations.\n\nQuantum dot solar cells (QDSCs) are based on the Gratzel cell, or dye-sensitized solar cell architecture, but employ low band gap semiconductor nanoparticles, fabricated with crystallite sizes small enough to form quantum dots (such as CdS, CdSe, , PbS, etc.), instead of organic or organometallic dyes as light absorbers. Due to the toxicity associated with Cd and Pb based compounds there are also a series of \"green\" QD sensitizing materials in development (such as CuInS CuInSe and CuInSeS). QD's size quantization allows for the band gap to be tuned by simply changing particle size. They also have high extinction coefficients and have shown the possibility of multiple exciton generation.\n\nIn a QDSC, a mesoporous layer of titanium dioxide nanoparticles forms the backbone of the cell, much like in a DSSC. This layer can then be made photoactive by coating with semiconductor quantum dots using chemical bath deposition, electrophoretic deposition or successive ionic layer adsorption and reaction. The electrical circuit is then completed through the use of a liquid or solid redox couple. The efficiency of QDSCs has increased to over 5% shown for both liquid-junction and solid state cells, with a reported peak efficiency of 11.91%. In an effort to decrease production costs, the Prashant Kamat research group demonstrated a solar paint made with and CdSe that can be applied using a one-step method to any conductive surface with efficiencies over 1%. However, the absorption of quantum dots (QDs) in QDSCs is weak at room temperature. The plasmonic nanoparticles can be utilized to address the weak absorption of QDs (e.g., nanostars). Adding an external infrared pumping sources to excite intraband and interband transition of QDs is another solution.\n\nOrganic solar cells and polymer solar cells are built from thin films (typically 100 nm) of organic semiconductors including polymers, such as polyphenylene vinylene and small-molecule compounds like copper phthalocyanine (a blue or green organic pigment) and carbon fullerenes and fullerene derivatives such as PCBM.\n\nThey can be processed from liquid solution, offering the possibility of a simple roll-to-roll printing process, potentially leading to inexpensive, large-scale production. In addition, these cells could be beneficial for some applications where mechanical flexibility and disposability are important. Current cell efficiencies are, however, very low, and practical devices are essentially non-existent.\n\nEnergy conversion efficiencies achieved to date using conductive polymers are very low compared to inorganic materials. However, Konarka Power Plastic reached efficiency of 8.3% and organic tandem cells in 2012 reached 11.1%.\n\nThe active region of an organic device consists of two materials, one electron donor and one electron acceptor. When a photon is converted into an electron hole pair, typically in the donor material, the charges tend to remain bound in the form of an exciton, separating when the exciton diffuses to the donor-acceptor interface, unlike most other solar cell types. The short exciton diffusion lengths of most polymer systems tend to limit the efficiency of such devices. Nanostructured interfaces, sometimes in the form of bulk heterojunctions, can improve performance.\n\nIn 2011, MIT and Michigan State researchers developed solar cells with a power efficiency close to 2% with a transparency to the human eye greater than 65%, achieved by selectively absorbing the ultraviolet and near-infrared parts of the spectrum with small-molecule compounds. Researchers at UCLA more recently developed an analogous polymer solar cell, following the same approach, that is 70% transparent and has a 4% power conversion efficiency. These lightweight, flexible cells can be produced in bulk at a low cost and could be used to create power generating windows.\n\nIn 2013, researchers announced polymer cells with some 3% efficiency. They used block copolymers, self-assembling organic materials that arrange themselves into distinct layers. The research focused on P3HT-b-PFTBT that separates into bands some 16 nanometers wide.\n\nAdaptive cells change their absorption/reflection characteristics depending to respond to environmental conditions. An adaptive material responds to the intensity and angle of incident light. At the part of the cell where the light is most intense, the cell surface changes from reflective to adaptive, allowing the light to penetrate the cell. The other parts of the cell remain reflective increasing the retention of the absorbed light within the cell.\n\nIn 2014, a system was developed that combined an adaptive surface with a glass substrate that redirect the absorbed to a light absorber on the edges of the sheet. The system also includes an array of fixed lenses/mirrors to concentrate light onto the adaptive surface. As the day continues, the concentrated light moves along the surface of the cell. That surface switches from reflective to adaptive when the light is most concentrated and back to reflective after the light moves along.\n\nFor the past years, researchers have been trying to reduce the price of solar cells while maximizing efficiency. Thin-film solar cell is a cost-effective second generation solar cell with much reduced thickness at the expense of light absorption efficiency. Efforts to maximize light absorption efficiency with reduced thickness have been made. Surface texturing is one of techniques used to reduce optical losses to maximize light absorbed. Currently, surface texturing techniques on silicon photovoltaics are drawing much attention. Surface texturing could be done in multiple ways. Etching single crystalline silicon substrate can produce randomly distributed square based pyramids on the surface using anisotropic etchants. Recent studies show that c-Si wafers could be etched down to form nano-scale inverted pyramids. Multicrystalline silicon solar cells, due to poorer crystallographic quality, are less effective than single crystal solar cells, but mc-Si solar cells are still being used widely due to less manufacturing difficulties. It is reported that multicrystalline solar cells can be surface-textured to yield solar energy conversion efficiency comparable to that of monocrystalline silicon cells, through isotropic etching or photolithography techniques. Incident light rays onto a textured surface do not reflect back out to the air as opposed to rays onto a flat surface. Rather some light rays are bounced back onto the other surface again due to the geometry of the surface. This process significantly improves light to electricity conversion efficiency, due to increased light absorption. This texture effect as well as the interaction with other interfaces in the PV module is a challenging optical simulation task. A particularly efficient method for modeling and optimization is the OPTOS formalism. In 2012, researchers at MIT reported that c-Si films textured with nanoscale inverted pyramids could achieve light absorption comparable to 30 times thicker planar c-Si. In combination with anti-reflective coating, surface texturing technique can effectively trap light rays within a thin film silicon solar cell. Consequently, required thickness for solar cells decreases with the increased absorption of light rays.\n\nSolar cells are commonly encapsulated in a transparent polymeric resin to protect the delicate solar cell regions for coming into contact with moisture, dirt, ice, and other conditions expected either during operation or when used outdoors. The encapsulants are commonly made from polyvinyl acetate or glass. Most encapsulants are uniform in structure and composition, which increases light collection owing to light trapping from total internal reflection of light within the resin. Research has been conducted into structuring the encapsulant to provide further collection of light. Such encapsulants have included roughened glass surfaces, diffractive elements, prism arrays, air prisms, v-grooves, diffuse elements, as well as multi-directional waveguide arrays. Prism arrays show an overall 5% increase in the total solar energy conversion. Arrays of vertically aligned broadband waveguides provide a 10% increase at normal incidence, as well as wide-angle collection enhancement of up to 4%. Active coatings that convert infrared light into visible light have shown a 30% increase. Nanoparticle coatings inducing plasmonic light scattering increase wide-angle conversion efficiency up to 3%. Optical structures have also been created in encapsulation materials to effectively \"cloak\" the metallic front contacts.\n\nSolar cells share some of the same processing and manufacturing techniques as other semiconductor devices. However, the stringent requirements for cleanliness and quality control of semiconductor fabrication are more relaxed for solar cells, lowering costs.\n\nPolycrystalline silicon wafers are made by wire-sawing block-cast silicon ingots into 180 to 350 micrometer wafers. The wafers are usually lightly p-type-doped. A surface diffusion of n-type dopants is performed on the front side of the wafer. This forms a p–n junction a few hundred nanometers below the surface.\n\nAnti-reflection coatings are then typically applied to increase the amount of light coupled into the solar cell. Silicon nitride has gradually replaced titanium dioxide as the preferred material, because of its excellent surface passivation qualities. It prevents carrier recombination at the cell surface. A layer several hundred nanometers thick is applied using PECVD. Some solar cells have textured front surfaces that, like anti-reflection coatings, increase the amount of light reaching the wafer. Such surfaces were first applied to single-crystal silicon, followed by multicrystalline silicon somewhat later.\n\nA full area metal contact is made on the back surface, and a grid-like metal contact made up of fine \"fingers\" and larger \"bus bars\" are screen-printed onto the front surface using a silver paste. This is an evolution of the so-called \"wet\" process for applying electrodes, first described in a US patent filed in 1981 by Bayer AG. The rear contact is formed by screen-printing a metal paste, typically aluminium. Usually this contact covers the entire rear, though some designs employ a grid pattern. The paste is then fired at several hundred degrees Celsius to form metal electrodes in ohmic contact with the silicon. Some companies use an additional electro-plating step to increase efficiency. After the metal contacts are made, the solar cells are interconnected by flat wires or metal ribbons, and assembled into modules or \"solar panels\". Solar panels have a sheet of tempered glass on the front, and a polymer encapsulation on the back.\n\nNational Renewable Energy Laboratory tests and validates solar technologies. Three reliable groups certify solar equipment: UL and IEEE (both U.S. standards) and IEC.\n\nSolar cells are manufactured in volume in Japan, Germany, China, Taiwan, Malaysia and the United States, whereas Europe, China, the U.S., and Japan have dominated (94% or more as of 2013) in installed systems. Other nations are acquiring significant solar cell production capacity.\n\nGlobal PV cell/module production increased by 10% in 2012 despite a 9% decline in solar energy investments according to the annual \"PV Status Report\" released by the European Commission's Joint Research Centre. Between 2009 and 2013 cell production has quadrupled.\n\nDue to heavy government investment, China has become the dominant force in solar cell manufacturing. Chinese companies produced solar cells/modules with a capacity of ~23 GW in 2013 (60% of global production).\n\nIn 2014, Malaysia was the world's third largest manufacturer of photovoltaics equipment, behind China and the European Union.\n\nSolar cell production in the U.S. has suffered due to the global financial crisis, but recovered partly due to the falling price of quality silicon.\n\n"}
{"id": "247558", "url": "https://en.wikipedia.org/wiki?curid=247558", "title": "Spork", "text": "Spork\n\nA spork (a portmanteau of \"spoon\" and \"fork\"), is a hybrid form of cutlery taking the form of a spoon-like shallow scoop with two to four tines. Spork-like utensils, such as the terrapin fork or ice cream fork, have been manufactured since the late 19th century; patents for spork-like designs date back to at least 1874, and the word \"spork\" was registered as a trademark in the US and the UK decades later. They are used by fast food restaurants, schools, prisons, the military, backpackers and in airline meals.\n\nThe word \"spork\" combines \"spoon\" and \"fork\". It appeared in the 1909 supplement to the \"Century Dictionary,\" where it was described as a trade name and \"a 'portmanteau-word' applied to a long, slender spoon having, at the end of the bowl, projections resembling the tines of a fork\".\n\nIn the US, patents for sporks and proto-sporks have been issued. A combined spoon, fork, and knife closely resembling the modern spork was invented by Samuel W. Francis and issued in February 1874. Other early patents predating the modern spork include , for a \"cutting spoon\", granted on November 24, 1908 to Harry L. McCoy and , for a spoon with a tined edge, granted to Frank Emmenegger in November 1912. Many of these inventions predated the use of the term \"spork\". Given this significant prior art, the basic concept of combining aspects of a spoon and fork is well established; more modern patents have limited themselves to the specific implementation and appearance of the spork. These design patents do not prevent anyone from designing and manufacturing a different version of a spork. Examples of modern US design patents for sporks include patent number D247,153 issued in February 1978 and patent D388,664 issued in January 1998.\n\nThe \"word\" spork originated in the early 20th century to describe such devices. In 1951, Hyde W. Ballard of Westtown, Pennsylvania filed an application with the United States Patent Office (USPO; now the United States Patent and Trademark Office) to register \"Spork\" as a trademark for a combination spoon and fork made of stainless steel. The Van Brode Milling Company subsequently registered SPORK for a combination plastic spoon, fork and knife at the USPO on October 27, 1970, but the registration expired 20 years later. The word Spork accompanied by a stylised design is registered in the U.S. in relation to hand tools, in the name of a UK-based individual.\n\nIn the UK, Plastico Limited registered Spork as a trademark in relation to cutlery with effect from September 18, 1975 (reg. no. 1052291, now expired). The trademark is also registered in the UK in relation to gardening tools in the name of the same UK based individual who owns US trademark registration no. 2514381. Another British company, Lifeventure, sells titanium and plastic versions using the name \"Forkspoon\".\n\nIn an unsuccessful lawsuit in 1999 where the company Regalzone sought to invalidate Plastico Limited's UK registration for Spork, Justice Neuberger wrote:\n\nMaterials such as stainless steel, silver, aluminum, titanium, and polycarbonate plastic have been used in spork manufacturing. Plastic sporks are common in prisons in the United States because they are difficult to form into shiv-type weapons to attack other inmates. Prepackaged meals may come with a disposable plastic spork. Sporks are also frequently used by backpackers, Boy Scouts and other outdoorsmen as they are a lightweight and space-saving alternative to carrying both a fork and spoon.\n\n\n"}
{"id": "360869", "url": "https://en.wikipedia.org/wiki?curid=360869", "title": "Stewart platform", "text": "Stewart platform\n\nA Gough-Stewart platform is a type of parallel robot that has six prismatic actuators, commonly hydraulic jacks or electric actuators, attached in pairs to three positions on the platform's baseplate, crossing over to three mounting points on a top plate. Devices placed on the top plate can be moved in the six degrees of freedom in which it is possible for a freely-suspended body to move. These are the three linear movements x, y, z (lateral, longitudinal and vertical), and the three rotations pitch, roll, & yaw. The terms \"six-axis\" or \"6-DoF\" (Degrees of Freedom) platform are also used, also \"synergistic\" (see below).\n\nThis specialised six-jack layout was first used by V E (Eric) Gough of the UK and was operational in 1954, the design later being publicised in a 1965 paper by D Stewart to the UK Institution of Mechanical Engineers. Although the short title \"Stewart Platform\" is now used for this jack layout, it would be fairer to Eric Gough to call it a \"Gough/Stewart platform\". To be more precise, the original Stewart platform had a slightly different design. See the more detailed references at the end of this article.\n\nBecause the motions are produced by a combination of movements of several of the jacks, such a device is sometimes called a \"synergistic motion platform\", due to the synergy (mutual interaction) between the way that the jacks are programmed.\n\nBecause the device has six jacks, it is often also known as a \"hexapod\" (six legs). The trademarked name \"hexapod\" (by Geodetic Technology) was originally for Stewart platforms used in machine tools. However, the term is now used for 6-jack platforms outside of the machine tool area, since it simply means \"six legs\".\n\nStewart platforms have applications in flight simulators, machine tool technology, crane technology, underwater research, air-to-sea rescue, mechanical bulls, satellite dish positioning, telescopes and orthopedic surgery.\n\nThe Stewart platform design is extensively used in flight simulation, particularly in the so-called full flight simulator for which all 6 degrees of freedom are required. This application was developed by Redifon, whose simulators featuring it became available for the Boeing 707, Douglas DC-8, Sud Aviation Caravelle, Canadair CL-44, Boeing 727, Comet, Vickers Viscount, Vickers Vanguard,\nConvair CV 990, Lockheed C-130 Hercules, Vickers VC10, and Fokker F-27 by 1962.\n\nIn this role, the payload is a replica cockpit and a visual display system, normally of several channels, for showing the outside-world visual scene to the aircraft crew that are being trained. Payload weights in the case of a full flight simulator for a large transport aircraft can be up to about 15,000 kilogrammes.\n\nSimilar platforms are used in driving simulators, typically mounted on large x-y tables to simulate short term acceleration. Long term acceleration can be simulated by tilting the platform, and an active research area is how to mix the two.\n\nJames S. Albus of the National Institute of Standards and Technology (NIST) developed the RoboCrane, where the platform hangs from six cables instead of being supported by six jacks.\n\nThe Low impact docking system developed by NASA uses a Stewart platform to manipulate space vehicles during the docking process.\n\nThe Computer Assisted Rehabilitation Environment developed by Motek Medical uses a Stewart platform coupled with virtual reality to do advanced biomechanical and clinical research.\n\nDr. J. Charles Taylor utilized the Stewart platform to develop the Taylor Spatial Frame, an external fixator used in orthopedic surgery for the correction of bone deformities and treatment of complex fractures.\n\n\n\n\n"}
{"id": "2900218", "url": "https://en.wikipedia.org/wiki?curid=2900218", "title": "Strap-on dildo", "text": "Strap-on dildo\n\nA strap-on dildo (also strap-on, genitalia or dildo heart) is a dildo designed to be worn, usually with a harness, during sexual activity. Harnesses and dildos are made in a wide variety of styles, with variations in how the harness fits the wearer, how the dildo attaches to the harness, as well as various features intended to facilitate stimulation of the wearer or a sexual partner.\n\nA strap-on dildo can be used for a wide variety of sexual activities, including vaginal sex, gay sex, oral sex, or solo or mutual masturbation. Sexual lubricant can be used to ease insertion, and strap-on dildos can be used by people of any gender or sexuality.\n\nDue to the often taboo nature of strap-on activities, information on their history is difficult to find.\nMany artifacts from the Upper Paleolithic have been found that appear to be dildos, including a double \"baton\" with a hole in the middle, theorized to be for a strap to hold it to a wearer. \n\nFemale-female dildo usage in ancient China has been documented, but it is not clear if this was double-dildos, strap-on dildos, or just a simple dildo being used by one woman on another. \n\nIn ancient Greece, dildos were made of stone or padded leather, and some evidence shows aforementioned leather was used to make a harness as well, with olive oil used for anal penetration. \n\nThe \"Kama Sutra\"(2nd century CE) includes mention of dildos (darshildo in Hindi) made from a wide variety of materials, and used by hand, with ties (straps), or in a harness. \n\nA double-penetration dildo was found in ancient France, but its use is lost to time. \n\nA 19th-century Chinese painting shows a woman using a dildo strapped to her shoe, showing that creative use of strap-ons was already well under way. \n\nAn 1899 report by Haberlandt documented current and historical use of double-ended dildos in Zanzibar, and is one of the few historical documents of this kind. \n\nIt is likely the history of the strap-on parallels the history of the dildo, and given the age of many discoveries, is a rather long history.\n\nThe first part of a strap-on setup is the harness, which connects the dildo to the wearer's body, usually in a position similar to that of a male's genitals. A good harness should be sturdy yet comfortable, and is often designed to provide stimulation for the wearer. Many types of harnesses are available, with different features and drawbacks. Some dildos do not need a harness or are built onto one; for these, please see the sections on dildo types and dildo attachment methods.\n\nA 2-strap harness, in its most basic form, is similar to a g-string. One strap goes around the wearer's waist, like a belt, while the other goes between the wearer's legs and connects to the other strap in the middle at the lower back. While these are simple, many people find them uncomfortable because the strap rubs against the anus and other areas, and they sometimes do not hold the dildo very firmly, causing it to sag, flop, twist, or squeak.\n\nThree-strap harnesses have one strap around the wearer's waist, but instead of one strap between the legs, they have two straps, one around each thigh, rejoining the first strap near the front. This design leaves the genitals and anus uncovered, and attaches the dildo more firmly, giving the wearer more control. Not all people find this design comfortable, and sometimes they are difficult to fit properly, and tend to slip.\n\nStrap-on harnesses built into various clothing items are available, most often as a corset or other item of lingerie. Some are designed to be worn underneath normal clothing for quick use (if done with the dildo in place, either to give the appearance of a penis or to be able to quickly initiate intercourse, this is sometimes called \"packing\"), while others take advantage of the additional strength and sturdiness an item of clothing can provide over a few straps, or just to integrate the strap-on into an erotic outfit.\n\nHarnesses are not limited to the crotch, and indeed many types are available for other body parts. A popular one is a thigh harness, which attaches a dildo to the wearer's thigh (or other part of the legs or arms, though this is much less common), allowing for many unique positions, as penetration is no longer limited to what could be done with a penis. Another unusual design attaches a dildo to the chin of the wearer, allowing vaginal penetration while performing anilingus or vice versa. Also, an additional design is a gag-style harness, in which a gag is inserted into the wearer's mouth and a dildo protrudes at the other end.\n\nHarnesses are available to attach dildos to just about any household object, allowing for many creative uses. A dildo could be attached to a chair, bed, or any other item of furniture, and penetrate someone during other activities, with or without a partner. Another item, while not technically a harness, but worth mentioning, is an inflatable ball, usually 9 to 18 inches (250 to 500 mm) diameter, made of sturdy rubber designed to support the weight of one or two people, with an attachment for a dildo on it. This allows many unique positions, such as double penetration for a woman by lying face down on the ball for vaginal penetration while her partner penetrates her anally doggy style, which is much more effective than a solid object due to the \"bounce\" of the ball. These inflatable balls are also quite popular for solo use.\n\nHarnesses are available in many different materials, and the choice of which to use depends greatly on individual preference.\n\nNylon webbing and soft foam-like synthetic leather are common, relatively affordable, and very durable. Synthetic harnesses are relatively easy to clean and require relatively little maintenance. Some, such as the \"Spare Parts\" harness, are machine-washable.\n\nLeather is comfortable and adjusts to the wearer's body, but still is strong. Leather is harder to clean and requires more work to maintain than other materials.\n\nCloth is used mostly for clothing harnesses such as corsets and other lingerie, although is occasionally used by extremely inexpensive harnesses.\n\nSome harnesses are made with soft plastic, such as flexible vinyl. These are often available in colors besides traditional black, and may be completely transparent (not possible with other materials). They may be less comfortable than other materials, and may be difficult to make fit well, however they are very easy to clean and fairly robust.\n\nLatex harnesses usually cover the entire area, rather than just using straps. They tend to be medium-priced, and have a limited lifespan, especially if used with oil-based lubricants. Latex can require much care, such as special cleaners or shiners to keep it from turning dry and dusty. Latex harnesses may or may not have the dildo(s) molded as part of the harness, and in either case, they tend to be floppy due to the flexibility of the latex. Similar harnesses are also available made of rubber or PVC, and are similar to latex harnesses, although PVC tends to be much less flexible and elastic.\n\nThe most noticeable feature of any strap-on setup is the dildo used. A wide variety of dildos are available, and while the choices may be limited by the type of harness in use, generally one can choose from several common types. This section discusses the shape and features of the penetrating end of the dildo, not of the entire dildo or how it's attached, which can be found in the section on dildo attachment methods for the double dildo and strapless dildo.\n\nThe standard dildo has a non-tapered or slightly tapered shaft, and often has an enlarged head, similar to a penis. The shaft may be slightly curved, but if it is strongly curved, it is often classified as a g-spot/prostate dildo as well. This type is by far the most popular, both for vaginal and anal use, although some beginners prefer a probe-type dildo. Depending on the type of harness the dildo is meant for, it may have molded testicles as part of the base, which many people say gives more pleasure and helps keep the dildo from \"bottoming out.\" Standard dildos are the most common by a large margin, and are available in virtually any length and width, material, texture, etc.\n\nA probe dildo is often highly tapered, and many resemble a cone in overall shape, or may have a narrow diameter its entire length, although ones resembling an elongated butt plug are also common, their defining feature being a bulb in the middle which tapers down again towards the harness before flaring wider. These dildos are often advertised as being for beginners, especially newcomers to pegging, who may find a narrow, tapered dildo easier to start with if they have never had anal penetration before. Many people find that once they are familiar with the activity, the probe dildos are inadequate and unsatisfying, and purchase a standard dildo to use with their harness. Due to this, many kits include both a probe dildo and a standard dildo, so it is not necessary to purchase another.\n\nDildos designed for g-spot or prostate stimulation often have a sharp upwards curve at the head, or gradual but still strong curvature the entire length. When used in many sexual positions, the curve causes strong pressure against the g-spot in women or the prostate in men. Some men report that strong prostate stimulation is important for an anal orgasm, while others report it as a distraction rather than a help. When using one of these dildos for the first time, care should be taken at first to make sure it's comfortable for the receiver, as the strong bend can be difficult to insert or control. For many positions, such as doggy style, the curved tip should point downwards, as otherwise it points in the wrong direction for either g-spot or prostate stimulation.\n\nFor added stimulation, many dildos for harness use are manufactured with bulbs along their length, or having ripples, bumps, or other shapes on the surface to increase the stimulation given to the receiver. A bulbous dildo causes repeated expansion of the vaginal opening when used for vaginal penetration or of the anus when used for anal penetration, and causes a unique pulsating effect and additional stimulation. Ripples along the length of the dildo increase friction and cause a washboard effect, which some people find increases their pleasure, while others find tend to cause numbness. Individual bumps or a water droplet texture on the shaft also causes extra stimulation, and often avoids the washboard effect of evenly spaced ripples. Some dildos modeled to be realistic penises contain very heavy vein textures on the surface, causing a similar effect.\n\nSome dildos, especially recent ones, may have rotating beads along the shaft, a wiggling shaft, vibrating ticklers or other clitoral stimulation devices, or other features to give more pleasure to the user. While their effectiveness is a matter of opinion, they are becoming increasingly popular. An inexpensive design is basically a standard rabbit vibrator designed for harness use (often exactly the same toy with a slightly different base), while more expensive dildos are designed from the ground-up for harness use and are usually superior. Rotating beads provide extra stimulation of the vulva and vaginal opening when used for vaginal penetration or stimulation of the anus when used for anal penetration, while the various clitoral stimulation devices are generally intended only for vaginal intercourse. These dildos are often bulky or heavy, and like all other vibrators need a power source (usually batteries in a pack that clips onto the harness or slips into a pocket on it), but can provide additional stimulation for those who desire it.\n\nDouble penetration dildos are generally two dildos molded onto a common base, designed for simultaneous vaginal and anal penetration or simultaneous vaginal and vaginal penetration, not to be confused with a man using a strap-on along with his penis for double penetration, which is discussed below. A typical double-penetration dildo has a longer, thicker main shaft for vaginal penetration, and a shorter, thinner, often more curved shaft for anal penetration. Although rare, dildos with the anal shaft being equally as large as the vaginal shaft are available for women who find a larger anal dildo more satisfying. These dildos tend to greatly limit the possible positions they are used in, as the angle has to be right for both vaginal and anal penetration when thrusting, however they can provide a unique experience for couples to try.\n\nMany specialty dildos are available, such as ones that expand when an attached inflator bulb is squeezed, simulate ejaculation by releasing warm water on demand when a reservoir is compressed, or function as enema nozzles, allowing an enema to be given while using the strap-on for anal intercourse. Inflatable dildos generally expand in girth when inflated (they usually come with a simple hand-squeeze inflator bulb), allowing the dildo to keep expanding during intercourse as the receiver slowly stretches, giving a unique completely filled feeling which is hard to obtain using normal dildos. Ejaculating dildos contain a squeeze bulb or other reservoir, which when filled with hot water beforehand, allows the wearer to \"come\" into the receiver at the proper moment. Enema nozzle dildos contain tubing connections, and when used for anal penetration (most often with silicone lube, as water-soluble lube would quickly break down when combined with an enema) allow the receiver to receive an enema during intercourse. Some BDSM mistresses specialize in offering this service. A relatively new product in this field is dildos with electrodes for erotic electrostimulation, further increasing the range of sensations the receiver can experience.\n\nHollow dildos are often sold as penis extensions to increase size (length and/or girth), but the most common use is for men with erectile dysfunction. The penis is inserted into the hollow inside of the dildo, then the harness is put on, allowing the man to penetrate his partner with the dildo, the thrusting of which is transferred to his penis.\n\nDildos with retractable foreskins or uncircumcised dildos are to try to give a natural feel. They can often have fake veins in order to enhance this feel.\n\nThe principal feature of any strap-on setup is the attachment—the dildo used to penetrate the other partner. While there's a huge array of different dildos available, most are attached to the harness in one of several ways. All methods have tradeoffs, and many couples will have different harnesses depending on which type of dildo they want to use.\n\nThe most common means of holding the dildo to the harness is to have an opening or ring in the harness through which a wide-based dildo is inserted. Inexpensive harnesses tend to just have a round hole in the fabric or leather, while more expensive ones will use a steel or rubber ring. The advantage of this method is that dildos which fit are widely available and inexpensive, and even many dildos not meant for harness use will work in one of these harnesses, such as most dildos with testicles. The major disadvantage is the dildo is often held loosely (especially on cheap harnesses) and tends to flop downwards, and the dildo often can rotate in the harness, making it difficult or impossible to use g-spot or other shaped dildos.\n\nUsed primarily by Doc Johnson, Vac-u-lock dildos have a rippled hollow in their base, which fits onto a matching plug on the harness or other toy. Powdered lubricant is used on the plug to facilitate removal. The advantage of this design is the dildo is firmly attached and can not easily rotate, and does not tend to flop downwards or slip like ring harnesses, as well as there being a wide variety of other devices the dildos can be attached to, such as handles and inflatable balls. The disadvantage is the relatively low availability and high cost of compatible attachments. A number of brands have non-compatible clones of the vac-u-lock system, but dildos and accessories for them are virtually unavailable.\n\nA recent design is a strapless strap-on, designed to be held in a vagina or anus with an egg-shaped bulb and thus not to require the use of a harness. This differs from a double dildo where both ends are phallic and a harness is required. The Feeldoe is a strapless dildo which was patented by Melissa Mia Kain in 1997. Advantages of this design are that it can be used spontaneously, that it provides deep internal thrusting to both partners, and that the lack of harness makes it more comfortable. Disadvantages are that the eggs do not prevent rotation or droop, leading to a reduced amount of control unless a harness is employed anyway; a requirement for strong muscles; and the practice needed to become familiar with its use. Many strapless strap ons can also be used with a harness when partners want to increase control. Not all harnesses are suitable for strapless strap ons. It is highly recommended that you use a strap on with very adjustable O-ring or a two-hole harness.\n\nA double dildo can be used with or without a harness, although most provide no means of using them with a harness. A double dildo, as its name implies, is a dildo in which both ends are designed for insertion, and often is 18 inches (450 mm) or more long. While special ones meant for use with a harness do exist, a normal double dildo is straight, is rarely at a comfortable angle for intercourse, and jabs into the cervix of a woman using one. However, double dildos can be used for a variety of creative positions for which a harness would be awkward, such as both partners in doggy style positions or sitting facing each other.\n\nSome harnesses and dildos are made of a single piece of molded latex, with the dildo permanently built into the front. These are often inexpensive, shoddy products, although higher quality ones are available. The advantages are simple use and an appearance some people find erotic, but this is often outweighed by the inability to use a different dildo, the often weak, floppy nature of the dildo, the inability to adjust to a given person for a comfortable fit, and their relatively short lifespan.\n\nSome very cheap \"strap-on dildos\" have straps or attachments for straps directly molded into the material of the dildo. This design is very flimsy, and is only used on the cheapest products. They are essentially useless for the traditional purpose of a strap-on dildo (one partner penetrating another using a dildo in a position similar to a penis), but can be strapped around chairs and other objects for a variety of other activities.\n\nPleasure for the wearer of the strap-on generally comes from pressure against the genitals, particularly the clitoris, during use and from psychological arousal. Many designs of strap-on have various features to increase the stimulation of the wearer.\n\nSome harnesses intentionally leave the genital area and anus open (either intentionally with an opening in the material or by the design simply not having any straps that would cover it), which allows any toy to be used for the stimulation of the wearer, or even for the wearer to be penetrated while wearing the strap-on. This can often be useful when the partners wish to switch roles during their play, as the strap-on can be put on before hand without interfering or needing to be taken off for play to continue. This type of harness is ill-suited for using toys, however, as the harness would not touch the toys, both preventing them from falling out while thrusting and not providing movement to them from the harness.\n\nSome harnesses and dildos provide raised bumps or other features designed to rub against the clitoris of the wearer, either attached to the inside of the harness, or on the back of the base of the dildo. Harnesses that work with such dildos have to have an open back, where the base of the dildo presses directly against the user's body. As high-quality harnesses usually have padding or other means of attaching the dildo to the harness than a simple opening, these features are usually only seen on low-quality, inexpensive dildos. They provide only limited stimulation, and while better than nothing, are usually considered inferior to other types.\n\nAnother means of providing stimulation to the wearer is a vibrating egg, \"clit blaster\", vibrating gel pads or whiskers, or other device mounted on the inside of the strapon. These are almost always intended for use by people with vulva, as the external vibrator is rarely positioned well nor provides stimulation for a penis. These devices provide external stimulation to the clitoris, vagina, and other parts of the vulva, but do not provide any penetrative stimulation or anal stimulation.\n\nWhile a double dildo obviously would provide stimulation to the wearer as well, the standard straight dildo is very poorly suited for harness use. To overcome this, many dildos are available for harness use that have an offset in the middle, with the main attachment and a smaller vaginal attachment for the wearer having a flat vertical section between them. This way, the main attachment is at a good angle and position for thrusting, while its movement is transmitted directly to the vaginal plug and clitoris of the wearer.\n\nThe latest technological achievement is totally new kind of strapless strap on. A special example of these are Feeldoe, Sysil, Super Strapless Silicone Dildo, Share, Nexus Maximus and Transfer, which has an egg shaped bulb designed to be inserted into the vagina or anus of the wearer. Due to the shape of the dildo, thrusting on the main dildo translates to lateral movement of the plug, providing great stimulation to the wearer. Additionally, the shape of the plug allows it to be used without a harness in many instances. It can also allow a man to perform a double penetration (actually doing anal while the dildo penetrate the vagina) while being anal-plugged himself, all with only a single toy.\n\nMany of the \"professional\" harnesses have one or two plugs (vaginal, anal, or both) on the inside of the harness, to penetrate the wearer. These plugs (may be shaped either like small dildos, the standard butt plug shape, or a combination/hybrid thereof) are usually firmly attached to the harness, and provide stimulation to the wearer as they thrust with the main dildo. Some harnesses may only come with the anal plug while others come with both plugs; most report the latter provides the more pleasure than using one.\n\nWhile a plug can be used in combination with most any harness, just by inserting the plugs before putting on the harness, all the harness tends to do is push the plugs in, and not move them as to provide stimulation when the wearer thrusts. Depending on the type of harness, different ways are used to properly attach plugs to the harness. A common type consists of an opening or rubber ring with a cloth or leather back, similar to what might be used to hold the main dildo to the front of the harness, but positioned over the anus, vagina, or ones for both. A dildo/plug with a wide base is inserted through the ring, then when the harness is put on, the material pulls tight against it, holding it firm. For vac-u-lock harnesses, one or two additional vac-u-lock plugs are mounted on the inside of the harness, allowing any vac-u-lock attachment to be used. Most vac-u-lock harnesses that have the connectors for internal plugs come with two plug-shaped vac-u-lock attachments, a smaller one for anal use and a larger one for vaginal use. Like other types of harnesses, both plugs may be used at once, and often are separately adjustable on the strap to fit the wearer's body.\n\nA number of positions can be performed using a strap-on dildo. These include:\n\n\nEven more than the different types of harnesses and dildos, there are different ways to make use of a strap-on setup. Generally ways of using them can be divided into several broad categories, however there are infinite variations.\n\n\nAlmost every position associated with sexual intercourse (or not, in the case of simultaneous penetration) can be performed with a strap-on. Indeed, with other harnesses that allow a dildo to be mounted on inanimate objects, endless new positions can be conceived.\n\n"}
{"id": "2438898", "url": "https://en.wikipedia.org/wiki?curid=2438898", "title": "Stuck-at fault", "text": "Stuck-at fault\n\nA stuck-at fault is a particular fault model used by fault simulators and automatic test pattern generation (ATPG) tools to mimic a manufacturing defect within an integrated circuit. Individual signals and pins are assumed to be \"stuck\" at Logical '1', '0' and 'X'. For example, an input is tied to a logical 1 state during test generation to assure that a manufacturing defect with that type of behavior can be found with a specific test pattern. Likewise the input could be tied to a logical 0 to model the behavior of a defective circuit that cannot switch its output pin.\nNot all faults can be analyzed using the stuck-at fault model. Compensation for static hazards, namely branching signals, can render a circuit untestable using this model. Also, redundant circuits cannot be tested using this model, since by design there is no change in any output as a result of a single fault.\n\nSingle stuck line is a fault model used in digital circuits. It is used for post manufacturing testing, not design testing. The model assumes one line or node in the digital circuit is stuck at logic high or logic low. When a line is stuck it is called a fault.\n\nDigital circuits can be divided into:\n\nThis fault model applies to gate level circuits, or a block of a sequential circuit which can be separated from the storage elements.\nIdeally a gate-level circuit would be completely tested by applying all possible inputs and checking that they gave the right outputs, but this is completely impractical: an adder to add two 32-bit numbers would require 2 = 1.8*10 tests, taking 58 years at 0.1 ns/test.\nThe \"stuck at\" fault model assumes that only one input on one gate will be faulty at a time, assuming that if more are faulty, a test that can detect any single fault, should easily find multiple faults.\n\nTo use this fault model, each input pin on each gate in turn, is assumed to be grounded, and a \"test vector\" is developed to indicate the circuit is faulty. The test vector is a collection of bits to apply to the circuit's inputs, and a collection of bits expected at the circuit's output. If the gate pin under consideration is grounded, and this test vector is applied to the circuit, at least one of the output bits will not agree with the corresponding output bit in the test vector. After obtaining the test vectors for grounded pins, each pin is connected in turn to a logic one and another set of test vectors is used to find faults occurring under these conditions. Each of these faults is called a single \"stuck-at-0\" or a single \"stuck-at-1\" fault, respectively.\n\nThis model worked so well for transistor-transistor logic (TTL), which was the logic of choice during the 1970s and 1980s, that manufacturers advertised how well they tested their circuits by a number called \"stuck-at fault coverage\", which represented the percentage of all possible stuck-at faults that their testing process could find.\nWhile the same testing model works moderately well for CMOS, it is not able to detect all possible CMOS faults. This is because CMOS may experience a failure mode known as a \"stuck-open\" fault, which cannot be reliably detected with one test vector and requires that two vectors be applied sequentially. The model also fails to detect bridging faults between adjacent signal lines, occurring in pins that drive bus connections and array structures. Nevertheless, the concept of single stuck-at faults is widely used, and with some additional tests has allowed industry to ship an acceptable low number of bad circuits.\n\nThe testing based on this model is aided by several things:\n\n"}
{"id": "3625247", "url": "https://en.wikipedia.org/wiki?curid=3625247", "title": "The Petroleum Trail", "text": "The Petroleum Trail\n\nThe Petroleum Trail is an international tourist trail which runs from Poland to Ukraine linking places associated with the petroleum industry of the 19th century.\n\nDuring the mid 19th and early 20th centuries, significant oil reserves were discovered and developed in Galicia near Drohobych and Boryslav, The first European attempt to drill for oil was in Bóbrka, Krosno County, Western Galicia, in 1854. By 1867, a well at Kleczany was drilled to about 200 meters using steam. On December 31, 1872, a railway line linking Borysław (now Boryslav) with the nearby city of Drohobycz (now Drohobych) was opened. American John Simon Bergheim and Canadian William Henry McGarvey came to Galicia in 1882. In 1883, their company, MacGarvey and Bergheim, bored holes of 700 to 1,000 meters and found large oil deposits. In 1885, they renamed their oil developing enterprise the \"Galician-Karpathian Petroleum Company\" (), headquartered in Vienna, with McGarvey as the chief administrator and Bergheim as field engineer, and built a huge refinery at Maryampole near Gorlice, in the southeast corner of Galicia. \n\nConsidered the biggest, most efficient enterprise in Austro-Hungary, Maryampole was built in six months and employed 1,000 men. Subsequently, investors from Britain, Belgium, and Germany established companies to develop the oil and natural gas industries in Galicia. This influx of capital caused the number of petroleum enterprises to shrink from 900 to 484 by 1884, and to 285 companies manned by 3,700 workers by 1890. However, the number of oil refineries increased from thirty-one in 1880 to fifty-four in 1904. By 1904, there were thirty boreholes in Borysław of over 1,000 meters. Production increased by 50% between 1905 and 1906 and then trebled between 1906 and 1909 because of unexpected discoveries of vast oil reserves of which many were gushers. By 1909, production reached its peak at 2,076,000 tons or 4% of worldwide production. Often called the \"Polish Baku\", the oil fields of Borysław and nearby Tustanowice accounted for over 90% of the national oil output of the Austria-Hungary Empire. From 500 residents in the 1860s, Borysław had swollen to 12,000 by 1898. At the turn of the century, Galicia was ranked fourth in the world as an oil producer. This significant increase in oil production also caused a slump in oil prices. A very rapid decrease in oil production in Galicia occurred just before the Balkans conflicts.\n\nGalicia was the Central Powers only major domestic source of oil during the Great War.\n\n\n\n\n"}
{"id": "32855569", "url": "https://en.wikipedia.org/wiki?curid=32855569", "title": "Thermochemical nanolithography", "text": "Thermochemical nanolithography\n\nThermochemical nanolithography (TCNL) or \"thermochemical scanning probe lithography\" (tc-SPL) is a scanning probe microscopy-based nanolithography technique which triggers thermally activated chemical reactions to change the chemical functionality or the phase of surfaces. Chemical changes can be written very quickly through rapid probe scanning, since no mass is transferred from the tip to the surface, and writing speed is limited only by the heat transfer rate. TCNL was invented in 2007 by a group at the Georgia Institute of Technology. Riedo and collaborators demonstrated that TCNL can produce local chemical changes with feature sizes down to 12 nm at scan speeds up to 1 mm/s.\n\nTCNL was used in 2013 to create a nano-scale replica of the Mona Lisa \"painted\" with different probe tip temperatures. Called the \"Mini Lisa\", the portrait measured , about 1/25,000th the size of the original.\n\nThe AFM thermal cantilevers are generally made from a silicon wafers using traditional bulk and surface micro-machining processes. Through the application of an electric current through its highly doped silicon wings, resistive heating occurs at the light doping zone around the probe tip, where the largest fraction of the heat is dissipated. The tip is able to change its temperature very quickly due to its small volume; an average tip in contact with polycarbonate has a time constant of 0.35 ms. The tips can be cycled between ambient temperature and 1100 °C at up to 10 MHz while the distance of the tip from the surface and the tip temperature can be controlled independently.\n\nThermally activated reactions have been triggered in proteins, organic semiconductors, electroluminescent conjugated polymers, and nanoribbon resistors. Deprotection of functional groups (sometimes involving a temperature gradients), and the reduction of graphene oxide has been demonstrated. The wettability of a polymer surface at the nanoscale has been modified, and nanostructures of poly(p-phenylene vinylene) (a electroluminescence conjugated polymer) have been created. Nanoscale templates on polymer films for the assembly of nano-objects such as proteins and DNA have also been created and crystallization of ferroelectric ceramics with storage densities up to 213 Gb/in have been produced.\n\nThe use of a material that can undergo multiple chemical reactions at significantly different temperatures could lead to a multi-state system, wherein different functionalities can be addressed at different temperatures.\n\n\"Thermo-mechanical\" scanning probe lithography relies on the application of heat and force order to create indentations for patterning purposes (see also: Millipede memory). \"Thermal scanning probe lithography\" (t-SPL) specializes on removing material from a substrate without the intent of chemically altering the created topography. Local oxidation nanolithography relies on oxidation reactions in a water meniscus around the probe tip.\n\n\n"}
{"id": "40013560", "url": "https://en.wikipedia.org/wiki?curid=40013560", "title": "Tibetan Buddhist Resource Center", "text": "Tibetan Buddhist Resource Center\n\nTibetan Buddhist Resource Center (TBRC) is a 501(c)(3) non-profit organization dedicated to seeking out, preserving, organizing, and disseminating Tibetan Buddhist texts and Tibetan literature. Founded in 1999 by E. Gene Smith, TBRC is located in Cambridge, Massachusetts and hosts a digital library of the largest collection of digitized Tibetan texts in the world.\n\nTBRC's Harvard Square headquarters facilitates its ongoing cooperative relationships with Harvard University. TBRC also has international offices in New Delhi, India and Kathmandu, Nepal, and a newly opened scanning and text-preservation center located within the E. Gene Smith Library at Southwest University for Nationalities in Chengdu, China.\n\nTo preserve and share the Tibetan literary heritage through the union of technology and scholarship.\n\nIn the early 1960s, while working on his PhD. at the University of Washington, E. Gene Smith studied with the Venerable Dezhung Rinpoche. In 1964, Dezhung Rinpoche encouraged Smith to move to India in order to seek out and study Tibetan books more directly. He gave Smith letters of introduction to show to the lamas living among the Tibetan diaspora.\n\nIn 1968 the U.S. Library of Congress hired Smith as a field director in New Delhi where he worked on the Food for Peace humanitarian effort Public Law 480. Through the program, Smith began to copy and print thousands of Tibetan texts while keeping a version of each one for his own collection. He moved from India to Indonesia in 1985 and then Egypt, along with his collection of 12,000 volumes of texts.\n\nIn 1997 Smith retired from the Library of Congress and began working to implement his vision of making the preserved texts accessible using the new scanning and digitization technologies that were, at that time, just beginning to become available. In 1999 with friends including Harvard professor and fellow Tibetologist Leonard van der Kuijp, he founded the Tibetan Buddhist Resource Center in Cambridge, Massachusetts. Smith's texts from India that were digitized at TBRC became the foundation for Tibetan studies in the United States.\n\nIn 2002 with the support of Shelley and Donald Rubin, TBRC moved to New York City where Smith became an advisor to the Rubin Museum of Art. Major grants from the Patricia and Peter Gruber Foundation, Khyentse Foundation, and the Shelley and Donald Rubin Foundation allowed TBRC to acquire a significant number of texts, develop its archiving system, and add more professional staff. Starting as Technical Director in 2001, Jeff Wallman was personally selected by Smith to be Executive Director and was appointed by the Board of Directors in 2009.\n\nGene Smith died on December 16, 2010. TBRC had scanned 7 million pages of Tibetan texts at the time of his death.\n\nTBRC seeks out and preserves undiscovered texts, organizes them into a library catalog system, and disseminates the library online and to remote locations on hard drives so anyone can read, print, or share the texts. Tibetan language texts are cataloged by work, genre, subject, person, and place.\n\nCurrently, the collection contains more than 7,000 works (17,000 volumes, totaling nearly 9 million pages) of Tibetan texts. Scholars and students are able to study the physical qualities of the texts since the scans are searchable and zoomable.\n\nOver 5,000 users from 66 countries currently access the website per day, up from 815 per day in 2006. Between 500,000 and 1,000,000 pages are added every year.\n\nTBRC's work was recognized by the 17th Karmapa Ogyen Trinley Dorje in a letter offering his support, gratitude, and prayers. Gene Smith's life and TBRC were the subject of the 2012 documentary Digital Dharma, directed by Dafna Yachin of Lunchbox Communications. Variety film critic John Anderson described the film as, \"A divinely inspired gift... also an affectionate tribute to the late E. Gene Smith, the scholar, librarian and ex-Mormon who waged a 50-year struggle to save the endangered texts of Tibetan Buddhism.\"\n\nIn summer 2012 TBRC relocated back to Harvard Square in Cambridge, MA, where the staff hand-picked by Smith continues its ongoing mission to preserve and provide access to Tibetan literature.\n\nIn cooperation with the Harvard University Open Access Project (HOAP), TBRC is currently working to make its entire library completely open access. TBRC also coordinates internships with graduate students from Harvard Divinity School and the Department of South Asian Studies at Harvard.\n\nIn 2007, Gene Smith bequeathed his personal collection of 12,000 Tibetan texts to the Southwest University for Nationalities in Chengdu, China. These texts are now housed in a newly constructed library that bears Smith's name, and which is now China's pre-eminent center for Tibetan literature.\n\nIn October 2013, TBRC opened a scanning and text-preservation center at SWUN's E. Gene Smith Library, where four full-time archivists scan, catalog, and digitize Tibetan manuscripts, some centuries old, that are being brought to the library from around the region as news of its existence spreads. Some of these recent additions, now preserved and accessible in TBRC's digital archive, represent the only known versions of ancient texts.\n\nTBRC announced the expansion of institutional mission to include the preservation of texts in languages beyond Tibetan, including Sanskrit, Pali and Chinese. To reflect this expansion, they have officially changed organizational name from Tibetan Buddhist Resource Center (TBRC) to Buddhist Digital Resource Center (BDRC). In 2017, BDRC will begin preserving and making accessible texts in languages beyond Tibetan, starting with Pali, Sanskrit, and Chinese.\n\nExecutive Director: Jeffrey D. Wallman\n\nCangioli K. Che, Patricia Gruber, Janet Gyatso, Leonard van der Kuijp, Derek Kolleeny, Richard Lanier, David Lunsford (emeritus), Michele Martin, Timothy J. McNeill, Tudeng Nima Rinpoche, Dzogchen Ponlop Rinpoche (honorary), Shelley F. Rubin (emeritus), E. Gene Smith (emeritus, in memoriam), Tulku Thondup Rinpoche, Gray Tuttle, Lama Zopa Rinpoche (honorary), Jeffrey D. Wallman\n\n"}
{"id": "40978013", "url": "https://en.wikipedia.org/wiki?curid=40978013", "title": "Turbine inlet air cooling", "text": "Turbine inlet air cooling\n\nTurbine inlet air cooling is a group of technologies and techniques consisting of cooling down the intake air of the gas turbine. The direct consequence of cooling the turbine inlet air is power output augmentation. It may also improve the energy efficiency of the system. This technology is widely used in hot climates with high ambient temperatures that usually coincides with on-peak demand period.\n\nGas turbines take in filtered, fresh ambient air and compress it in the compressor stage. The compressed air is mixed with fuel in the combustion chamber and ignited. This produces a high-temperature and high-pressure flow of exhaust gases that enter in a turbine and produce the shaft work output that is generally used to turn an electric generator as well as powering the compressor stage.\nAs the gas turbine is a constant volume machine, the air volume introduced in the combustion chamber after the compression stage is fixed for a given shaft speed (rpm). Thus the air mass flow in is directly related to the density of air, and the introduced volume.\n\nwhere formula_2 is the mass, formula_3 is the density and formula_4 is the volume of the gas. As the volume formula_4 is fixed, only density formula_3 of the air can be modified to vary air mass. The density of the air depends of the relative humidity, altitude, pressure drop and temperature.\n\nwhere:\n\nThe performance of a gas turbine, its efficiency (heat rate) and the generated power output strongly depend on the climate conditions, which may decrease the output power ratings by up to 40%.\nTo operate the turbine at ISO conditions and recover performance, several inlet air cooling systems have been promoted.\n\nDifferent technologies are available in the market. Each particular technology has its advantages and inconveniences according to different factors such as ambient conditions, investment cost and payback time, power output increase and cooling capacity.\n\nInlet air fogging consists of spraying finely atomized water (fog) into the inlet airflow of a gas turbine engine. The water droplets evaporate quickly, which cools the air and increases the power output of the turbine.\n\nDemineralized water is typically pressurized to 2000 psi (138 bar) then injected into the inlet air duct through an array of stainless steel fog nozzles. Demineralized water is used in order to prevent fouling of the compressor blades that would occur if water with mineral content were evaporated in the airflow. Fog systems typically produce a water spray, with about 90% of the water flow being in droplets that are 20 microns in diameter or smaller.\n\nInlet fogging has been in commercial use since the late 1980s and is a popular retrofit technology. As of 2015, there were more than 1000 inlet fog systems installed around the world. Inlet fog systems are, “simple, easy to install and operate” and less expensive than other power augmentation systems such as evaporative coolers and chillers.\n\nInlet fogging is the least expensive gas turbine inlet air cooling option and has low operating costs, particularly when one accounts for the fact that fog systems impose only a negligible pressure drop on the inlet airflow when compared to media-type evaporative coolers.\n\nFog nozzle manifolds are typically located in the inlet air duct just downstream of the final air filters but other locations can be desirable depending on the design of the inlet duct and the intended use of the fog system.\n\nOn a hot afternoon in a desert climate, it is possible to cool by as much as 40 °F (22.2 °C), while in a humid climate hot-afternoon cooling potential can be just 10 °F (5.6 °C) or less. Nevertheless, there are many successful inlet-fogging installations in humid climates such as Thailand, Malaysia and the American Gulf States.\n\nInlet fogging reduces emissions of Oxides of Nitrogen (NOx) because the additional water vapor quenches hot spots in the combustors of the gas turbine.\n\nFog systems can be used to produce more power than can be obtained by evaporative cooling alone. This is accomplished by spraying more fog than is required to fully saturate the inlet air. The excess fog droplets are carried into the gas turbine compressor where they evaporate and produce an intercooling effect, which results in a further power boost. This technique was first employed on an experimental gas turbine in Norway in 1903. There are many successful systems in operation today.\n\nSeveral gas turbine manufactures offer both fogging and wet compression systems. Systems are also available from third-party manufacturers.\n\nThe evaporative cooler is a wetted rigid media where water is distributed throughout the header and where air passes through the wet porous surface. Part of the water is evaporated, absorbing the sensible heat from the air and increasing its relative humidity. The air dry-bulb temperature is decreased but the wet-bulb temperature is not affected. Similar to the fogging system, the theoretical limit is the wet bulb temperature, but performance of the evaporative cooler is usually around 80%. Water consumption is less than that of fogging cooling.\n\nIn a mechanical compression chiller technology, the coolant is circulated through a chilling coil heat exchanger that is inserted in the filter house, downstream from the filtering stage. Downstream from the coil, a droplet catcher is installed to collect moisture and water drops. The mechanical chiller can increase the turbine output and performance better than wetted technologies due to the fact that inlet air can be chilled below the wet bulb temperature, indifferent to the weather conditions. Compression chiller equipment has higher electricity consumption than evaporative systems. Initial capital cost is also higher, however turbine power augmentation and efficiency is maximized, and the extra-cost is amortized due to increased output power.\n\nThe majority of such systems involve more than one chiller unit and the configuration of the chillers can have a great bearing on the system parasitic power consumption. The series counterflow configuration can reduce the compressor work needed on each chiller, improving the overall chiller system by as much as 8%.\n\nOther options such a steam driven compression are also used in industry.\n\nIn vapor-absorption chillers technology, thermal energy is used to produce cooling instead of mechanical energy. The heat source is usually leftover steam coming from combined cycle, and it is bypassed to drive the cooling system. Compared to mechanical chillers, absorption chillers have a low coefficient of performance, however, it should be taken into consideration that this chiller usually uses waste heat, which decreases the operational cost.\n\nA thermal energy storage tank is a naturally stratified thermal accumulator that allows the storage of chilled water produced during off-peak time, to use this energy later during on-peak time to chill the turbine inlet air and increment its power output. A thermal energy storage tank reduces operational cost and refrigerant plant capacity. One advantage is the production of chilled water when demand is low, using the excess of power generation, which usually coincides with the night, when ambient temperature is low and chillers have better performance. Another advantage is the reduction of the chilling plant capacity and operational cost in comparison with an on-line chilling system, which produce delays during periods of low demand.\n\nIn areas where there is demand cooling, daily summer on-peak periods coincide with the highest atmospheric temperatures, which may reduce the efficiency and power gas turbines. With the vapor mechanical compression technologies, cooling can be used during these periods so that the performance and the power output of the turbine may be less affected by ambient conditions\n\nAnother benefit is the lower cost per extra inlet-cooling kilowatt compared to newly installed gas turbine kilowatt. Moreover, the extra inlet-cooling kilowatt uses less fuel than the new turbine kilowatt due to the lower heat-rate (higher efficiency) of the chilled turbine. Other benefits may include the incrementation of steam mass flow in a combined cycle, the reduction of turbine emissions (SOx, NOx, CO2), and increase in power-to-installed volume ratio.\n\nCalculating the benefits of turbine air cooling requires a study to determine payback periods, taking into consideration several aspects like ambient conditions, cost of water, hourly electric demand values, cost of fuel.\n\n\n"}
{"id": "5361288", "url": "https://en.wikipedia.org/wiki?curid=5361288", "title": "University of Warwick Science Park", "text": "University of Warwick Science Park\n\nThe University of Warwick Science Park (also known as UWSP and Warwick Science Park) was one of the first university based science parks in the United Kingdom when it was opened by the Rt. Hon. Margaret Thatcher in 1984. It was a joint venture between the University of Warwick, Coventry City Council, Warwickshire County Council and Barclays. The latter are no longer shareholders having been replaced by WM Enterprise. The University of Warwick agreed in 2011 to purchase the shareholding of Coventry City Council. The acquisition was completed in 2012 and today the Science Park is wholly owned by the University of Warwick. UWSP currently covers four sites; the main campus abutting the University of Warwick, the Business Innovation Centre in Binley, Warwick Innovation Centre on Warwick Technology Park and Blythe Valley Innovation Centre near Solihull.\n\nUWSP typically provides office and lab space for 145 businesses employing more than 1800 people.\n\nThe University of Warwick Science Park was established in 1984 by the then Vice Chancellor of the University of Warwick, Lord Butterworth. The first building, the Barclay's Venture Centre (now the Venture Centre) was formally opened by UK Prime Minister the Rt. Hon. Margaret Thatcher on 24 February 1984. In 2004, UWSP was awarded the 'Incubator of the Year' award by UKBI. The founding Director of UWSP, David Rowe, was awarded the Queen's Award for Enterprise Promotion in 2006.\nIn 2012 the University of Warwick acquired sole ownership of the Science Park.\n\nThe University of Warwick Science Park’s main 42-acre campus is centred on the Venture Centre. It includes a further twenty buildings, some of which are owner occupied whilst others are leased. Many of the buildings on UWSP's campus are named after old British car models.\n\nThe main buildings include:<br>\nThe Venture Centre<br>\nHerald Court<br>\nSovereign Court<br>\nRiley Court<br>\nVanguard Centre<br>\nViscount Centres\n\nWarwick Innovation Centre opened at Warwick Technology Park in 1997 as a joint venture between UWSP and Warwickshire County Council. Further developments followed in 1999 & 2001 providing of lettable space.\n\nThe Business Innovation Centre at Binley, Coventry opened in April 2000 as a joint venture between UWSP and Coventry City Council.\n\nUWSP manages the Innovation Centre at Blythe Valley Business Park on behalf of Solihull Metropolitan Borough Council, Prologis and British Land.\n\nIn addition to property services, UWSP offers a range of business support activities targeted at innovation led SMEs.\n\nConnect Midlands is a not-for-profit network, that supports technology and high growth companies to gain investment through training and mentoring, investment showcasing, and networking between businesses, investors and the professional services community in the Midlands. Since 2001 Connect Midlands has assisted hundreds of companies who have gone on to raise over £200 million in investment.\n\nUWSP hosts and manages the Business Angel Network, Minerva. MiNErVA is actually an acronym for Midlands Network of Entrepreneurs, Venturers and Angels. In addition to supporting a community of high-net-worth individuals, Minerva undertakes investment readiness consultancy and raises finance from banks and venture capital funds. \nUWSP has also managed a number of seed investment funds including the fully invested UWSP Concepts Fund and the £7m Advantage Proof of Concept Fund.\n\nTechmark has provided marketing services to established, start-up and pre-start technology businesses since 1996. Services include market exploration, market development, strategic planning, marketing execution and overseas market access.\n\nIgnite provides mentoring and subsidised incubator space for start-up, knowledge led firms. This is spread across three sites, the main campus, Warwick Innovation Centre and the Business Innovation Centre.\n\nLabstart provides subsidised offices and laboratory space for start-up technology businesses on UWSP's main campus.\n\nThe UK Market Access Centre provides a blend of subsidised office space and professional service support for foreign owned companies seeking to establish UK trading arms. Warwick Science Park is one of the founding members of the UK Market Access Program.\n\n"}
{"id": "4275666", "url": "https://en.wikipedia.org/wiki?curid=4275666", "title": "Utility Radio", "text": "Utility Radio\n\nThe Utility Radio or Wartime Civilian Receiver was a valve domestic radio receiver, manufactured in Great Britain during World War II starting in July 1944. It was designed by G.D Reynolds of Murphy Radio. Both AC and battery-operated versions were made.\n\nWhen war broke out in 1939, British radio manufacturers devoted their resources to producing a range of military radio equipment required for the armed forces. This resulted in a shortage of consumer radio sets and spare parts, particularly valves, as all production was for the services. The war also prompted a shortage of radio repairmen, as virtually all of them were needed in the services to maintain vital radio and radar equipment. This meant it was very difficult for the average citizen to get a radio repaired, and with very few new sets available, there was a desperate need to overcome the problem.\n\nThe Government solved this by arranging for over 40 radio manufacturers to produce sets to a standard design with as few components as possible consistent with ability to source them. Earlier the Government had introduced the 'Utility' brand to ensure that all clothing, which was rationed, was produced to a reasonable quality standard as prior to its introduction a lot of shoddy goods had appeared on the market. So the 'Utility' brand was adopted for this wartime radio.\n\nThe Utility Set had limited reception on medium wave and lacked a longwave band to simplify the design. The tuning scale listed only BBC stations. After the war a version with LW was made available and modification kits to retrofit existing sets were marketed.\n\nAbout 175,000 sets were sold, at a price of £12 3s 4d each. The set is sometimes characterized as the British equivalent of the German Volksempfänger \"Peoples' Receiver\"; however there were dissimilarities. The \"Volksempfänger\" were radio sets designed to be inexpensive enough for any German citizen to purchase one. A supply of higher quality consumer radios were always available to Germans who could afford to pay higher prices. By contrast, the Utility Set was the only consumer radio receiver that could be purchased on the British market for much of the latter part of the war. Starting in June 1942, manufacture of consumer radio receivers in the United States also ceased due to military production needs.\n\nThe sets used a four-valve superhet circuit with an audio output of 4 watts at 10% total harmonic distortion; they performed as well as many pre-war sets. The valve complement consisted of a triode-hexode frequency changer, a variable-μ RF pentode IF amplifier and a high slope output pentode. A \"Westector\" solid-state copper oxide diode was used for demodulation, which saved one valve and allowed use of an available type of pentode for the audio stage. The HT line was derived from a full wave rectifier. All valves were on International Octal sockets apart from the rectifier which was on a British 4-pin base. There were minor variations between set makers; for instance Philips used IF transformers with adjustable ferrite cores (so-called slug tuning) rather than the conventional trimmer capacitors.\n\nMore than 40 manufacturers, such as Pye Ltd. and Marconiphone, made Utility radios. The manufacturer of a particular set was not readily apparent to the general public, although each manufacturer stamped a code letter on the radio to identify themselves to dealers. UK makers often used different designations for the same valve (tube). Octal tubes might be of USA origin. All valves in the Utility radio used standard designations prefixed by BVA (British Valve Association). They were produced by valve makers such as Mullard, MOV, Cossor, Mazda and Brimar. Dealers, knowing the maker of a set and which valve manufacturer that maker used, could easily deduce which pre-war types these were and make warranty claims on the manufacturer.\n\n\n"}
{"id": "42172780", "url": "https://en.wikipedia.org/wiki?curid=42172780", "title": "Zoomcar", "text": "Zoomcar\n\nZoomcar is a self-drive car rental company headquartered in Bangalore, India. The company was founded in 2013 by David Back and Greg Moran. As of February 2017, the company operates in 27 cities across the country.\n\nAmerican duo David Back & Greg Moran, met while studying at the University of Pennsylvania, where both graduated in 2007. After graduating, Back studied at Harvard Law School and Moran worked on energy financing projects. The pair also went on to business school with full scholarships, Moran at the University of Southern California and Back at the Judge Business School at Cambridge University.\n\nBack and Moran both dropped out of business school to move to India and pursue a venture in car rentals. In May 2015, Back resigned from the self-drive car rental company, and decided to move back to the US citing personal reasons.\n\nZoom officially launched operations in Bangalore in February 2013. Zoom started with $215,000 in capital and a fleet of seven cars.\n\nZoom initially raised capital from individual investors including Former US Treasury Secretary Lawrence Summers, Chair of the UK Institute of Directors Lady Barbara Judge, Vice-Dean of International Legal Studies at Harvard Law School William P. Alford, Wharton Statistics Chair Edward George, and the former Director of the Cambridge Centre for Entrepreneurial Studies Shai Vyakarnam. Back met Summers at Harvard University, where Back was a teaching assistant to Summers.\n\nZoomcar's institutional investors include: Ford Motor Company, Sequoia Capital, Nokia Growth Partners, FundersClub, Basset Investment Group, Athene Capital, Empire Angels, Venture Souk, OurCrowd, Globevestor., and Cyber Carrier CL. Zoomcar has also raised money from former Infosys CFO T.V. Mohandas Pai and his business partner Abhay Jain.\n\nAt Ford's investor day presentation in September 2016, CEO Mark Fields described difficulties with Ford's prior strategy in India, but commented \"Our new mobility business could be a factor that plays in there. That's why you saw us about a month ago take an equity position in Zoomcar, which is the largest car sharing service in India.\"\n\nZoomcar faces competition from Drivezy, MylesCars (backed by Carzonrent) and other similar car rental companies.\n\nZoom partners with the auto manufacturers like Ford & Mahindra which allowed them to become the first car rental company in India to offer an electric vehicle (the Mahindra REVA E2O by Mahindra) and the Ford EcoSport \"urban SUV by Ford\" in 2013. Zoom also works with locally established real estate developers, universities, hotels, and corporate IT parks to secure parking for its vehicles and offer pick-up points to its members.\n\nIn November 2013, Zoom, Uber (a cab booking mobile app provider) and the Ashoka Foundation came together and launched a month-long campaign in Bangalore called RideSmartBLR to encourage car-rental and discourage drunk driving for its health, economical and environmental benefits.\n\nZoom was a member of Microsoft's Accelerator Plus program in Bangalore in Spring 2014. This program is designed to provide more tailored assistance to later-stage start-ups as they scale their business and raise capital.\n\nIn July 2015, Zoomcar issued caution information to its customers following opposition from the Ladakh taxi union. Following an incident in which a convoy of 15 cars rented from the company was reportedly attacked by members of the union with stones and iron rods, Zoomcar issued a cautionary mail to customers traveling to Khardung La and Nubra valley. The following week Zoom issued an update stating that the situation had aggravated and the company urged extreme caution to customers traveling to the region. The taxi unions of Gulmarg and Ladakh continue to resist entry of non-local commercial cars in the region.\n\nIn December 2015, the Advertising Standards Council of India (ASCI) banned 54 controversial Indian ads which included a video advertisement by Zoomcar. The ad was found to be in violation of India's Dowry Prohibition Act, 1961 by the council, following which the advertisement was pulled off by the company.\n\n"}
