{"id": "53875028", "url": "https://en.wikipedia.org/wiki?curid=53875028", "title": "Akvo Foundation", "text": "Akvo Foundation\n\nAkvo is a not-for-profit internet and software developer, headquartered in Amsterdam, Netherlands. The foundation specializes primarily in building and operating data collection and visualization systems to be used in international development and aid activity. Akvo also operates the sustainable development wiki Akvopedia, which as of 2017 has 1956 content pages about water, sanitation and agriculture. Akvo was initially started as a project under the Netherlands Water Partnership in 2006.\n\nAkvo operates from 11 offices worldwide. Akvo has hub offices based in a number of different locations, including Stockholm, London, Washington DC, Nairobi and New Delhi. As of 2017, Akvo operates in 93 countries and works with more than 1800 organizations around the world from NGOs to national governments and multilateral aid organizations who use Akvo’s tools to report, publish, monitor and evaluate their work. Much of the foundation's work is concentrated within the water and sanitation sector. However, since 2010 it has widened its focus to areas such as health, education, technology, agriculture and economic development.\n\nThe foundation's mission is \"to build a highly effective, open, online platform and a trusted partner network.\"\n\nThomas Bjelkeman-Pettersson and Jeroen van der Sommen met at Stockholm World Water Week in 2006. At this time it was becoming clear that the Millennium Development Goals on water was going to be difficult to achieve and as a result there was the need to develop new ideas. Shortly thereafter, the idea of using open source software and open data for the development sector was developed. Consequently, in 2008 the Akvo Foundation was formed by the Netherlands Water Partnership and several other co founders, including Bjelkeman-Pettersson and van der Sommen as well as Mark Nitzberg, Mark Charmer, Gabriel von Heijne and Peter van der Linde.\n\nSeptember 2006 - Seed funding is provided by Netherlands Water Partnership (NWP). The partner organisations that participate in brainstorming about what Akvo should become are UNESCO-IHE, IRC International Resource Centre for Water and Sanitation (IRC) and the Movement Design Bureau.\n\nApril 2007 – Partners for Water and NWP provide the first batch of funding.\n\nAugust 2007 – Akvopedia is shown for the first time at World Water Week.\n\nIn 2015, Akvo was named as the winner of the Water and Sanitation Award. The award was in recognition of Akvo’s innovation within the water and sanitation sector in Latin America. Also, in November 2015, Akvo was awarded the Dutch Water Innovation Prize. Akvo won the award for the development of a smart irrigation inlet sensor that measures the salinity of surface water.\n\nAkvo designs and builds Software as a Service software as well as offering services that complement them, such as training and consulting. All Akvo's software is released as open source software. Akvo's tools are designed to work together at each stage of the monitoring cycle and therefore have three main purposes: first, capture and collect data; second, understand this data and turn it into decisions; and third, share this data. Organisations that use Akvo services include UNDP, UNICEF and a number of other NGOs and governments.\nAkvo Flow (Field Level Operation Watch) allows field surveys to be managed and carried out using mobile phones. It can be used to collect, manage, analyse and display geographically-referenced monitoring and evaluation data. Flow was originally developed by Water for People. In 2012 Water for People partnered with Akvo to allow for wider use of the system and Akvo took over the development of Flow. Initially Flow was used primarily in water and sanitation projects, but has more recently been used in a variety of fields, including fisheries management, school management and disaster relief. Since 2010, 3.990.763 data points have been captured with Flow.\n\nAkvo RSR (Really Simple Reporting) was launched in September 2008 and was the first product Akvo developed; it is a content management and reporting system for international development projects. It was described by Giulio Quaggiotto of the World Bank as \"Real-time, paper-free, straight from the trenches reporting. If RSR takes off, it could be the killer application of Development 2.0\". There are now over 4000 projects on the platform, with a combined value of over €150 billion.\n\nAkvopedia was created in 2007 and was the first project that Akvo worked on. It is a Web-based, free content, water and sanitation knowledge resource. Akvopedia is written collaboratively by volunteers and specialists, mainly from the water and sanitation sector but anyone can edit and contribute. It is organised within five portals – Water, Sanitation, Finance, Sustainability and Decision & assessment tools. Recently some of this content has been transferred to Wikiversity in the Rainwater Harvesting portal.\n\nAkvo Sites is a Wordpress based system for easy creation and maintenance of websites.\n\nAkvo Caddisfly is a water quality and soil quality testing system based on smartphones. Akvo Caddisfly was originally invented by Ternup Research Labs to detect the precedes of fluoride in drinking water. Ternup joined forces with Akvo in early 2014. The Caddisfly system has been extended to work with strip tests and phone-connected sensors as well as the original colorimetric tests.\n\nAkvo Lumen is a data visualisation platform and is Akvo's latest product. It is developed in the Clojure programming language.\n\nAkvo has gained notoriety for its unique external communication. Bruce Sterling said \"It's an open-source sanitation project, and if that isn’t weird enough, they’ve got a Bollywood-parody promotional angle.\" \n\nNational Geographic Society reported from World Water Week in Stockholm about the WaterCube, the brainchild of Mark Charmer at Akvo: \"Man-on-the-street journalism meets wonky water week. Frenetic water cube workers ushered a steady stream of conference attendees into their fishbowl lounge for quick Flip camera interviews. The close-ups may not be flattering, but it is interesting to hear the stream of consciousness that pours from the mouths of the unprepared and unedited. From high-profile corporate and academic leadership to those implementing projects on the ground around the world, the resulting online mosaic of more than 200 interviews collected over the last two years seems to present a near-complete view of the issues covered over six days of workshops, plenary sessions, and panels.\" \n\nAkvopedia expands into Wikiversity: In 2015, Akvopedia begun loading content into Wikiversity. Akvopedia has served as source content for the Wikiversity portal and the Rainwater harvesting portal.\n\nMonitoring water points during drought in Ethiopia: In 2016, Akvo worked with UNICEF to collect data during the worst drought in over 50 years in Ethiopia. Akvo recorded over 80,000 survey responses which were used to help direct emergency resources to the most relevant areas.\n\nSupporting Government disaster planning and response systems: Since 2014, Akvo has worked in Fiji, Vanuatu and the Solomon Islands to introduce locally run mobile-based data and asset management tools to improve the quality and availability of data for disaster-related decision-making. For this work Akvo won the innovationXchange award. In 2016, Akvo Flow was used in Fiji to collect data on the extent of the damage caused by Cyclone Winston.\n"}
{"id": "6303113", "url": "https://en.wikipedia.org/wiki?curid=6303113", "title": "Artifact (software development)", "text": "Artifact (software development)\n\nAn artifact is one of many kinds of tangible by-products produced during the development of software. Some artifacts (e.g., use cases, class diagrams, and other Unified Modeling Language (UML) models, requirements and design documents) help describe the function, architecture, and design of software. Other artifacts are concerned with the process of development itself—such as project plans, business cases, and risk assessments.\n\nThe term \"artifact\" in connection with software development is largely associated with specific development methods or processes e.g., Unified Process. This usage of the term may have originated with those methods.\n\nBuild tools often refer to source code compiled for testing as an artifact, because the executable is necessary to carrying out the testing plan. Without the executable to test, the testing plan artifact is limited to non-execution based testing. In non-execution based testing, the artifacts are the walkthroughs, inspections and correctness proofs. On the other hand, execution based testing requires at minimum two artifacts: a test suite and the executable. An artifact occasionally may be used to refer to the released code (in the case of a code library) or released executable (in the case of a program) produced but the more common usage is in referring to the byproducts of software development rather than the product itself. Open source code libraries often contain a testing harness to allow contributors to ensure their changes do not cause regression bugs in the code library.\n\nMuch of what are considered artifacts is software documentation.\n\nIn end-user development an artifact is either an application or a complex data object that is created by an end-user without the need to know a general programming language. Artifacts describe automated behavior or control sequences, such as database requests or grammar rules, or user-generated content.\n\nArtifacts vary in their maintainability. Maintainability is primarily affected by the role the artifact fulfills. The role can be either practical or symbolic. In the earliest stages of software development, artifacts may be created by the design team to serve a symbolic role to show the project sponsor how serious the contractor is about meeting the project's needs. Symbolic artifacts often convey information poorly, but are impressive-looking. Symbolic artifacts are sometimes referred to in the information architecture industry as Illuminated Scrolls, because the decorations do nothing to enhance understanding. Generally speaking, Illuminated Scrolls are also considered unmaintainable due to the diligence it requires to preserve the symbolic quality. For this reason, once Illuminated Scrolls are shown to the project sponsor and approved, they are replaced by artifacts which serve a practical role. Practical artifacts usually need to be maintained throughout the project lifecycle, and, as such, are generally highly maintainable.\n\nArtifacts are significant from a project management perspective as deliverables. The deliverables of a software project are likely to be the same as its artifacts with the addition of the software itself.\n\nThe sense of artifacts as byproducts is similar to the use of the term \"artifact\" in science to refer to something that arises from the process in hand rather than the issue itself, i.e., a result of interest that stems from the means rather than the end.\n\nTo collect, organize and manage artifacts, a Software development folder may be utilized.\n\n\n"}
{"id": "11130631", "url": "https://en.wikipedia.org/wiki?curid=11130631", "title": "Avidyne Entegra", "text": "Avidyne Entegra\n\nAvidyne Entegra is an integrated aircraft instrumentation system, produced by Avidyne Corporation, consisting of a primary flight display (PFD), and multi-function display (MFD). Cirrus became the first customer of the Entegra system and began offering it on the SR20 and SR22 aircraft in 2003 as the first integrated flight deck for light general aviation (GA). The original Entegra system was designed to use third-party components such as a GPS from Garmin and an autopilot system from S-TEC Corporation.\n\nOne of the advantages of these glass flight deck systems is upgradeability. Avidyne has demonstrated this with a continuous stream of hardware and software upgrades, including:\n\nWith the certification of Entegra Release 9 system in April 2009, Avidyne offers dual integrated flight displays (IFD) with the FMS900w, a WAAS-capable flight management system (FMS) that utilizes Avidyne’s all-digital DVX740 VHF NAV/COM radio modules and GPS722 GPS receiver modules, which eliminates the reliance on 3rd-party Garmin 430 radios. The Release 9 upgrade also includes the Avidyne MLB700 which provides Sirius/XM Satellite Datalink Weather from WSI.\n\nAvidyne has introduced a new attitude-based digital autopilot, the DFC100, which can replace the rate-based STEC 55X autopilot in most Release 9 installations.\n\nEntegra Release 9 is considered a 3rd-generation integrated flight deck because it is built on a brand new hardware platform using a modular architecture, dual Byteflight digital databus, all new fully digital radio designs, and incorporating a unique Page & Tab user interface and QWERTY-style FMS control/display unit that are designed to dramatically reduce workload during single-pilot IFR operations.\n\nEntegra Release 9 system was designed with a fully redundant dual-databus architecture that eliminates traditional \"Reversionary Modes.\"\n\nA typical Entegra Release 9 installation features two large-format IFD5000 Integrated Flight Displays (IFD), which are fully interchangeable for use as PFD or MFD. Since each IFD5000 is fully capable of performing the functions of the other, no unfamiliar or limited reversionary modes are required . In the event of a display failure, the remaining IFD5000 continues to operate as either display format with no loss of functionality.\n\nSome competing glass flight deck systems have limited redundancy, lose critical functionality such as datalink weather, traffic, or even autopilot, and their failure modes force the pilot to learn composite display symbology and \"reversionary modes.\"\n\nAvidyne was first to certify big glass for light GA with the 2003 launch of Entegra in Cirrus aircraft. This is considered a \"first generation\" big-glass system that integrates the six 3-inch instruments (6-pack) into a more usable package, along with an exceptionally reliable Air Data and Heading Reference System (ADAHRS) that replaces the “spinning mass” attitude and directional gyros. Entegra Release 8 still relies on a ‘federated’ radio stack (dual G430s) for GPS/NAV/COM capability, as well as audio and transponder.\n\nAvidyne Entegra systems are found in aircraft from such companies as:\n\nThe Avidyne Entegra competes with the Garmin G1000 and Chelton FlightLogic EFIS glass cockpits. However, there are significant differences with regard to the features, degree of integration, intuitive aspects of the design, and overall product utility. Note that the Chelton system is not typically found in airplanes that include the less expensive G1000 or Avidyne systems.\n\n"}
{"id": "9756767", "url": "https://en.wikipedia.org/wiki?curid=9756767", "title": "Barrack buster", "text": "Barrack buster\n\nBarrack buster is the colloquial name given to several improvised mortars, developed in the 1990s by the engineering group of the Provisional Irish Republican Army (IRA).\n\nThe first barrack buster—known to the British security forces as the Mark 15 mortar—fired a long metal propane cylinder with a diameter of , which contained around of home-made explosives and had a range of . The cylinder is an adaptation of a commercial 'Kosangas' gas cylinder, for heating and cooking gas, used in rural areas in Ireland.\n\nIt was first used in an attack on 7 December 1992 against an RUC/British Army base in Ballygawley, County Tyrone, injuring a number of Royal Ulster Constabulary officers.\n\nThe barrack buster belongs to a series of home-made mortars developed since the 1970s. The first such mortar—Mark 1—was used in an attack in May 1972 and it was soon followed by the first of a series of improved or differentiated versions stretching into the 1990s:\n\n\nThe intensification of the IRA's mortar campaign in the late 1980s forced the British government to increase the number of army troops in Northern Ireland from its lowest ebb of 9,000 in 1985 to 10,500 in 1992. The IRA's use of mortars combined with heavy machine guns compelled the British Army to build their main checkpoints more than a mile away from the Irish border by 1992.\n\nThese mortars have been used by the Real IRA, who also developed their own fuzing system, in the 2000s. Furthermore, what appears to be a similar or identical mortar technology has been used since 1998 by the Revolutionary Armed Forces of Colombia (FARC). ETA in Spain was in 2001 rumoured to have built mortars \"very similar\" to the IRA's. The possible transfer of this mortar technology to the FARC was a central issue in the arrest in August 2001 and later trial of the so-called Colombia Three group of IRA members who were alleged by Colombian authorities and the United States House Committee on Foreign Affairs to have trained FARC in the manufacture and use of this mortar technology.\n\nA derived term in Belfast refers to a two or three-litre bottle of inexpensive white cider.\n\n\n"}
{"id": "167132", "url": "https://en.wikipedia.org/wiki?curid=167132", "title": "Brain transplant", "text": "Brain transplant\n\nA brain transplant or whole-body transplant is a procedure in which the brain of one organism is transplanted into the body of another organism. It is a procedure distinct from head transplantation, which involves transferring the entire head to a new body, as opposed to the brain only. Theoretically, a person with advanced organ failure could be given a new and functional body while keeping their own personality, memories, and consciousness through such a procedure.\n\nNo human brain transplant has ever been conducted. Neurosurgeon Robert J. White has grafted the head of a monkey onto the headless body of another monkey. EEG readings showed the brain was later functioning normally. It was thought to prove that the brain was an immunologically privileged organ, as the host's immune system did not attack it at first, but immunorejection caused the monkey to die after nine days. Brain transplants and similar concepts have also been explored in various forms of science fiction.\n\nOne of the most significant barriers to the procedure is the inability of nerve tissue to heal properly; scarred nerve tissue does not transmit signals well (this is why a spinal cord injury is so devastating). Research at the Wistar Institute of the University of Pennsylvania involving tissue-regenerating mice (known as MRL mice) may provide pointers for further research as to how to regenerate nerves without scarring. It is possible that a completely clean cut will not generate scarred tissue.\n\nAlternatively, a brain–computer interface can be used connecting the subject to their own body. A study using a monkey as a subject shows that it is possible to directly use commands from the brain, bypass the spinal cord and enable hand function. An advantage is that this interface can be adjusted after the surgical interventions are done where nerves can not be reconnected without surgery.\n\nAlso, for the procedure to be practical, the age of the donated body must be sufficient: an adult brain cannot fit into a skull that has not reached its full growth, which occurs at age 9–12 years.\n\nThe brain was thought to be an immunologically privileged organ, so rejection would not be a problem. (When other organs are transplanted, aggressive rejection can occur; this is a major difficulty with kidney and liver transplants. Immune cells of the CNS contribute to the maintenance of neurogenesis and spatial learning abilities in adulthood, so the transplanted brain would not be unhindered from the transplant.)\n\nIn 1982, Dr. Dorothy T. Krieger, chief of endocrinology at Mount Sinai Medical Center in New York City, achieved success with a partial brain transplant in mice.\n\nIn 1998, a team of surgeons from the University of Pittsburgh Medical Center attempted to transplant a group of brain cells to Alma Cerasini, who had suffered a severe stroke that caused the loss of mobility in her right limbs as well as limited speech. The team's hope was that the cells would correct the listed damage.\n\nThe whole-body transplant is just one of several means of putting a consciousness into a new body that have been explored in science fiction.\n\nIn the film \"Get Out\" (2017), written and directed by Jordan Peele, a young, white woman, Rose Armitage (Allison Williams), lures her young, black boyfriend, Chris Washington (Daniel Kaluuya), to her family's estate to be auctioned-off for a brain transplant procedure that enables older, white residents of the Armitage community to obtain a pseudo-immortality in a black person's body. The Armitage family is the founder of the cult The Order of the Coagula, in which Missy Armitage (Catherine Keener), a psychiatrist, uses hypnosis to subdue the consciousness of black persons to The Sunken Place, and Dean Armitage (Bradley Whitford), a neurosurgeon, transplants the brain of the white person into the black person's body. Due to the human body's need for motor functions, only a partial brain transplant is possible. The Sunken Place is essential to suppressing the conscious minds of the hosts while the brain tissue is implanted into their skulls. Once the brain transplant has been completed, the white person's consciousness takes control over the black host body; meanwhile, the consciousness of the black person remains in The Sunken Place. While in the Sunken Place, the black person's consciousness is completely intact and aware of what is going on, yet has no control over their body.\n\nIn Maxwell Atoms' Cartoon Network series \"Evil Con Carne\" (2003-2004), Hector Con Carne was reduced to a brain and a stomach and both of them were put in jars attached to the body of a Russian circus bear. Hector's brain is the main character of the cartoon.\n\nEdgar Rice Burroughs' \"The Mastermind of Mars\" involves a surgeon who does this as his main operation, and a man from Earth, the narrator and main character, who is trained to do it as well.\n\nIn Neil R. Jones \"Professeur Jameson\" stories (1931), the main character is the last earthman, whose brain is saved by some alien cyborgs called the Zoromes, and is inserted into a robotic body, making him immortal.\n\nRobert Heinlein's 1970 science fiction novel, \"I Will Fear No Evil\", features a main character named Johann Sebastian Bach Smith whose entire brain is transplanted into his deceased secretary's skull.\n\nA similar procedure often found in science fiction is the transfer of one consciousness to another without moving the brain. This is found in many sources, most often a body swap between two characters of an ongoing television series; it occurs in the original \"\" series twice, in the X-Files episode \"Dreamland\", in the \"Doctor Who\" serial \"Mindwarp\", and in \"Freaky Friday\", \"Farscape\", \"Stargate SG-1\", \"Buffy the Vampire Slayer\", \"Dollhouse\", \"Red Dwarf\", \"Avatar\", \"Dark Shadows\"; even in Archie comics. Since there is no movement of the brain(s), this is not quite the same as a whole-body transplant.\n\nIn the horror film \"The Skeleton Key\", the protagonist, Caroline, discovers that the old couple she is looking after are poor Voodoo witch doctors who stole the bodies of two young, privileged children in their care using a ritual which allows a soul to swap bodies. Unfortunately the evil old couple also trick Caroline and their lawyer into the same procedure, and both end up stuck in old dying bodies unable to speak while the witch doctors walk off with their young bodies.\n\nIn the Spanish \"destape\" film \"Desembraga a fondo\", the brain of the protagonist, Fernando Esteso, is transplanted into his brother's skull.\n\nIn Anne Rice's \"The Tale of the Body Thief\", the vampire Lestat discovers a man, Raglan James, who can will himself into another person's body. Lestat demands that the procedure be used on him to allow him to be human once again, but soon finds that he has made an error and is forced to recapture James in his vampiric form so he can take his body back.\n\nSimilar in many ways to this is the idea of mind uploading, promoted by Marvin Minsky and others with a mechanistic view of natural intelligence and an optimistic outlook regarding artificial intelligence. It is also a goal of Raëlism, a small cult based in Florida, France, and Quebec. While the ultimate goal of transplanting is transfer of the brain to a new body optimized for it by genetics, proteomics, and/or other medical procedures, in uploading the brain itself moves nowhere and may even be physically destroyed or discarded; the goal is rather to duplicate the information patterns contained within the brain.\n\nAnother similar literary theme, though different from either procedure described above, is the transplanting of a human brain into an artificial, usually robotic, body. Examples of this include: \"Caprica\"; \"Fullmetal Alchemist\"; \"Ghost in the Shell\"; \"RoboCop\"; the DC Comics superhero Robotman; the Cybermen from the \"Doctor Who\" television series; the cymeks in the \"Legends of Dune\" series; or full-body cyborgs in many manga or works in the cyberpunk genre. In one episode of \"\", Spock's Brain is stolen and installed in a large computer-like structure; and in \"I, Mudd\" Uhura is offered immortality in an android body. The novel \"Harvest of Stars\" by Poul Anderson features many central characters who undergo such transplants, and deals with the difficult decisions facing a human contemplating such a procedure. In the \"\"Star Wars\" expanded universe\" the shadow droids were created by taking the brains of grievously wounded TIE fighter pilot aces. After surgically transplanting them into a protective cocoon filled with nutrient fluids. they were surgically connected to cybernetic hardware that gave them external sensors, flight control and tactical computers that augmented their reflexes beyond the biological limit; at the cost of their humanity. Emperor Palpatine also imbued them with the dark side giving them a sixth sense, and making them into extensions of his own will.\n\n\n\n"}
{"id": "37853", "url": "https://en.wikipedia.org/wiki?curid=37853", "title": "Bussard ramjet", "text": "Bussard ramjet\n\nThe Bussard ramjet is a theoretical method of spacecraft propulsion proposed in 1960 by the physicist Robert W. Bussard, popularized by Poul Anderson's novel \"Tau Zero\", Larry Niven in his \"Known Space\" series of books, Vernor Vinge in his \"Zones of Thought\" series, and referred to by Carl Sagan in the television series and book \"\".\n\nBussard proposed a ramjet variant of a fusion rocket capable of reasonable interstellar travel, using enormous electromagnetic fields (ranging from kilometers to many thousands of kilometers in diameter) as a ram scoop to collect and compress hydrogen from the interstellar medium. High speeds force the reactive mass into a progressively constricted magnetic field, compressing it until thermonuclear fusion occurs. The magnetic field then directs the energy as rocket exhaust opposite to the intended direction of travel, thereby accelerating the vessel.\n\nSince the time of Bussard's original proposal, it has been discovered that the region surrounding the Solar System has a much lower density of hydrogen than was believed at that time (see Local Interstellar Cloud). John Ford Fishback made an important contribution to the details for the Bussard ramjet in 1969, T. A. Heppenheimer analyzed Bussard's original suggestion of fusing protons, but found the Bremsstrahlung losses from compressing protons to fusion densities was greater than the power that could be produced by a factor of about 1 billion, thus indicating that the proposed version of the Bussard ramjet was infeasible. However Daniel P. Whitmire's 1975 analysis indicates that a ramjet may achieve net power via the CNO cycle, which produces fusion at a much higher rate (~10 times higher) than the proton-proton chain.\n\nRobert Zubrin and Dana Andrews analyzed one hypothetical version of the Bussard ramscoop and ramjet design in 1985. They determined that their version of the ramjet would be unable to accelerate into the solar wind. However, in their calculations they assumed that:\n\n\nIn the Zubrin/Andrews interplanetary ramjet design, they calculated that the drag force d/dt(\"mv\") equals the mass of the scooped ions collected per second multiplied by the velocity of the scooped ions within the solar system relative to the ramscoop. The velocity of the (scooped) collected ions from the solar wind was assumed to be 500,000 m/s.\n\nThe exhaust velocity of the ions when expelled by the ramjet was assumed not to exceed 100,000 m/s. The thrust of the ramjet d/dt(\"mv\") was equal to the mass of ions expelled per second multiplied by 100,000 meters per second. In the Zubrin/Andrews design of 1985, this resulted in the condition that d/dt(\"mv\") > d/dt(\"mv\"). This condition resulted in the drag force exceeding the thrust of the hypothetical ramjet in the Zubrin/Andrews version of the design.\n\nThe problem of using the interstellar medium as the sole fuel source led to study of the Ram Augmented Interstellar Rocket (RAIR). The RAIR carries its nuclear fuel supply and exhausts the reaction products to produce some of its thrust. However it greatly enhances its performance by scooping the interstellar medium and using this as extra reaction mass to augment the rocket. The propulsion system of the RAIR consists of three subsystems: a fusion reactor, a scoop field, and a plasma accelerator. The scoop field funnels interstellar gas into an \"accelerator\" (this could for example be a heat exchange system transferring thermal energy from the reactor directly to the interstellar gas) which is supplied power from a reactor. One of the best ways to understand this concept is to consider that the hydrogen nuclear fuel carried on board acts as a fuel (energy source) whereas the interstellar gas collected by the scoop and then exhausted at great speed from the back acts as a propellant (the reaction mass), the vehicle therefore has a limited fuel supply but an unlimited propellant supply. A normal Bussard ramjet would have an infinite supply of both, however theory suggests that where a Bussard ramjet would suffer drag from the fact that the interstellar gas ahead of it would have to be accelerated to its speed before entering the fusion reactor, whereas a RAIR system would be able to transfer energy via the \"accelerator\" mechanism from the reactor to the interstellar gas without having to accelerate the gas up to the ship's speed before putting this gas through the \"accelerator\", and so would suffer far less drag.\n\nBeamed energy coupled with a vehicle scooping hydrogen from the interstellar medium is another variant. A laser array in the solar system beams to a collector on a vehicle which uses something like a linear accelerator to produce thrust. This solves the fusion reactor problem for the ramjet. There are limitations because of the attenuation of beamed energy with distance.\n\nThe calculations (by Robert Zubrin and an associate) inspired the idea of a magnetic parachute or sail. This could be important for interstellar travel because it means that deceleration at the destination can be performed with a magnetic parachute rather than a rocket.\n\nSeveral of the obvious technical difficulties with the Bussard ramjet can be overcome by prelaunching fuel along the spacecraft's trajectory using something like a magnetic rail-gun.\n\nThe advantages of this system include\n\nThe major disadvantages of this system include\n\n\n"}
{"id": "13295204", "url": "https://en.wikipedia.org/wiki?curid=13295204", "title": "CBERS-2B", "text": "CBERS-2B\n\nChina–Brazil Earth Resources Satellite 2B (CBERS-2B), also known as Ziyuan I-02B or Ziyuan 1B2, was a remote sensing satellite operated as part of the China–Brazil Earth Resources Satellite programme between the China Centre for Resources Satellite Data and Application and Brazil's National Institute for Space Research. The third CBERS satellite to fly, it was launched by China in 2007 to replace CBERS-2.\n\nCBERS-2B was a spacecraft built by the China Academy of Space Technology and based on the Phoenix-Eye 1 satellite bus. The spacecraft was powered by a single solar array, which provided 1,100 watts of electricity for the satellite's systems. The instrument suite aboard the CBERS-2B spacecraft consisted of three systems: the Wide Field Imager (WFI) produced visible-light to near-infrared images with a resolution of and a swath width of ; a high-resolution CCD camera was used for multispectral imaging at a resolution of with a swath width of ; the third instrument, the High Resolution Camera (HRC) was a panchromatic imager with a resolution of and a swath width of . HRC replaced the lower-resolution Infrared Multispectral Scanner instrument flown on earlier CBERS satellites.\n\nA Chang Zheng 4B carrier rocket, operated by the China Academy of Launch Vehicle Technology, was used to launch CBERS-2B. The launch took place at 03:26:13 UTC on 19 September 2007, using Launch Complex 7 at the Taiyuan Satellite Launch Centre. The satellite was successfully placed into a sun-synchronous orbit.\n\nCBERS-2B suffered a power system failure on 10 May 2010, leaving it unable to continue operations. As of it remains in orbit, with a perigee of , an apogee of ,  degrees inclination and a period of  minutes. It has a semimajor axis of , and eccentricity of .\n"}
{"id": "54000410", "url": "https://en.wikipedia.org/wiki?curid=54000410", "title": "Claas Mercator", "text": "Claas Mercator\n\nThe Mercator is a series of combine harvesters produced by the German agricultural company Claas in Harsewinkel. Initially called Senator, the Mercator series was introduced in 1966. The first combine harvester of the series to be called Mercator was presented in 1967, the Protector followed in 1968. Both are less productive but also less expensive models. With the Facelift, the Senator was renamed Mercator 70, while the Protector models were given the names Mercator 60 and 50. Later, more models of the Mercator series were introduced, such as the Mercator 75.\n\nThe Senator is the first Claas combine harvester to feature the colour \"saatengrün\" (German: seed-green), the new Claas-logo and extensive metal covers.\n"}
{"id": "26094128", "url": "https://en.wikipedia.org/wiki?curid=26094128", "title": "Click farm", "text": "Click farm\n\nA click farm is a form of click fraud, where a large group of low-paid workers are hired to click on paid advertising links for the click fraudster (click farm master or click farmer). The workers click the links, surf the target website for a period of time, and possibly sign up for newsletters prior to clicking another link. For many of these workers, clicking on enough ads per day may increase their revenue substantially and may also be an alternative to other types of work. It is extremely difficult for an automated filter to detect this simulated traffic as fake because the visitor behavior appears exactly the same as that of an actual legitimate visitor. \n\nFake likes generating from click farms are essentially different from those arising from bots where computer programs are written by software experts. To deal with such issues, companies such as Facebook are trying to create algorithms that seek to wipe out accounts with unusual activity (e.g. liking too many pages in a short period of time).\n\nClick farms are usually located in developing countries, such as China, India, Nepal, Sri Lanka, Egypt, Indonesia, the Philippines, and Bangladesh. The business of click farms extends to generating likes and followers on social media platforms such as Facebook, Twitter, Instagram, Pinterest and more. Workers are paid, on average, one US dollar for a thousand likes or for following a thousand people on Twitter. Then click farms turn around and sell their likes and followers at a much higher price. According to the \"Daily Mail\", \"BuyPlusFollowers sells 250 Google+ shares for $12.95; InstagramEngine sells 1,000 [Instagram] followers for $12.00; and AuthenticHits sells 1,000 SoundCloud plays for $9.00.\"\n\nIn Thailand in June 2017, a click farm was discovered with hundreds of mobile phones and several hundred thousand SIM cards used to build up likes and views on WeChat.\n\nThere are two methods to click farming. The first is by hiring competitor fraudsters to deplete the advertising budget of the competitor so that they will be able to have their ads shown in higher pay-per-click rankings at a lower cost. In this case, the competitor is weakened instead of being outbid in the pay-per-click bidding system. The investment on the click farm made by the fraudster is only a very small fraction of the amount lost by the competitor. The second is by hiring the click farm workers to click on ads on the click farmer's own site. This way, the money lost by the advertisers is gained by the click farmer, rather than by the search engines and content networks as in the first method.\n\nThe need for click farming arises because, as \"The Guardian\" states, \"31% will check ratings and reviews, including likes and Twitter followers, before they choose to buy something.\" This shows the increasing importance that businesses, celebrities and other organisations put on the number of likes and followers they have. This creates monetary values for likes and followers which means that businesses and celebrities feel compelled to increase their likes to create a positive online profile.\n\nPay-per-click providers, including Google, Yahoo!, and MSN, have made substantial efforts to combat click fraud. Automated filters remove most click fraud attempts at the source. According to Deanna Yick, a spokeswoman for Mountain View, California-based Google, she said that “We design our systems to catch bot-related attacks.” “Because a significant amount of malicious traffic is automated, advertisers are protected from these kinds of attacks.” she added. In an effort to circumvent these filtering systems, click fraudsters have begun to use these click farms to mimic actual visitors.\n\nEngagement rate, a performance metric that measures the quality of social media activity such as Facebook likes or Twitter retweets, can be interpreted in terms of \"engagement per follower,\" measured by dividing the raw counts of social media activity by the number of followers. Users who engage in short term click farms services will see their engagement rate plummet in time as the initial increase in the volume of social media activity drops when the click farms services end, coupled with the increase in fake followers if not the same.\n\nItalian security researchers and bloggers Andrea Stroppa and Carla De Micheli found out in 2013 that $40 million to $360 million to date were earned from the sale of and the potential benefits of buying fake Twitter followers. $200 million a year is also earned from fake Facebook activities. About 40 to 80 percent of Facebook advertisements are bought on a pay-per-click basis. Advertisers have claimed that about 20 percent of Facebook clicks are invalid and they had tried to seek refunds. This could cost Facebook $2,494 billion of their 2014 revenue.\n\nSome companies have tried to mitigate the effects of click farming. Coca-Cola made its 2010 Super Bowl advert \"Hard Times\" private after learning it was shared on Shareyt and issued a statement that it \"did not approve of fake fans.\" Hasbro was alerted to an online casino, a sub-licensee of its Monopoly brand had added fake Facebook likes and hence contacted Facebook to remove the site. Hasbro issued a statement that it was “appalled to hear of what had occurred” and claimed no previous knowledge of the page.\n\nAlthough click farm services violate many social media user policies, there are no government regulations that render them illegal. However, Sam DeSilva, a lawyer specializing in IT and outsourcing law at Manches LLP in Oxford had mentioned that: \"Potentially, a number of laws are being breached – the consumer protection and unfair trading regulations. Effectively it's misleading the individual consumers.\"\n\nFacebook issued a statement stating: \"A like that doesn't come from someone truly interested in connecting with the brand benefits no one. If you run a Facebook page and someone offers you a boost in your fan count in return for money, our advice is to walk away – not least because it is against our rules and there is a good chance those likes will be deleted by our automatic systems. We investigate and monitor \"like-vendors\" and if we find that they are selling fake likes, or generating conversations from fake profiles, we will quickly block them from our platform.\" Andrea Faville reported that Alphabet Inc. companies, Google and YouTube, \"take action against bad actors that seek to game our systems.\" LinkedIn spokesman Doug Madey said buying connections \"dilutes the member experience violates their user agreement and can also prompt account closures.\" Chief executive and founder of Instagram, Kevin Systrom reports \"We've been deactivating spammy accounts from Instagram on an ongoing basis to improve your experience.\"\n\nFacebook's purging of fake likes and accounts occurred from August to September 2012. According to Facebook's 2014 financial report to the Securities and Exchange Commission, an estimated 83 million false accounts were deleted, accounting for approximately 6.4% of the 1.3 billion total accounts on Facebook. Likester reported pages affected include Lady Gaga, who lost 65,505 fans and Facebook, who lost 124,919 fake likes. Technology giant Dell lost 107,889 likes (2.87% of its total likes) in 24 hours. Billions of YouTube video fake views were deleted after being exposed by auditors. In December 2014, Instagram carried out a purge deemed the \"Instagram Rapture\" wherein many accounts were affected—including Instagram's own account, which lost 18,880,211 followers.\n\n"}
{"id": "45150748", "url": "https://en.wikipedia.org/wiki?curid=45150748", "title": "Customer knowledge", "text": "Customer knowledge\n\nCustomer knowledge (CK) is the combination of experience, value and insight information which is needed, created and absorbed during the transaction and exchange between the customers and enterprise.\nCampbell (2003) defines customer knowledge as: \"organized and structured information about the customer as a result of systematic processing\". According to Mitussis et al. (2006), customer knowledge is identified as one of the more complex types of knowledge, since customer knowledge can be captured from different sources and channels.\n\nVarious classifications exist:\nGebert et al. (2002), classified customer knowledge from an organization's perspective into three types:\n\nThe same categorization of customer knowledge has been made by others such as Bueren et al. (2005) and Feng and Tian (2005). In another categorization, Crié and Micheaux (2006) divide customer knowledge into two types, namely: \"Behavioural\" (or Quantitative) and \"Attitudinal\" (or Qualitative). Behavioral knowledge is easy to acquire and is basically quantitative by nature; that is, containing a customer transactional relations with the company. On the other hand, attitudinal knowledge is difficult to acquire because it deals with a customer's state of mind; but meanwhile it is an important factor for enhancement of customer knowledge because they are directly related to a customer's thoughts and insights.\n\n\nJournals:\n"}
{"id": "9089218", "url": "https://en.wikipedia.org/wiki?curid=9089218", "title": "Day trading software", "text": "Day trading software\n\nDay trading software is computer software intended to facilitate day trading of stocks or other financial instruments.\n\nDay trading software falls into three main categories: Data, Charting, and Trade Execution.\n\nA day trader needs to know the prices of the stocks, futures, or currencies that s/he wants to trade. In the case of stocks and futures, those prices come from the exchange where they are traded. Forex is a little different as there is no central exchange.\n\nThe vast majority of day traders will chart prices in some kind of charting software. Many charting vendors also supply data feeds.\n\nCharting packages all tend to offer the same basic technical analysis indicators. Advanced packages often include a complete programming language for creating more indicators, or testing different trading strategies.\n\nOnce traders have their data and can see and analyze it on a chart, they will at some point want to place a trade. To do so, they need to use some kind of trade execution software or electronic trading platform. Many trade execution software allow advanced traders to develop their own trading strategies by using an API.\n\nMost stock brokerage firms will provide proprietary software linked directly to their in-house systems, but many third party applications are available through Independent software vendors(ISV). The advantage of third party programs is that they allow the trader to trade through different brokers whilst retaining the same interface. They may also offer a number of advanced features such as automatic trade execution.\n\n"}
{"id": "50023853", "url": "https://en.wikipedia.org/wiki?curid=50023853", "title": "Digiboo", "text": "Digiboo\n\nDigiboo LLC, founded in 2009, is a location-based retail download service providing movie and TV show downloads to mobile devices. Kiosks with thousands of movie and TV show titles available on demand, allow the downloading of media direct to phones, laptops and USB 3.0 storage devices, at speeds of 1–3 minutes per full-length movie.\n"}
{"id": "30889569", "url": "https://en.wikipedia.org/wiki?curid=30889569", "title": "Digital signal", "text": "Digital signal\n\nA digital signal is a signal that is being used to represent data as a sequence of discrete values; at any given time it can only take on one of a finite number of values. This contrasts with an analog signal, which represents continuous values; at any given time it represents a real number within a continuous range of values. \n\nSimple digital signals represent information in discrete bands of analog levels. All levels within a band of values represent the same information state. In most digital circuits, the signal can have two possible values; this is called a binary signal or logic signal. They are represented by two voltage bands: one near a reference value (typically termed as \"ground\" or zero volts), and the other a value near the supply voltage. These correspond to the two values \"zero\" and \"one\" (or \"false\" and \"true\") of the Boolean domain, so at any given time a binary signal represents one binary digit (bit). Because of this discretization, relatively small changes to the analog signal levels do not leave the discrete envelope, and as a result are ignored by signal state sensing circuitry. As a result, digital signals have noise immunity; electronic noise, provided it is not too great, will not affect digital circuits, whereas noise always degrades the operation of analog signals to some degree.\n\nDigital signals having more than two states are occasionally used; circuitry using such signals is called multivalued logic. For example, signals that can assume three possible states are called three-valued logic. \n\nIn a digital signal, the physical quantity representing the information may be a variable electric current or voltage, the intensity, phase or polarization of an optical or other electromagnetic field, acoustic pressure, the magnetization of a magnetic storage media, etcetera. Digital signals are used in all digital electronics, notably computing equipment and data transmission.\n\nThe term \"digital signal\" has related definitions in different contexts.\n\nIn digital electronics a digital signal is a pulse train (a pulse amplitude modulated signal), i.e. a sequence of fixed-width square-wave electrical pulses or light pulses, each occupying one of a discrete number of levels of amplitude. A special case is a \"logic signal\" or a \"binary signal\", which varies between a low and a high signal level.\nIn digital signal processing, a digital signal is a representation of a physical signal that is a sampled and quantized. A digital signal is an abstraction which is discrete in time and amplitude. The signal's value only exists at regular time intervals, since only the values of the corresponding physical signal at those sampled moments are significant for further digital processing. The digital signal is a sequence of codes drawn from a finite set of values. The digital signal may be stored, processed or transmitted physically as a pulse-code modulation (PCM) signal.\nIn digital communications, a digital signal is a continuous-time physical signal, alternating between a discrete number of waveforms, representing a bitstream. The shape of the waveform depends the transmission scheme, which may be either a line coding scheme allowing baseband transmission; or a digital modulation scheme, allowing passband transmission over long wires or over a limited radio frequency band. Such a carrier-modulated sine wave is considered a digital signal in literature on digital communications and data transmission, but considered as a bitstream converted to an analog signal in electronics and computer networking.\n\nIn communications, sources of interference are usually present, and noise is frequently a significant problem. The effects of interference are typically minimized by filtering off interfering signals as much as possible and by using data redundancy. The main advantages of digital signals for communications are often considered to be the noise immunity to noise capability, and the ability, in many cases such as with audio and video data, to use data compression to greatly decrease the bandwidth that is required on the communication media.\nA waveform that switches representing the two states of a Boolean value (0 and 1, or low and high, or false and true) is referred to as a \"digital signal\" or \"logic signal\" or \"binary signal\" when it is interpreted in terms of only two possible digits.\n\nThe two states are usually represented by some measurement of an electrical property: Voltage is the most common, but current is used in some logic families. A threshold is designed for each logic family. When below that threshold, the signal is \"low\", when above \"high\".\n\nThe clock signal is a special digital signal that is used to synchronize many digital circuits. The image shown can be considered the waveform of a clock signal. Logic changes are triggered either by the rising edge or the falling edge. The rising edge is the transition from a low voltage (level 1 in the diagram) to a high voltage (level 2). The falling edge is the transition from a high voltage to a low one.\n\nAlthough in a highly simplified and idealized model of a digital circuit, we may wish for these transitions to occur instantaneously, no real world circuit is purely resistive and therefore no circuit can instantly change voltage levels. This means that during a short, finite transition time the output may not properly reflect the input, and will not correspond to either a logically high or low voltage.\n\nTo create a digital signal, an analog signal must be modulated with a control signal to produce it. The simplest modulation, a type of unipolar encoding, is simply to switch on and off a DC signal, so that high voltages represent a '1' and low voltages are '0'.\n\nIn digital radio schemes one or more carrier waves are amplitude, frequency or phase modulated by the control signal to produce a digital signal suitable for transmission.\n\nAsymmetric Digital Subscriber Line (ADSL) over telephone wires, does not primarily use binary logic; the digital signals for individual carriers are modulated with different valued logics, depending on the Shannon capacity of the individual channel.\n\nOften digital signals are \"sampled\" by a clock signal at regular intervals by passing the signal through an \"edge sensitive\" flip-flop. When this is done the input is measured at those points in time, and the signal from that time is passed through to the output and the output is then held steady till the next clock.\n\nThis process is the basis of synchronous logic, and the system is also used in digital signal processing.\n\nHowever, asynchronous logic also exists, which uses no single clock, and generally operates more quickly, and may use less power, but is significantly harder to design.\n"}
{"id": "16784989", "url": "https://en.wikipedia.org/wiki?curid=16784989", "title": "Drink carrier", "text": "Drink carrier\n\nA drink carrier, sometimes also known as a cup carrier, beverage carrier or cup holder is a device used to carry multiple filled beverage cups at the same time. \n\nThere are many different designs for drink carriers, but they commonly include relatively deep indentations, holes, or compartments into which the cups are placed. This keeps the drinks from falling over during transport, and distinguishes drink carriers from cafeteria trays, though both may be used to carry both drinks and food.\n\nDrink carriers may be made from paperboard, molded pulp, plastic or other materials.\n\n\n"}
{"id": "2370923", "url": "https://en.wikipedia.org/wiki?curid=2370923", "title": "Dual pipelining", "text": "Dual pipelining\n\nDual pipelining or dual pipeline is one of computer pipelining technique to execute instructions in parallel.\nIn case of instruction level parallelism, this world is almost equivalent to superscalar.\n\nIn 1993, Intel P5 microarchitecture Pentium processors is introduced with dual-pipeling.\nThis technology allows the processor to break down a command into two shorter commands and execute them simultaneously when it receives a long command. If there are separate tasks that must be completed for a result that are independent of one another, they can be executed simultaneously to save time.\n\nHere is an example. Suppose a command is given to find the result of 14 * 27 + 512^2. The order of operations requires that 512^2 must be done first and then 14 and 27 must be multiplied. The result of those two actions would then be added together. This takes three steps. With dual pipelining, the first two actions would be computed simultaneously by different pipelines and then those would be added together. This case takes only two steps, saving one step. In a more complex command, many more steps can be saved.\n\n"}
{"id": "55062864", "url": "https://en.wikipedia.org/wiki?curid=55062864", "title": "EZCast", "text": "EZCast\n\nEZCast is a line of digital media players, built by Actions Microelectronics, that allows users to mirror media content from smart devices, including mobile devices, personal computers, and project to high-definition televisions.\n\nThe first generation of EZCast was developed in 2013, shipped 1 million units within a year, and accumulated more than 2 million EZCast app users worldwide. The latest device in the family, called EZCast 4K, was launched in November 2016 which supports 4K HEVC video streaming.\n\nEZCast technology is built into a dongle that interacts with EZCast app to stream content from smart devices, and it works across Android, Chrome OS, iOS, macOS, Windows and Windows Phone.\n\nEZCast SDK has been released to enable third party development on Android and iOS.\n"}
{"id": "31421277", "url": "https://en.wikipedia.org/wiki?curid=31421277", "title": "Fresh (2009 film)", "text": "Fresh (2009 film)\n\nFresh is a 2009 documentary film directed by Ana Sofia Joanes. The film focuses on sustainable agriculture, and depicts farmers, activists and entrepreneurs who are changing America's food system.\n\nJoanes sets out to profile people who are breaking away from conventional models of agriculture and food production. In the Shenandoah Valley of Virginia, Joel Salatin explains how he keeps his cows, chickens, pigs and natural grasses flourishing without using artificial fertilizers by closing the nutrient cycle. At Growing Power farm in Milwaukee, we meet Will Allen, who is turning three acres of industrial wasteland into nourishing farmland for his neighborhood. In Kansas City, David Ball breaks away from the standard concept of a supermarket by stocking his stores with produce from a cooperative of local farmers.\n\nJeannette Catsoulis of \"The New York Times\" noted that the film \"casts a sympathetic eye on farmers under contract to the giants of agribusiness,\" and is \"less judgmental\" and \"more folksy in tone than the recent \"Food, Inc.\".\" Mark Feeney of the \"Boston Globe\" wrote, \"\"Fresh\" may be righteous (as well as right), but it’s not unrealistic,\" and noted that \"not once in the course of the movie is the word 'locavore' used.\"\n\n\n"}
{"id": "1198383", "url": "https://en.wikipedia.org/wiki?curid=1198383", "title": "Futaba Corporation", "text": "Futaba Corporation\n\nFutaba Corporation is divided into three business units — Electronic Components, Electronic Systems Division, and Machinery and Tooling Division.\n\n\nFutaba became one of the first companies of its type to provide comprehensive radio control products, selection and service to hobbyists. Futaba systems and products were quickly accepted and used by serious competitors and casual enthusiasts alike. Futaba products are used in the air, on the water, underwater and on the ground for all types of radio-controlled models. Futaba manufactures all components in-house, including tools and manufacturing facilities.\n\nThe hobby brand was distributed in North America by FutabaUSA, by Ripmax in the UK, Ireland, Germany and Austria, along with other distributors around the world.\n\n"}
{"id": "27330293", "url": "https://en.wikipedia.org/wiki?curid=27330293", "title": "Global Internet Freedom Consortium", "text": "Global Internet Freedom Consortium\n\nThe Global Internet Freedom Consortium is a consortium of organizations that develop and deploy anti-censorship technologies for use by Internet users in countries whose governments restrict Web-based information access. The organization was reportedly begun in 2001 by Chinese-born scientists living in the United States reacting against Chinese government oppression of the Falun Gong.\n\nThe main products are Freegate and Ultrasurf.\n\nThe organization states that the majority of its funding comes from its members. In May 2010, the group was offered a $1.5 million (USD) grant from the United States Department of State. This move received criticism from representatives of the Chinese government.\n\n\n"}
{"id": "3440881", "url": "https://en.wikipedia.org/wiki?curid=3440881", "title": "Grabit (cookware)", "text": "Grabit (cookware)\n\nGrab-Its are microwave-safe cookware easily identifiable by their tab handle. They were introduced by Corning Glass Works in 1977, under the Corning Ware brand and are now sold in a slightly different form by Corelle Brands. Grab-Its are notable as being among the first cookware specifically designed for microwave use - their design was recognized by the Smithsonian's Cooper-Hewitt, National Design Museum. Grab-Its strongly resemble porringers.\n\nGrab-Its were made available in two sizes, smaller 15 ounce and larger \"Grab-A-Meal\" 24 ounce versions. The 15 ounce Grab-Its were available with a plastic cover and/or a Pyrex glass lid. 24 ounce versions came with a glass lid only. In addition to microwave use, Corning Ware and Visions Grab-Its made of Pyroceram are safe on the stovetop, in the oven, and under a broiler (without cover). Newer Corning Ware Grab-Its made of stoneware are safe for microwave and oven use only.\n\nGrabits were originally produced and sold by Corning Glass Works, and made from opaque Pyroceram glass-ceramic material.\n\nCorning introduced Grab-Its under the Visions brand in 1988. These were made of transparent Pyroceram (known as Celexium in some regions) with an amber tint. A Cranberry variant was introduced in the early 1990s.\n\nNot long after the Corning Consumer Products Company (now known as Corelle Brands) was spun off in 1998, Pyroceram-based Grabits were discontinued in the USA with the close of the Martinsburg, WV plant in the early 2000's.\n\nThe 15 ounce versions were re-introduced in the USA as a stoneware product under the Corning Ware brand a short time later. Amber 15 ounce Visions Grab-Its are still made of transparent Pyroceram in France for sales in select European and Asia-Pacific regions.\n"}
{"id": "32906538", "url": "https://en.wikipedia.org/wiki?curid=32906538", "title": "Hoa Lac Hi-tech Park", "text": "Hoa Lac Hi-tech Park\n\nHoa Lac Hi-tech Park is the first and largest hi-tech park in Vietnam with total area of 1586 ha (app. 4000 acres). Located in the area of Hanoi Capital, convenient for transportation to Noi Bai International Airport and Hai Phong deep Seaport, Hoa Lac Hi-tech Park will be developed as model of a science city with over 200,000 people working and living and consists of the following main functional zones:\nCurrently, there are two software companies located here, including FPT Software and Viettel Software. They are two biggest software companies of Vietnam.\n\n\n"}
{"id": "200138", "url": "https://en.wikipedia.org/wiki?curid=200138", "title": "Interpress", "text": "Interpress\n\nInterpress is a page description language developed at Xerox PARC, based on the Forth programming language and an earlier graphics language called JaM. PARC was unable to commercialize Interpress. Two of its creators, Chuck Geschke and John Warnock, left Xerox, formed Adobe Systems, and produced a similar language called PostScript. Interpress is used in some Xerox printers, and is supported in Xerox Ventura Publisher. Interpress is also used as the output format for PARC's InterScript system, which is an editable word processor format for rich text documents.\n\n"}
{"id": "29214348", "url": "https://en.wikipedia.org/wiki?curid=29214348", "title": "List of computer books", "text": "List of computer books\n\nComputer fundamentals by bikramjit nath\n\n\n\n"}
{"id": "972944", "url": "https://en.wikipedia.org/wiki?curid=972944", "title": "List of sensors", "text": "List of sensors\n\nThis is a list of sensors sorted by sensor type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed sensors are machines used to detect the speed of an object, usually a transport vehicle. They include:\n\n\n\nList of Sensors with functions,working,advantages,disadvantages\n"}
{"id": "61118", "url": "https://en.wikipedia.org/wiki?curid=61118", "title": "Load balancing (computing)", "text": "Load balancing (computing)\n\nIn computing, load balancing improves the distribution of workloads across multiple computing resources, such as computers, a computer cluster, network links, central processing units, or disk drives. Load balancing aims to optimize resource use, maximize throughput, minimize response time, and avoid overload of any single resource. Using multiple components with load balancing instead of a single component may increase reliability and availability through redundancy. Load balancing usually involves dedicated software or hardware, such as a multilayer switch or a Domain Name System server process.\n\nLoad balancing differs from channel bonding in that load balancing divides traffic between network interfaces on a network socket (OSI model layer 4) basis, while channel bonding implies a division of traffic between physical interfaces at a lower level, either per packet (OSI model Layer 3) or on a data link (OSI model Layer 2) basis with a protocol like shortest path bridging.\n\nOne of the most commonly used applications of load balancing is to provide a single Internet service from multiple servers, sometimes known as a server farm. Commonly load-balanced systems include popular web sites, large Internet Relay Chat networks, high-bandwidth File Transfer Protocol sites, Network News Transfer Protocol (NNTP) servers, Domain Name System (DNS) servers, and databases.\n\nAn alternate method of load balancing, which does not require a dedicated software or hardware node, is called \"round robin DNS\". In this technique, multiple IP addresses are associated with a single domain name; clients are given IP in round robin fashion. IP is assigned to clients for a time quantum.\n\nAnother more effective technique for load-balancing using DNS is to delegate www.example.org as a sub-domain whose zone is served by each of the same servers that are serving the web site. This technique works particularly well where individual servers are spread geographically on the Internet. For example:\n\nHowever, the zone file for www.example.org on each server is different such that each server resolves its own IP Address as the A-record. On server \"one\" the zone file for www.example.org reports:\n\nOn server \"two\" the same zone file contains:\n\nThis way, when a server is down, its DNS will not respond and the web service does not receive any traffic. If the line to one server is congested, the unreliability of DNS ensures less HTTP traffic reaches that server. Furthermore, the quickest DNS response to the resolver is nearly always the one from the network's closest server, ensuring geo-sensitive load-balancing . A short TTL on the A-record helps to ensure traffic is quickly diverted when a server goes down. Consideration must be given the possibility that this technique may cause individual clients to switch between individual servers in mid-session.\n\nAnother approach to load balancing is to deliver a list of server IPs to the client, and then to have client randomly select the IP from the list on each connection. This essentially relies on all clients generating similar loads, and the Law of Large Numbers to achieve a reasonably flat load distribution across servers. It has been claimed that client-side random load balancing tends to provide better load distribution than round-robin DNS; this has been attributed to caching issues with round-robin DNS, that in case of large DNS caching servers, tend to skew the distribution for round-robin DNS, while client-side random selection remains unaffected regardless of DNS caching.\n\nWith this approach, the method of delivery of list of IPs to the client can vary, and may be implemented as a DNS list (delivered to all the clients without any round-robin), or via hardcoding it to the list. If a \"smart client\" is used, detecting that randomly selected server is down and connecting randomly again, it also provides fault tolerance.\n\nFor Internet services, a server-side load balancer is usually a software program that is listening on the port where external clients connect to access services. The load balancer forwards requests to one of the \"backend\" servers, which usually replies to the load balancer. This allows the load balancer to reply to the client without the client ever knowing about the internal separation of functions. It also prevents clients from contacting back-end servers directly, which may have security benefits by hiding the structure of the internal network and preventing attacks on the kernel's network stack or unrelated services running on other ports.\n\nSome load balancers provide a mechanism for doing something special in the event that all backend servers are unavailable. This might include forwarding to a backup load balancer, or displaying a message regarding the outage.\n\nIt is also important that the load balancer itself does not become a single point of failure. Usually load balancers are implemented in high-availability pairs which may also replicate session persistence data if required by the specific application.\n\nNumerous scheduling algorithms, also called load-balancing methods, are used by load balancers to determine which back-end server to send a request to.\nSimple algorithms include random choice, round robin, or least connections. More sophisticated load balancers may take additional factors into account, such as a server's reported load, least response times, up/down status (determined by a monitoring poll of some kind), number of active connections, geographic location, capabilities, or how much traffic it has recently been assigned.\n\nAn important issue when operating a load-balanced service is how to handle information that must be kept across the multiple requests in a user's session. If this information is stored locally on one backend server, then subsequent requests going to different backend servers would not be able to find it. This might be cached information that can be recomputed, in which case load-balancing a request to a different backend server just introduces a performance issue.\n\nIdeally the cluster of servers behind the load balancer should be session-aware, so that if a client connects to any backend server at any time the user experience is unaffected. This is usually achieved with a shared database or an in-memory session database, for example Memcached.\n\nOne basic solution to the session data issue is to send all requests in a user session consistently to the same backend server. This is known as \"persistence\" or \"stickiness\". A significant downside to this technique is its lack of automatic failover: if a backend server goes down, its per-session information becomes inaccessible, and any sessions depending on it are lost. The same problem is usually relevant to central database servers; even if web servers are \"stateless\" and not \"sticky\", the central database is (see below).\n\nAssignment to a particular server might be based on a username, client IP address, or be random. Because of changes of the client's perceived address resulting from DHCP, network address translation, and web proxies this method may be unreliable. Random assignments must be remembered by the load balancer, which creates a burden on storage. If the load balancer is replaced or fails, this information may be lost, and assignments may need to be deleted after a timeout period or during periods of high load to avoid exceeding the space available for the assignment table. The random assignment method also requires that clients maintain some state, which can be a problem, for example when a web browser has disabled storage of cookies. Sophisticated load balancers use multiple persistence techniques to avoid some of the shortcomings of any one method.\n\nAnother solution is to keep the per-session data in a database. Generally this is bad for performance because it increases the load on the database: the database is best used to store information less transient than per-session data. To prevent a database from becoming a single point of failure, and to improve scalability, the database is often replicated across multiple machines, and load balancing is used to spread the query load across those replicas. Microsoft's ASP.net State Server technology is an example of a session database. All servers in a web farm store their session data on State Server and any server in the farm can retrieve the data.\n\nIn the very common case where the client is a web browser, a simple but efficient approach is to store the per-session data in the browser itself. One way to achieve this is to use a browser cookie, suitably time-stamped and encrypted. Another is URL rewriting. Storing session data on the client is generally the preferred solution: then the load balancer is free to pick any backend server to handle a request. However, this method of state-data handling is poorly suited to some complex business logic scenarios, where session state payload is big and recomputing it with every request on a server is not feasible. URL rewriting has major security issues, because the end-user can easily alter the submitted URL and thus change session streams.\n\nYet another solution to storing persistent data is to associate a name with each block of data, and use a distributed hash table to pseudo-randomly assign that name to one of the available servers, and then store that block of data in the assigned server.\n\nHardware and software load balancers may have a variety of special features. The fundamental feature of a load balancer is to be able to distribute incoming requests over a number of backend servers in the cluster according to a scheduling algorithm. Most of the following features are vendor specific:\n\n\nLoad balancing can be useful in applications with redundant communications links. For example, a company may have multiple Internet connections ensuring network access if one of the connections fails. A failover arrangement would mean that one link is designated for normal use, while the second link is used only if the primary link fails.\n\nUsing load balancing, both links can be in use all the time. A device or program monitors the availability of all links and selects the path for sending packets. The use of multiple links simultaneously increases the available bandwidth.\n\nThe IEEE approved the IEEE 802.1aq standard May 2012, also known and documented in most books as Shortest Path Bridging (SPB). SPB allows all links to be active through multiple equal cost paths, provides faster convergence times to reduce down time, and simplifies the use of load balancing in mesh network topologies (partially connected and/or fully connected) by allowing traffic to load share across all paths of a network. SPB is designed to virtually eliminate human error during configuration and preserves the plug-and-play nature that established Ethernet as the de facto protocol at Layer 2.\n\nMany telecommunications companies have multiple routes through their networks or to external networks. They use sophisticated load balancing to shift traffic from one path to another to avoid network congestion on any particular link, and sometimes to minimize the cost of transit across external networks or improve network reliability.\n\nAnother way of using load balancing is in network monitoring activities. Load balancers can be used to split huge data flows into several sub-flows and use several network analyzers, each reading a part of the original data. This is very useful for monitoring fast networks like 10GbE or STM64, where complex processing of the data may not be possible at wire speed.\n\nLoad balancing is widely used in datacenter networks to distribute traffic across many existing paths between any two servers. It allows more efficient use of network bandwidth and reduces provisioning costs. In general, load balancing in datacenter networks can be classified as either static or dynamic. Static load balancing distributes traffic by computing a hash of the source and destination addresses and port numbers of traffic flows and using it to determine how flows are assigned to one of the existing paths. Dynamic load balancing assigns traffic flows to paths by monitoring bandwidth utilization of different paths. Dynamic assignment can also be proactive or reactive. In the former case, the assignment is fixed once made, while in the latter the network logic keeps monitoring available paths and shifts flows across them as network utilization changes (with arrival of new flows or completion of existing ones). A comprehensive overview of load balancing in datacenter networks has been made available.\n\nLoad balancing is often used to implement failover—the continuation of a service after the failure of one or more of its components. The components are monitored continually (e.g., web servers may be monitored by fetching known pages), and when one becomes non-responsive, the load balancer is informed and no longer sends traffic to it. When a component comes back online, the load balancer begins to route traffic to it again. For this to work, there must be at least one component in excess of the service's capacity (N+1 redundancy). This can be much less expensive and more flexible than failover approaches where each single live component is paired with a single backup component that takes over in the event of a failure (dual modular redundancy). Some types of RAID systems can also utilize hot spare for a similar effect.\n\n"}
{"id": "13069986", "url": "https://en.wikipedia.org/wiki?curid=13069986", "title": "Manuterge", "text": "Manuterge\n\nManuterge is the name given by the Roman Catholic Church to the towel used by the priest when engaged liturgically.\n\nThere are two kinds of manuterges. One serves the needs of the sacristy. The priest uses this at the washing of hands before mass, before distributing Communion outside of Mass, and before administering baptism. It can also be used for drying the hands after they have been washed on occasions not prescribed by the rubrics, but still customary after Mass. There are no prescriptions as to material and form for the towel used in the sacristy. It is usual to have it hanging over a roller, the two ends being sewn together so as to make it into a circular band. The custom of washing the hands before Mass may date from Early Christian tradition since the ceremony is expressly mentioned in the sacramentaries of the ninth and tenth centuries.\n\nThe other manuterge is used in the Mass for drying both the hands at the Lavabo, an action performed by the priest after the Offertory as he recites the psalm, \"Lavabo\", and also by the bishop before the Offertory and after the Communion. It is kept on the credence table with the finger-bowl and cruets. There are no ecclesiastical regulations regarding the form and material of this manuterge. The towel, which is used after the Offertory during the recital of the psalm \"Lavabo\", is usually small (18 in. by 14 in.), only the points of the thumb and two fingers, and not the whole hand, being usually washed. It usually has lace or embroidery at the ends.\n"}
{"id": "165062", "url": "https://en.wikipedia.org/wiki?curid=165062", "title": "Newcomen atmospheric engine", "text": "Newcomen atmospheric engine\n\nThe atmospheric engine was invented by Thomas Newcomen in 1712, and is often referred to simply as a Newcomen engine. The engine was operated by condensing steam drawn into the cylinder, thereby creating a partial vacuum which allowed the atmospheric pressure to push the piston into the cylinder. It was the first practical device to harness steam to produce mechanical work. Newcomen engines were used throughout Britain and Europe, principally to pump water out of mines. Hundreds were constructed through the 18th century.\n\nJames Watt's later engine design was an improved version of the Newcomen engine that roughly doubled fuel efficiency. Many atmospheric engines were converted to the Watt design, for a price based on a fraction of the savings in fuel. As a result, Watt is today better known than Newcomen in relation to the origin of the steam engine.\n\nPrior to Newcomen a number of small steam devices of various sorts had been made, but most were essentially novelties. Around 1600 a number of experimenters used steam to power small fountains working like a coffee percolator. First a container was filled with water via a pipe, which extended through the top of the container to nearly the bottom. The bottom of the pipe would be submerged in the water, making the container airtight. The container was then heated to make the water boil. The steam generated pressurized the container, but the inner pipe, immersed at the bottom by liquid, and lacking an airtight seal at top, remained at a lower pressure; expanding steam forced the water at the bottom of the container into and up the pipe to spurt out of a nozzle on top. These devices had limited effectiveness but illustrated the principle's viability.\n\nIn 1606, the Spaniard, Jerónimo de Ayanz y Beaumont demonstrated and was granted a patent for a steam powered water pump. The pump was successfully used to drain the inundated mines of Guadalcanal, Spain.\n\nIn 1662 Edward Somerset, second Marquess of Worcester, published a book containing several ideas he had been working on. One was for a steam-powered pump to supply water to fountains; the device alternately used a partial vacuum and steam pressure. Two containers were alternately filled with steam, then sprayed with cold water making the steam within condense; this produced a partial vacuum that would draw water through a pipe up from a well to the container. A fresh charge of steam under pressure then drove the water from the container up another pipe to a higher-level header before that steam condensed and the cycle repeated. By working the two containers alternately, the delivery rate to the header tank could be increased.\n\nIn 1698 Thomas Savery patented a steam-powered pump he called the \"Miner's Friend\", essentially identical to Somerset's design and almost certainly a direct copy. The process of cooling and creating the vacuum was fairly slow, so Savery later added an external cold water spray to quickly cool the steam.\n\nSavery's invention cannot be strictly regarded as the first steam \"engine\" since it had no moving parts and could not transmit its power to any external device. There were evidently high hopes for the Miner's Friend, which led Parliament to extend the life of the patent by 21 years, so that the 1699 patent would not expire until 1733. Unfortunately, Savery's device proved much less successful than had been hoped.\n\nA theoretical problem with Savery's device stemmed from the fact that a vacuum could only raise water to a maximum height of about ; to this could be added another , or so, raised by steam pressure. This was insufficient to pump water out of a mine. In Savery's pamphlet, he suggests setting the boiler and containers on a ledge in the mineshaft and even a series of two or more pumps for deeper levels. Obviously these were inconvenient solutions and some sort of mechanical pump working at surface level – one that lifted the water directly instead of \"sucking\" it up – was desirable. Such pumps were common already, powered by horses, but required a vertical reciprocating drive that Savery's system did not provide. The more practical problem concerned having a boiler operating under pressure, as demonstrated when the boiler of an engine at Wednesbury exploded, perhaps in 1705.\n\nLouis Figuier in his monumental work gives a full quotation of Denis Papin's paper published in 1690 in \"Acta eruditorum\" at Leipzig, entitled \"Nouvelle méthode pour obtenir à bas prix des forces considérables\" (A new method for cheaply obtaining considerable forces). It seems that the idea came to Papin whilst working with Robert Boyle at the Royal Society in London. Papin describes first pouring a small quantity of water into the bottom of a vertical cylinder, inserting a piston on a rod and after first evacuating the air below the piston, placing a fire beneath the cylinder to boil the water away and create enough steam pressure to raise the piston to the top end of the cylinder. The piston was then temporarily locked in the upper position by a spring catch engaging a notch in the rod. The fire was then removed, allowing the cylinder to cool, which condensed steam back into water, thus creating a vacuum beneath the piston. To the end of the piston rod was attached a cord passing over two pulleys and a weight hung down from the cord's end. Upon releasing the catch, the piston was sharply drawn down to the bottom of the cylinder by the pressure differential between the atmosphere and the created vacuum; enough force was thus generated to raise a weight. Although the engine certainly worked as far as it went, it was devised merely to demonstrate the principle and having got this far, Papin never developed it further, although in his paper he did write about the potential of boats driven by \"firetubes\". Instead he allowed himself to be distracted into developing a variant of the Savery engine.\n\nNewcomen took forward Papin's experiment and made it workable, although little information exists as to exactly how this came about. The main problem to which Papin had given no solution was how to make the action repeatable at regular intervals. The way forward was to provide, as Savery had, a boiler capable of ensuring the continuity of the supply of steam to the cylinder, providing the vacuum power stroke by condensing the steam, and disposing of the water once it had been condensed. The power piston was hung by chains from the end of a rocking beam. Unlike Savery's device, pumping was entirely mechanical, the work of the steam engine being to lift a weighted rod slung from the opposite extremity of the rocking beam. The rod descended the mine shaft by gravity and drove a force pump, or pole pump (or most often a gang of two) inside the mineshaft. The suction stroke of the pump was only for the length of the upward (priming) stroke, there consequently was no longer the 30-foot restriction of a vacuum pump and water could be forced up a column from far greater depths. The boiler supplied the steam at extremely low pressure and was at first located immediately beneath the power cylinder but could also be placed behind a separating wall with a connecting steam pipe. Making all this work needed the skill of a practical engineer; Newcomen's trade as an \"ironmonger\" or metal merchant would have given him significant practical knowledge of what materials would be suitable for such an engine and brought him into contact with people having even more detailed knowledge.\n\nIt is possible that the first Newcomen engine was in Cornwall. Its location is uncertain, but it is known that one was in operation at Wheal Vor mine in 1715. The earliest examples for which reliable records exist were two engines in the Black Country, of which the more famous was that erected in 1712 at the Conygree Coalworks near Dudley, This is generally accepted as the first successful Newcomen engine, but it may have been preceded by one built a mile and a half east of Wolverhampton. Both these were used by Newcomen and his partner John Calley to pump out water-filled coal mines. A working replica can today be seen at the nearby Black Country Living Museum, which stands on another part of what was Lord Dudley's Conygree Park.\n\nSoon orders from wet mines all over England were coming in, and some have suggested that word of his achievement was spread through his Baptist connections. Since Savery's patent had not yet run out, Newcomen was forced to come to an arrangement with Savery and operate under the latter's patent, as its term was much longer than any Newcomen could have easily obtained. During the latter years of its currency, the patent belonged to an unincorporated company, \"The Proprietors of the Invention for raising water by fire\".\n\nAlthough its first use was in coal-mining areas, Newcomen's engine was also used for pumping water out of the metal mines in his native West Country, such as the tin mines of Cornwall. By the time of his death, Newcomen and others had installed over a hundred of his engines, not only in the West Country and the Midlands but also in north Wales, near Newcastle and in Cumbria. Small numbers were built in other European countries, including in France, Belgium, Spain, and Hungary, also at Dannemora, Sweden. Evidence of the use of a Newcomen Steam Engine associated with early coal mines was found in 2010 in Midlothian, VA (site of some of the first coal mines in the US).\n(Dutton and Associates survey dated 24 November 2009).\n\nAlthough based on simple principles, Newcomen's engine was rather complex and showed signs of incremental development, problems being empirically addressed as they arose. It consisted of a boiler A, usually a haystack boiler, situated directly below the cylinder. This produced large quantities of very low pressure steam, no more than 1 – 2 psi (0.07 – 0.14 bar) – the maximum allowable pressure for a boiler that in earlier versions was made of copper with a domed top of lead and later entirely assembled from small riveted iron plates. The action of the engine was transmitted through a rocking \"Great balanced Beam\", the fulcrum E of which rested on the very solid end-gable wall of the purpose-built engine house with the pump side projecting outside of the building, the engine being located \"in-house\". The pump rods were slung by a chain from the arch-head F of the great beam. From the in-house arch-head D was suspended a piston P working in a cylinder B, the top end of which was open to the atmosphere above the piston and the bottom end closed, apart from the short admission pipe connecting the cylinder to the boiler; early cylinders were made of cast brass, but cast iron was soon found more effective and much cheaper to produce. The piston was surrounded by a seal in the form of a leather ring, but as the cylinder bore was finished by hand and not absolutely true, a layer of water had to be constantly maintained on top of the piston. Installed high up in the engine house was a water tank C (or \"header tank\") fed by a small in-house pump slung from a smaller arch-head. The header tank supplied cold water under pressure via a \"stand-pipe\" for condensing the steam in the cylinder with a small branch supplying the cylinder-sealing water; at each top stroke of the piston excess warm sealing water overflowed down two pipes, one to the in-house well and the other to feed the boiler by gravity.\n\nThe pump equipment was heavier than the steam piston, so that the position of the beam at rest was pump-side down/engine-side up, which was called \"out of the house\".\n\nTo start the engine, the regulator valve V was opened and steam admitted into the cylinder from the boiler, filling the space beneath the piston. The regulator valve was then closed and the water injection valve V' briefly snapped open and shut, sending a spray of cold water into the cylinder. This condensed the steam and created a partial vacuum under the piston. Pressure differential between the atmosphere above the piston and the partial vacuum below then drove the piston down making the power stroke, bringing the beam \"into the house\" and raising the pump gear.\n\nSteam was then readmitted to the cylinder, destroying the vacuum and driving the condensate down the sinking or \"eduction\" pipe. As the low pressure steam from the boiler flowed into the cylinder, the weight of the pump and gear returned the beam to its initial position whilst at the same time driving the water up from the mine.\n\nThis cycle was repeated around 12 times per minute.\n\nNewcomen found that his first engine would stop working after a while, and eventually discovered that this was due to small amounts of air being admitted to the cylinder with the steam. Water usually contains some dissolved air, and boiling the water released this with the steam. This air could not be condensed by the water spray and gradually accumulated until the engine became \"wind logged\". To prevent this a release valve called a \"snifting clack\" or snifter valve was added near the bottom of the cylinder. This opened briefly when steam was first admitted to and non-condensable gas was driven from the cylinder. Its name was derived from the noise it made when it operated to release the air and steam \"like a Man snifting with a Cold\".\n\nIn early versions, the valves or \"plugs\" as they were then called, were operated manually by the \"plug man\" but the repetitive action demanded precise timing, making automatic action desirable. This was obtained by means of a \"plug tree\" which was a beam suspended vertically alongside the cylinder from a small arch head by crossed chains, its function being to open and close the valves automatically when the beam reached certain positions, by means of tappets and escapement mechanisms using weights. On the 1712 engine, the water feed pump was attached to the bottom of the plug tree, but later engines had the pump outside suspended from a separate small arch-head. There is a common legend that in 1713 a \"cock boy\" named Humphrey Potter, whose duty it was to open and shut the valves of an engine he attended, made the engine self-acting by causing the beam itself to open and close the valves by suitable cords and catches (known as the \"potter cord\"); however the plug tree device (the first form of valve gear) was very likely established practice before 1715, and is clearly depicted in the earliest known images of Newcomen engines by Henry Beighton (1717) (believed by Hulse to depict the 1714 Griff colliery engine) and by Thomas Barney (1719) (depicting the 1712 Dudley Castle engine). Because of the very heavy steam demands, the engine had to be periodically stopped and restarted, but even this process was automated by means of a buoy rising and falling in a vertical stand pipe fixed to the boiler (the first pressure gauge?). The buoy was attached to the \"scoggen\", a weighted lever that worked a stop blocking the water injection valve shut until more steam had been raised.\n\nMost images show only the engine side, giving no information on the pumps. Current opinion is that at least on the early engines, dead-weight force pumps were used, the work of the engine being solely to lift the pump side ready for the next downwards pump stroke. This is the arrangement used for the Dudley Castle replica which effectively works at the original stated rate of 12 strokes per minute/10 gallons (54.6litres) lifted per stroke. The later Watt engines worked lift pumps powered by the engine stroke and it may be that later versions of the Newcomen engine did so too.\n\nTowards the close of its career, the atmospheric engine was much improved in its mechanical details and its proportions by John Smeaton, who built many large engines of this type during the 1770s. The urgent need for an engine to give rotary motion was making itself felt and this was done with limited success by Wasborough and Pickard using a Newcomen engine to drive a flywheel through a crank. Although the principle of the crank had long been known, Pickard managed to obtain a 12-year patent in 1780 for the specific application of the crank to steam engines; this was a setback to Boulton and Watt who got round the patent by applying the sun and planet motion to their advanced double-acting rotative engine of 1782.\n\nBy 1725 the Newcomen engine was in common use in mining, particularly collieries. It held its place with little material change for the rest of the century. Use of the Newcomen engine was extended in some places to pump municipal water supply; for instance the first Newcomen engine in France was built at Passy in 1726 to pump water from the Seine to the city of Paris. It was also used to power machinery indirectly, by returning water from below a water wheel to a reservoir above it, so that the same water could again turn the wheel. Among the earliest examples of this was at Coalbrookdale. A horse-powered pump had been installed in 1735 to return water to the pool above the Old Blast Furnace. This was replaced by a Newcomen engine in 1742–3. Several new furnaces built in Shropshire in the 1750s were powered in a similar way, including Horsehay and Ketley Furnaces and Madeley Wood or Bedlam Furnaces. The latter does not seem to have had a pool above the furnace, merely a tank into which the water was pumped. In other industries, engine-pumping was less common, but Richard Arkwright used an engine to provide additional power for his cotton mill.\n\nAttempts were made to drive machinery by Newcomen engines, but these were unsuccessful, as the single power stroke produced a very jerky motion.\n\nThe main problem with the Newcomen design was that it used energy inefficiently, and was therefore expensive to operate. After the water vapor within was cooled enough to create the vacuum, the cylinder walls were cold enough to condense some of the steam as it was admitted during the next intake stroke. This meant that a considerable amount of fuel was being used just to heat the cylinder back to the point where the steam would start to fill it again. As the heat losses were related to the surfaces, while useful work related to the volume, increases in the size of the engine increased efficiency, and Newcomen engines became larger in time. However, efficiency did not matter very much within the context of a colliery, where coal was freely available.\n\nNewcomen's engine was only replaced when James Watt improved it in 1769 to avoid this problem (Watt had been asked to repair a model of a Newcomen engine by Glasgow University; a small model that exaggerated the problem). In the Watt steam engine, condensation took place in an exterior condenser unit, attached to the steam cylinder via a pipe. When a valve on the pipe was opened, the vacuum in the condenser would, in turn, evacuate that part of the cylinder below the piston. This eliminated the cooling of the main cylinder walls and such, and dramatically reduced fuel use. It also enabled the development of a double-acting cylinder, with both upwards and downwards power strokes, increasing amount of power from the engine without a great increase in the size of the engine.\n\nWatt's design, introduced in 1769, did not eliminate Newcomen engines immediately. Watt's vigorous defence of his patents resulted in the continued use of the Newcomen engine in an effort to avoid royalty payments. When his patents expired in 1800 there was a rush to install Watt engines, and Newcomen engines were eclipsed, even in collieries.\n\nThe Newcomen Memorial Engine can be seen operating in Newcomen's home town of Dartmouth, where it was moved in 1963 by the Newcomen Society. This is believed to date from 1725, when it was initially installed at the Griff Colliery near Coventry.\n\nAn engine was installed at a colliery in Ashton-under-Lyne in about 1760. Known locally as \"Fairbottom Bobs\" it is now preserved at the Henry Ford Museum in Dearborn, Michigan.\n\nThe only Newcomen-style engine still extant in its original location is at what is now the Elsecar Heritage Centre, near Barnsley in South Yorkshire. This was probably the last commercially used Newcomen-style engine, as it ran from 1795 until 1923. The engine underwent extensive conservation works, together with its original shaft and engine-house, which were completed in autumn 2014.\n\nIn 1986, a full-scale operational replica of the 1712 Newcomen Steam Engine was completed at the Black Country Living Museum in Dudley. It is the only full-size working replica of the engine in existence. The 'fire engine' as it was known, is an impressive brick building from which a wooden beam projects through one wall. Rods hang from the outer end of the beam and operate pumps at the bottom of the mine shaft which raise the water to the surface. The engine itself is simple, with only a boiler, a cylinder and piston and operating valves. A coal fire heats the water in the boiler which is little more than a covered pan and the steam generated then passes through a valve into the brass cylinder above the boiler. The cylinder is more than 6 1/2 feet (2 metres) long and 20 1/2 inches (52 centimetres) in diameter. The steam in the cylinder is condensed by injecting cold water and the vacuum beneath the piston pulls the inner end of the beam down and causes the pump to move.\n\nA static example of a Newcomen Engine is in the Science Museum.\n\n\n\n"}
{"id": "1071853", "url": "https://en.wikipedia.org/wiki?curid=1071853", "title": "No. 100 Group RAF", "text": "No. 100 Group RAF\n\nNo. 100 (Bomber Support) Group was a special duties group within RAF Bomber Command. The group was formed on 11 November 1943 to consolidate the increasingly complex business of electronic warfare and countermeasures in one organisation. The group was responsible for the development, operational trial and use of electronic warfare and countermeasures equipment. It was based at RAF stations in East Anglia, chiefly Norfolk.\n\nThe group was a pioneer in countering the formidable force of radar-equipped Luftwaffe night fighters, using a range of electronic 'homers' fitted to de Havilland Mosquito fighters which detected night fighter radar and radio emissions and allowed the RAF fighters to home in onto the Axis aircraft and either shoot them down or disrupt their missions against the bomber streams. Other Mosquitoes would patrol around Luftwaffe fighter airfields ready to attack night fighters as they landed. \n\nThis constant harassment had a detrimental effect on the morale and confidence of many Luftwaffe crews and indirectly led to a high proportion of aircraft and aircrew wastage from crashes as night fighters hurried in to land to avoid the Mosquito threat (real or imagined). \n\nFrom 1944–45, the Mosquitos of 100 Group claimed 258 Luftwaffe aircraft shot down for 70 losses. The gradually increasing threat from the RAF fighters also created what the Luftwaffe crews nicknamed \"Moskito Panik\" as the night fighter crews were never sure when or where they may come under attack from the marauding 100 Group fighters.\n\nTop Mosquito ace with 100 Group was Wing Commander Branse Burbridge of 85 Squadron, with 21 claims from 1944–45.\n\nThe bomber squadrons of 100 Group utilised various specialist electronic jamming devices to disrupt German radio communications and radar. During 100 Group's existence over 32 different devices were evaluated and used. Specially equipped 100 Group aircraft would fly in the bomber stream. Much of this equipment was developed at the Telecommunications Research Establishment (TRE).\n\nSpecial equipment used included \"Airborne Cigar\" (ABC) jammer, \"Jostle\" (jammer), \"Mandrel\" (jammer), \"Airborne Grocer\" (jammer), \"Piperack\" (jammer), \"Perfectos\" (homer), \"Serrate\" (homer), \"Corona\" (spoofer), \"Carpet\" (jammer) and \"Lucero\" (homer), used against German equipment such as \"Lichtenstein\", \"Freya\", and \"Wurzburg\" radars. \n\nNo. 100 Group was headquartered at Bylaugh Hall, Norfolk from January 1944, a central location from which to administer the group's airfields in north Norfolk. No 100 Group operated from eight airfields with approximately 260 aircraft, 140 of which were various marks of Mosquito night fighter intruders with the remainder consisting of Handley Page Halifaxes, Short Stirlings, Vickers Wellingtons, Fortresses and Liberators carrying electronic jamming equipment. The group also operated the Bristol Beaufighter for a short time.\n\nThe group disbanded on 17 December 1945. During its existence it had one commander, Air Vice-Marshal Edward Addison.\n\nOther units and stations:\n\n\n"}
{"id": "964748", "url": "https://en.wikipedia.org/wiki?curid=964748", "title": "Odyssey (launch platform)", "text": "Odyssey (launch platform)\n\nLP \"Odyssey\" is a self-propelled semi-submersible mobile spacecraft launch platform converted from a mobile drilling rig in 1997.\n\nThe vessel was used by Sea Launch for equatorial Pacific Ocean launches. She works in concert with the assembly and control ship . Her home port is the Port of Long Beach in the United States.\n\nIn her current form, \"Odyssey\" is long and about wide, with an empty draft displacement of , and a submerged draft displacement of . The vessel has accommodations for 68 crew and launch system personnel, including living, dining, medical and recreation facilities. A large, environmentally-controlled hangar stores the rocket during transit and then rolls it out and erects it prior to fueling and launch.\n\nIn September 2016 the platform along with other Sea Launch assets was sold to S7 Group, the parent company of S7 Airlines.\n\nThe platform was built in 1982 for Ocean Drilling & Exploration Company (ODECO) by Sumitomo Heavy Industries. It drilled its first exploratory hole about south of Yakutat for ARCO Alaska, Inc. The rig cost about to build during the early eighties oil \"boom\".\n\nDuring construction the vessel was called \"Ocean Ranger II\", and was renamed \"Ocean Odyssey\" after capsized with all hands lost during a storm off Newfoundland on February 15, 1982.\n\nWhen built, \"Ocean Odyssey\" was classed +A1 +AMS by the American Bureau of Shipping for unrestricted worldwide ocean service. She was a long, wide, twin-hull design with a propulsion system. The rig's structure was designed to simultaneously withstand winds, waves, and a current. The derrick was fully enclosed with a heated drill floor permitting operations down to .\n\nThe rig had other advanced extreme-condition features as well. For example, the rig's columns were strengthened to withstand some ice impact and the marine riser had a feature similar to a cow-catcher to keep floating ice off the marine riser that connected the rig to the well on the ocean bottom.\n\nOn September 22, 1988, \"Ocean Odyssey\" suffered a blowout while operated by ODECO (now Diamond Offshore Drilling) on hire to ARCO (now a subsidiary of BP), drilling the 22/30b-3 well on a prospect in the North Sea. The ultimate direct cause of the incident was a failure of the subsea wellhead equipment after a prolonged period of well control. During the resulting fire the radio operator, Timothy Williams, was killed. He had been ordered from the lifeboats and back to the radio room by the rig's manager, who failed to countermand the order when the rig was evacuated.\n\nSurvivors were picked up by the rig's emergency standby vessel \"Notts Forest\" (38 rescued) and the nearby anchor handling tug \"British Fulmar\" (28 rescued). Four Sea King helicopters from and a Sea King from RAF Lossiemouth assisted rescue operations and transferred survivors from \"Notts Forest\" and \"British Fulmar\" to the drilling rig \"Sedneth 701\". A Royal Air Force Hawker Siddeley Nimrod provided coordination on scene.\n\nThe incident was featured in the 1990 STV television series \"Rescue\" episode \"Missing\".\n\n\"Ocean Odyssey\" spent the next several years as a rusting hulk in the docks of Dundee, Scotland. Her availability prompted Boeing to establish the Sea Launch consortium, for which she was bought in 1993 by Kværner Rosenberg of Stavanger, Norway, and renamed LP \"Odyssey\".\n\nFrom late 1995 to May 1997, Kværner extended the length of the platform and added a pair of support columns and additional propulsion systems. The upper deck — the location of the former drill floor — was rebuilt to accommodate the launch pad and launch vehicle service hangar. In May 1997, \"Ocean Odyssey\" arrived at Kværner Vyborg Shipyard for the installation of the launch vehicle equipment itself.\n\nBy 1999, the vessel was ready for service, and on March 27, 1999, a Zenit-3SL rocket successfully launched a demonstration satellite to a geostationary transfer orbit. The first commercial launch occurred on October 9, 1999, with the orbiting of the DirecTV 1-R satellite.\n\nOn January 30, 2007, a Zenit-3SL carrying the NSS-8 satellite exploded aboard \"Odyssey\" at liftoff due to a turbopump malfunction. There were no injuries, as the ship had been evacuated for launch operations. Damage to the launch platform was mostly superficial, though a flame deflector was knocked loose from underneath the platform and lost, along with damage to the hangar doors and antennae. The vessel was repaired at a shipyard in Vancouver, British Columbia.\n\n\"Odyssey\" returned to service with the January 15, 2008, successful launch of the Thuraya 3 satellite.\n\nOn February 1, 2013, the Zenit-3SL rocket carrying Intelsat 27 suffered a failure after its launch from \"Odyssey\", crashing a short distance from the launch platform. Its first stage engine appeared to shut down around 25 seconds after launch and telemetry from the rocket was lost about 15 seconds later. Telemetry indicated that excessive roll was detected 11 seconds after launch. The guidance system was programmed to shut down the engine, but only after the rocket was safely away from the launch platform. It is believed that a failure in a hydraulic pump that provides power for gimbaling the RD-171 engine was ultimately the cause. No damage was done to the launch platform.\n\n\n"}
{"id": "1176900", "url": "https://en.wikipedia.org/wiki?curid=1176900", "title": "Osseointegration", "text": "Osseointegration\n\nOsseointegration (from Latin \"ossum\" \"bone\" and \"integrare\" \"to make whole\") is the direct structural and functional connection between living bone and the surface of a load-bearing artificial implant (\"load-bearing\" as defined by Albrektsson et al. in 1981). A more recent definition (by Schroeder et al.) defines osseointegration as \"functional ankylosis (bone adherence)\", where new bone is laid down directly on the implant surface and the implant exhibits mechanical stability (also known as \"primary stability\" – i.e., resistance to destabilization by mechanical agitation or shear forces). Osseointegration has enhanced the science of medical bone and joint replacement techniques as well as dental implants and improving prosthetics for amputees.\n\nOsseointegration is also defined as: \"the formation of a direct interface between an implant and bone, without intervening soft tissue\". Osseointegrated implant is a type of implant defined as \"an endosteal implant containing pores into which osteoblasts and supporting connective tissue can migrate\". Applied to oral implantology, this thus refers to bone grown right up to the implant surface without interposed soft tissue layer. No scar tissue, cartilage or ligament fibers are present between the bone and implant surface. The direct contact of bone and implant surface can be verified microscopically.\n\nOsseointegration may also be defined as :\n\nOsseointegration was first observed—albeit not explicitly stated—by Bothe, Beaton, and Davenport in 1940. Bothe et al. were the first researchers to implant titanium in an animal and remarked how it had the tendency to fuse with bone. Bothe et al. reported that due to the elemental nature of the titanium, its strength, and its hardness, it had great potential to be used as future prosthesis material. Osseointegration was later described by Gottlieb Leventhal in 1951. Leventhal placed titanium screws in rat femurs and remarked how \"At the end of 6 weeks, the screws were slightly tighter than when originally put in; at 12 weeks, the screws were more difficult to remove; and at the end of 16 weeks, the screws were so tight that in one specimen the femur was fractured when an attempt was made to remove the screw. Microscopic examinations of the bone structure revealed no reaction to the implants. The trabeculation appeared to be perfectly normal.\" The reactions described by Leventhal and Bothe et al. would later be coined into the term \"osseointegration\" by Per-Ingvar Brånemark of Sweden. In 1952, Brånemark conducted an experiment where he utilized a titanium implant chamber to study blood flow in rabbit bone. At the conclusion of the experiment, when it became time to remove the titanium chambers from the bone, he discovered that the bone had integrated so completely with the implant that the chamber could not be removed. Brånemark called this \"osseointegration\", and, like Bothe et al. and Leventhal before him, saw the possibilities for human use.\n\nIn dental medicine the implementation of osseointegration started in the mid-1960s as a result of Brånemark's work. In 1965 Brånemark, who was at the time Professor of Anatomy at the University of Gothenburg, placed dental implants into the first human patient – Gösta Larsson. This patient had a cleft palate defect and required implants to support a palatal obturator. Gösta Larsson died in 2005, with the original implants still in place after 40 years of function.\n\nIn the mid-1970s Brånemark entered into a commercial partnership with the Swedish defense company Bofors to manufacture dental implants and the instrumentation required for their placement. Eventually an offshoot of Bofors, Nobel Pharma, was created to concentrate on this product line. Nobel Pharma subsequently became Nobel Biocare.\n\nBrånemark spent almost 30 years fighting the scientific community for acceptance of osseointegration as a viable treatment. In Sweden he was often openly ridiculed at scientific conferences. His university stopped funding for his research, forcing him to open a private clinic to continue the treatment of patients. Eventually an emerging breed of young academics started to notice the work being performed in Sweden. Toronto's George Zarb, a Maltese-born Canadian prosthodontist, was instrumental in bringing the concept of osseointegration to the wider world. The 1983 Toronto Conference is generally considered to be the turning point, when finally the worldwide scientific community accepted Brånemark's work. Today osseointegration is a highly predictable and commonplace treatment modality.\nMore recently since 2010 Al Muderis in Sydney Australia utilised a high tensile strength titanium implant with high prose plasma sprayed surface as an intramedullary prosthesis that is inserted into the bone residuum of amputees and then connect through an opening in the skin to a robotic limb prosthesis. This allows amputees to mobilise with more comfort and less energy consumption. Al Muderis also published the first series of combining osseointegration prosthesis with Joint replacement enabling below knee amputees with knee arthritis or short residual bone to mobilise without the need of a socket prosthesis.\n\nOn December 7, 2015, two Operation Iraqi Freedom/Operation Enduring Freedom veterans, Bryant Jacobs and Ed Salau, will be the first in America to get a percutaneous osseointegrated prosthesis. During the first stage, doctors at Salt Lake Veterans Affairs Hospital will embed a titanium stud in the femur of each patient. About six weeks later, they will go back and attach the docking mechanism for the prosthesis.\n\nOsseointegration is a dynamic process in which characteristics of the implant (i.e. macrogeometry, surface properties, etc.) play a role in modulating molecular and cellular behavior. While osseointegration has been observed using different materials, it is most often used to describe the reaction of bone tissues to titanium, or titanium coated with calcium phosphate derivatives. It was previously thought that titanium implants were retained in bone through the action of mechanical stabilization or interfacial bonding. Alternatively, calcium phosphate coated implants were thought to be stabilized via chemical bonding. It is now known that both calcium phosphate coated implants and titanium implants are stabilized chemically with bone, either through direct contact between calcium and titanium atoms, or by the bonding to a cement line-like layer at the implant/bone interface. While there are some differences (e.g. like the lack of chondrogenic progenitors), osseointegration occurs through the same mechanisms as bone fracture healing.\n\nFor osseointegrated dental implants, metallic, ceramic, and polymeric materials have been used, in particular titanium. To be termed osseointegration the connection between the bone and the implant need not be 100 percent, and the essence of osseointegration derives more from the stability of the fixation than the degree of contact in histologic terms. In short it represents a process whereby clinically asymptomatic rigid fixation of alloplastic materials is achieved, and maintained, in bone during functional loading. Implant healing times and initial stability are a function of implant characteristics. For example, implants utilizing a screw-root form design achieve high initial mechanical stability through the action of their screws against the bone. Following placement of the implant, healing typically takes several weeks or months before the implant is fully integrated into the surrounding bone. First evidence of integration occurs after a few weeks, while more robust connection is progressively effected over the next months or years. Implants that possess a screw-root form design result in bone resorption followed by interfacial bone remodeling and growth around the implant.\n\nImplants utilizing a plateau-root form design (or screw-root form implants with a wide enough gap between the pitch of the screws) undergo a different mode of peri-implant ossification. Unlike the aforementioned screw-root form implants, plateau-root form implants exhibit de novo bone formation on the implant surface. The type of bone healing exhibited by plateau-root form implants is known as intramembranous-like healing.\n\nThough the osseointegrated interface becomes resistant to external shocks over time, it may be damaged by prolonged adverse stimuli and overload, which may result in implant failure. In studies performed using \"Mini dental implants,\" it was noted that the absence of micromotion at the bone-implant interface was necessary to enable proper osseointegration. Further, it was noted that there is a critical threshold of micromotion above which a fibrous encapsulation process occurs, rather than osseointegration.\n\nOther complications may arise even in the absence of external impact. One issue is the growing of cement. In normal cases, the absence of cementum on the implant surface prevents the attachment of collagen fibers. This is normally the case due to the absence of cementum progenitor cells in the area receiving the implant. However, when such cells are present, cement may form on or around the implant surface, and a functional collagen attachment may attach to it.\n\nSince 2005, a number of orthopedic device manufacturers have introduced products that feature porous metal construction. Clinical studies on mammals have shown that porous metals, such as titanium foam, may allow the formation of vascular systems within the porous area. For orthopedic uses, metals such as tantalum or titanium are often used, as these metals exhibit high tensile strength and corrosion resistance with excellent biocompatibility.\n\nThe process of osseointegration in metal foams is similar to that in bone grafts. The porous bone-like properties of the metal foam contribute to extensive bone infiltration, allowing osteoblast activity to take place. In addition, the porous structure allows for soft tissue adherence and vascularization within the implant. These materials are currently deployed in hip replacement, knee replacement and dental implant surgeries.\n\nThere are a number of methods used to gauge the level of osseointegration and the subsequent stability of an implant. One widely used diagnostic procedure is percussion analysis, where a dental instrument is tapped against the implant carrier. The nature of the ringing that results is used as a qualitative measure of the implant’s stability. An integrated implant will elicit a higher pitched \"crystal\" sound, whereas a non-integrated implant will elicit a dull, low-pitched sound.\n\nAnother method is a reverse torque test, in which the implant carrier is unscrewed. If it fails to unscrew under the reverse torque pressure, the implant is stable. If the implant rotates under the pressure it is deemed a failure and removed. This method comes at the risk of fracturing bone that is mid-way in the process of osseointegration. It is also unreliable in determining the osseointegration potential of a bone region, as tests have yielded that a rotating implant can go on to be successfully integrated.\n\nA non-invasive and increasingly implemented diagnostic method is resonance frequency analysis (RFA). A resonance frequency analyzer device prompts vibrations in a small metal rod temporarily attached to the implant. As the rod vibrates, the probe reads its resonance frequency and translates it into an implant stability quotient (ISQ), which ranges from 1–100, with 100 indicating the highest stability state. Values ranging between 57 and 82 are generally considered stable, though each case must be considered independently.\n\n\n\n\n"}
{"id": "24619477", "url": "https://en.wikipedia.org/wiki?curid=24619477", "title": "PICMG 1.3", "text": "PICMG 1.3\n\nPICMG 1.3 is a PICMG specification which is commonly referred to as SHB Express. SHB Express is a modernization of PICMG 1.0 single board computer specification. SHB Express, or System Host Board – Express, uses the same physical form factor as PICMG 1.0 boards. The board-to-backplane interfaces are PCI Express instead of PCI and ISA, although the use of PCI remains as an option.\n\n\nAdopted : 8/20/2005\n\nCurrent Revision : 1.0\n\n"}
{"id": "30362381", "url": "https://en.wikipedia.org/wiki?curid=30362381", "title": "PROBA-V", "text": "PROBA-V\n\nPROBA-V is the fourth satellite in the European Space Agency's PROBA series; the V standing for vegetation.\n\n\"PROBA-V\" is a small satellite, assuring the succession of the Vegetation instruments on board the French SPOT-4 and SPOT-5 Earth observation missions. PROBA-V was initiated by the Space- and Aeronautics department of the BELgian Science Policy Office. It is built by QinetiQ Space N.V. and operated by ESA and uses a PROBA platform. PROBA-V will support applications such as land use, worldwide vegetation classification, crop monitoring, famine prediction, food security, disaster monitoring and biosphere studies. The mission was originally conceived as a \"gap filler\" between the SPOT-5 end-of-life (foreseen mid-2014) and the launch of the constellation of the Sentinel-3A and -3B satellites. Due to delays of the Sentinel programme and because some instrument specifications of the Sentinel3 satellites have meanwhile changed, PROBA-V no longer is a gap filler mission but will assure the continuation of the Vegetation programme as such. The Vegetation, International User Committee (IUC, an independent body consisting of Vegetation users, that provides user feedback and recommendations to the Vegetation Steering Committee) has recommended to foresee a successor mission for PROBA-V, because the current specifications of the Sentinel3 satellites no longer allow the continuation of the Vegetation products in the long run. This mission is the first full application mission with a PROBA platform and had a very tight development schedule.\n\nPROBA-V and its onboard instruments have been developed and built by QinetiQ Space N.V and subcontractors for the Directorate of TEChnology (DTEC) of ESA. These developments have been paid with Belgian and Luxembourg contributions to ESA. The In Orbit Commissioning Review (IOCR) was successfully achieved on 27/11/2013. After the launch and the commissioning, PROBA-V was handed over from DTEC to the Earth Observation Directorate of ESA on 12/12/2013. After this handover the Earth Observation Directorate of ESA will manage the satellite operations, instrument data collection and distribution of the traditional Vegetation products to the users. VITO will actually generate and distribute these products. The management of the new, higher resolution products will be assured by the Space- and Aeronautics department of the BELgian Science Policy Office. For these products too, VITO will actually generate and distribute these higher resolution products.\n\nThe primary payload of Proba-V is the Vegetation instrument, built by OIP Sensor Systems. This is a reduced-mass version of the Vegetation instrument which was on board the SPOT-4 and -5 satellites to provide a daily overview of global vegetation growth. Traditional Vegetation products generated by these instruments include the 1-day Synthesis products and the 10-day Synthesis products, both with a ground resolution of about 1 km (1 km x 1 km pixel size). Despite the fact that the Vegetation instrument onboard PROBA-V has a higher spatial resolution (smaller groundpixels) than the Vegetation instruments on board the SPOT satellites, the long time series (15 years) of the traditional Vegetation products will be continued by PROBA-V. Thus, PROBA-V will also generate the traditional Vegetation products at approximately 1 km x 1 km ground resolution. The spectral bands (see Electromagnetic spectrum) are nearly identical to the spectral bands of the SPOT Vegetation instruments. Other characteristics of the PROBA-V Vegetation instrument are :\n\nFor more detailed specifications and the resulting products that are available to the users, see \n\nThe other, secondary, onboard instruments are :\n\nThese secondary instruments are technology demonstrators.\n\nPROBA-V was launched from ELA-1 at Guiana Space Centre on board Vega flight VV02, on together with the Vietnamese VNREDSat 1A satellite, and Estonia's first satellite, ESTCube-1. The launch will mark the first test of the new Vespa dual-payload adapter; \"PROBA-V\" will ride in the upper position of the Vespa adapter, and VNREDSat 1A will sit in the lower position.\nThe usable lifetime of PROBA-V highly depends on the local time of the descending node LTDN. Given that PROBA-V has no onboard propulsion, the natural drift of this LTDN depends on the satellites' in orbit injection accuracy. Based on the injection accuracy specifications of the VEGA launcher, a usable lifetime between 2.5 and 5 years was predicted. The achieved in orbit injection accuracy is such that the LTDN will be out of the specifications after 5 years.\n\nThe data policy for the traditional Vegetation products, as provided by the SPOT-Vegetation instruments, was not freely accessible for all users, meaning that for some products, the user had to pay a fee. Only the products older than 3 months were for free for everybody and were delivered on a best effort basis. The data policy of the traditional Vegetation products, as provided by PROBA-V, will be freely accessible for all users. This, so called full, free and open data policy, was approved by the Programme Board for Earth Observation (PBEO) of ESA on 25/9/2013.\nThe new, higher resolution products of PROBA-V that are older than 1 month, have the same full, free and open data policy. Depending on the kind of user (scientific, commercial, ...) and the kind of higher resolution product (customised or not, guaranteed in time delivery or not, ...) a fee has to be paid for certain other, higher resolution products of PROBA-V. The complete data policy details can be found on the PROBA-V data policy\n\n\n"}
{"id": "6063928", "url": "https://en.wikipedia.org/wiki?curid=6063928", "title": "Pulse (interbank network)", "text": "Pulse (interbank network)\n\nPulse is an interbank electronic funds transfer (EFT) network in the United States. It serves more than 4,400 U.S. financial institutions and includes more than 380,000 ATMs, as well as POS terminals nationwide. Rivals of the network include First Data's STAR and Fidelity National Information Services's NYCE. It is owned by Discover Financial, issuer of the Discover Card, and is included in Discover's agreement with China UnionPay; cards can be used on each other's network leading to better acceptance outside large cities than the larger networks.\n\nThe Pulse system was based on software that operated the Take Your Money Everywhere (TYME) network operating in the central United States. The network was established as the banking rules that limited banks and branches ability to share services were removed. The data processing facilities were originally provided by First City Bank and later transitioned to Texas Commerce Bank.\n\n\n"}
{"id": "36213664", "url": "https://en.wikipedia.org/wiki?curid=36213664", "title": "Quitsato Sundial", "text": "Quitsato Sundial\n\nThe Quitsato Sundial is a cultural-tourist place located at La Mitad Del Mundo, near to Cayambe, 47 km north of Quito. It was built in 2006 and inaugurated in 2007 as an independent, non-profit project in a 24,756 ft² (2300 m²) area. Its main goal is to share crucial aspects of the astronomical knowledge of the prehispanic cultures of the region. The expositions are carried out by community members as a self-sustaining project. According to Google Maps, the central pillar is 4 meters south of the true equator.\n\nIt consists of a circular platform of 177.64 ft (54 m) in diameter which forms a mosaic of light and dark pebbles drawing an eight-pointed star that indicates the solstices and equinoxes, plus intermediate lines pointing to the cardinal directions. In the center of this platform there is a 10 m (32.80 ft) high, 1.30 m (4.27 ft) diameter cylindrical orange tube which serves as a gnomon, pointing to the corresponding hours and months of the year in the platform according to the shadow cast by the Sun. The ten meter high gnomon represents the metric system, based in the meter, which in its origin was intended to equal one ten-millionth of the quadrant of the Earth's circumference. The purpose of the color difference between the stones, apart from showing equinox and solstice lines, is to explain the meaning of albedo and its use in astronomical study. The Equator line is drawn by using smaller, darker pebbles between two metal plates.\n\nThe angles that form the geometric design of the eight-pointed star are given by the tilt of the Earth with respect to the ecliptic of the Earth, thus the platform itself also presents a reading of the celestial mechanics. Detailed positions of the solstices and equinoxes, as well as their respective axes, are presented.\n\nThe Ecuadorian Military Geographic Institute has placed two cylinders surrounded by concrete on a platform on top of the Equatorial line, with a 1 mm error margin determined by using GPS and GNSS equipment.\n\n"}
{"id": "26715490", "url": "https://en.wikipedia.org/wiki?curid=26715490", "title": "Refmex GL Glass", "text": "Refmex GL Glass\n\nRefmex GL Glass is a Mexican manufacturer of high-quality industrial glass products. Its main markets are borosilicate glass, f-silicate glass, and industrial quartz glass.\n\nThe company was founded in 1977 at Zapopan, Mexico as Refractarios Mexicanos SA de CV . The company later changed its name to Refmex GL SA de CV. It was founded related to the crisis Mexico suffered in 1976 to help the nation out of it creating employment for local people in re-selling glass, later, in 1985, they started producing their own glass for windows, the business was not very successful and they began the borosilicate glass business in Mexico.\n\nThe company employs 125 employees worldwide, according to its 2011 reports.\n\nThe company currently has four plants, two of them in Tesistan, Mexico, and two located in Zapopan, a nearby territory. The company announced plans for a fifth plant in 2013, but it has not been approved yet.\n\n"}
{"id": "34262770", "url": "https://en.wikipedia.org/wiki?curid=34262770", "title": "San Juan-Chama Project", "text": "San Juan-Chama Project\n\nThe San Juan-Chama Project is a U.S. Bureau of Reclamation interbasin water transfer project located in the states of New Mexico and Colorado in the United States. The project consists of a series of tunnels and diversions that take water from the drainage basin of the San Juan River – a tributary of the Colorado River – to supplement water resources in the Rio Grande watershed. The project furnishes water for irrigation and municipal water supply to cities along the Rio Grande including Albuquerque and Santa Fe.\n\nMost major agricultural and urban areas in New Mexico today lie along the narrow corridor of the Rio Grande as it cuts across the center of this predominantly desert state. Spanish settlers arrived in the area in the late 1500s, followed by Mexican and American settlers in the 18th and 19th centuries, building large irrigation systems and diversion dams to allow agricultural production in the arid region. In the early 1920s, water supply in the Rio Grande basin was already severely stressed, and studies were conducted as to the feasibility of procuring additional water by transbasin diversion from tributaries of the San Juan River. \n\nThe 1933-1934 Bunger Survey studied potential locations for diversions and storage reservoirs, and in 1939, the Rio Grande Compact was signed, dividing Rio Grande waters between Colorado, New Mexico and Texas including allocations from a potential future diversion from the San Juan basin. When the Upper Colorado River Basin Compact was established in 1948, it also included provisions for the tentative diversion project under its water allotment to New Mexico. In the 1950s, post-World War II population growth in central New Mexico put even larger strains on the Rio Grande's water, and the need for a transbasin water project rose because water supplies in the area quickly became overallocated.\n\nStudies for the project continued through the early 1950s, but actual implementation languished until 1962 when Congress amended the Colorado River Storage Act of 1956, allowing the diversion of part of New Mexico's share of Colorado River basin waters into the Rio Grande basin. The diversions proposed were for per year from three tributaries of the San Juan River in Colorado: the Rio Blanco, Navajo and Little Navajo Rivers, to the headwaters of the Rio Chama, a major tributary of the Rio Grande. The project would be constructed in two phases. However, Reclamation ran into difficulties because the Navajo Nation asserted rights to about of water from the San Juan River, which runs through their traditional lands. Resultantly, only the first phase of the project was ever constructed, delivering just under 47% of the original amount proposed by Reclamation.\n\nOn December 19, 1964, construction began on the Azotea Tunnel, the main water tunnel for the project, running from the Navajo River south to Azotea Creek in the Rio Chama watershed. Work started on the Oso and Little Oso tunnels in February 1966, and construction on the Blanco Tunnel began in March of the same year. In 1967, an enlargement of the outlets of existing El Vado Dam to accommodate increased flows from the diversion project was completed, and construction began on Heron Dam, which would impound the project's main storage reservoir. Azotea Tunnel was holed through and construction was finished on the project's three diversion dams in 1970. Heron Dam was completed the next year. Nambe Falls Dam, completed in 1976, was the last part of the project to be built. The dam was the only one built of a series of small independent irrigation units originally proposed under the project to serve Native American lands. In 1978, Reclamation announced the completion of the San Juan-Chama Project.\n\nThe San Juan-Chama Project taps the water of the Rio Blanco, Navajo and Little Navajo Rivers via a series of small diversion dams, tunnels and siphons. Blanco Diversion Dam, with a diversion capacity of , sends water into the Blanco Feeder Conduit, which connects to the -long Blanco Tunnel and flows south towards the Little Navajo River. The water passes underneath the river via the Little Oso Siphon and connects to the Oso Tunnel. Just upstream from the siphon, Little Oso Diversion Dam sends up to of water through the Little Oso Feeder Conduit, which also empties into the Oso Tunnel.\n\nOso Tunnel, with a capacity of , travels south to the Navajo River, which it passes under via the Oso Siphon. Oso Diversion Dam on the Navajo diverts additional water into the Oso Feeder Conduit, which joins with water from the Oso Tunnel and Siphon to form the Azotea Tunnel. The Azotea Tunnel, which has a capacity of , runs south for , passing under the Continental Divide. The tunnel terminates at Azotea Creek, a tributary of Willow Creek, which is in turn a tributary of the Rio Chama. The lower portion of Azotea Creek has been channelized to mitigate erosion from the higher flows.\n\nThe main storage facility for the project is Heron Lake, a reservoir formed by Heron Dam on Willow Creek about downstream of the terminus of Azotea Tunnel and southwest of Chama, New Mexico. The reservoir has a capacity of and has a surface area of . Heron Dam is an earthfill dam high and long, standing above the streambed. Heron Lake receives water from a catchment of , which has been augmented to over three times this size by the San Juan-Chama diversions.\n\nNambe Falls Dam is located about north of Santa Fe on the Rio Nambe, a tributary of the Rio Grande. The dam and reservoir are functionally independent from the other facilities of the San Juan-Chama Project. The curved earthfill dam forms Nambe Falls Lake, which has a capacity of and controls runoff from a catchment of . Its main purpose is to provide irrigation water for about in the Pojoaque Valley, which is situated west and downstream of the dam.\n\nEach year, a minimum of of San Juan-Chama water is allocated as follows. Because annual diversions average about , there is usually a surplus available for other uses along the river. About 75% of the water serves municipal and industrial uses; the remaining fourth furnishes irrigation supplies to approximately of land along the Rio Grande and Rio Nambe. Surplus water is also used to maintain a permanent pool at the Cochiti Lake flood-control reservoir on the Rio Grande.\n\n"}
{"id": "58206657", "url": "https://en.wikipedia.org/wiki?curid=58206657", "title": "Self-sealing suction cup", "text": "Self-sealing suction cup\n\nThe self-sealing suction cup is a suction cup that exerts a suction force only when it is in physical contact with an object. Unlike most other suction cups, it does not exert any suction force when it is not in contact with an object. Its grasping ability is achieved entirely through passive means without the use of sensors, valves, or actuators.\n\nIt was designed so that, when used as part of a suction cup array, the suction cups that don’t come in contact with the object remain sealed. By having only the suction cups that are in direct contact of the object to exhibit suction force, the researchers were able to minimize leak points where air could enter and increase the pressure that each active cup receives, maximizing the suction force. As a result, an array of self-sealing suction cups can grasp and pick up a wide range of object sizes and shapes. This comes in contrast to conventional suction cups that are typically designed for one specific object size and geometry. In addition, suction cups of various sizes have been manufactured, ranging from the palm of a hand to the point of a fingertip.\n\nThe self-sealing suction cup was first developed in 2010 by a collaboration of researchers from the U.S. Army Research Laboratory (ARL), the Edgewood Chemical Biological Center at Aberdeen Proving Ground, and the University of Maryland.\n\nThe design of the self-sealing suction cup was initially inspired by the suckers of the octopus and its ability to pick up different sized items by individually actuating its suction cups based on the item’s size and physical features.\n\nThe internal geometry of the self-sealing suction cup was designed to the smallest possible size and features a minimum wall thickness of 1.02 mm, a tube diameter of 1.59 mm, and minimum part spacing of 0.13 mm. The suction cup incorporates a mix of rubber and plastic components, where the cup lip, base, tube, springs, and plug are made out of soft rubber while the cup side, collar, hinges, and flange are made out of plastic. As part of its design, a central vacuum pump can be used to maximize the suction force of the suction cup. A multi-material 3D printer was used to create the prototype of the self-sealing suction cup in about 20 minutes.\n\nInside the self-sealing suction cup, the plug is positioned close to the tube opening so that it can get sucked into the tube seal the hole when the central suction line is powered. A pair of springs connected to the suction cup’s base helps maintain the plug’s position, restoring the plug seal in the absence of object forces. If the cup makes contact with an object, a hinge action raises the plug away from the suction tube. The moment the cup’s lips are pushed against the object, the passive reaction forces from the cup lips are transferred to the rubber base of the cup, which stretches over the collar and allow the structure to compress. Acting as a pivot for the hinges, the collar causes the hinges to rotate and the edges of the hinges slide along the underside of the flange and raise the plug away from the suction tube opening. As a result, the suction cup self-seals when not in contact with an object and self-opens the cup’s lips makes contacts with an object.\n\nIn 2015, several improvements were made to the design of the self-sealing suction cup to improve its grasping capabilities. The previous design demonstrated the following flaws:\n\n\nTo address these flaws, researchers from ARL decreased the number of components by consolidating the functions of several parts, which reduced the uncompressed height of the suction cup by almost 50% to 0.72 cm. The cup diameter was also reduced to 1.07 cm. A lever system was added to the base of the cup, which pivots the collar to lift the plug. In addition, the tube doubles as a spring, which helps restore the levers and the plug to their closed position. A plastic restraint was added around the cup to aid with handling the hyper-extension, shear, and torsional forces.\n\nThe self-sealing suction cup has been subjected to a series of tests to determine the quality of its performance. A flexible test rig with four dime-sized suction cups and plastic ribs connected with rubber tubes was created for force-displacement and testing.\n\nA force-displacement test that compared the performance between the self-sealing suction cup, an identical suction cup, and a commercially available suction cup found that the internal structures of the self-sealing cup allowed more force to be exerted for the same displacement compared to the other cups. However, under identical conditions, the self-sealing cup achieved a maximum force of 12.5 N while the commercially available cup achieved a maximum force of 12.9 N.\n\nA seal quality test measured the pressure generated from each self-sealing suction cup. The results showed that an array of four cups maintained a pressure of 93.8% atmospheric. The test also demonstrated that not all the cups were equally efficient at sealing after object contact. However, this could be the result of variation in the cups’ prior usage.\n\nDuring object grasping testing where the grasping range was examined, the test rig successfully grasped about 80% of the objects attempted. These items consisted of the following: TV remote, pill bottle, glue stick, eyeglasses, fork, disposable bottle, toothpaste, coffee mug, bowl, plate, book, cell phone, bar of soap, paper money, mail, keys, show, table knife, medicine box, credit card, coin, pillow, hairbrush, non-disposable bottle, wallet, magazine, soda can, newspaper, scissors, wrist watch, purse, lighter, compact disc, telephone receiver, full wine bottle, full wine glass, light bulb, lock, padded volleyball, wooden block. (4) As a demonstration of the cups’ strength, the ARL researchers were able to pick up a full bottle of wine using only four of the dime-sized suction cups.\n\nThe self-sealing suction cups have been incorporated in robots to improve their passive grasping capabilities. Due to the design of the suction cups, a central vacuum source can be used to effectively generate suction force from the cups and reduce the number of actuators and sensors for the robot.\n\nResearchers from ARL designed and developed a three-finger hand actuator system using a 3D printer in order for the robot to properly utilize the self-sealing suction cups. Four suction cups run along the bottom of each finger, which contains a narrow vacuum channel running through the center. A central vacuum pump serves to power the suction cups and facilitate grasping. The fingers can also curl around the object to better grasp it and release any object in its hold by feeding back the output of the vacuum pump and emitting a burst of positive pressure.\n\nThe three-finger hand has been used by aerial systems and has demonstrated considerable success in grasping objects on the ground while maintaining flight. According to ARL researchers, the self-sealing suction cups may exhibit higher rates of success underwater due to the extra pressure from the sea depths surrounding and pressing against the object and grasper. However, they noted that an underwater environment would require different manufacturing materials that would allow the suction cups to perform well in salt water, such as a thermal plastic.\n"}
{"id": "26848475", "url": "https://en.wikipedia.org/wiki?curid=26848475", "title": "Set-back box", "text": "Set-back box\n\nThe term set-back box (SBB) is used in the digital TV industry to describe a piece of consumer hardware that enables them to access both linear broadcast and internet-based video content, plus a range of interactive services like Electronic Programme Guides (EPG), Pay Per View (PPV) and Video on Demand (VOD) as well as internet browsing, and view them on a large screen television set. Unlike standard set-top boxes (STBs), which sit on top or below the TV set, a set-back box has a smaller form factor to enable it to be mounted to the rear of the display panel flat panel TV, hiding it from view. \n\nTo date, set-back boxes have been mainly focused on the cable industry, having been rolled out in four major cable markets in the United States. As of February 2010, these devices are available in both standard definition (SD) and high definition (HD) versions, provide a DOCSIS 2.0 high speed return channel, and are able to receive transmissions in all industry standard compression formats, including MPEG-2, MPEG-4/H.264 and SMPTE-421M/VC-. This enables broadcasters to maximise their broadcast bandwidth while creating a new consumer TV experience.\n\nIn October 2009 the ADB-4820C set-back box was voted the TV Innovation of the Year, by a panel of independent industry experts, overseen by IMS Research at the TV 3.0 Conference in Denver, Colorado, United States of America. The ADB set-back box was the first to use the latest HDMI-CEC technology, enabling a single remote control to be used for both the TV and set-back box and is tru2way compliant.\n\n"}
{"id": "8435121", "url": "https://en.wikipedia.org/wiki?curid=8435121", "title": "Shere FASTticket", "text": "Shere FASTticket\n\nThe Atos Worldline FASTticket system is a passenger-operated, self-service railway ticket issuing system, developed by the Guildford-based company Shere Ltd and first introduced on a trial basis in Britain in 1996, shortly after privatisation. It has been developed and upgraded consistently since then, and is now used by seven \nTrain Operating Companies (TOCs) as their primary self-service ticket issuing system. Other TOCs have FASTticket machines at some of their stations, sometimes supplementing other systems.\n\nIn the last years of British Rail, before privatisation, the main passenger-operated ticket issuing system (POTIS) on the network was the \"Quickfare\" B8050, developed in the late 1980s by Swiss company Ascom Autelca AG. These machines were geared towards high-volume, low-value transactions: they only accepted cash, offered a small and mostly unchanging range of destinations, and were a minor evolution from similar earlier machines whose computer technology was based in the early 1980s. Quickfares were widespread, especially in the erstwhile Network SouthEast area, but their limitations were increasing as technology became more sophisticated.\n\nShere Ltd, founded in its present form in 1992, initially specialised in self-service ticket sales/collection and check-in systems for airlines (notably the former British Midland and KLM UK). In the first instance, the FASTticket system was developed directly from these, with early FASTticket terminals resembling their airport equivalents in many respects. Only a small range of tickets were available, for example - mostly higher-value tickets to important destinations such as London; only debit and credit cards were accepted; touch-screen functionality was offered, but there were limited options and sub-menus; and some of the early machines only printed ATB-style tickets (Automated Ticket and Boarding Pass - an international standard format used by airlines, coaches, railways, ferries and other transport undertakings), which are large and inconvenient for passengers to carry, in comparison with standard credit-card-sized tickets.\n\nAs more TOCs showed an interest in the system, the hardware and software were developed further, and machines were universally provided with printers able to vend credit-card-sized tickets (although receipts, card sales vouchers and seat reservations were sometimes still printed by a separate printer within the same machine, on glossy flexible paper cut from a roll - batch reference RSP 3598/3: Example).\n\nBefore the now standard \"Common Stock\" layout and format was devised in 2003, credit-card-sized travel tickets were printed on either RSP 3598 or RSP 7599/SCT orange-banded, round-cornered, hopper-fed ticket stock with pre-printed headings. From September 2003, machines began to be converted to the Common Stock format, (printed by the Newbury Data ND4020 ticket printer) with the standard RSP 9599 stock (with no pre-printed headings) being used. The first machine to be converted in this way was at Didcot Parkway. Newly installed machines used the Common Stock format as from 2004, and almost all machines have now been converted to do so (as of 2006).\n\nVarious TOCs installed machines on trial, including the following (\"TOC names shown in the table are those current at the time of installation\"):\n\nThe following table shows the locations and dates of installations as of September 2006.\n\nSilverlink: In the autumn of 2007, Shere Fasticket machines were installed at stations on the Barking & Gospel Oak line. These offer a typical National Rail ticket selection, with no hint that Oyster PAYG will be valid on the line from 11 November 2007 when TfL take it over. At the time of writing (October 2007) the home screen suggests that prepaid (i.e. TOD) tickets can be printed on these machines, though the on screen buttons to do so aren't presented. The machines are able to take cash, but are (at the time of writing) payment card only, pending cash collection (i.e. emptying) arrangements being put in place. Oystercard validators have been installed at B&GO (including on platforms) ready for 11 November, and the ticket machines have a circular blanking plate, perhaps for an Oyster reader/writer.\n\n\nOnline screen interface demonstration for Southern Railway customers *\n"}
{"id": "27208573", "url": "https://en.wikipedia.org/wiki?curid=27208573", "title": "Slik Corporation", "text": "Slik Corporation\n\nSLIK Corporation is a manufacturer and brand of camera tripods with headquarters located in Hidaka City, Japan.\n\nThe company was founded in 1956 as SLICK Elevator Tripod Co., Ltd. by a mechanical engineer and photography enthusiast, Takatoshi Shiraishi. Shiraishi started designing and building his own tripods in a suburb of Tokyo, Japan, in 1948. In 1963, the company name was changed to SLICK Tripod Co., Ltd. In 1967 a factory was constructed in Saitama Prefecture, Japan. In 1988, SLIK (Thailand) Co., Ltd. was established with a factory located in a Bangkok, Thailand suburb. In 1989, sales and manufacturing operations were consolidated and the company was renamed SLIK Corporation.\n\n"}
{"id": "946226", "url": "https://en.wikipedia.org/wiki?curid=946226", "title": "SmarterChild", "text": "SmarterChild\n\nSmarterChild was a chatbot available on AOL Instant Messenger and Windows Live Messenger (previously MSN Messenger) networks.\n\nSmarterChild was an intelligent agent or bot developed by ActiveBuddy, Inc., with offices in New York and Sunnyvale. It was widely distributed across global instant messaging and SMS networks.\n\nFounded in 2000, ActiveBuddy was the brainchild of Robert Hoffer, Timothy Kay and Peter Levitan. The idea for instant messaging bots came from the team's vision to add natural language comprehension functionality to the increasingly popular instant messaging and SMS platforms. The original implementation took shape as a word-based adventure game but quickly grew to include a wide range of database applications including instant access to news, weather, stock information, movie times, yellow pages listings, and detailed sports data, as well as a variety of tools (personal assistant, calculators, translator, etc.). The company had not launched a public bot until the arrival of the eventual new-CEO, Stephen Klein. Shortly after he arrived at the company in May 2001, he insisted that all of the knowledge domains (sports, weather, movies, etc.) plus the chat functionality be bundled together and launched under the screen name \"SmarterChild\" which was one of the many test bots that were being run internally (the screen name \"SmarterChild\" was one of Timothy Kay's personal test bots). The bundled domains were launched publicly as SmarterChild (on AOL Instant Messenger initially) in June 2001. SmarterChild acted as a showcase for the quick data access and possibilities for fun personalized conversation that the company planned to turn into customized, niche specific products.\n\nThe rapid success of SmarterChild led to targeted marketing-oriented bots for Radiohead, Austin Powers, Intel, Keebler, \"The Sporting News\" and others. ActiveBuddy strengthened its hold on the intelligent agent market by receiving a U.S. patent in 2002.\n\nActiveBuddy changed its name to Colloquis and prospered selling a superior automated customer service SAS offering to large companies (including Comcast, Time Warner, Cingular and Vonage among others). Microsoft acquired Colloquis in 2007 and proceeded to decommission SmarterChild and discontinue the Automated Service Agent business as well.\n"}
{"id": "177948", "url": "https://en.wikipedia.org/wiki?curid=177948", "title": "Snowy Mountains Scheme", "text": "Snowy Mountains Scheme\n\nThe Snowy Mountains scheme or Snowy scheme is a hydroelectricity and irrigation complex in south-east Australia. The Scheme consists of sixteen major dams; seven power stations; one pumping station; and of tunnels, pipelines and aqueducts that were constructed between 1949 and 1974. The Scheme was completed under the supervision of Chief Engineer, Sir William Hudson and is the largest engineering project undertaken in Australia.\n\nThe water of the Snowy River and some of its tributaries, much of which formerly flowed southeast onto the river flats of East Gippsland, and into Bass Strait of the Tasman sea, is captured at high elevations and diverted inland to the Murray and Murrumbidgee Rivers irrigation areas, through two major tunnel systems driven through the Continental Divide of the Snowy Mountains, known in Australia as the Great Dividing Range. The water falls and travels through large hydro-electric power stations which generate peak-load power for the Australian Capital Territory, New South Wales and Victoria.\n\nIn 2016, the Snowy Mountains Scheme was added to the Australian National Heritage List.\n\nSince the 1830s, both the Murray and Murrumbidgee rivers have been subject to development and control to meet water supply and irrigation needs. By contrast, the Snowy River, that rises in the Australian Alps and flows through mountainous and practically uninhabited country until debouching onto the river flats of East Gippsland, had never been controlled in any way, either for the production of power or for irrigation, and a great proportion of its waters flowed into the sea. The Snowy River has the highest source of any in Australia and draws away a large proportion of the waters from the south-eastern New South Wales snowfields, and was considered a means of supplementing the flow of the great inland rivers, a means for developing hydro-electric power, also a source of increasing agricultural production in the Murray and Murrumbidgee valleys.\n\nFollowing World War II, the Government of New South Wales proposed that the flow of the Snowy River be diverted into the Murrumbidgee River for irrigation and agricultural purposes; however there was little emphasis placed on the generation of power. A counter proposal by the Government of Victoria involved a greater generation of power, and involved diversion of the Snowy River to the Murray River. Additionally, the Government of South Australia was concerned that downstream flows on the Murray River would be severely jeopardised.\n\nThe Commonwealth Government, looking at the national implications of the two proposals, initiated a meeting to discuss the use of the waters of the Snowy River, and a Committee was set up in 1946 to examine the question on the broadest possible basis. This Committee, in a report submitted in November 1948, suggested consideration of a far greater scheme than any previously put forward. It involved not only the simple question of use of the waters of the Snowy River, but consideration of the possible diversion of a number of rivers in the area, tributaries, not only of the Snowy, but of the Murray and Murrumbidgee. The recommendations of the Committee were generally agreed to by a conference of Ministers representing the Commonwealth, New South Wales, and Victoria, and it was also agreed that the Committee should continue its investigations.\n\nHowever, limitations in the Australian Constitution meant that the Commonwealth Government was limited in the powers it could exercise, without the agreement of the States. Subsequently, the Commonwealth Government introduced legislation into the Federal Parliament under its defence power; and enacted the that enabled the formation of the Snowy Mountains Hydroelectric Authority. Ten years later, the relevant States and Territories introduced their own corresponding legislation and in January 1959 the Snowy Mountains Agreement was reached between the Commonwealth and the States.\n\nThe legislation created the Snowy Mountains Hydroelectric Authority that was given responsibility for the final evaluation, design and construction of the Snowy Mountains Scheme. The final agreed plan was to divert the waters of the Snowy Mountains region to provide increased electricity generating capacity and to provide irrigation water for the dry west. It was \"greeted with enthusiasm by the people of Australia\" and was seen to be \"a milestone towards full national development\".\n\nThe chief engineer, New Zealand-born William Hudson (knighted 1955), was chosen to head the scheme as Chairman of the Snowy Mountains Hydroelectric Authority, and was instructed to seek workers from overseas. Hudson's employment of workers from 32 (mostly European) countries, many of whom had been at war with each other only a few years earlier, had a significant effect on the cultural mix of Australia.\n\nConstruction of the Snowy Scheme was managed by the Snowy Mountains Hydroelectric Authority, it officially began on 17 October 1949 and took 25 years, officially completed in 1974.\n\nAn agreement between the United States Bureau of Reclamation and Snowy Mountains Hydro to provide technical assistance and training of engineers was agreed between the USA and Australia in Washington, D.C. on 16 November 1951. A loan for $100 Million was obtained from the International Bank for Reconstruction and Development in 1962.\n\nTunneling records were set in the construction of the Scheme and it was completed on time and on budget in 1974, at a cost of 820 million; a dollar value equivalent in 1999 and 2004 to A$6 billion. Around two thirds of the workforce employed in the construction of the scheme were immigrant workers, originating from over thirty countries. The official death toll of workers on the Scheme stands at 121 people. Some of roads and tracks were constructed, seven townships and over 100 camps were built to enable construction of the 16 major dams, seven hydroelectric power stations, two pumping stations, of tunnel and of pipelines and aqueducts. Just 2% of the construction work is visible from above ground.\n\nTwo of the towns constructed for the scheme are now permanent; Cabramurra, the highest town in Australia; and Khancoban. Cooma flourished during construction of the Scheme and remains the headquarters of the operating company of the Scheme. Townships at Adaminaby, Jindabyne and Talbingo were inundated by the flooded waters from Lake Eucumbene, Lake Jindabyne and Jounama Reservoir. Improved vehicular access to the high country enabled ski-resort villages to be constructed at Thredbo and Guthega in the 1950s by former Snowy Scheme workers who realised the potential for expansion of the Australian ski industry.\n\nThe Scheme is in an area of , almost entirely within the Kosciuszko National Park. The design of the scheme was modelled on the Tennessee Valley Authority. Over 100,000 people from over 30 countries were employed during its construction, providing employment for many recently arrived immigrants, and was important in Australia's post-war economic and social development. Seventy percent of all the workers were migrants. During construction of the tunnels, a number of railways were employed to convey spoil from worksites and to deliver personnel, concrete and equipment throughout.\n\nThe project used Australia's first transistorised computer; one of the first in the world. Called 'Snowcom', the computer was used from 1960 to 1967.\n\nAt the completion of the project, the Australian Government maintained much of the diverse workforce and established the Snowy Mountains Engineering Corporation (SMEC), which is now an international engineering consultancy company. The Scheme is the largest renewable energy generator in mainland Australia and plays an important role in the operation of the National Electricity Market, generating approximately 67% of all renewable energy in the mainland National Electricity Market. The Snowy Scheme's primary function is as a water manager, however under the corporatised model must deliver dollar dividends to the three shareholder governments - the NSW, Commonwealth and Victorian Governments.\n\nThe Scheme also has a significant role in providing security of water flows to the Murray-Darling Basin. The Scheme provides approximately of water a year to the Basin, providing additional water for an irrigated agriculture industry worth about A$3 bn per annum, representing more than 40% of the gross value of the nation's agricultural production.\n\nThe Snowy Mountains Hydro-electric Scheme, is one of the most complex integrated water and hydro-electric power schemes in the world and is listed as a \"world-class civil engineering project\" by the American Society of Civil Engineers. The scheme interlocks seven power stations and 16 major dams through of trans-mountain tunnels and of aqueducts. The history of the Snowy Scheme reveals its important role in building post World War II Australia.\n\nSir William Hudson was appointed the first commissioner of the Snowy Mountains Hydroelectric Authority, serving between 1949 and 1967. The Commissioner's role was the overall management of the Scheme. He represented the Scheme at the highest levels of government, welcomed international scientists and engineers, encouraged scientific and engineering research, as well as attending many social and civic activities. Sir William's management style 'stressed cooperation between management and labour and scientific knowledge (facts) over opinion'.\n\nThe Scheme was completed with the official opening of the Tumut 3 Power Station project by the Governor-General of Australia, Sir Paul Hasluck on 21 October 1972.\n\nVarious stories and memoirs have been written about work on the Snowy Mountains Scheme. Most recently, Snowy Hydro, Woden Community Service, Gen S Stories and PhotoAccess partnered for a Digital Storytelling project to present a diverse collection of stories told from the point of view of seven ex-workers, two lifelong employees and a child of a Snowy worker. \n\nAs part of the project, participants created a short film about their experience on the Snowy Scheme, each story offering a unique perspective into what life was like building the Scheme between 1949 and 1974. The project's artistic director Jenni Savigny assisted participants to make the short films; enabling them to put together the scripts, record voice overs and edit the short films. In an interview with Andrew Brown (The Canberra Times), Savigny said it was important to create a history of the Snowy Hydro using the participant's own words,\"You just get a personal sense of what it was like to be there, and what it meant to people's lives.\" \n\nThe films premiered 7th of June 2018 at the Palace Electric Cinema in New Acton and can be viewed on the Woden Community Service YouTube Channel.\n\nThe Scheme is operated by Snowy Hydro Limited, an unlisted public company incorporated pursuant to the , owned by the Governments of New South Wales (58%), Victoria (29%) and Australia (13%).\n\nThe original plan was for 99% of the water of the Snowy River's natural flow to be diverted by the Scheme below Lake Jindabyne. Releases from the Scheme were based on the needs of only riparian users and took no account of ecosystem needs; it soon became known that the lower reaches of the river were in environmental crisis. An extensive public campaign led to the Snowy Water Inquiry being established in January 1998. The Inquiry reported to the New South Wales and Victorian Governments in October of that year, recommending an increase to 15% of natural flows. The two Governments were equivocal about this target; aside from economic considerations there was a view that the health of the Murray is more important than that of the Snowy and any extra environmental flows are better used there instead.\n\nIn the 1999 Victorian state election, the seat of Gippsland East was won by Craig Ingram, an independent and member of the Snowy River Alliance, based in large part on his campaign to improve Snowy flows. In 2000, Victoria and NSW agreed to a long-term target of 28%, requiring A$375 million of investment to offset losses to inland irrigators. In August 2002 flows were increased to 6%, with a target of 21% within 10 years. However, by October 2008 it was evident that the return of environmental flows to the Snowy River in 2009 would be no more than 4% of natural flow with governments arguing the Snowy River needs to \"pay back\" the \"Mowamba Borrowings\". At the 2010 state election, Ingram lost the seat of Gippsland East to the Nationals.\n\nIn 2017, it was announced that the 21% target would be reached for the first time.\n\nSome concerned water managers, conservationists, politicians and farmers continue to advocate for the return of environmental flows to the Snowy River. The Dalgety District and Community Association commemorates Snowy River Day annually, towards the end of August, to mark the 2002 anniversary of when the governments of Victoria, NSW and the Commonwealth first released water into the Snowy River over the Mowamba Weir.\n\nIn accordance with the Snowy Water Licence, Snowy Hydro Limited has 're-commissioned' the Mowamba Aqueduct. Seasonal variable flows are essential to river ecology including flushing flows to support vital ecosystems for the Australian platypus and native Australian Bass, the species over which Ingram initially fought for flows into the Snowy River. A major spillway upgrade now facilitates these flows.\n\nConstruction of the Scheme began in 1949 and was completed in 1974. Guthega power station commenced power production on 21 February 1955.\n\nThe total installed capacity is .\n\nThe Scheme's largest Dam is Talbingo Dam with an embankment volume of 14 488 000 m and a wall height of 161.5 meters. Khancoban Dam is the longest dam in the scheme with a crest length of . A variety of dam and spillway types were used in the construction.\n\nWith a capacity of , Lake Eucumbene is the largest reservoir in the Scheme. At the other end of the scale, Deep Creek Reservoir is the smallest reservoir with just . \n\nThe Snowy Mountains Scheme has two pumping stations. The Jindabyne Pumping Station pumps water from Lake Jindabyne through to the Snowy-Geehi Tunnel at Island Bend.\n\nThe pump storage facility at Tumut 3 Power Station returns water to Talbingo Reservoir.\n\nIn March 2017, the Australian government suggested a $2 billion project expanding the 4.1 GW Snowy Mountains Scheme by 2 GW of pump storage for a week, building new tunnels and power stations, but no new dams. The 80% efficiency of such storage can be sufficient in leveling differences between supply and demand.\n\nThe Snowy Scheme is a major tourist destination. Sightseeing driving tours to the key locations of the Scheme are popular out of regional centres like Cooma, Adaminaby and Jindabyne along roads built for the Scheme like the Snowy Mountains Highway and Alpine Way and towards sights like Cabramurra, as Australia's highest town, spectacular dam walls, and scenic lakes. Trout fishing is popular in the lakes of the Scheme, notably Lake Jindabyne and Lake Eucumbene.\n\nThe Snowy Scheme Museum opened at Adaminaby in 2011 to profile the history of the Scheme.\n\nThough skiing in Australia began in the northern Snowy Mountains in the 1860s, it was the construction of the vast Snowy Scheme from 1949, with its improvements to infrastructure and influx of experienced European skiers among the workers on the Scheme, that really opened up the mountains for the large scale development of a ski industry, and led to the establishment of Thredbo and Perisher as leading Australian resorts. The construction of Guthega Dam brought skiers to the isolated Guthega district and a rope tow was installed there in 1957. Charles Anton, a snowy worker identified the potential of the Thredbo Valley.\n\n\n\n"}
{"id": "3647754", "url": "https://en.wikipedia.org/wiki?curid=3647754", "title": "Structural system", "text": "Structural system\n\nThe term structural system or structural frame in structural engineering refers to the load-resisting sub-system of a building or object. The structural system transfers loads through interconnected elements or members.\n\nCommonly used structures can be classified into five major categories, depending on the type of primary stress that may arise in the members of the structures under major design loads. However any two or more of the basic structural types described in the following may be combined in a single structure, such as a building or a bridge in order to meet the structure's functional requirements.\n\nThe structural system of a high-rise building is designed to cope with vertical gravity loads as well as lateral loads caused by wind or seismic activity. The structural system consists only of the members designed to carry the loads, all other members are referred to as non-structural.\n\nA classification for the structural system of a high-rise was introduced in 1969 by Fazlur Khan and was extended to incorporate interior and exterior structures. The primary lateral load-resisting system defines if a structural system is an interior or exterior one. The following interior structures are possible:\n\nThe following exterior structures are possible:\n\n\n"}
{"id": "25821723", "url": "https://en.wikipedia.org/wiki?curid=25821723", "title": "TeachMeet", "text": "TeachMeet\n\nA TeachMeet is an organised but informal meeting (in the style of an unconference) for teachers to share good practice, practical innovations and personal insights in teaching. These events are often organised to coincide with other educational events like the Scottish Learning Festival and the British Educational Technology and Training Show BETT.\n\nParticipants volunteer (via the TeachMeet website) to demonstrate good practice they've delivered over the past year, or discuss a product that enhances classroom practice.\n\nTeachMeet events are open to all and do not charge an entry fee.\n\nOriginally conceived in the summer of 2006 in Edinburgh, Scotland, under the name \"ScotEduBlogs Meetup\". The new name, TeachMeet, was created by Ewan McIntosh and agreed upon by the attendees of the first event. The 2nd Edition was held in Glasgow on 20 September 2006.\n\nThe 5th TeachMeet was the first to be held at the BETT Show in London.\n\nIn 2010 TeachMeet 'Takeover' was introduced at BETT, where teachers took over vendors stands in the main conference to bring the TeachMeet discussion out of the Apex Room and onto the exhibition floor.\n\nTeachMeets are now regular occurrences in Scotland, England, Northern Ireland, Australia, Canada, Croatia, Czech Republic, Denmark, Ireland, Malaysia, Sweden, the USA, New Zealand.\nIn New Zealand the TeachMeet is virtual and is run totally via Google+ Hangout.\n\nThe following features are often part of a Teachmeet, but the format changes according to the size of the meeting and the preferences of the organisers:\n\n"}
{"id": "7452748", "url": "https://en.wikipedia.org/wiki?curid=7452748", "title": "Timeline of carbon nanotubes", "text": "Timeline of carbon nanotubes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "46786123", "url": "https://en.wikipedia.org/wiki?curid=46786123", "title": "UAE Drones for Good", "text": "UAE Drones for Good\n\nUAE Drones for Good is an annual international competition and award by the government of the United Arab Emirates to encourage useful and positive applications for drone technology. The largest of its kind, the award currently stands at $1 million for the international competition and UAE Dhs1 million for the UAE competition.\n\nThe award was launched at the UAE Government Summit 2014. The first 2015 award was contested by more than 800 entries from 57 countries including MIT's 'Waterfly' collaborative drone 'swarms'. The finalists included Spanish company CATUAV for a drone fitted with optical sensors to scan war-affected regions of Bosnia and Herzegovina for landmines buried during the 1990s.. The eventual winner was a Swiss company with their search and rescue Gimball drone.\n\nThe 2016 competition awarded Loon Copter's sea-hybrid UAV in February 2017.\n"}
{"id": "31608627", "url": "https://en.wikipedia.org/wiki?curid=31608627", "title": "VLF cable testing", "text": "VLF cable testing\n\nVLF cable testing is a technique for testing of medium and high voltage (MV and HV) cables. VLF systems are advantageous in that they can be manufactured to be small and lightweight; making them useful – especially for field testing where transport and space can be issues. Traditionally and more commonly, DC Hi-Pot testing has been used for field testing of cables. \n\nVLF testing of cables is supported in IEC 60502 (up to 35kV) and in IEEE 400.2 (up to 69 kV). As higher voltage VLF equipment is developed, standards may be adapted to increase the voltage level for application.\n\nThe VLF test can be used in a number of ways:\n\n\nHigh voltage withstand tests are used in conjunction with partial discharge measurements on solid dielectric cable and accessories within manufacturing plants to ensure the quality of completed cable system components from MV to EHV. Thus, it is quite natural for utilities to also use withstand and partial discharge tests as commissioning and maintenance tests for cable systems in the field. The goal of these tests is the same as in the factory test, namely to detect any defective components of the cable system before failure. Withstand tests can be conducted using a variety of voltage sources from DC to 300 Hz and are simple to operate and the equipment may be inexpensive. Some observations for the VLF withstand test are (Based on CDFI results):\n\nMedium voltage distribution cables and their accessories form a critical part of power delivery systems. The systems employ insulation materials that have a low permittivity and loss. The permittivity and the loss are dielectric properties of the insulation material. As the systems age, these dielectric properties can change. The dielectric loss can be assessed since it can increase several orders of magnitude during the service life of the systems. This approach correlates well some lossy growths in aged polymeric insulation such as water trees.\n\nTan delta measurement constitutes a cable diagnostic technique that assesses the general condition of the cable system insulation, which can be represented in a simplified equivalent circuit that consists of two elements; a resistor and a capacitor. When voltage is applied to the system, the total current is the result of the contributions from the capacitor current and the resistor current. The tan delta is defined as the ratio between the resistive current and the capacitive current. The measurements are carried out offline.\n\nIn practice, it is convenient to measure the dielectric properties at a VLF of 0.1 Hz. This both reduces the size and power requirements of the energizing source and increases the resolution of the resistive component (near DC component) of dielectric loss (not the capacitive component).\n\nUsing the IEEE 400.2 guide, three different criteria are applied for diagnosing a cable insulation system using the Tan δ value. One criterion uses the magnitude of the Tan δ value as a tool for diagnostics while the other uses the difference in Tan δ values for particular electrical stresses or voltage levels. The latter is commonly known as the “Tip-Up” of the Tan δ value. The results for both criteria are often interpreted using recommendations given in the guide. The guide provides a hierarchical level that evaluates the cable insulation system. The major caveats with this approach are:\n\n\nA VLF source can be used to energise insulation and initiate partial discharges from defects within the insulation. As the test is offline, the test voltage can be varied in order to measure the inception and extinction voltages of the partial discharge. TDR techniques can be used to localise the source of the discharge and a reference measurement can be made with a calibrator in order to present the measured pd in pC.\n\nVLF PD measurements have the same benefits and limitations as other PD measurements and the data that is obtained using different voltage sources has the same uncertainties.\n\nIt must be noted that different defects may exhibit different characteristics dependent upon the environment and the excitation source. The impact of this on the final decision is likely insignificant. Even at higher voltages the criteria for detection (e.g. in Cigre WG B1.28) and severity calculation are not defined and not dependent on the measured properties of the PD. Therefore, the detection of PD sources is currently more important than the characterisation of the defects.\n\nDetection of defects is especially useful for new cables where the severity analysis is perhaps less important to the decision that is made. ANy defects in new installations should be corrected. For aged systems the PD severity can be assessed by consideration of the various PD characteristics. Unfortunately there is no independent guide that can be used to classify the severity after a single measurement. A trend can be established from repeat measurements and it is therefore important that measurement conditions are carefully controlled and repeated so that the comparison of repeated measurements is valid.\n\nTypical characteristics of PD that can contribute to severity analysis include:\n\nThere is some industry debate (much of it commercially driven) over the use of different voltage sources to energise cables and on the benefits of the different diagnostic techniques when used in conjunction with the different sources.\n\nTheoretical Approach\n\nThe cable is subject to operational stresses at the system voltage and frequency and voltage sources that are different (in magnitude, waveshape or frequency) will provide different stresses to the cable than those experienced under operational conditions. Defects and damage may also respond differently and the diagnostic indications may be different depending upon the types of defects. Proponents of this approach will argue that these differences detract from the commercial benefits offered by the competing voltage sources.\n\nPractical Approach\n\nElectrical equipment has a failure rate which is the inverse of its reliability. Test techniques have the intention of improving the reliability of the insulation system and an analysis of the impact of the testing on the reliability of the network under test is evidence of the efficacy of the test technique; irrespective of the differences from operational stresses.\n\nProbability\n\nInsulation failure is a stochastic process and it is erroneous to identify single events and attribute this to a particular source. Failure of an insulation system after a good diagnostic indication (or vice versa) is to be expected for any test using any voltage source. Better tests will be better predictors of condition but no tests should be considered infallible.\n\n\n\n"}
{"id": "1186168", "url": "https://en.wikipedia.org/wiki?curid=1186168", "title": "Voice browser", "text": "Voice browser\n\nA voice browser is a software application that presents an interactive voice user interface to the user in a manner analogous to the functioning of a web browser interpreting Hypertext Markup Language (HTML). Dialog documents interpreted by voice browser are often encoded in standards-based markup languages, such as Voice Dialog Extensible Markup Language (VoiceXML), a standard by the World Wide Web Consortium.\n\nA voice browser presents information aurally, using pre-recorded audio file playback or text-to-speech synthesis software. A voice browser obtains information using speech recognition and keypad entry, such as DTMF detection. \n\nAs speech recognition and web technologies have matured, voice applications are deployed commercially in many industries and voice browsers are supplanting traditional proprietary interactive voice response (IVR) systems. Voice browser software is delivered in a variety of implementations models.\n\nSystems that present a voice browser to a user, typically provide interfaces to the public switched telephone network or to a private branch exchange.\n\n\n"}
{"id": "1977460", "url": "https://en.wikipedia.org/wiki?curid=1977460", "title": "Wet cleaning", "text": "Wet cleaning\n\nWet cleaning refers to methods of professional cleaning that, in contrast to traditional dry cleaning, avoids the use of chemical solvents, the most common of which is tetrachloroethylene (commonly called perchloroethylene or \"perc\"). Environmental groups and the United States Environmental Protection Agency have indicated that such alternative \"wet cleaning\" methods are better for the environment than perc, and proponents of wet cleaning state that these methods can be used without shrinking or otherwise damaging garments that typically require dry cleaning.\nIt is not the use of water that makes wet cleaning a safest method for cleaning clothes. Computer-controlled wet cleaning machines, special dryers, safe detergents and non-toxic spot removers are what make wet cleaning an environmentally sound method. Wet cleaning machines have controls that allow them to safely and efficiently clean a wide variety of garments in water. Detergents and spot removers are made of ingredients that are safer for workers and the environment, yet are as safe and effective at removing soils, stains and odors as dry cleaning solvents. Equipment, detergents and skill all contribute to successful wet cleaning. \n\nAccording to the Environmental Protection Agency (EPA), wet cleaning is the most environmentally sensitive professional method of garment cleaning. It does not use hazardous chemicals, it does not generate hazardous waste, nor does the process create air pollution and it reduces the potential for water and soil contamination. The specialized detergents and conditioner used in the wet clean process are milder than home laundry products. All of the products are disposed of down the drain and easily handled by the local waste water treatment facility. For professional cleaners, wet-cleaning is argued to offer several advantages, such as lowered costs for start-up capital, supplies, equipment and hazardous waste disposal, as well as less reliance on skilled labor. \n\nFrom American Dry Cleaner, \"74.7% of dry cleaners use wet cleaning when cleaning casual clothing and sportswear; specialty items, like draperies and gowns (42.3%); “business casual” or softly tailored clothing (38%); restoration work (25.4%); and tailored workwear (16.9%). \nSome clothing manufacturers may mislabel their clothing \"Dry Clean Only\", even though there is no \"reasonable basis\" for making the claim that the garment will be harmed if it is not dry cleaned.\n\n\n"}
{"id": "57534427", "url": "https://en.wikipedia.org/wiki?curid=57534427", "title": "XP-PEN", "text": "XP-PEN\n\nXP-Pen is founded in Japan in 2005, it specializes in graphics tablets, pen display monitors, light pads, stylus pens and digital graphical products. In 2008 they established an office in Taiwan. In 2013, XP-Pen Technology Co. was founded in the United States. In 2015 they opened their office in Shenzhen, China. \n\nIn December 2017, they were invited to DreamWorks campus in Glendale California. in October 2017, they exhibited in Stan Lee Comic Con during the Halloween weekend. In July 2017, they took part in Los Angeles' 25th Anime Expo.\n\nArtist series display\nXP-Pen supplies drivers for Windows 7, 8, 10 and Mac 10.8 and above.\n"}
