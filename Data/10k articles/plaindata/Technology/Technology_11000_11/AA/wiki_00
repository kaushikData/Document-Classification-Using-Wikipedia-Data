{"id": "34248801", "url": "https://en.wikipedia.org/wiki?curid=34248801", "title": "Accessory drive", "text": "Accessory drive\n\nThe accessory drive is a gearbox that forms part of a gas turbine engine. Although not part of the engine's core, it drives the accessories, fuel pumps etc., that are otherwise essential for the operation of the engine or the aircraft on which it is mounted. Accessory drives on large engines handle between 400–500 hp.\n\nPower for the accessory drive is taken from the central shaft linking the turbine and compressor sections of the engine. This requires an internal gearbox that couples the drive to a radial driveshaft or \"towershaft\" that drives an external gearbox.\n\nThe design of the internal gearbox is complicated by the heat and small space available in which to connect the driveshaft. It is usually placed between the compressor outlet and the combustor. In turboprops or designs with centrifugal compressors, it may be placed ahead of the compressor.\n\nFor two-shaft designs, an accessory drive will be taken from the high-pressure shaft, i.e. the outer and shorter of the two concentric shafts. This shaft comes up to speed more quickly when the engine is started. The drive and accessory gearboxes may also be split in two, one driven from each engine shaft, so as to distribute their loads. The engine-critical systems, including the starter drive, are arranged on the high-pressure shaft, with aircraft systems on the low-pressure shaft. The high-pressure shaft also rotates faster than the low-pressure shaft, which may influence the distribution of accessories.\n\nTo allow for thermal expansion, the drive from the main shaft may be taken by one of three means:\n\nTo make best use of the limited space for the driveshaft and internal gearbox, the driveshaft runs at high speed, thus allowing it to be of small diameter. This reduces the disruption to the airflow and the size of the hollow fairing that encloses it.\n\nIf it is not possible to arrange a single straight path for the driveshaft, it may be arranged in two sections and linked by an intermediate gearbox. This is most commonly required for high-bypass turbofans with large diameter fans.\n\nThe packaging of an engine within its nacelle is a complicated task. The accessory drive is usually arranged as a curved casing, so that the various accessories are mounted close to the engine. The casing is a pair of light alloy castings. Separate machined mounting pads are provided for each accessory.\n\nThe drive within the casing is provided by a train of spur gears. Accessories are arranged on both sides of the driveshaft entry, in reducing order of their speed. The gears are usually plain spur gears, running in roller bearings. Idler gears are commonly used between them, to increase the spacing between accessories. Helical gears are sometimes used for the high-torque drives, typically the starter, as these give smoother running. However helical gears also generate an end-thrust, which then requires a more complicated thrust bearing to support them.\n\nThe complexity of an accessory drive and its gears is so great that they were used as a theme by the anthropomorphic illustrator Boris Artzybasheff in advertising for the Avco Lycoming company, who were making drive gearboxes for the Westinghouse J40 engine.\n\nIn some engines, bleed air is also tapped to provide power for accessories, as well as a mechanical shaft drive. Bleed air is particularly useful when a source of compressed air is specifically needed, either to pressurise cabin air, or as a supply of cooling air to other components (to avoid excess heat, this is taken for a low-pressure tapping, or from the LP compressor of a two-shaft engine). One important use for bleed air is for cross-starting of other engines in a multi-engine aircraft.\n\nSome of the accessories that may be driven include:\n\n\nAdditional facilities are provided for a centrifugal oil breather, to separate the drive lubricating oil from the overboard breather air vent. Also access for hand-turning the engine, during ground maintenance.\n"}
{"id": "8399218", "url": "https://en.wikipedia.org/wiki?curid=8399218", "title": "Analog forestry", "text": "Analog forestry\n\nAnalog forestry is an approach to ecosystem restoration that considers the process of forest formation and the functioning of forest services to be critical in establishing a sustainable ecosystem characterised by a high biodiversity to biomass ratio. Design is produced through a synthesis of traditional and scientific knowledge. It seeks to optimise the productive potential of the design rather than maximise the production of one crop and to maximise ecosystem services by increasing the volumetric mass of the photosynthetic component.\n\nAnalogue forestry draws design input not only from traditional models but also from the natural forest successional dynamics. When an ecosystem is designed to be analogous to the indigenous climax state, the efficiency and dynamics of the natural processes can be replicated. These quasi-natural forests are designed to mimic the structural and functional aspects of indigenous forests and are referred to as analog forests. In addition to their ecological characteristics, analog forests are also designed to provide economic benefits. However, it is not until all the ecological requirements of the location are satisfied that economic values of species are considered. Therefore, an analog forest may comprise natural and exotic species in any proportion, the contribution to structure and function being the overriding factor that determines its use.\n\nThe theoretical underpinnings began in 1978 in San Diego and Guatemala, It was first implemented in Sri Lanka around 1981 by Ranil Senanayake as an alternative to monocultures of \"Pinus\" and \"Eucalyptus\" and has spread to India, Vietnam, Philippines, Australia, Peru, Ecuador, Colombia, Brazil, Costa Rica, Dominican Republic, Honduras, Mexico, Canada, Kenya and Zimbabwe at present.\n\nThe International Analog Forestry Network (IAFN) is currently hosted in Costa Rica.\n\nAnalog forestry is a system of silviculture that seeks to design, plan and manage ecosystems dominated by woody perennials. It has been primarily employed in tropical or subtropical areas, but can be used in temperate areas too. The design seeks to mimic the architectural structure and ecological function of the preexisting climax vegetation of the area, and can be designed to provide economic, social and environmental benefits.\n\nAnalog forestry always considers the inclusion of non-crop native species to be considered in the design and thus addresses the inclusion of native biodiversity in the design. As analog forestry also requires the inclusion of long-lived species of trees in the design it has the capacity to sequester carbon for a longer time than plantation forestry. Analogue forestry has the potential to produce very high values of photosynthetic biomass as its design calls for the inclusion of all the growth forms that occupy the three-dimensional space of the mature indigenous forest. By including many species of crops in one area analog forestry helps spread the risk of market failure on a single crop.\n\n\n\n"}
{"id": "285911", "url": "https://en.wikipedia.org/wiki?curid=285911", "title": "Autoinjector", "text": "Autoinjector\n\nAn autoinjector (or auto-injector) is a medical device designed to deliver a dose of a particular drug.\n\nMost autoinjectors are spring-loaded syringes. By design, autoinjectors are easy to use and are intended for self-administration by patients, or administration by untrained personnel. The site of injection depends on the drug loaded, but it typically is administered into the thigh or the buttocks. The injectors were initially designed to overcome the hesitation associated with self-administration of the needle-based drug delivery device.\n\nThe autoinjector keeps the needle tip shielded prior to injection and also has a passive safety mechanism to prevent accidental firing (injection). Injection depth can be adjustable or fixed and a function for needle shield removal may be incorporated. Just by pressing a button, the syringe needle is automatically inserted and the drug is delivered. Once the injection is completed some auto injectors have visual indication to confirm that the full dose has been delivered.\nAutoinjectors contain glass syringes, which can make them fragile and vulnerable to contamination. More recently, companies have been looking into making autoinjector syringes out of plastic to prevent this issue.\nAnother design has a shape and size of a smartphone which can be put into a pocket. This design also has retractable needle and automated voice instructions to assist the users on how to correctly use the autoinjectors. The \"Auvi-Q\" epinephrine autoinjector uses this design.\n\n\n\nA newer variant of the autoinjector is the gas jet autoinjector, which contains a cylinder of pressurised gas and propels a fine jet of liquid through the skin without the use of a needle. This has the advantage that patients who fear needles are more accepting of using these devices. The autoinjector can be reloaded, and a variety of different doses or different drugs can be used, although the only widespread application to date has been for the administration of insulin in the treatment of diabetes.\n\n"}
{"id": "42736826", "url": "https://en.wikipedia.org/wiki?curid=42736826", "title": "Black-ish", "text": "Black-ish\n\nBlack-ish (stylized as ) is an American sitcom television series starring Anthony Anderson and Tracee Ellis Ross, broadcast on ABC. The single-camera comedy centers on an upper-middle-class African-American family. The series premiered on September 24, 2014 and in April 2017 was renewed for a fourth season, which premiered on October 3, 2017. Since the second-season premiere, the show has received critical acclaim, receiving many awards and nominations including a Golden Globe Award for Best Actress for Tracee Ellis Ross, Emmy and Golden Globe nominations for Outstanding Comedy Series, and a TCA Award for Outstanding Achievement in Comedy.\n\nOn May 11, 2018, ABC renewed the series for a fifth season. The fifth season premiered on October 16, 2018.\n\n\n\n\"Black-ish\" first appeared on the development slate at ABC in October 2013, when it was reported that the project, which would star Anthony Anderson, had received a script commitment. On January 16, 2014, ABC greenlit the pilot episode. Two weeks later, Larry Wilmore joined the show as showrunner. In mid-February, Laurence Fishburne was cast as the father of Anderson's character, and Tracee Ellis Ross signed on as the female lead.\n\nOn May 8, 2014, ABC picked up the pilot to the series for the 2014–15 television season. A few days later, Anderson announced that Larry Wilmore would be stepping down as showrunner early in the show's run due to his forthcoming late night show, \"The Nightly Show with Larry Wilmore\".\n\nOn May 7, 2015, ABC renewed the series for a second season.\n\nOn March 3, 2016, ABC renewed the series for a third season.\n\nOn May 10, 2017, ABC renewed the series for a fourth season.\n\nOn May 11, 2018, ABC renewed the series for a fifth season.\n\nThe 23rd episode of the third season, \"Liberal Arts\", functioned as a backdoor pilot for a proposed spin-off of the same title, starring Yara Shahidi as her character, Zoey Johnson, goes to college away from the family. Other cast members in the pilot and proposed series were Chris Parnell, Mallory Sparks, Matt Walsh, and Trevor Jackson.\n\nIn early May 2017, ABC passed on the pilot, but its cable sister channel Freeform was in negotiations to move the project there. On May 19, 2017, Freeform officially ordered 13 episodes of the spin-off, now under the tentative title \"College-ish\". In August 2017, the series changed its title to \"Grown-ish\", and added Francia Raisa, Jordan Buhat and Chloe x Halle as cast members. The series's pilot premiered on January 3, 2018. Parnell and Jackson reprised their roles from the backdoor pilot, while Emily Arlook was also added as Nomi, replacing the character Miriam played by Mallory Sparks. The series has been renewed for a second season.\n\n\"Black-ish\" has been met with generally positive reviews from critics. Rotten Tomatoes gives season 1 an approval rating of 86% based on 56 reviews, with an average rating of 7.3/10. The site's consensus states, \"Although it seems uncertain of its target audience, \"Black-ish\" ingratiates with a diverse cast and engaging cultural issues.\" Metacritic gave season 1 a weighted average score of 77 out of 100, based on 31 critics, indicating \"generally favorable reviews\". \"Rolling Stone\"′s December 4, 2014, issue called it \"one of the only new network comedies worth watching,\" praising in particular Laurence Fishburne's performance.\n\nOn Rotten Tomatoes, season 4 holds an approval rating of 100% based on 11 reviews, with an average rating of 8.67/10. The site's consensus states, \"\"black-ish\" continues to push boundaries, but with a much more celebratory tone that seeks to educate as readily as it entertains.\"\n\nAnthony Anderson's performance was met with critical acclaim, earning him multiple nominations for the Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series.\n\n\"Black-ish\" addresses not only the racism that the Johnsons face as an upper-middle class African-American family, but also includes the racism African Americans from a variety of backgrounds face in America. The \"Pilot\" episode starts off the series by introducing Dre's fear that his children are too assimilated to their primarily white surroundings and are losing their black culture. The episode also addresses the racism African Americans face in the workplace when Dre gets excited for a promotion at his advertising agency, which turns out to be for Senior Vice President of the Urban Division. In response, Dre questions, \"Did they just put me in charge of black stuff?\" This episode raises the question of where the line is drawn so that you are not defined by your race but your culture still remains relevant.\n\nIn the 25th episode, \"The Word\", Jack performs Kanye West's \"Gold Digger\" at school and says \"nigger\". The rest of the episode discusses the generational and multicultural perspectives of the word and how it has a different meaning to different people, even between different African Americans. That different meaning comes with different guidelines and de facto regulations for the use of the N-word based on the speaker, the context, and the audience.\n\nIn the 22nd episode, \"Please Don't Ask, Please Don't Tell\". Dre's younger sister, Rhonda (Raven-Symoné), is introduced. Rhonda is a lesbian but never officially came out to her family. Family members just gradually figured it out because of Rhonda's live-in girlfriend, however, no one ever acknowledges it. Andre admits that homosexuality is a topic that most African Americans prefer to avoid, due to homophobia, which is why he never discusses it with his sister. This leads to Rhonda not inviting any of her family members to her wedding and Andre finally talking to Rhonda about her sexuality.\n\nIn the 40th episode, \"Hope\", the show tackles police brutality and Black Lives Matter as the family watches the news reporting about an unarmed young, black man's fatal run-in with police. Although the case was fictional, many real names, such as Freddie Gray and Sandra Bland, were included in the family's discussion. The debate format of the episode was able to address both sides of the situation and not completely villainize the police force. The format also allowed for perspectives from different generations (except for the youngest twins), backgrounds, and ideologies. The end of the episode revolved around a message of hope and the importance of protests, discussion, and attitudes when people are faced with tragedies from police brutality, assassinations, etc.\n\nIn January 2017, the 60th episode, \"Lemons\", the show tackles the issue of the 2016 presidential election. It features conversations with the Johnson family and at Dre's work about the election of Donald Trump as president. The episode originally aired a few weeks prior to Trump's inauguration.\n\n"}
{"id": "40880508", "url": "https://en.wikipedia.org/wiki?curid=40880508", "title": "Blavatnik Awards for Young Scientists", "text": "Blavatnik Awards for Young Scientists\n\nThe Blavatnik Awards for Young Scientists was established in 2007 through a partnership between the Blavatnik Family Foundation, headed by American industrialist and philanthropist Len Blavatnik (Russian: Леонид Валентинович Блаватник), chairman of Access Industries, and the New York Academy of Sciences, headed by president and CEO Mr. Ellis Rubinstein. The awards have been given annually to selected faculty and postdoctoral researchers age 42 years and younger who work in the life and physical sciences and engineering at institutions in the New York tri-state area. The first Blavatnik Awards were given in New York City on Monday, November 12, 2007. On June 3, 2013, the Blavatnik Family Foundation and the New York Academy of Sciences announced the expansion of the faculty competition to include young scientists from institutions throughout the United States. In April 2017, the Blavatnik Awards program was expanded to the United Kingdom (UK) and Israel. \n\nThe regional program will continue to recognize postdoctoral researchers working at institutions in the New York tri-state area. The regional program accepts nominations for scientists working in the life sciences, physical sciences, mathematics, and engineering. Nominations are accepted from institutions in New York, New Jersey, and Connecticut. Submissions for the regional program are reviewed by a Judging Panel of senior scientists, science editors, and past Blavatnik winners from the Mid-Atlantic area. As of 2013, winners of the postdoctoral competition receive US$30,000 and finalists receive US$10,000, each in unrestricted cash prizes.\n\nBeginning with the 2014 awards cycle, the national faculty competition accepts nominations for scientists working in three disciplinary categories: Life Sciences, Physical Sciences & Engineering, and Chemistry. Nominations are accepted from institutions throughout the United States. Members of the Awards’ Scientific Advisory Council may also submit nominations. Submissions are reviewed by a Judging Panel of senior scientists and past Blavatnik Awards winners. The awards will be conferred annually with one winner (“Laureate”) from each disciplinary category selected each year (for a total of three Laureates per year). Each Laureate will receive a US$250,000 unrestricted cash prize and be honored at a ceremony in New York City every fall.\n\nIn 2017 the Blavatnik Awards launched a national competition in Israel modeled on the U.S. Faculty awards. The Blavatnik Awards in Israel are administered by The New York Academy of Sciences in collaboration with the Israel Academy of Sciences and Humanities. Three Laureates from Israel are chosen each awards cycle and receive US$100,000 in unrestricted funds. The first awards were granted during a ceremony held at the Israel Museum in Jerusalem on February 4, 2018. \n\nIn 2017 the Blavatnik Awards launched a national competition across the United Kingdom modeled after the U.S. Faculty awards. A laureate and two finalists are chosen in the UK every awards cycle. Laureates are awarded US$100,000 and finalists receive US$30,000. The first awards were granted during a ceremony held at the Victoria and Albert Museum in London on March 7, 2018.\n"}
{"id": "45638306", "url": "https://en.wikipedia.org/wiki?curid=45638306", "title": "Clearinghouse for Networked Information Discovery and Retrieval", "text": "Clearinghouse for Networked Information Discovery and Retrieval\n\nThe Clearinghouse for Networked Information Discovery and Retrieval or CNIDR was an organization funded by the U.S. National Science Foundation from 1993 to 1997 and based at the Microelectronics Center of North Carolina (MCNC) in Research Triangle Park. CNIDR was active in the research and development of open source software and open standards, centered on information discovery and retrieval, in the emerging Internet.\n\nAmong the software developed at CNIDR were Isite, an open source Z39.50 implementation and successor to the free version of WAIS, and Isearch, an open source text retrieval system. CNIDR staff were involved in the development of open standards in the Internet Engineering Task Force, the Z39.50 Implementors Group and Dublin Core.\n\nCNIDR collaborated with the U.S. Patent and Trademark Office (USPTO) to develop the USPTO's first Internet-based patent search systems. One of these provided full text searching and images of medical patents related to the research and treatment of HIV/AIDS and issued by the US, Japanese and European patent offices. Another system, known as the US Patent Bibliographic Database, provided searching of \"front page\" bibliographic information for all US patents since 1976.\n"}
{"id": "57625", "url": "https://en.wikipedia.org/wiki?curid=57625", "title": "Common Criteria", "text": "Common Criteria\n\nThe Common Criteria for Information Technology Security Evaluation (referred to as Common Criteria or CC) is an international standard (ISO/IEC 15408) for computer security certification. It is currently in version 3.1 revision 5.\n\nCommon Criteria is a framework in which computer system users can \"specify\" their security \"functional\" and \"assurance\" requirements (SFRs and SARs respectively) in a Security Target (ST), and may be taken from Protection Profiles (PPs). Vendors can then \"implement \" or make claims about the security attributes of their products, and testing laboratories can \"evaluate\" the products to determine if they actually meet the claims. In other words, Common Criteria provides assurance that the process of specification, implementation and evaluation of a computer security product has been conducted in a rigorous and standard and repeatable manner at a level that is commensurate with the target environment for use.\n\nCommon Criteria evaluations are performed on computer security products and systems.\n\n\nThe evaluation process also tries to establish the level of confidence that may be placed in the product's security features through quality assurance processes:\n\n\nSo far, most PPs and most evaluated STs/certified products have been for IT components (e.g., firewalls, operating systems, smart cards).\nCommon Criteria certification is sometimes specified for IT procurement. Other standards containing, e.g., interoperation, system management, user training, supplement CC and other product standards. Examples include the ISO/IEC 17799 (Or more properly BS 7799-1, which is now ISO/IEC 27002) or the German .\n\nDetails of cryptographic implementation within the TOE are outside the scope of the CC. Instead, national standards, like FIPS 140-2 give the specifications for cryptographic modules, and various standards specify the cryptographic algorithms in use.\n\nMore recently, PP authors are including cryptographic requirements for CC evaluations that would typically be covered by FIPS 140-2 evaluations, broadening the bounds of the CC through scheme-specific interpretations.\n\nSome national evaluation schemes are phasing out EAL-based evaluations and only accept products for evaluation that claim strict conformance with an approved PP. The United States currently only allows PP-based evaluations. Canada is in the process of phasing out EAL-based evaluations.\n\nCC originated out of three standards:\n\n\nCC was produced by unifying these pre-existing standards, predominantly so that companies selling computer products for the government market (mainly for Defence or Intelligence use) would only need to have them evaluated against one set of standards. The CC was developed by the governments of Canada, France, Germany, the Netherlands, the UK, and the U.S.\n\nAll testing laboratories must comply with ISO 17025, and certification bodies will normally be approved against either ISO/IEC Guide 65 or BS EN 45011.\n\nThe compliance with ISO 17025 is typically demonstrated to a National approval authority:\n\nCharacteristics of these organizations were examined and presented at ICCC 10.\n\nAs well as the Common Criteria standard, there is also a sub-treaty level Common Criteria MRA (Mutual Recognition Arrangement), whereby each party thereto recognizes evaluations against the Common Criteria standard done by other parties. Originally signed in 1998 by Canada, France, Germany, the United Kingdom and the United States, Australia and New Zealand joined 1999, followed by Finland, Greece, Israel, Italy, the Netherlands, Norway and Spain in 2000. The Arrangement has since been renamed Common Criteria Recognition Arrangement (CCRA) and membership continues to expand. Within the CCRA only evaluations up to EAL 2 are mutually recognized (Including augmentation with flaw remediation). The European countries within the former ITSEC agreement typically recognize higher EALs as well. Evaluations at EAL5 and above tend to involve the security requirements of the host nation's government.\n\nIn September 2012, a majority of members of the CCRA produced a vision statement whereby mutual recognition of CC evaluated products will be lowered to EAL 2 (Including augmentation with flaw remediation). Further, this vision indicates a move away from assurance levels altogether and evaluations will be confined to conformance with Protection Profiles that have no stated assurance level. This will be achieved through technical working groups developing worldwide PPs, and as yet a transition period has not been fully determined.\n\nOn July 2, 2014, a new CCRA was ratified per the goals outlined within the 2012 vision statement. Major changes to the Arrangement include:\n\nCommon Criteria is very generic; it does not directly provide a list of product security requirements or features for specific (classes of) products: this follows the approach taken by ITSEC, but has been a source of debate to those used to the more prescriptive approach of other earlier standards such as TCSEC and FIPS 140-2.\n\nCommon Criteria certification cannot guarantee security, but it can ensure that claims about the security attributes of the evaluated product were independently verified. In other words, products evaluated against a Common Criteria standard exhibit a clear chain of evidence that the process of specification, implementation, and evaluation has been conducted in a rigorous and standard manner.\n\nVarious Microsoft Windows versions, including Windows Server 2003 and Windows XP, have been certified, but security patches to address security vulnerabilities are still getting published by Microsoft for these Windows systems. This is possible because the process of obtaining a Common Criteria certification allows a vendor to restrict the analysis to certain security features and to make certain assumptions about the operating environment and the strength of threats faced by the product in that environment. Additionally, the CC recognizes a need to limit the scope of evaluation in order to provide cost-effective and useful security certifications, such that evaluated products are examined to a level of detail specified by the assurance level or PP. Evaluations activities are therefore only performed to a certain depth, use of time, and resources and offer reasonable assurance for the intended environment.\n\nIn the Microsoft case, the assumptions include A.PEER: \n\n\"Any other systems with which the TOE communicates are assumed to be under the same management control and operate under the same security policy constraints. The TOE is applicable to networked or distributed environments only if the entire network operates under the same constraints and resides within a single management domain. There are no security requirements that address the need to trust external systems or the communications links to such systems.\"\nThis assumption is contained in the Controlled Access Protection Profile (CAPP) to which their products adhere. Based on this and other assumptions, which may not be realistic for the common use of general-purpose operating systems, the claimed security functions of the Windows products are evaluated. Thus they should only be considered secure in the assumed, specified circumstances, also known as the \"evaluated configuration\".\n\nWhether you run Microsoft Windows in the precise evaluated configuration or not, you should apply Microsoft's security patches for the vulnerabilities in Windows as they continue to appear. If any of these security vulnerabilities are exploitable in the product's evaluated configuration, the product's Common Criteria certification should be voluntarily withdrawn by the vendor. Alternatively, the vendor should re-evaluate the product to include the application of patches to fix the security vulnerabilities within the evaluated configuration. Failure by the vendor to take either of these steps would result in involuntary withdrawal of the product's certification by the certification body of the country in which the product was evaluated.\n\nThe certified Microsoft Windows versions remain at EAL4+ without including the application of any Microsoft security vulnerability patches in their evaluated configuration. This shows both the limitation and strength of an evaluated configuration.\n\nIn August 2007, \"Government Computing News\" (GCN) columnist William Jackson critically examined Common Criteria methodology and its US implementation by the Common Criteria Evaluation and Validation Scheme (CCEVS). In the column executives from the security industry, researchers, and representatives from the National Information Assurance Partnership (NIAP) were interviewed. Objections outlined in the article include:\n\n\nIn a 2006 research paper, computer specialist David A. Wheeler suggested that the Common Criteria process discriminates against free and open-source software (FOSS)-centric organizations and development models. Common Criteria assurance requirements tend to be inspired by the traditional waterfall software development methodology. In contrast, much FOSS software is produced using modern agile paradigms. Although some have argued that both paradigms do not align well, others have attempted to reconcile both paradigms. Political scientist Jan Kallberg raised concerns over the lack of control over the actual production of the products once they are certified, the absence of a permanently staffed organizational body that monitors compliance, and the idea that the trust in the Common Criteria IT-security certifications will be maintained across geopolitical boundaries.\n\nThroughout the lifetime of CC, it has not been universally adopted even by the creator nations, with, in particular, cryptographic approvals being handled separately, such as by the Canadian / US implementation of FIPS-140, and the CESG Assisted Products Scheme (CAPS) in the UK.\n\nThe UK has also produced a number of alternative schemes when the timescales, costs and overheads of mutual recognition have been found to be impeding the operation of the market:\n\nIn early 2011, NSA/CSS published a paper by Chris Salter, which proposed a Protection Profile oriented approach towards evaluation. In this approach, communities of interest form around technology types which in turn develop protection profiles that define the evaluation methodology for the technology type. The objective is a more robust evaluation. There is some concern that this may have a negative impact on mutual recognition.\n\nIn Sept of 2012, the Common Criteria published a Vision Statement implementing to a large extent Chris Salter's thoughts from the previous year. Key elements of the Vision included:\n\n\n"}
{"id": "41676484", "url": "https://en.wikipedia.org/wiki?curid=41676484", "title": "Dark store", "text": "Dark store\n\nThe term dark store, dark supermarket or dotcom centre refers to a retail outlet or distribution centre that caters exclusively for online shopping. A dark store is generally a large warehouse that can either be used to facilitate a \"click-and-collect\" service, where a customer collects an item they have ordered online, or as an order fulfilment platform for online sales. The format was initiated in the United Kingdom, and its popularity has also spread to France followed by rest of European Union.\n\nNot open to the public, the interior of a dark supermarket may appear like a conventional supermarket, set out with aisles of shelves containing groceries and other retail items. However, without having to deal with retail customers, the stores are not located in the high street or shopping centres, but mostly in areas that are preferred for good road connections. The buildings themselves are often utilitarian and nondescript from the outside. Inside, the stores dispense with assistants who provide product advice, check-out counters and point of sale displays.\n\nAfter processing orders received via the Internet, the orders are sent to the shop floor. These electronically generated orders, processed and routed according to the store layout for optimal picking, are picked by store employees, known as \"personal shoppers\" (colloquially \"pickers\"), who work around the clock fulfilling the orders displayed on a tablet computer attached to their shopping trolley. More than one order can often be collected simultaneously. Tesco opened a \"fourth generation dotcom store\" in Erith in October 2013, with a much larger product range – 30,000 lines – and higher degree of mechanisation that brings items to pickers rather than requiring them to collect individual products manually. Fulfilled orders are then delivered to the customer by a fleet of vans. A certain time of day, usually in the early hours of the morning, is set aside for stock replenishment.\n\nWhile most popular dark stores serve groceries, some of them are clothing shops, helping brands to cut the costs.\n\nThe format is also popular in France, where, , some 2,000 dark stores operated for the \"click-and-collect\" model.\n\nThe first UK supermarket to trial the concept of a specific store for online goods was Sainsbury's, which operated a distribution centre at Park Royal in London during the early 2000s, but the retailer closed the outlet because of a low order quantity. It was over a decade afterwards, in October 2013 that they announced plans for another, at Bromley-by-Bow, in East London.\n\nThe term dark store first appeared in the UK in 2009 when Tesco opened their first such supermarkets in Croydon, Surrey, and Aylesford, Kent. At the time, Tesco were receiving around 475,000 orders per week which were being fulfilled from its existing retail supermarkets. Supermarkets began opening dark stores to assist with distribution in geographical areas where there was a high demand for online delivery. The dark store format was seen by Tesco as a more efficient way of dealing with the expansion in online sales. The retailer planned to open one dark store per year \"for the foreseeable future\". By 2013, Tesco had opened six dotcom centres in and around London, and was responsible for 47.5% of online deliveries made in the UK. The latest of these was a store that opened in Erith in October 2013, and which the industry publication \"Retail Gazette\" described as a \"fourth generation dotcom store\" because of the greater emphasis on a mechanised system that brought items to pickers rather than requiring them to collect individual products manually, while chilled goods are conveyed directly from refrigerator to delivery van. The Erith store holds a range of 30,000 products, and has a capacity to process 4,000 online orders a day.\n\nIn November 2012, Zoe Wood of \"The Guardian\" reported that a number of dark stores had been opened by major supermarket chains in the UK, including Tesco and Waitrose, with more planned. Waitrose opened their first online distribution centre at the site of a former John Lewis warehouse in London in April 2011, and in September 2013 announced plans for a second, purpose-built centre at Coulsdon that would open in 2014. The company had previously used the Ocado distribution service to dispatch its goods to customers, but wished to begin rolling out its own delivery service.\n\n"}
{"id": "1907548", "url": "https://en.wikipedia.org/wiki?curid=1907548", "title": "Extended Data Services", "text": "Extended Data Services\n\nExtended Data Services (now XDS, previously EDS), is an American standard classified under Electronic Industries Alliance standard CEA-608-E for the delivery of any ancillary data (metadata) to be sent with an analog television program, or any other NTSC video signal.\n\nXDS is used by TV stations, TV networks, and TV program syndication distributors in the US for several purposes. \n\nHere are some of the most common uses of XDS:\n\n\nXDS is also used by the American TV network ABC for their Network Alert System (NAS). NAS is a one-way communication system used by ABC to inform and alert their local affiliate stations across the US of information regarding ABC's network programming (such as program timings & changes, news special report information, etc.), using a special decoder manufactured for ABC by EEG Enterprises , a manufacturer of related equipment for the TV broadcast industry such as closed captioning and general-purpose XDS encoders. The CBS Television Network uses a similar method to transmit three separate internal messaging services to stations: one for programming departments, one for master control operations, and one for newsrooms.\n\nMany standard definition receivers produced by Dish Network encode XDS data into their output signal. Data encoded includes time of day, program name, program description, program time remaining, channel identification, and content rating. This data is obtained from the satellite service's EPG and replaces any data which may have been present when the signal was uplinked.\n\nXDS uses the same line in the vertical blanking interval as closed captioning (NTSC line 21), and shares the available second video field bandwidth with the closed captioning channels CC3 and CC4, and with the text channels TXT3 and TXT4.\n\n"}
{"id": "2894782", "url": "https://en.wikipedia.org/wiki?curid=2894782", "title": "Facial tissue", "text": "Facial tissue\n\nFacial tissue, paper handkerchief, and Kleenex refers to a class of soft, absorbent, disposable papers that are suitable for use on the face. They are disposable alternatives for cloth handkerchiefs. The terms are commonly used to refer to the type of paper tissue, usually sold in boxes, that is designed to facilitate the expulsion of nasal mucus from the nose (nose-blowing) although it may refer to other types of facial tissues including napkins and wipes.\n\nFacial tissue is often referred to as a \"tissue\", or (in the United States) by the generic trademark \"Kleenex\" which popularized the invention and its use.\n\nFacial tissue and paper handkerchiefs are made from the lowest basis weights tissue paper (14 18 g/m). The surface is often made smoother by light calendering. These paper types consist usually of 2–3 plies. Because of high quality requirements the base tissue is normally made entirely from pure chemical pulp, but might contain added selected recycled fiber. The tissue paper might be treated with softeners, lotions or added perfume to get the right properties or \"feeling\". The finished facial tissues or handkerchiefs are folded and put in pocket-size packages or a box dispenser.\n\nFacial tissue has been used for centuries in Japan, in the form of washi () or Japanese tissue, as described in this 17th-century European account of the voyage of Hasekura Tsunenaga:\n\nIn 1924, facial tissues as they are known today were first introduced by Kimberly-Clark as Kleenex. It was invented as a means to remove cold cream. Early advertisements linked Kleenex to Hollywood makeup departments and sometimes included endorsements from movie stars (Helen Hayes and Jean Harlow) who used Kleenex to remove their theatrical makeup with cold cream. It was the customers that started to use Kleenex as a disposable handkerchief, and a reader review in 1926 by a newspaper in Peoria, Illinois found that 60% of the users used it for blowing their nose. The other 40% used it for various reasons, including napkins and toilet paper.\n\nKimberly-Clark also introduced pop-up, colored, printed, pocket, and 3-ply facial tissues.\n\nThe first unbleached bamboo facial tissues were invented by Alps Group Pte Ltd Singapore under the brand of Cloversoft. This invention was first retailed in Singapore and Asia in September 2014. \n\nA few months on, Alps Group Pte Ltd came up with white bamboo tissues under Dr Blanc. Alps Group Pte Ltd is the first company to use bamboo to turn them into tissues. They have now created wet wipes using unbleached bamboo. All the wet wipes in the market are made from plastic. \n\nAlps Group Pte Ltd has achieved another milestone now to have created the world's first bamboo facial tissues that are packaged in Kraft paper as opposed to the conventional plastic packaging. This is retailed in Singapore and Asia in September 2018.\n\n\n"}
{"id": "8412741", "url": "https://en.wikipedia.org/wiki?curid=8412741", "title": "Floating airport", "text": "Floating airport\n\nA floating airport is an airport built and situated on a very large floating structure (VLFS) located many miles out at sea utilizing a flotation type of device or devices such as pneumatic stabilized platform (PSP) technology.\n\nAs the population increases and land becomes more expensive and scarce, very large floating structures (VLFS) such as floating airports could help solve land use, pollution and aircraft noise issues.\n\nThe first discussion of a floating airport was for trans-Atlantic flights. At that time a passenger aircraft capable of making the trip could be built, but because of the massive need for fuel for the flight, it had a limited payload. An article appeared in the January 1930 issue of \"Popular Mechanics\" in which a model of a floating airport located in the Atlantic was proposed. To make safe flight possible with the aviation technology of that time, it called for eight such airports in the Atlantic. But unlike future floating airport ideas which were free floating, this 1930 concept had a floating airport platform, but with stabilizer legs which prevent the flight deck from pitching and rolling, similar in concept to some of today's off shore oil rigs. The cost of establishing eight such floating airports in 1930 was estimated at approximately USD$12,000,000. The idea of floating airports was forgotten until in 1935 the famous French aviation pilot and builder Bleriot gave one of his last interviews in which he made the case for floating airports in mid-Atlantic; he called them Seadromes as a solution to economical trans-Atlantic passenger flights.\n\nIn theory, issues and problems of land-based airports could be minimized by locating airports several miles off the coast. Takeoffs and landings would be over water, not over populated areas, thereby eliminating noise pollution and reducing risks of aircraft crashes to the land-locked population.\n\nSince little of the ocean's surface is currently being used for human activity, growth and alterations in configuration would be relatively easy to achieve with minimal impact to the environment or to local residents who would utilize the airport. Water taxis or other high speed surface vessels would be a part of an offshore mass transit system that could connect the floating airport to coastal communities and minimize traffic issues.\n\nA floating structure, such as a floating airport, is theorised to have less impact on the environment than the land-based alternative. It would not require much, if any, dredging or moving of mountains or clearing of green space and the floating structure provides a reef-like environment conducive to marine life. In theory, wave energy could be harnessed, using the structure to convert waves into energy to help sustain the energy needs of the airport.\n\nIn 2000, the Japanese Ministry of Land, Infrastructure, and Transport sponsored the construction of Mega-Float, a 1000-metre floating runway in Tokyo Bay. After conducting several real aircraft landings, the Ministry concluded that floating runways' hydro-elastic response would not affect aircraft operations, including precision instrument approaches in a protected waterway such as a large bay. The structure has been dismantled and is no longer in use.\n\nThe pneumatic stabilized platform (PSP) was proposed as a means for constructing a new floating airport for San Diego in the Pacific Ocean, at least three miles off the tip of Point Loma. However, this proposed design was rejected in October, 2003 due to very high cost, the difficulty in accessing such an airport, the difficulty in transporting jet fuel, electricity, water, and gas to the structure, failure to address security concerns such as a bomb blast, inadequate room for high-speed exits and taxiways, and environmental concerns.\n\nAchmad Yani International Airport, the first floating airport in the world started construction on 17 June 2014, and completed in 2018. However, only the passenger terminal and apron is floating.\n\n\n"}
{"id": "13590", "url": "https://en.wikipedia.org/wiki?curid=13590", "title": "House", "text": "House\n\nA house is a building that functions as a home. They can range from simple dwellings such as rudimentary huts of nomadic tribes and the improvised shacks in shantytowns to complex, fixed structures of wood, brick, concrete or other materials containing plumbing, ventilation, and electrical systems. Houses use a range of different roofing systems to keep precipitation such as rain from getting into the dwelling space. Houses may have doors or locks to secure the dwelling space and protect its inhabitants and contents from burglars or other trespassers. Most conventional modern houses in Western cultures will contain one or more bedrooms and bathrooms, a kitchen or cooking area, and a living room. A house may have a separate dining room, or the eating area may be integrated into another room. Some large houses in North America have a recreation room. In traditional agriculture-oriented societies, domestic animals such as chickens or larger livestock (like cattle) may share part of the house with humans. The social unit that lives in a house is known as a household.\n\nMost commonly, a household is a family unit of some kind, although households may also be other social groups, such as roommates or, in a rooming house, unconnected individuals. Some houses only have a dwelling space for one family or similar-sized group; larger houses called townhouses or row houses may contain numerous family dwellings in the same structure. A house may be accompanied by outbuildings, such as a garage for vehicles or a shed for gardening equipment and tools. A house may have a backyard or frontyard, which serve as additional areas where inhabitants can relax or eat.\n\nThe English word \"house\" derives directly from the Old English \"hus\" meaning \"dwelling, shelter, home, house,\" which in turn derives from Proto-Germanic \"husan\" (reconstructed by etymological analysis) which is of unknown origin. The house itself gave rise to the letter 'B' through an early Proto-Semitic hieroglyphic symbol depicting a house. The symbol was called \"bayt\", \"bet\" or \"beth\" in various related languages, and became \"beta\", the Greek letter, before it was used by the Romans.\n\nIdeally, architects of houses design rooms to meet the needs of the people who will live in the house. Feng shui, originally a Chinese method of moving houses according to such factors as rain and micro-climates, has recently expanded its scope to address the design of interior spaces, with a view to promoting harmonious effects on the people living inside the house, although no actual effect has ever been demonstrated. Feng shui can also mean the \"aura\" in or around a dwelling, making it comparable to the real-estate sales concept of \"indoor-outdoor flow\".\n\nThe square footage of a house in the United States reports the area of \"living space\", excluding the garage and other non-living spaces. The \"square metres\" figure of a house in Europe reports the area of the walls enclosing the home, and thus includes any attached garage and non-living spaces. The number of floors or levels making up the house can affect the square footage of a home.\n\nMany houses have several large rooms with specialized functions and several very small rooms for other various reasons. These may include a living/eating area, a sleeping area, and (if suitable facilities and services exist) separate or combined washing and lavatory areas. Some larger properties may also feature rooms such as a spa room, indoor pool, indoor basketball court, and other 'non-essential' facilities. In traditional agriculture-oriented societies, domestic animals such as chickens or larger livestock (like cattle) often share part of the house with human beings. Most conventional modern houses will at least contain a bedroom, bathroom, kitchen or cooking area, and a living room. A typical \"foursquare house\" (as pictured) occurred commonly in the early history of the US where they were mainly built, with a staircase in the center of the house, surrounded by four rooms, and connected to other sections of the home (including in more recent eras a garage).\n\nLittle is known about the earliest origin of the house and its interior, however it can be traced back to the simplest form of shelters. Roman architect Vitruvius' theories have claimed the first form of architecture as a frame of timber branches finished in mud, also known as the primitive hut.\nPhilip Tabor later states the contribution of 17th century Dutch houses as the foundation of houses today.\n\nIn the Middle Ages, the Manor Houses facilitated different activities and events. Furthermore, the houses accommodated numerous people, including family, relatives, employees, servants and their guests. Their lifestyles were largely communal, as areas such as the Great Hall enforced the custom of dining and meetings and the Solar intended for shared sleeping beds.\n\nDuring the 15th and 16th centuries, the Italian Renaissance Palazzo consisted of plentiful rooms of connectivity. Unlike the qualities and uses of the Manor Houses, most rooms of the palazzo contained no purpose, yet were given several doors. These doors adjoined rooms in which Robin Evans describes as a \"matrix of discrete but thoroughly interconnected chambers.\" The layout allowed occupants to freely walk room to room from one door to another, thus breaking the boundaries of privacy. \n\nAlthough very public, the open plan encouraged sociality and connectivity for all inhabitants.\n\nAn early example of the segregation of rooms and consequent enhancement of privacy may be found in 1597 at the Beaufort House built in Chelsea. It was designed by English architect John Thorpe who wrote on his plans, \"A Long Entry through all\". The separation of the passageway from the room developed the function of the corridor. This new extension was revolutionary at the time, allowing the integration of one door per room, in which all universally connected to the same corridor. English architect Sir Roger Pratt states \"the common way in the middle through the whole length of the house, [avoids] the offices from one molesting the other by continual passing through them.\" Social hierarchies within the 17th century were highly regarded, as architecture was able to epitomize the servants and the upper class. More privacy is offered to the occupant as Pratt further claims, \"the ordinary servants may never publicly appear in passing to and fro for their occasions there.\" This social divide between rich and poor favored the physical integration of the corridor into housing by the 19th century.\n\nSociologist Witold Rybczynski wrote, \"the subdivision of the house into day and night uses, and into formal and informal areas, had begun.\" Rooms were changed from public to private as single entryways forced notions of entering a room with a specific purpose.\n\nCompared to the large scaled houses in England and the Renaissance, the 17th Century Dutch house was smaller, and was only inhabited by up to four to five members. This was due to their embracing \"self-reliance\", in contrast to the dependence on servants, and a design for a lifestyle centered on the family. It was important for the Dutch to separate work from domesticity, as the home became an escape and a place of comfort. This way of living and the home has been noted as highly similar to the contemporary family and their dwellings. House layouts also incorporated the idea of the corridor as well as the importance of function and privacy.\n\nBy the end of the 17th Century, the house layout was soon transformed to become employment-free, enforcing these ideas for the future. This came in favour for the industrial revolution, gaining large-scale factory production and workers. The house layout of the Dutch and its functions are still relevant today.\nThe names of parts of a house often echo the names of parts of other buildings, but could typically include:\n\n In the American context, some professions, such as doctors, in the 19th and early 20th century typically operated out of the front room or parlor or had a two-room office on their property, which was detached from the house. By the mid 20th century and the increase in high-tech equipment saw a marked shift whereby the contemporary doctor typically works from an office complex or hospital.\nThe introduction of technology and electronic systems within the house has questioned the impressions of privacy as well as the segregation of work from home. Technological advances of surveillance and communications allow insight of personal habits and private lives. As a result, the \"private becomes ever more public, [and] the desire for a protective home life increases, fuelled by the very media that undermine it\" writes Hill. Work also, has been altered due to the increase of communications. The \"deluge of information\", has expressed the efforts of work, conveniently gaining access inside the house. Although commuting is reduced, \"the desire to separate working and living remains apparent.\" In Jonathan Hill's book \"Immature Architecture\", he identifies this new invasion of privacy as \"Electromagnetic Weather\". Natural or man-made weather remains concurrent inside or outside the house, yet the electromagnetic weather is able to generate within both positions. On the other hand, some architects have designed homes in which eating, working and living are brought together.\n\nIn the United States, modern house-construction techniques include light-frame construction (in areas with access to supplies of wood) and adobe or sometimes rammed-earth construction (in arid regions with scarce wood-resources). Some areas use brick almost exclusively, and quarried stone has long provided walling. To some extent, aluminum and steel have displaced some traditional building materials. Increasingly popular alternative construction materials include insulating concrete forms (foam forms filled with concrete), structural insulated panels (foam panels faced with oriented strand board or fiber cement), and light-gauge steel framing and heavy-gauge steel framing.\n\nMore generally, people often build houses out of the nearest available material, and often tradition or culture govern construction-materials, so whole towns, areas, counties or even states/countries may be built out of one main type of material. For example, a large fraction of American houses use wood, while most British and many European houses use stone or brick or mud.\nIn the 1900s (decade), some house designers started using prefabrication. Sears, Roebuck & Co. first marketed their Sears Catalog Homes to the general public in 1908. Prefab techniques became popular after World War II. First small inside rooms framing, then later, whole walls were prefabricated and carried to the construction site. The original impetus was to use the labor force inside a shelter during inclement weather. More recently builders have begun to collaborate with structural engineers who use computers and finite element analysis to design prefabricated steel-framed homes with known resistance to high wind-loads and seismic forces. These newer products provide labor savings, more consistent quality, and possibly accelerated construction processes.\n\nLesser-used construction methods have gained (or regained) popularity in recent years. Though not in wide use, these methods frequently appeal to homeowners who may become actively involved in the construction process. They include:\n\nIn the developed world, energy-conservation has grown in importance in house-design. Housing produces a major proportion of carbon emissions (studies have show that it is 30% of the total in the United Kingdom).\n\nDevelopment of a number of types and techniques continues. They include the zero-energy house, the passive solar house, the autonomous buildings, the superinsulated and houses built to the \"Passivhaus\" standard.\n\nOne tool of earthquake engineering is base isolation which is increasingly used for earthquake protection. Base isolation is a collection of structural elements of a building that should substantially decouple it from the shaking ground thus protecting the building's integrity and enhancing its seismic performance. This technology, which is a kind of seismic vibration control, can be applied both to a newly designed building and to seismic upgrading of existing structures.\n\nNormally, excavations are made around the building and the building is separated from the foundations. Steel or reinforced concrete beams replace the connections to the foundations, while under these, the isolating pads, or \"base isolators\", replace the material removed. While the \"base isolation\" tends to restrict transmission of the ground motion to the building, it also keeps the building positioned properly over the foundation. Careful attention to detail is required where the building interfaces with the ground, especially at entrances, stairways and ramps, to ensure sufficient relative motion of those structural elements.\n\nBamboo is an earthquake-resistant material, and is very versatile because it comes from fast-grow plants. Adding that bamboos are common in Asia, bamboo-made houses are popular in some Asian countries.\n\nIn many parts of the world, houses are constructed using scavenged materials. In Manila's Payatas neighborhood, slum houses are often made of material sourced from a nearby garbage dump.\n\nIn Dakar, it is not uncommon to see houses made of recycled materials standing atop a mixture of garbage and sand which serves as a foundation. The garbage-sand mixture is also used to protect the house from flooding.\n\nBuildings with historical importance have legal restrictions.\n\nNew houses in the UK are not covered by the Sale of Goods Act. When purchasing a new house the buyer has different legal protection than when buying other products. New houses in the UK are covered by a National House Building Council guarantee.\n\nWith the growth of dense settlement, humans designed ways of identifying houses and parcels of land. Individual houses sometimes acquire proper names, and those names may acquire in their turn considerable emotional connotations. For example, the house of \"Howards End\" or the castle of \"Brideshead Revisited\". A more systematic and general approach to identifying houses may use various methods of house numbering.\n\nHumans often build houses for domestic or wild animals, often resembling smaller versions of human domiciles. Familiar animal houses built by humans include birdhouses, henhouses and doghouses, while housed agricultural animals more often live in barns and stables.\n\nHouses may express the circumstances or opinions of their builders or their inhabitants. Thus, a vast and elaborate house may serve as a sign of conspicuous wealth whereas a low-profile house built of recycled materials may indicate support of energy conservation.\n\nHouses of particular historical significance (former residences of the famous, for example, or even just very old houses) may gain a protected status in town planning as examples of built heritage or of streetscape. Commemorative plaques may mark such structures.\n\nHome ownership provides a common measure of prosperity in economics. Contrast the importance of house-destruction, tent dwelling and house rebuilding in the wake of many natural disasters.\n\n\n\n\n\n\n\n"}
{"id": "20078698", "url": "https://en.wikipedia.org/wiki?curid=20078698", "title": "Impulse facility", "text": "Impulse facility\n\nA testing facility that relies on rapid release of stored energy to generate a short period of high enthalpy test conditions for testing of aerodynamic flow, aerodynamic heating and atmospheric reentry, combustion, chemical kinetics, ballistics, and other effects. The rapid release of energy can result in very high instantaneous energy release rates even though the total energy released is modest. This effect also produces short test times, however, with some types of tests in these facilities lasting less than 100 microseconds. Impulse facilities are a special case of blow down facilities where an energy storage mechanism is charged over a period of time and then released to initiate a test and must be charged again before the next test. This contrasts with continuous facilities such as wind tunnels that may run continuously. Examples of impulse facilities are the shock tube, the shock tunnel, the expansion tube, the expansion tunnel, and the Ludwieg tube.\n"}
{"id": "28336776", "url": "https://en.wikipedia.org/wiki?curid=28336776", "title": "Information Technology Industry Council", "text": "Information Technology Industry Council\n\nThe Information Technology Industry Council (ITI) is a Washington, D.C.-based trade association that represents companies from the information and communications technology (ICT) industry. As an advocacy organization, ITI works to influence policy issues aimed at encouraging innovation and promoting global competitiveness.\n\nArs Technica has described the Information Technology Industry Council as \"a lobbying group with a membership list that includes almost all the heavy-hitters of the tech world\". In 2006, \"InformationWeek's Digital Life Weblog\" called it \"a lobbying group of 40 of the most powerful tech firms, including Cisco, Dell, eBay, IBM, Intel, Microsoft, Oracle SAP, and Sun\".\n\nITI works to help shape policy pertaining to tax, trade, talent, security, access, and sustainability issues for its member companies through its three main divisions: Environment and Sustainability, Global Policy, and Government Relations. ITI further supports its members by organizing industry-wide consensus on policy issues and providing access to global markets.\n\nDean C. Garfield is the current President and Chief Executive Officer of ITI.\n\nAccording to its website, it was founded in 1916 in Chicago as the \"National Association of Office Appliance Manufacturers\", renamed the \"Office Equipment Manufacturers Institute\" in 1929, and became the \"Business Equipment Manufacturers Association\" (BEMA) in 1961. In 1973, it became the \"Computer and Business Equipment Manufacturers Association\" (CBEMA), before receiving its current name in 1994.\n\nITI’s Environment and Sustainability division focuses on energy efficiency. Internationally, ITI engages with product stewardship and electronics recycling, product design and materials restrictions, energy efficiency and climate change, and environmentally preferable purchasing.\n\nAccording to the Basel Action Network, a \"non-governmental charitable organization working to combat the export of toxic waste, toxic technology and toxic products from industrialized societies to developing countries,\" the ITI has also proposed exemptions to international regulation of e-waste that \"would allow untested or non-functional electronic waste, often containing toxic lead, cadmium, mercury and brominated flame retardants, to be considered a non-waste and subject to free-trade in many circumstances so long as the exporter can claim that the old equipment might be ‘repairable.’\" \n\nITI’s Environmental Leadership Council is a separate membership entity that represents principal manufacturers of information technology equipment, wireless and consumer electronics devices, and other electronic and high-tech products and systems.\n\nITI promotes the interface between its member companies and policymakers and thought leaders. In order to accomplish this, it facilitates meetings with key policy individuals at ITI, on Capitol Hill, and with the Administration.\n\nITI works to pinpoint annual policy priorities for the ICT industry. The association works to help shape policy development for corporate tax, global market access, health IT, intellectual property, STEM education and workforce policy and telecommunications.\n\nITI’s Global Policy division works to maintain and promote market access for the ICT industry. ITI collaborates with the U.S. Trade Representative, U.S. Department of Commerce, U.S. Department of State, and U.S. embassies abroad to address expanding global market access. ITI also fosters strong relationships with domestic member company representatives and local industry associations to further its international trade policy priorities.\n\nITI’s Global Policy division targets accessibility, technical standards, regulatory compliance, international trade policy, cybersecurity and China policy, as specific issue areas.\n\nITI sponsors the International Committee for Information Technology Standards (INCITS) and has provided funding for the Information Technology and Innovation Foundation (ITIF), a Washington, D.C.-based think tank.\n\nITI member companies include:\n\n"}
{"id": "6095862", "url": "https://en.wikipedia.org/wiki?curid=6095862", "title": "Joseph W. Kennedy", "text": "Joseph W. Kennedy\n\nJoseph William Kennedy (May 30, 1916 – May 5, 1957) was an American chemist who was a co-discoverer of plutonium, along with Glenn T. Seaborg, Edwin McMillan and Arthur Wahl. During World War II he was head of the CM (Chemistry and Metallurgy) Division at the Manhattan Project's Los Alamos laboratory, where he oversaw research onto the chemistry and metallurgy of uranium and plutonium. After the war, he was recruited as a professor at Washington University in St. Louis, where he is credited with transforming a university primarily concerned with undergraduate teaching into one that also boasts strong graduate and research programs. He died of cancer of the stomach at the age of 40.\n\nJoseph William Kennedy was born in Nacogdoches, Texas on May 30, 1916, the son of Joseph and Mattie Kennedy. He also lived in Center, Texas for a period of seven years before entering college. He attended Stephen F. Austin State Teachers College, from which he received a Bachelor of Arts (BA) degree, and the University of Kansas, which awarded him a Master of Arts (MA) degree. He then entered the University of California, Berkeley, where he earned his Doctor of Philosophy (PhD) degree, writing his thesis on \"Studies of nuclear isomerism in tellurium, element 43, and zinc\", under the supervision of George Ernest Gibson.\n\nIn February 1940, Glenn Seaborg and Edwin McMillan produced plutonium-239 through the bombardment of uranium. In their experiments bombarding uranium with deuterons, they observed the creation of neptunium, element 93, which then underwent beta-decay to form a new element, plutonium, with 94 protons. Kennedy built a series of detectors and counters to verify the presence of plutonium. He used mica sliced razor thin to produce a window to count alpha particle emissions, and ionization chamber with a magnetic field to separate the beta particles from the neptunium from alpha particles from the plutonium.\n\nOn March 28, 1941, Seaborg, physicist Emilio Segrè and Kennedy were able to demonstrate not only the presence of plutonium, but that was fissile, an important distinction that was crucial to the decisions made in directing Manhattan Project research. Arthur Wahl then began exploring the chemistry of the newly discovered element. In 1966, Room 307 of Gilman Hall on the campus at the Berkeley, where they did this work, was declared a U.S. National Historic Landmark.\n\nKennedy was one of the early recruits to Manhattan Project's Los Alamos National Laboratory, arriving in March 1943. He became acting head of the Chemistry and Metallurgy (CM) Division. There was concern amongst the project leadership about Kennedy, as he was only 26 years old at the time. An approach was therefore made to Charles Thomas from Monsanto. Thomas agreed to co-ordinate the Chemistry efforts of the different Manhattan Project laboratories, but he did not wish to move to New Mexico. Despite his youth, Kennedy officially became CM Division leader in April 1944.\n\nThe CM Division was responsible for the purification and fabrication of materials for the bomb, including the core, tamper and initiator. The chemistry and metallurgy of uranium was fairly well known, although it did yield a few surprises, but that of plutonium was almost completely unknown. The element had only been discovered a short time before, and existed only in microgram amounts. Educated guesses about its chemistry tended to be wrong, and as research progressed it was found to have unusual properties, including no less than six allotropes. There was rivalry between its discoverers, with Wahl and Kennedy's group at Los Alamos competing with Seaborg's in Chicago to produce the best process for purifying the metal. This competition ended abruptly when Segrè's group at Los Alamos discovered that high levels of a hitherto undiscovered plutonium-240 isotope in reactor-produced plutonium meant that an implosion-type nuclear weapon was required, and a high degree of purity was therefore unnecessary.\n\nKennedy's chemists were able to reduce uranium hydride to uranium-235 metal with 99.96% efficiency, and the metallurgists worked out how to cast and press it into the required shapes. While the chemists worked out how to purify plutonium, the metallurgists had to figure out how to cast it into a solid sphere. Eric Jette's CM-8 (Uranium and Plutonium Metallurgy) group found that they could stabilise plutonium in its malleable δ phase by alloying it with gallium. For his services, he was awarded the Medal for Merit by the President Harry S. Truman in 1946.\n\nIn 1945, Kennedy was recruited as a professor at Washington University in St. Louis, and was installed as Chairman of the Department of Chemistry in 1946, a role he continued in until his death. Kennedy brought with him Wahl, Lindsay Helmholz, David Lipkin, Herbert Potratz, and Samuel Weissman, who all served on the faculty at Washington University. Up to this time, Washington University was primarily concerned with undergraduate teaching. Kennedy is credited with transforming it into a university that also has boasts strong graduate and research programs.\n\nAlong with Seaborg, McMillan and Wahl, Kennedy received $400,000 dollars from the Atomic Energy Commission in compensation for their scientific work. He died on May 5, 1957 at the age of 40 after a battle with cancer of the stomach. The Kennedy Lecture series is named in his honor. It is given every year in Washington University.\n\n\n"}
{"id": "39649787", "url": "https://en.wikipedia.org/wiki?curid=39649787", "title": "Labor burden", "text": "Labor burden\n\nLabor burden is the actual cost of a company to have an employee, aside from the salary the employee earns. Labor burden costs include benefits that a company must, or chooses to, pay for employees included on their payroll. These costs include but are not limited to payroll taxes, pension costs, health insurance, dental insurance, and any other benefits that a company provides an employee.\n\nCompany paid time off such as paid sick, holiday or training time must also be considered as part of the Labor Burden as it is a cost to the company.\n\nLabor burden cost is important to compute and understand because it includes a variety of significant costs that are often viewed as company overhead, but are in fact, costs related to employment. Many businesses fail because they focus simply on payroll and payroll taxes, and neglect to consider the entire actual cost required to enable an employee to perform the work he or she was hired to do.\n\nOverhead should be spread out over all of a company's income-generating employees based on the hours they work (Example: A part-time employee working 20 hours a week will absorb half as much company overhead as would a full-time employee at 40 hours per week.\n\nFully-burdened costs for individual employees can be expressed as a yearly total to provide an estimate of how much the company will spend that year on an employee. It can also be expressed as an hourly cost by dividing the total yearly cost by the number of hours the employee will work. This number is often 50% to 150% higher than the gross hourly wage. As costs are often used as the basis for pricing services or products, this is why it is so critical to obtain an in-depth understanding of the true cost of an employee.\n\n"}
{"id": "28857723", "url": "https://en.wikipedia.org/wiki?curid=28857723", "title": "Lanier W. Phillips", "text": "Lanier W. Phillips\n\nLanier W. Phillips (March 14, 1923 – March 11, 2012) was a survivor of the wreck of the USS \"Truxtun\" off the coast of Newfoundland, a retired oceanographer and a recipient of the U.S. Navy Memorial's Lone Sailor award for his distinguished post Navy civilian career. Phillips was an African American who was raised by sharecroppers in Lithonia, GA and who became the US Navy's first black sonar technician. Phillips died on March 12, 2012, at the Armed Forces Retirement Home in Gulfport, Mississippi.\n\nWhile growing up in the segregated South, Phillips witnessed the terror of the Ku Klux Klan and was taught to fear white people. \"[N]ever look a white man in the eye. ... If you do you'll get a whipping, or maybe lynched,\" his great-grandmother once warned him.\n\nIn order to escape the South, Philips joined the Navy in 1941. Although the Navy was still segregated and blacks were confined to duty as mess attendants, Phillips considered this the lesser of two evils.\n\nOn February 18, 1942 Phillips was aboard the while it was battered by a severe winter storm. Eventually the \"Truxtun\" and the supply ship the were forced onto the rocks of the southeast coast of Newfoundland. Hundreds of men from both ships died, but Phillips was among the survivors.\n\nInitially afraid to leave his doomed ship because he thought he was off the coast of Iceland where he had been told blacks were forbidden to go ashore, Phillips boarded a lifeboat which capsized as it reached land. Exhausted and covered in oil that had leaked from the sinking ships, Phillips collapsed on the beach. Gently prodded to his feet by a local resident who told him he'd freeze to death if he didn't get up, Phillips was confronted by an experience that was totally new to him: \"I had never heard a kind word from a white man in my life.\"\n\nPhillips was taken to a place where the local women were washing oil from the survivors, and when they realized they could not scrub his skin white he was afraid their kind treatment would end. Instead a local woman, Violet Pike, insisted that he come home to her house where she nursed him with soup and put him to bed with blankets and rocks she'd warmed on her wood stove.\n\nProfoundly touched and forever changed by the kindness of the residents of St. Lawrence, Newfoundland, Phillips went on to become the Navy's first black sonar technician and vowed to do everything in his power to repay the kindness he had experienced, eventually donating enough money to St. Lawrence for them to build a children's playground.\n\nAfter giving speeches at schools across the U.S., Phillips was awarded an honorary degree from Memorial University of Newfoundland in 2008 for his efforts to end discrimination. In 2011, Phillips was given honorary membership into the Order of Newfoundland and Labrador for his work in civil rights in the U.S. In 2012 \"Oil and Water\", a play about Lanier's experience in St. Lawrence after the shipwreck and the influence it had on him, was produced by Newfoundland's Artistic Fraud theater company.\n"}
{"id": "47305658", "url": "https://en.wikipedia.org/wiki?curid=47305658", "title": "MD&amp;DI", "text": "MD&amp;DI\n\nMD&DI is a trade magazine for the medical device and diagnostic industry published by UBM Canon (Los Angeles). It includes peer-reviewed articles on specific technology issues and overviews of key business, industry, and regulatory topics. It was established in 1979. In 2009 it had a monthly print circulation of 48,040 but is now an online publication with a claimed circulation of 89,000. UBM Canon and the magazine has also sponsored the Medical Design and Manufacturing (MD&D) West Conference & Exposition (formerly the MD&DI West Conference & Expo), a medical device trade show, since 1978.\n\nThe magazine sponsored the Medical Design Excellence Awards and produces a list of 100 Notable People in the Medical Device Industry. The term \"use error\" was first used in May 1995 in an MD&DI guest editorial, \"The Issue Is 'Use,' Not 'User,' Error,\" by William Hyman.\n"}
{"id": "50263819", "url": "https://en.wikipedia.org/wiki?curid=50263819", "title": "MXCHIP", "text": "MXCHIP\n\nShanghai-Branch Information Technology Co., Ltd. (MXCHIP) is a focus on embedded wireless module products and Internet companies, the company has more than 20 software copyrights and patents in wireless networking protocols and RF technology, in the years of the development process, and the world's top semiconductor manufacturers Broadcom, STMicroelectronics, Foxconn, and so close to launch a series of embedded Wi-Fi, ZigBee, BTLE, NFC and other products for networking applications to provide a complete short-range wireless network access solutions, at present, product has been successfully applied to bulk white goods, telemedicine, smart grid, intelligent transportation and other fields, and services customers worldwide up to more than 800.\n\n2004\nCore Team was established\n2009\nJoin ARM ACC organizations to become a member of the Global Alliance, ARM\n2012\nSigned a strategic cooperation agreement with Haier\n2013\nWon ten million angel round of financing, started 2.0 upgrade; Qing Branch Easylink patented technology to become one of the standard smart devices WiFi access scheme\n2014\nHeld MXCHIP intelligent hardware innovation design contest; released the world's first operating system MICO things; hold the first MICO Union summit\n2015\nIn cooperation with TI, Marvel, Atmel, a strategic extension Banda\n2015\nMICO Developers Conference held the first release MICO2.0 upgraded version, in cooperation with Haier, Hisense, Galanz and other manufacturers, the introduction of double eleven one-day 100W Lynx, Jingdong, Suning and other electronic business platform + shipments.\n"}
{"id": "7589621", "url": "https://en.wikipedia.org/wiki?curid=7589621", "title": "Material selection", "text": "Material selection\n\nMaterial selection is a step in the process of designing any physical object. In the context of product design, the main goal of material selection is to minimize cost while meeting product performance goals. Systematic selection of the best material for a given application begins with properties and costs of candidate materials. For example, a thermal blanket must have poor thermal conductivity in order to minimize heat transfer for a given temperature difference.\n\nSystematic selection for applications requiring multiple criteria is more complex. For example, a rod which should be stiff and light requires a material with high Young's modulus and low density. If the rod will be pulled in tension, the specific modulus, or modulus divided by density formula_1, will determine the best material. But because a plate's bending stiffness scales as its thickness cubed, the best material for a stiff and light plate is determined by the \"cube root\" of stiffness divided by density formula_2. For a stiff beam in bending the material index is formula_3 (please clarify cubed vs. square root difference).\n\nAn Ashby plot, named for Michael Ashby of Cambridge University, is a scatter plot which displays two or more properties of many materials or classes of materials. These plots are useful to compare the ratio between different properties. For the example of the stiff/light part discussed above would have Young's modulus on one axis and density on the other axis, with one data point on the graph for each candidate material. On such a plot, it is easy to find not only the material with the highest stiffness, or that with the lowest density, but that with the best ratio formula_1. Using a log scale on both axes facilitates selection of the material with the best plate stiffness formula_5.\nThe first plot on the right shows density and Young's modulus, in a linear scale. The second plot shows the same materials attributes in a log-log scale. Materials families (polymers, foams, metals, etc.) are identified by colors.\n\nThus as energy prices have increased and technology has improved, automobiles have substituted increasing amounts of lightweight magnesium and aluminium alloys for steel, aircraft are substituting carbon fiber reinforced plastic and titanium alloys for aluminium, and satellites have long been made out of exotic composite materials.\n\nOf course, cost per kg is not the only important factor in material selection. An important concept is 'cost per unit of function'. For example, if the key design objective was the stiffness of a plate of the material, as described in the introductory paragraph above, then the designer would need a material with the optimal combination of density, Young's modulus, and price. Optimizing complex combinations of technical and price properties is a hard process to achieve manually, so rational material selection software is an important tool.\n\nUtilizing an \"Ashby chart\" is a common method for choosing the appropriate material. First, three different sets of variables are identified:\nNext, an equation for the performance index is derived. This equation numerically quantifies how desirable the material will be for a specific situation. By convention, a higher performance index denotes a better material. Lastly, the performance index is plotted on the Ashby chart. Visual inspection reveals the most desirable material.\n\nIn this example, the material will be subject to both tension and bending. Therefore, the optimal material will perform well under both circumstances.\n\nIn the first situation the beam experiences two forces: the weight of gravity formula_6 and tension formula_7. The material variables are density formula_8 and strength formula_9. Assume that the length formula_10 and tension formula_7 are fixed, making them design variables. Lastly the cross sectional area formula_12 is a free variable. The objective in this situation is to minimize the weight formula_6 by choosing a material with the best combination of material variables formula_14. Figure 1 illustrates this loading.\n\nThe stress in the beam is measured as formula_15 whereas weight is described by formula_16. Deriving a performance index requires that all free variables are removed, leaving only design variables and material variables. In this case that means that formula_12 must be removed. The axial stress equation can be rearranged to give formula_18. Substituting this into the weight equation gives formula_19. Next, the material variables and design variables are grouped separately, giving formula_20. \n\nSince both formula_10 and formula_7 are fixed, and since the goal is to minimize formula_6, then the ratio formula_24 should be minimized. By convention, however, the performance index is always a quantity which should be maximized. Therefore, the resulting equation is formula_25\n\nNext, suppose that the material is also subjected to bending forces. The max tensile stress equation of bending is formula_26, where formula_27 is the bending moment, formula_28 is the distance from the neutral axis, and formula_29 is the moment of inertia. This is shown in Figure 2. Using the weight equation above and solving for the free variables, the solution arrived at is formula_30, where formula_10 is the length and formula_32 is the height of the beam. Assuming that formula_32, formula_10, and formula_27 are fixed design variables, the performance index for bending becomes formula_36.\n\nAt this point two performance indices that have been derived: for tension formula_37 and for bending formula_38. The first step is to create a log-log plot and add all known materials in the appropriate locations. However, the performance index equations must be modified before being plotted on the log-log graph.\n\nFor the tension performance equation formula_39, the first step is to take the log of both sides. The resulting equation can be rearranged to give formula_40. Note that this follows the format of formula_41, making it linear on a log-log graph. Similarly, the y-intercept is the log of formula_42. Thus, the fixed value of formula_42 for tension in Figure 3 is 0.1.\n\nThe bending performance equation formula_36 can be treated similarly. Using the power property of logarithms it can be derived that formula_45. The value for formula_42 for bending is ≈ 0.0316 in Figure 3. Finally, both lines are plotted on the Ashby chart.\n\nFirst, the best bending materials can be found by examining which regions are higher on the graph than the formula_38 bending line. In this case, some of the foams (blue) and technical ceramics (pink) are higher than the line. Therefore those would be the best bending materials. In contrast, materials which are far below the line (like metals in the bottom-right of the gray region) would be the worst materials.\n\nLastly, the formula_37 tension line can be used to \"break the tie\" between foams and technical ceramics. Since technical ceramics are the only material which is located higher than the tension line, then the best-performing tension materials are technical ceramics. Therefore, the overall best material is a technical ceramics in the top-left of the pink region such as boron carbide.\n\nThe performance index can then be plotted on the Ashby chart by converting the equation to a log scale. This is done by taking the log of both sides, and plotting it similar to a line with formula_49 being the y-axis intercept. This means that the higher the intercept, the higher the performance of the material. By moving the line up the Ashby chart, the performance index gets higher. Each materials the line passes through, has the performance index listed on the y-axis. So, moving to the top of the chart while still touching a region of material is where the highest performance will be.\n\nAs seen from figure 3 the two lines intercept near the top of the graph at Technical ceramics and Composites. This will give a performance index of 120 for tensile loading and 15 for bending. When taking into consideration the cost of the engineering ceramics, especially because the intercept is around the Boron carbide, this would not be the optimal case. A better case with lower performance index but more cost effective solutions is around the Engineering Composites near CFRP.\n"}
{"id": "3488604", "url": "https://en.wikipedia.org/wiki?curid=3488604", "title": "Melilotus officinalis", "text": "Melilotus officinalis\n\nMelilotus officinalis, known as yellow sweet clover, yellow melilot, ribbed melilot and common melilot is a species of legume native to Eurasia and introduced in North America, Africa and Australia.\n\n\"Melilotus officinalis\" can be an annual or biennial plant, and is high at maturity. Leaves alternate on the stem and possess three leaflets. Yellow flowers bloom in spring and summer and produce fruit in pods typically containing one seed. Seeds can be viable for up to 30 years. Plants have large taproots and tend to grow in groups. Plants have a characteristic sweet odor.\n\n\"M. officinalis\" is native to Europe and Asia and has been introduced to North America as a forage crop. It commonly grows in calcareous loamy and clay soils with a pH above 6.5 and can tolerate cold temperatures and drought; it does not tolerate standing water. Common places where it can be found include open disturbed land, prairies, and savannahs, and it grows in full or partial sunlight. It is an invasive species in areas where it has been introduced, especially in open grasslands and woodlands where it shades and outcompetes native plant species.\n\nSweet clover contains coumarin that converts to dicoumarol, which is a powerful anticoagulant toxin, when the plant becomes moldy. This can lead to bleeding diseases (internal hemorrhaging) and death in cattle. Consequently, hay containing the plant must be properly dried and cured, especially in wet environments.\n\nSweetclover can be used as pasture or livestock feed. It is most palatable in spring and early summer, but livestock may need time to adjust to the bitter taste of coumarin in the plant. Prior to World War II before the common use of commercial agricultural fertilizers, the plant was commonly used as a cover crop to increase nitrogen content and improve subsoil water capacity in poor soils. Sweet clover is a major source of nectar for domestic honey bees as hives near sweetclover can yield up to 200 pounds of honey in a year.\n\nSweetclover has been used as a phytoremediation—phytodegradation plant for treatment of soils contaminated with dioxins.\n\nIn the chemical industry, dicoumarol is extracted from the plant to produce rodenticides.\n\nWhen \"M. officinalis\" is invasive, it can be managed by mulching, hand-pulling, mowing, or herbicide applications such (e.g., 2,4-D) before flowering. Prescribed burns in late fall or early spring followed by another burn in late spring can reduce the number of plants before seed set.\n"}
{"id": "653000", "url": "https://en.wikipedia.org/wiki?curid=653000", "title": "Mercury silvering", "text": "Mercury silvering\n\nMercury silvering or fire gilding is a silvering technique for applying a thin layer of precious metal such as silver or gold (mercury gilding) to a base metal object. The process was invented during the Middle Ages and is documented in Vannoccio Biringuccio's 1540 book \"De la pirotechnia\". An amalgam of mercury and the precious metal is prepared and applied to the object which is then heated, sometimes in oil, vaporizing most of the mercury. The technique is dangerous since mercury is highly toxic, especially in its vapor phase. Mercury silvering can be detected through a variety of methods.\n\nThe technique was also used in Asia, for example \"tokin\" plating in Edo-period Japan.\n\n"}
{"id": "53115925", "url": "https://en.wikipedia.org/wiki?curid=53115925", "title": "Microflotation", "text": "Microflotation\n\nMicroflotation is a further development of standard dissolved air flotation (DAF). Microflotation is a water treatment technology operating with microbubbles of 10–80 μm in size instead of 80-300 μm like conventional DAF units.\n\nThe general operating method of microflotation is similar to standard recycled stream DAF units. The advancements of microflotation are lower pressure operation, smaller footprints and less energy consumption.\n\nThe method of Microflotation is comparable to recycled stream DAF.\nA portion of the clarified effluent water leaving the Microflotation tank is pumped into a small pressure vessel into which compressed air is also introduced. This results in saturating the pressurized effluent water with air. The air-saturated water stream is recycled to the front of the Microflotation cell and flows through a pressure release valve just as it enters the front of the float tank, which results in the air being released in the form of tiny bubbles. Bubbles form at nucleation sites on the surface of the suspended particles, adhering to the particles. As more bubbles form, the lift from the bubbles eventually overcomes the force of gravity. This causes the suspended matter to float to the surface where it forms a froth layer which is then removed by a skimmer. The froth-free water exits the float tank as the clarified effluent from the Microflotation unit. A particular circular DAF system is called \"Zero speed\", allowing quite water status then highest performances; a typical example is a Easyfloat 2K DAF system.\n\nMicroflotation is an enhanced method to float particles to the surface with the aid of adherent air bubbles.\n\nThe adherence of suspended solids to bubbles is easier and more intensive, the smaller the bubbles are. Because of the improved adherence capacity of small microbubbles, the saturation of the introduced air as well as the reduction capability of particles lead to an improved suspended solids reduction, a higher solids content in the float sludge and a more stable float sludge on the surface of the microflotation cell. \nA difference has to be made to dispersed flotation used in mining industry in mineral segregation processes where the bubble are bigger being 500-2000 μm in size and volume of air is many fold compared to the water volume. Traditional Dissolved Air flotation (DAF) mainly operates with bubble sizes ranging from 80-300 μm with very inhomogeneous bubble size distribution.\nA major difference of low pressure dissolved air flotation and other flotation processes lies in the volumes of bubbles, amount of air and raising speeds. One macro bubble can be 1000 times bigger in volume compared to one micro bubble. And vice versa the number of micro bubbles can be 1000 fold in number compared to one macro bubble having same volume.\n\nMicroflotation enables bubbles in size 40-70 μm with rise rates from 3–10 m/h. The rise rate is slow enough not to destroy the fragile flocks forming an agglomeration of particles with weak mutual bonding and high enough to allow time for separation of the agglomeration. With the attachment of particles to bubbles the size range of “flock-bubble” grows, and the rise velocities grow simultaneously. The separation rate is accelerated leading to residence times of combined chemical precipitation and flotation from 10 to 60 minutes with need of small footprint areas of treatment plants and decreasing the cost structures of treatment processes.\n\nA distribution of bubble sizes between 20 and 50 microns is the necessary requirement for an optimum flotation result. Even a small number of bubbles with diameters of above 100 microns can disable a flotation separation process, because larger bubbles rise more quickly and cause turbulence, which severely destroys already build air-flocks-agglomerates.\n\nMicroflotation is technically appropriately and primarily economic to substitute classic technology like sand filtration and sedimentation. Beyond there are several applications at which low pressure Microflotation is an alternative to membrane technology or represents a convincing addition.\n\nMicroflotaion can be used as:\n"}
{"id": "52525745", "url": "https://en.wikipedia.org/wiki?curid=52525745", "title": "Microsoft Classroom", "text": "Microsoft Classroom\n\nMicrosoft Classroom was an online blended learning platform for schools that aims to simplify grading assignments and student communication in a paperless way. It was introduced for Office 365 Education subscribers in April 2016.\n\nOn May 18, 2017 Microsoft announced the retirement of Microsoft Classroom, which was completed on January 31, 2018. Some features of Microsoft Classroom became part of Microsoft Teams in Office 365 Education.\n\n"}
{"id": "18331296", "url": "https://en.wikipedia.org/wiki?curid=18331296", "title": "Milliwatt test", "text": "Milliwatt test\n\nA Milliwatt test (Milliwatt line) is a test method or test facility used in telecommunications to measure line quality and transmission loss between stations or points in an analog telephone system.\n\nThe test consists of transmitting an analog sinusoidal signal at the frequency of 1004 Hz with the power level of 0 (zero) dBm. By definition, this is the equivalent of a continuous power dissipation of 1 mW (milliWatt), i.e., the power consumed if a voltage of 0.775 V(RMS) is applied to a telephone line with 600 Ohm nominal impedance.\n\nIn the Bell System, central offices provided this type of service on a dedicated telephone number (102 type Milliwatt line) for remote subscriber line testing. In conjunction, a second line (type 100 line) provided quiet termination (cf. loop around). Various types of test lines were called \"100\", \"102\", \"104\" etc., because these numbers accessed the test line in tandem offices in lieu of an area code.\n\nIn digital central office installations, the Milliwatt test facility was implemented using a synthesized version of the 1004 Hz signal, see digital milliwatt. +1-(503)-697-1000 is an example of such a Milliwatt from a digital central office.\n\n"}
{"id": "21558274", "url": "https://en.wikipedia.org/wiki?curid=21558274", "title": "Ministry of Mines and Energy (Cambodia)", "text": "Ministry of Mines and Energy (Cambodia)\n\nThe Ministry of Industry, Mining and Energy (\"MIME\") is a government ministry responsible for governing and the mining industry and the energy industry of Cambodia. It is located in Phnom Penh.\n\n\n"}
{"id": "36881292", "url": "https://en.wikipedia.org/wiki?curid=36881292", "title": "Ministry of Oil and Gas (Oman)", "text": "Ministry of Oil and Gas (Oman)\n\nThe Ministry of Oil and Gas (MOG) is the governmental body in the Sultanate of Oman responsible for developing and implementing the government policy for exploiting the oil and gas resources in Oman.\n\nThe current Minister of Oil and Gas is Mohammed bin Hamad Al Rumhi and the current Undersecretary of the Ministry of Oil and Gas is Nasser bin Khamis Al Jashmi.\n\nMOG was originally established as the Ministry of Oil and Minerals and was renamed in the year 1997 as the Ministry of Oil and Gas.\n\nThe competences of MOG are as follows:<ref name=\"Royal Decree No 2/2008\"></ref> \n\nMOG has six General Directorates: \n\n\n"}
{"id": "4241965", "url": "https://en.wikipedia.org/wiki?curid=4241965", "title": "Napkin holder", "text": "Napkin holder\n\nA napkin holder is a device used to hold napkins. A napkin holder can be made from virtually any solid material and is built so that the napkins do not slip from its hold, either by way of sandwiching them between two surfaces, or simply enclosing them on their sides in a horizontal design. Napkin holders range in price and styles from wooden designs to wrought iron or ceramic styles and many others. One iteration of the napkin holder, better known as a napkin dispenser, offers additional functionality with its design: folded napkins are enclosed in a snug metal casing, allowing users to retrieve a single napkin each time they reach into the container; this particular device is usually found in restaurants, diners, and other public eateries, while its simpler—often more aesthetically pleasing—counterpart, the holder, is common to households and classrooms.\nThere is also an item which holds a napkin or serviette in a button hole or the top of a conventional knecktie knot. It is conjectured as a clamp for the corner of a napkin and an hook which hooks into the top of the tie knot.They are most usually in sterling silver and date back to at least edwardian times. Hence often to be found in antique outlets as functional collectors items. Certain \"gentleman's clubs\" include a button hole in a corner of their napkins for direct coupling to an upper shirt button.\n\nNapkin holders, as their name implies, are tools in which napkins are held and stored, most often sandwiched between two surfaces. Among basic holders, there are several kinds, those principally belonging in two categories; vertical and horizontal. While their main function is to hold napkins, napkin holders can also serve to complement decorations, either internally or externally. In addition, the creation of napkin holders by amateur woodsmiths and metalworkers serves as a fairly easy project, and has been touted by do it yourself magazines such as \"Popular Mechanics\" and \"Popular Science\". Even simpler designs have been sold as projects that children can do, the napkin holders in these being made of paper plates and yarn.\n\nNapkin holders are used in many locations, ranging from classrooms to eateries.\n\nThe popularity of napkin holders corresponded with the invention (and popularization) of the paper napkin by the Scott Paper Company in 1930, although cloth napkins had existed–often as handkerchiefs–since Greek and Roman times. Wrought iron napkin holders and rings were a common part of a blacksmith's repertoire during the 19th century as well as other holders and household items. Mechanically made napkin holders have replaced many of those made by hand, as blacksmithing is now primarily an art form, as opposed to a means of creation of household utility items.\n\nVintage napkin holders were an integral part of kitchens during the 1950s, 60s, and 70s. Made in large quantities, these napkin holders were often made of brightly colored plastic, either transparent or solid. Other non-plastic napkin holders were also produced, but in smaller quantities.\n\nThe two main styles of napkin holders, vertical and horizontal, function in similar ways. Vertical napkin holders and some horizontal napkin holders sandwich napkins between two surfaces. In vertical napkin holders, the surfaces tend to be the same size and, often, shape, making the napkin holders symmetrical. In vertical holders that sandwich, however, the bottom is usually around the size of a conventional paper napkin, about by while the top side can be any virtually any shape, as it acts as a paperweight. Within this style, there is variation. Some napkin holders have edges as the weight alone could not keep the napkins down (see photo), while others rely entirely on their weight to secure the napkins. Another type of horizontal holder lacks a weight altogether, and is simply a base and four edges.\n\nThe design of napkin holders is largely based upon preconditions of the space or spaces that the napkin holder will occupy. For example, if limited table space is available, a vertical design may be more practical. However, if conserving space is not an issue, a horizontal napkin holder is advantageous, as well as slightly easier for the subject to access. Color, style, and durability are also based on the environment and the architectural styles of the selected room or rooms. Lastly, price and budget are determining factors of a napkin holder's design, which usually range between 10 and 50 US dollars.\n\nWhile the basic design of napkin holders is consistent, the tools and construction materials will vary based on what material the maker is using.\n\nWhile less common than metal or wrought iron napkin holders, wooden napkin holders are often made as art projects, due to the relative ease of making them, and easy access to the materials needed. Common materials used in wooden napkin holder construction include:\n\nMetal napkin holders may be made from wrought iron, wire, or sheet metal.\n\nThe variant of the napkin holder, the napkin dispenser, is most often seen as a part of restaurant equipment. Napkin dispensers serve in restaurants not only because they are easy to access, but more importantly because they provide diners with autonomy from the restaurant staff, so if a spill or other accident occurs, the diners can clean it up on their own, which frees the burden from waiters and other busy members of the staff.\n\n\n"}
{"id": "37597038", "url": "https://en.wikipedia.org/wiki?curid=37597038", "title": "ORCA (computer system)", "text": "ORCA (computer system)\n\nORCA was a mobile-optimized web application used as a component of the \"get out the vote\" (GOTV) efforts for Mitt Romney's 2012 presidential campaign. It was intended to enable volunteers in polling stations around the country to report which voters had turned out, so that \"missing\" Republican voters and underperforming precincts could be targeted for last-minute efforts to get voters to the polls. According to Romney himself, it would provide an \"unprecedented advantage\" to the campaign to \"ensure that every last supporter makes it to the polls.\"\n\nThe system had major technical problems during Election Day that prevented many volunteers from using it. It crashed periodically and at one point was intentionally taken down when a surge of traffic from campaign volunteers was misinterpreted as a denial of service attack. Frustrated volunteers reported being unable to access ORCA and criticised a lack of prior briefing, misleading instructions and patchy on-the-day support. A Romney aide commented that \"Orca is lying on the beach with a harpoon in it.\" The system's failings have been attributed by technology writers to a combination of factors including not doing prior quality assurance or beta testing, inadequate documentation and poor design.\n\nThe Romney campaign subsequently defended ORCA as a success, though campaign officials admitted that the system \"had its challenges\". Conservative activists and writers blamed ORCA for depressing Republican turnout on election day. While political scientists have rebutted these claims, suggested that it probably did not have a decisive effect on the outcome, it may have negatively affected turnout figures. ORCA has been compared unfavorably with a \"get out the vote\" and data effort from President Obama, including Project Narwhal, seen as more robust.\n\nIn the 2008 US presidential election, the Obama campaign utilized a system called Houdini to enable volunteers to report voting data to a national hotline. While this system encountered problems, the 2012 Romney campaign's ORCA system aimed to go further by enabling volunteers to report such data to campaign headquarters in real time via their smartphones. It was intended to be rolled out to around 37,000 volunteers at polling places in swing states.\n\nGail Gitcho, the Romney campaign's communications director, told PBS on November 5 that with the deployment of ORCA on election day, the campaign would be able to tell who had voted in which precincts. She described the system's key function as not being to predict the outcome, but to identify low turnouts in target precincts so that the campaign could take action by contacting missing voters and urging them to go to the polls. Gitcho commented: \"The Obama campaign likes to brag about their ground operation, but it's nothing compared to this.\" The name ORCA was chosen to reference the Obama GOTV system, called Project Narwhal; in nature, the orca or killer whale is the only known non-human predator of narwhals.\n\nIn a training call for Republican volunteers on October 31, they were told: \"There's nothing that the Obama data team, there's nothing that the Obama campaign, there's nothing that President Obama himself can do to even come close to what we are putting together here.\" According to reports, \"The governor [Romney] loves seeing data, he loves seeing numbers and he's a very strategic person; he's a very smart man. So he actually loves being inside these war rooms, seeing the data come in and seeing exactly what's going on out there, so we can all put our heads together and say, 'Okay, we need to move resources here. We need to shift resources from here.'\" The campaign's spokeswoman Andrea Saul told the \"Huffington Post\" that ORCA would provide Romney with \"an enormous advantage ... By knowing the current results of a state, we can continue to adjust and micro target our get-out-the-vote efforts to ensure a Romney victory.\"\n\nAccording to the campaign, ORCA would identify how between 18 and 23 million people had voted on election day, providing \"the most accurate ballot projections ever\" and ensuring \"hyper-accuracy of our supporter targeting as we work to turn them out to the polls.\" In a pre-election video, Romney told volunteers: \"As part of this task force, you'll be the key link in providing critical, real-time information to me and to the staff so that we can ensure that every last supporter makes it to the polls. With state-of-the-art technology, and an extremely dedicated group of volunteers, our campaign will have an unprecedented advantage on Election Day.\"\n\nThe Obama campaign declined to comment on ORCA but Scott Goodstein, the external online director for the 2008 Obama campaign, questioned whether it would actually make much difference to potential voters who were sitting out the election. He commented, \"In a national campaign, what additional things are the headquarters really going to do to move resources? Will an additional auto-call last minute really make a difference in a market like Northeast Ohio, which has been saturated for three months full of auto-calls?\" Some pro-Democrat bloggers expressed concerns that the system would facilitate voter suppression but the ORCA training material emphasized that Romney volunteers should under no circumstances talk to or confront voters.\n\nThe system was initially reported to have been developed by an application consulting firm and Microsoft, but later reports attributed its development to \"an internal 'skunkworks'\" comprising \"a makeshift team of IT people and volunteers\", rather than an outside consultancy, while a number of small consulting companies helped with the implementation of ORCA during election day. According to campaign insiders, it was kept secret among \"a close circle in Boston\" and state officials were not informed of how it would operate until only a few days before the election. Volunteer users of ORCA in Boston were given no hands-on training until the day of the election itself, when the system was turned on at 6 am.\n\nORCA was conceived by Rich Beeson, the campaign's political director, and Dan Centinello, Romney's director of voter contact. Centinello served as the political manager of the Orca project.\n\nORCA spending represented only a small portion of the campaign's overall investment in information technology.\n\nORCA was designed to work on a variety of devices, including iPhones and iPads, Android phones and tablets and BlackBerry phones.\n\nThe system was designed to show the names and addresses of every eligible voter in a particular precinct. When the voter had gone through the polling station, a logged-in volunteer would simply slide a bar on their phone screen to note that fact. If there were any problems in the polling station, such as erroneous voting lists, illegal activities or issues with the voting machines, they could press an on-screen yellow button to send an alert to the campaign's lawyers. A Twitter-style instant messenger system would also enable volunteers and the campaign to share information in real time. Those without smartphones were also catered for; the Romney campaign would provide a list of voters to enable volunteers to check off individual names and phone the information into the campaign's headquarters. The data they gathered would be monitored by 800 volunteers at campaign headquarters on the floor of TD Garden in Boston via a Web-based application; it would be used to coordinate contacts throughout election day to pro-Romney voters who had not shown up at the polls. As a fallback, a voice response system would also be established, to allow mobile phone users to call in information if the online system was not working.\n\nORCA consisted of 11 back-end database servers and a single web server and application server providing the front end. The servers were said to have all been hosted in Boston.\n\nThroughout election day, volunteers experienced frequent and widespread problems using ORCA, which crashed periodically. As volunteers tried to log in, the surge of traffic caused the system to collapse altogether for about an hour and a half, leading to scenes of panic among Romney staffers at the TD Garden. The Romney campaign's digital director, Zac Moffatt, conceded: \"The Garden definitely kind of buckled under the strain. The system wasn't ready for the amount of information incoming.\" The traffic surge was so great that at one point Comcast, the campaign's Internet Service Provider, shut off its network connection in the belief that it was coming under a denial of service attack. Reporter Erin McPike tweeted that some suspected that the system had been hacked and that Republican sources had confirmed to her that something had gone wrong. One Romney aide commented that \"ORCA is lying on the beach with a harpoon in it.\"\n\nJohn Ekdahl Jr., a Romney volunteer and web developer in Jacksonville, Florida, wrote a widely discussed account of his experiences with ORCA. The Romney campaign had sent him a 60-page document listing voters and instructions the day before the election, which he struggled to print, but when he reached his local polling station on election day he was told that he needed a certificate to be allowed to work there. The certification was not mentioned in his documentation and his attempts to reach campaign headquarters got nowhere, causing him to give up by 2 pm. Calling ORCA \"an unmitigated disaster,\" Ekdahl said that he was \"hearing almost universal condemnation of the thing. It seemed like the basic coordination between ground ops and overall team was lacking.\" Ekdahl also called the training manuals vague and uninformative; ORCA was regularly described as an \"app\", leading to volunteers looking unsuccessfully for it on the iOS App Store and Google Play. In fact, it was a web application, a mobile-enabled website that did not require additional software to use. The training materials were also riddled with errors such as duplicate checklist items and erroneous responses to frequently asked questions. ORCA exclusively used an HTTP Secure (HTTPS) connection but its designers had apparently forgotten to redirect those attempting to use the equivalent HTTP address to the HTTPS address. Anyone who incorrectly typed in an address beginning with \"www\" was unable to reach the system, causing many volunteers to assume that it was down.\n\nOther volunteers reported being unable to get through to technical support and found themselves receiving either a busy signal or a \"try again later\" message. One volunteer wrote on a Romney campaign message board: \"I have called the ORCA helpline. It was supposed to be live at 5 a.m. ... still getting a recording. Com [\"sic\"] on Boston we can't help Mitt if you won't help us.!!!!!\" Many volunteers could not get their security PINs to work. According to a campaign official in Colorado, \"we were called by hundreds (or more) volunteers who couldn't use the app or the backup phone system. The usernames and passwords were wrong, but the reset password tool didn't work, and we couldn't change phone PINs. We were told the problems were limited and asked to project confidence, have people use pencil and paper, and try to submit again later. Then at 6 p.m. they admitted they had issued the wrong PINs to every volunteer in Colorado, and reissued new PINS (which also didn't work).\" In North Carolina, another campaign official said that \"the system went down for a half hour during peak voting, but for hundreds or more, it never worked all day... Many members of our phone bank got up and left.\" One frustrated volunteer tweeted that it was \"a clusterf**k of biblical proportions.\"\n\nEkdahl described the effect of ORCA as being that \"30,000+ of the most active and fired-up volunteers were wandering around confused and frustrated when they could have been doing anything else to help, like driving people to the polls, phone-banking, walking door-to-door, etc.\" Campaign workers were left \"flying blind\", as several put it, unable to identify non-voters or precincts which needed a last-minute robocalling campaign to drive up turnout. The targeted information promised by the campaign did not materialize and only the generic raw vote tallies were available in key areas. According to the \"Washington Examiner\", by late afternoon on election day ORCA was still predicting a Romney victory with somewhere between 290–300 electoral college votes – nearly 100 more than Romney actually received. A Romney campaign official told ABC News, which predicted on the eve of the election that Obama would win by a 50%–47% margin, \"Your numbers don't matter to us.\" Without accurate information from ORCA, Republican officials instead turned to using public news sources or calling counties for information on the outcome of votes in those areas.\n\nMoffatt acknowledged that \"without a doubt, ORCA had its challenges\" but argued that the system had actually worked, despite the reports of problems: \"We don't think Orca's problems had a material impact on the campaign, it was not election determinative. We had 30,000 plus volunteers across the country putting information into the system. We had 91 percent of all counties report into the system, 14.3 million voters were accounted for as having voted, and we received 5,397 reports on voting issues, such as instances where they ran out of ballots. The information came in, so you can't say it didn't work. You run into issues because it's so massive in scale.\" He noted that the Romney campaign had had only six months to develop its system, whereas Obama had the benefit of six years of preparation. Another Romney aide told \"National Review's\" Katrina Trinko that in fact ORCA's problems had \"no relation to the outcome. We achieved in a large part what we set out to do in the swing states in terms of our electorate. The reality is the President did what he said he was going to do. The Obama campaign said that they were going to increase turnout from 2008, and they were able to do that. And that had nothing to do with a reporting system on Election Day.\"\n\nConservative writer Joel B. Pollak suggested that ORCA had ended up suppressing Romney's own vote by tying up campaign volunteers at a critical time. He noted the narrow margin in the key swing states – only some 500,000 to 700,000 votes – and calculated that if each of the 37,000 ORCA volunteers had brought 20 voters to the polls in those states, the gap could have been closed. Ekdahl saw a \"bitter irony\" in the fact that \"a supposedly small government candidate gutted the local structure of GOTV efforts in favor of a centralized, faceless organization in a far off place (in this case, their Boston headquarters).\" Erick Erickson of RedState.com compared the system to Shamu, a 1970s SeaWorld orca, \"because it bit the leg of the campaign and wouldn't let go.\" While political scientists doubted that its failure had made much of a difference to the outcome, they suggested that if it had worked properly it could have resulted in a closer election. Lara Brown of Villanova University said that it was likely that ORCA had \"had a substantial effect\" on the turnout, particularly in rural counties of Ohio where Romney had underperformed.\n\nThe ORCA system had not received extensive beta testing before election day, nor did the campaign know how it would interact with the data infrastructure in the TD Garden until the day itself. Ekdahl said that he had raised concerns about the lack of testing beforehand. He had asked whether it had been stress tested, whether redundancy had been put in place and whether steps had been taken to combat an external attack on the system, but \"these types of questions were brushed aside (truth be told, they never took one of my questions). They assured us that the system had been relentlessly tested and would be a tremendous success.\" Moffatt admitted that the system had been \"beta-tested in a different environment ... There was so much data coming in – 1200 records or more per minute – it shut down the system for a time. Users were frustrated by lag, and some people dropped off and we experienced attrition as a result.\"\n\nRobert X. Cringely, writing in InfoWorld, concluded that \"everything in the Orca rollout went great, except for a failure to do any quality assurance, proof its documentation, or beta test in the seven months from conception to implementation. Whoever was behind Orca apparently also failed to hire a competent Web designer, anticipate server loads, beef up its bandwidth, or notify its ISP to expect a bump in traffic.\" Sean Gallagher of Ars Technica commented that the key failure was the dependency on automated testing rigs, which \"can't show what the system's performance will look like to the end user. And whatever testing environment Romney's campaign team and IT consultants used, it wasn't one that mimicked the conditions of Election Day. As a result, Orca's launch on Election Day was essentially a beta test of the software – not something most IT organizations would do in such a high-stakes environment.\"\n\n\"Slate\" writer Sasha Issenberg argued that the problems ran far deeper than ORCA's technical failings, as the Romney campaign had been left behind by the cutting edge of data science. He noted that while a system like ORCA could not have changed the demographics, data science did make a great difference to the ability of the two campaigns to target and mobilize their voters. As he put it, \"The Democrats have it and the Republicans don't.\" He suggested that ORCA's ability to affect the outcome had been over-hyped by the Romney campaign, as there was only so much that could be done on election day itself: \"On short notice, you can send robocalls, reorder a call list and employ paid phone banks, but you are not radically changing the shape of the electorate. They acted like they had invented the wheel, but really all it would have been was a slightly better tread on the tire.\"\n\n\n"}
{"id": "46504825", "url": "https://en.wikipedia.org/wiki?curid=46504825", "title": "Open Letter on Artificial Intelligence", "text": "Open Letter on Artificial Intelligence\n\nIn January 2015, Stephen Hawking, Elon Musk, and dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI. The letter affirmed that society can reap great potential benefits from artificial intelligence, but called for concrete research on how to prevent certain potential \"pitfalls\": artificial intelligence has the potential to eradicate disease and poverty, but researchers must not create something which cannot be controlled. The four-paragraph letter, titled \"Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter\", lays out detailed research priorities in an accompanying twelve-page document.\n\nBy 2014, both physicist Stephen Hawking and business magnate Elon Musk had publicly voiced the opinion that superhuman artificial intelligence could provide incalculable benefits, but also can end the human race if deployed incautiously (see Existential risk from advanced artificial intelligence). Hawking and Musk both sit on the scientific advisory board for the Future of Life Institute, an organisation working to \"mitigate existential risks facing humanity\". The institute drafted an open letter directed to the broader AI research community, and circulated it to the attendees of its first conference in Puerto Rico during the first weekend of 2015. The letter was made public on January 12.\n\nThe letter highlights both the positive and negative effects of artificial intelligence. According to Bloomberg Business, Professor Max Tegmark of MIT circulated the letter in order to find common ground between signatories who consider superintelligent AI a significant existential risk, and signatories such as Professor Oren Etzioni, who believe the AI field was being \"impugned\" by a one-sided media focus on the alleged risks. The letter contends that:\n\nThe potential benefits (of AI) are huge, since everything that civilization has to offer is a product of human intelligence; we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable. Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls.\n\nOne of the signatories, Professor Bart Selman of Cornell University, said the purpose is to get AI researchers and developers to pay more attention to AI safety. In addition, for policymakers and the general public, the letter is meant to be informative but not alarmist. Another signatory, Professor Francesca Rossi, stated that \"I think it's very important that everybody knows that AI researchers are seriously thinking about these concerns and ethical issues\".\n\nThe signatories ask: How can engineers create AI systems that are beneficial to society, and that are robust? Humans need to remain in control of AI; our AI systems must \"do what we want them to do\". The required research is interdisciplinary, drawing from areas ranging from economics and law to various branches of computer science, such as computer security and formal verification. Challenges that arise are divided into verification (\"Did I build the system right?\"), validity (\"Did I build the right system?\"), security, and control (\"OK, I built the system wrong, can I fix it?\")\n\nSome near-term concerns relate to autonomous vehicles, from civilian drones and self-driving cars. For example, a self-driving car may, in an emergency, have to decide between a small risk of a major accident and a large probability of a small accident. Other concerns relate to lethal intelligent autonomous weapons: Should they be banned? If so, how should 'autonomy' be precisely defined? If not, how should culpability for any misuse or malfunction be apportioned?\n\nOther issues include privacy concerns as AI becomes increasingly able to interpret large surveillance datasets, and how to best manage the economic impact of jobs displaced by AI.\n\nThe document closes by echoing Microsoft research director Eric Horvitz's concerns that:\n\nwe could one day lose control of AI systems via the rise of superintelligences that do not act in accordance with human wishes – and that such powerful systems would threaten humanity. Are such dystopic outcomes possible? If so, how might these situations arise? ...What kind of investments in research should be made to better understand and to address the possibility of the rise of a dangerous superintelligence or the occurrence of an \"intelligence explosion\"?\n\nExisting tools for harnessing AI, such as reinforcement learning and simple utility functions, are inadequate to solve this; therefore more research is necessary to find and validate a robust solution to the \"control problem\".\n\nSignatories include physicist Stephen Hawking, business magnate Elon Musk, the co-founders of DeepMind, Vicarious, Google's director of research Peter Norvig, Professor Stuart J. Russell of the University of California Berkeley, and other AI experts, robot makers, programmers, and ethicists. The original signatory count was over 150 people, including academics from Cambridge, Oxford, Stanford, Harvard, and MIT.\n\n"}
{"id": "2274628", "url": "https://en.wikipedia.org/wiki?curid=2274628", "title": "Optacon", "text": "Optacon\n\nThe Optacon (OPtical to TActile CONverter) is an electromechanical device that enables blind people to read printed material that has not been transcribed into Braille.\n\nThe Optacon consists of a main electronics unit about the size of a portable tape recorder connected by a thin cable to a camera module about the size of a penknife (See Fig. 1).\n\nThe main electronics unit contains a \"tactile array\" onto which the blind person places their index finger. The Optacon user moves the camera module across a line of print, and an image of an area about the size of a letterspace is transmitted via the connecting cable to the main electronics unit. The tactile array in the main electronics unit contains a 24-by-6 matrix of tiny metal rods, each of which can be independently vibrated by a piezoelectric reed connected to it. Rods are vibrated that correspond to black parts of the image, thus forming a tactile image of the letter being viewed by the camera module. As the user moves the lens module along the print line, tactile images of print letters are felt moving across the array of rods under the user's finger. The Optacon includes a knob to adjust the intensity at which the tactile array rods vibrate, a knob to set the image threshold between white and black needed to turn on the vibration of the rods in the tactile array, and a switch that determines whether images will be interpreted as dark print on a light background or as light print on a dark background.\n\nLyle Thume, an Optacon user and director of blind rehabilitation services at the Rehabilitation Institute in Detroit summed up the Optacon this way in 1973: \"It opens up a whole new world to blind people. They aren't restricted anymore to reading material set in braille.\"\n\nThe Optacon was the brainchild of John Linvill, a professor of Electrical Engineering at Stanford University, who later became head of the Electrical Engineering Department. The Optacon was developed with researchers at Stanford Research Institute (now SRI International). Linvill was one of Telesensory's founders and Chairman of the Telesensory Board. The initial stimulus for development of the Optacon was Linvill's daughter, Candy (born 1952, blind since the age of 3). Using the Optacon, Candy graduated from Stanford and received a PhD. She has worked as a clinical psychologist since, so, like her father, she is often referred to in the press as \"Dr. Linvill\".\n\nIn 1962, during a sabbatical year in Switzerland, Linvill visited an IBM laboratory in Germany, where he observed a high speed printer that used a set of small pins—like hammers—to print letters onto strips of paper. He thought, \"If you could feel the hammers with your fingertip, you could surely recognize the image.\" So on our return to Zurich, I told my wife and son and daughter, Candy, who was blind: \"Guys, I've got the most magnificent idea. We'll make something that will let Candy read ordinary printed material.\" And although his family laughed at this notion, \"oh, that'll never work!\" the idea for the Optacon was born.\n\nUpon returning to Stanford, Linvill, together with graduate students G.J. Alonzo and John Hill, developed the concept further with the support of the Office of Naval Research. A key aspect of Linvill's concept was to use vibrating piezoelectric reeds, called bimorphs, to move the pins in a two-dimensional array to produce tactile images. The idea of using vibrating bimorphs was critical for several reasons:\n\nIn 1964 Linvill applied for a patent, and U.S. Patent 3,229,387 was granted in January 1966.\n\nAmazingly, in 1913 a reading machine for the blind, called the optophone, was built by Fournier d’Albe in England. It used selenium photosensors to detect black print and convert it into an audible output which could be interpreted by a blind person. Only a few units were built and reading was exceedingly slow. In 1943, Vannevar Bush and Caryl Haskins of the wartime Office of Scientific Research and Development directed resources toward the development of technologies to assist wounded veterans. The Battelle Institute was provided with funding to develop an improved Optophone and Haskins Laboratories was funded to conduct research toward a synthetic speech reading machine. This group turned “sour” on the Optophone approach after concluding that reading would be too slow.\n\nIn 1957 U.S. Veteran's Administration, Prosthetic and Sensory Aids Service (PSAS), under Dr. Eugene Murphy, began funding the development of a reading machine for the blind. The principal investigator on this project was Hans Mauch, a German scientist brought to the U.S. after World War II. (During World War II Mauch worked for the German Air Ministry as part of the German V-1 missile development team.)\n\nMauch worked on reading machines having an “optophone-like” output, a “speech-like” sound output, and a synthetic speech output. The only one of these that was competitive to the Optacon development was the Stereotoner, basically an improved optophone. The Stereotoner design concept was that the user would move a vertical array of photosensors across a line of text. Each photosensor would send its signal to an audio oscillator set to a different frequency, with the top photosensor driving the highest frequency and the bottom photosensor driving the lowest frequency. The user would then hear tones and chords from which the letters could be identified.\n\nInitially Linvill was unaware that the Optacon was not the only reading machine for blind people under development. However, in 1961 James Bliss had returned to SRI from MIT where he had done a doctoral dissertation in a group working on the application of technology for the problems of blindness. Bliss was interested in basic research on the tactile sense, to better understand how it could be used to substitute for loss of vision. While at MIT, Bliss became aware of the existing research and development on reading machines for the blind, as well as the researchers and funding agencies. At SRI Bliss had obtained funding for his tactile research from the Department of Defense and NASA, who were interested in tactile displays for pilots and astronauts. This had enabled him to obtain a small computer and develop software to drive hundreds of tactile stimulators he had developed for research purposes. These tactile stimulators were small air jets, which were ideal for research because their arrangement and spacing could easily be changed and the contact to the skin was always assured. Bliss was studying how well subjects could recognize dynamic patterns presented on his array of air jet stimulators.\n\nAfter Linvill and Bliss decided to join forces to work on Linvill's vision of a reading machine, it became apparent that they needed to obtain funding for this objective, rather than the objectives of Department of Defense and NASA which had provided the funding up until that time. As a start, Bliss suggested that they visit Dr. Murphy at the VA, since he was the only then currently active government source of reading machine funding. However, Bliss knew that the research on “Optophone-like” reading machines had created negativity toward this “direct translation” approach because of the slow reading rates obtained. To counter this negativity, Bliss programmed an SRI computer to present text in a moving belt display, similar to that in Times Square New York City, on both his air jet stimulator array and on the Stanford bimorph array. Linvill's blind daughter, Candy, was then the subject who attempted to learn to read the text presented in this fashion. After several hours of training and practice, Candy was reading in excess of 30 words per minute. Bliss and Linvill felt this computer driven test was a valid simulation of the reading machine they proposed to develop. They felt the 30 words per minute reading rate achieved in a short time by Candy proved that if such a reading machine were developed, it would be useful. They didn’t know what the upper limit of reading speed would be, but had hopes that 100 words per minute could be achieved, since this was typical Braille reading rate.\n\nArmed with this result, Bliss and Linvill made an appointment to visit Dr. Murphy in Washington, D.C. Initially the meeting was going very well, with Dr. Murphy seeming to be very positive toward the possibility of funding the development. Murphy then mentioned that Linvill would have to assign his patent to the Veterans Administration. Linvill refused and the meeting abruptly ended.\n\nAs it turned out, this rejection was fortunate. The Office of Education was directed by a colleague of Linvill's from when he worked at Bell Laboratories. Development of a reading aid for the blind was very relevant to their mission since providing instructional material to blind mainstreamed students was an important problem. Linvill presented the Optacon idea to the Office of Education and it was enthusiastically received. This led to funding at a higher level (over $1.8 million of 1970 dollars over 4 years) than would have been likely from the Veterans Administration.\n\nThis higher level of funding was necessary to develop the custom integrated circuits that enabled the Optacon's small size, which was critical to its success. The Optacon project also assisted Stanford in establishing their Integrated Circuits facilities, leading MIT's Dean of Engineering to remark that Stanford got the lead in integrated circuit research because of the Optacon.\n\nWith funding established, Bliss joined the Stanford faculty half-time, the other half being at SRI. At SRI tactile reading experiments were conducted to maximize the reading rates achievable with the Optacon, as well as development of the bimorph tactile array and the optics for the camera. At Stanford custom integrated circuits were developed including the silicon retina and the drivers for the bimorphs, since they required a higher voltage than normal for solid state circuits at that time.\n\nThe first technical challenge toward developing the reading machine was how to build a \"tactile screen\" that could create a dynamic tactile image which was perceivable by the user and that had a refresh rate fast enough for useful reading rates. Linvill's initial work with graduate students Alonzo and Hill indicated that a piezoelectric bimorph could be suitable as the transducer to convert an electrical signal into a mechanical motion. The advantages of bimorphs were efficient transduction of electrical to mechanical energy (important for battery operation), small size, fast response, and relatively low cost.\n\nAlonzo determined that at vibration frequencies around 300 Hz, the amplitude needed for detection was much less than for frequencies around 60 Hz. Moreover, for reading rates of 100 words per minute, vibration rates of at least 200 Hz were needed. Linvill calculated the length, width, and thickness of a bimorph reed necessary for a resonance frequency of 200 Hz that could produce enough mechanical energy to stimulate a fingertip above the threshold of the sense of touch.\n\nBased on these calculations, an array of bimorphs was constructed for reading rate tests with the computer simulation at SRI. The computer simulation presented tactile images of perfectly formed and aligned letters in a stream that moved across the bimorph array. Candy Linvill and other blind subjects learned to read text presented in this fashion with encouraging results. However, this simulation differed from the conditions that the user would encounter with an Optacon in the real world. There would be a wide range of type fonts and print qualities, plus the user would have to move the camera across the text rather than the computer moving the text across the tactile screen at a fixed rate. It wasn’t known how much the mental load of controlling the camera would reduce the reading rate.\n\nIn considering the transition from the text being presented by the computer to the user moving a camera across a printed page, Bliss realized that there was a critical flaw in the design of the Veteran Administration Stereotoner. Since English alphabetic characters can be adequately displayed with 12 vertical pixels, the Stereotoner designer had assumed that only 12 photocells would be needed in the camera. However, this assumes perfect alignment between the camera and the printed text, which is never the case with a hand held camera. When the alignment is random, as with a hand held camera, a well known engineering theorem states that twice as many pixels are needed. Therefore, the Optacon was designed with 24 vertical pixels instead of 12. This theorem isn’t applicable in the horizontal dimension, so the columns in a two dimensional array can be twice as far apart as the rows.\n\nWhen a single column of 24 pixels is scanned across a line of text, all of the information is acquired. However, with the sense of touch, people are capable of perceiving two dimensional images. Bliss wondered if the reading rate would be higher if more than one column of 24 pixels were used, and if so, how many columns would be appropriate? Experiments with the computer simulation determined that reading rate increased dramatically up to 6 columns, which was a window width of about one letter space and this was about the maximum number of columns that could be placed on one finger. Jon Taenzer, one of Bliss’ Stanford graduate students, ran visual reading experiments on the same computer simulation and determined that for visual reading, reading rates continued to increase up to a window width of up to about 6 letter spaces. This led to a number of experiments toward trying to increase the tactile reading rate by increasing the number of columns in the tactile screen so more than one letter would be in view at a time. Instead of moving the text across only the index fingertip, tests were run with a screen wide enough for both the index finger and the middle finger to be used so two letters could be simultaneously tactually sensed. In another experiment the moving belt of text was run down the length of the fingers, rather than across them. The only approach that showed any promise of increasing the reading rate was when both index fingers were used, rather than the index finger and the adjacent middle finger. However, the use of both index fingers was incompatible with the design concept of using one hand to control the camera while the other hand sensed the tactile screen. The Optacon design was therefore based on an array of 24-by-6 pixels in both the camera retina and bimorph array.\n\nOther questions had to do with the spacing between the tactile pins in the bimorph array and their frequency of vibration. It was well known from experiments reported in the literature that people could distinguish two points from one with their index finger when they were a millimeter apart. However these previous experiments had not been done with vibrating pins. What effect would the vibration have and was there an optimum vibration frequency? These questions were answered by experiments conducted by Charles Rogers, a Stanford graduate student working with Bliss.\n\nWhile the neurophysiological data suggested that the smallest two point thresholds would be at vibration frequencies less than 60 Hertz, Roger's experiments showed that the two point thresholds around 200 Hertz were actually smaller. Bliss hosted a conference at SRI, including some leading neurophysiologists and psychophysicists, to try to resolve this discrepancy, but no one had an explanation. From a practical standpoint, Roger's result was very fortunate because the higher frequencies were required for refresh rates fast enough for reading up to 100 words per minute and for use of bimorphs small enough to construct a 24-by-6 array that fit on a fingertip.\n\nThe question of whether 144 tactile stimulators on a fingertip could be independently distinguished led to a confrontation at a scientific conference between Bliss and Frank Geldard, a University of Virginia professor. Geldard had written a major book on the human senses and was a leading researcher on using the sense of touch to communicate information. When asked how many tactile stimulators should be used in a tactile display, he maintained that no more than 8 tactile stimulators could be independently distinguished, and these should be on widely separated parts of the body. Bliss’ data showing useful reading with 144 stimulators on a fingertip appeared to be in conflict with Geldard's research. The difference was between communicating using two dimensional tactile images versus an 8-point code. Both Bliss and Geldard were reporting similar reading rates, but in the days before high accuracy optical character recognition, the Optacon approach was much more practical.\n\nThese experiments determined the design parameters for Optacon's man-machine interface: a 24-by-6 array of tactile stimulators, vibrating between 250 and 300 Hz, and with the rows spaced at 1 mm and the columns spaced at 2 mm (See Fig. 2).\n\nIn parallel with this human factors research was a pioneering effort to realize this design in a convenient portable unit, which would be critical for its success. In July 1972, Harry Garland suggested a new design for the Optacon that incorporated the sensor, tactile array, and electronics in a single hand-held unit. Roger Melen and Max Maginness developed a prototype of the unit, called the \"one-hand\" Optacon, at Stanford University.\n\nIn the 1960s, when the Optacon was being developed, integrated circuitry was in its infancy, and no suitable integrated solid state arrays of photo detectors was available. The earliest complete Optacon-like reading aids were built at Stanford and SRI with a lens system that focused the images from the printed page on a fiber optic bundle with individual fibers connected to discrete phototransistors. Not only was this system large and bulky, it was expensive and difficult to assemble. An effort was launched to develop a monolithic silicon retina with an array of 24-by-6 phototransistors about the size of one letter space so simple optics with no magnification could be used. Basic research in integrated circuit technology available at that time had to be conducted, resulting in Ph.D. theses by several Stanford graduate students, including J. S. Brugler, J. D Plummer, R. D. Melen, and P. Salsbury. The phototransistors had to be sufficiently sensitive, fast enough for the required refresh rate, have a spectral response appropriate for detecting ink on paper, in a closely packed matrix without blind spots, and interconnected so only connections to the rows and columns were needed.\n\nThe successful fabrication of such a silicon retina was a major milestone toward a practical Optacon.\n\nThe first Optacon prototype using this retina was completed on September 1, 1969. It was portable and completely self-contained in that it combined the stimulator array, electronics, batteries, and camera in a single package measuring 13.5″ by 8″ by 2.25″. The total weight was 9 pounds. The low power electronics design in this unit was a joint effort by J. S. Brugler and W. T. Young which made possible about 12 hours of sustained operation from the rechargeable batteries. This unit included an improved optical system and camera plus a tactile bimorph driven screen, both developed by James Baer and John Gill at SRI.\n\nAs integrated circuit technology progressed, another custom integrated circuit was developed in the Stanford laboratories. This integrated circuit contained 12 bimorph drivers and interfaced between the 5 Volt circuitry and the 45 Volts required to drive the bimorphs. The incorporation of this circuit and the use of lower power components enabled the size to be reduced to 8″ by 6″ by 2″ and the weight to be reduced to four pounds. Again the team of Brugler, Young, Baer, and Gill were responsible for the design of electronics, optics, and packaging. The first Optacon incorporating these advances, Model S-15, was a significant milestone. It won an IR-100 Award as one of 100 best designed products in 1971 and was the prototype of the Telesensory Optacon. It is now at the Computer History Museum in Mountain View, California.\n\nWith a number of operational prototype Optacons available, an effort was made to get them in daily use by blind people in the community. The engineers were anxious to know how well the Optacon components held up in a real life environment, what uses were made of the Optacon, how much was it used, and how important was it in educational, vocational, and daily living. Several blind people in the Palo Alto community volunteered to participate, and Carolyn Weil was hired to coordinate, teach, and document this part of the project.\n\nThe first issue was how should a blind person be taught to read with and Optacon? Some blind people were unaware of letter shapes, and most were not familiar with the various type fonts. In addition spelling was usually not a strong point, since the education of blind students had often been in Braille, which has about 180 contractions. Of course, none were familiar with recognizing vibratory tactile images of letters moving across their index finger.\n\nWeil developed lessons to teach recognition of letters presented in this fashion using both the computer simulation and the Optacon prototypes. It soon became apparent that while letter recognition could be taught in a few days, building reading speed was much more time consuming. However, there were soon a number of blind people effectively using an Optacon prototype in their daily lives. These people contributed greatly to the project not only in providing important information for the design of future models, but also for motivating the Optacon development team toward making the Optacon widespread. Among this group of pioneering Optacon users were:\n\nThe Optacon was manufactured and marketed from 1971 to 1996 by Telesensory Systems Inc. of Silicon Valley, California. Throughout the 1970s and into the 1980s, the Optacon underwent upgrades, including the development of a new model, known as the Optacon II, which featured improved capabilities to interface to a computer.\n\nAs the Optacon project progressed and more obstacles and unknowns were overcome, the importance of making the Optacon generally available was apparent. TeleSensory's initial sales were to provide Optacons for test evaluations for the U.S. Office of Education, St. Dunstan's for blinded veterans in London, England, the Berufsbildungswerk in Heidelberg, Germany, and Sweden. The success of these evaluations led to larger dissemination programs funded by the U.S. Department of Education, private U.S. foundations such as Melen and Pew, state Departments of Rehabilitation, and various programs in many countries around the world such as Japan, Italy, Germany, France, and Scandinavia. The number of Optacons purchased privately by individuals was small. Approximately 15,000 Optacons were eventually sold.\n\nThroughout the 1970s and into the 1980s, the Optacon underwent upgrades, and various accessories were added, including different lens modules to be used with the camera for reading text in a typewriter and on computer and calculator screens. In 1985 Canon Inc. and Telesensory cooperated in the development of the Optacon II, which featured improved packaging and capabilities to interface to a computer (See Fig. 3).\n\nThe design decision to reduce the number of image pixels from 144 to 100 to lower cost resulted in Optacon II not being successful.\n\nIn the 1990s Telesensory increasingly shifted its emphasis toward the low-vision market and became less devoted to the Optacon. Page scanners with optical character recognition had come to be the tool of choice for blind people wanting access to print. Page scanners were less expensive and had a much shallower learning curve than the Optacon. In addition, blind people could generally read through material more quickly with a page scanner than with an Optacon.\n\nIn 1996 Telesensory announced that it would no longer manufacture the Optacon and that it would cease to service the device in 2000. Many users purchased used machines and cannibalized them for parts, presumably with much help from sighted, electromechanically-talented friends. In March 2005, TSI suddenly shut down. Employees were \"walked out\" of the building and lost accrued vacation time, medical insurance, and all benefits. Customers could not buy new machines or get existing machines fixed. Some work was done by other companies to develop an updated version of the Optacon to reduce the cost of the device and take advantage of newer technology, but no device with the versatility of the Optacon had been developed as of 2007.\n\nMany blind people continue to use their Optacons to this day. The Optacon offers capabilities that no other device offers including the ability to see a printed page or computer screen as it truly appears including drawings, typefaces, and specialized text layouts.\n\n"}
{"id": "3214474", "url": "https://en.wikipedia.org/wiki?curid=3214474", "title": "Organizational intelligence", "text": "Organizational intelligence\n\nOrganizational Intelligence (OI) is the capability of an organization to comprehend and conclude knowledge relevant to its business purpose. In other words, it is the intellectual capacity of the entire organization. With relevant organizational intelligence comes great potential value for companies and therefore organizations find study where their strengths and weaknesses lie in responding to change and complexity. Organizational Intelligence embraces both knowledge management and organizational learning, as it is the application of knowledge management concepts to a business environment, additionally including learning mechanisms, comprehension models and business value network models, such as the balanced scorecard concept. Organizational Intelligence consists of the ability to make sense of complex situations and act effectively, to interpret and act upon relevant events and signals in the environment. It also includes the ability to develop, share and use knowledge relevant to its business purpose as well as the ability to reflect and learn from experience\n\nWhile organizations in the past have been viewed as compilations of tasks, products, employees, profit centers and processes, today they are seen as intelligent systems that are designed to manage knowledge. Scholars have shown that organizations engage in learning processes using tacit forms of intuitive knowledge, hard data stored in computer networks and information gleaned from the environment, all of which are used to make sensible decisions. Because this complex process involves large numbers of people interacting with diverse information systems, organizational intelligence is more than the aggregate intelligence of organizational members; it is the intelligence of the organization itself as a larger system.\n\nOrganizational Intelligence and operational intelligence are usually seen as subsets of business analytics, since both are types of know-how that have the goal of improving business performance across the enterprise. Operational Intelligence is often linked to or compared with real-time business intelligence (BI) since both deliver visibility and insight into business operations. Operational Intelligence differs from BI in being primarily activity-centric, whereas BI is primarily data-centric and relies on a database (or Hadoop cluster) as well as after-the-fact and report-based approaches to identifying patterns in data. By definition, Operational Intelligence works in real-time and transforms unstructured data streams—from log file, sensor, network and service data—into real-time, actionable intelligence.\nWhile Operational Intelligence is activity-focused and BI is data-focused, Organizational Intelligence differs from these other approaches in being workforce- or organization-focused. Organizational Intelligence helps companies understand the relationships that drive their business—by identifying communities as well as employee workflow and collaborative communications patterns across geographies, divisions, and internal and external organizations.\n\nThere are many aspects that organizations must consider in the three steps that they take to gain information. Without these considerations, organizations may experience strategic challenges.\n\nFirst of all, organizations must acquire applicable information to make beneficial predictions. An organization must ask what they already know and need to know. They must also know the timeframe in which the information is needed and where and to find it. To make the best judgements, they must also evaluate the value of the information. Seemingly valuable information that costs more to find than gain from can hurt the company. If judged valuable, the organization must find the most efficient means of acquiring it.\n\nAfter acquiring the right information, an organization must know how to properly process it. They need to know how they can make new information more retrievable and how they can make sure that the information gets disseminated to the right people. The organization must figure out how to secure it and how long and if long, how they need to preserve it.\n\nThe last step includes the utilization of the information. An organization should ask themselves if they are looking at the right information and if so, if they are placing them in the right context. They must consider the possible environmental changes alter the informational value and determine all the relevant connections and patterns. Not forgetting to know if they are including the right people in the decision making process and if there are any technology that can improve the decision making.\n\nThere are briefly four dimensions of problems that many organizations face when dealing with information. This is also referred to as organizational ignorance.\n\nAn organization may be uncertain when it does not possess enough or the right information. To exemplify, a company may be uncertain in a competitive landscape because it does not have enough information to see how the competitors will act. This does not imply that the context of the situation is complex or unclear. Uncertainty can even exist when the range of possibilities is small and simple. There are different degrees of uncertainty. First of all an organization can be completely determined (complete certainty), have some probabilities (risk), probabilities estimated with lesser confidence (subjective uncertainty), unknown probabilities (traditional uncertainty) or undefined (complete uncertainty). However even with the lack of clarity, uncertainty assumes that the context of the problem is clear and well understood.\n\nAn organization may be processing more information than they can manage. Complexity doesn’t always correlate with vagueness or unpredictability. Rather, it occurs when there are too much or when the scope is too large to process. Organizations with complexity problems have interrelated variables, solutions and methods. Managing these problems is dependent of the individuals and the organizations. For instance, uninformed and novices must deal with each elements and relationships one by one but experts can perceive the situation better and find familiar patterns more easily. Organizations facing complexity must have the capacity to locate, map, collect, share, exploit on what the organizations need to know.\n\nAn organization may not have a conceptual framework for interpreting the information. If uncertainty represents not having answers, and complexity represents difficulty in finding them, ambiguity represents not being able to formulate the right questions. Ambiguity cannot be resolved by increasing the amount of information. An organization must be able to interpret and explain the information in collective agreement. Hypotheses should be continuously made and discussed and key communication activities such as face-to-face conversations must be made. Resolving ambiguity in the earlier stages than competitors gives organizations much advantage because it helps organizations to make more appropriate and strategic decisions and have better awareness.\n\nAn organization may be having competing frameworks for interpreting a job. Equivocality refers to multiple interpretations of the field. Each interpretation is unambiguous but differ from each other and they may be mutually exclusive or in conflict. Equivocality result not only because everyone’s experiences and values are unique but also from unreliable or conflicting preferences and goals, different interests or vague roles and responsibilities.\n\nA culture of the organization describes how the organization will work in order to succeed. It can simply be described as the organization’s atmosphere or values. Organizational culture is important because it can be used as a successful leadership tool to shape and improve the organization. Once the culture is settled, it can be used by the leader to deliver his/her vision to the organization. Moreover, if the leader deeply understands the organizational culture, he/she can also use it to predict a future outcome in certain situations.\n\nAn organization with control culture is company oriented and reality oriented. They will succeed by controlling and keeping restrictions. The organization will value timeliness of information, security and hierarchical standardization. They make plans and maintain a process. This organization has stability, predictability and authority. For example, an organization with control culture can be monarchy.\n\nAn organization with competence culture is company oriented and possibility oriented. They will succeed by being the best with exclusivity of the information. The organization values efficiency, accuracy and achievement. They look for creativity and expertise from the people in the organization. For example, an organization with competence culture can be...\n\nAn organization with cultivation culture is people oriented and possibility oriented. They will succeed by growing people, who fulfill the shared vision. The organization values self-actualization and brilliance. They also prioritizes the idea from people. For example, an organization with cultivation culture can be technological utopianism. \n\nAn organization with collaboration culture is people oriented and reality oriented. They will succeed by working together. The organization values affiliation and teamwork. They also prioritizes people in the organization. This organization has accessibility and inclusiveness of information. For example, an organization with collaboration culture can be anarchy.\n\nAn organization’s leadership effectiveness is closely related to the organization’s intelligence and innovation. There are six leadership factors that determines organization’ atmosphere : flexibility (how freely people can communicate with each other and innovate), responsibility (sense of loyalty to the organization), the standards set by people in the organization, appropriate feedback and rewards, the clear vision shared by people and the amount of commitment to the goal. Combination of these factors result in six different leadership styles: Coercive/Commanding, Authoritative/Visionary, Affiliative, Democratic, Coaching and Pacesetting.\n\nFurthermore, organizational intelligence is a collection of individual intelligence. The leadership style of the organization and its atmosphere are related to the organization's innovation. Innovation happens when there are new information getting shared and processed efficiently in the organization.\n\nIn King Arthur's Round Table, Harvard professor David Perkins uses the metaphor of the Round Table to discuss how collaborative conversations create smarter organizations. The Round Table is one of the most familiar stories of Arthurian legend since it’s meant to signal the shift in power from a king who normally sat at the head of a long table and made long pronouncements while everyone else listened. By reducing hierarchy and making collaboration easier, Arthur discovered an important source of power—organizational intelligence—that allowed him to unite medieval England.\n\nThe lawnmower paradox, another metaphor from Perkins’ book, describes the fact that, while pooling physical effort is easy, pooling mental effort is hard. \"It’s a lot easier for 10 people to collaborate on mowing a large lawn than for 10 people to collaborate on designing a lawnmower.\" An organization’s intelligence is reflected by the types of conversations—face-to-face and electronic, from the mailroom to the boardroom—which members have with one another. \"At the top, top level, organizational intelligence depends on ways of interacting with one another that show good knowledge processing and positive symbolic conduct.\"\n\nHarold Wilensky argued that organizational intelligence benefited from healthy argument and constructive rivalry.\n\n\n\n"}
{"id": "10164211", "url": "https://en.wikipedia.org/wiki?curid=10164211", "title": "Petrosix", "text": "Petrosix\n\nPetrosix is the world’s largest surface oil shale pyrolysis retort with an diameter vertical shaft kiln, operational since 1992. It is located in São Mateus do Sul, Brazil, and it is owned and operated by the Brazil energy company Petrobras. Petrosix means also the Petrosix process, an externally generated hot gas technology of shale oil extraction. The technology is tailored to Irati oil shale formation, a Permian formation of the Paraná Basin.\n\nPetrobras started oil shale processing activities in 1953 by developing Petrosix technology for extracting oil from oil shale of the Irati formation. A inside diameter semi-works retort (the Irati Profile Plant) with capacity of 2,400 tons per day, was brought on line in 1972, and began limited commercial operation in 1980. The first retort that used current Petrosix technology was a internal diameter retort pilot plant started in 1982. It was followed by a retort demonstration plant in 1984. A retort was brought into service in December 1991, and commercial production started in 1992. The company operates two retorts which process 8,500 tons of oil shale daily.\n\nThe Petrosix vertical shaft retort is the world's largest operational surface oil shale pyrolysis reactor. It was designed by Cameron Engineers. The retort has the upper pyrolysis section and lower shale coke cooling section. The retort capacity is 6,200 tons of oil shale per day, and it yields a nominal daily output of 3,870 barrels of shale oil (i.e., 550 tons of oil, approximately 1 ton of oil per 11 tons of shale), 132 tons of oil shale gas, 50 tons of liquefied oil shale gas, and 82 tons of sulfur.\n\nPetrosix is one of four technologies of shale oil extraction in commercial use. It is an above-ground retorting technology, which uses externally generated hot gas for the oil shale pyrolysis. After mining, the shale is transported by trucks to a crusher and screens, where it is reduced to particles (lump shale). These particles are between and and have an approximately parallelepipedic shape. These particles are transported on a belt to a vertical cylindrical vessel, where the shale is heated up to about for pyrolysis. Oil shale enters through the top of the retort while hot gases are injected into the middle of the retort. The oil shale is heated by the gases as it moves down. As a result, the kerogen in the shale decomposes to yield oil vapor and more gas. Cold gas is injected into the bottom of the retort to cool and recover heat from the spent shale. Cooled spent shale is discharged through a water seal with drag conveyor below the retort. Oil mist and cooled gases are removed through the top of the retort and enter a wet electrostatic precipitator where the oil droplets are coalesced and collected. The gas from the precipitator is compressed and split into three parts.\n\nOne part of the compressed retort gas is heated in a furnace to and recirculated back to the middle of the retort for heating and pyrolyzing the oil shale, and another part is circulated cold into the bottom of the retort, where it cools down the spent shale, heats up itself, and ascends into the pyrolysis section as a supplementary heat source for heating the oil shale. The third part undergoes further cooling for light oil (naphtha) and water removal and then sent to the gas treatment unit, where fuel gas and liquefied petroleum gas (LPG) are produced and sulfur recovered.\n\nOne drawback of this process is that the potential heat from the combustion of the char contained in the shale is not utilized. Also oil shale particles smaller than can not be processed in the Petrosix retort. These fines may account for 10 to 30 per cent of the crushed feed.\n\n"}
{"id": "47621173", "url": "https://en.wikipedia.org/wiki?curid=47621173", "title": "Piezoelectric speaker", "text": "Piezoelectric speaker\n\nA piezoelectric speaker (also known as a piezo bender due to its mode of operation, and sometimes colloquially called a \"piezo\", buzzer, crystal loudspeaker or beep speaker) is a loudspeaker that uses the piezoelectric effect for generating sound. The initial mechanical motion is created by applying a voltage to a piezoelectric material, and this motion is typically converted into audible sound using diaphragms and resonators. Compared to other speaker designs piezoelectric speakers are relatively easy to drive; for example they can be connected directly to TTL outputs, although more complex drivers can give greater sound intensity. Typically they operate well in the range of 1-5kHz and up to 100kHz in ultrasound applications. \n\nPiezoelectric speakers are frequently used to generate sound in digital quartz watches and other electronic devices, and are sometimes used as tweeters in less-expensive speaker systems, such as computer speakers and portable radios. They are also used for producing ultrasound in sonar systems. Piezoelectric speakers have several advantages over conventional loudspeakers: they are resistant to overloads that would normally destroy most high frequency drivers, and they can be used without a crossover due to their electrical properties. There are also disadvantages: some amplifiers can oscillate when driving capacitive loads like most piezoelectrics, which results in distortion or damage to the amplifier. Additionally, their frequency response, in most cases, is inferior to that of other technologies, especially with regards to bass and midrange. This is why they are generally used in applications where volume and high pitch are more important than sound quality.\n\nPiezoelectric speakers can have extended high frequency output, and this is useful in some specialized circumstances; for instance, sonar applications in which piezoelectric variants are used as both output devices (generating underwater sound) and as input devices (acting as the sensing components of underwater microphones). They have advantages in these applications, not the least of which is simple and solid state construction that resists seawater better than a ribbon or cone based device would.\n\nIn 2013, Kyocera introduced piezoelectric ultra-thin medium-size film speakers with only 1 millimeter of thickness and 7 grams of weight for their 55\" OLED televisions and they hope the speakers will also be used in PCs and tablets. Besides medium-size, there are also large and small sizes which can all produce similar quality of sound and volume within 180 degrees. The highly responsive speaker material provides better clarity than traditional TV speakers.\n\n"}
{"id": "216752", "url": "https://en.wikipedia.org/wiki?curid=216752", "title": "Processor affinity", "text": "Processor affinity\n\nProcessor affinity, or CPU pinning, enables the binding and unbinding of a process or a thread to a central processing unit (CPU) or a range of CPUs, so that the process or thread will execute only on the designated CPU or CPUs rather than any CPU. This can be viewed as a modification of the native central queue scheduling algorithm in a symmetric multiprocessing operating system. Each item in the queue has a tag indicating its kin processor. At the time of resource allocation, each task is allocated to its kin processor in preference to others.\n\nProcessor affinity takes advantage of the fact that remnants of a process that was run on a given processor may remain in that processor's state (for example, data in the cache memory) after another process was run on that processor. Scheduling that process to execute on the same processor improves its performance by reducing performance-degrading events such as cache misses. A practical example of processor affinity is executing multiple instances of a non-threaded application, such as some graphics-rendering software.\n\nScheduling-algorithm implementations vary in adherence to processor affinity. Under certain circumstances, some implementations will allow a task to change to another processor if it results in higher efficiency. For example, when two processor-intensive tasks (A and B) have affinity to one processor while another processor remains unused, many schedulers will shift task B to the second processor in order to maximize processor use. Task B will then acquire affinity with the second processor, while task A will continue to have affinity with the original processor.\n\nProcessor affinity can effectively reduce cache problems, but it does not reduce the persistent load-balancing problem. Also note that processor affinity becomes more complicated in systems with non-uniform architectures. For example, a system with two dual-core hyper-threaded CPUs presents a challenge to a scheduling algorithm.\n\nThere is complete affinity between two virtual CPUs implemented on the same core via hyper-threading, partial affinity between two cores on the same physical processor (as the cores share some, but not all, cache), and no affinity between separate physical processors. As other resources are also shared, processor affinity alone cannot be used as the basis for CPU dispatching. If a process has recently run on one virtual hyper-threaded CPU in a given core, and that virtual CPU is currently busy but its partner CPU is not, cache affinity would suggest that the process should be dispatched to the idle partner CPU. However, the two virtual CPUs compete for essentially all computing, cache, and memory resources. In this situation, it would typically be more efficient to dispatch the process to a different core or CPU, if one is available. This could incur a penalty when process repopulates the cache, but overall performance could be higher as the process would not have to compete for resources within the CPU.\n\nOn Linux, the CPU affinity of a process can be altered with the taskset(1) program and the sched_setaffinity(2) system call. The affinity of a thread can be altered with one of the library functions: pthread_setaffinity_np(3) or pthread_attr_setaffinity_np(3).\n\nOn SGI systems, dplace binds a process to a set of CPUs.\n\nNetBSD 5.0, FreeBSD 7.2 and later versions can use pthread_setaffinity_np and pthread_getaffinity_np. In NetBSD, the psrset utility to set a thread's affinity to a certain CPU set. In FreeBSD, cpuset utility is used to create CPU sets and to assign processes to these sets.\n\nOn Windows NT and its successors, thread and process CPU affinities can be set separately by using SetThreadAffinityMask and SetProcessAffinityMask API calls or via the Task Manager interface (for process affinity only).\n\nmacOS exposes an affinity API that provides hints to the kernel how to schedule threads according to affinity sets.\n\nOn Solaris it is possible to control bindings of processes and LWPs to processor using the pbind(1) program. To control the affinity programmatically processor_bind(2) can be used. There are more generic interfaces available such as pset_bind(2) or lgrp_affinity_get(3LGRP) using processor set and locality groups concepts.\n\n"}
{"id": "41595", "url": "https://en.wikipedia.org/wiki?curid=41595", "title": "Psophometer", "text": "Psophometer\n\nIn telecommunications, a psophometer is an instrument that measures the perceptible noise of a telephone circuit.\n\nThe core of the meter is based on a true RMS voltmeter, which measures the level of the noise signal. This was used for the first psophometers, in the 1930s. As the human-perceived level of noise is more important for telephony than their raw voltage, a modern psophometer incorporates a weighting network to represent this perception. The characteristics of the weighting network depend on the type of circuit under investigation, such as whether the circuit is used to normal speech standards (300 Hz – 3.3 kHz), or for high-fidelity broadcast-quality sound (50 Hz – 15 kHz).\n\nThe name was coined in the 1930s, on a basis from , itself derived from . It is unrelated to .\n\nThe '-meter' suffix was already widely used in English, but also derives originally from Greek.\n\n"}
{"id": "3881557", "url": "https://en.wikipedia.org/wiki?curid=3881557", "title": "Pyrotechnic fastener", "text": "Pyrotechnic fastener\n\nA pyrotechnic fastener (also called an explosive bolt, or pyro, within context) is a fastener, usually a nut or bolt, that incorporates a pyrotechnic charge that can be initiated remotely. One or more explosive charges embedded within the bolt are typically activated by an electric current, and the charge breaks the bolt into two or more pieces. The bolt is typically scored around its circumference at the point(s) where the severance should occur. Such bolts are often used in space applications to ensure separation between rocket stages, because they are lighter and much more reliable than mechanical latches.\n\nFor safety and reliability, exploding bridgewire detonators and slapper detonators are often used in aerospace technology instead of classical blasting caps.\n\nMore recent developments have used pulsed laser diodes to detonate initiators through fiber-optic cables, which subsequently fire the main charge.\n\nGas generators are close cousins of pyrotechnic fasteners. They are used to generate large amounts of gas, as for turbopumps, to inflate balloons, especially airbags; to eject parachutes; and for similar applications.\n\nVarious pyrotechnic compositions can be used, depending on the desired burn rate and required amount of energy and volume of gas produced. Some materials, such as RDX, sublimate in vacuum, which limits their usefulness in aerospace applications. Composition with the character of bipropellants and flash powders are often used.\n\n\n"}
{"id": "56133555", "url": "https://en.wikipedia.org/wiki?curid=56133555", "title": "RGK Mobile", "text": "RGK Mobile\n\nRGK Mobile is a global m-commerce service provider.\n\nRGK Mobile specializes is a global provider of mobile carrier payment solutions, specializing in payment aggregation. Facilitating direct billing services with over 65 leading mobile carriers telecommunications company worldwide in over 30 countries, RGK Mobile allows service merchants content providers direct access to millions of new, potential users.The company was named as one of the key providers of Direct Carrier Billing Aggregation services in the world by MRS.\n\nIn November 2017, the company signed a deal with telecommunications company BITE, providing the mobile operator with a new content subscription revenue channel, and with direct operator billing service. RGK also provides customer support. BITE is a telecommunications company which serves subscribers in Latvia and Lithuania.\n\nIn December 2017, RGK Mobile partnered with the Lazar Angelov Academy, distributing the company’s online personal training and body building programs to mobile operators worldwide. RGK also provides subscriber support for Angleov’s users. \n\nIn January 2018, the company signed a deal with telecommunications company Bharti Airtel, providing India's largest mobile operator with a new content subscription services and direct operator billing service. \n\nIn March 2018, the company announced a deal with telecommunications company Axiata Digital, providing one of Asia's largest mobile carriers with a new content subscription services and direct operator billing service. \n\nIn May 2018, the company announced its European expansion, signing a Direct Carrier Billing agreement with telecommunications company Sunrise Communications AG, providing one of Switzerland's leading mobile carriers with a new content subscription and billing services. \n\nIn November 2018, RGK Mobile announced its first carrier billing agreement with one of Europe's largest mobile carriers telecommunications company Vodafone. \n\n\nThe company’s in-house automatic fraud detection and prevention system detects up to 70% of fraudulent traffic preventatively, while 25% is detected within a few minutes to a few hours of the breech.\n\n\nRGK Mobile was a Gold Sponsor at World Telemedia 2017, in which Vladimir Yuzak, Business Development Officer of RGK, spoke about “Virtual Content Provider-next step in evolution?”\nRGK Mobile was a Sponsor at TelcoDays Prague 2017, in which Vladimir Yuzak, Business Development Officer of RGK, introduced the RGK Engine”\n\nRGK Mobile was named as one of the key providers of Direct Carrier Billing Aggregation services in the world by MRS.\n\nIn June 2018, Forbes Magazine has named RGK Mobile a thought leader on GDPR privacy policies, in the EU. \n"}
{"id": "4696265", "url": "https://en.wikipedia.org/wiki?curid=4696265", "title": "Reference model", "text": "Reference model\n\nA reference model in systems, enterprise, and software engineering is an abstract framework or domain-specific ontology consisting of an interlinked set of clearly defined concepts produced by an expert or body of experts in order to encourage clear communication. A reference model can represent the component parts of any consistent idea, from business functions to system components, as long as it represents a complete set. This frame of reference can then be used to communicate ideas clearly among members of the same community.\n\nReference models are often illustrated as a set of concepts with some indication of the relationships between the concepts.\n\nAccording to OASIS (Organization for the Advancement of Structured Information Standards) a reference model is \"an abstract framework for understanding significant relationships among the entities of some environment, and for the development of consistent standards or specifications supporting that environment. A reference model is based on a small number of unifying concepts and may be used as a basis for education and explaining standards to a non-specialist. A reference model is not directly tied to any standards, technologies or other concrete implementation details, but it does seek to provide a common semantics that can be used unambiguously across and between different implementations.\"\n\nThere are a number of concepts rolled up into that of a 'reference model.' Each of these concepts is important:\n\nThere are many uses for a reference model. One use is to create standards for both the objects that inhabit the model and their relationships to one another. By creating standards, the work of engineers and developers who need to create objects that behave according to the standard is made easier. Software can be written that meets a standard, and developers can copy that software to use it again, or build a software factory that generates that code. When done well, a standard can make use of design patterns that support key qualities of software, such as the ability to extend the software in an inexpensive way.\n\nAnother use of a reference model is to educate. Using a reference model, leaders in software development can help break down a large problem space into smaller problems that can be understood, tackled, and refined. Developers who are new to a particular set of problems can quickly learn what the different problems are, and can focus on the problems that they are being asked to solve, while trusting that other areas are well understood and rigorously constructed. The level of trust is important to allow software developers to efficiently focus on their work.\n\nA third use of a reference model is to improve communication between people. A reference model breaks up a problem into entities, or \"things that exist all by themselves.\" This is often an explicit recognition of concepts that many people already share, but when created in an explicit manner, a reference model is useful by defining how these concepts differ from, and relate to, one another. This improves communication between individuals involved in using these concepts.\n\nA fourth use of a reference model is to create clear roles and responsibilities. By creating a model of entities and their relationships, an organization can dedicate specific individuals or teams, making them responsible for solving a problem that concerns a specific set of entities. For example, if a reference model describes a set of business measurements needed to create a balanced scorecard, then each measurement can be assigned to a specific business leader. That allows a senior manager to hold each of their team members responsible for producing high quality results.\nA fifth use of a reference model is to allow the comparison of different things. By breaking up a problem space into basic concepts, a reference model can be used to examine two different solutions to that problem. In doing so, the component parts of a solution can be discussed in relation to one another. For example, if a reference model describes computer systems that help track contacts between a business and their customers, then a reference model can be used by a business to decide which of five different software products to purchase, based on their needs. A reference model, in this example, could be used to compare how well each of the candidate solutions can be configured to meet the needs of a particular business process.\n\nInstances of reference models include, among others: \n\n"}
{"id": "58078043", "url": "https://en.wikipedia.org/wiki?curid=58078043", "title": "Remote Sensing Applications Center", "text": "Remote Sensing Applications Center\n\nThe Remote Sensing Applications Center (RSAC) is a facility of the United States Forest Service (USFS). It receives weather and fire-monitoring data remotely-sensed by satellites, and converts it into maps and reports for the USFS.\n\nThe RSAC is a software development organization, housed at the same location as its corresponding operational organization, the USFS Geospatial Technology and Applications Center (GTAC), in Salt Lake City, Utah. The RSAC and GTAC assist USFS field operations with combined data from fire-monitoring and weather satellites and other remotely-sensed data, along with mapping and geological surveys (“geospatial technology”).\n\nThe RSAC's principal goal is to develop and implement less costly ways for the Forest Service to obtain information it needs to manage wildfires, water, and forest resources.\n"}
{"id": "17265713", "url": "https://en.wikipedia.org/wiki?curid=17265713", "title": "Roller mower", "text": "Roller mower\n\nA roller mower, or rollermower, is a tractor-powered multi-spindled rotary mowers that have full width rollers front and rear. Most rollermowers attach to a four-wheeled tractor via the three-point linkage and are powered by the tractor’s power take-off (PTO), though larger models connect three or more complete mowing decks to a separate chassis that is towed behind the tractor. Good rollermowers can produce a finish rivalling that of reel (cylinder) mowers while generally being far more robust and requiring considerably less maintenance. Modern machines can also cope efficiently with a wider range of grass lengths and densities than cylinder mowers.\n\nThe original developer of the rollermower is uncertain, but Howard Rotavator Pty Ltd manufactured and sold the 72RLM (later the HS20RLM/180) Rollamowa in Australia from the early 1970s. These were linkage-mounted three-spindled rotary mowers with a cutting width of 1.8m (72”). A larger version (HS20RLM/300) was also produced before manufacturing ceased in about 1984. The company was absorbed by Howard Australia Pty Ltd and from 1985 sourced its rollermowers from Trimax Industries Ltd (later Trimax Mowing Systems) in New Zealand. Trimax then sold the same machines under the Trimax brand around the world. \n\nAs of 2011, several other companies throughout the world manufacture a variety of rollermowers.\n"}
{"id": "49630581", "url": "https://en.wikipedia.org/wiki?curid=49630581", "title": "Roy Cizek", "text": "Roy Cizek\n\nRoy R. Cizek (January 29, 1943 – April 12, 1993) was an American inventor, HI-FI designer, and manufacturer.\n\nFounder of \"Cizek Audio System\" in Andover (Massachusetts), he has become fairly well known in the audiophile world, especially in Italy during the late '70s for his company's high quality speakers. Born January 29, 1943, he worked for historical HI-FI manufacturers as Acoustic Research, JBL and Altec Lansing before starting his own business. Blind since the age of three years, he developed a special sensitivity for music reproduction and patented a special crossover to ensure a flat impedance curve to his speakers.\n\nRoy became blind when he was a child, following a severe burn which also caused him to develop a chronic leg ulcers that accompanied him throughout his life. His severe disabilities probably contributed to the development of his fine feeling for the music and sounds in general. He grew up in Bloomington Indiana where he produced his first speaker. His original speakers were handcrafted prototypes, as demonstrated by some rare photos.\nAccording to the testimony of Ron Nadeau, a close friend who knew him at Indiana University in Bloomington, Roy was a man of great generosity, with fine skills in woodworking. He built loudspeakers for students of the University in exchange for very little money. This \"philosophy\" inspired his whole future production: his goal was to build low-cost quality speakers.\n\nIn the 1970s he moved with his wife Fran and son Carl to Cambridge (Massachusetts) where he worked as a consultant for Acoustic Research. In this state, precisely at Andover, he founded a few years later the company that bore his name, CIZEK Audio System.\n\nIn 1976, Roy produced his most successful product: \"The Cizek Model One\". This speaker introduced the concept of flat impedance module, with only one peak at the resonance frequency, achieved by a particular crossover, and at the same time used some innovative solutions as the upward inclination of the baffle, obtained with the special stands and the use of damping material (foam) in the baffle, in order to reduce the diffraction phenomena.\n\nThe Cizek One also had a switch that allowed its operator to choose the damping factor (Q factor) of the low frequencies between 0.6 and 1.0; this allowed operators decide whether to listen music with driest and braked low (Q = 0.6), or with slightly emphasized low (Q = 1.0). Roy Cizek personally controlled all \"Model One\" production to ensure the met his standards. Being blind, he could not see the measurement, but used a special trick: he placed both the index and middle fingers so that the needle readout instrument should touch his fingers, and so the margin of tolerance were established. If the needle touched a finger, the speaker was discarded.\n\nThe article published in no 54 of the Italian magazine \"Stereoplay\" (April 1978), signed by Renato Giussani, describes the innovative features of the Roy Cizek speaker system.\n\nSubsequently, 2 other speaker models were produced: The CIZEK Model 2 and Model 3, in addition to a stereo subwoofer with 2 woofers in a double closed box called MG 27; the latter, together with the Model 3 was the subject of an article in the magazine Stereoplay and which text is available online; in this article, it was pointed out that the Cizek system was able to reproduce the sounds between 27 and 700 Hertz in a range of ± 1 dB; moreover, in the conclusions of the review, the association Model 3 + MG 27 was compared to the best of existing speaker systems of that time. A few years later it was produced a mini monitor: the KA-1 Classic model, built entirely of solid KOA wood, a Hawaiian tree which harvesting was subsequently forbidden; KA-1 could be combined with a subwoofer, the KA-20, thus constituting the \"System Classic 20\", the subject of a trial that appeared in the Italian magazine \"Suono\" No 113 of June 1982 and where in the conclusions was stressed the high product quality level of the system, which was considered \"one of the best speakers ever heard.\n\nAnother particular speakers system which CIZEK built was the \"Sound Window\", a speaker for wall, flat and square with rounded edges, small to medium in size.\n\nIn the 80's Roy Cizek sold his company in Sheldon Feinstein who became the President of CIZEK Audio System. Sheldon died suddenly for a heart attack on his way to work and the company never recovered after this event. The new President of Cizek was a poor organizer, preferring to personally store all the addresses and contacts of suppliers and customers. After the death of Feinstien, 30% of the company was taken over by Giancarlo Bonetti, an Italian importer of Cizek's brand in the 1970s and 1980s. Thus, for a short period Cizek Speaker Series \"Classic\" (KA-1, KA-20 and KA-18) were built in Italy, commissioned by the Bonetti's company \"ESOTER\", to the young Franco Serblin, HIFI designer, who in 1983 founded the Sonus Faber, a leading Italian manufacturer of loudspeakers; and in fact, many Sonus Faber speaker systems aesthetically recall the Cizek Model KA-1.\n\nRoy Cizek continued to produce speakers in the 1980s and early 1990s with his new company, the \"High Tech Aspirin by Cizek\", together with his third wife, in Torrance, California. These speakers were presented to at least two editions of the Consumer Electronics Show (CES) in Las Vegas.\n\nRoy died on April 12, 1993 in Torrance.\n\nThe MG 27 sub-woofer system together with 2 Quad electrostatic speakers is considered a milestone and used for comparison of modern speakers at the magazine Stereophile.\n"}
{"id": "38108077", "url": "https://en.wikipedia.org/wiki?curid=38108077", "title": "STANAG 4586", "text": "STANAG 4586\n\nSTANAG 4586 \"(NATO Standardization Agreement 4586)\" is a NATO Standard Interface of the Unmanned Control System (UCS) Unmanned Aerial Vehicle (UAV) interoperability. It defines architectures, interfaces, communication protocols, data elements and message formats. It includes data link, command and control, and human/computer interfaces. The current revision is STANAG 4586 Edition No 4 with mission phase enhancements, an updated list of vehicle identifiers etc.\n"}
{"id": "31886297", "url": "https://en.wikipedia.org/wiki?curid=31886297", "title": "Smart lighting", "text": "Smart lighting\n\nSmart lighting is a lighting technology designed for energy efficiency. This may include high efficiency fixtures and automated controls that make adjustments based on conditions such as occupancy or daylight availability. Lighting is the deliberate application of light to achieve some aesthetic or practical effect. It includes task lighting, accent lighting, and general lighting.\n\n19% of energy use in the world is used for lighting, and 6% of greenhouse emissions in the world derive from this energy used for lighting. In the United States, 65 percent of energy consumption is used by commercial and industrial sectors, and 22 percent of this is used for lighting.\n\nSmart lighting is the good way which enables to minimize and save light by allowing the householder to control remotely cooling and heating, lighting, and the control of appliances. This ability saves energy and provides a level of comfort and convenience. From outside the traditional lighting industry, the future success of lighting will require involvement of a number of stakeholders and stakeholder communities. The concept of smart lighting also involves utilizing natural light from the sun to reduce the use of man-made lighting, and the simple concept of people turning off lighting when they leave a room.\n\nThe use of automatic light dimming is an aspect of smart lighting that serves to reduce energy consumption. Manual light dimming also has the same effect of reducing energy use.\n\nIn the paper \"Energy savings due to occupancy sensors and personal controls: a pilot field study\", Galasiu, A.D. and Newsham, G.R have confirmed that automatic lighting systems including occupancy sensors and individual (personal) controls are suitable for open-plan office environments and can save a significant amount of energy (about 32%) when compared to a conventional lighting system, even when the installed lighting power density of the automatic lighting system is ~50% higher than that of the conventional system.\n\nA complete sensor consists of a motion detector, an electronic control unit, and a controllable switch/relay. The detector senses motion and determines whether there are occupants in the space. It also has a timer that signals the electronic control unit after a set period of inactivity. The control unit uses this signal to activate the switch/relay to turn equipment on or off. For lighting applications, there are three main sensor types: passive infrared, ultrasonic, and hybrid.\n\nIn response to daylighting technology, daylight-linked automated response systems have been developed to further reduce energy consumption. These technologies are helpful, but they do have their downfalls. Many times, rapid and frequent switching of the lights on and off can occur, particularly during unstable weather conditions or when daylight levels are changing around the switching illuminance. Not only does this disturb occupants, it can also reduce lamp life. A variation of this technology is the 'differential switching' or 'dead-band' photoelectric control which has multiple illuminances it switches from to reduce occupants being disturbed.\n\nSmart lighting that utilizes occupancy sensors can work in unison with other lighting connected to the same network to adjust lighting per various conditions. The table below shows potential electricity savings from using occupancy sensors to control lighting in various types of spaces.\n\nThe advantages of ultrasonic devices are that they are sensitive to all types of motion and generally there are zero coverage gaps, since they can detect movements not within the line of sight.\n\nMotion-detecting (microwave), heating-sensing (infrared), and sound-sensing; optical cameras, infrared motion, optical trip wires, door contact sensors, thermal cameras, micro radars,daylight sensors.\n\nThe function of a traditional emergency lighting system is the supply of a minimum illuminating level when a line voltage failure appears. Therefore, they have to store energy in a battery module to supply the lamps in that case of failure. In this kind of lighting systems the internal damages for example battery overcharging, damaged lamps and starting circuit failure must be detected and repaired by specialist workers.\n\nFor this reason, the smart lighting prototype can check its functional state every fourteen days and dump the result into a LED display. With these features they can test themselves checking their functional state and displaying their internal damages. Also the maintenance cost can be decreased.\n\nThe main idea is the substitution of the simple line voltage sensing block that appears in the traditional systems by a more complex one based on a microcontroller. This new circuit will assume the functions of line voltage sensing and inverter activation, by one side, and the supervision of all the system: lamp and battery state, battery charging, external communications, correct operation of the power stage, etc., by the other side.\n\nThe system has a great flexibility, for instance, it would be possible the communication of several devices with a master computer, which would know the state of each device all the time.\n\nA new emergency lighting system based on an intelligent module has been developed. The micro-controller as a control and supervision device guarantees increase in the installation security and a maintenance cost saving.\n\nAnother important advantage is the cost saving for mass production specially whether a microcontroller with the program in ROM memory is used.\n\nSmart lighting systems can be controlled using the internet to adjust lighting brightness and schedules. One technology involves a smart lighting network that assigns IP addresses to light bulbs.\n\nSchubert predicts that revolutionary lighting systems will provide an entirely new means of sensing and broadcasting information. By blinking far too rapidly for any human to notice, the light will pick up data from sensors and carry it from room to room, reporting such information as the location of every person within a high-security building. A major focus of the Future Chips Constellation is smart lighting, a revolutionary new field in photonics based on efficient light sources that are fully tunable in terms of such factors as spectral content, emission pattern, polarization, color temperature, and intensity. Schubert, who leads the group, says smart lighting will not only offer better, more efficient illumination; it will provide “totally new functionalities.”\n\nThe advances achieved in photonics are already transforming society just as electronics revolutionized the world in recent decades and it will continue to contribute more in the future. From the statistics, North America’s optoelectronics market grew to more than $20 billion in 2003. The LED (light-emitting diode) market is expected to reach $5 billion in 2007, and the solid-state lighting market is predicted to be $50 billion in 15–20 years, as stated by E. Fred Schubert, Wellfleet Senior Distinguished Professor of the Future Chips Constellation at Rensselaer.\n\n\n\n\n"}
{"id": "20518990", "url": "https://en.wikipedia.org/wiki?curid=20518990", "title": "Softkinetic", "text": "Softkinetic\n\nSoftKinetic is a Belgian company which develops gesture recognition hardware and software for real-time range imaging (3D) cameras (such as time-of-flight cameras). It was founded in July 2007. SoftKinetic provides gesture recognition solutions based on its technology to the interactive digital entertainment, consumer electronics, health & fitness, and serious game industries. SoftKinetic technology has been applied to interactive digital signage and advergaming, interactive television, and physical therapy.\n\nOriginally centered on providing software development kits and middleware, SoftKinetic branched out into internal game and application development with the establishment of SoftKinetic Studios in September 2009. SoftKinetic then progressed into hardware, partnering with time-of-flight camera developer Optrima in September 2010, with the two companies merging into a reformed SoftKinetic by March 2011.\n\nSoftKinetic's gesture recognition software platform, named \"iisu\", can recognize and distinguish or isolate different scenic elements, can identify and track the body parts of a user, and can adapt the user's shape, posture, and movements to an existing physical model, and vice versa. iisu is compatible with all major real-time range imaging cameras, and the middleware guards developers from the particularities of the hardware.\n\nSoftKinetic's hardware solution, named \"DepthSense\" (formerly \"OptriCam\"), is a line of 3D time-of-flight imagers based on patented CMOS sensor and time-of-flight (TOF) technologies. Based on Current Assisted Photonic Demodulation (CAPD), the company's patented CMOS pixel technology, DepthSense hardware products range from sensors to consumer and professional 3D cameras.\n\nSoftKinetic Studios has collaborated on numerous gesture recognition-based projects for various platforms such as PC, consoles, set-top boxes, arcade, and interactive digital signage; with companies such as Orange Vallée and Fuel Industries.\n\nOn October 8th 2015, Sony Corporation announced the acquisition of the company.\n\n\n"}
{"id": "2313057", "url": "https://en.wikipedia.org/wiki?curid=2313057", "title": "Stephanie Wilson", "text": "Stephanie Wilson\n\nStephanie Diana Wilson (born September 27, 1966) is an American engineer and a NASA astronaut. She flew to space onboard three Space Shuttle missions, and is the second African American woman to go into space, after Mae Jemison. Her 42 days in space are the most of any African American astronaut, male or female.\n\nStephanie Wilson was born in Boston, Massachusetts on September 27, 1966. About a year later, her parents, Eugene and Barbara Wilson, decided to move to Pittsfield, Massachusetts. Eugene, a native of Nesmith, South Carolina, used his electronics training from his time in the Navy to get himself a degree from Northeastern University and a long career in electrical engineering for Raytheon, Sprague Electric, and Lockheed Martin, while Barbara worked as a production assistant for Lockheed Martin. After attending Stearns Elementary School, she attended Crosby Junior High School. For a career awareness class in middle school, Wilson was assigned to interview someone in a field that interested her. Since she liked to look up at the sky, she interviewed Williams College astronomer Jay Pasachoff, clarifying her potential career interest in space. In high school, Eugene encouraged Stephanie to go into engineering, so she decided to become an aerospace engineer. Wilson graduated from Taconic High School, Pittsfield, Massachusetts, in 1984. She attended Harvard University, receiving a bachelor of science degree in engineering science in 1988. Wilson earned a Master of Science degree in aerospace engineering from the University of Texas, in 1992. Wilson has returned to Harvard as a member of the Harvard Board of Overseers. She was the Chief Marshal for the 362nd Harvard Commencement on May 30, 2013.\n\nWilson worked for two years for the former Martin Marietta Astronautics Group in Denver, Colorado. As a Loads and Dynamics engineer for the Titan IV rocket, Wilson was responsible for performing coupled loads analyses for the launch vehicle and payloads during flight events. Wilson left Martin Marietta in 1990 to attend graduate school at the University of Texas. Her research focused on the control and modeling of large, flexible space structures.\n\nFollowing the completion of her graduate work, Wilson began working for the Jet Propulsion Laboratory in Pasadena, California, in 1992. As a member of the Attitude and Articulation Control Subsystem for the Galileo spacecraft, Wilson was responsible for assessing attitude controller performance, science platform pointing accuracy, antenna pointing accuracy and spin rate accuracy. She worked in the areas of sequence development and testing as well. While at the Jet Propulsion Laboratory, Wilson also supported the Interferometery Technology Program as a member of the Integrated Modeling Team, which was responsible for finite element modeling, controller design, and software development.\n\nSelected by NASA as an Astronaut Candidate in April 1996, Wilson reported to the Johnson Space Center in August 1996. Having completed two years of training and evaluation, she is qualified for flight assignment as a mission specialist. She was initially assigned technical duties in the Astronaut Office Space Station Operations Branch to work with Space Station payload displays and procedures. She then served in the Astronaut Office CAPCOM Branch, working in Mission Control as a prime communicator with on-orbit crews. Following her work in Mission Control, Wilson was assigned technical duties in the Astronaut Office Shuttle Operations Branch involving the Space Shuttle Main Engines, External Tank and Solid Rocket Boosters.\n\nWilson has flown on three shuttle missions. On STS-121, Wilson flew aboard as a mission specialist. She also flew on the STS-120 mission that delivered the Harmony connecting module to the International Space Station. In April 2010, Wilson flew as a Mission Specialist on STS-131.\n\nSTS-121 (July 4–17, 2006), was a return-to-flight test mission and assembly flight to the International Space Station. During the 13-day flight the crew of Space Shuttle Discovery tested new equipment and procedures that increase the safety of space shuttles, repaired a rail car on the International Space Station and produced never-before-seen, high-resolution images of the Shuttle during and after its July 4 launch. Wilson supported robotic arm operations for vehicle inspection, Multi-Purpose Logistics Module installation and EVAs and was responsible for the transfer of more than 28,000 pounds of supplies and equipment to the ISS. The crew also performed maintenance on the space station and delivered a new Expedition 13 crew member to the station. The mission was accomplished in 306 hours, 37 minutes and 54 seconds.\n\nSTS-120 (October 23 – November 7, 2007) was a 6.25 million mile Space Shuttle mission to the International Space Station (ISS). It delivered the \"Harmony\" module, and reconfigured the P6 truss in preparation for future assembly missions. STS-120 carried a new Expedition 16 crewmember, Daniel Tani, and returned Expedition 15 and Expedition 16 crewmember Clayton Anderson. The crew conducted four spacewalks and performed a previously untested repair method on the station's solar array. The mission was accomplished in 15 days, 2 hours, 23 minutes, during 238 orbits.\n\nSTS-131 (April 5–20, 2010) was a resupply mission to the International Space Station. Space Shuttle Discovery was launched pre-dawn from Kennedy Space Center. Once docked to the space station, the crew delivered more than 27,000 pounds of hardware, supplies, experiments and equipment, including a tank full of ammonia coolant that required three spacewalks and robotics to install, new crew sleeping quarters, a window observation facility and a freezer for experiments. During the mission, Wilson was responsible for robotics for spacewalking support using the space station robotic arm and for robotic removal of the \"Leonardo\" Multi-Purpose Logistics Module from the payload bay of Discovery. For the return to Earth, Wilson robotically installed Leonardo, which was packed with more than 6,000 pounds of hardware, science results and used supplies, inside Discovery's payload bay. The STS-131 mission was accomplished in 15 days, 2 hours, 47 minutes and 10 seconds and traveled 6,232,235 statute miles in 238 orbits.\n\nWilson is a Christian. Her hobbies include skiing and stamp collecting.\n\n\n"}
{"id": "22441647", "url": "https://en.wikipedia.org/wiki?curid=22441647", "title": "Swedish National Museum of Science and Technology", "text": "Swedish National Museum of Science and Technology\n\nThe Swedish National Museum of Science and Technology () is a Swedish museum in Stockholm. It is Sweden’s largest museum of technology, and has a national charter to be responsible for preserving the Swedish cultural heritage related to technological and industrial history. Its galleries comprise around 10,000 square meters, and the museum attracts annually about 350, 000 visitors. The collections consist of more than 50,000 objects and artifacts, 600 shelf metres of archival records and documents, 200,000 drawings, 620,000 images and just over 50,000 books. The National Museum of Science and Technology also documents technologies, processes, stories and memoirs in order to preserve them for generations to come.\n\nThe National Museum of Science and Technology was founded in 1924 by the Royal Swedish Academy of Engineering Sciences, the Confederation of Swedish Enterprise (formerly the Federation of Swedish Industries), the Swedish Inventors' Association and the Swedish Association of Graduate Engineers (formerly Svenska Teknologföreningen – roughly, the Swedish Association of Technologists). Its present building is designed in the functionalistic style by architect Ragnar Hjorth, and was opened in 1936. The museum became a foundation in 1947; and has been operated with government funding since 1964.\n\n"}
{"id": "168223", "url": "https://en.wikipedia.org/wiki?curid=168223", "title": "Theodore Hall", "text": "Theodore Hall\n\nTheodore Alvin Hall (October 20, 1925 – November 1, 1999) was an American physicist and an atomic spy for the Soviet Union, who, during his work on US efforts to develop the first and second atomic bombs during World War II (the Manhattan Project), gave a detailed description of the \"Fat Man\" plutonium bomb, and of several processes for purifying plutonium, to Soviet intelligence. His brother, Edward N. Hall, was a rocket scientist who worked on intercontinental ballistic missiles for the United States government.\n\nTheodore Alvin Holtzberg was born in Far Rockaway, New York City to a devout Jewish couple, Barnett Holtzberg and Rose Moskowitz. His father was a furrier, and the Great Depression affected his business significantly. When his father's business became unable to support the household, the family moved to Washington Heights in Upper Manhattan.\n\nEven at a young age, Theodore showed an aptitude in mathematics and science, mostly being tutored by his elder brother Edward. After skipping three grades at Public School 173 in Washington Heights, in the fall of 1937, Hall entered the Townsend Harris High School for gifted boys. After graduation from high school, he was accepted into Queens College at the age of 14 in 1940, and transferred to Harvard University in 1942, where he graduated at the age of 18 in 1944.\n\nIn the fall of 1936, despite the protests of their parents, Edward, his brother, changed both his and Theodore's last name to Hall in an effort to avoid anti-Semitic hiring practices.\n\nAt the age of 19, and through the recommendation of John Van Vleck, Hall was among the youngest scientists to be recruited to work on the Manhattan Project at Los Alamos. At Los Alamos Hall handled experiments for the implosion device (\"Fat Man\") and helped determine the critical mass of uranium for \"Little Boy\".\n\nTheodore Hall later claimed that he became concerned about the consequences of an American monopoly of atomic weapons after the war. He was especially worried about the possibility of the emergence of a fascist government in the United States.\n\nWhile on a vacation in New York City in October 1944, he visited the CPUSA offices, instead of the Soviet Consulate (where he feared FBI surveillance), in order to locate a contact to pass information on the Manhattan Project along to the Soviet Union. After a few recommendations, he met Sergey Kurnakov, a military writer for \"Soviet Russia Today\" and \"Russky Golos\", and handed him a report on the scientists who worked at Los Alamos, the conditions at Los Alamos, and the basic science behind the bomb. Saville Sax delivered the same report to the Soviet Consulate a few days later under the guise of inquiring about relatives still in the Soviet Union. The two eventually met with Anatoly Yatskov, the New York station chief, who later transmitted the information to the NKVD using a one-time pad cipher. After officially becoming an informant for the Soviet Union, Hall was given the code-name MLAD, a Slavic root meaning \"young\", and Sax was given the code-name STAR, a Slavic root meaning \"old\".\n\nKurnakov reported in November 1944: \"Rather tall, slender, brown-haired, pale and a bit pimply-faced, dressed carelessly, boots appear not cleaned for a long time, slipped socks. His hair is parted and often falls on his forehead. His English is highly cultured and rich. He answers quickly and very fluently, especially to scientific questions. Eyes are set closely together; evidently, neurasthenic. Perhaps because of premature mental development, he is witty and somewhat sarcastic but without a shadow of undue familiarity and cynicism. His main trait — a highly sensitive brain and quick responsiveness. In conversation, he is sharp and flexible as a sword ... He comes from a Jewish family, though doesn't look like a Jew. His father is a furrier; his mother is dead ... He is not in the army because, until now, young physicists in government jobs at a military installation were not being drafted. Now, he is to be drafted but has no doubts that he will be kept at the same place, only dressed in a military uniform and with a correspondingly lower salary.\"\n\nUnbeknownst to Hall, Klaus Fuchs, a Los Alamos colleague, and others still unidentified, were also spying for the USSR; none seems to have known of the others. Harvard friend Saville Sax acted as Hall's courier until spring of 1945 when he was replaced by Lona Cohen. Igor Kurchatov, a brilliant scientist and the head of the Soviet atomic bomb effort, probably used information provided by Klaus Fuchs to confirm corresponding information provided earlier by Hall. Despite other scientists giving information to the Soviet Union, Hall was the only known scientist to give details on the design of an atomic bomb.\n\nIn the autumn of 1946, Hall left Los Alamos for the University of Chicago, where he finished out his Master's and Doctoral degrees in Physics, met his wife, and started a family. He continued feeding information to the Soviet Union about a new generation of nuclear weapons being developed at the University of Chicago. After graduating he became a biophysicist. In Chicago, he pioneered important techniques in X-ray microanalysis until, in 1962, he became unsatisfied with his equipment and the techniques available to him. He then moved to Vernon Ellis Cosslett's electron microscopy research laboratory at Cambridge University in England. At Cambridge he created the Hall method of continuum normalization, developed for the specific purpose of analyzing thin sections of biological tissue. He remained working at Cambridge until he retired at the age of 59 in 1984.\n\nHall later became active in obtaining signatures for the Stockholm Peace Pledge.\n\nOn November 1, 1999, Theodore Hall died at the age of 74, in Cambridge, England. Although he had suffered from Parkinson's disease, he ultimately succumbed to renal cancer.\n\nThe Venona project decrypted some Soviet messages and uncovered evidence about Hall, but until their public release in July 1995, nearly all of the espionage regarding the Los Alamos nuclear weapons program was attributed to Klaus Fuchs. Hall was questioned by the Federal Bureau of Investigation in March 1951 but wasn't charged. Alan H. Belmont, the number-three man in the FBI, decided that information coming out of the Venona project would be inadmissible in court as hearsay evidence, and so its value in the case was not worth compromising the program.\n\nThe Venona project became public knowledge in 1995.\n\nIn a written statement published in 1997, Hall came very close to admitting that the accusations against him were true, although obliquely, saying that in the immediate postwar years, he felt strongly that \"an American monopoly\" on nuclear weapons \"was dangerous and should be avoided\":\n\nHe repeated this near-confession in an interview for the TV-series \"Cold War\" on the Cable News Network in 1998, saying:\n\n\n\n"}
{"id": "2541603", "url": "https://en.wikipedia.org/wiki?curid=2541603", "title": "Wood gas generator", "text": "Wood gas generator\n\nA wood gas generator is a gasification unit which converts timber or charcoal into wood gas, a syngas consisting of atmospheric nitrogen, carbon monoxide, hydrogen, traces of methane, and other gases, which - after cooling and filtering - can then be used to power an internal combustion engine or for other purposes. Historically wood gas generators were often mounted on vehicles, but present studies and developments concentrate mostly on stationary plants.\n\nGasification was an important and common technology during the 19th and early 20th century. Town gas produced from coal was widely used, mainly for lighting purposes. When stationary internal combustion engines based on the Otto cycle became available in the 1870s, they began displacing steam engines as prime movers in many works requiring stationary motive power. Adoption accelerated after the Otto engine's patent expired in 1886. The potential and practical applicability of gasification to internal combustion engines were well understood from the earliest days of their development.\n\nIn 1873, Thaddeus S. C. Lowe developed and patented the water gas process by which large amounts of hydrogen gas could be generated for residential and commercial use in heating and lighting. Unlike the common coal gas, or coke gas which was used in municipal service, this gas provided a more efficient heating fuel.\n\nDuring the late 19th century internal combustion engines were commonly fueled by town gas, and during the early 20th century many stationary engines switched to using producer gas created from coke which was substantially cheaper than town gas which was based on the distillation (pyrolysis) of more expensive coal.\nDuring World War II gasoline was rationed and in short supply. Due to the lack of gasoline from petroleum, older people recalled how to build gasifiers for both wood and coal, and how to convert internal combustion engines to run on gaseous fuel, and wood gas generators were in active production. In Great Britain, France, the United States and Germany, large numbers of such generators were constructed or improvised to convert wood and coal into fuel for vehicles. Commercial generators were in production before and after the war for use in special circumstances or in distressed economies. Some World War II era wood gas generators were of the \"Imbert\" downdraft type, designed around 1920 by French inventor Georges Imbert.\n\nGermany produced Gazogene units for vehicles including cars, trucks, artillery tractors and even tanks, to preserve the limited supplies of fuel. Even in non-combatant countries, such as Sweden or Brazil, gasogene was popular, as oil became hard to obtain. In Brazil, a racer named Chico Landi won at São Paulo's Interlagos circuit in 1944, driving a wood gas-powered Alfa Romeo.\n\nCoal-based town gas production was generally displaced by petroleum-based gas and natural gas. However, Great Britain continued her use of coal-based town gas until the North Sea natural gas discoveries in the 1960s and 1970s.\n\nWhen oil prices rose there was renewed interest in wood gas generators. The US Federal Emergency Management Agency (FEMA) published a book in March 1989 describing how to build a gas generator in an emergency when oil was not available. It described a design called the \"stratified downdraft gasifier\" which solves several drawbacks of earlier types.\n\nThe European Union sponsored a wood gas project in Güssing, Austria, starting in 2005.\nThis project was an electric power plant with a wood gas generator and a gas engine to convert the wood gas into 2 MW electric power and 4.5 MW heat. There was also an experimental device to use the Fischer-Tropsch process to convert wood gas to a diesel-like fuel. By October 2005, it was possible to convert 5 kg wood into 1 litre of fuel.\n\nThere is a rich literature on gas-works, town-gas, gas-generation, wood-gas, and producer gas, that is now in the public domain due to its age.\n\nMost successful wood gas generators in use in Europe and the United States are some variation of the earlier Imbert design. Wood gas generators often use wood; however, charcoal can also be used as a fuel. It is denser and produces a cleaner gas without the tarry volatiles and excessive water content of wood.\n\nThe FEMA wood gas generator is (by definition of the FEMA manual) an emergency gasifier. It is designed to be rapidly assembled in a true fuel crisis. This simplified design has distinct benefits over the earlier European units such as easier refueling and construction but is less popular than the earlier Imbert design because of significant new problems, which include a lack of a fixed oxidization zone and allows the oxidization zone to creep to a larger area, causing a drop in temperature; a lower operating temperature leads to tar production and it lacks a true reduction zone further increasing this design's propensity to produce tar. Tar in the wood gas stream is considered a dirty gas and tar will gum up a motor quickly, possibly leading to stuck valves, and rings.\n\nA new design known as the Keith gasifier improves on the FEMA unit, incorporating extensive heat recovery and eliminating the tar problem. Testing at Auburn University has shown it to be 37% more efficient than running gasoline. This system set the world speed record for biomass powered vehicles and has made several cross country tours.\n\nThe United Nations produced the FOA 72 document with details about their wood gas generator design and construction, as does World Bank technical paper 296.\n\nWood gas generators have a number of advantages over use of petroleum fuels:\n\nThe disadvantages of wood gas generators are:\n\nWhen not carefully designed and used, there exists considerable potential for injury or death due to wood gas containing a large percentage of poisonous carbon monoxide (CO) gas. Wood gasifiers of proven design and thoroughly tested construction are considered safe to use outdoors, or in a partially enclosed space, for example, under a shelter open to the air on two sides; they may also be considered relatively safe to use in an extremely well ventilated (e.g. negative pressure) indoor area not connected to any indoor area used for sleeping, equipped with redundant (more than 1), completely independent, battery-powered, regularly tested carbon-monoxide gas detectors. However, prudence must dictate that any sort of experimental wood gasifier design or new construction be thoroughly tested outdoors, and only outdoors, with a \"buddy\" at all times, and with constant vigilance for any sign of headache, drowsiness, or nausea, as these are the first symptoms of carbon monoxide poisoning.\n\nIn addition, mixtures of excessive quantities of air and gas should be avoided as this could lead to the deflagration (explosion) of the gas in question if a combustion source is present. Long-term storage of wood-gas, except through the use of a gasholder-type water-displacement apparatus, should not be attempted, due to the volatile elements present in the gas, which, if allowed to excessively precipitate, will condense in the storage vessel. Under no circumstances should wood-gas ever be compressed to more than above ambient, as this may induce condensation of volatiles, as well as lead to the likelihood of severe injury or death due to carbon monoxide or deflagration if the vessel leaks or fails.\n\nIn 2008, an example of designing and constructing a working wood gas generator powered truck was shown on the National Geographic Channel's \"Planet Mechanics\" in the eighth episode, \"Tree Powered Car\".\n\nIn 2009, another example of designing and constructing a working wood gas powered generator engine was in the TV series \"The Colony\" in the second episode of the first season \"Power Struggle\". Also used in the tenth episode \"Exodus\" to power an escape vehicle.\n\nA 2010 Mother Earth News article discussed and showed pictures of a wood gas powered engine installed in a pickup truck.\nAs part of the BBC science series \"Bang Goes The Theory\", a Volkswagen Scirocco was converted to a design by Martin Bacon to run on used coffee grounds, and after its build in 2010 was driven solely on coffee from London to Manchester successfully. Part of the team are now working on a more advanced design leaning towards top speed as opposed to range.\n\nOn September 14, 2011, at the Bonneville Salt Flats a truck modified with a wood gas powered engine set a new world speed record for vehicles powered by wood gas with a speed of 73 mph.\n\nOn the popular US radio program \"Car Talk\", a caller in episode 1201 (which aired on January 7, 2012, and was subsequently named \"20 Miles Per Woodchip\"), described a wood gas generating vehicle he rode in as a boy during World War II in Germany. The hosts were not familiar with the technology, likely because it was never widely adopted in the US.\n\nOn March 12, 2012, on a season 2 episode of Doomsday Preppers, a wood gas generator is shown running a Ford truck and a house electric generator by prepper Scott Hunt on his multi-acre woodland property in South Carolina.\n\nSerbian TV sitcom \"Truckdrivers 2\" (=\"Kamiondzije II\") from 1983. talks, as a part of plot, about a gas generator affixed to a chassis of a lorry.\n\nAn article appeared in Mother Earth News in April 2012 featuring the Keith gasifier, showing pictures and talking with inventor Wayne Keith.\n\nIn the BBC documentary Wartime Farm, Episode 5 (aired October 2012) they built a coal gas powered ambulance according to the specifications of a 1943 gas powered vehicle.\n\nIn Season 3 Of Mountain Men On The History Channel, Eustice Conway is shown operating a wood gasified Toyota pickup for hauling goods\n\nThe Finnish prime minister Juha Sipilä has received some media attention also for the wood gas powered car he built as a hobby prior to his political career.\nThe \"El Kamina\", model year 1987 Chevrolet El Camino, is the fastest wood-gas powered car, 140 km/h, 87 mph. See http://www.ekoautoilijat.fi/tekstit/kuvatekstit/ElCamina.htm.\n\nThere are only a few companies that produce wood gasifier systems commercially. A list can be found here below. Since wood gas systems have the tendency of being rather large, most focus on stationary applications (electricity production). Some may be suitable for building into vehicles though.\n\n\n\n\n"}
