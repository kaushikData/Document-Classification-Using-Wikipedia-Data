{"id": "52875487", "url": "https://en.wikipedia.org/wiki?curid=52875487", "title": "10K resolution", "text": "10K resolution\n\nUW10K resolution (\"UW\" standing for \"Ultra Wide\") is horizontal resolution of about 10,000 pixels. UW10K resolution was added to HDMI 2.1 to support computer monitors. This resolution is expected to be used in computer monitors, and is not a standard format in digital television and digital cinematography, which feature 4K resolutions and 8K resolutions.\n\nOn June 5, 2015, Chinese manufacturer BOE showed a UW10K display with an aspect ratio of 21:9 and a resolution of 10240 × 4320.\n\nOn January 4, 2017, HDMI 2.1 was officially announced and the specification was originally scheduled to be released in Q2 2017, but later delayed to November 28, 2017. HDMI 2.1 added support for higher resolutions and higher refresh rates which includes 4K/120 Hz, 8K/120 Hz, and UW10K/120 Hz. HDMI 2.1 specified a new 48G cable which supports a bandwidth of 48 Gbit/s and Display Stream Compression (DSC) 1.2 is used for video that is higher than 8K resolution with 4:2:0 chroma subsampling.\n\n\n"}
{"id": "39331606", "url": "https://en.wikipedia.org/wiki?curid=39331606", "title": "Alexander Klöpping", "text": "Alexander Klöpping\n\nAlexander Paul Klöpping (; born 21 January 1987) is a Dutch internet entrepreneur specializing in consumer electronics, blogging, and is also a print and online journalist and speaker. Klöpping studies new media at the University of Amsterdam and is a self-described nerd. He is consulted by Dutch-language media for his expertise. He himself states that his expertise is \"relative\". \n\nKlöpping started an online company, \"The Gadget Company\", at 16 years of age. In 2005, he went to the United States to study, for half a year at a university-oriented to the Democratic Party and another half year at one oriented towards the Republican Party and participated in political campaigns of both parties. He wrote a thesis about the Barack Obama campaign. Starting in March 2008, he wrote for Dutch financial journalist Jort Kelder's website \"925.nl\", became editor a few months later and stayed in that position until June 2009.\n\nHe also writes articles for Dutch media Emerce, NU.nl, NRC Next (once every two weeks on Wednesdays), Vrij Nederland, NTR Schooltv and STER Nieuws.\n\nKlöpping has been working on Dutch television since 2009. He produced and presented Internet news for \"Bij ons in de BV\" and \"NTR School TV\". He is best known for appearing regularly as a technology expert on the talk show De Wereld Draait Door. There, he has demonstrated items including the iPad and Kinect, and commented on the rise of social media, WikiLeaks, the Anonymous cyber-attacks, and Facebook privacy policy. He is an advisor for Mediafonds and member of the jury on \"TV Lab\".\n\nIn 2011, he published the book \"Wikileaks, alles wat je niet mocht weten\" (\"Wikileaks, everything you weren't allowed to know\").\n\nIn 2013, he joined the Committee of Recommendation of Dutch whistleblower foundation Publeaks that launched a whisteblowing initiative in September 2013 based on the GlobaLeaks software.\n\nIn 2013, Klöpping founded the Universiteit van Nederland (\"University of the Netherlands\"), that organizes and publishes lectures. That year, Klöpping himself taught three classes on the television series \"DWDD University\" about the history, present and future of Silicon Valley. Klöpping is a co-founder of Blendle, a platform with articles of thirty major Dutch newspapers and magazines. At Blendle, launched in April 2014, readers pay per article.\n\n\n"}
{"id": "4950897", "url": "https://en.wikipedia.org/wiki?curid=4950897", "title": "Association of Old Crows", "text": "Association of Old Crows\n\nThe Association of Old Crows is an international professional organization specializing in electronic warfare, tactical information operations, and associated disciplines headquartered in Alexandria, Virginia. This nonprofit organization's mission is to \"advocate the need for a strong defense capability emphasizing electronic warfare and information operations to government, industry, academia, and the public.\"\n\nThe name \"Old Crows\" emerged from the first use of electronic warfare in World War II to disrupt Axis communications and radars. Allied equipment and operators were known by the code name \"Raven\". Common jargon changed the name to \"Crows\" and those engaged in the profession became known as \"Old Crows\".\n\nThe organization draws expertise and information from its members, a pool of thousands of individuals including technology specialists and actual military personnel, and is involved in advancing electronic warfare and information-gathering techniques, disseminating information on these topics, and supporting the education of personnel in related scientific matters.\n\nIt publishes the \"Journal of Electronic Defense,\" a journal covering international developments in the fields of electronic warfare, signals intelligence, electronic intelligence and communications intelligence.\n\n"}
{"id": "55960563", "url": "https://en.wikipedia.org/wiki?curid=55960563", "title": "Autogenous welding", "text": "Autogenous welding\n\nAn autogenous weld is a form of welding, where the filler material is either supplied by melting the base material, or is of identical composition. The weld may be formed entirely by melting parts of the base metal and no additional filler rod is used. \n\nThere is some variation in the use of this term. Those bodies concerned with teaching the craft skill of welding tend to define it as using no filler rod, i.e. the technique is based purely on the base metal. Those concerned with the welded joint's metallurgy may make no distinction between a filler rod and the base metal, provided that the final metallurgy is identical.\n\nMost welding processes may be either autogenous, or may use additional filler. Some are characteristically autogenous and avoid filler. Some arc welding processes, including such major process such as Manual Metal Arc (stick) welding and MAGS (wire-feed) welding, cannot be used autogenously, as they rely on the consumption of a filler rod to provide the arc.\n\nSome processes are typically autogenous. These include some gas welding processes such as lead burning (although fillers may optionally be used) and oxy-acetylene welding in some positions, such as seam welding the edges of two overlapping sheets. Resistance welding, both spot welding and seam welding, is inherently autogenous as there is no convenient way to apply a filler. Friction and laser welding have similar restrictions.\n\nSome alloys are prone to changing their composition when heated, particularly a loss of zinc from brass by its evaporation as vapour. In these cases, an excess of 2–3% extra zinc may be provided in the filler rod to compensate. Silicon may also be used as an additive to reduce this loss.\n\nA few materials, such as the HY-80 series of high-strength steels, require a non-autogenous process to control their metallurgy. However advanced processes, such as hybrid laser arc welding, have been used to achieve the same effect autogenously.\n"}
{"id": "47337575", "url": "https://en.wikipedia.org/wiki?curid=47337575", "title": "Barbara Grant", "text": "Barbara Grant\n\nDr. Barbara Grant is an American businesswoman. She served in a variety of senior management positions at IBM for 21 years including vice president and General Manager in the Data Storage Division of Removable Media Storage Solutions. She continues to serve start-up and major corporations in a variety of capacities.\n\nGrant received her Bachelor of Science degree in Chemistry from Arizona State University and her Ph.D. in Organic Chemistry from Stanford University.\n\nGrant was employed at IBM for 21 years where she held several executive positions. Her last position was Vice President and General Manager in the Data Storage Division.\n\nOver her career she helped develop and introduce over 50 new products and received 8 patents. She has authored many publications in a variety of technology sectors. In 1996 she was elected to the inaugural group of the Women in Technology Hall of Fame.\n\n\n"}
{"id": "33263727", "url": "https://en.wikipedia.org/wiki?curid=33263727", "title": "Bridon Ropes", "text": "Bridon Ropes\n\nBridon Ropes, also known as Bridon International Ltd is a manufacturing company in Doncaster in South Yorkshire that makes wire rope. It is now part of the Belgium-based Bekaert company. The name Bridon originates from the concatenation or contraction of the words British and Doncaster; it is not a family name.\n\nThe company can trace its history in Doncaster back to 1789.\n\nIt was formed in 1924 as British Ropes Limited. British Ropes Ltd was headquartered at 32 Cavendish Square. British Ropes was state-owned. From the 1960s to 1980s, British Ropes headquarters was at Warmsworth Hall at Warmsworth, off the A630 near the B6376 junction.\n\nIn 1974 it became known as Bridon; Bridon was the name of one of their brands of wire rope. In 1976 the company was turning over £243 million. In 1978 the company was registered on the Stock Exchange. Bridon and British Steel jointly owned the large Templeborough Rolling Mills; this is now the Magna Science Adventure Centre; 40% of Templeborough's production went to Bridon. \n\nIn 1991 its workforce was 5,400 which dropped to 4,000 at the end of 1993, and produced in 20 countries with 76 service centres; it was the world's biggest supplier of wire rope. In April 1993 it won a Queen's Award for Technological Achievement for its Brifen fencing.\n\nPrince Andrew, Duke of York visited the site on 20 October 2011.\n\nIn 1997 it was bought by FKI, who were bought by Melrose Industries in 2008. In 2014 it was bought by the Ontario Teachers' Pension Plan for £365 million. In December 2015 it merged with a Belgium wire rope company.\n\nIt is headquartered in the south-west of Doncaster. It makes wire rope at four sites in the UK, and in Germany and the USA.\n\nIt makes wire rope for bridges, cranes, elevators and mine shafts. It claims to be the only company in the world that makes wire rope from hot rolled steel. A brand of its wire rope is called Dyform; Dyform was introduced in the late 1960s as a seven-wire tendon for prestressed concrete construction.\n\nTheir wire rope has been made widespread for prestressed concrete bridges; the BRIDON brand is found on many prestressed concrete bridges around the world. \n\n\n"}
{"id": "44373733", "url": "https://en.wikipedia.org/wiki?curid=44373733", "title": "Brightstorm", "text": "Brightstorm\n\nBrightstorm’’’’is an online learning platform for teenagers. It features thousands of study videos as well as other study tools and resources such as Math Genie and College Counseling. Study videos cover math courses ranging from pre-algebra to calculus as well as English, science, and test prep for SAT, ACT, and Advanced Placement tests. The website is subscription-based and allows users to watch study videos without third-party advertisements. It is reported that Brightstorm has delivered over 20 million lessons to more than 240,000 registered users from over 200 countries.\n\nBrightstorm was founded by Bumsoo Kim, Jeff Marshall, and Chris Walsh in 2008. Bumsoo Kim, then investment principal at KTB Ventures, had interests in investing in a K-12 online education company as a result of the unfulfilled potential he perceived within the industry at the time. Kim is originally from South Korea where college entrance is hyper-competitive and, thanks to the country’s early adoption of nationwide high-speed internet, has had many successful online education services. During the development stage of the business idea for Brightstorm, Kim met Marshall and Walsh through KTB and the three co-founded Brightstorm for which KTB led a Series A investment. Chris Walsh left Brightstorm in spring of 2009 and Jeff Marshall in fall of 2010.\n\nBrightstorm’s business model has changed several times throughout the course of its existence. From the launch of its online services in 2008 until August 2009, the course model of its content was subject-oriented. In September 2009, Brightstorm launched its “free math help” section which contained more the 2,500 math videos across 6 math subjects. At this time, Brightstorm adopted a freemium business model which offered free content in math and science and paid content in test prep for SAT, ACT, and Advanced Placement tests. In July 2012, Brightstorm changed to an all-you-can-eat business model where users need a monthly subscription to watch all videos offered on the site. This subscription model is their current business model. In September 2014, Brightstorm started offering Math Genie and College Counseling to its users.\n\nBrightstorm is an advocate and example of flipped classroom teaching where students can learn on their own through online videos. Many schools across the U.S. have used Brightstorm to implement this style of teaching for their students. Another major advocate of flip teaching or blended learning is Khan Academy.\n\nBrightstorm offers over 3,500 study videos in over 20 subjects. This includes over 1,000 sample problems. Study videos recreate the classroom experience with real teachers in front of a whiteboard. Sample-problem videos use pencast technology to provide users with step-by-step instruction. The majority of Brightstorm teachers possess master's degrees or PhDs from widely recognized universities.\n\nSubjects and courses covered by Brightstorm include:\n\nAll test prep courses offer worksheets, study guides, and practice tests. SAT and ACT courses also offer diagnostic tests as well as two full-length tests. Other services provided by Brightstorm include Math Genie and College Counseling.\n\nMath Genie provides step-by-step solutions to math problems uploaded by users. Genie covers math problems ranging from pre-algebra to calculus. Users may ask up to three math problems per month and are guaranteed to receive solutions within 48 hours.\n\nCollege Counseling provides answers to academic questions commonly asked by high school students and their parents. These videos are created by a professional college admissions counselor and include topics such as How to Apply to College, How to Write a College Essay, How to Choose Extracurricular Activities, and How to Get Better Grades. Users may also upload and ask to have their own personal questions answered.\n\n"}
{"id": "956907", "url": "https://en.wikipedia.org/wiki?curid=956907", "title": "Carsharing", "text": "Carsharing\n\nCarsharing or car sharing (AU, NZ, CA, & US) or car clubs (UK) is a model of car rental where people rent cars for short periods of time, often by the hour. They are attractive to customers who make only occasional use of a vehicle, as well as others who would like occasional access to a vehicle of a different type than they use day-to-day. The organization renting the cars may be a commercial business or the users may be organized as a company, public agency, cooperative, or \"ad hoc\" grouping. It differs from traditional car rental in that the owners of the cars are often private individuals themselves, and the carsharing facilitator is generally distinct from the car owner. Carsharing is part of a larger trend of shared mobility. Shared mobility includes all modes of travel that offer short-term access to transportation on an on-needed basis either for personal transportation or goods delivery. \n\nCarsharing services are available in over 1,000 cities in several countries. \n\n, there were an estimated 1.7 million car-sharing members in 27 countries, including so-called peer-to-peer services, according to the Transportation Sustainability Research Center at U.C. Berkeley. Of these, 800,000 were car-sharing members in the United States. \n\nAs of July 2017, car2go is the largest carsharing company in the world with 2,500,000 registered members and a fleet of nearly 14,000 vehicles in 26 locations in North America, Europe and Asia., followed by Zipcar with 767,000 members and 11,000 vehicles .\n\nAccording to Navigant Consulting, global carsharing services revenue will approach billion in 2013 and grow to billion by 2020, with over 12 million members worldwide. The main factors driving the growth of carsharing are the rising levels of congestion faced by city dwellers; shifting generational mindsets about car ownership; the increasing costs of personal vehicle ownership; and a convergence of business models. Carsharing contributes to sustainable transport because it is a less car intensive means of urban transport, and according to \"The Economist\", carsharing can reduce car ownership at an estimated rate of one rental car replacing 15 owned vehicles.\n\nIn the United Kingdom, where it is a recent development, the term \"car clubs\" is used for what in the United States is called \"carsharing\", \"car sharing\" or \"car-sharing\". In the UK, \"car sharing\" refers to what is called \"carpooling\" or \"ride sharing\" in the US, namely the shared use of a car for a specific journey, in particular for commuting to work, often by people who each have a car but travel together to save costs; in South Africa, this is called a \"lift scheme\". In the UK, a \"car pool\" refers to a fleet of cars made available by an organization to its employees (which is usually referred to as a \"motor pool\" in the US), for example to travel to customers or between different office locations.\n\nIn India, in contrast to the vast majority of the Indian car rental market that is defined by chauffeured-service, Indian people refer to car-sharing as \"self-drive.\"\n\nCarsharing benefits individuals who can gain the benefits of private cars without having costs and responsibilities associated with car ownership. Instead a household accesses a fleet of vehicles on an as-needed basis. Carsharing may be thought of as organized short-term car rental.\n\nCarsharing has sprung up in different parts of the world and operations are organized in many different ways in different places. Sizes of organizations vary from one shared car, and only a handful of sharers to organizations that serve a complete urban area.\n\nGenerally, carsharing programs can be qualified as one of four sharing types: Roundtrip, one-way, peer-to-peer, and fractional. In roundtrip carsharing, members begin and end their trip, often paying by the hour, mile, or both. One-way carsharing, on the other hand, enables members to begin and end their trip at different locations through free floating zones or station-based models with designated parking locations. Peer-to-peer carsharing (sometimes referred to as Personal Vehicle Sharing) operates similarly to roundtrip carsharing in trip and payment type; however, the vehicles themselves are typically privately owned or leased with the sharing system operated by a third-party. The fractional ownership model allows users to co-own a vehicle and share its costs and use. \n\nCarsharing differs from traditional car rentals in the following ways:\n\nSome carshare operations (CSOs) cooperate with local car rental firms, in particular in situations wherein classic rental may be the cheaper option.\n\nCarsharing can provide numerous transportation, land use, environmental, and social benefits. Neighborhood carsharing is often promoted as an alternative to owning a car where public transit, walking, and cycling can be used most of the time and a car is only necessary for out-of-town trips, moving large items, or special occasions. It can also be an alternative to owning multiple cars for households with more than one driver. A long-term study of City CarShare members found that 30 percent of households that joined sold a car; others delayed purchasing one. Transit use, bicycling, and walking also increased among members. City CarShare's early adopters were found to be mostly non-car-owners, resulting in increased car-miles traveled one-year into the program ; two years into the program, car-sharing began to enter the mainstream of San Francisco's traveling public, prompting many families to give up second cars, and by year five, nearly a third of City CarShare's member households had shed a private motorized vehicle. A study of driving behavior of members from major carsharing organizations found an average decline in 27% of annual vehicle kilometres travelled (VKT). That said, it is important to note that the success of carsharing programs depends on if it provides the consumers with \"better mobility or sufficient mobility at reduced cost.\" \n\nCarsharing is generally not cost-effective for commuting to a full-time job on a regular basis. Most carsharing advocates, operators, and cooperating public agencies believe that those who do not drive daily or who drive less than 10,000 kilometers (about 6,200 statute miles) annually may find carsharing to be more cost-effective than car ownership. But variations of 50% on this figure are reported by operators and others depending on local context. If occasional use of a shared vehicle costs significantly less than car ownership, then carsharing makes automobile use more accessible to low-income households.\n\nCarsharing can also help reduce congestion and pollution. It is noted as a tool for achieving VMT and GHG reduction targets in the California Transport Plan (CTP) 2040. Replacing private automobiles with shared ones directly reduces demand for parking spaces. The fact that only a certain number of cars can be in use at any one time may reduce traffic congestion at peak times. Even more important for congestion, the strong metering of costs provides a cost incentive to drive less. With owned automobiles many expenses are sunk costs and thus independent of how much the car is driven (such as original purchase, insurance, registration, and some maintenance).\n\nSuccessful carsharing development has tended to be associated mainly with densely populated areas, such as city centers and more recently university and other campuses. There are some programs (mostly in Europe) for providing services in lower density and rural areas. Low-density areas are considered more difficult to serve with carsharing because of the lack of alternative modes of transportation and the potentially larger distance that users must travel to reach the cars.\n\nPeople who have joined carsharing tend to sell either their primary, secondary, or another off-hand car, after using the service. This reduces the cost of transportation per month by an average of $135 – $435, based on University of Berkeley's Research in 2008.\n\nThe technology of CSOs varies enormously, from simple manual systems using key boxes and log books to increasingly complex computer-based systems (e.g. partially automated and fully automated systems) with supporting software packages that handle a growing array of back office functions. The simplest CSOs have only one or two pick-up points, but more advanced systems allow cars to be picked up and dropped off at any available public parking space within a designated operating area.\n\nWhile differing markedly in their objectives, size, business models, levels of ambition, technology and target markets, these programs do share many features. The more established operations usually require a check of past driving records and a monthly or annual fee in order to become a member. The cost and maximum time a car may be used also varies.\n\nTo make a reservation, one can either make a reservation online, by phone, or by text messages depending on the company’s flexibility. There is a higher chance of availability the earlier the reservation is. If a reservation is cancelled however, one may still be charged.\n\nOnce the reservations are completed and confirmed, the car will then be delivered at the time and place scheduled. There will be a small card reader mounted on the windshield. Once the customer places their membership card on the reader, it will use what is called blink technology to activate the time and unlock the car. The reader will not work until it is time for that specific reservation. The keys can then be found somewhere inside the car such as the glove compartment. Depending on the company, the customer may be provided with a key to a lock box that contains the ignition key itself. Once the customer is set, they are off to their next destination.\n\nAlthough members are often responsible for cleaning the car and filling up the tank when low, the carsharing company is generally responsible for the long-term maintenance of the vehicles. Members have to make sure that when they are finished, the car is ready for the next user to move on.\n\nThe first reference to carsharing in print identifies the \"Selbstfahrergenossenschaft\" carshare program in a housing cooperative that got underway in Zürich in 1948, but there was no known formal development of the concept in the next few years. By the 1960s, as innovators, industrialists, cities, and public authorities studied the possibility of high-technology transportation—mainly computer-based small vehicle systems (almost all of them on separate guideways)—it was possible to spot some early precursors to present-day service ideas and control technologies.\n\nThe early 1970s saw the first whole-system carshare projects. The ProcoTip system in France lasted only about two years. A much more ambitious project called the Witkar was launched in Amsterdam by the founders of the 1965 white bicycles project. A sophisticated project based on small electric vehicles, electronic controls for reservations and return, and plans for a large number of stations covering the entire city, the project endured into the mid-1980s before finally being abandoned.\n\nIn July, 1977, the first official British experiment in carsharing started in Suffolk. An office in Ipswich provided a \"Share-a-Car\" service for \"putting motorists who are interested in sharing car journeys in touch with each other.\" In 1978, the Agricultural Research Council granted the University of Leeds £16,577 \"for an investigation and simulation of carsharing\". The scheme was not intended for different drivers of a single car but for a driver offering seats in his car (Real-time ridesharing).\n\nThe 1980s and first half of the 1990s was a \"coming of age\" period for carsharing, with continued slow growth, mainly of smaller non-profit systems, many in Switzerland and Germany but also on a smaller scale in Canada, the Netherlands, Sweden, and the United States.\n\nCarsharing in NA was founded in Quebec City in 1994 by Benoît Robert, with a company called Communauto that is still a leader in carsharing globally. Cycling advocate and environmentalist Claire Morissette (1950–2007) played a major role in its evolution starting in 1995, when Communauto established itself in Montreal as a private company. The company goal is to provide a convenient and economical alternative to owning a car.\n\nZipcar, Flexcar (bought by Zipcar in 2007), and City Car Club were all started in 2000. Several car rental companies launched their own carsharing services beginning in 2008, including Avis On Location by Avis, Hertz on Demand (formerly known as Connect by Hertz), operating in the U.S. and Europe; Uhaul Car Share owned by U-Haul, and WeCar by Enterprise Rent-A-Car. By 2010, when various peer-to-peer carsharing systems were introduced. Zipcar accounted for 80% of the U.S carsharing market and half of all car-sharers worldwide with 730,000 members sharing 11,000 vehicles.\n\nCarsharing has also spread to the developing world (Argentina, Brazil, China, India, Mexico, and Turkey) because population density is often a critical determinant of success for carsharing, and developing nations often have dense urban populations.\n\nMany building developers are now incorporating share-cars into their developments as an added value to tenants, and municipal government bodies around the world are starting to stipulate the implementation of a carsharing service in new buildings, as a sustainability initiative. These trends have created a demand for a new model of carsharing - residential, private-access share-cars that are typically underwritten by the Homeowner association.\n\nAdapting carsharing vehicles to persons with physical disabilities presents special challenges not faced by traditional car rental. With carsharing no mechanic is present to install or adjust adaptive equipment, and that equipment is left unattended after each use. In 2008, City CarShare introduced the first wheelchair carrying car share vehicle, the \"Access Mobile\", specifically designed as a fleet vehicle shared with, not segregated from, non-wheelchair users.\n\nCarsharing operators are increasingly opting to brand parts of their fleets with third-party advertising in order to increase revenue and improve competitiveness. Transit media, as this out-of-home advertising medium is referred to, is a strategy currently (or soon to be) employed by larger carsharing operators such as Canada's AutoShare and the UK's City Car Club.\n\nFor future applications, many carsharing companies are now investing in plug-in hybrid electric vehicles (PHEV). With the use of these types of vehicles, cost of gas consumption can be greatly reduced. Since most customers do not need the vehicle for long amounts of time or distance, it gives the carsharing company time to collect and recharge these vehicles for additional use. This application can greatly reduce carbon emissions and improve city environments. Another innovation is to calculate and compensate all emissions on behalf of your drivers according to the Kyoto protocol, e.g. via reforestation schemes. The world´s first certified carbon neutral carsharing service is Respiro carsharing in Madrid and is also done by Australian p2p car sharing platform Car Next Door.\n\nAlso, carsharing can be considered as an example of a technological change in consumption, which is a ‘process of mutual adjustment between innovation and its socio-economic environment.’ This type of innovation poses much potential due to the current state of technological change based on today’s passenger transportation capabilities.\n\nOther countries are already moving into designing concept cars that is solely based as an urban public vehicle. The Phiaro P70t Conch, Japan’s new concept vehicle, is a completely battery-powered, three-seater vehicle which was designed for the purpose of carsharing. The vehicle was made to be small and compact enough to be driven around urban environments without sacrificing parking. The promotion of these kinds of concept vehicles have caught the attention of automotive companies worldwide.\n\nIn Germany a pilot project has been started by the semiconductor manufacturer Infineon to replace regular pool vehicles with a corporate carsharing system. Corporate carsharing is a more economical solution than company cars in regard to administration, travel and running expenses and maintenance.\n\nAlong with EVs, one of the most important technological innovations to affect the carsharing market is self-driving cars. It is expected that most self-driving vehicles won't be owned by individuals, but will rather be shared. Some companies, like Ernst and Young, have also started to use blockchain technology to record ownership, usage of shared vehicles and insurance information.\n\nThe amount of insurance provided greatly varies among companies, but all carsharing firms provide insurance that at least meets the legal minimum requirements for the given region of operation. However, critics such as Rob Lieber of \"The New York Times\" has criticized firms such as Zipcar for the paltry coverage afforded carsharing drivers.\n\n\n"}
{"id": "18807965", "url": "https://en.wikipedia.org/wiki?curid=18807965", "title": "Devico AS", "text": "Devico AS\n\nDevico AS is the sole company to offer directional core drilling services worldwide. Devico AS also rents various borehole survey instruments for the mining industry and geotechnical projects. The head office is located in Melhus, Norway. The company have until now been operating in more than 30 countries worldwide.\n\nLaunched in 1988 by Mr. Viktor Tokle.\n\nThe DeviDrill, Devico AS directional core barrel and rotary steerable system, was launched in 1988. It was in use until 2001 when the wireline version of the DeviDrill was introduced. The new version was a big improvement since it allows the drill string to stay in the borehole during steering and coring.\n\nDevico AS has developed their own set of borehole survey instruments to be used together with the DeviDrill. In 2002 a miniature version of the survey tool was launched that made it possible to survey throughout the drill bit.\n\nTo survey in magnetic environment, Devico has developed a strain gauge based survey tool named DeviFlex. This tool became available in 2004.\n\nwww.devico.com<br>\n"}
{"id": "15042168", "url": "https://en.wikipedia.org/wiki?curid=15042168", "title": "Document engineering", "text": "Document engineering\n\nDocument engineering is a document-centric synthesis of complementary ideas from information and systems analysis, electronic publishing, business process analysis, and business informatics to ensure that the documents and processes make sense to the people and applications that need them. Originating from research published by Robert J. Glushko and Tim McGrath, document engineering attempts to unify these different analysis and modeling perspectives and helps to specify, design, and implement documents and the processes that create and consume them.\n\nIn the context of document engineering, \"document\" generally refers to ordered pieces of information used by computer applications or web services rather than directly by people. It has particular relevance in the areas of XML vocabulary design. The principles of document engineering were applied to the development of the OASIS Universal Business Language.\n\nFrom 2003 to 2007, the University of California, Berkeley operated a research center for document engineering, which has been subsumed by its program in Information and Service Design.\n\nThe conventional discipline that most resembles document engineering is probably business informatics. However, document engineering emphasizes the need for conceptual modeling of documents and processes at an implementable granularity.\n\nThe ACM Symposium on Document Engineering is a yearly conference of computer scientists interested in text or document processing.\n\n"}
{"id": "36080474", "url": "https://en.wikipedia.org/wiki?curid=36080474", "title": "Don Pottery", "text": "Don Pottery\n\nThe Don Pottery was a 19th-century manufacturer of porcelain, whose factory was located in the town of Swinton in South Yorkshire, England. It is not to be confused with the Swinton Pottery.\n\nThe pottery was established in 1801 by John and William Green, largely producing good quality, mostly cream-coloured, earthenware.\n\nThe production of porcelain at the factory began in 1810. Potteries in the West Riding of Yorkshire had been increasing in number throughout the 18th century but English experimentation with porcelain had been concentrated in the south and Midlands until this point, although a number of potteries in the vicinity of the Don Pottery began making porcelain shortly afterwards, including the world-famous Rockingham Pottery.\n\nDon pottery porcelain which was produced between 1810 and 1830 was of a very high quality and was exported worldwide.\n\nWhilst the pottery was initially very successful, financial problems arose in the 1830s and the Greens were forced to sell the pottery in 1839, when it was purchased by Barker and Sons, who owned the nearby Mexborough Pottery.\n\nThe factory was closed in 1893 as a result of further financial problems.\n\n"}
{"id": "57124265", "url": "https://en.wikipedia.org/wiki?curid=57124265", "title": "Electric road", "text": "Electric road\n\nAn electric road or \"eroad\" is a road which supplies electric power to vehicles travelling on it, either through trolley wires above the road or through conductor rails embedded in its surface. Two concepts are being tested, which may be likened to trolleybuses and to slot cars respectively.\n\nThe Swedish government has allocated a budget of 123 million kronor to part-finance two experimental stretches of electric road, as part of its plan to eliminate fossil fuelled road transport by 2030. These tests will continue through 2018.\n\n\n\nThe German Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) and the state of Hessen have supported the installation of trolley wires above a 10-kilometre length of the A5 \"Autobahn\", between the Zeppelinheim/Cargo City Süd junction near Frankfurt Airport and the Darmstadt/Weiterstadt junction. It powers lorries fitted with pantographs.\n\nThis system is also being trialled on a mile-long stretch of South Alameda Street between East Lomita Boulevard and the Dominguez Channel in Carson, California.\n\n\n\n"}
{"id": "1340430", "url": "https://en.wikipedia.org/wiki?curid=1340430", "title": "Emergency light", "text": "Emergency light\n\nAn emergency light is a battery-backed lighting device that switches on automatically when a building experiences a power outage. Emergency lights are standard in new commercial and high occupancy residential buildings, such as college dormitories, apartments, and hotels. Most building codes require that they be installed in older buildings as well.\n\nIncandescent light bulbs were originally used in emergency lights, before fluorescent lights and later light-emitting diodes (LEDs) superseded them in the 21st century.\n\nBy the nature of the device, an emergency light is designed to come on when the power goes out. Every model, therefore, requires some sort of a battery or generator system that could provide electricity to the lights during a blackout. The earliest models were incandescent light bulbs which could dimly light an area during a blackout and perhaps provide enough light to solve the power problem or evacuate the building. It was quickly realized, however, that a more focused, brighter, and longer-lasting light was needed. The modern emergency floodlight provides a high-lumen, wide-coverage light that can illuminate an area quite well. Some lights are halogen, and provide a light source and intensity similar to that of an automobile headlight.\n\nEarly battery backup systems were huge, dwarfing the size of the lights for which they provided power. The systems normally used lead acid batteries to store a full 120-volt charge. For comparison, an automobile uses a single lead acid battery as part of the ignition system. Simple transistor or relay technology was used to switch on the lights and battery supply in the event of a power failure. The size of these units, as well as the weight and cost, made them relatively rare installations. As technology developed further, the voltage requirements for lights dropped, and subsequently the size of the batteries was reduced as well. Modern lights are only as large as the bulbs themselves - the battery fits quite well in the base of the fixture.\n\nModern emergency lighting is installed in virtually every commercial and high occupancy residential building. The lights consist of one or more incandescent bulbs or one or more clusters of high-intensity light-emitting diodes (LED). The emergency lighting heads are usually either PAR 36 sealed beams or wedge base lamps. All units have some sort of a device to focus and intensify the light they produce. This can either be in the form of a plastic cover over the fixture, or a reflector placed behind the light source. Most individual light sources can be rotated and aimed for where light is needed most in an emergency, such as toward fire exits. Modern fixtures usually have a test button of some sort which temporarily overrides the unit and causes it to switch on the lights and operate from battery power even if the main power is still on. Modern systems are operated with relatively low voltage, usually from 6-12 volts. This both reduces the size of the batteries required and reduces the load on the circuit to which the emergency light is wired. Modern fixtures include a small transformer in the base of the fixture which steps-down the voltage from main current to the low voltage required by the lights. Batteries are commonly made of lead-calcium, and can last for 10 years or more on continuous charge. U.S. fire safety codes require a minimum of 90 minutes on battery power during a power outage along the path of egress.\n\nTo indicate that a power outage has occurred, some models of emergency lights can only be shut off manually after they have been activated, even if the main building power has come back on. The system will stay lit until the reset button on the side of the unit is pressed.\n\nEmergency lighting is often referred to as egress lighting. Emergency lights are used in commercial buildings as a safety precaution to power outages, so that people will be able to find their way out of a building. Exit signs are often used in conjunction with emergency lighting.\n\nNew York City requires emergency lights to carry a Calendar Number signifying approval for local installation, Chicago requires emergency lighting to have a metal face plate, and Los Angeles requires additional exit signs be installed within of the floor around doors to mark exits during a fire, as smoke rises and tends to block out higher installed units.\n\nAs there are strict requirements to provide an average of one foot candle of light along the path of egress, emergency lighting should be selected carefully to ensure codes are met. \n\nIn recent years, emergency lighting has made less use of the traditional two-head unit - with manufacturers stretching the concept of emergency lighting to accommodate and integrate emergency lighting into the architecture.\n\nAn emergency lighting installation may be either a central standby source such as a bank of lead acid batteries and control gear/chargers supplying slave fittings throughout the building, or may be constructed using self-contained emergency fittings which incorporate the lamp, battery, charger and control equipment.\n\nSelf-contained emergency lighting fittings may operate in \"Maintained\" mode (illuminated all the time or controlled by a switch) or \"Non-Maintained\" mode (illuminated only when the normal supply fails).\n\nSome emergency lighting manufacturers offer dimming solutions for common area emergency lighting to allow energy savings for building owners when unoccupied using embedded sensors.\n\nAnother popular method for lighting designers, architects and contractors are battery backup ballasts that install within or adjacent to existing lighting fixtures. Upon sensing power loss, the ballasts switch into emergency mode turning the existing lighting into emergency lighting in order to meet both the NFPA's Life Safety Code and the national Electric Code without the need of wiring separate circuits or external wall mounts.\n\nCodes of practice for remote mounted emergency lighting generally mandate that wiring from the central power source to emergency luminaires be kept segregated from other wiring, and constructed in fire resistant cabling and wiring systems.\n\nCodes of practice lay down minimum illumination levels in escape routes and open areas. Codes of practice also lay down requirements governing siting of emergency lighting fittings, for example the UK code of practice, BS5266, specifies that a fitting must be within horizontal distance of a fire alarm call point or location for fire fighting appliances.\n\nThe most recent codes of practice require the designer to allow for both failure of the supply to the building and the failure of an individual lighting circuit. BS5266 requires that when Non Maintained fittings are used, they must be supplied from the same final circuit as the main lighting circuit in the area.\n\nIEC 60598-2-22 Ed. 3.0: Luminaires - Part 2-22: Particular requirements - Luminaires for emergency lighting \n\nIEC 60364-5-56 Ed. 2.0: Low-voltage electrical installations - Part 5-56: Selection and erection of electrical equipment - Safety services\n\nISO 30061:2007 (CIE S 020/E:2007): Emergency lighting (specifies the luminous requirements for emergency lighting systems)\n\nFor UK regulations, two types are distinguished:\n\n\n"}
{"id": "1237690", "url": "https://en.wikipedia.org/wiki?curid=1237690", "title": "Emotional Design", "text": "Emotional Design\n\nEmotional Design is both the title of a book by Donald Norman and of the concept it represents.\n\nThe main topic covered is how emotions have a crucial role in the human ability to understand the world, and how they learn new things. For example: aesthetically pleasing objects appear to the user to be more effective, by virtue of their sensual appeal. This is due to the affinity the user feels for an object that appeals to them, due to the formation of an emotional connection with the object.\n\nNorman's approach is based on classical ABC model of attitudes. However, he changed the concept to be suitable for application in design. The three dimensions have new names (visceral, behavioral and reflective level) and partially new content. In the book, Norman shows that design of most objects are perceived on all three levels (dimensions). Therefore, a good design should address all three levels. Norman also mentions in his book that \"technology should bring more to our lives than the improved performance of tasks: it should be richness and enjoyment.\" (pg 101) He stresses the importance of creating fun and pleasurable products instead of dull and dreary ones. By mixing all three design levels and the four pleasures by Patrick W. Jordan, the product should evoke an emotion when the user is interacting with the product.\n\nEmotional design is an important element when generating ideas for human-centred opportunities. People can more easily relate to a product, a service, a system, or an experience when they are able to connect with it at a personal level. Rather than thinking that there is one solution for all, both Norman's three design levels and Jordan's four pleasures of design can help us design for each individuals needs. Both concepts can be used as tools to better connect with the end user that it is being design for.\n\nThe visceral level concerns itself with the aesthetic or attractiveness of an object. The behavioural level considers the function and usability of the product. The reflective level takes into account prestige and value; this is often influenced by the branding of a product.\n\nThe front cover of \"Emotional Design\" showcases Philippe Starck's Juicy Salif, an icon of industrial design that Norman heralds as an \"item of seduction\" and the manifestation of his thesis.\n\nEmotional design is also influenced by the four pleasures, identified in \"Designing Pleasurable Products\" by Patrick W. Jordan. In this book Patrick W. Jordan builds on the work of Lionel Tiger to identify the four kinds of pleasures. Jordan describes these as “modes of motivation that enhance a product or a service. Life is unenjoyable without appreciating what we do, and it is human intuition to seek pleasure.” The idea of incorporating pleasure into products is to provide the buyer with an added experience. Patrick W. Jordan points out in his book that a product should be more than something functional and/or aesthetically pleasing and it should evoke an emotion through the use of pleasures. Although it is hard to achieve all four pleasures into one product, by simply focusing on one, it might be what can bring a product from being chosen over another. The four pleasures that could be implemented into products or a service are:\n\nPhysio-pleasure deals with the body and pleasure derived from the sensory organs. This includes taste, touch, and smell, as well as sexual and sensual pleasure. In the context of products, these pleasures can be associated with tactile properties (the way interaction with the product feels) or olfactory properties (the leather smell in a new car, for example).\n\nSocio-pleasure is the enjoyment derived from the company of others. Products can facilitate social interaction in a number of ways, either through providing a service that brings people together (a coffee-maker enabling a host to provide their guests with fresh coffee) or by being a talking point in and of itself.\n\nPsycho-pleasure is defined as pleasure which is gained from the accomplishment of a task. In a product context, psycho-pleasure relates to the extent in which a product can help in task completion and make the accomplishment a satisfying experience. This pleasure may also take into account the efficiency with which a task can be completed (a word processor with built-in formatting decreasing the amount of time spent on creating a document, for example).\n\nIdeo-pleasure refers to pleasure derived from theoretical entities such as books, music, and art. It may relate to the aesthetics of a product and the values it embodies. A product made of bio-degradable material, for example, can be seen as holding value in the environment which, in turn, may appeal to someone who wishes to be environmentally responsible.\n\n"}
{"id": "5567886", "url": "https://en.wikipedia.org/wiki?curid=5567886", "title": "Expandable tubular technology", "text": "Expandable tubular technology\n\nExpandable tubular technology is a system for increasing the diameter of the casing or liner of an oil well by up to 30% after it has been run down-hole.\n\nDuring the planning stages, one of the primary considerations that the well engineer takes into account is the density (or \"weight\") of the drilling fluid. It must be heavy enough to suppress the pressure in exposed permeable formations (thus avoiding a blowout), and yet light enough to avoid breaking down the rock itself. Both parameters generally increase with depth, and create a window in which the drilling fluid density is safe. However, the mud density required to suppress fluid pressures at, say, 9000' would usually break down the formation further up the well at, say, 1000'. Therefore, the well is drilled in sections by running casing strings to cover depth ranges between which the required mud densities are suitable for that entire range. However, each bit must be smaller than the previous casing string, which in turn has to be smaller than the previous hole.\n\nThis requirement of well design creates a well where each hole section is ever-decreasing in diameter. The only conventional way to combat this effect is to start with an enormous hole at the top (sometimes 30\") in order to run as many as five casing strings and still end up with a 6\" hole in the targeted reservoir. And this well design leaves little room for error: if the drillers are forced to run yet another casing string on account of unexpected formation pressures or hole conditions, then the target formation may be impossible to reach. Thus there is much to favor a casing system that loses minimal - or possibly no - diameter while still isolating a whole section. Many systems under development have the aim of producing a monobore well, that is, a well with a single diameter from top to bottom.\n\nTo reduce the loss of diameter each time a new casing string or liner is set, a cold working process has been developed whereby the casing or liner can be expanded by up to 20% in diameter after being run down-hole.\n\nFor this purpose, an expansion tool that exceeds the inner diameter of the tube by the required amount of expansion is forced through the pipe. This is done either hydraulically, by applying mud pressure, or mechanically, by pulling the conical/tapered expansion tool. The expansion needs to be reliable, when expanding several thousand feet below the surface. This can be from 30–6,000 ft in length.\n\nCarey Naquin was responsible for the first commercial application of expandable tubular technology.\n\nThe applications can be grouped into two main categories – Cased hole and Open hole. Cased hole work is mainly down during the work over or completion phase of a well. The open hole products are used during the drilling period of a well.\n\nThe products developed and available today in cased hole work are the expandable liner hanger and the cased hole clad. The expandable liner hanger is basically an evolution of existing equipment currently used in the oil industry, a product with better thru bore and envisaged higher reliability. The Case hole clad provide a casing patch across a damaged section of casing, or to close off previously perforated casing. This product has two main advantages – minimal through bore loss [basically two times the wall thickness of tubular being expanded] and high pressure integrity performance.\n\nOpen hole applications is where expandable technology brings real advantages to the operator. Currently the products available are open hole liner and open hole clads.\n\n"}
{"id": "1900195", "url": "https://en.wikipedia.org/wiki?curid=1900195", "title": "Finings", "text": "Finings\n\nFinings are substances that are usually added at or near the completion of the processing of brewing wine, beer, and various nonalcoholic juice beverages. They are used to remove organic compounds, either to improve clarity or adjust flavor or aroma. The removed compounds may be sulfides, proteins, polyphenols, benzenoids, or copper ions. Unless they form a stable sediment in the final container, the spent finings are usually discarded from the beverage along with the target compounds that they capture.\n\nSubstances such as finings include egg whites, blood, milk, isinglass, and Irish moss. These are still used by some producers, but more modern substances have also been introduced and are more widely used, including bentonite, gelatin, casein, carrageenan, alginate, diatomaceous earth, pectinase, pectolyase, PVPP, kieselsol (colloidal silica), copper sulfate, dried albumen, hydrated yeast, and activated carbon.\n\nTheir actions may be broadly categorized as either electrostatic, adsorbent, ionic, or enzymatic.\n\nThe electrostatic types comprise the vast majority; including all but activated carbon, fining yeast, PVPP, copper sulfate, pectinase and pectolase. Their purpose is to selectively remove proteins, tannins (polyphenolics) and coloring particles (melanoidins). They must be used as a batch technique, as opposed to flow-through processing methods such as filters. Their particles each have an electric charge which is attracted to the oppositely charged particles of the colloidal dispersion that they are breaking. The result is that the two substances become bound as a stable complex; their net charge becoming neutral. Thus the agglomeration of a semi-solid follows, which may be separated from the beverage either as a floating or settled mass.\n\nThe only adsorbent types of finings in use are activated carbon and specialized fining yeasts. Although activated carbon may be implemented as a flow-through filter, it is also commonly utilized as a batch ingredient, which later must be separated and discarded from the beverage. It can completely/partially remove benzenoid compounds and all classes of polyphenols non-specifically, decolorizing and deodorizing juices and wines. Traditionally, yeast fining has involved the addition of hydrated yeasts used as adsorption agents. Consisting of approximately 30% protein, yeast cell walls have a chemical affinity with wine compounds, such as those that may be polyphenolic or metallic. Indeed, yeast fining is a practical means of removing excess copper ions (greater than 0.5 mg/L) when copper sulfate is used to bind selected volatile sulfur compounds (VSCs).\n\nThe ionic finings are copper sulfate and PVPP. When dissolved in aqueous beverages, copper sulfate's copper ions can chemically bind undesirable sulfides. The resulting complexes must be removed by other finings. The action of PVPP appears to be through the formation of hydrogen bonds between its carbonyl groups and the phenolic hydrogens of the polyphenols. It attracts the low molecular weight polyphenols rather than the condensed tannins and leucanthocyanins that are removed by gelatin.\n\nThe enzymatic finings are pectin and pectinase. They aid in destroying the large polysaccharide molecule named pectin, which otherwise causes haze in fruit wines and juices. They are among the few finings that are added before juices are fermented.\n\nUnfortunately, beneficial antioxidant flavonoids are removed by some finings. Quercetin is removed from red wines via the finings gelatin, casein, and PVPP to reduce astringent flavors. If other fining methods are used, the quercetin remains in the wine. Similarly the catechin flavonoids are removed by PVPP and other finings that target polyphenolic compounds.\n\nIn the absence of \"animal products used here\" labels, vegetarians may be unaware that the processing of a commercially produced beverage may have utilized animal based finings: either gelatin, casein, albumen, or isinglass.\n\n\n"}
{"id": "4070704", "url": "https://en.wikipedia.org/wiki?curid=4070704", "title": "Floorcloth", "text": "Floorcloth\n\nA floorcloth, or floor-cloth, is a cloth, normally of flannel, used for cleaning floors. The term was previously used also for materials used in place of carpeting or to protect expensive carpets, such as oilcloth, Kamptulicon, linoleum or other materials. This use is considered somewhat antiquated today, though do-it-yourself decorators still use floorcloths as a customizable alternative to rugs. Some artists have elected to use floorcloths as a medium of expression. Most modern floorcloths are made of heavy, unstretched canvas with two or more coats of gesso. They are then painted and varnished to make them waterproof.\n\nArea canvas rugs, today known as floorcloth, had their start in 18th century England. Initially used by the wealthy, the designs and patterns mimicked parquet flooring, tile and marble. As these useful furnishings found their way into middle-class homes, the variety of patterns grew. When American colonists became independent from England, they also began to create their own floorcloths. Eventually the development of linoleum eliminated the interest in these rugs. However, in the past few decades, the desire to decorate homes in a more personal way has stimulated their popularity. Unique designs are made in a variety of styles and colors, using many techniques. This gives today's floorcloths the ability to be created for any style interior.\n\n"}
{"id": "30814357", "url": "https://en.wikipedia.org/wiki?curid=30814357", "title": "Gamestorming", "text": "Gamestorming\n\nGamestorming is a set of practices for facilitating innovation in the business world. A facilitator leads a group towards some goal by way of a game, a structured activity that provides scope for thinking freely, even playfully.\n\nThe word \"gamestorming itself,\" as a neologism, is a portmanteau suggestive of using \"games\" for \"brainstorming\".\n\nA game may be thought of as an alternative to the standard business meeting. Most games involve 3 to 20 people and last from 15 minutes to an hour and a half. A game suspends some of the usual protocols of life and replaces them with a new set of rules for interaction. Games may require a few props such as sticky notes, poster paper, markers, random pictures from magazines, or thought provoking objects. Gamestorming skills include asking questions (opening, navigating, examining, experimenting, closing), structuring large diagrams, sketching ideas, fusing words and pictures into visual language, and most importantly, improvising to choose and lead a suitable game or invent a new one.\n\nThe Gamestorming book is used in classes on interactive design and user experience, and social media marketing and referenced in innovation, product development, visual note taking and self-realization.\n\nThe gamestorming culture originated in the 1970s in Silicon Valley. Some of the games have earlier roots, for example, \"Button\" is inspired by the Native American Talking Stick tradition, and Show and Tell is known from elementary school.\n"}
{"id": "53468615", "url": "https://en.wikipedia.org/wiki?curid=53468615", "title": "General regression neural network", "text": "General regression neural network\n\nGeneralized regression neural network (GRNN) is a variation to radial basis neural networks. GRNN was suggested by D.F. Specht in 1991.\n\nGRNN can be used for regression, prediction, and classification. GRNN can also be a good solution for online dynamical systems.\n\nGRNN represents an improved technique in the neural networks based on the nonparametric regression. The idea is that every training sample will represent a mean to a radial basis neuron. \n\nwhere:\n\nformula_7\n\nwhere formula_8 is the squared euclidean distance between the training samples formula_9 and the input formula_10\n\nGRNN has been implemented in many computer languages including MATLAB, R- programming language and Python (programming language).\n\nSimilar to RBFNN, GRNN has the following advantages:\n\n\nThe main disadvantages of GRNN are:\n"}
{"id": "283610", "url": "https://en.wikipedia.org/wiki?curid=283610", "title": "Juicer", "text": "Juicer\n\nA juicer (also known as juicing machine or juice extractor) is a tool used to extract juice from fruits, herbs, leafy greens and other types of vegetables in a process called juicing. It crushes, grinds, and/or squeezes the juice out of the pulp.\n\nSome types of juicers can also function as a food processor. Most of the twin gear and horizontal masticating juicers have attachments for crushing herbs and spices, extruding pasta, noodles or bread sticks, making baby food and nut butter, grinding coffee, making nut milk, etc.\n\nReamers are used for squeezing juice from citrus such as grapefruits, lemons, limes, and oranges. Juice is extracted by pressing or grinding a halved citrus along a juicer's ridged conical center and discarding the rind. Some reamers are stationary and require a user to press and turn the fruit, while others are electrical, automatically turning the ridged center when fruit is pressed upon.\n\nA centrifugal juicer cuts up the fruit or vegetable with a flat cutting blade. It then spins the produce at a high speed to separate the juice from the pulp.\n\nA masticating juicer known as cold press juicer or slow juicer uses a single auger to compact and crush produce into smaller sections before squeezing out its juice along a static screen while the pulp is expelled through a separate outlet.\n\nTriturating juicers (twin gear juicers) have twin augers to crush and press produce.\n\nA juicing press, such as a fruit press or wine press, is a larger scale that are used in agricultural production. These presses can be stationary or mobile. A mobile press has the advantage that it can be moved from one orchard to another. The process is primarily used for apples and involves a stack of apple mash, wrapped in fine mesh cloth, which is then pressed under approx 40 tonnes. These machines are popular in Europe and have now been introduced to North America.\n\nA stovetop steam juice extractor is typically a pot to generate steam that used to heat a batch of berries (or other fruit) in a perforated pot stacked on top of a juice collecting container that is above the steam pot. The juice is extracted without mechanical means so it is remarkably clear and because of the steam heating it is also pasteurized for long term storage.\n\n"}
{"id": "1172403", "url": "https://en.wikipedia.org/wiki?curid=1172403", "title": "List of Sinclair QL clones", "text": "List of Sinclair QL clones\n\nThe following is a list of clones of Sinclair Research's Sinclair QL microcomputer:\n\n\nThe following hardware devices provided QL compatibility for other computer platforms:\n\n\nThe ICL One Per Desk (also sold as the BT Merlin Tonto or the Telecom Australia Computerphone) shared some hardware components with the QL but was not intended to be software-compatible.\n\nIn addition, several software emulators of the QL exist including QPC, uQLX, QLay and Q-emuLator.\n"}
{"id": "3920625", "url": "https://en.wikipedia.org/wiki?curid=3920625", "title": "List of integrated circuit packaging types", "text": "List of integrated circuit packaging types\n\nIntegrated circuits are put into protective packages to allow easy handling and assembly onto printed circuit boards and to protect the devices from damage. A very large number of different types of package exist. Some package types have standardized dimensions and tolerances, and are registered with trade industry associations such as JEDEC and Pro Electron. Other types are proprietary designations that may be made by only one or two manufacturers. Integrated circuit packaging is the last assembly process before testing and shipping devices to customers.\n\nOccasionally specially-processed integrated circuit dies are prepared for direct connections to a substrate without an intermediate header or carrier. In flip chip systems the IC is connected by solder bumps to a substrate. In beam-lead technology, the metallized pads that would be used for wire bonding connections in a conventional chip are thickened and extended to allow external connections to the circuit. Assemblies using \"bare\" chips have additional packaging or filling with epoxy to protect the devices from moisture.\n\nThrough-hole technology uses holes drilled through the PCB for mounting the components. The component has leads that are soldered to pads on the PCB to electrically and mechanically connect them to the PCB.\n\nA chip carrier is a rectangular package with contacts on all four edges. Leaded chip carriers have metal leads wrapped around the edge of the package, in the shape of a letter J. Leadless chip carriers have metal pads on the edges. Chip carrier packages may be made of ceramic or plastic and are usually secured to a printed circuit board by soldering, though sockets can be used for testing.\n\nBall Grid Array BGA uses the underside of the package to place pads with balls of solder in grid pattern as connections to PCB.\n\n\nAll measurements below are given in mm. To convert mm to mils, divide mm by 0.0254 (i.e., 2.54 mm / 0.0254 = 100 mil).\n\nA variety of techniques for interconnecting several chips within a single package have been proposed and researched:\n\n\n\n"}
{"id": "5940541", "url": "https://en.wikipedia.org/wiki?curid=5940541", "title": "List of mobile telephone prefixes by country", "text": "List of mobile telephone prefixes by country\n\nThe telecommunication administrations in many countries assign specific telephone number prefixes to mobile phones within their telephone numbering plan, however some do not. The North American Numbering Plan Administration (NANPA) in the United States and its territories, Canada, and much of the Caribbean, uses normal geographic area codes for the mobile services so that a mobile telephone number is not easily recognized.\n\n"}
{"id": "42302752", "url": "https://en.wikipedia.org/wiki?curid=42302752", "title": "List of vacuum cleaners", "text": "List of vacuum cleaners\n\nThis is a list of vacuum cleaners and vacuum cleaner manufacturers. A vacuum cleaner is a device that uses an air pump to create a partial vacuum to suck up dust and dirt, usually from floors, and optionally from other surfaces as well. The dirt is collected by either a dustbag or a rigid cartridge, which may be emptied and reused. Vacuum cleaners are used in homes as well as in industry, and exist in a variety of sizes and models.\n\nA robotic vacuum cleaner is an autonomous robotic vacuum cleaner that has intelligent programming and a limited vacuum cleaning system. Some designs use spinning brushes to reach tight corners. Others combine a number of cleaning features (mopping, UV sterilization, etc.) simultaneous to vacuuming, thus rendering the machine into more than just a robot \"vacuum\" cleaner. These novelty vacuum cleaners are made to clean up a minimum amount of surface dust. They won’t even come close to catching all of the dirt and debris that makes its way beneath a carpet.\n\nManufacturers and specific models include:\n\n\n\n\n"}
{"id": "1239933", "url": "https://en.wikipedia.org/wiki?curid=1239933", "title": "Maneuverable reentry vehicle", "text": "Maneuverable reentry vehicle\n\nThe maneuverable reentry vehicle (abbreviated MARV or MaRV) is a type of ballistic missile whose warhead is capable of autonomously tracking ground targets. It often requires some terminal active homing guidance (like Pershing II active radar homing) to make sure the missile does not miss the target, because of the frequent trajectory shifts. Refer to atmospheric reentry.\n\nThere are several types, the examples of which include:\n\n\"Advanced Maneuverable Reentry Vehicle\" (AMaRV) was a prototype MARV built by McDonnell-Douglas Corp.. Four AMaRVs were made and represented a significant leap in Reentry Vehicle sophistication. Three of the AMaRVs were launched by Minuteman-1 ICBMs on 20 December 1979, 8 October 1980 and 4 October 1981. AMaRV had an entry mass of approximately 470 kg, a nose radius of 2.34 cm, a forward frustum half-angle of 10.4°, an inter-frustum radius of 14.6 cm, aft frustum half angle of 6°, and an axial length of 2.079 meters. No accurate diagram or picture of AMaRV has ever appeared in the open literature. However, a schematic sketch of an AMaRV-like vehicle along with trajectory plots showing hairpin turns has been published.\n\nAMaRV's attitude was controlled through a split body flap (also called a \"split-windward flap\") along with two yaw flaps mounted on the vehicle's sides. Hydraulic actuation was used for controlling the flaps. AMaRV was guided by a fully autonomous navigation system designed for evading anti-ballistic missile (ABM) interception.\n\n\n"}
{"id": "7312016", "url": "https://en.wikipedia.org/wiki?curid=7312016", "title": "Marian Mazur", "text": "Marian Mazur\n\nMarian Mazur (Radom, December 7, 1909 – Warsaw, January 21, 1983) was a Polish scientist who specialized in electrothermics and cybernetics, and the founding father of the Polish school of cybernetics.\n\nIn 1937 Mazur pioneered work on automatic telephone switchboards, and developed a working prototype just before World War II. After the war he established a thermoelectrical laboratory and researched infrared heating. Mazur attained professorship in 1954 and later worked on standardizing terminology related to electrical engineering and wrote numerous of articles and a book on the subject. Mazur was a member of numerous Polish and international scientific organizations, including the 27th Studies Committee of Thermoelectrics of the International Electrotechnical Commission of which he was president. In 1977 Mazur acted as a consultant in the field of artificial intelligence at Rice University in Houston, USA.\n\nMazur’s main contributions to the field of cybernetics were the theory of autonomous systems and the qualitative theory of information.\n\nMazur became interested in subjects related to control theory and what would later be called cybernetics during World War II. He started developing the theory of autonomous systems in 1942, but the destruction of the original manuscript in which he described his theory in the Warsaw Uprising and postwar events delayed the publication of his \"Cybernetyczna Teoria Systemów Autonomicznych (A Cybernetic Theory of Autonomous Systems)\" until 1966. Mazur defined an autonomous system as a system capable of controlling its own actions and of acting to prevent the loss of this capability, such as a living organism. He also introduced a terminology sufficiently general to describe all such systems and their interactions with their environment.\n\nIn order to describe the flow of information in a general situation Mazur developed a new theory of information. Mazur's qualitative theory of information improved upon previously existing theories by distinguishing between information as such, the amount of information contained in a message, and the information required to identify a message. This theory is described in \"Jakościowa Teoria Informacji (A Qualitative Theory of Information)\".\n\nMazur's theory of autonomous systems included a formula that described the reactivity of a culture as a ratio of internal reaction to external stimulus where \"the value of reactivity is a function of the system's power to transform stimuli into reactions\".\n\nIn his book \"Dynamism of Character in Shakespeare's Mature Tragedies\" literary theorist Piotr Sadowski adapted Mazur's concept of dynamism of character to Shakespeare's characters. In Mazur's theory he defined \"character\" (being the properties of a system independent of the environment) as \"a set of rigid controlling properties of the system\".\n\n"}
{"id": "1135893", "url": "https://en.wikipedia.org/wiki?curid=1135893", "title": "Mark of the Unicorn", "text": "Mark of the Unicorn\n\nMark of the Unicorn (MOTU) is a music-related computer software and hardware supplier. It is based in Cambridge, Massachusetts and has created music software since 1984.\n\nProducts by MOTU include:\n\nPast notable products:\n\n"}
{"id": "12634793", "url": "https://en.wikipedia.org/wiki?curid=12634793", "title": "Medison", "text": "Medison\n\nMedison is a Swedish consulting company registered in the UK. In July 2007, Medison caused massive attention world-wide as it announced its Medison Celebrity notebook computer which it would be selling for just $150. However, critics suspected it to be a fraud and orders have yet to be delivered. As of September 2007, Medison's account at 2CheckOut had been suspended.\n\nThe announced computer had a 14-inch (35.5 cm) WXGA screen, an Intel Celeron processor at 1.5 GHz, 256MB of DDR333/400 RAM, a 40GB hard drive, VIA PN800 integrated graphics, a CD/DVD Combo drive and 802.11g wireless networking. It would ship with the open-source Fedora Linux distribution and an unspecified office application suite.\n\nThe first orders were expected to begin shipping in August 2007, according to Medison CEO Valdi Ivancic however they did not materialise. On July 27, 2007, Medison reported that its e-mail system had failed due to heavy traffic, and stated that the company offers a full refund for undelivered orders.\n\nOn September 12, 2Checkout.com, the company that handles payments for Medison, announced that it would begin refunding customers for undelivered orders. On September 20, Medison's account had been suspended until further notice.\n\nThe low price of the Medison Celebrity had raised concerns that the computer may be a fraud. In addition, the physical appearance of the hardware was reported to be identical to a product from Taiwanese manufacturer Clevo. On the Medison website, the copyright disclaimer was identical to that of computer manufacturer Apple. Also, the site initially contained an unauthorized advertisement from Swedish telecom operator Tele2. Tele2 representative Thomas Ekman said his company had not purchased any space on the website. Robert Klanjack, Scandinavian CEO of Medison, had no comments when asked by Swedish financial site \"E24\", stating only that Medison will explain itself in an upcoming press event. At the press conference held in Stockholm, Sweden on August 1, 2007, at which the computer was shown to Swedish press, the manufacturer and serial number information had been removed from the case, to further add to the belief that it may be vaporware. Ivancic asserted that the computer will be available and that most of the revenues will be generated from advertisements on the Medison website and not the sales of Medison Celebrity itself.\nSupport will, according to Ivancic, be handled by electronics service company InfoCare, but InfoCare representative Anna Rosander claims there is no such agreement yet.\n\nIvancic claims to have taken part in the design process of IKEA furniture, and be responsible for the color schemes of the first Apple iMacs. Ivancic also announced that he will be running for the prime minister post of Sweden at the next election.\n\nOn August 9, 2007, Medison issued a press release, stating that it would no longer talk to media. According to the press release, negative media attention regarding the controversial Medison Celebrity had been harmful to company relations. It also stated that it would deliver the product as promised, which as of September 2007 it has not.\n\nOn September 10, 2007, Medison CEO Valdi Ivancic issued another press release, dismissing recent criticism in media as lies. In addition, the Medison website had been taken down only days before. The company stated this was a temporary technical error, however a representative from its web hotel Manufrog claimed the account had been misconfigured and later suspended, as reported by a blogger. Ivancic dismissed all accusations of the company being a fraud, and speculations of the website downtime being Medison's escape with early buyers' money. Due to great demand, the company has experienced logistical problems, but will deliver the Medison Celebrity, according to Ivancic. The press release encouraged Medison customers not to believe in media speculations, stating that Medison is a serious company with intentions to deliver.\n\nHowever, on September 13, 2007, 2CheckOut announced that customers would be refunded within 7–10 days. According to 2CheckOut, customers that still wanted the computer would have to place a new order.\n\n"}
{"id": "41653781", "url": "https://en.wikipedia.org/wiki?curid=41653781", "title": "Money.Net", "text": "Money.Net\n\nMoney.Net Inc is a privately held financial data technology company and financial data vendor based in New York City.\n\nMoney.Net provides real-time live streaming financial market information such as prices, breaking financial news, technical analysis charts, trade idea generation tools, and a spreadsheet API over the internet to individual traders and institutional trading floors. The product coverage is global, and is multi-asset class, including equities, fixed income, foreign exchange and commodities. It also includes reference fundamental market data such as economic statistics and corporate actions.\n\nThe Money.Net product provides \"access to real-time market data and trends for a sliver of what\" traditional market data terminals cost.\n\nMoney.Net is a cloud-based platform for market data. According to current CEO Morgan Downey, Money.Net has about 50,000 paying subscribers. It is one of the several cloud-based Financial Technology (FinTech) companies challenging dominant vendors in financial markets. The product is available as a desktop application, via mobile devices, and through an excel spreadsheet API.\n\nIn late 2016, the company announced that it had hired former Bloomberg Chief Content Officer, Norman Pearlstine, to develop a new financial news division relying heavily on artificial intelligence.\n\n"}
{"id": "26164190", "url": "https://en.wikipedia.org/wiki?curid=26164190", "title": "Non-radiative lifetime", "text": "Non-radiative lifetime\n\nNon-radiative life time is the average time before an electron in the conduction band of a semiconductor recombines with a hole non-radiatively. It is an important parameter in optoelectronics where radiative recombination is required to produce a photon; if the non-radiative life time is shorter than the radiative, then a carrier is more likely to recombine non-radiatively. This results in low internal quantum efficiency.\n"}
{"id": "817538", "url": "https://en.wikipedia.org/wiki?curid=817538", "title": "OQO", "text": "OQO\n\nOQO was a U.S. computer hardware company that was notable for manufacture of handheld computers. Its systems possess the functionality of a tablet PC in a size slightly larger than a personal digital assistant (PDA). According to \"Guinness World Records\", the \"OQO\" was the smallest full-powered, full-featured personal computer in 2005. The company's first version of subnotebook computer is the OQO model 01. Recently, it has been compared with the Ultra Mobile PC platform, although it was introduced before the UMPC took flight.\n\nOQO was reported to have stopped production in April 2009 because of financial difficulties. The company's web site indicates that OQO ceased operations entirely as of April 2009.\n\nThe original OQO model 01 was announced several years before prototypes were even seen, leading many people to call it vaporware until it was finally released in the fall of 2004. The computer shipped with Windows XP installed (Home Edition or Professional, but the Tablet PC Edition was not available until the model 01+ was released) and featured a 1 GHz Transmeta Crusoe processor, 20 GB hard drive, and 256 MB of RAM. It included USB 1.1, FireWire 400, a headphone port, and a built-in microphone, integrated 802.11b wireless radio, as well as Bluetooth. The OQO uses an Wacom electromagnetic induction-type pen stylus with a magnetic field sensitive 800x480 resolution transflective screen. Retail shipments began on October 14, 2004.\nIts size is 4.9 in by 3.4 in by 0.9 in and it weighs 0.9 lbs.\n\nThe OQO model 01+ was announced and released on September 27, 2005. Representing an incremental update to the model 01, the OQO model 01+ features a larger 30 GB hard drive, 512 MB of RAM (Double that of the OQO 01), USB 2.0, and an internal speaker. It also adds support for portrait display mode (An update for the model 01 is available from OQO). It also has a redesigned screen bezel intended to improve the accuracy of its Wacom enabled display (By increasing the space between the bezel and edge of the display).\n\nOriginally it was only available running Windows XP Home or Pro editions. The OQO model 01+ running on Microsoft Windows XP Tablet PC Edition 2005 was introduced on January 4, 2006, taking advantage of handwriting recognition and improved navigation.\n\nThe OQO model 02 was introduced by Bill Gates in January 2007 at the Consumer Electronics Show in Las Vegas. He showcased OQO's next generation of ultra-mobile PC, the model 02, in his keynote address that weighed in at a mere one pound and is small enough to fit in a pocket, the model 02 is the world's smallest fully functional Windows Vista PC.\n\nLike its predecessor, the model 02 Ultra-Mobile PC is a handheld device that runs Windows (choice of XP Professional or Vista Business.) The new product sported a new black casing and a backlit keyboard, a much brighter 5 inch LCD screen, 800x480 pixel display with an active digitizer for pen-based input.\n\nSeveral models were introduced at the time with VIA C7-M ULV processors that ranged from 1.2 GHz, 1.5 GHz, and 1.6 GHz, choice of 512MB or 1GB of RAM, and choice of hard drive. In the solid state drive (SSD) category a 32 or a 64 GB are available, or standard HD choices are from 60 to 120 GB. Also included is Bluetooth 2.0, 802.11a/b/g WiFi, USB 2.0, a 3.5 mm x 1 line out/line in audio jack and an HDMI-out port. The model 02 also offered optional integrated EV-DO mobile broadband with choice of Verizon or Sprint as service provider.\n\nAdvanced security features are also provided, including an on-board Trusted Platform Module, and thanks to the VIA C7-M's built-in Padlock features, hardware-level encryption, hashing and random number generation functions.\n\nThe model 02 also has the ability to zoom to 1000x600 and 1200x720 interpolated modes, horizontal/vertical screen rotation, and a sensor that automatically protects the hard drive in case of a drop. Accessories include a novelly-designed docking station (with a DVD+-RW drive, HDMI and VGA video out, three USB 2.0 ports, and a 3.5 mm audio jack), an extended battery that lasts up to six hours, and choice of a soft leatherette \"executive\" case or durable aluminum \"stronghold\" case.\n\nSince its introduction in 2007, the model 02 has won the following awards:\n\nOQO in September 2007 announced the availability of the model e2 with embedded HSDPA mobile broadband capability, providing customers in Europe and Asia with widely available high-speed Internet connectivity.\n\nThe OQO model e2 with embedded HSDPA is designed specifically to meet international demand for anytime/anywhere computing with access to the Internet and networked PC applications in a pocketable and ergonomic design. the OQO model e2 supports \"open SIM\" HSDPA (\"3.5G\") as well as UMTS, EDGE, and GPRS.\n\nThe model e2 with embedded mobile broadband supports data access through UMTS/HSDPA at 2100 MHz, and is downward compatible with GPRS/EDGE networks at 900 MHz and 1800 MHz. Download speeds of up to 3.6 Mbit/s are currently supported.\n\nThe product is available SIM-free and network unlocked, allowing customers maximum flexibility in selecting their preferred wireless operator and data plan. Users of the model e2 have their choice of connectivity options, including tri-band wide-area wireless, WiFi 802.11a/b/g and Bluetooth 2.0.\n\nThe OQO model 02 and OQO model e2 come with a VIA C7-M ULV processor (1.6 GHz, 1.5 GHz, or 1.2 GHz), up to 1 GB of RAM, up to a 120 GB hard drive, with the option of a 32 GB or 64 GB solid-state drive, has Bluetooth 2.0 (with Enhanced Data Rate), tri-mode WiFi (802.11a/b/g) and optional 3G mobile broadband (EV-DO in the US, UMTS/HSDPA internationally).\n\nOQO Atom-based \"model 2+\" was unveiled at the Intel Developer Forum in San Francisco labeled as \"OQO MID\" as a \"technology demonstration.\" The model 2+ has been formally announced at CES 2009, and is available with a 1.33 GHz or 1.86 GHz Intel Atom processor, 2GB RAM, Qualcomm Gobi global wireless internet, touch screen, and is the first PC to feature an active matrix OLED display.\n\nProjected shipping date was around May 22, 2009, but the product never shipped. OQO ceased production because of financial difficulties.\n\nAs of 2010, OQO has ceased operations. In April 2009, OQO started returning unrepaired in-warranty devices - a sure sign that the company was in dire financial trouble. In addition, the company stated that it would no longer offer any repair or service support, although third party warranties remain in effect. In May 2009, their phone numbers were disconnected, and emails were not deliverable. Their website also indicates \"We are sorry to report that OQO Inc. is out of Business as of April 2009, OQO has closed.\"\n"}
{"id": "21041889", "url": "https://en.wikipedia.org/wiki?curid=21041889", "title": "Patricia Selinger", "text": "Patricia Selinger\n\nPatricia G. Selinger is an American computer scientist and IBM Fellow, best known for her work on relational database management systems.\n\nShe played a fundamental role in the development of System R, a pioneering relational database implementation, and wrote the canonical paper on relational query optimization. She is a pioneer in relational database management and inventor of the technique of cost-based query optimization. She was a key member of the original System R team that created the first relational database research prototype. The dynamic programming algorithm for determining join order proposed in that paper still forms the basis for most of the query optimizers used in modern relational systems. She also established and led IBM’s Database Technology Institute, considered one of the most successful examples of a fast technology pipeline from research to development and personally has technical contributions in the areas of database optimization, data parallelism, distributed data, and unstructured data management. Before her retirement from IBM, she was the Vice President of Data Management Architecture and Technology at IBM.\n\nDr. Selinger was appointed an IBM Fellow in 1994, IBM’s highest technical recognition, and is an ACM Fellow (2009), a member of the National Academy of Engineering (1997), and a Fellow of the American Academy of Arts and Sciences. Dr. Selinger has published more than 40 papers, and also received the ACM Systems Software Award for her work on System R. She received the SIGMOD Innovation Award in 2002, the highest ACM award given in the area of data management.\n\nShe received A.B. (1971), S.M. (1972), and Ph.D. (1975) degrees in applied mathematics from Harvard University.\n\nFrom 2014 through 2016, Dr. Selinger was the Chief Technology Officer at Paradata (Paradata.io) where she worked on challenging problems in data harmonization, curation, provenance, and entity resolution to provide transparency to the supply chain. Paradata technology transforms real-time data into verified insights and executable actions.\n\nBeginning in 2017, Dr. Selinger is a Principal Architect at Salesforce.com.\n\n"}
{"id": "7870952", "url": "https://en.wikipedia.org/wiki?curid=7870952", "title": "Paul Green (engineer)", "text": "Paul Green (engineer)\n\nPaul Eliot Green, Jr. (January 14, 1924 – March 22, 2018) was an American electrical engineer who researched spread spectrum and radar technology. He was the son of playwright Paul Green.\n\nGreen was born in Chapel Hill, North Carolina on January 14, 1924. Green majored in physics at the University of North Carolina. He also served in the Naval ROTC and continued in the Navy Reserve for many years, eventually retiring as a lieutenant commander. He received a master's degree in electrical engineering from North Carolina State University in 1948. His masters studies focused on cryptographic research, and were followed by Ph.D. from M.I.T. (1953) on a thesis on spread spectrum, supervised by Wilbur Davenport, Robert Fano and Jerome Wiesner. This involved co-creating the Rake receiver (with Robert Price) and supervision of its deployment in a first-ever spread-spectrum system, the Lincoln F9C (1950).\n\nFollowing his studies, Green and Price (at MIT Lincoln Laboratory), attempting to bounce radar waves off the planet Venus (1958). With Gordon Pettengill, he worked out a theory of range-Doppler mapping that was used on the Magellan probe mapping of Venus' surface twenty years later. He also designed the LASA (Large Aperture Seismic Array) for earthquake prediction, first deployed in Montana and Norway (at NORSAR) in 1963.\n\nIn 1969, Green became head of IBM Research, communications dept., involved in the Systems Network Architecture, in particular the Advanced peer-to-peer networking protocol. Since 1988 he headed the optical communications (focusing on wavelength division multiplexing) research group that was acquired by Tellabs company where he worked 1997-2000. Since his retirement he had lobbied for expanded public access to broadband technology.\n\nGreen published extensively during his career; major works include \"Fiber Optic Networks\" (1992) and \"Fiber to the Home: The New Empowerment\" (2005). Since 1981 he authored around 300 CommuniCrostics crosswords in the IEEE Communications Magazine, published as a book in 2008. He died at his home in Chapel Hill, North Carolina on March 22, 2018.\n\n\n\n"}
{"id": "49973132", "url": "https://en.wikipedia.org/wiki?curid=49973132", "title": "Philippine Earth Data Resources Observation Center", "text": "Philippine Earth Data Resources Observation Center\n\nThe Philippine Earth Data Resources Observation Center, also known as the PEDRO Center, is a satellite ground station situated at the Department of Science and Technology–Advanced Science and Technology Institute (DOST–ASTI) facility at the University of Philippines Diliman in Quezon City, Philippines.\n\nIt is part of the Philippine Scientific Earth Observation Micro-satellite (Phil-Microsat) program by the Department of Science and Technology, which includes the deployment of the Diwata-1 and Diwata-2 microsatellites. It also receives information from commercial satellites.\n\nThe ground station was initially planned to be located inside the Subic Bay Freeport Zone in Subic, Zambales. This plan was reportedly changed in March 2016, with the ground station to be built in Diliman, Quezon City instead. Construction began in 2016 and PEDRO became operational by June 2017.\n"}
{"id": "20326706", "url": "https://en.wikipedia.org/wiki?curid=20326706", "title": "Pneumatic trail", "text": "Pneumatic trail\n\nPneumatic trail or trail of the tire is a trail-like effect generated by compliant tires rolling on a hard surface and subject to side loads, as in a turn. More technically, it is the distance that the resultant force of side-slip occurs behind the geometric center of the contact patch.\n\nPneumatic trail is caused by the progressive build-up of lateral force along the length of the contact patch, such that lateral forces are greater towards the rear of the contact patch (though less so when the rear of the contact patch begins sliding) and this creates a torque on the tire called the self aligning torque. Because the direction of the side-slip is towards the outside of a turn, the force on the tire is towards the center of the turn. Therefore, this torque tends to turn the front wheel in the direction of the side-slip, away from the direction of the turn.\n\nPneumatic trail is at its maximum when the slip angle is zero and decreases as slip angle increases. Pneumatic trail increases with vertical load.\n\n"}
{"id": "55481097", "url": "https://en.wikipedia.org/wiki?curid=55481097", "title": "RNA origami", "text": "RNA origami\n\nRNA origami is the nanoscale folding of RNA, enabling the RNA to create particular shapes to organize these molecules. It is a new method that was developed by researchers from Aarhus University and California Institute of Technology. RNA origami is synthesized by enzymes that fold RNA into particular shapes. The folding of the RNA occurs in living cells under natural conditions. RNA origami is represented as a DNA gene, which within cells can be transcribed into RNA by RNA polymerase. Many computer algorithms are present to help with RNA folding, but none can fully predict the folding of RNA of a singular sequence.\n\nIn nucleic acids nanotechnology, artificial nucleic acids are designed to form molecular components that can self-assemble into stable structures for use ranging from targeted drug delivery to programmable biomaterials. DNA nanotechnology uses DNA motifs to build target shapes and arrangements. It has been used in a variety of situations, including nanorobotics, algorithmic arrays, and sensor applications. The future of DNA nanotechnology is filled with possibilities for applications.\n\nThe success of DNA nanotechnology has allowed designers to develop RNA nanotechnology as a growing discipline. RNA nanotechnology combines the simplistic design and manipulation characteristic of DNA, with the additional flexibility in structure and diversity in function similar to that of proteins. RNA’s versatility in structure and function, favorable \"in vivo\" attributes, and bottom-up self-assembly is an ideal avenue for developing biomaterial and nanoparticle drug delivery. Several techniques were developed to construct these RNA nanoparticles, including RNA cubic scaffold, templated and non-templated assembly, and RNA origami.\n\nThe first work in RNA origami appeared in \"Science\", published by Ebbe S. Andersen of Aarhus University. Researchers at Aarhus University used various 3D models and computer software to design individual RNA origami. Once encoded as a synthetic DNA gene, adding RNA polymerase resulted in the formation of RNA origami. Observation of RNA was primarily done through atomic force microscopy, a technique that allows researchers to look at molecules a thousand times closer than would normally be possible with a conventional light microscope. They were able to form honeycomb shapes, but determined other shapes are also possible.\n\nCody Geary, a scholar in the field of RNA origami, described the uniqueness of the method of RNA origami. He stated that its folding recipe is encoded in the molecule itself, and determine by its sequence. The sequence gives the RNA origami both its final shape and movements of the structure as it folds. The primary challenge associated with RNA origami stems from the fact RNA folds on its own and can thus easily tangle itself.\n\nComputer-aided design of the RNA origami structure requires three main processes; creating the 3D model, writing the 2D structure, and designing the sequence. First, a 3D model is constructed using tertiary motifs from existing databases. This is necessary to ensure the created structure has feasible geometry and strain. The next process is creating the 2D structure describing the strand path and base pairs from the 3D model. This 2D blueprint introduces sequence constraints, creating primary, secondary, and tertiary motifs. The final step is designing sequences compatible with designed structure. Design algorithms can be used to create sequences that can fold into various structures. \n\nTo produce a desired shape, the RNA origami method uses double-crossovers (DX) to arrange the RNA helices in parallel to each other to form a building block. While DNA origami requires the construction of DNA molecules from multiple strands, researchers were able to devise a method in making DX molecules from only one strand for RNA. This was done through adding hairpin motifs to the edges and kissing-loop complexes on internal helices. The addition of more DNA molecules on top of one another creates a junction known as the dovetail seam. This dovetail seam has base pairs that cross between adjacent junctions; thus, the structural seam along the junction becomes sequence-specific. An important aspect of these folding interactions is its folding; the order that interactions form can potentially create a situation in which one interaction blocks another, creating a knot. Because the kissing-loop interactions and dovetail interactions are a half-turn or shorter, they do not create these topological issues.\n\nRNA and DNA nanostructures are used for the organization and coordination of important molecular processes. However, there exist several distinct differences between the fundamental structure and applications between the two. Although inspired by the DNA origami techniques established by Paul Rothemund, the process for RNA origami is vastly different. RNA origami is a much newer process than DNA origami; DNA origami has been studied while approximately a decade now, while the study of RNA origami has only recently begun.\n\nIn contrast to DNA origami, which involves chemically synthesizing the DNA strands and arranging the strands to form any shape desired with the aid of \"staple strands\", RNA origami is made by enzymes and subsequently folds into pre-rendered shapes. RNA is able to fold into unique ways in complex structures due to a number of secondary structural motifs, such as conserved motifs and short structural elements. A major determinant for RNA topology is the secondary-structure interaction, which include motifs such as pseudoknots and kissing loops, adjacent helices stacking on one another, hairpin loops with bulge content, and coaxial stacks. This is largely a result of four different nucleotides: adenine (A), cytosine (C), guanine (G) and uracil (U), and ability to form non-canonical base pairs.\n\nThere also exist more complex and longer-range RNA tertiary interactions. DNA are unable to forms these tertiary motifs and thereby cannot match the functional capacity of RNA in performing more versatile tasks. RNA molecules that are correctly folded can serve as enzymes, due to positioning metal ions at their active sites; this gives the molecules a diverse array of catalytic abilities. Because of this relationship to enzymes, RNA structures can potentially be grown within living cells and used to organize cellular enzymes into distinct groups.\n\nAdditionally, the DNA origami's molecular breakup is not easily incorporated into the genetic material of an organism. However, RNA origami is capable of being written directly as a DNA gene and transcribed using RNA polymerase. Therefore, while DNA origami requires expensive culturing outside of a cell, RNA origami can be produced in mass, cheap quantities directly within cells just by growing bacteria. The feasibility and cost effectiveness of manufacturing RNA in living cells and combined with the extra functionality of RNA structure is promising for the development of RNA origami.\n\nRNA origami is a new concept and has great potential for applications in nanomedicine and synthetic biology. The method was developed to allow new creations of large RNA nanostructures that create defined scaffolds for combining RNA based functionalities. Because of the infancy of RNA origami, many of its potential applications are still in the process of discovery. Its structures are able to provide a stable basis to allow functionality for RNA components. These structures include riboswitches, ribozymes, interaction sites, and aptamers. Aptamer structures allow the binding of small molecules which gives possibilities for construction of future RNA based nanodevices. RNA origami is further useful in areas such as cell recognition and binding for diagnosis. Additionally, targeted delivery and blood-brain barrier passing have been studied. Perhaps the most important future application for RNA origami is building scaffolds to arrange other microscopic proteins and allow them to work with one another.\n"}
{"id": "27380743", "url": "https://en.wikipedia.org/wiki?curid=27380743", "title": "Range extender (vehicle)", "text": "Range extender (vehicle)\n\nA range extender vehicle is a battery electric vehicle that includes an auxiliary power unit (APU) known as a 'range extender'. The range extender drives an electric generator which charges a battery which supplies the vehicle's electric motor with electricity. This arrangement is known as a series hybrid drivetrain. The most commonly used range extenders are internal combustion engines, but fuel-cells or other engine types can be used.\n\nRange extender vehicles are also referred to as extended-range electric vehicles (EREV), range-extended electric vehicles (REEV), and range-extended battery-electric vehicle (BEVx) by the California Air Resources Board (CARB).\n\nThe key function of the range extender is to increase the vehicle's range. Range autonomy is one of the main barriers for the commercial success of electric vehicles, and extending the vehicle's range when the battery is depleted helps alleviate range anxiety.\n\nA range extending vehicle design can also reduce the consumption of the range extending fuel (such as gasoline) by using the primary fuel (such as battery power), while still maintaining the driving range of a single fuel vehicle powered by a range extending fuel such as gasoline. The range extending fuel is generally considered to be less environmentally and economically friendly to use than the primary fuel source, so the vehicle control system gives preference to using the primary fuel if it's available. However, due to range limitations with the primary fuel source, the range extending fuel allows the vehicle to get many of the cost and environmental benefits of the primary fuel, while maintaining the full driving range of the range extending fuel source. For example, in the Chevy Volt, battery power from the electric grid can be cheaper and more environmentally sustainable than burning gasoline (depending on the electric generation source), but due to the trade offs between the range of a pure electric vehicle and its battery size, adding the range extending gasoline is considered by many to be good compromise to give the Chevy Volt a significantly greater driving range. How many benefits are derived from using the primary fuel however depend on how the vehicles are driven. For example, a first generation Chevy Volt will operate 100% on battery power from the electric grid for the first , while the second generation Volt will operate on 100% battery power for the first when fully recharged between trips. However, if the same Chevy Volt is driven for hundreds of miles a day it will require significant gasoline as the battery will be quickly depleted. Using the gasoline engine to generate power for the motors, the economy ratings are and for the different generation models. Therefore, it is critical to understand the driving patterns of the average commuter to fully understand the impact these range extending vehicles will have in the real world.\n\nMany range extender vehicles, including the Chevrolet Volt and the BMW i3, are able to charge their batteries from the grid as well as from the range extender, and therefore are a type of plug-in hybrid electric vehicle (PHEV).\n\nWhen a range extender uses conventional fuels they can re-fuel at regular fuel stations guarantees to give them a similar driving ranges to conventional automobiles.\n\nAs an REEV is only propelled by the electric motor it can do away with the weight and cost associated with the gearbox transmission system typically used in internal combustion engine cars. Further, as the range extender does not need to increase or decrease output in line with the power needs of the vehicle (this task is handled by the electric motor) the range extender can be sized to satisfy the vehicle's average power requirement rather than its peak power requirement (such as when accelerating). The range extender can also operate much closer to its most efficient rotational speed. These design features allow an REEV to convert fossil fuel energy to electric power and vehicle motion very efficiently.\n\n\nAccording to 2012 Amendments to the Zero Emission Vehicle Regulations adopted in March 2012 by the California Air Resources Board (CARB), a range-extended battery-electric vehicle, designated as BEVx, should comply, among others, with the following criteria:\n\nRange extenders are commonly used in marine (autonomous underwater vehicle), aircraft and Generator/Utility, automotive and hybrid electric vehicle applications.\n\nGeneral Motors describes the Chevrolet Volt as an electric vehicle equipped with a 16 kWh battery plus a \"range extending\" gasoline powered internal combustion engine (ICE) as a genset and therefore dubbed the Volt an \"Extended Range Electric Vehicle\" or E-REV. In a January 2011 interview, the Chevy Volt's Global Chief Engineer, Pamela Fletcher, referred to the Volt as \"an electric car with extended range.\" The Volt operates as a purely electric car for the first in charge-depleting mode. When the battery capacity drops below a pre-established threshold from full charge, the vehicle enters charge-sustaining mode, and the Volt's control system will select the most optimally efficient drive mode to improve performance and boost high-speed efficiency.\n\nAccording to General Motors' real time tally of miles driven by Volt owners in North America, by mid June 2014 they had accumulated more than . GM also reported that Volt owners driving is more than 63% in all-electric mode. Volt owners who charge regularly typically drive more than between fill-ups and visit the gasoline station less than once a month. A similar report, issued by GM in August 2016, reported that Volt owners have accumulated almost driven in EV mode, representing 60% of their total miles traveled.\n\nThe BMW i3 all-electric car with at least 22 kWh battery capacity offers an optional gasoline-powered range extender APU. The range extender is the same 647 cc two-cylinder gasoline engine used in the BMW C650 GT scooter with a fuel tank. The US model offers a smaller 7 L tank. The range extender engages when the battery level drops to 6%. It generates electricity to extend the range from to Performance in range-extending mode may be more limited than when it is running on battery power, as BMW designed the range extender as a backup to enable reaching a recharging location.\n\nAccording to BMW, at the beginning of the i3 release, the use of range-extender was much more than the carmaker expected, more than 60%. Over time it has decreased significantly, with some people almost never using it, and by 2016 it is being regularly used in fewer than 5% of i3s.\n\nThe range-extender option costs an additional in the United States, an additional (~ ) in France, and (~ ) in the Netherlands.\n\nThe range-extender option of the BMW i3 was designed to meet the CARB regulation for an auxiliary power unit (APU) called REx. According to rules CARB adopted in March 2012, the 2014 BMW i3 with a REx unit fitted will be the first car to qualify as a range-extended battery-electric vehicle or \"BEVx.\" CARB describes this type of electric vehicle as \"a relatively high-electric range battery-electric vehicle (BEV) to which an APU is added.\" The APU, which maintains battery charge at about 6% after the pack has been depleted in normal use, is strictly limited in the additional range it can provide.\n\nOther range-extended electric vehicles include the discontinued Cadillac ELR and the discontinued Fisker Karma. In June 2016, Nissan announced it will introduce a compact range extender car in Japan before March 2017. The series plug-in hybrid will use a new hybrid system, dubbed e-Power, which debuted with the Nissan Gripz concept crossover showcased at the 2015 Frankfurt Auto Show.\n\nThe LEVC TX London taxi was launched in 2017 and features a 33kWh battery that is charged by a 1.5-litre petrol engine.\n\nThis approach has also been used for heavy vehicles, such as Wrightbus's Gemini 2 and New Routemaster buses.\n\nThe 2010 Wolverine 3 program included an ICE range extender for its unmanned aerial vehicle.\n\nA range-extended electric vehicle uses a series hybrid drivetrain.\n\n"}
{"id": "14663899", "url": "https://en.wikipedia.org/wiki?curid=14663899", "title": "Raúl Pateras Pescara", "text": "Raúl Pateras Pescara\n\nRaúl Pateras Pescara de Castelluccio (1890 – 1966), marquis of Pateras-Pescara, was an engineer and inventor from Argentina who specialized in automobiles, helicopters and free-piston engines.\n\nPescara is credited for being one of the first people to successfully utilize cyclic pitch, as well as pioneering the use of autorotation for the safe landing of a damaged helicopter. Pescara also set a world record (at the time) in 1924 for achieving a speed of in a helicopter.\n\nPescara was born in Buenos Aires and at the beginning of the 20th century, his family left Argentina to return to Europe.\n\nIn 1911, using a workshop that Pescara was involved with, Gustave Eiffel tested a scale model (1:20) of a seaplane (monoplane design) named the \"Pateras Pescara\", designed by Pescara and Italian engineer Alessandro Guidoni, in a wind tunnel. In 1912, the Italian Ministry of the Navy commissioned Guidoni to build a torpedo bomber based on the Pescara model; but following tests in 1914, Guidoni was unable to create a successful design.\n\nFrom 1919, Pescara built several coaxial helicopters and submitted numerous patents across several countries. He first tested his machine indoors in 1921, before moving to Paris, France, where government funding was available. His \"No. 3\" design had two contra-rotating \"screws\", with each screw having four blades, and each blade a biplane wing with wing warping for control.\n\nOn January 16, 1924, at Issy-les-Moulineaux near Paris, Pescara broke his own world record for helicopter flight with his model 2F, \"The Marquis Pateras\", by remaining in the air 8 minutes and 13 4/5 seconds, whilst flying - about – in a vertical line.\" On January 29, 1924, whilst attempting to win the French Aero Club prize for a closed circuit, Pescara completed the course in 10 minutes, 33 seconds, but was not permitted to qualify due to his machine's brief contact with the ground. Equipped with coaxial double rotor apparatus, the engineer then set the first helicopter record recognized by the Fédération Aéronautique Internationale (FAI); On April 18, 1924, he flew a distance of , with a duration of 4 minutes, 11 seconds (approximately ) and a height of .\n\nIn 1929, together with his brother Henri, the Italian engineer Edmond Moglia and the Spanish government, Pescara founded La Fábrica Nacional de Automóviles (\"National Automobile Factory\") with an investment of 70 million pesetas. Pescara utilized the new factory to focus upon automobiles, and in 1931 he exhibited his creation, the \"Nacional Pescara\", at the 1931 Paris Motor Show. The vehicle appeared at the Grand Palais next to Voisin's and in 1931, the eight-cylinder car won the European Grand Prix for hillclimbing.\n\nThe Spanish Civil War forced Pescara to return to France, and on February 28, 1933, the \"Pescara Auto-compressor Company\" was unveiled in Luxembourg. The auto-compressor company remained in business for 30 years, supported by six French patents with one of its shareholders, and the Pescara & Raymond Corporation, based in Dover, Delaware, USA. Pescara auto-compressors fulfill two basic designs: symmetrical and asymmetrical.\n\nDuring the Second World War, Pescara worked on electrical power in Portugal. Free-piston engines received new attention when they were mass-produced by the Société Industrielle Générale de Mécanique Appliquée (SIGMA), a French company that developed the GS-34, a 1138-horsepower generator. In 1963, Pescara rejoined his sons in Paris where he served as an expert for S.N. Marep during the testing of its 2000-horsepower EPLH-40. Pescara subsequently proposed the production of more powerful machines - new tandem generators based on the existing EPLH-40 and GS-34 - but died in Paris, France, before the company that was to develop the engineer's proposal was established.\n\n\n\n"}
{"id": "14686864", "url": "https://en.wikipedia.org/wiki?curid=14686864", "title": "Royal Leerdam Crystal", "text": "Royal Leerdam Crystal\n\nRoyal Leerdam Crystal, also known as Royal Leerdam, is the designing and glass blowing department of Dutch glassware producing factory, . The company was founded in 1765 as a manufacturer of bottles in the Dutch city of Leerdam.\n\nDesigning and glassblowing were in the past strictly separated. Several well-known designers as Hendrik Petrus Berlage, Andries Copier, Sybren Valkema, and Willem Heesen have contributed to the Royal Leerdam reputation with a wide range of designer glass, both serica and unica. Best known has become the so-called range of products \"Gilde Glass\" (1930) by Andries Copier, as an example of both simplicity and beauty.\nGlass art created by Royal Leerdam Crystal was and still is very sought after by museums and collectors worldwide.\n\n\n\n\n\n"}
{"id": "30582557", "url": "https://en.wikipedia.org/wiki?curid=30582557", "title": "Seventh Edition Unix terminal interface", "text": "Seventh Edition Unix terminal interface\n\nThe Seventh Edition Unix terminal interface is the generalized abstraction, comprising both an Application Programming Interface for programs and a set of behavioural expectations for users, of a terminal as historically available in Seventh Edition Unix. It has been largely superseded by the POSIX terminal interface.\n\nThe terminal interface provided by Seventh Edition Unix and UNIX/32V, and also presented by BSD version 4 as the \"old terminal driver\", was a simple one, largely geared towards teletypewriters as terminals. Input was entered a line at a time, with the terminal driver in the operating system (and not the terminals themselves) providing simple line editing capabilities. A buffer was maintained by the kernel in which editing took place. Applications reading terminal input would receive the contents of the buffer only when the key was pressed on the terminal to end line editing. The key sent from the terminal to the system would erase (\"kill\") the entire current contents of the editing buffer, and would be normally displayed as an '@' symbol followed by a newline sequence to move the print position to a fresh blank line. The key sent from the terminal to the system would erase the last character from the end of the editing buffer, and would be normally displayed as an '#' symbol, which users would have to recognize as denoting a \"rubout\" of the preceding character (teletypewriters not being physically capable of erasing characters once they have been printed on the paper).\n\nFrom a programming point of view, a terminal device had transmit and receive baud rates, \"erase\" and \"kill\" characters (that performed line editing, as explained), \"interrupt\" and \"quit\" characters (generating signals to all of the processes for which the terminal was a controlling terminal), \"start\" and \"stop\" characters (used for software flow control), an \"end of file\" character (acting like a carriage return except discarded from the buffer by the codice_1 system call and therefore potentially causing a zero-length result to be returned) and various \"mode flags\" determining whether local echo was emulated by the kernel's terminal driver, whether modem flow control was enabled, the lengths of various output delays, mapping for the carriage return character, and the three input modes.\n\nThe three input modes for terminals in Seventh Edition Unix were:\n\nIn the POSIX terminal interface, these modes have been superseded by a system of just two input modes: canonical and non-canonical. The handling of signal-generating special characters in the POSIX terminal interface is independent of input mode, and is separately controllable.\n\nIn Seventh Edition Unix there was no terminal job control and a process group was considered to be not what it is considered to be nowadays.\n\nEach process in the system had either a single \"controlling terminal\", or no controlling terminal at all. A process inherits its controlling terminal from its parent. A controlling terminal was acquired when a process with no controlling terminal codice_2s a terminal device file that isn't already the controlling terminal for some other process. All of the processes that had the same controlling terminal were part of a single \"process group\".\n\nThe programmatic interface for querying and modifying all of these modes and control characters was the codice_3 system call. (This replaced the codice_4 and codice_5 system calls of Sixth Edition Unix.) Although the \"erase\" and \"kill\" characters were modifiable from their defaults of and , for many years after Seventh Edition development inertia meant that they were the pre-set defaults in the terminal device drivers, and on many Unix systems, which only altered terminal device settings as part of the login process, in system login scripts that ran \"after\" the user had entered username and password, any mistakes at the login and password prompts had to be corrected using the historical editing key characters inherited from teletypewriter terminals.\n\nThe symbolic constants, whose values were fixed and defined, and data structure definitions of the programmatic interface were defined in the codice_6 system header.\n\nThe codice_3 operations were as follows:\nOne data structure used by the terminal system calls is the codice_9 structure, whose C programming language definition is as follows:\n\nUnlike the POSIX terminal interface, the Seventh Edition Unix terminal interface recorded input and output baud rates directly in the data structure.\n\nThe input and output speeds in the codice_11 and codice_12 fields were those of the DEC DH-11, and were the numbers 0 to 15, represented by the symbolic constants (in ascending order) codice_13, codice_14, codice_15, codice_16,codice_17, codice_18, codice_19, codice_20, codice_21, codice_22, codice_23, codice_24, codice_25, codice_26, codice_27, and codice_28, where the baud rate was as in the name (with the last two being \"external A\" and \"external B\"). Setting a baud rate of zero forced the terminal driver to hang up a modem (if the terminal was a modem device).\n\nThe codice_29 and codice_30 fields were simply the character values of the \"erase\" and \"kill\" characters, respectively, defaulting to the (ASCII) values for '#' and '@' respectively.\n\nThe codice_31 field specified various input and output control flags, as in the following table.\nOne data structure used by the terminal system calls is the codice_32 structure, whose C programming language definition is as follows:\nThe values of these fields were the values of the various programmatically configurable special characters. A -1 value in any field disabled its recognition by the terminal driver.\n\n"}
{"id": "56859193", "url": "https://en.wikipedia.org/wiki?curid=56859193", "title": "Solvated metal atom dispersion", "text": "Solvated metal atom dispersion\n\nSolvated Metal Atom Dispersion is a method of producing highly reactive solvated nanoparticles. Samples of the metal (or ceramic) are heated to produce free atoms (or species), as in PVD evaporation. These are then co-depositied with a suitable organic frozen solvent (eg. toluene) at very low temperatures (on the order of 70K). This is then warmed towards room temperature, producing solvated metal ions or (over time) larger clusters. Sometimes, catalysts supports (such as SiO or AlO) are added to improve nucleation, as it can more readily take place on surface OH groups.\n"}
{"id": "5202850", "url": "https://en.wikipedia.org/wiki?curid=5202850", "title": "Stirling radioisotope generator", "text": "Stirling radioisotope generator\n\nThe Stirling radioisotope generator (SRG) is a generator based on a Stirling engine powered by a large radioisotope heater unit. The hot end of the Stirling converter reaches high temperature and heated helium drives the piston, heat being rejected at the cold end of the engine. A generator or alternator converts the motion into electricity. Given the very constrained supply of plutonium, the Stirling converter is notable for producing about four times as much electric power from the plutonium fuel as compared to a radioisotope thermoelectric generator (RTG). The Stirling generators were extensively tested on Earth by NASA, but their development was cancelled in 2013 before they could be deployed on actual spacecraft missions. A similar NASA project still under development as of , called Kilopower, also utilizes Stirling engines, but uses a small uranium fission reactor as the heat source.\n\nStirling engine development began at NASA Glenn Research Center (then NASA Lewis) in the early 1970s. The Space Demonstrator Engine (SPDE) was the earliest 12.5 kWe per cylinder engine that was designed, built and tested. A later engine of this size, the Component Test Power Converter (CTPC), used a \"Starfish\" heat-pipe heater head, instead of the pumped-loop used by the SPDE. In the 1992-93 time period, this work was stopped due to the termination of the related SP-100 nuclear power system work and NASA's new emphasis on \"better, faster, cheaper\" systems and missions.\n\nIn the early 21st century, a major project using this concept was undertaken: the Advanced Stirling Radioisotope Generator (ASRG), a power source based on a 55-watt electric converter. The thermal power source for this system was the General Purpose Heat Source (GPHS). Each GPHS contained four iridium-clad Pu-238 fuel pellets, stood 5 cm tall, 10 cm square and weighed 1.44 kg. The hot end of the Stirling converter reached 650 °C and heated helium drove a free piston reciprocating in a linear alternator, heat being rejected at the cold end of the engine. The alternating current (AC) generated by the alternator was then converted to 55 watts direct current (DC). Thus each ASRG unit would use two Stirling converter units with about 500 watts of thermal power supplied by two GPHS units and would deliver 100-120 watts of electric power. The ASRG underwent qualification testing at NASA Glenn, as a power supply for a future NASA mission. The ASRG was designed into many mission proposals in this era, but was cancelled in 2013.\n\n\n"}
{"id": "604816", "url": "https://en.wikipedia.org/wiki?curid=604816", "title": "Susan Helms", "text": "Susan Helms\n\nSusan Jane Helms (born February 26, 1958) is a retired lieutenant general in the United States Air Force and a former NASA astronaut. She was the commander, 14th Air Force (Air Forces Strategic); and commander, Joint Functional Component Command for Space at Vandenberg Air Force Base in California. Helms was a crew member on five Space Shuttle missions and was a resident of the International Space Station (ISS) for over five months in 2001. While participating in ISS Expedition 2, she and Jim Voss conducted an 8-hour and 56 minute spacewalk, the world record. Helms officially retired from the United States Air Force in 2014.\n\nHelms was born in Charlotte, North Carolina, but considers Portland, Oregon, to be her hometown. She enjoys piano and other musical activities, jogging, traveling, reading, computers, and cooking. Her parents, Lieutenant Colonel (retired, USAF) Pat and Dori Helms, reside in Denver, Colorado. She has three sisters.\n\nHelms graduated from the U.S. Air Force Academy in 1980. She received her commission and was assigned to Eglin Air Force Base, Florida, as an F-16 weapons separation engineer with the Air Force Armament Laboratory. In 1982, she became the lead engineer for F-15 weapons separation. In 1984, she was selected to attend graduate school. She received a Master of Science in Aeronautics and Astronautics from Stanford University in 1985 and was assigned as an assistant professor of aeronautics at the U.S. Air Force Academy. In 1987, she attended the Air Force Test Pilot School at Edwards Air Force Base, California. After completing one year of training as a flight test engineer, Helms was assigned as a USAF Exchange Officer to the Aerospace Engineering Test Establishment, at Canadian Forces Base Cold Lake in Alberta, Canada, where she worked as a flight test engineer and project officer on the CF-18 aircraft. She was managing the development of a CF-18 flight control system simulation for the Canadian Forces when selected for the astronaut program.\nAfter a 12-year NASA career that included 211 days in space, Helms returned to the U.S. Air Force in July 2002 to take a position at HQ USAF Space Command. After a stint as the division chief of the Space Superiority Division of the Requirements Directorate of Air Force Space Command in Colorado Springs, Colorado, she served as vice commander of the 45th Space Wing at Patrick Air Force Base near Cape Canaveral, Florida. She then served as deputy director of operations (Technical Training) for Air Education and Training Command at Randolph Air Force Base near San Antonio, Texas. Helms served on the Return To Flight task group after the Columbia accident. She was promoted to brigadier general in June 2006 and became commander of the 45th Space Wing on the same day of her promotion.\n\nHelms was promoted to major general in August 2009. She served as the Director of Plans and Policy, U.S. Strategic Command, Offutt Air Force Base, Nebraska. She was directly responsible to the U.S. Strategic Command commander for the development and implementation of national security policy and guidance; military strategy and guidance; space and weapons employment concepts and policy; and joint doctrine as they apply to the command and the execution of its missions. She was also responsible for the development of the nation's strategic war plan, strategic support plans for theater combatant commanders and contingency planning for the global strike mission.\n\nIn January 2011, Helms was promoted to lieutenant general and assumed duties as commander, 14th Air Force (Air Forces Strategic), Air Force Space Command and Commander, Joint Functional Component Command for Space, US Strategic Command\n\nAs a flight test engineer, Helms has flown in 30 different types of U.S. and Canadian military aircraft.\n\nIn 2013, Helms was nominated by President Barack Obama to become vice commander of the Air Force Space Command. Senator Claire McCaskill placed a permanent hold on the nomination because Helms had dismissed a charge of a sexual assault and punished Captain Herrera on a lesser charge leading to his dismissal from the USAF, in her role as the General Court-Martial Convening Authority, who is required to review all findings. As Helms's lawyer explained, Helms felt the prosecution had failed to prove its case beyond a reasonable doubt. Obama eventually withdrew Helms's nomination and she retired from the Air Force in 2014.\n\nSelected by NASA in January 1990, Helms became an astronaut in July 1991. She flew on STS-54 (1993), STS-64 (1994), STS-78 (1996), STS-101 (2000) and served aboard the International Space Station as a member of the \"ISS Expedition 2\" crew (2001). A veteran of five space flights, Helms logged 5,064 hours in space, including an EVA of 8 hours and 56 minutes (world record).\n\nSTS-54 \"Endeavour\", January 13–19, 1993. The primary objective of this mission was the deployment of a $200-million NASA Tracking and Data Relay Satellite (TDRS-F). A diffuse X-ray spectrometer (DXS) carried in the payload bay, collected over 80,000 seconds of quality X-ray data that will enable investigators to answer questions about the origin of the Milky Way galaxy. The crew demonstrated the physics principles of everyday toys to an interactive audience of elementary school students across the United States. A highly successful extra-vehicular activity (EVA) resulted in many lessons learned that will benefit International Space Station assembly. Mission duration was 5 days, 23 hours, 38 minutes, 17 seconds.\nSTS-64 \"Discovery\", September 9–20, 1994. On this flight, Helms served as the flight engineer for orbiter operations and the primary RMS operator aboard Space Shuttle. The major objective of this flight was to validate the design and operating characteristics of Lidar in Space Technology Experiment (LITE) by gathering data about the Earth's troposphere and stratosphere. Additional objectives included the deploy and retrieval of SPARTAN-201, a free-flying satellite that investigated the physics of the solar corona, and the testing of a new EVA maneuvering device. The Shuttle Plume Impingement Flight Experiment (SPIFEX) was used to collect extensive data on the effects of jet thruster impingement, in preparation for proximity tasks such as space station docking. Mission duration was 10 days, 22 hours, 51 minutes.\n\nSTS-78 \"Columbia\", June 20 to July 7, 1996, Helms was the payload commander and flight engineer aboard \"Columbia\", on the longest Space Shuttle mission to date (later that year the STS-80 mission broke its record by nineteen hours). The mission included studies sponsored by ten nations and five space agencies, and was the first mission to combine both a full microgravity studies agenda and a comprehensive life science investigation. The Life and Microgravity Spacelab mission served as a model for future studies on board the International Space Station. Mission duration was 16 days, 21 hours, 48 minutes.\n\nSTS-101 \"Atlantis\", May 19–29, 2000, was a mission dedicated to the delivery and repair of critical hardware for the International Space Station. Helms prime responsibilities during this mission were to perform critical repairs to extend the life of the Functional Cargo Block (FGB). In addition, she had prime responsibility of the onboard computer network and served as the mission specialist for rendezvous with the ISS. Mission duration was 9 days, 20 hours and 9 minutes.\n\nExpedition 2 March 8 to August 22, 2001 was a mission to the International Space Station and Helms was as a member of the second crew to inhabit the International Space Station Alpha. The \"Expedition 2\" crew (two American astronauts and one Russian cosmonaut) launched on March 8, 2001 on board STS-102 \"Discovery\" and successfully docked with the station on March 9, 2001. The \"Expedition 2\" crew installed and conducted tests on the Canadian-made Space Station Robotic arm (SSRMS), conducted internal and external maintenance tasks (Russian and American), in addition to medical and science experiments. During her stay on board, Helms installed the airlock (brought up on the STS-104 mission) using the SSRM. She and her crewmates also performed a 'fly around' of the Russian \"Soyuz\" spacecraft and welcomed the visiting Soyuz crew that included the first space tourist, Dennis Tito. On March 11 she performed a world-record 8 hour and 56 minute space walk to install hardware to the external body of the laboratory module. Helms spent a total of 163 days aboard the space station. She returned to earth with the STS-105 crew aboard \"Discovery\" on August 22, 2001.\n\n\n"}
{"id": "5650682", "url": "https://en.wikipedia.org/wiki?curid=5650682", "title": "The Regenerative Medicine Institute", "text": "The Regenerative Medicine Institute\n\nThe Regenerative Medicine Institute (REMEDI), was established in 2003 as a Centre for Science, Technology & Engineering in collaboration with National University of Ireland, Galway. It obtained an award of €14.9 million from Science Foundation Ireland over five years.\n\nIt conducts basic research and applied research in regenerative medicine, an emerging field that combines the technologies of gene therapy and adult stem cell therapy. The goal is to use cells and genes to regenerate healthy tissues that can be used to repair or replace other tissues and organs in a minimally invasive approach.\n\nCentres for Science, Engineering & Technology help link scientists and engineers in partnerships across academia and industry to address crucial research questions, foster the development of new and existing Irish-based technology companies, attract industry that could make an important contribution to Ireland and its economy, and expand educational and career opportunities in Ireland in science and engineering. CSETs must exhibit outstanding research quality, intellectual breadth, active collaboration, flexibility in responding to new research opportunities, and integration of research and education in the fields that SFI supports.\n\n"}
{"id": "10640506", "url": "https://en.wikipedia.org/wiki?curid=10640506", "title": "Theories about Stonehenge", "text": "Theories about Stonehenge\n\nStonehenge has been the subject of many theories about its origin, ranging from the academic worlds of archaeology to explanations from mythology and the paranormal.\n\nMany early historians were influenced by supernatural folktales in their explanations. Some legends held that Merlin had a giant build the structure for him or that he had magically transported it from Mount Killaraus in Ireland, while others held the Devil responsible. Henry of Huntingdon was the first to write of the monument around AD 1130 soon followed by Geoffrey of Monmouth who was the first to record fanciful associations with Merlin which led the monument to be incorporated into the wider cycle of European medieval romance. According to Geoffrey's \"Historia Regum Britanniae\", when asked what might serve as an appropriate burial place for Britain's dead princes, Merlin advised King Aurelius Ambrosius to raise an army and collect some magical stones from Mount Killarus in Ireland. Whilst at Mount Killarus, Merlin laughed at the soldiers' failed attempts to remove the stones using ladders, ropes, and other machinery. Shortly thereafter, Merlin oversaw the removal of stones using his own machinery and commanded they be loaded onto the soldiers' ships and sailed back to England where they were reconstructed into Stonehenge. Contrary to popular belief Geoffrey did not claim Merlin had commanded a giant to build Stonehenge for him, it appears this detail was embellished by Robert Wace who later translated Geoffrey's original text into French.\n\nIn 1655, the architect John Webb, writing in the name of his former superior Inigo Jones, argued that Stonehenge was a Roman temple, dedicated to Caelus, (a Latin name for the Greek sky-god Uranus), and built following the Tuscan order. Later commentators maintained that the Danes erected it. Indeed, up until the late nineteenth century, the site was commonly attributed to the Saxons or other relatively recent societies.\n\nThe first academic effort to survey and understand the monument was made around 1640 by John Aubrey. He declared Stonehenge the work of Druids. This view was greatly popularised by William Stukeley. Aubrey also contributed the first measured drawings of the site, which permitted greater analysis of its form and significance. From this work, he was able to demonstrate an astronomical or calendrical role in the stones' placement. The architect John Wood was to undertake the first truly accurate survey of Stonehenge in 1740. However Wood’s interpretation of the monument as a place of pagan ritual was vehemently attacked by Stukeley who saw the druids not as pagans, but as biblical patriarchs.\n\nBy the turn of the nineteenth century, John Lubbock was able to attribute the site to the Bronze Age based on the bronze objects found in the nearby barrows.\n\nThe early attempts to identify the people who had undertaken this colossal project have since been debunked. While there have been precious few in the way of real theories to explain who built the site, or why, there can be an assessment of what is known to be fact and what has been disproven.\n\nRadiocarbon dating of the site indicates that the building of the monument at the site began around the year 3100 BC and ended around the year 1600 BC. This allows the elimination of a few of the theories that have been presented. The theory that the Druids were responsible may be the most popular one; however, the Celtic society that spawned the Druid priesthood came into being only after the year 300 BC. Additionally, the Druids are unlikely to have used the site for sacrifices, since they performed the majority of their rituals in the woods or mountains, areas better suited for \"earth rituals\" than an open field. The fact that the Romans first came to the British Isles when Julius Caesar led an expedition in 55 BC negates the theories of Inigo Jones and others that Stonehenge was built as a Roman temple.\n\nThe classical Greek writer Diodorus Siculus (1st century BC) may refer to Stonehenge in a passage from his \"Bibliotheca historica\". Citing the 4th-century BC historian Hecataeus of Abdera and \"certain others\", Diodorus says that in \"a land beyond the Celts\" (i.e. Gaul) there is \"an island no smaller than Sicily\" in the northern sea called \"Hyperborea\", so named because it is beyond the source of the north wind or \"Boreas\". The inhabitants of this place chiefly worship Apollo, and there is \"both a magnificent sacred precinct of Apollo and a notable temple which is adorned with many votive offerings and is spherical in shape.\" Some writers have suggested that Diodorus' \"Hyperborea\" may indicate Great Britain, and that the spherical temple may be an early reference of Stonehenge.\n\nChristopher Chippindale commented that \"This \"might\" be Stonehenge, but the description is short and vague, and there are discrepancies = the climate of the Hyperboreans is so mild they grow two crops a year.\" Aubrey Burl noted that other parts of Diodorus' description make it a poor fit for Stonehenge and its neighbourhood. Diodorus also says that in that area Apollo (meaning, the sun or the moon) \"skimmed the earth at a very low height\". However, both the moon and the sun always move far above the horizon at the latitude of Stonehenge; it is only 500 miles farther north that they can be observed to remain near the horizon.\n\nJ. F. S. Stone felt that a bluestone monument had earlier stood near the nearby Stonehenge Cursus and been moved to their current site from there. If Mercer's theory is correct then the bluestones may have been transplanted to cement an alliance or display superiority over a conquered enemy although this can only be speculation. An oval-shaped setting of bluestones similar to those at Stonehenge 3iv occurs at Bedd Arthur in the Preseli Hills, but that does not imply a direct cultural link. Some archaeologists have suggested that the igneous bluestones and sedimentary sarsens had some symbolism, of a union between two cultures from different landscapes and therefore from different backgrounds. Others believe that that is pure fantasy or mystical appearances.\n\nRecent analysis of contemporary burials found nearby known as the Boscombe Bowmen, has indicated that at least some of the individuals associated with Stonehenge 3 came either from Wales or from some other European area of ancient rocks. Petrological analysis of the stones themselves has verified that some of them have come from the Preseli Hills but that others have come from the north Pembrokeshire coast and possibly the Brecon Beacons.\n\nThe main source of the bluestones is now identified with the dolerite outcrops around Carn Goedog although work led by Olwen Williams-Thorpe of the Open University has shown that other bluestones came from outcrops up to 10 km away. Dolerite is composed of an intrusive volcanic rock of plagioclase feldspar that is harder than granite.\n\nAubrey Burl and a number of geologists and geomorphologists contend that the bluestones were not transported by human agency at all and were instead brought by glaciers at least part of the way from Wales during the Pleistocene. There is good geological and glaciological evidence that glacier ice did move across Preseli and did reach the Somerset coast. It is uncertain that it reached Salisbury Plain, although a spotted dolerite boulder was found in a long barrow at Heytesbury, which was built long before the stone settings at Stonehenge were installed. One current view is that glacier ice transported the stones as far as Somerset, and that they were transported from there by the builders of Stonehenge.\n\nBritain's Geoffrey Wainwright, president of the London Society of Antiquaries, and Timothy Darvill, on 22 September 2008, speculated that it may have been an ancient healing and pilgrimage site, since burials around Stonehenge showed trauma and deformity evidence: \"It was the magical qualities of these stones which ... transformed the monument and made it a place of pilgrimage for the sick and injured of the Neolithic world.\" Radio-carbon dating places the construction of the circle of bluestones at between 2400—2200 BC, but they discovered charcoals dating 7000 BC, showing human activity in the site. It could be the primeval equivalent of Lourdes, since the area was already visited 4,000 years before the oldest stone circle, and attracted visitors for centuries after its abandonment. Some tentative support for this view comes from the first-century BC Greek historian, Diodorus Siculus, who cites a lost account set down three centuries earlier, which described \"a magnificent precinct sacred to Apollo and a notable spherical temple\" on a large island in the far north, opposite what is now France. Amongst other attributes Apollo was recognised as the god of medicine and healing. This theory is hotly disputed, on the grounds that it is not adequately underpinned by evidence on the ground, either in the Preseli Hills area or at Stonehenge.\n\nA study by researchers at the Royal College of Art, London, has proposed that the bluestones may have been attractive for their acoustic properties.\n\nMany archaeologists believe Stonehenge was an attempt to render in permanent stone the more common timber structures that dotted Salisbury Plain at the time, such as those that stood at Durrington Walls. Modern anthropological evidence has been used by Mike Parker Pearson and the Malagasy archaeologist Ramilisonina to suggest that timber was associated with the living and stone with the ancestral dead amongst prehistoric peoples. They have argued that Stonehenge was the terminus of a long, ritualised funerary procession for treating the dead, which began in the east, during sunrise at Woodhenge and Durrington Walls, moved down the Avon and then along the Avenue reaching Stonehenge in the west at sunset. The journey from wood to stone via water was, they consider, a symbolic journey from life to death. There is no satisfactory evidence to suggest that Stonehenge's astronomical alignments were anything more than symbolic and current interpretations favour a ritual role for the monument that takes into account its numerous burials and its presence within a wider landscape of sacred sites. Many also believe that the site may have had astrological/spiritual significance attached to it.\n\nSupport for this view also comes from the historian of religions Mircea Eliade, who compares the site to other megalithic constructions around the world devoted to the cult of the dead (ancestors). \n\nLike other similar English monuments [For example, Eliade identifies, Woodhenge, Avebury, Arminghall, and Arbor Low] the Stonehenge cromlech was situated in the middle of a field of funeral barrows. This famous ceremonial centre constituted, at least in its primitive form, a sanctuary built to insure relations with the ancestors. In terms of structure, Stonehenge can be compared with certain megalithic complexes developed, in other cultures, from a sacred area: temples or cities. We have the same valourisation of the sacred space as \"centre of the world,\" the privileged place that affords communication with heaven and the underworld, that is, with the gods, the chtonian goddesses, and the spirits of the dead.\n\nIn addition to the English sites, Eliade identifies, among others, the megalithic architecture of Malta, which represents a \"spectacular expression\" of the cult of the dead and worship of a Great Goddess.\n\nRadar mapping also reveals that three chalk ridges in the stonehenge area are aligned by geological accident on the midsummer sunrise/midwinter axis. This natural solstitial alignment would have symbolized cosmic unity to the ancients, a place where Heaven and Earth were unified by some supernatural force. This seems to have set the blueprint for solstitial alignments in stonehenge and the timber circles at Durrington Walls and Woodhenge as well. \n\nMike Parker Pearson also believes that the stonehenge was a monument of unification, bringing together different groups with different ancestries. He surmises that the five trilithons in the center of stonehenge could have symbolized five tribal lineages charting their descent from five original ancestors. The Preseli Hills might have had some ancestral significance for the stonehenge builders as well (perhaps this was their place of origin), this may have been the motive behind dragging the bluestones all the way from Preseli Hills to Wiltshire. The trilithons may have also represented a D shaped meeting house of which similar structures have been found at other Neolithic sites in Britain. This may have represented a meeting place for the ancestors of the stonehenge builders. Others have suggested that the trilithons represented doorways to another world.\n\nA recently published analysis draws attention to the fact that the stones display mirrored symmetry and that the only undisputed alignment to be found is that of the solstices, which can be regarded as the axis of that symmetry. This interpretation sees the monument as having been designed off-site, largely prefabricated and set out to conform to survey markers set out to an exact geometric plan.\n\nThe idea of ‘precision’ (below) demands that exact points of reference were used, both between the structural elements and in relation to the axis (i.e. that of the solstices). Johnson's theory asserts that prehistoric survey markers could not have been placed within the footprint of the stones, but must have been (as in any construction) external to the stones. That almost all the stones have one ‘better’ i.e. flatter face, and that face is almost invariably inwards, suggests that the construction was set out so that the prehistoric builders could use the center point of the inner faces as reference. This is very significant in respect of the Great Trilithon; the surviving upright has its flatter face outwards (see image on right), towards the midwinter sunset, and was raised from the inside. The remainder of the trilithon array (and almost all of the stones of the Sarsen Circle) had construction ramps which sloped inwards, and were therefore set up from the outside. Placing the centre face of the stones (regardless of their thickness) against markers would mean that the ‘gaps’ between the stones were simply consequential. The study of the geometric layout of the monument shows that such methods were used and that there is a clear argument for regarding other outlying elements as part of a geometric scheme (e.g. the ‘Station Stones’ and the stoneholes 92 and 94 which mark two opposing facets of an octagon). A geometric design is scalable from concept to construction, removing much of the need for measurements to be made at all.\n\nMuch speculation has surrounded the engineering feats required to build Stonehenge. Assuming the bluestones were brought from Wales by hand, and not transported by glaciers as Aubrey Burl has claimed, various methods of moving them relying only on timber and rope have been suggested. In a 2001 exercise in experimental archaeology, an attempt was made to transport a large stone along a land and sea route from Wales to Stonehenge. Volunteers pulled it for some miles (with great difficulty) on a wooden sledge over land, using modern roads and low-friction netting to assist sliding, but it became clear that it would have been incredibly difficult for even the most organized of tribal groups to have pulled large numbers of stones across the densely wooded, rough and boggy terrain of West Wales.\n\nIn 2010, \"Nova\"'s \"Secrets of Stonehenge\" broadcast an effective technique for moving the stones over short distances using ball bearings in a wooden track as originally envisioned by Andrew Young, a graduate student of Bruce Bradley—director of experimental archaeology at the University of Exeter.\n\nIn 1997 Julian Richards teamed up with Mark Witby and Roger Hopkins to conduct several experiments to replicate the construction at Stonehenge for \"Nova\"'s \"Secrets of Lost Empires\" mini series. They arranged for a gang of 130 people to attempt to tow a 40-ton concrete replica on a sledge which was placed on wooden tracks. They used grease to make it easier to tow up a slight incline and still they were unable to budge it. They gathered additional men and had some of them use levers to try to pry the megalith while others towed it at the same time. When they all worked together at the same time they were able to move it forward. They were uncertain whether this would be the way they would have transported the largest stones 25 miles. To do this would require an enormous amount of track and a lot of coordination for a large number of people. In some cases this would involve towing the stones over rougher terrain. They also conducted an experiment to erect 2 forty ton replicas and put a 9-ton lintel on top. After a lot of experimenting they were able to erect 2 megaliths using a large number of people towing and using levers. They also managed to tow the lintel up a steel ramp. They were unable to determine this was the final answer but they demonstrated that this was a possible method. At times they were forced to use modern technology for safety reasons.\n\nJosh Bernstein and Julian Richards organized an experiment to pull a 2-ton stone on wooden tracks with a group of about 16 men. They placed the stone on a wooden sledge then placed the sledge on a wooden track. They pulled this with two gangs of about eight men. To move the stones as many miles across Southern England, the creators of Stonehenge would have had to build a lot of track, or move and rebuild track in pieces, as the stones were taken to their final destination.\n\nA recent article has argued that the massive stones could be moved by submerging them in water and towing them below an ancient vessel or group of vessels. This technique would have two significant advantages. It would reduce the load borne by the vessel while part of the stone's weight is displaced by the water. Secondly, the arrangement of the load below the vessel would be much more stable and reduce the risk of catastrophic failure. Naturally, this would apply only for transportation over water. The technique was tried during the Millennium Stone Project 2000, with a single bluestone slung beneath two large curraghs. The sling frayed away, and the stone plunged to the bed of Milford Haven.\n\nIt has been suggested that timber A-frames were erected to raise the stones, and that teams of people then hauled them upright using ropes. The topmost stones may have been raised up incrementally on timber platforms and slid into place or pushed up ramps. The carpentry-type joints used on the stones imply a people well skilled in woodworking and they could easily have had the knowledge to erect the monument using such methods. In 2003 retired construction worker Wally Wallington demonstrated ingenious techniques based on fundamental principles of levers, fulcrums and counterweights to show that a single man can rotate, walk, lift and tip a ten-ton cast-concrete monolith into an upright position. He is progressing with his plan to construct a simulated Stonehenge with eight uprights and two lintels.\n\nAlexander Thom was of the opinion that the site was laid out with the necessary precision using his megalithic yard.\n\nThe engraved weapons on the sarsens are unique in megalithic art in the British Isles, where more abstract designs were invariably favoured. Similarly, the horseshoe arrangements of stones are unusual in a culture that otherwise arranged stones in circles. The axe motif is, however, common to the peoples of Brittany at the time, and it has been suggested at least two stages of Stonehenge were built under continental influence. This would go some way towards explaining the monument's atypical design, but overall, Stonehenge is still inexplicably unusual in the context of any prehistoric European culture.\n\nEstimates of the manpower needed to build Stonehenge put the total effort involved at millions of hours of work. Stonehenge 1 probably needed around 11,000 man-hours (or 460 man-days) of work, Stonehenge 2 around 360,000 (15,000 man-days or 41 years). The various parts of Stonehenge 3 may have involved up to 1.75 million hours (73,000 days or 200 years) of work. The working of the stones is estimated to have required around 20 million hours (830,000 days or 2,300 years) of work using the primitive tools available at the time. Certainly, the will to produce such a site must have been strong, and an advanced social organization would have been necessary to build and maintain it. However, Wally Wallington's work suggests that Stonehenge's construction may have required fewer man-hours than previously estimated.\n\nNote that the estimate of 20 million man-hours means that 10,000 men working on the site for 20 days each year, for 8 hours per day, could have completed it in 12.5 years.\n\nSome commentators have suggested that the stones may have been the core supporting framework for a larger wooden building with a roof.\n\nBritish author John Michell wrote that Alfred Watkins's ley lines appeared to be in alignment with various traditional sacred sites around the country. Michell wrote that \"There is a curious symmetry about the positioning of the three Perpetual Choirs in Britain. Stonehenge and Llantwit Major are equidistant from Glastonbury, some 38.9 miles away, and two straight lines drawn on the map from Glastonbury to the other two choirs form an angle of 144 degrees...The axis of Glastonbury Abbey points toward Stonehenge, and there is some evidence that it was built on a stretch on ancient trackway which once ran between the two Choirs\". Michell created diagrams that illustrated correlations between the design of Stonehenge and astronomical proportions and relationships. However, the Welsh Triads refer not to Stonehenge but to the village of Amesbury which is two miles from Stonehenge.\n\n\n\n"}
{"id": "408239", "url": "https://en.wikipedia.org/wiki?curid=408239", "title": "Tracer ammunition", "text": "Tracer ammunition\n\nTracer ammunition (tracers) are bullets or cannon caliber projectiles that are built with a small pyrotechnic charge in their base. Ignited by the burning powder, the pyrotechnic composition burns very brightly, making the projectile trajectory visible to the naked eye during daylight, and very bright during nighttime firing. This enables the shooter to make aiming corrections without observing the impact of the rounds fired and without using the sights of the weapon. Tracer fire can also be used to signal to other shooters where to concentrate their fire during battle.\n\nWhen used, tracers are usually loaded as every fifth round in machine gun belts, referred to as four-to-one tracer. Platoon and squad leaders will load some tracer rounds in their magazine or even use solely tracers to mark targets for their soldiers to fire on. Tracers are also sometimes placed two or three rounds from the bottom of magazines to alert the shooter that his weapon is almost empty. During World War II, aircraft with fixed machine guns or cannons mounted would sometimes have a series of tracer rounds added near the end of the ammunition belts, to alert the pilot that he was almost out of ammunition. However, this practice similarly alerted astute enemies that their foes were nearly out of ammunition. More often, however, the entire magazine was loaded four-to-one, on both fixed offensive and flexible defensive guns, to help mitigate the difficulties of aerial gunnery. Tracers were very common on most WWII aircraft, with the exception of night fighters, which needed to be able to attack and shoot down the enemy before they realized they were under attack, and without betraying their own location to the enemy defensive gunners. The United States relied heavily on tracer ammunition for the defensive Browning M2 .50 caliber machine guns on its heavy bombers such as the B-24 Liberator, but they found that during deflection shooting, gunners who tried to aim using the tracers rather than the sights ended up not \"leading\" the enemy aircraft enough and missing, because of an optical illusion that made it appear that the tracers were striking the aircraft, when in reality they were passing significantly behind it. This proved true of fixed forward firing guns as well. In some cases, gunners were ordered not to use tracer ammunition at all, but in the end, with greater emphasis during training on using the sights and not trusting the tracers, the problem was solved to a large degree.\n\nTracer rounds can also have a mild incendiary effect, and can ignite flammable substances on contact, provided the tracer compound has started burning and is still burning on impact.\n\nBefore the development of tracers, gunners relied on seeing their bullet impacts to adjust their aim. However, these were not always visible, especially as the effective range of ammunition increased dramatically during the later half of the 19th century, meaning the bullets could impact a mile or more away in long range area fire. In the early 20th century, ammunition designers developed \"spotlight\" bullets, which would create a flash or smoke puff on impact to increase their visibility. However, these projectiles were deemed in violation of the Hague Conventions prohibition of \"exploding bullets.\" This strategy was also useless when firing at aircraft, as there was nothing for the projectiles to hit if they missed the target. Designers also developed bullets that would trail white smoke. However, these designs required an excessive amount of mass loss to generate a satisfactory trail. The loss of mass en route to the target severely affected the bullet's ballistics.\n\nThe United Kingdom was the first to develop and introduce a tracer round, a version of the .303 cartridge in 1915. The United States introduced a .30-06 tracer in 1917. Prior to adopting red (and later, other color) bullet tips for tracers, American tracers were identified by blackened cartridge cases.\n\nTracers proved useful as a countermeasure against Zeppelins used by Germany during World War I. The airships were used for reconnaissance, surveillance and bombing operations. Normal bullets merely had the effect of causing a slow leak, but tracers could ignite the hydrogen gasbags, and bring down the airship quickly.\n\nIn World War II US naval and marine aircrew were issued tracer rounds with their side arms for emergency signaling use as well as defense.\n\nA tracer projectile is constructed with a hollow base filled with a pyrotechnic flare material, made of a mixture of a very finely ground metallic fuel, oxidizer, and a small amount of organic fuel. Metallic fuels include magnesium, aluminum, and occasionally zirconium. The oxidizer is a salt molecule which contains oxygen combined with a specific atom responsible for the desired color output. Upon ignition, the heated salt releases its oxygen to sustain combustion of the fuel in the mixture. The color-emitting atom in the salt is also released and reacts chemically with excess oxygen providing the source of the colored flame. In NATO standard ammunition (including US), the oxidizer salt is usually a mixture of strontium compounds (nitrate, peroxide, etc.) and the metallic fuel is magnesium. Burning strontium yields a bright red light. Russian and Chinese tracer ammunition generates green light using barium salts. An oxidizer and metallic fuel alone, however, do not make a practical pyrotechnic for the purposes of producing colored light. The reaction is too energetic, consuming all materials in one big flash of white lightwhite light being the characteristic output of magnesium-oxide (MgO), for example. Therefore, in the case of using strontium nitrate and magnesium, to produce a red colored flame that is not over-powered by the white light from the burning fuel, a chlorine donor is provided in the pyrotechnic mixture, so that strontium chloride can also form in the flame, cooling it so that the white light of MgO is greatly reduced. Cooling the flame in this manner also lengthens the reaction rate so that the mixture has an appreciable burn time. Polyvinyl chloride (PVC) is a typical organic fuel in colored light for this purpose. Some modern designs use compositions that produce little to no visible light and radiate mainly in infrared, being visible only on night vision equipment.\n\nThere are three types of tracers: bright tracer, subdued tracer and dim tracer. Bright tracers are the standard type, which start burning very shortly after exiting the muzzle. A disadvantage of bright tracers is that they give away the shooter's location to the enemy; as a military adage puts it, \"tracers work both ways\". Bright tracers can also overwhelm night-vision devices, rendering them useless. Subdued tracers burn at full brightness after a hundred or more yards to avoid giving away the gunner's position. Dim tracers burn very dimly but are clearly visible through night-vision equipment.\n\nThe M196 tracer cartridge (54-grain bullet) 5.56mmx45mm NATO was developed for the original M16 rifle and is compatible with the M16A1 barrel also using 1:12 rifling twist. It has a red tip and is designed to trace out to 500 yards, and trajectory match the M193 (56-grain) ball cartridge, which has no tip color. Trajectory match, or ballistic match, is achieved between two bullets of slightly different weight and aerodynamic characteristics by adjusting the cartridge propellant weight, propellant type, and muzzle velocity, to remain within safe pressure limits, yet provide each bullet with a trajectory to the target that is nearly identical over all atmospheric conditions and target engagement ranges, while using the same gunsight aimpoint. Trajectory match is not intended to be perfect, an engineering impossibility under the closest of similarities between the two bullets, that is further complicated in the case of the tracer losing mass and changing its drag properties as it flies. The intent is that the tracer matches the ball round well enough for the purposes of machinegun fire.\n\nThe M856 tracer cartridge (63.7-grain bullet) is used in the M16A2/3/4, M4-series, and M249 weapons (among other 5.56mm NATO weapons). This round is designed to trace out to 875 yards and has an orange tip color, and is trajectory matched to the M855 (62-grain, green tip) ball cartridge. The M856 tracer should not be used in the M16A1 except under emergency conditions and only in relatively warm weather, because the M16A1's slower 1:12 rifling twist is not sufficient to properly stabilize this projectile at colder combat service temperatures (freezing down to –40 degrees), when the air density is much greater, disrupting the gyroscopic stability of the bullet. The M16A2 and newer models have a rifling twist of 1 in 7\" necessary to stabilize the M856 tracer round under all temperature conditions. (The M196, however, does function safely in all 1:7 twist barrels, as well as those with 1:12 twist.)\n\nThe M25 is an orange-tipped .30-06 Springfield tracer cartridge consisting of a 145 gr bullet with 50 grains of IMR 4895 powder. The tracer compound contains composition R 321 which is 16% polyvinyl chloride, 26% magnesium powder, 52% strontium nitrate.\n\nThe M62 is an orange-tipped 7.62×51mm NATO tracer consisting of a 142 gr bullet with 46 grains of WC 846 powder. The tracer compound contains composition R 284 which is 17% polyvinyl chloride, 28% magnesium powder, and 55% strontium nitrate. (This is the same composition used on the M196.)\n\nThe M276 is a violet-tipped 7.62×51mm NATO dim tracer that uses composition R 440, which is barium peroxide, strontium peroxide, calcium resinate for example calcium abietate, and magnesium carbonate.\n\nTracer compositions can also emit primarily in infrared, for use with night-vision devices. An example composition is boron, potassium perchlorate, sodium salicylate, iron carbonate or magnesium carbonate (as combustion retardant), and binder. Many variants exist.\n\nTracers can also serve to direct fire at a given target, because they are visible to other combatants. The disadvantage is that they betray the gunner's position; the tracer path leads back to its source. To make it more difficult for an enemy to do this, most modern tracers have a delay element, which results in the trace becoming visible some distance from the muzzle. Depending on the target, tracer bullet lethality may be similar to standard ball ammunition. The forward portion of a tracer bullet contains a substantial slug of lead filler, nearly as much as the non-tracer ball round that it trajectory matches. In the case of the M196/M193 bullet set, the lethality differences are probably negligible for this reason. However, with the M856/M855 bullet set, the M855 ball round contains a steel penetrator tip that is not present in the M856 tracer bullet. As a result, different lethality effects can be expected against various targets. Nevertheless, under some circumstances, a slight degradation in lethality can often be made up for by the psychological and suppressive-fire effects tracer bullets can have on an enemy who is receiving them. \n\nBesides guiding the shooter's direction of fire, tracer rounds can also be loaded at the end of a magazine to alert the shooter that the magazine is almost empty. This is particularly useful in weapons that do not lock the bolt back when empty (such as the AK-47). During World War II, the Soviet Air Force also used this practice for aircraft machine guns. One disadvantage in this practice is that the enemy is alerted that the pilot or shooter is low on ammunition and possibly vulnerable. For ground forces, this generally offers no tactical advantage to the enemy, since a soldier with a crew-served weapon such as a machine gun who is out of ammunition is supposed to alert his team that he is \"dry\" and rely on their cover fire while he reloads the machine gun. Thus, an enemy must risk exposing himself in order to attack the reloading soldier. Modern jet fighters primarily rely on radar and infrared seeker missiles to track and destroy enemy planes and laser-guided missiles to attack surface targets, rather than the plane's cannon, which may be just an ancillary weapon for air-to-air combat; although in the ground attack role, cannon fire may be emphasized. However, modern fighter aircraft use gyroscopes and inertial sensors coupled with radar and optical computing gunsights that make the use of tracers in cannon ammunition unnecessary. As long as the pilot can put the \"pipper\" (aiming point) in the HUD (head-up-display) onto the target, he can be assured that the burst will be on target, since the computers automatically compute range, closing rate, deflection, lateral accelerations, even weather conditions to calculate target lead and aimpoint. Thus one of the primary reasons for using tracers on aircraft in the first place, uncertainty over where the bullets will end up in relation to the target, is removed. Another use for the tracer is in tank hull machineguns, of mostly out-dated tanks, where the machinegun operator cannot sight directly along the barrel, thus he has to rely on tracer bullets to guide his aim. Modern main battle tanks and armored fighting vehicles, however, employ advanced fire control systems that can accurately aim secondary weapons along with the main armament; although the continued use of tracers provides reassurance to gunners on the direction of machinegun fire.\n\nIn the UK, use of tracer rounds is restricted on National Rifle Association of the United Kingdom-operated ranges, due to an increased risk of fire. Unauthorized use is punished at the discretion of the acting range officer. Use of tracers is usually only authorized during military training.\n\nIn July 2009, a large fire was started by tracer ammunition near Marseille, France, an area where shrub vegetation is very dry and flammable in the summer, and where normally this kind of ammunition should not be used.\n\nOn February 24, 2013, a fire was started at DFW Gun Club in Dallas, TX by the use of a tracer round inside the facility.\n\nOn July 2, 2018, the Lake Christine Fire near Basalt, CO was started by tracer rounds fired at a gun range. Two people have been charged with 4th degree arson.\n\n\n"}
{"id": "46540724", "url": "https://en.wikipedia.org/wiki?curid=46540724", "title": "U-blox", "text": "U-blox\n\nu-blox is a Swiss company that creates wireless semiconductors and modules for consumer, automotive and industrial markets. They operate as a fabless IC and design house. Their wireless solutions connect machines, vehicles and people to locate their exact positions and communicate via short range (WI-FI, Bluetooth) or cellular networks. Using their portfolio of chips, modules or software solutions it’s possible to create subsystems and products to fulfill needs for the Internet of Things (IoT), M2M or Car2Car (or Vehicle to Vehicle) solutions quick and cost-effectively.\n\nThey acquired a dozen companies after their IPO in 2007, the last two acquisitions were connectblue in 2014 and Lesswire in 2015 \n\nu-blox was founded in 1997 and is headquartered in Thalwil, Switzerland. The company is listed at the Swiss Stock Exchange (SIX:UBXN) and has offices in the USA, Singapore, China, Taiwan, Korea, Japan, India, Pakistan, Australia, Ireland, the UK, Belgium, Germany, Sweden, Finland, Italy and Greece.\n\nIn April 2015 u-blox successfully issued a 60 Million CHF Bond. The bonds bear a 1.625% coupon and have a duration of 6 years. The bonds were placed with institutional and private investors in Switzerland under the lead management of Credit Suisse AG and with Zürcher Kantonalbank acting as a co-manager.\n\nu-blox develops and sells chips and modules that support global navigation satellite systems (GNSS), including receivers for GPS, GLONASS, Galileo, BeiDou and QZSS. The wireless range consists of GSM-, UMTS- and CDMA2000 and LTE modules, as well as Bluetooth- and WiFi-modules. All these products enable the delivery of complete systems for location-based services and M2M applications (machine-to-machine communication) in the Internet of Things, that rely on the convergence of 2G/3G/4G, Bluetooth-, Wi-Fi technology and satellite navigation.\n\nu-blox provides starter kits which allow quick prototyping of variety of applications for the Internet of Things.\n\nAssisted GPS:The globally available online service assisted GNSS (A-GNSS) permits GNSS receivers to compute a position within seconds, even under very poor signal conditions.\n\n3-Dimensional Automotive Dead Reckoning (“3D ADR”): It aids traditional GPS/GNSS navigation via intelligent algorithms based on distance, direction and elevation changes made during satellite signal interruption. u-blox' 3D ADR GNSS chip blends satellite navigation data with individual wheel speed, gyroscope and accelerometer information to deliver accurate positioning regardless of changes in a vehicle’s speed, heading or vertical displacement, even when satellite signals are partially or completely blocked. This is especially important when quick navigation decisions must be made immediately upon exiting tunnels and parkhouses.\n\nCellLocate®: Although GPS is a widespread technology, GPS positioning is not always possible, particularly in shielded environments such as indoors and enclosed park houses, or when a GPS jamming signal is present. The situation can be improved by augmenting GPS receiver data with mobile network cell attributes to provide a level of redundancy that can benefit numerous applications.\n\nTiming: GPS/GNSS signals are widely used as low-cost precision time or frequency references by remote or distributed wireless communication, industrial, financial, and power-distribution equipment. By capitalizing on atomic clocks which are onboard positioning satellites, GPS/GNSS signals containing embedded timing information can be used to synchronize equipment to within 15 ns, as well as provide UTC time to an accuracy within 90 ns\n\nLTE (Long Term Evolution) stands for a standard in wireless communication of high-speed data for mobile phones and data terminals. u-blox delivers several products for this technique and is certified for all global markets and providers (e.g. Verizon, USA)\n\nGNSS Applications: delivering a complete set of wireless chips and modules u-blox can support system providers to create products with integrated location based functionality.\n\nShort Range / Radio: Bluetooth and WiFi are technologies to exchange data within short distances.\n\nNarrowband IoT (NB-IoT) is a Low Power Wide Area (LPWA) technology designed for Internet of Things (IoT) applications such as utility meters, building automation and smart cities, and available for deployment over existing mobile networks in licensed radio spectrum. Such applications have low data rates, require long battery lives up to 10 years, and operate unattended for long periods of time, often in remote or hard to reach locations such as basements and with varying temperatures. u-blox is participating in the 3rd Generation Partnership Project (3GPP) RAN working group meetings ramping up to the implementation of the 3GPP Release 13 of NB-IoT standards in June 2016.\n\nCentimeter-level precise positioning at u-blox is achieved by combining Real Time Kinematics (RTK) technology and u-blox Global Navigation Satellite Systems (GNSS) expertise. The base station module sends corrections via a communication link that uses Radio Technical Commission for Maritime Services (RTCM) protocol to the moving end-user, the rover module, enabling it to output its position relative to the base station at centimeter-level accuracy. The demand for lower price high precision technology is growing, in particular for applications such as precision agriculture, UAVs, or robotic guidance systems. \n\nThe products of u-blox as well as the solutions are utilized by almost 5,200 OEMs in integrated GNSS and wireless applications. E.g. fleet management, remote monitoring and control, wireless data transmission, tracking of goods and people, navigation, emergency call systems (eCall, ERA-GLONASS), wearable devices, cameras, tablets and mobile phones.\n\n"}
{"id": "16078025", "url": "https://en.wikipedia.org/wiki?curid=16078025", "title": "Uplift modelling", "text": "Uplift modelling\n\nUplift modelling, also known as incremental modelling, true lift modelling, or net modelling is a predictive modelling technique that directly models the incremental impact of a treatment (such as a direct marketing action) on an individual's behaviour.\n\nUplift modelling has applications in customer relationship management for up-sell, cross-sell and retention modelling. It has also been applied to political election and personalised medicine. Unlike the related Differential Prediction concept in psychology, Uplift Modelling assumes an active agent.\n\nUplift modelling uses a randomised scientific control to not only measure the effectiveness of an action but also to build a predictive model that predicts the incremental response to the action. It is a data mining technique that has been applied predominantly in the financial services, telecommunications and retail direct marketing industries to up-sell, cross-sell, churn and retention activities.\n\nThe uplift of a marketing campaign is usually defined as the difference in response rate between a \"treated\" group and a randomized \"control\" group. This allows a marketing team to isolate the effect of a marketing action and measure the effectiveness or otherwise of that individual marketing action. Honest marketing teams will only take credit for the incremental effect of their campaign.\n\nHowever, many marketers define lift (rather than uplift) as the difference in response rate between treatment and control, so uplift modeling can be defined as improving (upping) lift through predictive modeling.\n\nThe table below shows the details of a campaign showing the number of responses and calculated response rate for a hypothetical marketing campaign. This campaign would be defined as having a response rate uplift of 5%. It has created 50,000 incremental responses (100,000 - 50,000).\n\nTraditional response modelling typically takes a group of \"treated\" customers and attempts to build a predictive model that separates the likely responders from the non-responders through the use of one of a number of predictive modelling techniques. Typically this would use decision trees or regression analysis.\n\nThis model would only use the treated customers to build the model.\n\nIn contrast uplift modelling uses both the treated and control customers to build a predictive model that focuses on the incremental response. To understand this type of model it is proposed that there is a fundamental segmentation that separates customers into the following groups:\n\n\nThe only segment that provides true incremental responses is the \"Persuadables\".\n\nUplift modelling provides a scoring technique that can separate customers into the groups described above.\n\nTraditional response modelling often targets the \"Sure Things\" being unable to distinguish them from the \"Persuadables\".\n\nBecause uplift modelling focuses on incremental responses only, it provides very strong return on investment cases when applied to traditional demand generation and retention activities. For example, by only targeting the persuadable customers in an outbound marketing campaign, the contact costs and hence the return per unit spend can be dramatically improved.\n\nOne of the most effective uses of uplift modelling is in the removal of negative effects from retention campaigns. Both in the telecommunications and financial services industries often retention campaigns can trigger customers to cancel a contract or policy. Uplift modelling allows these customers, the Do Not Disturbs, to be removed from the campaign.\n\nIt is rarely the case that there is a single treatment and control group. Often the \"treatment\" can be a variety of simple variations of a message or a multi-stage contact strategy that is classed as a single treatment. In the case of A/B or multivariate testing, uplift modelling can help in understanding whether the variations in tests provide any significant uplift compared to other targeting criteria such as behavioural or demographic indicators.\n\nThe first appearance of \"true response modelling\" appears to be in the work of Radcliffe and Surry.\n\nVictor Lo also published on this topic in \"The True Lift Model\" (2002), and later Radcliffe again with \"Using Control Groups to Target on Predicted Lift: Building and Assessing Uplift Models\" (2007).\n\nRadcliffe also provides a very useful frequently asked questions (FAQ) section on his web site, Scientific Marketer. Lo (2008) provides a more general framework, from program design to predictive modeling to optimization, along with future research areas.\n\nIndependently uplift modelling has been studied by Piotr Rzepakowski. Together with Szymon Jaroszewicz he adapted information theory to build multi class uplift decision trees and published the paper in 2010. And later in 2011 they extended the algorithm to multiple treatment case.\n\nSimilar approaches have been explored in personalised medicine. Szymon Jaroszewicz and Piotr Rzepakowski (2014) designed uplift methodology for survival analysis and applied it to randomized controlled trial analysis. Yong (2015) combined a mathematical optimization algorithm via dynamic programming with machine learning methods to optimally stratify patients.\n\nUplift modelling is a special case of the older psychology concept of Differential Prediction. In contrast to differential prediction, uplift modelling assumes an active agent, and uses the uplift measure as an optimization metric.\n\nUplift modeling has been recently extended and incorporated into diverse machine learning algorithms, like Inductive Logic Programming, Bayesian Network, Statistical relational learning, Support Vector Machines, Survival Analysis and Ensemble learning.\n\nEven though uplift modeling is widely applied in marketing practice (along with political elections), it has rarely appeared in marketing literature. Kane, Lo and Zheng (2014) published a thorough analysis of three data sets using multiple methods in a marketing journal and provided evidence that a newer approach (known as the Four Quadrant Method) worked quite well in practice. Recently, Lo and Pachamanova (2015) extended uplift modeling to prescriptive analytics for multiple treatment situations and proposed algorithms to solve large deterministic optimization problems and complex stochastic optimization problems where estimates are not exact.\n\n\n\n\n"}
{"id": "2686766", "url": "https://en.wikipedia.org/wiki?curid=2686766", "title": "Wakamaru", "text": "Wakamaru\n\nWakamaru is a Japanese robot made by Mitsubishi Heavy Industries that is intended to perform natural communication with human beings. The yellow, 3-foot domestic robot debuted in 2005 at a $14,300-$15,000 USD price-point exclusively for Japanese households. Through its development, the Wakamaru has been used for presenting at exhibitions, guiding customers, and working as a desk receptionist. However, the Wakamaru has not advanced beyond its first model that was released in 2005.\n\nThe earliest form of the Wakamaru was in 2000, when Mitsubishi released information on a project entitled \"MHI Frontier 21 Project.\" This project was launched with a small group of younger employees that proposed a \"Service Robot Business Project.\" In 2001, development on a \"Life-Assist Robot\" was started and a prototype was built by product designer Toshiyuki Kita.\n\nOn February 2, 2003, the Wakamaru was announced to media outlets in Japan and was displayed at the Robodex 2003 robot exhibition later in April. In 2005, Mitsubishi reported that they exclusively did a test sale of 100 units of Wakamaru robots to \"ordinary households\" in twenty three Tokyo neighborhoods.\n\nBy 2007, the Wakamaru was available for rent for Japanese companies to use the robot as a receptionist for $1,000 a day.\n\nThe robot resurfaced on the internet in 2014, when a Twitter user tweeted a photo of the robots in a garbage collection unit.\n\nThe Wakamaru was designed by Toshiyuki Kita. He wanted to create \"a humanoid robot that can approach its user\" so that the Wakamaru could be seen as an independent personality rather than a machine. However, the robot was designed to look neither male or female. The robot was marketed to look not-threatening and likable.\n\nThe yellow robot is three feet tall, consisting of a bottom base with a wheel and three body components on top. The arms are silver, while the hands are yellow. The circular head has round eyes which work as sensors and a wide mouth which makes the robot look toy-like.\n\nThe Wakamaru moves by a wheel that can go as fast as one kilometer per hour. Its sensors can detect self position, infrared ray obstacles, and ultrasonic obstacles, and collisions. The Wakamaru product comes with the robot main unit, a charging station, a broadband router, touch panel,\nand computer. Its height is 100 cm and diameter is 45 cm.\n\nThe bottom component contains the touch sensor, odometers (encoders) that internally indicates the robot's distance traveled, and infrared range sensors that prevent it from running into objects or people. The component above the bottom contains more ultrasonic range sensors and another infrared range sensor that also prevent the robot from running into objects or walls. The \"chest\" component contains an LED indicator to indicate if the robot is on or off. This component also contains the directional microphone and speaker. The \"head\" component has the front camera and omni-directional camera, a rotating camera so the robot can see in all directions.\n\nIt operates on a rechargeable lithium-ion battery that can\noperate for two hours. The Wakamaru automatically moves to the charging station when it needs to be charged. The Wakamaru runs on the operating system Linux.\n\nSince the Wakamaru was designed for daily domestic life, the design for safety was prioritized. Mitsubishi used an international risk assessment method and tested all risks to ensure the product would be safe for use. From these tests, the arms and wheels of the Wakamaru have not caused any injury from collision.\n\nUsing its ultrasonic and infrared sensors, the Wakamaru can avoid collision with objects and humans. It can be manually stopped at any contact with the bumpers on its shoulders or hands. There are also switches on its bumper, side, and bag to stop movement.\n\nThere are no reports of safety issues.\n\nThe robot can recognize 10,000 Japanese words and carry on communication with human beings. It can also connect to wireless internet to tell you the current weather forecasts, reception information, and news. Mitsubishi claims the robot can recognize ten faces at a time, but reports say the robot can recognize eight. The robot can also take photos of unrecognizable faces in the house in case of a robbery and send the photos to the consumer's mobile phone.\n\nDue to its US$14,000–$15,000 price-point, the Wakamaru has been criticized as being too expensive for what it does. Mitsubishi had predicted at least 10,000 sales in the United States a year after the launch, but only shipped about a dozen models overseas. As of 2008, the robot went off the market but remained for rent as a receptionist for $1,000 a day. On June 23, 2014 a photo of a number of Wakamaru units in a garbage collection unit at a Japanese university was tweeted by a Japanese Twitter user.\n\nUnlike the Wakamaru, domestic robots such as the Roomba and The Litter Robot remain popular amongst consumers because of their affordability and functionality of household chores.\n\nAstrid M. Rosenthal-von der Pütten of University of Duisburg-Essen, Germany conducted research on how characteristics of robots determine responses from humans through an online web poll. The Wakamaru was used in this research project in a cluster of robots to determine four factors: threatening, likability, submissiveness, and unfamiliarity. From the web online poll, the results concluded that Wakamaru and other robots such as the Dynamoid, Icat, Asoy, Riman, and Snackbot were non-threatening, submissive, not human and mechanical but also unfamiliar and less likable. The research says that this is may be due to their colorful but unusual shape.\n\n"}
{"id": "18716636", "url": "https://en.wikipedia.org/wiki?curid=18716636", "title": "WebQuest", "text": "WebQuest\n\nA WebQuest is an inquiry-oriented lesson format in which most or all the information that learners work with comes from the web. These can be created using various programs, including a simple word processing document that includes links to websites.\n\nA WebQuest is distinguished from other Internet-based research by four characteristics. First, it is classroom-based. Second, it emphasizes higher-order thinking (such as analysis, creativity, or criticism) rather than just acquiring information. And third, the teacher preselects the sources, emphasizing information use rather than information gathering. Finally, though solo WebQuests are not unknown, most WebQuests are group work with the task frequently being split into roles.\n\nA WebQuest has 6 essential parts: introduction, task, process, resources, evaluation, and conclusion. The original paper on WebQuests had a component called guidance instead of evaluation.\n\nThe task is the formal description of what the students will produce in the WebQuest. The task should be meaningful and fun. Creating the task is the most difficult and creative part of developing a WebQuest.\n\nThe steps the students should take to accomplish the task. It is frequently profitable to reinforce the written process with some demonstrations.\n\nThe resources the students should use. Providing these helps focus the exercise on processing information rather than just locating it. Though the instructor may search for the online resources as a separate step, it is good to incorporate them as links within the process section where they will be needed rather than just including them as a long list elsewhere. Having off-line resources like visiting lecturers and sculptures can contribute greatly to the interest of the students.\n\nThe way in which the students' performance will be evaluated. The standards should be fair, clear, consistent, and specific to the tasks set.\n\nTime set aside for reflection and discussion of possible extensions.\n\nWebquests can be a valuable addition to a collaborative classroom. One of the goals is to increase critical thinking by employing higher levels of Bloom’s Taxonomy and Webb’s Depth of Knowledge. This is a goal of the American educational system's Common Core and many new American state standards for public education. Since most webquests are done in small collaborative groups, they can foster cooperative learning and collaborative activities. Students will often be assigned roles, allowing them to roleplay in different positions, and learn how to deal with conflict within the group.\n\nWebquests can be a versatile tool for teaching students. They can be used to introduce new knowledge, to deepen knowledge, or to allow students to test hypotheses as part of a final interaction with knowledge. The integration of computers and the Internet also increase students’ competency with technology. By having specific task lists, students can stay on task. By having specific sources of information, students can focus on using resources to answer questions rather than vetting resources to use which is a different skill altogether. \n\nIn inclusive classrooms (classrooms that have students of varying exceptionalities interacting such as learning disabled, language impaired, or giftedness) tasks can be differentiated to a skill level or collaborative groups for the same level of task. A skill level may have students with learning disabilities working on a basic task to meet the minimum standard of learning skills and gifted students pushing their task to the higher end of the learning skill. More commonly, groups are composed of learners of all skill levels and completing the same level of task. This is typically easier because the teacher is only creating one webquest, but can cause less student interaction from lower students and less learning from higher students.\n\nWebQuests are only one tool in a teacher's toolboxes. They are not appropriate to every learning goal. In particular, they are weak in teaching factual total recall, simple procedures, and definitions.\n\nWebQuests also usually require good reading skills, so are not appropriate to the youngest classrooms or to students with language and reading difficulties without accommodations. One might ask an adult to assist with the reading or use screen-reading technologies, such as VoiceOver or Jaws.\n\nLearners typically complete WebQuests as cooperative groups. Each learner within a group can be given a \"role,\" or specific area to research. WebQuests may take the form of role-playing scenarios, where students take on the personas of professional researchers or historical figures.\n\nA teacher can search for WebQuests on a particular topic or they can develop their own using a web editor like Microsoft FrontPage or Adobe Dreamweaver. This tool allows learners to complete various tasks using other cognitive toolsboxes (e.g. Microsoft Word, PowerPoint, Access, Excel, and Publisher). With the focus of education increasingly being turned to differentiated instruction, teachers are using WebQuests more frequently. WebQuests also help to address the different learning styles of each students. The number of activities associated with a WebQuest can reach almost any student.\n\nWebQuests may be created by anyone; typically they are developed by educators. The first part of a WebQuest is the introduction. This describes the WebQuest and gives the purpose of the activity. The next part describes what students will do. Then is a list of what to do and how to do it. There is usually a list of links to follow to complete the activity.\n\nFinally, WebQuests do not have to be developed as a true web site. They may be developed and implemented using lower threshold (less demanding) technologies, (e.g. they may be saved as a word document on a local computer).\n\nMany Webquests are being developed by college students across the United States as a requirement for their k-12 planning e-portfolio.\n\nThe WebQuest methodology has been transferred to language learning in the 3D virtual world Second Life to create a more immersive and interactive experience.\n\nWebQuests are simple webpages, and they can be built with any software that allows you to create websites. Tech-savvy users can develop HTML in Notepad or Notepad++, while others will want to use the templates available in word processing suites like Microsoft Word and OpenOffice. More advanced web development software, like Dreamweaver and FrontPage, will give you the most control over the design of your webquest. Webquest templates allow educators to get a jump start on the development of WebQuest by providing a pre-designed format which generally can be easily edited. These templates are categorized as \"Framed\" or \"Unframed,\" and they can have a navigation bar at the top, bottom, left, or right of the content.\n\nThere are several websites that are specifically geared towards creating webquests. Questgarden, Zunal, and Teacherweb all allow teachers to create accounts, and these websites walk them through the process of creating a webquest. OpenWebQuest is a similar service, although it is based in Greece and much of the website is in Greek. These websites offer little control over design, but they make the creation process very simple and straightforward.\n\nAlternatively, teachers can use one of a number of free website services to create their own website and structure it as a webquest. Wordpress and Edublogs both allow users to create free blogs, and navigation menus can be created to string a series of pages into a webquest. This option offers a greater deal of flexibility than pre-made webquests, but it requires a little more technical know-how.\n\n\n"}
