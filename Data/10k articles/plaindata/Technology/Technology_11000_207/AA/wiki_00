{"id": "54158645", "url": "https://en.wikipedia.org/wiki?curid=54158645", "title": "3N170", "text": "3N170\n\nThe 3N170 is an enhancement mode N-Channel MOSFET standard product designed for use as a General Purpose amplifier or switch. The part was produced previously by Intersil and other companies. and is currently produced by Linear Integrated Systems, Inc.\n\nCharacteristics include:\n"}
{"id": "33020823", "url": "https://en.wikipedia.org/wiki?curid=33020823", "title": "56-bit encryption", "text": "56-bit encryption\n\nIn computing, 56-bit encryption refers to a key size of fifty-six bits, or seven bytes, for symmetric encryption. While stronger than 40-bit encryption, this still represents a relatively low level of security in the context of a brute force attack.\n\nThe US government traditionally regulated encryption for reasons of national security, law enforcement and foreign policy. Encryption was regulated from 1976 by the Arms Export Control Act until control was transferred to the Department of Commerce in 1996.\n\n56-bit refers to the size of a symmetric key used to encrypt data, with the number of unique possible permutations being formula_1 (72,057,594,037,927,936). 56-bit encryption has its roots in DES, which was the official standard of the US National Bureau of Standards from 1976, and later also the RC5 algorithm. US government regulations required any users of stronger 56-bit symmetric keys to submit to key recovery through algorithms like CDMF or key escrow, effectively reducing the key strength to 40-bit, and thereby allowing organisations such as the NSA to brute-force this encryption. Furthermore, from 1996 software products exported from the United States were not permitted to use stronger than 56-bit encryption, requiring different software editions for the US and export markets. In 1999, US allowed 56-bit encryption to be exported without key escrow or any other key recovery requirements.\n\nThe advent of commerce on the Internet and faster computers raised concerns about the security of electronic transactions initially with 40-bit, and subsequently also with 56-bit encryption. In February 1997, RSA Data Security ran a brute force competition with a $10,000 prize to demonstrate the weakness of 56-bit encryption; the contest was won four months later. In July 1998, a successful brute-force attack was demonstrated against 56-bit encryption with Deep Crack in just 56 hours.\n\nIn 2000, all restrictions on key length were lifted, except for exports to embargoed countries.\n\n56-bit DES encryption is now obsolete, having been replaced as a standard in 2002 by the 128-bit (and stronger) Advanced Encryption Standard. DES continues to be used as a symmetric cipher in combination with Kerberos because older products do not support newer ciphers like AES.\n\n"}
{"id": "315949", "url": "https://en.wikipedia.org/wiki?curid=315949", "title": "Airspeed indicator", "text": "Airspeed indicator\n\nThe airspeed indicator (ASI) or airspeed gauge is a flight instrument indicating the airspeed of an aircraft in miles per hour (MPH), knots, or both. The ASI measures the pressure differential between static pressure from the static port, and total pressure from the pitot tube. This difference in pressure is registered with the ASI pointer on the face of the instrument.\n\nThe ASI has standard color coded markings for the safe operation within the limitations of the aircraft. At a glance, the pilot can determine a recommended speed (V speeds) or if he needs to make any speed adjustments. Single and multi-engine aircraft have common markings. For instance, the green arc indicates the normal operating range of the aircraft, from \"V\" to \"V\". The white arc indicates the flap operating range, \"V\" to \"V\", used for approaches and landings. The yellow arc cautions that flight should be conducted in this range only in smooth air, while the red line (\"V\") at the top of the yellow arc indicates damage or structural failure may result at higher speeds.\n\nThe ASI in multi-engine aircraft include two additional radial markings, one red and one blue, associated with potential engine failure. The radial red line near the bottom of green arc indicates \"V\", the minimum indicated airspeed at which the aircraft can be controlled with the critical engine inoperative. The radial blue line indicates \"V\", the speed for best rate of climb with the critical engine inoperative.\n\nThe ASI is only flight instrument that uses both the static system and the pitot system. Static pressure enters the ASI case, while total pressure flexes the diaphragm, which is connected to the ASI pointer via mechanical linkage. The pressures are equal when the aircraft is stationary on the ground, and hence shows a reading of zero. When the aircraft is moving forward, air entering the pitot tube is at a greater pressure than the static line, which flexes the diaphragm, moving the pointer. The ASI is checked before takeoff for a zero reading, and during takeoff that it is increasing appropriately.\n\nThe pitot tube may become blocked, because of insects, dirt or failure to remove the pitot cover. Blockage will prevent ram air from entering the system. If the pitot opening is blocked, but the drain hole is open, the system pressure will drop to ambient pressure, and the ASI pointer will drop to a zero reading. If both the opening and drain hole are blocked, the ASI will not indicate any change in airspeed. However, the ASI pointer will show altitude changes, as the associated static pressure changes. If both the pitot tube and the static system are blocked, the ASI pointer will read zero. If the static ports are blocked but the pitot tube remains open, the ASI will operate, but\ninaccurately.\n\nIndicated airspeed (IAS}, is read directly off the ASI. It has no correction for air density variations, installation or instrument errors. Calibrated airspeed (CAS) is corrected for installation and instrument errors. An airspeed calibration chart is available for any remaining errors. True airspeed (TAS) is CAS corrected for altitude and nonstandard temperature. TAS is used for flight planning. TAS increases as altitude increases, as air density decreases. TAS may be determined via a flight computer, such as the E6B. Some ASIs have a TAS ring. Alternatively, a rule of thumb is to add 2 per cent to the CAS for each 1000 feet of altitude gained.\n\nJet aircraft do not have \"V\" and \"V\" like piston-engined aircraft, but instead have a maximum operating speed expressed in knots, \"V\" and Mach number, \"M\". Thus, a pilot of a jet airplane needs both an airspeed indicator and a Machmeter, with appropriate red lines. An ASI will include a red-and-white striped pointer, or \"barber's pole\", that automatically moves to indicate the applicable speed limit at any given time.\n\nAn airplane can stall at any speed, so monitoring the ASI alone will not prevent a stall. The critical angle of attack (AOA) determines when an aircraft will stall. For a particular configuration, it is a constant independent of weight, bank angle, temperature, density altitude, and the center of gravity of an aircraft. An AOA indicator provides stall situational awareness as a means for monitoring the onset of the critical AOA. The AOA indicator will show the current AOA and its proximity to the critical AOA.\n\nSimilarly, the Lift Reserve Indicator (LRI) provides a measure of the amount of lift being generated. It uses a pressure differential system to provide the pilot with a visual representation of reserve lift available.\n\n\nInstalling and flying the Lift Reserve Indicator, article and photos by Sam Buchanan http://home.hiwaay.net/~sbuc/journal/liftreserve.htm\n"}
{"id": "49509538", "url": "https://en.wikipedia.org/wiki?curid=49509538", "title": "AlphaSights", "text": "AlphaSights\n\nAlphaSights is an information services company, specializing in connecting clients with experts, sometimes referred to as an expert network. The company's clients include professionals operating in management & strategy consulting, investment management, private equity, corporate and professional services firms with interests in a range of markets including technology, industrials, consumer goods, telecommunications, utilities, financial services, healthcare, consumer services, basic materials, and oil and gas. AlphaSights is headquartered in London and has offices in New York, Hamburg, Dubai, Hong Kong, San Francisco, Shanghai, Seoul, and Tokyo.\n\nThe company has been mentioned or covered in various media outlets including \"The Sunday Times\", \"The Telegraph\", \"The Guardian\", \"Business Insider\", and The \"Mirror\".\n\nThe company was founded in 2008 and incorporated in 2009 in London by Max Cartellieri and Andrew Heath. They met at Stanford Business School in California, but each founded different companies before founding AlphaSights. Cartellieri co-founded Ciao AG, a leading European price comparison website, ultimately acquired by Microsoft (MSFT). Heath co-founded GoIndustry, an online marketplace that trades surplus industrial goods all over Europe, now part of GoIndustry DoveBid.\n\nAlphaSights opened its second office in New York City and third office in Hong Kong in 2011 before opening further offices in Dubai in 2013, San Francisco in 2015, Seoul, Hamburg and Shanghai in 2016. In 2013, the company was named the third fastest-growing company headquartered in the UK by \"The Sunday Times\" Fast Track 100 and has appeared on the list every year since, placing 11th in 2014, 21st in 2015, 41 in 2016 and 81 in 2017. In April 2017, AlphaSights was named as one of the fastest-growing and most disruptive companies headquartered in Europe by the \"Financial Times\".\n\nIn 2014, AlphaSights launched Knowledge for Good, its social impact business unit. Through Knowledge for Good, AlphaSights partners with social impact organizations including impact investors, social enterprises and nonprofit consultancies to provide its services at no or low cost. AlphaSights' partners include Acumen, Ashoka, TechnoServe, Nesta, and CDC Group.\n\n\n"}
{"id": "40816295", "url": "https://en.wikipedia.org/wiki?curid=40816295", "title": "Analysis &amp; Policy Observatory", "text": "Analysis &amp; Policy Observatory\n\nAnalysis & Policy Observatory (also known as APO) is a not-for-profit open access subject repository or digital library, specialising in public policy and practice grey literature and journal articles, mainly from Australia and New Zealand, with some coverage of other countries. Formerly known as Australian Policy Online, the organisation underwent a name change to Analysis & Policy Observatory (APO) in 2017.\n\nAPO was established in 2002 by the Swinburne Institute for Social Research, at Swinburne University in Melbourne. It was intended as a way to collate and disseminate academic research reports and other grey literature that was increasingly proliferating online. It has since established itself as a notable resource for people involved in policy research in Australia and New Zealand. It now sits as part of the Centre of Urban Transitions within Swinburne University of Technology.\n\nAPO is a not-for-profit organisation supported by partnerships with academic institutions and government agencies, grants donations and advertising revenue. Partners include the Australian National Data Service, ANZSOG, the University of Canberra, the Henry Halloran Trust at the University of Sydney, and the University of South Australia. APO has also been an awarded a number of Australian Research Council grants.\n\nAPO was established to address the transformation of the means of production for publications brought about by the internet. Frustrations with the limitations of the academic publishing system – long delays, lack of access or audience – were causing producers of academic and other content to move online and increasingly produce grey literature (informally published written material, such as reports, that may be difficult to trace via conventional channels). The informal channels used to disseminate digital grey literature have meant that libraries and other services have only been able to collect it in an ad hoc manner. APO aims to make digital research content that is relevant to Australian policy debates more easily discoverable, accessible and able to be managed.\n\nAPO specialises in collecting public policy reports and articles from academic research centres, think tanks, government and non-government organisations. As well as research, the site includes a smaller collection of opinion and commentary pieces, video, audio and web resources focused on policy issues. Most content is produced by other organisations and collected and catalogued on the site, although some commentary is original to APO.\n\nIn 2011, the then Secretary of the Department of the Prime Minister and Cabinet described APO as providing “some of the most valuable and constructive contributions to Australia’s policy debate”. However, APO has also been criticised on the grounds that it “could perhaps do more to engage with the formal political workings of government through providing access to formal government policy information.”\n\nIn 2012, the report of the Independent Inquiry into Media and Media Regulation by the Hon Raymond Antony Finkelstein QC stated that: \"While the start-up costs for new print publications have been prohibitive and inhibited new enterprises emerging, the streamlining of the relationship between content producers and consumers has led to many new websites and web-based services. Among the most important such websites that have grown up in Australia over the last decade are Inside Story, Australian Policy Online, Online Opinion, The Drum, The Conversation, and New Matilda.\"\n\nIn August 2014, APO won the \"Information\" category at the Australia & New Zealand Internet Awards (ANZIA).\n"}
{"id": "827230", "url": "https://en.wikipedia.org/wiki?curid=827230", "title": "Antique furniture", "text": "Antique furniture\n\nA piece of antique furniture is a collectible interior furnishing of considerable age. Often the age, rarity, condition, utility, or other unique features make a piece of furniture desirable as a collectors' item, and thus termed an antique. The antique furniture pieces reflect the style and features of the time they were made; this can be called the antique's \"period\" (Eduardian, Tudor, Colonial, etc.).\n\nAntique furniture may support the human body (such as seating or beds), provide storage, or hold objects on horizontal surfaces above the ground. Storage furniture (which often makes use of doors, drawers, and shelves) is used to hold or contain smaller objects such as clothes, tools, books, and household goods. Furniture can be a product of artistic design and is considered a form of decorative art. In addition to furniture's functional role, it can serve a symbolic or religious purpose. Domestic furniture works to create, in conjunction with furnishings such as clocks and lighting, comfortable and convenient interior spaces. Furniture can be made from many materials, including metal, plastic, and wood. Cabinetry and cabinet making are terms for the skill set used in the building of furniture.\n\nThe earliest furniture was simple and practical, but as furniture became crafted and decorated it became an early status symbol. Wealthy homeowners demanded that their furnishings reflect their status and lifestyles.\n\n\nantique furniture cabinet makers\n"}
{"id": "40081372", "url": "https://en.wikipedia.org/wiki?curid=40081372", "title": "Aquatic weed harvester", "text": "Aquatic weed harvester\n\nAn aquatic weed harvester, also known as a water mower, mowing boat and weed cutting boat, is an aquatic machine specifically designed for inland watercourse management to cut and harvest underwater weeds, reeds and other aquatic plant life. The action of removing aquatic plant life in such a manner has been referred to as \"aquatic harvesting\". \n\nWater is an important resource and in many countries, waterways are increasingly clogged by aquatic plant growth. This is particularly so in tropical countries where warmer water means the plants grow more quickly, and increasing run-off of fertilisers and effluent has exacerbated the problem. Irrigation ditches and pumps can become overgrown with vegetation, power station and factory water intakes can get blocked, boats can get hindered, fish stocks can be disrupted, and water moves more slowly, resulting in greater evapotranspiration and a greater risk of flooding. In some large irrigation projects in India, canals have become so overgrown with vegetation that water flow has been reduced to a fifth of its previous amount. In Bangladesh, floodwater has washed mats of water hyacinth onto paddy fields, overwhelming the emerging rice crops. Small fish can become entangled in excessive algal growth.\nRice is the main aquatic plant grown for human food, but smaller areas of watercress and water chestnut are also cultivated. In their native environments, aquatic weeds are part of a balanced ecosystem, and it is mainly introduced species of water plant that become invasive and cause problems by congesting water bodies. The worst culprits, found in both temperate and tropical waterways, are floating plants such as water hyacinth, water lettuce and \"Salvinia\", fully submerged rooting plants such as \"Hydrilla\" and water milfoil and rooting plants that reach the surface such as cattail, papyrus, bulrush and reed.\n\nWeed cutting boats are developed to enable the maintenance of canals, lakes and rivers and to remove excessive aquatic life such as algae and other plants that may negatively affect a waterway's ecology. Mechanical harvesters are large floating machines that have underwater cutting blades that sever the stems of underwater plants, gather the weeds and raise them on conveyor belts, storing the vegetation on board in a hold. Periodically this is discharged to a barge or an onshore facility. The harvested product can be composted, sent to a landfill site or used in land reclamation. In developing countries aquatic vegetation may be harvested by hand or by net from the shore, cut and harvested by boat and lifted ashore by hand, crane, pump or conveyor system. The harvested vegetation may be used for the feeding of livestock. To reduce the high moisture content and to make it easier to transport, the weed can be chopped and pressed. Other uses to which the harvested vegetation can be put include ensiling the material for livestock fodder, adding it to the soil as a bulky organic fertilizer, manufacturing the raw material into pulp, paper or fibre, and fermenting it to produce methane for energy production.\n\nMechanical harvesters can be effective at clearing aquatic weeds but the machines are expensive and the process may need to be repeated several times in a growing season. Small fragments of weed remain in the water and may spread to other locations thereby aiding in the dispersal of invasive species. Some areas may be too shallow for the mechanical harvester and it may be unable to access restricted locations. Submerged tree stumps can damage the machine. Aquatic weeds can also be utilized as a source of biofuel. An alternative to mechanical harvesting is the use of herbicides, which are easy to apply and less expensive, but may have unwanted impacts on the environment.\n\nIn June 2015 in Uganda, use of the aquatic weed harvester was recommended by the country's Ministry of Agriculture to reduce water hyacinth growth in Lake Victoria, which has caused a scarcity of fish in the lake. The scarcity of fish has negatively-affected the livelihood of locals who live in the lake's region. Additional types of machines were recommended to address the problem, which were the hydraulic harvester, take out elevator and dredger.\n\nIn August 2015 in Hyderabad, India, an aquatic weed harvester was used to remove algal weeds and trash from Hussainsagar Lake. An amphibious hydraulic excavator was also used. The work was performed to address pollution problems at the lake, which had accumulated significant algae and rubbish such as plastic bags and plastic waste, food wrappers, and various garbage, some of which was floating atop the lake.\n\nSome U.S. companies manufacture aquatic weed harvesters. One such machine manufactured by a U.S. company can cut around one-half an acre of weeds a day, and costs over $100,000.\n\nTampa, Florida used an aquatic weed harvester in 2013 to clear aquatic plant life from lakes in the area.\n\nIn 2012 in Leoni, Michigan, an aquatic weed harvester was used to remove algae from Center Lake. The algae appeared to have roots, and other techniques such as attempting to kill it with chemicals were unsuccessful. The machine used collects algae and weeds that live at the bottom of the lake, removing them from the waterway.\n\n\n"}
{"id": "26778765", "url": "https://en.wikipedia.org/wiki?curid=26778765", "title": "Backyard Farms", "text": "Backyard Farms\n\nBackyard Farms is a Madison, Maine-based agricultural company specializing in massive greenhouses to produce vine-ripened tomatoes. It had been owned by Devonshire Investors, a Boston-based branch of Fidelity Investments but was sold in June 2017 to a Canadian produce company, Mastronardi Produce of Ontario.\n\nThe company was started in 2004. In 2007, the company’s first greenhouse was built in Madison, Maine, making it the largest building by volume in the state. It covers the area of 20 football fields. The 240,000 plants grow up to 10 feet tall and are projected to yield 1 million tomatoes each week. Their second greenhouse, connected to the first, was built in 2009. The combined area was then . Plants are grown in rock wool and pollinated by bumblebees that are kept inside the greenhouses. Rainwater that sheds off the greenhouse roofs is recycled to supplement the water used to irrigate the plants.\n\nAlthough the tomatoes originally had been a specialty variety called \"Backyard Beauty\", they had been replaced with other, undisclosed varieties. There had been plans for the operation to expand to include peppers, eggplants, cucumbers, herbs, and strawberries.\n"}
{"id": "503961", "url": "https://en.wikipedia.org/wiki?curid=503961", "title": "Bloomberg Terminal", "text": "Bloomberg Terminal\n\nThe Bloomberg Terminal is a computer software system provided by the financial data vendor Bloomberg L.P. that enables professionals in the financial service sector and other industries to access the \"Bloomberg Professional\" service through which users can monitor and analyze real-time financial market data and place trades on the electronic trading platform. The system also provides news, price quotes, and messaging across its proprietary secure network. It is well-known among the financial community for its black interface, which is not optimized for user experience but has become a recognizable trait of the service.\n\nMost large financial firms have subscriptions to the Bloomberg Professional service. Many exchanges charge their own additional fees for access to real time price feeds across the terminal. The same applies to various news organizations.\n\nAll Bloomberg Terminals are leased in two-year cycles (in the late 1990s and early 2000s, three-year contracts were an option), with leases originally based on how many displays were connected to each terminal (this predated the move to Windows-based application). Most Bloomberg setups have between two and six displays. It is available for an annual fee of $20,000 per user ($25,080 per year for the small number of firms that use only one terminal). As of October 2016, there were 325,000 Bloomberg Terminal subscribers worldwide.\n\nSales from the Bloomberg terminal account for more than 85 percent of Bloomberg L.P.'s annual revenue. The financial data vendor's proprietary computer system starts at $22,500 per user per year.\n\nThe terminal implements a client-server architecture with the server running on a multiprocessor Unix platform. The client, used by end users to interact with the system, is a Windows application that typically connects directly through a router provided by Bloomberg and installed on-site. End users can also make use of an extra service (\"Bloomberg Anywhere\") to allow the Windows application to connect via internet/IP, or Web access via a Citrix client. There are also applications that allow mobile access via Android, BlackBerry, and iOS. The server side of the terminal was originally developed using mostly the programming languages Fortran and C. Recent years have seen a transition towards C++ and embedded JavaScript on the clients and servers.\n\nEach server machine runs multiple instances of the server process. Using a proprietary form of context-switching, the servers keep track of the state of each end user, allowing consecutive interactions from a single user to be handled by different server processes. The graphical user interface (GUI) code is also proprietary.\n\nMichael Bloomberg's 1997 autobiography contains a chapter entitled \"Computers for Virgins\", which explains the differences in the design of the terminal and its keyboard from the standard IBM PC keyboard layout that was popular at that time. The terminal's keyboard layout was designed for traders and market makers who had no prior computer experience. While the look and feel of the Bloomberg keyboard is very similar to the standard computer keyboard, there are several enhancements that help users navigate through the system, from the idea for a user-friendly system when originally designed in the early 1980s.\n\nKeyboard keys are commonly referred to inside angle brackets with full commands being contained in curly brackets  e.g., {VOD LN Equity <GO>}. The function keys names were replaced (from the technical name, e.g., F10) and the then standard beige color, opting for a memorable color and user-friendly name, \"Yellow\". The F10 key is thus a \"Yellow\" key named <Index>. The \"Esc\" is coloured red and named <CANCEL> in the Bloomberg system, with the red to catch one's eye to stop a task. The \"Enter\" key is referred to as <GO> with a green color, deriving from the \"Monopoly\" game board, by passing \"Go\" and collecting $200 in a hope that the user could make money on the information he would find.\n\nThe Bloomberg keyboard includes a unique <MENU> key which navigates back to the previous function used. If no previous commands are found, <MENU> displays a list of related functions. Similarly, the \"History\" key will populate the command-line with previously used functions in reverse chronological order, as the \"Up\" arrow key function does in certain command prompts.\n\nThe yellow hotkeys along the top of the keyboard are used to enter market sectors, and are generally used as suffixes to allow the terminal to correctly identify a security.\n\nFor example, if someone is interested in the Vodafone stock listed in the London market, one enters {VOD LN <Equity> <GO>} where VOD is the company's ticker symbol, LN is the venue code for London, and <Equity> is the market sector. A detailed option list related to Vodafone UK stock will pop up, the person can then choose different options by pressing related keys or using the mouse to select the option.\n\nSimilarly, {USDEUR <Curncy> <GO>} displays the U.S. dollar–Euro exchange spot rate.\n\nOther common Bloomberg commands for Equity include:\n\n\nThus, if someone interested in the Vodafone UK stock price, they can directly type in {VOD LN <Equity> HP <GO>}.\n\nThe Bloomberg keyboard has traditionally been heavier and sturdier than standard keyboards (a previous version, the SEA100 Bloomberg keyboard weighed around 3 kg) with 3mm key travel and 19mm key pitch; it also comes with built-in speakers for multimedia features. The SEA100 version has a built-in, 500 PPI, 0.26 sq inch biometric sensor for user login verification. The current Starboard (Keyboard 4) version is 1.08 kg and uses flatter, chiclet-style keys which are quieter and have less key travel than Freeboard (Keyboard 3) and prior.\nSelf-contained operating system running on custom hardware—commonly referred to as a \"Bloomberg Box\"—the Bloomberg Terminal now functions as an application within the Windows environment. From a user's perspective, there are essentially 3 distinct levels to the system:\n\n\"Core Terminal\" refers to the original Bloomberg system; typically consisting of four windows, or \"Panels\", each Panel contains a separate instance of the terminal command line. As the user enters tickers and functions, they can call up and display the real-time data of the market, with each different screen simultaneously running a program to analyze other tickers, functions, values and markets in real time. This use of multiple screens with user-demanded, specific pieces of differing data—across all relevant markets—allows the user to view diverse and countless volumes of information in real-time. Accessing market data, as it develops, allows the user to make trades and investments in all markets across the world, without having any lag in information. Users can run all four windows on a single monitor or spread them out amongst many monitors, maximizing the information shown on each, to effectually create up to four terminals.\n\nIn February 2012, Bloomberg LP publicly announced an upgrade to the Terminal called \"Bloomberg NEXT\". The stated goals of this multi-year, $100 million project were to improve the discoverability and usability of the Core Terminal's functionality, making it easier and more intuitive to use.\n\nLaunchpad is a customizable display consisting of a number of smaller windows, called \"components\", each dedicated to permanently displaying one set of data. A typical user would be a stockbroker who wishes to keep a list of 30 stocks visible at all times: Launchpad allows the user to create a small component which will show these prices constantly, saving the user from having to check each stock independently in the 4 terminal windows. To turn on Launchpad the command {BLP <GO>} is used, {PDFB<GO>} allows users to set Lpad to open automatically on login. Older keyboards had an <Lpad> key which replicated the {BLP<GO>} command. Other functions, such as email inboxes, calculation tools and news tickers can be similarly displayed. The Instant Bloomberg messaging/chat tool is a Launchpad component, as are the chat windows it creates. To launch a normal function from the Bloomberg Terminal's 4 Screens into launchpad type {LLP<GO>} from the target screen you wish to turn into a launchpad item.\n\nThe Bloomberg Open API (BLPAPI) application programming interface (API) allows third-party applications, such as Microsoft Excel, to access Bloomberg data via the Terminal and Bloomberg's market data products. A user might wish to use Bloomberg data from the Terminal to create their own calculations; by accessing streaming, historical, and reference market data from another program, they can build these formulae. The Bloomberg Terminal installation ships with Excel add-ins which facilitate building spreadsheets which consume market data. In addition, Bloomberg offers free BLPAPI SDKs allowing Bloomberg subscribers to build their own software which accesses market data in C, C++, Java, .NET, Perl, and Python, on Windows, Linux, macOS, and Solaris.\n\nThe largest competitor to the Bloomberg terminal is Thomson Reuters with its Reuters 3000 Xtra system, which was replaced by Eikon platform in 2010, with Bloomberg and Thomson Reuters splitting 30% each of the market share in 2011. This was a major improvement for Bloomberg as the share in 2007 was Bloomberg's 26% to Reuters' 36%.\n\nOther major competitors include LevelTradingField.com, Money.Net, the Infront Professional Terminal, SIX Financial Information, Morningstar Direct, Markit, Zacks Investment Research, Research Exchange, FactSet Research Systems, Capital IQ, PrivateRaise.com, Advantage Data Inc, Fidessa, Cogencis and Dow Jones. According to Burton-Taylor International Consulting, the market for financial data and analytics was worth almost $25 billion as of 2011.\n\n\n"}
{"id": "55409896", "url": "https://en.wikipedia.org/wiki?curid=55409896", "title": "Castner Medal", "text": "Castner Medal\n\nThe Castner Gold Medal on Industrial Electrochemistry is an biennial award given by the Electrochemical Technology Group of Society of Chemical Industry (SCI) to an authority on applied electrochemistry or electrochemical engineering connected to industrial research. The award is named in honor of Hamilton Castner, a pioneer in the field of industrial electrochemistry, who patented in 1892 the mercury cell for the chloralkali process. Castner was an early member of SCI.\n\nThe medal is presented in a public lecture, usually at the annual Electrochem conference, which is organised by the Royal Society of Chemistry (RSC) Electrochemistry Interest Group and the SCI Electrochemical Technology Group. When this is not possible, the medal presentation and lecture takes place at SCI's headquarters.\n\nThe medal's design was conceived by Humphrey Paget by commission of SCI. At least until 1958, the award was called Castner Gold Medal.\n\nTo the date, 23 academics and industrialists have received the award.\n\n\n\n"}
{"id": "886860", "url": "https://en.wikipedia.org/wiki?curid=886860", "title": "Company store", "text": "Company store\n\nA company store is a retail store selling a limited range of food, clothing and daily necessities to employees of a company. It is typical of a company town in a remote area where virtually everyone is employed by one firm, such as a coal mine. In a company town, the housing is owned by the company but there may be independent stores there or nearby.\n\nThe store typically accepts scrip or non-cash vouchers issued by the company in advance of weekly cash paychecks, and gives credit to employees before payday. Except in very remote areas, company stores became scarcer after the miners bought automobiles and could travel to a range of stores. Even so, the stores could survive because they provided convenience and easy credit.\n\nCompany stores have had a reputation as monopolistic institutions, funnelling workers' incomes back to the owners of the company. Company stores often faced little or no competition and prices were therefore uncompetitive. Allowing purchases on credit enforced a kind of debt slavery, obligating employees to remain with the company until the debt was cleared.\n\nRegarding this reputation, economic historian Price V. Fishback wrote that:\nThe company store is one of the most reviled and misunderstood of economic institutions. In song, folktale, and union rhetoric the company store was often cast as a villain, a collector of souls through perpetual debt peonage. Nicknames, like the \"pluck me\" and more obscene versions that cannot appear in a family newspaper, seem to point to exploitation. The attitudes carry over into the scholarly literature, which emphasizes that the company store was a monopoly.\n\nCompany stores existed elsewhere than the United States, in particular in the early 1900s in Mexico, textile workers at the largest cotton mill were paid in scrip. In 1907 workers attacked and looted the Río Blanco, Veracruz textile company's store. The workers were gunned down by the Mexican military, but in the aftermath of the violence, more retail outlets were opened in Rio Blanco.\n\nThe stores served numerous functions, such as a locus for the government post office, and as the cultural and community center where people could freely gather.\n\n\n"}
{"id": "1222615", "url": "https://en.wikipedia.org/wiki?curid=1222615", "title": "Dallas Semiconductor", "text": "Dallas Semiconductor\n\nDallas Semiconductor, acquired by Maxim Integrated Products in 2001, designed and manufactured analog, digital, and mixed-signal semiconductors (integrated circuits, or ICs). Its specialties included communications products (including T/E and Ethernet products), microcontrollers, battery management, thermal sensing and thermal management, non-volatile random-access memory, microprocessor supervisors, delay lines, silicon oscillators, digital potentiometers, real-time clocks, temperature-compensated crystal oscillators (TCXOs), iButton, and 1-Wire products. The Dallas, Texas-based company was founded in 1984 and purchased by Maxim Integrated Products in 2001. Both the Maxim and Dallas Semiconductor brands were actively used until 2007. Since then, the Maxim name has been used for all new products, though the Dallas Semiconductor brand has been retained for some older products, which can be identified by \"DS\" at the beginning of their part numbers.\n\n"}
{"id": "31322231", "url": "https://en.wikipedia.org/wiki?curid=31322231", "title": "Dynamic design analysis method", "text": "Dynamic design analysis method\n\nThe dynamic design analysis method (DDAM) is a US Navy-developed analytical procedure for evaluating the design of equipment subject to dynamic loading caused by underwater explosions (UNDEX). The analysis uses a form of shock spectrum analysis that estimates the dynamic response of a component to shock loading caused by the sudden movement of a naval vessel. The analytical process simulates the interaction between the shock-loaded component and its fixed structure, and it is a standard naval engineering procedure for shipboard structural dynamics.\n\nAll mission-essential equipment on board military surface ships and submarines must be qualified for underwater shock loads caused by depth charges, naval mines, missiles, and torpedoes. An underwater explosion nearby a ship or submarine can be devastating to the combat readiness of the vessel. Damage may occur in the form of dished hull plating or even more serious holing of the hull. Moreover, some damage may not be obvious and can occur as a result of shock-wave loading of equipment and systems aboard the vessel. Equipment damage may incapacitate a vessel. Much research effort has been expended in the study of underwater shock, especially during the period after World War II where it became obvious that navy vessels could be disabled by a non-contact underwater explosion. Thus a concerted effort was made to try to make shipboard equipment more resistant to shock. This was achieved through laboratory shock testing of equipment prior to its installation aboard vessels. With the advances in computer simulation and modeling capabilities, it is now possible to simulate a vessel's response to an underwater explosion and to identify potential problems or failures without extensive field testing. By using DDAM analytical techniques, money and time are saved.\n\nThe DDAM simulates the interaction between the shock-loaded component and its fixed structure as the free motion of a naval vessel in water produces a higher shock spectrum than a heavy structure would when mounted to a terrestrial surface. The DDAM takes interaction into account in relation to the mass of the equipment, its mounting location, and the orientation of the equipment on the vessel.\n\nEngineers use finite element method analysis software to verify designs using DDAM computer simulations that model the known characteristics of underwater explosion phenomena as well as the surface ship or submarine body responses to shock loading and application of a shock spectra in order to apply the appropriate shock responses at the mountings of shipboard equipment (e.g., masts, propulsion shafts, rudders, rudderstocks, bearings, exhaust uptakes and other critical structures) due to underwater explosions. The analytical process is described in \"NAVSEA 0908-LP-000-3010, Shock Design Criteria for Surface Ships\" which provides technical criteria for shock design calculations, and provides general background and educational material concerning application of the DDAM.\n\nA number of commercially available computer modeling and simulation programs are available to assist in this task. \nAfter the analyst performs a natural frequency analysis to determine the mode shapes and natural frequencies, the DDAM process then uses an input spectrum of shock design values (i.e., displacements or accelerations) based on data from a series of unclassified Naval Research Laboratory reports (primarily \"MR-1396, Design Values for Shock Design of Shipboard Equipment\" and \"FR-6267, Background for Mechanical Shock Design of Ships\"). Compliance standards for DDAM simulation and analysis software are maintained by the Naval Sea Systems Command (NAVSEA).\n\nThe Naval Sea Systems Command (NAVSEA) established a standardized format to describe the content and formats for publishing results of the DDAM analyses and technical reports. These templates are called Data Item Descriptions (DID); once these are specified or tailored for a specific contract, they become Contract Data Requirements List items (CDRLs) that represent the deliverable items of a contract. Exactly which data items are required for delivery depends on the nature of the project. The DIDs for DDAM activities are the Analysis Report, Dynamic Shock\", \"Mathematical Model Report, Dynamic Shock Analysis\", and \"Dynamic Shock Analysis Extension Request\".\n"}
{"id": "47190821", "url": "https://en.wikipedia.org/wiki?curid=47190821", "title": "EEG DIN connector", "text": "EEG DIN connector\n\nThe EEG DIN connector (also referred to as DIN 42802 or EEG safety DIN connector) is an electrical connector used to connect medical and biomedical recording systems, such as electrodes to electroencephalograph (EEG) as used in neurology. This type of connector is the \"de facto\" standard for electromedical instruments as it was adopted by the industry under pressure from regulatory bodies (including the FDA) to impose insulated connectors in clinical settings for safety reasons through the IEC 60601-1 (subclause 56.3(c)) norm.\n\nIn 1989, the Deutsches Institut für Normung has issued the 42802 standard which specified a \"touch-proof connector for electromedical application\".\n\nThe EEG DIN connector mainly exist in two types both featuring touch-proof sockets around in-line rigid plugs :\n\n"}
{"id": "1376411", "url": "https://en.wikipedia.org/wiki?curid=1376411", "title": "Electronic counter-countermeasure", "text": "Electronic counter-countermeasure\n\nElectronic counter-countermeasures (ECCM) is a part of electronic warfare which includes a variety of practices which attempt to reduce or eliminate the effect of electronic countermeasures (ECM) on electronic sensors aboard vehicles, ships and aircraft and weapons such as missiles. ECCM is also known as electronic protective measures (EPM), chiefly in Europe. In practice, EPM often means resistance to jamming.\n\nEver since electronics have been used in battle in an attempt to gain superiority over the enemy, effort has been spent on techniques to reduce the effectiveness of those electronics. More recently, sensors and weapons are being modified to deal with this threat. One of the most common types of ECM is radar jamming or spoofing. This originated with the Royal Air Force's use of what they codenamed \"window\" during World War II, which is now often referred to as \"chaff\". Jamming also may have originated with the British during World War II, when they began jamming German radio communications.\n\nIn perhaps the first example of ECCM, the Germans increased their radio transmitter power in an attempt to 'burn through' or override the British jamming, which by necessity of the jammer being airborne or further away produced weaker signals. This is still one of the primary methods of ECCM today. For example, modern airborne jammers are able to identify incoming radar signals from other aircraft and send them back with random delays and other modifications in an attempt to confuse the opponent's radar set, making the 'blip' jump around wildly and be impossible to range. More powerful airborne radars means that it is possible to 'burn through' the jamming at much greater ranges by overpowering the jamming energy with the actual radar returns. The Germans were not really able to overcome the chaff spoofing very successfully and had to work around it (by guiding the aircraft to the target area and then having them visually acquire the targets).\n\nToday, more powerful electronics with smarter software for operation of the radar might be able to better discriminate between a moving target like an aircraft and an almost stationary target like a chaff bundle.\n\nWith the technology going into modern sensors and seekers, it is inevitable that all successful systems have to have ECCM designed into them, lest they become useless on the battlefield. In fact, the 'electronic battlefield' is often used to refer to ECM, ECCM and ELINT activities, indicating that this has become a secondary battle in itself.\n\nThe following are some examples of EPM (other than simply increasing the fidelity of sensors through techniques such as increasing power or improving discrimination):\n\nSensor logic may be programmed to be able to recognize attempts at spoofing (e.g., aircraft dropping chaff during terminal homing phase) and ignore them. Even more sophisticated applications of ECCM might be to recognize the type of ECM being used, and be able to cancel out the signal.\n\nOne of the effects of the pulse compression technique, is boosting the apparent signal strength as perceived by the radar receiver. The outgoing radar pulses are chirped, that is, the frequency of the carrier is varied within the pulse, much like the sound of a cricket chirping. When the pulse reflects off a target and returns to the receiver, the signal is processed to add a delay as a function of the frequency. This has the effect of 'stacking' the pulse so it seems stronger, but shorter in duration, to further processors. The effect can increase the received signal strength to above that of noise jamming. Similarly, jamming pulses (used in deception jamming) will not typically have the same chirp, so will not benefit from the increase in signal strength.\n\nFrequency agility ('frequency hopping') may be used to rapidly switch the frequency of the transmitted energy, and receiving only that frequency during the receiving time window. This foils jammers which cannot detect this frequency switch quickly enough nor predict the next hop frequency, and switch their own jamming frequency accordingly during the receiving time window. The most advanced jamming techniques have a very wide and fast frequency range, and might possibly jam out also an antijammer.\n\nThis method is also useful against barrage jamming, in that it forces the jammer to spread its jamming power across multiple frequencies in the jammed system's frequency range, reducing its power in the actual frequency used by the equipment at any one time. The use of spread-spectrum techniques allow signals to be spread over a wide enough spectrum to make jamming of such a wideband signal difficult.\n\nRadar jamming can be effective from directions other than the direction the radar antenna is currently aimed. When jamming is strong enough, the radar receiver can detect it from a relatively low gain sidelobe. The radar, however, will process signals as if they were received in the main lobe. Therefore, jamming can be seen in directions other than where the jammer is located. To combat this, an omnidirectional antenna is used for a comparison signal. By comparing the signal strength as received by both the omnidirectional and the (directional) main antenna, signals can be identified that are not from the direction of interest. These signals are then ignored.\n\nPolarization can be used to filter out unwanted signals, such as jamming. If a jammer and receiver do not have the same polarization, the jamming signal will incur a loss that reduces its effectiveness. The four Basic polarizations are linear horizontal, linear vertical, right-hand circular, and left-hand circular. The signal loss inherent in a cross polarized (transmitter different from receiver) pair is 3 dB for dissimilar types, and 17 dB for opposites.\n\nAside from power loss to the jammer, radar receivers can also benefit from using two or more antennas of differing polarization and comparing the signals received on each. This effect can effectively eliminate all jamming of the wrong polarization, although enough jamming may still obscure the actual signal.\n\nThe other main aspect of ECCM, is to program sensors or seekers to detect attempts at ECM and possible even to take advantage of it. For example, some modern fire-and-forget missiles like the Vympel R-77 and the AMRAAM are able to home in directly on sources of radar jamming if the jamming is too powerful to allow them to find and track the target normally. This mode, called 'home-on-jam', actually makes the missile's job easier. Some missile seekers actually target the enemy's radiation sources, and are therefore called \"anti-radiation missiles\" (ARM). The jamming in this case effectively becomes a beacon announcing the presence and location of the transmitter. This makes the use of such ECM a difficult decision; it may serve to obscure an exact location from a non-ARM missile, but in doing so it must put the jamming vehicle at risk of being targeted and hit by ARMs.\n\n\n"}
{"id": "2804877", "url": "https://en.wikipedia.org/wiki?curid=2804877", "title": "Federation Against Software Theft", "text": "Federation Against Software Theft\n\nThe Federation Against Software Theft (FAST) is a not-for-profit organisation, formed in 1984 with the aim of eliminating copyright infringement of software in the UK. FAST was the world's first Anti-piracy organisation to work on protecting the copyrights of software publishers. Initially concentrating on lobbying parliament to revise Copyright law, FAST also prosecutes organisations and individuals for copyright infringement on behalf of its members and publicises the legal penalties and security risks.\n\nPrior to the agreement with FAST, Investors in Software were a not-for-profit organisation limited by guarantee with a mission to support and advance professionalism in Software Asset Management and related IT asset management, to enable individuals and organisations to improve effectiveness and efficiency. As a direct result of their work the ISO SAM ISO 19770 standard was successfully launched in May 2006.\n\nIn September 2008 FAST and Investors in Software signed an exclusive agreement to operate under the Federation name to strengthen and clarify the advice given to the end user community relating to best practice for Software Asset Management (SAM) and achieving cost efficient licence compliance. The new organisation will operate under the name FAST IiS.\n\nIn May 2008 FAST IiS launched an initiative with the aim of making it easier for end users to manage their software licensing requirements to contain costs and ensure effective \"Software Asset Management / software licence management\" encompassing compliance, the Software Industry Research Board (SIRB), was formed in consultation with five of the tier one software vendors.\n\nFAST IiS remains a not-for-profit organisation limited by guarantee and wholly owned by its members who include software publishers, resellers, distributors, SAM practitioners and law firms. FAST IiS works to champion the professional management of software and protect members’ rights.\n\nSince its inception FAST has successfully lobbied for changes in the software IP system in the UK and EU through its dealings with the UK Intellectual Property Office. FAST's lobbying efforts brought about amendments to the Copyright Act 1956, culminating in software being recognised as a literary work.\n\nIn 2007 FAST, with the Alliance Against IP Theft, was successful in pushing for the implementation of Section 107A Copyright, Designs and Patents Act 1988 and having IP crime as a measurement on the National Policing Safety Plan.\n\n"}
{"id": "20430521", "url": "https://en.wikipedia.org/wiki?curid=20430521", "title": "Flaming sword (effect)", "text": "Flaming sword (effect)\n\nA flaming sword is a sword that has been coated with some type of combustible fuel, with the fuel being set on fire. This is most widely done for entertainment purposes in circuses, magic performances and other forms of display as a side act of sword swallowers, fire eating, etc.\n\n"}
{"id": "15623882", "url": "https://en.wikipedia.org/wiki?curid=15623882", "title": "Four factor formula", "text": "Four factor formula\n\nThe four-factor formula, also known as Fermi's four factor formula is used in nuclear engineering to determine the multiplication of a nuclear chain reaction in an infinite medium.\nThe six factor formula defines each of these terms in much more detail.\n\nThe multiplication factor, , is defined as (see Nuclear chain reaction): \n\n\nIn an infinite medium, neutrons cannot leak out of the system and the multiplication factor becomes the infinite multiplication factor, formula_2, which is approximated by the four-factor formula.\n\n"}
{"id": "16275840", "url": "https://en.wikipedia.org/wiki?curid=16275840", "title": "G. N. Glasoe", "text": "G. N. Glasoe\n\nG. Norris Glasoe (29 October 1902 – May 1987) was an American nuclear physicist. He was a member of the Columbia University team which was the first in the United States to verify the European discovery of the nuclear fission of uranium via neutron bombardment. During World War II, he worked at the MIT Radiation Laboratory. He was a physicist and administrator at the Brookhaven National Laboratory.\n\nGynther Norris Glasoe was born in Northfield, MN to Dr. Paul M. Glasoe and Gena (Kirkwold) Glasoe, both children of Norwegian immigrants. He had two younger brothers: Paul K. Glasoe (1913–2008) and Alf M. Glasoe (1909–2006)\n\nGlasoe completed his undergraduate degree in 1924 at St. Olaf College in Northfield, Minnesota. He received his advanced degrees, including his doctorate, from the University of Wisconsin; a degree was awarded in 1926. After earning his doctorate, Glasoe did research at the University of Wisconsin and then joined the physics faculty at the Columbia University.\n\nJohn R. Dunning, professor of physics at Columbia, closely followed the work of Ernest Lawrence on the cyclotron. Dunning wanted a more powerful neutron source and the cyclotron appeared as an attractive tool to achieve this end. During 1935 and 1936, he was able construct a cyclotron using many salvaged parts to reduce costs and funding from industrial and private donations. Glasoe, Dana P. Mitchell, and Hugh Paxton, junior members of the physics faculty at Columbia, worked on the cyclotron part-time. At the suggestion of Mitchell, Dunning offered Herbert L. Anderson a teaching assistant position if he would also help with the design and building of the cyclotron during work on his doctorate in physics, which he did. Others assisting in the construction of the cyclotron were Eugene T. Booth and Hugh Glassford. The cyclotron would in a few years be used by Dunning, Glasoe, and Anderson in a historic experiment based on the discovery of nuclear fission in Europe in December 1938 and January 1939.\n\nIn December 1938, the German chemists Otto Hahn and Fritz Strassmann sent a manuscript to \"Naturwissenschaften\" reporting they had detected the element barium after bombarding uranium with neutrons; simultaneously, they communicated these results to Lise Meitner. Meitner, and her nephew Otto Robert Frisch, correctly interpreted these results as being nuclear fission. Frisch confirmed this experimentally on 13 January 1939. In 1944, Hahn received the Nobel Prize for Chemistry for the discovery of nuclear fission. Some historians have documented the history of the discovery of nuclear fission and believe Meitner should have been awarded the Nobel Prize with Hahn.\n\nEven before it was published, Meitner's and Frisch's interpretation of the work of Hahn and Strassmann crossed the Atlantic Ocean with Niels Bohr, who was to lecture at Princeton University. Isidor Isaac Rabi and Willis Lamb, two Columbia University physicists working at Princeton, heard the news and carried it back to Columbia. Rabi said he told Fermi; Fermi gave credit to Lamb. Bohr soon thereafter went from Princeton to Columbia to see Fermi. Not finding Fermi in his office, Bohr went down to the cyclotron area and found Anderson. Bohr grabbed him by the shoulder and said: \"Young man, let me explain to you about something new and exciting in physics.\" It was clear to a number of scientists at Columbia that they should try to detect the energy released in the nuclear fission of uranium from neutron bombardment. On 25 January 1939, Glasoe was a member of the experimental team at Columbia University which conducted the first nuclear fission experiment in the United States, which was done in the basement of Pupin Hall; the other members of the team were Herbert L. Anderson, Eugene T. Booth, John R. Dunning, Enrico Fermi, and Francis G. Slack.\n\nDuring World War II, Glasoe was a staff member and associate group leader at the Radiation Laboratory of the Massachusetts Institute of Technology.\n\nNo later than 1948, and as late as 1965, Glasoe was at the Brookhaven National Laboratory (BNL), Upton, Long Island, New York. He was associate chairman of the BNL physics department no later than 1952 and associate director of BNL no later than 1965.\n\nGlasoe received the Distinguished Alumnus Award from St. Olaf College in 1965.\n\n\n"}
{"id": "39206830", "url": "https://en.wikipedia.org/wiki?curid=39206830", "title": "Gentec International", "text": "Gentec International\n\nGentec International is a Canadian distributor of consumer accessories, headquartered in Markham, Ontario, Canada.\n\nFounded in 1990 by Joel Seigel and Margaret Adat, Gentec International began distributing photography accessories to the Canadian retail market. Gentec outgrew two facilities in eight years, finding their current home in Markham, Ontario, in 1998. Over the years, Gentec has continued to expand into new markets, including: electronics, mobile/wireless accessories, home audio products, sporting goods and car audio.\n\nGentec currently operates a state of-the-art, fully computerized 100,000 sq. ft. distribution centre and employs approximately 100 full-time staff.\n\nOn October 31, 2012, Alpine Electronics announced a partnership with Gentec International that would see Gentec take over sales, marketing and service of all Alpine Mobile Media Solutions for the Canadian market. The partnership went into effect January 1, 2013.\n\nOn January 1, 2013, Gentec also took over distribution for Vitec products in Canada, including Manfrotto, Gitzo, Kata Bags and National Geographic Bags.\n\nGentec International represents products for over sixty brands across six markets in the Canadian consumer accessories market. Those markets include Digital Imaging, Electronics, Mobile, Audio Products, Sporting Goods and Car Audio.\n\nAvenger, Blackrapid, DNP, Gary Fong, Gitzo, iON, Kata, Manfrotto, National Geographic, Optex, Roots, SanDisk, Sevenoak and Sigma.\n\nBell'O Furniture, Electrohome, Energy, iQ, Loctek, Midlite, NHL and Verso.\n\niDeal Case, iQ, iShieldz, NHL, Planet Wireless, Roots, Roots Tuff Skin and White Diamonds.\n\nEnergy Speakers, Jamo, Klipsch and Mirage.\n\nBoyt Hunting and Gun Cases, Bushnell, HotMocs, Puma, Safari and Zeiss.\n\nAlpine, Best Kits & Harnesses, Energy Speakers, Metra, PAC, Phoenix Gold, Sony, and Stinger.\n\nGentec has been a recipient of numerous awards in the Canadian marketplace.\n\nIn 1999, Gentec received the Markham Board of Trade's Business Excellence Award, presented to Joel Seigel and Margaret Adat by then-Mayor of Markham, Don Cousens.\n\nIn 2012, Gentec was named one of Canada's 50 Best Managed Companies for the sixth consecutive year by Deloitte & Touche's Best Managed Companies program.\n\n"}
{"id": "9244322", "url": "https://en.wikipedia.org/wiki?curid=9244322", "title": "Golden Corridor", "text": "Golden Corridor\n\nThe Golden Corridor is the area around the Jane Addams Memorial Tollway (Interstate 90), formerly known as the Northwest Tollway, in the Chicago metropolitan area. It is coined as such since the corridor generates a \"gold\" mine of economic profit for communities in the area. Several Fortune 500 company headquarters, office parks, industrial parks, exhibition and entertainment centers, medical facilities, hotels, shopping centers, and restaurants are located along the Golden Corridor. With the exception of the O'Hare area of Chicago, all of the communities located in this region are part of a larger region known as the \"Northwest Suburbs\".\n\nCities and villages located within the scope of the Golden Corridor include:\n\nSeveral important companies are headquartered in or have a significant presence in the corridor. They include:\n\nA variety of higher educational institutions are located along the Golden Corridor, ranging from branch locations to community colleges to four year colleges.\n\nRanging from regional indoor malls to chic lifestyle centers, \"the Golden Corridor\" is a bustling center of retail activity. Major shopping centers include:\n\nFrom major arenas to convention centers to theaters to gambling centers, the Golden Corridor is a thriving entertainment center. Major facilities include:\n\nLocated within five miles (8 km) of the Northwest Tollway are a variety of major hospitals, including:\n\nNearly every national hotel and restaurant chain can be found along the corridor, as well as numerous independently owned and local chains.\n\nThe highest concentration of hotels can be found in the Schaumburg/Arlington Heights and Rosemont/O'Hare areas, with secondary concentrations in Hoffman Estates, Elgin, and Itasca.\n\n\n\n\nLocal east-west highways that serve the region include Stearns Road, U.S. Route 20 (Lake Street), Illinois Route 19 (Irving Park Road), Thorndale Avenue, Biesterfield Road, Devon Avenue, Illinois Route 72 (Higgins Road and Touhy Avenue), Schaumburg Road, Illinois Route 58 (Golf Road), Illinois Route 62 (Algonquin Road), Kirchoff Road, Dempster Street, Oakton Street, Euclid Avenue, Central Avenue, U.S. Route 14 (Northwest Highway/Ronald Reagan Highway), U.S. Route 12 (Rand Road), Palatine Road, Illinois Route 68 (Dundee Road), Huntley Road, and Big Timber Road.\n\nLocal north-south highways that serve the region include Illinois Route 47, Randall Road, Illinois Route 31, Illinois Route 25 (Dundee Avenue), Beverly Road, Illinois Route 59 (Sutton Road), Bartlett Road, Barrington Road, Springinsguth Road, Ela Road, Roselle Road, Plum Grove Road, Quentin Road, Meacham Road, Arlington Heights Road, Illinois Route 83 (Elmhurst Road and Busse Road), York Road, Mount Prospect Road, Wolf Road, U.S. Route 45 (River Road and Lee Street/Mannheim Road), and Illinois Route 171 (Cumberland Avenue).\n\n"}
{"id": "25102062", "url": "https://en.wikipedia.org/wiki?curid=25102062", "title": "Green building on college campuses", "text": "Green building on college campuses\n\nGreen building on college campuses is the purposeful construction of buildings on college campuses that decreases resource usage in both the building process and also the future use of the building. The goal is to reduce emissions, energy use, and water use, while creating an atmosphere where students can be healthy and learn. Universities across the country are building to green standards set forth by the USGBC, United States Green Building Council. The USGBC is a non-profit organization that promotes sustainability in how buildings are designed and built. This organization created the Leadership in Energy and Environmental Design (LEED) rating system, which is a certification process that provides verification that a building is environmentally sustainable. In the United States, commercial and residential buildings account for 70 percent of the electricity use and over 38 percent of emissions. Because of these huge statistics regarding resource usage and emissions, the room for more efficient building practices is dramatic. Since college campuses are where the world’s future leaders are being taught, colleges are choosing to construct new buildings to green standards in order to promote environmental stewardship to their students. Colleges across the United States have taken leading roles in the construction of green building in order to reduce resource consumption, save money in the long run, and instill the importance on environmental sustainability on their students.It is a better way to motivate new generation to live a sustainable life.\n\nGreen buildings on college campuses provide benefits to the campus in several different ways. Campuses can benefit from the short and long term economic benefits. Initially, federal and state governments will sometimes provide tax incentives for buildings constructed that surpass the standards set by the government. There are also long term savings. According to the USGBC, with an upfront investment of 2% in green building design, the resulting life savings is 20% of the total construction costs. With many universities lacking funding, this kind of savings could dramatically help the yearly budget. Along with this increase in monetary savings, green building and architecture has been proven to make the occupants more productive. Studies have shown a link between improved lighting design and a 27% reduction in the incidence of headaches. Also, students with the most daylighting in their classrooms progressed 20% faster on math tests and 26% faster on reading tests in one year than those with less daylighting. Both of these studies show that better lighting conditions, which are one of the main features of green buildings, can increase the productivity of its occupants. Students at colleges where green buildings are being used will benefit by increasing their potential to gain knowledge. The last important benefit of green buildings on college campuses is having the university seen as environmentally sustainable. Students are becoming increasingly aware of the issues the Earth faces with carbon emissions and increased consumption. These students want to attend universities that are striving to reduce their environmental impact. Universities participating in sustainable initiatives, like constructing green buildings, will attract more highly qualified students. Green buildings on campuses benefit both the school as well as the students.\n\nMany institutions in the United States are administering the LEED (Leadership in Energy and Environmental Design) Green Building Rating System. The development of the LEED Rating System has been nationally recognized as the leading method to construct green buildings. The rating system incorporates the design, construction, and maintenance of the building. LEED promotes a cradle-to-cradle approach in regards to construction and design materials. The rating system is composed of six sections: Site Planning, Water Management, Energy Management, Material Use, Indoor Air Quality, and the Innovation & Design Process. Each section is composed of credits and points, which ultimately determine how \"green\" the building is constructed, designed, and maintained.\n\nLEED has four different levels of certification. All depending on how many credits and points were obtained through the LEED Rating System. There are 100 possible base points plus an additional 6 points for Innovation in Design and 4 points for Regional Priority.\n\nBuildings can qualify for 4 types of certification:\n\nThe USGBC has issued an application guide for administration of LEED Rating System on college, corporate, or government installations that include multiple buildings. This application is designed for projects where several buildings will be constructed at once, in phases, or a single building is constructed in a setting of existing buildings with common ownership. Note, however, that the AGMBC applies to LEED Rating System Versions 2.1 and 2.2. The methods described still apply to new construction on campuses.\n\nThe sustainable sites category is the most challenging category, and it is the most detailed section in the AGMBC.\n\n\nThese are 10 colleges all around the US determined to build for a sustainable future. Each college outlines their commitment in Campus Sustainability Initiatives and Mission statements. \n\nThe following methods are becoming more prevalent on campuses around the nation. Because of the large scale of college campuses, the impact of these methods are truly praise for energy savings and enhanced occupants' comfort.\n\nThe following are some examples of sustainable products used in green building. These materials are less harmful to the environment. Now-a-days many materials have a \"green\" substitute.\n\n\n\n\n\n\n\n\n\n\nUniversities have a leadership role in advancing knowledge, technology and tools to create a sustainable future. To fulfill this role effectively and with high credibility, they need to include a focus on sustainability also in their own operations and facilities. Campus projects, be they educational or corporate campus developments, present interesting sustainability challenges and opportunities. Firstly, their size is at the borderline between single building projects and small towns, a fruitful scale for innovative energy and transport solutions. And secondly, they are to a certain degree one-purpose neighborhoods focused on education, research, development or distribution of new ideas, products or services.\n\nPartners: Australian National University, Berkeley University, City of Zurich, Dundalk Institute of Technology, Swiss Federal Institute of Technology in Lausanne (EPFL), Swiss Federal Institute of Technology in Zurich (ETH Zurich), Harvard University, HEEPI, Hosei University, KTH Royal Institute of Technology, Los Angeles Community College District, National University of Singapore, Pontifical Catholic University of Peru, Stanford University, The Sustainability Forum, Tongji University, University of Applied Sciences of Trier-Birkenfeld, University of Copenhagen, University of Zurich – CCRS, University of Gothenburg, University of Luxembourg and Yale University.\n\nThe International Green Construction Code is a part of the International Code Council (ICC). As part of its commitment to green and sustainable safety concepts, the Code Council is excited to develop a new set of green codes under the multi-year initiative called \"IGCC: Safe and Sustainable by the Book.\" This initiative will include collaboration from the Council’s closest allies and pre-eminent thought leaders in green building, as well as outreach and feedback from our members and the general public. The International Green Construction Code is committed to developing an effective and efficient code that will continue our long tradition of international code guidance.\n\nThe World Green Building Council is an international organization that facilitates the green building councils of many developed and developing nations. The Council started in 1999 with its first meeting in California. Eight members attended the first meeting: U.S. Green Building Council, Green Building Council of Australia, Spain Green Building Council, United Kingdom Green Building Council, Japan Green Building Council, United Arab Emirates, Russia and Canada. THE WorldGBC incorporated in 2002 and operates from Toronto, Canada. There are currently over 15 established GBCs and 35 emerging and prospective countries with GBCs.\n\nStanford is a leading university in the green movement and the school is striving to achieve a LEED Platinum certification for their new graduate school of business, the Knight Management Center. The goal for this building is to open in the winter of 2011. The center will have eight buildings around three quadrangles with of interior space. According to the principal architect, Stan Boles of Boora Architects in Portland, Oregon, \"The orientation of the buildings is narrow in the north-south dimension. They are designed for optimum daylighting, ventilation, and for shading of one another. The exterior walls are designed so that areas of glass are created but shaded by exterior screens to prevent excessive heat gain.\" This project aims to:\nStanford’s president, John L. Hennessy, said, \"One of the biggest global challenges facing us today is the sustainable use of our planets natural resources. The Graduate School of Business will play a key role in helping us address these challenges by leading the way in its sustainable development of this new campus.\" Stanford University is taking an active role in constructing green buildings on their campus and the Knight Management Center will be a great example of how a building can be sustainable.\n\nThe Donald Bren School of Environmental Science & Management is located at the University of California, Santa Barbara, California. The academic laboratory and classroom facility demonstrates cost-effective, energy-efficient technologies and operations. The concrete and steel frame structure was complete in 2002 and cost approximately $27,500,000. Donald Bren Hall was the first laboratory to receive LEED Platinum accreditation, the highest rating achievable through the US Green Building Council’s national rating system, with the following building design features:\nAccording to Great Buildings, \"The Donald Bren School at the University of California, Santa Barbara takes advantage of a beautiful setting near the Pacific Ocean to become a green building that embraces its environment not only for efficiency, but for experience. With a striking open courtyard, it provides ample opportunity for social interaction that makes the transition between indoors and outdoors much smoother and ephemeral than most buildings. Building Bren Hall with sustainable materials and methods is estimated to have added only 2% to the building cost, which will easily be offset over time by energy savings.\"\n\nThe Education Center is located at the University of North Carolina at Chapel Hill. The building consists of three major sections connected by covered breezeways. The central wing welcomes visitors to the education center as they enter the garden through a large breezeway. The east wing offers classrooms for students enrolled in workshops and classrooms, and the west wing features the Reeves Auditorium. This large multipurpose space is used for lectures, conferences, and special events. The Education Center plans to achieve a LEED Platinum rating, most likely the first ever in North Carolina, with these features:\nThe new Education Center expresses a sense of place and celebrates relationships between humans and nature through the integration of indoor and outdoor spaces. Open breezeways, comfortable porches, natural light in every room, beautiful native plant landscaping, and educational exhibits inform, delight, and invite visitors to the Conservation Garden. Most of all, the building is a center of learning, teaching both the science and the enjoyment of plants and nature.\n\nThe University of Florida’s new football complex, the James W. Heavener Football Complex, was completed in 2008 and received LEED Platinum rating for the environmental sustainability of the building. The facility contractor was PPI Construction Management and the architect was RDG Planning and Design. The building includes offices, conference rooms, an atrium to display the football teams accomplishments, and a weight training facility. The LEED rated the complex 52 out of the 69 available points for the certification, which gave the building the Platinum rating. This facility is the first platinum athletic facility in the United States as well as the first platinum rated building in the state of Florida. The $28 million building exceeded the original goal of obtaining a LEED Silver rating.\n\nThis building has many features that helped it to achieve the Platinum level. The features dealing with water usage reduce the buildings indoor water use by 40 percent. Due to all of the facility’s energy saving features the building has exceeded the state and national energy requirements by 35 percent. Another interesting fact about the construction of this building is that most of the material used in the construction came from within 500 miles of the University of Florida, which reduce the emissions created form transporting the material. Also 78 percent of the building debris was recycled. The assistant director of LEED at UF, Bahar Armaghani, said, \"Green Buildings are not exclusively concerned with saving money through more efficient technology. They are also investments for the well-being of the people and environment.\" The University of Florida has taken on an initiative to have all new construction be LEED Gold certified or higher and with the construction of this facility the school has surpassed their own requirements by achieving the Platinum rating.\n\nKey Features of the Heavener Football Complex:\n\nHigh Point University, located in High Point, North Carolina, has a LEED-Certified building that houses the School of Education. The 31,000-square-foot building houses the education and psychology departments in technologically advanced classrooms, computer labs and offices. It features high-tech educational equipment, such as smart boards, a children’s book library, math and science touch screen games, a methods lab designed to look and feel like a real elementary school classroom, a Mac lab and psychology research booths. The School of Education building is setting an example for modern-day energy conservation with things like floor to ceiling windows for natural lighting and light sensors in the rooms.\n\nKey Statistics: \n\nThe Charles Hostler Student Center on the campus of the American University of Beirut provides a model for environmentally responsive design that meets the social needs of the campus and the larger region. Situated on Beirut's seafront and main public thoroughfare, the new . facility houses competitive and recreational athletic facilities for swimming, basketball, handball, volleyball, squash, exercise and weight training. The space also includes an auditorium with associated meeting rooms, cafeteria with study space, and underground parking for 200 cars.\n\nGreen Building methods:\n\nDubai International Academic City Phase-III (DIAC phase-III) comprises four academic buildings and a food court spread over a total built up area of . It has received the Silver LEED certification, and is expected to save approximately AED2.3 million per year from reduced energy costs, district cooling demand changes, irrigation water costs, sewage tanker and domestic water costs.\n\nGreen Building component Features:\nThese features will make this cluster 21.7% more energy efficient than the American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) 90.1 - 2004 standards. will also consume 30% less water than the standards set by U.S. Environmental Protection Agency (EPA) as well as 40% less irrigation water. These savings have been achieved by the installation of ultra-low flow water restrictors in wash basins and dual-flush tanks in wash rooms, as well as additives in the soil for the landscape areas.\n\n\n"}
{"id": "22001658", "url": "https://en.wikipedia.org/wiki?curid=22001658", "title": "Hancock air whistle", "text": "Hancock air whistle\n\nThe Hancock air whistle was a railroad whistle intended for use on North American diesel locomotives in place of the standard air horn.\n\nHancock was a familiar name to railroads during the days of steam. The company produced many appliances, one of which was the popular long-bell three-chime steam whistle. When railroads began dieselising, Hancock, along with other manufacturers of railroad equipment, adjusted their offerings in order to remain competitive. And so Hancock modified their whistle design so they could be used on diesel locomotives.\n\nHancock already had vast experience with whistles, and practically none with horns. Also, diesel locomotives were an emerging technology, and most early models were equipped with single-note 'honkers'. These horns were anything but appealing to the general public. Therefore, Hancock developed their line of air whistles in an attempt to romanticize the diesel locomotives. Production of these air whistles spanned from the late 1950s until the late 1960s.\n\nThe New York, New Haven and Hartford Railroad was the largest user of these whistles, and every FL9 purchased by the New Haven came from the factory equipped with a Hancock 4700 air whistle on the roof in the front, and an H4700 whistle (no reflector dish) on the rear. Additionally, the NH's 40 Budd RDCs had two Hancock 4700 whistles, plus their Mack FCD-1 & FCD-2 railbuses, 30 GP9s, 15 RS-11s, 15 H-16-44s, 20 SW1200s & their fleet of Pullman-Westinghouse stainless steel MUs. One DL-109 was also retrofitted with a pair. Other railroads occasionally used them, notably the New York Central on some RS-3s, the Seaboard Air Line on their SDP-35s, the Cambria & Indiana on its SW9s, and the Minneapolis, Northfield & Southern. Unfortunately, it was found that at high speeds, the whistles were difficult to hear. Class I railroads eventually replaced their Hancock whistles with horns, namely for safety reasons. Short line users were generally steel mill railroads, where it was found that the whistle was easier to hear than a horn within the mill.\n\nHancock offered three different models of their air whistle. The most common was the 4700, which consisted of the whistle along with a large, rectangular bowl in the same plane as the languid plate. This bowl, or reflector, is used to project the sound of the whistle ahead of the locomotive, instead of omnidirectional as in the case of most whistles. The second model was the 4700-2. This whistle is basically the same as the 4700 except that it has an electric heating element installed in the bowl to keep it from freezing. The final offering was the H4700, which did not include the reflector bowl. Aside from this difference, it is basically identical to the 4700.\n\nThe Hancock air whistles were all designed so that the bell of the whistle is horizontal when mounted on a standard horn mount. All the whistles are a single-bell chime, playing the notes E A C (A major triad) when blown on air. However, over time, many of the whistles got off-key and produced some variants. One common chord was E A C#.\n\n"}
{"id": "11505812", "url": "https://en.wikipedia.org/wiki?curid=11505812", "title": "History of optical storage media", "text": "History of optical storage media\n\nAlthough research into optical data storage has been ongoing for many decades, the first popular system was the Compact Disc, introduced in 1982, adapted from audio (CD-DA) to data storage (the CD-ROM format) with the 1985 \"Yellow Book\", and re-adapted as the first mass market optical storage medium with CD-R and CD-RW in 1988. Compact Disc is still the \"de facto\" standard for audio recordings, although its place for other multimedia recordings and optical data storage has largely been superseded by DVD.\n\nDVD (initially an initialism abbreviation of \"Digital Video Disc,\" then modified to \"Digital Versatile Disc,\" then officially just \"DVD\") was the mass-market successor to CD. DVD was rolled out in 1996, again initially for video and audio. DVD recordable formats developed some time later: DVD-R in late 1997 and DVD+R in 2002. Although DVD was initially intended to prevent a format war in fact one did arise between these two formats. It was resolved with both surviving however: DVD-R predominating for stand-alone DVD recorders and players, and (for computers) most DVD devices being engineered as dual format, to be compatible with both.\n\nWith the development of high-definition television, and the popularization of broadband and digital storage of movies, a further format development took place, again giving rise to two camps: HD DVD and Blu-ray Disc, based upon a switch from red to blue-violet laser and tighter engineering tolerances. After suffering a number of significant losses to Blu-ray, Toshiba announced their withdrawal from HD DVD on 19 February 2008.\n\n, future development beyond Blu-ray Disc appear to be based upon one or more of the following technologies, all in varying stages of development:\n"}
{"id": "53347752", "url": "https://en.wikipedia.org/wiki?curid=53347752", "title": "Home idle load", "text": "Home idle load\n\nHome idle load is the continuous residential electric energy consumption as measured by smart meters. It differs from standby power (loads) in that it includes energy consumption by devices that cycle on and off within the hourly period of standard smart meters (such as fridges, aquarium heaters, wine coolers, etc.). As such, home idle loads can be measured accurately by smart meters. According to Stanford Sustainable Systems Lab, home idle load constitutes an average of 32% of household electricity consumption in the U.S.\n\nThe primary categories of devices that contribute to Home Idle Load include:\n\nApproaches to reduce home idle loads include:\n\nHome idle load may be measured differently depending on the electric meter and local utility used. A smart meter with a local utility that supports “green button” data is the most accurate option to measure home idle load. Another option involves using the user’s utility website to access consumption charts showing hourly electric use. If green button data is not available, the user may measure home idle load by analysing the home’s electric meter while all home electronic devices are in innactive mode. This may involve using a timer to track the time for a single revolution of the spinning dial of an analog electric meter.\n"}
{"id": "291590", "url": "https://en.wikipedia.org/wiki?curid=291590", "title": "In-circuit emulation", "text": "In-circuit emulation\n\nIn-circuit emulation (ICE) is the use of a hardware device or in-circuit emulator used to debug the software of an embedded system. It operates by using a processor with the additional ability to support debugging operations, as well as to carry out the main function of the system. Particularly for older systems, with limited processors, this usually involved replacing the processor temporarily with a hardware emulator: a more powerful although more expensive version. It was historically in the form of bond-out processor which has many internal signals brought out for the purpose of debugging. These signals provide information about the state of the processor.\n\nMore recently the term also covers Joint Test Action Group (JTAG) based hardware debuggers which provide equivalent access using on-chip debugging hardware with standard production chips. Using standard chips instead of custom bond-out versions makes the technology ubiquitous and low cost, and eliminates most differences between the development and runtime environments. In this common case, the \"in-circuit emulator\" term is a misnomer, sometimes confusingly so, because emulation is no longer involved.\n\nEmbedded systems present special problems for programmers because they usually lack keyboards, monitors, disk drives and other user interfaces that are present on computers. These shortcomings make in-circuit software debugging tools essential for many common development tasks.\n\nAn in-circuit emulator (ICE) provides a window into the embedded system. The programmer uses the emulator to load programs into the embedded system, run them, step through them slowly, and view and change data used by the system's software.\n\nAn \"emulator\" gets its name because it emulates (imitates) the central processing unit (CPU) of the embedded system's computer. Traditionally it had a plug that inserts into the socket where the CPU integrated circuit chip would normally be placed. Most modern systems use the target system's CPU directly, with special JTAG-based debug access. Emulating the processor, or direct JTAG access to it, lets the ICE do anything that the processor can do, but under the control of a software developer.\n\nICEs attach a computer terminal or personal computer (PC) to the embedded system. The terminal or PC provides an interactive user interface for the programmer to investigate and control the embedded system. For example, it is routine to have a source code level debugger with a graphical windowing interface that communicates through a JTAG adapter (emulator) to an embedded target system which has no graphical user interface.\n\nNotably, when their program fails, most embedded systems simply become inert lumps of nonfunctioning electronics. Embedded systems often lack basic functions to detect signs of software failure, such as a memory management unit (MMU) to catch memory access errors. Without an ICE, the development of embedded systems can be extremely difficult, because there is usually no way to tell what went wrong. With an ICE, the programmer can usually test pieces of code, then isolate the fault to a particular section of code, and then inspect the failing code and rewrite it to solve the problem.\n\nIn usage, an ICE provides the programmer with execution breakpoints, memory display and monitoring, and input/output control. Beyond this, the ICE can be programmed to look for any range of matching criteria to pause at, in an attempt to identify the origin of a failure.\n\nMost modern microcontrollers use resources provided on the manufactured version of the microcontroller for device programming, emulating, and debugging features, instead of needing another special emulation-version (that is, bond-out) of the target microcontroller. Even though it is a cost-effective method, since the ICE unit only manages the emulation instead of actually emulating the target microcontroller, trade-offs must be made to keep prices low at manufacture time, yet provide enough emulation features for the (relatively few) emulation applications.\n\nVirtually all embedded systems have a hardware element and a software element, which are separate but tightly interdependent. The ICE allows the software element to be run and tested on the hardware on which it is to run, but still allows programmer conveniences to help isolate faulty code, such as \"source-level debugging\" (which shows a program as it was originally written) and \"single-stepping\" (which lets programmers run programs step-by-step to find errors).\n\nMost ICEs consist of an adaptor unit that sits between the ICE host computer and the system to be tested. A pin header and cable assembly connects the adaptor to a socket where the actual central processing unit (CPU) or microcontroller mounts within the embedded system. Recent ICEs enable programmers to access the on-chip debug circuit that is integrated into the CPU via JTAG or background debug mode interface (BDM) to debug the software of an embedded system. These systems often use a standard version of the CPU chip, and can simply attach to a debug port on a production system. They are sometimes called in-circuit debuggers or ICDs, to distinguish the fact that they do not replicate the functionality of the CPU, but instead control an already existing, standard CPU. Since the CPU need not be replaced, they can operate on production units where the CPU is soldered in and cannot be replaced. On x86 Pentiums, a special 'probe mode' is used by ICEs to aid in debugging.\n\nIn the context of embedded systems, the ICE is not emulating hardware. Rather, it is providing direct debug access to the actual CPU. The system under test is under full control, allowing the developer to load, debug and test code directly.\n\nMost host systems are ordinary commercial computers unrelated to the CPU used for development. For example, a Linux PC might be used to develop software for a system using a Freescale 68HC11 chip, a processor that cannot run Linux.\n\nThe programmer usually edits and compiles the embedded system's code on the host system, as well. The host system will have special compilers that produce executable code for the embedded system, termed cross compilers or cross assemblers.\n\n\"On-chip debugging\" is an alternative to in-circuit emulation. It uses a different approach to address a similar goal.\n\nOn-chip debugging, often loosely termed as \"Joint Test Action Group\" (JTAG), uses the provision of an additional debugging interface to the live hardware, in the production system. It provides the same features as in-circuit debugging, such as inspection of internal state or variables and may have the ability to set checkpoints, breakpoints and watchpoints. The difference is that this is provided by additional silicon within the production processor, rather than swapping the processor for an off-board debugging emulator. For this reason, some of the functions of the ICE is changed by the specifications of the processor. An additional JTAG interface is added to the controller board, and this is required on every production system, but as this only requires a few signal pins the extra cost is minimal. JTAG interfaces were originally developed for, and are still useful for, end of production testing.\n\n\n"}
{"id": "55063733", "url": "https://en.wikipedia.org/wiki?curid=55063733", "title": "Insilico Medicine", "text": "Insilico Medicine\n\nInsilico Medicine is an American biotechnology company based in Rockville in Johns Hopkins University’s Emerging Technology Centers. The company combines genomics, big data analysis, and deep learning for the in silico drug discovery.\n\nCEO Alex Zhavoronkov founded Insilico Medicine in 2014, as an alternative to animal testing for research and development programs in the pharmaceutical industry. By using artificial intelligence and deep-learning techniques, Insilico is able to analyze how a compound will affect cells and what drugs can be used to treat the cells in addition to possible side effects. Through its \"Pharma.AI\" division, the company provides machine learning services to different pharmaceutical, biotechnology, and skin care companies. Insilico is known for hiring mainly through hackathons such as their own molhack online hackathon. \n\nThe company has multiple collaborations in the applications of next-generation artificial intelligence technologies such as the generative adversarial networks and reinforcement learning to the generation of novel molecular structures with desired properties. In conjunction with Alan Aspuru-Guzik's group at Harvard, they have published an improved GAN architecture for molecular generation which combines GANs, reinforcement learning, and a differentiable neural computer.\n\nInsilico's recent paper in Nature Communications describing the iPANDA dimensionality reduction algorithm included collaborators from 11 institutions. Some of the publications go beyond biotechnology and focus on organizational diversity and anti-racism. Insilico has also partnered with GSK, BioTime, Juvenescence, and quantum computer company YMK Photonics.\n\nIn 2017, Insilico was named one of the Top 5 AI companies by NVIDIA for its potential for social impact. Insilico also has R&D resources in Belgium, Russia, and the UK and hires talent through “hackathons” and other local competitions. By 2017, InSilico had raised $8.26 million in funding from investors including Deep Knowledge Ventures, JHU A-Level Capital, Jim Mellon, and Juvenescence. Investors announced in 2018 include WuXi AppTec, a Chinese integrated R&D services platform, Peter Diamandis’ BOLD Capital, and Pavilion Capital.\n"}
{"id": "4791387", "url": "https://en.wikipedia.org/wiki?curid=4791387", "title": "Interconnect bottleneck", "text": "Interconnect bottleneck\n\nThe interconnect bottleneck comprises limits on integrated circuit (IC) performance due to connections between components instead of their internal speed.\nIn 2006 it was predicted to be a \"looming crisis\" by 2010.\n\nImproved performance of computer systems has been achieved, in large part, by downscaling the IC minimum feature size. This allows the basic IC building block, the transistor, to operate at a higher frequency, performing more computations per second. However, downscaling of the minimum feature size also results in tighter packing of the wires on a microprocessor, which increases parasitic capacitance and signal propagation delay. Consequently, the delay due to the communication between the parts of a chip becomes comparable to the computation delay itself. This phenomenon, known as an “interconnect bottleneck”, is becoming a major problem in high-performance computer systems.\n\nThis interconnect bottleneck can be solved by utilizing optical interconnects to replace the long metallic interconnects. Such hybrid optical/electronic interconnects promises better performance even with larger designs. Optics has widespread use in long-distance communications; still it has not yet been widely used in chip-to-chip or on-chip interconnections because they (in centimeter or micrometer range) are not yet industry-manufacturable owing to costlier technology and lack of fully mature technologies. As optical interconnections move from computer network applications to chip level interconnections, new requirements for high connection density and alignment reliability have become as critical for the effective utilization of these links. There are still many materials, fabrication, and packaging challenges in integrating optic and electronic technologies.\n\n"}
{"id": "48801715", "url": "https://en.wikipedia.org/wiki?curid=48801715", "title": "Jet ski fishing", "text": "Jet ski fishing\n\nJet Ski Fishing is the practice of rigging a personal watercraft with fishing rods and accessories in order to travel to an area for fishing.\n\nJet ski fishing is one of the fastest growing categories in the personal watercraft and fishing industries. There are four types of fishing rigs for jet ski fishing:\n\nPontoons(manufactured from fibre glass) that is attached to any jetski or PWC,adding stability you jetski as it's always in the water.There is ample storage space to organize your jetski.\nJetwings has live well, luna tubes,bait boards,trolling rod holders,storage rod holders .It has built in buoyancy witch make it impossible for the jetski to sink\n\n\nThis sport is said to have originated in South Africa and has become popular in Australia and New Zealand, and in 2015 the sport has started to get traction in North America. A personal watercraft can support two fishermen on the water. There are several methods of fishing from a PWC: trolling, casting and drift jigging, all of which are productive and are used regularly. When trolling there are two positions where the rods can be installed, off the aft and off the steering column.\n\nThere are several obstacles to overcome when taking a stock PWC and converting it to a fishing platform. Firstly the fisherman needs to have a place to store and mount fishing gear such as rods and reels, nets and gaffs. All three methods above address that problem. Storage space is required, and the most popular method is to use saddle bags or a catch bag and a console bag, giving extra storage area for touring and fishing. In addition there are three compartments on a Jet Ski, the front compartment, the console hatch and there is generally a bucket under the rear seat. The final hurdle is where to store the catch, either in a catch bag (the easiest and most economical method) or a more costly cooler and basket.\n\nPersonal watercrafts are regularly used for touring, they are used by groups of people to travel from port to port. Many of these machines are being used for overnight camping in combination with fishing.\n\nThe majority of jet ski fishing is done off three or four seat PWCs, the larger area allowing for the movement required to handle a fish whether there are one or two fishermen on board.\n"}
{"id": "20213382", "url": "https://en.wikipedia.org/wiki?curid=20213382", "title": "Journal of Computer-Mediated Communication", "text": "Journal of Computer-Mediated Communication\n\nThe Journal of Computer-Mediated Communication JCMC is a quarterly peer-reviewed academic journal that covers the interdisciplinary field of computer-mediated communication. It was established in 1994 and is published by Oxford University Press on behalf of the International Communication Association. According to the \"Journal Citation Reports\", its 2017 5 year impact factor was 5.629, placing it as one of the highest ranked communication journals. \n\nThe following persons have been editor-in-chief of the journal:\n\n"}
{"id": "652262", "url": "https://en.wikipedia.org/wiki?curid=652262", "title": "Keck Graduate Institute", "text": "Keck Graduate Institute\n\nKeck Graduate Institute (KGI) is a private graduate school in Claremont, California. Founded by Henry Riggs in 1997, it is the seventh and newest member of the Claremont Consortium.\n\nHenry Riggs, then president of Harvey Mudd College, established the institute in 1997 to address what he perceived as a lack of scientists trained to convert new scientific discoveries into practical uses. He also became the institute's first president, serving until 2003.\n\nThe decision to establish Keck Graduate Institute as a seventh Claremont College met with some opposition, particularly from faculty of the other Claremont Colleges who objected to its lack of tenure, and environmentalists who opposed its plans to build a campus next to the Bernard Field Station, an area of undeveloped scrubland. The environmental issue was largely settled when KGI decided to establish its campus at a different location, and other opposition gradually faded.\n\nThe institute received a $50 million endowment from the W. M. Keck Foundation, after which it was named. It awarded its first Master of Bioscience degree in 2002.\n\nIn 2003, Sheldon Schuster became the second president in the institute's history. He took over from Riggs, who became chairman of the school's board of trustees. Schuster is a biochemist who previously served as director of the University of Florida's biotechnology research program.\n\nKeck Graduate Institute of Applied Life Sciences is accredited by the Accrediting Commission for Senior Colleges and Universities of the Western Association of Schools and Colleges. The School of Pharmacy and Health Sciences is accredited by the Accreditation Council for Pharmacy Education (ACPE).\n\nAcademic programs at KGI are organized into two schools, the School of Applied Life Sciences and the School of Pharmacy and Health Sciences. In addition, KGI offers a four-year undergraduate program, the Minerva Schools at KGI, in partnership with the Minerva Project.\n\nKGI maintains four research centers: the Center for Rare Disease Therapies, the Center for Biomarker Research, the Science Heritage Center and the Amgen Bioprocessing Center. The Amgen Bioprocessing Center was funded by a 2004 grant of $2 million to KGI from Amgen, a pharmaceutical company based in Thousand Oaks, California. On December 28, 2016, KGI announced a plan to start a Master of Science in Human Genetics and Genetic Counseling program funded by an additional $1.5 million grant from Amgen.\n\nIonian Technologies was founded in 2000, and was the first spin-off company to commercialize technology developed at KGI. Ionian focuses on molecular diagnostics for emerging and infectious diseases, and in 2004 was awarded a contract to develop a handheld biothreat detector using isothermal amplification of DNA. Other KGI startups include Zuyder Pharmaceuticals and Claremont BioSolutions.\n\n\n"}
{"id": "17933", "url": "https://en.wikipedia.org/wiki?curid=17933", "title": "Latency (engineering)", "text": "Latency (engineering)\n\nLatency is a time interval between the stimulation and response, or, from a more general point of view, a time delay between the cause and the effect of some physical change in the system being observed. Latency is physically a consequence of the limited velocity with which any physical interaction can propagate. The magnitude of this velocity is always less than or equal to the speed of light. Therefore, every physical system will experience some sort of latency, regardless of the nature of stimulation that it has been exposed to.\n\nThe precise definition of latency depends on the system being observed and the nature of stimulation. In communications, the lower limit of latency is determined by the medium being used for communications. In reliable two-way communication systems, latency limits the maximum rate that information can be transmitted, as there is often a limit on the amount of information that is \"in-flight\" at any one moment. In the field of human–machine interaction, perceptible latency has a strong effect on user satisfaction and usability.\n\nOnline games are sensitive to latency since fast response times to new events occurring during a game session are rewarded while slow response times may carry penalties. Lag is the term used to describe latency in gaming. Due to a delay in transmission of game events, a player with a high latency internet connection may show slow responses in spite of appropriate reaction time. This gives players with low latency connections a technical advantage.\n\nMinimizing latency is of interest in the capital markets, particularly where algorithmic trading is used to process market updates and turn around orders within milliseconds. Low-latency trading occurs on the networks used by financial institutions to connect to stock exchanges and electronic communication networks (ECNs) to execute financial transactions. Joel Hasbrouck and Gideon Saar (2011) measure latency based on three components: the time it takes for information to reach the trader, execution of the trader’s algorithms to analyze the information and decide a course of action, and the generated action to reach the exchange and get implemented. Hasbrouck and Saar contrast this with the way in which latencies are measured by many trading venues who use much more narrow definitions, such as, the processing delay measured from the entry of the order (at the vendor’s computer) to the transmission of an acknowledgement (from the vendor’s computer). Electronic trading now makes up 60% to 70% of the daily volume on the NYSE and algorithmic trading close to 35%. Trading using computers has developed to the point where millisecond improvements in network speeds offer a competitive advantage for financial institutions.\n\nNetwork latency in a packet-switched network is measured as either one-way (the time from the source sending a packet to the destination receiving it), or round-trip delay time (the one-way latency from source to destination plus the one-way latency from the destination back to the source). Round-trip latency is more often quoted, because it can be measured from a single point. Note that round trip latency excludes the amount of time that a destination system spends processing the packet. Many software platforms provide a service called ping that can be used to measure round-trip latency. Ping uses the Internet Control Message Protocol (ICMP) \"echo request\" which causes the recipient to send the received packet as an immediate response, thus it provides a rough way of measuring round-trip delay time. Ping cannot perform accurate measurements, principally because ICMP is intended only for diagnostic or control purposes, and differs from real communication protocols such as TCP. Furthermore, routers and internet service providers might apply different traffic shaping policies to different protocols. For more accurate measurements it is better to use specific software, for example: hping, Netperf or Iperf.\n\nHowever, in a non-trivial network, a typical packet will be forwarded over multiple links and gateways, each of which will not begin to forward the packet until it has been completely received. In such a network, the minimal latency is the sum of the transmission delay of each link, plus the forwarding latency of each gateway. In practice, minimal latency also includes queuing and processing delays. Queuing delay occurs when a gateway receives multiple packets from different sources heading towards the same destination. Since typically only one packet can be transmitted at a time, some of the packets must queue for transmission, incurring additional delay. Processing delays are incurred while a gateway determines what to do with a newly received packet. Bufferbloat can also cause increased latency that is an order of magnitude or more. The combination of propagation, serialization, queuing, and processing delays often produces a complex and variable network latency profile.\n\nLatency limits total throughput in reliable two-way communication systems as described by the bandwidth-delay product.\n\nLatency in fiber optics is largely a function of the speed of light, which is 299,792,458 meters/second in vacuum. This would equate to a latency of 3.33 µs for every kilometer of path length. The index of refraction of most fibre optic cables is about 1.5, meaning that light travels about 1.5 times as fast in a vacuum as it does in the cable. This works out to about 5.0 µs of latency for every kilometer. In shorter metro networks, higher latency can be experienced due to extra distance in building risers and cross-connects. To calculate latency of a connection, one has to know the distance traveled by the fibre, which is rarely a straight line, since it has to traverse geographic contours and obstacles, such as roads and railway tracks, as well as other rights-of-way. \n\nDue to imperfections in the fibre, light degrades as it is transmitted through it. For distances of greater than 100 kilometers, amplifiers or regenerators are deployed. Latency introduced by these components needs to be taken into account.\n\nThis is illustrated when a news presenter in a studio talks with a reporter in a distant place via a geostationary communication satellite. The signal travels from the newsreader up to and down from the satellite to the reporter and then the reply from the reporter does the same, resulting in four satellite hops incurring a latency on the order of a half second, not including the typically smaller latencies inside the communications equipment.\n\nLow Earth orbit is sometimes used to cut this delay, at the expense of more complicated satellite tracking on the ground and requiring more satellites in the satellite constellation to ensure continuous coverage.\n\nAudio latency is the delay between when an audio signal enters and when it emerges from a system. Potential contributors to latency in an audio system include analog-to-digital conversion, buffering, digital signal processing, transmission time, digital-to-analog conversion and the speed of sound in air.\n\nAny individual workflow within a system of workflows can be subject to some type of operational latency. It may even be the case that an individual system may have more than one type of latency, depending on the type of participant or goal-seeking behavior. This is best illustrated by the following two examples involving air travel.\n\nFrom the point of view of a passenger, latency can be described as follows. Suppose John Doe flies from London to New York. The latency of his trip is the time it takes him to go from his house in England to the hotel he is staying at in New York. This is independent of the throughput of the London-New York air link – whether there were 100 passengers a day making the trip or 10000, the latency of the trip would remain the same.\n\nFrom the point of view of flight operations personnel, latency can be entirely different. Consider the staff at the London and New York airports. Only a limited number of planes are able to make the transatlantic journey, so when one lands they must prepare it for the return trip as quickly as possible. It might take, for example:\n\nAssuming the above are done one after another, minimum plane turnaround time is:\n\nHowever, cleaning, refueling and loading the cargo can be done at the same time. Passengers can be loaded after cleaning is complete. The reduced latency, then, is:\n\nThe people involved in the turnaround are interested only in the time it takes for their individual tasks. When all of the tasks are done at the same time, however, it is possible to reduce the latency to the length of the longest task. If some steps have prerequisites, it becomes more difficult to perform all steps in parallel. In the example above, the requirement to clean the plane before loading passengers results in a minimum latency longer than any single task.\n\nAny mechanical process encounters limitations modeled by Newtonian physics. The behavior of disk drives provides an example of mechanical latency. Here, it is the time needed for the data encoded on a platter to rotate from its current position to a position adjacent to the read-write head as well as the seek time required for the actuator arm for the read-write head to be positioned above the appropriate track. This is also known as rotational latency and seek time since the basic term latency is also applied to the time required by a computer's electronics and software to perform polling, interrupts, and direct memory access.\n\nComputers run sets of instructions called a process. In operating systems, the execution of the process can be postponed if other processes are also executing. In addition, the operating system can schedule when to perform the action that the process is commanding. For example, suppose a process commands that a computer card's voltage output be set high-low-high-low and so on at a rate of 1000 Hz. The operating system may choose to adjust the scheduling of each transition (high-low or low-high) based on an internal clock. The latency is the delay between the process instruction commanding the transition and the hardware actually transitioning the voltage from high to low or low to high.\n\nOn Microsoft Windows, it appears that the timing of commands to hardware is not exact. Empirical data suggest that Windows (using the Windows sleep timer which accepts millisecond sleep times) will schedule on a 1024 Hz clock and will delay 24 of 1024 transitions per second to make an average of 1000 Hz for the update rate. This can have serious ramifications for discrete-time algorithms that rely on fairly consistent timing between updates such as those found in control theory. The sleep function or similar windows API were at no point designed for accurate timing purposes. Certain multimedia-oriented API routines like codice_1 and its siblings provide better timing consistency. However, consumer- and server-grade Windows ( those based on NT kernel) were not to be real-time operating systems. Drastically more accurate timings could be achieved by using dedicated hardware extensions and control-loop cards.\n\nLinux may have the same problems with scheduling of hardware I/O. The problem in Linux is mitigated by support for posix real-time extensions, and the possibility of using a kernel with the PREEMPT_RT patch applied.\n\nOn embedded systems, the real-time execution of instructions is often supported by the low-level embedded operating system.\n\nIn simulation applications, 'latency' refers to the time delay, normally measured in milliseconds (1/1,000 sec), between initial input and an output clearly discernible to the simulator trainee or simulator subject. Latency is sometimes also called transport delay.\n\n\n\n\n"}
{"id": "948716", "url": "https://en.wikipedia.org/wiki?curid=948716", "title": "List of car-free places", "text": "List of car-free places\n\nThe areas in this list of car-free places make up a sizeable fraction of a city, town, or island; public transport connections do not in themselves constitute a car free area.\n\nColor-coding is used as follows:\nIn Bulgaria almost every city or town has pedestrians-only zone, usualy in downtown.\nFrench cities planning to ban diesel and petrol vehicles: Paris (diesel by 2024 and petrol by 2030).\n\nThe several Prince's Islands near Istanbul do not permit cars (aside from public service vehicles such as police, garbage, electricity etc.) Four of these islands can be reached by ferry from Istanbul, which takes about an hour.\n\n\"NB\": Much of Canada's near north (the subarctic or boreal forest) and virtually all of the area north of the tree line is trackless, containing no roads. However, there are few large settlements in this area. In the towns and villages which do exist, motorized vehicles, in the form of ATVs and snowmobiles are commonplace. There are also isolated fishing villages on the North Atlantic coast called Newfoundland outports which are mostly car-free.\n\nThis is a list of populated car-free areas. For uninhabited car-free area in the US, see inventoried roadless area.\nWith a few notable exceptions, most US cities have only relatively small car free zones.\n\n"}
{"id": "25433713", "url": "https://en.wikipedia.org/wiki?curid=25433713", "title": "MSC Malaysia", "text": "MSC Malaysia\n\nMSC Malaysia, formerly the Multimedia Super Corridor and known as the MSC in Malaysia, is a Special Economic Zone and high-technology business district in central-southern Selangor, Malaysia.\n\nThe MSC's northern end is at the Petronas Towers in downtown Kuala Lumpur, extending through Cyberjaya, Putrajaya, the Kuala Lumpur International Airport and most of Sepang constituency, as well as parts of Bandar Tun Razak, Puchong and Serdang constituencies, before terminating at Nilai, Negeri Sembilan.\n\nThe Multimedia Super Corridor (MSC) program was officially inaugurated by the fourth Malaysian Prime Minister Mahathir Mohamad on 12 February 1996. The establishment of the MSC program was crucial to accelerate the objectives of Vision 2020 and to transform Malaysia into a modern state by 2020, with the adoption of a knowledge-based society framework.\n\nThe MSC flagship applications were launched to boost the MSC Malaysia initiatives and to create a hub for innovative producers and users of multimedia technology. Consortia comprising local and foreign companies (MNCs) collaborated with government agencies, departments and ministries to enhance the socio-economic development of Malaysia in the information age. The vision and mission of the Multimedia Super Corridor (MSC) as expressed by Dr Mahathir Mohammad, the Prime Minister of Malaysia at the time (1981–2003), is essentially this:\n\n“MSC is paramount to leapfrog (Malaysia) into the 21st century and to achieve Malaysia’s Vision 2020, the MSC was created to endeavour the best environment to harness the full potential of the multimedia without any artificial limits. MSC is a global test bed (hub), where the limits of the possible can be explored, and new ways of living, working, and playing in the new area of the Information Age.”\n\nThe Multimedia Super Corridor is a government-designated zone in designed to leapfrog Malaysia into the information and knowledge age. It aims to attract companies with temporary tax breaks and facilities such as high-speed Internet access and proximity to the Kuala Lumpur International Airport.\n\nMSC Malaysia covers an area of approximately × (that is, ) stretching from the Petronas Twin Towers to the Kuala Lumpur International Airport, and including the towns of Putrajaya and Cyberjaya.\n\nIt was announced by former Prime Minister Datuk Seri Dr Mahathir at the Multimedia Asia Conference on 1 August 1996. Mahathir's visit to the United States of America in January 1997 to promote the MSC to companies succeeded in attracting the interest of many large information technology companies. During the visit, an international advisory panel comprising 30 information technology experts were formed to exchange ideas toward the success of the MSC.\n\nThe Multimedia Development Corporation (MDeC, formerly MDC) was created to oversee development of the MSC. It was later renamed to Malaysia Digital Economy Corporation (MDEC).\n\nThough Malaysian law had provided for strict government controls on print media since 1984, a founding principle of MSC Malaysia was that government censorship of the Internet would not be permitted. Seeing a loophole, journalist Steven Gan and colleague Premesh Chandran started an online news resource that would be free of the controls that they felt stifled print media. In November 1999, the pair founded Malaysiakini, an online, independent news source that would become one of Malaysia's most popular websites. It was awarded a Free Media Pioneer award from the International Press Institute in 2001, and Gan was awarded one of the 2000 CPJ International Press Freedom Awards for his work with the site.\n\nThe reverse side of the 1996 series RM5 banknote featured a map of the Multimedia Super Corridor.\n\n"}
{"id": "31663277", "url": "https://en.wikipedia.org/wiki?curid=31663277", "title": "Magnetic chip detector", "text": "Magnetic chip detector\n\nA magnetic chip detector is an electronic instrument that attracts ferromagnetic particles (mostly iron chips). It is mainly used in aircraft engine oil chip detection systems. Chip detectors can provide an early warning of an impending engine failure and thus greatly reduce the cost of an engine overhaul.\n\nChip Detectors consist of small plugs which can be installed in an engine oil filter,oil sump or aircraft drivetrain gear boxes. Over a period of time, engine wear and tear causes small metal chips to break loose from engine parts and circulate in the engine oil.\n\nThe detector houses magnets incorporated into an electric circuit. Magnetic lines of force attract ferrous particles. Collection of these particles continues until the insulated air gap between the magnets (two magnet configuration) or between the magnet and housing (one magnet configuration) is bridged, effectively closing the circuit. The result is an electronic signal for remote indication. Thus, warning light on the instrument panel illuminates, indicating the presence of metal chips.\n\nChip detectors may be positioned in the application with a self-closing valve/adapter through either a bayonet or threaded interface. As the chip detector is disengaged from the valve, the valve closes minimizing any fluid loss from the system.\n\nThe chip detectors used on aircraft are inspected in every 'A check' and higher. They may also be specified intervals such as every 30–40 hours for an engine unit and 100 hours for an APU unit.\n"}
{"id": "2521102", "url": "https://en.wikipedia.org/wiki?curid=2521102", "title": "MegaTransect", "text": "MegaTransect\n\nMegaTransect was the name for a project conducted in Africa in 1999 by J. Michael Fay to spend 455 days on the expedition hike of 2000 miles across the Congo Basin of Africa to survey the ecological and environmental status of the region.\n\nA transect is a term in ecology that denotes a survey of the natural vegetation through a particular area. The concept of a megatransect was conceived as a vegetation transect on a large scale that could be used to take an ecological census of the natural vegetation and ecosystems.\n\nShortly after the hike, Fay successfully lobbied alongside the President of Gabon to create 13 new national parks. In 2002, US Secretary of State Colin Powell and other Bush Administration members gave 53 million dollars to help preserve the Congo Basin.\n\nMike Fay later went on to carry out the MegaFlyover in 2004.\n\nAlso in 2004, an international team conducted a \"Megatransect\" of the island of Madagascar. Dubbed \"Hike Madagascar\", the journey covered the entire island. Members met with rural farmers to help them improve their agricultural techniques and discuss their impact on the environment.\n\nOne of the first megatransects in the United States, was conducted by Dr. Robert R. Humphrey when he rephotographed 535 miles of the natural vegetation along the United States and Mexico border at the 1890s permanent border monument locations, spaced about five miles apart, and published the work in \"90 Years and 535 miles: Vegetation Changes along the Mexican Border\" (1987, pub. Univ. of NM Press, 448 pages).\n\nCraig C. Dremann, in 1997, conducted a megatransect surveying over 3,000 miles and at each mile-marker, noting the roadside vegetation, the perennial native grass, and exotic grass status, through the Great Basin ecosystem. The route was from Reno, Nevada eastward to Hot Springs, South Dakota, and from South Dakota, through Wyoming, Idaho, Nevada and returning westward to Bishop, California, and then north to Reno.\n\nIn 2005, Dremann conducted another vegetation megatransect, this time of the California portion of the Mojave desert, mapping over 1,000 miles on a mile-by-mile basis, for a fast-spreading exotic mustard species, Brassica tournefortii, noting the locations and density of the Mojave desert Mustard infestation in California.\n\nDremann suggests that a method of remotely conducting a large-scale vegetation megatransect, is with photographs. Photographs that have been taken at ground-level at intervals from known locations, can be stitched together, to create large-scale to Continent-scale megatransect pictures of the ecosystems.\n\nIn 2006, Dr. Michael C. McGrann, and his wife Amy M. McGrann, hiked the length of the California section of Pacific Crest National Scenic Trail (PCT) in the western United States from Mexico to the Oregon border (2,736 km) while systematically collecting avian-habitat data on 3,578 survey plots separated by 10-minute hiking intervals. This work was completed in a single field season from 2 April to 8 September. From this data, Dr. McGrann and his collaborators described the elevation and latitudinal distributions of birds along the PCT and statistically modeled avian species richness relationships with elevation, climate, and environmental factors, including temperature, precipitation, and primary productivity. The results from this work show that birds can exhibit heterogeneous relationships to temperature, precipitation, and productivity, depending on the distinct environmental and climate conditions of each of the ecological regions traversed by the PCT. The PCT Mega-transect is an ongoing research project for Dr. McGrann in collaboration with several other researchers. He has continued his biodiversity surveys along sections of the PCT in 2007, 2010, and 2015, and he is working to expand taxa surveyed along the PCT and to involve his undergraduate students at William Jessup University in the PCT Mega-Transect.\n\nIn 2007-2008 J. Michael Fay and Lindsey Holm completed a 1300 mile Redwood Transect of 333 days. This was a transect that spanned from the southernmost to the northernmost redwood tree in California and Oregon. They walked extensively on private timberland and public land recording data on historical exploitation, current forest stand characteristics, silviculture and many other aspects of the redwood ecosystem. The results will be published in National Geographic in 2009.\n\nThe Appalachian Trail Conservancy and the National Park Service are adapting the idea of a megatransect for the Appalachian Trail (A.T.) Like other megatransects, the A.T. MEGA-Transect aims to monitor the natural resources along the trail, understand the status and trends of these resources, and inform and engage the public and stakeholders. The 2,178 mile long trail crosses 14 states, from Georgia to Maine, and is visited by one to two million people, and completed by about 400 people each year. Volunteer citizen scientists are beginning to implement monitoring protocols to track natural resources such as wildlife presence, water quality, forest health, invasive plants, endangered species, mountain birds, phenology, and air quality. Professional scientists are also using the transect for independent research. Because the A.T. is oriented along the predicted migratory direction of species responding to climate change, it is a particularly important megatransect to establish and maintain.\n\nFor a rapidly changing planet, megatransects establish baseline data from which to draw future trends, and they can focus attention on particular ecosystems which are disappearing faster than others.\n\nEstablishing standard megatransects on specific regions or through various ecosystems of each continent, and periodic re–measurement of the ecological conditions along routes, every five to ten years, would provide very valuable measured data on environmental trends.\n\n\n"}
{"id": "37458539", "url": "https://en.wikipedia.org/wiki?curid=37458539", "title": "Mower blade", "text": "Mower blade\n\nMower blades are the cutting components of lawn mowers. They are usually made of sturdy metals as they must be able to withstand high-speed contact with a variety of objects in addition to grass. The materials used (as well as size, thickness, and design of the blades) vary by manufacturer.\n\nThe first known lawn mower sported a cylinder cutting gear made of iron. It was used to mow sporting grounds and wide-ranging gardens. As manufacturers changed the design and structure of mowers, the cutting mechanism also developed and evolved into several varieties, including cylinder/reel blades, deck blades, mulching blades, and lifting blades.\n\nUsed in reel or cylinder mowers, cylinder blades are composed of three to seven helical blades welded in a horizontally rotating cylindrical reel, creating a scissor-like cutting action. Unlike other types of mower blades, reel/cylinder blades cannot be replaced; therefore, a broken blade requires replacement of the entire mower. For dull or rusty blades, cleaning and sharpening kits are available.\n\nAlso known as the standard or straight mower blade, this is the most commonly used blade on rotary mowers.\n\nA mulching blade, also known as an all-purpose blade, features a curved surface which allows it to work in three ways: lifting, mowing, and mulching. First, the blade pulls the grass up and cuts it. Then, clippings are sucked inside the deck and are chopped into tinier pieces. Finally, the blade’s innermost curve produces air pressure to blow the small clippings out, where they are used to feed the soil.\n\nThe lifting blade features a slightly curved surface which creates a vertical upward airflow that lifts the grass up and is assumed to provide a cleaner result than the other types of blades.\n\nLow suction power; recommended for mowing terrain with sandy soil.\n\nMedium suction power; uses less horsepower than high-lift blades.\n\nProvides the greatest suction power among the three lifting blades, but also requires the most horsepower. This is the best blade for cutting tall, compact grass.\n\n"}
{"id": "31128579", "url": "https://en.wikipedia.org/wiki?curid=31128579", "title": "Order of Railroad Telegraphers", "text": "Order of Railroad Telegraphers\n\nThe Order of Railroad Telegraphers (ORT) was a United States labor union established in the late nineteenth century to promote the interests of telegraph operators working for the railroads.\n\nWhile early telegraph lines often ran alongside railroad tracks in the United States, it was not until 1851 that the telegraph was first used for train routing by Charles Minot, Superintendent of the Erie Railroad. As the practice gained wider acceptance in the 1860s and 1870s, telegraphers would be stationed in individual depots along the railroad line in order to receive train orders from a centrally located dispatcher and report back on train movements; telegraphed train orders would be written out on paper and \"handed up\" to the crews of passing trains. \n\nThis practice greatly increased the efficiency of single-track railroads by enabling two trains traveling in opposite directions to use the track at the same time. The dispatcher would designate one of the trains as the \"superior\" train and give it right of way over the \"inferior\" train, which would be required to pull into a siding until the superior train had passed. Local depot telegraph operators would keep track of train arrival times at each station (referred to as \"OSing\" the train) and pass the information on to other operators and the dispatcher. The local depot operator would also set the track switches to enable the inferior train to pull into the siding upon the approach of the superior train. \n\nThe ability of the local depot operators to keep track of the actual time of train movements was particularly important in the era before the establishment of time zones, when local \"sun time\" might be different at each station; a miscalculation of the \"meet time\" of two trains running on a single track could result in disaster. Thus railroad telegraphers played a role in the operation of the railroads that was not unlike the role of air traffic controllers in the modern airline industry; they enabled the trains to run safely and on time.\n\nIn the 1860s, U.S. telegraphers began to form labor organizations, including the National Telegraphic Union and the Telegraphers' Protective League, in order to promote professionalism, negotiate for higher wages, and demand better working conditions. However, following an unsuccessful strike by the Brotherhood of Telegraphers, an affiliate of the Knights of Labor, in 1883, the railroad operators began to see themselves as occupationally distinct from the commercial telegraph operators, who worked in telegraph offices and primarily handled commodities reports, news reports, and personal messages.\nA meeting of telegraphers representing the major U.S. railroads met in Cedar Rapids, Iowa, on June 9, 1886. Organized by Ambrose D. Thurston (1852–1913), publisher of the trade journal \"Railroad Telegrapher\", in Vinton, Iowa, the group formed the Order of Railway Telegraphers of North America, with membership limited to telegraphers who were or who had been employed in railroad service. The Order of Railway Telegraphers was initially intended to be more of a fraternal organization than a trade union; it was ideologically closer to the conservative railroad unions than the more militant commercial telegraphers. Initially, its constitution forbade members to strike except in extreme conditions. By March 1887, the union had attracted 2250 members, the number of members grew to 9000 by March 1889.\n\nIn the early 1890s, members began to demand that the union take a more assertive role in negotiating wages and working conditions with the railroads. In 1891, the constitution was changed to explicitly make the ORT a \"protective\" organization, with the right to call strikes if negotiations with the railroads were unsuccessful; at the same time, the name was changed from \"Order of \"Railway\" Telegraphers\" to \"Order of \"Railroad\" Telegraphers.\" \n\nOne unintended consequence of this was a relatively disorganized period in which operators would stage \"wildcat\" strikes without consulting ORT leadership, who would then be forced into the position of sanctioning strikes that were already in progress. During the relatively prosperous years of 1890-92, the railroads were inclined to recognize the union and negotiate agreements with the ORT to guarantee wages and working hours. As a result, agreements were signed during this period with the Atlantic & Pacific Railroad, the Denver & Rio Grande, the Union Pacific, the Missouri Pacific Railroad, the Baltimore and Ohio Railroad, and the Chesapeake and Ohio Railroad. \n\nA.D. Thurston, who had served as Grand Chief of the ORT since its founding, was replaced by D.G. Ramsay in 1892, who promised to take a more aggressive stance in dealing with the railroads.\n\nThe Panic of 1893 had a severe impact on the railroad industry; major lines that had overexpanded during the prosperous years, including the Reading Railroad, the Union Pacific, the Atchison, Topeka and Santa Fe, and the Northern Pacific all went into receivership. The economic hard times led to more confrontations between the ORT and the railroads. A strike was declared against the Lehigh Valley Railroad on November 18, 1893 after the railroad refused to bargain with a joint commission composed of the ORT and the \"Big Four\" railroad unions, the Brotherhood of Locomotive Engineers, the Order of Railway Conductors, the Brotherhood of Locomotive Firemen, and the Brotherhood of Railroad Brakemen. \n\nThe state Boards of Mediation and Arbitration of New York and New Jersey intervened and settled the dispute through mediation, a tactic that would be increasingly used by the ORT in later years. The strike was ended on December 6, 1893, and was considered to be a significant victory for the ORT when the railroad agreed to reemploy all those who had gone out on strike.\n\nThe Panic of 1893 also had a negative effect on ORT membership, dropping from 18,000 in 1893 to only 5000 in 1895. When the ORT leadership refused to sanction a sympathy strike with the workers involved in the Pullman Strike of 1894, many members left the ORT to join the American Railway Union, formed by Eugene V. Debs in June 1893. Walker V. Powell was elected Grand Chief in 1894.\n\nThe ORT organized its membership into a set of system divisions in the late 1890s, which improved communications and coordination of activities. A Mutual Benefit Department was added in 1898 to provide assistance for members who were ill or unemployed. The increased use of block signaling led to the admission of switch levermen as members in 1897. The ORT took on an international focus in 1896 through its support of a strike against the Canadian Pacific Railway, called on September 28, 1896 to protest the CPR's dismissal of union employees and the railroad's refusal to negotiate with the union on wages and work rules. \n\nThe strike ended after nine days, with the railroad recognizing the right of the ORT to represent the railroad telegraphers in collective bargaining. This victory gave the ORT a foothold in Canada, and recognition as an international union. At about the same time, the ORT began to organize railroad telegraphers in Mexico. One result of this international focus was the appearance of district reports in the \"Railroad Telegrapher\" in French for the French Canadian operators, and Spanish for the Mexican operators. \n\nHeadquarters of the ORT were relocated from Vinton, Iowa, to Peoria, Illinois, in September 1895. In 1899, the ORT relocated again to St. Louis, Missouri, and became a member of the American Federation of Labor. By 1901, the ORT comprised 30 system divisions with 10,000 members, double the number of members in 1895. H. B. Perham replaced W. V. Powell as Grand Chief in 1901.\n\nThe ORT accepted women telegraphers as members from the beginning. The \"Railroad Telegrapher\" devoted a column to the issues and concerns of women operators, who played an active role in recruiting new members and participating in the activities of local districts. Hattie Todd Pickard joined the ORT when she began operating for the St. Louis, Iron Mountain & Southern Railway in 1890; after she and her husband moved to Rawlins, Wyoming, in 1892, she began working for the Union Pacific and was elected Assistant Chief of Overland Division No. 196 of the ORT. Katherine B. Davidson, a telegrapher in Cambridge, Ohio, was elected District Representative of the Baltimore and Ohio Railway System, ORT District 33, in 1905. Ola Delight Smith joined the ORT in Gainesville, Georgia, in 1906, and began a long career as a labor organizer and journalist.\n\nLong working hours were a major issue among railroad telegraphers. Railroad operators were the \"air traffic controllers\" of the railroads; long hours and the resulting fatigue could result in errors in judgment and serious accidents. In 1907, a bill was introduced in Congress to limit the maximum number of hours that railroad employees had to work in a twenty-four-hour period, known as the La Follette Hours of Service Act, after its chief sponsor, Senator Robert La Follette Sr. of Wisconsin. While this bill did not specifically address railroad telegraphers, a similar bill had been introduced and passed in the House of Representatives to limit the working hours of railroad telegraphers to no more than nine hours in a 24-hour period. \n\nThis bill was offered as an amendment to the La Follette bill in the Senate; however, members of the committee to which it was referred bowed to pressure from the railroads and attempted to increase the maximum number of working hours allowed in a single day to twelve hours, a move which enraged ORT members. They immediately began using the telegraph to bombard their congressional representatives with messages of protest, which resulted in the original nine-hour limit being reinstated. The La Follette Hours of Service Act was passed by Congress and signed into law by President Theodore Roosevelt on March 4, 1907.\n\nAlthough the passage of the Hours of Service Act improved conditions for railroad operators, operators at small stations might still be required to remain \"on duty\" for as much as twelve hours in a day, or have their total working hours spread over a longer interval, known as a \"split trick.\" The ORT supported an amendment to reduce the workday still further to eight hours and eliminate the split trick. However, the ORT did not coordinate its efforts with the other railroad unions, and when the Adamson Act, passed in 1917, mandated an eight-hour working day for most railroad employees, it did not explicitly include telegraphers.\n\nWith the entry of the U.S. into the first World War, the railroad and telegraph industries were placed under government control. On December 26, 1917, the United States Railroad Administration (USRA) took control of the railroads. The USRA was generally supportive of the interests of the union members; during the period of nationalization, railroad operators were limited to an eight-hour workday and the ORT was granted collective bargaining capability on a national level. The ORT was able to sign agreements with the Santa Fe, the Missouri-Kansas-Texas Railroad, the Reading Railroad, and the Pennsylvania Railroad. By 1917, ORT membership had grown to 46,000.\n\nA Board of Railroad Wages and Working Conditions was set up by the Railroad Administration. ORT Grand Chief Perham appeared before the Board to request a 40 percent increase in pay, an eight-hour day, and relief from handling government mail by railroad telegraphers. While the Board did provide minimal wage increases, they were far less than the requested 40 percent, and the resulting discontent among ORT members resulted in Perham's defeat in a 1919 election and his replacement as Grand Chief by E. J. Manion. Federal control of the railroads ended on March 1, 1920. When the contracts that had been negotiated while under federal control expired the following year, the ORT had to re-negotiate agreements with the individual railroads.\n\nThe early 1920s were the peak years for ORT membership; by 1922, the union boasted 78,000 members in the U.S., Canada, and Mexico. Some railroads, including the Santa Fe, the Louisville and Nashville, and the Great Northern, maintained essentially the same agreements with the ORT that they had established during the years of government control. A dispute with the Atlantic Coast Line led to a strike in 1925 that ended with the railroad making concessions to the union and signing a new agreement.\n\nMembership in the ORT began to decline after the stock market crash of 1929, due not only to economic conditions but also to the increasing use of centralized traffic control, which no longer required the presence of a telegrapher in each station. The membership fell to 63,000 in 1929, and continued to fall throughout the Depression years of the 1930s. As a result of the declining membership and the loss of revenue from dues, the ORT was forced to suspend payment of pensions to retired members through the mutual benefit program.\n\nBy the beginning of World War II, ORT membership had fallen to around 40,000 as the railroads became involved in the war effort. Railroad telegraphers often had multiple employment as station agents, express agents, and Western Union telegraphers, with each employer paying a portion of the operators' salary. In 1930, the Railway Express Agency, which employed some ORT members as express agents, began handling shipments of perishable items, which had previously been handled as freight shipments on the Seaboard Air Line Railroad. The ORT members who served as express agents as well as telegraphers for the railroad were offered individual pay adjustments on short notice, which the ORT claimed was a violation of the Railway Labor Act of 1926, which required 30 days' notice of any pay adjustment. Additionally, the ORT charged that the individual pay adjustments were in violation of a collective bargaining agreement that had been signed with Railway Express in 1917. \n\nWhen the company refused to participate in a Board of Adjustment, a suit was brought by the ORT in U.S. District Court. The court's decision in favor of the ORT was reversed by the Circuit Court of Appeals, and the suit was finally brought to the U.S. Supreme Court, which decided in favor of the union in 1944. The Supreme Court decision established the primacy of agreements reached by collective bargaining over individual agreements, and upheld the union's right to engage in collective bargaining on behalf of its members.\n\nThe decline of the railroad industry in the 1950s and 1960s led to layoffs and discharges of ORT members. When the ORT attempted to protect its members' jobs by demanding the right to veto job cutbacks, it was accused of \"featherbedding,\" requiring the employment of unnecessary workers, by railroad executives. The ORT called a strike in 1962 to protest layoffs of 600 telegraphers on the Chicago & Northwestern Railroad. A mediation board created by President John F. Kennedy found in favor of the railroad in October 1962, calling the layoffs justified and denying the union the right to veto layoffs. However, the railroad was required to give 90 days' notice to terminated employees, and to pay laid-off telegraphers 60 percent of their annual salary for as much as five years.\n\nIn 1965, the ORT changed its name to the Transportation Communications Employees Union; that union was merged into the Brotherhood of Railway & Airline Clerks, Freight Handlers, Express & Station Employes (BRAC) in 1969. At the time of the merger, the ORT had about 30,000 members. In 1985, BRAC chose to revive the TCU identity, and in July 2005, the \"new\" TCU affiliated with the International Association of Machinists, with full merger to occur no later than January 2012. \n\n\n"}
{"id": "56057186", "url": "https://en.wikipedia.org/wiki?curid=56057186", "title": "Pagan Man", "text": "Pagan Man\n\nPagan Man was a British brand of budget after shave.\n\nPagan Man was popular in 1970s and 1980s, and advertised on television.\n"}
{"id": "26681707", "url": "https://en.wikipedia.org/wiki?curid=26681707", "title": "Pionen", "text": "Pionen\n\nPionen is a former civil defence center built in the White Mountains Södermalm borough of Stockholm, Sweden in 1943 to protect essential government functions from nuclear strike. The address of the Pionen data center is Renstiernas gata 35 and 37.\n\nIt was converted into a data center by the Swedish Internet service provider Bahnhof. It opened on 11 September 2008 and Bahnhof continues to use the facility today. Because of the facility being buried under the mountain, secured by a 40-centimeter thick door, and only reachable by an entrance tunnel, the data center is capable of withstanding a hydrogen bomb. The Pionen data center is also a colocation centre. In 2010 WikiLeaks used Pionen's colocation services to store their servers.\n\nPionen is a data center deep below 30 meters of granite, with three physical datalinks into the mountain. Also, Pionen is located in Central Stockholm, with 1,100 square meters of space. Pionen features fountains, greenhouses, simulated daylight and a huge salt water fish tank. Its data center has two backup power generators, which are actually submarine engines.\n\nIt previously hosted WikiLeaks.\n"}
{"id": "80758", "url": "https://en.wikipedia.org/wiki?curid=80758", "title": "RF modulator", "text": "RF modulator\n\nAn RF modulator (or radio frequency modulator) is an electronic device whose input is a baseband signal which is used to modulate a radio frequency source.\n\nRF modulators are used to convert games like signals from devices such as media players, VCRs and game consoles to a format that can be handled by a device designed to receive a modulated RF input, such as a radio or television receiver.\n\nPrior to the introduction of specialised video connector standards such as SCART, TVs were designed to only accept signals through the aerial connector: signals originate at a TV station, are transmitted over the air, and are then received by an antenna and demodulated within the TV. When equipment was developed which could use a television receiver as its display device, such as VCRs, DVD players, early home computers, and video game consoles, the signal was modulated and sent to the RF input connector.\n\nThe aerial connector is standard on all TV sets, even very old ones. Since later television designs include composite, S-Video, and component video jacks, which skip the modulation and demodulation steps, modulators are no longer included as standard equipment, and RF modulators are now largely a third-party product, purchased primarily to run newer equipment such as DVD players with an old television set.\n\nTechnically, RF modulation usually means combining the data with a carrier signal at a standardized frequency. Either amplitude or frequency modulation may be used, as required by the receiving equipment.\n\nIn North America, RF modulators generally output on channel 3 or 4 (VHF), which may be selectable, although the Atari consoles offer channels 2 and 3. In Europe standard modulators usually used channel 36 (UHF) by default, but are usually tunable over part or all of the UHF band.\n\nModulating a TV signal with stereo sound is relatively complex; most low-cost home TV modulators produce a signal with monaural audio. Even some units that have two or more audio inputs simply combine the left and right audio channels into one mono audio signal. Some used on very early home computers had no sound capability at all. Most cheaper modulators (i.e. not intended for professional use) lack vestigial sideband filtering.\n\nTV modulators generally feature analog passthrough, meaning that they take input both from the device and from the usual antenna input, and the antenna input \"passes through\" to the TV, with minor insertion loss due to the added device. In some cases the antenna input is always passed through, while in other cases the antenna input is turned off when the device is outputting a signal, and only the device signal is sent onward, to reduce interference.\n\nRF modulators produce a relatively poor picture, as image quality is lost during both the modulation from the source device, and the demodulation in the television.\n\nRF modulators are commonly integrated into VCRs, in video game consoles up to and including the fourth generation, and in 8- and 16-bit home computers.\n\nSome systems were supplied with an external modulator unit that connected to both the system and to the antenna jacks of a television. One reason for this is that a device which outputs an RF signal must in general be certified by regulatory authorities—such as the U.S. Federal Communications Commission (FCC) — and thus by having an external RF modulator, only the modulator itself needed to be certified, rather than the entire video game system.\n\nRF modulators may also be used to take the audio and video signal from a PAL or NTSC composite video, RGB, YUV or other composite AV source, and generate a broadcast RF signal that can be fed into a television's aerial/coaxial connector.\n\nMulti-channel RF modulators are commonly used in home audio/video distribution. These devices have multiple audio and video inputs and one RF output. Audio/video outputs from source devices such as a DVD player, VCR, or DSS receiver are connected to the audio/video inputs on the modulator. The modulator is then programmed to broadcast the signals on a certain frequency. That RF broadcast is then received by the connected TV. When the TV is tuned to the programmed channel, the video and audio signal of the source device is accessed. RF modulation can become difficult in a CATV system. High pass, low pass, and notch filters must be used to block certain frequencies, or channels, so that the modulator can broadcast the audio/video signal of the source device on that channel.\n\n\"Professional\" modulators such as those used in the CATV industry generally include vestigial sideband filtering which is generally absent on \"consumer\" grade modulators.\n\nAudio RF modulators are used in low-end car audio to add devices like CD changers without requiring dashboard hardware upgrades. For example, a portable CD player's headphone jack is connected to the modulator, which outputs a low-power FM radio signal that is played through the car radio. Car FM modulators suffer from loss of quality and interference issues. Later devices which would use these types of modulators would be iPods and similar portable media players.\n\n\n"}
{"id": "9031121", "url": "https://en.wikipedia.org/wiki?curid=9031121", "title": "Real-time transcription", "text": "Real-time transcription\n\nReal-time transcription is the general term for transcription by court reporters using real-time text technologies to deliver computer text screens within a few seconds of the words being spoken. Specialist software allows participants in court hearings or depositions to make notes in the text and highlight portions for future reference.\n\nTypically, real-time writers can produce text using machines at the rate of at least 200 words per minute. Stenographers can typically type up to 300 words per minute for short periods of time, but most cannot sustain such a speed.\n\nReal-time transcription is also used in the broadcasting environment where it is more commonly termed \"captioning.\"\n\nReal-time reporting is used in a variety of industries, including entertainment, television, the Internet, and law.\n\nSpecific careers include the following:\n\n"}
{"id": "25754616", "url": "https://en.wikipedia.org/wiki?curid=25754616", "title": "Roxxxy", "text": "Roxxxy\n\nRoxxxy, termed a \"sex robot\" is a full-size interactive sex doll. The robot is built by the New Jersey-based company TrueCompanion.\nIts engineer is Douglas Hines, founder and president of the company, who worked as an artificial intelligence engineer at the Bell Labs before he founded TrueCompanion. Development of Roxxxy is claimed to have cost between $500,000 and $1 million.\n\nRoxxxy made its public debut at the AVN Adult Entertainment Expo (AEE) in Las Vegas on January 9, 2010. The gynoid stands 5 feet 7 inches tall and weighs 120 pounds (170 cm and 54 kg). It has synthetic skin and an artificial intelligence engine programmed to learn the owner's likes and dislikes. \nAn articulated skeleton allows Roxxxy to be positioned like a human being but the doll cannot move its limbs independently. A pump installed in the robot powers an internal liquid cooling system. The Roxxxy prototype is claimed to have been modeled after a college-aged Caucasian fine arts student.\n\nAccording to the website of the company, Roxxxy is not limited to sexual uses and \"can carry on a discussion and expresses her love to you. She can talk to you, listen to you and feel your touch.\" Other features include touch sensors that give Roxxxy the ability to sense when it is being moved. The robot's vocabulary may be updated with the help of a laptop (connected to the back by cables) and the Internet.\n\nCustomers can ask TrueCompanion to customize the Roxxxy according to personal preferences, such as hair color, eye color, skin color, breast size and other features. Roxxxy is priced at $7,000 to $9,000 plus a separate subscription fee.\n\nAccording to Douglas Hines, Roxxxy garnered about 4,000 pre-orders shortly after its AEE reveal in 2010. However, as reported by Jenny Kleeman in April 2017, no actual customers have ever surfaced with a Roxxxy doll, and the public has remained skeptical that any commercial Roxxxy dolls have ever been produced.\n"}
{"id": "13239461", "url": "https://en.wikipedia.org/wiki?curid=13239461", "title": "Sandsinker", "text": "Sandsinker\n\nSandsinkers are lead-free fishing sinkers made of fabric and filled with sand. Although they do not cast as easily or as far for surf fishing, they are a healthy alternative to lead for fishing from jetties or any situation where casting distance is not a prime consideration.\n\n"}
{"id": "20643480", "url": "https://en.wikipedia.org/wiki?curid=20643480", "title": "Speirs and Major Associates", "text": "Speirs and Major Associates\n\nSpeirs + Major is a UK lighting design practice founded by Jonathan Speirs (1958-2012) and Mark Major in 1993. The practice is noted for its illumination of many prominent buildings, including Barajas International Airport, 30 St Mary Axe (‘The Gherkin’), the Millennium Dome and the interior of St. Pauls Cathedral. The firm has also developed lighting master plans for several British cities, including Cambridge, Coventry, Durham, Newcastle, and for major private developments including Greenwich Peninsula and King’s Cross Central, London.\n\nSpeirs + Major has been credited with helping to raise awareness of the lighting design profession in the UK. Today it employs approximately 38 people drawn from disciplines including architecture, art, lighting, interior, graphic and theatrical design. Its studios are based in London and Edinburgh, UK.\n\nBoth founding members Jonathan Speirs, who died in 2012, and Mark Major trained and practiced as architects before focusing on lighting design. In interviews they have argued that light should be embedded at the heart of the architectural design process rather than applied as a ‘cosmetic add-on’. This integral approach to light has led them to adopt the term ‘lighting architect’ to describe their role as ‘building with light as opposed to bricks and mortar’.\n\n\nRIBA Stirling Prize collaborations\nSpeirs and Major Associates have designed lighting for a number of buildings that have either won or been nominated for the RIBA Stirling Prize for architecture.\n\nStirling Prize winners:\nStirling Prize shortlisted projects:\n\n\nIn 2006 Jonathan Speirs and Mark Major authored the book \"Made of Light: The Art of Light and Architecture\", a series of visual essays on lighting design. The book is co-authored by Anthony Tischhauser and published by Swiss architectural publisher Birkhäuser.\n\n"}
{"id": "22488906", "url": "https://en.wikipedia.org/wiki?curid=22488906", "title": "Steel plate construction", "text": "Steel plate construction\n\nSteel plate construction is a rapid method of constructing heavy reinforced concrete items. It was developed in Korea in 2004. At a steel fabricator, assemblies are constructed. Each assembly has two parallel plates joined with welded stringers. The assemblies are moved to the job site and placed with a crane. The plates are welded so that they form parallel walls joined by stringers. Finally, the space between the plate walls is filled with concrete.\n\nSteel plate construction is roughly twice the speed of other reinforced concrete construction, because it avoids tying rebar and constructing forms on-site. The parallel plate assemblies can be constructed quickly in specialized off-site fabrication facilities. \nThe method has excellent strength because the steel is on the outside, where tensile forces are often greatest.\n\nBecause the construction time is about half, sunk interest costs for heavy construction projects are about half when this method is used.\n\nThe method is of special interest for rapidly constructing nuclear power plants, which use large reinforced concrete structures, and typically have long construction times, with heavy interest costs.\n\n"}
{"id": "45381160", "url": "https://en.wikipedia.org/wiki?curid=45381160", "title": "Stephen Meade", "text": "Stephen Meade\n\nStephen Meade is an American entrepreneur, business executive and strategic adviser. Meade has successfully incubated nine technology-based companies in the past 20 years. Currently, Stephen serves as the Chairman at BigBamboo LLC, a holding company that incubates start-up companies and is also head of MagMo, My Wet Rock, RONAstar and ComCom (Community Commerce) Networks.\n\nStephen graduated from the University of Missouri–Kansas City with a bachelor's degree in Business and Marketing. By the age of 21, he had written three books on credit improvement and finance, and an infomercial called \"Give Yourself Credit\". He then spent 6 years in financial services and in 1996, started his first internet company called Virtual Sellers which became an innovation for an e-commerce based transaction processing system that proved to be an early precursor to PayPal.\n\nStephen Meade is particularly notable for his contributions to technology based companies, including:\n\nStephen founded VirtualSellerc, the first Internet based master merchant for eCommerce in 1996, where he helped develop a transaction processing systems for internet based retailers. The VirtualSellerc was sold in 1999 to a public company that assumed the name of Virtualsellers.com. The company provided up to 7 payment options, and easy transaction processing and clearing services for small and medium businesses looking to sell their products online.>\n\nStephen Meade was the founder and CEO of Cenoplex, an audio messaging and services company that helps carriers in Lifecycle Management (LCM) efforts to reduce churn, promote Value Added Services (VAS), and increase Revenue (ARPU) with non-intrusive audio content. Fully funded, with an A+ Management team and Advisory Board, Cenoplex is the first company in the world to insert a four-second audio message in the call sequencing gap of an outbound mobile call, all without disrupting the call path.\n\nIn August 2000, Meade and former government commissioner Bob Gerometta founded KnockNOW, a non-profit organization committed to accelerating opportunities for entrepreneurs. Meade and Gerometta wanted to educate and support a community professionals from five key business sectors: service; politics, press and academics; investment; corporate; and entrepreneurial. Today, the KnockNOW community continues to hold regular meetings called \"Isolation is a Good Thing\".\n\nStephen Meade founded Big Bamboo, LLC as a holding company and incubator for start-up companies. Meade serves as business architect, venture capitalist, and relationship builder.\n\nStephen Meade created My WetRock, a consumer product that saves a targeted 20 billions of gallons of water per year by reducing the amount of wasted water when flushing toilets. At My WetRock, Meade promotes environmental awareness and provides creative educational tools through a comic character called \"Johnny Waterdrop\". In March 2011, Meade was named TCVN's FastPitch Winner for his work with My WetRock.\n\nStephen has been a notable speaker and mentor to the Founder Institute, world's largest entrepreneur training and startup launch program. Meade is also noted for his active participation in the Clinton Global Initiative and Opportunity Green.\n\nMeade has stood as guest lecturer at several universities, including MIT, Harvard, USC, UCLA, Northwestern and Peter Kiewit Institute among others.\n"}
{"id": "28353373", "url": "https://en.wikipedia.org/wiki?curid=28353373", "title": "Thermal destratification", "text": "Thermal destratification\n\nThermal destratification is the process of mixing the internal air in a building to eliminate stratified layers and achieve temperature equalization throughout the building envelope. \n\nDestratification is the reverse of the natural process of thermal stratification, which is the layering of differing (normally rising) air temperatures from floor to ceiling. Stratification is caused by hot air rising up to the ceiling or roof space because it is lighter than the surrounding cooler air. Conversely, cool air falls to the floor as it is heavier than the surrounding warmer air.\n\nIn a stratified building, temperature differentials of up to 1.5°C per vertical foot is common, and the higher a building's ceiling, the more extreme this temperature differential can be. In extreme cases, temperature differentials of 10°C have been found over a height of 1 meter. Other variables that influence the level of thermal stratification include heat generated by people and processes present in the building, insulation of the space from outside weather conditions, solar gain, specification of the HVAC system, location of supply and return ducts, and vertical air movement inside the space, usually supplied by destratification fans. Computational fluid dynamics can be used to predict the level of stratification in a space.\n\nIn a study conducted by the Building Scientific Research Information Association, the wasted energy due to stratification increased consistently based on temperature differential from floor to ceiling (ΔT). The study indicates that stratified buildings tend to overheat or overcool based on the temperature at the thermostat, which tends to be lower than the overall heat energy present in the room. The study also showed that energy waste due to stratification was present at ceiling heights ranging from 20 ft. to 40 ft, and higher ceilings caused higher energy waste, even at the same ΔT. Since ΔT tends to be higher in taller ceilings, the effect of stratification is compounded, causing substantial energy waste in tall buildings.\n\nSince stratification and the costs associated with it are linear, the definition of destratification will differ based on opinion and use case. Full destratification, or a 0° ΔT from floor to ceiling, is unlikely to occur in any building. Since the costs of stratification decrease linearly as ΔT approaches 5.4°F, and no study has yet looked at the effects of stratification below 5.4°F, it is not uncommon to consider any space with a ΔT below 5°F to be destratified. In the United States, ASHRAE Standard 55 prescribes 3°C as the limit for the vertical air temperature difference between head and ankle levels, but has no standard recommending an ideal ΔT between floor and ceiling.\n\nReducing thermal stratification can be accomplished by controlling the variables that are associated with increased stratification. Since many of the variables, including ceiling height, people and processes, solar gain, and outside weather conditions cannot be controlled, the most common technologies used are related to the building's HVAC (heating, ventilation, and air conditioning) system. One of the cheapest, most effective, and easiest to install technologies are destratification fans, including both axial destratification fans and HVLS (high-volume low-speed) fans.\n\nAxial destratification fans are self-contained units that are installed in an array at the ceiling with the goal of blowing conditioned air in the ceiling down to the floor, where people live and work. Because axial fans are designed to blow air straight down at the floor, they can be used in ceiling and roof structures over 100 ft. tall. Because axial destratification fans can achieve destratification with low CFMs, it is imperative that the air leaving the nozzle achieve an air speed at the floor of between 0.2 and 0.5 m/s. The result of this level of air movement is the integration of conditioned air from the ceiling with air at the floor level. Failing to impact the floor will result in destratification of medial layers of air but not achieve destratification at the floor. Since the area around the thermostat will not be destratified in this instance, it is hypothesized that there will be little or no cost savings, as the thermostat will continue to overheat or overcool the room.\n\nAn experiment in a room with a 21 ft. ceiling yielded a savings of 23.5% with the use of axial destratification fans.\n\nBecause of their size, HVLS fans are normally installed in new construction, rather than retrofits, as the roof structure may have to be redesigned to accommodate the increased weight and size. It's not uncommon to require the relocation of lights, due to strobing as large fan blades pass under them, and sprinkler systems, which typically require unobstructed access to the floor to meet fire code. When used in the summer to encourage evaporative cooling, HVLS fans are run forward, blowing air at the floor. When used for destratification in the winter, the fans are run in reverse, blowing air towards ceiling which then circulates around the room. The height at which HVLS fans can be effective is limited compared to axial destratification fans.\n\nThis method has the most benefits through its application in the heating, ventilation, and air conditioning (HVAC) industry and in heating and cooling for buildings and it has been found that \"stratification is the single biggest waste of energy in buildings today.\" \n\nBy incorporating thermal destratification technology into buildings, energy requirements are reduced as heating systems are no longer over-delivering in order to constantly replace the heat that rises away from the floor area, by redistributing the already heated air from the unoccupied ceiling space back down to floor level, until temperature equalisation is achieved. With regards to cooling destratification systems ensure the cooled air supplied is circulated fully and distributed evenly throughout internal environments, eliminating hot and cold spots and satisfying thermostats for longer periods of time. As a result, destratification technology has great potential for carbon emission reductions due to the reduced energy requirement, and is in turn capable of cutting costs for businesses, sometimes by up to 50%. This is supported by The Carbon Trust which recommends destratification in buildings as one of its top three methods to reduce carbon dioxide emissions.\n\nDestratification naturally increases air movement at the floor, reducing \"hot spots\" and \"cold spots\" in a room. It can be used in typically cold areas, like grocery store freezer cases, to warm patrons shopping nearby. In addition, air movement from destratification fans can be used to help meet ASHRAE Standard 62.1 by increasing the amount of air movement at the floor.\n\n"}
{"id": "41911440", "url": "https://en.wikipedia.org/wiki?curid=41911440", "title": "Thermodynamic solar panel", "text": "Thermodynamic solar panel\n\nA thermodynamic solar panel is a type of air source heat pump. Instead of a large fan to take energy from the air, it has a flat plate collector. This means the system gains energy from the sun as well as the ambient air. Thermodynamic water heaters use a compressor to transfer the collected heat from the panel to the hot water system using refrigerant fluid that circulates in a closed cycle.\n\nIn the UK, thermodynamic solar panels cannot be used to claim the Renewable Heat Incentive. This is due to the lack of technical standards for the testing and installation. The UK Microgeneration Certification Scheme is working to develop a testing standard, either based on MIS 3001 or MIS 3005 or a brand new scheme document if appropriate.\n\nLab testing has been carried out by Das Wärmepumpen-Testzentrum Buchs (WPZ) in Buchs Switzerland on an Energi Eco 200esm/i thermodynamic solar panel system. This showed a Coefficient of performance of 2.8 or 2.9 (depending on tank volume).\nIn the UK, the first independent test is under-way at Narec Distributed Energy. So far data is available for January to April 2014. As with the Carnot cycle, the achievable efficiency is strongly dependent on the temperatures on both side of the system.\n\n"}
{"id": "49967329", "url": "https://en.wikipedia.org/wiki?curid=49967329", "title": "Vehicle-to-everything", "text": "Vehicle-to-everything\n\nVehicle-to-everything (V2X) communication is the passing of information from a vehicle to any entity that may affect the vehicle, and vice versa. It is a vehicular communication system that incorporates other more specific types of communication as V2I (Vehicle-to-Infrastructure), V2N (Vehicle-to-network), V2V (Vehicle-to-vehicle), V2P (Vehicle-to-Pedestrian), V2D (Vehicle-to-device) and V2G (Vehicle-to-grid).\n\nThe main motivations for V2X are road safety, traffic efficiency, and energy savings. There are two types of V2X communication technology depending on the underlying technology being used:\n\n1) WLAN based, and\n\n2) cellular based.\n\nStandardization of WLAN based V2X supersedes that of cellular based V2X system. IEEE first published the specification of WLAN based V2X (IEEE 802.11p) in 2012. It supports direct communication between vehicles (V2V) and between vehicle and infrastructure (V2I). DSRC uses the underlying radio communication provided by 802.11p.\n\nIn 2016, 3GPP published V2X specifications based on LTE as the underlying technology. It is generally referred to as \"cellular V2X\" (C-V2X) to differentiate itself from the 802.11p based V2X technology. In addition to the direct communication (V2V, V2I), C-V2X also supports wide area communication over cellular network (V2N). This additional mode of communication and native migration path to 5G are two main advantages over 802.11p based V2X system.\n\nAs of December 2017, a European automotive manufacturer has announced to deploy V2X technology based on 802.11p from 2019. Recent study and analysis in 2017 and 2018, all performed by 5GAA, the industry organisation supporting and developing the C-V2X technology, indicate that cellular-based C-V2X technology in direct communication mode is superior to 802.11p in multiple aspects, such as performance, communication range, and reliability. Many of these claims are disputed, e.g. in a whitepaper published by NXP, one of the companies active in the 802.11p based V2X technology.\n\nThe original V2X communication uses WLAN technology and works directly between vehicles and vehicles (V2V) and traffic infrastructure (V2I), which form a vehicular ad-hoc network as two V2X senders come within each other’s range. Hence it does not require any communication infrastructure for vehicles to communicate, which is key to assure safety in remote or little-developed areas. WLAN is particularly well-suited for V2X communication, due to its low latency. It transmits messages known as Cooperative Awareness Messages (CAM) or Basic Safety Message (BSM), and Decentralised Environmental Notification Messages (DENM). Other roadside infrastructure related messages are Signal Phase and Timing Message (SPAT), In Vehicle Information Message (IVI), and Service Request Message (SRM). The data volume of these messages is very low. The radio technology is part of the WLAN IEEE 802.11 family of standards and known in the US as Wireless Access in Vehicular Environments (WAVE) and in Europe as ITS-G5. To complement the direct communication mode, vehicles can be equipped with traditional cellular communication technologies, supporting V2N based services.\n\nMore recent V2X communication uses Cellular network and is called Cellular V2X (or C-V2X) to differentiate it from the WLAN based V2X. There have been multiple industry organizations, such as 5G Automotive Association (5GAA) promoting C-V2X due to its advantages over WLAN based V2X. C-V2X is initially defined as LTE in 3GPP Release 14 and is designed to operate in several modes: 1) Device-to-device (V2V or V2I), and 2) Device-to-network (V2N). In 3GPP Release 15, the V2X functionalities are expanded to support 5G. The main advantage of C-V2X includes support of both direct communication between vehicles (V2V) and traditional cellular-network based communication. Also, C-V2X provides migration path to 5G based systems and services.\n\nThe direct communication between vehicle and other devices (V2V, V2I) uses so-called PC5 interface. PC5 refers to a reference point where the User Equipment (UE), i.e. mobile handset, directly communicates with another UE over the direct channel. In this case, the communication with the base station is not required. In system architectural level, proximity service (ProSe) is the feature that specifies the architecture of the direct communication between UEs. In 3GPP RAN specifications, \"sidelink\" is the terminology to refer to the direct communication over PC5. PC5 interface was originally defined to address the needs of mission-critical communication for public safety community (Public Safety-LTE, or PS-LTE) in Release 13. The motivation of the mission-critical communication was to allow law enforcement agencies or emergency rescue to use the LTE communication even when the infrastructure is not available, such as natural disaster scenario. In Release 14 onwards, the use of PC5 interface has been expanded to meet various market needs, such as communication involving wearable devices such as smartwatch. In C-V2X, PC5 interface is re-applied to the direct communication in V2V and V2I.\n\nIn addition to the direct communication over PC5, C-V2X also allows the C-V2X device to use the cellular network connection in the traditional manner over Uu interface. Uu refers to the logical interface between the UE and the base station. This is generally referred to as vehicle-to-network (V2N). V2N is a unique use case to C-V2X and does not exist in 802.11p based V2X given that the latter supports direct communication only. However, similar to WLAN based V2X also in case of C-V2X, two communication radios are required to be able to communicate simultaneously via a PC5 interface with nearby stations and via the UU interface with the network.\n\nThrough its instant communication V2X allows road safety applications such as (non-exhaustive list):\n\n\nThe US National Highway Traffic Safety Administration’s (NHTSA) report “Vehicle-to-Vehicle Communications: Readiness of V2V Technology for Application“ lists the initial use cases envisioned for the US. European standardisation body ETSI and SAE published standards on what they see as use cases. Early use cases focus on road safety and efficiency. \n\nIn the medium term V2X is perceived as a key enabler for autonomous driving, assuming it would be allowed to intervene into the actual driving. In that case vehicles would be able to join platoons, the way HGVs do.\n\nWLAN-based V2X communication is based on a set of standards drafted by the American Society for Testing and Materials (ASTM). The ASTM E 2213 series of standards looks at wireless communication for high-speed information exchange between vehicles themselves as well as road infrastructure. The first standard of this series was published 2002. Here the acronym Wireless Access in Vehicular Environments (WAVE) was first used for V2X communication.\n\nFrom 2004 onwards the Institute Electrical and Electronics Engineers (IEEE) started to work on wireless access for vehicles under the umbrella of their standards family IEEE 802.11 for Wireless Local Area Networks (WLAN). Their initial standard for wireless communication for vehicles is known as IEEE 802.11p and is based on the work done by the ASTM. Later on in 2012 IEEE 802.11p was incorporated in IEEE 802.11.\n\nAround 2007 when IEEE 802.11p got stable, IEEE started to develop the 1609.x standards family standardising applications and a security framework (IEEE uses the term WAVE), and soon after SAE started to specify standards for V2V communication applications. SAE uses the term DSRC for this technology (this is how the term was coined in the US). In parallel at ETSI the technical committee for Intelligent transportation system (ITS) was founded and started to produce standards for protocols and applications (ETSI coined the term ITS-G5). All these standards are based on IEEE 802.11p technology.\n\nBetween 2012 and 2013, the Japanese Association of Radio Industries and Businesses (ARIB) specified, also based on IEEE 802.11, a V2V and V2I communication system in the 700 MHz frequency band.\n\nIn 2015 ITU published as summary of all V2V and V2I standards that are worldwide in use, comprising the systems specified by ETSI, IEEE, ARIB, and TTA (Republic of Korea, Telecommunication Technology Association).\n\n3GPP started standardization work of cellular V2X (C-V2X) in Release 14 in 2014. It is based on LTE as the underlying technology. Specifications were published in 2016. Because this C-V2X functionalities are based on LTE, it is often referred to as LTE-V2X. The scope of functionalities supported by C-V2X includes both direct communication (V2V, V2I) as well as wide area celluar network communication (V2N). \n\nIn Release 15, 3GPP continued its C-V2X standardization to be based on 5G. Specifications are published in 2018 as Release 15 comes to completion. To indicate the underlying technology, the term 5G-V2X is often used in contrast to LTE-based V2X (LTE-V2X). Either case, C-V2X is the generic terminology that refers to the V2X technology using the cellular technology irrespective of the specific generation of technology.\n\nIn Release 16, 3GPP further enhances the C-V2X functionality. The work is currently in progress. In this way, C-V2X is inherently future proof by supporting migration path to 5G.\n\nStudy and analysis were done to compare the effectiveness of direct communication technologies between LTE-V2X PC5 and 802.11p from the perspective of accident avoided and reduction in fatal and serious injuries. The study shows that LTE-V2X achieves higher level of accident avoidance and reduction in injury. It also indicates LTE-V2X performs higher percentage of successful packet delivery and communication range. Another link-level and system-level simulation result indicates that, to achieve the same link performance for both line-of-sight (LOS) and non-line-of-sight (NLOS) scenarios, lower signal-to-noise-ratio (SNR) are achievable by LTE-V2X PC5 interface compared to IEEE 802.11p.\n\nCellular based V2X solution also leads to the possibility of further protecting other types of road users (e.g. pedestrian, cyclist) by having PC5 interface to be integrated into smartphones, effectively integrating those road users into the overall C-ITS solution. Vehicle-to-person (V2P) includes Vulnerable Road User (VRU) scenarios to detect pedestrians and cyclists to avoid accident and injuries involving those road users. \n\nAs both direct communication and wide area cellular network communication are defined in the same standard (3GPP), both modes of communication will likely be integrated into a single chipset. Commercialization of those chipsets further enhances economy of scale and leads to possibilities to wider range of business models and services using both types of communications.\n\nIn 1999 the US Federal Communications Commission (FCC) allocated 75 MHz in the spectrum of 5.850-5.925 GHz for intelligent transport systems. Since then the US Department of Transportation (USDOT) has been working with a range of stakeholders on V2X. In 2012 a pre-deployment project was implemented in Ann Arbor, Michigan. 2800 vehicles covering cars, motorcycles, buses and HGV of different brands took part using equipment by different manufacturers. The US National Highway Traffic Safety Administration (NHTSA) saw this model deployment as proof that road safety could be improved and that WAVE standard technology was interoperable. In August 2014 NHTSA published a report arguing vehicle-to-vehicle technology was technically proven as ready for deployment. On 20 August 2014 the NHTSA published an Advance Notice of Proposed Rulemaking (ANPRM) in the Federal Register, arguing that the safety benefits of V2X communication could only be achieved if a significant part of the vehicles fleet was equipped. Because of the lack of an immediate benefit for early adopters, the NHTSA proposed a mandatory introduction. On 25 June 2015 the US House of Representatives held a hearing on the matter, where again the NHTSA, as well as other stakeholders argued the case for V2X.\n\nTo acquire EU-wide spectrum, radio applications require a harmonised standard, in case of ITS-G5 ETSI EN 302 571, first published in 2008. A harmonised standard in turn requires an ETSI System Reference Document, here ETSI TR 101 788. Commission Decision 2008/671/EC harmonises the use of the 5 875-5 905 MHz frequency band for transport safety ITS applications. In 2010 the ITS Directive 2010/40/EU was adopted. It aims to assure that ITS applications are interoperable and can operate across national borders, it defines priority areas for secondary legislation, which cover V2X and requires technologies to be mature. In 2014 the European Commission’s industry stakeholder “C-ITS Deployment Platform” started working on a regulatory framework for V2X in the EU. It identified key approaches to an EU-wide V2X security Public Key infrastructure (PKI) and data protection, as well as facilitating a mitigation standard to prevent radio interference between ITS-G5 based V2X and road charging systems. The European Commission recognised ITS-G5 as the initial communication technology in its 5G Action Plan and the accompanying explanatory document, to form a communication environment consisting of ITS-G5 and cellular communication as envisioned by EU Member States. Various pre-deployment projects exist at EU or EU Member State level, such as SCOOP@F, the Testfeld Telematik, the digital testbed Autobahn, the Rotterdam-Vienna ITS Corridor, Nordic Way, COMPASS4D or C-ROADS.\n\nSpectrum allocation for C-ITS in various countries is shown in the following table. Due to the standardization of V2X in 802.11p preceding C-V2X standardization in 3GPP, spectrum allocation was originally intended for the 802.11p based system. However, the regulations are technology neutral so that the deployment of C-V2X is not excluded.\n\nThe deployment of V2X technology (either C-V2X or 802.11p based products) will occur gradually over time. New cars will be equipped with either of the two technologies starting around 2020 and its proportion on the road is expected to increase gradually. In the meantime, existing (legacy) vehicles will continue to exist on the road. This implies that the V2X capable vehicles will need to co-exist with non-V2X (legacy) vehicles or with V2X vehicles of incompatible technology.\n\nThe main obstacles to its adoption are legal issues and the fact that, unless almost all vehicles adopt it, its effectiveness is limited. British weekly \"The Economist\" argues that autonomous driving is more driven by regulations than by technology.\n\nHowever, more recent study indicates that there are benefits in reducing traffic accidents even during the transitional period in which the technology is being adopted in the market.\n\n\n\n"}
