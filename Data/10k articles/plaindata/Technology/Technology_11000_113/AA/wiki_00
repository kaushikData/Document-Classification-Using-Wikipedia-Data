{"id": "149874", "url": "https://en.wikipedia.org/wiki?curid=149874", "title": "ATA Airlines", "text": "ATA Airlines\n\nATA Airlines, Inc. – formerly known as American Trans Air and commonly referred to as ATA – was an American low-cost scheduled service and charter airline based in Indianapolis, Indiana. ATA operated scheduled passenger flights throughout the U.S. mainland and Hawaii and San Juan, Puerto Rico, as well as military and commercial charter flights around the world. ATA also operated flights to Portugal (LIS, OPO, TER, PDL and SMA). In its early days the airline flew charters on a worldwide basis and had bases in Chicago, Detroit, New York, Indianapolis, Oakland and Milwaukee. Later when they entered scheduled service the airline maintained focus cities at Chicago Midway International Airport, Honolulu International Airport, and Oakland International Airport.\n\nThe airline's parent company – New ATA Holdings, Inc. (the successor to ATA Holdings Inc., which was also once known as Amtran) – later changed its name to Global Aero Logistics, Inc. and purchased World Air Holdings, Inc. for $315 million in an all-cash transaction with the financial backing of the investment firm, MatlinPatterson. World Air Holdings, Inc. owned and operated North American Airlines and World Airways as two separate US-certified air carriers. ATA was North America's largest charter airline, and until its shutdown transported more troops for the United States military than any other commercial airline.\n\nOn April 2, 2008, ATA filed for Chapter 11 bankruptcy protection. ATA then announced it was ceasing all services, effective 4:00 AM EDT, Thursday April 3, 2008, citing the unexpected loss of a major contract for its military charter business, as a subcontractor of FedEx Express, along with recent increases in jet fuel prices. Red-eye flights in the air at the time of the announcement proceeded to their destinations. Flight 4586 from Honolulu to Phoenix was the last ATA flight, departing almost two hours late at 12:10am (HST) arriving April 3, 2008 at 8:48am (MST).\n\nSouthwest Airlines owns and retains all ownership of the operating certificate and all assets held by ATA Airlines including trademarks, logos, etc. These were purchased for $7.5 million by Southwest while ATA was operating under bankruptcy protection.\n\nATA was established in February 1973 as Ambassadair travel club. Its first aircraft was a Boeing 720 named \"Miss Indy\", with a second Boeing 720 (\"Spirit of Indiana\") being added in 1978. ATA received its common-air carrier certificate in March 1981. Operations started as a charter carrier in 1981, with a fleet of eight Boeing 707s based in Indianapolis, Indiana. In 1983, American Trans Air introduced its first McDonnell Douglas DC-10, a series -10, and was followed in 1984 by another, a long range series -40. Amtran, Inc. was founded by owner J. George Mikelsons in 1984, as the holding company for Ambassadair, ATA, and any future subsidiaries. The airline replaced the 707s with Boeing 727-100 trijets in 1984, and added Rolls-Royce powered Lockheed L-1011 TriStar wide body jetliners (most of which were ex-Delta Air Lines and TWA) in 1985, and Boeing 757-200s in 1989. ATA was an all charter airline flying to destinations all over the World. American Trans Air started its first scheduled service in 1986 between Indianapolis, Indiana (Indianapolis International Airport) and Fort Myers, Florida (Southwest Florida International Airport).\n\nIn 1990, ATA began scheduled nonstop service from New York JFK Airport to Belfast, Northern Ireland in the United Kingdom continuing to Riga, Latvia using Boeing 757-200 aircraft. The founder of ATA is of Latvian ancestry. The service was unprofitable and was discontinued after a few years. \n\nATA performed services for the United States Department of Defense and US military during the 1991 Gulf War, transporting 108,000 military personnel on 494 missions for Operation Desert Storm and yet again during Iraqi Freedom and Enduring Freedom with the activation of the Civil Reserve Air Fleet (\"CRAF\"). During these periodic activations, ATA flight crews often and routinely spent as many as 19 hours aboard ATA aircraft in support of U.S. troops and the overall national defense missions. ATA's L-1011 aircraft were also chartered by the Department of Defense to fly personnel and their families between Philadelphia International Airport and Lajes Field, Naval Air Station Sigonella, Aviano Air Base and Incirlik Air Base (called \"Freedom Flights\" by personnel returning to the U.S. from these overseas installations).\n\nIn February 1991, ATA won a contract for daily 727-100 shuttle operations between Nellis Air Force Base and Tonopah Test Range in Nevada. This particular contract, formerly operated by defunct Key Airlines, was awarded to ATA and ended in late 1992 when Tonopah F-117 Stealth Fighter operations ceased. The 727-100s were replaced with Boeing 727-200s in 1993. Also in 1993, according to the Official Airline Guide (OAG), ATA was operating scheduled passenger service between New York JFK Airport and Lagos, Nigeria via a stop at Santa Maria Island in the Azores with a Lockheed L-1011 TriStar wide body jetliner.\n\nBy the mid-1990s, ATA began focusing on increasing its domestic scheduled services including Hawaii as well as international routes to Mexico and the Caribbean (based on leisure travel) and began using the slogan, \"On ATA, You're on Vacation.\" The airline began operating a sizable hub at Chicago Midway International Airport, and offered scheduled services throughout the United States, as well as flights to Hawaii, Mexico and the Caribbean while continuing extensive military and government contract air charter flights.\n\nATA also began a twice weekly charter service from Orlando to both London Gatwick Airport and Manchester in the UK. All services operated via Gander in eastern Canada where the aircraft would refuel. Most seats were reserved by UK tour operator Travel City Direct who specialised in Florida fly-drive vacations. Travel City boasted higher luggage allowances, complimentary meals and snacks plus leather seats. Remaining seats were often filled by American military staff.\n\nServices to the UK ceased in 2002 when Travel City switched to another airline; Air Atlanta Europe.\n\nIn 2000, ATA placed a large order for 39 new Boeing 737-800 aircraft and 12 Boeing 757-300 aircraft to expand its fleet for additional flights from Midway. That year, the airline also began scheduled flights to Mexico and was designated as a major carrier by the United States Department of Transportation.\n\nIn June 2001, ATA received the delivery of their first new aircraft, Boeing 737-800 registered as \"N301TZ\". In August of that same year, the airline received the delivery of another new type of aircraft, the Boeing 757-300; whom ATA became the North American Launch customer of this particular type. ATA's first 757-300 was registered \"N550TZ\" and the airline also introduced a new logo on these new airplanes, replacing ATA as a \"vacation airline\" and putting more emphasis on ATA as a \"business airline.\"\n\nAfter 2001 the 737-800 with their ETOPS capabilities became the fleet's mainstay of ATA's medium haul operations from the west coast to and from Hawaii and Mexico.\n\nIn 2000, ATA and Chicago Express Airlines launched ATA Connection, a regional affiliate of ATA Airlines that would link regional mid-western cities with ATA's Chicago hub and Indianapolis focus city. Chicago Express was purchased for $1.9 million on June 1, 1999, and operated as a separate subsidiary. After ATA entered bankruptcy in late 2004, a decision was made to end ATA's regional airline service and terminate Chicago Express/ATA Connection resulting in the permanent layoff of its entire staff. Chicago Express' assets were auctioned off, ATA terminated its ATA Connection turboprop service and Chicago Express ceased all operations on March 28, 2005.\n\nThe similarity of the American Trans Air and AirTran Airways names to those of other airlines caused confusion among customers and the general public. The airline had been known informally as ATA from early in its history, and from the mid-1990s on had been advertised as such, so in 2002 the name of the holding company was changed to ATA Holdings Corp. In 2003, the name of the airline itself was changed to ATA Airlines, Inc. In 2007, ATA Holdings changed names again; this time to Global Aero Logistics, Inc., immediately after the acquisition of World Air Holdings.\n\nAfter the economic upheaval caused by the 9/11 attacks upon the airline industry of the United States, ATA and its then parent company AMTRAN suffered substantial financial hardships. Among one of the small group of airlines to receive ATSB backing from the US Government, this alone proved not enough for American Trans Air to remain out of reorganization caused from the hardships and ill-timing of the refleeting to 737-800's just prior to the terrorism attacks.\n\nOn October 26, 2004, ATA Holdings and its subsidiaries filed for Chapter 11 bankruptcy protection. Eventually, shareholders of ATA Holdings stock lost all their money and received no shares. The stock, previously traded on the Nasdaq stock exchange as \"ATAH\", was delisted.\n\nIn 2004, AirTran Airways agreed to pay $90 million for ATA's 14 gates at Chicago-Midway. Southwest made a higher bid and AirTran's deal fell apart.\n\nIn December 2004, ATA entered into an agreement with Southwest Airlines to transfer six gates at Chicago Midway International Airport and 27% of non-voting stock in exchange for a cash influx and codeshare agreement.\n\nIn the beginning of 2005, the airline drastically reduced flights at its Indianapolis hub to only three destinations and centered scheduled flights at Chicago Midway International Airport in order to complement Southwest Airlines codeshare flights. ATA also focused on serving markets that were business oriented and did not have Southwest service, such as San Francisco, Dallas/Fort Worth, and New York–LaGuardia. Additionally, ATA began offering point-to-point service not connecting to its Midway Hub, as to benefit other Southwest Airlines focus cities, such as Las Vegas, Orlando, and Phoenix, with connections to non-Southwest destinations such as Denver and Hawaii. Southwest CEO, Gary Kelly, said that revenues were up nearly 20% due to the new codesharing agreement.\n\nOn March 28, 2005, ATA shut down its commuter airline service, ATA Connection operated by Chicago Express Airlines via a codeshare agreement, and later sold the assets to a private buyer. ATA Connection had initially operated British Aerospace Jetstream 31 turboprops which were then replaced with Saab 340B turboprops and was providing passenger feed for ATA at Chicago Midway Airport via a code sharing agreement. In attempt to reduce operating costs, the airline also downsized its fleet by returning twenty Boeing 737-800 and eight Boeing 757-300 aircraft, along with numerous Boeing 757-200 aircraft. The eight 757-300 airframes were subsequently refurbished by Boeing, the lessor, and then leased to Continental Airlines.\n\nIn mid-2005, ATA entered an agreement to lease three ex-United Airlines Boeing 737-300 aircraft. Three 737-300s entered service with ATA in late November 2005. Due to high lease rates, the three 737-300s were taken out of service in November 2007, and returned to their owners.\n\nIn September 2005, ATA outsourced all its Heavy Maintenance Checks to overseas and domestic contractors. Also planned was an agreement with Continental Airlines to trade ATA's remaining four 757-300 aircraft for four 737-700 aircraft. In early October 2005, ATA terminated these negotiations due to the Boeing machinists strike, which would delay the delivery of the aircraft.\n\nOn October 13, 2005, ATA announced major service reductions, ending flights to Boston, Minneapolis/St. Paul, and Newark. In addition, the planned addition of flights to Miami and Sarasota, Florida was cancelled. This ended Southwest codeshare service to Minneapolis and Newark. Later that year, on November 1, 2005, a second round of flight cuts were announced, including the suspension of scheduled service to Denver, San Juan, and their headquarters and former hub Indianapolis.\n\nOn November 17, 2005, ATA Airlines received court approval to sell its Ambassadair Travel Club division to Grueninger Cruises and Tours.\n\nIn a third round of cuts announced on December 6, 2005, ATA announced that it would discontinue service to three additional cities. ATA would suspend flights from Chicago Midway International Airport to San Francisco, Orlando, and Fort Myers in late April 2006. Following these cancellations, ATA would have only 18 daily scheduled departures from its former Chicago hub and 52 scheduled departures company-wide. Moreover, the company would be left with only 1 gate at Midway, down from its previous total of 14, surrendering the balance to Southwest or the city.\n\nOn December 15, 2005, ATA announced an expansion of its codeshare agreement with Southwest Airlines. ATA Airlines would expand codesharing with Southwest Airlines between Dallas-Fort Worth International Airport and cities in the Southwest system that connect via Chicago Midway International Airport.\n\nIn January 2006, MatlinPatterson and certain pre-bankruptcy creditors invested over $100 million in ATA and took the company private, also taking over ATA Holdings, Inc. Following the transaction, on February 28, 2006, ATA Airlines emerged from Chapter 11 bankruptcy protection. However, the airline was still shrinking. ATA continued to return more aircraft, including the 1,500th Boeing 737 Next Generation produced, N333TZ, which was delivered new to ATA on May 14, 2004.\n\nFollowing its first emergence from Chapter 11 protection ATA made several efforts to return to profitability, but due to the rising cost of fuel and negative pressures on ticket price ATA was unable to recover and ended operations on April 2, 2008. These late efforts included:\n\n\n\n\n\nOn April 2, 2008, ATA declared bankruptcy and ceased all operations. This sudden end came about after FedEx abruptly canceled ATA's longstanding military charter contract. Combined with a significant increase in the price of jet fuel, ATA did not have enough capital to stay in business. It was the third of four U.S. airlines to announce a complete shut down in the week of March 30, 2008 after Aloha Airlines did so on March 30, Minnesota-based charter Champion Air did so March 31, and Skybus Airlines terminated service on April 5. The shutdown of ATA took effect at 4:00 AM EDT, Thursday April 3, 2008, although some flights were airborne at the time and continued to their destinations, with the final arrival being ATA flight 4586 from Honolulu to Phoenix, which landed at 8:46 AM, MST, or seven hours and 46 minutes after the announced shutdown. At the time of the shutdown ATA employed around 2,300 people all of whom were permanently laid off. According to press reports, up to 10,000 passengers were affected and many of them had to scramble for help on several airlines. Most of them, however, had to pay for new tickets.\n\nOn November 19, 2008, Southwest Airlines announced their intent to acquire the remaining assets of ATA Airlines. The $7.5 million bid includes the rights to 14 slots at LaGuardia Airport that belonged to ATA, as well as various other assets such as trademarks and logos. Southwest specifically stated their bid \"doesn't include any aircraft, facilities or employees of ATA.\"\n\nAt the time of its shut down, ATA Airlines served 13 destinations throughout Mexico and the United States. All routes were discontinued on April 2, 2008 due to ATA's bankruptcy filing, with the exception of some en route red-eye flights, which arrived on April 3. With ATA's additions of Kona and Lihue, Hawaii in June 2007, the airline was serving more Hawaiian destinations nonstop from the mainland United States than any other airline in the world at the time.\nScheduled passenger service to a number of destinations listed below was discontinued prior to ATA's bankruptcy. At its peak, ATA served 35 destinations worldwide. Some destination information has been taken from ATA route maps from 1994 to 2003.\n\nCaribbean (excluding U.S. territories)\n\nIreland\n\nLatvia\n\nMexico\nNigeria\n\n\nNorthern Ireland, UK\n\nUnited States\nU.S. insular areas in the Caribbean:\n\nATA also flew charter flights to London Gatwick Airport, London Stansted Airport and Manchester, UK via technical stops in Gander, Newfoundland in Canada.\n\nCommuter air carrier Chicago Express Airlines operating as the ATA Connection served the following destinations with British Aerospace BAe Jetstream 31 and/or Saab 340B commuter turboprop aircraft from Chicago Midway Airport which was an ATA hub:\n\nOn March 15, 2008, ATA was supposed to resume service to Miami International Airport, and fly scheduled flights to Central America for the first time with the addition of flights to Guatemala City and San José, Costa Rica. USALatin Sky was to market the airline's flights to Central American destinations from Miami. As part of the deal, ATA Airlines would have based a single Boeing 737-800 aircraft in Miami to fly the routes to Guatemala City and San José. Due to problems with ATA and USALatin Sky, the service was cancelled prior to commencing, and no flights under USALatin Sky ever operated.\n\nAt the time of its shut down, ATA had 29 aircraft (3 owned, 26 leased) in its fleet. At its largest in October 2004, the company operated a fleet of 82 short, medium, and long-haul aircraft.\n\nAs of August 2009, ATA's average fleet age was 13.5 years old.\n\nATA's Boeing customer number was 3N.\n\nThe aircraft registration and tail numbers of the ATA fleet include the following significant aircraft.\n\nATA ExecuJet, a Part 135 air taxi subsidiary of ATA Holdings, the parent company of ATA Airline, also retired the following aircraft in 2001:\n\nLaunched in 2003, ATA's frequent flyer program, ATA Travel Awards, offered one of the lowest thresholds for earning travel, with the added benefit of allowing reward redemption free of blackout date restrictions. After three roundtrips booked on the company's website, customers earned a coach companion ticket on any flight operated by ATA Airlines throughout the contiguous United States. All tickets booked online received double credit toward that particular trip versus purchasing through a telephone or travel agent. While award availability to Hawaii was very limited, travelers did earn twice the normal number of credits when they purchased airfare on Hawaii-bound flights. With the shutdown of operations, ATA's frequent flyer programs were suspended and all earned points voided.\n\nAs a result of the recent enhancements in ATA's codeshare agreement with Southwest Airlines, those purchasing flights directly from ATA reservations and ata.com were given the option of earning points toward either ATA Travel Awards, or Southwest's Rapid Rewards. When ATA ceased operations, the relationship with Southwest Airlines was effectively terminated.\n\nLaunched in 2006, ATA created a unique program called FlightBank, a rewards program for the frequent traveler between the U.S. mainland and the Hawaiian Islands. For a set \"fee\", the \"bank\" provided the traveler with a flexible number of flight credits that could be used over the course of a year, at vastly reduced savings and with no blackout dates for advance bookings. [The Flightbank program was preceded by the airpass program which was run informally by Pleasant Hawaiian Holidays from year 2000 until 2006]. With the shutdown of operations, ATA's FlightBank program was also suspended.\n\nAlthough ATA Airlines was marketed and advertised as a \"low-cost carrier\", it maintained many of the features which marked this airline as full service, at least by the standards American and European travelers have become accustomed to. Unlike many discount airline carriers in Europe, ATA offered complimentary features such as window shades and reclining airline seats on all of its airplanes, leather seats on most of its airplanes, adjustable head rest \"wings\" on many of its planes, limited AVOD audio visual on demand systems, complimentary assigned seating, complimentary checked luggage, complimentary soft drinks and non-alcoholic beverage, complimentary bookings via website reservations, complimentary inter-airline baggage connection transfers, and frequent flyer programs.\n\nATA sold snacks and snack packs under the label Skyway Café. Upon military and most charter flights, ATA provided fully complimentary airline meals or depending upon flight length, snacks. On some flights ATA provided in-flight entertainment such as documentaries, comedies, \"classic television,\" music videos, and music. ATA aircraft included up to eight audio channels. Some flights over five hours included films.\n\nAt the time of its shutdown, ATA Airlines had a codeshare agreement with Southwest Airlines. As far back as 2001, ATA explored a passenger sharing agreement, with a now defunct airline called Access Air which also had midwestern United States flight operations. This agreement was short lived due to the tedious economic condition of this post deregulation \"start up\" carrier. The ATA Connection service flown by commuter air carrier Chicago Express Airlines was also operated on a codeshare basis.\n\nATA was not involved in an alliance.\n\nATA Airlines, one of Southwest Airlines' main competitors in the Chicago market, historically operated out of Midway Airport alongside Southwest. After ATA declared bankruptcy in 2004, Southwest injected capital into ATA that (among other things) would have resulted in Southwest's 27.5% ownership stake in ATA upon their exit from Chapter 11 bankruptcy proceedings.\n\nIn a departure from its traditional \"go it alone\" strategy, Southwest entered into its first domestic codesharing arrangement with ATA, which enabled Southwest Airlines to serve ATA markets in Hawaii, Washington D.C., and New York City. Some years earlier, Southwest had a short-lived traditional codeshare arrangement with Icelandair at Baltimore/Washington International Airport.\n\nIn late 2005, ATA secured $100 million in committed financing from the firm of MatlinPatterson, and Southwest's original deal with ATA was modified such that Southwest no longer retained the 27.5% stake, (or any other financial interest), in ATA. The codeshare arrangement was expanded, with some internal controversy, to include all of ATA's domestic destinations and more than 60 of Southwest's 63 destinations. In 2006, Southwest's pilot union approved a codeshare sideletter to their contract with limitations on the growth of this and other codeshare agreements. While these restrictions today are minor, outsourcing remains a growing concern in the unions' current contract negotiations.\n\nIn 2006, Southwest Airlines (Flight Code WN) began marketing ATA's two-letter TZ Coded Flights. ATA's dependence on the Southwest network continued to grow in 2006 to where ATA offered over 70 flights a week to Hawaii from Southwest's focus cities in PHX, LAS, LAX, and OAK. Additional connecting service was available to many other cities across the United States. Plans had been announced for ATA to offer exclusive international service for Southwest by 2010. In 2006, ATA announced its intention to purchase nine widebody DC-10 aircraft from Northwest Airlines. Southwest took over all ground operations for ATA at MDW, OAK, PHX, LAX, and LAS. These contracts provided that Southwest ramp personnel would now handle all ground operations for ATA, (loading of aircraft, ground servicing, etc.). The details of these contracts were not made public, but represented Southwest's and ATA's growing codeshare relationship.\n\nIn February 2005, after J. George Mikelsons stepped down as CEO of ATA Airlines, John Denison, Southwest's former Chief Financial Officer took over. Effective January 1, 2007, Denison turned things over to Subodh Karnik, who became President and Chief Executive Officer. Denison remained Chairman of Global Aero Logistics Inc., and was renamed as interim President and CEO when Subodh Karnik stepped down in March 2008.\n\nIn ATA Airlines' 35-year history, the airline had three different mainline liveries and two special liveries.\n\nATA's last livery, known as the \"Flag Livery\", was introduced when the airline announced rapid expansion in 2001. It is primarily white with \"ATA\" painted on both sides of the aircraft. The company logo was also slanted upward on the aircraft tail resembling a flag. There was a gold stripe that spanned across the outward side of the engines and nacelles, and the winglets (on the 737-800s) were blue on the outside and unpainted on the inside. This livery was first introduced on ATA's new 737-800 and 757-300 aircraft and while it has been integrated onto some of ATA's 757-200s, it was never painted on any of ATA's 727s which were retired in late 2001.\n\nAt the time of ATA's demise, many 757-200s and some Lockheed L-1011s still carried the airline's previous livery. The \"Palm Tree Livery\", which was introduced in 1996, was also primarily white with \"ATA\" painted on both sides of the aircraft; the letters were painted in a \"bubble-like\" fashion. There was a palm tree and a sun on the tail, as well as \"ATA\" in small letters. The engine nacelles were painted blue, with the outboard side of each nacelle displaying a stylized sun. This livery, introduced to emphasize ATA as a \"vacation airline\", was synonymous with the phrase \"On ATA, You're on Vacation\".\n\nATA's original livery, known as the \"Runway Livery\", was introduced when the airline began passenger service in 1981. Because ATA's first aircraft were ex-American Airlines aircraft, ATA's original livery was based on American's livery. The livery featured three stripes running the length of the aircraft in the following order: gold, white, and blue. The words \"American Trans Air\" followed by ATA's \"runway logo\" were painted above the gold stripe on the fuselage. The aircraft tail also featured gold, white, and blue stripes along the bottom with a bigger ATA runway logo in the center. All of ATA's Boeing 707s, and a majority of the airline's 727s, 757-200s, and Lockheed L-1011s wore this livery at some point. This livery was painted on every ATA aircraft until 1996, making this ATA's longest lasting livery at 15 years.\n\nThroughout ATA Airlines' history, the company had two dedicated paint schemes. In addition to these, ATA's first Boeing 737-800 had the words \"American Dream\" inscribed alongside the nose of the aircraft; it was the only aircraft in ATA's fleet to have those words written on it. Other schemes were used to celebrate the company's twenty-fifth anniversary, as well as a marketing deal with Hawaiian tour operator, Pleasant Hawaiian Holidays.\n\nIn 1998, ATA Airlines celebrated its 25th Anniversary. The airline decided to commemorate the anniversary in a big way. In addition to a year-long celebration, two separate aircraft, N772AT (a Boeing 727-200) and N520AT (a Boeing 757-200), were given a special livery which was commonly referred to as the \"25th Anniversary Scheme\". The design featured the entire airplane painted blue with \"25th Anniversary\" painted in large gold writing near the front of the aircraft fuselage. The aircraft tail featured \"ATA\" in gold lettering with pieces of confetti scattered around the ATA logo. Red, orange, pink, and yellow streamers adorned the sides of the fuselage, as well as the engines.\n\nIn 1994, ATA partnered with tour operator Pleasant Hawaiian Holidays, which was the largest tour operator flying to Hawaii. To promote the alliance, several L-1011s were adorned in a \"Hawaiian livery\". The \"Pleasant Hawaiian Holidays\" livery has appeared only on two types of ATA's aircraft; the Lockheed L-1011, and later, the Boeing 757-300. The livery had two different forms; the more extravagant was painted on the Lockheed L-1011s. The livery was primarily white and featured \"ATA\" in big bubble letters near the front of the aircraft, and \"Pleasant Hawaiian Holidays\" spelled out after \"ATA\" near the top of the fuselage. Like the mainline livery, the engines were painted with a sun. The livery basically resembled the \"Palm Tree Livery\" that mainline aircraft adorned at the time. There was one big difference. \"Hawaii\" was spelled out in large letters horizontally across the aircraft tail. After the L-1011s were removed from scheduled service in 2002, two Boeing 757-300s were painted in the Pleasant Hawaiian Holidays livery; the elaborate TriStar livery was replaced with a toned-down livery. The new livery was ATA's current livery with the words \"Pleasant Holidays\" painted in small letters near the front of the fuselage. This livery and ATA's partnership with Pleasant Hawaiian Holidays was terminated in 2005 when Pleasant Hawaiian Holidays signed a larger network deal with United Airlines.\n\nOn May 12, 1996, a Boeing 727-290, N775AT, operated as American Trans Air flight 406, experienced a decompression at 33,000 feet. The flight which was bound for St. Petersburg, Florida, made an emergency landing at the Indianapolis International Airport, Indianapolis, Indiana.\n\nOn August 10, 1986, an ATA DC-10-40 parked at the ramp at Chicago O'Hare International Airport was destroyed by fire due to a mishandled loose oxygen canister. There were no fatalities.\n\n"}
{"id": "21916864", "url": "https://en.wikipedia.org/wiki?curid=21916864", "title": "Adar, Inc", "text": "Adar, Inc\n\nAdar, Inc. is an Information Technology (IT) company headquartered in Chicago, United States. The company provides streaming IT and IT-as-a-service to small and medium-sized enterprises (SMEs). Adar, Inc. is known for its comprehensive cloud IT brand of platform, Nerdio. It currently offers two products-Nerdio Private Cloud and Nerdio for Azure. \n\nAdar, Inc. is a parent company that was founded in 2005 by Vadim Vladimirskiy, Stuart Gabel and Niall Keegan to provide online backup systems to SMEs. Soon after it was founded, the company transitioned into and managing virtual infrastructure. \n\nIn 2007, it introduced Adar Private Cloud. \n\nIn January 2014, the company secured $2.4 million in Series A funding from MK Capital, which allowed it to expand and improve its products and services.\n\nAdar Private Cloud was nominated for the CityLIGHTS Rising Star Award in July 2015. One year later, the company rebranded and launched Nerdio Private Cloud. \n\nIn 2017, the company introduced Nerdio for Azure. The company’s CEO, Vadim Vladimirskiy, was nominated for the CityLIGHTS Technologist of the Year Award in 2018. \n\nNerdio Private Cloud delivers comprehensive IT infrastructure, including virtual hardware, software, security backup and disaster recovery, and complete IT support for managed service providers and SMBs. It uses VMware vSphere technology to create virtual server infrastructure in a cloud-hosted environment. Access is provided through Virtual Desktop Infrastructure (VDI). A golden image is created to standardize the virtualdesktop environment, which includes all applications needed to access network resources. Nerdio's data centers natively encrypt data-at-rest and use military-grade physical security measures.\n\nOne of Nerdio Private Cloud’s key features is its centralized management platform, where partners and IT administrators can fully manage any IT environment from one central location, all within three clicks. Software is collated and managed by Nerdio and its partners, including patches, upgrades, and compatibility tests. The virtual and physical architecture of each client’s configuration is constantly monitored for performance and anomalies. Any problems are investigated and corrected\n\nNerdio Private Cloud can be used from any location and device with internet access. It also provides a network infrastructure with a 10GbE network backbone to eliminate any noticeable latency. A private VLAN and isolated server environment is created for each company, including a virtual firewall, which supports intrusion prevention, content filtering and gateway anti-877 malware. Encrypted Virtual Private Network (VPN) tunnels are configured between the data center and client offices to facilitate secure, remote access to network resources.\n\nNerdio Private Cloud has three service plans: Professional, Performance, and Enterprise. EIn addition to the Nerdio Private Cloud plans, customers can add a help desk support plan provided by Nerdio partners.\n\nIt includes the following technical specifications:\n\nThe servers provide the following capabilities to users:\n\n\n\n\n\nNerdio for Azure is an IT automation technology that delivers easy provisioning, management, and cost-optimization of virtual, desktop-centric environments on the Microsoft Cloud. It also automates complete IT environments on Azure, provides services dedicated to DaaS, RDS and LOB app server workloads, autoscales Remote Desktop Services (RDS) collections. Customers can set parameters for scaling, which then takes place automatically and keeps hosts on standby. \n\nNerdio for Azure is a toolset to help Manage Service Providers(MSPs) get their customers up and running on an ITaaS stage based on Azure. The solution provides discovery tools to record the existing IT environment, create a new IT environment, migrate users and data via onboarding tools, automate complete system backups, auto-scale computer resources and monitor desktop performance by measuring the end-user experience.\nAdditional products for NFA include the Azure Cost Estimator. This web-based tool lets anyone calculate the costs of running IT in Microsoft’s public cloud. \n\n\n"}
{"id": "2782877", "url": "https://en.wikipedia.org/wiki?curid=2782877", "title": "Address Verification System", "text": "Address Verification System\n\nThe Address Verification System (AVS) is a system used to verify the address of a person claiming to own a credit card. The system will check the billing address of the credit card provided by the user with the address on file at the credit card company. The other security features for the credit card include the CVV2 number.\n\nAVS is used when the merchant verifies credit card data, such as billing address and ZIP code, against the Visa/MasterCard billing information of the cardholder. AVS verifies that the billing address of the credit or debit card matches the address that was given by the customer. Because AVS only verifies the numeric portion of the address, certain anomalies like apartment numbers can cause false declines; however, it is reported to be a rare occurrence.\n\nAVS verifies the numeric portions of a cardholder's billing address. For example, if the address is 101 Main Street, Highland, CA 92346, in the United States, AVS will check \"101\" and \"92346\". Cardholders may receive false negatives, or partial declines for AVS from e-commerce verification systems, which may require manual overrides, voice authorization, or reprogramming of the AVS entries by the card issuing bank.\n\nAVS is a service to combat fraudulent activity for non-face-to-face transactions by cross-referencing the cardholder’s address information with the card issuer’s records. AVS is widely supported by Visa, MasterCard and American Express in the USA, Canada and United Kingdom.\n\nCardholders with a bank that does not support AVS may receive an error from Internet stores due to the lack of data.\n\nBesides the automated verification, some banks do provide merchants with a manual verification system. Usually this is done for foreign credit card accounts as the AVS only works in the same country. This facility helps the merchants to prevent fraud arising from other countries. The merchant's bank calls the customer banks (or send a fax for banks that request them). Some countries like Denmark however prevent banks from verifying customer data.\n\n\"Declined due to AVS mismatch\", the authorization code, along with the hold on the authorized funds, will remain on the customer's card until the merchant account reverses the authorization or the issuing bank have it expire (typically 7 day for all business types except hotels and car rentals that can have up to 30 days). As a result, the held funds may be subtracted from the customer's available balance, and an online statement may reflect the authorization request which might be mistaken for an actual charge. Most card issuing banks will remove authorizations within 1–2 days if they are not claimed for settlement.\n"}
{"id": "21043943", "url": "https://en.wikipedia.org/wiki?curid=21043943", "title": "Air-Britain", "text": "Air-Britain\n\nAir-Britain, traditionally sub-titled \"The International Association of Aviation Enthusiasts\", is a non-profit aviation society founded in July 1948. In April 1968, the association was incorporated into a company limited by guarantee, Air-Britain (Historians) Ltd. With over 4,000 members worldwide, Air-Britain publishes books about worldwide civilian and military aviation, that are sold via bookshops, air show stands, mail order and via a secure sales website. Four regular journals are published, with coverage of both current and historical aviation themes. Air-Britain periodically organises events such as aircraft fly-ins and aircraft recognition competitions.\n\nOn 16 April 2015, the status of Air-Britain changed from a Private company limited by guarantee, in the form of Air-Britain (Historians) Ltd, to a British charity, in the form of Air-Britain Trust Ltd.\n\nArchived scans of many of these are progressively being made available as PDF files on CDs.\n\n\n"}
{"id": "15140187", "url": "https://en.wikipedia.org/wiki?curid=15140187", "title": "CashPool", "text": "CashPool\n\nCashPool is a cooperation of a multitude of smaller or virtual German private banks, in which they mutually waive ATM usage fees for their customers. It is not an interbank network but uses the pre-existing German ATM or Maestro/Cirrus networks. With more than 3200 ATMs, the cooperating banks' ATM networks form the smallest ATM group in Germany.\n\nThe cooperation was founded in 2000. Its primary competitor in Germany is Cash Group.\n\nMost banks in Germany, while connected through the German ATM network, charge ATM usage fees for customers of other banks.\n\nIn 1998, the six largest German private banks established Cash Group, mutually waiving these fees within the Group.\n\nAfter the formation, other private banks tried to join Cash Group but were not accepted into the Group. Being smaller than the six large private banks, they operated fewer ATMs and thus would have unilaterally benefited from the use of the other bank's larger networks.\n\nAs a consequence, several of these smaller banks founded CashPool and also mutually waived ATM usage fees within the group. For comparison, the big banks CashGroup network has 9,000 ATMs, the co-operative banks (as far as being members of the Bankcard-Servicenetz) share 18,600 ATMs and the saving banks have list of 25,700 ATMs for their SparkassenCard.\n\n\n\n"}
{"id": "5736", "url": "https://en.wikipedia.org/wiki?curid=5736", "title": "Catherine Coleman", "text": "Catherine Coleman\n\nCatherine Grace \"Cady\" Coleman (born December 14, 1960) is an American chemist, a former United States Air Force officer, and a former NASA astronaut. She is a veteran of two Space Shuttle missions, and departed the International Space Station on May 23, 2011, as a crew member of Expedition 27 after logging 159 days in space.\n\nColeman graduated from Wilbert Tucker Woodson High School, Fairfax, Virginia, in 1978; in 1978–1979, she was an exchange student at Røyken upper secondary school in Norway with the AFS Intercultural Programs. She received a B.S. degree in chemistry from the Massachusetts Institute of Technology in 1983, and a Ph.D. degree in polymer science and engineering from the University of Massachusetts Amherst in 1991 as a member of the Air Force ROTC. She was advised by Professor Thomas J. McCarthy on her doctorate. She was a member of the intercollegiate crew and was a resident of Baker House.\n\nAfter completing her regular education, Coleman joined the U.S. Air Force as a Second Lieutenant while continuing her graduate work for a PhD at the University of Massachusetts Amherst. In 1988 she entered active duty at Wright-Patterson Air Force Base as a research chemist. During her work she participated as a surface analysis consultant on the NASA Long Duration Exposure Facility experiment. In 1991, she received her doctorate in polymer science and engineering. She retired from the Air Force in November 2009.\n\nColeman was selected by NASA in 1992 to join the NASA Astronaut Corps. In 1995, she was a member of the STS-73 crew on the scientific mission USML-2 with experiments including biotechnology, combustion science, and the physics of fluids. During the flight, she reported to Houston Mission Control that she had spotted an unidentified flying object. She also trained for the mission STS-83 to be the backup for Donald A. Thomas; however, as he recovered on time, she did not fly that mission. STS-93 was Coleman's second space flight in 1999. She was mission specialist in charge of deploying the Chandra X-ray Observatory and its Inertial Upper Stage out of the shuttle's cargo bay.\n\nColeman served as Chief of Robotics for the Astronaut Office, to include robotic arm operations and training for all Space Shuttle and International Space Station missions. In October 2004, Coleman served as an aquanaut during the mission aboard the Aquarius underwater laboratory, living and working underwater for eleven days.\n\nColeman was assigned as a backup U.S. crew member for Expeditions 19, 20 and 21 and served as a backup crew member for Expeditions 24 and 25 as part of her training for Expedition 26.\n\nColeman launched on December 15, 2010 (December 16 Baikonur time), aboard Soyuz TMA-20 to join the Expedition 26 mission aboard the International Space Station. She retired from NASA on December 1, 2016.\n\nSTS-73 on Space Shuttle \"Columbia\" (October 20 to November 5, 1995) was the second United States Microgravity Laboratory mission. The mission focused on materials science, biotechnology, combustion science, the physics of fluids, and numerous scientific experiments housed in the pressurized Spacelab module. In completing her first space flight, Coleman orbited the Earth 256 times, traveled over 6 million miles, and logged a total of 15 days, 21 hours, 52 minutes and 21 seconds in space.\n\nSTS-93 on \"Columbia\" (July 22 to 27, 1999) was a five-day mission during which Coleman was the lead mission specialist for the deployment of the Chandra X-ray Observatory. Designed to conduct comprehensive studies of the universe, the telescope will enable scientists to study exotic phenomena such as exploding stars, quasars, and black holes. Mission duration was 118 hours and 50 minutes.\n\nSoyuz TMA-20 / Expedition 26/27 (December 15, 2010, to May 23, 2011) was an extended duration mission to the International Space Station.\n\nColeman is married to glass artist Josh Simpson who lives in Massachusetts. They have one son. She is part of the band Bandella, which also includes fellow NASA astronaut Steven Robinson, Canadian astronaut Chris Hadfield, and Micki Pettit (astronaut Don Pettit's wife). Coleman is a flute player and has taken several flutes with her to the ISS, including a pennywhistle from Paddy Moloney of the Chieftains, an old Irish flute from Matt Molloy of the Chieftains, and a flute from Ian Anderson of Jethro Tull. On February 15, 2011, she played one of the instruments live from orbit on National Public Radio. On April 12, 2011, she played live through video link for the audience of Jethro Tull's show in Russia in honour of the 50th anniversary of Yuri Gagarin's flight. She played the duet from orbit while Anderson played on the ground in Russia. On May 13 of that year, Coleman delivered a taped commencement address to the class of 2011 at the University of Massachusetts Amherst.\n\nAs do many other astronauts, Coleman holds an amateur radio license (callsign: KC5ZTH).\n\nAs of 2015, she is also a member of the board of directors for the Hollywood Science Fiction Museum.\n\nAs of 2015 she is also known to be working as a guest speaker at the Baylor College of Medicine, for the children's program 'Saturday Morning Science'.\n\n"}
{"id": "18584194", "url": "https://en.wikipedia.org/wiki?curid=18584194", "title": "Change detection (GIS)", "text": "Change detection (GIS)\n\nChange detection for GIS (geographical information systems) is a process that measures how the attributes of a particular area have changed between two or more time periods. Change detection often involves comparing aerial photographs or satellite imagery of the area taken at different times. Change detection has been widely used to assess shifting cultivation, deforestation, urban growth, impact of natural disasters like tsunamis, earthquakes, and use/land cover changes etc.\n\n"}
{"id": "7613845", "url": "https://en.wikipedia.org/wiki?curid=7613845", "title": "Design brief", "text": "Design brief\n\nA Design brief is a document for a design project developed by a person or team (the 'designer' or 'design team') in consultation with the 'client'. They outline the deliverables and scope of the project including any products or works (function and aesthetics), timing and budget. They can be used for many projects including those in the fields of architecture, interior design and industrial design. Design briefs are also used to evaluate the effectiveness of a design after it has been produced and during the creation process to keep the project on track and on budget. Some firms rely on them more than others but there is a move towards greater accountability in the design process and thus many people find them most useful. They usually change over time and are adjusted as the project scope evolves. Often they are 'signed off' by the client and designer at set stages in the project.\n\nA design brief may use the following layout:\n\n\n"}
{"id": "43472010", "url": "https://en.wikipedia.org/wiki?curid=43472010", "title": "Federated presence", "text": "Federated presence\n\nFederated Presence, also known as Federated Presence Management (FPM) is a technology that allows users to communicate via IM (instant messaging), text messaging, etc. across multiple platforms and devices.\n"}
{"id": "13276035", "url": "https://en.wikipedia.org/wiki?curid=13276035", "title": "Fire plough", "text": "Fire plough\n\nA fire plough (or fire plow) is a firelighting tool. In its simplest form, it is two sticks rubbed together. Rubbing produces friction and heat, and eventually an ember. More advanced are \"stick-and-groove\" forms, which typically uses a V-shaped base piece of wood, and a \"friction stick\" as the activator.\n\nThe typical fire plough consists of a stick cut to a dull point, and a long piece of wood with a groove cut down its length. The point of the first piece is rubbed quickly against the groove of the second piece in a \"plowing\" motion, to produce hot dust that then becomes a coal. A split is often made down the length of the grooved piece, so that oxygen can flow freely to the coal/ember. Once hot enough, the coal is introduced to the tinder, more oxygen is added by blowing and the result is ignition.\n"}
{"id": "7395285", "url": "https://en.wikipedia.org/wiki?curid=7395285", "title": "Food Technology Industrial Achievement Award", "text": "Food Technology Industrial Achievement Award\n\nThe Food Technology Industrial Achievement Award has been awarded by the Institute of Food Technologists since 1959. It is awarded for development of an outstanding food process or product that represents a significant advance in the application of food technology to food production. The process or product must have been successfully applied in an actual commercial operations between six months and seven years before December 1 in the year of the nomination.\n\nSponsored by \"Food Technology\" magazine, award winners receive a plaque from IFT.\n\n"}
{"id": "23435006", "url": "https://en.wikipedia.org/wiki?curid=23435006", "title": "Geospatial predictive modeling", "text": "Geospatial predictive modeling\n\nGeospatial predictive modeling is conceptually rooted in the principle that the occurrences of\nevents being modeled are limited in distribution. Occurrences of events are neither uniform\nnor random in distribution – there are spatial environment factors (infrastructure, sociocultural,\ntopographic, etc.) that constrain and influence where the locations of events occur.\nGeospatial predictive modeling attempts to describe those constraints and influences by\nspatially correlating occurrences of historical geospatial locations with environmental factors\nthat represent those constraints and influences. Geospatial predictive modeling is a process\nfor analyzing events through a geographic filter in order to make statements of likelihood for\nevent occurrence or emergence.\nThere are two broad types of geospatial predictive models: deductive and inductive.\n\nThe deductive method relies on qualitative data or a subject matter expert (SME) to describe\nthe relationship between event occurrences and factors that describe the environment. As a\nresult, the deductive process generally will rely on more subjective information. This means\nthat the modeler could potentially be limiting the model by only inputting a number of factors that the human brain can comprehend.\n\nAn example of a deductive model is as follows:\nSets of events are typically found …\n\nIn this deductive model, high suitability locations for the set of events are constrained and\ninfluenced by non-empirically calculated spatial ranges for airports, land cover, and elevation: lower\nsuitability areas would be everywhere else. The accuracy and detail of the deductive model is\nlimited by the depth of qualitative data inputs to the model.\n\nThe inductive method relies on the empirically-calculated spatial relationship between\nhistorical or known event occurrence locations and factors that make up the environment\n(infrastructure, socio-culture, topographic, etc.). Each event occurrence is plotted in\ngeographic space and a quantitative relationship is defined between the event occurrence\nand the factors that make up the environment. The advantage of this method is that software\ncan be developed to empirically discover – harnessing the speed of computers, which is\ncrucial when hundreds of factors are involved – both known and unknown correlations\nbetween factors and events. Those quantitative relationship values are then processed by a\nstatistical function to find spatial patterns that define high and low suitability areas for event\noccurrence.\n\n"}
{"id": "32846840", "url": "https://en.wikipedia.org/wiki?curid=32846840", "title": "Grattoir de côté", "text": "Grattoir de côté\n\nA Grattoir de côté (translates from French as Side Scraper) is an archaeological term for a ridged variety of steep-scrapers distinguished by a working edge on one side. They were found at various archaeological sites in Lebanon including Ain Cheikh and Jdeideh II and are suggested to date to Upper Paleolithic stages three or four (Antelian).\n"}
{"id": "147473", "url": "https://en.wikipedia.org/wiki?curid=147473", "title": "HIV vaccine", "text": "HIV vaccine\n\nAn HIV vaccine may have the purpose of protecting individuals who do not have HIV from being infected with the virus (a preventive vaccine), or treating an HIV-infected person (a therapeutic vaccine). There are two approaches to an HIV vaccine: an active vaccination approach in which a vaccine aims to induce an immune response against HIV; and a passive vaccination approach in which preformed antibodies against HIV are administered.\n\nCurrently, there is no licensed HIV vaccine on the market but multiple research projects are trying to find an effective vaccine. There is evidence from humans that a vaccine may be possible. Some, but certainly not all, HIV-infected individuals naturally produce broadly neutralizing antibodies which keep the virus suppressed and these people remain asymptomatic for decades. Potential broadly neutralizing antibodies have been cloned in the laboratory (monoclonal antibodies) and are being tested in passive vaccination clinical trials.\n\nMany trials have shown no efficacy but thus far, one HIV vaccine regimen, RV 144, has been shown to prevent HIV in some individuals in Thailand.\n\nThe urgency of the search for a vaccine against HIV stems from the AIDS-related death toll of over 25 million people since 1981. In 2002, AIDS became the primary cause of death due to an infectious agent in Africa.\n\nAlternative medical treatments to a vaccine exist. For the treatment of HIV-infected individuals, Highly Active Antiretroviral Therapy (HAART) medication has been demonstrated to provide many benefits to HIV-infected individuals, including improved health, increased lifespan, control of viremia, and prevention of transmission to babies and partners. HAART must be taken lifelong without interruption to be effective, and cannot cure HIV. Options for the prevention of HIV infection in HIV-uninfected individuals include safer sex (for example abstinence, partner reduction and condom use), antiretroviral strategies (pre-exposure prophylaxis and post-exposure prophylaxis) and medical male circumcision. Vaccination has proved a powerful public health tool in vanquishing other diseases, and an HIV vaccine is generally considered as the most likely, and perhaps the only way by which the HIV pandemic can be halted. However, remains a challenging target for a vaccine.\n\nIn 1984, after the confirmation of the etiological agent of AIDS by scientists at the U.S. National Institutes of Health and the Pasteur Institute, the United States Health and Human Services Secretary Margaret Heckler declared that a vaccine would be available within two years. However, the classical vaccination approach that is successful in the control of other viral diseases - priming the adaptive immunity to recognize the viral envelope proteins - have failed to work against HIV. Many factors make the development of an HIV vaccine different to other classic vaccines:\n\n\nThe epitopes of the viral envelope are more variable than those of many other viruses. Furthermore, the functionally important epitopes of the gp120 protein are masked by glycosylation, trimerisation and receptor-induced conformational changes making it difficult to block with neutralizing antibodies.\n\nThe ineffectiveness of previously developed vaccines primarily stems from two related factors:\n\nThe difficulties in stimulating a reliable antibody response has led to the attempts to develop a vaccine that stimulates a response by cytotoxic T-lymphocytes.\n\nAnother response to the challenge has been to create a single peptide that contains the least variable components of all the known HIV strains.\n\nThe typical animal model for vaccine research is the monkey, often the macaque. Monkeys can be infected with SIV or the chimeric SHIV for research purposes. However, the well-proven route of trying to induce neutralizing antibodies by vaccination has stalled because of the great difficulty in stimulating antibodies that neutralise heterologous primary HIV isolates. Some vaccines based on the virus envelope have protected chimpanzees or macaques from homologous virus challenge, but in clinical trials, humans who were immunised with similar constructs became infected after later exposure to HIV-1.\n\nThere are some differences between SIV and HIV that may introduce challenges in the use of an animal model. The animal model can be extremely useful but at times controversial.\n\nThere is a new animal model strongly resembling that of HIV in humans. Generalized immune activation as a direct result of activated CD4+ T cell killing - performed in mice allows new ways of testing HIV behaviour.\n\nNIAID-funded SIV research has shown that challenging monkeys with a cytomegalovirus (CMV)-based SIV vaccine results in containment of virus. Typically, virus replication and dissemination occurs within days after infection, whereas vaccine-induced T cell activation and recruitment to sites of viral replication takes weeks. Researchers hypothesized that vaccines designed to maintain activated effector memory T cells might impair viral replication at its earliest stage.\n\nSeveral vaccine candidates are in varying phases of clinical trials.\n\nMost initial approaches have focused on the HIV envelope protein. At least thirteen different gp120 and gp160 envelope candidates have been evaluated, in the US predominantly through the AIDS Vaccine Evaluation Group. Most research focused on gp120 rather than gp41/gp160, as the latter are generally more difficult to produce and did not initially offer any clear advantage over gp120 forms. Overall, they have been safe and immunogenic in diverse populations, have induced neutralizing antibody in nearly 100% recipients, but rarely induced CD8+ cytotoxic T lymphocytes (CTL). Mammalian derived envelope preparations have been better inducers of neutralizing antibody than candidates produced in yeast and bacteria. Although the vaccination process involved many repeated \"booster\" injections, it was very difficult to induce and maintain the high anti-gp120 antibody titers necessary to have any hope of neutralizing an HIV exposure.\n\nThe availability of several recombinant canarypox vectors has provided interesting results that may prove to be generalizable to other viral vectors. Increasing the complexity of the canarypox vectors by including more genes/epitopes has increased the percent of volunteers that have detectable CTL to a greater extent than did increasing the dose of the viral vector. CTLs from volunteers were able to kill peripheral blood mononuclear cells infected with primary isolates of HIV, suggesting that induced CTLs could have biological significance. In addition, cells from at least some volunteers were able to kill cells infected with HIV from other clades, though the pattern of recognition was not uniform among volunteers. The canarypox vector is the first candidate HIV vaccine that has induced cross-clade functional CTL responses. The first phase I trial of the candidate vaccine in Africa was launched early in 1999 with Ugandan volunteers. The study determined the extent to which Ugandan volunteers have CTL that are active against the subtypes of HIV prevalent in Uganda, A and D. In 2015, a Phase I trial called HVTN 100, chaired by two South African researchers Linda-Gail Bekker and Fatima Laher, tested the combination of a canarypox vector and a gp120 protein adapted for the subtype C HIV virus common in sub-Saharan Africa, and initial results showed that those who received the vaccine regimen produced strong immune responses early on.\n\nOther strategies that have progressed to phase I trials in uninfected persons include peptides, lipopeptides, DNA, an attenuated Salmonella vector, p24, etc. Specifically, candidate vaccines that induce one or more of the following are being sought:\n\n\nIn 2011, researchers in National Biotech Centre in Madrid unveiled data from the Phase I clinical trial of their new vaccine, MVA-B. The vaccine was effective in inducing an immunological response in 92% of the healthy subjects.\n\nIn 2016, results were published of the first Phase I human clinical trial of a killed whole-HIV-1 vaccine, SAV001. HIV used in the vaccine was chemically and physically deadened through radiation. The trial, conducted in Canada in 2012, demonstrated a good safety profile and elicited antibodies to HIV-1. According to Dr. Chil-Yong Kang of Western University's Schulich School of Medicine & Dentistry in Canada, the developer of this vaccine, antibodies against gp120 and p24 increased to 8-fold and 64-fold, respectively after vaccination.\n\n\"Preventative HIV vaccines\"\n\n\n\"Therapeutic HIV vaccines\"\n\nBiosantech developed a therapeutic vaccine called Tat Oyi, which targets the tat protein of HIV. It was tested in France in a double-blind Phase I/II trial with 48 HIV-positive patients who had reached viral suppression on Highly Active Antiretroviral Therapy and then stopped antiretrovirals after getting the intradermal Tat Oyi vaccine.\n\n\"Preventative HIV vaccines\"\n\nThere have been no passive preventative HIV vaccines to reach Phase III yet, but some active preventative HIV vaccine candidates have reached Phase III.\n\n\n\"Therapeutic HIV vaccines\"\n\nNo therapeutic HIV vaccine candidates have reached phase 3 testing yet.\n\nA July 2012 report of the HIV Vaccines & Microbicides Resource Tracking Working Group estimates that $845 million was spent on AIDS vaccine research in 2011.\n\nEconomic issues with developing an AIDS vaccine include the need for advance purchase commitment (or advance market commitments) because after an AIDS vaccine has been developed, governments and NGOs may be able to bid the price down to marginal cost.\n\nTheoretically, any possible HIV vaccine must inhibit or stop the HIV virion replication cycle. The targets of a vaccine could be the following stages of the HIV virion cycle:\n\nTherefore, the following list comprises the current possible approaches for an HIV vaccine:\n\n\n\nHere, “damage” means inhibiting or stopping the ability of virion to process any of the \"Phase II-VII\". Here are the different classification of methods:\n\n\n\nInhibiting the life functions of infected cells:\n\nThere have been reports that HIV patients coinfected with hepatitis G virus, also called GB virus type C (GBV-C), can survive longer than those without GBV-C, but the patients may be different in other ways. GBV-C is potentially useful in the future development of an HIV vaccine.\n\nLive attenuated vaccines are highly successful against polio, rotavirus and measles, but has not been tested against HIV in humans. Reversion to live virus has been a theoretical safety concern that has to date prevented clinical development of a live attenuated HIV-1 vaccine. Scientists are researching novel strategies to develop a non-virulent live attenuated HIV-1 vaccine. For example, a genetically modified form of HIV has been created in which the virus' codons (a sequence of three nucleotides that form genetic code) are manipulated to rely on an unnatural amino acid for proper protein translation, which allows it to replicate. Because this amino acid is foreign to the human body, the virus cannot reproduce.\n\nScientists at The Scripps Research Institute found a way to attach HIV-fighting antibodies to immune cells, creating a HIV-resistant cell population.\n\n\n"}
{"id": "39217725", "url": "https://en.wikipedia.org/wiki?curid=39217725", "title": "Hacking Health", "text": "Hacking Health\n\nHacking Health is a social organization that pairs innovators with healthcare experts to build solutions to front-line healthcare problems through the use of emerging technology.\n\nThe organization started off in Montreal, Quebec, Canada in 2012 with a weekend-long hackathon to encourage collaboration between healthcare professionals and IT experts. Since then, Hacking Health events have been held in cities across Canada, the USA and internationally.\n\nHeld over a weekend, Hacking Health hackathons consist of 200-300 participants where designers and developers collaborate with doctors, nurses, clinic managers and other healthcare professionals to develop prototypes that can be put to test in clinics and hospitals. The event also attracts industry professionals, venture capitalists and entrepreneurs.\n\nHeld typically in the evening, Hacking Health Café meetups are regular events to bring together entrepreneurs in the healthcare industry and foster relationships between technology talent and healthcare experts. These shorter, casual events help interested parties stay up-to-date on local collaborations, projects and start-ups.\n\n\nHacking Health has spread globally since 2013 to Cape Town, Strasbourg, Hong Kong, Zurich, Bucharest, and Detroit.\n\n2013\n\nStrasbourg (France)\n\n2014\n\n\n2015\n\n\n2016\n\n\n2017\n\n\n"}
{"id": "850959", "url": "https://en.wikipedia.org/wiki?curid=850959", "title": "Hexanitrobenzene", "text": "Hexanitrobenzene\n\nHexanitrobenzene, also known as HNB, is a high-density explosive compound with chemical formula CNO, obtained by oxidizing the amine group of pentanitroaniline with hydrogen peroxide in sulfuric acid.\n\nHNB has the undesirable property of being moderately sensitive to light and therefore hard to utilize safely. It is not currently used in any production explosives applications, though it is used as a precursor chemical in one method of production of TATB, another explosive.\n\nHNB was experimentally used as a gas source for explosively pumped gas dynamic laser. In this application, HNB and tetranitromethane are preferred to more conventional explosives because the explosion products CO and N are a simple enough mixture to simulate gas dynamic processes and quite similar to conventional gas dynamic laser medium. The water and hydrogen products of many other explosives could interfere with vibrational states of CO in this type of laser.\n\nDuring World War II a method of synthesis of hexanitrobenzene was suggested in Germany, and the product was supposed to be manufactured on a semi-industrial scale according to the following scheme:\nComplete nitration of benzene is practically impossible, because the nitro groups are deactivating groups for further nitration.\n\n\n\n"}
{"id": "2457359", "url": "https://en.wikipedia.org/wiki?curid=2457359", "title": "Horse collar", "text": "Horse collar\n\nA horse collar is a part of a horse harness that is used to distribute the load around a horse's neck and shoulders when pulling a wagon or plough. The collar often supports and pads a pair of curved metal or wooden pieces, called hames, to which the traces of the harness are attached. The collar allows the horse to use its full strength when pulling, essentially enabling the animal to push forward with its hindquarters into the collar. If wearing a yoke or a breastcollar, the horse had to pull with its less-powerful shoulders. The collar had another advantage over the yoke as it reduced pressure on the horse's windpipe. \n\nFrom the time of the invention of the horse collar, horses became more valuable for plowing and pulling. When the horse was harnessed in the collar, the horse could apply 50% more power to a task in a given time period than could an ox, due to the horse's greater speed. Additionally, horses generally have greater endurance than oxen, and thus can work more hours each day. The importance and value of horses as a resource for improving agricultural production increased accordingly. \n\nThe horse collar was very important to the development of many areas of the world. Wherever oxen were used and could be replaced with horses, the use of horses boosted economies, and reduced reliance on subsistence farming. This allowed people more free time to take on specialized activities, and consequently to the development of early industry, education, and the arts in the rise of market-based towns.\n\nA horse collar is oval rather than circular and it is by design not very flexible. It is a padded appliance that conforms well to the shape of the horse's body. It is constructed so that at all points of contact with the body of the horse it avoids the air passage. By protecting the airway of the horse it became possible for the animal to use its full force to pull a load.\n\nLong before the horse collar harness, there was the less efficient throat-girth harness. This, it was claimed, could be found in many ancient civilizations, according to early 20th century French cavalry officer Lefebvre des Noëttes. This type of collar was supposedly used in ancient Chaldea (3rd millennium BC), both Sumeria and Assyria (1400–800 BC), ancient Egypt during the New Kingdom (1570–1070 BC), Shang Dynasty China (1600–1050 BC), Minoan Crete (2700–1450 BC), Classical Greece (550–323 BC), and ancient Rome (510 BC–476 AD). With this \"ancient harness,\" ploughs and carts were pulled using harnesses that had flat straps across the neck and chest of the animal, with the load attached at the top of the collar, above the neck, in a manner similar to a yoke. These straps pressed against the horse's sterno-cephalicus muscle and trachea which restricted its breathing and reducing the pulling power of the horse. Thus, the harder a horse pulled, the more strongly it choked off its own breathing. Because of these supposed physical constraints, oxen were used in preference to horses for heavy work, as they do not have this problem due to anatomical differences and could be yoked to their loads.\n\nIn 1972, Spruytte published \"Ancient Harness Systems\" which argued that there were at least three ancient traction systems shown in art, none of which choked the horses. The shoulder traction (ancient Egyptian) and breast traction (Greek and Roman) artwork had been mis-seen and mis-drawn as a composite that matched neither. This he sought to demonstrate by building reproduction chariots and harness, and running them with suitable teams. These had to be borrowed ponies as horses were too large for the surviving Egyptian chariot he used as a model.\n\nThe throat-girth design was not improved until the Chinese breast-strap or \"breastcollar\" harness developed during the Warring States (481–221 BC) era in China. The Chinese breast harness became known throughout Central Asia by the 7th century, introduced to Europe by the 8th century.\n\nIts first depiction in artwork was on lacquer-ware boxes from the ancient State of Chu. This type of harness put pressure upon the sternum, where the line of traction is directly linked with the skeletal system of the horse, allowing for nearly full exertion. It was in universal use by the time of the Chinese Han Dynasty (202 BC–220 AD), depicted in artwork of hundreds of different carvings, stone reliefs, and stamped bricks showing it featured on horses pulling chariots. This type of breast-strap harness became known in Central Asia and elsewhere with the Avars, Magyars, Bohemians, Poles, and Russians during the 7th to 10th centuries. After Central Asia, the first breast-strap harness was spread to Europe by the 8th century (in depicted artwork), and became more widespread by the following 9th century (for example, depicted in a tapestry of the Oseberg ship burial).\n\nThe problem with a breastcollar harness was that the actual shafts of the cart, chariot, or other vehicle are attached to a surcingle around the barrel of the horse. The breastplate primarily kept the surcingle from slipping back, not as the primary pushing object. This results in the horse literally pulling the load, a less efficient use of the animal. The modern breastcollar has traces which transfer the pull directly from the breastcollar, but a horse collar still is more effective for pulling heavy loads.\n\nAfter the breastcollar harness, the next and final evolutionary stage was the collar harness. The collar allows a horse to use its full strength when pulling, essentially allowing the horse to push forward with its hindquarters into the collar. The fully developed collar harness was developed in Southern and Northern Dynasties China during the 5th century AD. The first questionable depiction of it in art appears on painted moulded-bricks in the Three Kingdoms (220–265 AD) era tomb of Bao Sanniang at Zhaohua, Sichuan province, China. These paintings display an amply padded horse collar with no sign of a yoke. However, the earliest legitimate depiction of it in art is on a Dunhuang cave mural (cave 257) from the Chinese Northern Wei Dynasty, the painting dated to 477–499 AD. In this painting the arching cross bar is clear, but the artist failed to clearly show the cushioned collar behind it, without which the whole design would have been rendered useless. \n\nThe same basic design is seen in other painted Chinese frescoes, one from 520–524 AD (with shafts projecting beyond the horses chest for sternal traction), and another circa 600 AD (Sui Dynasty). This Sui Dynasty depiction (in cave 302) is of particular interest, since its depiction of the horse collar is not only more accurate (the same seen even in north and northwest China today), but it is used for a camel, not a horse. The Chinese had used camels often from the 2nd century BC onwards during the Han Dynasty, and there was even a Camel Corps serving the military on the frontier of the Tarim Basin. However, the adapted horse collar for camels would not have been common until the 6th century. In cave 156, there is a panorama painting of the Tang Dynasty Chinese general and provincial governor Zhang Yichao riding triumphantly after the recapture and conquest of the Dunhuang region from the Tibetan Empire in 834 AD. According to evidence provided by Dr. Chang Shuhong, the date of the painting is precisely 851 AD, yet Needham points out that there is universal consensus amongst historians that it was painted anytime between roughly 840 to 860 AD. This latter painting accurately depicts the horse collar, with a well-padded collar coming low on the chest and rising behind the cross-bar.\nThe horse collar eventually spread to Europe c. 920 AD, and became universal by the 12th century. The Scandinavians were among the first to utilize a horse collar that did not constrain the breathing passages of the horses. Prior to this development, oxen still remained the primary choice of animal for farm labor, as all the previous harnesses and collars could only be worn by them without physical penalty. Additionally, the yoke used to harness oxen were made exclusive to each individual animal. However it was sometimes difficult to cultivate the land; based upon soil condition, it may have taken up to sixteen oxen to effectively use a single heavy plow. This made it difficult for farmers who lacked the capital to sustain such large numbers.\n\nWhen the horse was harnessed with a horse collar, the horse could apply 50% more power to a task than an ox due to its greater speed. Horses generally also have greater endurance and can work more hours in a day. The centuries-long association that the Europeans had with the use of horses allowed an easier transition from oxen-based harnesses to the horse collar.\n\nThe creation of the horse collar removed the previous physical restrictions the old harness had on the animal, and allowed the horse to be able to exert its full strength in plowing. Originally, the structure of the old harness forced the horse to literally pull its workload, the horse collar’s development instead allowed the horse to push its workload, increasing the efficiency of its labor output.\n\nFollowing the introduction of the horse collar to Europe and its use being clearly evident by 1000 AD, the use of horses for ploughing became more widespread. Horses work roughly 50 percent faster than oxen. With the collar, combined with the horseshoe, the heavy plow, and other developments in the agricultural system, the efficiency of the European peasant farmer in producing food increased, allowing further societal development in Europe. The surplus in food allowed labor specialization as farmers could change their occupation and focus on other skills, such as the purchase and selling of goods, resulting in the emergence of a merchant class within European society. The horse collar was one of the factors in the ending of the feudal system and transition from the Middle Ages.\n\nThe French cavalry officer Lefebvre des Noëttes experimented with the ancient throat-and-girth harness in comparison the later trace breast-harness and then finally the matured form of the medieval collar harness. In his experiment of 1910, he found that two horses (aided by effective traction) using the throat-and-girth harness were limited to pulling about 1100 lbs. (½ ton). However, a single horse with a more efficient collar harness could draw a weight of about 1½ tons.\n\nHowever, the findings of Lefebvre des Noëttes were not without challenges, notably the argument that there was an early partial horse collar, a dorsal yoke system, dating to ancient Rome, and that Lefebvre's designs did not accurately reflect those actually used, but rather created an inaccurate design that was less efficient than any actual ancient harnesses used. While Lefebvre's experiments clearly demonstrated that the throat and girth design he used rode up on horses and cut off their air, images from ancient art and partial yokes found by archaeologists suggested that with proper placement and the addition of a stiff partial yoke, the breastcollar remained on the chest, and wind was not in fact cut off while pulling. Further studies conducted in 1977 by Spruytte and Littauer, followed up by Georges Raepsaet, with more accurately reconstructed ancient designs suggested that horses with ancient harness designs could pull nearly as much as with the more modern horse collar. The primary benefit to the use of the modern horse collar, it is argued, was that it allowed a lower point of attachment and in so doing increased the usability of horses for ploughing.\n\n\n"}
{"id": "1246810", "url": "https://en.wikipedia.org/wiki?curid=1246810", "title": "Humidifier", "text": "Humidifier\n\nA humidifier is a device, primarily an electrical appliance that increases humidity (moisture) in a single room or an entire building. In the home, point-of-use humidifiers are commonly used to humidify a single room, while whole-house or furnace humidifiers, which connect to a home's HVAC system, provide humidity to the entire house. Medical ventilators often include humidifiers for increased patient comfort. Large humidifiers are used in commercial, institutional, or industrial contexts, often as part of a larger HVAC system.\n\nLow humidity may occur in hot, dry desert climates, or indoors in artificially heated spaces. In winter, especially when cold outside air is heated indoors, the humidity may drop to as low as 10–20%. This low humidity can cause adverse health effects, by drying out mucous membranes such as the lining of the nose and throat, and can cause respiratory distress. The low humidity also can affect wooden furniture, causing shrinkage and loose joints or cracking of pieces. Books, papers, and artworks may shrink or warp and become brittle in very low humidity.\n\nIn addition, static electricity may become a problem in conditions of low humidity, destroying semiconductor devices, causing static cling of textiles, and causing dust and small particles to stick stubbornly to electrically charged surfaces.\n\nOveruse of a humidifier can raise the relative humidity to excessive levels, promoting the growth of dust mites and mold, and can also cause hypersensitivity pneumonitis (humidifier lung). A relative humidity of 30% to 50% is recommended for most homes. A properly installed and located hygrostat should be used to monitor and control humidity levels automatically, or a well-informed and conscientious human operator must constantly check for correct humidity levels.\n\nIndustrial humidifiers are used when a specific humidity level must be maintained to prevent static electricity buildup, preserve material properties, and ensure a comfortable and healthy environment for workers or residents.\n\nStatic problems are prevalent in industries such as packaging, printing, paper, plastics, textiles, electronics, automotive manufacturing and pharmaceuticals. Friction can produce static buildup and sparks when humidity is below 45% relative humidity (RH). Between 45% and 55% RH, static builds up at reduced levels, while humidity above 55% RH ensures that static will never build up. The American Society of Heating, Refrigerating and Air Conditioning Engineers (ASHRAE) has traditionally recommended a range of 45–55% RH in data centers to prevent sparks that can damage IT equipment. Humidifiers are also used by manufacturers of semiconductors and in hospital operating rooms.\n\nPrinters and paper manufacturers use humidifiers to prevent shrinkage and paper curl. Humidifiers are needed in cold storage rooms to preserve the freshness of food against the dryness caused by cold temperatures. Art museums use humidifiers to protect sensitive works of art, especially in exhibition galleries, where they combat the dryness caused by heating for the comfort of visitors during winter.\n\nA \"portable\" humidifier may range in size from a small tabletop appliance to a large floor-mounted unit. The water is usually supplied by manually filling the unit on a periodic basis.\n\nThe most common portable humidifier, an \"evaporative\", \"cool moisture\", or \"wick humidifier\", consists of just a few basic parts: a reservoir, wick and fan.\n\nThe wick is made of a porous material that absorbs water from the reservoir and provides a larger surface area for it to evaporate from. The fan is adjacent to the wick and blows air onto the wick to aid in the evaporation of the water. Evaporation from the wick is dependent on relative humidity. A room with low humidity will have a higher evaporation rate compared to a room with high humidity. Therefore, this type of humidifier is partially self-regulating; as the humidity of the room increases, the water vapor output naturally decreases.\n\nThese wicks become moldy if they are not dried out completely between fillings, and become saturated with mineral deposits over time. They regularly need rinsing or replacement; if this does not happen, air cannot pass through them, and humidifier stops humidifying the area it is in and the water in the tank remains at the same level.\n\nOne type of evaporative humidifier makes use of just a reservoir and wick. Sometimes called a \"natural humidifier\", these are usually non-commercial devices that can be assembled at little or no cost. One version of a natural humidifier uses a stainless steel bowl, partially filled with water, covered by a towel. A waterproof weight is used to sink the towel in the center of the bowl. There is no need for a fan, because the water spreads through the towel by capillary action and the towel surface area is large enough to provide for rapid evaporation. The stainless steel bowl is much easier to clean than typical humidifier water tanks. This, in combination with daily or every other day replacement of the towel and periodic laundering, can control the problem of mold and bacteria.\n\nHouseplants may also be used as natural humidifiers, since they evaporate water into the air through transpiration. Care must still be taken to prevent bacteria or mold in the soil from growing to excessive levels, or from dispersing into the air.\n\nA vaporizer (steam humidifier, warm mist humidifier) heats or boils water, releasing steam and moisture into the air. A medicated inhalant can also be added to the steam vapor to help reduce coughs. Vaporizers may be healthier than cool mist types of humidifiers because steam is less likely to convey mineral impurities or microorganisms from the standing water in the reservoir. However, boiling water requires significantly more energy than other techniques. The heat source in poorly designed humidifiers can overheat, causing the product to melt, leak, and start fires.\n\nAn impeller humidifier (cool mist humidifier) uses a rotating disc to fling water at a diffuser, which breaks the water into fine droplets that float into the air. The water supply must be kept scrupulously clean, or there is a risk of spreading bacteria or mold into the air.\nThese types of humidifiers are usually noisier than others.\n\nAn ultrasonic humidifier uses a ceramic diaphragm vibrating at an ultrasonic frequency to create water droplets that silently exit the humidifier in the form of cool fog. Usually the mist gets forced out by a tiny fan, while some ultra mini models have no fans. The models without fans are meant mainly for personal use. Ultrasonic humidifiers use a piezoelectric transducer to create a high frequency mechanical oscillation in a film of water. This forms an extremely fine mist of droplets about one micron in diameter, that is quickly evaporated into the air flow.\n\nUnlike the humidifiers that boil water, these water droplets will contain any impurities that are in the reservoir, including minerals from hard water (which then forms a difficult-to-remove sticky white dust on nearby objects and furniture). Any pathogens growing in the stagnant tank will also be dispersed in the air. Ultrasonic humidifiers should be cleaned regularly to prevent bacterial contamination from being spread throughout the air.\n\nThe amount of minerals and other materials can be greatly reduced by using distilled water. Special disposable demineralization cartridges may also reduce the amount of airborne material, but the EPA warns, \"the ability of these devices to remove minerals may vary widely.\" The mineral dust may have negative health effects. Wick humidifiers trap the mineral deposits in the wick; vaporizer types tend to collect minerals on or around the heating element and require regular cleaning with vinegar or citric acid to control buildup.\n\nFor buildings with a forced-air furnace, a humidifier may be installed into the furnace. They can also protect wooden objects, antiques and other furnishings which may be sensitive to damage from overly dry air. In colder months, they may provide modest energy savings, since as humidity increases, occupants may feel warm at a lower temperature.\n\nBypass humidifiers are connected between the heated and cold air return ducts, using the pressure difference between these ducts to cause some heated air to make a bypass through the humidifier and return to the furnace.\n\nAny humidifiers should usually be disabled during the summer months if air conditioning is used; air conditioners partially function to reducing indoor humidity, and having a humidifier continue to operate will waste significant amounts of energy.\n\nDrum style (bypass) uses a pipe to bring water directly to a reservoir (a pan) attached to the furnace. The water level in the pan is controlled by a float valve, similar to a small toilet tank float. The wick is typically a foam pad mounted on a drum and attached to a small motor; hot air enters the drum at one end and is forced to leave through the sides of the drum. When the hygrostat calls for humidity, the motor is turned on causing the drum to rotate slowly through the pan of water and preventing the foam pad from drying out.\n\nFor the latter reason especially, drum-style humidifiers should always be turned off at the water supply during summer (air conditioning) months, and should always be used with high quality furnace air filters (MERV ratings as high as possible to ensure small numbers of mold spores reaching the humidifier pan) when the water supply is turned on.\n\nA disc wheel style (bypass) is very similar in design to the drum style humidifiers; this type of furnace humidifier replaces the foam drumming with a number of plastic discs with small grooves on both sides. This allows for a very large evaporative surface area, without requiring a great deal of space. Unlike the drum style humidifiers, the disc wheel does not need regular replacement.\n\nBypass flow-through style (bypass – also known as \"biscuit style\" or many other, similar variant names) uses a pipe to bring water directly to an electrically controlled valve at the top of the humidifier. Air passes through an aluminum \"biscuit\" (often called a pad; the term \"biscuit\" emphasizes the solid rather than foamy form) which is similar to a piece of extremely coarse steel wool. The \"biscuit\" has a coating of a matte ceramic, resulting in an extremely large surface area within a small space. When the hygrostat calls for humidity, the valve is opened and causes a spray of water onto the \"biscuit\". Hot air is passed through the \"biscuit\", causing the water to evaporate from the pad and be carried into the building.\n\nSpray mist type uses a pipe, usually a small plastic one, to bring water directly to an electrically controlled valve (atomizer-this forces the water through a tiny orifice causing it to break up into tiny particles) in the humidifier. Water mist is sprayed directly into the supply air, and the mist is carried into the premises by the air flow.\n\nAdditional types include non-bypass flow-through (fan augmented), steam, impeller or centrifugal atomizer, and under duct designs.\n\nThe USEPA provides detailed information about health risks as well as recommended maintenance procedures. If the tap water contains a lot of minerals (also known as \"hard water\") then the ultrasonic or impeller humidifiers will produce a \"white dust\" (calcium is the most common mineral in tap water), which usually spreads over furniture, and is attracted to static electricity generating devices such as CRT monitors. The white dust can be prevented by using distilled water or a demineralization cartridge in ultrasonic humidifiers.\n\nIn addition, a stuck or malfunctioning water supply valve can deliver large amounts of water, causing extensive water damage if undetected for any period of time. A water alarm, possibly with an automatic water shutoff, can help prevent this malfunction from causing major problems.\n\n\n"}
{"id": "39868986", "url": "https://en.wikipedia.org/wiki?curid=39868986", "title": "Hungarian Testing Board", "text": "Hungarian Testing Board\n\nThe Hungarian Testing Board (HTB or HSTQB - Hungarian Software Testing and Qualifications Board) was founded in 2007 as a non-profit organization, association of specialists dedicated to good practices in software testing as well as information systems. In addition to testing in itself, the HTB is an official panel for all testing related disciplines/activities in Hungary.\n\nThe Board consists of testing experts from a wide range of organizations: the IT industry, consultancies, training providers and other professional and scientific/academic communities who volunteer their time to the achievement of organization's goals.\nThe HTB is a member Board of the ISTQB (International Software Testing and Qualification Board) and as such it is the single legal representative of ISTQB in Hungary. Also members of the HTB are the only ones who can represent HTB internationally as a national board for Hungary within the ISTQB.\n\nThe HTB provides the below certifications on behalf of ISTQB and IREB.\n\nThe ISTQB Certified-Tester Program is recognized worldwide as a standard qualification scheme for software testers. It consists of three levels:\nThe syllabus contents and examination questions are developed and published in Hungary by the Hungarian Testing Board (HTB).\n\nThe CTFL qualification is aimed at anyone involved in software testing. This includes people in roles such as testers, test analysts, test engineers, test consultants, test managers, user acceptance testers and software developers. This CTFL qualification is also appropriate for anyone who wants a basic understanding of software testing, such as project managers, quality managers, software development managers, business analysts, IT directors and management consultants. Holders of the CTFL Certificate will be able to go on to a higher level software testing qualification.\n\nThe ISTQB CTAL qualification is aimed at people who have achieved an advanced level in their careers in software testing. This includes people in roles such as testers, test analysts, test engineers, test consultants, test managers, user acceptance testers, and software developers. This CTAL qualification is also appropriate for anyone who wants a deeper understanding of software testing, such as project managers, quality manager's, software development managers, business analysts, IT directors and management consultants.\n\nISTQB AL TM (Advanced Level Test Manager)<br>\nThe Advanced Test Manager Level focuses on the advanced working responsibilities of the Test Manager or Test Lead:\n<br>\nISTQB AL TA (Advanced Level Test Analyst)<br>\nThe Advanced Test Analyst Level focuses on the advanced working responsibilities of the Test Analyst:\n<br>\nISTQB AL TTA (Advanced Level Technical Test Analyst)<br>\nThe Advanced Technical Test Analyst Level focuses on the advanced working responsibilities of the Technical Test Analyst:\n\nIREB CPRE FL (Certified Professional for Requirements Engineering - Foundation Level): \nThe International Requirements Engineering Board is responsible for the international scheme called Certified Professional Requirements Engineering. The HTB is recognized by the International Requirements Engineering Board (IREB) as an IREB – CPRE Exam Provider.\n\nFoundation level comprises basic knowledge of eliciting, analyzing, specifying, documenting, validating and managing requirements. A person with a CPRE FL certificate is familiar with the terminology of requirements engineering/business analysis and requirements management. Understands the basic techniques and methods of requirements engineering and their application and is familiar with the most established notations for requirements.\n\nInternational Software Testing Qualifications Board (ISTQB)\n\n"}
{"id": "7364643", "url": "https://en.wikipedia.org/wiki?curid=7364643", "title": "I.D. (magazine)", "text": "I.D. (magazine)\n\nI.D. (\"The International Design Magazine\") was a magazine covering the art, business and culture of design. It was published eight times a year by F+W Media.\n\n\"I.D.\" was founded in 1954 as \"Industrial Design\". The name was later abbreviated to an initialism; in the 1980s the initials came to stand for \"International Design\" to reflect the magazine's broadened scope.\n\nSince 1954, the magazine published the Annual Design Review, a juried design competition curated by \"I.D.\" staff and industry practitioners.\n\n\"I.D.\" won five National Magazine Awards: three for General Excellence (1995, 1997, 1999), one for Design (1997), and one for Special Interests (2000).\n\nThe last issue of \"I.D.\" was published in January/February 2010.\n\nIn June 2011, \"I.D.\" magazine was re-launched online in partnership with Behance. The new \"I.D.\" magazine featured user-submitted designs that were curated to offer examples of innovative work happening today.\n\nBy March 2016, the magazine's website had been shut down.\n"}
{"id": "8089589", "url": "https://en.wikipedia.org/wiki?curid=8089589", "title": "Incentive spirometer", "text": "Incentive spirometer\n\nAn incentive spirometer is a medical device used to help patients improve the functioning of their lungs. It is provided to patients who have had any surgery that might jeopardize respiratory function, particularly surgery to the lungs themselves, but also commonly to patients recovering from cardiac or other surgery involving extended time under anesthesia and prolonged in-bed recovery. The incentive spirometer is also issued to patients recovering from pneumonia or rib damage to help minimize the chance of fluid build-up in the lungs. It can be used as well by wind instrument players, who want to improve their air flow.\n\nThe patient breathes in from the device as slowly and as deeply as possible, then holds his/her breath for 2–6 seconds. This provides back pressure which pops open alveoli. It is the same maneuver as in yawning. An indicator provides a gauge of how well the patient's lung or lungs are functioning, by indicating sustained inhalation vacuum. The patient is generally asked to do many repetitions a day while measuring his or her progress by way of the gauge.\n\n"}
{"id": "14573046", "url": "https://en.wikipedia.org/wiki?curid=14573046", "title": "Kwikpoint", "text": "Kwikpoint\n\nKwikpoint is an Alexandria, Virginia based company that develops visual language and icon translation tools designed to help users communicate without having to read written information or foreign languages. Kwikpoint's target markets include the military and public service communities, as well as corporate and leisure travelers.\n\nKwikpoint was founded in the early 1990s by Alan Stillman, following a 15,000-mile, 28-country bike trip. While on his trip, Stillman cut out pictures from magazines to help communicate his dinner selection. On his return, he worked with a team of six designers, linguists and members of the diplomatic community to create a series of universally recognizable icons that make up the Kwikpoint visual translators.\n\nThe resulting product, the Kwikpoint visual translators, include 1,000 visually recognizable symbols for use worldwide. Kwikpoint translators are used by the U.S. military and National Guard in Iraq and Afghanistan. In addition, Kwikpoint Medical Translators are used by emergency medical personnel in the United States to help communicate with patients who do not speak English.\n\n"}
{"id": "11284966", "url": "https://en.wikipedia.org/wiki?curid=11284966", "title": "Larner–Johnson valve", "text": "Larner–Johnson valve\n\nA Larner–Johnson valve is a mechanism used in dams and water pumping to control the flow of water through large pipes. It was manufactured in the early 20th century by the Larner-Johnson Company in the by Blackhall Engineering Ltd. These valves have been constructed in sizes up to diameter and controlling a hydraulic head of .\n\nIn 2009, Blackhall Engineering supplied four 60\" bore Larner–Johnson valves to New York City Department of Environmental Protection for the Ashokan Reservoir in upstate New York. Each valve is capable of passing a flow rate of 19 cubic metres/second, which is the equivalent of 19 tons of water per second. The valves control the flow of water out of the Ashokan Reservoir into the Catskill Aqueduct down to New York City.\n\nThe valve is housed within a bulged section of the penstock pipe. The valve mechanism forms a cylindrical body within this pipe, with the water flowing around it. This downstream section of this valve body is conical and free to move axially. When it moves downstream, it seals against a conical surface at the outlet of the valve, closing off the flow. When closed, the valve is held shut by the supply water pressure, providing a good seal.\n\nThe valve is hydraulically actuated, by the pressure of the flow that it is controlling. A servomechanism is used to generate the large forces needed for these huge valves, power being derived from the pressure of the water itself. Although the pressure within the pipe cannot be increased above that of the supply, it can be decreased. The pipe cross-section at the lower (downstream) part of the valve is reduced in section compared to that above the valve. By Bernoulli's principle, this increases the flow velocity of the water, thus reducing pressure. If water pressure is bled away through a drain valve, obviously that will reduce its pressure too.\n\nInternally the fixed valve body is constructed as a cylinder, with the moving part of the valve within this as a piston. The conical valve is part of this piston. The piston is double-acting and splits the internal valve body in two, with water pressure on both sides. The upper side of the piston is larger, the full diameter of the piston, and is supplied with water through a throttling valve; normally adjusted and then left in position. The lower side of the piston is an annular space surrounding the moving valve body and maintained at the lower (Bernoulli) water pressure. The tip of the conical valve contains a small pilot valve, linking the upper chamber of the valve to the downstream drain below the valve. This pilot valve is controlled mechanically from outside the valve chamber and is the main control over the valve position.\n\nWhen water pressure builds up in the upper chamber, through the throttle valve, this forces the valve downstream and closes it against the seat. As the area of the piston on this side is larger, the pressure easily outbalances the smaller area of the annular piston below. \n\nIf the pilot valve is opened, water drains from the upper chamber and its pressure is reduced. The pilot valve is always larger in flowrate than the filling through the throttle valve. Pressure in the annular chamber now forces the piston upstream, lifting the valve body from the conical seat and opening the valve.\n\nAt intermediate positions, pressure in the two chambers is balanced and the valve remains part-open. This depends on the piston size, the pressure reduction downstream and the flows through the two control valves. The pilot valve gives a proportional control over main valve flowrate, with the opening being controlled, slow and gradual.\n\nIf the pilot valve is closed, pressure now builds up in the upper chamber, overwhelming the lower pressure and force in the downstream chamber. The valve is once again forced closed.\n\nThe enormous forces involved in a valve of this type can give trouble for many types of valve. In the Larner-Johnson valve a great advantage is that these forces are always balanced and act only within the valve body, not on its actuators. This gives freedom from distortion and great reliability.\n\n"}
{"id": "655044", "url": "https://en.wikipedia.org/wiki?curid=655044", "title": "Leash", "text": "Leash\n\nA leash (also called a lead, lead line or tether) is a rope or similar material used to control an animal by attaching to it or to a separate object on it; some leashes clip or tie to a collar, harness, or halter, while others go directly around the animal's neck or head.\n\nLeashes take many forms; for example:\n\nThere are also bicycle dog leashes, especially designed for people who enjoy taking their pet in a ride with the bike. The leash is an aluminum tube with a plastic coated cable which runs down through the tube. It extends out of the tube end a couple of feet to allow for ease of movement for the dog. One end connects to the bike and the other to the dog's collar. This keeps them safely away the bike.\n\nCat leashes and harnesses are also available on the market and are convenient for people who are not comfortable letting their pet free.\n\nMany cities have passed legislation that requires dogs to be on leash in public areas; in some areas, cats are also required to be restrained (under control) on a leash, in a kennel, or in a cat-proof yard or house.\n\nPurposes of a leash include: preventing animals from frightening or biting people or other animals, defecating and urinating in inappropriate places, endangering traffic, digging up lawns, causing other damage, getting lost, and getting away from owners. Leashes also provide a clear method of communication and ensure control during training of dogs.\n\nIn the United States, leash laws are different within each state. While some states do not have statewide leash laws and give localities power to make leash law, there are some other states in which leash laws apply statewide.\n\nStates that do not have statewide leash laws are Alabama, Alaska, Arizona, Arkansas, California, Colorado, Florida, Georgia, Hawaii, Idaho, Iowa, Kansas, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Montana, Nevada, New Jersey, New Mexico, North Dakota, Oklahoma, Oregon, Rhode Island, South Carolina, South Dakota, Texas, Utah, Vermont, Virginia, Washington, and Wyoming.\n\nIn Connecticut, dogs are not permitted to run at large except in the situation of hunting. Still, if the dog has vicious propensities and the owner still allows it to run at large and a person is bitten, the owner can be fined for up to $1,000 and is also liable for 6 months of prison unless the victim has abused the dog and provoked the harmful behavior.\n\nIn Delaware, dogs are not allowed to run at large unless in situations when the owner is present and has control over the pet. An exception is for farm dogs. Also, during the night dogs must be kept in an enclosure from which they cannot escape, firmly secured with a collar or chain or other device, so they cannot stray from the premises, or are under the reasonable control of the owner or custodian. If an owner does not respect these laws and if the dog bites someone, the owner is subject to civil liability and for fines of up to $1,500.\n\nDogs in the District of Columbia must be kept on a leash as well. They are also not permitted on school grounds when school is in session or on any public recreation area without a leash.\n\nIndiana is one of the states that has a restraint statute, which means that dogs must be restrained at all times. Otherwise, if the dog bites a person when not restrained the owner is subject to civil liability and criminal penalties. (Cite needed; definition of \"restraint\" needed; discussion of local Indiana ordinances forbidding dog tethering needed.)\n\nDogs are allowed to run at large during the night in Kentucky only if they are accompanied by and under control of their owner.\n\nAccording to the leash laws of Louisiana, dogs are prohibited to run at large at all times of the day. The same law applies in Maine, where the only exception is for hunting dogs.\n\nMissouri legislation requires that dogs are kept in leashes that are no longer than 10 feet when they are in state parks or on historic sites. Also, dogs that have rabies are not permitted to run at large.\n\nIn Nebraska, dogs may run at large only in counties where the population does not reach 80,000.\n\nNew Hampshire legislation does not allow dogs to run at large unless they are accompanied by their owner or custodian or when dogs are used for training or are trained for hunting, herding or exhibitions.\n\nIllinois legislation prohibits owners from walking their dogs when they are not in a leash.\n\nDogs in New York must be restrained or confined at all times of the day. However, certain NYC parks allow dogs off leash at certain hours.\n\nAccording to the North Carolina law, dogs are allowed to run at large during the night only if they are accompanied by their owner or a person who has received the owner's permission to do so.\n\nOhio law requires one to “keep the dog under the reasonable control of some person,” but does not require a leash except for “any female dog … at any time the dog is in heat.” There are additional provisions for “dangerous dogs” that have injured a person or killed another dog.\n\nPennsylvania legislation states that dogs must be confined or firmly secured or reasonably controlled by a person, within the property of the owner.\n\nTennessee law prohibits dogs to run at large except in cases in which dogs are engaged in legal hunting or herding.\n\nWest Virginia and Wisconsin are states that do not have a law that requires dogs to be leashed. Still, they do have laws that hold dog owners and keepers liable for all damages caused by dogs that are permitted to run at large.\n\nDifferent law applies to dangerous dogs and female dogs as in different states they are prohibited to run at large at all times. Also, in states such as Connecticut and Louisiana, guide dogs must also be leashed.\n\nDog leashes are used to walk the dog in public places. Having the dog wear a leash is a way of protecting the dog and other persons (e.g., if the dog runs away and bites someone). Also, the length is one of the important aspects of the leash. The length of the leash must be chosen according to the size of the dog and it is important because it allows a good control. Leashes should not be either too long or too short. Too long leashes do not provide good control of the pet which can result in unpleasant accidents with more aggressive dogs whereas too short leashes are uncomfortable for both the dog and the owner. The perfect leash can restrain the dog but at the same time is not viewed as a punishment for the pet.\n\nHigh quality dog leashes have a good quality metal clip and they can be made of leather, nylon or even chain. The metal clip must securely fasten to a metal ring on the collar in order to maintain good control of the dog. The material of which the leash is made of is not of great importance as long as the leash does not show evidence of wear or fraying. Therefore, leashes should be periodically checked to ensure they are maintained in proper condition.\n\nAn important aspect of dog leashes is their sturdiness. Although rope leashes are quite cheap, they are vulnerable to chewing and fraying and are not amongst the most recommended types of leashes. However, it is considered that a better type of leash is the one made of nylon because this material provides a bit of elasticity which is meant to result in more comfort for the dog. On the other hand, nylon leashes can cause chafe or can cut into the skin of the dog.\n\nLeather leashes are often preferred over the nylon ones because of the resistance of the material and because it becomes more flexible with age and it is softer. Leather is however more prone to be chewed when compared to nylon.\n\nThe retractable dog leash is one of the most comfortable leashes for the dogs because they allow them to go as far as they want as long as the owner does not consider it a danger. Retractable leashes are usually made of nylon and the retractable device is made of plastic or a stronger composite. Although these leashes can be convenient for both the dog and the owner as it allows some control, they make it difficult to keep an aggressive dog under control which can result in persons or other dogs being attacked. Aggressive dogs should not be walked with such a leash, and puppies should be kept closely to ensure their protection from various dangers such as cars. Dogs with a tendency to bolt without warning should be walked with caution, as a retractable lead can allow the dog to accelerate to significant speed before being stopped suddenly presenting the possibility of injury to both dog and owner.\n\nSome leashes are made of reflective materials and are suitable for walking the dog at night. They are convenient because they make the dog and the owner much more visible in the traffic, reducing the likelihood of accidents.\n\nCat leashes are used with the purpose of preventing the cat getting lost. Unlike dogs, cats rarely get into fights or attack persons on the street, so cat leashes are mainly a safety measure to protect the pet itself. Very often the collars are replaced with harnesses, because they avoid the dangers of collars which include escaping and running away or choking. Cats are more likely to not be willing to be walked in a harness than dogs are. This is why cats are considered to need up to months to be able to adjust to wearing a harness.\n\nCat leashes come in a variety of colors, designs and models and are made of different materials. There are cat leashes made of leather, nylon and rope. Whereas the leather leash is one of the best qualities because of the characteristics of the material, it is also one of the most expensive and not very comfortable for the cat at the same time. Nylon cat leashes and harnesses are, however, more elastic, and thus more comfortable and also provide more control.\n\nLeashes are used on large livestock—such as cattle, camels, llamas, and horses—to lead them so that they will follow and come to a desired area—as well as to tether them to a specific area, such as to a fencepost or tree trunk, so that they will remain stationary and not run away. Oftentimes, leashes are used to tether such animals when they require seperation, examination, or work to be done to them, such as mouthing, grooming, and tacking up.\n\n\n\n"}
{"id": "44330949", "url": "https://en.wikipedia.org/wiki?curid=44330949", "title": "List of Avid DNxHD resolutions", "text": "List of Avid DNxHD resolutions\n\nThis is a list of Avid DNxHD resolutions, mainly available in multiple HD encoding resolutions based on the frame size and frame rate of the media being encoded. The list below shows the available encoding choices for each of the available frame size and frame rate combinations.\n\nIts sister codec, Avid DNxHR, supports resolutions beyond FullHD.\n"}
{"id": "41317672", "url": "https://en.wikipedia.org/wiki?curid=41317672", "title": "Liz Bacon", "text": "Liz Bacon\n\nLiz Bacon (born 27 September 1963) is a Professor of Software Engineering and Deputy Pro Vice-Chancellor at the University of Greenwich, England.\n\nLiz Bacon grew up in Kenley, Surrey. She studied for an undergraduate degree in Computer Science at Thames Polytechnic, spending her third year on an industrial placement at CERN, graduating in 1986. She studied for her PhD in the field of Artificial Intelligence at the University of Greenwich, and was awarded her doctorate in 1993.\nBacon is currently the Past President of the BCS, The Chartered Institute for IT , and has had many serious roles, including the Chair of the BCS Academy, a BCS Council Member and Trustee of the BCS. She has been a Council member of PITCOM, the Parliamentary IT Committee, responsible for communications, EQANIE (European Quality Assurance Network for Informatics Education), the National HE STEM Programme and an ICT Thought Leader for the University of Cambridge International Examinations. Bacon has been involved in software engineering and e-learning research for more than 10 years \nShe was one of the 30 women identified in the BCS Women in IT Campaign in 2014 source\nand was then featured in the e-book of these 30 women in IT, \"Women in IT: Inspiring the next generation\" produced by the BCS, The Chartered Institute for IT, as a free download e-book, from various sources.\nIn 2015, Liz bacon became a Principal Fellow of the HEA and was identified as the 35th Most Influential Women in UK IT 2015, by Computer Weekly.\n\n\ne-learning, serious games, software engineering, crisis management, affective computing, cybersecurity, e-Health, personalisation.\n\nLiz Bacon is married to Stuart Kabler. Her hobbies are horse jumping, skiing and scuba diving.\n"}
{"id": "4060510", "url": "https://en.wikipedia.org/wiki?curid=4060510", "title": "Masonry heater", "text": "Masonry heater\n\nA masonry heater (or masonry stove, ceramic stove, tile stove) is a device for warming an interior space through radiant heating, by capturing the heat from periodic burning of fuel (usually wood), and then radiating the heat at a fairly constant temperature for a long period. The technology has existed in different forms, from back into the Neoglacial and Neolithic periods. Archaeological digs have revealed excavations of ancient inhabitants utilizing hot smoke from fires in their subterranean dwellings, to radiate into the living spaces. These early forms have evolved into modern systems.\n\nEvidence found from 5,000 B.C. of massive blocks of masonry used to retain heat foreshadowed early forms of fire hearths that were used as multifunctional heating sources. Later evolutions came in the Roman \"hypocaust\", Austrian/German (\"kachelofen\", baths) using the smoke and exhaust of a single fire. In Eastern and Northern Europe and North Asia, these kachelofens (or \"steinofens\") evolved in many different forms and names: for example the Russian Stove/Fireplace (), the Finnish Stove (in Finnish: \"pystyuuni\" or \"kaakeliuuni\", \"tile oven\") and the Swedish Stove (in Swedish: \"kakelugn\", \"tile stove\" or \"contra-flow stove\") associated with Carl Johan Cronstedt. The Chinese developed the same principle into their Kang bed-stove. The masonry heater has gained renewed domestic popularity recently because of its heating efficiency.\n\nASTM International calls a masonry heater \"a vented heating system of predominantly masonry construction having a mass of at least 800 kg (1760 lbs), excluding the chimney and masonry heater base. In particular, a masonry heater is designed specifically to capture and store a substantial portion of the heat energy from a solid fuel fire in the mass of the masonry heater through internal heat exchange flue channels, enable a charge of solid fuel mixed with an adequate amount of air to burn rapidly and more completely at high temperatures in order to reduce emission of unburned hydrocarbons, and be constructed of sufficient mass and surface area such that under normal operating conditions, the external surface temperature of the masonry heater (except in the region immediately surrounding the fuel loading door(s)), does not exceed 110°C (230°F).\"\n\nThe stove is made of masonry such as brick (firebrick), soapstone, tile, stone, stucco, or a combination of materials, rather than steel or cast iron. It is freestanding, and usually requires special support to bear its weight. It consists of a firebox and heat-exchange channels or partitions that provide additional surface area. These absorb heat from the hot exhaust gases before the gases exit into the chimney. The fire in a masonry heater burns much hotter than in a metal stove. Very hot fires reduce emissions significantly. When not being fired, the connection from the masonry heater to the chimney sometimes has a damper to prevent heat from escaping up the chimney; the heat is then radiated from the masonry.\n\nMasonry takes longer to heat than metal; but once warm, the heater will radiate this heat over a much longer period of time and at a much lower temperature than a metal stove would use (the metal is hot only when there is a fire burning inside the stove and for a short time thereafter). A masonry heater is warmed by fires that burn for a short time; it is mostly the heat stored by the heater's mass that heats the living space. Both in Europe and in America seating and even beds are occasionally built adjoining the masonry stove; this is possible because the heater's exterior surfaces are cool enough to touch safely. This cycle of short firings and long periods of heat-release makes a masonry heater a much more convenient option for actually heating a house with wood, than a metal wood stove. Masonry stoves can release heat from anywhere up to 36 hours after the fire has gone out (the larger the thermal mass the more heat the appliance can retain). This means that a householder might typically light a fire in the evening on returning home. In a more modern masonry heater with a glass door, this means that they can enjoy the fire during the evening. But overnight and throughout the next day when the fire is no longer alight the heater will still be warming the house, ready for the cycle to start again next evening. Compare this to a metal stove which is often either too hot or too cold (unlit) and the requirement for the owner to be present to load with logs at regular intervals and the benefits should be clear.\n\nHeat stress is a major concern during the construction of masonry heaters. Differences in temperature inside the masonry core of the heater can result in differential expansion. A skilled heater mason knows how to provide for this stress when designing and constructing the heater, thereby preventing uneven expansion from causing cracking in the exterior. There are two general ways this concern is addressed. One is to incorporate a gap between the inner core of the heater and its outer \"skin.\" The other is to build a more monolithic design with post-tension aspects to mechanically compensate for expansion and contraction. Both methods are used with success.\n\nBrick by Brick Masonry heaters take a long time (from 5 hours up to two days) to get up to the right temperature and so are not always practical for taking the chill off a single cool evening or morning. American Masonry Heaters has their own blend of refractory which allows the heater and surrounding area to heat up in an hour or two. This responsiveness is unmatched. The speed with which a masonry heater achieves the right temperature is called its responsiveness. Responsiveness is determined by the specific thickness and characteristics of the materials used in its construction. Very responsive heaters warm up faster and are good for quicker adjustments to indoor temperature. Less responsive heaters take longer to warm, but they are well-suited for long periods of cold weather because they store heat so well and provide dependable, even heat all day and night. Because the radiant heat is given off at a low level a masonry heater is not likely to overheat a home the way a metal stove might in warmer parts of the year like fall or spring.\n\nThe kachelofen design, a relatively large home heater surrounded with ceramic tile, has existed for at least five centuries. During the Renaissance period, the builders of kachelofens were part of a distinct trade and were called \"hafnermeister\". A kachelofen uses a maze-like passage created out of firebrick to release gases and smoke from the wood fire slowly, allowing the firebrick to retain as much heat as possible from the gases and smoke. The ceramic tile surrounding the kachelofen also acts as insulation to retain heat. Kachelofens were carefully designed so that the minimum amount of heat would escape, only as much as needed to warm the flue to maintain a proper air draught. The firebrick used in kachelofen construction holds 80% more heat than ferrous metals such as cast iron, while its heat conductivity is 1/45 that of iron or steel. A kachelofen is efficient enough to warm a house for up to 6 to 12 hours after the fire has stopped burning.\n\nRussian stove or oven, another typical masonry heater, evolved in the Russia in 15th century, after the brick flue was added to the traditional \"black-fired\" fireplace, which lacked the smokestack and vented directly into the room. The addition of the flue allowed for the better heat utilisation by passing the smoke and gases through the brick labyrinth called \"коленья\" (\"knees\" or \"bends\") before allowing it into the smokestack. The large thermal mass of these bends captured the heat, slowly releasing it afterwards. The typical Russian oven is a large, generally cuboid mass of masonry, usually weighing around 1-2 tons, built in the center of a traditional izba log hut, covered in stucco and carefully whitewashed.\n\nMost Russian ovens consist of a massive firebrick hearth, often large enough for a grown man to fit into, with a flue continuing into a maze-like heat exchanger built of a normal brick, usually with a built-in stove for cooking, which sometimes used a secondary fireplace to quickly cook foods without heating the whole affair; all covered with an outer brick shell, normally with a pedestal for a kitchen work and beds built into it. The stove was usually constructed by one of the house's walls, or, in the larger, multi-room houses, \"into\" one of the walls, in which case the room without the fireplace, and thus the smoke, but heated by the brick side of the oven, was called \"svetlitsa\" (\"light one\") and used as a living room, while the other was used as a kitchen. The small spaces left behind the stove and under its log foundation were called \"zapechye\" (\"behind the stove\") and \"podpechye\" (\"under the stove\"), and used as dry, warm storage. These were stereotyped as dwellings of the domovoi. The whole trade of oven builders (\"pechniki\") developed, and given the difficulties of proper stove construction and skill involved, these were seen as an influential bunch not to be trifled with, as an offended pechnik had numerous ways to get back at the employer.\n\nThese heaters are primarily fired by wood, and those fires are meant to burn hot and quickly (never damped down, as is often the case with standard wood stoves). They are not burned continuously. This method of heating may have been a reaction to the dwindling resource of wood before the advent of coal and other mineral energy sources. Open hearth fireplaces were an important source of light, as well as heat, and with an \"unlimited\" supply of wood to fire them, there is no incentive to increase the efficiency of their heat output, which is rather poor. However, once firewood became a scarcer resource, fireplace builders began to enclose the firebox to capture as much heat as possible.\n\nSince masonry heaters burn hot and fast, they can accept any dry, split (usually three to five inches in diameter) wood. In some areas of central and eastern Europe, these heaters are sometimes effectively fired using grass, straw, and hay. It is also common in eastern Europe to modify these efficient heaters so that they are connected to the gas network and are fueled with gas. Some modern models incorporate electric heating elements connected to timers. These are used only as a backup heat source during periods when the structure will be left unattended for long duration in winter (and therefore with no one to build a new fire in the heater each day or as needed) to prevent the structure from freezing.\n\nSome contemporary masonry heaters don't have a ceramic-tile exterior. Instead, the refractory bricks are covered by a heat-resistant kind of plaster. A glass door allows you to see the burning fire. As in the past once the firewood has burned, the warmed mass of the stove continues to radiate heat, but the size of the flue passages of modern masonry heaters are more exactly calculated than they used to be; this is done to provide increased efficiency and output and use less wood.\n\n\n\n"}
{"id": "35296440", "url": "https://en.wikipedia.org/wiki?curid=35296440", "title": "Memeburn", "text": "Memeburn\n\nMemeburn is a website that focuses on digital news and events in the emerging markets sphere. Founded by South African entrepreneur Matthew Buckland, the Cape Town-based website focuses on current events in the world of social media, mobile and general technology, and has featured interviews with the likes of WordPress creator and CEO Matt Mullenweg, Foursquare founder Dennis Crowley, Evernote CEO Phil Libin, the co-creator of Google Now and entrepreneur Richard Branson as well Naspers CEO Koos Bekker, Wired founder Chris Anderson and Stumbleupon CEO Mark Bartels. It is particularly interested in the BRICS, as well as digital innovation, viral online marketing campaigns and general tech trends.\n\nIn 2011, Memeburn expanded to launch Gearburn.com, a sister site which is focused on gadget and game reviews and news. In 2012, Ventureburn.com was created to focus on entrepreneurship and startups. The three sites were rebranded collectively in 2013 as Burn Media.\n\nIn 2010, Memeburn was awarded the UCT Graduate School of Business' Best Blog award at the South African Blog Awards, and later won a bronze award for Specialist Publisher Site at the Bookmark Awards.\n"}
{"id": "184570", "url": "https://en.wikipedia.org/wiki?curid=184570", "title": "Microbotics", "text": "Microbotics\n\nMicrobotics (or microrobotics) is the field of miniature robotics, in particular mobile robots with characteristic dimensions less than 1 mm. The term can also be used for robots capable of handling micrometer size components.\n\nMicrobots were born thanks to the appearance of the microcontroller in the last decade of the 20th century, and the appearance of miniature mechanical systems on silicon (MEMS), although many microbots do not use silicon for mechanical components other than sensors. The earliest research and conceptual design of such small robots was conducted in the early 1970s in (then) classified research for U.S. intelligence agencies. Applications envisioned at that time included prisoner of war rescue assistance and electronic intercept missions. The underlying miniaturization support technologies were not fully developed at that time, so that progress in prototype development was not immediately forthcoming from this early set of calculations and concept design. As of 2008, the smallest microrobots use a Scratch Drive Actuator.\n\nThe development of wireless connections, especially Wi-Fi (i.e. in domotic networks) has greatly increased the communication capacity of microbots, and consequently their ability to coordinate with other microbots to carry out more complex tasks. Indeed, much recent research has focused on microbot communication, including a 1,024 robot swarm at Harvard University that assembles itself into various shapes; and manufacturing microbots at SRI International for DARPA's \"MicroFactory for Macro Products\" program that can build lightweight, high-strength structures.\n\nWhile the 'micro' prefix has been used subjectively to mean small, standardizing on length scales avoids confusion. Thus a nanorobot would have characteristic dimensions at or below 1 micrometer, or manipulate components on the 1 to 1000 nm size range. A microrobot would have characteristic dimensions less than 1 millimeter, a millirobot would have dimensions less than a cm, a minirobot would have dimensions less than , and a small robot would have dimensions less than .\n\nDue to their small size, microbots are potentially very cheap, and could be used in large numbers (swarm robotics) to explore environments which are too small or too dangerous for people or larger robots. It is expected that microbots will be useful in applications such as looking for survivors in collapsed buildings after an earthquake, or crawling through the digestive tract. What microbots lack in brawn or computational power, they can make up for by using large numbers, as in swarms of microbots.\n\nThe way microrobots move around is a function of their purpose and necessary size. At submicron sizes, the physical world demands rather bizarre ways of getting around. The Reynolds number for airborne robots is close to unity; the viscous forces dominate the inertial forces, so “flying” could use the viscosity of air, rather than Bernoulli's principle of lift. Robots moving through fluids may require rotating flagella like the motile form of E. coli. Hopping is stealthy and energy-efficient; it allows the robot to negotiate the surfaces of a variety of terrains. Pioneering calculations (Solem 1994) examined possible behaviours based on physical realities.\n\nOne of the major challenges in developing a microrobot is to achieve motion using a very limited power supply. The microrobots can use a small lightweight battery source like a coin cell or can scavenge power from the surrounding environment in the form of vibration or light energy. Microrobots are also now using biological motors as power sources, such as flagellated \"Serratia marcescens\", to draw chemical power from the surrounding fluid to actuate the robotic device. These biorobots can be directly controlled by stimuli such as chemotaxis or galvanotaxis with several control schemes available. A popular alternative to an on-board battery is to power the robots using externally induced power. Examples include the use of electromagnetic fields, ultrasound and light to activate and control micro robots.\n"}
{"id": "3233817", "url": "https://en.wikipedia.org/wiki?curid=3233817", "title": "Military anti-shock trousers", "text": "Military anti-shock trousers\n\nMilitary anti-shock trousers, or pneumatic anti-shock garments (PASG), are medical devices used to treat severe blood loss.\n\nThere is significant controversy over the use of MAST and most modern EMS and trauma programs have abandonded their use following data from a Cochrane review which indicated no mortality or survival benefit when MAST were applied to patients in shock. \n\nI. G. Roberts et al. sought to quantify the effect on mortality and morbidity of the use of MAST in patients following trauma, and published the data in the Cochrane Database of Systematic Reviews.\n\n\n"}
{"id": "27190592", "url": "https://en.wikipedia.org/wiki?curid=27190592", "title": "Mining in the Upper Harz", "text": "Mining in the Upper Harz\n\nMining in the Upper Harz region of central Germany was a major industry for several centuries, especially for the production of silver, lead, copper, and, latterly, zinc as well. Great wealth was accumulated from the mining of silver from the 16th to the 19th centuries, as well as from important technical inventions. The centre of the mining industry was the group of seven Upper Harz mining towns of Clausthal, Zellerfeld, Sankt Andreasberg, Wildemann, Grund, Lautenthal und Altenau.\n\nThe Upper Harz was once one of the most important mining regions in Germany. The major products of its mines were silver, copper, lead, iron and, from the 19th century, zinc as well. The main source of income, however, was silver. From the 16th to the middle of the 19th centuries about 40–50% of the entire German silver production originated in the Upper Harz. The taxes raised from this contributed significantly to the revenue of the royal houses in Hanover and Brunswick-Wolfenbüttel and helped to secure their positions of power and influence within the empire.\n\nIts lucrativeness justified a high commitment in terms of investment and effort. The Upper Harz mining industry produced a considerable number of innovations and inventions, including such important advances as the man engine, the water-column engine and the wire cable.\n\nIn the Upper Harz, vein mining (\"Gangerzbergbau\") predominated. Excavation followed the almost vertically standing lodes or veins (\"Erzgängen\") downwards. In their heyday the Upper Harz Mines were among the deepest in the world. For example, as early as 1700 or so shafts were already exceeding depths of 300 metres and, around 1830, a depth of 600 metres was achieved – which was considered significant at that time because it was below sea level.\n\nMining activity in the Harz goes back to the 10th and 11th centuries. The first water wheels to supply energy to the mines were constructed in the 13th century in the Pandelbach valley southeast of Seesen. At that time mining, including this early use of water systems, was carried out by the Cistercian abbey of Walkenried. At first outcropping lodes on the surface of the ground were sought out and sections of ore near the surface were dug out with hammers and chisels. Mining first boomed between 1200 and 1360. In the upper workings there were particularly rich veins of silver ore (up to 9% Ag).\n\nPlague epidemics during the Middle Ages depopulated the Harz to a great extent and almost brought mining operations to a standstill. Another factor was probably that mining had reached its technical limits at the time with depths of up to about 60 m.\n\nA clear recovery followed from about 1520 onwards, initially at the instigation of the Duke of Brunswick-Wolfenbüttel, Henry the Younger. But it was his son, Julius, Duke of Brunswick-Lüneburg, who gave added impetus to existing mining operations in the Upper Harz and initiated the creation of further infrastructure, especially the structures of the Upper Harz Water Regale to provide water power for the mines. In order to entice the necessary labourers, tradesmen and even mining companies to the Harz, the dukes granted 'mining freedoms' (\"Bergfreiheiten\") based on Bohemian and Saxon practice.\n\nBecause the considerable energy needed to drain the mines increased as the mines became deeper and deeper, attempts were made early on to reduce energy consumption by driving drainage adits. This entailed cutting tunnels from the mine into the neighbouring valleys, through which water could drain away downhill under gravity. The deeper the water level lay, the longer these adits needed to be. The longest of these tunnels was the Ernst August Tunnel, built in the mid-19th century, which was 26 kilometres long. It collected water from the mines in Bockswiese, Lautenthal, Zellerfeld, Clausthal and Wildemann and transported it to Gittelde on the edge of the Harz.\n\nThe Upper Harz mines attained their greatest productivity in the 16th and 17th centuries, even though there were frequent crises during that time. In 1690 the metal produced reached a quantity that was not exceeded until 1850. That was especially thanks to the construction of artificial water supply structures and the introduction of gunpowder for rock blasting from 1630 onwards. During the course of the 18th century there were constant crises as a result of the lack of wood. The problem was eased by the introduction of coking coal for the smelters around 1800. On 1 January 1864 the mines were nationalised by the Kingdom of Hanover.\n\nFollowing the annexation of the Kingdom of Hanover by the Kingdom of Prussia in 1866 the Royal Prussian Mining Inspectorate (\"Königlich-Preußische Bergbauinspektion\") took over the running of mines in the Upper Harz. It was succeeded in 1924 by Preussag. Around 1900 shaft depths of 1,000 metres were reached and the mining of ore became increasingly costly. At the same time the mines had to compete with other domestic and foreign mines in a climate of ever-improving transportation. Overexploitation during the First World War and plummeting metal prices resulted in major closures at the height of the Great Depression in 1930, when the big mines around Clausthal-Zellerfeld, Bockswiese and Lautenthal had to close. Mining operations continued in Bad Grund, however, until 1992.\n\nFollowing the closure of the mines in 1930, several shafts switched to the generation of electricity. Here, water from the Upper Harz Water Regale's network of ponds and channels was transported down chutes into the shafts, in which turbines were driven to produce electricity at the level of the deepest drainage adit. The generation of electricity was carried out by Preussag until 1980 in the Kaiser Wilhelm (maximum output 4.5 MW) and Ottiliae (maximum output 1.5 MW) shafts. The hydropower stations were closed in the early 1980s when the water rights expired and the profitability of the power stations continued to fall at a time of sharply rising wages and stagnating electricity prices. These years saw the permanent closure of the last surviving mines.\n\nIn the early days of mining in the Upper Harz simple open cast working (\"Schurfe\") was the predominant method of mining. With increasing depth a form of mixed mining developed that was somewhere between open cast and underground mining. These mines were known as glory holes (\"Pingen\") or simply dip mines (\"Unterwerksbau\"). The ore deposits that lay immediately on the surface were quickly exhausted and, as early as the 12th and 13th century miners were forced to switch entirely over to underground mining. The mining methods that could be used were limited by the steep, almost vertical, lenticles of ore, which were only a few metres wide, but dipped for several hundred metres into the earth. Hauling shafts were usually positioned in centre of the ore allotment on the lode and followed it into the ground. This resulted in inclined shafts with their characteristic, right-angled, longitudinal sections and frequent changes of angle away from the vertical. There were two reasons for this approach: firstly, it had to be possible to extract ore from the beginning (as soon as the shaft was sunk) in order to make the pit economic as early as possible. Secondly, the rock in the ore lode, which formed a 'zone of disturbance', was much softer than the surrounding rock. The typical Harz grauwacke was far harder than concrete. As a result, the majority of drainage adits followed the vein. From the shaft, main gangways, the so-called \"Feldortstrecken\", were driven out to the boundary of the mine allotment. From these gangways, miners began to extract the ore, heading downwards into the floor, by 'brushing down' (\"Nachreißen\") in stepped fashion, a technique known as underhand stoping. The stopes had a height of up to 3 metres and followed one another about 5 to 6 metres apart. In longitudinal section, therefore, a pit looked like a Christmas tree standing on its head. The deepest point of the pit was usually the main shaft. This enabled it to collect pit water in the shaft 'sump'. As mining progressed the shaft was sunk deeper.\n\nThe packing (gangue material used for filling) from the upper main gangways was placed in the exhausted cavities (the so-called 'old man' or \"Alter Mann\"). This required the erection of a wooden ceiling over the active workings so that packing material did not fall into it and onto the face workers there. If the expected supply of ore or its quality did not justify sinking the main shaft deeper, or if the workings were a long way from it, draw-shafts were sunk. These blind shafts saved having to pack the 'old man'. In the Hornstatt, 1 or 2 labourers (\"Knechte\") worked a hand winch and lifted the ore to the next highest main gallery.\n\nFrom 1633 gunpowder was used both for ore extraction and for driving gangways. This increased the daily headway considerably, from a few centimetres into the lode to a metre or more. The disadvantage, however, was that even more wood was needed to extend the mine, because blasting caused the rock to fissure. When blasting, first a cut in the lode was made about 3 metres high and deep and a little less than a metre wide using hammer and chisel. Next one or two transverse boreholes with a 6–7 cm diameter were drilled by hand Usually two-man boring was employed: one turned the borer whilst a second hit it with his sledge. The holes were filled with gunpowder and stuffed with a wooden peg which had a hole for a slow-match wick. Unlike blasting with modern explosive, the stemming had to be wedged in using an iron rod centred on the borehole and a thick wooden prop in a slot (\"Bühnloch\") on the opposite side. This operation frequently led to serious accidents when the gunpowder self-ignited as a result of friction-generated heat. Normal detonation was carried out using cord that had been impregnated with sulphur and gunpowder.\n\nAfter clearing the blast debris, the material to be screened was loaded into wagons (\"Hunde\" or \"Hunte\") using rakes (\"Kratze\") and tubs (\"Trog\"). Larger boulders (\"Wände\") were first broken up with sledges and crowbars.\n\nFrom the second half of the 18th century the method of mining was reversed. Now the roof was always mined and so extraction proceeded upwards. That meant the miners worked on top of the packing and could transport the ore under gravity using so-called chute holes (\"Rollöcher\" or \"Rollen\") rather than shafts. Overhand stoping remained the only mining method in the Upper Harz mines until the end and was perfected in the final years through the use of trackless wagons, roof bolts\n(\"Ankern\"), shotcrete and lean concrete packing. Trials with sublevel stoping (\"Teilsohlenbruchbau\") and square set timbering (\"Blockbau mit Rahmenzimmerung\") did not get past the experimental stage.\n\nIn the middle of the 19th century, the many individual pits transferred to larger mine complexes with central shafts, at which point the sinking of inclined shafts and the mixing of layout and equipment with the workings was abandoned entirely. The central, vertical shafts lay in the host rock (usually in the hanging wall), just as permanently established as the main gangways (usually in the footwall).\n\nTo begin with the ore was chiseled free and carted to the surface of the open pits or shallow mines in baskets. Once shaft depths increased to between about 10–60 metres hand winches (\"Handhäspel\") were used, operated by one or two workers (\"Knechten\"). The crude ore was placed in wooden buckets for transportation. For the rather short, horizontal gangways leading to the shaft the ore was carried in \"Trogs\" for several centuries (long before the introduction of blasting). In the 17th century the shafts reached depths of between 100 and 200 m. Ore could no longer be removed by hand and horsepower was increasingly used. The horses worked in a cone-shaped building, the \"Göpel\" or \"Gaipel\", which housed a horse whim, a winch that was driven by the horses walking in a circle. The hauling cable (made of natural fibre) or cast-iron chain was wound up and down over a vertical axle. The cable was routed down the shaft and hauled barrels of ore up and down. Due to the shaft's incline, barrels were covered with iron runners on one side, resting partly on the side of the shaft. Above ground at the pithead the ore was emptied out and transported away by horse and cart for processing. From the 18th century shaft depths of several hundred metres were being achieved and horse whims were reaching the limits of their capability. Where the mines were lucrative and their energy consumption high as a result of shaft depth or the ingress of water, water power had been used since the 16th century. Water wheels (\"Kunsträder\") drove piston pumps in order to keep the mine dewatered (\"zu Sumpfe\"). Reversible water wheels (\"Kehrräder\") powered the transportation of ore or winnings. Depending on the terrain conditions the reversible wheels were located either in underground wheel houses (\"Radstuben\") near the shaft (the cable drum being set on the same axle as the water wheel) or above ground in the valley. When using the latter method the wheel's rotation was converted into reciprocating motion using a crank mechanism (\n\"Krummen Zapfen\") and transmitted over twin flat rods, several hundred metres long, to the shaft. Here, reciprocating motion was re-converted into rotary motion.\n\nDue to the availability of water power this system was used until the closure of the Clausthal and Lautenthal Pits in the 1930s (e.g. at the Silbersegen Shaft and the Black Pit or \"Schwarze Grube\"). Steam power was first used in earnest when the stone coal necessary for its operation could be delivered by railway towards the end of the 19th century. Electricity began to be generated at about the same time using water power from the Upper Harz Water Regale - an extensive network of ponds, dams, ditches and tunnels, originally built to supply the mines with water power. In 1900 water was passed through turbines and electrical winding engines. At that time modern pits emerged with steel hoist frames. The most important innovation in the Upper Harz hauling technology was the \"Albert Cable\" (\"Albert-Seil\"). Chief Mining Engineer (\"Oberbergrat\") Wilhelm Albert (1787–1846) made a cable out of steel wire which was first successfully tested on 23 July 1834 at the Carolina Shaft. That was the birth of the wire cable. As the distance between shaft and workings lengthened and increasing quantities of material had to be moved, wheelbarrows or small wagons (the \"Hunte\" or \"Hunde\") were used underground as horizontal methods of transportation. Up to 1800 they ran on wooden planks with flangeless wheels and guide pins (\"Spurnägeln\"). Thereafter iron rails took over, initially as hand-forged rails (\"Hammelpfote\") only one metre long. Until 1900 the wagons were almost always pushed by hand. Pit ponies were not used in the Upper Harz. From 1905 at the Clausthal Ore Mine (\"Erzbergwerk Clausthal\") underground haulage was carried out using conductor engines in the gallery known as the \"Tiefsten Wasserstrecke\" or \"Deepest Watercourse\". In the Grund Ore Mine (\"Erzbergwerk Grund\") battery-driven locomotives were used from the 1970s and, finally, diesel engines on wheels with rubber tyres. One feature mining in the Upper Harz was the underground transportation of material in boats on the \"Tiefe Wasserstrecke\" about 300 metres deep, in Clausthal and Zellerfeld from 1835 to 1898.\n\nUntil the beginning of the 19th century the miners of the Upper Harz had to enter and leave the mine using ladders. Towards the end, for shaft depths of around 700 metres this took up to 2 hours of the daily work time. This effort was almost impossible for older miners. In 1833, master miner (\"Oberbergmeister\") Georg Ludwig Wilhelm Dörell (1793–1854) came up with a simple, but ingenious mechanical method of getting in and out of the mine, the man engine. Following successful pilot trials in the Spiegelthal Hope Shaft (\"Spiegelthaler Hoffnungsschacht\"), a light shaft for the Tiefen George Gallery (\"Tiefen-Georg-Stollen\") in Wildemann the first main shaft to be equipped with a man engine was the Duke George William Shaft (\"Herzog Georg Wilhelm\") in the Burgstätter Mining Field. The first man engines had wooden rods with a high dead weight. Due to the water wheel drive and frequent bends in the inclined shafts only a few miners could be transported simultaneously to begin with and they had to periodically switch over to ladders. The use of steel wire cables as rods in the Samson Shaft at St. Andreasberg and steel man engines with steam or water-column engine drives (Queen Maria Shaft) and Emperor William II Shaft) brought improvements. On the introduction of electrical power around 1900 cable-hauled lifts also became common and remained so until the end. In 1905 passenger trains appeared in the underground galleries for the first time (the so-called \"Leuteförderwagen\" or people-transport wagons).\n\nThe processing of minerals in the Upper Harz depended on the type of ore extracted. For example, the density of the Upper Harz lodes was very variable. Unlike the ore at Rammelsberg, the ore minerals were less intermingled with one another and the host rock. This enabled, from the beginning of mining operations in the Upper Harz, ore minerals to be processed into concentrations with a higher metal content than that of unroasted ore.\n\nIn the Middle Ages until the start of the Early Modern Period the ore was broken up above ground using sledges and sorted by hand into silver, lead and copper ores and gangue. The pounding stones (\"Pochsteine\") or stamps used have occasionally been found in recent times during archaeological excavations. The use of water power increased around the turn of the 16th and 17th centuries and it began to be employed in processing to enrich the ore concentration. On the one hand water was used as an energy source; on the other it was used to wash out the unwanted clay and to separate ore from gangue by making use of the different density of the minerals. The tailings from the washing process were simply emptied into the rivers of the Harz along with the used driving water. The low efficiency of the first ore processing machines resulted in a high content of heavy metals in the rivers. As a consequence of using the aforementioned water-based method of processing the stamp mills (\"Pochwerke\") were located in the deeper river valleys. As a rule, they obtained water from the pits, where it had been used to drive water wheels and reversing wheels. Until the beginning of the industrial era, mechanical processing was carried out as follows:\n\n\nThe resulting concentrates (\"Schlieg\" or \"Schliech\") were sold to the smelters. The preparation of the different types of ore was carried out as far as possible by visually sorting the concentrates by hand in order e.g. to separate out lead from copper concentrates.\n\nAfter 1850 the small and scattered stamp mills and ore washeries were replaced by central ore dressing plants. The basic steps - coarse crushing - manual separation - sieving - jigging - fine crushing - table work and slime washing - remained much the same. The process was increasingly mechanised and perfected. In 1905 the most modern ore dressing plant in Germany went into operation in Clausthal using the gravity dressing process. It was located near the Ottiliae Shaft on the site of the old central ore processing plant of 1872. It employed up to 650 workers and processed all ore from the Clausthal and Zellerfeld pits until 1930. A change occurred in the 1920s with the introduction of the froth floatation in Bad Grund and later in Lautenthal. This technique enabled the required production of metal concentration without manual pre-sorting and a much higher yield. The flotation process was steadily developed during the 20th century and was used right up to the end of vein mining in the Upper Harz in 1992.\n\nMining in the Upper Harz is inextricably bound up with metallurgy. It is the preparation and smelting of ore that enables metals to be extracted and used. Only by adapting and developing the smelting processes over the course of the centuries could mining in the region be maintained, because the lodes changed their primary metal content sharply with increasing depth.\n\nThe beginnings of smelting go back to the start of mining in the Upper Harz in the Early Middle Ages. In medieval metallurgy, so-called nomadic smelting (\"Wanderverhüttung\") predominated. The smelting sites were only used for a few weeks and followed the logging of the requisite wood. For the charcoal that was needed for the reduction of the ore, oak and beech wood were especially well-suited. The billets of wood were located near the smelting sites. The low shaft kilns (\"Schachtöfen\") were built of natural rock and earth from the vicinity, and were by no means simple in their construction. They could only be used for a few days of continuous furnace campaign. Fixed buildings were not erected. Over 200 slag sites and smelting sites have been archaeologically recorded from this smelting period. Since the 1980s the mining archaeology team of \"Lothar Klappauf\" and \"Friedrich-Albert Linke\" have carried out excavations and undertaken a considerable amount of archeological and archaeometallurgical research. The high medieval smelting technology of the 10th to 12th centuries at the Rammelsberg was well established and complex. The wood dweller (\"Silvani\"), i.e. those who were doing the smelting in the woodlands, were able to produce copper, lead and silver from the poly-metallic ores of the Rammelsberg.\n\nIn the second major phase of mining in the Upper Harz from 1524, smelting was gradually moved into fixed sites. The transportation of logs as rafts and the use of water power led to the selection of advantageous sites on the rivers in the Harz - such as the Innerste, Grane and Oker. At one location that had already been used in medieval times (1180), the \"Frankenscharrn Hut\" emerged, which later became the Clausthal Lead Smelting Works (\"Bleihütte Clausthal\"), the most famous one in the Upper Harz. It was worked until 31 December 1967. Other important smelters were the silver works (\"Silberhütte\") in Lautenthal (later merged with the \"Bleihütte Clausthal\"), the silver works in Altenau (to 1911) and the Andreasberg Silver Works (\"Silberhütte Andreasberg\", to 1912). After the Upper Harz metal works were closed the ores of the remaining Grund Ore Mine were reduced in the Upper Harz works (to 1981) and finally in the Binsfeldhammer Lead Works near Aachen. The various metalworks, especially the Clausthal Works left behind considerable environmental damage. By contrast, the buildings and facilities in the Upper Harz have completely disappeared.\nFrom the first mining period until just before the industrial age the so-called precipitation method (\"Niederschlagsarbeit\") was used in the Upper Harz. Instead of the usual roasting (desulphurising) of the ore, the slag was melted using charcoal with granulated iron (\"Eisengranalien\") as a reduction medium using the roast-reaction process (\"Röst-Reaktions-Verfahren\") (direct conversion from metal sulphide to metal) in arched kilns (\"Krummofen\"). The comparatively low kiln temperatures of around 1000 °C produced no liquid slag, the residue (gangue) remained in solid form. Not until the development of more powerful fan shaft kilns around 1850 were the concentrates roasted in double-deck ovens (\"Etagenöfen\") and sintering pans and then melted in crucible shaft kilns (\"Tiegelschaftofen\") on silver-containing argentiferous lead (\"Werkblei\") and molten slag. The argentiferous lead was initially worked immediately in the German tests on lightened silver. At the start of the 20th century a multi-stage refining process was carried out in \"Kesselherden\" and silver extracted using the Parkes process.\n\nThe steadily rising demand for wood from the pits and smelting works led to overexploitation of the forests by the Early Middle Ages. Construction wood was needed above ground for accommodation huts as well as mining and smelting buildings. Below ground it was needed to extend the pits. The greatest consumption of wood, however, was for the smelting of ore with charcoal. There were some 30,000 wood billets in the Harz alone.\n\nBy the Early Middle Ages ore had to be transported over kilometres to the smelting works due to the lack of wood. One particularly well-known route is the transportation road from Goslar's Rammelsberg on the northern edge of the Harz over the Upper Harz to Riefensbeek and Kamschlacken on its southern perimeter. Traces of the road may be seen at many places in the Upper Harz forests.\n\nFrom the 18th century a systematic reforestation of the largely destroyed forests was begun. As a result, the Upper Harz contributed significantly to the development of modern forestry. Although not typical of the region, fast-growing spruce trees were exclusively grown in monocultures. The consequences of this intensive forestry, which continued until the 1970s, are still to be seen in many areas of the Upper Harz today.\n\nBecause the shortage of wood was time and again one of the limiting factors for mining and smelting, the forestry situation was a standing agenda item at meetings in the mining office.\n\n\n\n"}
{"id": "3485752", "url": "https://en.wikipedia.org/wiki?curid=3485752", "title": "Nanoart", "text": "Nanoart\n\nNanoArt is a novel art discipline related to science and technology. It depicts natural or synthetic structures with features sized at the nanometer scale, which are observed by electron or scanning probe microscopy techniques in scientific laboratories. The recorded two or three dimensional images and movies are processed for artistic appeal and presented to the general audience.\n\nOne of the aims of NanoArt is to familiarize people with nanoscale objects and advances in their synthesis and manipulation. NanoArt has been presented at traditional art exhibitions around the world. Besides, online competitions have been launched in the 2000s such as the “NANO” 2003 show at Los Angeles County Museum of Art and “Nanomandala”, the 2004 and 2005 installations in New York and Rome by Victoria Vesna and James Gimzewski, and the regular \"Science as Art\" section launched at the 2006 Materials Research Society Meeting.\n\nA characteristic example of nanoart is \"A Boy and His Atom\", a one-minute stop-motion animated film created in 2012 by IBM Research from 242 images sized by 45×25 nm, which were recorded with a scanning tunneling microscope. The movie tells the story of a boy and a wayward atom who meet and become friends. The film was accepted into the Tribeca Online Film Festival and shown at the New York Tech Meet-up and the World Science Festival.\n\nEarlier in 2007 a book \"Teeny Ted from Turnip Town\" was created at the Simon Fraser University in Canada using a gallium-ion beam with a diameter of ~7 nanometers. The book contains 30 silicon-based pages sized by 0.07×0.10 mm; it was published in 100 copies and has an ISBN number.\n\nIn 2015, Jonty Hurwitz pioneered a new technique for creating nanosculpture using multiphoton lithography and photogrammetry. His work \"Trust\" was prepared in collaboration with Karlsruhe Institute of Technology and set a Guinness World Record as the \"Smallest Sculpture of a Human Form\".\n\n"}
{"id": "5916377", "url": "https://en.wikipedia.org/wiki?curid=5916377", "title": "National University of Natural Medicine", "text": "National University of Natural Medicine\n\nThe National University of Natural Medicine (NUNM) is a school of naturopathic medicine and Classical Chinese medicine located in Portland, Oregon, United States. Founded in 1956, it is the oldest naturopathic programs in North America that is accredited by the Council on Naturopathic Medical Education. The school has approximately 553 students.\n\nNUNM has four colleges/schools: College of Naturopathic Medicine, College of Classical Chinese Medicine, School of Graduate Studies, and School of Undergraduate and Part-Time Studies. It offers eight professional graduate degree programs: Doctor of Naturopathic Medicine (ND), Doctor of Science in Oriental Medicine (DSOM), Master of Science in Integrative Medicine Research (MSiMR), Master of Science in Oriental Medicine (MSOM), Master of Science in Nutrition, Master of Science in Global Health, Master of Science in Ayurveda, and Master of Science in Integrative Mental Health. Undergraduate programs include nutrition, integrative health sciences, and integrative therapeutics. These programs include preparation and clinical practice in Holism.\n\nThe School of Graduate Studies offers a two-year Master of Science in Integrative Medicine Research (MSiMR), a program for students interested in complementary and alternative medicine. The Master of Science in Oriental Medicine (MSOM) program is a four-year program in the classical foundations of Chinese medicine. Students receive training in herbalism, acupuncture, moxibustion, Asian bodywork, qigong and nutrition. The Master of Acupuncture (MAc) is a three-year program focusing on classical acupuncture and moxibustion, and providing a shorter course of study, with less theory and herbal instruction.\n\nNUNM is a member of the American Association of Naturopathic Medical Colleges and is accredited by the Northwest Commission on Colleges and Universities, the Council on Naturopathic Medical Education and the Accreditation Commission for Acupuncture and Oriental Medicine.\n\nThe Princeton Review reports that naturopathic medicine program had an acceptance rate of 82% with an average undergraduate GPA of 3.38.\n\nThe NUNM Health Center is a teaching clinic where licensed naturopathic doctors and acupuncturists work with and train students. It is owned and managed by the university. The health center features a medicinary, private offices, conference rooms and a state-licensed laboratory.\n\nThe university also has several community clinics, in conjunction with other agencies and as a member of the Coalition of Community Clinics, which offers low-cost naturopathic care and acupuncture in the Portland metropolitan area. In 2013, the NUNM Community Clinics provided services to more than 40,000 patients.\n\nThe National University of Natural Medicine is the oldest programmatically accredited naturopathic medical school in North America. NUNM began in the early 1950s, in response to the termination of the naturopathic program at Western States Chiropractic College. Members of the profession from Oregon, Washington and British Columbia planned the founding of the school and in May 1956, Charles Stone, W. Martin Bleything and Frank Spaulding executed the Articles of Incorporation of the National College of Naturopathic Medicine in Portland, Oregon.\n\nNCNM opened other satellite campus locations in Seattle and Kansas. NCNM's board of trustees and college administration (including John Bastyr, Joe Boucher, Robert Fleming, Gerald Farnsworth, Joe Pizzorno and Bruce Canvasser) decided to unify all of its campus locations in Portland. The first physical location owned by the college was the Market Street campus in southeast Portland.\nBy 1995, the college began negotiations to purchase its current location in downtown Portland. Classes were relocated to this campus in September 1996 and clinical education was housed in two clinics (Natural Health Center and the Pettygrove Clinic). The historic building that has served as NCNM's main campus since 1996 was built in 1912 as an elementary school named School (in honor of former mayor Josiah Failing) and from 1961 until the 1990s was a Portland Community College campus. In 2009, these clinics were consolidated into one location on campus, the NCNM Clinic.\n\nIn July 2006, NCNM changed its name to the National College of Natural Medicine. In June 2016, the school changed its name to the National University of Natural Medicine.\n\nNUNM's main building was constructed in 1912 as an elementary school in the Portland Public Schools system, named School, for former mayor Josiah Failing. It was a replacement for an 1883-built wooden school building with that name, located about two blocks away, which was torn down in 1922. The NUNM building was designed by Whitehouse & Fouilhoux, the architectural firm of Morris H. Whitehouse and Jacques Fouilhoux. A distinctive feature is the sundial, instead of a traditional clock, adorning the south façade near the roof.\n\nIn June 1996, Bill Naito's company, H. Naito Corporation, purchased the building, with tentative plans to convert it into condominiums. Bill Naito said that part of his motivation was to save the historic structure. Naito died suddenly in May 1996, and the plans to convert the building were dropped. A few months later, in September 1996, the Naito Corp. sold the building to the National College of Naturopathic Medicine.\n\nResearch conducted at NCNM has been called a misuse of limited research funds, as 2.4 million dollars from 2005-2012 were granted by the National Center for Complementary and Integrative Health (NCCIH) and used to support unproven therapies.\n\nThe naturopathic curriculum has been criticized for teaching pseudoscience and quackery, as courses in homeopathy, herbalism, acupuncture, and other alternative treatments without a solid evidence basis are taught as \"primary care medicine\".\n\n\n"}
{"id": "8143783", "url": "https://en.wikipedia.org/wiki?curid=8143783", "title": "New Technologies Demonstrator Programme", "text": "New Technologies Demonstrator Programme\n\nThe New Technologies Demonstrator Programme is a scheme part of Defra's Waste Implementation Programme, New Technologies Workstream, to demonstrate advanced solid waste processing technologies in England. A pot of £30million was allocated to fund 10 demonstrator projects with the project being headed by Dave Brooks at Defra. The scheme is not on schedule for the ambitious targets that were initially set out by Defra, however 9 projects out of the initial 10 are now projected to be operational by April 2009, over 2 years behind schedule.\n\nThe scheme initially was allocated £32 million, of which £2 million was to help fund research and development into waste technology. The scheme for the distribution of the main £30 million pot commenced in 2004 and was originally split into two rounds:\n\nTheir project had a huge response for the first round, with 71 pre-qualification questionnaire submissions being filed from interested parties. The quality of some of the initial bids were criticised by Martin Brockelhurst, Head of Waste Strategy, at the Environment Agency who remarked some of the applications were poor and came from a \"young industry\".\n\nThere have been concerns that the project is taking too long and some participants threatened to walk out. On 11 April 2006, Defra declared that its initial timescales were ambitious and projects were not on target. Of the 10 original projects planned a total of 9 have now been signed and includes gasification, in-vessel composting, anaerobic digestion and mechanical heat treatment. From the original target dates for operational demonstrator plants outlined in the initial assessment criteria only 2 projects are now operational (true as of 27 November 2006). On 24 November 2006, Dave Brooks announced that the new target for all plants being operational is April 2009.\n\n\n\n\n"}
{"id": "37438984", "url": "https://en.wikipedia.org/wiki?curid=37438984", "title": "Oeffa bills", "text": "Oeffa bills\n\nÖffa bills (\"ö\" is a Germanic umlaut that can be transcribed \"oe\") or job-creation bills were promissory notes created in 1932 by the German government. They were aimed at additional fund-raising for public building initiatives and later for job creation schemes. \n\nThe Öffa bills were the blueprint for the Mefo bills which followed the same scheme.\n\nIn 1932, Öffa bills were created by the second cabinet under chancellor Heinrich Brüning after consultation with the then President of the Reichsbank, Hans Luther.\nThe bills were issued by the \"Deutsche Gesellschaft für öffentliche Arbeiten AG\" (), founded 1 August 1930, and rediscounted by the Reichsbank. With the capital thus raised, the \"Deutsche Gesellschaft für öffentliche Arbeiten AG\" financed public building initiatives. \nIt was a shell company without sufficient shareholders' equity. Nevertheless, the bills were discounted by the Reichsbank. This way, the Reichsbank financed public building projects. \n\nIn the wake of the Great Depression, this hidden money creation stimulated the German economy. The German \"Deutsche Gesellschaft für öffentliche Arbeiten AG\" brought into circulation Öffa bills worth 1.26 billion Reichsmark. In general, the duration of a bill was three months but it could be prolonged to five years. \n\nEconomically, this meant an expansion of the money supply. As this would tend towards increasing inflation, Hans Luther agreed to only a small volume.\n\nKurt von Schleicher's second cabinet decided to expand the Öffa bill scheme. Öffa bills could now be issued by other (mostly public) financial institutions such as the \"Deutsche Verkehrskreditbank AG\" which had issued Öffa bills worth 1 billion Reichsmark. After becoming Chancellor in January 1933, Adolf Hitler wanted to extend the scheme to the German re-armament, Hans Luther disagreed, and he was replaced on 16 March 1933 by Hjalmar Schacht. Schacht instituted the Mefo bills, a similar system to the Öffa bills. \n\n"}
{"id": "15676540", "url": "https://en.wikipedia.org/wiki?curid=15676540", "title": "Product design specification", "text": "Product design specification\n\nA product design specification (PDS) is a statement of how a design is made (specify the design), what it is intended to do, and how far it complies with the requirements. Requirements may be gathered in the Product Requirement Specification (PRS). Its aim is to ensure that the subsequent design and development of a product meets the needs (or requirements) of the user. Product design specification is one of the elements of product lifecycle management.\n\n"}
{"id": "1805419", "url": "https://en.wikipedia.org/wiki?curid=1805419", "title": "Programmable interrupt controller", "text": "Programmable interrupt controller\n\nIn computing, a programmable interrupt controller (PIC) is a device that is used to combine several sources of interrupt onto one or more CPU lines, while allowing priority levels to be assigned to its interrupt outputs. When the device has multiple interrupt outputs to assert, it asserts them in the order of their relative priority. Common modes of a PIC include hard priorities, rotating priorities, and cascading priorities. PICs often allow the cascading of their outputs to inputs between each other.\n\nPICs typically have a common set of registers: Interrupt Request Register (IRR), In-Service Register (ISR), Interrupt Mask Register (IMR). The IRR specifies which interrupts are pending acknowledgement, and is typically a symbolic register which can not be directly accessed. The ISR register specifies which interrupts have been acknowledged, but are still waiting for an End Of Interrupt (EOI). The IMR specifies which interrupts are to be ignored and not acknowledged. A simple register schema such as this allows up to two distinct interrupt requests to be outstanding at one time, one waiting for acknowledgement, and one waiting for EOI.\n\nThere are a number of common priority schemas in PICs including hard priorities, specific priorities, and rotating priorities.\n\nInterrupts may be either edge triggered or level triggered.\n\nThere are a number of common ways of acknowledging an interrupt has completed when an EOI is issued. These include specifying which interrupt completed, using an implied interrupt which has completed (usually the highest priority pending in the ISR), and treating interrupt acknowledgement as the EOI.\n\nOne of the best known PICs, the 8259A, was included in the x86 PC. In modern times, this is not included as a separate chip in an x86 PC, but rather as part of the motherboard's southbridge chipset. In other cases, it has been replaced by the newer Advanced Programmable Interrupt Controllers which support more interrupt outputs and more flexible priority schemas.\n\nMore information on the Intel APIC can be found in the \"IA-32 Intel Architecture Software Developer's Manual, Volume 3A: System Programming Guide, Part 1, Chapter 10\", freely available on the Intel website.\n\n\n"}
{"id": "30875037", "url": "https://en.wikipedia.org/wiki?curid=30875037", "title": "Project RAINBOW", "text": "Project RAINBOW\n\nProject RAINBOW was the name given by the CIA to a research project aimed at reducing the radar cross-section of the Lockheed U-2 and lowering the chance that it would be detected and tracked by Soviet radars during its overflights of the USSR. However, the Soviets continued to track the U-2 flights in spite of experimentation with various technological fixes.\n\nThe U-2 was developed by Lockheed Aircraft Corporation for the CIA to perform aerial reconnaissance overflights of the Soviet Union. Project director Richard M. Bissell assured President Dwight Eisenhower that the aircraft's high altitude (70,000 feet) would render it invisible to Soviet radars. However, the earliest flights in July 1956 were, in fact, tracked. On 5 July, an A-100 \"Kama\" radar detected Carmine Vito as he flew over Smolensk, en route to Moscow. The operators even calculated his altitude as , which was later rejected by experts who did not believe that an aircraft could fly that high. S-25 Berkut missiles (NATO designation SA-1 Guild) were not kept at the air defense sites around Moscow, and no intercept was attempted.\n\nIn mid-August, Bissell assembled a group of advisers to begin work on solving the tracking problem. Among the group were Edwin H. Land, founder of the Polaroid Corporation and head of Project Three the Technological Capabilities Panel; Edward Purcell, a Nobel laureate physicist from Harvard; and Clarence L. \"Kelly\" Johnson, head of Lockheed Advanced Development Projects (ADP)—the Skunk Works.\n\nThe group conducted initial discussions. Then Land went to the MIT Lincoln Laboratory to recruit radar specialists for the work. The leader of the Lincoln Lab team was Franklin Rodgers, associate head of the radar division. Working in isolation from the rest of the lab, his group began trying to find ways to reduce the U-2's radar cross section. As their work progressed, they traveled to California to work with Lockheed and to various military bases to perform radar measurements of U-2s in flight.\n\nLockheed developed their own expertise in RCS techniques. A small group headed by L. D. MacDonald included chemist Mel George, physicist Edward Lovick, and other scientists and engineers.\n\nThe radar cross-section (RCS) of an object is a measure of how much electromagnetic (EM) energy is reflected by an object, expressed as an area, typically square meters. The RCS of an object is a function of the object's size, shape, and materials. It also varies depending upon the frequency of the EM energy. Because long distance search/acquisition radars use different frequencies than short range fire control radars, a variety of techniques would have to be used to protect the U-2.\n\nAll parts of the aircraft created reflections—the fuselage, tail, wings, engine inlets and exhaust. The anti-radar techniques investigated fell into two categories, either absorbing the radar energy or creating reflections that interfered with the reflections from the aircraft.\n\nPurcell's first concept was an absorption material to be placed on the U-2's fuselage. Developed by the Lincoln Lab team and Lockheed, it became known as \"Wallpaper.\" It consisted of a conductive pattern printed on a flexible sheet called \"grid\" that was then glued to honeycomb that was then applied to the aircraft. It was intended to be effective against the higher frequency radars.\n\nTo reduce really low-frequency (70 MHz) reflections from the leading and trailing edges of the wings, a wire was placed parallel to and ahead of each wing's leading edge and another parallel to and behind each wing's trailing edge. To anchor the outboard end of each wire, a fiberglass pole was attached to each wingtip to give anchor points ahead and behind the wings. Each wire then ran from the front end of each pole to the slipper tank (which projected in front of the wing) and from the slipper tank to the fuselage. Behind each wing, a wire ran from the back end of the fiberglass pole to the fuselage. The horizontal stabilizer was treated in a similar manner.\n\nTo protect the engine inlets, another wire ran diagonally from the nose to the slipper tank on each wing.\n\nThis scheme was called \"Trapeze.\"\n\nTo reduce low-frequency reflections from the fuselage and vertical stabilizer, wires were strung horizontally from the nose of the aircraft to the tail, and horizontally from the leading edge to the trailing edge of the vertical stabilizer. Ferrite beads were placed on the wires to tune them to the expected frequencies. This technique was called simply, \"Wires.\"\n\nThe disadvantage of Wallpaper was that it was a thermal insulator and trapped heat in the fuselage. Initially it was applied to the upper and lower surfaces, but after the heating problem was recognized, it was applied only to the lower half of the fuselage.\n\nNevertheless, the overheating was to prove fatal. On 2 April 1957 pilot Robert Sieker was conducting a test flight with Wallpaper applied to the U-2 prototype, Article 341. The heat buildup caused the engine to stall. Without power to maintain cockpit pressurization, the faceplate on Sieker's helmet popped open and he lost consciousness. Uncontrolled, the U-2 went into a flat spin. Sieker recovered and bailed out, but at too low an altitude and he was killed.\n\nThe effect of the Wires and Trapeze installations was increased drag. This cost the U-2 5,000 feet in altitude and 20% in range. The pilots were not enthusiastic about the reduced performance, nor in flying an aircraft that one of them likened to being \"wired like a guitar.\"\n\nOn 6 May 1957, Bissell reported to the President about the progress being made, saying that in operational missions, \"the majority of incidents would go undetected.\" In July the first \"dirty bird\" arrived at an operational detachment. The first mission of a \"Covered Wagon,\" as they were also known, took place on 21 July 1957. In all, there were nine flights of the treated aircraft. By May 1958, it had become apparent that the system was not effective, and its use ended.\n\nBy the Fall of 1957, only months after the first deployment of a dirty bird, it had become obvious to Bissell and the scientific team that the treatments would only have a marginal effect on tracking, and that a new aircraft would be needed. By designing in anti-radar features from the beginning, it was hoped that the succeeding aircraft would escape detection.\n\nBissell and his Air Force assistant, Col. Jack Gibbs, had been in discussions with aircraft and materials manufacturers, as well as various laboratories in an effort to understand what materials and designs might succeed. On 4 December 1957, Bissell conducted a meeting at which the various techniques were summed up:\n\n\nA large number of people had become aware of Project RAINBOW. To reduce the spread of information about the follow-up, the work was moved into a new project. Called \"GUSTO,\" only those with a need to know were cleared into it. The end result of GUSTO would be the Lockheed A-12 OXCART.\n\n"}
{"id": "55683256", "url": "https://en.wikipedia.org/wiki?curid=55683256", "title": "RMK-BRJ", "text": "RMK-BRJ\n\nRMK-BRJ was an American construction consortium of four of the largest American companies, put together by the United States Navy during the Vietnam War to build critically needed infrastructure in South Vietnam so that the Americans could escalate the introduction of American combat troops and materiel into Vietnam. This construction contract, amounting to $1.9 billion (equivalent to $14 billion in 2017 dollars), completed a construction program deemed to be the largest in history up to that time (, p.v), (, p. 58), and .\n\nOver the ten-year life of the contract, RMK-BRJ trained 200,000 Vietnamese workers in construction and administrative trades (, p. 3). The use of a civilian contractor and construction force in an active theater of combat operations was authorized for the first time in U.S. history (, p.v) and (, p. 81).\n\nIn the 1950s, the United States Department of Defense assigned responsibility for contract construction in support of military assistance and military construction in regions around the world to the three major branches of defense: the Army, the Navy, and the Air Force. The Navy was assigned as the Department of Defense contract construction agent in Southeast Asia, among other regions (, p. 13) and (, pp. 16–17).\n\nIn late 1961, the U.S. Navy's Bureau of Yards and Docks, known after 1966 as the Naval Facilities Engineering Command (NAVFAC), entered into a contract with some of the largest American construction companies to build infrastructure in Vietnam in support of the Republic of Vietnam. Based upon their experience with dams, ports, highways, and roads, Raymond International, Inc. was selected in partnership with Morrison-Knudsen International, Inc., known for heavy international construction. Raymond had extensive experience driving piles around the world, including Mexico and Tokyo, as well as The Pentagon in WWII. They had both been part of a consortium to build Naval bases in the Pacific in WWII with a $1.5 billion contract (, pp. 28–29). Morrison-Knudsen was designated as the managing partner for the new contract. This consortium was then known as RMK.\n\nBy August 1965, it had become clear that the construction program was growing much larger than originally expected, so the Navy broadened the construction consortium by adding Brown & Root, Inc. and J.A. Jones Construction Co., Inc. (, pp. 139–140). The consortium then became known as RMK-BRJ. The consortium was also known informally as \"The Vietnam Builders\" (, p. 46).\n\nThe original letter contract (NBy-44105) with a fixed price was signed on 8 December 1961 (, p. 30). But as the security condition in Vietnam deteriorated and new construction requirements arose, the contract was changed to a cost-plus contract with a fixed percentage management fee. RMK-BRJ could then be directed to begin projects before design was started or completed, at remote sites, with uncertainty of the local labor forces, and reduced freedom of action due to the security situation (, pp. 31–32). In 1966 as the value of the contract approached $1 billion, the contract was renegotiated to lower the management fee commensurate with the increased scope and award the fee percentage based upon the contractor's performance, a cost-plus-award-fee contract (, pp. 215–216). Under this contract, the Navy provided all materials, equipment, shipping, and transportation.\n\nConstruction work under the contract was completed in June 1972, and the contractor facilities In Saigon were turned over to the Vietnamese government on 3 July 1972 . The final closeout report was presented in October 1972. The final contract value was $1.865 billion, which does not include the value of government-furnished materials, equipment, shipping, and transportation (, p. 427, figure 27).\n\nThe Contracting Officer for the Navy was the Officer in Charge of Construction, Republic of Vietnam (OICC-RVN), with its main office in downtown Saigon. The OICC directed the contractor's work program as well as observing the construction and evaluating the contractor's performance. In February 1967, OICC staff was 1,050, including 90 Navy Civil Engineer Corps officers, at 47 sites and 782 separate projects (, p. 288).\n\nIn 1960, the government of South Vietnam requested the U.S. Military Assistance Advisory Group (MAAG) to develop plans for new military airfields at Bien Hoa north of Saigon, and at the central highlands town of Pleiku, as well as improvements to the French-built airfields in Saigon and Da Nang (, p. 22). One of the first projects for RMK-BRJ was construction of a new airfield at Pleiku. The MAAG made this their priority in January 1962, and wanted the completed airfield by July 1962. Design for the facility had not been started yet. But RMK-BRJ completed it on time and it was opened in July (, p. 30). Air control radar stations at Tan Son Nhut Air Base in Saigon and Monkey Mountain Facility in Da Nang were constructed at the same time.\n\nAfter the Tonkin Gulf incident in August 1964, the deteriorating political situation of the southern government after the assassination of President Ngô Đình Diệm, and an increase in Viet Cong large unit actions, the U.S. government decided to introduce American ground combat troops into Vietnam. On 8 March 1965, 3,500 U.S. Marines of the 3rd Marine Division landed over the beach at Da Nang to protect the airfield at Da Nang, now operated by the U.S. Air Force (, pp. 99–102) and (, p. 19). In the first five months of 1965, U.S. troop levels increase to 55,000. By the end of 1965, 200,000 troops had been introduced into Vietnam (, p. 135) and (, p. 19). Additional escalation of U.S. troop levels to 543,000 continued through 1969. But a build-up of logistics facilities of all kinds was required prior to introduction of more troops into Vietnam ( p. 406).\n\nExisting military logistics facilities within Vietnam were vastly inadequate to support increased troop levels and the materiel required to support them ( p. 406). Only three airfields were capable of jet aircraft operations (, p. 45). Port capacity was limited to the Saigon Port on the Saigon River, and ships were waiting months to offload. Shipping of war materiel as well as economic aid and construction materials and equipment for RMK-BRJ quickly outstripped the port capacity. 99% of all ammunition, and all of the petroleum products required for war operations arrived by sea. RMK-BRJ required 100,000 tons of shipping per month (, p. 202). Additional ports were required to be built as soon as possible (, p. 190).\n\nThe logistics plan developed by General William Westmoreland in early 1965 realized that several more deep-draft seaports must be constructed as quickly as possible, along with accompanying jet-capable airfields with 10,000 foot concrete runways. The war had no fixed front, and it was clear operations would be required throughout the country. So the logistics plans developed \"logistic islands\" or bases around Vietnam from which to seek out the enemy (, pp. 135–136). New ports, air bases, ammunition dumps, petroleum storage, and supply bases would provide a grid in the country from which troops and matériel could be distributed to operating bases inland (, pp. 137–138) and (, p. 162). In November 1965, Secretary of Defense Robert McNamara met with General Westmoreland in Saigon and promised to provide $1 billion in funding for this construction, as well as $200 million to order construction materials and equipment immediately (, p. 198) and (, p. 18).\n\nAdditional deep-draft seaports with 29 berths were to be constructed at Cam Ranh Bay, Qui Nhon, Da Nang, Vung Ro Bay, and Vung Tau, as well as the largest new port in Saigon. Accompanying air bases were to be constructed at Bien Hoa, Cam Ranh, Chu Lai, Phan Rang, Tuy Hoa, and Phu Cat. Storage for matériel was to be constructed at all of these locations, in addition to troop cantonments. All of these requirements were to be fulfilled within two years (, p. 2) and (, pp. 40–41).\n\nAll of the logistical projects were completed in time for the major build-up of U.S. troop levels in 1967 and 1968. At the same time, six naval bases with slips for small craft were constructed, as well as 26 hospitals with 8,280 beds, 20 base camps 10.4 million square feet of warehousing, 3.1 million barrels of petroleum storage, 5,460 square feet of ammunition storage, 75 airfields capable of supporting C-130 supply aircraft, 4,100 kilometers of highways, 182 water wells, and housing for 450,000 Vietnamese service men and their families (, p. 2) and (, pp. 40–41).\n\nOver the ten-year life of the contract, RMK-BRJ moved 91 million cubic yards (71 million cubic meters) of earth, equivalent to a hole 1/4 mile (0.4 km) square and 1/4 mile (0.4 km) deep. 48 million tons of rock products were placed, enough to ballast a railroad halfway around the world. 10.8 million tons of asphalt were placed, enough to pave a 5,500-mile (8,800 km) roadway from Vietnam to Europe. 3,700,000 cubic yards (2.8 million cubic meters) of concrete were placed, enough to build a wall 2-feet (0.6 meters) wide and 5-feet (1.5 meters) tall completely around southern Vietnam. 11.5 million concrete blocks were produced and laid, sufficient to build 16,700 two-bedroom homes. 33 million square-feet (3 million m2) of buildings were erected, equivalent to a skyscraper 6.2 miles (10 km) high, or 550 six-story buildings like the U.S. Embassy built in Saigon .\n\nThe peak of RMK-BRJ employment to meet all these requirements was 51,044 in July 1966. Of these, about 9.5% of the employees were American, 13.5% Third country nationals, and 77% Vietnamese (, p. 201). The work-in-place per month reached $64 million in March 1967 (, p. 281), at 40 construction sites. The actual work-in-place was 50% beyond the planned $40 million work-in-place (, p. 287).\n\nOver 60% of all of the construction done in South Vietnam over the period of the Vietnam War was accomplished by RMK-BRJ, with the remainder done primarily by military engineering construction forces ( p. 406).\n\nIn March 1967, RMK-BRJ held 5,560 pieces of construction equipment with a value of $115 million, plus 1,000 pieces of rented equipment, and the value of construction materials available was $185 million (, p. 287). In early 1966, 196 million board feet of lumber was ordered, which had the effect of absorbing all U.S. west coast lumber sources in that year. 10,000 doors were ordered, as well as 750,000 tons of cement (, p. 200).\n\nIn 1966, RMK leased or chartered 16 aircraft, two LSTs, ten LCMs, 30 barges, and ten tugboats (, p. 201).\n\n52 RMK-BRJ employees were killed and 248 wounded as a result of hostile enemy action. However, RMK-BRJ did 550 million man-hours of work under contract, yet their safety rate was four times less than that of contractors in the United States at that time ( p. 410). RMK-BRJ maintained a medical staff of 130 people in site dispensaries throughout the country, performing over 2 million examinations and treatments .\n\nIn 1966, The U.S. Senate Permanent Subcommittee on Investigations began an investigation into alleged corruption or graft in connection with loss of shipments into Vietnam, including foreign aid materials, Post exchange products, and military construction materials . The subsequent investigation did uncover losses from the Post exchanges in particular. There was loss of RMK-BRJ construction materials due to open storage at major construction sites and at seaports. RMK-BRJ had been directed by the military not to build contractor facilities for materials storage until after the critical ports and airbases had been constructed (, p. 212). Beginning in 1967, RMK-BRJ was then allowed to construct storage facilities, leading to 97 warehouses at 20 sites around the country (, p. 281).\n\nBy 1966, it had become apparent that there was a $200 million shortfall in funding for military construction by RMK-BRJ (, p. 240, p. 331). It was initially thought that this was due to RMK-BRJ mismanagement, but after the subsequent investigation, the Department of Defense reported to the Senate Appropriations Subcommittee that the cost overruns were caused by their own internal processes. The Associated Press reported that \"The Pentagon admits it misled civilian contractors in the billion dollar Vietnam construction program by overstating probable contract awards and under-estimating costs. In the wake of reports alleging company waste and mismanagement, Defense officials praised the private combine known as RMK-BRJ for doing 'an amazingly competent' job under tough circumstances .\"\n\n\n\n\n\n\n\n\n\nThe six seaports, eight jet airports, and highways and bridges continue to serve the people and support the economy of Vietnam today. 200,000 Vietnamese workers were trained in construction and administrative trades by RMK-BRJ, and they continue to work or train their successors today in building up the Vietnamese construction industry (, p.v). At the time, it was recognized that the training of these workers was contributing to increased prosperity of Vietnamese people .\n\nAt the 3 July 1972 close-out ceremony for the RMK-BRJ contract, U.S. Ambassador Ellsworth Bunker stated: \"I am pleased and proud to join in commemorating the completion of the RMK-BRJ construction program in Vietnam. This occasion, which marks the successful conclusion of a decade of achievement, is an especially gratifying and hopeful moment, for it reminds us that construction in the cause of war has also brought construction in the cause of peace and progress… At a time when all too many forces are bent on destruction, RMK- BRJ's ten years of accomplishment have been in my opinion one of the finest episodes in our nation's history\" (, p. 437).\n\n\n"}
{"id": "43734626", "url": "https://en.wikipedia.org/wiki?curid=43734626", "title": "Rezence (wireless charging standard)", "text": "Rezence (wireless charging standard)\n\nRezence (pronounced reh-zense) is an interface standard developed by the Alliance for Wireless Power (A4WP) for wireless electrical power transfer based on the principles of magnetic resonance. The Rezence system consists of a single power transmitter unit (PTU) and one or more power receiver units (PRUs). The interface standard supports power transfer up to 50 watts, at distances up to 5 centimeters. The power transmission frequency is 6.78 MHz, and up to eight devices can be powered from a single PTU depending on transmitter and receiver geometry and power levels. A Bluetooth Smart link is defined in the A4WP system intended for control of power levels, identification of valid loads and protection of non-compliant devices.\n\nThe A4WP was formed in early 2012 with the intent to create a wireless power transfer standard to compete with the existing Qi standard. Board member companies include Broadcom, Gill Electronics, Integrated Device Technology (IDT), Intel, Qualcomm, Samsung Electronics, Samsung Electro-Mechanics, and WiTricity.\n\nIn January 2015 A4WP and the Power Matters Alliance announced that the two organizations intended to merge into AirFuel Alliance.\n\n\n"}
{"id": "965924", "url": "https://en.wikipedia.org/wiki?curid=965924", "title": "Roller (agricultural tool)", "text": "Roller (agricultural tool)\n\nThe roller is an agricultural tool used for flattening land or breaking up large clumps of soil, especially after ploughing or disc harrowing. Typically, rollers are pulled by tractors or, prior to mechanisation, a team of animals such as horses or oxen. As well as for agricultural purposes, rollers are used on cricket pitches and residential lawn areas.\n\nFlatter land makes subsequent weed control and harvesting easier, and rolling can help to reduce moisture loss from cultivated soil. On lawns, rolling levels the land for mowing and compacts the soil surface.\n\nRollers may be weighted in different ways. For many uses a heavy roller is used. These may consist of one or more cylinders made of thick steel, a thinner steel cylinder filled with concrete, or a cylinder filled with water. A water-filled roller has the advantage that the water may be drained out for lighter use or for transport. In frost-prone areas a water filled roller must be drained for winter storage to avoid breakage due to the expansion for water as it turns to ice.\n\nOn tilled soil a one-piece roller has the disadvantage that when turning corners the outer end of the roller has to rotate much faster than the inner end, forcing one or both ends to skid. A one-piece roller turned on soft ground will skid up a heap of soil at the outer radius, leaving heaps, which is counter-productive. Rollers are often made in two or three sections to reduce this problem, and the Cambridge roller overcomes it altogether by mounting many small segments onto one axle so that they can each rotate at local ground-speed.\n\nThe surface of rollers may be smooth, or it may be textured to help break up soil or to groove the final surface to reduce scouring from rain. Each segment of a Cambridge roller has a rib around its edge for this purpose. The name cultipacker is often used for such ridged types, especially in the United States. \n\nRollers are a secondary tillage tool used for flattening land or breaking up large clumps of soil, especially after ploughing or disc harrowing. Rollers are typically pulled by tractors today. Before mechanised agriculture, a team of working animals such as horses or oxen provided the power. Animal power is still used today in some contexts, such as on Amish farms in the United States and in regions of Asia where draft oxen are still widely used. \n\nRollers prepare optimal seedbeds by making them as flat as is practical and moderately firmed. Flatness is important at planting because it is the only practical way to control average seed planting depth without laborious hand planting of each seed; it is not practical to follow an instruction of (for example) 1-cm planting depth if the contour of the seedbed varies by 2 cm or more between adjacent spots. This is why breaking up of even small clods/lumps, and well-leveled spreading of soil, is important at planting time. \n\nFlatter land also makes subsequent weed control and harvesting easier. For example, in mechanical weed control, controlling cultivator tooth depth is practical only with a decently flat soil contour, and in combining, controlling combine head height is practical only with a decently flat soil contour. \n\nRolling is also believed to help reduce moisture loss from cultivated soil. \n\nRollers may be ganged to increase the width of each pass/swath. Rollers may be trailed after other equipment such as ploughs, disc harrows, or mowers.\n\nIn cricket, rollers are used to make the pitch flat and less dangerous for batsmen.\n\nSeveral size rollers have been used in the history of cricket, from light rollers that were used in the days of uncovered pitches and at some stages during the 1950s to make batting less easy, to the modern “heavy roller” universally used in top-class cricket today. Regulations permit a pitch only to be rolled at the commencement of each innings or day’s play, but this has still had a massive influence on the game by eliminating the shooters that were ubiquitous on all but light soils before heavy rollers were used. Heavy rollers have sometimes been criticised for making batting too easy and for reducing the rate at which pitches dry out after rain in the cool English climate.\n\nLawn rollers are designed to even out or firm up the lawn surface, especially in climates where heaving causes the lawn to be lumpy. Heaving may result when the ground freezes and thaws many times over winter. Where this occurs, gardeners are advised to give the lawn a light rolling with a lawn roller in the spring. Clay or wet soils should not be rolled as they become compacted.\n\n\n"}
{"id": "16225796", "url": "https://en.wikipedia.org/wiki?curid=16225796", "title": "SQEP", "text": "SQEP\n\nSQEP is an acronym, standing for Suitably Qualified and Experienced Person. It is usually used to designate:-\nThis overall route is usually well-documented to satisfy external regulatory authorities that appropriate personnel are in place along the supply chain to undertake critical responsibilities.\n\nAlthough not widely used, the term has gained some currency in the UK Nuclear power industry, see for example this safety management audit report from the Health and Safety Executive.\nIn the UK nuclear context, it is a standard requirement for licensed sites that \"The licensee shall make and implement adequate arrangements to ensure that only suitably qualified and experienced persons \nperform any duties which may affect the safety of operations on the site or any other duties assigned by or under these conditions or any arrangements required under these conditions.\". \nIn this context, the term is not restricted to professionally qualified personnel or to duties requiring significant technical expertise: \"any\" means \"any\" It is essential that all personnel whose activities have the potential to impact on nuclear safety are suitably qualified and experienced (SQEP) to carry out their jobs. This includes both those who directly carry out operations and others such as directors, managers, designers, safety case authors etc whose roles, if inadequately conceived or executed, may affect safety in less visible ways – for example, through introducing latent technical or organisational weaknesses. and conversely \"suitably\" means \"suitably\", not \"particularly well\": the Office of Nuclear Regulation takes SQEPness to be broadly equivalent to the International Atomic Energy Agency concept of 'competence' IAEA has defined competence as “the ability to put skills and knowledge into practice in order to perform a job in an effective and efficient manner to an established standard” ONR concurs with this definition, which is widely accepted within the international nuclear community. Other factors contributing to a person’s competence include the person’s prior experience, aptitudes, attitudes, behaviours, skills and qualifications.\n"}
{"id": "1791259", "url": "https://en.wikipedia.org/wiki?curid=1791259", "title": "Smart antenna", "text": "Smart antenna\n\nSmart antennas (also known as adaptive array antennas, digital antenna arrays, multiple antennas and, recently, MIMO) are antenna arrays with smart signal processing algorithms used to identify spatial signal signatures such as the direction of arrival (DOA) of the signal, and use them to calculate beamforming vectors which are used to track and locate the antenna beam on the mobile/target. Smart antennas should not be confused with reconfigurable antennas, which have similar capabilities but are single element antennas and not antenna arrays.\n\nSmart antenna techniques are used notably in acoustic signal processing, track and scan radar, radio astronomy and radio telescopes, and mostly in cellular systems like W-CDMA, UMTS, and LTE. \n\nSmart antennas have many functions: DOA estimation, beamforming, interference nulling, and constant modulus preservation..\n\nThe smart antenna system estimates the direction of arrival of the signal, using techniques such as MUSIC (MUltiple SIgnal Classification), estimation of signal parameters via rotational invariance techniques (ESPRIT) algorithms, Matrix Pencil method or one of their derivatives. They involve finding a spatial spectrum of the antenna/sensor array, and calculating the DOA from the peaks of this spectrum. These calculations are computationally intensive.\n\nMatrix Pencil is very efficient in case of real time systems, and under the correlated sources.\n\nBeamforming is the method used to create the radiation pattern of the antenna array by adding constructively the phases of the signals in the direction of the targets/mobiles desired, and nulling the pattern of the targets/mobiles that are undesired/interfering targets.\nThis can be done with a simple Finite Impulse Response (FIR) tapped delay line filter. The weights of the FIR filter may also be changed adaptively, and used to provide optimal beamforming, in the sense that it reduces the Minimum Mean Square Error between the desired and actual beampattern formed. Typical algorithms are the steepest descent, and Least Mean Squares algorithms. In digital antenna arrays with multi channels use the digital beamforming, usually by DFT or FFT.\n\nTwo of the main types of smart antennas include switched beam smart antennas and adaptive array smart antennas. Switched beam systems have several available fixed beam patterns. A decision is made as to which beam to access, at any given point in time, based upon the requirements of the system. Adaptive arrays allow the antenna to steer the beam to any direction of interest while simultaneously nulling interfering signals. Beamdirection can be estimated using the so-called direction-of-arrival (DOA) estimation methods.\n\nIn 2008, the United States NTIA began a major effort to assist consumers in the purchase of digital television converter boxes. Through this effort, many people have been exposed to the concept of smart antennas for the first time. In the context of consumer electronics, a \"smart antenna\" is one that conforms to the EIA/CEA-909 Standard Interface.\n\nIn 2017, the Israeli Aerospace Industries have unvailed an adaptive array antenna called ADA, and stated that it is already operational and shall be fitted onto \"major platforms\" used by the IDF.\n\nPrior to the final transition to ATSC digital television in the United States on June 11, 2009, two smart antenna models were brought to market:\n\n\nAnd two models are causing consumer confusion:\n\n\n\nSmart antenna systems are also a defining characteristic of MIMO systems , such as the IEEE 802.11n standard. Conventionally, a smart antenna is a unit of a wireless communication system and performs spatial signal processing with multiple antennas. Multiple antennas can be used at either the transmitter or receiver. Recently, the technology has been extended to use the multiple antennas at both the transmitter and receiver; such a system is called a multiple-input multiple-output (MIMO) system . As extended Smart Antenna technology, MIMO supports spatial information processing, in the sense that conventional research on Smart Antennas has focused on how to provide a digital beamforming advantage by the use of spatial signal processing in wireless channels. Spatial information processing includes spatial information coding such as Spatial multiplexing and Diversity Coding, as well as beamforming.\n\n\n"}
{"id": "6504050", "url": "https://en.wikipedia.org/wiki?curid=6504050", "title": "Stevie Awards", "text": "Stevie Awards\n\nThe Stevie Awards are a set of hundreds of business awards given annually by the American Business Awards organization. They were created in 2002 to recognize accomplishments and contributions of companies and business people worldwide. The 2002 awards were called \"The American Business Awards\"; the 2003, \"The International Business Awards\", since then the present title has been used. Approximately 30-40% of entrants receive an award.\n\nMichael P. Gallagher, an American businessman, conceived the Stevie Awards as a way to \"restore public confidence and investor trust\" after the Enron scandal in 2001. Gallagher left his job in 2001 and founded American Business Awards to administer the Stevies. When launched in 2002, the awards were described by the \"New York Post\" as being intended to \"distinguish the good guys from the scoundrels\" during a period heightened scrutiny and distrust of managers and CEOs. The first Stevies was awarded in 48 categories in April 2003 and judged by a panel including Rich Karlgaard, the editor of \"Forbes\" magazine and Richard Klimoski, Dean of the School of Management at George Mason University. \n\nStevie is taken from the name Stephen, which is derived from the Greek for \"crowned\".\n\nThe charge to be considered for a Stevie in 2003 ranged from $200 to $400. As of 2014, entry fees range up to $505. There is an additional fee for attending the awards dinner.\n\nAwards are judged each year by figures in business worldwide who participate in an evaluation process of nominees. Their recommendations for winners are announced at annual awards ceremonies held in New York City and other locations.\n\nAccording to the organization, awards are given in hundreds of categories, and 30-40% of entrants receive an award. In 2017, there were 14 main categories for which awards were given including: company/organization, customer service, human resources, IT, live event, management, marketing, mobile website & app, new product, public relations, publications, support, video, and website.\n\nThe trophy is manufactured by R. S. Owens as a 16-inch tall, hand-cast statuette finished in 24-karat gold, holding a crystal pyramid representing Maslow's hierarchy of needs.\n\n"}
{"id": "4269572", "url": "https://en.wikipedia.org/wiki?curid=4269572", "title": "Stripline", "text": "Stripline\n\nStripline is a transverse electromagnetic (TEM) transmission line medium invented by Robert M. Barrett of the Air Force Cambridge Research Centre in the 1950s. Stripline is the earliest form of planar transmission line.\n\nA stripline circuit uses a flat strip of metal which is sandwiched between two parallel ground planes. The insulating material of the substrate forms a dielectric. The width of the strip, the thickness of the substrate and the relative permittivity of the substrate determine the characteristic impedance of the strip which is a transmission line. As shown in the diagram, the central conductor need not be equally spaced between the ground planes. In the general case, the dielectric material may be different above and below the central conductor.\n\nTo prevent the propagation of unwanted modes, the two ground planes must be shorted together. This is commonly achieved by a row of vias running parallel to the strip on each side.\n\nLike coaxial cable, stripline is non-dispersive, and has no cutoff frequency. Good isolation between adjacent traces can be achieved more easily than with microstrip.\nStripline provides for enhanced noise immunity against the propagation of radiated RF emissions, at the expense of slower propagation speeds when compared to microstrip lines. The effective permittivity of striplines equals the relative permittivity of the dielectric substrate because of wave propagation only in the substrate. Hence striplines have higher effective permittivity in comparison to microstrip lines, which in turn reduces wave propagation speed (see also velocity factor) according to\n\n\"Stripline\", now used as a generic term, was originally a proprietary brand of Airborne Instruments Laboratory Inc. (AIL). The version as produced by AIL was essentially air insulated (air stripline) with just a thin layer of dielectric material - just enough to support the conducting strip. The conductor was printed on both sides of the dielectric. The more familiar version with the space between the two plates completely filled with dielectric was originally produced by Sanders Associates who marketed it under the brand name of \"triplate\".\n\nStripline was initially preferred to its rival, microstrip, made by ITT. Transmission in stripline is purely TEM mode and consequently there is no dispersion (provided that the dielectric of substrate is not itself dispersive). Also, discontinuity elements on the line (gaps, stubs, posts etc) present a purely reactive impedance. This is not the case with microstrip; the differing dielectrics above and below the strip result in longitudinal non-TEM components to the wave. This results in dispersion and discontinuity elements have a resistive component causing them to radiate. In the 1950s Eugene Fubini, at the time working for AIL, jokingly suggested that a microstrip dipole would make a good antenna. This was intended to highlight the drawbacks of microstrip, but the microstrip patch antenna has become the most popular design of antenna in mobile devices. Stripline remained in the ascendent for its performance advantages through the 1950s and 1960s but eventually microstrip won out, especially in mass produced items, because it was easier to assemble and the lack of an upper dielectric meant that components were easier to access and adjust. As the complexity of printed circuits increased, this convenience issue became more important until today microstrip is the dominant planar technology. Miniaturisation also leads to favouring microstrip because its disadvantages are not so severe in a miniaturised circuit. However, stripline is still chosen where operation over a wide band is required.\n\nMicrostrip is similar to stripline transmission line except that the microstrip is not sandwiched, it is on a surface layer, above a ground plane.\nStripline is more expensive to fabricate than microstrip, and because of the second groundplane, the strip widths are much narrower for a given impedance and board thickness than for microstrip.\n\n\n\n"}
{"id": "41784", "url": "https://en.wikipedia.org/wiki?curid=41784", "title": "Teletraining", "text": "Teletraining\n\nTeletraining is training that \n\n\"Synonyms\"\n\n"}
{"id": "24049949", "url": "https://en.wikipedia.org/wiki?curid=24049949", "title": "The Millionaire Calculator", "text": "The Millionaire Calculator\n\nThe Millionaire calculator was the first commercially successful mechanical calculator that could perform a direct multiplication. It was in production from 1893 to 1935 with a total of about five thousand machines manufactured.\n\nThe principle of a calculation machine with progressive transmission of tens was invented by Chebyshev and demonstrated at the 1878 World's fair in Paris. In 1881 Chebyshev demonstrated a model of the calculation machine with automatic multiplication but did not take out a patent for it.\n\nIn 1834 Luigi Torchi of Milan invented a direct multiplication machine. The first patented multiplying machines was due to Edmund Barbour (1872), Ramón Verea (1878) and Léon Bollée (1889). The Bollée machine could be considered the direct ancestor of the \"Millionaire\".\n\nDesigned by Otto Steiger, a Swiss engineer, the moving carriage of the Millionaire has a 20 decimal digit accumulator that shows the product after multiplication and into which dividend is entered prior to division. The 10-digit multiplicand or divisor is entered on the sliders\n(or keyboard, on later models) above the carriage, while successive digits of the multiplier or quotient are entered with a push-button lever on the upper left. A large control knob on the upper right can be set to add, multiply, divide or subtract positions.\n\nThe \"Millionaire\" was first patented in Germany in 1892. Patents were issued in France, Switzerland, Canada and the USA in 1893, and production started in 1893. From 1899 to 1935 Hans W. Egli of Zürich handled the machine. The American agent for the Millionaire was W. A. Morschhauser of New York. During this long life, 4,655 \"Millionaire\" machines were sold.\n\nThe Millionaire was advertised as being the \"only calculating machine on the market ... that requires but one turn of the crank ... for each figure in the multiplier or quotient,\" making it the fastest calculator available. Advertising from 1913 claims that the United States government had purchased over 100 Millionaire calculators.\n\nAll mechanical calculators commercialized prior to the Millionaire, like the arithmometer, the Odhner arithmometer or the comptometer were simple adding machines; they implemented multiplication by continued addition under operator control. In 1889, Léon Bollée, in France, invented a machine that required only one turn of the crank handle to multiply the number entered on the sliders by a multiplier number. This was accomplished by creating a \"mechanical representation\" of the multiplication table which could be read and used by the machine. The manufacturing cost of Bollée's machine was too high and the production was discontinued after a few units. The Millionaire was built with the same target of direct mechanical multiplication in mind. \n\nIn first decades of 20th century two other machines with direct multiplication were produced: the Moon-Hopkins and Kuhrt-US. These two companies were then taken over by Burroughs and Brunsviga. These machines filled quite a different niche from the Millionaire. They were book-keeping machines with printing features, and were too unwieldy to perform divisions and complex computations. The Millionaire, however, was better suited for technical computations.\n\nThis machine was very big and heavy and occupied an entire desk. Its size made it awkward to operate.\n\nIt was commercialized as \"The Millionaire\" in English speaking countries, \"La Millionnaire\" in French and \"Millionär\" in German speaking countries.\n\n"}
{"id": "4803487", "url": "https://en.wikipedia.org/wiki?curid=4803487", "title": "U.S. Poultry &amp; Egg Association", "text": "U.S. Poultry &amp; Egg Association\n\nThe U.S. Poultry & Egg Association is an American industry trade group located in Tucker, Georgia that \"represents its poultry and egg members through research, education, communications and technical services.\"\n\nFounded in 1947, it is the world's largest and most active poultry organization. Billed as an \"All Feather\" association, membership includes producers and processors of broilers, turkeys, ducks, eggs, and breeding stock, as well as allied companies. \n\nIt currently has affiliations in 26 states and member companies worldwide. They also sponsor the International Poultry Expo. \n\nThe group posts position papers on topics related to poultry and egg production, including controversial and timely topics such as factory farming, genetically modified organisms, induced molting for egg-laying chickens, regulatory efforts, and avian influenza.\n\n"}
{"id": "42599573", "url": "https://en.wikipedia.org/wiki?curid=42599573", "title": "Youngs Trophy", "text": "Youngs Trophy\n\nThe Young Trophy is the annual inter-institution sporting competition for young engineers. The event was first held in 1933 when a joint team from the Institution of Mechanical Engineers and the Institution of Civil Engineers challenged the Institute of Electrical and Electronics Engineers (now part of the Institution of Engineering and Technology) to a cricket and tennis match. Subsequent tournaments have been contested over a range of sports such as football, badminton, volleyball, basketball and dodgeball. The participants have also expanded beyond the original three institutions with teams from Royal Institute of British Architects and Chartered Institution of Building Services Engineers competing in 2013.\n"}
