{"id": "42866081", "url": "https://en.wikipedia.org/wiki?curid=42866081", "title": "5Rocks", "text": "5Rocks\n\n5Rocks is a company established in South Korea that specializes in providing business intelligence for lifetime value maximization for mobile game developers, publishers and ad network service providers. Its analytics tool offers not only analysis but also operation tool for mobile game clients. Based on its unique cohort analytics and combined operation, mobile game operator or developer are enabled to communicate with their game users. 5Rocks provides push notification and in-app-campaign such as announcement, in-app product promotion, rewards, and cross promotion.\nIn Apr 2014, the company announced that it had secured more than 400 clients over the world.\n5Rocks is funded by KDDI, Adways, Stonebridge Capital and recently Global Brain. Mobile monetization and ad-tech firm Tapjoy acquired 5Rocks. \n"}
{"id": "1952396", "url": "https://en.wikipedia.org/wiki?curid=1952396", "title": "APC by Schneider Electric", "text": "APC by Schneider Electric\n\nAPC by Schneider Electric, formerly known as American Power Conversion Corporation, is a manufacturer of uninterruptible power supplies, electronics peripherals and data center products.\n\nIn 2007, Schneider Electric acquired APC and combined it with MGE UPS Systems to form Schneider Electric's Critical Power & Cooling Services Business Unit, which recorded 2007 revenue of $3.5 billion (EUR 2.4 billion) and employed 12,000 people worldwide. Until February 2007, when it was acquired, it had been a member of the S&P 500 list of the largest publicly traded companies in the United States. Schneider Electric, with 113,900 employees and operations in 102 countries, had 2008 annual sales of $26 billion (EUR 18.3 billion).\n\nIn 2011 APC by Schneider Electric became a product brand only, while the company was rebranded as the IT Business Unit of Schneider Electric.\n\nAPC was founded in 1981 by three MIT Lincoln Lab electronic power engineers. Originally, the engineers focused on solar power research and development. When government funding for their research dried up, APC shifted its focus to power protection by introducing its first UPS in 1984.\n\nIn July 1988, APC became a publicly held company, traded on the NASDAQ exchange under the ticker symbol APCC. In February 2007, American Power Conversion was delisted from the NASDAQ exchange, and replaced in the S&P 500 Index by Hudson City Bancorp.\n\nSchneider Electric announced its acquisition of APC on October 30, 2006 and completed it on February 14, 2007. \nAPC share-holders approved the deal on January 16, 2007. The European Union authorized the merger, provided that Schneider divest itself of the MGE UPS SYSTEMS global UPS business below 10kVA. Late in 2007 Eaton Powerware bought the MGE Office Protection Systems division of Schneider.\n\nToday, the company focuses its efforts on four application areas: \n\nAPC Symmetra LX is a line of uninterruptible power supply products, aimed at network and server applications. Symmetras come in power configurations ranging from 4 kVA to 16 kVA. Symmetras are built for use in a data center, (in a 19-inch rack for example). They include features such as integrated manageability, hot-swappability, user replaceable power, battery and intelligence modules. Typical applications include web and other application servers, IP based and traditional PBX voice solutions, and enterprise type network switches.\n\nAPC Symmetra PX1 (THOR 20-80 KVA) is a line of Uninterruptible Power Supply Products aimed at the small, medium market of data servers rooms. With user changeable Power Modules, Battery Modules, and 24/7 Hotline Support which typically can diagnose a problem via telephone call and/or direct connection access of its SNMP (Simple Network Management Protocol) Card, have become \"THE LEGENDARY RELIABILITY\" Leader in the UPS Marketplace with a demonstrative\n99.4% Online Uptime. This platform has become the most stable, modular redundancy (no hard failure faults to turn off unit in event of a module failure), and placed itself firmly in the hearts of all IT Business Executives at all levels.\n\nAPC's signature product is InfraStruXure.\n\nAPC Smart-UPS is a line of smaller units intended for home and office use, available as floor-standing and rackmount versions. With the exception of the Smart-UPS Online series (SURT and SRT models), Smart-UPS units are line-interactive UPS systems, running their outputs off the inverters only when the grid power is unavailable.\n\nPowerChute is a computer program by APC used to control the uninterruptible power supplies (UPS) the company produces. It provides unattended shutdown of servers and workstations in the event of an extended power outage. It also monitors and logs the UPS status. Some versions with reduced functionality are shipped together with the UPS units sold by APC, while other versions have to be bought separately.\n\nPowerChute Business Edition requires servers to be connected via serial port or USB to the monitored Smart-UPS equipment. It provides UPS management and safe server shutdown for up to 25 servers. UPS Network Management Cards made by APC are enabling UPS management, by directly connecting the UPS to the network with its own IP address, avoiding dependence or the need for a server, which is particularly useful in wiring closets where frequently no servers are present. PowerChute Network Shutdown, together with the UPS Network Management Card, enables safe server shutdown by communicating over a network to any network-enabled APC Smart-UPS (those that contain an UPS network management card).\n\n\n"}
{"id": "7528108", "url": "https://en.wikipedia.org/wiki?curid=7528108", "title": "Air shower (room)", "text": "Air shower (room)\n\nAir showers are specialized enclosed antechambers which are incorporated as entryways of cleanrooms and other controlled environments to reduce particle contamination. Air showers utilize high-pressure, HEPA- or ULPA-filtered air to remove dust, fibrous lint and other contaminants from personnel or object surfaces. The forceful \"cleansing\" of surfaces prior to entering clean environments reduces the number of airborne particulates introduced. \n\nWhen properly incorporated into cleanroom design, air showers provide an ISO-classified transition vestibule to ensure the cleanliness of the classified cleanroom . Air showers are typically placed between a gowning area and cleanroom; after workers don appropriate garb and personal protective equipment, they enter the shower so that the pressurized air nozzles remove any residual particles from coveralls. Once the program cycle is complete, users exit out through a second door, into the cleanroom. Air showers (or air tunnels) may also be placed between cleanrooms of different ISO ratings.\n\nAir showers are generally constructed from cleanroom-compatible steel or plastic materials and feature electronically-powered blowers, filters and high-pressure jet nozzles, the latter being incorporated into the walls and ceiling of the chamber. Air, at velocities of 3,000 to 7,000 feet-per-minute (FPM), continuously streams from the jet nozzles for 30-45 seconds, effectively removing loose particulate matter. Personnel inside the enclosure will lift their arms and turn their bodies for uniform exposure to the air streams, a procedure usually specified in protocol. Air currents from the jets create shearing and flapping forces, which lift and remove contaminants from both flat surfaces and the creases of garments. HEPA filtration within the air shower is capable of removing 99.97% of particles greater than 0.3µm diameter.\n\nAir is channeled within the air shower using a closed-loop system, where it is continuously re-circulated. Air is forced through a motorized filter/blower module into a large plenum, then into the shower through jet nozzles. Particle-laden, contaminated air is routed out of the shower compartment through floor vents and returned to the filtration unit. This process ensures that only decontaminated air is used to remove particulates from personnel and other equipment, such as supply carts. \n\nOften, air showers are equipped with air ionizers to reduce static electricity, as large volumes of high-velocity air create electric charges. Since laboratory equipment, electronic measuring instruments and many hi-tech manufactured goods can often be damaged by electrostatic discharge, ionizers are essential in rendering material surfaces electrically neutral prior to entering the cleanroom.\n\nInterlocking mechanisms are a common air shower feature to prevent both exits from being opened simultaneously, which would allow outside air to enter a tightly controlled environment. This requires occupants to stay inside until the decontamination cycle has completed. For this reason, safety features such as emergency stops are required by most safety administrations. Alternatively, the air shower may consist of a long tunnel not equipped with doors; personnel slowly walk through to reach the controlled area. Air handling equipment creates an isolated atmosphere using pressure differentials to create fluid boundaries between the inner and outer environments.\n"}
{"id": "19868959", "url": "https://en.wikipedia.org/wiki?curid=19868959", "title": "Australian Institute of Building", "text": "Australian Institute of Building\n\nThe Australian Institute of Building(AIB) is a professional society founded in 1951, incorporated in 1955 and granted a Royal Charter in 1969. The Institute is an association of building professionals, associate professionals and technicians engaged in building practice, teaching, or research throughout Australia and overseas. It has chapter offices in Hong Kong and Singapore and had a chapter office in New Zealand until the formation of the New Zealand Institute of Building in 1984.\n\nServing building professionals, the AIB has helped create standards for professional competency and regulation of education standards for the various educational programs related to its mission. It has played the lead role in the establishment of all building and construction management undergraduate programs offered by Australian universities.\n\nThe mission of AIB is to be a leading body of focus for the building industry, valued for its services to its members, reflecting its ideals for education, standards and ethics and the source of authoritative and visionary comment on behalf of professionals in the industry. Its objectives are: to promote excellence in the construction of buildings and just and honourable practices in the conduct of business; to advance the study of Building and all kindred matters, arts and sciences; to encourage the friendly exchange between members of knowledge in practical, technical and ethical subjects; and to uphold the dignity of the profession of Building and the status of the Institute.\n\nFollowing World War II the building industry was confronted with a huge programme of civil and commercial building. This called for improved productivity, more intelligent use of resources available and the development of new techniques and new material. Leaders of the industry appreciated that building was developing from empirical craft processes to a technological discipline spanning physical sciences and involving construction techniques which were unknown in the early part of the 20th century. Development in the science of environment and building services had also added a new dimension to building technology.\n\nAs building involved investment of public and private capital to a greater extent than any other commodity, rational, economic and efficient working was essential. Skill in the management of building work is as important as the technology itself. A trend to higher educational standards and a more positive approach to training executive and technical staff of Building organisations was clearly required.\n\nThe effect of this impact on the building industry was to highlight the need for a professional body to promote efficiency and establish a high standard of technological education for those engaged on the construction side of the industry.\n\nA 'Committee of Investigation' was set up in 1947 by a convention called by the then Master Builders' Federation of Australia (MBFA) which, although firmly established since 1890, was not acceptable as a professional body because it was an employers' organisation. There was agreement that an Institute should be constituted on professional lines to represent the construction side of the building industry, and both the Royal Australian Institute of Architects (RAIA) and the Institution of Engineers Australia (IEAust) agreed to assist in selecting the foundation membership.\n\nThe Australian Institute of Builders was established in 1951 with a foundation membership determined by a selection committee in each state, comprising two representatives of the RAIA, one representative of the IEAust and two representatives of the MBFA. The Foundation Members also included members of the Building Diplomats Association of NSW, associated with the Sydney Technical College.\n\nOn 23 November 1951 the Foundation Dinner was held in Sydney and the then Mr Robert Menzies, Prime Minister of Australia, was presented with a Certificate of Honorary Membership. The Institute was incorporated on 15 November 1955, granted Armorial Bearings on 20 August 1960 and incorporated by Royal Charter on 7 October 1969, despite the protests of the CIOB(UK). It had, on 26 May 1967, changed its name to The Australian Institute of Building (AIB).\n\nHaving established itself and received recognition accorded by a Royal Charter, the Institute began to redefine its educational requirements at Licentiate and Corporate grade.\nIt then set about ensuring that the then degree and certificate courses in building which were available met the standards which the Institute required. From this has developed a course assessment/qualification accreditation system which has proven to be of great value to academic institutions and the industry.\n\nWhile graduates in building were recognised immediately by the private sector, recognition in the public sector was harder to achieve. The breakthrough came in 1977 with the recognition by the Public Service Board of Degrees in Building for the positions of Project Manager Class 1-3, Area Manager Class 1-3 and Construction Manager. Recognition of the AIB as a professional institute was accorded in the Commonwealth Gazette in March 1983. This approval, under the Industrial Research and Development Incentives Act, recognises Corporate members of the Institute as professionally qualified.\n\nSince its establishment in 1968 the Australian Institute of Building Research and Education Foundation has funded developments in education and specific research projects. One of these projects determined the duties and responsibilities of, and the levels of competence required by, professionals and technicians in the Building industry. In 1990 the AIB became an Associate of the International Council for Building Research (CIB). The AIB Papers, a journal of refereed academic papers, has been published by the Institute since 1986.\n\nThe By-laws and Regulations were amended to allow for changes to grades of member in 1981, 1992 and 1997.\n\nWith the Foundation of a New Zealand Institute of Building (NZIOB) in 1984, the New Zealand Chapter of the AIB was closed down. The AIB continues to have a close relationship with NZIOB.\n\nIn 1997 the AIB became a foundation member of the International Association for the Professional Management of Construction (IAPMC).\n\nSo as to streamline management decisions, in 1996 the Council delegated most of its authority to an Executive Board of Management.\n\nPending development of industry wide competency based standards, the Institute produced standards for its own membership purposes. These standards are also used for assessing applicants for the National Building Professionals Register (NBPR) which the AIB launched in 1997.\n\nA Hong Kong Chapter was established in 1998, and AIB's overseas operations expanded considerably ten years later.\n\nAIB is also the convenor of the Australian Building and Construction Higher Education Round Table, the peak body to bring together the construction industry and Australian universities.\n\nHRH Prince Philip is an Honorary Fellow.\n\n\nAIB participates in generating building standards and helping regulate bringing new and existing buildings up to code in areas like asbestos and bush fire safety.\n"}
{"id": "35741423", "url": "https://en.wikipedia.org/wiki?curid=35741423", "title": "Bioconcentration", "text": "Bioconcentration\n\nBioconcentration is the accumulation of a chemical in or on an organism when the source of chemical is solely water. Bioconcentration is a term that was created for use in the field of aquatic toxicology. Bioconcentration can also be defined as the process by which a chemical concentration in an aquatic organism exceeds that in water as a result of exposure to a waterborne chemical.\n\nThere are several ways in which to measure and assess bioaccumulation and bioconcentration. These include: octanol-water partition coefficients (K), bioconcentration factors (BCF), bioaccumulation factors (BAF) and biota-sediment accumulation factor (BSAF). Each of these can be calculated using either empirical data or measurements as well as from mathematical models. One of these mathematical models is a fugacity-based BCF model developed by Don Mackay.\n\nBioconcentration factor can also be expressed as the ratio of the concentration of a chemical in an organism to the concentration of the chemical in the surrounding environment. The BCF is a measure of the extent of chemical sharing between an organism and the surrounding environment.\n\nIn surface water, the BCF is the ratio of a chemical's concentration in an organism to the chemical's aqueous concentration. BCF is often expressed in units of liter per kilogram (ratio of mg of chemical per kg of organism to mg of chemical per liter of water). BCF can simply be an observed ratio, or it can be the prediction of a partitioning model. A partitioning model is based on assumptions that chemicals partition between water and aquatic organisms as well as the idea that chemical equilibrium exists between the organisms and the aquatic environment in which it is found\n\nBioconcentration can be described by a bioconcentration factor (BCF), which is the ratio of the chemical concentration in an organism or biota to the concentration in water:\n\nformula_1\n\nBioconcentration factors can also be related to the octanol-water partition coefficient, K. The octanol-water partition coefficient (K) is correlated with the potential for a chemical to bioaccumulate in organisms; the BCF can be predicted from log K, via computer programs based on structure activity relationship (SAR) or through the linear equation:\n\nformula_2\n\nWhere:\n\nformula_3 at equilibrium\n\nFugacity and BCF relate to each other in the following equation:\n\nformula_4 \n\nwhere Z is equal to the Fugacity capacity of a chemical in the fish, P is equal to the density of the fish (mass/length), BCF is the partition coefficient between the fish and the water (length/mass) and H is equal to the Henry's law constant (Length/Time)\n\nThrough the use of the PBT Profiler and using criteria set forth by the United States Environmental Protection Agency under the Toxic Substances Control Act (TSCA), a substance is considered to be not bioaccumulative if it has a BCF less than 1000, bioaccumulative if it has a BCF from 1000–5000 and very bioaccumulative if it has a BCF greater than 5,000.\n\nThe thresholds under REACH are a BCF of > 2000 l/kg bzw. for the B and 5000 l/kg for vB criteria.\n\nA bioconcentration factor greater than 1 is indicative of a hydrophobic or lipophilic chemical. It is an indicator of how probable a chemical is to bioaccumulate. These chemicals have high lipid affinities and will concentrate in tissues with high lipid content instead of in an aqueous environment like the cytosol. Models are used to predict chemical partitioning in the environment which in turn allows the prediction of the biological fate of lipophilic chemicals.\n\nBased on an assumed steady state scenario, the fate of a chemical in a system is modeled giving predicted endpoint phases and concentrations.\n\nIt needs to be considered that reaching steady state may need a substantial amount of time as estimated using the following equation (in hours).\n\nformula_5\n\nFor a substance with a log(K) of 4, it thus takes approximately five days to reach effective steady state. For a log(K) of 6, the equilibrium time increases to nine months.\n\nFugacity is another predictive criterion for equilibrium among phases that has units of pressure. It is equivalent to partial pressure for most environmental purposes. It is the absconding propensity of a material. BCF can be determined from output parameters of a fugacity model and thus used to predict the fraction of chemical immediately interacting with and possibly having an effect on an organism.\n\nIf organism-specific fugacity values are available, it is possible to create a food web model which takes trophic webs into consideration. This is especially pertinent for conservative chemicals that are not easily metabolized into degradation products. Biomagnification of conservative chemicals such as toxic metals can be harmful to apex predators like orca whales, osprey, and bald eagles.\n\nBioconcentration factors facilitate predicting contamination levels in an organism based on chemical concentration in surrounding water. BCF in this setting only applies to aquatic organisms. Air breathing organisms do not take up chemicals in the same manner as other aquatic organisms. Fish, for example uptake chemicals via ingestion and osmotic gradients in gill lamellae.\n\nWhen working with benthic macroinvertebrates, both water and benthic sediments may contain chemical that affects the organism. Biota-sediment accumulation factor (BSAF) and biomagnification factor (BMF) also influence toxicity in aquatic environments.\n\nBCF does not explicitly take metabolism into consideration so it needs to be added to models at other points through uptake, elimination or degradation equations for a selected organism.\n\nChemicals with high BCF values are more lipophilic, and at equilibrium organisms will have greater concentrations of chemical than other phases in the system. Body burden is the total amount of chemical in the body of an organism, and body burdens will be greater when dealing with a lipophilic chemical.\n\nIn determining the degree at which bioconcentration occurs biological factors have to be kept in mind.The rate at which an organism is exposed through respiratory surfaces and contact with dermal surfaces of the organism, competes against the rate of excretion from an organism. The rate of excretion is a loss of chemical from the respiratory surface, growth dilution, fecal excretion, and metabolic biotransformation. Growth dilution is not an actual process of excretion but due to the mass of the organism increasing while the contaminant concentration remains constant dilution occurs.\n\nThe interaction between inputs and outputs is shown here:\nformula_6\nThe variables are defined as:\nCis the concentration in the organism (g*kg).\nt represents a unit of time (d).\nk is the rate constant for chemical uptake from water at the respiratory surface (L*kg*d).\nC is the chemical concentration dissolved in water (g*L).\nk,k,k,k are rate constants that represent excretion from the organism from the respiratory surface, fecal excretion, metabolic transformation, and growth dilution (d).\n\nStatic variables influence BCF as well. Because organisms are modeled as bags of fat, lipid to water ratio is a factor that needs to be considered. Size also plays a role as the surface to volume ratio influence the rate of uptake from the surrounding water. The species of concern is a primary factor in influencing BCF values due to it determining all of the biological factors that alter a BCF.\n\nTemperature may affect metabolic transformation, and bioenergetics. An example of this is the movement of the organism may change as well as rates of excretion. If a contaminant is ionic, the change in pH that is influenced by a change in temperature may also influence the bioavailability\n\nThe natural particle content as well as organic carbon content in water can affect the bioavailability. The contaminant can bind to the particles in the water, making uptake more difficult, as well as become ingested by the organism. This ingestion could consist of contaminated particles which would cause the source of contamination to be from more than just water.\n\n"}
{"id": "6866775", "url": "https://en.wikipedia.org/wiki?curid=6866775", "title": "Bruce Report", "text": "Bruce Report\n\nThe Bruce Report (or the Bruce Plan) is the name commonly given to the \"First Planning Report to the Highways and Planning Committee of the Corporation of the City of Glasgow\" published in March 1945. It influenced an intensive programme of regeneration and rebuilding efforts which took place in the city and surroundings from the mid-1950s and lasted until the late 1970s. The author was Robert Bruce, Glasgow Corporation Engineer at the time.\n\nA few years later in 1949 the Scottish Office in Edinburgh issued its rival \"Clyde Valley Regional Plan 1946\" ('CVP'). This was authored by a team led by Sir Patrick Abercrombie and Robert H Matthew and disagreed with the Bruce Report in a number of important areas. In particular the CVP recommended an overspill policy for Glasgow and the rehousing of much of the population in new towns outside the city. The Bruce Report preferred rebuilding and rehousing within the city boundary. The friction and debate between the supporters and spheres of influence for these two reports led to a series of initiatives designed to transform the city over the following fifty years.\n\nSome of the Bruce Report initiatives were put into practice; others were not. The report and its implementation significantly shaped modern day Glasgow. A good example of the scope of its impact is the M8 motorway which was built following proposals in the report. Also the mid-20th century policy or resettling much of the city's population to peripheral housing schemes arose from recommendations in the Bruce Report, reflecting Glasgow Corporation's resistance to overspill and new towns until it co-operated in the designation of Cumbernauld new town in 1956. The civic, economic, political, architectural, geographic and demographic landscape of modern Glasgow would have been radically different without the influence of these two reports. Had the Bruce Report itself been implemented unaltered in its entirety, the city today would probably have been unrecognisable.\n\nCentral to the Bruce Report's recommendations were a set of radical proposals which amounted to wholesale demolition of a large section of the city centre. These would have involved knocking down many historic and architecturally important Victorian and Georgian buildings. The report advocated rebuilding most of the city centre to a single design with the aim of creating a coherently planned city. Part of this plan involved removing residential dwellings from the central area and replacing them with commercial developments that would house new service industries, whilst the city's Victorian grid plan of streets would almost be completely re-arranged into a structured series of \"zones\" containing distinct spaces for city functions such as housing, commerce, and education.\n\nAmong the buildings earmarked for demolition by Bruce were many which are now regarded as Glasgow's most significant architectural assets. These included Glasgow Central Station, The Kelvingrove Art Gallery and Museum, Glasgow School of Art, designed by the renowned architect Charles Rennie Mackintosh and the Glasgow City Chambers and the Glasgow Royal Infirmary. Bruce's justification for these radical proposals was the creation of a new \"healthy and beautiful city\" based on formal 1950s architecture. Ultimately less draconian measures were sought for the regeneration of the city centre.\n\nAlthough the proposals for the city centre were rejected, the later concept of the Comprehensive Development Area (CDA) can trace its roots directly to the Bruce Report. In the 1950s, Glasgow Corporation designated CDAs as districts suffering from severe overcrowding and insanitary housing, where the only solution would be complete demolition and rebuilding. Of the 20 CDAs which the Corporation identified, two of them - Anderston and Townhead - lay partially within the city centre and saw nearly total destruction to make way for roads, high-rise housing and concrete office buildings. For example, the southern reaches of Townhead were re-zoned for educational use in preparation for the former Royal College of Science and Technology's growth into a university. The resulting John Anderson Campus of what became the University of Strathclyde now occupies a huge proportion of Townhead. In Anderston, entire communities were wiped off the map to make way for the controversial ring road (see below) as well as commercial developments on the western edge of the city centre - new buildings such as Elmbank Gardens and the Anderston Centre being prime examples.\n\nOne proposal in the report was implemented almost in its entirety: the demolition of Glasgow's slum housing. Bruce suggested that many of Glasgow's residential areas be torn down, as a great many of these unplanned developments had become slums. He proposed that their inhabitants be rehoused in new developments on the periphery of the city. The key goals of this proposal was the creation of a less densely populated city and a greater quality of life for its inhabitants. Beginning in the 1950s Glasgow's clearance programme relocated some 300,000 of the city's population. New towns, such as East Kilbride and Cumbernauld were created in the areas surrounding the city as part of this redevelopment. The programme also involved the creation of new outer suburbs on the fringes of the city boundaries such as Castlemilk, Pollok, Milton, Drumchapel and Easterhouse, all of which are on the edges of the city. Although the Bruce Report in itself did not precisely specify the \"manner\" in which its housing proposals should be implemented, the city fathers would ultimately look to the ideas of the French architect Le Corbusier for their inspiration in how those goals should be achieved. The end result was the mass construction of numerous high-rise tower block estates on green belt sites within the city boundaries.\n\nAs with most other aspects of the city's redevelopment, the housing clearances were not carried out exactly to Bruce's proposed plan. Bruce wanted all of Glasgow's citizens to be rehoused within the city boundaries. In building the new towns, a significant portion of the city's population were moved outwith the jurisdiction of the Glasgow Corporation. This resulted in a dramatic reduction in the city's reported population between the start of the 20th and 21st centuries. The area's actual population increased during this period and the urban spread of Glasgow now covers a much larger area than it did at the start of the 20th century. Bruce's underlying aim of a less densely populated city was ultimately achieved. At its peak in the 1930s Glasgow's inner city population was 1.1 million, today it is roughly 600,000. Glasgow remains Scotland's largest city however, with the population of Greater Glasgow close to 1.8 million and the entire Greater Glasgow conurbation is now 2.3 million. 44% of Scotland's entire population. The 'social engineering' which underpinned the new housing schemes, has been largely concluded as being a failure by contemporary historians; since many of the planned suburbs quickly developed social problems and deteriorated into slums themselves by the 1980s. Some of the estates, most notably those in Pollok, Red Road and Easterhouse, have either been demolished completely or are earmarked for eventual clearance.\n\nThe scope of Bruce's proposals was not limited to housing reform, Glasgow's transport infrastructure was also a target for change. Bruce proposed that quality, high speed transport links were vital if the city was going to transition to a service based economy against the background of a declining industrial base.\n\nBruce proposed the creation of a system of arterial motorways into the city converging to form a Glasgow Inner Ring Road - a motorway \"box\" which would have encircled the city centre. The proposed network included a Renfrew motorway, a Monklands motorway, a Maryhill motorway, a Stirling motorway and a south eastern motorway connecting Glasgow with the wider motorway network. These proposals were not acted on until the 1960s when the initiative began as a sweeping programme of clearing and construction. The Monklands and Renfrew motorways were completed forming what is now known as the M8. These two motorways form the northern and western flanks of the planned ring road. However carving a motorway through long standing communities - much of Charing Cross and Anderston were completely destroyed in its construction - caused such protest that the rest of the Inner Ring Road initiative was shelved. The Stirling motorway became the present day M80 and its initial Glasgow section was openened in 1992 as the Stepps bypass, and the complete route through Cumbernauld was fully opened in 2011.\n\nAn extension to the M74 into the city centre to meet the M8 motorway was completed in June 2011. This will see the completion of the southern section of Bruce's planned ring road, albeit on a marginally different route than originally proposed.\n\nBruce's report recommended that the Eastern flank of the Inner Ring Road be constructed along the route of Glasgow's High Street. Again this would have necessitated the demolition of many properties, including some of historic and/or architectural value. It was shelved in the face of strong local opposition.\n\nAlthough not directly part of the Bruce Report, the wider plans for the Monkland Motorway contained proposals to create a South Link Motorway which would have continued south from the Stirling Motorway terminus at Provan Gas Works and bisected the East End to link with the M74. This proposal has eventually evolved into the Glasgow East End Regeneration Route. This plan proposes a road (not a motorway) which would link the M74 to the M8 through the East End of Glasgow. If successfully implemented this plan together with the M74 extension would effectively see Bruce's planned inner ring completed.\n\nThere are no signs that the Bruce Plan's proposed Maryhill motorway will ever be implemented in any way. Bruce's proposals for a ring road with four arterial routes running off it seems unlikely to ever be fully realised.\n\nAs well as recommending changes to Glasgow's road network, Bruce also suggested radical changes to Glasgow's railways. At the time of the Bruce Report, the city had four major railway stations. Central and St Enoch both served primarily southbound destinations. Queen Street and Buchanan Street mainly served northbound destinations.\n\nIn order to rationalise the city's mainline services, Bruce suggested that all four Victorian railway stations be demolished and replaced with two new purpose built stations. A new \"Glasgow North\" station was proposed roughly on the site of Buchanan Street station (occupying a larger area) to replace Buchanan Street and Queen Street stations. A \"Glasgow South\" station was proposed on the approximate site of Glasgow Central station to replace Central and St Enoch stations. Bruce's plan then called for a new bus station on the Queen Street site, with the \"low level\" railway station there remaining to provide suburban services and to connect the new bus station to the rail network.\n\nThese plans were never implemented, and all four stations remained until the 1960s when the Beeching Axe reforms changed the shape of rail services across Scotland, England, and Wales. Beeching's reforms spelt the end for both Buchanan Street and St Enoch stations which closed, effectively rationalising rail services in the city along similar lines to Bruce's two station plan, but without requiring the demolition of four stations and construction of two new ones. Beeching's reforms also removed low level services from Glasgow Central station, but these were reintroduced in the late 1970s. Glasgow Central has recently undergone extensive, careful and sympathetic renovation and remains one of the city's architectural assets to this day.\n\nTwo new bus stations would ultimately result from the Bruce Report, positioned at either corner of the city's central area to make full use of the Ring Road, a new terminus at Anderston to the south (eventually opening in 1972 as part of the Anderston Centre), thus replacing the older terminus at Waterloo Street whilst Dundas Street bus station was replaced by Buchanan Street Bus Station in 1976. Anderston eventually closed in 1993, with all services consolidated at the Buchanan terminal.\n\n"}
{"id": "17599701", "url": "https://en.wikipedia.org/wiki?curid=17599701", "title": "Burn recovery bed", "text": "Burn recovery bed\n\nA burn recovery bed or burn bed is a special type of bed designed for hospital patients who have suffered severe skin burns across large portions of their body.\n\nGenerally, concentrated pressure on any one spot of the damaged skin can be extremely painful to the patient, so the primary function of a burn bed is to distribute the weight of the patient so evenly that no single bed contact point is pressed harder than any other.\n\nOne type of weight-distributing burn bed uses a series of interlinked inflatable air chambers which have the surface appearance of an upside-down egg carton. Although inflatable, the air chambers are maintained in a partially deflated state so that the air pressure can freely distribute itself. Heavier parts of the patient's body can sink deeper into the grid of chambers and the air moves to chambers with less weight.\n\nAir volume in the chambers may be regulated so as to make the bed firmer when the patient is first being placed on the bed, and then air is released to allow for a more conformal shape once lying flat across the bed surface.\n\nThis type of burn bed is similar in construction to a typical water bed, except the surface covering of the water pool has a large amount of slack and extra folds of material around the perimeter of the pool.\n\nTo limit the depth of immersion into the burn bed water pool, the water's density may be increased by adding several hundred pounds of salt to the water, as is done with a relaxation float tank.\n\nAs the patient is placed onto the bed, they displace the water and can freely sink down into the pool, unlike a typical consumer water bed. As they sink down, the slack around the edges is played out so that the patient is now sunk into the water, in a form-fitting, very gentle, and dry depression in the pool.\n\nGenerally the pool is not deep enough to permit a patient to lie on their side in the pool, but even so, lying sideways is a safe condition since the patient is at no risk of breathing in water and drowning due to the water-isolation covering.\n\n"}
{"id": "21918859", "url": "https://en.wikipedia.org/wiki?curid=21918859", "title": "Camfil", "text": "Camfil\n\nThe Camfil Group is a producer and developer of air filters and clean air products.\nCamfil is also a global air filtration specialist with 24 production units and R&D centres in four countries in the Americas, Europe and the Asia-Pacific region. The Group, headquartered in Stockholm, Sweden, has approximately 3,000 employees and sales in the range of SEK 4.3 billion. International markets account for more than 90 percent of sales. The company’s business is to provide customers with air filtration products and services within four main segments: Comfort Air, Clean Processes, Power Systems and Safety & Protection.\n\nThe global Camfil Group started out as a family business in the town of Trosa, Sweden, located about south of Stockholm. Camfil was founded in Trosa in 1963 by the Larson family, still one of the company’s principal owners, as a joint venture with Cambridge Filtration Corporation in the United States. In 1983, the family bought Cambridge’s shares. Camfil was wholly owned by the family until 2000, when Ratos AB, a Swedish private equity company, received 29.7 percent of the shares in connection with the acquisition of Farr in the United States, which was then listed on NASDAQ. Ratos sold its shares in 2010 to the other owners, families Larson and Markman.\n"}
{"id": "11701763", "url": "https://en.wikipedia.org/wiki?curid=11701763", "title": "Cellulose acetate film", "text": "Cellulose acetate film\n\nCellulose acetate film, or safety film, is used in photography as a base material for photographic emulsions. It was introduced in the early 20th century by film manufacturers as a safe film base replacement for unstable and highly flammable nitrate film.\n\nCellulose diacetate film was first created by the German chemists Arthur Eichengrün and Theodore Becker, who patented it under the name Cellit, from a process they devised in 1901 for the direct acetylation of cellulose at a low temperature to prevent its degradation, which permitted the degree of acetylation to be controlled, thereby avoiding total conversion to its triacetate. Cellit was a stable, non-brittle cellulose acetate polymer that could be dissolved in acetone for further processing. A cellulose diacetate film more readily dissolved in acetone was developed by the American chemist George Miles in 1904. Miles's process (partially hydrolysing the polymer) was employed commercially for photographic film in 1909 by Eastman Kodak and the Pathé Fréres. Starting with cellulose diacetate, this innovation continued with cellulose acetate propionate and cellulose acetate butyrate in the 1930s, and finally in the late 1940s, cellulose triacetate was introduced, alongside polyester bases. These less flammable substitutes for nitrate film were called safety film.\n\nThe motion picture industry continued to use cellulose nitrate supports until the introduction of cellulose triacetate in 1948, which met the rigorous safety and performance standards set by the cinematographic industry. The chemical instability of cellulose acetate material, unrecognized at the time of its introduction, has since become a major threat for film collections.\n\nThe first instance of cellulose triacetate degradation was reported to the Eastman Kodak Company within a decade of its introduction in 1948. The first report came from the Government of India, whose film was stored in hot, humid conditions. It was followed by further reports of degradation from collections stored in similar conditions. These observations resulted in continuing studies in the Kodak laboratories during the 1960s.\n\nBeginning in the 1980s, there was a great deal of focus upon film stability following frequent reports of cellulose triacetate degradation. This material releases acetic acid, the key ingredient in vinegar and responsible for its acidic smell. The problem became known as the \"vinegar syndrome.\"\n\nIn acetate film, acetyl (CHCO) groups are attached to long molecular chains of cellulose. With exposure to moisture, heat, or acids, these acetyl groups break from their molecular bonds and acetic acid is released. While the acid is initially released inside the plastic, it gradually diffuses to the surface, causing a characteristic vinegary smell.\n\nThe decay process follows this pattern:\n\n\nA testing product developed by the Image Permanence Institute, A-D, or \"acid-detection\" indicator strips change color from blue through shades of green to yellow with increasing exposure to acid. According to the test User's Guide, they were \"...created to aid in the preservation of collections of photographic film, including sheet and roll films, cinema film, and microfilm. They provide a nondestructive method of determining the extent of vinegar syndrome in film collections.\" These tools can be used to determine the extent of damage to a film collection and which steps should be taken to prolong their usability.\n\nCurrently there is no practical way of halting or reversing the course of degradation. Many film collectors use camphor tablets but it is not known what the long term effects on the film would be. While there has been significant research regarding various methods of slowing degradation, such as storage in molecular sieves, temperature and moisture are the two key factors affecting the rate of deterioration. According to the Image Permanence Institute, fresh acetate film stored at a temperature of 70 °F (21 °C) and 40% relative humidity will last approximately 50 years before the onset of vinegar syndrome. Reducing the temperature by 15° while maintaining the same level of humidity brings a dramatic improvement: at a temperature of 55 °F (13 °C) and 40% relative humidity, the estimated time until onset of vinegar syndrome is 150 years. A combination of low temperature and low relative humidity represents the optimum storage condition for cellulose acetate base films, with the caveat that relative humidity should not be lowered below 20%, or the film will dry out too much and become brittle.\n\nCold storage options for the preservation of acetate film range from insulated cold storage rooms, or vaults, with relative humidity control (typical settings in the range of 35-40 °F temperature, and 30-35% relative humidity), which might be used by archival institutions for large and medium-sized collections, to free-standing freezer units, which can be cost-effective for small collections, but necessitate vapor-proof packaging of the films to protect against relative humidity extremes and condensation. Commercial storage facilities may offer varying environmental conditions at different rates.\n\nMicroenvironments—the conditions inside an enclosure—can also affect the condition of cellulose acetate film. Enclosures that are breathable or that contain an acid absorbent are instrumental in reducing the rate of decay due to vinegar syndrome. Sealed metal containers can trap the decay products released by the film, promoting the spread of vinegar syndrome.\n\nDuring early stages of decay, the film content can be rescued by transferring it to new film stock. Once the film becomes brittle it cannot be copied in its original quality in sight and sound. Because the gelatin emulsion usually stays intact during the degradation process, it is possible to save the image on sheet film using solvents to dissolve the base off the emulsion. Once the emulsion has been freed from the shrunken support, it can be photographed or transferred to a new support. Because of the solvents used, this is a delicate and potentially hazardous procedure and is an expensive process for a large collection. Degraded motion picture film cannot be restored in this way, but sheet films often can.\n\nWhile digitization would be an ideal way to preserve the contents of cellulose acetate film, current standards do not allow for scanning at sufficient resolutions to produce a copy of the same picture and sound quality as the original. Currently, the U.S. National Film Preservation Foundation advocates film-to-film transfer as the best method for film preservation, with the copies stored in proper environmental conditions.\n\nCellulose acetate film is also used to make replicates of materials and biological samples for microscopy. The techniques were developed for metallographic needs to examine the grain structure of polished metals. Replication can be used to understand the distribution, for example, of different types of iron in carbon steel samples, or the fine distribution of damage to a sample subject to mechanical wear.\n\n\n"}
{"id": "56620190", "url": "https://en.wikipedia.org/wiki?curid=56620190", "title": "Clay panel", "text": "Clay panel\n\nClay panel (also known as clay board, clay wallboard, clay building board, clay building panel) is a panel made of clay with some additives. The clay is mixed with sand, water, and fiber, typically wood fiber, and sometimes other additives like starch. Most often this means employing the use of high-cellulose waste fibres. To improve the breaking resistance clay boards are often embedded in a hessian skin on the backside or similar embeddings.\n\nClay board is an alternative to gypsum plasterboard. It is suitable for drywall applications for interior walls and ceilings. It can be applied to either timber or metal studwork. Usually the application of clay boards is completed with clay finishing plaster.\n\nThe boards have fire retardant properties and medium levels of acoustic insulation. Due to the clay component they have the ability to absorb large amounts of humidity through the plaster and helping to protect vulnerable buildings from excess moisture generated by modern living.\n\nThe cutting of clay boards can be done with a cutter knife, jigsaw or circular saw. The clay board is mounted in vertical or horizontal form. It is fastened with screws with plate head or holding plate, alternatively also with wide back clamps.\n\nA clay plaster skim can be directly applied to the boards following joint filling and reinforcement.\n\n\n"}
{"id": "5149026", "url": "https://en.wikipedia.org/wiki?curid=5149026", "title": "Digital subchannel", "text": "Digital subchannel\n\nIn broadcasting, digital subchannels are a method of transmitting more than one independent program stream simultaneously from the same digital radio or television station on the same radio frequency channel. This is done by using data compression techniques to reduce the size of each individual program stream, and multiplexing to combine them into a single signal. The practice is sometimes called \"multicasting\".\n\nThe ATSC digital television standard used in the United States supports multiple program streams over-the-air, allowing television stations to transmit one or more subchannels over a single digital signal. A virtual channel numbering scheme distinguishes broadcast subchannels by appending the television channel number with a period digit (\".xx\"). Simultaneously, the suffix indicates that a television station offers additional programming streams. By convention, the suffix position \".1\" is normally used to refer to the station's main digital channel and the \".0\" position is reserved for analog channels. For example, most of the owned-and-operated stations/affiliates of Ion Television transmit six streams in the following format:\nThe most of any large broadcaster in the United States, Trinity Broadcasting Network stations transmit five channels (in standard definition) and its subchannel services Hillsong Channel, JUCE TV/Smile (two networks that technically operate as separate 24-hour services, but since June 2015, air portions of their respective schedules on a single subchannel over-the-air), TBN Enlace USA and TBN Salsa. More programming streams can be fit into a single channel space at the cost of broadcast quality. Among smaller stations, KAXT-CD in San Francisco is believed to have the most feeds of any individual over-the-air broadcaster, offering twelve video and several audio feeds (all transmitted in standard definition). WANN-CD in Atlanta, Georgia, with ten video and six audio feeds, comes at a close second. Several cable-to-air broadcasters, such as those in Willmar, Minnesota and Cortez, Colorado, have multiplexed more than five separate cable television channels into subchannels of one signal.\n\nOperating in a sector traditionally lacking subchannels, digital cable television provider Music Choice packages its nearly 50 music channels (including Music Choice Play) as digital subchannels of one channel. This is possible as the only information sent over each channel are audio feeds and a still slide which rotates every 20 seconds, displaying an advertisement and information about the current playing song on the individual channel. The audio feed and rotating stills occupy significantly less bandwidth than video feeds, leaving space for more multiplexed content.\n\nA broadcaster saves significant costs in power and bandwidth through multiplexing in comparison to the cost of operating additional analog television stations to accommodate the extra programming. In practice, operating extra stations is impossible due to the required channel and distance separations combined with the available number of channels.\n\nMost ATSC tuners will automatically add a new digital subchannel to their internal channel map, once it is tuned to the station carrying the new channel. However, some of these will not delete the channel if the station removes it.\n\nMobile DTV is also carried on ATSC stations, but as a separate service, according to the ATSC-M/H standard.\n\nThe Federal Communications Commission (FCC) considers all subchannels carried by a single station to have the same call letters for legal identification purposes. However, within the broadcast sales industry, to differentiate subchannels, the initial letter of a call sign changes per subchannel. As per Nielsen, digital stations identified with a \"W\" call letter will generally have their subchannels identified with an \"E\" (.2), \"G\" (.3), \"H\" (.4), \"I\" (.5) or \"J\" (.6). Digital stations identified with a \"K\" call-letter will generally have their subchannels identified with an \"N\" (.2), \"O\" (.3), \"Q\" (.4), \"R\" (.5) or \"S\" (.6). For example, if the call letters are WFRC and the station broadcasts on channel 10, then the 10.2 subchannel is identified as EFRC, 10.3 is GFRC, 10.4 is HFRC, 10.5 is IFRC and 10.6 is JFRC. If the call letters are KFRC and the station broadcasts on channel 10, then 10.2 is identified as NFRC, 10.3 is OFRC, 10.4 is QFRC, 10.5 is RFRC and 10.6 is SFRC.\n\nAlthough digital television services in Canada use the same ATSC technology as the United States, none of the stations currently broadcasting a digital signal transmit any subchannel other than a possible HD service or a standard definition simulcast of the main channel. Unlike the FCC in the United States, the body that governs Canadian broadcasting licenses, the Canadian Radio-television and Telecommunications Commission (CRTC), requires stations to file license amendments in order to be considered for permission to carry digital subchannels (this differs from the Commission's rules for premium cable television services, which allow the addition of multiplex channels consistent with the service's license requirements without the need to amend the license). On August 17, 2012, the CRTC gave approval to Leamington, Ontario community station CFTV-TV to broadcast four local subchannels on its digital signal, making it the first station in Canada to launch original content on its multiplex channels.\n\nSome Mexican TV stations use digital subchannels as they are used in the United States. The Sistema Público de Radiodifusión del Estado Mexicano, a public broadcaster, operates 26 multiplexed transmitters throughout Mexico carrying five to six public television services, while XHTRES-TDT carries Imagen Radio audio on a subchannel.\n\nOne notable experiment involving digital subchannels in Mexico was undertaken by TV Azteca, which used its three muxes in the Mexico City area to broadcast a service called Hi-TV, featuring several channels encoded in H.264 MPEG-4 encoding, which while available in the ATSC standard is not common on TV sets. This use of subchannels as pseudo-restricted signals within non-restricted channels was placed under investigation and litigation with authorities at COFETEL (the Federal Teleommunications Commission), involving a fine of 4,453,150 Mexican pesos. HiTV subchannels began broadcasting on an intermittent basis in 2013 and were almost completely deactivated in late 2014.\n\nTelevisa and TV Azteca use subchannels in rural areas in order to ensure national network service. As a result, since 2016, many areas that formerly had only one Azteca or Televisa network now have both from the same transmitter. Additionally, TV Azteca has two national services that are broadcast as subchannels in most areas, a+ and adn40.\n\nIn October 2016, the IFT put into effect new guidelines for the numbering of virtual channels. As a result, national networks use consistent numbers nationwide; SPR transmitters now use four or five major channel numbers (11, 14, 20, 22, and 45 in some areas). Prior to this, digital television stations usually used virtual channels corresponding to their former analog positions, still the case for certain local stations.\n\nThe IFT enforces minimum bitrates for digital television channels, and as such it is not possible for a station to broadcast two HD feeds in MPEG-2 encoding. Most HD feeds are provided in 1080i with all subchannels in 480i standard definition.\n\nAustralian digital subchannels are currently divided between high definition (HD), standard definition (SD) and radio subchannels (the latter type is only carried by the stations of non-commercial networks SBS Television and ABC Television). Due to technical reasons, each network is currently only permitted one HD sub channel. All networks use their HD subchannel to provide a simulcast of their primary channel.\n\nInclusive of their primary standard definition channels (ignoring HD):\n\nCommunity television stations in Melbourne (C31) and Adelaide (44 Adelaide) also broadcast digital signals, however they typically only broadcast a single SD subchannel which simulcasts that station's primary channel.\n\nThere have been a number of issues surrounding the introduction of digital subchannels in Australia. The first subchannels launched by the ABC – ABC Kids and Fly TV – closed after less than two years in operation in 2003 as a reaction to budget cuts by the conservative Howard Government under Communications Minister Alston and low viewership (partly due to the limited distribution of set-top boxes); and commercial broadcasters could not legally air a digital subchannel other than a single high-definition service until 2009.\n\nAs most digital services in Europe rely on more complex methods of multiplexing, where a large number of digital channels by many different broadcasters can be broadcast on one single frequency, the concept of a subchannel is instead applied to the variety of channels that are produced by a single company. This can vary widely depending on the country: for example, ITV currently has four of its digital channels (ITV, ITV2, ITV3 and ITV4) broadcasting on one multiplexed service, while two others (ITV2 +1 and CITV) are each broadcast on another, separate multiplex.\n\nIn Japan and Brazil, ISDB (similar to the DVB format) is used, and was specifically designed with physical RF segments that could be split to use for different subchannels.\n\nAs the amount of data which can be carried on one digital television channel at one time is limited, the addition of multiple channels of programming as digital subchannels comes at the expense of having less available bandwidth for other purposes, such as the ability to transmit high definition content. A station carrying multiple subchannels will normally limit itself to one high-definition channel (or in some cases, two HD channels), with any additional channels being carried in standard definition. Because of the tradeoffs, stations owned by CBS Corporation through its CBS Television Stations subsidiary (which include owned-and-operated stations of CBS and The CW, and some independent stations) generally opted not to carry digital subchannels and transmitted only a 1080i high definition main feed; this changed in 2013 with the addition of dedicated local news channels on CBS O&Os in New York City and Philadelphia (the company later announced the creation of Decades, a multicast network part-owned by CBS which now airs on all CBS-owned stations).\n\nIt is possible for stations to carry more than two subchannel feeds in HD, at least nominally. Actual picture quality may be comparable to DVD video. Some examples of stations broadcasting in this format are:\nOutside the United States – especially in Europe – high-definition feeds are rarer, and most countries only provide a single high-definition service for each broadcaster. For example, in France, there are only five HD services: one each for TF1, France 2, Canal+, M6 and Arte; in the United Kingdom, four HD services are currently transmitted over terrestrial frequencies: BBC One HD, BBC Two HD, ITV HD and Channel 4 HD (S4C Clirlun is broadcast in Wales instead of Channel 4 HD).\n\nIn the United States, digital subchannels have been used to provide programming from multiple major networks on a single television station. This has become prevalent since the late 2000s in smaller markets that have as few as one or two commercial stations, which during the era of analog television, would not have been able to carry the complete programming lineups of all four major commercial networks (CBS, NBC, ABC and Fox) because of the station's own local and syndicated programming commitments, and overlapping network programs that would be tough to schedule outside of regular timeslots. A prime example is the Wheeling, West Virginia/Steubenville, Ohio market, which for decades was home to only two stations (CBS affiliate WTRF-TV and NBC affiliate WTOV-TV; the cable-only WBWO also served the market as a WB and now as a CW affiliate) and had to mostly rely on stations in Pittsburgh (and to a lesser extent Columbus and Youngstown, Ohio) to view programming from other networks. However, the advent of digital television allowed WTRF to launch two digital channels (one as a primary Fox/secondary MyNetworkTV affiliate, the other affiliated with ABC) while still carrying CBS programming in full on its main signal (WTOV later took the Fox affiliation for its second subchannel in September 2014).\n\nUpon their launches in September 2006, The CW and MyNetworkTV were among the first conventional networks to actively utilize subchannel-only affiliations in markets where a standalone station is not available to affiliate with; this is particularly true of The CW's small-market feed, The CW Plus, which originally consisted mostly of cable-only affiliations (by way of inheriting the model and much of the affiliate body of predecessor The WB 100+ Station Group). Since its launch, affiliates of other major networks have taken over the operations of cable-only CW Plus affiliates (or even outright replacing WB 100+ cable channels at the launch of The CW) and began transmitting the service over subchannels to reach viewers who do not subscribe to a pay television service. Some Spanish language networks (such as MundoFox, Estrella TV and Mexicanal) have also been carried on digital subchannels, either as subchannel-exclusive services or to provide programming to markets where a main channel affiliation may not be available. Other stations have launched subchannels with an independent station format on their DT2 signals (such as WTTV in Indianapolis, Indiana – a market with enough commercial stations able to support affiliations with all six networks and a standalone independent, although the seventh (WTTK) instead acts as a WTTV satellite – which converted its 4.2 subchannel as an independent station in January 2015 as a result of owner Tribune Media selling the local rights to the CW affiliation that was to move from its main feed on 4.1 to WISH-TV, whose CBS affiliation was assumed by WTTV).\n\nDigital subchannels are also used to relay stations beyond their traditional signal coverage areas to reach an entire market. In the Upper Peninsula of Michigan and northern Minnesota, many of these stations are on duplicate frequencies to cover a large market area. This is used to duplicate network service for stations that are part of duopolies, where transmitters scattered through a large geographical area allow multiple networks and channels to be carried. The most prominent example is the Granite Broadcasting Corporation's virtual quadropoly in Duluth, Minnesota, which consists of two separate full-power stations, NBC affiliate KBJR-TV and CBS affiliate KDLH, which combined carry three subchannels (two affiliated with major networks – CW Plus affiliate \"Northland CW 2\" on KDLH and MyNetworkTV affiliate \"My9\" on KBJR – and the third, a local weather subchannel on KBJR). While KDLH carries the CW subchannel on their DT2 feed and KBJR carries the MyNetworkTV subchannel on its DT2 feed on their primary signals, all five channels are carried on satellite station KRII in Chisholm, providing the Iron Range region (located north of Duluth) programming from networks that were previously unavailable over-the-air. In the Traverse City-Cheboygan market in Upper Michigan, NBC affiliate WPBN/WTOM also simulcasts sister station WGTU/WGTQ, providing that station's ABC programming to the entire market; CBS affiliate WWTV/WWUP carries its Fox-affiliated sister WFQX/WFUP on their DT2 subchannel to expand their coverage area further north into the eastern portion of the Upper Peninsula.\n\nIn many cases, these \"new\" channels are existing secondary channels that were carried by a low-power or Class A station or by a cable television channel. Often, the owner of a full-power television station acquires or already owns a low-power secondary station in the same market to carry another network. The use of a digital subchannel on a full-power television station as a replacement for low-power station greatly increases the available coverage area for its programming.\n\nBecause of interference issues that stations transmitting on the low VHF band (channels 2 to 6) often experience, some stations broadcasting on these frequencies are relayed on the subchannels of stations that are less prone to interference. An example of this is CBS affiliate WRGB in Albany, New York. While WRGB broadcasts its main digital on VHF channel 6 in high definition, CW-affiliated sister station WCWN relays a standard-definition subchannel feed of WRGB over its digital channel 45.\n\nNetworks dedicated to sports programming have been launched specifically for use on digital subchannels. Until 2010, CBS affiliates often subdivided four temporary subchannels in order to show all of the early round games of the NCAA Men's Basketball Tournament in addition to those broadcast on the main digital channel (this was superseded as a result of a new television agreement with the NCAA that took effect in 2011, which gave cable networks TBS, TNT and TruTV partial rights to the tournament). Most of the major professional sports leagues, however, have strict prohibitions against using subchannels for carrying multiple game broadcasts and only allow one game to be aired in a market at one time (outside of Los Angeles, where if the Rams and Chargers play at the same time, Fox is allowed to broadcast the second game on MyNetworkTV affiliate KCOP-TV, or CBS on independent KCAL-TV, depending on the game's carrier that specific week); all four of the major sports leagues (the NFL, the NBA, Major League Baseball and the NHL) have out-of-market sports packages that require a pay television subscription and generate significant revenue for the leagues.\n\nMost sports programming on digital subchannel broadcasters has been relegated to low-budget content such as amateur athletics, extreme sports, and hunting and fishing programming geared toward outdoorsmen, though minor league baseball, American Hockey League hockey and other minor league sports may also be seen. Prominent team sports programming on digital subchannels is rare; the general trend for sports programming tends to eschew the free-to-air model that digital subchannels use, and the cost of rights fees for most sports requires that they air on channels that air on cable and satellite television services and thus can recuperate costs through retransmission consent. Channels such as Sportsman Channel (and the now-defunct Universal Sports) that began as digital subchannel networks now operate as cable and satellite-exclusive services. There are nonetheless a few multicast channels that have broadcast familiar sports programs: Bounce TV, for instance, carried college football from historically black colleges and universities until 2013.\n\nIn January 2016, Sinclair Broadcast Group launched a 24-hour feed of its American Sports Network sports syndication service on subchannels of ten stations owned and/or operated by the group; the ASN multicast network was subsequently replaced by Stadium in August 2017, following the formation of a multi-platform network venture with the Chicago White Sox's Silver Chalice unit and 120 Sports.\n\nAlthough not to the same level as in the late 2000s due to the population of entertainment-based multicast services, many local stations have used or currently use subchannels to carry continuous news or local weather content; in particular, there have been at least four networks that have been created to serve this audience: NBC Weather Plus (a service exclusive to NBC stations that operated from 2004 to 2008), The AccuWeather Channel, WeatherNation TV (which also maintains limited exclusive distribution on pay television services) and TouchVision. Locally programmed news subchannels (such as News 9 Now / News on 6 Now on KWTV in Oklahoma City and KOTV in Tulsa, Oklahoma or NewsChannel 5+ on WTVF in Nashville, Tennessee) often carry rebroadcasts and simulcasts of local news programs seen on the station's main feed, in some cases displaying a ticker with news headlines and weather forecasts to provide updated information.\n\nSubchannels also allow stations to air news programs without fully pre-empting normally scheduled programing on the station's main feed. During significant breaking news or severe weather events, for instance, a station may choose to air extended news coverage on either its main channel or a subchannel and air network programming on the other. Thus, the station can accommodate viewers wanting to watch either regular programming or news coverage. Some sports leagues, most notably the NFL, have strict rules against their game broadcasts airing on a subchannel.\n\nSince the late 2000s, entertainment-based specialty networks (also known as \"diginets\") have been created specifically for subchannels, most commonly those dedicated to airing reruns of classic television series (such as Me-TV, the Retro Television Network, Cozi TV and Antenna TV) and movies (such as This TV, GetTV and Movies!). Some networks (such as the African-American focused Bounce TV, the female-targeted Escape and the male-targeted Grit) feature programming aimed at specific demographics. With few exceptions (such as Bounce TV and the now-defunct Live Well Network), diginets typically do not offer first-run original programming, relying on acquired content from programming distributors (most popularly, television series from the 1980s and earlier) to fill their schedules. Some stations (such as K38IZ-D in Phoenix, Arizona) carry locally programmed channels offering classic television shows and music videos on their digital subchannels. With MTV's shift away from music videos since the 1990s, subchannel networks focusing entirely on music videos have also been attempted (such as ZUUS Country, Heartland, Tr3s and TheCoolTV), however many have either been unable to gain national coverage or have lost significant market share due to various issues (for example, LIN Media, the Sinclair Broadcast Group and the Journal Broadcast Group terminated or opted against renewing deals with TheCoolTV between 2011 and 2013; Journal, in particular, cited TheCoolTV parent Cool Music Network, LLC's failure to pay licensing fees behind its removal of the network in a lawsuit filed against the company in 2011).\n\nAlthough the revenue potential is limited, many broadcasters have found subchannel-only networks to be a means of generating extra advertising revenue, as they are easily marketable to a given demographic (although they do not enjoy the same retransmission consent revenue stream from cable providers as other networks do). There have been a few notable holdouts as late as 2014 such as the Meredith Corporation (only a few of its stations have subchannels, and some of these primarily carry only local news or weather services, or in the case of WGGB-TV/Springfield, Massachusetts, allows a Fox subchannel affiliate to operate in a market with limited full-power signals traditionally beholden in the past to the Hartford, Connecticut market to the south for CBS, Fox, WB and UPN affiliates besides WGGB's ABC programming and WWLP's NBC affiliation) and the Nexstar Broadcasting Group (which previously used subchannels for the sole purposes of simulcasting co-owned/managed sister stations to reach an entire market or to carry programming from major networks in smaller markets, and often eliminated subchannels affiliated with multicast services following station acquisitions). Nexstar has since added multicast services such as Bounce TV and WeatherNation TV on some of its stations. Unlike the major broadcast networks, diginets carried by local stations not associated with the five major networks or MyNetworkTV are often carried on higher cable channel placements (usually within the digital cable tiers) not readily found by most subscribers. Satellite and IPTV providers generally do not carry multicast networks unless the local station also secondarily carries a major network on that subchannel.\n\nMost diginets reach affiliation agreements with a limited number of station owners prior to launch, before expanding their national coverage through additional deals made after their debut (by comparison, Fox, The CW, MyNetworkTV and to a lesser extent, The WB and UPN, had initial station coverage reaching 60%+ of all U.S. television households through affiliation deals that were largely made before their launches, in order to have wide distribution in at least the top 100 markets). Since the majority of multicast networks are carried on major network affiliates, some full-power stations are recitant about further compressing bitrate space to fit more than one subchannel at the expense of the picture quality of their high-definition main feed. As well, in markets with fewer than six stations, available subchannel space is tighter and some multicast networks may not be able to gain affiliations especially if one of the stations uses a subchannel for the primary purpose of carrying programming from a major network (which are also often transmitted in HD, limiting bitrate space). Some networks remedy this in certain markets by affiliating with low-power stations that do not carry a major network. As of 2014, only 12 primarily subchannel-only networks reach at least 50% of all U.S. markets (with MeTV, which is available in 91% of the country — a national reach comparable to the six major commercial networks and PBS — being the largest).\n\nDiginets with wider national coverage and decent viewership are more likely to attract major advertisers, although most rely on smaller-scale advertisers such as national law firms, mortgage providers and direct response advertisers. Multicast networks often make barter deals to secure affiliations in which advertising inventory is split between the network and the station, however some networks enter into subchannel leasing deals (often if they are unable to secure sufficient cable distribution), in which the network handles the sale of advertising inventory and pays its affiliates a monthly licensing fee to carry its programming.\n\nMany PBS member stations around the United States broadcast their main channel in high definition and up to three standard definition subchannels; however, a few reconfigure their digital channels depending on daypart, carrying four standard definition channels during the daytime, reducing them to one HD and one SD channel at night due to technical limitations at the station's level that may prevent it from carrying PBS programming in HD full-time and maintain multiple full-time subchannels like other member stations. PBS stations often carry additional national channels such as PBS HD (PBS Satellite Service), PBS Kids, World, and Create. In the Washington, D.C. area, MHz Networks is available as ten subchannels transmitted by two stations, with their virtual channels mapped uniformally, making them appear as if they are transmitted by one station.\n\nIn some U.S. states, statewide educational, cultural or public affairs services are carried on a digital subchannel of a PBS member station or network (such as the Minnesota Channel, Wisconsin Channel or New York State broadcaster ThinkBright TV). The use of subchannels has also allowed educational television broadcasters to sell off former secondary PBS analogue stations to commercial broadcasters (such as WNEQ in Buffalo, which its sister station WNED-TV sold in 1999 to LIN TV (now owned by Media General) to become WNLO, now a CW affiliate), as the additional educational content these separate stations once provided can now be carried by multiple subchannels of a single parent station. Subchannels also allow some educational stations to devote an entire channel to telecourses, which are recorded by instructors and students for later use, allowing the station's main channel to air a generalized schedule in the morning and overnight hours.\n\nA digital subchannel can be used to restore service from a station that has been knocked off the air due to an antenna tower collapse; the affected signal would be made available in standard-definition, or even in compressed high definition, on a subchannel of another local station, most often a competitor. ABC affiliate KATV in Little Rock, Arkansas was forced to follow this path in 2008, moving its digital signal to a subchannel of MyNetworkTV affiliate KWBF (now KARZ-TV) after a tower collapse knocked its main signal offline. The virtual channel numbering scheme allows an existing licensed broadcaster to keep its displayed channel number unchanged (in the case of KATV, PSIP channel 7) even if the signal is carried physically as a subchannel of another local station.\n\nDuring the Station Fire in 2009, NBC owned-and-operated station KNBC in Los Angeles temporarily replaced programming from NBC Plus and Universal Sports on its subchannels with standard-definition feeds of its two Spanish language sister stations – independent station KWHY-TV (now a MundoFox affiliate, since sold to the Mueriello Group) and Telemundo owned-and-operated station KVEA – as an emergency measure in the event that the transmitters of those stations were destroyed or disabled as the fire reached Mount Wilson, where the transmitter facilities of most Los Angeles area stations are based.\nIn rare cases, digital television broadcasters have included a service known as DTV radio, in which the audio of a commonly owned broadcast radio station is simulcast over a subchannel (for instance, KPJK in San Mateo, California broadcasts former FM sister KCSM on its DT3 signal). WANN-CD in Atlanta offers six radio stations owned by iHeartMedia, in addition to ten television channels.\n\nNon-broadcast content, subscription television channels or datacasting operations unrelated to the main television programming are also permitted by the digital television standards but are less-commonly used. USDTV was an over-the-air pay television service that used H.264 compression instead of standard MPEG-2. Mobile DTV now uses MPEG-4 compression, which like H.264 yields a much lower bitrate for the same video quality. For example, the Sezmi TV/DVR service uses broadcast digital subchannels (not in the clear) in selected cities to stream a limited number of \"cable\" channels to its subscribers for an additional fee to supplement its otherwise free digital video recorder (DVR) service allowing recordings of local broadcast channels and free and subscription internet content.\n\nDigital television supports multiple digital subchannels if the 19.39 Mbit/s (megabits per second) bitstream is divided. Therefore, station managers and broadcast engineers could run any of the following scenarios using one 6 MHz channel (note that the actual bitrate moves up and down, due to usage of variable bitrate encoding):\n\nWith improvements in MPEG encoding, and tighter VBR encoding, more subchannels can be combined. 1×720p + 4×480i is becoming more common.\n\nFor a frame rate of 30p or 60i, uncompressed DTV channels have the following data rates in megapixels per second:\nFor ATSC, these must be compressed into 19.4Mbit/s total per physical 6 MHz RF channel over the air, and 38.8Mbit/s for cable.\n\nVarious forms of digital radio also allow for multiple program streams.\n\n"}
{"id": "450703", "url": "https://en.wikipedia.org/wiki?curid=450703", "title": "Display device", "text": "Display device\n\nA display device is an output device for presentation of information in visual or tactile form (the latter used for example in tactile electronic displays for blind people). When the input information that is supplied has an electrical signal, the display is called an \"electronic display\".\n\nCommon applications for \"electronic visual displays\" are televisions or computer monitors.\n\nIn the history of display technology, a variety of display devices and technologies have been used.\n\nThere are various designs for display devices, using various technologies. Several components are common to most display devices.\n\n\nThese are the technologies used to create the various displays in use today.\n\nSome displays can show only digits or alphanumeric characters. They are called segment displays, because they are composed of several segments that switch on and off to give appearance of desired glyph. The segments are usually single LEDs or liquid crystals. They are mostly used in digital watches and pocket calculators. There are several types:\n\n\n2-dimensional displays that cover a full area (usually a rectangle) are also called video displays, since it is the main modality of presenting video.\n\nFull-area 2-dimensional displays are used in, for example:\n\n\nUnderlying technologies for full-area 2-dimensional displays include:\n\n\nThe multiplexed display technique is used to drive most display devices.\n\n\n\n"}
{"id": "33879721", "url": "https://en.wikipedia.org/wiki?curid=33879721", "title": "EnCor Biotechnology", "text": "EnCor Biotechnology\n\nEnCor Biotechnology is a United States company that manufactures antibodies to neural and yeast proteins. Founded in 1999 as a spin-off from the University of Florida by Gerry Shaw, a British scientist and professor at the University of Florida, the company is based in Gainesville, Florida and markets antibody reagents originally made for research purposes, but which also have commercial value.\n\nEnCor Biotechnology was formed in at the end of 1999 to market antibody reagents originally made for research purposes, but which also had some commercial potential. Three years later EnCor moved into rented lab space at the Sid Martin Biotechnology Incubator, a facility dedicated to commercialization of intellectual property generated in the University of Florida. The company quickly become profitable and, in 2006, relocated to new premises in Gainesville.\nIn 2005, EnCor began collaborating with basic scientists and clinicians to produce articles in peer reviewed scientific publications focused on the examination of various plasma, serum and CSF biomarkers of nervous system damage and degeneration. One of these is the phosphorylated, axonal form of the major neurofilament protein heavy chain protein which has the HGNC name NEFH, though is usually referred to as pNF-H in the scientific literature. Two further studies describe novel EnCor assays for UCHL1 and alpha-synuclein, two major brain proteins implicated in the development of Parkinson's and other neurological diseases.\n\nBy 2012, EnCor product line had increased to almost 100 products, the antibodies mostly being used for research purposes, with a particular focus on immunocytochemistry and western blotting, though many are also utilized for immunoprecipitation and ELISA. Some have become useful for diagnostic histopathology and for monitoring the levels of protein biomarkers, of research and potential clinical utility. EnCor supplies reagents to research labs and other reagent companies such as Abcam, Covance, Invitrogen, Millipore Corporation, and Novus Biologicals.\n\n"}
{"id": "58222614", "url": "https://en.wikipedia.org/wiki?curid=58222614", "title": "Farm Forward", "text": "Farm Forward\n\nFarm Forward is a 501(c)(3) nonprofit organization that seeks to promote conscientious eating, reduce suffering for farmed animals, and advance sustainable agriculture. It was founded in 2007 by its current CEO, Aaron Gross, Ph. D., whose main work has been in animal advocacy, as well as in the emerging scholarly field of animals and religion. Farm Forward is an animal advocacy group that is dedicated to the welfare of farmed animals, working to ensure that farmed animals are treated in a way that is humane and sustainable. In addition to working with farmers and other animal advocacy groups, Farm Forward works with farmers, animal advocacy groups, and various institutions in order to encourage consumers to seek out animal products that promote animal welfare and sustainability.\n\nBuying Poultry is a website that was launched by Farm Forward in 2015 with the goal of empowering consumers to purchase poultry products that align with their values. Buying Poultry is a national database that rates poultry products based on how humanely the chickens were treated on their farms. It describes the different labels that are associated with these products, and explains what these labels entail in the industry, so that consumers may be better informed about the products they buy. On the website, a number of poultry products are assessed in accordance with their welfare certifications―, and ―and the various labels that are associated with the products.\n\nUltimately, the primary aim of the Buying Poultry website is to increase the demand for humane and sustainable poultry products through consumers. Accordingly, it is meant to bring about an increase in the supply for such products, while simultaneously decreasing both supply and demand for standard industry poultry products.\n\nBuying Mayo was a campaign launched in 2014 by Farm Forward against Best Food and Hellman's mayonnaise, both of which are owned by Unilever. At the time, Best Foods and Hellmann’s were participating in the practice of maceration, the process of sending male birds of the egg laying genetic strain to their deaths shortly after they hatch due to their inability to lay eggs. The purpose of the Buying Mayo campaign was to convince Best Foods and Hellmann’s to end this practice.\nThe Buying Mayo campaign succeeded in convincing Best Foods and Hellmann’s to end its use of maceration. It did this in two ways. First, it started a petition that demanded the companies to find other ways to deal with these male chicks than maceration. Second, it posted two videos with essentially the same message as the petition. These resulted in a promise by Unilever (Best Foods and Hellmann’s parent company) to seek alternatives to maceration.\n\nAg-gag state legislation is aimed at punishing participants of undercover farm investigations. In 2012, Farm Forward launched a campaign to prevent ag-gag laws from being made. It established a website, ag-gag.org, to gather public opposition to ag-gag. Between 2012 and March 2016, 16 states introduced ag-gag bills which failed to be implemented. Still, other states have succeeded in passing ag-gag bills, some of which are currently in effect.\n\nIn 2014 Idaho successfully passed an ag-gag bill. As a response to this, Farm Forward joined a coalition of animal protection organizations, civil rights groups, and media outlets to file a lawsuit against Idaho in federal court. The result of this lawsuit was that Idaho’s ag-gag laws were ruled unconstitutional due to infringement of the first amendment. According to University of Denver Law professor Justin Marceau, “[the ruling] means that these laws all over the country are in real danger.” Farm Forward, along with the coalition it joined, continues to fight ag-gag across the United States.\n\nFarm Forward is working alongside the Humane Society of the United States (HSUS) on the Faith in Food initiative. This initiative is meant to reach out to various religious institutions in order to motivate them to create ethical food policies that address animal welfare in accordance with their specific faiths and values.\n\nThrough its religious outreach, Farm Forward helped to arrange a partnership between the HSUS and Hazon. Through their partnerships, the two groups hope to promote compassionate food choices, especially in the Jewish community.\n\nIn addition to reaching out to religious institutions, Farm Forward also supports publications that encourage scholarly discussions of the role of animals in religion. CEO Aaron Gross’ \"The Question of the Animal and Religion\", David Clough’s \"On Animals Volume 1\", and Jonathan Safran Foer’s \"Eating Animals\" are some of the books supported by Farm Forward for their promotions of animals as religious subjects.\n\nOne particular part of Farm Forward's religious outreach is the Jewish Initiative for Animals (JIFA). JIFA is a Farm Forward initiative that aims at educating the Jewish community on the ethics of factory farming and encouraging Jewish institutions to promote eating policies that elevate animal welfare as a significant Jewish value. JIFA urges these institutions to act in any way they can to promote animal welfare. JIFA focuses particularly on Jewish institutions that serve animal products in order to assist them in lowering meat consumption and finding higher welfare sources. It also helps these institutions establish new supply chains so that their animal products will support more humane farming methods.\n\nEating Animals is a non-fiction novel written in collaboration with Farm Forward by one of its current board members, Jonathan Safran Foer. Eating Animals provides a discussion of the many consequences that have followed the proliferation of factory farms. Moreover, it attempts to explain why and how humans can be so loving to some animals while simultaneously being indifferent to others, and explores what this inconsistency tells us about ourselves. The book offers a significant emphasis on “storytelling,” which is the title of both the first and the last chapters of the book. This is Foer’s way of recognizing and dealing with the complexity of the subject that is eating animals, and suggests that, ultimately, “stories about food are stories about us―our history and our values.”\n\nFor Farm Forward, Eating Animals is a way to bring people into a discussion about the current state of animal agriculture. Farm Forward encourages the use of Eating Animals as an educational tool for classrooms around the world, reaching out to colleges and high schools to suggest an inclusion of the book into the classroom. Farm Forward organizes annual “virtual classroom visits” with Jonathan Safran Foer, during which classrooms all over the world discuss the prevailing themes of animal and food ethics found in Eating Animals.\n\nThe book was adapted and extended into a 2018 documentary film with the same name, directed by Christopher Dillon Quinn and co-narrated by Foer and Natalie Portman. Like the book, the documentary is meant to explore contemporary animal agriculture alongside the complexities of food ethics. Farm Forward hopes to see this documentary expand the reach of Eating Animals’ message so that more people think of eating animals in new ways.\n"}
{"id": "49690837", "url": "https://en.wikipedia.org/wiki?curid=49690837", "title": "Flexible debris-resisting barrier", "text": "Flexible debris-resisting barrier\n\nFlexible debris-resisting barrier, also known as protective rock barrier fence and rockfall catch fence, is a steel ring net barrier to resist the impact of landslide debris flow or boulders and prevent further downslope flow. Flexible debris-resisting barriers have been adopted as a mitigation measure for retention of potential landslide debris.\n"}
{"id": "17910841", "url": "https://en.wikipedia.org/wiki?curid=17910841", "title": "Flexible glass", "text": "Flexible glass\n\nFlexible glass is a legendary lost invention from the time of the reign of the Roman Emperor Tiberius Caesar (between 14–37 AD). As recounted by Isidore of Seville, the craftsman who invented the technique brought a drinking bowl made of flexible glass before Caesar who tried to break it, whereupon the material dented, rather than shattering. The inventor then repaired the bowl easily with a small hammer. After the inventor swore to the Emperor that he alone knew the technique of manufacture, Tiberius had the man beheaded, fearing such material could undermine the value of gold and silver.\n\nThe story of the sad fate of the inventor of unbreakable glass (vitrum flexile) at the hands of Tiberius (42 BC–37 AD) was first related by two more-or-less contemporary compilers, namely Petronius (c. 27–66 AD, \"Satyricon\" 51) and Pliny the Elder (23–79 AD, Naturalis Historia XXXVI.lxvi.195). Pliny apparently did not believe in vitrum flexile, remarking that the story is, \"more frequently told than it is reliable\". Cassius Dio (c. AD 150–235, Historia Romana 57.21.7) also told a similar story, writing that ‘an architect whose name no one knows’ whom Tiberius had exiled out of jealousy for his skill, ‘approached him to crave pardon, and while doing so purposely let fall a crystal goblet; and though it was bruised in some way or shattered, yet by passing his hands over it he promptly exhibited it whole once more. For this he hoped to obtain pardon, but instead the emperor put him to death.’ The story was picked up and retold by Isidore of Seville (c. 560–636, Etymologiae XVI.16.6, ‘De vitro’). Subsequently the 13th century collection of technical recipes by the pseudo-Heraclius included this story, in the section treating glass recipes, using Isidore’s own words (De coloribus et artibus Romanorum III.vi [256]). This is thus an example of ‘technical knowledge’ wending its way from historical gossip (Petronius), to a technical encyclopaedia (Pliny, with reservations), and then via a philological text (Isidore) to a collection of practical craft recipes (pseudo-Heraclius).\n"}
{"id": "2824495", "url": "https://en.wikipedia.org/wiki?curid=2824495", "title": "Fred Baker (IETF chair)", "text": "Fred Baker (IETF chair)\n\nFrederick J. Baker, better known as Fred Baker (born February 28, 1952), is an American engineer, specializing in developing computer network protocols for the Internet.\n\nBaker attended the New Mexico Institute of Mining and Technology from 1970 to 1973. He developed computer network technology starting in 1978 at Control Data Corporation (CDC), Vitalink Communications Corporation, and Advanced Computer Communications.\n\nHe joined Cisco Systems in 1994.\nHe became a Cisco Fellow in 1998, working in university relations and as a research ambassador, and in the IETF.\n\nSince 1989, Baker has been involved with the Internet Engineering Task Force (IETF), the body that develops standards for the Internet.\nHe chaired a number of IETF working groups, including several that specified the management information bases (MIB) used to manage network bridges and popular telecommunications links.\nBaker served as IETF chair from 1996 to 2001, when he was succeeded by Harald Tveit Alvestrand. \nHe served on the Internet Architecture Board from 1996 through 2002. He has co-authored or edited around 50 Request for Comments (RFC) documents on Internet protocols and contributed to others. The subjects covered include network management, Open Shortest Path First (OSPF) and Routing Information Protocol (RIPv2) routing, quality of service (using both the Integrated services and Differentiated Services models), Lawful Interception, precedence-based services on the Internet, and others.\n\nIn addition, he served as a member of the Board of Trustees of the Internet Society 2002 through 2008, and as its chair from 2002 through 2006. He was a member of the Technical Advisory Council of the US Federal Communications Commission from 2005 through 2009.\nHe has worked as liaison to other standards organizations such as the ITU-T.\nIn 2009 he became chair of the RFC Series Oversight Committee.\n\nBaker also has several patents.\n\n"}
{"id": "482952", "url": "https://en.wikipedia.org/wiki?curid=482952", "title": "Frequency mixer", "text": "Frequency mixer\n\nIn electronics, a mixer, or frequency mixer, is a nonlinear electrical circuit that creates new frequencies from two signals applied to it. In its most common application, two signals are applied to a mixer, and it produces new signals at the sum and difference of the original frequencies. Other frequency components may also be produced in a practical frequency mixer.\n\nMixers are widely used to shift signals from one frequency range to another, a process known as heterodyning, for convenience in transmission or further signal processing. For example, a key component of a superheterodyne receiver is a mixer used to move received signals to a common intermediate frequency. Frequency mixers are also used to modulate a carrier signal in radio transmitters.\n\nThe essential characteristic of a mixer is that it produces a component in its output which is the product of the two input signals. A device that has a non-linear (e.g. exponential) characteristic can act as a mixer. Passive mixers use one or more diodes and rely on their non-linear relation between voltage and current to provide the multiplying element. In a passive mixer, the desired output signal is always of lower power than the input signals.\n\nActive mixers use an amplifying device (such as a transistor or vacuum tube) to increase the strength of the product signal. Active mixers improve isolation between the ports, but may have higher noise and more power consumption. An active mixer can be less tolerant of overload.\n\nMixers may be built of discrete components, may be part of integrated circuits, or can be delivered as hybrid modules.\nMixers may also be classified by their topology: \nSelection of a mixer type is a trade off for a particular application.\n\nMixer circuits are characterized by their properties such as conversion gain (or loss), and noise figure.\n\nNonlinear electronic components that are used as mixers include diodes, transistors biased near cutoff, and at lower frequencies, analog multipliers. Ferromagnetic-core inductors driven into saturation have also been used. In nonlinear optics, crystals with nonlinear characteristics are used to mix two frequencies of laser light to create optical heterodynes.\n\nA diode can be used to create a simple unbalanced mixer. This type of mixer produces the original frequencies as well as their sum and their difference. The significant property of the diode here is its non-linearity (or non-Ohmic behavior), which means its response (current) is not proportional to its input (voltage). The diode does not reproduce the frequencies of its driving voltage in the current through it, which allows the desired frequency manipulation. \nThe current \"I\" through an ideal diode as a function of the voltage \"V\" across it is given by\nwhere what is important is that \"V\" appears in \"e\"'s exponent. The exponential can be expanded as\nand can be approximated for small \"x\" (that is, small voltages) by the first few terms of that series:\n\nSuppose that the sum of the two input signals formula_4 is applied to a diode, and that an output voltage is generated that is proportional to the current through the diode (perhaps by providing the voltage that is present across a resistor in series with the diode). Then, disregarding the constants in the diode equation, the output voltage will have the form\nThe first term on the right is the original two signals, as expected, followed by the square of the sum, which can be rewritten as formula_6, where the multiplied signal is obvious. The ellipsis represents all the higher powers of the sum which we assume to be negligible for small signals.\n\nSuppose that two input sinusoids of different frequencies are fed into the diode, such that formula_7 and formula_8. The signal formula_9 becomes:\nExpanding the square term yields: \nIgnoring all terms except for the formula_12 term and utilizing the prosthaphaeresis (product to sum) identity, \nyields, \ndemonstrating how new frequencies are created from the mixer.\n\nAnother form of mixer operates by switching, with the smaller input signal being passed inverted or non inverted according to the phase of the local oscillator (LO). This would be typical of the normal operating mode of a packaged double balanced mixer, with the local oscillator drive considerably higher than the signal amplitude.\n\nThe aim of a switching mixer is to achieve linear operation over the signal level by means of hard switching driven by the local oscillator. Mathematically the switching mixer is not much different from a multiplying mixer, just because instead of the LO sine wave term we would use the signum function. In the frequency domain the switching mixer operation leads to the usual sum and difference frequencies, but also to further terms e.g. ±3\"f\", ±5\"f\", etc.\nThe advantage of a switching mixer is that it can achieve (with the same effort) a lower noise figure (NF) and larger conversion gain. This is because the switching diodes or transistors act either like a small resistor (switch closed) or large resistor (switch open), and in both cases only a minimal noise is added. From the circuit perspective, many multiplying mixers can be used as switching mixers, just by increasing the LO amplitude. So RF engineers simply talk about mixers and mean switching mixers.\n\nThe mixer circuit can be used not only to shift the frequency of an input signal as in a receiver, but also as a product detector, modulator, phase detector or frequency multiplier. For example, a communications receiver might contain two mixer stages for conversion of the input signal to an intermediate frequency and another mixer employed as a detector for demodulation of the signal.\n\n\n"}
{"id": "3720714", "url": "https://en.wikipedia.org/wiki?curid=3720714", "title": "George Cowan", "text": "George Cowan\n\nGeorge A. Cowan (; February 15, 1920 – April 20, 2012) was an American physical chemist, a businessman and philanthropist.\n\nHe conducted early research in the Manhattan Project. George served 39 years at Los Alamos National Laboratory as director of chemistry, associate director of research and senior laboratory fellow. He participated in founding the Santa Fe Opera in 1953. He founded the Los Alamos National Bank in 1963 to provide a means to obtain housing for Los Alamos employees and served for 30 years as its chair. He was also the driving influence in founding the Santa Fe Institute together with Nobel Prize winner Murray Gell-Mann and others in 1984, based upon his recognition of the need for a place where scientists could be offered a broader curriculum for the development of \"a kind of twenty-first century Renaissance man\" and associated research. A graduate of Worcester Polytechnic Institute (bachelor of science in Chemistry) and Carnegie Institute of Technology (doctorate of science), Princeton University, and the University of Chicago, he worked on the top secret Manhattan Project at Los Alamos during World War II. He received the Enrico Fermi Award for \"a lifetime of exceptional achievement in the development and use of energy,\" the New Mexico Academy of Science Distinguished Scientist Award, the Robert H. Goddard Award, the E.O. Lawrence Award, and the Los Alamos National Laboratory Medal, which is the highest honor the Laboratory bestows upon an individual or small group.\n\nCowan was born in Worcester, Massachusetts. In 1941, at the age of twenty one, after graduating from Worcester Polytechnic Institute in chemistry, he worked on the cyclotron project at Princeton University with the intention of taking graduate courses in physics. He worked there with future Nobel Prize Laureate Eugene Wigner, who would design the first uranium chain reactor. In 1941, George participated in taking measurements essential to determining whether the chain reaction in uranium could be achieved. His knowledge of chemistry and nuclear physics experience provided expertise on a number of things necessary to the Manhattan Project. In 1942, Wigner, Cowan, and several others transferred to the Metallurgy Lab at the University of Chicago where the first atomic pile was being developed under Enrico Fermi. Starting as a junior member, Cowan became a jack-of-all-trades, capable of such skills as machining graphite blocks used for control of the pile's reaction rate and in casting uranium metal. In 1942, the Chicago Pile 1 (CP-1) generated the first controlled nuclear reaction. This controlled release of energy from the nucleus of the atom enabled development of a method to obtain nuclear fuel for the first atomic weapons. His experience made him one of the experts on the chemistry of radioactive elements in the field of applied nuclear fission. Since he was single and in possession of high expertise, project managers transferred him around the nation to help resolve bottlenecks. He was one of the select group with knowledge of the separate components of the project, kept separate for security reasons. He received a draft deferment from the president of the United States for possessing skills uniquely useful to the war effort.\n\nFollowing the end of the war and obtaining his PhD in physical chemistry from Carnegie Tech, Cowan returned to work for Los Alamos in 1950. Only weeks after his arrival, he directed the detection of radioactive fallout from samples collected near the Russian border indicating the Soviets were in possession of a nuclear bomb. He also participated for some years in the Bethe Panel, whose first chairman was Hans Bethe. One of his early participatory functions on the panel was to convince U.S. government officials that the radiochemistry of the samples proved that it was not the result of a peaceful nuclear reactor problem, but a Soviet bomb, which was dubbed \"Joe-1\" after Joseph Stalin.\n\nIn 1953, Cowan was a member of the group which founded the Santa Fe Opera. Another member of this group was Arthur Spiegel, of the Spiegel Catalog fortune. Art was later to help Cowan in his initial fund raising efforts to finance the Santa Fe Institute.\n\nIn 1982, Cowan accepted a seat on the White House Service Council. While serving in this capacity and facing problems involving interlinked aspects of science, policy, economics, environment and more, he realized that this demanded a broad range expertise above the current reductionist approach and fragmentation of the sciences. He believed that our educational culture was enforcing intellectual fragmentation through conservative university programs that depended on specialized grants and funded work. It seemed that cross-disciplinary team efforts were discouraged by membership in traditional, isolated science and social science disciplines. He knew that beginning in the 1980s numerical experiments through computer simulations were capable of providing the tools to think about very complex problems in a more holistic fashion. He began to imagine a new and independent type of institute that would combine the charter of a university while sharing some of Los Alamos' personnel and computer power. This could be a place where senior researchers would have a place to work on very speculative ideas. Where one could educate a man starting in science, but that could deal with the real messy world, which is not elegant, which science doesn't really deal with. In 1983, Cowan assembled a group of senior scientists interested in researching complex, adaptive systems. One year later, this assembly became the Santa Fe Institute. Initial funding came from the National Science Foundation, the Department of Energy, Citicorp and others. George was enthusiastic about complex systems, which he declared to be the next major thrust in science. The Santa Fe Institute fosters interdisciplinary research between physicists, mathematicians, economists, computer scientists, and others. Although most of his duties as president did not allow time for research, as Distinguished Fellow of the Institute, Cowan applied neuroscience principles to investigate relationships between children's brain physiological changes and behavioral development.\n\nIn 1988, Cowan became a senior fellow emeritus at Los Alamos, a member of a group of six longtime Los Alamos employees rewarded with research positions free from administrative chores that would also advise the laboratory director on policy issues. Cowan served as president of the Santa Fe Institute until his retirement in 1991.\n\nCowan died on April 20, 2012 from complications of pneumonia in his Los Alamos home.\n\n\n\n\n"}
{"id": "1559010", "url": "https://en.wikipedia.org/wiki?curid=1559010", "title": "Grapple (tool)", "text": "Grapple (tool)\n\nA grapple is a hook or claw used to catch or hold something. A ship's anchor is a type of grapple, especially the \"grapnel\" anchor.\nA throwing grapple (or \"grappling hook\") is a multi-pronged hook that is tied to a rope and thrown to catch a grip, as on a parapet or branch of a tree. It may also be used in a boat to \"drag\" the bottom of a waterway to hook debris or to find missing objects.\nIn logging and other engineering vehicles, a grapple is a hydraulically powered claw with two or more opposing levers that pinch a log or other materials, usually to lift or drag them.\n\nThe logging grapple used in swing yarding is not moved by hydraulics but by cables. To open and close the tongs of the grapple, two cables are used. One is tensioned and the other is slacked off to move the tongs. A third cable goes back to the tail hold then to the yarder. This third cable is used to pull the grapple out into the setting and to create tension for lifting the grapple in the air. \n\nA grapple can be mounted to a tractor or excavator with a movable arm that may lift, extend/retract, and move side-to-side (pivot or rotate). Some machines also have a separate control for rotating the grapple. \n\nSimpler grapple machines consist of a hydraulically liftable fork, rake (\"grapple rake\"), or bucket and a movable, opposing \"thumb\" (one or more hooks or levers) that enclose and grip materials for lifting or dragging. A \"demolition bucket\" or \"multi-purpose bucket\" on a loader may also operate as a grapple whereby the bottom and rear side of the bucket are hinged and can be forced apart or together with hydraulic cylinders.\n\nA lifting grapple is a type of hardware that can be attached to most large, heavy, or bulky objects to provide a feature on the item to which material handling equipment can attach. Lifting grapples sometimes double as tie downs, allowing heavy items to be held firmly in place by providing a point to which ropes or chains can be attached to the item to hold it in place.\n\nThe term \"grapple\" is also used in the surfboard industry to refer to a leash connector.\n\n"}
{"id": "5876226", "url": "https://en.wikipedia.org/wiki?curid=5876226", "title": "IC programming", "text": "IC programming\n\nIC programming is the process of transferring a computer program into an integrated computer circuit. Older types of IC including PROMs and EPROMs and some early programmable logic was typically programmed through parallel busses that used many of the device's pins and basically required inserting the device in a separate programmer.\n\nModern ICs are typically programmed in circuit though a serial protocol (sometimes JTAG sometimes something manufacturer specific). Some (particularly FPGAs) even load the data serially from a separate flash or prom chip on every startup.\n"}
{"id": "57725002", "url": "https://en.wikipedia.org/wiki?curid=57725002", "title": "ISO 9060", "text": "ISO 9060\n\nISO 9060, \"Specification and classification of instruments for measuring hemispherical solar and direct solar radiation\", is an ISO standard for the classification of pyranometers and pyrheliometers.\n"}
{"id": "13752554", "url": "https://en.wikipedia.org/wiki?curid=13752554", "title": "Isofoton", "text": "Isofoton\n\nISOFOTON was a global company, present in over 60 countries. ISOFOTON was involved in designing, manufacturing, and supplying Solar Energy products. Its activities were centered in three technology categories: Photovoltaic, Thermal, and High Concentration Photovoltaic.\n\nISOFOTON was founded in Málaga (Spain) in 1981 as a spin-off of a university project driven by Professor D. Antonio Luque of the Polytechnic University of Madrid, its first Chairman of the Board.\n\nIn 1985 ISOFOTON consolidated its activities in the solar energy field, incorporating the production technology of thermal collectors.\n\nIn 1997 the Bergé Group became the owner of ISOFOTON.\nIn July 2007 the Alba Corporation bought 26% of the company with the intention of joining the stock market.\n\nFrom 2000 to 2005 ISOFOTON ranked among the world's top 10 photovoltaic manufacturers\n\nIn February 2008, company CEO José Luis Manzano was relieved from duty. Carlos Torres, former managing director of Endesa, took his place.\n\nIn July of the same year, the Bergé Group bought back 26% of ISOFOTON, property of Alba Corporation.\n\nIn July 2010 ISOFOTON was acquired by AFFIRMA Business Group (80% ownership)and TOPTEC (20% ownership), a South Korean company that specializes in industrial automation.\n\nIn 2013 the company was investigated for inappropriate use of public funds and approached bankruptcy, finally closed down its facilities in January 2014.\n\nThe factory in Málaga had more than 28000 m of facilities and is located in the Andalusia Technology Park. \nProduction did include:\n\nISOFOTON worked in collaboration with universities and research centers in Spain and around the world. Among its principal strategic alliances, they have agreements with companies such as INDRA for the development of trackers designed for HCPV modules, or the ISFOC for research in this field.\n\nMany research projects held by ISOFOTON had an international dimension. ISOFOTON was involved in several projects in the FP7 (Seventh Framework Programme for research and technological development). One of them is related to high concentration photovoltaic technology and three are dedicated to silicon technology.\n\n2015, the company went bankrupt and its remaining assets, which were valued at 57 millions, were auctioned to cover a debt of 160 millions.\n\n"}
{"id": "7495326", "url": "https://en.wikipedia.org/wiki?curid=7495326", "title": "J. George Mikelsons", "text": "J. George Mikelsons\n\nJuris George Mikelsons (Latvian: Georgs Juris Miķelsons) is a former airline executive and airline pilot in the United States and the founder of ATA Airlines. He was born in Riga, Latvia, in 1938 on the eve of World War II. His family fled to Germany during the mid-1940s to escape the Soviet occupation of his native land.\n\nAs a child, Mikelsons would peer out of bomb shelters to catch any glimpse he could of the planes being flown in the skies. This was the birth of his passion in life, which was to fly planes. His family moved to Indianapolis, Indiana, during the 1950s where his father was offered a job as a violinist for the Indianapolis Symphony Orchestra.\n\nMikelsons finally began pursuing his passion when he saw a sign offering flights for under $10. It was then that he flew for the first time. This flight sparked his desire to enter the aviation industry. He immediately began flying lessons and became chief pilot and director of the \"Voyager 1000 travel club\".\n\nIn 1973, Mikelsons started his own travel club, \"Ambassadair\", taking a loan and mortgaging his home to purchase a Boeing 720, which he titled \"Miss Indy\". Ambassadair was a charter-based airline which provided cheap vacation fares. Mikelsons and an employee piloted the plane, loaded the luggage, cleaned the cabin, and served as the tour guide. His hard work and determination allowed him to purchase additional planes for his charter-based airline service. In 1984, after the deregulation of the Airline industry, Mikelsons formed Amtran, Inc., the former parent company of American Trans Air, which later became ATA Airlines, headquartered near Indianapolis International Airport. He also formed ATA Leisure Corp., Amber Travel, ATA Training Corp., ATA Cargo, and ExecuJet.\n\nIn 1993, ATA made its initial public offering, trading on the NASDAQ National Market System under the symbol \"AMTR\". Mikelsons, a noted conservative always notorious for putting his money where his mouth is, purchased a 75% stake in the company. In 1998, Mikelsons retired, giving the reigns of the company to John Tague. Tague began a massive expansion with new aircraft and the establishment of a hub at Chicago's Midway airport. After the September 11 terrorist attacks in 2001, the slump in airline travel and rising fuel costs severely impacted ATA.\n\nMikelsons came out of retirement to save his troubled airline despite his archaic managerial style. Many of ATA's finest leaders left because of this. ATA emerged from bankruptcy on February 28, 2006 after Mikelson had secured a major code-share agreement with Southwest Airlines. Upon emergence from bankruptcy, Mikelson retired from ATA and its airline holdings, turning the reins of the company to a MatlinPatterson lead board of directors and former Southwest executive John G. Denison. ATA Airlines filed for bankruptcy protection again on April 2, 2008, and announced the discontinuation of all operations \"following the loss of a key contract for our military charter business\", ATA said on its website.\n\nMikelsons and his wife Muriel live near Indianapolis and have two children (Jay and David Mikelsons). He is the 1996 recipient of the \"Tony Jannus Award\" for outstanding leadership in the commercial aviation industry. Mikelsons owns and flies a Bell Jet Ranger III helicopter. His wife Muriel is a concert violinist. They are active in a number of area charitable organizations, including the Indianapolis Children's Museum and the Indianapolis Symphony Orchestra.\n\n"}
{"id": "57590207", "url": "https://en.wikipedia.org/wiki?curid=57590207", "title": "KOMP (communication device)", "text": "KOMP (communication device)\n\nKOMP is a communication device for seniors, developed by the Norwegian startup No Isolation. The KOMP device is placed at the senior's home, while family members or friends send photos, messages or start video calls from an app for iOS and Android. The product was launched November 2017.\n\nThe device also supports widgets, like weather alerts provided by the Norwegian Meteorological Institute.\n\nThe project received a Seal of Excellence from the European Commission with the proposal “KOMP - the one-button communication device tailored to put an end to loneliness among seniors” as part of the Horizon 2020 \"SME\" instrument phase 2 in the category “Accelerating market introduction of ICT solutions for Health, Well-Being and Ageing Well.”\n\n"}
{"id": "44372555", "url": "https://en.wikipedia.org/wiki?curid=44372555", "title": "List of microconsoles", "text": "List of microconsoles\n\nThis is a list of microconsoles from the first created to the present, in chronological order.\n\nThe microconsole market started in the seventh generation era of video game consoles, and this market has quickly grown during the eighth generation era of gaming consoles, at the same time as other types of video game consoles.\n\n"}
{"id": "2543967", "url": "https://en.wikipedia.org/wiki?curid=2543967", "title": "List of submarine operators", "text": "List of submarine operators\n\nThe following countries operate or have operated submarines for naval or other military purposes.\n\n\nBallistic missile submarines are larger than any other type of submarine, in order to accommodate ballistic missiles capable of carrying nuclear warheads.\n\n\n\n"}
{"id": "51150571", "url": "https://en.wikipedia.org/wiki?curid=51150571", "title": "Mega Food Parks", "text": "Mega Food Parks\n\nMega Food Park is an inclusive concept and a scheme of the Ministry of Food Processing of the Government of India, aimed at establishing a \"direct linkage from farm to processing and then to consumer markets\" through a network of collection centres and primary processing centres. Its purpose was to increase processing of perishables from 6% to 20% and to increase India's Share in global food trade by at least 3% up to year 2015.\n\nAccording to an estimate, the Indian food industry was to grow from $200 million USD to $310 million USD in 2015 by this scheme.\n\nA total of 42 Mega Food Parks have been sanctioned so far by MoFPI in six phases\nThese MFPs are to ensure backward linkages to the farmers, SHGs, JLVs etc. & enhance farmer income. Each MFP is supposed to connect with 25000 farmers\nGovernment has envisaged building quality labs at each of the food parks as well\n\nA sanction of 42 food parks has been planned, out of which 25 in various states have already been sanctioned with 17 pending, expression of interest is available from companies with the government. According to the Government, as of October 2016, 8 mega food parks have become operational and all 42 would be operational in the next 2 years.\n"}
{"id": "27459288", "url": "https://en.wikipedia.org/wiki?curid=27459288", "title": "Minimum safe altitude warning", "text": "Minimum safe altitude warning\n\nMinimum safe altitude warning (MSAW) is an automated warning system for air traffic controllers (ATCO). It is a ground-based safety net intended to warn the controller about increased risk of controlled flight into terrain accidents by generating, in a timely manner, an alert of aircraft proximity to terrain or obstacles.\n\nICAO Doc 4444 requires that radar systems should provide for the display of safety-related alerts including the presentation of minimum safe altitude warning. It is worth mentioning that ICAO Doc 4444 does not provide a definition of the term MSAW. Instead the term MSAW is ambiguously used in ATC community to identify such warnings as well as for data processing systems providing the alert function.\n\n"}
{"id": "1602656", "url": "https://en.wikipedia.org/wiki?curid=1602656", "title": "Ministry of Energy (Ontario)", "text": "Ministry of Energy (Ontario)\n\nThe Ministry of Energy’s responsibility is ensuring that Ontario’s electricity system functions with reliability and productivity, and promoting innovation in the energy sector. In April 2002, it was renamed the Ministry of Energy, with the newly created Ministry of Enterprise, Opportunity and Innovation taking over responsibility for its science and technology portfolio. It was integrated as the Ministry of Energy and Infrastructure between 2007 and 2010, before it was split back into the Ministry of Energy on August 18, 2010.\n\nThe Minister of Energy is the Honourable Greg Rickford.\n\nSeveral agencies and crown corporations are under the Ministry:\n\n\nFrom 1974 to 1999 Ontario Hydro reported to the Minister, but the public utility was broken up into various agencies and crown corporations. Prior to 1974 Ontario Hydro reported to Commissioners appointed by the Premier and a few were Ministers without Portfolio.\n\nThe Commission was headed by a Chairman:\n\n\n\n\n\n"}
{"id": "58908087", "url": "https://en.wikipedia.org/wiki?curid=58908087", "title": "Modern Meadow", "text": "Modern Meadow\n\nModern Meadow is a company that produces Zoa, a biologically-produced, ethical and environmentally-friendly leather alternative that is based on bovine collagen. The company was recognized by the World Economic Forum as a 2018 Technology Pioneer, and a Zoa shirt is on permanent display at the MOMA. Modern Meadows was started in 2011 by CEO Andras Forgacs, formerly of Organovo, a company that uses 3-D printing of human tissue for medical use. Modern Meadow has raised approximately $54 million from investors including Singapore's Temasek and Horizon Ventures, the private investment fund of the Hong Kong billionaire, Li Ka-shing. In 2018, Modern Meadow signed an agreement with Evonik, a chemical company that specializes in microbial fermentation with the goal of commercially producing biofabricated leather. It takes two weeks to create Zoa in a lab, and the material can be made to match any thickness, texture or color. Zoa can also be created in liquid form.\n"}
{"id": "3575278", "url": "https://en.wikipedia.org/wiki?curid=3575278", "title": "Neon message board", "text": "Neon message board\n\nA neon (or glass) message board or glassboard is an erasable luminescent whiteboard-like writing pad that enables users to leave messages for others. They have been a common way of displaying daily specials at restaurants. Colored fluorescent non-permanent wet wipe markers are used to write notes, messages or menus on the board, which is illuminated from the edge by a fluorescent light, usually a blacklight tube.\n\nThe message board is cleaned by wiping a wet tissue or cloth across the surface to dissolve the markings. A dry tissue or soft cloth is then used to remove the residue. Board surfaces are typically black in color, and neon message boards often come with bright colored markers that are green, yellow, orange or pink.\n\n"}
{"id": "811550", "url": "https://en.wikipedia.org/wiki?curid=811550", "title": "Over-the-air programming", "text": "Over-the-air programming\n\nOver-the-Air programming (OTA) refers to various methods of distributing new software, configuration settings, and even updating encryption keys to devices like cellphones, set-top boxes or secure voice communication equipment (encrypted 2-way radios). One important feature of OTA is that one central location can send an update to all the users, who are unable to refuse, defeat, or alter that update, and that the update applies immediately to everyone on the channel. A user could \"refuse\" OTA but the \"channel manager\" could also \"kick them off\" the channel automatically.\n\nIn the context of the mobile content world these include over-the-air service provisioning (OTASP), over-the-air provisioning (OTAP) or over-the-air parameter administration (OTAPA), or provisioning handsets with the necessary settings with which to access services such as WAP or MMS.\n\nAs mobile phones accumulate new applications and become more advanced, OTA configuration has become increasingly important as new updates and services come on stream. OTA via SMS optimizes the configuration data updates in SIM cards and handsets and enables the distribution of new software updates to mobile phones or provisioning handsets with the necessary settings with which to access services such as WAP or MMS. OTA messaging provides remote control of mobile phones for service and subscription activation, personalization and programming of a new service for mobile operators and telco third parties.\n\nVarious standardization bodies were established to help develop, oversee, and manage OTA. One of them is the Open Mobile Alliance (OMA).\n\nMore recently, with the new concepts of Wireless Sensor Networks and the Internet of Things, where the networks consist of hundreds or thousands of nodes, OTA is taken to a new direction: for the first time OTA is applied using unlicensed frequency bands (868 MHz, 900 MHz, 2400 MHz) and with low consumption and low data rate transmission using protocols such as 802.15.4 and ZigBee.\n\nMotes are often located in places that are either remote or difficult to access. As an example, Libelium has implemented a smart and easy-to-use OTA programming system for ZigBee WSN devices. This system enables firmware upgrades without the need of physical access, saving time and money if the nodes must be re-programmed.\n\nOn modern mobile devices such as smartphones, an over-the-air update may refer simply to a software update that is distributed over Wi-Fi or mobile broadband using a function built into the operating system, with the \"over-the-air\" aspect referring to its use of wireless internet instead of requiring the user to connect the device to a computer via USB to perform the update.\n\nFirmware updates are available for download from the OTA service.\n\nThe OTA mechanism requires the existing software and hardware of the target device to support the feature, namely the receipt and installation of new software received via the wireless network from the provider.\n\nNew software is transferred to the phone, installed, and put into use. It is often necessary to turn the phone off and back on for the new programming to take effect, though many phones will automatically perform this action.\n\nDepending on implementation, OTA software delivery can be initiated upon action, such as a call to the provider's customer support system or other dialable service, or can be performed automatically. Typically it is done via the former method to avoid service disruption at an inconvenient time, but this requires subscribers to manually call the provider. Often, a carrier will send a broadcast SMS text message to all subscribers (or those using a particular model of phone) asking them to dial a service number to receive a software update.\n\nVerizon Wireless in the U.S. provides a number of OTA functions to its subscribers via the *228 service code. Option 1 updates phone configuration, option 2 updates the PRL. Similarly Voitel Wireless and StraightTalk, which both use Verizon network, use *22890 service code to program Verizon based wireless phones. Interop Technologies provides a number of nationwide wireless operators in the US with an SS7 Based Over-the-Air device management solution. This solution allows operators to manage wireless device functionality including renumbering handsets, updating phone settings, applications and subscriber data and adjusting PRL to manage cost structures.\n\nTo provision parameters in a mobile device OTA, the device needs to have a provisioning client capable of receiving, processing and setting the parameters. For example, a Device Management client in a device may be capable of receiving and provisioning applications, or connectivity parameters.\n\nIn general, the term OTA implies the use of wireless mechanisms to send provisioning data or update packages for firmware or software updates to a mobile device — this is so that the user does not have to go to a store or a service center to have applications provisioned, parameters changed or firmware or software updated. Non-OTA options for a user are a) to go to a store and seek help b) use a PC and a cable to connect to the device and change settings on a device, add software to device, etc.\n\nThere are a number of standards that describe OTA functions. One of the first was the GSM 03.48 series.\nThe ZigBee suite of standards includes the ZigBee Over-the-Air Upgrading Cluster which is part of the ZigBee Smart Energy Profile and provides an interoperable (vendor-independent) way of updating device firmware.\n\nOTA is similar to firmware distribution methods used by other mass-produced consumer electronics, such as cable modems, which use TFTP as a way to remotely receive new programming, thus reducing the amount of time spent by both the owner and the user of the device on maintenance.\n\nOver-the-air provisioning (OTAP) is also available in wireless environments (though it is disabled by default for security reasons). It allows an access point (AP) to discover the IP address of its controller. When enabled, the controller tells the other APs to include additional information in the Radio Resource Management Packets (RRM) that would assist a new access point in learning of the controller. It is sent in plain text however, which would make it vulnerable to sniffing. That's why it is disabled by default.\n\n"}
{"id": "1175262", "url": "https://en.wikipedia.org/wiki?curid=1175262", "title": "Performance indicator", "text": "Performance indicator\n\nA performance indicator or key performance indicator (KPI) is a type of performance measurement. KPIs evaluate the success of an organization or of a particular activity (such as projects, programs, products and other initiatives) in which it engages. \n\nOften success is simply the repeated, periodic achievement of some levels of operational goal (e.g. zero defects, 10/10 customer satisfaction, etc.), and sometimes success is defined in terms of making progress toward strategic goals. Accordingly, choosing the right KPIs relies upon a good understanding of what is important to the organization. What is deemed important often depends on the department measuring the performance – e.g. the KPIs useful to finance will differ from the KPIs assigned to sales. \n\nSince there is a need to understand well what is important, various techniques to assess the present state of the business, and its key activities, are associated with the selection of performance indicators. These assessments often lead to the identification of potential improvements, so performance indicators are routinely associated with 'performance improvement' initiatives. A very common way to choose KPIs is to apply a management framework such as the balanced scorecard.\n\nKey performance indicators define a set of values against which to measure. These raw sets of values, which can be fed to systems that aggregate the data, are called \"indicators\". There are two categories of measurements for KPIs. \n\nAn 'indicator' can only measure what 'has' happened, in the past tense, so the only type of measurement is descriptive or lagging. Any KPI that attempts to measure something in a future state as predictive, diagnostic or prescriptive is no longer an 'indicator' it is a 'prognosticator' - at this point its analytics (possibly based on a KPI).\n\n\"Performance\" focuses on measuring a particular \"element\" of an \"activity\". An activity can have four elements: input, output, control, and mechanism. At a minimum, an activity is required to have at least an input and an output. Something goes into the activity as an \"input\"; the activity transforms the input by making a change to its \"state\"; and the activity produces an \"output\". An activity can also have enabling \"mechanisms\" that are typically separated into \"human\" and \"system\" mechanisms. It can also be constrained in some way by a \"control\". Lastly, its actions can have a temporal construct of \"time\".\n\n\nPerformance indicators differ from business drivers and aims (or goals). A school might consider the failure rate of its students as a key performance indicator which might help the school understand its position in the educational community, whereas a business might consider the percentage of income from returning customers as a potential KPI.\n\nThe key stages in identifying KPIs are:\n\nKey performance indicators (KPIs) are ways to periodically assess the performances of organizations, business units, and their division, departments and employees. Accordingly, KPIs are most commonly defined in a way that is understandable, meaningful, and measurable. They are rarely defined in such a way such that their fulfillment would be hampered by factors seen as non-controllable by the organizations or individuals responsible. Such KPIs are usually ignored by organizations.\n\nKPIs should follow the SMART criteria. This means the measure has a Specific purpose for the business, it is Measurable to really get a value of the KPI, the defined norms have to be Achievable, the improvement of a KPI has to be Relevant to the success of the organization, and finally it must be Time phased, which means the value or outcomes are shown for a predefined and relevant period.\n\nIn order to be evaluated, KPIs are linked to target values, so that the value of the measure can be assessed as meeting expectations or not.\n\nKey performance indicators are the non-financial measures of a company's performance - they do not have a monetary value but they do contribute to the company's profitability. \n\nSome examples are:\n\n\nMany of these customer KPIs are developed and managed with customer relationship management software.\n\nFaster availability of data is a competitive issue for most organizations. For example, businesses which have higher operational/credit risk (involving for example credit cards or wealth management) may want weekly or even daily availability of KPI analysis, facilitated by appropriate IT systems and tools.\n\nOverall equipment effectiveness is a set of broadly accepted non-financial metrics which reflect manufacturing success.\n\nMost professional services firms (for example: management consultancies, systems integration firms, or digital marketing agencies) use three key performance indicators to track the health of their businesses. They typically use professional services automation (PSA) software to keep track of and manage these metrics.\n\n\n\nBusinesses can utilize KPIs to establish and monitor progress toward a variety of goals, including lean manufacturing objectives, minority business enterprise and diversity spending, environmental \"green\" initiatives, cost avoidance programs and low-cost country sourcing targets.\n\nAny business, regardless of size, can better manage supplier performance with the help of KPIs robust capabilities, which include:\n\nMain SCM KPIs will detail the following processes:\n\nSuppliers can implement KPIs to gain an advantage over the competition. Suppliers have instant access to a user-friendly portal for submitting standardized cost savings templates. Suppliers and their customers exchange vital supply chain performance data while gaining visibility to the exact status of cost improvement projects and cost savings documentation.\n\nThe provincial government of Ontario, Canada has been using KPIs since 1998 to measure the performance of higher education institutions in the province. All post secondary schools collect and report performance data in five areas – graduate satisfaction, student satisfaction, employer satisfaction, employment rate, and graduation rate.\n\n\n\nHuman Resource Management\n\n\nIn practice, overseeing key performance indicators can prove expensive or difficult for organizations. Some indicators such as staff morale may be impossible to quantify. As such, dubious KPIs can be adopted that can be used as a rough guide rather than a precise benchmark. \n\nKey performance indicators can also lead to perverse incentives and unintended consequences as a result of employees working to the specific measurements at the expense of the actual quality or value of their work. \n\nSometimes the collecting of statistics can become a substitute for a better understanding of the problems so the use of dubious KPIs can result in progress in aims and measured effectiveness becoming different. For example, US soldiers during the Vietnam War were shown to be effective in kill ratios and high body counts, but this was misleading when used to measure aims as it did not show the lack of progress towards the US goal of increasing South Vietnamese government control of its territory. Another example would be to measure the productivity of a software development team in terms of lines of source code written. This approach can easily result in large amounts of dubious code being added, thereby inflating the line count but adding little of value in terms of systemic improvement. A similar problem arises when a footballer kicks a ball uselessly in a match in order to build up his statistics.\n\n\n"}
{"id": "58952059", "url": "https://en.wikipedia.org/wiki?curid=58952059", "title": "Photonically Optimized Embedded Microprocessors", "text": "Photonically Optimized Embedded Microprocessors\n\nThe Photonically Optimized Embedded Microprocessors ( POEM) is DARPA program. It should demonstrate photonic technologies that can be integrated within embedded microprocessors and enable energy-efficient high-capacity communications between the microprocessor and DRAM. For realizing POEM technology CMOS and DRAM-compatible photonic links should operate at high bit-rates with very low power dissipation.\n\nCurrently research in this field is at University of Colorado, Berkley University, and Nanophotonic Systems Laboratory ( Ultra-Efficient CMOS-Compatible Grating Coupler Design).\n\n"}
{"id": "2367285", "url": "https://en.wikipedia.org/wiki?curid=2367285", "title": "Priest (tool)", "text": "Priest (tool)\n\nA priest (poacher's, game warden's or angler's \"priest\"), is a tool for killing game or fish.\n\nThe name \"priest\" comes from the notion of administering the \"last rites\" to the fish or game. Anglers often use priests to quickly kill fish.\n\nPriests usually come in the form of a heavy metal head attached to a metal or wooden stick. The small baton is a blunt instrument used for quickly killing fish or game. Early versions are made of \"lignum vitae\" (Latin for \"wood of life\"), the densest hardwood. One example is described as \"Lead filled head. Brass ring to handle. With large Head for dispatching Game. Size overall 14 inches long\".\n\nIdentified as a \"poacher's priest\" the tool is a featured murder weapon in Series 12 of the BBC's \"Dalziel and Pascoe\", Episodes 2 and 3, \"Under Dark Stars\", which left a round bruised mark on impact.\n\n"}
{"id": "31870501", "url": "https://en.wikipedia.org/wiki?curid=31870501", "title": "Proceedings of the Institution of Mechanical Engineers, Part A", "text": "Proceedings of the Institution of Mechanical Engineers, Part A\n\nThe Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy is a peer-reviewed scientific journal that covers research on the technology of energy conversion systems. The journal was established in 1990 and is published by Sage Publications on behalf of the Institution of Mechanical Engineers.\n\nThe journal is abstracted and indexed by:\n\nAccording to the \"Journal Citation Reports\", its 2013 impact factor is 0.596, ranking it 91st out of 126 journals in the category \"Engineering, Mechanical\".\n"}
{"id": "1956288", "url": "https://en.wikipedia.org/wiki?curid=1956288", "title": "RF switch", "text": "RF switch\n\nAn RF Switch or Microwave Switch is a device to route high frequency signals through transmission paths. RF (radio frequency) and microwave switches are used extensively in microwave test systems for signal routing between instruments and devices under test (DUT). Incorporating a switch into a switch matrix system enables you to route signals from multiple instruments to single or multiple DUTs. This allows multiple tests to be performed with the same setup, eliminating the need for frequent connects and disconnects. The entire testing process can be automated, increasing the throughput in high-volume production environments.\n\nLike other electrical switches, RF and microwave switches provide different configurations for many different applications. Below is a list of typical switch configurations and usage:\n\n\nThe two main kinds of RF and microwave switches have different capabilities:\n\n\nRF and microwave applications range in frequency from 100 MHz for semiconductor to 60 GHz for satellite communications. Broadband accessories increase test system flexibility by extending frequency coverage. However, frequency is always application dependent and a broad operating frequency may be sacrificed to meet other critical parameters. For example, a network analyzer may perform a 1 ms sweep for an insertion loss measurement, so for this application settling time or switching speed becomes the critical parameter for ensuring measurement accuracy.\n\nIn addition to proper frequency selection, insertion loss is critical to testing. Losses greater than 1 or 2 dB will attenuate peak signal levels and increase rising and falling edge times. A low insertion loss system can be achieved by minimizing the number of connectors and through-paths, or by selecting low insertion loss devices for system configuration. As power is expensive at higher frequencies, electromechanical switches provide the lowest possible loss along the transmission path.\n\nReturn loss is caused by impedance mismatch between circuits. At microwave frequencies, the material properties as well as the dimensions of a network element play a significant role in determining the impedance match or mismatch caused by the distributed effect. Switches with excellent return loss performance ensure optimum power transfer through the switch and the entire network.\n\nLow insertion loss repeatability reduces sources of random errors in the measurement path, which improves measurement accuracy. The repeatability and reliability of a switch guarantees measurement accuracy and can cut the cost of ownership by reducing calibration cycles and increasing test system uptime.\n\nIsolation is the degree of attenuation from an unwanted signal detected at the port of interest. Isolation becomes more important at higher frequencies. High isolation reduces the influence of signals from other channels, sustains the integrity of the measured signal, and reduces system measurement uncertainties. For instance, a switch matrix may need to route a signal to a spectrum analyzer for measurement at –70 dBm and to simultaneously route another signal at +20 dBm. In this case, switches with high isolation, 90 dB or more, will keep the measurement integrity of the low-power signal.\n\nSwitching speed is defined as the time needed to change the state of a switch port (arm) from “ON’ to “OFF” or from “OFF” to “ON”.\n\nAs switching time only specifies an end value of 90% of the settled/final value of the RF signal, settling time is often highlighted in solid state switch performance where the need for accuracy and precision is more critical. Settling time is measured to a level closer to the final value. The widely used margin-to-final value of settling time is 0.01 dB (99.77% of the final value) and 0.05 dB (98.86% of the final value). This specification is commonly used for GaAs FET switches because they have a gate lag effect caused by electrons becoming trapped on the surface of the GaAs.\n\nPower handling defines the ability of a switch to handle power and is very dependent on the design and materials used. There are different power handling ratings for switches such as hot switching, cold switching, average power and peak power. Hot switching occurs when RF/microwave power is present at the ports of the switching at the time of the switching. Cold switching occurs when the signal power is removed before switching. Cold switching results in lower contact stress and longer life.\n\nA 50-ohm load termination is critical in many applications, since each open unused transmission line has the possibility to resonate. This is important when designing a system that works up to 26 GHz or higher frequencies where switch isolation drops considerably. When the switch is connected to an active device, the reflected power of an unterminated path could possibly damage the source.\n\nVideo leakage refers to the spurious signals present at the RF ports of the switch when it is switched without an RF signal present. These signals arise from the waveforms generated by the switch driver and, in particular, from the leading edge voltage spike required for high-speed switching of PIN diodes. The amplitude of the video leakage depends on the design of the switch and the switch driver.\n\nA long operating life reduces cost per cycle and budgetary constraints allowing manufacturers to be more competitive.\n\n"}
{"id": "23402717", "url": "https://en.wikipedia.org/wiki?curid=23402717", "title": "Slotted optical switch", "text": "Slotted optical switch\n\nThe slotted optical switch, sometimes known as opto switch or optical switch but not to be confused with the optical component, is a device comprising a photoemitter (e.g. LED) and a photodetector (e.g. photodiode) mounted in a single package so that the photoemitter normally illuminates the photodetector, but an opaque object can be inserted in a slot between them so as to break the beam. Associated circuitry is provided which changes state when the beam is interrupted. For example, the carriage of a computer printer may be fitted with a projection which interrupts the beam of a slotted switch when it reaches the end of its travel, causing circuitry to react appropriately. Another application of the slotted switch is in the type of computer mouse with a rotating ball. The ball measures distances moved by rotating orthogonal shafts which drive optical chopper wheels turning in the slots of slotted switches.\n\nThis device uses the same basic components as an opto-coupler, but is operated by manipulating the light path instead of the photoemitter input.\n\nIllustrations and data on slotted optical switches are to be found in catalogues and manufacturers' data sheets.\n"}
{"id": "5767802", "url": "https://en.wikipedia.org/wiki?curid=5767802", "title": "Spatial power combiner", "text": "Spatial power combiner\n\nA spatial power combiner generally refers to a microwave system in which the output power of several solid state circuits are combined in free space as opposed to in a lossy substrate. Many spatial power combiners use concepts from free-space optics in which dielectric lenses are used to focus a microwave beam into and out of a solid-state circuit array. For this reason, this field of research is also known as quasioptics.\n"}
{"id": "26976624", "url": "https://en.wikipedia.org/wiki?curid=26976624", "title": "Spread-spectrum time-domain reflectometry", "text": "Spread-spectrum time-domain reflectometry\n\nSpread-spectrum time-domain reflectometry (SSTDR) is a measurement technique to identify faults, usually in electrical wires, by observing reflected spread spectrum signals. This type of time-domain reflectometry can be used in various high-noise and live environments. Additionally, SSTDR systems have the additional benefit of being able to precisely locate the position of the fault. Specifically, SSTDR is accurate to within a few centimeters for wires carrying 400 Hz aircraft signals as well as MIL-STD-1553 data bus signals. AN SSTDR system can be run on a live wire because the spread spectrum signals can be isolated from the system noise and activity.\n\nAt the most basic level, the system works by sending spread spectrum signals down a wireline and waiting for those signals to be reflected back to the SSTDR system. The reflected signal is then correlated with a copy of the sent signal. Mathematical algorithms are applied to both the shape and timing of the signals to locate either the short or the end of an open circuit.\n\nSpread-spectrum time domain reflectometry is used in detecting intermittent faults in live wires. From buildings and homes to aircraft and naval ships, this technology can discover irregular shorts on live wire running 400 Hz, 115 V. For accurate location of a wiring system's fault the SSTDR associates the PN code with the signal on the line then stores the exact location of the correlation before the arc dissipates. Present SSTDR can collect a complete data set in under 5 ms. \n\nSSTDR technology allows for analysis of a network of wires. One SSTDR sensor can measure up to 4 junctions in a branched wire system.\n\n\n"}
{"id": "57555759", "url": "https://en.wikipedia.org/wiki?curid=57555759", "title": "Star Division", "text": "Star Division\n\nThe German software company Star Division (also spelled Star-Division) was founded in 1985 by the 16-year-old Marco Börries in Lüneburg as a garage company. After a neighbour denunciated the operation of a business in a residential area to the Ordnungsamt, the company moved to Hamburg.\n\nAs its first product the company distributed StarWriter, a word processor application developed by friends.\n\nPositioned as a cheaper alternative to Microsoft's office suite, StarWriter later became StarOffice, for which the company became famous. The software was sold over 25 million times worldwide, and at its peak it had a market share among other office suites of about 25% in Germany.\n\nCaldera, Inc. supported the Linux-port of StarOffice 3.1 with ca. 800.000 DM in order to offer the product with their forthcoming OpenLinux distribution in 1997.\n\nIn 1998 Börries released StarOffice free of charge for private use.\n\nStar Division was acquired by the software and hardware vendor Sun Microsystems on 5 August 1999 for a higher double-digit million amount in US dollars. Sun reintroduced the software as StarOffice 5.1a, and for the first time also free of charge for commercial use.\n\nThe StarOffice suite has since been further developed by Sun Microsystems and the OpenOffice and LibreOffice communities. \n\nSun Microsystems became a wholly owned subsidiary of Oracle Corporation on 27 January 2010.\n\n"}
{"id": "54605762", "url": "https://en.wikipedia.org/wiki?curid=54605762", "title": "Strate School of Design", "text": "Strate School of Design\n\nStrate School of Design (formally known as Strate College) is a French private institution for technical education founded in 1993. Its main campus is in Sèvres south-west of Paris. It is dedicated to the teaching of industrial design, 3D modeling and design thinking. The school is recognized by the French state and its design degree is certified by the French Ministry of Higher Education.\n\n1993\n\nJean-René Talopp, former designer and director of the \"ESDI\" design school, founds the « Strate Collège ».\n\n2010\n\nThe school moves to a new campus in Sèvres.\n\n2013\n\nDominique Sciamma becomes the director of Strate.\n\n2014\n\nThe school changes its name to « Strate School of Design », its logo and adopts a new motto: «Making the world +simple, +fair, +beautiful».\n\n2015\n\nThe Strate Executive Education programme is launched to train professionals and managerial staff to Design Thinking and innovation by design.\n\nStrate is recognized by the French state in the publication of the \"\"Bulletin Officiel de l'Enseignement Supérieur et de la Recherche\" on 7 December 2015. This has entitled the school to welcome government scholars for the design and modeling courses since 2017.\n\n2017\n\nThe Ministry’s of Higher Education \"Bulletin Officiel de l'Enseignement Supérieur et de la Recherche\"\" lists the school as one of the few design schools in France to deliver a diploma recognized by the Minister in charge of higher education.\n\nSome programs last five years (bac+5), others three (MBA). The two main subjects in the curriculum are design and modeling.\n\nThe first two years of the Design course are about teaching the fundamentals of drawing, perspective, modeling and design methodology.\n\nIn the third year the students specialize in one major: product, transportation, packaging & retail, or interaction. A first six-month internship takes place in the third year.\n\nIn year four, the students are requested to spend the first semester abroad either at a partner design school of the CUMULUS association, or by doing an internship. The students then come back to school for a period of projects with partner companies.\n\nThe fifth year is dedicated to the preparation of the diploma. Students are requested to write a memoir and formalize their project, which they present to a jury of professionals (independent designers, designers in agencies, office managers, marketing managers). After this presentation students are requested to do a final internship in France or abroad.\n\nThe 3D modeling course is a 3-year undergraduate diploma which teaches traditional and digital modeling.\n\nStrate delivers the diploma of «industrial designer », which is recognized by the government as a level I title for the 5 years of studies after the French baccalaureate (niveau I RNCP code 200n, JO du 06-07-08). Since the publication of the Higher Education Ministry \"Bulletin Officiel de l'Enseignement Supérieur et de la Recherche\" on June the 29th 2017 the school is one of the few design schools in France to deliver a degree referred to by the Minister of higher education.\n\nFor the design course two double diploma agreements exist with Grenoble School of Management and Sciences Po Paris.\n\nTwo double degree designer-manager options are offered to students by doing an additional tuition-free year at Grenoble School of Management or Sciences Po Paris.\n\nThe master in Design in Transportation is a 2-year post-graduate master's degree. It is recognized by the French State through its registration by the National Council of Professional Certification (RNCP) at Level 1.\n\nThe master in design for smart cities is a 2-year post-graduate master's degree. The course can take place in Paris and Singapore. It is recognized by the French State through its registration by the National Council of Professional Certification (RNCP) at Level 1.\n\nFor the design course student exchanges are made with other schools of the international association of art & design universities CUMULUS.\n\nTwo double degree agreements exist with Grenoble School of Management and Sciences Po Paris.\n\nThe school has developed the collaborative project \"CPi\" (Conception de produit innovant) with engineering school Centrale Supélec and ESSEC Business School. For 9 months mixed teams of students of the three schools work on innovation challenges submitted by partner companies. Since 2005 over fifty compagnies have supported the program producing 150 students projects.\n\nSince 2014 the \"Design & Science Université Paris-Saclay\" prize has been led by Strate. It was previously called \"ArtScience Prize\" from 2014 to 2016. It gathers students of Strate with students from prestigious French engineering schools such as Télécom ParisTech, CentraleSupélec, ENS Paris-Saclay or École Polytechnique to work on a same project. It aims to develop innovative ideas upon advanced scientific themes. Every year about 40 students work together on a same brief during six months and are coached by professors from their schools, the winners of the prize get a scholarship for the development of their project and other help.\n\nStrate is one of the founding schools of the \"\"Web School Factory\" school in Paris.\n\nThe school has signed various agreements with compagnies to train the employees through the \"Strate Executive Education\" branch and companies to offer innovation, research projects and internship to students:\n\nThe school has a research branch \"Strate Research\" and is part of various French institutes:\nIn June 2017 the laboratory \"EXALT Design Lab\" for design valorization in businesses is created at Strate in cooperation with 5 big companies and two major academic laboratories. All the partners are engaged for a 4 year research project.\n\nThe French student magazine \"l'Étudiant\" published some years a ranking of the \"product design schools preferred by professionals\" (\"écoles de design produit préférées des pros\"). There are about 77 design schools in France and Strate is regularly mentioned as one of the best.\n\nIn 2017 the student news website \"l'Étudiant\" said Strate School of Design distinguished itself for its dynamism, following closely the mutations of the designer's profession. And also by creating partnerships with foreign schools and collaboration with various research actors.\n\nThe school is well known in the car design world since its transportation design course exists since its schools opening in 1993. The website \"Car Design News\" by adding up all the successive results for the years 2011 to 2015 of student participations in the \"Car Design Awards\" competition ranked the school as one of the top transportation design school in the world.\n\nThe association \"Strate Alumni\" has over 1200 members.\n\nStrate School of design is a member of the international association CUMULUS. For the design course student exchanges are made with other schools of the international association of art & design universities CUMULUS.\n\nThe school is also a member of the World Design Organization, an international non-governmental organization that promotes the profession of industrial design and its ability to generate better products, systems, services, and experiences.\n\nStrate School of Design established in Singapore in 2017, and received in 2018 the registration from the Committee For Private Education / SkillsFuture (part of Ministry of Education). Strate also signed a partnership with « Social Innovation Park », a not for profit organization aiming at Educate, Empower and Enhance social entrepreneurs and innovators. The campus of Strate is located in the epicenter of Design for Singapore, namely the National Design Centre. \nStrate Singapore offers a suite of Design curricula of which the Master in Design for Smart Cities, in both full-time and part-time formats. .\n\nStrate School of Design opened in 2018 a large campus in Bangalore, India, together with other institutions from the Studialis group.\n\nThe school is a member of the Studialis group that belongs to the investment fund GALILEO.\n"}
{"id": "19763678", "url": "https://en.wikipedia.org/wiki?curid=19763678", "title": "T-splice", "text": "T-splice\n\nIn electrical wiring, a T-splice is a splice that is used for connecting the end of one wire to the middle of another wire, thus forming a shape like that of the letter \"T.\" This splice can be used with solid or stranded wires. The existing wire is called the main wire. The new wire that connects to the main wire is called the branch wire or tap wire. This is a prevalent junction type used in knob and tube wiring.\n"}
{"id": "41620348", "url": "https://en.wikipedia.org/wiki?curid=41620348", "title": "Thermal mass refrigerator", "text": "Thermal mass refrigerator\n\nA thermal mass refrigerator is a refrigerator that is foreseen with thermal mass as well as insulation to decrease the energy use of the refrigerator. A particularly popular thermal mass refrigerator was conceived by Michael Reynolds and detailed in the book \"Earthship Volume 3\". This refrigerator was a DIY refrigerator designed around a (Sun-Frost) DC refrigeration unit run on P.V. panels.\n\nThe thermal mass used in Michael Reynolds' design is a combination of a liquid (i.e. water or beer) together with concrete mass. Concrete's temperature can be decreased quickly, while a liquid's (such as beer or water) with its higher thermal mass requires more energy to change temperature, holding the cold for longer. In Michael Reynolds' design, the liquid is added in the form of beer cans, placed in the back of the refrigerator.\n\nBesides the use of a large quantity of thermal mass, he also made sure that the inside of the box could be in direct contact with the outside air, so as to allow the unit to be cooled without electricity/compressor-assistance during winter. This is done by having the top of the unit openable by placing a skylight on top. A pipe connects the bottom to the outside as well, so as to allow natural circulation of air. The inflow of outside air from the top could be closed off by closing the skylight as well as by closing the removable insulated damper.\n"}
{"id": "27944981", "url": "https://en.wikipedia.org/wiki?curid=27944981", "title": "Timeline of United States inventions (before 1890)", "text": "Timeline of United States inventions (before 1890)\n\nA timeline of United States inventions (before 1890) encompasses the ingenuity and innovative advancements of the United States within a historical context, dating from the Colonial Period to the Gilded Age, which have been achieved by inventors who are either native-born or naturalized citizens of the United States. Copyright protection secures a person's right to his or her first-to-invent claim of the \"original\" invention in question, highlighted in Article I, Section 8, Clause 8 of the United States Constitution, which gives the following enumerated power to the United States Congress:\nIn 1641, the first patent in North America was issued to Samuel Winslow by the General Court of Massachusetts for a new method of making salt. On April 10, 1790, President George Washington signed the Patent Act of 1790 (1 Stat. 109) into law proclaiming that patents were to be authorized for \"any useful art, manufacture, engine, machine, or device, or any improvement therein not before known or used\". On July 31, 1790, Samuel Hopkins of Pittsford, Vermont became the first person in the United States to file and to be granted a patent for an improved method of \"Making Pot and Pearl Ashes\". The Patent Act of 1836 (Ch. 357, 5 Stat. 117) further clarified United States patent law to the extent of establishing a patent office where patent applications are filed, processed, and granted, contingent upon the language and scope of the claimant's invention, for a patent term of 14 years with an extension of up to an additional 7 years. However, the Uruguay Round Agreements Act of 1994 (URAA) changed the patent term in the United States to a total of 20 years, effective for patent applications filed on or after June 8, 1995, thus bringing United States patent law further into conformity with international patent law. The modern-day provisions of the law applied to inventions are laid out in Title 35 of the United States Code (Ch. 950, sec. 1, 66 Stat. 792).\n\nFrom 1836 to 2011, the United States Patent and Trademark Office (USPTO) has granted a total of 7,861,317 patents relating to several well-known inventions appearing throughout the timeline below.\n\n\n\n\n\n\n1749 Lightning rod\n\n1752 Flexible urinary catheter\n\n1761 Armonica\n\n1776 Swivel chair\n\n1782 Flatboat\n\n1784 Bifocals\n\n\n1785 Artificial diffraction grating\n\n\n1787 Automatic flour mill\n\n\n1792 Cracker\n\n1793 Cotton gin\n\n1795 Wheel cypher\n\n\n1796 Rumford fireplace\n\n\n1796 Cupcake\n\n\n1801 Suspension bridge\n\n\n1801 Fire hydrant\n\n1802 Banjo clock\n\n\n1804 Burr Truss\n\n\n1805 Amphibious vehicle\n\n\n1805 Vapor-compression refrigeration\n\n\n1806 Coffee percolator\n\n\n1808 Lobster trap\n\n1812 Columbiad\n\n\n1813 Circular saw\n\n\n1815 Dental floss\n\n1816 Milling machine\n\n1818 Profile lathe\n\n1827 Detachable collar\n\n1829 Graham cracker\n\n\n1830 Platform scale\n\n\n1831 Flanged T rail\n\n1831 Multiple coil magnet\n\n1831 Doorbell (electric)\n\n1833 Sewing machine (lock-stitch)\n\n\n1834 Combine harvester\n\n\n1835 Steam shovel\n\n\n1835 Solar compass\n\n\n1835 Relay\n\n\n1836 Morse code\n\n\n1836 Gridiron (cooking)\n\n1836 Circuit breaker\n\n\n1837 Self-polishing cast steel plow\n\n\n1839 Corn sheller\n\n\n1839 Sleeping car\n\n1839 Vulcanized rubber\n\n1839 Babbitt (metal)\n\n1840 Howe truss\n\n1842 Inhalational anaesthetic\n\n1842 Grain elevator\n\n1843 Ice cream maker (hand-cranked)\n\n\n1843 Multiple-effect evaporator\n\n\n1843 Rotary printing press\n\n\n1844 Pratt truss\n\n\n1845 Pressure-sensitive tape\n\n1845 Maynard tape primer\n\n1845 Baseball\n\n\n1846 Transverse shuttle\n\n1846 Printing telegraph\n\n1847 Gas mask\n\n\n1847 Doughnut (ring-shaped)\n\n\n1848 Pin tumbler lock\n\n1849 Jackhammer\n\n\n1849 Safety pin\n\n\n1850 Dishwasher\n\n\n1850 Feed dogs\n\n1850 Vibrating shuttle\n\n\n1850 Inverted microscope\n\n\n1851 Rotary hook\n\n\n1851 Fire alarm box\n\n1852 Elevator brake\n\n1853 Burglar alarm\n\n1853 Potato chips\n\n1853 spring Clothespin\n\n\n1854 Breast pump\n\n\n1855 Calliope\n\n\n1856 Egg beater\n\n\n1856 Condensed milk\n\n\n1856 Equatorial sextant\n\n\n1857 Toilet paper (mass-produced and rolled)\n\n1857 Pink lemonade\n\n1857 Brown Truss\n\n1858 screw top Pepper shaker\n\n\n1858 Mason jar\n\n\n1858 Pencil eraser\n\n\n1858 Ironing board\n\n\n1858 Twine knotter\n\n\n1858 Dustpan\n\n1859 Electric stove\n\n1859 Escalator\n\n\n1860 Vacuum cleaner\n\n\n1860 Repeating rifle (lever action)\n\n1861 Jelly bean\n\n\n1861 Twist drill\n\n\n1861 Kinematoscope\n\n1861 Postcard\n\n1861 Machine gun (hand-cranked)\n\n1863 Breakfast cereal\n\n\n1863 Ratchet wrench\n\n\n1863 Quad skates\n\n\n1863 Double-barreled cannon\n\n\n1864 Spar torpedo\n\n\n1865 Cowboy hat\n\n\n1865 Rotary printing press (web)\n\n1866 Urinal (restroom version)\n\n1866 Chuckwagon\n\n1867 Motorcycle (steam-powered)\n\n\n1867 Paper clip\n\n\n1867 Barbed wire\n\n1867 Ticker tape\n\n1867 Water-tube boiler\n\n1867 Refrigerator car\n\n1868 Paper bag\n\n\n1868 Tape measure\n\n\n1869 Vibrator\n\n\n1869 American football\n\n\n1869 Pipe wrench\n\n1869 Clothes hanger\n\n1870 Bee smoker\n\n\n1870 Can opener (rotary)\n\n\n1870 Sandblasting\n\n\n1870 Feather duster\n\n1871 Rowing machine\n\n1872 Railway air brake\n\n\n1872 Diner\n\n\n1873 Earmuffs\n\n\n1873 Silo\n\n\n1873 Jeans\n\nJeans are trousers generally made from denim. Jeans became popular among teenagers starting in the 1950s which remains as a distinct icon of American fashion. In 1873, Levi Strauss and Jacob Davis co-invented and co-patented the idea of using copper rivets at the stress points of sturdy work pants. After one of Davis' customers kept purchasing cloth to reinforce torn pants, he had an idea to use copper rivets to reinforce the points of strain, such as on the pocket corners and at the top of the button fly. Davis did not have the required money to purchase a patent, so he wrote to Strauss suggesting that they both go into business together. Early Levis, called \"waist overalls\", came in a brown canvas duck fabric and a heavy blue denim fabric. His business became extremely successful, revolutionizing the apparel industry.\n\n1873 Knuckle coupler\n\nAlso known as a Janney coupler and the buckeye coupler, the knuckle coupler is the derivative of a coupling device that links and connects rolling railway cars such as passenger, refrigerator, freight, and stock cars together on railroad track. The knuckle coupler have a bifurcated drawhead and a revolving hook, which, when brought in contact with another coupler, automatically interlocks with its mate. Knuckle couplers replaced the much more dangerous link-and-pin couplers and became the basis for standard coupler design for the rest of the 19th century. The knuckle coupler was invented and patented by Eli H. Janney in 1873.\n\n1874 Fire sprinkler (automated)\n\nA fire sprinkler is the part of a fire sprinkler system that discharges water when the effects of a fire have been detected, such as when a pre-determined temperature has been reached. Henry S. Parmelee of New Haven, Connecticut invented and installed the first closed-head or automated fire sprinkler in 1874.\n\n1874 Spork\n\nA spork or a foon is a hybrid form of cutlery taking the form of a spoon-like shallow scoop with three or four fork tines. The spork is a portmanteau word combining \"spoon\" and \"fork\". The spork was invented in 1874 by Samuel W. Francis. U.S. patent #147,119 was filed on January 22, 1874, and issued to Francis on February 3, 1874.\n\n1874 Ice cream soda\n\n1874 Quadruplex telegraph\n\n1874 Jockstrap\n\nA jockstrap, also known as a jock, jock strap, strap, supporter, or athletic supporter, is an undergarment designed for supporting the male genitalia during sports or other vigorous physical activity. A jockstrap consists of a waistband (usually elastic) with a support pouch for the genitalia and two elastic straps affixed to the base of the pouch and to the left and right sides of the waistband at the hip. The jockstrap has been part of men's undergarments since 1874 when it was invented by C.F. Bennett of Chicago to protect and support bicycle riders (back then they were known as \"jockeys\") who were navigating the cobblestone streets common to the era.\n\n1874 Forstner bit\n\nForstner bits, also known as Forstner flange bits or webfoot augers, bore precise, flat-bottomed holes in wood, in any orientation with respect to the wood grain. Forstner bits can cut on the edge of a block of wood, and can cut overlapping holes. Because of the flat bottom to the hole, they are useful for drilling through veneer already glued to add an inlay. Forstner bits were invented and patented by Benjamin Forstner in 1874.\n\n1874 QWERTY\n\nQWERTY is the most used modern-day keyboard layout on English-language computer and typewriter keyboards. It takes its name from the first six characters seen in the far left of the keyboard's top row of letters. The QWERTY design was invented and patented by Christopher Sholes in 1874.\n\n1875 Biscuit cutter\n\n1875 Dental drill (electric)\n\n1875 Mimeograph\n\n1876 Synthesizer\n\n1876 Airbrush\n\n1876 Tattoo machine\n\n1877 Phonograph\n\nThe phonograph, record player or gramophone is an instrument for recording, reproducing and playing back sounds. The earliest phonographs used cylinders containing an audio recording engraved on the outside surface which could be reproduced when the cylinder was played. Later, the gramophone record with modulated spiral grooves set atop a rotating turntable. The phonograph was invented in 1877 by Thomas Alva Edison at his laboratory in Menlo Park, New Jersey. On February 8, 1878, Edison was issued the first patent (U.S. patent #200,521) for the phonograph.\n\n1877 District heating\n\nDistrict heating distributes heat generated in a centralized location for residential and commercial heating requirements. The heat is often obtained from a cogeneration plant burning fossil fuels but increasingly biomass, although heat-only boiler stations, geothermal heating and central solar heating are also used, as well as nuclear power. A system was built in France in the 14th Century and the United States Naval Academy in Annapolis, Maryland began steam district heating service in 1853. However, the first commercially successful district heating system was launched in Lockport, New York, in 1877 by American hydraulic engineer Birdsill Holly, considered the founder of modern district heating.\n\n1878 Carbon microphone\n\n1878 Free jet water turbine\n\n1878 Bolometer\n\n1879 mechanical production of Photographic plate\n\n1879 Carton\n\n1879 Cash register\n\nThe cash register is a device for calculating and recording sales transactions. When a transaction was completed, the first cash registers used a bell that rang and the amount was noted on a large dial on the front of the machine. During each sale, a paper tape was punched with holes so that the merchant could keep track of sales. Known as the \"Incorruptible Cashier\", the mechanical cash register was invented and patented in 1879 by James Ritty of Dayton, Ohio. John H. Patterson bought Ritty's patent and his cash register company in 1884.\n\n1880 Oil burner\n\nAn oil burner is a heating device which burns fuel oil. The oil is directed under pressure through a nozzle to produce a fine spray, which is usually ignited by an electric spark with the air being forced through by an electric fan. In 1880, Amanda Jones invented the oil burner in the oil fields of northern Pennsylvania where Jones completed her trial and error efforts of heating furnaces.\n\n1880 Candlepin bowling\n\nCandlepin bowling is a North American variation of bowling that is played primarily in the Canadian Maritime provinces, Quebec, Maine, Massachusetts, and New Hampshire. A candlepin bowling lane somewhat resembles lanes used in tenpin bowling. However, unlike tenpin bowling lanes that are flat, candlepin lanes are slightly depressed ahead of the pindeck. The candlepins themselves take on a cylindrical shape which are tapered at the tops and bottoms, thus giving them a resemblance to wax candles. In 1880, candlepin bowling was invented by Justin White of Worcester, Massachusetts.\n\n1881 Electric chair\n\n1881 Metal detector\n\n1881 Iron (electric)\n\n\n1882 Fan (electric)\n\n1883 Salt water taffy\n\n1883 Solar cell\n\nA solar cell is any device that directly converts the energy in light into electrical energy through the process of photovoltaics. Although French physicist Antoine-César Becquerel discovered the photovoltaic effect much earlier in 1839, the first solar cell, according to Encyclopædia Britannica, was invented by Charles Fritts in 1883, who used junctions formed by coating selenium with an extremely thin layer of gold. In 1941, the silicon solar cell was invented by another American named Russell Ohl. Drawing upon Ohl's work, three American researchers named Gerald Pearson, Calvin Fuller, and Daryl Chapin essentially introduced the first practical use of solar panels through their improvement of the silicone solar cell in 1954, which by placing them in direct sunlight, free electrons are turned into electric current enabling a six percent energy conversion efficiency.\n\n1883 Thermostat\n\nA thermostat is a device for regulating the temperature of a system so that the system's temperature is maintained near a desired setpoint temperature. The thermostat does this by switching heating or cooling devices on or off, or regulating the flow of a heat transfer fluid as needed, to maintain the correct temperature. The thermostat was invented in 1883 by Warren S. Johnson.\n\n1884 Machine gun\n\nThe machine gun is defined as a \"fully automatic\" firearm, usually designed to fire rifle cartridges in quick succession from an ammunition belt or large-capacity magazine. The world's first true machine gun, the Maxim gun, was invented in 1884 by the American inventor Hiram Stevens Maxim, who devised a recoil power of the previously fired bullet to reload rather than the crude method of a manually operated, hand-cranked firearm. With the ability to fire 750 rounds per minute, Maxim's other great innovation was the use of water cooling to reduce overheating. Maxim's gun was widely adopted and derivative designs were used on all sides during World War I.\n\n1884 Dissolvable pill\n\nA dissolvable pill is any pharmaceutical in tablet form that is ingested orally, which are crushable and able to dissolve in the stomach unlike tablets with hard coatings. The dissolvable pill was invented in 1884 by William E. Upjohn.\n\n1884 Skyscraper\n\nA skyscraper is a tall building that uses a steel-frame construction. After the Great Fire of 1871, Chicago had become a magnet for daring experiments in architecture as one of those was the birth of the skyscraper. The edifice known as the world's first skyscraper was the 10-story Home Insurance Company Building built in 1884. It was designed by the Massachusetts-born architect William Le Baron Jenney.\n\n1885 Popcorn machine\n\n1885 Photographic film\n\n1885 Mixer (cooking)\n\n1885 Fuel dispenser\n\nA fuel dispenser is used to pump gasoline, diesel, or other types of fuel into vehicles or containers. As the automobile was not invented yet, the gas pump was used for kerosene lamps and stoves. Sylvanus F. Bowser of Fort Wayne, Indiana invented the gasoline/petrol pump on September 5, 1885. Coincidentally, the term \"bowser\" is still often used in countries such as New Zealand and Australia as a reference to the fuel dispenser.\n\n1886 Filing cabinet (horizontal)\n\nA filing cabinet is a piece of office furniture used to store paper documents in file folders. It is an enclosure for drawers in which items are stored. On November 2, 1886, Henry Brown patented his invention of a \"receptacle for storing and preserving papers\". This was a fire- and accident-safe container made of forged metal, which could be sealed with a lock and key. It was special in that it kept the papers separated.\n\n1886 Telephone directory\n\nA telephone directory is a listing of telephone subscribers in a geographical area or subscribers to services provided by the organization that publishes the directory. R. H. Donnelley created the first official telephone directory which was referred to as the Yellow Pages in 1886.\n\n1887 Screen door\n\nA screen door can refer to a hinged storm door (cold climates) or hinged screen door (warm climates) covering an exterior door; or a screened sliding door used with sliding glass doors. In any case, the screen door incorporates screen mesh to block flying insects from entering and pets and small children from exiting interior spaces, while allowing for air, light, and views. The screen door was invented in 1887 by Hannah Harger.\n\n1887 Gramophone record\nA gramophone record, commonly known as a record, or a vinyl record, is an analog sound storage medium consisting of a flat disc with an inscribed, modulated spiral groove. The groove usually starts near the periphery and ends near the center of the disc. Ever since Thomas Edison invented the phonograph in 1877, it produced distorted sound because of gravity's pressure on the playing stylus. In response, Emile Berliner invented a new medium for recording and listening to sound in 1887 in the form of a horizontal disc, originally known as the \"platter\".\n\n1887 Slot machine\n\nA slot machine is a casino gambling machine. Due to the vast number of possible wins with the original poker card based game, it proved practically impossible to come up with a way to make a machine capable of making an automatic pay-out for all possible winning combinations. The first \"one-armed bandit\" was invented in 1887 by Charles Fey of San Francisco, California who devised a simple automatic mechanism with three spinning reels containing a total of five symbols – horseshoes, diamonds, spades, hearts and a Liberty Bell, which also gave the machine its name.\n\n1887 Softball\n\nAs a bat-and-ball team sport, softball is a variant of baseball. The difference between the two sports is that softball uses larger balls and requires a smaller playing field. Beginning as an indoor game in Chicago, softball was invented in 1887 by George Hancock.\n\n1887 Comptometer\n\n1888 Induction motor\n\nAn induction motor is an AC electric motor in which the electric current in the rotor needed to produce torque is induced by electromagnetic induction from the magnetic field of the stator winding instead of using mechanical commutation (brushes) that caused sparking in earlier electric motors. They are also self-starting. The Serbian-American inventor Nikola Tesla explored the idea of using a rotating magnetic induction field principle, using it in his invention of a poly-phase induction motor using alternating current which he received a patent for on May 1, 1888. The rights to Tesla's invention were licensed by George Westinghouse for the AC power system his company was developing.\n\nThe induction motor Tesla patented in the U.S. is considered to have been an independent invention since the Europe Italian physicist Galileo Ferraris published a paper on a rotating magnetic field based induction motor on 11 March 1888, almost two months before Tesla was granted his patent. A working model of the Ferraris inductionmotor may have been demonstrated at the University of Turin as early as 1885.\n\n1888 Kinetoscope\n\nThe Kinetoscope was an early motion picture exhibition device. It was designed for films to be viewed individually through the window of a cabinet housing its components. The Kinetoscope introduced the basic approach that would become the standard for all cinematic projection before the advent of video, creating the illusion of movement by conveying a strip of perforated film bearing sequential images over a light source with a high-speed shutter. First described in conceptual terms by Thomas Alva Edison in 1888, his invention was largely developed by one of his assistants, William Kennedy Laurie Dickson, between 1889 and 1892.\n\n1888 Trolley pole\n\nA trolley pole is a tapered cylindrical pole of wood or metal placed in contact with an overhead wire to provide electricity to the trolley car. The trolley pole sits atop a sprung base on the roof of the trolley vehicle, the springs maintaining the tension to keep the trolley wheel or shoe in contact with the wire. Occasionally, a Canadian named John Joseph Wright is credited with inventing the trolley pole when an experimental tramway in Toronto, Ontario, was built in 1883. While Wright may have assisted in the installation of railways at the Canadian National Exhibition (CNE), and may even have used a pole system, there is no hard evidence to prove it. Likewise, Wright never filed or was issued a patent. Official credit for the invention of the electric trolley pole has gone to an American, Frank J. Sprague, who devised his working system in Richmond, Virginia, in 1888. Known as the Richmond Union Passenger Railway, this 12-mile system was the first large-scale trolley line in the world, opening to great fanfare on February 12, 1888.\n\n1888 Drinking straw\n\nThe drinking straw is a tube used for transferring a liquid to the mouth, usually a drink from one location to another. The first crude forms of drinking straws were made of dry, hollow, rye grass. Marvin Stone is the inventor of the drinking straw. Stone, who worked in a factory that made paper cigarette holders, did not like this design because it made beverages taste like grass. As an alternative, on January 3, 1888, Stone got a piece of paper from his factory and wrapped it around a pencil. By coating it with wax, his drinking straw became leak-proof so that it would not get waterlogged.\n\n1888 Stepping switch\n\nIn electrical controls, a stepping switch, also known as a stepping relay, is an electromechanical device which allows an input connection to be connected to one of a number of possible output connections, under the control of a series of electrical pulses. The major use for these devices was in early automatic telephone exchanges to route telephone calls. It can step on one axis (called a uniselector), or on two axes (a Strowger switch). As the first automated telephone switch using electromagnets and hat pins, stepping switches were invented by Almon Brown Strowger in 1888. Strowger filed his patent application on March 12, 1889, and it was issued on March 10, 1891.\n\n1888 Revolving door\n\nA revolving door has three or four doors that hang on a center shaft and rotate around a vertical axis within a round enclosure. In high-rise buildings, regular doors are hard to open because of air pressure differentials. In order to address this problem, the revolving door was invented in 1888 by Theophilus Van Kannel of Philadelphia, Pennsylvania. Van Kannel patented the revolving door on August 7, 1888.\n\n1888 Ballpoint pen\n\nA ballpoint pen is a writing instrument with an internal ink reservoir and a sphere for a point. The internal chamber is filled with a viscous ink that is dispensed at its tip during use by the rolling action of a small sphere. The first ballpoint pen is the creation of American leather tanner John Loud of Weymouth, Massachusetts in 1888 which contained a reservoir for ink and a roller ball to mark up his leather hides. Despite Loud being the inventor of the ballpoint pen, it wasn't a practical success since the ink often leaked or clogged up. Loud took out a patent (British patent #15630) in the United Kingdom on October 30, 1888. However, it wasn't until 1935 when Hungarian newspaper editor László Bíró offered an improved version of the ballpoint pen that left paper smudge-free.\n\n1888 Telautograph\n\nThe telautograph, an analog precursor to the modern fax machine, transmits electrical impulses recorded by potentiometers at the sending station to stepping motors attached to a pen at the receiving station, thus reproducing at the receiving station a drawing or signature made by sender. It was the first such device to transmit drawings to a stationary sheet of paper. The telautograph's invention is attributed to Elisha Gray, who patented it in 1888.\n\n1888 Touch typing\n\n1888 Salisbury steak\n\n1889 Flexible flyer\n\nA flexible flyer or steel runner sled is a steerable wooden sled with thin metal runners whereby a rider may sit upright on the sled or lie on their stomach, allowing the possibility to descend a snowy slope feet-first or head-first. To steer the sled, the rider may either push on the wooden cross piece with their hands or feet, or pull on the rope attached to the wooden cross-piece. The flexible flyer was invented in 1889 by Philadelphia resident Samuel Leeds Allen. U.S. patent #408,681 was issued to Allen on August 13, 1889.\n\n1889 Payphone\n\nA payphone or pay phone is a public telephone, usually located in a stand-alone upright container such as a phone booth, with payment done by inserting money (usually coins), a credit or debit card, or a telephone card before the call is made. Pay telephone stations preceded the invention of the pay phone and existed as early as 1878. These stations were supervised by telephone company attendants or agents who collected the money due after people made their calls. In 1889, the first coin-operated telephone was installed by inventor William Gray at a bank in Hartford, Connecticut. However, it was a \"postpay\" machine that only accepted coins deposited after the call was placed.\n\nTimelines of United States inventions\n\nRelated topics\n\n"}
{"id": "24088245", "url": "https://en.wikipedia.org/wiki?curid=24088245", "title": "Waltham Aircraft Clock Corporation", "text": "Waltham Aircraft Clock Corporation\n\nWaltham Aircraft Clock Corporation is a company specializing in the manufacturing and maintenance of mechanical aircraft clocks, established in 1994 in Ozark, Alabama.\n\nThe Waltham Aircraft Clock Corporation's history originates 1854 on the banks of the Charles River, in Waltham, Massachusetts, with the Waltham Watch Company.\n\nIn 1957, at the demise of the watch manufacturing division, the clock division was separated, firstly under the name Waltham Clock Company, and at a later date, Waltham Precision Instruments Company.\n\nThe manufacturing of aircraft clocks continued in the original Waltham Building until February 1994, when Prime Time Clocks Co., purchased the remaining manufacturing line and incorporated it in the state of Alabama under its current name.\n\nPrime Time Clocks had already been the depot level repair facility for all mechanical aircraft clocks for the U.S. Department of Defense since 1982.\n\n\n"}
{"id": "222172", "url": "https://en.wikipedia.org/wiki?curid=222172", "title": "Whistled language", "text": "Whistled language\n\nWhistled languages use whistling to emulate speech and facilitate communication. A whistled language is a system of whistled communication which allows fluent whistlers to transmit and comprehend a potentially unlimited number of messages over long distances. Whistled languages are different in this respect from the restricted codes sometimes used by herders or animal trainers to transmit simple messages or instructions. Generally, whistled languages emulate the tones or vowel formants of a natural spoken language, as well as aspects of its intonation and prosody, so that trained listeners who speak that language can understand the encoded message.\n\nWhistled language is rare compared to spoken language, but it is found in cultures around the world. It is especially common in tone languages where the whistled tones transmit the tones of the syllables (tone melodies of the words). This might be because in tone languages the tone melody carries more of the functional load of communication while non-tonal phonology carries proportionally less. The genesis of a whistled language has never been recorded in either case and has not yet received much productive study.\n\nWhistled languages differ according to whether the spoken language is tonal or not, with the whistling being either tone or articulation based (or both).\n\nTonal languages are often stripped of articulation, leaving only suprasegmental features such as duration and tone, and when whistled retain the spoken melodic line. Thus whistled tonal languages convey phonemic information solely through tone, length, and, to a lesser extent, stress, and most segmental phonemic distinctions of the spoken language are lost.\n\nIn non-tonal languages, more of the articulatory features of speech are retained, and the normally timbral variations imparted by the movements of the tongue and soft palate are transformed into pitch variations. Certain consonants can be pronounced while whistling, so as to modify the whistled sound, much as consonants in spoken language modify the vowel sounds adjacent to them.\n\n\"All whistled languages share one basic characteristic: they function by varying the frequency of a simple wave-form as a function of time, generally with minimal dynamic variations, which is readily understandable since in most cases their only purpose is long-distance communication.\"\n\nDifferent whistling styles may be used in a single language. Sochiapam Chinantec has three different words for whistle-speech: sie for whistling with the tongue against the alveolar ridge, jui̵ for bilabial whistling, and juo for finger-in-the-mouth whistling. These are used for communication over varying distances. There is also a kind of loud falsetto (hóh) which functions in some ways like whistled speech.\n\nThere are a few different techniques of how to produce whistle speech, the choice of which is dependent on practical concerns. Bilabial and labiodental techniques are common for short and medium distance discussions (in a market, in the noise of a room, or for hunting); whereas the tongue retroflexed, one or two fingers introduced in the mouth, a blow concentrated at the junction between two fingers or the lower lip pulled while breathing in air are techniques used to reach high levels of power for long distance speaking. Each place has its favorite trend that depends on the most common use of the village and on the personal preferences of each whistler. Whistling with a leaf or a flute is often related to courtship or poetic expression (reported in the Kickapoo language in Mexico and in the Hmong and Akha cultures in Asia).\n\nWhistling techniques do not require the vibration of the vocal cords: they produce a shock effect of the compressed air stream inside the cavity of the mouth and/or of the hands. When the jaws are fixed by a finger, the size of the hole is stable. The air stream expelled makes vibrations at the edge of the mouth. The faster the air stream is expelled, the higher is the noise inside the cavities. If the hole (mouth) and the cavity (intra-oral volume) are well matched, the resonance is tuned, and the whistle is projected more loudly. The frequency of this bioacoustical phenomenon is modulated by the morphing of the resonating cavity that can be, to a certain extent,\nrelated to the articulation of the equivalent spoken form.\n\nThe expressivity of whistled speech is likely to be somewhat limited compared to spoken speech (although not inherently so), but such a conclusion should not be taken as absolute, as it depends heavily on various factors including the phonology of the language. For example, in some tonal languages with few tones, whistled messages typically consist of stereotyped or otherwise standardized expressions, are elaborately descriptive, and often have to be repeated. However, in languages which are heavily tonal, and therefore convey much of their information through pitch even when spoken, such as Mazatec and Yoruba, extensive conversations may be whistled. In any case, even for non-tonal languages, measurements indicate that high intelligibility can be achieved with whistled speech (90%) of intelligibility of non-standardized sentences for Greek and the equivalent for Turkish.\n\nThis lack of understanding can be seen with a confusion matrix. It was tested using two speakers of Silbo (Jampolsky 1999). The study revealed that generally, the vowels were relatively easy to understand, and the consonants a bit more difficult.\n\nIn continental Africa, speech may be conveyed by a whistle or other musical instrument, most famously the \"talking drums\". However, while drums may be used by griots singing praise songs or for inter-village communication, and other instruments may be used on the radio for station identification jingles, for regular conversation at a distance whistled speech is used. As two people approach each other, one may even switch from whistled to spoken speech in mid-sentence.\n\nSilbo on the island of La Gomera in the Canary Islands, based on Spanish, is one of the best-studied whistled languages. The number of distinctive sounds or phonemes in this language is a matter of disagreement, varying according to the researcher from two to five vowels and four to nine consonants. This variation may reflect differences in speakers' abilities as well as in the methods used to elicit contrasts. The work of Meyer clarifies this debate by providing the first statistical analyses of production for various whistlers as well as psycholinguistic tests of vowel identification.\n\nOther whistled languages exist or existed in such parts of the world as Turkey (Kuşköy, \"Village of the Birds\"), France (the village of Aas in the Pyrenees), Mexico (the Mazatecs and Chinantecs of Oaxaca), South America (Pirahã), India (Kongthong village of Meghalaya),(the Chepang of Nepal), and New Guinea. They are especially common and robust today in parts of West Africa, used widely in such populous languages as Yoruba and Ewe. Even French is whistled in some areas of western Africa.\n\nAs well as the Canary Islands, whistled speech occurs in some parts of Southern Africa and Eastern Africa.\n\nMost whistle languages, of which there are several hundred, are based on tonal languages.\n\nOnly the tone of the speech is saved in the whistle, while aspects as articulation and phonation are eliminated. These are replaced by other features such as stress and rhythmical variations. However, some languages, like that of the Zezuru who speak a Shona-derived dialect, include articulation so that consonants interrupt the flow of the whistle. A similar language is the Tsonga whistle language used in the highlands in the Southern parts of Mozambique. This should not be confused with the whistled sibilants of Shona.\n\nIn early China, the technique of transcendental whistling was a kind of nonverbal language with affinities to the spiritual aspects of Daoist meditation.\n\nIn the Greek village of Antia, few whistlers remain now but in 1982 the entire population knew \"sfyria\", the local whistled speech.\n\nWhistled speech may be very central and highly valued in a culture. Shouting is very rare in Sochiapam Chinantec. Men in that culture are subject to being fined if they do not handle whistle-speech well enough to perform certain town jobs. They may whistle for fun in situations where spoken speech could easily be heard.\n\nIn Sochiapam, Oaxaca, and other places in Mexico, and reportedly in West Africa as well, whistled speech is men's language: although women may understand it, they do not use it.\n\nThough whistled languages are not secret codes or secret languages (with the exception of a whistled language used by ñañigos insurgencies in Cuba during Spanish occupation), they may be used for secretive communication among outsiders or others who do not know or understand the whistled language though they may understand its spoken origin. Stories are told of farmers in Aas during World War II, or in La Gomera, who were able to hide evidence of such nefarious activities as milk-watering because they were warned in whistle-speech that the police were approaching.\n\nWhistle languages have naturally developed in response to the necessity for humans to communicate in conditions of relative isolation, with possible causes being distance, noise levels, and night, as well as specific activities, such as social information, shepherding, hunting, fishing, courtship, or shamanism. Because of this usage, they are mostly related to places with mountains or dense forests. Southern China, Papua New Guinea, the Amazon forest, subsaharan Africa, Mexico, and Europe encompass most of these locations.\n\nThey have been more recently found in dense forests like the Amazon where they may replace spoken dialogue in the villages while hunting or fishing to overcome the pressure of the acoustic environment. The main advantage of whistling speech is that it allows the speaker to cover much larger distances (typically but up to in mountains and less in reverberating forests) than ordinary speech, without the strain (and lesser range) of shouting. More specifically, whistle speech can reach a loudness of 130 dB, and the transmission range can reach up to 10 km (as verified in La Gomera, Canary Island). The long range of whistling is enhanced by the mountainous terrain found in areas where whistled languages are used. Many areas with such languages work hard to preserve their ancient traditions, in the face of rapidly advancing telecommunications systems in many areas.\n\nA whistled tone is essentially a simple oscillation (or sine wave), and thus timbral variations are impossible. Normal articulation during an ordinary lip-whistle is relatively easy though the lips move little causing a constant of labialization and making labial and labiodental consonants (p, b, m, f, etc.) problematical. \"Apart from the five vowel-phonemes [of Silbo Gomero]—and even these do not invariably have a fixed or steady pitch—all whistled speech-sound realizations are glides which are interpreted in terms of range, contour, and steepness.\" \n\nThere are two different types of whistle tones - hole tones and edge tones. A hole (or 'orifice') tone is produced by a fast-moving cylinder (or 'vena contracta') of air that interacts with the slow-moving anulus of air surrounding it. Instability in the boundary layer leads to perturbations that increase in size until a feedback path is established whereby specific frequencies of the resonance chamber are emphasized. An edge tone, on the other hand, is generated by a thin jet of air that strikes an obstacle. Vortices are shed near the point of disturbance in the flow, alternating on each side of the obstacle or 'wedge'.\n\nA way in which true whistled languages differ from other types of whistled communication is that they encode auditory features of\nspoken languages by transposing key components of speech sounds. There are two types of whistled languages: those based on non-tone languages, which transpose F2 patterns (dealing with formants), and those based on tone languages, which transpose tone melodies. However, both types of whistle tones have a phonological structure that is related to the spoken language that they are transposing.\n\nIn a non-tonal language, segments may be differentiated as follows:\n\nThe following list is of languages that exist or existed in a whistled form, or of ethnic groups that speak such languages. In some cases (e.g. Chinantec) the whistled speech is an important and integral part of the language and culture; in others (e.g. Nahuatl) its role is much lesser.\n\n\n\n"}
{"id": "38428702", "url": "https://en.wikipedia.org/wiki?curid=38428702", "title": "Xolve", "text": "Xolve\n\nXolve, Inc. is a Madison, Wisconsin-based nanomaterial company that uses its proprietary technology to improve the attributes and performance of polymer composites and energy storage materials. The company is known for developing a process that uses organic compounds or polymers to either dissolve or place true solutions of nanoparticles previously thought to be insoluble, including carbon nanotubes and graphene.\n\nXolve won the Wisconsin Governor’s Business Plan Contest in 2008, and was named one of the top startups of 2008 by Businessweek. The company was also a national finalist in the 2010 CleanTech Open San Jose, CA. The company originated from the fundamental research of then 17-year-old student Philip Streich and University of Wisconsin-Platteville Chemistry and Engineering Physics Professor James P. Hamilton and was founded by serial entrepreneurs Professor Hamilton and Eric Apfelbach as well as Philip Streich.\n\nFounded in 2007 as Graphene Solutions, the firm was incubated in the UW-Platteville Nanotechnology Center for Collaborative Research and Development, the NCCRD. Xolve licenses some of the earliest patents on graphene from Professor Hamilton's Group that date back to work done in 2006 and 2007.\n\nIn 2010, the company changed its name to Xolve and went on to raise $2 million in its first round of funding. Primary investors included DSM, a Dutch material sciences company, and the Nordic Group of Companies in Baraboo, Wisconsin. In 2011, the company moved to its own labs in Middleton, Wisconsin.\n\nThe potential of nanoparticles rests on their surface area. However, practical applications of these materials have been limited by their tendency to form clumps and bundles, destroying that surface area. Beginning with its ability to place nanomaterials into true solutions, Xolve has developed additional technology to bring dispersed nanomaterials into industrial polymers and energy storage materials and keep them dispersed. With this technology, Xolve aims to lower the cost of producing nanomaterials, such as graphene, and to use these nanomaterials to dramatically improve the performance of industrial materials while maintaining their standard cost structure.\n"}
