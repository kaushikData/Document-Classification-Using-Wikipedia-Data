{"id": "2986016", "url": "https://en.wikipedia.org/wiki?curid=2986016", "title": "2,4,5-Trichlorophenoxyacetic acid", "text": "2,4,5-Trichlorophenoxyacetic acid\n\n2,4,5-Trichlorophenoxyacetic acid (also known as 2,4,5-T), a synthetic auxin, is a chlorophenoxy acetic acid herbicide used to defoliate broad-leafed plants. It was developed in the late 1940s and was widely used in the agricultural industry until being phased out, starting in the late 1970s due to toxicity concerns. Agent Orange, a defoliant used by the British in the Malayan Emergency and the U.S. in the Vietnam War, was equal parts 2,4,5-T and 2,4-D (2,4-dichlorophenoxyacetic acid). 2,4,5-T itself is toxic with a NOAEL of 3 mg/kg/day and a LOAEL of 10 mg/kg/day. Additionally, the manufacturing process for 2,4,5-T contaminates this chemical with trace amounts of 2,3,7,8-tetrachlorodibenzo-\"p\"-dioxin (TCDD). TCDD is a carcinogenic persistent organic pollutant with long-term effects on the environment. With proper temperature control during production of 2,4,5-T, TCDD levels can be held to about .005 ppm. Before the TCDD risk was well understood, early production facilities lacked proper temperature controls and individual batches tested later were found to have as much as 60 ppm of TCDD.\n\nIn 1970, the United States Department of Agriculture halted the use of 2,4,5-T on all food crops except rice, and in 1985, the EPA terminated all remaining uses in the U.S. of this herbicide. In Canada, the use and sale of 2,4,5-T was prohibited after 1985. The international trade of 2,4,5-T is restricted by the Rotterdam Convention. 2,4,5-T has since largely been replaced by dicamba and triclopyr.\n\nHuman health effects from 2,4,5-T at low environmental doses or at biomonitored levels from low environmental exposures are unknown. Intentional overdoses and unintentional high dose occupational exposures to chlorophenoxy acid herbicides have resulted in weakness, headache, dizziness, nausea, abdominal pain, myotonia, hypotension, renal and hepatic injury, and delayed neuropathy.. Cometabolism of 2,4,5-T is possible to produce 3,5-dichlorocatechol which, in turn, can be degraded by \"Pseudomonas\" bacteria.\nIARC considers the chlorophenoxyacetic acids group of chemicals as possibly carcinogenic to humans. In 1963 a production vessel for 2,4,5-T exploded in the Philips-Duphar plant in the Netherlands. Six workers that cleaned up afterwards got seriously intoxicated and developed chloracne. After twelve years, four of the six cleaners had died.\n\n\n"}
{"id": "1383986", "url": "https://en.wikipedia.org/wiki?curid=1383986", "title": "Absorption (chemistry)", "text": "Absorption (chemistry)\n\nIn chemistry, absorption is a physical or chemical phenomenon or a process in which atoms, molecules or ions enter some bulk phase – liquid or solid material. This is a different process from adsorption, since molecules undergoing absorption are taken up by the volume, not by the surface (as in the case for adsorption). A more general term is \"sorption\", which covers absorption, adsorption, and ion exchange. Absorption is a condition in which something takes in another substance.\n\nIn many processes important in technology, the chemical absorption is used in place of the physical process, e.g., absorption of carbon dioxide by sodium hydroxide – such acid-base processes do not follow the Nernst partition law.\n\nFor some examples of this effect, see liquid-liquid extraction. It is possible to extract from one liquid phase to another a solute without a chemical reaction. Examples of such solutes are noble gases and osmium tetroxide.\n\nThe process of absorption means that a substance captures and transforms energy. The absorbent distributes the material it captures throughout whole and adsorbent only distributes it through the surface.\n\nThe process of gas or liquid which penetrate into the body of adsorbent is commonly known as absorption.\n\nIf absorption is a physical process not accompanied by any other physical or chemical process, it usually follows the Nernst distribution law: \n\nThe value of constant K depends on temperature and is called partition coefficient. This equation is valid if concentrations are not too large and if the species \"x\" does not change its form in any of the two phases \"1\" or \"2\". If such molecule undergoes association or dissociation then this equation still describes the equilibrium between \"x\" in both phases, but only for the same form – concentrations of all remaining forms must be calculated by taking into account all the other equilibria.\n\nIn the case of gas absorption, one may calculate its concentration by using, e.g., the Ideal gas law, c = p/RT. In alternative fashion, one may use partial pressures instead of concentrations.\n\nAbsorption is a process that may be chemical (reactive) or physical (non-reactive). \n\nChemical absorption or reactive absorption is a chemical reaction between the absorbed and the absorbing substances. Sometimes it combines with physical absorption. This type of absorption depends upon the stoichiometry of the reaction and the concentration of its reactants.\n\nHydrophilic solids, which includes many solids of biological origin, can readily absorb water. Polar interactions between water and the molecules of the solid favour partition of the water into the solid, which can allow significant absorption of water vapour even in relatively low humidity. \n\nA plant fibre (or other hydrophilic material) that has been exposed to the atmosphere will usually contain some water even if it feels dry. The water can be driven off by heating in an oven, leading to a measurable decrease in weight, which will gradually be regained if the fibre is returned to a 'normal' atmosphere. This effect is important in the textile industry, where the proportion of a material's weight made up by water is called the \"moisture regain\". \n"}
{"id": "4645594", "url": "https://en.wikipedia.org/wiki?curid=4645594", "title": "Advanced Technology Leisure Application Simulator", "text": "Advanced Technology Leisure Application Simulator\n\nThe Advanced Technology Leisure Application Simulator, or ATLAS, is a large hydraulic motion simulator. It was designed, as the name implies, for the theme park industry. The ATLAS is a product of Rediffusion Simulation in Sussex, England, now owned by Thales Group and known as Thales Training & Simulation. Disney filed multiple patents on their variant of the device, including US Utility Patent #5161104 .\n\nThe ATLAS was derived from military flight simulation technology. It uses six hydraulic actuators to provide a broad range of movement.\n\nIn the later half of the 1980s, Walt Disney Imagineering bought and refined this technology for two theme park attractions; Star Tours at Disneyland (and later Disney's Hollywood Studios, Tokyo Disneyland, and Disneyland Paris) and Body Wars at Epcot. The technology was also used in 2016 for the Iron Man Experience at Hong Kong Disneyland. The Disney attractions feature large, 40-person cabins hidden from outside view, arranged lengthwise with four or six simulators per installation. There are four simulators at Disneyland's Star Tours and EPCOT's Body Wars, while the remaining Star Tours installations have six. Body Wars is now defunct and the simulators have been removed from the building in the years since the closure of the Wonders of Life pavilion.\n"}
{"id": "435796", "url": "https://en.wikipedia.org/wiki?curid=435796", "title": "Advanced meat recovery", "text": "Advanced meat recovery\n\nAdvanced meat recovery (AMR) is a slaughterhouse deboning process by which the last traces of skeletal muscle meat are removed from animal bones after the primal cuts have been carved off manually.\n\nThe machinery used in this process separates meat from bone by scraping, shaving, or pressing the meat from the bone without breaking or grinding the bone. Products produced by advanced meat recovery machinery can be labeled using terms associated with hand-deboned product (i.e., \"beef\", \"pork\", \"beef trimmings\", etc.). AMR meat typically is used as an ingredient in products requiring further processing, such as hot dogs.\n\nThis meat is comparable in appearance, texture, and composition to meat trimmings and similar meat products derived by hand.\n\nUSDA regulations for procurement of frozen fresh ground beef products state that \"Beef that is mechanically separated from bone with automatic deboning systems, advanced lean (meat) recovery (AMR) systems or powered knives, will not be allowed\".\n\nIn the United States, USDA regulations stipulate that AMR machinery cannot grind, crush, or pulverize bones to remove edible meat tissue, and bones must emerge intact. The meat produced in this manner can contain no more than 150(±30) milligrams of calcium per 200 grams product, as calcium in such high concentrations in the product would be indicative of bone being mixed with the meat. Products that exceed the calcium content limit must instead be labeled \"mechanically separated beef or pork\" in the ingredients statement.\n\nIn 1994, the Food Safety and Inspection Service (FSIS) issued a rule allowing such meat to be labeled as meat for human consumption, providing that the bones from which it was removed were still intact after processing. In 1997, following tests indicating that central nervous system (CNS) tissue was showing up in mechanically removed meat, FSIS issued a directive to its inspectors instructing them to ensure that spinal cord tissue was removed from bones before the AMR process. Following the identification of a BSE-infected U.S. dairy cow in December 2003, FSIS issued new regulations expanding the definition of prohibited CNS tissue to include additional cattle parts. Furthermore, all AMR-processed product from cattle more than 30 months old now is prohibited from being used for food, and such product from younger cattle and from other\nlivestock species also is prohibited if it contains CNS material.\n\nThe USDA's AMR guidelines restrict the processing of the parts of cattle that may contain central nervous tissue from AMR systems in cattle over 30 months of age. However, non-CNS tissue meat can be processed and is considered meat, as are the muscle cuts. Although some sources claim AMR systems use ammonia (or anhydrous ammonia, ammonia hydroxide, etc.) to treat the meat, this appears to be due to confusion between AMR and the production of lean finely textured beef (LFTB, commonly referred to as pink slime). LFTB is in fact treated with ammonia, and so is substantially more restricted than most AMR products.\n\nIn 2012, the European Commission changed the classification of desinewed meat (DSM) so that it could no longer be called 'meat' but must be included in the category of mechanically recovered meat (MRM). The downgrade of DSM took effect in the UK in May 2012. Unlike other types of MRM, which have the appearance of a paste, DSM resembled minced meat and was previously sold in the UK as 'meat'.\n\n\n"}
{"id": "56954282", "url": "https://en.wikipedia.org/wiki?curid=56954282", "title": "Alfons Rissberger", "text": "Alfons Rissberger\n\nAlfons Rissberger (born May 25, 1948 in Worms) is a German entrepreneur, business consultant and author as well as initiator and founding board member of, the biggest initiative between politics and economy for Germany's digital future.\n\nSince 43 years Rissberger has been a consultant for politics and economy with a focus on success and efficiency in the IT-age, especially regarding IT usage in management and education. In 1987 he was the first German scientist who was responsible for a model experiment by the Federal-State-Commission for education management and research support (BLK) which looked at e-learning at an elementary school. \nFrom 1985 until 1993 Rissberger was responsible for IT basic education (ITG) and e-learning in the educational system of the Rhineland-Palatinate ministry of education and cultural affairs. From 1993 until 2005 he was managing director of DVZ Datenverarbeitungszentrums Mecklenburg-Vorpommern GmbH (data processing center) and since that time he owns the business Rissberger Strategie Consulting in Hamburg and Schwerin.\n\nHe is married for the second time. He has two children with his first wife.\n\nAfter an apprenticeship in television technology-craftsmanship he studied electrical engineering, computer sciences and vocational education - with a focus on educational psychology and political sciences – in Frankfurt and Darmstadt. He holds a diploma in engineering and completed the first and second state examination in vocational education.\n\nAs son of a locksmith he financed his studies by working as a night cargo workman at Frankfurt Airport and by setting up informatics-course-systems at the community colleges Frankfurt-Höchst and Worms, his hometown. The concept of these informatics-courses was the foundation for recommendations by the Gesellschaft für Informatik (\"German Informatics Society\") and by the Deutsche Volkshochschul-Verband e.V. (\"German Community Collage Union\") for vocational education.\n\nFrom 1970 until 1991 he pursued teaching and leading activities in the school and college domain as well as in the area of advanced training of executive managers in the economy. His focus was applied informatics; among others from 1979 until 1991 as college professor in the areas informatics, trade, foreign business, taxation and tourism at the University of Applied Sciences of the state Rhineland-Palatinate, located in Worms. He lectured for 10 years at the BASF-Führungskolleg at the University Seminary of Economy in the water castle Gracht in Liblar near Cologne.\n\nSince 1974 Rissberg is consultant for politics and economy with an emphasis on success and efficiency in the IT-age. He is surveyor at the Enquete-Komission \"Deutschlands Weg in die Informationsgesellschaft\" (Enquete-Commission Germany's path to an information society) of the German Bundestag, he is a member of the science advisory council of the Konrad Adenauer Foundation, he is initiator and chairman of the Mecklenburg-Vorpommern multimedia advisory council, he is leader and initiator of various research projects in the area of e-learning, he is initiator and author of television and video productions as well as initiator and author of textbooks and numerous other publications.\n\nFrom 1985 until 1993 Rissberger headed up the department for new information and communication technologies at the Ministry of Education, Science and Advanced Training Rheinland-Pfalz in Mainz. He was responsible for the introduction of IT basic education in secondary education in all public schools within the state. He is also editor of the first German IT basic education schoolbook published by Klett-Verlag. Rissberger was initiator and leader of a number of German and European first-time BLK-model-experiments in the area of e-learning. He is idea generator for new school formats (such as the technical collage for informatics) and for new vocational training areas.\n\nIn 1994 he was initiator of the Berlin memorandum Aktiver Lernen: Multimedia für eine bessere Bildung (active learning: multimedia for a better education).\n\nIn 1999 he was idea generator and founding board member of Initiative D21 with chancellor Gerhard Schröder as chairman of the advisory council.\n\nFrom 2001 until 2005 Rissberger was founder and managing director of DVZ Consulting GmbH in Schwerin.\n\nIn 2001 Rissberger had demanded a focused national project \"e-learning in the German-speaking educational system\" with a concept for the first virtual University in Germany \"VirtuS- Virtual University Schwerin\", but the project did not yet come about (effective: 2016).\n\nIn 2007 he was initiator and co-editor of the Berlin memorandum \"VirtusD Virtuelle Universität Deutschland – E-Learning für eine bessere Bildung an den Hochschulen\" (VirtusD Virtual University Germany – e-learning for a better education at Universities).\n\nSince 2006 he owns Rissberger Strategie Consulting in Hamburg and Schwerin.\n\nSince 1976 he is a social democrat and was party leader and fraction speaker in his former hometown Worms-Horchheim.\n\n\n\n\n"}
{"id": "23630945", "url": "https://en.wikipedia.org/wiki?curid=23630945", "title": "Anidolic lighting", "text": "Anidolic lighting\n\nAnidolic lighting systems use anidolic optical components to light rooms. Light redirected by these systems does not converge to a focal point or form an image, whence the name (from \"an\", without, and \"eidolon\", image).\n\nAnidolic lighting uses non-imaging mirrors, lenses, and light guides to capture exterior sunlight and direct it deeply into rooms, while also scattering rays to avoid glare. The human eye's response to light is non-linear, so a more even distribution of the same amount of light makes a room appear brighter.\n\nIt is most challenging to effectively capture and redistribute of light on cloudy, overcast days, when the sunlight is diffuse.\n\nMirrors are typically parabolic or elliptical mirrors. Lenses are frequently made in multiple sections, like a Fresnel lens. Light guides include light pipes and anidolic ceilings.\n\nLens systems use reflection and refraction within optical prisms to redirect daylight. Some forms of prims lighting have been used for centuries, and others are 21st-century.\n\nDeck prisms were set into the upper decks of ships to light the decks below. Pavement lights were set into floors or sidewalks to let light into a basement below. The underside was frequently extended into prisms to direct and spread the light.\n\nPrism tiles were designed to bend sunbeams coming through a window upwards, so that they would reach deeper into a room. They were placed in the upper parts of window frames, where they were called \"transom lights\".\n\nDaylight redirecting window film (DRF) is a thin, flexible plastic version of the old glass prism tiles. It can be used as a substitute for opaque blinds.\n\nAnidolic mirror lighting systems can be divided into three parts:\n\nArchitectural design also require optimal integration into the building facade. \n\nTypically, light is captured with a compound parabolic collector (CPC) or elliptical collector (CEC) mounted on the exterior wall. These mirrors provide a wide and even collection pattern. The vertical capture angle approaches 90 degrees, from the horizon to the vertical plane of the supporting wall. An even capture pattern alleviates the need for a solar tracker: a permanently fixed anidolic collector remains effective at any time of day. \n\nExternal parabolic collectors require proper heat insulation (usually double-glazed windows over the zenithal opening) and roller blinds to reduce excessive lighting, glare and heat on sunny days.\n\nSnow and weatherproofing are also a consideration.\n\nUnlike the industrial parabolic troughs used in solar concentrators, architectural CPC mirrors do not concentrate captured light into a single focal point or focal line (which creates a fire hazard). Instead, light is directed into the building through a relatively wide opening.\n\nA second CPC or CEC mirror acting as an angle transformer disperses this beam into a wide-angle, diffused pattern. If it transmits light from a wide external CPC, a light tube actually becomes a flat \"anidolic ceiling\". \n\n\"Integrated anidolic systems\" reduce external protrusion and attempt to visually blend into traditional facades. However, like other anidolic systems, they are susceptible to glare and offer no protection from overheating on sunny days.\n\nFor example, the external CPC in the reference lights a 6-metre deep room. It protrudes 0.67 metres from the exterior wall and employs a 3.6 metre long, 0.5 meter tall light tube, followed by a 0.9 metre long interior CPC, to deliver captured light into the back of the room. This arrangement provided 32% energy savings over a six-month period compared to a reference facade.\n\n\n"}
{"id": "16039139", "url": "https://en.wikipedia.org/wiki?curid=16039139", "title": "Automatic-tracking satellite dish", "text": "Automatic-tracking satellite dish\n\nAutomatic Tracking Satellite Dishes are satellite dishes used while a vehicle is in motion. Automatic tracking satellite dishes utilize gyroscopes, GPS position sensors, and uses unique satellite identification data and an integrated DVB decoder to aid in identification of the satellite that it is pointing at.\n\nThe dishes consist usually of stepper motors to drive and aim the dish, gyroscopes to detect changes in position while the vehicle is in motion, a parabolic reflector, low-noise block converter, and control unit.\n\n"}
{"id": "37263123", "url": "https://en.wikipedia.org/wiki?curid=37263123", "title": "Bag", "text": "Bag\n\nA bag (also known regionally as a sack) is a common tool in the form of a non-rigid container. The use of bags predates recorded history, with the earliest bags being no more than lengths of animal skin, cotton, or woven plant fibers, folded up at the edges and secured in that shape with strings of the same material.\n\nDespite their simplicity, bags have been fundamental for the development of human civilization, as they allow people to easily collect loose materials such as berries or food grains, and to transport more items than could readily be carried in the hands. The word probably has its origins in the Norse word \"baggi\", from the reconstructed Proto-Indo-European bʰak, but is also comparable to the Welsh baich (load, bundle), and the Greek βάσταγμα (\"bástagma\", load).\n\nCheap disposable paper bags and plastic shopping bags are very common in the retail trade as a convenience for shoppers, and are often supplied by the shop for free or for a small fee. Customers may also take their own shopping bags to use in shops. Although, paper had been used for purposes of wrapping and padding in ancient China since the 2nd century BC, the first use of paper bags (for preserving the flavor of tea) in China came during the later Tang Dynasty (618–907 AD).\n\nBags have been around for hundreds of years and have been used by both men and women. Bags have been prevalent as far back as ancient Egypt. Many hieroglyphs depict males with bags tied around their waist. The Bible mentions pouches, especially with regard to Judas Iscariot carrying one around, holding his personal items. In the 14th century, wary of pickpockets and thieves, many people used drawstring bags, in which to carry their money. These bags were attached to \"girdles\" via a long cord fastened to the waist.\n\nWomen also wore more ornate drawstring bags, typically called \"hamondeys\" or \"tasques\", to display their social status. The 14th-century handbags evolved into wedding gifts from groom to bride. These medieval pouches were embroidered, often with depictions of love stories or songs. Eventually, these pouches evolved into what is known as a \"chaneries\", which were used for gaming or food for falcons. During the Renaissance, Elizabethan England's fashions were more ornate than ever before. Women's wore their pouches underneath the vast array of petticoats and men wore leather pockets or \"bagges\" inside their breeches. Aristocrats began carrying \"swete bagges\" filled with sweet-smelling material to make up for poor hygiene.\n\nIn the modern world, bags are ubiquitous, with many people routinely carrying a wide variety of them in the form of cloth or leather briefcases, handbags, and backpacks, and with bags made from more disposable materials such as paper or plastic being used for shopping, and to carry home groceries. A bag may be closable by a zipper, snap fastener, etc., or simply by folding (e.g. in the case of a paper bag). Sometimes a money bag or travel bag has a lock. The bag likely predates the inflexible variant, the basket, and bags usually have the additional advantage over baskets of being foldable or otherwise compressible to smaller sizes. On the other hand, baskets, being made of a more rigid material, may better protect their contents.\n\nAn empty bag may or may not be very light and foldable to a small size. If it is, this is convenient for carrying it to the place where it is needed, such as a shop, and for storage of empty bags. Bags vary from small ones, like purses, to large ones for use in traveling like a suitcase. The pockets of clothing are also a kind of bag, built into the clothing for the carrying of suitably small objects.\n\nThere are environmental concerns regarding use and disposal of plastic shopping and trash bags. Efforts are being taken to control and reduce their use in some European Union countries, including Ireland and the Netherlands. In some cases the cheap bags are taxed so the customer must pay a fee where they may not have done previously. Sometimes heavy duty reusable plastic and fabric bags are sold, typically costing €0.5 to €1, and these may replace disposable bags entirely. Sometimes free replacements are offered when the bag wears out. A notable exception to this trend is the UK, where disposable plastic bags are still freely available and are dominant. This trend has spread to some cities in the United States.\n\nA bag may or may not be disposable; however, even a disposable bag can often be used many times, for economic and environmental reasons. On the other hand, there may be logistic or hygienic reasons to use a bag only once. For example, a garbage bag is often disposed of with its content. A bag for packaging a disposable product is often disposed of when it is empty. Similarly, bags used as receptacles in medical procedures, such as the colostomy bag used to collect waste from a surgically diverted biological system, are typically disposed of as medical waste. Many snack foods, such as pretzels, cookies, and potato chips, are available in disposable single-use sealed bags.\n\n\n\n"}
{"id": "12764618", "url": "https://en.wikipedia.org/wiki?curid=12764618", "title": "Bite indicator", "text": "Bite indicator\n\nA bite indicator is a mechanical or electronic device which indicates to an angler that something is happening at the hook end of the fishing line.\n\nThere are many types of bite indicators. Which ones work best depends on the type of fishing.\n\n\n"}
{"id": "17987436", "url": "https://en.wikipedia.org/wiki?curid=17987436", "title": "Bucket (machine part)", "text": "Bucket (machine part)\n\nA bucket (also called a scoop to qualify shallower designs of tools) is a specialized container attached to a machine, as compared to a bucket adapted for manual use by a human being. It is a bulk material handling component.\n\nThe bucket has an inner volume as compared to other types of machine attachments like blades or shovels.\n\nThe bucket could be attached to the lifting hook of a crane, at the end of the arm of an excavating machine, to the wires of a dragline excavator, to the arms of a power shovel or a tractor equipped with a backhoe loader or to a loader, or to a dredge.\n\nThe name \"bucket\" may have been coined from buckets used in water wheels, or used in water turbines or in similar-looking devices.\n\nBuckets in mechanical engineering can have a distinct quality from the traditional bucket (pail) whose purpose is to contain things. Larger versions of this type of bucket equip bucket trucks to contain human beings, buckets in water-hauling systems in mines or, for instance, in helicopter buckets to hold water to combat fires.\n\nTwo other types of mechanical buckets can be distinguished according to the final destination of the device they equip: energy-consumer systems like excavators or energy-capturer systems like water bucket wheels or turbines.\n\nBuckets exist in a variety of sizes or shapes. They can be quite large like those equipping Hulett cranes, used to discharge ore out of cargo ships in harbours or very small such as those used by deep-sea exploration vehicles.\n\nThe shape of the bucket can vary from the truncated conical shape of an actual bucket to more scoop-like or spoon-like shapes akin to water turbines. The cross section can be round or square.\n\nThis is the same shape of a domestic form, the one-piece-standing single element, but often with an augmented size.\n\nIn early developments of mining, a large simple bucket allowed easy insertion of both miners and construction materials such as pit props, and later extraction of miners and ore. Common terms used in various parts of the world include: Bowk; Kibble; Hoppit; Hoppet. Latterly they have been called sinking buckets, as they are now only used when sinking new mine shafts before insertion of the cage, or for emergency rescue.\n\nConcrete buckets help deliver concrete on a specific site of a building by the means of a tower crane. They have a bottom opening to allow concrete to flow out of the bucket when in-place. See also tremie.\n\nThey are placed at the end of an excavator-like arm and have to be made from a material that provides isolation from electricity, like fiber glass, to help the workers protect themselves. There may be a door on the side of the bucket.\n\nExcavator buckets are made of solid steel and generally present teeth protruding from the cutting edge, to disrupt hard material and avoid wear-and-tear of the bucket. Subsets of the excavator bucket are: the ditching bucket, trenching bucket, A ditching bucket is a wider bucket with no teeth, used for excavating larger excavations and grading stone. A trenching excavator bucket is normally wide and with protruding teeth.\n\nThe clamshell bucket is a more sophisticated articulated several-piece device, including two elementary buckets associated on a hinged structure forming a claws-like appendage with an internal volume.\n\nThe design is used in bucket-wheel excavators. The buckets in the wheel have to be made of solid material to withstand the resistance of the material it cuts through.\n\nThe bucket wheel design is also used to capture the water energy in water-wheels or water turbines like Pelton wheels. The buckets also have to be made of solid material to withstand the force of the water flow. Their shape is optimized according to their purpose.\nOther designs include vertical shaft wind turbines designs like on the Savonius wind turbine. In this case, the buckets have to be made of a light material.\n\nThe buckets-ladders are used in bucket elevators or in the dredge design of some dredgers.\n"}
{"id": "22148690", "url": "https://en.wikipedia.org/wiki?curid=22148690", "title": "Campbell diagram", "text": "Campbell diagram\n\nA Campbell diagram plot represents a system's response spectrum as a function of its oscillation regime. It is named for Wilfred Campbell, who introduced the concept.\nIt is also called an interference diagram.\n\nIn rotordynamical systems, the eigenfrequencies often depend on the rotation rates due to the induced gyroscopic effects or variable hydrodynamic conditions in fluid bearings. It might represent the following cases:\n\n1. \"Analytically\" computed values of eigenfrequencies as a function of the shaft's rotation speed. This case is also called \"whirl speed map\". Such chart can be used in turbine design as shown in the numerically calculated Campbell diagram example illustrated by the image: analysis shows that there are well-damped critical speed at lower speed range. Another critical speed at mode 4 is observed at 7810 rpm (130 Hz) in dangerous vicinity of nominal shaft speed, but it has 30% damping - enough to safely ignore it.\n\n2. \"Experimentally\" measured vibration response spectrum as a function of the shaft's rotation speed (waterfall plot), the peak locations for each slice usually corresponding to the eigenfrequencies.\n\nIn acoustical engineering, the Campbell diagram would represent the pressure spectrum waterfall plot vs the machine's shaft rotation speed (sometimes also called \"3D noise map\").\n"}
{"id": "2190195", "url": "https://en.wikipedia.org/wiki?curid=2190195", "title": "Changeover", "text": "Changeover\n\nIn manufacturing, changeover is the process of converting a line or machine from running one product to another. Changeover times can last from a few minutes to as much as several weeks in the case of automobile manufacturers retooling for new models. The terms \"set-up\" and \"changeover\" are sometimes used interchangeably however this usage is incorrect. Set-up is only one component of changeover. Example: A soft drink bottler may run 16oz glass bottles one day, perform a changeover on the line and then run 20oz plastic bottles the next day.\n\nChangeover can be divided into the 3 Ups:\n\n\"Clean-up product, materials and components from the line. It may range from minor, if only the label of a package is being changed (for example from an English to a Spanish label) to major, requiring complete disassembly of the equipment, cleaning and sterilizing of the line components in the case of an injectable pharmaceutical product.\n\n\"Set-up\" is the process of actually converting the equipment. This may be achieved by adjusting the equipment to correspond to the next product or by changing non-adjustable \"change parts\" to accommodate the product. Typically it will be a combination of both. \n\n\"Start-up\" is the time spent fine tuning the equipment after it has been restarted. It is characterized by frequent stoppages, jams, quality rejects and other problems. It is generally caused by variability in the clean-up and set-up or by variability in the product or its components.\n\n"}
{"id": "622746", "url": "https://en.wikipedia.org/wiki?curid=622746", "title": "Columbian exchange", "text": "Columbian exchange\n\nThe Columbian exchange, also known as the Columbian interchange, named for Christopher Columbus, was the widespread transfer of plants, animals, culture, human populations, technology, and ideas between the Americas, West Africa, and the Old World in the 15th and 16th centuries. It also relates to European colonization and trade following Christopher Columbus's 1492 voyage. Invasive species, including communicable diseases, were a byproduct of the Exchange. The changes in agriculture significantly altered and changed global populations. The most significant immediate impact of the Columbian exchange was the cultural exchanges and the transfer of people (both free and enslaved) between continents.\n\nThe new contact between the global population circulated a wide variety of crops and livestock, which supported increases in population in both hemispheres, although diseases initially caused precipitous declines in the numbers of indigenous peoples of the Americas. Traders returned to Europe with maize, potatoes, and tomatoes, which became very important crops in Europe by the 18th century.\n\nThe term was first used in 1972 by American historian Alfred W. Crosby in his environmental history book \"The Columbian Exchange\". It was rapidly adopted by other historians and journalists and has become widely known.\n\nIn 1972 Alfred W. Crosby, an American historian at the University of Texas at Austin, published \"The Columbian Exchange\". He published subsequent volumes within the same decade. His primary focus was mapping the biological and cultural transfers that occurred between the Old and New World. He studied the effects of Columbus' voyages between the two. Specifically, the global diffusion of crops, seeds, and plants from the New World back into the Old. His research made a lasting contribution to the way scholars understand the variety of contemporary ecosystems that arose due to these transfers. \n\nThe term has become popular among historians and journalists, such as Charles C. Mann, whose book \"\" expands and updates Crosby's original research.\n\nSeveral plants native to the Americas have spread around the world, including potato, maize, tomato, and tobacco. Before 1500, potatoes were not grown outside of South America. By the 19th century they were found in nearly every cookpot in Europe and had conquered India and North America. Potatoes eventually became an important staple of the diet in much of Europe, contributing to about 25% of the population growth in Afro-Eurasia between 1700 and 1900. Many European rulers, including Frederick the Great of Prussia and Catherine the Great of Russia, encouraged the cultivation of the potato.\n\nMaize and cassava, introduced by the Portuguese from South America in the 16th century, have replaced sorghum and millet as Africa's most important food crops. 16th-century Spanish colonizers introduced new staple crops to Asia from the Americas, including maize and sweet potatoes, and thereby contributed to population growth in Asia. On a larger scale, the coming of potatoes and maize to the old world \"resulted in caloric and nutritional improvements over previously existing staples\" throughout the Eurasian landmass as they created more varied and abundant food production.\n\nTomatoes, which came to Europe from the New World via Spain, were initially prized in Italy mainly for their ornamental value (see below). From the 19th century tomato sauces became typical of Neapolitan cuisine and, ultimately, Italian cuisine in general. Coffee (introduced in the Americas circa 1720) from Africa and the Middle East and sugarcane (introduced from South Asia) from the Spanish West Indies became the main export commodity crops of extensive Latin American plantations. Introduced to India by the Portuguese, chili and potatoes from South America have become an integral part of Indian cuisine.\n\nRice was another crop that became widely cultivated during the Columbian exchange. As the demand in the New World grew, so did the knowledge on how to cultivate it. The two primary species used were oryza glaberrima and oryza sativa; originating from West Africa and Southeast Asia respectively. Slave holders in the New World relied upon the skills of enslaved Africans to further cultivate both species. . North and South Carolina were key places where rice was grown during the slave trade, and islands of the Caribbean like Puerto Rico and Cuba were equally great centers of production. Enslaved Africans brought their knowledge of water control, milling, winnowing, and other general agrarian practices to the fields. This widespread knowledge amongst enslaved Africans eventually led to rice becoming a staple dietary item in the New World. \n\nCitrus fruits and grapes were brought to the Americas from the Mediterranean. At first these crops struggled to adapt to the climate in the new world but by the late 19th century they were growing more consistently. \n\nThe guava plant (Diospyros mespiliformis) originated in West Africa and is now grown in Hawaii, Florida and areas of California. \n\nBananas were introduced to the Americas by Portuguese sailors who brought the fruits from West Africa during their enslavement of Africans in the 16th century. Bananas were still only consumed in minimal amounts around the 1880's. The U.S. didn't see major rises in banana consumption until banana plantations in the Caribbean grew. The History of modern banana plantations in the Americas details the spread of this crop within the Americas.\n\nIt took three centuries after their introduction in Europe for tomatoes to become widely accepted. Tobacco, potatoes, chili peppers, tomatillos, and tomatoes are all members of the nightshade family and all of these plants bear some resemblance to the European nightshade that even an amateur could deduce just by simple observation of the flowers and berries; tomatoes and potatoes can be quite lethal if the wrong part of the plant is consumed at the wrong quantity or at least cause a person to experience copious amounts of vomiting and diarrhea. Of all the New World plants introduced to Italy, only the potato took as long as the tomato to gain acceptance. 16th-century physicians, thus, had good reason to be wary that this native Mexican fruit was poisonous and the generator of \"melancholic humours\". In 1544, Pietro Andrea Mattioli, a Tuscan physician and botanist, suggested that tomatoes might be edible, but no record exists of anyone consuming them at this time. On October 31, 1548, the tomato was given its first name anywhere in Europe when a house steward of Cosimo I de' Medici, Duke of Florence, wrote to the De' Medici's private secretary that the basket of \"pomi d'oro\" \"had arrived safely\". At this time, the label \"pomi d'oro\" was also used to refer to figs, melons, and citrus fruits in treatises by scientists.\n\nIn the early years, tomatoes were mainly grown as ornamentals in Italy. For example, the Florentine aristocrat Giovan Vettorio Soderini wrote how they \"were to be sought only for their beauty\" and were grown only in gardens or flower beds. Tomatoes were grown in elite town and country gardens in the fifty years or so following their arrival in Europe and were only occasionally depicted in works of art. However, in 1592 the head gardener at the botanical garden of Aranjuez near Madrid, under the patronage of Philip II of Spain, wrote, \"it is said [tomatoes] are good for sauces\". Besides this account, tomatoes remained exotic plants grown for ornamental purposes, but rarely for culinary use. The combination of pasta with tomato sauce was developed only in the late nineteenth century. Today around of tomatoes are cultivated in Italy, although there are still areas where relatively few tomatoes are grown and consumed.\n\nInitially at least, the Columbian exchange of animals largely went in one direction, from Europe to the New World, as the Eurasian regions had domesticated many more animals. Horses, donkeys, mules, pigs, cattle, sheep, goats, chickens, large dogs, cats and bees were rapidly adopted by native peoples for transport, food, and other uses. One of the first European exports to the Americas, the horse, changed the lives of many Native American tribes. The mountain tribes shifted to a nomadic lifestyle, as opposed to agriculture, based on hunting bison on horseback and moved down to the Great Plains. The existing Plains tribes expanded their territories with horses, and the animals were considered so valuable that horse herds became a measure of wealth.\n\nThe effects of the introduction of European livestock on the environments and peoples of the New World were not always positive. In the Caribbean, the proliferation of European animals had large effects on native fauna and undergrowth and damaged \"conucos,\" plots managed by indigenous peoples for subsistence.\n\nEuropean exploration of tropical areas was aided by the New World discovery of quinine, the first effective treatment for malaria. Europeans suffered from this disease, but some indigenous populations had developed at least partial resistance to it. In Africa, resistance to malaria has been associated with other genetic changes among sub-Saharan Africans and their descendants, which can cause sickle-cell disease. In fact, the resistance of sub-Saharan Africans to malaria in the Southern United States contributed to the development of slavery in those regions.\n\nBefore regular communication had been established between the two hemispheres, the varieties of domesticated animals and infectious diseases that jumped to humans, such as smallpox, were substantially more numerous in the Old World than in the New due to more extensive long-distance trade networks. Many had migrated west across Eurasia with animals or people, or were brought by traders from Asia, so diseases of two continents were suffered by all occupants. While Europeans and Asians were affected by the Eurasian diseases, their endemic status in those continents over centuries resulted in many people gaining acquired immunity.\n\nBy contrast, \"Old World\" diseases had a devastating effect when introduced to Native American populations via European carriers, as the people in the Americas had no natural immunity to the new diseases. Measles caused many deaths. The smallpox epidemics are believed to have caused the largest death tolls among Native Americans, surpassing any wars and far exceeding the comparative loss of life in Europe due to the Black Death. It is estimated that upwards of 80–95 percent of the Native American population died in these epidemics within the first 100–150 years following 1492. Many regions in the Americas lost 100%. The beginning of demographic collapse on the North American continent has typically been attributed to the spread of a well-documented smallpox epidemic from Hispaniola in December 1518. At that point in time, approximately only 10,000 indigenous people were still alive in Hispaniola.\n\nSimilarly, yellow fever is thought to have been brought to the Americas from Africa via the Atlantic slave trade. Because it was endemic in Africa, many people there had acquired immunity. Europeans suffered higher rates of death than did African-descended persons when exposed to yellow fever in Africa and the Americas, where numerous epidemics swept the colonies beginning in the 17th century and continuing into the late 19th century. The disease caused widespread fatalities in the Caribbean during the heyday of slave-based sugar plantation. The replacement of native forests by sugar plantations and factories facilitated its spread in the tropical area by reducing the number of potential natural mosquito predators. The means of yellow fever transmission was unknown until 1881, when Carlos Finlay suggested that the disease was transmitted through mosquitoes, now known to be female mosquitoes of the species \"Aedes aegypti\".\n\nThe history of syphilis has been well-studied, but the exact origin of the disease is unknown and remains a subject of debate. There are two primary hypotheses: one proposes that syphilis was carried to Europe from the Americas by the crew of Christopher Columbus in the early 1490s, while the other proposes that syphilis previously existed in Europe but went unrecognized. These are referred to as the \"Columbian\" and \"pre-Columbian\" hypotheses. The first written descriptions of the disease in the Old World came in 1493. The first large outbreak of syphilis in Europe occurred in 1494/1495 in Naples, Italy, among the army of Charles VIII, during their invasion of Naples.\n\nOne of the influences related to the migration of people were cultural exchanges. For example, in the article \"The Myth of Early Globalization: The Atlantic Economy, 1500–1800\" Pieter Emmer makes the point that \"from 1500 onward, a 'clash of cultures' had begun in the Atlantic\". This clash of culture transferred European values to indigenous cultures. For example, the emergence of private property in regions where there were little to no rights to lands, the concepts of monogamy and the nuclear family, the role of women and children in the family system, and the \"superiority of free labor\". An example of this type of cultural exchange occurred during the 1500s in North America. When these early European colonizers first entered North America, they encountered fence-less lands which indicated to them that this land was unimproved. For these Europeans, they were seeking economic opportunities, therefore, land and resources were important for the success of the mission. When these colonizers entered North America they encountered a fully established culture of people called the Powhatan. The Powhatan farmers in Virginia scattered their farm plots within larger cleared areas. These larger cleared areas were a communal place for naturally growing and useful plants. As the Europeans viewed fences as \"hallmarks of civilization\" they set about transforming \"the land into something more suitable for themselves\". In implementing their practices, the Europeans enslaved, murdered, and exploited indigenous populations. Furthermore, in cases of enslaved peoples (and in particular, enslaved Africans) the Europeans simultaneously implemented their value system while at the same time justifying enslaving people through a philosophy which reduced the enslaved people to property. Thus, the slave traders and some of the plantation owners used the concept of family to exploit and control the enslaved people. In other subtle ways, which had a large impact the cultural exchanges involved sharing practices and traditions. An example of this can be found in the tobacco industry.\n\nTobacco was one of the luxury goods which was spread as a direct result of the Columbian exchange. As is discussed in regard to the trans-Atlantic slave trade, the tobacco trade increased demand for free labor and spread tobacco worldwide. In discussing the widespread uses of tobacco, the Spanish physician Nicolas Monardes (1493–1588) noted that \"The black people that have gone from these parts to the Indies, have taken up the same manner and use of tobacco that the Indians have\". As the European colonizers and enslaved Africans traveled the globe and came into contact with indigenous peoples, they took with them the cultural practices related to tobacco, and spread them to additional regions. Therefore, demand for tobacco grew in the course of the cultural exchanges and increased contacts among peoples.\n\nThe Trans Atlantic Slave Trade was the transfer of Africans from primarily West Africa to parts of the Americas between the 16th and 19th century. About 10 million slaves arrived in the Americas from Africa. The journey that enslaved Africans took from parts of Africa to America is commonly known as the middle passage. \n\nEnslaved Africans had a significant influence on the emerging African American culture in the New World as well as all other nations to where they were transported, especially the Caribbean and Brazil. The presence of enslaved Africans not only represented skilled labor but it also gave way to a new population which represented a hybrid of the two cultures. \"The Birth of African American Culture: An Anthropological Perspective\" is a book written by Sidney Mintz and Richard Price further detailing the cultural impact of Enslaved Africans in America. Mintz and Price's book helped to publicize how integral the socialization aspects of plantation life were to the structures of black culture.\n\nThe treatment of enslaved Africans during the Trans Atlantic Slave Trade became one of the most controversial topics in the history of the New World. Since its abolishment in 1865 in the USA and its total extinction by 1890 in the rest of the New World, it has remained a key subject in politics, pop culture and media.\n\nPlants that arrived by land, sea, or air in the times before 1492 are called archaeophytes, and plants introduced to Europe after those times are called neophytes. Invasive species of plants and pathogens also were introduced by chance, including such weeds as tumbleweeds (\"Salsola\" spp.) and wild oats (\"Avena fatua\"). Some plants introduced intentionally, such as the kudzu vine introduced in 1894 from Japan to the United States to help control soil erosion, have since been found to be invasive pests in the new environment. \n\nFungi have also been transported, such as the one responsible for Dutch elm disease, killing American elms in North American forests and cities, where many had been planted as street trees. Some of the invasive species have become serious ecosystem and economic problems after establishing in the New World environments. A beneficial, although probably unintentional, introduction is \"Saccharomyces eubayanus\", the yeast responsible for lager beer now thought to have originated in Patagonia. Others have crossed the Atlantic eastwards to Europe and have had the power to change the course of history: in the 1840s phytophthera infestans crossed the oceans and caused problems with the potato crop in several nations of Europe but totally destroyed the crop of Ireland and lead millions to starve to death and die in the Irish Potato Famine.\n\nIn addition to these, many animals were introduced to new habitats on the other side of the world either accidentally or incidentally. These include such animals as brown rats, earthworms (apparently absent from parts of the pre-Columbian New World), and zebra mussels, which arrived on ships. Escaped and feral populations of non-indigenous animals have thrived in both the Old and New Worlds, often negatively impacting or displacing native species. In the New World, populations of feral European cats, pigs, horses and cattle are common, and the Burmese python and green iguana are considered problematic in Florida. In the Old World, the Eastern gray squirrel have been particularly successful in colonising Great Britain and populations of raccoons can now be found in some regions of Germany, the Caucasus and Japan. Fur farm escapees such as coypu and American mink have extensive populations.\n\n\n"}
{"id": "42290411", "url": "https://en.wikipedia.org/wiki?curid=42290411", "title": "Cyborg antenna", "text": "Cyborg antenna\n\nA cyborg antenna is an osseointegrated device implanted in a human skull. The antenna, composed of a wireless camera on one end and a wireless sound vibration implant on the other end, allows wireless communication and wireless transmission of images, sound or video from skull to skull.\nThe antenna uses audible vibrations in the skull to report information. This includes measurements of electromagnetic radiation, phone calls, music, as well as video or images which are transmitted through audible vibrations. The Wi-Fi enabled antenna also allows the reception signals and data from satellites.\n\nThe first antenna was created in England in 2003 by Adam Montandon in collaboration with colorblind artist Neil Harbisson. The invention, under the name \"eyeborg\" and under the heading \"Bridging the Island of the Colourblind Project\", won a British award in Innovation (Submerge 2004) and a European award in Content Tools and Interface Design (Europrix 2004). In 2007, Peter Kese, a software developer from Kranj, Slovenia, made further developments to the antenna by increasing the number of color hues to 360 and adding color saturation through different volume levels. In 2009, Matias Lizana, a student from Universitat Politècnica de Catalunya developed the antenna into a chip as part of his final year project. The chip allows users to have the antenna implanted and to hear colors beyond the limits of human perception such as infrared and ultraviolet.\n\nHarbisson's Sonochromatic Music Scale (2003) is a microtonal and logarithmic scale with 360 notes in an octave. Each note corresponds to a specific degree of the color wheel. The scale was introduced to the first antenna in 2004.\n\nHarbisson's Pure Sonochromatic Scale (2005) is a non-logarithmic scale based on the transposition of light frequencies to sound frequencies. The scale discards color as being part of a color wheel and ignores musical/logarithmic perception so it can overstep the limits of human perception. The introduction of the new scale to the eyeborg in 2010, allows users to decide whether they want to perceive colors logarithmically or not.\n\nSince 2005, antennas have been donated to blind communities in Europe, Asia and America with the aim of helping the blind develop the sense of color. The first blind person to try out an eyeborg was Sabriye Tenberken followed by blind students from Braille Without Borders in Tibet and members of the Sociedad de Ciegos de Pichincha in Ecuador.\n\nIn 2011, vice-president of Ecuador Lenin Moreno announced that his government would collaborate with the Cyborg Foundation to create antennas and new sensory extensions. In 2012, after lecturing at Escola Politécnica de Pernambuco in Recife, the Cyborg Foundation signed a partnership to create antennas and other new human extensions in collaboration with Universidade de Pernambuco in Brazil.\n\nAntennas are currently being treated as body parts rather than as devices, and therefore are donated rather than sold.\n\n\n"}
{"id": "51717770", "url": "https://en.wikipedia.org/wiki?curid=51717770", "title": "Derivative code", "text": "Derivative code\n\nDerivative Code or Chameleon Code is source code which has been derived entirely from one or more other machine readable file formats. If Recursive Transcompiling is used in the development process, some code will survive all the way through the pipeline from beginning to end, and then back to the beginning again.\n\nThis code is, by definition, derivative code. The following procedure can be used to easily test if any source code is derivative code or not. \nIf the build process simply replaces the source code which has been deleted, it is (obviously) code which has been derived from something else and is therefore, by definition, derivative code.\n\nIf the build process fails, and a human being needs to re-create the deleted code by hand this is again, by definition, hand code.\n\nIronically, the transcompilers and other tools which create derivative code, are usually themselves either in part, or entirely hand code.\n"}
{"id": "4190859", "url": "https://en.wikipedia.org/wiki?curid=4190859", "title": "Dimethyl dicarbonate", "text": "Dimethyl dicarbonate\n\nDimethyl dicarbonate (DMDC) is an organic compound which is a colorless liquid with a pungent odor at high concentration at room temperature. It is primarily used as a beverage preservative, processing aid, or sterilant (INS No. 242), and acts by inhibiting the enzymes acetate kinase and L-glutamic acid decarboxylase. It has also been proposed that DMDC inhibits the enzymes alcohol dehydrogenase and glyceraldehyde 3-phosphate dehydrogenase by causing the methoxycarbonylation of their histidine components.\n\nIn wine, it is often used to replace potassium sorbate, as it inactivates wine spoilage yeasts such as \"Brettanomyces\". Once it has been added to beverages, the efficacy of the chemical is provided by the following reactions:\n\nThe application of DMDC is particularly useful when wine needs to be sterilized but cannot be sterile filtered, pasteurized, or sulfured. DMDC is also used to stabilize non-alcoholic beverages such as carbonated or non-carbonated juice beverages, isotonic sports beverages, iced teas and flavored waters.\n\nDMDC is added before the filling of the beverage. It then breaks down into small amounts of methanol and carbon dioxide, which are both natural constituents of fruit and vegetable juices.\n\nThe EU Scientific Committee on Food, the FDA in the United States and the JECFA of the WHO have confirmed the safe use in beverages. The FDA approved its use in wines in 1988, with the maximum level being permitted set at 200 mg/L, and only if there were fewer than 500 yeast cells/mL at time of dosage. It is also approved in the EU, where it is listed under E number E242, as well as Australia and New Zealand.\n\n\n"}
{"id": "841877", "url": "https://en.wikipedia.org/wiki?curid=841877", "title": "Earthbag construction", "text": "Earthbag construction\n\nEarthbag construction is an inexpensive method using mostly local soil to create structures which are both strong and can be quickly built.\n\nIt is a natural building technique developed from historic military bunker construction techniques and temporary flood-control dike building methods. The technique requires very basic construction materials: sturdy sacks filled with organic material usually available on site. \n\nStandard earthbag fill material has internal stability. Either moist subsoil that contains enough clay to become cohesive when tamped, or a water-resistant angular gravel or crushed volcanic rock is used. Walls are gradually built up by laying the bags in courses—forming a staggered pattern similar to bricklaying.\n\nThe walls can be curved or straight, domed with earth or topped with conventional roofs. Curved walls provide good lateral stability, forming round rooms and/or domed ceilings like an igloo.\n\nBuildings with straight walls longer than in length need intersecting walls or bracing buttresses. International standards exist for bracing wall size and spacing for earthen construction in different types of seismic risk areas, most notably the performance-based standards of New Zealand recommended by the ASTM International's earth building standards. Static shear testing shows that earthbag can reach similar strengths to New Zealand's reinforced adobe standards with specific soil strengths and reinforcement although unreinforced weak soil earthbag can have lower shear strength than unreinforced adobe. \nTo improve friction between bags and wall tensile strength barbed wire is usually placed between courses. Twine is also sometimes wrapped around the bags to tie one course to the next, to hold in-progress structures together and keep courses well-seated on barbed wire prongs. Rebar can be hammered into walls to strengthen corners and opening edges and provide resistance against overturning.\n\nThe structure is typically finished with plaster, either cement stucco on a strong mesh layer or an adobe or lime plaster, to shed water and prevent fabric UV damage.\n\nThis construction technique is one of the most versatile natural building methods and can be used for benches, freestanding walls, emergency shelters, temporary or permanent housing, or barns and commercial buildings. Earthbag is frequently chosen for many small-to-medium-sized institutional structures in the developing world. Subgrade structures including underground and bermed dwellings (such as Earthships), cisterns, spring boxes, root cellars, and retaining walls can be built with stabilized soil fill or with additional reinforcement and water-resistant gravel or sand fill.\n\nWhile Gernot Minke, the German professor of earthen architecture, first developed a technique of using bags filled with pumice to build walls, it was architect and builder Nader Khalili who first popularized earthbag construction (particularly for residential buildings).\nKhalili called his technique superadobe, because he filled the bags with moistened adobe soil. Buildings at the Cal Earth Institute in Hesperia, CA, which he established in 1991, include domes and vaulted roofs. Khalili pioneered code approval of earthbag domes for seismic risk regions. Several books and videos have been produced by the institute to demonstrate his methods, however a number of other individuals and groups now offer training workshops.\n\nAlthough Joseph Kennedy probably invented the term earthbag (as well as contained earth), Paulina Wojciechowska wrote the first book on the topic of earthbag building in 2001, \"Building with Earth: A Guide to Flexible-Form Earthbag Construction\". Kelly Hart developed a massive online database of earthbag information that encouraged idea sharing. Kaki Hunter and Doni Kiffmeyer worked on a variety of projects after studying with Khalili, calling earthbag 'flexible form rammed earth'. Their 2004 book, \"Earthbag Building: the Tools, Tricks and Techniques\", is available as an e-book.\n\nFree online booklets have been developed by different authors, including Owen Geiger and Patti Stouter. These include structural research and field testing techniques developed for rural areas.\n\nA 2011 e-book by Geiger, \"Earthbag Building Guide: Vertical Walls Step-by-Step\", provides photo illustrations of the process and discussions of new techniques for low-risk areas.\n\nMany like Akio Inoue, from Tenri University in Japan and Scott Howard of Earthen Hand have tested and built buildings. Hart, with Geiger, encouraged earthbag's development into different culturally and climatically-appropriate shapes. Robert Shear built an earthship inspired earthbag house in Utah and Morgan Caraway of Sustainable Life School is building a house that incorporates earthship design principles as well.\n\nDr. John Anderton of South Africa has tested a triple channel bag version that reduces the slumping problems inherent in non-cohesive fill material like sand, and pioneered work in a narrow wall contained sand system which he calls E-khaya.\n\nFernando Pacheco of Brazil pioneered the use of lighter HDPE mesh tubing for simpler hyperadobe walls. \n\nRebuilding after natural disasters and in low-income regions around the world has included earthbag. Although heavy earthen walls are usually dangerous in quakes, Nepal's spring 2015 earthquakes left earthbag buildings in good condition near destroyed buildings. \n\nEngineer Nabil Taha developed the first general specifications for one type of exterior pinning reinforcement appropriate for the highest seismic risk zones. Several engineering students have tested uncured or low strength earthbag, and Build Simple has tested cured cohesive walls. Organizations building in Nepal are currently working with engineers to improve and refine reinforcement options for seismic-resistant earthbag. \n\nConstruction usually begins by digging a trench to undisturbed mineral subsoil, which is partially filled with stones and/or gravel to create a rubble trench foundation. In high seismic risk regions a reinforced concrete footing or grade beam may be recommended. Earthbag buildings can also be built on conventional concrete slabs (though this is more expensive and uses more embodied energy than a rubble trench foundation) and can have a bermed or underground \"floating\" foundation like an earthship as well.\n\nSeveral courses of gravel in doubled woven bags form a water-resistant foundation. Each layer usually has two strands of barbed wire on top, that attaches to the bag to prevents slippage and resists any tendency for the outward expansion of dome or rectangular walls.\n\nBags on the course above are offset by —half of the wall width—similar to running bond in masonry. Bags can either be pre-filled with material and hoisted up, or bags or tubes are filled in place. The weight of the earthen fill locks the bag in place on the barbed wire below. A light tamping of the bags or tubes consolidates the moist clay-containing fill and creates interlocking bags or tubes anchored on the barbed wire. \n\nSolid-weave polypropylene is most popular, available around the world to transport rice or other grains. Polypropylene is low cost and resists water damage, rot, and insects. Tubes are often available from manufacturers who sew them into bags. Mesh tubes of soft crocheted poly fibers are also used, although stiff extruded mesh or woven mesh bags can also be used.\n\nOrganic/natural materials such as hemp, burlap (like \"gunny sacks\") can be used. Since these may rot, they should only be used with cohesive fills (containing a significant proportion of clay) that form solid masses when tamped.\n\nEarthbag is now a varied family of techniques. Each type of fill and container has different strength and reinforcement requirements. \n\nFor hazardous locations, accurate terminology is needed. Contained Earth (CE) is based on the original technique, but with specific soil strengths and reinforcement chosen for hazard levels. CE uses damp, cohesive, tamped bag fill, which bonds strongly with barbed wire and other reinforcement as the wall cures.\n\nCE is not 'sandbags'. Contained Sand (CS) uses sand fill or any fill too dry or with poor cohesion that performs structurally like sandbags. CS must be built with solid-weave fabric bags and have good protection from fabric damage, relying on the strength of the bag fabric for wall strength. CS needs more vertical reinforcement for both shear and out-of-plane strength than CE, or may require a structural skin. Some builders use narrow bags of contained sand as wall infill. \n\nContained Gravel (CG) uses fill of any aggregate larger than coarse sand, usually in doubled rice bags, although strong mesh can be used. CG limits dampness transmission from footings. \n\nModular CE is built in grain bags or similar tubes. Walls rely on attachment between barbed wire barbs and/ or added pins between courses. Solid CE is hyperadobe built in some type of knit raschel mesh tube, so that the damp earthen fill solidifies between courses.\n\nGenerally inorganic material is used as filler, but some organic material (such as rice hulls) can be used if a strong matrix like wire mesh reinforces the plaster. \n\nEarthen fill may contain 5–50% clay, and can be 'reject fines,' 'road base,' 'engineered fill,' or local subsoil. 'Raw' or un-stabilized soils cure as solid units but cannot withstand prolonged soaking. Subsoils with clay mold tightly and attach well to barbed wire prongs and rebar. \n\nSoil fill can contain a high proportion of aggregate, as long as it tamps and cures strongly. Crushed bottles, strong rubble, or plastic trash can be used, but high aggregate mixes may interfere with inserting rebar. \n\nSands, stone dust and gravels can survive prolonged flood conditions, but most require special bracing during construction as well as some form of structural skin. Sand fill may be appropriate for several courses to provide a vibration damping building base, but becomes unstable in ordinary bags above in height. \n\nCement, lime or bitumen stabilization can allow clay soil to withstand flooding or allow sands to be used in traditional bags with a non-structural plaster skin. Because earthbag walls are usually thick a large amount of stabilizer is needed.\nThermal insulating properties are important for climates that experience temperature extremes. The thermal insulating value of a material is directly related to both the porosity of the material and the thickness of the wall. Crushed volcanic rock, pumice or rice hulls yield higher insulation value than clay or sand. Untreated organic materials that could decay should not be used as part of a structural wall, although they can be used as infill. \n\nUnited Earth Builders has tried a light straw clay in the hyperadobe mesh tubing to form a layer 200 mm (8\") thick outside of a dome.\nThermal mass properties of earthen fill moderate temperature swings in climates that experience high temperature fluctuations from night to day. This thermal flywheel effect makes massive earth walls ideal for mild or hot and dry climates. Clay or sand also have excellent heat retention characteristics and, when properly insulated from the home's exterior, can serve as thermal mass in a passive solar building design in cool climates, keeping interior temperatures stable year-round.\n\nSolid CE may be built with less barbed wire in low-risk areas because walls solidify between courses. Earthbag using woven bags or tubes need barbed wire for any level of natural hazard since the bag-to-bag surfaces are slippery. Pins between courses do not contribute important linear out-of-plane strength. Walls of earthbag with barbed wire are more flexible than adobe and may resist collapse when carefully detailed. \n\nEarthbag of weak soil with no steel can be half the shear strength of unreinforced adobe, which is easily damaged in earthquakes. New Zealand's code detailing and plans allow unreinforced adobe walls to survive almost 0.6 g forces (comparable to Ss values for 2% probability of excedance in 50 years), but earthbag needs stronger soil to match this strength. Earthbag in Nepal surpassed this strength slightly by resisting forces above 0.7 g in early 2015 . Domes tested in California resisted approximately 1 g forces, due to the stable shape of these less than diameter buildings .\n\nCurrent earthbag techniques of inserting rebar unattached to base and overlapping without connection may only resist 1.2 g or less, even if using very strong soil. Special reinforcement is needed \n\nSolid CE of strong soil has higher shear and out of plane strength than modular CE. It may also allow the use of mesh for horizontal reinforcement in addition to or in place of barbed wire.\n\nContained gravel or contained sand may perform best with wire wrapped around the sides of straight wall sections, alternating with the next course having barbed wire gift-wrapped under and over the same straight sections. Base walls of CG in high risk regions may need additional buttresses at the foundation level where builders cannot afford a reinforced concrete (RC) grade beam or footing. A narrower plastic mesh tube often used for erosion control wattle could be filled with gravel to allow a half-width RC ring beam under the wide walls.\n\nA roof can be formed by gradually sloping the walls inward to construct a dome. Vaulted roofs can be built on forms. Or a bond beam is used under a traditional roof type. Hip roofs, gable-type trusses or vigas may be needed to reduce outward stress on earthen walls.\n\nEarth domes are inexpensive to build, but waterproofing them is complex or expensive in humid regions.\nWindows and doors can be formed with a traditional masonry lintel or with corbeling or brick-arch techniques, on temporary forms. Light may also be brought in by skylights, glass-capped pipes, or bottles placed between bag courses during construction.\n\nCover the wall to prevent damage to the bags from UV rays or moisture with cement-based stucco, or lime or earthen plaster. If walls are 'raw' earth, an infill plaster of earth with straw is used to fill the nooks between bags or courses. A finish plaster is applied on top.\n\nRoof overhangs are helpful to reduce plaster waterproofing requirements, although plaster on lower walls may be stronger and more water-resistant than plaster on upper walls.\n\nSome buildings use a planted-earth \"living roof\" (\"green-roof\") to top the structure, while others use a more conventional framing and roof placed atop earth-bag walls.\n\nEarthbag construction uses very little energy compared to other durable construction methods. Unlike concrete, brick or wood, no energy is needed to produce the earthen fill other than gathering soil. If on-site soil is used, little energy is needed for transportation. Unlike rammed earth construction, only human labor energy is required to tamp the soil lightly. The energy-intensive materials that \"are\" used — plastic (for bags & twine), steel wire, and perhaps the outer shell of plaster or stucco — are used in relatively small quantities compared to other types of construction, often totaling less than 5% of the building materials. Buildings last a long time when maintained. However, if 'raw' or unstabilized soil is used as fill, when the building is no longer useful the earthen fill can be recycled into either garden areas, backfill, or new earthen buildings.\n\nEarthbag building techniques were also explored in Sri Lanka after the 2004 tsunami. Multiple earthbag construction projects have been completed in Haiti, most of these after the earthquake. First Steps Himalaya and other charities had built more than 50 earthbag buildings in Nepal prior to the April 2015 earthquake. Since then, local builders flocked to ongoing earthbag training opportunities, including those by Good Earth Nepal, which have led to official Nepal building code acceptance of this technique. International NPOs have built hundreds of contained earth or earthbag buildings in Nepal as well.\n\nKhalili proposed using the techniques of earthbag construction for building structures on the Moon or other planets. Currently, it is quite expensive to lift a positive-mass payload from Earth. Thus, Khalili's techniques would seem to be an ideal solution as the requisite supplies would consist of lightweight bags and a few tools to fill them. He specified that such bags would probably have pre-sewn \"hook and loop\" (i.e. Velcro) fastener strips in lieu of barbed wire.\n\n"}
{"id": "48547495", "url": "https://en.wikipedia.org/wiki?curid=48547495", "title": "EktaPro Motioncorder", "text": "EktaPro Motioncorder\n\nThe KODAK Motioncorder is a 512 x 480 High-speed camera. It is part of the Photron FASTCAM line of cameras, introduced in 1996.\nPhotron Kodak Motioncorder was introduce in 1996. The camera was manufacture by Photron but trade branded as a KODAK MASD product. The Kodak Motioncorder and the Photron FASTCAM Super 10K Motioncorder are the same camera, just different trade names.\n\nThe Kodak Motioncorder native resolution is 512 x 480 pixels x 8 bits at 250 FPS. By reducing the resolution, the frame rate for recording can be increased. As an example, 1000 FPS is achieved with a resolution of 256 x 240 pixels at 8 bits. The 10,000 FPS requires the resolution be reduced to 128 x 34 pixels at 8 bits. The Kodak Motioncorder came with three different memory storages capabilities. The Processor could hold either 128 MB, 384 MB or 512 MB. With the maximum memory, the camera can record 2,184 images or 8.73 seconds of record time at 250 FPS. Digital image data could be read from the Processor through a SCSI interface. Live video images could be displayed on NTSC or PAL monitors. Ancillary information would be display as OSD (On-Screen-Data). The camera cable could be up to 16m from the Processor. The system could be controlled from a computer through an RS-232 interface sending simple ASCII commands.\n\nThe Kodak Motioncorder has been used in many diverse applications such as production line troubleshooting, packaging machine design and university research. It has been verified by the manufacture that over 4000 Motioncorders/Super 10K cameras were produced from 1996 to 2005.\n\nAnother popular High-speed camera from this era was the Redlake Motionscope. Both products used the same CCD sensor (7.4 um pixel. The Kodak Motioncorder could record 250 FPS while the Redlake Motionscope could record 240 fps. These two products ushered in a new class of cameras which were low cost with relatively high speed image capture that was not previously available.\n\n\n"}
{"id": "37606777", "url": "https://en.wikipedia.org/wiki?curid=37606777", "title": "Emotions in virtual communication", "text": "Emotions in virtual communication\n\nEmotions in virtual communication differ in a variety of ways from those in face-to-face interactions due to the characteristics of computer-mediated communication (CMC). CMC may lack many of the auditory and visual cues normally associated with the emotional aspects of interactions. Research in this area has investigated how and when individuals display and interpret various emotions in virtual settings.\n\nWhile text-based communication eliminates audio and visual cues, there are other methods for adding emotion. Emoticons, or emotional icons, can be used to display various types of emotions. Similar to emotional displays in face-to-face communication, it was found that females tend to use more emoticons than their male counterparts. Beyond simply using emoticons, in virtual communication platforms, people tend to capitalize letters or words to add emphasis to speaking.\n\nThere are a variety of characteristics of virtual communication that result in an increase in the amount of emotion displayed. The lack of social cues in CMC has been found to have a depersonalizing effect. Additionally, there can be greater anonymity or perceptions of anonymity in virtual communication. This combination of anonymous and social detached communication has been shown to increase the likelihood of flaming, or angry and hostile language as a result of uninhibited behavior.\n\nFurthermore, it has been shown that virtual communication can reduce normative social pressures. As a result of decreased social pressures, individuals may feel more comfortable disclosing either positive or negative affect, which may not be considered appropriate in normal face-to-face interactions. For example, in a large part due to decreased social hierarchies, Gilmore and Warren (2007) found many instances of feelings of intimacy, playfulness, and pride in a virtual teaching environment.\n\nThe lack of social and emotional cues over virtual communication platforms can result in increased instances of misinterpreting emotion and intentions. Kruger, Epley, Parker, and Ng (2005) found that individuals overestimate both their ability to clearly relay and interpret emotions via email. They attribute this inability to relay emotions effectively to others over CMC to a combination of egocentrism and a lack of paralinguistic cues including gestures, emphasis, and intonations.\n\nOne of the reasons that emails that are intended to be positive may come across as more neutral is that the process of email itself tends to be less stimulating than face-to-face communication. Since many people tend to associate emails with work-related matters, they come to expect less positive affect to be displayed in emails. Furthermore, the emotional ambiguity of email messages may actually lead to them to be interpreted as more negatively than they were intended. Byron (2008) notes that emails from senders higher in status will be more likely be perceived as negative than emails received from people who are lower in status.\n\nGiven the permanent and potentially public nature of virtual communication, it is much more likely that unintended parties will view and interpret messages as opposed face-to-face communication, which is fleeting. It has been found that when third parties view virtual communications, the third parties may interpret interactions as contentious disputes, when in fact there may not have actually been any conflict.\n\nDue to the excess disclosure of and the potential to misinterpret emotions, conflicts can arise in virtual communication. Communication mediums that have the greatest amount of emotional cues and immediacy of feedback will be best to reduce conflict. Increased emotional cues allow for better detection of negative affect, and greater displays of positive affect to counter any negative emotions. Immediacy of feedback relates to how quickly messages are transmitted via a particular communication medium, and the expectation for which they will be responded. For example, instant messaging has a higher degree of immediacy of feedback than email because instant messaging tends to result in much more synchronous communication than email. Immediacy of feedback allows individuals to detect and address frustration and other negative emotions more quickly. Furthermore, communication mediums that are more synchronous better allow for spontaneous comments, such as jokes, which are necessary for positive affect. Increased positive affect helps to create positive interactions which reduce the likelihood of conflict.\n\n"}
{"id": "19536528", "url": "https://en.wikipedia.org/wiki?curid=19536528", "title": "Engineering sample", "text": "Engineering sample\n\nEngineering samples are the beta versions of integrated circuits that are meant to be used for compatibility qualification or as demonstrators. They are usually loaned to OEM manufacturers prior to the chip's commercial release to allow product development or display. Usually, they are picked out of a very large batch and perform correctly. However, rarely they may have faults that were fixed in the production model.\n\nEngineering samples are usually handed out under a non-disclosure agreement or an other type of confidentiality agreement.\n\nSome engineering samples, such as Pentium 4 processors were rare and favoured for having unlocked base-clock multipliers. More recently, Core 2 engineering samples have become more common and popular. Asian sellers were selling the Core 2 processors at major profit. Some engineering samples have been put through strenuous tests.\n\nEngineering sample processors are also offered on a technical loan to some full-time employees at Intel, and are usually desktop extreme edition processors.\n"}
{"id": "9972860", "url": "https://en.wikipedia.org/wiki?curid=9972860", "title": "Flagging (tape)", "text": "Flagging (tape)\n\nFlagging is a colored non-adhesive tape used in marking objects. It is commonly made of PVC or vinyl, and wood fibre cellulose-based biodegradable flagging also exists.\n\nFlagging is used in surveying to mark grade levels, utility lines, survey stakes and other boundary markers.\nSurveyors frequently attach their flagging to wooden stakes or lathes, with writing on it. One side tends to have a long number which they reference in a log book. The other side tends to have abbreviations suggesting what the stake marks. Choice of color depends on many factors, and can include availability, and personal preference, or may adhere to some sort of color code. No color codes appear to be mandatory or universal, but certain colors do tend to be used for specific purposes.\n\nIn forestry flagging is commonly used to mark trees for various purposes. It can be used to mark trees for logging, to mark dangerous or unhealthy trees, to mark invasive species, or to mark saplings. State and National forests often use a wide variety of flagging tape, sometimes even getting specially printed tape when the full range of color codes is used up.\n\nFlagging is widely used in wildland fire suppression both as a navigational aid for firefighters and to mark trees. When walking to a wildfire a crew may use flagging to flag their way to the fire, both to aid other firefighters in quickly finding the site and so they can find their way back out easily. Specially marked flagging also exists for fire use, imprinted with terms such as \"spot fire\" or \"escape route\".\n\nIn triage, flagging is used in lieu of a triage tag to mark patients in a mass casualty disaster situation. Four colors of flagging are typically used:\n\n\nFlagging is used as a navigational aid by hunters, hikers, geocachers, spelunkers, mountain bikers, offroad vehicle users, and for other uses such as paintball.\n"}
{"id": "46860482", "url": "https://en.wikipedia.org/wiki?curid=46860482", "title": "Fotocapio", "text": "Fotocapio\n\nFotocapio is a brand of photographic equipment for cameras based in London, UK. It was founded by former commercial photographer Tony Chau.\n\nFotocapio produces the BounceLite flash diffuser. The BounceLite flash diffuser is aimed at professional and amateur photographers\n\n"}
{"id": "31421610", "url": "https://en.wikipedia.org/wiki?curid=31421610", "title": "GIADS", "text": "GIADS\n\nGIADS is the standard ACC System of the GAF TACCS. It is operated in the static Control and Reporting Centres (CRC Erndtebrück and CRC Schönewalde) and the deployable CRC (on Holzdorf Air Base) in order to provide Airspace Surveillance, to control Air Force Operations, and to meet the military commitment of the Bundeswehr.\n\nUp to the year 2000 the standard AC2 System of the GAF TACCS was ARKONA which had been taken over former the former East German Air Force. In the time to come it had to be replaced by the successor system GIADS. In July 2000 the first GIADS CRC in Schönewalde became operational. Since that time GIADS has been improve, further developed and introduced to the other German CRCs (including Deployable CRC). By the introduction of GIADS III in 2010/11, the probable final GAF AC2 System’s generation might have been procured.\n\nGIADS lll should be replaced in line with the joint NATO procurement programme by the successor AC2 product, the \"Air Command and Control System (ACCS)\".\n\n\n\nThe GAF Material Command, followed by the GAF Weapon Systems Command – WSC (de: Waffensystemkommando der Luftwaffe – WaSysKdoLw), have been in charge of the GIADS In-Service Support Management (ISSM). Today this WSC provides the obsolescence management, the hardware and software configuration control and the software related instructions to that GAF C3 SSC and EADS, in charge of GIADS software change and maintenance.\n\n"}
{"id": "8207891", "url": "https://en.wikipedia.org/wiki?curid=8207891", "title": "Gary Vaynerchuk", "text": "Gary Vaynerchuk\n\nGary Vaynerchuk (born Gennady Vaynerchuk; November 14, 1975; , ) is a Belarusian American entrepreneur, author, speaker and internet personality. First known as a wine critic who grew his family's wine business from $3 million to $60 million, Vaynerchuk is best known for his work in digital marketing and social media, leading New York-based companies VaynerMedia and VaynerX.\n\nVaynerchuk was born in Babruysk in the Soviet Union (today part of Belarus), and emigrated to the United States in 1978 at the age of three. He lived in a studio-apartment in Queens, New York with eight other family members. After living in Queens, Vaynerchuk and his family moved to Edison, New Jersey where Vaynerchuk operated a lemonade-stand franchise and earned thousands of dollars on weekends trading baseball cards. At age 14, he joined his family's retail-wine business. After his family moved, he graduated from North Hunterdon High School. Vaynerchuk graduated with a bachelor's degree from Mount Ida College in Newton, Massachusetts in 1998.\n\nAfter graduating from college in 1998, Vaynerchuk assumed day-to-day control of his father's Springfield, New Jersey store, Shopper's Discount Liquors. Gary renamed the store to Wine Library, launched sales online and in 2006 started Wine Library TV, a daily webcast covering wine.\n\nThrough e-commerce and pricing Vaynerchuk grew the business from $3 million to $60 million a year by 2005. In August 2011, Vaynerchuk announced he would be stepping away to build VaynerMedia, the digital ad agency he co-founded with his brother in 2009.\n\nIn 2009, Gary, along with his brother AJ Vaynerchuk, founded VaynerMedia, a social media-focused digital agency. The company provides social media and strategy services to Fortune 500 companies such as General Electric, Anheuser-Busch, Mondelez and PepsiCo. In 2015, VaynerMedia was named one of AdAge's A-List agencies. With 600 employees in 2016, VaynerMedia grossed $100 million in revenue. The company also partnered with Vimeo to connect brands and filmmakers for digital content.\n\nIn 2017 The Wall Street Journal reported that Vaynerchuk formed The Gallery, a new company that houses PureWow following its acquisition by Vaynerchuk and RSE Ventures along with other media and creative-content properties. PureWow CEO, Ryan Harwood, is The Gallery's CEO. A sister company to digital agency, VaynerMedia, Marketing Dive wrote that \"joining forces with VaynerMedia grants access to increased video capabilities given the in-house teams and resources.\"\n\nVaynerchuk has made a number of personal investments as an angel investor including in women's publisher PureWow in 2017. He has also invested in Facebook, Twitter, Venmo, and dozens of other startups. In 2017, \"Entrepreneur\" estimated Vaynerchuk's net worth at $160 million.\n\nAfter exits in Tumblr and Buddy Media, Vaynerchuk started VaynerRSE as a $25 Million investment fund with RSE Ventures' Matt Higgins and backed by Miami Dolphins owner Stephen Ross. The fund focuses on consumer technology and acts as an incubator in addition to traditional angel investing.\n\nIn 2014, Vaynerchuk partnered with social TV entrepreneurs Jesse Redniss and David Beck to form BRaVe Ventures. The firm advises television networks on emerging technology and funds and incubates emerging multi-screen and social network startups and technologies. In November 2016 Variety magazine reported that Turner Broadcasting System acquired the advisory business of BRaVe Ventures to develop business and strategy for its flagship brands, TBS and TNT.\n\nIn 2016 Vaynerchuk invested in the sports agency, Symmetry, to form VaynerSports to provide full-service athlete representation. In 2017 VaynerSports signed NFL draft participants including Jalen Reeves Maybin, Jon Toth and Josh Jackson.\n\nIn February 2017, Apple and Propagate announced the launch of Planet of the Apps, a reality television series with a recurring cast that includes Vaynerchuk, will.i.am and Gwyneth Paltrow. Described as Shark Tank meets American Idol, in the show Vaynerchuk and team evaluate pitches from app developers vying for investment. The series cast joined with Product Hunt for a tour to Austin, San Francisco, Los Angeles and New York.\n\nDailyVee is a daily, video-documentary series on YouTube that chronicles Vaynerchuk's life as a businessman. Started in 2015, Vaynerchuk records live, interviewing others and broadcasting investor meetings and strategy sessions at VaynerMedia. In the series Vaynerchuk implements social media strategies, especially through Snapchat to demonstrate social-media marketing.\n\nIn 2014, Vaynerchuk launched The #AskGaryVee Show on YouTube with his personal content-production team. In the show, Vaynerchuk canvases questions from Twitter and Instagram and responds in a signature, extemporaneous manner. Show questions, most commonly on entrepreneurship, family and business topics, are pre-screened by the production team but remain unseen by Vaynerchuk until each show's taping. The AskGaryVee Show inspired Vaynerchuk's fourth book, \"AskGaryVee: One Entrepreneur's Take on Leadership, Social Media, and Self-Awareness\".\n\nVaynerchuk hosted a video blog on YouTube called \"Wine Library TV\" (\"WLTV\" or \"The Thunder Show\") from 2006 to 2011, featuring wine reviews, tastings, and wine advice. The show debuted in February 2006 and was produced daily at the Wine Library store in Springfield, New Jersey. Vaynerchuk appeared on the cover of the December 2008 issue of Mutineer Magazine, launching the \"Mutineer Interview\" series. Celebrity guests included Jancis Robinson, Heidi Barrett, Kevin Rose, Timothy Ferriss, Jim Cramer of CNBC's Mad Money, Wayne Gretzky, and Dick Vermeil.\n\nAt 1,000 episodes in 2011 Vaynerchuk retired the show and replaced it with a video podcast, The Daily Grape. In August 2011, Vaynerchuk announced on Daily Grape that he was retiring from wine video blogging.\n\nIn 2010, Vaynerchuk launched Wine & Web on Sirius XM satellite radio. The show's programming paired new wine tastings in a \"Wine of the Week\" segment with coverage of gadgets, trends and startups in its \"Web of the Week\" segment.\n\nIn March 2009, Vaynerchuk signed a 10-book deal with HarperStudio, reportedly for over $1,000,000, and released his first book, \"Crush It! Why Now is the Time to Cash in on your Passion\", in October 2009. In the first weeks of its release \"Crush It!\" climbed to #1 on the Amazon Best Seller list for Web Marketing books. It also opened at number two on the New York Times Hardcover Advice bestseller list and on the \"Wall Street Journal\" bestseller List. \"Crush It!\" was also among the first books released on the Vook platform.\n\nIn 2011, Vaynerchuk's second book was released. \"The Thank You Economy\" explores the numbers and soft factors that drive successful relationships between businesses and consumers. \"The Thank You Economy\" reached the number two spot on the New York Times hardcover advice bestseller list.\n\nIn 2013, Vaynerchuk released his third book, \"Jab, Jab, Jab, Right Hook: How to Tell Your Story in a Noisy Social World\", through publisher Harper Collins. By highlighting campaigns and strategies that both succeeded and failed across all of the major social media platforms, Vaynerchuk's third book shows social media marketing strategies and tactics that he believes businesses should be avoiding or employing. \"Jab, Jab, Jab, Right-Hook\" debuted at the top of the Wall Street Journal's business books list and at number four on the New York Times hardcover advice bestseller list.\n\nIn March 2016, Vaynerchuk's fourth book, \"AskGaryVee: One Entrepreneur's Take on Leadership, Social Media, and Self-Awareness\", was published by Harper Business, part of Harper Collins. Based on Vaynerchuk's YouTube series, #AskGaryVee, Vaynerchuk compiled questions and answers from his YouTube show into a book, based on categories including self-awareness, parenting, and entrepreneurial hustle. #AskGaryVee became Vaynerchuk's fourth New York Times bestseller.\n\nIn January 2018, Vaynerchuk published a fifth book, \"Crushing It: How great entrepreneurs build and influence- and how you can, too\".\n\nVaynerchuk has been featured in \"The New York Times\", \"The Wall Street Journal\", \"GQ\", and \"Time\", and has appeared on \"Late Night with Conan O'Brien\" and \"Ellen\".\n\nIn the 2000s, Vaynerchuk was described as \"the first wine guru of the YouTube era\", \"the wine world's new superstar\", and by Rob Newsom, a Washington State wine maker, \"outside of Robert Parker, probably the most influential wine critic in the United States\". In 2003, Market Watch magazine awarded Gary Vaynerchuk its Market Watch Leader\" award, making him its youngest recipient. In July 2009 \"Decanter\" ranked Vaynerchuk at #40 on \"The Power List\" ranking of the wine industry's individuals of influence, citing that he \"represents the power of blogging\".\n\nIn 2011, The Wall Street Journal named Vaynerchuk to its list of Twitter's Small Business Big Shots and Bloomberg's Business Week named him to its list of 20 People Every Entrepreneur Should Follow. In 2013, Vaynerchuk made the November cover of Inc. magazine in a feature on \"How to Master the 4 Big Social-Media Platforms.\"\n\nIn 2014, he was named to Fortune's 40 Under 40 and selected to judge the Miss America pageant. In 2015 he was named to Crain's New York Business 40 Under 40 and named to Inc.’s list of \"Top 25 Social Media Keynote Speakers You Need to Know.\" In 2016 Vaynerchuk was a judge for the Genius Awards.\n\n\n"}
{"id": "45225971", "url": "https://en.wikipedia.org/wiki?curid=45225971", "title": "Green information system", "text": "Green information system\n\nGreen information system (green IS) or Green IT as per International Federation of Global & Green ICT \"IFGICT\" is a combination of environment and information technology (IT). The relationship between information technology and the environment is complex, because of the negative environmental impact of IT production, use, and disposal then, making this effect greener has been termed Green Information Technology, which considers IT's environmental impact primarily as a problem to be mitigated. Another effect involves the positive impact of using information systems (IS) to improve the eco-sustainability of businesses and society; this is termed green IS. This green IS viewpoint sees IS as a partial solution to many environmental problems. So green IS is a solution for the negative environmental effect of information technology. IS facilitates the reuse of waste and energy and can serve as a tool for industrial symbiosis, which involves \"the mutualistic interaction of different industries for beneficial reuse of waste flows or energy cascading that results in a more resource-efficient production system and fewer adverse environmental impacts\".\n\n"}
{"id": "28737625", "url": "https://en.wikipedia.org/wiki?curid=28737625", "title": "ISO 9564", "text": "ISO 9564\n\nISO 9564 is an international standard for personal identification number (PIN) management and security in financial services.\n\nThe PIN is used to verify the identity of a customer (the user of a bank card) within an electronic funds transfer system, and (typically) to authorize the transfer or withdrawal of funds. Therefore, it is important to protect PINs against unauthorized disclosure or misuse. Modern banking systems require interoperability between a variety of PIN entry devices, smart cards, card readers, card issuers, acquiring banks and retailers – including transmission of PINs between those entities – so a common set of rules for handling and securing PINs is required, both to ensure technical compatibility and a mutually agreed level of security. ISO 9564 provides principles and techniques to meet these requirements.\n\nISO 9564 comprises three parts, under the general title of \"Financial services — Personal Identification Number (PIN) management and security\".\n\nISO 9564-1:2011 specifies the basic principles and techniques of secure PIN management. It includes both general principles and specific requirements.\n\nThe basic principles of PIN management include:\n\n\nThe standard specifies some characteristics required or recommended of \"PIN entry devices\" (also known as PIN pads), i.e. the device into which the customer enters the PIN, including:\n\n\nA PIN may be stored in a secure smart card, and verified offline by that card. The PIN entry device and the reader used for the card that will verify the PIN may be integrated into a single physically secure unit, but they do not need to be.\n\nAdditional requirements that apply to smart card readers include:\n\n\nOther specific requirements include:\n\n\nThe standard specifies that PINs shall be from four to twelve digits long, noting that longer PINs are more secure but harder to use. It also suggests that the issuer should not assign PINs longer than six digits.\n\nThere are three accepted methods of selecting or generating a PIN:\n\n\n\n\nThe standard includes requirements for keeping the PIN secret while transmitting it, after generation, from the issuer to the customer. These include:\n\n\nTo protect the PIN during transmission from the PIN entry device to the verifier, the standard requires that the PIN be encrypted, and specifies several formats that may be used. In each case, the PIN is encoded into a \"PIN block\", which is then encrypted by an \"approved algorithm\", according to of the standard).\n\nThe PIN block formats are:\n\nThe PIN block is constructed by XOR-ing two 64-bit fields: the \"plain text PIN field\" and the \"account number field\", both of which comprise 16 four-bit nibbles.\n\nThe plain text PIN field is:\n\nThe account number field is:\n\nThis format should be used where no PAN is available. The PIN block is constructed by concatenating the PIN with a transaction number thus:\n\nFormat 2 is for local use with off-line systems only, e.g. smart cards. The PIN block is constructed by concatenating the PIN with a filler value thus:\n(Except for the format value in the first nibble, this is identical to the plain text PIN field of format 0.)\n\nFormat 3 is the same as format 0, except that the \"fill\" digits are random values from 10 to 15, and the first nibble (which identifies the block format) has the value 3.\n\nFormats 0 to 3 are all suitable for use with the Triple Data Encryption Algorithm, as they correspond to its 64-bit block size. However the standard allows for other encryption algorithms with larger block sizes, e.g. the Advanced Encryption Standard has a block size of 128 bits. In such cases the PIN must be encoding into an \"extended PIN block\", the format of which is defined in a 2015 amendment to ISO 9564-1.\n\nISO 9564-2:2014 specifies which encryption algorithms may be used for encrypting PINs. The approved algorithms are:\n\nISO 9564-3 \"Part 3: Requirements for offline PIN handling in ATM and POS systems\", most recently published in 2003, was withdrawn in 2011 and its contents merged into .\n\nISO 9564-4:2016 defines minimum security requirements and practices for the use of PINs and PIN entry devices in electronic commerce.\n"}
{"id": "18295", "url": "https://en.wikipedia.org/wiki?curid=18295", "title": "Legacy system", "text": "Legacy system\n\nIn computing, a legacy system is an old method, technology, computer system, or application program, \"of, relating to, or being a previous or outdated computer system,\" yet still in use. Often referencing a system as \"legacy\" means that it paved the way for the standards that would follow it. This can also imply that the system is out of date or in need of replacement.\n\nThe source of the term \"legacy\" in a technical sense is its use in university admissions. In that context , a \"legacy\" is a euphemism, used to describe someone receiving special treatment during admissions, because descended from an alumnus.\n\nThe first use of the term \"legacy\" to describe computer systems probably occurred in the 1970s. By the 1980s it was commonly used to refer to existing computer systems to distinguish them from the design and implementation of new systems. Legacy was often heard during a conversion process, for example, when moving data from the legacy system to a new database.\n\nWhile this term may indicate that some engineers may feel that a system is out of date, a legacy system may continue to be used for a variety of reasons. It may simply be that the system still provides for the users' needs. In addition, the decision to keep an old system may be influenced by economic reasons such as return on investment challenges or vendor lock-in, the inherent challenges of change management, or a variety of other reasons other than functionality. Backward compatibility (such as the ability of newer systems to handle legacy file formats and character encodings) is a goal that software developers often include in their work.\n\nEven if it is no longer used, a legacy system may continue to impact the organization due to its historical role. Historic data may not have been converted into the new system format and may exist within the new system with the use of a customized schema crosswalk, or may exist only in a data warehouse. In either case, the effect on business intelligence and operational reporting can be significant. A legacy system may include procedures or terminology which are no longer relevant in the current context, and may hinder or confuse understanding of the methods or technologies used.\n\nOrganizations can have compelling reasons for keeping a legacy system, such as:\n\nLegacy systems are considered to be potentially problematic by some software engineers for several reasons.\n\nWhere it is impossible to replace legacy systems through the practice of application retirement, it is still possible to enhance (or \"re-face\") them. Most development often goes into adding new interfaces to a legacy system. The most prominent technique is to provide a Web-based interface to a terminal-based mainframe application. This may reduce staff productivity due to slower response times and slower mouse-based operator actions, yet it is often seen as an \"upgrade\", because the interface style is familiar to unskilled users and is easy for them to use. John McCormick discusses such strategies that involve middleware.\n\nPrinting improvements are problematic because legacy software systems often add no formatting instructions, or they use protocols that are not usable in modern PC/Windows printers. A print server can be used to intercept the data and translate it to a more modern code. Rich Text Format (RTF) or PostScript documents may be created in the legacy application and then interpreted at a PC before being printed.\n\nBiometric security measures are difficult to implement on legacy systems. A workable solution is to use a telnet or http proxy server to sit between users and the mainframe to implement secure access to the legacy application.\n\nThe change being undertaken in some organizations is to switch to automated business process (ABP) software which generates complete systems. These systems can then interface to the organizations' legacy systems and use them as data repositories. This approach can provide a number of significant benefits: the users are insulated from the inefficiencies of their legacy systems, and the changes can be incorporated quickly and easily in the ABP software.\n\nModel-driven reverse and forward engineering approaches can be also used for the improvement of legacy software.\n\nAndreas Hein, from the Technical University of Munich, researched the use of legacy systems in space exploration. According to Hein, legacy systems are attractive for reuse if an organization has the capabilities for verification, validation, testing, and operational history. These capabilities must be integrated into various software life cycle phases such as development, implementation, usage, or maintenance. For software systems, the capability to use and maintain the system are crucial. Otherwise the system will become less and less understandable and maintainable.\n\nAccording to Hein, verification, validation, testing, and operational history increases the confidence in a system's reliability and quality. However, accumulating this history is often expensive. NASA's now retired Space Shuttle program used a large amount of 1970s-era technology. Replacement was cost-prohibitive because of the expensive requirement for flight certification. The original hardware completed the expensive integration and certification requirement for flight, but any new equipment would have had to go through that entire process again. This long and detailed process required extensive tests of the new components in their new configurations before a single unit could be used in the Space Shuttle program. Thus any new system that started the certification process becomes a \"de facto\" legacy system by the time it is approved for flight.\n\nAdditionally, the entire Space Shuttle system, including ground and launch vehicle assets, was designed to work together as a closed system. Since the specifications did not change, all of the certified systems and components performed well in the roles for which they were designed. Even before the Shuttle was scheduled to be retired in 2010, NASA found it advantageous to keep using many pieces of 1970s technology rather than to upgrade those systems and recertify the new components.\n\nThe term \"legacy support\" is often used in conjunction with legacy systems. The term may refer to a feature of modern software. For example, Operating systems with \"legacy support\" can detect and use older hardware. The term may also be used to refer to a business function; e.g. A software or hardware vendor that is supporting, or providing software maintenance, for older products.\n\nA \"legacy\" product may be a product that is no longer sold, has lost substantial market share, or is a version of a product that is not current. A legacy product may have some advantage over a modern product making it appealing for customers to keep it around. A product is only truly \"obsolete\" if it has an advantage to nobody – if no person making a rational decision would choose to acquire it new.\n\nThe term \"legacy mode\" often refers specifically to backward compatibility. A software product that is capable of performing as though it were a previous version of itself, is said to be \"running in legacy mode.\" This kind of feature is common in operating systems and internet browsers, where many applications depend on these underlying components.\n\nThe computer mainframe era saw many applications running in legacy mode. In the modern business computing environment, n-tier, or 3-tier architectures are more difficult to place into legacy mode as they include many components making up a single system.\n\nVirtualization technology is a recent innovation allowing legacy systems to continue to operate on modern hardware by running older operating systems and browsers on a software system that emulates legacy hardware.\n\nProgrammers have borrowed the term \"brownfield\" from the construction industry, where previously developed land (often polluted and abandoned) is described as \"brownfield\".\n\n\nThere is an alternate favorable opinion — growing since the end of the Dotcom bubble in 1999 — that legacy systems are simply computer systems in working use:\n\nIT analysts estimate that the cost of replacing business logic is about five times that of reuse, even discounting the risk of system failures and security breaches. Ideally, businesses would never have to rewrite most core business logic: \"debits = credits\" is a perennial requirement. \n\nThe IT industry is responding with \"legacy modernization\" and \"legacy transformation\": refurbishing existing business logic with new user interfaces, sometimes using screen scraping and service-enabled access through web services. These techniques allow organizations to understand their existing code assets (using discovery tools), provide new user and application interfaces to existing code, improve workflow, contain costs, minimize risk, and enjoy classic qualities of service (near 100% uptime, security, scalability, etc.).\n\nThis trend also invites reflection on what makes legacy systems so durable. Technologists are relearning the importance of sound architecture from the start, to avoid costly and risky rewrites. The most common legacy systems tend to be those which embraced well-known IT architectural principles, with careful planning and strict methodology during implementation. Poorly designed systems often don't last, both because they wear out and because their inherent faults invite replacement. Thus, many organizations are rediscovering the value of both their legacy systems and the theortical underpinnings of those systems.\n\n"}
{"id": "47256601", "url": "https://en.wikipedia.org/wiki?curid=47256601", "title": "Leica Biosystems", "text": "Leica Biosystems\n\nLeica Biosystems (founded 1989) is a Medical Devices company that develops and supplies clinical diagnostics to the pathology market. It is also a research, instrument, and medical device company as well as a division of Danaher Corporation.\n\nThe company has headquarters in Germany with operations in the United Kingdom, Ireland, Netherlands, Australia, India, China, Singapore, Mexico and the United States (California, Illinois, and Ohio). A research and development facility concentrating on companion diagnostics for cancer drugs has been in operation in Danvers, Massachusetts since August 2012.\nThe Massachusetts facility has partnered with Galena Biopharma to develop companion diagnostics for a breast cancer vaccine.\n\nLeica Biosystems acquired Aperio, an ePathology solutions company, in October 2012. In October 2014, the company announced they were purchasing Devicor Medical Products, which is headquartered in Cincinnati and makes medical devices used in breast biopsies.\n\nThe company partners with Mayo Clinic’s Department of Laboratory Medicine and Pathology in developing cytogenetics imaging software.\n"}
{"id": "22264758", "url": "https://en.wikipedia.org/wiki?curid=22264758", "title": "List of Compact Disc and DVD copy protection schemes", "text": "List of Compact Disc and DVD copy protection schemes\n\nThis is a list of notable copy protection schemes for CD and DVD.\n\nFor other medias, see List of Copy Protection Schemes.\n\n\n\n\n"}
{"id": "45099171", "url": "https://en.wikipedia.org/wiki?curid=45099171", "title": "MPLAB devices", "text": "MPLAB devices\n\nThe MPLAB series of devices are programmers and debuggers for Microchip PIC and dsPIC microcontrollers, developed by Microchip Technology.\n\nThe ICD family of debuggers has been produced since the release of the first Flash-based PIC microcontrollers, and the latest ICD 3 currently supports all current PIC and dsPIC devices. It is the most popular combination debugging/programming tool from Microchip.\n\nThe REAL ICE emulator is similar to the ICD, with the addition of better debugging features, and various add-on modules that expand its usage scope. The ICE is a family of discontinued in-circuit emulators for PIC and dsPIC devices, and is currently superseded by the REAL ICE.\n\nThe MPLAB ICD is the first in-circuit debugger product by Microchip, and is currently discontinued and superseded by ICD 2. The ICD connected to the engineer's PC via RS-232, and connected to the device via ICSP.\n\nThe ICD supported devices within the PIC16C and PIC16F families, and supported full speed execution, or single step interactive debugging. Only one hardware breakpoint was supported by the ICD.\n\nThe MPLAB ICD 2 is a discontinued in-circuit debugger and programmer by Microchip, and is currently superseded by ICD 3. The ICD 2 connects to the engineer's PC via USB or RS-232, and connects to the device via ICSP.\n\nThe ICD 2 supports most PIC and dsPIC devices within the PIC10, PIC12, PIC16, PIC18, dsPIC, rfPIC and PIC32 families, and supports full speed execution, or single step interactive debugging. At breakpoints, data and program memory can be read and modified using the MPLAB IDE. The ICD 2 firmware is field upgradeable using the MPLAB IDE.\n\nThe ICD 2 can be used to erase, program or reprogram PIC MCU program memory, while the device is installed on target hardware, using ICSP. Target device voltages from 2.0V to 6.0V are supported.\n\nThe MPLAB ICD 3 is an in-circuit debugger and programmer by Microchip, and is the latest in the ICD series. The ICD 3 connects to the engineer's PC via USB, and connects to the device via ICSP. The ICD 3 is entirely USB-bus-powered, and is 15x faster than the ICD 2 for programming devices.\n\nThe ICD 3 supports all current PIC and dsPIC devices within the PIC10, PIC12, PIC16, PIC18, dsPIC, rfPIC and PIC32 families, and supports full speed execution, or single step interactive debugging. At breakpoints, data and program memory can be read and modified using the MPLAB IDE. The ICD 3 firmware is field upgradeable using the MPLAB IDE.\n\nThe ICD 3 can be used to erase, program or reprogram PIC MCU program memory, while the device is installed on target hardware, using ICSP. Target device voltages from 2.0V to 5.5V are supported.\n\nThe ICD 3 has over-voltage protection in the probe drivers to guard against power surges from the target. All lines have over-current protection. The ICD 3 can also provide power to a target, up to 100 mA.\n\nThe MPLAB REAL ICE (In-Circuit Emulator) is a high-speed emulator for Microchip devices. It debugs and programs PIC and dsPIC microcontrollers in conjunction with the MPLAB IDE, while the target device is \"in-circuit\". The REAL ICE is significantly faster than the ICD 2, for programming and debugging.\n\nThe REAL ICE connects to the engineer's PC via a USB 2.0 interface, and connects to the target device via ICSP (PGC/PGD programming pins), typically using a RJ11 connector. LVDS is also available for high-speed data transfer between the device and the REAL ICE. MPLAB REAL ICE is field upgradeable through firmware downloads in MPLAB IDE.\n\nThe REAL ICE supports 8-bit devices (PIC10, PIC12, PIC16, PIC18), 16-bit devices (PIC24, dsPIC) and 32-bit devices (PIC32MX).\n\nThe REAL ICE Performance Pak is an optional add-on to the REAL ICE, that consists of a High Speed Probe Driver and Receiver that employ two CAT5 cables. Debug pins are driven using LVDS communications, and the additional trace connections allow high speed serial trace uploads to the PC.\n\nThe REAL ICE Isolator is an optional add-on to the REAL ICE, that enables connectivity to AC and High-voltage applications not referenced to ground. Control signals are magnetically or optically isolated providing up to 2.5 kV equivalent isolation protection. The isolator acts as an isolated bridge, where signals are passed through with complete transparency to the MPLAB REAL ICE or MPLAB IDE.\n\nThe MPLAB ICE2000 is a discontinued in-circuit emulator for PIC and dsPIC devices. It has been superseded by the REAL ICE.\n\nThe ICE2000 connects to the engineer's PC via a parallel port interface, and a USB converter is available. The ICE2000 requires emulator modules, and the test hardware must provide a socket which can take either an emulator module, or a production device.\n\nThe MPLAB ICE4000 is a discontinued in-circuit emulator for PIC and dsPIC devices. It has been superseded by the REAL ICE. The ICE4000 is no longer directly advertised on Microchip's website, and Microchip states that it is not recommended for new designs.\n\nThe ICE4000 connects to the engineer's PC via a USB 2.0 interface. PIC devices under debug with the ICE4000 ran at full speed, and the emulator supported unlimited breakpoints, and complex break/trigger logic. The emulator supported multiple external inputs and external outputs to sync with other instruments.\n"}
{"id": "2574219", "url": "https://en.wikipedia.org/wiki?curid=2574219", "title": "Mandy Haberman", "text": "Mandy Haberman\n\nMandy Nicola Haberman (born 1956) is an English inventor and entrepreneur. She is a visiting Fellow at Bournemouth University, from where she has an honorary doctorate. She is known for inventing the Haberman Feeder and the Anywayup Cup.\n\nHaberman's daughter was born in 1980 with Stickler syndrome, a congenital abnormality that included a cleft palate.\n\nOut of necessity, she invented the Haberman Feeder special bottle for infants with sucking difficulties, now used in hospitals throughout the world. Her second invention, the Anywayup Cup, is available on the commercial market, selling 10 million cups per year. The Anywayup Cup has received numerous awards for both innovation and design.\n\nHaberman has become \"a campaigner for improvements in the patent system,\" and \"sits on many committees, including the IP Court User Advisory Committee.\"\n\nHaberman was the British Female Inventor and Innovative Network (BFIIN) Female Inventor of the Year 2000. She won the Design Effectiveness Awards 2000.\n\n"}
{"id": "6910999", "url": "https://en.wikipedia.org/wiki?curid=6910999", "title": "Millettia pinnata", "text": "Millettia pinnata\n\nPongamia pinnata (L.) Pierre is a species of tree in the pea family, Fabaceae, native in tropical and temperate Asia including parts of Indian subcontinent, China, Japan, Malaysia, Australia and Pacific islands. It is often known by the synonym Pongamia pinnata as it was moved to the genus \"Millettia\" only recently. Common names include Indian beech, Pongam oiltree, karanj (Hindi), honge/karajata (ಹೊಂಗೆ/ಕರಜಾತ in Kannada), pungai (புங்கை in Tamil), kānuga (కానుగ in Telugu), karach (করচ গাছ in Bengali), naktamāla (नक्तमाल in Sanskrit), Magul karanda (මැගුල් කරන්ද in Sinhala), Sukh Chain (سکھ چین in Urdu).\n\n\"Pongamia pinnata (L.) Pierre\" is a legume tree that grows to about in height with a large canopy which spreads equally wide. It may be deciduous for short periods. It has a straight or crooked trunk, in diameter, with grey-brown bark which is smooth or vertically fissured. Branches are glabrous with pale stipulate scars. The imparipinnate leaves of the tree alternate and are short-stalked, rounded or cuneate at the base, ovate or oblong along the length, obtuse-acuminate at the apex, and not toothed on the edges. They are a soft, shiny burgundy when young and mature to a glossy, deep green as the season progresses with prominent veins underneath.\n\nFlowering generally starts after 3–4 years with small clusters of white, purple, and pink flowers blossoming throughout the year. The raceme-like inflorescence bear two to four flowers which are strongly fragrant and grow to be long. The calyx of the flowers is bell-shaped and truncate, while the corolla is a rounded ovate shape with basal auricles and often with a central blotch of green color.\n\nCroppings of indehiscent pods can occur by 4–6 years. The brown seed pods appear immediately after flowering and mature in 10 to 11 months. The pods are thick-walled, smooth, somewhat flattened and elliptical, but slightly curved with a short, curved point. The pods contain within them one or two bean-like brownish-red seeds, but because they do not split open naturally the pods need to decompose before the seeds can germinate. The seeds are about long with a brittle, oily coat and are unpalatable to herbivores.\n\nNaturally distributed in tropical and temperate Asia, from India to Japan to Thailand to Malesia to north and north-eastern Australia to some Pacific islands; It has been propagated and distributed further around the world in humid and subtropical environments from sea-level to 1200m, although in the Himalayan foothills it is not found above 600m. Withstanding temperatures slightly below 0 °C (32 °F) and up to about 50 °C (120 °F) and annual rainfall of 500–2,500 mm (20–100 in), the tree grows wild on sandy and rocky soils, including oolitic limestone, and will grow in most soil types, even with its roots in salt water.\n\nThe tree is well suited to intense heat and sunlight and its dense network of lateral roots and its thick, long taproot make it drought-tolerant. The dense shade it provides slows the evaporation of surface water and its root nodules promote nitrogen fixation, a symbiotic process by which gaseous nitrogen (N) from the air is converted into ammonium (NH, a form of nitrogen available to the plant). M. pinnata is also a fresh water flooded forest species as it can survive total submergence in sweet water for few months continuously. M. pinnata tree is the pioneer tree in Ratargul fresh water flooded forest in Bangladesh and Tonlesap lake swamp forests in Cambodia\n\n\"Millettia pinnata\" is an outbreeding diploid legume tree, with a diploid chromosome number of 22. Root nodules are of the determinate type (as those on soybean and common bean) formed by the causative bacterium \"Bradyrhizobium\".\n\n\"Pongamia pinnata (L.) Pierre\" is well-adapted to arid zones and has many traditional uses. It is often used for landscaping purposes as a windbreak or for shade due to the large canopy and showy fragrant flowers. The flowers are used by gardeners as compost for plants requiring rich nutrients. The bark can be used to make twine or rope and it also yields a black gum that has historically been used to treat wounds caused by poisonous fish. The wood is said to be beautifully grained but splits easily when sawn thus relegating it to firewood, posts, and tool handles.\n\nWhile the oil and residue of the plant are toxic and will induce nausea and vomiting if ingested, the fruits and sprouts, along with the seeds, are used in many traditional remedies. Juices from the plant, as well as the oil, are antiseptic and resistant to pests. In addition \"M. pinnata\" has the rare property of producing seeds of 25–40% lipid content of which nearly half is oleic acid. Oil made from the seeds, known as pongamia oil, is an important asset of this tree and has been used as lamp oil, in soap making, and as a lubricant for thousands of years. The oil has a high content of triglycerides, and its disagreeable taste and odor are due to bitter flavonoid constituents including karanjin, pongamol, tannin and karanjachromene. It can be grown in rainwater harvesting ponds up to in water depth without losing its greenery and remaining useful for biodiesel production.\n\nThe residue of oil extraction, called press cake, is used as a fertilizer and as animal feed for ruminants and poultry.\n\nLong used as shade tree, \"M. pinnata\" is heavily self-seeding and can spread lateral roots up to over its lifetime. If not managed carefully it can quickly become a weed leading some, including Miami-Dade County, to label the tree as an invasive species. However this dense network of lateral roots makes this tree ideal for controlling soil erosion and binding sand dunes.\n\nThe seed oil has been found to be useful in diesel generators and, along with \"Jatropha\" and Castor, it is being explored in hundreds of projects throughout India and the third world as feedstock for biodiesel. It is especially attractive because it grows naturally through much of arid India, having very deep roots to reach water, and is one of the few crops well-suited to commercialization by India's large population of rural poor. Several unelectrified villages have recently used pongamia oil, simple processing techniques, and diesel generators to create their own grid systems to run water pumps and electric lighting.\n\nIn 1997, the Indian Institute of Science started researching and promoting the use of the seed oil as a vegetable oil fuel for stationary generators for electricity and irrigation pumps in the rural areas of Karnataka and Andhra. The program, SuTRA, successfully demonstrated the sustainability of such oil use in several villages all over India.\n\nIn 2003, the Himalayan Institute of Yoga Science and Philosophy as part of its Biofuel Rural Development Initiative started a campaign of education and public awareness to rural farmers about \"M. pinnata\" in two Indian states. One of the Himalayan Institute's partners developed a consistently high yield scion that reduced the time it takes to mature from 10 years to as little as three. To help the farmers in the transition from traditional crops to \"M. pinnata\" the Indian government has contributed over $30 million in low-interest loans and donated 4.5 million kg (5,000 short tons) of rice to sustain impoverished drought-stricken farmers until the trees begin to produce income. Since the project began in 2003 over 20 million trees have been planted and 45,000 farmers are now involved.\nIn 2006, the Himalayan Institute began looking at locations in Africa to transplant \"M. pinnata\" into. Initially they began in Uganda but due to the lack of infrastructure and growing desertification the project has been growing very slowly. They have also begun a project in the Kumbo region of Cameroon where conditions are better. There has been some suggestions that \"M. pinnata\" could be grown all the way across the continent as a way to prevent the encroachment of the Sahara.\n\nThe University of Queensland node of the Australian Research Council Center for Excellence in Legume Research, under the directorship of Professor Peter Gresshoff, in conjunction with Pacific Renewable Energy are currently working on \"M. pinnata\" for commercial use for the production of biofuel. Projects are currently focused on understanding aspects of \"M. pinnata\" including root biology, nodulation, nitrogen fixation, domestication genes, grafting, salinity tolerance, and the genetics of the oil production pathways. Emphasis is given to analyzing carbon sequestration (in relation to carbon credits) and nitrogen gain.\n\nResearch has also been put into using the material left over from the oil extraction as a feed supplement for cattle, sheep and poultry as this byproduct contains up to 30% protein. Other studies have shown some potential for biocidal activity against \"V. cholerae\" and \"E. coli\", as well an anti-inflammatory, antinociceptive (reduction in sensitivity to painful stimuli) and antipyretic (reduction in fever) properties. There is also research indicating that \"M. pinnata\" can be used as a natural insecticide.\n\n\n"}
{"id": "24603285", "url": "https://en.wikipedia.org/wiki?curid=24603285", "title": "Molten salt oxidation", "text": "Molten salt oxidation\n\nMolten salt oxidation is a non-flame, thermal process that destroys all organic materials while simultaneously retaining inorganic and hazardous components in the melt. It is used as either hazardous waste treatment(with air) or energy harvesting similar to coal and wood gasification(with steam).\nThe molten salt of choice has been sodium carbonate (m.p 851°C), but other salts can be used. Sulfur, halogens, phosphorus and similar volatile pollutants are oxidized and retained in the melt. Most organic carbon content leaves as relatively pure CO//H/HO gas (depending on the feed conditions, whether steam or air is used), and the effluent only requires a cold trap and a mild aqueous wash (except mercury-containing wastes). It has been used for safe biological and chemical weapons destruction, and processing waste such as scrap tires where direct incineration/effluent treatment is difficult.\nThe major downside of the process compared to direct incineration is the eventual saturation of the melt by contaminants, and needing reprocessing/replacement.\n\n"}
{"id": "30140888", "url": "https://en.wikipedia.org/wiki?curid=30140888", "title": "Noise-domain reflectometry", "text": "Noise-domain reflectometry\n\nNoise-domain reflectometry is a type of reflectometry where the reflectometer exploits existing data signals on wiring and does not have to generate any signals itself. Noise-domain reflectometry, like time-domain and spread-spectrum time domain reflectometers, is most often used in identifying the location of wire faults in electrical lines.\n\nTime-domain reflectometers work by generating a signal and then sending that signal down the wireline and examining the reflected signal. Noise-domain reflectometers (NDRs) provide the benefit of locating wire faults without introducing an external signal because the NDR examines the existing signals on the line to identify wire faults. This technique is particularly useful in the testing of live wires where data integrity on the wires is critical. For example, NDRs can be used for monitoring aircraft wiring while in flight.\n\n"}
{"id": "3185688", "url": "https://en.wikipedia.org/wiki?curid=3185688", "title": "Nuclear reactor physics", "text": "Nuclear reactor physics\n\nNuclear reactor physics is the branch of science that deals with the study and application of chain reaction to induce a controlled rate of fission in a nuclear reactor for the production of energy.\nMost nuclear reactors use a chain reaction to induce a controlled rate of nuclear fission in fissile material, releasing both energy and free neutrons. A reactor consists of an assembly of nuclear fuel (a reactor core), usually surrounded by a neutron moderator such as regular water, heavy water, graphite, or zirconium hydride, and fitted with mechanisms such as control rods that control the rate of the reaction. \n\nThe physics of nuclear fission has several quirks that affect the design and behavior of nuclear reactors. This article presents a general overview of the physics of nuclear reactors and their behavior.\n\nIn a nuclear reactor, the neutron population at any instant is a function of the rate of neutron production (due to fission processes) and the rate of neutron losses (due to non-fission absorption mechanisms and leakage from the system). When a reactor’s neutron population remains steady from one generation to the next (creating as many new neutrons as are lost), the fission chain reaction is self-sustaining and the reactor's condition is referred to as \"critical\". When the reactor’s neutron production exceeds losses, characterized by increasing power level, it is considered \"supercritical\", and when losses dominate, it is considered \"subcritical\" and exhibits decreasing power.\n\nThe \"Six-factor formula\" is the neutron life-cycle balance equation, which includes six separate factors, the product of which is equal to the ratio of the number of neutrons in any generation to that of the previous one; this parameter is called the effective multiplication factor k, also denoted by K, where k = Є \"L\" ρ \"L\" \"f\" η, where Є = \"fast-fission factor\", \"L\" = \"fast non-leakage factor\", ρ = \"resonance escape probability\", \"L\" = \"thermal non-leakage factor\", \"f\" = \"thermal fuel utilization factor\", and η = \"reproduction factor\". This equation's factors are roughly in order of potential occurrence for a fission born neutron during critical operation. As already mentioned before, k = (Neutrons produced in one generation)/(Neutrons produced in the previous generation). In other words, when the reactor is critical, k = 1; when the reactor is subcritical, k < 1; and when the reactor is supercritical, k > 1. \n\n\"Reactivity\" is an expression of the departure from criticality. δk = (k − 1)/k. When the reactor is critical, δk = 0. When the reactor is subcritical, δk < 0. When the reactor is supercritical, δk > 0. Reactivity is also represented by the lowercase Greek letter rho (ρ). Reactivity is commonly expressed in decimals or percentages or pcm (per cent mille) of Δk/k. When reactivity ρ is expressed in units of delayed neutron fraction β, the unit is called the \"dollar\".\n\nIf we write 'N' for the number of free neutrons in a reactor core and formula_1 for the average lifetime of each neutron (before it either escapes from the core or is absorbed by a nucleus), then the reactor will follow the differential equation (\"evolution equation\")\n\nwhere formula_3 is a constant of proportionality, and formula_4 is the rate of change of the neutron count in the core. This type of differential equation describes exponential growth or exponential decay, depending on the sign of the constant formula_3, which is just the expected number of neutrons after one average neutron lifetime has elapsed:\n\nHere, formula_7 is the probability that a particular neutron will strike a fuel nucleus, formula_8 is the probability that the neutron, having struck the fuel, will cause that nucleus to undergo fission, formula_9 is the probability that it will be absorbed by something other than fuel, and formula_10 is the probability that it will \"escape\" by leaving the core altogether. formula_11 is the number of neutrons produced, on average, by a fission event—it is between 2 and 3 for both U and Pu.\n\nIf formula_3 is positive, then the core is \"supercritical\" and the rate of neutron production will grow exponentially until some other effect stops the growth. If formula_3 is negative, then the core is \"subcritical\" and the number of free neutrons in the core will shrink exponentially until it reaches an equilibrium at zero (or the background level from spontaneous fission). If formula_3 is exactly zero, then the reactor is \"critical\" and its output does not vary in time (formula_15, from above). \n\nNuclear reactors are engineered to reduce formula_10 and formula_9. Small, compact structures reduce the probability of direct escape by minimizing the surface area of the core, and some materials (such as graphite) can reflect some neutrons back into the core, further reducing formula_10. \n\nThe probability of fission, formula_8, depends on the nuclear physics of the fuel, and is often expressed as a cross section. Reactors are usually controlled by adjusting formula_9. Control rods made of a strongly neutron-absorbent material such as cadmium or boron can be inserted into the core: any neutron that happens to impact the control rod is lost from the chain reaction, reducing formula_3. formula_9 is also controlled by the recent history of the reactor core itself (see below).\n\nThe mere fact that an assembly is supercritical does not guarantee that it contains any free neutrons at all. At least one neutron is required to \"strike\" a chain reaction, and if the spontaneous fission rate is sufficiently low it may take a long time (in U reactors, as long as many minutes) before a chance neutron encounter starts a chain reaction even if the reactor is supercritical. Most nuclear reactors include a \"starter\" neutron source that ensures there are always a few free neutrons in the reactor core, so that a chain reaction will begin immediately when the core is made critical. A common type of startup neutron source is a mixture of an alpha particle emitter such as Am (americium-241) with a lightweight isotope such as Be (beryllium-9).\n\nThe primary sources described above have to be used with fresh reactor cores. For operational reactors, secondary sources are used; most often a combination of antimony with beryllium. Antimony becomes activated in the reactor and produces high-energy gamma photons, which produce photoneutrons from beryllium.\n\nUranium-235 undergoes a small rate of natural spontaneous fission, so there are always some neutrons being produced even in a fully shutdown reactor. When the control rods are withdrawn and criticality is approached the number increases because the absorption of neutrons is being progressively reduced, until at criticality the chain reaction becomes self-sustaining. Note that while a neutron source is provided in the reactor, this is not essential to start the chain reaction, its main purpose is to give a shutdown neutron population which is detectable by instruments and so make the approach to critical more observable. The reactor will go critical at the same control rod position whether a source is loaded or not.\n\nOnce the chain reaction is begun, the primary starter source may be removed from the core to prevent damage from the high neutron flux in the operating reactor core; the secondary sources usually remains in situ to provide a background reference level for control of criticality.\n\nEven in a subcritical assembly such as a shut-down reactor core, any stray neutron that happens to be present in the core (for example from spontaneous fission of the fuel, from radioactive decay of fission products, or from a neutron source) will trigger an exponentially decaying chain reaction. Although the chain reaction is not self-sustaining, it acts as a multiplier that increases the equilibrium number of neutrons in the core. This \"subcritical multiplication\" effect can be used in two ways: as a probe of how close a core is to criticality, and as a way to generate fission power without the risks associated with a critical mass. \n\nIf formula_23 is the neutron multiplication factor of a subcritical core and formula_24 is the number of neutrons coming per generation in the reactor from an external source, then at the instant when the neutron source is switched on, number of neutrons in the core will be formula_24. After 1 generation, this neutrons will produce formula_26 neutrons in the reactor and reactor will have a totality of formula_27 neutrons considering the newly entered neutrons in the reactor. Similarly after 2 generation, number of neutrons produced in the reactor will be formula_28 and so on. This process will continue and after a long enough time, the number of neutrons in the reactor will be,\n\nThis series will converge because for the subcritical core, formula_30. So the number of neutrons in the reactor will be simply,\n\nThe fraction formula_32 is called subcritical multiplication factor.\n\nSince power in a reactor is proportional to the number of neutrons present in the nuclear fuel material (material in which fission can occur), the power produced by such a subcritical core will also be proportional to the subcritical multiplication factor and the external source strength.\n\nAs a measurement technique, subcritical multiplication was used during the Manhattan Project in early experiments to determine the minimum critical masses of U and of Pu. It is still used today to calibrate the controls for nuclear reactors during startup, as many effects (discussed in the following sections) can change the required control settings to achieve criticality in a reactor. As a power-generating technique, subcritical multiplication allows generation of nuclear power for fission where a critical assembly is undesirable for safety or other reasons. A subcritical assembly together with a neutron source can serve as a steady source of heat to generate power from fission. \n\nIncluding the effect of an external neutron source (\"external\" to the fission process, not physically external to the core), one can write a modified evolution equation:\n\nwhere formula_34 is the rate at which the external source injects neutrons into the core. In equilibrium, the core is not changing and dN/dt is zero, so the equilibrium number of neutrons is given by:\n\nIf the core is subcritical, then formula_3 is negative so there is an equilibrium with a positive number of neutrons. If the core is close to criticality, then formula_3 is very small and thus the final number of neutrons can be made arbitrarily large.\n\nTo improve formula_8 and enable a chain reaction, uranium-fueled reactors must include a neutron moderator that interacts with newly produced fast neutrons from fission events to reduce their kinetic energy from several MeV to thermal energies of less than one eV, making them more likely to induce fission. This is because U is much more likely to undergo fission when struck by one of these thermal neutrons than by a freshly produced neutron from fission.\n\nNeutron moderators are thus materials that slow down neutrons. Neutrons are most effectively slowed by colliding with the nucleus of a light atom, hydrogen being the lightest of all. To be effective, moderator materials must thus contain light elements with atomic nuclei that tend to scatter neutrons on impact rather than absorb them. In addition to hydrogen, beryllium and carbon atoms are also suited to the job of moderating or slowing down neutrons.\n\nHydrogen moderators include water (HO), heavy water(DO), and zirconium hydride (ZrH), all of which work because a hydrogen nucleus has nearly the same mass as a free neutron: neutron-HO or neutron-ZrH impacts excite rotational modes of the molecules (spinning them around). Deuterium nuclei (in heavy water) absorb kinetic energy less well than do light hydrogen nuclei, but they are much less likely to absorb the impacting neutron. Water or heavy water have the advantage of being transparent liquids, so that, in addition to shielding and moderating a reactor core, they permit direct viewing of the core in operation and can also serve as a working fluid for heat transfer. \n\nCarbon in the form of graphite has been widely used as a moderator. It was used in Chicago Pile-1, the world's first man-made critical assembly, and was commonplace in early reactor designs including the Soviet RBMK nuclear power plants such as the Chernobyl plant.\n\nThe amount and nature of neutron moderation affects reactor controllability and hence safety. Because moderators both slow and absorb neutrons, there is an optimum amount of moderator to include in a given geometry of reactor core. Less moderation reduces the effectiveness by reducing the formula_8 term in the evolution equation, and more moderation reduces the effectiveness by increasing the formula_10 term. \n\nMost moderators become less effective with increasing temperature, so \"under-moderated\" reactors are stable against changes in temperature in the reactor core: if the core overheats, then the quality of the moderator is reduced and the reaction tends to slow down (there is a \"negative temperature coefficient\" in the reactivity of the core). Water is an extreme case: in extreme heat, it can boil, producing effective voids in the reactor core without destroying the physical structure of the core; this tends to shut down the reaction and reduce the possibility of a fuel meltdown. \"Over-moderated\" reactors are unstable against changes in temperature (there is a \"positive temperature coefficient\" in the reactivity of the core), and so are less inherently safe than under-moderated cores.\n\nSome reactors use a combination of moderator materials. For example, TRIGA type research reactors use ZrH moderator mixed with the U fuel, an HO-filled core, and C (graphite) moderator and reflector blocks around the periphery of the core.\n\nFission reactions and subsequent neutron escape happen very quickly; this is important for nuclear weapons, where the objective is to make a nuclear core release as much energy as possible before it physically explodes. Most neutrons emitted by fission events are prompt: they are emitted effectively instantaneously. Once emitted, the average neutron lifetime (formula_1) in a typical core is on the order of a millisecond, so if the exponential factor formula_3 is as small as 0.01, then in one second the reactor power will vary by a factor of (1 + 0.01), or more than ten thousand. Nuclear weapons are engineered to maximize the power growth rate, with lifetimes well under a millisecond and exponential factors close to 2; but such rapid variation would render it practically impossible to control the reaction rates in a nuclear reactor.\n\nFortunately, the \"effective\" neutron lifetime is much longer than the average lifetime of a single neutron in the core. About 0.65% of the neutrons produced by U fission, and about 0.20% of the neutrons produced by Pu fission, are not produced immediately, but rather are emitted from an excited nucleus after a further decay step. In this step, further radioactive decay of some of the fission products (almost always negative beta decay), is followed by immediate neutron emission from the excited daughter product, with an average life time of the beta decay (and thus the neutron emission) of about 15 seconds. These so-called delayed neutrons increase the effective average lifetime of neutrons in the core, to nearly 0.1 seconds, so that a core with formula_3 of 0.01 would increase in one second by only a factor of (1 + 0.01), or about 1.1: a 10% increase. This is a controllable rate of change.\n\nMost nuclear reactors are hence operated in a \"prompt subcritical\", \"delayed critical\" condition: the prompt neutrons alone are not sufficient to sustain a chain reaction, but the delayed neutrons make up the small difference required to keep the reaction going. This has effects on how reactors are controlled: when a small amount of control rod is slid into or out of the reactor core, the power level changes at first very rapidly due to \"prompt subcritical multiplication\" and then more gradually, following the exponential growth or decay curve of the delayed critical reaction. Furthermore, \"increases\" in reactor power can be performed at any desired rate simply by pulling out a sufficient length of control rod. However, without addition of a neutron poison or active neutron-absorber, \"decreases\" in fission rate are limited in speed, because even if the reactor is taken deeply subcritical to stop prompt fission neutron production, delayed neutrons are produced after ordinary beta decay of fission products already in place, and this decay-production of neutrons cannot be changed.\n\nThe kinetics of the reactor is described by the balance equations of neutrons and nuclei (fissile, fission products).\n\nAny element that strongly absorbs neutrons is called a reactor poison, because it tends to shut down (poison) an ongoing fission chain reaction. Some reactor poisons are deliberately inserted into fission reactor cores to control the reaction; boron or cadmium control rods are the best example. Many reactor poisons are produced by the fission process itself, and buildup of neutron-absorbing fission products affects both the fuel economics and the controllability of nuclear reactors.\n\nIn practice, buildup of reactor poisons in nuclear fuel is what determines the lifetime of nuclear fuel in a reactor: long before all possible fissions have taken place, buildup of long-lived neutron absorbing fission products damps out the chain reaction. This is the reason that nuclear reprocessing is a useful activity: spent nuclear fuel contains about 96% of the original fissionable material present in newly manufactured nuclear fuel. Chemical separation of the fission products restores the nuclear fuel so that it can be used again. \n\nNuclear reprocessing is useful economically because chemical separation is much simpler to accomplish than the difficult isotope separation required to prepare nuclear fuel from natural uranium ore, so that in principle chemical separation yields more generated energy for less effort than mining, purifying, and isotopically separating new uranium ore. In practice, both the difficulty of handling the highly radioactive fission products and other political concerns make fuel reprocessing a contentious subject. One such concern is the fact that spent uranium nuclear fuel contains significant quantities of Pu, a prime ingredient in nuclear weapons (see breeder reactor).\n\nShort-lived reactor poisons in fission products strongly affect how nuclear reactors can operate. Unstable fission product nuclei transmute into many different elements (\"secondary fission products\") as they undergo a decay chain to a stable isotope. The most important such element is xenon, because the isotope Xe, a secondary fission product with a half-life of about 9 hours, is an extremely strong neutron absorber. In an operating reactor, each nucleus of Xe becomes Xe (which may later sustain beta decay) by neutron capture almost as soon as it is created, so that there is no buildup in the core. However, when a reactor shuts down, the level of Xe builds up in the core for about 9 hours before beginning to decay. The result is that, about 6–8 hours after a reactor is shut down, it can become physically impossible to restart the chain reaction until the Xe has had a chance to decay over the next several hours. This temporary state, which may last several days and prevent restart, is called the iodine pit or xenon-poisoning. It is one reason why nuclear power reactors are usually operated at an even power level around the clock.\n\nXe buildup in a reactor core makes it extremely dangerous to operate the reactor a few hours after it has been shut down. Because the Xe absorbs neutrons strongly, starting a reactor in a high-Xe condition requires pulling the control rods out of the core much farther than normal. However, if the reactor does achieve criticality, then the neutron flux in the core becomes high and Xe is destroyed rapidly—this has the same effect as very rapidly removing a great length of control rod from the core, and can cause the reaction to grow too rapidly or even become prompt critical.\n\nXe played a large part in the Chernobyl accident: about eight hours after a scheduled maintenance shutdown, workers tried to bring the reactor to a zero power critical condition to test a control circuit. Since the core was loaded with Xe from the previous day's power generation, it was necessary to withdraw more control rods to achieve this. As a result, the overdriven reaction grew rapidly and uncontrollably, leading to steam explosion in the core, and violent destruction of the facility.\n\nWhile many fissionable isotopes exist in nature, the only usefully fissile isotope found in any quantity is U. About 0.7% of the uranium in most ores is the 235 isotope, and about 99.3% is the non-fissile 238 isotope. For most uses as a nuclear fuel, uranium must be \"enriched\" - purified so that it contains a higher percentage of U. Because U absorbs fast neutrons, the critical mass needed to sustain a chain reaction increases as the U content increases, reaching infinity at 94% U (6% U).\nConcentrations lower than 6% U cannot go fast critical, though they are usable in a nuclear reactor with a neutron moderator.\nA nuclear weapon primary stage using uranium uses HEU enriched to ~90% U, though the secondary stage often uses lower enrichments. Nuclear reactors with water moderator require at least some enrichment of U. Nuclear reactors with heavy water moderation can operate with natural uranium, eliminating altogether the need for enrichment and preventing the fuel from being useful for nuclear weapons; the CANDU power reactors used in Canadian power plants are an example of this type. \n\nUranium enrichment is difficult because the chemical properties of U and U are identical, so physical processes such as gaseous diffusion, gas centrifuge or mass spectrometry must be used for isotopic separation based on small differences in mass. Because enrichment is the main technical hurdle to production of nuclear fuel and simple nuclear weapons, enrichment technology is politically sensitive.\n\nModern deposits of uranium contain only up to ~0.7% U (and ~99.3% U), which is not enough to sustain a chain reaction moderated by ordinary water. But U has a much shorter half-life (700 million years) than U (4.5 billion years), so in the distant past the percentage of U was much higher. About two billion years ago, a water-saturated uranium deposit (in what is now the Oklo mine in Gabon, West Africa) underwent a naturally occurring chain reaction that was moderated by groundwater and, presumably, controlled by the negative void coefficient as the water boiled from the heat of the reaction. Uranium from the Oklo mine is about 50% depleted compared to other locations: it is only about 0.3% to 0.7% U; and the ore contains traces of stable daughters of long-decayed fission products.\n\n\nFermi age theory\n\nNotes on nuclear diffusion by Dr. Abdelhamid Dokhane\n"}
{"id": "51726771", "url": "https://en.wikipedia.org/wiki?curid=51726771", "title": "Paper-ruling machine", "text": "Paper-ruling machine\n\nA paper-ruling machine is a device developed by William Orville Hickok in the second half of the 19th century for ruling paper. As the device is designed for drawing lines on paper, it can produce tables and ruled paper.\n\nThe functionality of the machine is based on pens manufactured especially for the device. The pens have multiple tips side by side, and water-based ink is led into them along threads. It is possible to program stop-lines on the equipment by mounting pens on shafts equipped with cams that lower and raise them at predetermined points.\n\nThe spread of computerized accounting between the 1960s and 1980s significantly decreased the demand for accounting tables and ruled paper. Nowadays, their demand is primarily filled by using offset printing.\n\n"}
{"id": "8039576", "url": "https://en.wikipedia.org/wiki?curid=8039576", "title": "Patio heater", "text": "Patio heater\n\nA patio heater (also called a mushroom or umbrella heater) is a radiant heating appliance for generating thermal radiation for outdoor use.\n\nA burner on top of a pole, it burns liquefied petroleum gas (LPG), propane or butane, and directs the flames against a perforated metal screen. Heat is radiated from the surface of the screen in a circular pattern around the appliance. A reflector a top the burner reflects heat that would be otherwise lost upwards. This is because the reflecting hood is usually silvered which makes it a poor absorber of heat but excellent at reflecting infra-red radiation back. This reduces the amount of heat lost by conduction as silvered surfaces will not absorb infra-red light. The chimenea is an alternative to the patio heater for home use, which burns wood instead of gas.\n\nSome newer types of patio heaters are electrically powered radiative heaters that emit infrared energy onto nearby surfaces, which in turn heat up the surrounding air.\n\nOther styles of outdoor patio heaters include:\n\nPatio heaters have become popular with bars and restaurants since they extend the day and the season for their customers to sit outdoors. This increase in the popularity of the patio heater has led to concerns over their environmental effects. One patio heater can produce four tons of carbon dioxide annually.\n\nPropane patio heaters are the most popular type as they are portable and easy to find refill locations such as gas stations or convenience stores. The downside to propane is that you need to purchase a separate tank for each heater you own and can be more costly to operate than electric or natural gas.\n\nNatural gas patio heaters are great as more and more houses come outfitted with natural gas lines. This makes it very convenient to hook into, however it is less portable than propane. Extension hoses are available, but can be a tripping hazard especially after the sun has gone down.\n\nElectric patio heaters are a great choice for easy setup, and has indoor applications as well, for partially enclosed indoor-outdoor space. Electric heaters are typically generally a bit weaker, therefore it’s best for a small group of people.\n\n"}
{"id": "1303155", "url": "https://en.wikipedia.org/wiki?curid=1303155", "title": "Plus (interbank network)", "text": "Plus (interbank network)\n\nPlus System, Inc. (also known as Visa Plus or simply Plus) is a Denver-based ATM network that provides cash to Visa cardholders. As a subsidiary of Visa Inc., Plus System connects all Visa credit, debit and prepaid cards, as well as ATM cards issued by various banks worldwide bearing the Visa logo.\n\nPlus System started out as a consortium formed by 34 major U.S. banks to build a national network of automated teller machines (ATM). It initially was composed of 2,000 ATMs linking 1,000 banks and their customers in 47 states.\nAs the booming ATM industry outgrew regional networks and began to go nationwide in the mid-1980s, credit-card giant Visa sought entry in the lucrative ATM network business and acquired a third of Plus System in 1987. Currently, there are over one million Plus-linked ATMs in 170 countries worldwide.\n\nPlus cards can be linked in the following ways: as a standalone network, linked with a local interbank network or linked with any Visa product displaying the Visa flag on the front (Visa, Visa Debit and Visa Electron). Currently, there are 144 million proprietary Plus cards, not including a number of cards which have Plus as a secondary network.\n\nPlus is widely used as a local interbank network most common in the United States where networks such as STAR, NYCE and Pulse also compete. It is also used in Canada, though it is significantly smaller than Interac there, and in places such as India and Indonesia where there are many interbank networks. The main competitor of Plus System is the Cirrus network, which is owned by Visa longtime rival MasterCard.\n\n"}
{"id": "21153971", "url": "https://en.wikipedia.org/wiki?curid=21153971", "title": "Potassium picrate", "text": "Potassium picrate\n\nPotassium picrate, or potassium 2,4,6-trinitrophenolate, is an organic chemical, a picrate of potassium. It is a reddish yellow or green crystalline material. It is a primary explosive. Anhydrous potassium picrate forms orthorhombic crystals.\n\nPotassium picrate was first prepared as impure in mid-17th century by Johann Rudolf Glauber by dissolving wood in nitric acid and neutralizing with potassium carbonate. It is commonly made by neutralizing picric acid by potassium carbonate. It was used since 1869. Its chief applications are in pyrotechnics, in some whistle mixes, as a component of explosives (with potassium nitrate and charcoal), propellants (with potassium chlorate), and explosive primers (with lead picrate and potassium chlorate). \n\nPotassium picrate is not a very powerful explosive. It is somewhat shock-sensitive. In contact with flame it deflagrates with a loud sound. If ignited in confined space, it will detonate. It is more sensitive than picric acid. \n\nIn contact with metals (e.g. lead, calcium, iron), potassium picrate, like ammonium picrate and picric acid, forms picrates of said metals. These are often more dangerous and more sensitive explosives. Contact with such materials therefore should be prevented.\n\nPotassium picrate is used to determine the concentration of nonionic surfactants in water; materials detectable by this method are called potassium picrate active substances (PPAS).\n\nAs with other picrates, potassium picrate may be produced by the neutralization of picric acid with the corresponding carbonate.\nAs picric acid is barely soluble in water the reaction must be done in an appropriate solvent like methanol.\nFirst dissolving the picric acid in methanol and then adding potassium carbonate will result in potassium picrate. Temperature control is important to prevent detonation or excessive methanol evaporation.\n\nAccording to Urbanski, Potassium picrate detonated 10% of the time when struck by a mass of 2kg dropped from the height of 21cm.\nBy comparison, the more sensitive anhydrous lead picrate detonated 10% of the time when struck by the same mass dropped from the height of 2cm.\n\n\n"}
{"id": "24463266", "url": "https://en.wikipedia.org/wiki?curid=24463266", "title": "RTTOV (radiative transfer code)", "text": "RTTOV (radiative transfer code)\n\nRTTOV - the fast radiative transfer model for calculations of radiances for satellite infrared or microwave nadir scanning radiometers (see push broom scanner).\n\nGiven an atmospheric profile of temperature, variable gas concentrations, cloud and surface properties RTTOV calculates radiances and brightness temperatures. The only mandatory input is water vapour. Optionally ozone, carbon dioxide, nitrous oxide, methane and carbon monoxide can be variable with all other constituents assumed\nto be constant. The range of temperatures and water vapour concentrations over which the optical\ndepth computations are valid depends on the training datasets which were used. The spectral range of the RTTOV9.1 model is 3-20 micrometres (500 – 3000 cm-1) in the infrared.\n\nRTTOV contains forward, tangent linear, adjoint and K (full Jacobian matrices) versions of the model; the latter three modules for variational\nassimilation or retrieval applications.\n\nOne of several applications of RTTOV are retrievals of brightness temperature and sea surface temperature from Advanced Very High Resolution Radiometer sensor.\n\nSurface emissivity model which parameterizes surface emissivity. FASTEM2 computes the surface emissivity averaged over all facets representing the surface of the ocean and an effective path correction factor for the down-welling brightness temperature. FASTEM2 is applicable for frequencies between 10 and 220 GHz, for earth incidence angles less than 60 degrees.\n\n\n\n"}
{"id": "47113160", "url": "https://en.wikipedia.org/wiki?curid=47113160", "title": "RW3 Technologies", "text": "RW3 Technologies\n\nRW3 Technologies is a software company that provides SaaS field sales, survey, and reporting applications for the consumer packaged goods (CPG) industry. It is headquartered in the Bay Area of California.\n\nRW3 Technologies was founded in the Bay Area by Bruce Nagle in 1992. The company's primary focus was to streamline daily data entry processes for the sales industry.\n\nIn 1992, RW3 introduced one of the first Land-Line CPG broker sales systems that allowed for mobile data entry. It was initially used in food brokerage, though eventually expanded to include functionality for the consumer packaged goods industry.\n\nIn 2000, the company expanded its business model to include business-to-business account management. \n\nIn 2010, the company began development of their first general SaaS product application since the late 1990s; the SaaS application called InStore Mobile (now MarketCheck) was released in 2011. The in-store survey application allows for two-way communication between the account rep and broker, allowing organizations to improve and track retail conditions.\n\nIn 2013, RW3 released the BI Suite, a business intelligence environment that enables rganizations to create views across departments and utilize multiple data sources to align sales strategies.\n\nIn 2014 Smartcall was launched and marketed to the CPG industry. The application enables field sales reps to conduct traditional store calls and manage sales routes. It is packaged with MarketCheck into their InStore Execution Suite, providing retail execution and monitoring applications for the consumer goods industry.\n\nRW3 offers four SaaS products for the retail, wholesale, and B2B industries:\n\n\nRW3 serves three primary markets:\n\n"}
{"id": "44179", "url": "https://en.wikipedia.org/wiki?curid=44179", "title": "Satellite temperature measurements", "text": "Satellite temperature measurements\n\nSatellite temperature measurements are inferences of the temperature of the atmosphere at various altitudes as well as sea and land surface temperatures obtained from radiometric measurements by satellites. These measurements can be used to locate weather fronts, monitor the El Niño-Southern Oscillation, determine the strength of tropical cyclones, study urban heat islands and monitor the global climate. Wildfires, volcanos, and industrial hot spots can also be found via thermal imaging from weather satellites.\n\nWeather satellites do not measure temperature directly. They measure radiances in various wavelength bands. Since 1978 microwave sounding units (MSUs) on National Oceanic and Atmospheric Administration polar orbiting satellites have measured the intensity of upwelling microwave radiation from atmospheric oxygen, which is related to the temperature of broad vertical layers of the atmosphere. Measurements of infrared radiation pertaining to sea surface temperature have been collected since 1967.\n\nSatellite datasets show that over the past four decades the troposphere has warmed and the stratosphere has cooled. Both of these trends are consistent with the influence of increasing atmospheric concentrations of greenhouse gases.\n\nSatellites do not measure temperature. They measure radiances in various wavelength bands, which must then be mathematically inverted to obtain indirect inferences of temperature. The resulting temperature profiles depend on details of the methods that are used to obtain temperatures from radiances. As a result, different groups that have analyzed the satellite data have produced differing temperature datasets. Among these are the UAH dataset prepared at the University of Alabama in Huntsville and the RSS dataset prepared by Remote Sensing Systems.\n\nThe satellite time series is not homogeneous. It is constructed from a series of satellites with similar but not identical sensors. The sensors also deteriorate over time, and corrections are necessary for orbital drift and decay. Particularly large differences between reconstructed temperature series occur at the few times when there is little temporal overlap between successive satellites, making intercalibration difficult.\n\nSatellites may also be used to retrieve surface temperatures in cloud-free conditions, generally via measurement of thermal infrared from AVHRR. Weather satellites have been available to infer sea surface temperature (SST) information since 1967, with the first global composites occurring during 1970. Since 1982, satellites have been increasingly utilized to measure SST and have allowed its spatial and temporal variation to be viewed more fully. For example, changes in SST monitored via satellite have been used to document the progression of the El Niño-Southern Oscillation since the 1970s. Over the land the retrieval of temperature from radiances is harder, because of the inhomogeneities in the surface. Studies have been conducted on the urban heat island effect via satellite imagery. Use of advanced very high resolution infrared satellite imagery can be used, in the absence of cloudiness, to detect density discontinuities (weather fronts) such as cold fronts at ground level. Using the Dvorak technique, infrared satellite imagery can be used to determine the temperature difference between the eye and the cloud top temperature of the central dense overcast of mature tropical cyclones to estimate their maximum sustained winds and their minimum central pressures. Along Track Scanning Radiometers aboard weather satellites are able to detect wildfires, which show up at night as pixels with a greater temperature than . The Moderate-Resolution Imaging Spectroradiometer aboard the Terra satellite can detect thermal hot spots associated with wildfires, volcanoes, and industrial hot spots.\n\nFrom 1979 to 2005 the microwave sounding units (MSUs) and since 1998 the Advanced Microwave Sounding Units on NOAA polar orbiting satellites have measured the intensity of upwelling microwave radiation from atmospheric oxygen. The intensity is proportional to the temperature of broad vertical layers of the atmosphere, as demonstrated by theory and direct comparisons with atmospheric temperatures from radiosonde (balloon) profiles. Upwelling radiance is measured at different frequencies; these different frequency bands sample a different weighted range of the atmosphere. \nThe brightness temperature (T) measured by satellite is given by:\n\nformula_1\n\nwhere formula_2 is the surface weight, formula_3 and formula_4 are the temperatures at the surface and at the atmospheric level formula_5 and formula_6 is the atmospheric weighting function.\n\nBoth the surface and atmospheric weights are dependent on the surface emissivity formula_7, the absorption coefficient formula_8 and the earth incidence angle formula_9; the surface weight is the product of formula_7 and an attenuation factor:\n\nformula_11\n\nwhere\n\nformula_12\n\nThe atmospheric weighting functions formula_6 can be written as:\n\nformula_14\n\nThe first term in this equation is related to the radiation emitted upward from the level formula_5 and attenuated along the path to the top of the atmosphere (∞), the second include the radiation emitted downward from the level z to the surface (0) and the radiation reflected back by the surface (proportional to formula_7) to the top of the atmosphere, the exact form of formula_6 is dependent upon the temperature, water vapor and liquid water content of the atmosphere.\n\nMSU Channel 1 is not used to monitor atmospheric temperature because it's too much sensitive to the emission from the surface, furthermore it is heavily contaminated by water vapor/liquid water in the lowermost troposphere.\n\nChannel 2 or TMT is broadly representative of the troposphere, albeit with a significant overlap with the lower stratosphere (the weighting function has its maximum at 350 hPa and half-power at about 40 and 800 hPa). In an attempt to remove the stratospheric influence, Spencer and Christy developed the synthetic \"2LT or TLT\" product by subtracting signals at different view angles; this has a maximum at about 650 hPa. However, this amplifies noise, increases inter-satellite calibration biases and enhances surface contamination. The 2LT product has gone through numerous versions as various corrections have been applied.\n\nAnother methodology to reduce the influence of the stratosphere has been developed by Fu and Johanson, the TTT(Total Troposphere Temperature) channel is a linear combination of the TMT and TLS channel: TTT=1.156*TMT-0.153*TLS for the global average and TTT=1.12*TMT-0.11*TLS at tropical latitudes.\n\nThe T4 or TLS channel in representative of the temperature in the lower stratosphere with a peak weighting function at around 17 km above the earth surface.\n\nSince 1979 the Stratospheric sounding units (SSUs) on the NOAA operational satellites provided near global stratospheric temperature data above the lower stratosphere.\nThe SSU is a far-infrared spectrometer employing a pressure modulation technique to make measurement in three channels in the 15 μm carbon dioxide absorption band. The three channels use the same frequency but different carbon dioxide cell pressure, the corresponding weighting functions peaks at 29 km for channel1, 37 km for channel2 and 45 km for channel3.\n\nRecords have been created by merging data from nine different MSUs, each with peculiarities (e.g., time drift of the spacecraft relative to the local solar time) that must be calculated and removed because they can have substantial impacts on the resulting trend.\n\nThe process of constructing a temperature record from a radiance record is difficult. The satellite temperature record comes from a succession of different satellites and problems with inter-calibration between the satellites are important, especially NOAA-9, which accounts for most of the difference between various analyses. NOAA-11 played a significant role in a 2005 study by Mears \"et al.\" identifying an error in the diurnal correction that leads to the 40% jump in Spencer and Christy's trend from version 5.1 to 5.2. There are ongoing efforts to resolve differences in satellite temperature datasets.\n\nChristy \"et al.\" (2007) find that the tropical temperature trends from radiosondes matches closest with his v5.2 UAH dataset. Furthermore, they assert there is a growing discrepancy between RSS and sonde trends beginning in 1992, when the NOAA-12 satellite was launched. This research found that the tropics were warming, from the balloon data, +0.09 (corrected to UAH) or +0.12 (corrected to RSS) or 0.05 K (from UAH MSU; ±0.07 K room for error) a decade.\n\nUsing the T2 or TMT channel (which include significant contributions from the stratosphere, which has cooled), Mears et al. of Remote Sensing Systems (RSS) find (through January 2017) a trend of +0.140 °C/decade. Spencer and Christy of the University of Alabama in Huntsville (UAH), find a smaller trend of +0.08 °C/decade.\n\nA no longer updated analysis of Vinnikov and Grody found +0.20 °C per decade (1978–2005). Another satellite temperature analysis is provided by NOAA/NESDIS STAR Center for Satellite Application and Research and use simultaneous nadir overpasses (SNO) to remove satellite intercalibration biases yielding more accurate temperature trends. The STAR-NOAA analysis finds a 1979-2016 trend of +0.129 °C/decade for TMT channel.\n\nLower stratospheric cooling is mainly caused by the effects of ozone depletion with a possible contribution from increased stratospheric water vapor and greenhouse gases increase. There is a decline in stratospheric temperatures, interspersed by warmings related to volcanic eruptions. Global Warming theory suggests that the stratosphere should cool while the troposphere warms The long term cooling in the lower stratosphere occurred in two downward steps in temperature both after the transient warming related to explosive volcanic eruptions of El Chichón and Mount Pinatubo, this behavior of the global stratospheric temperature has been attributed to global ozone concentration variation in the two years following volcanic eruptions.\nSince 1996 the trend is slightly positive due to ozone recovery juxtaposed to a cooling trend of 0.1K/decade that is consistent with the predicted impact of increased greenhouse gases.\n\nThe process of deriving trends from SSUs measurement has proved particularly difficult because of satellites drift, inter-calibration between different satellite with scant overlap and gas leak in the instrument carbon dioxide pressure cell, furthermore since the radiances measured by SSUs are due to emission by carbon dioxide the weighting functions move to higher altitudes as the carbon dioxide concentration in the stratosphere increase.\nMid to upper stratosphere temperature show strong negative trend interspersed by transient volcanic warming after the explosive volcanic eruptions of El Chichón and Mount Pinatubo, little temperature trend has been observed since 1995.\nThe greatest cooling occurred in the tropical stratosphere consistent with enhanced Brewer-Dobson circulation under greenhouse gas concentrations increase.\n\nThe satellite records have the advantage of global coverage, whereas the radiosonde record is longer. There have been complaints of data problems with both records.\n\nTo compare to the trend from the surface temperature record (+0.1610.033 °C/decade from 1979 to 2012 according to NASA GISS) it is most appropriate to derive trends for the part of the atmosphere nearest the surface, i.e., the lower troposphere. Doing this, through January 2017:\n\nAn alternative adjustment to remove the stratospheric contamination has been introduced by Fu \"et al.\" (2004), after the correction the vertical weighting function is nearly the same of the T2(TMT) channel in the troposphere, the University of Washington analysis finds 1979-2012 trends of +0.13 °C/decade when applied to the RSS data set and +0.10 °C/decade when applied to the UAH data set.\n\nThe IPCC fifth assessment report concluded: \" based on multiple independent analyses of measurements from radiosondes and satellite sensors it is virtually certain that globally the troposphere has warmed and the stratosphere has cooled since the mid-20th century. Despite unanimous agreement on the sign of the trends, substantial disagreement exists among available estimates as to the rate of temperature changes, particularly outside the NH extratropical troposphere, which has been well sampled by radiosondes.\n\nSurface temperature data showed substantial global average warming, while early versions of satellite and radiosonde data showed little or no warming above the surface. While the satellite data now show global warming, there is still a significant difference between what Climate models predict and what the satellite data show for warming of the lower troposphere with the climate models predicting significantly more warming than what the satellites measure. The climate models also overpredict the results of the radiosonde measurements.\n\nThe most recent climate model simulations give a range of results for changes in global average temperature. Some models show more warming in the troposphere than at the surface, while a slightly smaller number of simulations show the opposite behavior. There is no fundamental inconsistency among these model results and observations at the global scale, with the trends now being similar.\n\nGlobally, the troposphere is predicted by models to warm about 1.2 times more than the surface; in the tropics, the troposphere should warm about 1.5 times more than the surface. Most climate models used by the IPCC in preparation of their third assessment show a slightly greater warming at the TLT level than at the surface (0.03 °C/decade difference) for 1979-1999 while the GISS trend is +0.161 °C/decade for 1979 to 2012, the lower troposphere trends calculated from satellite data by UAH and RSS are +0.12 °C/decade and +0.184 °C/decade.\n\nThis greater global average warming in the troposphere compared to the surface (present in the models but not observed data) is most marked in the tropics. \n\nAlthough all the datasets show the expected tropospheric amplification at seasonal and annual timescales it is still debated whether or not the long term trends are consistent with the expected moist adiabatic lapse rate amplification due to difficulty of producing homogenized datasets, some satellite temperature reconstruction are consistent with the expected amplification while others are not.\n\nThe IPCC fifth assessment report concluded \"Although there have been substantial methodological debates about the calculation of trends and their uncertainty, a 95% confidence interval of around ±0.1°C per decade has been obtained consistently for both LT and MT (e.g., Section 2.4.4; McKitrick et al., 2010). In summary, despite unanimous agreement on the sign of the observed trends, there exists substantial disagreement between available estimates as to the rate of temperature changes in the tropical troposphere, and there is only low confidence in the rate of change and its vertical structure.\"\n\nFor some time the only available satellite record was the UAH version, which (with early versions of the processing algorithm) showed a global cooling trend for its first decade. Since then, a longer record and a number of corrections to the processing have revised this picture: the UAH dataset has shown an overall warming trend since 1998, though less than the RSS version. In 2001, an extensive comparison and discussion of trends from different data sources and periods was given in the Third Assessment Report of the Intergovernmental Panel on Climate Change (IPCC) (section 2.2.4).\n\nIn June 2017, RSS released v4 which significantly increased the trend making the difference between RSS and UAH substantial again.\n\n\n"}
{"id": "24686102", "url": "https://en.wikipedia.org/wiki?curid=24686102", "title": "Scenario optimization", "text": "Scenario optimization\n\nThe scenario approach or scenario optimization approach is a technique for obtaining solutions to robust optimization and chance-constrained optimization problems based on a sample of the constraints. It also relates to inductive reasoning in modeling and decision-making. The technique has existed for decades as a heuristic approach and has more recently been given a systematic theoretical foundation.\n\nIn optimization, robustness features translate into constraints that are parameterized by the uncertain elements of the problem. In the scenario method, a solution is obtained by only looking at a random sample of constraints (heuristic approach) called \"scenarios\" and a deeply-grounded theory tells the user how “robust” the corresponding solution is related to other constraints. This theory justifies the use of randomization in robust and chance-constrained optimization. \nAt times, scenarios are obtained as random extractions from a model. More often, however, scenarios are instances of the uncertain constraints that are obtained as observations (data-driven science). In this latter case, no model of uncertainty is needed to generate scenarios. Moreover, most remarkably, also in this case scenario optimization comes accompanied by a full-fledged theory because all scenario optimization results are distribution-free and can therefore be applied even when a model of uncertainty is not available.\n\nFor constraints that are convex (e.g. in semidefinite problems involving LMIs, Linear Matrix Inequalities), a deep theoretical analysis has been established which shows that the probability that a new constraint is not satisfied follows a distribution that is dominated by a Beta distribution. This result is tight since it is exact for a whole class of convex problems. More generally, various empirical levels have been shown to follow a Dirichlet distribution, whose marginals are beta distribution. The scenario approach with formula_1 regularization has also been considered, and handy algorithms with reduced computational complexity are available. Extensions to more complex, non-convex, set-ups are still objects of active investigation.\n\nAlong the scenario approach, it is also possible to pursue a risk-return trade-off. Moreover, a full-fledged method can be used to apply this approach to control. First formula_2 constraints are sampled and then the user starts removing some of the constraints in succession. This can be done in different ways, even according to greedy algorithms. After elimination of one more constraint, the optimal solution is updated, and the corresponding optimal value is determined. As this procedure moves on, the user constructs an empirical “curve of values”, i.e. the curve representing the value achieved after the removing of an increasing number of constraints. The scenario theory provides precise evaluations of how robust the various solutions are. \n\nA remarkable advance in the theory has been established by the recent wait-and-judge approach: one assesses the complexity of the solution (as precisely defined in the referenced article) and from its value formulates precise evaluations on the robustness of the solution. These results shed light on deeply-grounded links between the concepts of complexity and risk. A related approach, named \"Repetitive Scenario Design\" aims at reducing the sample complexity of the solution by repeatedly alternating a scenario design phase (with reduced number of samples) with a randomized check of the feasibility of the ensuing solution.\n\nConsider a function formula_3 which represents the return of an investment; it depends on our vector of investment choices formula_4 and on the market state formula_5 which will be experienced at the end of the investment period.\n\nGiven a stochastic model for the market conditions, we consider formula_2 of the possible states formula_7 (randomization of uncertainty). Alternatively, the scenarios formula_8 can be obtained from a record of observations. \n\nWe set out to solve the scenario optimization program\n\nThis corresponds to choosing a portfolio vector \"x\" so as to obtain the best possible return in the worst-case scenario.\n\nAfter solving (1), an optimal investment strategy formula_10 is achieved along with the corresponding optimal return formula_11. While formula_11 has been obtained by looking at formula_2 possible market states only, the scenario theory tells us that the solution is robust up to a level formula_14, that is, the return formula_11 will be achieved with probability formula_16 for other market states. \n\nIn quantitative finance, the worst-case approach can be overconservative. One alternative is to discard some odd situations to reduce pessimism; moreover, scenario optimization can be applied to other risk-measures including CVaR - Conditional Value at Risk, so adding to the flexibility of its use.\n\nFields of application include: prediction, systems theory, regression analysis, Actuarial science, optimal control, financial mathematics, machine learning, decision making, supply chain, and management.\n"}
{"id": "41854005", "url": "https://en.wikipedia.org/wiki?curid=41854005", "title": "Skillshare", "text": "Skillshare\n\nSkillshare is an online learning community for people who want to learn from educational videos. The courses, which are not accredited, are available through subscription. The majority of courses focus on interaction rather than lecturing, with the primary goal of learning by completing a project. The main course categories are creative arts, design, entrepreneurship, lifestyle, technology, and many more subtopics.\n\nMichael Karnjanaprakorn and Malcolm Ong started Skillshare in New York City, New York in November 2010; the site was live in April 2011. Previously, Karnjanaprakorn led the product team at Hot Potato, a social media product bought by Facebook. Ong was product manager at OMGPop. In August 2011, Skillshare raised $3.1 million in Series A funding led by Union Square Ventures and Spark Capital. In late 2013, Skillshare had raised $4.65 million in funding, and $6 million by February 2014, with financing co-led by Union Square Ventures and Spark Capital. Total funding reached $10 million.\n\nSkillshare held the Penny Conference in April 2012, a one-day discussion on the current educational system and how to reform it, with Michael Karnjanaprakorn, Codecademy’s cofounder Zach Sims, and Pencils of Promise founder Adam Braun as speakers.\n\nSkillshare launched 15 self paced, online courses in August 2012, with students collaborating to complete a project. By November 2013, it hosted over 250 courses, and launched its School of Design.\n\nSkillshare collaborated with Levi's to launch the School of MakeOurMark in October 2013, focusing on individual creativity with courses in photography, tattooing, and various forms of illustration.\n\nIn March 2014, Skillshare moved to a membership model for $9.95 a month. Later that year, the company announced a new open platform, where anyone could be a course instructor, and a free membership option to watch a limited amount of class content each month.\n\nSkillshare organizes courses into advertising, business, design, fashion and style, film and video, food and drink, music, photography, technology, and writing and publishing, often taught by industry leaders. All online courses are self-paced.\n\nNotable instructors include Seth Godin (entrepreurship), Jessica Hische (lettering), Young Guru (audio mixing and recording), Marc Ecko (entrepreneurship and brand creation), Gary Vaynerchuk (social media strategy), Guy Kawasaki (entrepreneurship), and Barbara Corcoran (entrepreneurship).\n\n"}
{"id": "56127644", "url": "https://en.wikipedia.org/wiki?curid=56127644", "title": "Starlight (interstellar probe)", "text": "Starlight (interstellar probe)\n\nProject Starlight is a research project of the University of California, Santa Barbara to develop a fleet of laser beam-propelled spacecraft and sending them to a star neighboring the Solar System, potentially Alpha Centauri. The project aims to send organisms on board the spacecraft.\n\nStarlight aims to accelerate the spacecrafts with powerful lasers, a method the project refers to as DEEP-IN (Directed Energy Propulsion for Interstellar Exploration), thus allowing them to reach stars near the Solar System in a matter of years, in contrast to traditional propulsion methods which will require thousands of years. Each spacecraft will be the size of a DVD disc and will be powered by plutonium. They will fly at one-fifth of the speed of light, and in the case of Alpha Centauri, it will arrive after traveling more than twenty years from Earth. Starlight is a program of the Experimental Cosmology Group of University of California, Santa Barbara (UCSB), and has received funding from NASA. In 2015, the NASA Innovative Advanced Concepts (NIAC) selected DEEP-IN as a phase-1 project.\n\nOne goal of Starlight is to send terrestrial organisms along with the spacecraft, and observe how the interstellar environment and extreme acceleration affects them. This effort is known as \"Terrestrial Biomes in Space\", and the lead candidate is \"Caenorhabditis elegans\", a minuscule nematode. The organism will spend most of the voyage in a frozen state, and once the spacecraft approaches its target they will be thawed by heat from the onboard plutonium. Following their revival, the organisms will be monitored by various sensors, and the data they produce will be sent back to Earth. \"C. elegans\" have been used extensively in biological research as a model organism, owing to the fact that the worm has one of the least number of cells for an animal possessing a nervous system. A backup option for \"C. elegans\" are tardigrades, micro-animals that are known for their resilience to various conditions lethal to other animals, such as the vacuum environment of space and strong doses of ionizing radiation.\n\nNASA's funding does not cover the \"Terrestrial Biome in Space\" portion of Starlight, as the experiment may potentially contaminate exoplanets.\n\n\n"}
{"id": "48633496", "url": "https://en.wikipedia.org/wiki?curid=48633496", "title": "Tama Iron Cobra", "text": "Tama Iron Cobra\n\nThe Tama Iron Cobra is a bass drum pedal line made between 1992 and the present. They are a full-baseplate, double chain or kevlar strap drive, professional quality bass drum pedal used by many leading drummers. The Iron Cobra comes in both single and double pedal configurations with 3 different drive choices and, recently, a longboard Speed Cobra option. Tama also produces an Iron Cobra Junior pedal for the beginner/intermediate audience. The pedal's initial popularity was in its adjustability, but it has also become known for its durability over time.\n\nTama Drums introduced the first Iron Cobra pedal prototype in 1992. This pedal had some of the eventual features of the production Iron Cobras but, with quickly machined, blocky parts, it looked quite a bit less refined than the eventual consumer version.\n\nIn 1993 the first generation Iron Cobra went on the market. The pedals looked similar to previous Tama models, having the typical silver, squared off footboard of the Tama pedals of the 1980s. The main difference with the new Cobra was the added adjustability of many of the components. The pedal was offered in a slightly less adjustable standard version and a full-featured professional version. The pedal was mostly the native silver color of the metal, with a few black painted details.\n\nIn 1998 the second generation Iron Cobra was released. This version had a more refined, curved footboard, updated branding and logo, and a black powder-coated finish. The bearings were upgraded and the hoop clamp was upgraded. All of the adjustment mechanisms were refined for the second generation, as the pedal had built its name on adjustability the past 5 years. Around this time was when the standard non-professional version became known as the Iron Cobra Junior. The Junior version had only a single chain drive, as opposed to the double chain of the full professional version.\n\nIn 2011 the third generation Iron Cobra came out with a new set of features. The pedal retained the black styling of the second generation, but with a few changes in features. This Cobra had a footboard surface that was smoother, the bearing housing was redesigned, the cam shapes were altered slightly, and the baseplate came with a feature called the Cobra Coil. The Cobra coil was a spring mounted beneath the footboard that was supposed to increase speed and responsiveness. Unlike the other upgrades, this Cobra Coil feature is unique to the Iron Cobra line and has come to be seen as a gimmick, rather than a true upgrade, adding very little to the playability of the pedals.\n\nIn 2013 the third generation pedals were joined by two new variants, the duo-glide, which allowed the cam shape to be altered, and the Speed Cobra, a longboard version with a unique light sprocket cam, new fastball bearing type, and a different beater shape.\n\nSince the beginning the Iron Cobra was known for adjustability. Through all the generations, the pedal has had the ability to adjust the footboard angle independently of the beater shaft angle, adjust the beater head angle, adjust the hoop clamp size, adjust the beater shaft length, adjust the feel of the beater with a sliding weight, and adjust the spring tension. Not all of these adjustments are universally possible on all other pedals from other manufacturers. The cam shape is not adjustable on the Iron Cobra, except on the third gen. duo-glide model, but the pedal has historically been offered with two different optional cams. The rolling-glide cam is round for a smooth action, while the powerglide cam changes radius mid-stroke for more acceleration. While most of the Iron Cobra pedals are double chain drive, the flexi-glide version has a kevlar strap drive. Since at least the second generation, the pedals have come with a hard plastic carrying case.\n\nProminent drummers to use the pedal include:\n\nDave Lombardo, Scott Travis, Joey Jordison, Mike Portnoy, Gavin Harrison, Stewart Copeland, Lars Ulrich, Derrick Plourde, etc.\n"}
{"id": "41708438", "url": "https://en.wikipedia.org/wiki?curid=41708438", "title": "The FracTracker Alliance", "text": "The FracTracker Alliance\n\nFracTracker Alliance is a 501(c)(3) non-profit that shares maps, images, data, and analysis related to the oil and gas industry hoping that a better informed public will be able to make better informed decisions regarding the world's energy future. FracTracker's information is focused in large part on unconventional extraction methods. FracTracker Alliance is based in the United States and has offices in Camp Hill, Pennsylvania; Pittsburgh, Pennsylvania; Cleveland, Ohio; Washington, DC; New York; Berkeley, and California.\n\nFracTracker reportedly aims to provide non-partisan information, and has no official position on the practice of hydraulic fracturing.\n\nFracTracker Alliance originated as FracTracker.org, a project of the Center for Healthy Environments and Communities at the University of Pittsburgh Graduate School of Public Health with the objective of crowd-sourcing data concerning unconventional gas extraction from the Marcellus Shale. Between 2010 and early 2012, FracTracker was funded by grants from The Heinz Endowments and The William Penn Foundation.\n\nFracTracker.org's original director, Dr. Volz, left the University of Pittsburgh in April 2011. Soon after Dr. Volz left the University, FracTracker.org split off from the University as well (in early 2012), and formed a new non-profit named The FracTracker Alliance. Many members of the original team from the University of Pittsburgh who had been working on FracTracker.org left the University to work for the new non-profit as (or shortly after) it was created.\n\nFracTracker Alliance is known for its mapping of energy issues, available on FracTracker.org FracTracker offers a variety of maps detailing drilling-related activity, and more recently renewable energy progress on its website. FracTracker's data comes from a variety of sources including state environmental agencies, news reports, freedom of information requests, user reports, collaborations with other groups, and information from other agencies. FracTracker makes its data available for download, and makes clear where the data came from by including a variety of metadata along with its data (e.g. information about who created the original content, what is included in the dataset, when the dataset was taken, where the data features were located, and information about any changes from the original dataset). FracTracker.org also offers regular in-person trainings about how to use their mapping tools.\n\nFracTracker.org originally used a proprietary mapping system designed by Rhiza Labs in Pittsburgh, but transitioned to a customized mapping platform based on Esri's ARCGis Online that allows users to download full datasets both through FracTracker and through Esri.\n\nFracTracker Alliance collaborates with many other nonprofit organizations, providing them with visualizations and information that help them communicate about energy choices and their impacts. These services are typically provided on a pro-bono basis, but for larger projects they apply a fee-for-service structure.\n\n\n"}
{"id": "1039570", "url": "https://en.wikipedia.org/wiki?curid=1039570", "title": "Traveler's cheque", "text": "Traveler's cheque\n\nA traveler's cheque is a medium of exchange that can be used in place of hard currency. They can be denominated in one of a number of major world currencies and are preprinted, fixed-amount cheques designed to allow the person signing it to make an unconditional payment to someone else as a result of having paid the issuer for that privilege. \n\nThey were generally used by people on vacation in foreign countries instead of cash, as many businesses used to accept traveler's cheques as currency. The incentive for merchants and other parties to accept them lay in the fact that as long as the original signature (which the buyer is supposed to place on the cheque in ink as soon as they receive the cheque) and the signature made at the time the cheque is used are the same, the cheque's issuer will unconditionally guarantee payment of the face amount even if the cheque was fraudulently issued, stolen, or lost. This means that a traveler's cheque can never 'bounce' unless the issuer goes bankrupt and out of business. If a traveler's cheque were lost or stolen, it can be replaced by the issuing financial institution. \n\nThe financial institutions issuing traveler's cheques earn income in a number of ways. Firstly, they would charge a fee on sale of such cheques. In addition, they can earn interest for the period that the cheques are uncashed, while not paying any interest to the cheque holder, making them effectively interest-free loans. Also, the foreign exchange rate commonly used on traveler's cheques (generally based on rates applicable at the time of purchase) is less favourable compared to other forms of obtaining foreign currency, especially those on credit card transactions (which use a rate applicable at the statement date). On the other hand, the set up cost and the cost of issuing and processing traveler's cheques is much higher than for credit card transactions. The cheque issuer carries the exchange rate risk, and would normally pay a fee to hedge against the risk.\n\nTheir use has been in decline since the 1990s, when a variety of more convenient alternatives, such as credit cards, debit cards, pre-paid currency cards and automated teller machines, became more widely available and were easier for travelers to use. Traveler's cheques are no longer widely accepted and cannot easily be cashed, even at the banks that issued them. The alternatives to traveler's cheques were generally cheaper and more flexible. Travel money cards offer similar features to traveler's cheques, including prepurchase of foreign currency at rates applicable at date of purchase, but offer greater ease and flexibility, such as use like a regular credit card, no need to get change in a local currency, besides other features.\n\nLegally, the parties to traveler's cheque transactions are as follows. The organization that produces a traveler's cheque is the obligor or issuer. The bank or other place that sells it is the agent of the issuer. The natural person who buys the cheque is the purchaser. The entity to whom the purchaser hands the cheque in payment for goods or services is the payee or merchant. For purposes of clearance, the obligor is both maker and drawee.\n\nTraveler's cheques were first issued on 1 January 1772 by the \"London Credit Exchange Company\" for use in 90 European cities, and in 1874, Thomas Cook was issuing 'circular notes' that operated in the manner of traveler's cheques.\n\nAmerican Express developed a large-scale international traveler's cheque system in 1891, to supersede the traditional letters of credit. It is still the largest issuer of traveler's cheques today by volume. American Express's introduction of traveler's cheques is traditionally attributed to employee Marcellus Flemming Berry, after company president J.C. Fargo had problems in smaller European cities obtaining funds with a letter of credit.\n\nBetween the 1650s and the 1990s, traveler's cheques became one of the main ways that people took money on vacation for use in foreign countries without the risks associated with carrying large amounts of cash.\n\nSeveral brands of travelers cheques have been marketed; the most familiar of those were Thomas Cook Group, Bank of America and American Express.\n\nThe convenience and wider acceptance of such alternatives as credit and debit cards and the wider availability of ATMs has led to a significant decline in the use of traveler's cheques since the 1990s. In addition, security concerns of retailers has led to many businesses ceasing to accept them, in turn making them less attractive to travelers. This has led to complaints about the difficulty that holders have in using them. In much of Europe and Asia, traveler's cheques are no longer widely accepted and cannot be easily cashed, even at the banks that issued them.\n\nSince traveler's cheques do not earn interest, one of the main incentives financial institutions have to sell traveler's cheques is that they effectively represent an interest-free loan from the purchaser to the seller. The sustained decline in interest rates in most of the developed world since the early-to-mid 1990s has substantially reduced the profitability of traveler's cheques for their issuers. Financial institutions have responded to this development by charging new fees for traveler's cheques, increasing existing fees, or by exiting the business altogether.\n\nTraveler's cheques are sold by banks and agencies to customers for use at a later time. Upon obtaining custody of a purchased supply of traveler's cheques, the purchaser would immediately sign each cheque. The purchaser will also receive a receipt and some other documentation that should be kept in a safe place other than where they carry the cheques. Traveler's cheques can usually be replaced if lost or stolen, assuming the owner still has the receipt issued with the purchase of the cheques showing the serial numbers allocated.\n\nTo cash a traveler's cheque to make a purchase, the purchaser would, in the presence of the payee, date and countersign the cheque in the indicated space.\n\nTraveler's cheques are available in several currencies such as U.S. dollars, Canadian dollars, Pounds sterling, Japanese yen, Chinese Yuan and Euro; denominations usually being 20, 50, or 100 (x100 for Yen) of whatever currency, and are usually sold in pads of five or ten cheques, e.g., 5 × €20 for €100. Traveler's cheques do not expire, so unused cheques can be kept by the purchaser to spend at any time in the future. The purchaser of a supply of traveler's cheques effectively gives an interest-free loan to the issuer, which is why it is common for banks to sell them \"commission free\" to their customers. The commission, where it is charged, is usually 1–2% of the total face value sold.\n\nAny change for a purchase transaction would be given in the local currency.\n\nA payee receiving a traveler's cheque would follow its normal procedures for depositing cheques into its bank account: usually, endorsement by stamp or signature and listing the cheque and its amount on the deposit slip. The bank account will be credited with the amount of the cheque as with any other negotiable item submitted for clearance.\n\nIn the United States, if the payee is equipped to process cheques electronically at point of sale (\"see:\" Check 21 Act), they would still take custody of the cheque and submit it to a financial institution, particularly to avoid any confusion on the part of the purchaser.\n\nOne of the main advantages traveler's cheques provide is the replacement if lost or stolen. \n\nHowever, this feature has also created a black market where fraudsters buy traveler's cheques, sell them at 50% of their value to other people (such as travelers) and falsely report their traveler's cheque stolen with the company from which the cheque was obtained. As such, they get back the value of the traveler's cheque and make 50% of the value as profit.\n\nThe widespread problem of counterfeit traveler's cheques has caused a number of businesses to no longer accept them or to impose stringent safeguards when they are used. It is a reasonable security procedure for the payee to ask to inspect the purchaser's picture ID; a driver's license or passport should suffice, and doing so would most usefully be towards the end of comparing the purchaser's signature on the ID with those on the cheque. The best first step, however, that can be taken by any payee who has concerns about the validity of any traveler's cheque, is to contact the issuer directly; a negative finding by a third-party cheque verification service based on an ID check may merely indicate that the service has no record about the purchaser (to be expected, practically by definition, of many travelers), or at worst that they have been deemed incompetent to manage a personal chequing account (which would have no bearing on the validity of a traveler's cheque).\n\nSome purchasers have found the process of filing a claim for lost or stolen cheques is cumbersome, and have been left without recourse after their cheques were lost or stolen.\n\nThe widespread acceptance of credit cards and debit cards around the world starting in the 1980s and 1990s significantly replaced the use of traveler's cheques for paying for things on vacation.\n\nIn 2005, American Express released the \"American Express Travelers Cheque Card\", a stored-value card that serves the same purposes as a traveler's cheque, but can be used in stores like a credit card. It discontinued the card in October 2007. A number of other financial companies went on to issue stored-value or pre-paid debit cards containing several currencies that could be used like credit or debit cards at shops and at ATMs, mimicking the traveler's cheque in electronic form. One of the major examples is the Visa TravelMoney card.\n\n\n"}
{"id": "21294991", "url": "https://en.wikipedia.org/wiki?curid=21294991", "title": "Verreville Glass and Pottery Works, Glasgow", "text": "Verreville Glass and Pottery Works, Glasgow\n\nVerreville Glassworks was established on the north bank of the River Clyde in the village of Finnieston in 1777. Glass making was discontinued in 1842. The buildings of the works, including the 120 feet high glasshouse cone, were converted into a pottery works which remained in production until 1918.\nIn addition to a rich historical and documentary archive, evidence for the operations and technology of these industries was gained from excavations in 2005. The archaeological excavation, carried out by Headland Archaeology uncovered several buildings and structures relating to both the glass and the pottery works.\n\nThe Verreville Glassworks was established by a group of Glasgow merchants in 1777. Workmen were brought from England and Germany to build the cone, a major Glasgow landmark of its day, which reached a height of 120 feet. The company amalgamated with the Glasgow Bottlework Company in c. 1786 and in 1806 the business was sold to the Dumbarton Glass Company. The new business was immediately sold to John Geddes, on the condition that he did not make window glass or bottles.\n\nJohn Geddes founded the ‘Glasgow and Verreville Glass and Pottery Company’, which quickly established itself and developed an export trade to North America and Ireland. However, pottery production was a very competitive market and Geddes was soon declared bankrupt. In 1830 the pottery was taken over by Robert Montgomery, a former manager. Montgomery’s involvement with the pottery was short-lived and by 1833 he too was declared bankrupt. The glassworks were closed in 1834.\n\nThe glassworks and John Geddes’ house and grounds (Verreville House) were bought by Robert Alexander Kidston who was a partner in the nearby Anderston Pottery (often called Lancefield Pottery). By this time the pottery had two earthenware kilns and one china kiln. Kidston tried to raise the quality of the products - he added the production of porcelain, imported Staffordshire workers and extended the works into the grounds of Verreville House – but by 1841 he himself was in financial trouble and the firm was taken over by a consortium, one of whom was Robert Cochran, whose family later owned the much larger Britannia Pottery in Glasgow. Glassmaking appears to have ceased production in 1842 and a number of small pottery kilns were built inside the original glassworks cone. Only white and earthenwares were being made by this time. The Cochran family remained in charge of the pottery until its eventual closure in 1918.\n\nThe Headland excavations in 2005 revealed several features related to three major phases of development: glass manufacturing activity (c. 1777 – mid-1840s), pottery manufacturing phase (late 1840s – c. 1859) and the later pottery works (after c. 1859 – c. 1918).\n\n\nThe earliest phase of activity identified on site was associated with the production of glass. Evidence for the glass cone structure was identified by four evenly spaced brick built piers. These were constructed of hand made red bricks and formed a half circle in plan. It is probable that this is the remains of the northern half of the original octagonal glass cone structure built in 1777. These piers possibly supported an arcaded external wall, allowing access to the interior of the glass cone.\nLeading into the central area of the ‘glass cone’ was a series of flues. The largest was constructed of hand made red brick with a vaulted ceiling. The interior of the brick was blackened by soot and the floor was blackened natural sand, which contained a large quaintly of broken glass or cullet. These flues would have pulled cold air into a central (unidentified) glass furnace, obliterated by the post-1920s buildings.\n\n\nThe sub square building (hovel), which housed the glass cone was only identified by one wall during the excavation. Two lengths of hand made red brick wall, interpreted as the original northern wall of the hovel were excavated. Truncated by later stone pillar bases the fragmentary walls were upstanding to a height of approximately 2 m. The western wall fragment had a remains of a double archway – possibly an original entrance into the glass cone.\n\n\nTwo glass working ‘floor’ structures were identified. The northern structure was constructed of yellow refractory bricks with single coarse brick walls spaced at regular intervals. The floor base was vitrified with a thick layer of melted glass covering it. The southern structure appeared to be of a similar construction although was heavily truncated. These structures have been initially interpreted as possible fritting (or melting) floors for the primary production of melted glass.\n\nA heavily compacted surface of crushed flint covered the eastern area of the site. Several parts of the surface had been repaired by rounded cobbles and obvious areas of wear, including possible cart ruts, were apparent.\nFive buildings associated with pottery manufacture were identified during the excavation including a pottery drying room, workshops and furnaces. These all appeared to date from the expansion of the pottery works c. 1820.\n\nSix brick built kilns were identified during the course of the excavations. All were fragmentary and truncated by both the modern buildings and later 19th century buildings. All that survived of each was the partial remains of circular structures forming the base of the pottery-firing kiln. A number of rectangular ash pits, evenly spaced along the edges of the main kiln bases, were identified with each kiln. These contained ash and kiln furniture and were used to collect the burnt material falling from the fires above. Surrounding each of the kilns were cobbled floors, which sloped downward to the external edges of the ash pits – to allow easy emptying of the content of the ash pits.\n\nThree kilns were situated within the probable location of the large circular glass cone, built in c. 1777. Historical reference note that after the glass cone went out of use (c. 1840) a number of small pottery kilns were built within it, reusing the glass cone building to house them.\n\nDuring the final 50 years of production existing buildings were adapted to new functions and new technology. Steam power was by now incorporated within the factory.\n\nThe archaeological evidence for the final phase of Verreville Pottery factory offers glimpses into the complex operations of pottery manufacturing such as processing, constructing, drying and firing of vessels. Clay would arrive into the area via a short gauge rail line in the warehouse. It would then be mixed by paddles in water (blunging), placed into tanks and put into the slip drying room to evaporate off the water. After this the clay would be thrown (wedged) to remove air bubbles and moulded or slip cast before placing into the pot drying room and finally into the kilns for firing.\n\nArchaeological remains included a slip drying oven, a tunnel furnace and a pot drying room as well as evidence for modifications to the warehouse, courtyard area and workshop and the removal of the rectangular oven.\n\n\n\n"}
