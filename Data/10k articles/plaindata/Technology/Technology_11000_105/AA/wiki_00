{"id": "21897683", "url": "https://en.wikipedia.org/wiki?curid=21897683", "title": "1,3,5-Trinitrobenzene", "text": "1,3,5-Trinitrobenzene\n\n1,3,5-Trinitrobenzene is a nitrated benzene-derivative that is classified as a high explosive, being moderately explosive in liquid form and extremely explosive in its dry powder form. It has a clear to light yellow sludgy appearance. It will detonate under strong shock. High temperatures, whether by sudden heating of any quantity, or by the accumulation of heat when large quantities are burning, will also cause detonation. The material can react vigorously with reducing materials and is incompatible with sodium dichromate or sulfuric acid. It must be stored in a cool, ventilated place, away from acute fire hazards and easily oxidized materials. It also reacts violently with aluminium, boron phosphide, cyanides, esters, PNH, phosphorus, NaCN, SnC, sodium hypophosphite, thiocyanates, etc. When heated to decomposition it emits highly toxic fumes of NO. It is an extremely powerful oxidizing agent which may cause violent reaction with reducing materials.\n\nTrinitrobenzene is used primarily as a high explosive for commercial mining and military use. Some other uses include a narrow-range pH indicator, an agent to vulcanize natural rubber, and a mediating agent to mediate the synthesis of other explosive compounds.\n\nIt is recommended that people avoid contact if possible. Avoid breathing dusts, and fumes from burning or reacting material. Wear appropriate chemical protective gloves and goggles. Do not handle broken packages unless wearing appropriate personal protective equipment. Wash away any material which may have contacted the body with copious amounts of water or soap and water. Do not store near other chemicals (especially other oxidizers), near sources of heating, or places where rapid temperature changes may occur. In case of spillage immediately soak with water and carefully and slowly clean up. In case of fire let burn; DO NOT attempt to put out the resulting fire. In any case where the compound is ingested call Poison Control Center immediately.\n\nThe most common modes of exposure are either direct contact with the substance or through drinking contaminated water. 1,3,5-Trinitrobenzene is believed to cause similar health problems as TNT. Exposure to high concentrations most commonly causes anemia, or the reduced ability for blood to carry oxygen. As a result of the lack of oxygen, the skin typically becomes blue or purple in color. Other symptoms of exposure include headache, nausea, and dizziness. The long-term effects from exposure are not known because no long-term studies of health effects have been performed. It is believed that long-term exposure will cause sterility (especially in males) and cataracts. It is unknown if Trinitrobenzene causes birth defects or cancer.\n\n"}
{"id": "8247419", "url": "https://en.wikipedia.org/wiki?curid=8247419", "title": "AN/WQX-2", "text": "AN/WQX-2\n\nThe AN/WQX-2 is a diver-detector sonar used in anti-frogman precautions. It is in service with the US Navy. It uses Kongsberg Mesotech components. It can detect divers up to 2400 feet (= nearly half a mile) away.\n\nThe AN/WQX-2 finds range and bearing of the detected target relative to the sonar. The C3D console developed by SPAWAR converts this to a GPS position. It has software to distinguish diver echoes from shoals of fish, marine mammals, debris, bubbles from boat wakes, etc.\n\n\n"}
{"id": "33270624", "url": "https://en.wikipedia.org/wiki?curid=33270624", "title": "Accel World", "text": "Accel World\n\nHaruyuki \"Haru\" Arita is a short, overweight boy who is frequently ridiculed by delinquents at the Umesato Junior High School. Using his Neuro Linker to escape the torment of real life, he logs onto the school's Local Network cyberspace where he always plays virtual squash alone, and his innate video game skills bring him to the attention of Kuroyukihime (literally meaning \"Black Snow Princess\"), the school's popular, highly intellectual and attractive female Student Council Vice-President.\n\nAfter helping him against the delinquents, Kuroyukihime introduces Haruyuki to Brain Burst, a secret program that is able to accelerate the human cognitive process to the point at which time appears to stop. Haruyuki soon learns that Brain Burst is more than just a program, but an Augmented Reality Massively Multiplayer Online (ARMMO) Fighting Game where people fight each other in fierce duels in order to obtain Burst Points which can be spent for acceleration abilities in the real world.\n\nKuroyukihime then enlists Haruyuki's help in reaching Level 10 within Brain Burst by defeating the \"Six Kings of Pure Colour\" and ultimately meet the creator of Brain Burst to learn its true purpose. With every challenge they face in the Accelerated World, Haru and Kuroyukihime, under their aliases \"Silver Crow\" and \"Black Lotus\", gather trusted allies, confront treacherous enemies and their bond grows stronger while working to attain their ultimate objective: to reach the highest in-game level and meet the game's creator, who will \nreveal why the game was created and the true purpose of it.\n\n\"Accel World\" began as a light novel series written by Reki Kawahara and illustrated by HiMA. Originally, Kawahara entered the first novel in the series into ASCII Media Works' 15th Dengeki Novel Prize in 2008 and the novel won the Grand Prize. The first novel was published by ASCII Media Works on February 10, 2009 under their Dengeki Bunko imprint. As of September 2018, 23 volumes have been published. At their Japan Expo USA panel, Yen Press announced the rights to publish the light novels in English. The first volume was released on July 22, 2014.\n\nA manga adaptation titled \"Accel World\" illustrated by Hiroyuki Aigamo began serialization in the May 2010 issue of ASCII Media Works' \"Dengeki Bunko Magazine\". The series has been collected in five \"tankōbon\" volumes, released between July 27, 2011, and February 27, 2014. A four-panel comic strip manga titled illustrated by Ryuryū Akari also began serialization in the May 2010 issue of \"Dengeki Bunko Magazine\". Three \"tankōbon\" volumes were released between July 27, 2011, and October 26, 2013. In February 2014, Yen Press announced they had licensed \"Accel World\" for an English release in North America. The first three volumes were released on September 23, 2014, December 16, 2014, and March 24, 2015 respectively.\n\n\n\nAn anime television series adaptation aired in Japan between April 7, 2012 and September 22, 2012. The opening theme until episode 13 is \"Chase the world\" by May'n and the ending theme is \"→unfinished→\" by Kotoko. From episode 14 onwards the opening theme is \"Burst the Gravity\" by Altima and the ending theme is \"unite.\" by Sachika Misawa. Warner Home Video, ASCII Media Works, Namco Bandai Games, Sunrise and Genco were involved in the production of this adaptation. Viz Media announced the streaming of the anime on Hulu as of April 6, 2012. An English-dubbed version premiered on Viz Media's online streaming service, Neon Alley, on April 19, 2013. An anime film titled \"\" featuring an original story by Kawahara has been released on July 23, 2016.\n\nTwo video games based on the series were developed by Banpresto and published by Namco Bandai Games for PlayStation 3 and PlayStation Portable. The first of these, , was released in Japan on September 13, 2012. The second one, , was released in Japan on January 31, 2013. The limited edition versions of these games include an original video animation of the anime series (Blu-ray Disc for the PS3 version, DVD for the PSP version). In \"Z.H.P. Unlosing Ranger VS Darkdeath Evilman\" for PSP, Silver Crow is an unlockable costume. In \"\", Black Lotus is an obtainable character and is also one of the opponents you can defeat.\n\nAn action role-playing game titled \"Accel World VS Sword Art Online: Millennium Twilight\" was announced in October 2016. The game is a crossover with \"Sword Art Online\", developed by Bandai Namco Entertainment for PlayStation 4 and PlayStation Vita and released on July 7, 2017. The Windows PC version was released on September 12, 2017.\n\nAnime News Network reviewed the series and gave a story rating of A-, stating a similar basis of \"Sword Art Online\" with improved characters. Richard Eisenbeis of Kotaku praised the anime for its well-thought out story, relatable main character, and great villain protagonist, but criticizes the anime for its third arc. At the end, he says that the anime starts out strong, but ends on a weak note. Nifrigel declared great things about the anime in terms of soundtrack, characters, and story, but strongly criticized Reki Kawahara's lack of knowledge in the subject of virtual reality.\n\n"}
{"id": "25962911", "url": "https://en.wikipedia.org/wiki?curid=25962911", "title": "Alternate Reconstitution Base", "text": "Alternate Reconstitution Base\n\nAn Alternate Reconstitutional Base (ARB) is a concept used during the Cold War by the United States Air Force's Strategic Air Command for the rearming of nuclear bombers. The idea was, after a nuclear exchange, primary SAC airfields would be destroyed and returning bombers would have no location to rearm their stores and reattack additional targets. ARB allowed trained teams to depart their home installation and create landing locations for returning bombers.\n\nA few civilian airfields, such as Clinton-Sherman Airport, were originally SAC bases. The long-length runways were maintained, as well as the Christmas tree alert structure for possible reuse by SAC forces in the event of nuclear war.\n\nVarious SAC communications squadrons would conduct ARB training by installing a mobile high frequency radio set at a pre-planned site and establish a communications net.\n\n"}
{"id": "8563704", "url": "https://en.wikipedia.org/wiki?curid=8563704", "title": "Antistatic device", "text": "Antistatic device\n\nAn antistatic device is any device that reduces, dampens, or otherwise inhibits electrostatic discharge; the buildup or discharge of static electricity, which can damage electrical components such as computer hard drives, and even ignite flammable liquids and gases.\n\nMany methods exist for neutralizing, varying in use and effectiveness depending on the application. Antistatic agents are chemical compounds that can be added to an object, or the packaging of an object, to help deter the buildup or discharge of static electricity. For the neutralization of static charge in a larger area, such as a factory floor or workshop, antistatic systems may utilize electron emission effects such as corona discharge or photoemission that introduce ions into the area that combine with and neutralize any electrically charged object. In many situations, sufficient ESD protection can be achieved with electrical grounding.\n\nVarious symbols can be found on products, indicating that the product is electrostatically sensitive, as with sensitive electrical components, or that it offers antistatic protection, as with antistatic bags.\n\nANSI/ESD standard S8.1-2007 is most commonly seen on applications related to electronics. Several variations consist of a triangle with a reaching hand depicted inside of it using negative space.\nVersions of the symbol will often have the hand being crossed out as a warning for the component being protected, indicating that it is ESD sensitive and is not to be touched unless antistatic precautions are taken. \nAnother version of the symbol has the triangle surrounded by an arc. This variant is in reference to the antistatic protective device, such as an antistatic wrist strap, rather than the component being protected. It usually does not feature the hand being crossed out, indicating that it makes contact with the component safe.\n\nAnother common symbol takes the form of a bold circle being intersected by three arrows. Originating as a U.S. military standard, it has been adopted industry-wide. It is intended as a depiction of a device or component being breached by static charges, indicated by the arrows.\n\nOne version on the circle with three arrows can be seen at the left of this picture.\n\nTypes of antistatic devices include:\n\nAn antistatic bag is a bag used for storing or shipping electronic components which may be prone to damage caused by electrostatic discharge (ESD).\n\nAn ionizing bar, sometimes referred to as a static bar, is a type of industrial equipment used for removing static electricity from a production line to dissipate static cling and other such phenomena that would disrupt the line. It is important in the manufacturing and printing industries, although it can be used in other applications as well.\n\nIonizing bars are most commonly suspended above a conveyor belt or other apparatus in a production line where the product can pass below it; the distance is usually calibrated for the specific application. The bar works by emitting an ionized corona onto the products below it. If then a product on the line has a positive or negative static charge, as it passes through the ionized aura created by the bar, it will attract the correspondingly charged positive or negative ions and become electrically neutral.\n\nAntistatic garments or antistatic clothing is required to prevent damage to electrical components or to prevent fires and explosions when working with flammable liquids and gases.\n\nOne of the ways to bond or electrically connect personnel to ground is the use of an ESD garment. ESD garments have conductive threads in them, creating a wearable version of a faraday cage. ESD garments attempt to shield ESD sensitive devices from harmful static charges from clothing such as wool, silk, and synthetic fabrics on people working with them. For these garments to work properly, they must also be connected to ground with a strap. Most ESD garments are not conductive enough to provide personal grounding so antistatic foot straps and antistatic wrist straps are also worn. ESD garments are considered an optional method to control ESD.\n\nAn ESD protected area is a defined location with the necessary materials, tools, and equipment capable of controlling static electricity to a level that minimizes damage to ESD susceptible items. In the ESD protected area, all conductors in the environment, including personnel, shall be bonded or electrically connected and attached to a known ground or contrived ground. This attachment creates an equipotential balance between all items and personnel. Electrostatic protection can be maintained at a potential above a \"zero\" voltage ground potential as long as all items in the system are at the same potential.\n\nAntistatic garments are used in many industries such as electronics, communications, telecommunications and defense applications. As computers and electronics become ever more pervasive in consumer products so an increasing number of manufacturers will need to apply anti-static control measures. One such measure is antistatic apparel because people are the greatest source of static charge in the workplace.\n\nTransportation of electrostatic sensitive devices also requires packaging that provides protection from electrostatic hazards in the transportation or storage system. In the case of an ESD protected area designed with continuous grounding of all conductors and dissipative items (including personnel), packaging may not be necessary.\n\nThe amount of static electricity we feel varies according to factors such as our body and foot size. A larger body and bigger feet require more charge to be stored to produce the same voltage. The material our clothes are made from and the soles of our shoes can influence static electricity too. Weather affects it as well. There is more build-up of static charge when the air is dry. Most people feel harmless shocks at around 2,000-4,000 volts. However electrical components can be damaged by as little as a few volts. It is estimated that between eight percent and 33 percent of product losses—-the proportion of products which are rendered faulty—-are due to static electricity. Static electricity is generally harmless to the individual but if not controlled, electrostatic discharge can cause product damage to electrostatic sensitive devices and lead to machinery downtime, lost man-hours, returned products and warranty costs particularly in the semiconductor and electronics industries, which caused worth of damage to products each year.\n\nAn antistatic floor mat or ground mat is one of a number of antistatic devices designed to help eliminate static electricity. It does this by having a controlled low resistance: a metal mat would keep parts grounded but would short out exposed parts; an insulating mat would provide no ground reference and so would not provide grounding. Typical resistance is on the order of 10 to 10 ohms between points on the mat and to ground. The mat would need to be grounded (earthed). This is usually accomplished by plugging into the grounded line in an electrical outlet. It's important to discharge at a slow rate, therefore a resistor should be used in earthing the mat. The resistor, as well as allowing high-voltage charges to leak through to earth, also prevents a shock hazard when working with low-voltage parts. Some ground mats allow you to connect an antistatic wrist strap to them. Versions are designed for placement on both the floor and desk.\n\nAn antistatic wrist strap, ESD wrist strap, or ground bracelet is an antistatic device used to safely ground a person working on very sensitive electronic equipment, to prevent the buildup of static electricity on their body, which can result in electrostatic discharge (ESD). It is used in the electronics industry by workers working on electronic devices which can be damaged by ESD, and also sometimes by people working around explosives, to prevent electric sparks which could set off an explosion. It consists of an elastic band of fabric with fine conductive fibers woven into it, attached to a wire with a clip on the end to connect it to a ground conductor. The fibers are usually made of carbon or carbon-filled rubber, and the strap is bound with a stainless steel clasp or plate. They are usually used in conjunction with an antistatic mat on the workbench, or a special static-dissipating plastic laminate on the workbench surface.\n\nThe wrist strap is usually worn on the nondominant hand (the left wrist for a right-handed person). It is connected to ground through a coiled retractable cable and 1 megohm resistor, which allows high-voltage charges to leak through but prevents a shock hazard when working with low-voltage parts. Where higher voltages are present, extra resistance (0.75 megohm per 250 V) is added in the path to ground to protect the wearer from excessive currents; this typically takes the form of a 4 megohm resistor in the coiled cable (or, more commonly, a 2 megohm resistor at each end).\n\nWrist straps designed for industrial use usually connect to earth bonding points, ground connections built into the workplace, via either a standard 4 mm plug or 10 mm press stud, whereas straps designed for consumer use often have a crocodile clip for the ground connection.\n\nIn addition to wrist straps, ankle and heel straps are used in industry to bleed away accumulated charge from a body. These devices are usually not tethered to earth ground, but instead incorporate high resistance in their construction, and work by dissipating electrical charge to special floor tiles. Such straps are used when workers need to be mobile in a work area and a grounding cable would get in the way. They are used particularly in an operating theatre, where oxygen or explosive anesthetic gases are used.\n\n\"Wireless\" or \"dissipative\" wrist straps are available, which claim to protect against ESD without needing a ground wire, typically by air ionization or corona discharge. These are widely regarded as ineffective, if not fraudulent, and examples have been tested and shown not to work. Professional ESD standards all require wired wrist straps.\n\n"}
{"id": "43276905", "url": "https://en.wikipedia.org/wiki?curid=43276905", "title": "Arthur Beale", "text": "Arthur Beale\n\nArthur Beale is a yacht-chandler on London's Shaftesbury Avenue, which stocks a wide variety of nautical equipment and accessories. The business started as the rope-maker John Buckingham by the nearby Fleet river in the 16th century and has been based in premises in Bloomsbury since then. In the 19th century, they became known as the exclusive suppliers of climbing rope to the Alpine Club. They still make and stock a large variety of ropes and lines and so also supply theatrical rigging and ornamental ropes for decoration and crowd control. \n\nThe business started as rope-maker, John Buckingham, on the Fleet river at the start of the 16th century. In the early 19th century, they had premises at number 6 in the Middle Row of St Giles—an impressive terrace in the middle of Broad Street—but had to move when this was demolished in 1843.\n\nThey then operated from premises on Shaftesbury Avenue when John Buckingham and then the new proprietor, Arthur Beale, were exclusive suppliers of climbing rope to early members of the Alpine Club. This was made to the club's specification so that it was both light and strong, being made from three strands of manila hemp, treated to be rot proof and marked with a red thread of worsted yarn. This rope was used for British expeditions to Mount Everest and Antarctica. It also supplied ice axes to polar explorer, Ernest Shackleton; the flagpole for Buckingham Palace and rigging for escapologists and the window displays of Selfridges department store.\nThe business now trades mainly as a yacht chandler, stocking and supplying nautical equipment such as a monkey's fist – a weighted ball of rope used for line-throwing. To support the retailing, they have a workshop downstairs, where they produce special orders for ropes and rigging. This business was declining but, in 2014, veteran sailor and theatrical chandler, Alasdair Flint, took over with business partner Gerry Jeatt and plans to revive it.\n\nEugenia Bell, writing in \"The Traditional Shops and Restaurants of London\", praised the establishment:\n\nIn 1999, \"Time Out\" likewise commented on their incongruity but praised the range of nautical equipment:\n\n"}
{"id": "979665", "url": "https://en.wikipedia.org/wiki?curid=979665", "title": "Baggage", "text": "Baggage\n\nBaggage or luggage consists of bags, cases, and containers which hold a traveller's articles while the traveler is in transit. \n\nThe modern traveller can be expected to have packages containing clothing, toiletries, small possessions, trip necessities, and on the return-trip, souvenirs. For some people, luggage and the style thereof is representative of the owner's wealth.\n\nBaggage (not luggage), or \"baggage train\", can also refer to the train of people and goods, both military and of a personal nature, which commonly followed pre-modern armies on campaign.\n\nLuggage has changed over time. Historically the most common types of luggage were chests or trunks made of wood or other heavy materials. These would be shipped by professional movers. Since the Second World War smaller and more lightweight suitcases and bags that can be carried by an individual have become the main form of luggage.\n\nAccording to the Oxford English Dictionary, the word \"baggage\" comes from Old French \"bagage\" (from \"baguer\" \"tie up\") or from \"bagues\" (\"bundles\"). It may also be related to the word \"bag\". \n\nAlso according to the Oxford English Dictionary, the word \"luggage\" originally meant inconveniently heavy baggage and comes from the verb \"lug\" and the suffix \"-age\".\n\n\n\nLuggage carriers – light-weight wheeled carts or harnesses on which luggage could be temporarily place or that can be temporarily attached to luggage – date at least to the 1930s, such as in US patent 2,132,316 \"Luggage carrier\" by Anne W. Newton (filed 1937, published 1938). These were refined over the following decades, as reflected in patents such as US patent 2,650,105 A \"Luggage carriage\" (filed 1949, published 1953) and US patent 2,670,969 \"Luggage carriage harness, both by Kent R. Costikyan. However, the wheels were \"external\" to the suitcases. Patents were published for wheeled luggage – a wheeled trunk in 1887, and a wheeled suitcase in 1945 – but these were not successfully commercialized.\n\nThe first commercially successful rolling suitcases was invented in 1970, when Bernard D. Sadow applied for a patent that was granted in 1972 as United States patent 3,653,474 for \"Rolling Luggage\". The patent application cited the increase in air travel, and \"baggage handling [having] become perhaps the single biggest difficulty encountered by an air passenger\", as background of the invention. Sadow's four-wheeled suitcases, pulled using a loose strap, were later surpassed in popularity by suitcases that feature two wheels and are pulled in an upright position using a long handle. These were invented in 1987 by US pilot Robert Plath, and initially sold to crew members. Plath later commercialized them, after travelers became interested after seeing them in use by crew members, and founded the Travelpro company, which marketing the suitcases under the trademark \"Rollaboard\". The terms \"rollaboard\" and \"roll-aboard\" are used generically, however. While initially designed for carry-on use (to navigate through a large terminal), as implied by the analogous name, similar designs are also used for checked baggage.\n\nMore recently, four-wheeled luggage with casters has become popular, notably since their use by Samsonite in the 2004 version of their signature \"Silhouette\" line. These are otherwise similar in design to two-wheel roll-aboards, with a vertical orientation and a retracting handle, but are designed to be pushed beside or in front of the traveler, rather than pulled behind them. These are often referred to as \"spinner\" luggage, since they can spin about their vertical axis.\n\nSadow attributes the late invention of luggage on wheels to a \"macho thing\" where \"men would not accept suitcases with wheels\". Others attribute the late invention to \"the abundance of luggage porters with carts in the 1960s, the ease of curbside drop-offs at much smaller airports and the heavy iron casters then available.\"\n\nSome vehicles have an area specifically for luggage to be held, called the automobile \"trunk\" in the United States. Items stored in the hold are known as hold luggage. A typical example would be a suitcase. If travelling by coach passengers will often be expected to place their own luggage in the hold, before boarding. Aeroplanes in contrast are loaded by professional baggage handlers.\n\nPassengers are allowed to carry a limited number of smaller bags with them in the vehicle, these are known as hand luggage (more commonly referred to as carry-on in North America), and contain valuables and items needed during the journey. There is normally storage space provided for hand luggage, either under seating, or in overhead lockers. Trains often have luggage racks at the ends of the carriage near the doors, or above the seats if there are compartments. There are differing views between North America and Europe in relation to the rules concerning the amount of baggage carried on to aircraft. In North America there is considerable debate as to whether passengers carry too many bags on board and that their weight could be a risk to other passengers and flight safety. US airlines are beginning to introduce weight and size restrictions for carry-on baggage. Whereas in Europe, many airlines, especially low-cost airlines, impose what is commonly known as \"the one-bag rule\". This is a restriction imposed to stop excessive weight on board and airlines claim that this policy allows them to speed the boarding of the aircraft. Airports in Europe have mounted a campaign with the European Commission in an attempt to overturn these hand luggage regulations. They claim that it is affecting their duty-free and other airport retail sales and is reducing their revenues.\n\nSmart luggage is baggage that has a built-in or a removable battery within. It often includes features designed to help with travel, including GPS tracking and USB ports to charge electronics. Some bags include a WiFi hotspot and electric wheels for personal transportation.\n\nSeveral smart luggage companies have shut down as a result of a ban which came into effect in January 2018 on smart luggage with non-removable batteries being carried as check-in luggage on flights.\n\nIn airport terminals, a baggage claim or reclaim area is an area where arriving passengers claim checked-in baggage after disembarking from an airline flight. At most airports and many train stations, baggage is delivered to the passenger on a baggage carousel.\n\nLeft luggage, also luggage storage or bag storage, is a place where one can temporarily store one's luggage so as to not have to carry it. Left luggage is not synonymous with lost luggage. Often at an airport or train station there may be a staffed 'left luggage counter' or simply a coin-operated or automated locker system. With higher threats of terrorism all around the globe, this type of public storage is disappearing.\n\nLuggage forwarding, also known as luggage shipping or luggage logistics, is a type of speciality shipping service that has been available for approximately 10 years and has grown in demand, particularly after the September 11, 2001 attacks. Luggage forwarding is an alternative to checking in baggage during air travel. \n\nBaggage can also refer to the train of people and goods, both military and of a personal nature, which commonly followed pre-modern armies on campaign. The baggage was considered a strategic resource and guarded by a rear guard. Its loss was considered to weaken and demoralize an army, leading to rearguard attacks such as that at the Battle of Agincourt.\n\n"}
{"id": "144839", "url": "https://en.wikipedia.org/wiki?curid=144839", "title": "Bill Mollison", "text": "Bill Mollison\n\nBruce Charles \"Bill\" Mollison (4 May 1928 – 24 September 2016) was an Australian researcher, author, scientist, teacher and biologist. \n\nHe is referred to as the \"father of permaculture.\" Permaculture (a portmanteau of \"permanent agriculture\") is an integrated system of ecological and environmental design which Mollison co-developed with David Holmgren, and which they together envisioned as a perennial and sustainable form of agriculture. In 1974, Mollison began his collaboration with Holmgren, and in 1978 they published their book \"Permaculture One\", which introduced this design system to the general public. \n\nMollison founded The Permaculture Institute in Tasmania, and created the education system to train others under the umbrella of permaculture. This education system of \"train the trainer\", utilized through a formal Permaculture Design Course and Certification (PDC), has taught thousands of people throughout the world how to grow food and be sustainable using permaculture design principles. In 1981 he was awarded with the Right Livelihood Award (also known as \"Alternative Nobelprize\") for his accomplishments in the field of permaculture. \n\nBruce Charles \"Bill\" Mollison was born in 1928, in the Bass Strait fishing village of Stanley located on the north-west part of Tasmania, Australia. He moved from Tasmania to Tyalgum in the Tweed Valley of northern New South Wales in 1987 where he lived for the next decade before returning to Tasmania. He spent his final years in Sisters Beach in north-west Tasmania. He died in Hobart, Tasmania, in 2016, aged 88. He is survived by his fifth wife Lisa, four daughters, and two sons.\n\nMollison left school at age 15 to help run the family bakery. In the following ten years he worked as a shark fisherman, seaman, forester, mill worker, trapper, snarer, tractor-driver and naturalist.\n\nIn 1954, at the age of 26, Mollison joined and worked for the 'Wildlife Survey Section' of the Commonwealth Scientific and Industrial Research Organisation (CSIRO). In the 1960s, he worked as a curator at the Tasmanian Museum. He also worked with the Inland Fisheries Commission, where he was able to resume his field work. In 1966, he entered the University of Tasmania. After he received his degree in bio-geography, he stayed on to lecture and teach, and developed the unit of Environmental Psychology. He retired from teaching in 1979.\n\nMollison's work with the CSIRO laid the foundation for his life-long passion: Permaculture. Mollison told his student Toby Hemenway that the original idea for permaculture came to him in 1959 while he was observing marsupials browsing in the Tasmanian rain forests, because he was \"inspired and awed by the life-giving abundance and rich interconnectedness of this eco-system.\" At that moment, Mollison jotted down the following words in his diary: \"I believe that we could build systems that would function as well as this one does.\" By the late 1960s, he started developing ideas about stable agricultural systems on the southern Australian island state of Tasmania. This resulted from his own personal observations of the growth and use of the industrial-agricultural methods that he believed had rapidly degraded the soil of his native state. In his view, these same methods posed a danger because they were highly dependent on non-renewable resources, and were additionally poisoning land and water, reducing biodiversity, and removing billions of tons of topsoil from previously fertile landscapes. Writes Mollison:\nIn 1974, he and his University of Tasmania student David Holmgren \"jointly evolved a framework for a sustainable agricultural system based on a multi-crop of perennial trees, shrubs, herbs (vegetables and weeds), fungi, and root systems\" for which they coined the word \"permaculture\". \n\nSoon after permaculture was first introduced and then put into practice by the public, Mollison recognized that permaculture principles encompassed a movement that included not only agriculture, horticulture, architecture, and ecology, but also economic systems, land access strategies, and legal systems for businesses and communities:\nHe helped found the first Permaculture Institute, established in 1979 to \"teach the practical design of sustainable soil, water, plant, and legal and economic systems to students worldwide.\" In 1981, the first graduates of the permaculture design course (PDC) that he had helped to initiate, started to design permaculture systems in their respective communities. In this way, the philosophy of permaculture had begun to move beyond its original context in \"land management\" to cover most, if not all, aspects of human life.\n\nIn 1987, Mollison taught the first PDC course that was offered in India. By 2011 there had been over 300,000 such graduates practicing and teaching throughout the world. \n\n\nArticles\n\n\n\n\n"}
{"id": "25057013", "url": "https://en.wikipedia.org/wiki?curid=25057013", "title": "Broadcast and Multicast Service", "text": "Broadcast and Multicast Service\n\nBroadcast and Multicast Service (BCMCS) is an interface for providing broadcast and multicast services in 3GPP2 CDMA2000 mobile networks. BCMCS can be used to transfer light video and audio clips or other data to a large group of mobile subscribers in an efficient manner. To do so, BCMCS is a so-called point-to-multipoint service. This means that multiple users receive the same information using the same radio resources. \n\nBCMCS can be used for two different kind of services. Broadcast services in which all users within the broadcasting area can receive the same information and a multicast services in which only users that have subscribed to the service can receive the information \nAlthough BCMCS can be used for mobile TV, it has some limitations in the capacity that can be used for this kind of services within the network. \n\nEBCMCS is an enhanced version of BCMCS. EBCMCS uses a new radio interface based on OFDM to combat problems with echoes (multipath) in the transmission. \n\n"}
{"id": "53897801", "url": "https://en.wikipedia.org/wiki?curid=53897801", "title": "Chassis Management Controller", "text": "Chassis Management Controller\n\nChassis Management Controller (also known as a CMC) is an embedded system management hardware and software solution to manage multiple servers, networking, and storage.\n\nChassis Management Controllers provide a secure browser-based interface that enables an Information Technology system administrator to take inventory, perform configuration and monitoring tasks, remotely power on/off blade servers, enable alerts for events on servers, as well as components in the blade chassis. It has its own microprocessor and memory and powered by the modular chassis it is plugged into. The discovery is built-in and a Chassis Management Controller has a dedicated internal network. The blade enclosure, which can hold multiple blade servers, provides power, cooling, various interconnects and additional systems management capabilities. Unlike a tower or rack server, a blade server cannot run by itself; it requires a compatible blade enclosure.\n"}
{"id": "56395480", "url": "https://en.wikipedia.org/wiki?curid=56395480", "title": "Chat Garcia Ramilo", "text": "Chat Garcia Ramilo\n\nChat Garcia Ramilo (born 1960, Philippines) is a feminist activist and an Information and communications technology (ICT) pioneer. Since April 2017, she leads the Association for Progressive Communications (APC). She is the Board Chair of the Center for Migrant Advocacy in the Philippines and a Board Member of the Association for Women’s Rights in Development (AWID).\n\nGarcia Ramilo is the Board Chair of the Center for Migrant Advocacy in the Philippines. She has served as a gender expert for various ICT for development initiatives initiatives and institutions such as the World Bank and the United Nations Division for the Advancement of Women. \n\nChat Garcia Ramilo became the executive director of the Association for Progressive Communications after serving as manager of its Communications Women’s Rights Programme for seven years. She manages gender evaluation methodologies and she co-authored Gender Evaluation Methodologies for Internet and ICTs (GEM), which has been used by organisations in over 25 countries.\n\nShe is a co-founder of the Take Back the Tech! campaign, which addresses digital gender violence through technology.\n\n\n"}
{"id": "6724462", "url": "https://en.wikipedia.org/wiki?curid=6724462", "title": "Chumby", "text": "Chumby\n\nThe Chumby is a consumer electronics product formerly made by Chumby Industries, Inc. It is an embedded computer which provides Internet and LAN access via a Wi-Fi connection. Through this connection, the Chumby runs various software widgets. In 2010 Sony introduced a single product based on an offshoot version of Chumby, the Sony Dash.\n\nRoughly resembling a small clock radio, the original Chumby features a small resistive touch-screen housed in a leather and plastic exterior with six color options. Power to the original Chumby and the Chumby 8 is supplied through an AC adapter. A later model, the Chumby One, also offered the option of a 9v backup battery. Related devices, the Infocast 3.5 and Infocast 8, devices manufactured by Best Buy based on the Chumby software, are also only AC powered. The device is designed to be customizable by users: after agreeing to the Chumby HDK License, users may download schematics and other hardware information. \"Wired\" magazine named Chumby one of its top gadgets for 2008. Its software is mostly open source, running on Linux.\n\nIn 2012, Chumby ceased operation and was liquidated, with the assets being purchased by Duane Maxwell, the former Chief Technology Officer of Chumby Industries, who formed Blue Octy, LLC. The server needed to keep the devices running were kept online as a full service by Blue Octy. LLC until March 2013. At that point, the server went offline and all devices only displayed a single widget, referred to as the \"Space Clock.\" Blue Octy, LLC relaunched the full Chumby service on July 1, 2014 as a paid subscription service, currently charging $3 USD per month. An open source firmware is available for free that allows existing devices some of the functionality of the paid service at no cost. Devices without a subscription still receive the Space Clock widget.\n\nAndrew \"bunnie\" Huang was the lead hardware engineer at Chumby.\nThe Chumby premiered on August 25, 2006 at Foo Camp and was released to around 100 alpha release testers at the event.\n\nShortly after Foo Camp, Chumby announced a free offer, where applicants would receive the same alpha-level Chumby as those previously given away. Applicants submitted ideas for software applications or hardware modifications. One of the goals for the free offer was to have Chumbys in the hands of developers who were willing to begin building applications.\n\nIn July 2007, a \"First 50\" was released to 50 random applicants, who received the next generation of Chumbys. This was followed, in September, with an \"Insiders Release\". Interested parties could send e-mail to Chumby requesting release information, and were given the opportunity to join in the \"Insiders Release\". Finally, in February 2008, the commercial release was made public on the Chumby Store. In May 2008, the price was $179.95 for any one of three colors, latte, basic black, and pearl. In Japan, Chumby was available through Zyyx, Inc. as www.chumby.jp since October 23, 2008. In Australia, the Chumby was available through ISP Internode.\n\nIn November 2009 the Chumby One was released: a similar, all-plastic version of the original in white with blue trim. The major difference was the hard plastic case replacing the soft leather. Other changes include a slightly faster processor, only one USB port on the rear of the device, and inclusion of an FM tuner and physical volume knob. The hard plastic case allowed Chumby Industries to offer the Chumby One at a reduced price of $119.95.\n\nIn April 2012, Chumby announced the cessation of hardware sales, having ceased manufacture of their own hardware the previous year and exhausted their inventory. On April 20 it was confirmed that the company itself was being broken up. Dedicated fans managed to keep the service running for a period following the company's demise, but on 20 February 2013 Chumby shut down its servers, leaving users with a simple clock that shows time, calendar, and date. A brief message appears on the Chumby Web site, explaining the suspension of service. Alternative open source firmware prepared for such an eventuality became available at this point. \n\n, Blue Octy was in the process of reviving the chumby technology, with one of the original chumby developers working on the project. Visiting www.chumby.com shows details.\n\nTowards the end of March 2014, Blue Octy began beta testing the soon to be revived chumby service.\n\nOn July 1, 2014, Blue Octy relaunched the chumby service as a sustainable, subscription based platform.\nIn July 2017, Blue Octy and Chumby undertook an effort to rescue the Sony Dash after Sony discontinued support for it. \n\nIn August 2017, Blue Octy and Chumby released a patch for the Sony Dash HID-C10 models to allow them to connect to the Chumby servers, thus extending their useful life. \n\nThe Chumby is designed to be modified by users, with schematics, printed circuit board layouts and packaging/outerware designs available. Hardware specifications are as follows\n\nThe Original Chumby\n\nThe Chumby One \n\nComparison Table\nHacking the Chumby hardware was encouraged by the manufacturer. Schematics and other hardware information may be downloaded after the user agrees to the Chumby HDK License. For example, users on the Chumby Forums have experimented with and documented some battery hacks, allowing the Chumby to be operated without AC power for short periods of time.\n\nThere also exists a Chumby Hacker Board that mostly resembles a Chumby One motherboard. There are some differences to hardware connectivity. Chumby Industries did not officially support the board.\n\nChumby units run a modified Linux kernel. The software originally installed on the device was designed to play a set of user-customizable widgets, small Adobe Flash animations that deliver real-time information. This is possible, because an embedded version of Adobe Flash Player is installed. The animations have the ability to control and interact with the low-level hardware, thereby enabling functionality such as smart alarm clocks that bring the hardware out of sleep, a Web-based picture viewer, a Web-based camera, online RSS feeds, and physical user interface features, such as gesture recognition by squeezing the soft housing.\n\nThe software for the Chumby automatically updated when something new became available. The updates came from the free access to the Chumby network, and a modified BitTorrent client was used to upgrade the open-source portions of its firmware.\n\nAlthough the prototypes did not support video playback, all versions since May 2007 use Flash Lite 3 which allows for Sorenson, FLV, H.264, VP6 and On2 video playback.\n\n\n\n"}
{"id": "10625295", "url": "https://en.wikipedia.org/wiki?curid=10625295", "title": "Dalén light", "text": "Dalén light\n\nA Dalén light is a light produced from burning of carbide gas (acetylene), combined with a solar sensor which automatically operates the light only during darkness.\n\nThe technology was the predominant form of light source in lighthouses from the 1900s through the 1960s, when electric lighting had become dominant. The system was invented by Gustaf Dalén and marketed by his company AGA. Dalén later invented the AGA cooker in 1922. The Dalén light is notable because of its sun valve (a.k.a. solar valve), which earned its inventor the Nobel prize in physics. The Carbide lamp was developed in the early 1900s. While the lamps proved useful in many applications, the problem of safely storing acetylene meant they needed regular refilling which constrained their use in applications such as lighthouses.\n\nLighthouses using Dalén lighting have included:\n\n"}
{"id": "19042247", "url": "https://en.wikipedia.org/wiki?curid=19042247", "title": "Dark fermentation", "text": "Dark fermentation\n\nDark fermentation is the fermentative conversion of organic substrate to biohydrogen. It is a complex process manifested by diverse groups of bacteria, involving a series of biochemical reactions using three steps similar to anaerobic conversion. Dark fermentation differs from photofermentation in that it proceeds without the presence of light.\n\nFermentative/hydrolytic microorganisms hydrolyze complex organic polymers to monomers which are further converted to a mixture of lower-molecular-weight organic acids and alcohols by obligatory producing acidogenic bacteria.\n\nUtilization of wastewater as a potential substrate for biohydrogen production has been drawing considerable interest in recent years especially in the dark fermentation process. Industrial wastewater as a fermentative substrate for H production addresses most of the criteria required for substrate selection viz., availability, cost and biodegradability (Angenent, \"et al.\", 2004; Kapdan and Kargi, 2006). Chemical wastewater (Venkata Mohan, \"et al.\", 2007a,b), cattle wastewater (Tang, \"et al.\", 2008), dairy process wastewater (Venkata Mohan, \"et al.\" 2007c, Rai et al. 2012), starch hydrolysate wastewater (Chen, \"et al.\", 2008) and designed synthetic wastewater (Venkata Mohan, \"et al.\", 2007a, 2008b) have been reported to produce biohydrogen apart from wastewater treatment from dark fermentation processes using selectively enriched mixed cultures under acidophilic conditions. Various wastewaters viz., paper mill wastewater (Idania, \"et al.\", 2005), starch effluent (Zhang, \"et al.\", 2003), food processing wastewater (Shin \"et al.\", 2004, van Ginkel, \"et al.\", 2005), domestic wastewater (Shin, \"et al.\", 2004, 2008e), rice winery wastewater (Yu \"et al.\", 2002), distillery and molasses based wastewater (Ren, \"et al.\", 2007, Venkata Mohan, \"et al.\", 2008a), wheat straw wastes (Fan, \"et al.\", 2006) and palm oil mill wastewater (Vijayaraghavan and Ahmed, 2006) have been studied as fermentable substrates for H production along with wastewater treatment. Using wastewater as a fermentable substrate facilitates both wastewater treatment apart from H production. The efficiency of the dark fermentative H production process was found to depend on pre-treatment of the mixed consortia used as a biocatalyst, operating pH, and organic loading rate apart from wastewater characteristics (Venkata Mohan, \"et al.\", 2007d, 2008c, d, Vijaya Bhaskar, \"et al.\", 2008d).\n\nIn spite of its advantages, the main challenge observed with fermentative H production processes is the relatively low energy conversion efficiency from the organic source. Typical H yields range from 1 to 2 mol of H/mol of glucose, which results in 80-90% of the initial COD remaining in the wastewater in the form of various volatile organic acids (VFAs) and solvents, such as acetic acid, propionic acid, butyric acid, and ethanol. Even under optimal conditions about 60-70% of the original organic matter remains in solution. Bioaugmentation with selectively enriched acidogenic consortia to enhance H production was also reported (Venkata Mohan, \"et al.\", 2007b). Generation and accumulation of soluble acid metabolites causes a sharp drop in the system pH and inhibits the H production process. Usage of unutilized carbon sources present in acidogenic process for additional biogas production sustains the practical applicability of the process. One way to utilize/recover the remaining organic matter in a usable form is to produce additional H by terminal integration of photo-fermentative processes of H production (Venkata Mohan, \"et al.\" 2008e, Rai et al. 2012) and methane by integrating acidogenic processes to terminal methanogenic processes.\n\n\n"}
{"id": "2090314", "url": "https://en.wikipedia.org/wiki?curid=2090314", "title": "Digital cinematography", "text": "Digital cinematography\n\nDigital cinematography is the process of capturing (recording) a motion picture using digital image sensors rather than through film stock. As digital technology has improved in recent years, this practice has become dominant. Since the mid-2010s, most of the movies across the world are captured as well as distributed digitally.\n\nMany vendors have brought products to market, including traditional film camera vendors like Arri and Panavision, as well as new vendors like RED, Blackmagic, Silicon Imaging, Vision Research and companies which have traditionally focused on consumer and broadcast video equipment, like Sony, GoPro, and Panasonic.\n\n, professional 4K digital film cameras are approximately equal to 35mm film in their resolution and dynamic range capacity, however, digital film still has a slightly different look to analog film. Some filmmakers still prefer to use analogue picture formats to achieve the desired results.\n\nBeginning in the late 1980s, Sony began marketing the concept of \"electronic cinematography,\" utilizing its analog Sony HDVS professional video cameras. The effort met with very little success. However, this led to one of the earliest high definition video shot feature movies, \"Julia and Julia\" (1987).\n\n\"\" (1996) was the world's first film utilizing extensive digital post production techniques. Shot entirely with Sony's first Solid State Electronic Cinematography cameras and featuring over 35 minutes of digital image processing and visual effects, all post production, sound effects, editing and scoring were completed digitally. The Digital High Definition image was transferred to 35mm negative via electron beam recorder for theatrical release.\n\nThe first digitally filmed and post produced feature film was \"Windhorse\", shot in Tibet and Nepal in 1996 on a prototype of the digital-beta Sony DVW-700WS and the prosumer Sony DCE-VX1000. The offline editing (avid) and the online post and color work (Roland House / da Vinci) were also all digital. The film, transferred to 35mm negative for theatrical release, won Best U.S. Feature at the Santa Barbara Film Festival in 1998.\n\nIn 1998, with the introduction of HDCAM recorders and 1920 × 1080 pixel digital professional video cameras based on CCD technology, the idea, now re-branded as \"digital cinematography,\" began to gain traction in the market. Shot and released in 1998, \"The Last Broadcast\" is believed by some to be the first feature-length video shot and edited entirely on consumer-level digital equipment.\n\nIn May 1999 George Lucas challenged the supremacy of the movie-making medium of film for the first time by including footage filmed with high-definition digital cameras in \"\". The digital footage blended seamlessly with the footage shot on film and he announced later that year he would film its sequels entirely on hi-def digital video. Also in 1999, digital projectors were installed in four theaters for the showing of \"The Phantom Menace\". In June 2000, \"\" began principal photography shot entirely using a Sony HDW-F900 camera as Lucas had previously stated. The film was released in May 2002. In May 2001 \"Once Upon a Time in Mexico\" was also shot in 24 frame-per-second high-definition digital video, partially developed by George Lucas using a Sony HDW-F900 camera, following Robert Rodriguez's introduction to the camera at Lucas' Skywalker Ranch facility whilst editing the sound for \"Spy Kids\". Two lesser-known movies, \"Vidocq\" (2001) and \"Russian Ark\" (2002), had also been shot with the same camera, the latter notably consisting of a single long take.\n\nToday, cameras from companies like Sony, Panasonic, JVC and Canon offer a variety of choices for shooting high-definition video. At the high-end of the market, there has been an emergence of cameras aimed specifically at the digital cinema market. These cameras from Sony, Vision Research, Arri, Silicon Imaging, Panavision, Grass Valley and Red offer resolution and dynamic range that exceeds that of traditional video cameras, which are designed for the limited needs of broadcast television.\n\nIn 2009, \"Slumdog Millionaire\" became the first movie shot mainly in digital to be awarded the Academy Award for Best Cinematography and the highest-grossing movie in the history of cinema, \"Avatar\", not only was shot on digital cameras as well, but also made the main revenues at the box office no longer by film, but digital projection.\n\nIn late 2013, Paramount became the first major studio to distribute movies to theaters in digital format eliminating 35mm film entirely. \"Anchorman 2\" was the last Paramount production to include a 35mm film version, while \"The Wolf of Wall Street\" was the first major movie distributed entirely digitally.\n\nDigital cinematography captures motion pictures digitally in a process analogous to digital photography. While there is no clear technical distinction that separates the images captured in digital cinematography from video, the term \"digital cinematography\" is usually applied only in cases where digital acquisition is substituted for film acquisition, such as when shooting a feature film. The term is seldom applied when digital acquisition is substituted for video acquisition, as with live broadcast television programs.\n\nProfessional cameras include the Sony CineAlta(F) Series, Blackmagic Cinema Camera, RED ONE, Arriflex D-20, D-21 and Alexa, Panavisions Genesis, Silicon Imaging SI-2K, Thomson Viper, Vision Research Phantom, IMAX 3D camera based on two Vision Research Phantom cores, Weisscam HS-1 and HS-2, GS Vitec noX, and the Fusion Camera System. Independent filmmakers have also pressed low-cost consumer and prosumer cameras into service for digital filmmaking.\n\nDigital cinematography cameras capture images using CMOS or CCD sensors, usually in one of two arrangements.\n\nSingle chip cameras designed specifically for the digital cinematography market often use a single sensor (much like digital photo cameras), with dimensions similar in size to a 16 or 35 mm film frame or even (as with the Vision 65) a 65 mm film frame. An image can be projected onto a single large sensor exactly the same way it can be projected onto a film frame, so cameras with this design can be made with PL, PV and similar mounts, in order to use the wide range of existing high-end cinematography lenses available. Their large sensors also let these cameras achieve the same shallow depth of field as 35 or 65 mm motion picture film cameras, which many cinematographers consider an essential visual tool.\n\nUnlike other video formats, which are specified in terms of vertical resolution (for example, 1080p, which is 1920×1080 pixels), digital cinema formats are usually specified in terms of horizontal resolution. As a shorthand, these resolutions are often given in \"\"n\"K\" notation, where \"n\" is the multiplier of 1024 such that the horizontal resolution of a corresponding \"full-aperture\", digitized film frame is exactly formula_1 pixels. Here the \"K\" has a customary meaning corresponding to the binary prefix \"kibi\" (ki).\n\nFor instance, a 2K image is 2048 pixels wide, and a 4K image is 4096 pixels wide. Vertical resolutions vary with aspect ratios though; so a 2K image with an HDTV (16:9) aspect ratio is 2048×1152 pixels, while a 2K image with a SDTV or Academy ratio (4:3) is 2048×1536 pixels, and one with a Panavision ratio (2.39:1) would be 2048×856 pixels, and so on. Due to the \"\"n\"K\" notation not corresponding to specific horizontal resolutions per format a 2K image lacking, for example, the typical 35mm film soundtrack space, is only 1828 pixels wide, with vertical resolutions rescaling accordingly. This led to a plethora of motion-picture related video resolutions, which is quite confusing and often redundant with respect to nowadays few projection standards.\n\nAll formats designed for digital cinematography are progressive scan, and capture usually occurs at the same 24 frame per second rate established as the standard for 35mm film. Some films such as \"\" have a High Frame Rate of 48 fps, although in some theatres it was also released in a 24 fps version which many fans of traditional film prefer.\n\nThe DCI standard for cinema usually relies on a 1.89:1 aspect ratio, thus defining the maximum container size for 4K as 4096×2160 pixels and for 2K as 2048×1080 pixels. When distributed in the form of a Digital Cinema Package (DCP), content is letterboxed or pillarboxed as appropriate to fit within one of these container formats.\nIn the early years of digital cinematography, 2K was the most common format for digitally acquired major motion pictures however, as new camera systems gain acceptance, 4K is becoming more prominent. The Arri Alexa captured a 2.8k image. During 2009 at least two major Hollywood films, \"Knowing\" and \"District 9\", were shot in 4K on the RED ONE camera, followed by \"The Social Network\" in 2010. , 4k cameras are now commonplace, with most high-end films being shot at 4k resolution.\n\nBroadly, two workflow paradigms are used for data acquisition and storage in digital cinematography.\n\nWith video-tape-based workflow, video is recorded to tape on set. This video is then ingested into a computer running non-linear editing software, using a deck. Upon ingestion, a digital video stream from tape is converted to computer files. These files can be edited directly or converted to an intermediate format for editing. Then video is output in its final format, possibly to a film recorder for theatrical exhibition, or back to video tape for broadcast use. Original video tapes are kept as an archival medium. The files generated by the non-linear editing application contain the information necessary to retrieve footage from the proper tapes, should the footage stored on the computer's hard disk be lost. With increasing convenience of file-based workflows, the tape-based workflows have become marginal in recent years.\n\nDigital cinematography has mostly shifted towards \"tapeless\" or \"file-based\" workflows. This trend has accelerated with increased capacity and reduced cost of non-linear storage solutions such as hard disk drives, optical discs, and solid-state memory. With tapeless workflows digital video is recorded as digital files onto random-access media like optical discs, hard disk drives or flash memory-based digital \"magazines\". These files can be easily copied to another storage device, typically to a large RAID (array of computer disks) connected to an editing system. Once data is copied from the on-set media to the storage array, they are erased and returned to the set for more shooting.\n\nSuch RAID arrays, both of \"managed\" (for example, SANs and NASs) and \"unmanaged\" (for example, JBoDs on a single computer workstation), are necessary due to the throughput required for real-time (320 MB/s for 2K @ 24fps) or near-real-time playback in post-production, compared to throughput available from a single, yet fast, hard disk drive. Such requirements are often termed as \"on-line\" storage. Post-production not requiring real-time playback performances (typically for lettering, subtitling, versioning and other similar visual effects) can be migrated to slightly slower RAID stores.\n\nShort-term archiving, \"if ever\", is accomplished by moving the digital files into \"slower\" RAID arrays (still of either managed and unmanaged type, but with lower performances), where playback capability is poor to non-existent (unless via proxy images), but minimal editing and metadata harvesting still feasible. Such intermediate requirements easily fall into the \"mid-line\" storage category.\n\nLong-term archiving is accomplished by backing up the digital files from the RAID, using standard practices and equipment for data backup from the IT industry, often to data tapes (like LTOs).\n\nMost digital cinematography systems further reduce data rate by subsampling color information. Because the human visual system is much more sensitive to luminance than to color, lower resolution color information can be overlaid with higher resolution luma (brightness) information, to create an image that looks very similar to one in which both color and luma information are sampled at full resolution. This scheme may cause pixelation or color bleeding under some circumstances. High quality digital cinematography systems are capable of recording full resolution color data (4:4:4) or raw sensor data.\n\nMost compression systems used for acquisition in the digital cinematography world compress footage one frame at a time, as if a video stream is a series of still images. This is called intra-frame compression. Inter frame compression systems can further compress data by examining and eliminating redundancy between frames. This leads to higher compression ratios, but displaying a single frame will usually require the playback system to decompress a number of frames from before & after it. In normal playback this is not a problem, as each successive frame is played in order, so the preceding frames have already been decompressed. In editing, however, it is common to jump around to specific frames and to play footage backwards or at different speeds. Because of the need to decompress extra frames in these situations, inter-frame compression can cause performance problems for editing systems. Inter-frame compression is also disadvantageous because the loss of a single frame (say, due to a flaw writing data to a tape) will typically ruin all the frames until the next keyframe occurs. In the case of the HDV format, for instance, this may result in as many as 6 frames being lost with 720p recording, or 15 with 1080i. An inter-frame compressed video stream consists of groups of pictures (GOPs), each of which has only one full frame, and a handful of other frames referring to this frame. If the full frame, called I-frame, is lost due to transmission or media error, none of the P-frames or B-frames (the referenced images) can be displayed. In this case, the whole GOP is lost.\n\nFor theaters with digital projectors, digital films may be distributed digitally, either shipped to theaters on hard drives or sent via the Internet or satellite networks. Digital Cinema Initiatives, LLC, a joint venture of Disney, Fox, MGM, Paramount, Sony Pictures Entertainment, Universal and Warner Bros. Studios, has established standards for digital cinema projection. In July 2005, they released the first version of the Digital Cinema System Specification, which encompasses 2K and 4K theatrical projection. They also offer compliance testing for exhibitors and equipment suppliers.\n\nTheater owners initially balked at installing digital projection systems because of high cost and concern over increased technical complexity. However new funding models, in which distributors pay a \"digital print\" fee to theater owners, have helped to alleviate these concerns. Digital projection also offers increased flexibility with respect to showing trailers and pre-show advertisements and allowing theater owners to more easily move films between screens or change how many screens a film is playing on, and the higher quality of digital projection provides a better experience to help attract consumers who can now access high-definition content at home. These factors have resulted in digital projection becoming an increasingly attractive prospect for theater owners, and the pace of adoption has been rapidly increasing.\n\nSince some theaters currently don't have digital projection systems, even if a movie is shot and post-produced digitally, it must be transferred to film if a large theatrical release is planned. Typically, a film recorder will be used to print digital image data to film, to create a 35 mm internegative. After that the duplication process is identical to that of a traditional negative from a film camera.\n\nUnlike a digital sensor, a film frame does not have a regular grid of discrete pixels.\n\nDetermining resolution in digital acquisition seems straightforward, but it is significantly complicated by the way digital camera sensors work in the real world. This is particularly true in the case of high-end digital cinematography cameras that use a single large bayer pattern CMOS sensor. A bayer pattern sensor does not sample full RGB data at every point; instead, each pixel is biased toward red, green \"or\" blue, and a full color image is assembled from this checkerboard of color by processing the image through a demosaicing algorithm. Generally with a bayer pattern sensor, actual resolution will fall somewhere between the \"native\" value and half this figure, with different demosaicing algorithms producing different results. Additionally, most digital cameras (both bayer and three-chip designs) employ optical low-pass filters to avoid aliasing; suboptimal antialiasing filtering can further reduce system resolution.\n\nFilm has a characteristic grain structure. Different film stocks have different grain.\n\nDigitally acquired footage lacks this grain structure. It has electronic noise.\n\nThe process of using digital intermediate workflow, where movies are color graded digitally instead of via traditional photochemical finishing techniques, has become common.\n\nIn order to utilize digital intermediate workflow with film, the camera negative must first be processed and then scanned to a digital format. Some filmmakers have years of experience achieving their artistic vision using the techniques available in a traditional photochemical workflow, and prefer that finishing/editing process.\n\nDigitally shot movies can be printed, transferred or archived on film. Large scale digital productions are often archived on film, as it provides a safer medium for storage, benefiting insurance and storage costs. As long as the negative does not completely degrade, it will always be possible to recover the images from it in the future, regardless of changes in technology, since all that will be involved is simple photographic reproduction.\n\nIn contrast, even if digital data is stored on a medium that will preserve its integrity, highly specialized digital equipment will always be required to reproduce it. Changes in technology may thus render the format unreadable or expensive to recover over time. For this reason, film studios distributing digitally-originated films often make film-based separation masters of them for archival purposes.\n\nFilm proponents have argued that digital cameras lack the reliability of film, particularly when filming sequences at high speed or in chaotic environments, due to digital cameras' technical glitches. Cinematographer Wally Pfister noted that for his shoot on the film \"Inception\", \"Out of six times that we shot on the digital format, we only had one useable piece and it didn't end up in the film. Out of the six times we shot with the Photo-Sonics camera and 35mm running through it, every single shot was in the movie.\" Michael Bay stated that when filming \"\", 35mm cameras had to be used when filming in slow-motion and sequences where the digital cameras were subject to strobing or electrical damage from dust.\n\nSome film directors such as Christopher Nolan, Paul Thomas Anderson and Quentin Tarantino have publicly criticized digital cinema, and advocated the use of film and film prints. Tarantino has suggested he may retire because he will no longer be able to have his films projected in 35mm in most American cinemas. Tarantino considers digital cinema to be simply \"television in public.\" Christopher Nolan has speculated that the film industry's adoption of digital formats has been driven purely by economic factors as opposed to digital being a superior medium to film: \"I think, truthfully, it boils down to the economic interest of manufacturers and [a production] industry that makes more money through change rather than through maintaining the status quo.\"\n\nAnother concern with digital image capture is how to archive all the digital material. Archiving digital material is turning out to be extremely costly, and it creates issues in terms of long-term preservation. In a 2007 study, the Academy of Motion Picture Arts and Sciences found that the cost of storing 4K digital masters is \"enormously higher – 1100% higher – than the cost of storing film masters.\" Furthermore, digital archiving faces challenges due to the insufficient longevity of today's digital storage: no current media, be it magnetic hard drives or digital tape, can reliably store a film for a hundred years, something that properly stored and handled film can do. Although this also used to be the case with optical disc, in 2012 Millenniata, Inc. a digital storage company based in Utah, released M-DISC, an optical storage solution, designed to last up to 1,000 years, thus, offering a possibility of digital storage as a viable storage solution.\n\n"}
{"id": "49261105", "url": "https://en.wikipedia.org/wiki?curid=49261105", "title": "Directed assembly of micro- and nano-structures", "text": "Directed assembly of micro- and nano-structures\n\nDirected assembly of micro- and nano-structures are methods of mass-producing micro to nano devices and materials. Directed assembly allows the accurate control of assembly of micro and nano particles to form even the most intricate and highly functional devices or materials.\n\nDirected self-assembly (DSA) is a type of directed assembly which utilizes block co-polymer morphology to create lines, space and hole patterns, facilitating for a more accurate control of the feature shapes. Then it uses surface interactions as well as polymer thermodynamics to finalize the formation of the final pattern shapes. To control the surface interactions enabling sub-10 nm resolution, a team of Massachusetts Institute of Technology, University of Chicago, and Argonne National Laboratory developed a way to use vapor-phase deposited polymeric top layer on the block co-polymer film in 2017.\n\nThe DSA is not a standalone process, but rather are integrated with traditional manufacturing processes in order to mass-produce micro and nano structures at a lower cost. Directed self-assembly is mostly used in the semiconductor and hard drive industries. Semiconductors industries uses this assembly method in order to be able increase the resolution (trying to fit in more gates), while the hard drive industry uses DSA to manufacture “bit patterned media” according to the specified storage densities.\n\nThere are many applications of directed assembly in the micro-scale, from tissue engineering to polymer thin-films. In tissue engineering, directed assembly have been able to replace scaffolding approach of building tissues. This happens by controlling the position and organisation of different cells, which are the “building-blocks” of the tissue, into different desired micro-structures. This eliminates the error of not being able to reproduce the same tissue, which is a major issue in the scaffolding approach.\n\nNano-technology provides methods to organizing materials such as molecules, polymers, building blocks, etc. to form the precise nano-structures which have many applications. In the process and application of peptide self-assembly into nano tubes, the single-wall carbon nano tubes is an example which consists of a graphene sheet seamlessly wrapped to a cylinder. This produced in the outside flow of a carbon and yield by laser vaporization of graphite enriched by a transition metal.\n\nNano-imprint lithography is a popular method to fabricate nano-meter scale pattern. The patterns are made by mechanical deformation of imprint resist (monomer or polymer formulation) and subsequent processes. Then, it is cured by heat or ultraviolet light, and tight level of the resist and template is controlled at appropriate conditions depend on our purposes. In addition, nano-imprint lithography has high resolution and throughput with low cost. Disadvantages include increased time for templating procedures, a lack of standard procedures results in multiple fabrication methods, and the patterns that are able to be formed are limited.\n\nWith the goal of mitigating these advantages while applying nanotechnology to electronics, researchers at the National Science Foundation’s Nano-scale Science and Engineering Center for High-Rate Nanomanufacturing (CHN) at Northeastern University with partners UMass Lowell and University of New Hampshire have developed a directed assembly process of single-walled carbon nano tube (SWNT) networks to create a circuit template that can be transfer from one substrate to another.\n\nSelf-assembled monolayers (SAMs) are made of a layer of organic molecules which forms naturally as an ordered lattice on the surface of a desired substrate. Their molecules in the lattice have connections chemically at one end (head group), while the other end (end group) creates the exposed surface of the SAM.\n\nMany types of SAMs can be formed. For example: thiols form SAMs on gold, silver, copper, or on some compound semiconductors such as InP and GaAs. By changing the tail group of the molecules, different surface properties can be obtained; therefore SAMs can be used to render surfaces hydrophobic or hydrophilic as well as change surface states of semiconductor. With self-assembly, positioning of SAMs is used to define chemical system precisely to find the target location in a molecular-inorganic device. With this characteristic, SAMs is a good candidates for molecular electronic devices such as use SAMs to build electronic devices and maybe the circuits is an intriguing prospect. Because of their ability to provide the basis for very high-density data storage and high-speed devices.\n\nDirected assembly using the acoustic methods manipulate waves in order to allow non-invasive assembling of micro and nano structures. Due to this, acoustics are especially widely used in the biomedical industry to manipulate droplets, cells and other molecules.\n\nAcoustic waves are generated by a piezoelectric transducer controlled from the pulse generator. These waves are able to then manipulate droplets of liquid and move them together, in order to form a packed assembly. Moreover, the frequency and amplitude of the waves can be modified in order to achieve a more accurate control of the particular behavior of the droplet or cell.\n\nDirected assembly or more specifically directed self-assembly, can produce a high pattern resolution (~10 nm) with high efficiency and compatibility. However, when using DSA in high volume manufacturing, one must have a way to quantify the degree of order of line/space patterns formed by DSA in order to reduce defect.\n\nNormal approach such as critical dimension-scanning electron microscopy (CD-SEM) to obtain data for pattern quality inspection takes too much time and is also labor-intensive. On the other hand, the optical scatterometer-based metrology is a non-invasive technique and has very high throughput due to its larger spot size. These results in the collection of more statistical data than by using SEM, and that data processing is also automated with the optical technique making it more feasible than traditional CD-SEM.\n\nMagnetic field directed self-assembly (MFDSA) allows the manipulation of dispersion and subsequent assembly of magnetic nanoparticles. This is widely used in the development of advanced materials whereby inorganic nanoparticles (NPs) are dispersed in polymers, in order to enhance the properties of the materials.\n\nThe magnetic field technique allows the assembling of particles in 3D by doing the assembly in a dilute suspension where the solvent does not evaporate. It also does not need to use a template, and the approach also improve the magnetic anisotropy along the chain direction.\n\nDielectrophoretic directed self-assembly utilizes an electric field that controls metal particles, such as gold nanorods, by inducing a dipole in the particles. By varying the polarity and strength of the electric field, the polarized particles are either attracted to positive regions or repelled from negative regions where the electric field has higher strength. This direct manipulation method transports the particles to position and orient them into a nano-structure on a receptor substrate.\n"}
{"id": "3225945", "url": "https://en.wikipedia.org/wiki?curid=3225945", "title": "DuPont Pioneer", "text": "DuPont Pioneer\n\nDuPont Pioneer, formerly Pioneer Hi-Bred is a large U.S. producer of hybrid seeds for agriculture. They are a major producer of genetically modified organisms (GMOs), including genetically modified crops with insect and herbicide resistance.\n\nIn 1926, farm journal editor and future U.S. Vice President Henry A. Wallace, along with a group of Des Moines, Iowa businessmen, founded the \"Hi-Bred Corn Company\". Wallace had been experimenting with hybridization of corn and became convinced that hybrid seed corn would become important.\n\n\nHeadquarters of DuPont Pioneer are located in Johnston, Iowa, with additional offices around the world. Pioneer produces, markets and sells hybrid seed corn in nearly 70 countries worldwide. The company also markets and sells hybrids or improved varieties of sorghum, sunflower, soybean, alfalfa, canola, rice and wheat, as well as forage and grain additives. Worldwide, Pioneer sells products through a variety of organizations, including wholly owned subsidiaries( Curry Seed, NuTech Seed, Hoegemeyer Hybrids, Doeblers Seed, Seed Consultants Inc, Terral Seeds, AgVenture Inc) joint ventures, sales representatives, and independent dealers (Burrus Hybrids, Beck's Superior Hybrids).\n\nPioneer makes and sells hybrid seed and genetically modified seed, some of which goes on to become genetically modified food. Genes engineered into their products include the LibertyLink gene, which provides resistance to Bayer's Ignite/Liberty herbicides; the Herculex I Insect Protection gene which provides protection against various insects; the Herculex RW insect protection trait which provides protection against other insects; the YieldGard Corn Borer gene, which provides resistance to another set of insects; and the Roundup Ready Corn 2 trait that provides crop resistance against glyphosate herbicides. In 2010 Dupont Pioneer received approval to start marketing Plenish soybeans, which contains \"the highest oleic acid content of any commercial soybean product, at more than 75%\". Plenish is genetically engineered to \"block the formation of enzymes that continue the cascade downstream from oleic acid (that produces saturated fats), resulting in an accumulation of the desirable monounsaturated acid.\"\n\nA lawsuit was filed in 2011 by 150 residents of Waimea, Kauai against Pioneer. The 58-page lawsuit alleges that Pioneer's practices in the farming of genetically modified seed crops on fields next to Waimea unlawfully allowed pesticides and pesticide-laden fugitive dust to blow into residents’ homes on almost a daily basis for more than 10 years.\n\n\n"}
{"id": "18519070", "url": "https://en.wikipedia.org/wiki?curid=18519070", "title": "Elastic interface bus", "text": "Elastic interface bus\n\nElastic interface buses, abbreviated as EI bus connections, can be generalized as bus connections which are high speed interfaces that send clock signals with data. \n\nThe data bits that are sent through EI bus connections are aligned to the clock so that they latch to the data at the high speeds. EI bus connections require that the net topology and timing characteristics for each net on the bus are at least similar to each other in order to make lining up the edges of the data to the clock signals possible. In this environment, re-working connections in the connection module was not easily possible because all nets needed to have similar topology and timing characteristics. This increased the difficulty of a re-work solution or made it impossible and increased the modules that needed to be scrapped as unusable.\n\nElastic Interface repair involves a spare wire that is built into the bus interface in the connection module that has the same topology and characteristics of the rest of the nets in the bus. It includes hardware that is able to switch from the bad net in the interface to the spare net (as of now, this operation must be supported by the original manufacturer of the EI bus connector). The connection module is tested at several different process corners such as low and high temperature and low and high voltages. When a net on the interface is known to be bad, the spare net is used on the bus for testing and the bad net is not tested. When the bus does not have a defect, the spare net is tested with the functional nets. In the original design specification for the EI spare, the wire was driven with a constant zero when not used.\n\n\n"}
{"id": "462546", "url": "https://en.wikipedia.org/wiki?curid=462546", "title": "FICO", "text": "FICO\n\nFICO (legal name: Fair Isaac Corporation), originally Fair, Isaac and Company, is a data analytics company based in San Jose, California focused on credit scoring services. It was founded by Bill Fair and Earl Isaac in 1956. Its FICO score, a measure of consumer credit risk, has become a fixture of consumer lending in the United States.\n\nIn 2013, lenders purchased more than 10 billion FICO scores and about 30 million American consumers accessed their scores themselves.\n\nFICO was founded in 1956 as Fair, Isaac and Company by engineer William Fair and mathematician Earl Isaac. The two met while working at the Stanford Research Institute in Menlo Park, California. Selling its first credit scoring system two years after the company's creation, FICO pitched its system to fifty American lenders.\n\nFICO went public in 1986 and is traded on the New York Stock Exchange. The company debuted its first general-purpose FICO score in 1989. FICO scores are based on credit reports and \"base\" FICO scores range from 300 to 850, while industry-specific scores range from 250 to 900.\n\nLenders use the scores to gauge a potential borrower's creditworthiness.\n\nFannie Mae and Freddie Mac first began using FICO scores to help determine which American consumers qualified for mortgages bought and sold by the companies in 1995.\n\nOriginally called Fair, Isaac and Company (hence the abbreviation FICO), this name was changed to Fair Isaac Corporation in 2003. The company renamed itself FICO in 2009.\n\nOriginally based in San Rafael, California, FICO moved its headquarters to Minneapolis, Minnesota, in 2004. In 2013, it moved back to California and is currently based in San Jose, California.\n\n\nFICO is headquartered in San Jose, California, and it has additional U.S. locations in Roseville, Minnesota; San Diego; San Rafael, California; Fairfax, Virginia; New York City and Austin, Texas.\n\nThe company has international locations in Australia, Brazil, Canada, China, Germany, India, Italy, Japan, Korea, Lithuania, Poland, Malaysia, the Philippines, Russia, Singapore, South Africa, Spain, Taiwan, Thailand, Turkey and the United Kingdom.\n\nA measure of credit risk, FICO scores are available through all of the major consumer reporting agencies in the United States: Equifax, Experian, and TransUnion. FICO scores are also offered in other markets, including Mexico and Canada, as well as through the fourth U.S. credit reporting bureau, PRBC.\n\n"}
{"id": "25683711", "url": "https://en.wikipedia.org/wiki?curid=25683711", "title": "French Aerostatic Corps", "text": "French Aerostatic Corps\n\nThe French Aerostatic Corps or Company of Aeronauts () was the world's first air force, founded in 1794 to use balloons, primarily for reconnaissance.\n\nNumerous suggestions had been made for the use of balloons during the French Revolutionary Wars, and in 1793 the Committee of Public Safety began testing their potential. Initial tests of airship designs proved unsatisfactory. However, experiments conducted near the Tuileries from September to October 1793 to produce the required hydrogen without the use of sulphuric acid, which was in short supply, were successful, producing more than 20 cubic metres. As a result, the Committee determined to use this technique to float tethered balloons.\n\nAt the end of October 1793, chemist Jean-Marie-Joseph Coutelle and his assistant, the engineer Nicolas Lhomond, were sent to join the Army of the North, with 50,000 livre to acquire equipment. They were given a letter from Lazare Carnot commending them to General Jean-Baptiste Jourdan and representative Ernest Dominique François Joseph Duquesnoy, which informed them that \"Citizen Coutelle is not a charlatan\". However, on arrival, Jourdan ridiculed the project, ordering Coutelle back to Paris, with the message that an Austrian attack was imminent, and a battalion was required, not a balloon.\n\nBack in Paris, the Committee of Public Safety ordered further tests on the balloon technology, to be conducted at the Chateau de Meudon, where the Aerostatic Development Centre was founded. Nicolas-Jacques Conté led the research, refining balloon shapes and materials, and also improving the hydrogen production process. This culminated in a series of ascensions, viewed by leading figures on the Committee, who passed an Act creating the Aerostatic Corps on 2 April 1794. The corps consisted of a captain and a lieutenant, a sergeant-major and sergeant, two corporals and twenty privates. All these men were required to have skills relevant to ballooning, such as chemistry or carpentry. The Act creating the corps envisaged three roles: reconnaissance, signalling and the distribution of propaganda. Coutelle was created captain, and Lhomond lieutenant.\n\nIn May 1794, the new corps joined Jourdan's troops at Mauberge, bringing one balloon: \"L'Entreprenant\". They began by constructing a furnace, then extracting hydrogen. The first military use of the balloon was on 2 June, when it was used for reconnaissance during an enemy bombardment. On 22 June, the corps received orders to move the balloon to the plain of Fleurus, in front of the Austrian troops at Charleroi. This was achieved by twenty soldiers who dragged the inflated balloon across thirty miles of ground. For the three following days, an officer ascended to make further observations. On 26 June, the Battle of Fleurus was fought, and the balloon remained afloat for nine hours, during which Coutelle and Antoine Morlot took notes on the movements of the Austrian Army, dropping them to the ground for collection by the French Army, and also signalled messages using semaphore.\n\nThe French won the Battle of Fleurus, but reports of the usefulness of the balloon corps varied. Louis-Bernard Guyton de Morveau, who had been present throughout the battle, strongly supported it, but Jourdan believed that it had contributed little. Guyton had already supervised the construction at Meudon of the \"Martial\", a cylindrical balloon, which was supplied to the corps soon after the battle, but it proved too unstable for use. The corps followed the Army of the North into Belgium and was present at battles in Liege and Brussels, although they did not see action. With winter approaching, they constructed a balloon depot at Borcette near Aachen.\n\nOn 23 June, an Act creating a second aerostatic company had been passed, to be trained at Meudon by Conté. It was provided with two new balloons, \"Hercule\" and \"L'Intrépide\", and in March 1795 it was attached to the Army of the Rhine. Coutelle was recalled in order to head the new company, Lhomond being promoted to captain of the first company, while Conté remained at Meudon. In October, he was made head of a new school of ballooning, where replacement soldiers for the two companies were trained.\n\nThe second company conducted ascensions at the Battle of Mainz, and were also active during the evacuation of Mannheim. For the winter, they established a base at Frankheim, then followed the Army north and conducted observations at Stuttgart, Rastatt and Donauwörth.\n\nMeanwhile, in 1795, the first company was transferred to the Army of Sambre-et-Meuse, which was now led by Jourdan. They were not directly involved in any action, but Jourdan appears to have warmed to the balloonists, printing official correspondence forms depicting a balloon above his army. In September 1796, they were at the Battle of Würzburg when the French Army was defeated, and the entire company was taken captive with its balloon \"L'Intrépide\", which is now on display at the \"Heeresgeschichtliches Museum\" in Vienna.\n\nFollowing this disaster, the second company was attached to the reconstructed Army of Sambre-et-Meuse. Coutelle withdrew to Meudon, overcome by fever, and new commander Delaunay was unable to work with the new General, Lazare Hoche, who refused to let them participate in any action.\n\nThe first company were released in April 1797, under the terms of the Treaty of Leoben, and petitioned for the reinstatement of Coutelle as their commander. This was permitted; Coutelle was made a colonel, while Lhomond was promoted to major and permitted to remain second-in-command. In 1798, the company joined the Napoleonic Campaign in Egypt. On arrival, they decided to initially leave the ballooning equipment on their ship. This was destroyed in the Battle of the Nile, and the company was assigned to other duties. They were able to conduct a few demonstrations of more basic balloons for entertainment purposes.\n\nOn 15 January 1799, the Directory passed an act disbanding the balloon corps. The second company was immediately disbanded, but the first was still in action in Egypt and remained in existence until its return to France in 1802.\n"}
{"id": "28328496", "url": "https://en.wikipedia.org/wiki?curid=28328496", "title": "GADM", "text": "GADM\n\nGADM, the Database of Global Administrative Areas, is a high-resolution database of country administrative areas, with a goal of \"all countries, at all levels, at any time period.\" \n\nThe database is available in a few export formats, including shapefiles that are used in most common GIS applications. Files for use with the data analysis language R are also available. \n\nThe files allow for data analysis as well as the easy creation of descriptive data plots that include geographical maps.\n\nAlthough it is a public database, GADM has a higher spatial resolution than other free databases, and also higher than commercial software such as ArcGIS.\n\nGADM is not freely available for commercial use. The GADM project created the spatial data for many countries from spatial databases provided by national governments, NGO, and/or from maps and lists of names available on the Internet (e.g. from Wikipedia).\n\n"}
{"id": "21884060", "url": "https://en.wikipedia.org/wiki?curid=21884060", "title": "Heat generation in integrated circuits", "text": "Heat generation in integrated circuits\n\nThe heat dissipation in integrated circuits problem has gained an increasing interest in recent years due to the miniaturization of semiconductor devices. The temperature increase becomes relevant for cases of relatively small-cross-sections wires, because such temperature increase may affect the normal behavior of semiconductor devices.\n\nJoule Heating is a predominant heat mechanism for heat generation in integrated circuits and is an undesired effect.\n\nThe governing equation of the physics of the problem to be analyzed is the heat diffusion equation. It relates the flux of heat in space, its variation in time and the generation of power.\n\nWhere formula_2 is the thermal conductivity, formula_3 is the density of the medium, formula_4 is the specific heat\nthe thermal diffusivity and formula_6 is the rate of heat generation per unit volume. Heat diffuses from the source following equation ([eq:diffusion]) and solution in an homogeneous medium of ([eq:diffusion]) has a Gaussian distribution.\n\n"}
{"id": "40878815", "url": "https://en.wikipedia.org/wiki?curid=40878815", "title": "IBM Docs", "text": "IBM Docs\n\nIBM Docs is an interactive software product from IBM for online editing of office documents. The purpose of IBM Docs is to simplify the way individual users or teams edit and review office documents. Users can create word documents, spreadsheets, and presentations independently or in teams, and they can also leverage professional templates to create new documents, or upload existing Microsoft Office and Lotus Symphony files to edit and share. Multiple users are able to edit and review a document simultaneously using co-editing. IBM Docs also enables users to view comments that their colleagues, assigned as Editors, have made on a document.\n\nIBM Docs was developed to support use cases where real time collaboration is needed. It allows up to five editors to simultaneously co-edit a document. For example, if you work with a team on one presentation different colors are assigned to the editors and they can all collaborate on the same document. \nWhen you need to work on a document together but at different times, IBM Docs supports commenting and discussion services allowing conversations to take place inside the document and in context. Comments are anchored to the text, spreadsheet cell, or presentation artifact allowing easy navigation to the context of the comment. Comments can be general comments or directed at a user by using the @ symbol (@userid). Furthermore, it is possiblte to filter comments by author and recipient.\n\nIBM Docs provides capabilities to edit and view documents, spreadsheets and presentations in Microsoft Office binary formats like .ppt, .doc, OpenDocument format like .odt, .ods, as well as Microsoft Office XML based formats like .docx or .xlsx. It is also possible to upload a template in order to create a document, a spreadsheet or presentation from it.\n\nIBM Docs leverages the file management features of IBM Connections. Documents are stored centrally within IBM Connections Files, which allows users to upload, share and manage documents. This helps to make sure that users always work on the most current version and are able restore back to a previous version if needed. IBM Docs lets users share online documents with others as readers or editors. Editors have full rights to edit the document with other editors. Readers can view the most recently published version of a document.\n\nIBM Docs is available on premises and as a cloud offering in IBM SmartCloud for Social Business. To use IBM Docs on premises a deployment of IBM Connections is a prerequisite. To use IBM Docs in the SmartCloud for Social Business customers either have to select the service SmartCloud Engage Advanced or purchase IBM Docs as an Add-on to SmartCloud Engage Standard or SmartCloud Connections.\nIBM Docs is web based and can be accessed through the browser. The supported browsers are Safari, Firefox, Internet Explorer and Google Chrome.\n\nIBM Docs offers a mobile app for the iPad that is embedded in IBM's social network IBM Connections. To use the App you have to download the latest version of the IBM Connections Mobile App from the Appstore and install it on the device. With IBM Docs on the iPad you can edit documents and view presentations and spreadsheets. With the IBM Connections Mobile application (iPad only), you can connect your iPad to a projector and play a slide show for your audience in full-screen while seeing your slides, speaker notes, a timer, and an annotation tool in dual-screen mode. \n\n"}
{"id": "56456840", "url": "https://en.wikipedia.org/wiki?curid=56456840", "title": "James Spilker", "text": "James Spilker\n\nJames Julius Spilker Jr. (August 4, 1933) is an American engineer and a Consulting Professor in the Aeronautics and Astronautics Department at Stanford University. He was one of the principle architects of the Global Positioning System (GPS), and founder of the space communications company Stanford Telecommunications and is currently executive chairman of AOSense Inc., Sunnyvale, CA.\n\nJames Spilker is an elected member of the National Academy of Engineering (1998) and was inducted to the Air Force GPS Hall of Fame (2000) and the Silicon Valley Engineering Hall of Fame (2007). He is a Life Fellow of the IEEE and a Fellow of the Institute of Navigation (ION). As one of the originators of GPS, James Spilker shared in the Goddard Memorial Trophy (2012). He won the Arthur Young Entrepreneur of the Year Award in 1987, the ION Kepler Award (the highest award of the ION) in 1999 and Burka Award in 2002, and the US Air Force Space Command Recognition Award for 9 years of service on GPS Independent Review Team in 2000. In 2015, he received the IEEE Edison Gold Medal for contributions to the technology and implementation of the GPS civilian navigation system.\n\nThe James and Anna Marie Spilker Engineering and Applied Sciences Building at Stanford, and in 2005 co-founded the Stanford University Center for Position, Navigation and Time.\n\nJames J. Spilker Jr. attended Stanford University for 5 years under scholarships, obtained Deans Honors and Hewlett Packard Fellowship, and received BS Degree in 1955, MS Degree in 1956, and Ph.D. in 1958 all in Electrical Engineering. He completed the Senior Management Program at UCLA in 1985.\n\nFrom 1958 to 1963, Spilker worked as a research supervisor at Lockheed Research Labs in Palo Alto, California, where he invented an optimal tracking device for spread-spectrum signals and devised technology to communicate with aircraft flying to/from Berlin when Russia blockaded Berlin.\n\nIn 1963 he became manager of the Communications Sciences Department of Ford Aerospace Corporation where he led and managed efforts on both satellite communications ground terminals and military communications satellite payloads for the first quasi-stationary communications satellites, and developed multiple access technologies for various satellite communications and became Director of Communications Systems.\n\nIn 1973 he co-founded Stanford Telecommunications Inc., the first of his three Silicon Valley startup companies, with three people and no VC funding. As the company’s Executive Chairman, he grew the military satellite communications and GPS company to over 1,300 employees in 5 states when he sold it in 1999.\n\nDuring Spilker’s leadership at Stanford Telecommunications Inc., he also designed semiconductor ASICs (application-specific integrated circuits) for error correction, number-controlled oscillators, and quadrature amplitude modulation. Aviation Week and Space Technology in 1997 ranked Stanford Telecom as the #2 most competitive aerospace company in the world and top 100 fastest growing companies.\n\nSince 2001, Spilker has been a consulting professor at Stanford University in the Electrical Engineering and Aeronautics and Astronautics Department.\n\nIn 2005, Prof. Spilker co-founded the Stanford University Research Center for Position, Navigation and Time, which continues today and has an annual International Symposium at Stanford University with invited speakers from around the world.\n\nIn 2005, Spilker also co-founded AOSense Inc., an atomic physics company specializing in inertial navigation using cold atom interferometry. He is Executive Chairman at AOSense Inc.\n\nHe was also co-founder and chairman of Rosum, a high-tech company using digital and analog television signals for indoor positioning services and augmentation of GPS.\n\nIn 2012, Spilker and his wife, Anna Marie Spilker, a real estate broker and investor, donated $28 million to Stanford University to gift The James and Anna Marie Spilker Engineering and Applied Sciences Building and endow a professorship in the School of Engineering. The building is one of four new structures in Stanford Science and Engineering Quad.\n\nSpilker has been a member of the Stanford University Engineering advisory board, a member of the University of Southern California (USC) Communication Sciences Institute, a member of the US Defense Science Board GPS Task Force, and the Air Force Space Command GPS Independent Review Team. Spilker was an invited lecturer and keynote speaker at Samsung Corporation in Korea (2003), Tsinghua University in Beijing, China (2010), the Marconi lab near Bologna, Italy (1970s), and in Munich and Berlin, Germany (2011).\n\nSpilker is a Member of the National Academy of Engineering (NAE), a member of the NAE Peer Review Committee of the NAE for Electronics, a Life Fellow of the IEEE, and has been the Chairman of the IEEE Technical Advisory Committee.\n\nIn 1961, Spilker published an IRE paper (the predecessor of the IEEE) showing that the optimal tracking system was the delay lock loop (DLL), not the early-late-gate type of delay discriminator. Instead the optimal DLL uses the differentiated signal as the reference and the conventional early-late-gate discriminator is only optimal for a trapezoidal pseudonoise wave shape.\n\nAbsolutely critical to the success of GPS for the civil community, which is by far the largest user of GPS navigation, is its ability to operate in a multiple-access environment. This is achieved by allowing all GPS satellites broadcasting to users on the Earth at the same frequency without interfering with one another using code-division multiple access (CDMA). With CDMA, each satellite transmits in a common spectral band with a noise-like pseudorandom signal. Each satellite transmits a different code with properties that its code has little interference with codes from other satellites.\n\nThe US Air Force GPS Joint Program Office gave Spilker and his small team of two the contract to recommend the GPS satellite signal structure, especially the civil signal called the clear/acquisition (CA) signal for the civil community. The same signal architecture is also used as the acquisition signal for the GPS military signal.\n\nCode families with this low crosscorrelation property have been used for years in fixed point-to-point ground or geostationary satellites and today in cellphones with negligible Doppler shift. These can be easily analyzed using Galois field theory.\n\nHowever, as Spilker pointed out, that zero-Doppler analysis does not apply to GPS with its rapidly moving satellites, and does not reveal the worst case limits of performance. Therefore, critical to the success of GPS is the use of relatively short CDMA codes. Spilker and his team analyzed the worst-case cross-correlation between codes for the L band GPS signals with their +/- 5 KHz Doppler offsets and clearly show that the codes with Doppler offset indeed were the worst case, and he recommended the 1023-period codes even though with no Doppler they were no better than the 511-period codes. Those 1023-period codes are the C/A codes now supporting more than 2 billion users.\n\nSpilker’s results were first published in the Stanford Telecom document for the Air Force, \"Defense Navigation Satellite Special Study\", 130 page Report, April, 1974.\n\nUpon initial launch of the first GPS satellites, there was the need to assure that the precise signal modulation matched bit-by-bit and chip-by-chip with the desired signal. This measurement required a large tracking antenna and special receiver. Since the satellites are exposed to Van Allen belt radiation and vibration effects at launch, these were important tests. Spilker and his team at Stanford Telecommunications successfully designed, implemented, and carried our these GPS in-orbit tests to assure that the P-code chips and other signals were precisely correct.\n\nEven though at the time there were four GPS satellites in orbit, plus on-orbit spares, one of the initial operational tests was to track a navy missile precisely. For this reason, there was an advantage to have a ground-based pseudolite at a fixed site that transmitted the GPS signals in a time-gated manner so as not to interfere with satellite transmissions most of the time. Spilker and his team developed a precision GPS pseudolite that also included a rubidium standard clock for this signal generator and a calibration receiver to assure that the signal matched a selected GPS satellite code. This time-gated pseudolite signal along with the in-orbit GPS satellites permitted precisely tracking of the navy missile and the program was a success.\n\nThe operational GPS control segment is crucial to GPS to compute precision orbits and clock errors for each of the GPS satellites – information that must be transmitted to each and every GPS receiver with precision. Spilker proposed, designed, and implemented special monitor station receivers, part of the new IBM/Stanford Telecom control segment that performed precision coherent code/carrier satellite tracking from horizon to horizon with GPS pseudorange rms error of only 7 mm. This performance is described by IBM in the Parkinson/Spilker GPS book.\n\nThe initial GPS civil signal operated only on one frequency at L1 band (1.57542 GHz). It did not permit computation of the excess delay of the ionosphere and only operated at the lower clock rate of 1.023 M chip/s. The modernized GPS civil signal was designed by Spilker and A. J. van Dierendonck, who received the ION Burka award for their contribution. This so called L5 signal operates on a lower L band frequency and has a 10 times higher clock rate and a longer period thus permitting ionospheric delay correction when used with the L1 frequency and a greater accuracy with the higher chip rate. It further operates in an aeronautical navigation protected frequency band L5. The new GPS/GNSS L5 civil signal will be used for precision navigation by the world’s airliners and many new precision navigation applications.\n\nThe 21st century world of satellite navigation includes not only a modernized GPS but also global navigation satellite constellations from Russia (GLONASS), China (BDS), Europe (Galileo), and regional systems from Japan (QZSS), and India (IRNSS). With these GNSS and RNSS constellations plus GPS, the world will have more than 132 satellites in orbit and if each has at least 2 civil signals there will be more than 264 navigation signals being broadcast.\n\nSpilker pointed out that from a communication theory point of view there is an alternative to tracking each satellite one at a time as separate estimates. Instead he considers the composite of all signals as a single composite signal which is now simply a function of the user's state vector which includes at least the user's position and velocity vectors. The receiver for such a system he defines as the vector delay-lock loop. This receiver operates on the total power received from the entire set of satellites of satellite constellation, not just the power of a single satellite. The gain of the vector processing which tracks the user state vector in a single step is greater than would have been feasible years ago but now computer chips are many times faster. Vector processing can have many advantages for disadvantaged users in challenging environments such as in urban canyons, under forest canopy, during space weather disturbances.\n\nThe civil signals are based on clear unencrypted codes published and available to all. A terrorist or other troublemakers can attempt to jam or spoof the true signal. The spoofer would attempt to transmit a similar signal so that it is received by the user with a similar delay and Doppler shift to the true signal. This is a very simple operation if the user is employing a conventional receiver that tracks each satellite signal separately one at a time with a separate correlator for each satellite.\n\nHowever with vector processing the problem is quite different – we are now tracking the composite of all 134 satellites in parallel. The spoofer now must generate spoofing signals that match the user state vector delay for all or most of the satellites – a task of enormous difficulty. For example if the signal has a 10 Mcps chip rate, the user state vector position estimate in 3D must be matched the spoofer to less than roughly 20 meters in all 3 dimensions and also very precisely in the velocity vector especially if the user has a good clock. Thus vector processing can provide enormous benefits to protect the user from spoofing attacks.\n\n\n\nJames Spilker is married to Anna Marie Spilker, a licensed real estate broker and the founder and president of New Pacific Investments Inc. in the Sillicon Valley.\n"}
{"id": "15210042", "url": "https://en.wikipedia.org/wiki?curid=15210042", "title": "Kitchen utensil", "text": "Kitchen utensil\n\nA kitchen utensil is a small hand held tool used for food preparation. Common kitchen tasks include cutting food items to size, heating food on an open fire or on a stove, baking, grinding, mixing, blending, and measuring; different utensils are made for each task. A general purpose utensil such as a chef's knife may be used for a variety of foods; other kitchen utensils are highly specialized and may be used only in connection with preparation of a particular type of food, such as an egg separator or an apple corer. Some specialized utensils are used when an operation is to be repeated many times, or when the cook has limited dexterity or mobility. The number of utensils in a household kitchen varies with time and the style of cooking.\n\nA cooking utensil is a utensil for cooking. Utensils may be categorized by use with terms derived from the word \"ware\": kitchenware, wares for the kitchen; ovenware and bakeware, kitchen utensils that are for use inside ovens and for baking; cookware, merchandise used for cooking; and so forth.\n\nA partially overlapping category of tools is that of eating utensils, which are tools used for eating (c.f. the more general category of tableware). Some utensils are both kitchen utensils and eating utensils. Cutlery (i.e. knives and other cutting implements) can be used for both food preparation in a kitchen and as eating utensils when dining. Other cutlery such as forks and spoons are both kitchen and eating utensils.\n\nOther names used for various types of kitchen utensils, although not strictly denoting a utensil that is specific to the kitchen, are according to the materials they are made of, again using the \"-ware\" suffix, rather than their functions: earthenware, utensils made of clay; silverware, utensils (both kitchen and dining) made of silver; glassware, utensils (both kitchen and dining) made of glass; and so forth. These latter categorizations include utensils — made of glass, silver, clay, and so forth — that are not necessarily kitchen utensils.\n\nBenjamin Thompson noted at the start of the 19th century that kitchen utensils were commonly made of copper, with various efforts made to prevent the copper from reacting with food (particularly its acidic contents) at the temperatures used for cooking, including tinning, enamelling, and varnishing. He observed that iron had been used as a substitute, and that some utensils were made of earthenware. By the turn of the 20th century, Maria Parloa noted that kitchen utensils were made of (tinned or enamelled) iron and steel, copper, nickel, silver, tin, clay, earthenware, and aluminium. The latter, aluminium, became a popular material for kitchen utensils in the 20th century.\n\nCopper has good thermal conductivity and copper utensils are both durable and attractive in appearance. However, they are also comparatively heavier than utensils made of other materials, require scrupulous cleaning to remove poisonous tarnish compounds, and are not suitable for acidic foods. Copper pots are lined with tin to prevent discoloration or altering the taste of food. The tin lining must be periodically restored, and protected from overheating.\n\nIron is more prone to rusting than (tinned) copper. Cast iron kitchen utensils, in particular, are however less prone to rust if, instead of being scoured to a shine after use, they are simply washed with detergent and water and wiped clean with a cloth, allowing the utensil to form a coat of (already corroded iron and other) material that then acts to prevent further corrosion (a process known as seasoning). Furthermore, if an iron utensil is solely used for frying or cooking with fat or oil, corrosion can be reduced by never heating water with it, never using it to cook with water, and when washing it with water to dry it immediately afterwards, removing all water. Since oil and water are immiscible, since oils and fats are more covalent compounds, and since it is ionic compounds such as water that promote corrosion, eliminating as much contact with water reduces corrosion. For some iron kitchen utensils, water is a particular problem, since it is very difficult to dry them fully. In particular, iron egg-beaters or ice cream freezers are tricky to dry, and the consequent rust if left wet will roughen them and possibly clog them completely. When storing iron utensils for long periods, van Rensselaer recommended coating them in non-salted (since salt is also an ionic compound) fat or paraffin.\n\nIron utensils have little problem with high cooking temperatures, are simple to clean as they become smooth with long use, are durable and comparatively strong (i.e. not as prone to breaking as, say, earthenware), and hold heat well. However, as noted, they rust comparatively easily.\n\nStainless steel finds many applications in the manufacture of kitchen utensils. Stainless steel is considerably less likely to rust in contact with water or food products, and so reduces the effort required to maintain utensils in clean useful condition. Cutting tools made with stainless steel maintain a usable edge while not presenting the risk of rust found with iron or other types of steel.\n\nEarthenware utensils suffer from brittleness when subjected to rapid large changes in temperature, as commonly occur in cooking, and the glazing of earthenware often contains lead, which is poisonous. Thompson noted that as a consequence of this the use of such glazed earthenware was prohibited by law in some countries from use in cooking, or even from use for storing acidic foods. Van Rensselaer proposed in 1919 that one test for lead content in earthenware was to let a beaten egg stand in the utensil for a few minutes and watch to see whether it became discoloured, which is a sign that lead might be present.\n\nIn addition to their problems with thermal shock, enamelware utensils require careful handling, as careful as for glassware, because they are prone to chipping. But enamel utensils are not affected by acidic foods, are durable, and are easily cleaned. However, they cannot be used with strong alkalis.\n\nEarthenware, porcelain, and pottery utensils can be used for both cooking and serving food, and so thereby save on washing-up of two separate sets of utensils. They are durable, and (van Rensselaer notes) \"excellent for slow, even cooking in even heat, such as slow baking\". However, they are comparatively \"un\"suitable for cooking using a direct heat, such as a cooking over a flame.\n\nJames Frank Breazeale in 1918 opined that aluminium \"is without doubt the best material for kitchen utensils\", noting that it is \"as far superior to enamelled ware as enamelled ware is to the old-time iron or tin\". He qualified his recommendation for replacing worn out tin or enamelled utensils with aluminium ones by noting that \"old-fashioned black iron frying pans and muffin rings, polished on the inside or worn smooth by long usage, are, however, superior to aluminium ones\".\n\nAluminium's advantages over other materials for kitchen utensils is its good thermal conductivity (which is approximately an order of magnitude greater than that of steel), the fact that it is largely non-reactive with foodstuffs at low and high temperatures, its low toxicity, and the fact that its corrosion products are white and so (unlike the dark corrosion products of, say, iron) do not discolour food that they happen to be mixed into during cooking. However, its disadvantages are that it is easily discoloured, can be dissolved by acidic foods (to a comparatively small extent), and reacts to alkaline soaps if they are used for cleaning a utensil.\nIn the European Union, the construction of kitchen utensils made of aluminium is determined by two European standards: EN 601 (\"Aluminium and aluminium alloys — Castings — Chemical composition of castings for use in contact with foodstuffs\") and EN 602 (\"Aluminium and aluminium alloys — Wrought products — Chemical composition of semi-finished products used for the fabrication of articles for use in contact with foodstuffs\").\n\nA great feature of non-enameled ceramics is that clay does not come into a reaction with food, does not contain toxic substances, and it is safe for food use because it does not give off toxic substances when heated. \n\nThere are several types of ceramic utensils.\nTerracotta utensils, which are made of red clay and black ceramics. The clay utensils for preparing food can also be used in electric ovens, microwaves and stoves,\nwe can also place them in fireplaces. It is not advised to put the clay utensil in the 220-250\ntemperature oven directly, because it will break. It also is not recommended to place the clay pot over an open fire.\nClay utensils do not like sharp change in temperature. The dishes prepared in clay pots come to be\nparticularly juicy and soft – this is due to the clay’s porous surface. Due to this porous nature of the surface the clay utensils inhale aroma and grease. The coffee made in clay coffee boilers is very aromatic, but such pots need special care. It is not advised to scrub the pots with metal scrubs, it is better to pour soda water in the pot and let it stay there and afterwards to wash the pot with warm water. The clay utensils must be kept in a dry place, so that they will not get damp.\n\nPlastics can be readily formed by molding into a variety of shapes useful for kitchen utensils. Transparent plastic measuring cups allow ingredient levels to be easily visible, and are lighter and less fragile than glass measuring cups. Plastic handles added to utensils improve comfort and grip. While many plastics deform or decompose if heated, a few silicone products can be used in boiling water or in an oven for food preparation. Non-stick plastic coatings can be applied to frying pans; newer coatings avoid the issues with decomposition of plastics under strong heating.\n\nHeat-resistant glass utensils can be used for baking or other cooking. Glass does not conduct heat as well as metal, and has the drawback of breaking easily if dropped. Transparent glass measuring cups allow ready measurement of liquid and dry ingredients.\n\n\"Of the culinary utensils of the ancients\", wrote Mrs Beeton, \"our knowledge is very limited; but as the art of living, in every civilized country, is pretty much the same, the instruments for cooking must, in a great degree, bear a striking resemblance to one another\".\n\nArchaeologists and historians have studied the kitchen utensils used in centuries past. For example: In the Middle Eastern villages and towns of the middle first millennium AD, historical and archaeological sources record that Jewish households generally had stone measuring cups, a \"meyḥam\" (a wide-necked vessel for heating water), a \"kederah\" (an unlidded pot-bellied cooking pot), a \"ilpas\" (a lidded stewpot/casserole pot type of vessel used for stewing and steaming), \"yorah\" and \"kumkum\" (pots for heating water), two types of \"teganon\" (frying pan) for deep and shallow frying, an \"iskutla\" (a glass serving platter), a \"tamḥui\" (ceramic serving bowl), a \"keara\" (a bowl for bread), a \"kiton\" (a canteen of cold water used to dilute wine), and a \"lagin\" (a wine decanter).\n\nOwnership and types of kitchen utensils varied from household to household. Records survive of inventories of kitchen utensils from London in the 14th century, in particular the records of possessions given in the coroner's rolls. Very few such people owned any kitchen utensils at all. In fact only seven convicted felons are recorded as having any. One such, a murderer from 1339, is recorded as possessing only the one kitchen utensil: a brass pot (one of the commonest such kitchen utensils listed in the records) valued at three shillings. Similarly, in Minnesota in the second half of the 19th century, John North is recorded as having himself made \"a real nice rolling pin, and a pudding stick\" for his wife; one soldier is recorded as having a Civil War bayonet refashioned, by a blacksmith, into a bread knife; whereas an immigrant Swedish family is recorded as having brought with them \"solid silver knives, forks, and spoons [...] Quantities of copper and brass utensils burnished until they were like mirrors hung in rows\".\n\nThe 19th century, particularly in the United States, saw an explosion in the number of kitchen utensils available on the market, with many labour-saving devices being invented and patented throughout the century. Maria Parloa's \"Cook Book and Marketing Guide\" listed a \"minimum\" of 139 kitchen utensils without which a contemporary kitchen would not be considered properly furnished. Parloa wrote that \"the homemaker will find [that] there is continually something new to be bought\".\n\nA growth in the range of kitchen utensils available can be traced through the growth in the range of utensils recommended to the aspiring householder in cookbooks as the century progressed. Earlier in the century, in 1828, Frances Byerley Parkes had recommended a smaller array of utensils. By 1858, Elizabeth H. Putnam, in \"Mrs Putnam's Receipt Book and Young Housekeeper's Assistant\", wrote with the assumption that her readers would have the \"usual quantity of utensils\", to which she added a list of necessary items:\n\nCopper saucepans, well lined, with covers, from three to six different sizes; a flat-bottomed soup-pot; an upright gridiron; sheet-iron breadpans instead of tin; a griddle; a tin kitchen; Hector's double boiler; a tin coffee-pot for boiling coffee, or a filter — either being equally good; a tin canister to keep roasted and ground coffee in; a canister for tea; a covered tin box for bread; one likewise for cake, or a drawer in your store-closet, lined with zinc or tin; a bread-knife; a board to cut bread upon; a covered jar for pieces of bread, and one for fine crumbs; a knife-tray; a spoon-tray; — the yellow ware is much the stringest, or tin pans of different sizes are economical; — a stout tin pan for mixing bread; a large earthen bowl for beating cake; a stone jug for yeast; a stone jar for soup stock; a meat-saw; a cleaver; iron and wooden spoons; a wire sieve for sifting flour and meal; a small hair sieve; a bread-board; a meat-board; a lignum vitae mortar, and rolling-pin, &c.\n\nMrs Beeton, in her \"Book of Household Management\", wrote:\n\nThe following list, supplied by Messrs Richard & John Slack, 336, Strand, will show the articles required for the kitchen of a family in the middle class of life, although it does not contain all the things that may be deemed necessary for some families, and may contain more than are required for others. As Messrs Slack themselves, however, publish a useful illustrated catalogue, which may be had at their establishment gratis, and which it will be found advantageous to consult by those about to furnish, it supersedes the necessity of our enlarging that which we give:\n\n— Isabella Mary Beeton, \"The Book of Household Management\"\nParloa, in her 1880 cookbook, took two pages to list all of the essential kitchen utensils for a well-furnished kitchen, a list running to 93 distinct sorts of item. The 1882 edition ran to 20 pages illustrating and describing the various utensils for a well-furnished kitchen. Sarah Tyson Rorer's 1886 \"Philadelphia Cook Book\" listed more than 200 kitchen utensils that a well-furnished kitchen should have.\n\nHowever, many of these utensils were expensive and not affordable by the majority of householders. Some people considered them unnecessary, too. James Frank Breazeale decried the explosion in patented \"labour-saving\" devices for the modern kitchen—promoted in exhibitions and advertised in \"Household Guides\" at the start of the 20th century—, saying that \"the best way for the housewife to peel a potato, for example, is in the old-fashioned way, with a knife, and not with a patented potato peeler\". Breazeale advocated simplicity over dishwashing machines \"that would have done credit to a moderate sized hotel\", and noted that the most useful kitchen utensils were \"the simple little inexpensive conveniences that work themselves into every day use\", giving examples, of utensils that were simple and cheap but indispensable once obtained and used, of a stiff brush for cleaning saucepans, a sink strainer to prevent drains from clogging, and an ordinary wooden spoon.\n\nThe \"labour-saving\" devices didn't necessarily save labour, either. While the advent of mass-produced standardized measuring instruments permitted even householders with little to no cooking skills to follow recipes and end up with the desired result and the advent of many utensils enabled \"modern\" cooking, on a stove or range rather than at floor level with a hearth, they \"also\" operated to raise expectations of what families would eat. So while food was easier to prepare and to cook, ordinary householders at the same time were expected to prepare and to cook more complex and harder-to-prepare meals on a regular basis. The labour-saving effect of the tools was cancelled out by the increased labour required for what came to be expected as the culinary norm in the average household.\n\n\n"}
{"id": "10279908", "url": "https://en.wikipedia.org/wiki?curid=10279908", "title": "Lee algorithm", "text": "Lee algorithm\n\nThe Lee algorithm is one possible solution for maze routing problems based on Breadth-first search.\nIt always gives an optimal solution, if one exists, but is slow and requires considerable memory.\n\n1) Initialization\n2) Wave expansion\n\n3) Backtrace\n4) Clearance\n\nOf course the wave expansion marks only points in the routable area of the chip, not in the blocks or already wired parts, and to minimize segmentation you should keep in one direction as long as possible.\n\n\n"}
{"id": "28019629", "url": "https://en.wikipedia.org/wiki?curid=28019629", "title": "List of pyrotechnic incidents", "text": "List of pyrotechnic incidents\n\nPyrotechnics have been the proximate cause of many accidents and incidents over time, which have resulted in property damage, injury and in severe cases loss of life. These incidents can be the results of poorly manufactured product, unexpected or unforeseen events, or in many cases, operator error.\n\nThis page contains a list of incidents involving pyrotechnic substances.\n\nIn August 2006, when TNA Wrestling held its annual Pay Per View Hard Justice, a fire ignited in the rafters where the pyrotechnics were held. Everybody was evacuated from the building. The fire lasted about 20 minutes and the show was continued.\n\nOn March 30, 2008, WWE had pyrotechnics go off for the ending of WrestleMania XXIV, but the pyrotechincs cable snapped and sparks flew down from the upper decks to the lower and middle decks. 45 fans were injured, but only 3 needed medical attention from a doctor.\n\nAt the 2010 Elimination Chamber Pay Per View, Professional Wrestler The Undertaker suffered burns to the chest and neck area as a result of a pyrotechnic accident. A mistake allowed a huge burst of fire to erupt directly under his feet, burning The Undertaker's leather coat. Remarkably, he completed his match despite suffering second and third-degree burns. The pyrotechnician responsible for the accident was fired on the spot and escorted from the building at the behest of The Undertaker himself.\n\n\nMargaret Hamilton was badly burned during a scene in which her character 'vanished' in a burst of flame and smoke, a delay in activating a trap-door catching her in the pyrotechnic device during the filming of The Wizard of Oz (1939 film). Her stuntwoman was also injured in a scene involving a smoking broomstick.\n\nA powerful blast razed a fireworks factory 50 kilometers south of the Philippines capital Manila in January 2009, killing at least eight people and injuring more than 70 others.\n\n\nKeith Moon and Pete Townshend were injured during the taping of an episode of \"The Smothers Brothers Comedy Hour\" in 1967.\n\nAnthea Turner was set alight by a pyrotechnic display in 1989 during the filming of an episode of the program \"UP2U\".\n\nMichael Jackson had suffered from injuries sustained when a pyrotechnic went wrong during filming for a Pepsi advertisement on January 27, 1984, when they went off too early and caused him to suffer from burns to his hair and scalp.\n\nA pyrotechnic-induced fire incident similar to The Station nightclub fire in 2003 destroyed the Republica Cromagnon nightclub in Buenos Aires, Argentina, killing 194 people.\n\nAnother incident in Russia saw the roof catch fire of the Lame Horse nightclub in the Ural mountain city of Perm. Smoke inhalation and stampede caused 112 deaths.\n\nAn incident involving highly explosive fireworks burned the Television Cultural Center in Beijing, China in 2009 on the final day of Chinese New Year celebrations. One person died of smoke inhalation and 7 people were injured.\n"}
{"id": "139004", "url": "https://en.wikipedia.org/wiki?curid=139004", "title": "Lists of swords", "text": "Lists of swords\n\nLists of swords:\n"}
{"id": "26670795", "url": "https://en.wikipedia.org/wiki?curid=26670795", "title": "Localeze", "text": "Localeze\n\nLocaleze, a service of Neustar, is a content manager for local search engines. The company provides businesses with tools to verify and manage the identity of their local listings across the Web. The company works with local search platform partners and location-based service partners, national brands and local business clients.\n\nLocaleze was created in 2005 to help businesses ensure that they have accurate name, address and phone number data available on search engines, Internet Yellow Pages and vertical directories. \n\nIn 2010, business listings were also included in personal navigation devices, mobile apps and on social networking services.\n\nAs a local search business listings provider, Localeze collects and distributes business listings that can be verified by businesses themselves. Its business listings are used by search, social and mobile companies in the domain of Local Search and location-based services. Such partners include Yahoo!, Bing, Yellow Pages, TomTom, Siri (acquired by Apple), Twitter and Facebook.\n"}
{"id": "2432911", "url": "https://en.wikipedia.org/wiki?curid=2432911", "title": "Mass flow sensor", "text": "Mass flow sensor\n\nA mass (air) flow sensor (MAF) is a sensor used to determine the mass flow rate of air entering a fuel-injected internal combustion engine.\n\nThe air mass information is necessary for the engine control unit (ECU) to balance and deliver the correct fuel mass to the engine. Air changes its density with temperature and pressure. In automotive applications, air density varies with the ambient temperature, altitude and the use of forced induction, which means that mass flow sensors are more appropriate than volumetric flow sensors for determining the quantity of intake air in each cylinder.\n\nThere are two common types of mass airflow sensors in use on automotive engines. These are the vane meter and the hot wire. Neither design employs technology that measures air mass directly. However, with additional sensors and inputs, an engine's ECU can determine the mass flow rate of intake air.\n\nBoth approaches are used almost exclusively on electronic fuel injection (EFI) engines. Both sensor designs output a 0.0–5.0 volt or a pulse-width modulation (PWM) signal that is proportional to the air mass flow rate, and both sensors have an intake air temperature (IAT) sensor incorporated into their housings for most post on-board diagnostics (OBDII) vehicles. Vehicles prior to 1996 could have MAF without an IAT. An example is 1994 Infiniti Q45.\n\nWhen a MAF sensor is used in conjunction with an oxygen sensor, the engine's air/fuel ratio can be controlled very accurately. The MAF sensor provides the open-loop controller predicted air flow information (the measured air flow) to the ECU, and the oxygen sensor provides closed-loop feedback in order to make minor corrections to the predicted air mass. Also see manifold absolute pressure sensor (MAP sensor).\n\nThe VAF (volume air flow) sensor measures the air flow into the engine with a spring-loaded air vane (flap/door) attached to a variable resistor (potentiometer). The vane moves in proportion to the airflow. A voltage is applied to the potentiometer and a proportional voltage appears on the output terminal of the potentiometer in proportion to the angle the vane rotates, or the movement of the vane may directly regulate the amount of fuel injected, as in the K-Jetronic system.\n\nMany VAF sensors have an air-fuel adjustment screw, which opens or closes a small air passage on the side of the VAF sensor. This screw controls the air-fuel mixture by letting a metered amount of air flow past the air flap, thereby leaning or richening the mixture. By turning the screw clockwise the mixture is enriched and counterclockwise the mixture is leaned.\n\nThe vane moves because of the drag force of the air flow against it; it does not measure volume or mass directly. The drag force depends on air density (air density in turn depends on air temperature), air velocity and the shape of the vane, see drag equation. Some VAF sensors include an additional intake air temperature sensor (IAT sensor) to allow the engines ECU to calculate the density of the air, and the fuel delivery accordingly.\n\nThe vane meter approach has some drawbacks: \n\nA \"hot wire mass airflow sensor\" determines the mass of air flowing into the engine’s air intake system. The theory of operation of the hot wire mass airflow sensor is similar to that of the hot wire anemometer (which determines air velocity). This is achieved by heating a wire suspended in the engine’s air stream, like a toaster wire, with either a constant voltage over the wire or a constant current through the wire. The wire's electrical resistance increases as the wire’s temperature increases, which varies the electrical current flowing through the circuit, according to Ohm's law. When air flows past the wire, the wire cools, decreasing its resistance, which in turn allows more current to flow through the circuit, since the supply voltage is a constant. As more current flows, the wire’s temperature increases until the resistance reaches equilibrium again. The current increase or decrease is proportional to the mass of air flowing past the wire. The integrated electronic circuit converts the proportional measurement into a calibrated signal which is sent to the ECU.\n\nIf air density increases due to pressure increase or temperature drop, but the air volume remains constant, the denser air will remove more heat from the wire indicating a higher mass airflow. Unlike the vane meter's paddle sensing element, the hot wire responds directly to air density. This sensor's capabilities are well suited to support the gasoline combustion process which fundamentally responds to air mass, not air volume. (See stoichiometry.)\n\nThis sensor sometimes employs a mixture screw, but this screw is fully electronic and uses a variable resistor (potentiometer) instead of an air bypass screw. The screw needs more turns to achieve the desired results. A hot wire burn-off cleaning circuit is employed on some of these sensors. A burn-off relay applies a high current through the platinum hot wire after the vehicle is turned off for a second or so, thereby burning or vaporizing any contaminants that have stuck to the platinum hot wire element.\n\nThe hot film MAF sensor works somewhat similar to the hot wire MAF sensor, but instead it usually outputs a frequency signal. This sensor uses a hot film-grid instead of a hot wire. It is commonly found in late 80’s early 90’s fuel-injected vehicles. The output frequency is directly proportional to the air mass entering the engine. So as mass flow increases so does frequency. These sensors tend to cause intermittent problems due to internal electrical failures. The use of an oscilloscope is strongly recommended to check the output frequency of these sensors. Frequency distortion is also common when the sensor starts to fail. Many technicians in the field use a tap test with very conclusive results. Not all HFM systems output a frequency. In some cases, this sensor works by outputting a regular varying voltage signal.\n\nSome of the benefits of a hot-wire MAF compared to the older style vane meter are:\n\nThere are some drawbacks:\n\nThe GM LS engine series (as well as others) use a coldwire MAF system (produced by AC Delco) that works similarly to the hot-wire MAF system; however, it uses an additional \"cold\" resistor to measure the ambient air and provide a reference for the \"hot\" resistor element used to measure the air flow.\n\nThe mesh on the MAF is used to smooth out airflow to ensure the sensors have the best chance of a steady reading. It is not used for measuring the air flow per se. In situations where owners use oiled-gauze air filters, it is possible for excess oil to coat the MAF sensor and skew its readings. Indeed, General Motors has issued a Technical Service Bulletin, indicating problems from rough idle all the way to possible transmission damage resulting from the contaminated sensors. To clean the delicate MAF sensor components, a specific MAF sensor cleaner or electronics cleaner should be used, \"not\" carburetor or brake cleaners, which can be too aggressive chemically. Instead, the liquid phase of MAF sensor cleaners and electronics cleaners is typically based on hexanes or heptanes with little to no alcohol content and use either carbon dioxide or HFC-152a as aerosol propellants. The sensors should be gently sprayed from a careful distance to avoid physically damaging them and then allowed to thoroughly dry before reinstalling. Manufacturers claim that a simple but extremely reliable test to ensure correct functionality is to tap the unit with the back of a screwdriver while the car is running, and if this causes any changes in the output frequency then the unit should be discarded and an OEM replacement installed.\n\nA Kármán vortex sensor works by disrupting the air stream with a perpendicular bow. Providing that the incoming flow is laminar, the wake consists of an oscillatory pattern of Kármán vortices. The frequency of the resulting pattern is proportional to the air velocity.\n\nThese vortices can either be read directly as a pressure pulse against a sensor, or they can be made to collide with a mirror which will then interrupt or transmit a reflected light beam to generate the pulses in response to the vortices. The first type can only be used in pull-thru air (prior to a turbo- or supercharger), while the second type could theoretically be used push- or pull-thru air (before or after a forced induction application like the previously mentioned super- or turbocharger). Instead of outputting a constant voltage modified by a resistance factor, this type of MAF outputs a frequency which must then be interpreted by the ECU. This type of MAF can be found on all DSMs (Mitsubishi Eclipse, Eagle Talon, Plymouth Laser), many Mitsubishis, some Toyotas and Lexus, and some BMWs, among others.\n\nAn emerging technology utilizes a very thin electronic membrane placed in the air stream. The membrane has a thin film temperature sensor printed on the upstream side, and one on the downstream side. A heater is integrated in the center of the membrane which maintains a constant temperature similar to the hot-wire approach. Without any airflow, the temperature profile across the membrane is uniform. When air flows across the membrane, the upstream side cools differently from the downstream side. The difference between the upstream and downstream temperature indicates the mass airflow. The thermal membrane sensor is also capable of measuring flow in both directions, which sometimes occur in pulsating situations. Technological progress allows this kind of sensor to be manufactured on the microscopic scale as microsensors using microelectromechanical systems technology. Such a \"microsensor\" reaches a significantly higher speed and sensitivity compared with macroscopic approaches. See also MEMS sensor generations.\n\nLaminar flow elements measure the volumetric flow of gases directly. They operate on the principle that, given laminar flow, the pressure difference across a pipe is linear to the flow rate. Laminar flow conditions are present in a gas when the Reynolds number of the gas is below the critical figure. The viscosity of the fluid must be compensated for in the result. Laminar flow elements are usually constructed from a large number of parallel pipes to achieve the required flow rating.\n\n\n"}
{"id": "35507332", "url": "https://en.wikipedia.org/wiki?curid=35507332", "title": "Mike Krieger", "text": "Mike Krieger\n\nMichel \"Mike\" Krieger (born March 4, 1986) is a Brazilian entrepreneur and software engineer who co-founded Instagram along with Kevin Systrom. On September 24, 2018, it was announced that Krieger has resigned from Instagram and will be leaving in few weeks.\n\nKrieger was born in São Paulo, Brazil, and moved to California in 2004 to attend Stanford University. At Stanford, where he studied symbolic systems, he met Kevin Systrom. The two co-founded Instagram in 2010. He made the updates for Instagram for the first 3 years it was public. He is married to Kaitlyn Trigger.\n\nIn April 2015, Krieger announced a partnership with charity evaluator GiveWell, committing US$750,000 over the next two years. The funds are to support operations, with 90% allocated to grants identified and recommended through the Open Philanthropy Project process. Kaitlyn, in a blog about the partnership, describes the couple's philanthropic vision as:\nWe believe that all people deserve a free, vibrant, and productive life. To support this vision, we identify and champion forward-thinking ideas, and help scale solutions that work. To create significant, sustainable change, we are committed to systems-level thinking and rigorous analysis. We advocate collaboration and transparency to engage a broader community and magnify our impact.\n\n"}
{"id": "56234464", "url": "https://en.wikipedia.org/wiki?curid=56234464", "title": "Moored training ship", "text": "Moored training ship\n\nA Moored training ship (MTS) is a United States Navy designation for nuclear powered submarines that have been converted to training ships for the Naval Nuclear Power Training Command's Nuclear Power Training Unit (NPTU) at Naval Support Activity Charleston in South Carolina. The NPTU is part of the Navy's Nuclear Power School at Goose Creek, S.C.. The Navy uses decommissioned nuclear submarines and converts them to MTSs to train personnel in the operation and maintenance of submarines and their nuclear reactors. The first moored training ship was a fleet ballistic missile submarine, redesignated as (MTS-635) in 1989, followed a year later by , a ballistic missile submarine, redesignated as (MTS-626). Conversion of these two boats took place at the Charleston Naval Shipyard and modifications included special mooring arrangements with a mechanism to absorb power generated by the main propulsion shaft. \n\nThe Navy plans to add two more moored training ships to this facility, and , a pair of attack submarines. The conversions for these two will take place at the Norfolk Naval Shipyard and they will then be taken to NSA Charleston. \"La Jolla\" became inactive in early 2015 and began the 32 month conversion to a training ship. Changes include having the hull cut into three sections, with the center section being recycled and the other two joined with three new sections, manufactured by Electric Boat, extending the length on the boat by 23 m (76 ft). The project is expected to be completed by the end of 2018. \"San Francisco\" arrived at Norfolk to begin her conversion in January 2018\n\n\n"}
{"id": "16750945", "url": "https://en.wikipedia.org/wiki?curid=16750945", "title": "Multiple exciton generation", "text": "Multiple exciton generation\n\nIn solar cell research, carrier multiplication is the phenomenon wherein the absorption of a single photon leads to the excitation of multiple electrons from the valence band to conduction band. In the theory of a conventional solar cell, each photon is only able to excite one electron across the band gap of the semiconductor, and any excess energy in that photon is dissipated as heat. In a material with carrier multiplication, high-energy photons excite on average more than one electron across the band gap, and so in principle the solar cell can produce more useful work.\n\nIn quantum dot solar cells, the excited electron in the conduction band interacts with the hole it leaves behind in the valence band, and this composite uncharged object is known as an exciton. The carrier multiplication effect in a dot can be understood as creating multiple excitons, and is called multiple exciton generation (MEG). MEG may considerably increase the energy conversion efficiency of nanocrystal based solar cells, though extracting the energy may be difficult because of the short lifetimes of the multiexcitons.\n\nThe quantum mechanical origin of MEG is still under debate and several possibilities have been suggested:\n\nAll of the above models can be described by the same mathematical model (density matrix) which can behave differently depending on the set of initial parameters (coupling strength between the X and multi-X, density of states, decay rates).\n\nMEG was first observed in 2004 using colloidal PbSe quantum dots and later was found in quantum dots of other compositions including PbS, PbTe, CdS, CdSe, InAs, Si, and InP. Multiple exciton generation was first demonstrated in a functioning solar cell in 2011, also using colloidal PbSe quantum dots. Multiple exciton generation was also detected in semiconducting single-walled carbon nanotubes (SWNTs) upon absorption of single photons. For (6,5) SWNTs, absorption of single photons with energies corresponding to three times the SWNT energy gap results in an exciton generation efficiency of 130% per photon. The multiple exciton generation threshold in SWNTs can be close to the limit defined by energy conservation.\n\nGraphene, which is closely related to nanotubes, is another material in which multiple exciton generation has been observed.\n\nDouble-exciton generation has additionally been observed in organic pentacene derivatives through singlet exciton fission with extremely high quantum efficiency.\n"}
{"id": "12101002", "url": "https://en.wikipedia.org/wiki?curid=12101002", "title": "National Safe Place", "text": "National Safe Place\n\nNational Safe Place (doing business as National Safe Place Network) is a non-profit organization based out of Louisville, Kentucky. It originated in 1983 from an initiative known as \"Project Safe Place\", established by a short-term residential and counseling center for youth 12 to 17. The organization is intended to provide access to immediate help and support for children and adolescents who are \"at risk\" or in crisis situations. The purpose is to both defuse a potential crisis situation as well as provide immediate counsel and support so the child in crisis may be directed to an appropriate shelter or accredited care facility.\n\nBusinesses and community buildings such as fire stations and libraries are designated as \"Safe Place\" sites. Any youth in crisis can walk into one of the nearly 20,000 Safe Places across the country and ask an employee for help. These locations display the yellow, diamond-shaped Safe Place sign on their location. Inside, employees are trained and prepared to assist any young person asking for help. Youth who go to a Safe Place location are quickly connected to the nearby youth shelter. The shelter then provides the counseling and support necessary to reunify family members and develop a plan to address the issues presented by the youth and family.\n\nIn October 2009, National Safe Place launched the TXT 4 HELP initiative, which provides youth immediate access to help and resources through texting. Youth can text the word \"safe\" and their current location (address/city/state) to 69866 and receive an immediate text response with the location of the closest Safe Place site or youth shelter and the youth shelter phone number. If a site or shelter is not within a 50-mile range, the youth receives the number to the National Runaway Safeline (1-800-RUNAWAY). In 2012, National Safe Place added the option for live, interactive texting with a trained mental health professional. With this addition, youth can immediately connect with Master's-level mental health professionals by text.\n\nIn 2013, National Safe Place merged with the Youth & Family Services Network (YFSN) to create the National Safe Place Network. NSPN provides training and technical assistance to licensed Safe Place agencies and NSPN member organizations across the country. More information about NSPN is available at www.nspnetwork.org.\n\nThe National Safe Place Network also operates the Runaway and Homeless Youth Training and Technical Assistance Center (RHYTTAC), a national training resource for FYSB-funded Runaway and Homeless Youth grantees, as well as several other federally funded projects focused on human trafficking and other issues critical to youth service providers.\n\nThe first Safe Place case was in 1983 in Louisville Ky. At firehouse at 6th and Hill St\nFacilitated by then Sergeant Matthew L Kaelin. Fire Co. Truck 3\n"}
{"id": "227131", "url": "https://en.wikipedia.org/wiki?curid=227131", "title": "Nikolaus Otto", "text": "Nikolaus Otto\n\nNikolaus August Otto (14 June 1832, Holzhausen an der Haide, Nassau – 26 January 1891, Cologne) was a German engineer who successfully developed the compressed charge internal combustion engine which ran on petroleum gas and led to the modern internal combustion engine. The Association of German Engineers (VDI) created DIN standard 1940 which says \"Otto Engine: internal combustion engine in which the ignition of the compressed fuel-air mixture is initiated by a timed spark\", which has been applied to all engines of this type since.\n\nNikolaus August Otto was born on 14 June 1832 in Holzhausen an der Haide, Germany. He was the youngest of six children. His father died in 1832. He began school in 1838. After six years of good performance he moved to the high school in Langenschwalbach until 1848. He did not complete his studies but was cited for good performance.\n\nHis main interest in school had been in science and technology but he graduated after three years as a business apprentice in a small merchandise company. After completing his apprenticeship he moved to Frankfurt where he worked for Philipp Jakob Lindheimer as a salesman of \"colonial goods\" and agricultural products (he was a grocery salesman). Otto worked for various companies, first for IC Alpeter and then in 1860 for Carl Mertens. He traveled throughout Western Germany and sold colonial goods - coffee, tea, rice, and sugar.\n\nIn late autumn of 1860 Otto and his brother learned of a novel gas (illuminating gas) engine that Jean Joseph Etienne Lenoir had built in Paris. The brothers built a copy of the Lenoir engine and applied for a patent in January 1861 for a liquid fueled engine based on the Lenoir (Gas) engine with the Prussian Ministry of Commerce, but it was rejected.\n\nOtto was aware of the concept of compressed fuel charge and tried to make an engine using this principle in 1861. It ran for just a few minutes before breaking. Otto's brother gave up on the concept, resulting in Otto looking for help elsewhere.\n\nFrom 1862 to 1863 Otto experimented with the help of Cologne Mechanic Michael J. Zons in an effort to improve the engine. Running low on funds, in 1862 Otto worked for Carl Mertens in order to continue work on his engine.\n\nEarly in 1864, Otto sought investors to fund his research. He found Eugen Langen, whose father was a sugar industrialist. Together they entered into a partnership on 31 March 1864 and named it NA Otto & Cie in Cologne. This was the world's first company focused entirely on the design and production of internal combustion engines.\n\nThe 1864 Otto & Langen engine was a free piston atmospheric engine (the explosion of gas was used to create a vacuum and the power came from atmospheric pressure returning the piston). It consumed less than half the gas of the Lenoir and Hugon atmospheric engines and so was a commercial success. The Lenoir engine was a double acting engine. In essence these engines are a steam engine altered to run on illuminating gas. The engines of Italian inventors Eugenio Barsanti and Felice Matteucci in their British Patent no 1625 of 1857, were built and are in a museum. Unlike Otto's engine these are two stroke atmospheric engines which are not in any way comparable.\n\nLenoir's engines were the first to be put into serial production with numbers sold being around 700.\n\nThe Otto engine which is the predecessor of the modern engine as specified by the VDI is Otto's fourth design. He built the following engines:\n\nFor all its commercial success, with the company producing 634 engines a year by 1875, the Otto and Langen engine had hit a technical dead end: it produced only , yet required headroom to operate.\n\nOtto turned his attention to the four stroke cycle which he had failed at in 1862. Largely due to the efforts of Franz Rings and Herman Schumm, who were brought into the company by Gottlieb Daimler Otto succeeds in making the Four Stroke, Compressed Charge engine. It is this engine (the Otto Silent Engine), and not the Otto & Langen engine, to which the \"Otto cycle\" refers. This was the first commercially successful engine to use in-cylinder compression. The Rings-Schumm engine appeared in autumn 1876 and was immediately successful.\n\nOtto married Anna Gossi and the couple had seven recorded children. His son Gustav Otto grew up to become an aircraft builder.\n\nThe Otto engine was designed as a stationary engine and in the action of the engine, the stroke is an upward or downward movement of a piston in a cylinder. Used later in an adapted form as an automobile engine, four strokes are involved:\n\n\nOtto had obtained many patents from several different nations and for several different features. When his former manager Gottlieb Daimler wanted to build small engines for transportation Otto showed no interest. Daimler left and took Maybach with him. Daimler had no desire to pay royalties to Otto (Deutz AG) and so hired a lawyer to find a solution. What the lawyer found was a patent for the concept of a four cycle engine that had been issued to Beau De Rochas, a French engineer, in 1862. This resulted in Otto losing one of his patents and allowed Daimler to sell his engines in Germany without paying royalties. Neither Otto nor Daimler were aware of the Rochas patent. Rochas never built an engine. It is likely he could not have done so.\n\nSeveral of the inventions that are sometimes mentioned as having preceded the Otto engine, such as Marcus, Barsanti, etc. are for two cycle (two stroke) atmospheric engines which do not compress the fuel charge. Otto's atmospheric engine is not the VDI (and other associations) Otto engine type. The only significant engines were those from Lenoir. His engines were the first to go into serial production. Lenoir eventually sold approximately 700 engines.\n\nOver 50,000 engines were produced in the 17 years following introduction.\n\nOtto received numerous honors for his engines. \n\n\n\n\n"}
{"id": "42174228", "url": "https://en.wikipedia.org/wiki?curid=42174228", "title": "Open Course Library", "text": "Open Course Library\n\nOpen Course Library (OCL) is an effort by the State of Washington to identify and make available digitally, to community and technical college instructors and students across that state, free textbooks, interactive assignments, and videos. Instructional materials can be \"a smorgasbord of teaching modules and exercises developed by other open-learning projects. . . Interactive-learning Web sites and even instructional videos on YouTube . . .\" However, OCL is not an OER publishing project, although it did contribute to the development of some widely used resources. Goals include: lowering textbook costs for students, providing new resources for faculty to use in their courses; and fully engaging in the global OER or open educational resources discussion.\n\nThe project was funded by matching grants of $750,000 from the Bill and Melinda Gates Foundation and the Washington State legislature. In 2009-2010 the affected Washington State student body totaled 470,000 and was increasing. Many of the materials made available are open educational resources or OERs. Specifically, they include syllabi, course activities, readings, and assessments and some are paired with low cost textbooks, costing $30 or less. In subjects across the sciences and humanities, the OCL team created curriculum support for Washington State's most popular 81 courses in the state's 34 community and technical colleges. Instructors were free to use the materials as they wish, in part or an entire course. The project was headed by Cable Green, then eLearning Director for the Washington State Board for Community and Technical Colleges.\n\nIt emerged from a two-year discussion that ultimately produced a Strategic Technology Plan. The plan outlines a unified vision known as Washington Student Completion Initiative.\n\nOCL participants were selected from a grant proposal competition. The process, which led to production of the materials, was open and several preliminary Town Meetings were used employing Eluminate Live. All participants were welcome. The meetings are archived. Topics for discussion included interactions with publishers, content presentation, copyright policies, and various Creative Commons licenses.\n\nSuccessful applicants received $15,000 to complete a course redesign. Librarians, instructional designers and institutional researchers were also asked to apply. Like faculty, these successful applicants also received grants of $15,000. Each winning faculty member or team designed a ready-to-use digital course module. Teams were composed of community college instructors, librarians, and web-designers. In fall 2011 the first 42 courses created were released. The use of OCL materials is not mandated for Washington's community colleges and technical schools. Faculty course designers, however, are asked to adopt what they have designed. There were over 25,000 visits from 125 countries over the first four months. Nonetheless, there have been challenges: good material is not always available online for adoption and sometimes the best materials (for instance translations of primary sources published in foreign languages) are not available free.\n\nCourses are made accessible using the WashingtonOnline learning system. Externally, OCL partners with the Saylor Foundation, the Connexions Consortium, and the Open Courseware Consortium. The Saylor site can be used access course content by self-learners.\n\nA goal of the project requires instructors to become aware of the open educational resources (OERs) that are already available. A complementary goal is to share their content and adaptation by contributing to a global effort. Both directly and indirectly, Cable Green, the project's then director, observed that it has resulted in the building of networks with like-minded individuals and institutions irrespective of geography. An interest was also expressed in gauging use of materials and modules for the tenure and advancement of participants.\n\nA 2009 \"New York Times\" article reported that college students spend between $700 and $1,000 annually on textbooks. Full-time tuition in the Washington system is approximately $3,000 annually, with textbooks costing approximately $1,000 annually. Community college tuition is lower than at most traditional four year institutions, and, therefore, textbook costs may be proportionally higher. Any number of digital copies of a free textbook can be made for the price of one. Print-on demand copies generally cost under $10. Representative Reuven Carlyle (D-Seattle, Washington House of Representatives) estimates that because of OCL community college students saved over $1.25 million in textbook costs during the 2011-2012 school year. OCL seeks to contribute to the creation of better courses and to reduce costs for students. In this way it seeks to respond positively to the \"completion\" concerns outlined in the state's tipping point research report of 2008. On 17 June 2010 the Washington State Board for Community & Technical Colleges (SBCTC) approved a state-level open licensing policy. All digital works created using grant funds administered by SBCTC must now carry a Creative Commons Attribution-only (CC BY) license. This license allows materials created by one institution to be updated by another. It was within this context that OCL was launched in 2010. Nicole Allen, a textbook advocate for the national Student Public Interest Research Group (PIRG) commended the state for putting its money where its mouth is.\n\nThe results are reported in Affordable Textbooks for Washington Students: An Updated Cost Analysis of the Open Course Library (2013),<ref name=\"http://www.studentpirgs.org/sites/student/files/resources/PIRG%20OCL.pdf\"></ref> an update of Affordable Textbooks for Washington's Students: A Cost Analysis of the Open Course Library (2011). The latter reported on the first phase of courses in October 2011. In April 2013 the State Board announced the completion of all 81 courses and the updated report was released. The report concluded:\n\nOn the other hand, in January 2014 \"The Chronicle of Higher Education\" \"reported that the effort to make free or low cost materials available in 42 courses was making little progress. Based on a survey of community-college stores, with responses from 25 campuses, only nine said that any materials had been used in 17 of the 42 courses. Only 2,386 of the 98,130 students enrolled in these courses, in 75 of the eligible 2,722 sections, used the materials. In 16 of the 75 sections students paid nothing; in the other 59 sections the average cost was $25. These numbers reported the work of OnCampus Research, an arm of the National Association of College Stores, which in fall 2013 sent a survey to 34 campus stores in the Washington Community and Technical College system. The survey focused on the first 42, or phase one, courses. It showed, according to the director of OnCampus Research, \"that the recommendation of specific free or lower-priced course materials for popular courses did not equate to significant use of these materials by faculty.\" However, Marty Brown, executive director of the Washington State Board of Community and Technical Colleges, took exception in a \"Chronicle of Higher Education\" piece he explained: \"The study analyzes the use of OCL materials based on adoption information from campus bookstores. This methodology provides an incomplete picture, as bookstores are not always aware when faculty members assign free, digital resources. Therefore, the study's findings do not justify its conclusion that OCL has resulted in \"insignificant\" savings to students. The Student PIRGs estimates the OCL has saved students more than $5.5-million, more than triple the original investment. We believe this is very significant.\" SPARC referred to the same presumed misunderstanding.\n\nOne reviewer states that OCL is best at \"presenting introductory college course material in a condensed, simple manner via Google docs or presentations.\" It is noted, however, that the materials are not highly refined. Further observing that \"there's something to be said about grabbing an algebra quiz or those French vocabulary words quickly.\" A doctoral dissertation focused specifically on the project concludes in its abstract: \". . . that while faculty may be motivated to adopt new innovations like OER, for some, the time it takes to identify and integrate OER into courses presents a significant barrier to adoption.\" \n\nWashington State communities have integrated OCL into educational efforts in innovative ways. For instance, Bellevue College, Washington, the state's largest community college, with help with a grant from the U.S. Department of Education, purchased 500 inexpensive netbook laptops in November 2011 for its students to rent for $35 per quarter to \"download and read Internet material.\" The anticipation was that the machines would also be used with OCL material. Since its initiation, OCL is credited with inspiring open textbook initiatives by the legislatures of California (2012), British Columbia (2012) and actions by the legislatures of Illinois, Minnesota, and Virginia all similar to California's. North Dakota proposed a resolution asking faculty and college administrations to support the use of open textbooks. Further, the Trade Adjustment Community College and Career Training grants program requires that all material created using federal funds be available to the public through an open license. Materials produced through this vehicle will be added to the OCL.\n\nThe Saylor Foundation will create modular versions of the courses. Project Kaleidoscope intends to modify OCL materials to meet the needs of California's community college students. The department of education in São Paulo Brazil plans Portuguese translations of the courses.\n\n\n"}
{"id": "45295258", "url": "https://en.wikipedia.org/wiki?curid=45295258", "title": "Packet Digital", "text": "Packet Digital\n\nPacket Digital LLC, headquartered in Fargo, North Dakota, designs power management integrated circuits (PMICs) to reduce power consumed and heat produced by computer servers, mobile devices and Unmanned Aircraft Systems (UAS).\n\nPacket Digital is designing circuitry for solar-powered UAS to more efficiently use electrical output from solar panels on the aircraft wings and fuselage to power avionics and charge on-board batteries. By reducing wasteful power dissipation, the company aims to increases flight times.\n\nPacket Digital is designing power electronics to be used conjointly with advanced photovoltaics being developed by the United States Naval Research Laboratory, Washington, D.C. Ultimately, the two organizations want to help the Navy achieve unlimited flight times for its solar-powered UAS. The circuitry will implement high-frequency power tracking algorithms to accommodate dynamic movements of the Unmanned Aerial Vehicle (UAV) or clouds that could change how much sunlight is absorbed and converted by the solar cells.\n\nPacket Digital announced in June 2015 that it would create Botlink LLC as a joint venture with drone app developer Aerobotic Innovations LLC. Botlink will develop and market a hardware-software platform combining Packet Digital's power management circuits for improved drone endurance and Aerobotic Innovation's cloud-based operations platform for safety, communications, data processing and control of a drone from a drag-and-drop app on a tablet or smartphone—a feature known as \"Drag. Drop. Drone.\"\n\nIn February 2015, it was announced Peter Lindgren, president and CEO of Otter Products, and Linda Pancratz, retired chairman of the board and CEO of TDL Infomedia Limited in the United Kingdom, joined the Packet Digital board of directors.\n\nPacket Digital was named the Electronics and Overall Innovation Challenge Winner at Aviation Week 2012 in Washington, D.C. Gartner named Packet Digital in its 2014 Cool Vendors in Semiconductors report. Inc. listed the company number 468 among its 500 Fastest Growing Companies in 2008.\n"}
{"id": "25208582", "url": "https://en.wikipedia.org/wiki?curid=25208582", "title": "Park Systems", "text": "Park Systems\n\nPark Systems Corp. was founded as PSIA in 1997 by Dr. Sang-il Park, a co-founder of Park Scientific Instruments, one of the pioneers in developing commercialized AFM. PSIA changed its name to Park Systems to reflect the company’s focus on total metrological solutions and atomic force Microscopes and scanning probe microscopes for both small and large-sample measurement. Beside the nanoscale microscopy tools for research applications the company also offers an industrial product line that extends its innovative NX technology to a variety of metrological applications, including hard disk inspection, next-generation sliders, sidewall/overhang imaging and profiling,Semiconductor and Wafer-Fab manufacturing. In December 2015, Park Systems held its IPO, joining the KOSDAQ Composite Index. The company holds several\nunique distinctions such as being the first company listed on the KOSDAQ to receive multiple “AA” ratings on technical evaluations of their technologies. \nPark Systems' AFM product lines are designed specifically to be used in materials science, electronics, life science, nanotechnology, and other areas of research and industry. They are characteristic for their innovative features, such as True-Non Contact™ mode, decoupled XY and Z architecture, SmartScan operating software and full automation.\n\nPark Systems offers 2 main product lines:\n\nA. Research Products: Park NX10, Park NX20, Park NX-Hivac, Park NX12-Bio, Park XE7, Park XE15.\n\nB. Industrial Products: Park WAFER series, Park 3DM series, Park HDM series, Park PTR series.\n\nDr. Park is the pioneer in AFM industry where he took the lead in commercializing the early AFM technology by founding Park Scientific Instruments (PSI) in California, where he served as the Chairman and CEO for 9 years (1988~1997). Prior to founding PSI, he worked with Prof. C.F. Quate at Stanford University, the birthplace of the AFM. His contribution to the AFM industry and business has been recognized by numerous awards, including Iron Tower Order of Industrial Service Merit (by Republic of Korea, 2005) and Industrial Technology Innovation Award (by Minister of Commerce, Industry, and Energy, Republic of Korea, 2004). He has authored numerous research papers, text books, and eighteen U.S. patents. He also serves as the Director of the Korean Nanotechnology Research Society, a member of the Nanotechnology Steering Committee at Korean Ministry of Science and Technology, and the Nanotechnology Information Committee at KISTI. Dr. Park has a Ph.D. in applied physics from Stanford University and B.S. in physics from Seoul National University.\n"}
{"id": "3577039", "url": "https://en.wikipedia.org/wiki?curid=3577039", "title": "Power history", "text": "Power history\n\nPower History refers to the power of a nuclear reactor over an extended period of time. Power history is important for calculations and operations that involve decay heat and fission product poisons and to avoid the iodine pit during reactor shutdowns.\n\nFor example, a nuclear reactor that has operated at 100% power for 100 hours and then has dropped down to 20% power for 5 hours will have a different amount of decay heat and fission product poisons than the same nuclear reactor operating at 20% power for 105 hours. This is because the second reactor has a different power history.\n"}
{"id": "5288063", "url": "https://en.wikipedia.org/wiki?curid=5288063", "title": "Power network design (IC)", "text": "Power network design (IC)\n\nIn integrated circuits, electrical power is distributed to the components of the chip over a network of conductors on the chip. Power network design includes the analysis and design of such networks. As in all engineering, this involves tradeoffs - the network must have adequate performance, be sufficiently reliable, but should not use more resources than required.\n\nThe power distribution network distributes power and ground voltages from pad locations to all devices in a design. Shrinking device dimensions, faster switching frequencies and increasing power consumption in deep sub-micrometer technologies cause large switching currents to flow in the power and ground networks which degrade performance and reliability. A robust power distribution network is essential to ensure reliable operation of circuits on a chip. Power supply integrity verification is a critical concern in high-performance designs. Due to the resistance of the interconnects constituting the network, there is a voltage drop across the network, commonly referred to as the \"IR-drop\". The package supplies currents to the pads of the power grid either by means of package leads in wire-bond chips or through \"C4 bump arrays\" in flip chip technology. Although the resistance of package is quite small, the inductance of package leads is significant which causes a voltage drop at the pad locations due to the time varying current drawn by the devices on die. This voltage drop is referred to as the \" di/dt-drop\". Therefore, the voltage seen at the devices is the supply voltage minus the IR-drop and di/dt-drop.\n\nExcessive voltage drops in the power grid reduce switching speeds and noise margins of circuits, and inject noise which might lead to functional failures. High average current densities lead to undesirable wearing out of metal wires due to electromigration (EM). Therefore, the challenge in the design of a power distribution network is in achieving excellent voltage regulation at the consumption points notwithstanding the wide fluctuations in power demand across the chip, and to build such a network using minimum area of the metal layers. These issues are prominent in high performance chips such as microprocessors, since large amounts of power have to be distributed through a hierarchy of many metal layers. A robust power distribution network is vital in meeting performance guarantees and ensuring reliable operation.\n\nCapacitance between power and ground distribution networks, referred to as decoupling capacitors or \"decaps\", acts as local charge storage and is helpful in mitigating the voltage drop at supply points. Parasitic capacitance between metal wires of supply lines, device capacitance of the non-switching devices, and capacitance between N-well and substrate, occur as implicit decoupling capacitance in a power distribution network. Unfortunately, this implicit decoupling capacitance is sometimes not enough to constrain the voltage drop within safe bounds and designers often have to add intentional explicit decoupling capacitance structures on the die at strategic locations. These explicitly added decoupling capacitances are not free and increase the area and leakage power consumption of the chip. Parasitic interconnect resistance, decoupling capacitance and package/interconnect inductance form a complex RLC circuit which has its own resonance frequency. If the resonance frequency lies close to the operating frequency of the design, large voltage drops can develop in the grid.\n\nThe crux of the problem in designing a power grid is that there are many unknowns until the very end of the design cycle. Nevertheless, decisions about the structure, size and layout of the power grid have to be made at very early stages when a large part of the chip design has not even begun. Unfortunately, most commercial tools focus on post-layout verification of the power grid when the entire chip design is complete and detailed information about the parasitics of the power and ground lines and the currents drawn by the transistors are known. Power grid problems revealed at this stage are usually very difficult or expensive to fix, so the preferred methodologies help to design an initial power grid and refine it progressively at various design stages.\n\nDue to the growth in power consumption and switching speeds of modern high performance microprocessors, the \" di/dt\" effects are becoming a growing concern in high speed designs. Clock gating, which is a preferred scheme for power management of high performance designs, can cause rapid surges in current demands of macro-blocks and increase \"di/dt\" effects. Designers rely on the on-chip parasitic capacitances and intentionally added decoupling capacitors to counteract the \"di/dt\" variations in the voltage. But it is necessary to model accurately the inductance and capacitance of the package and chip and analyze the grid with such models, as otherwise the amount of decoupling to be added might be underestimated or overestimated. Also it is necessary to maintain the efficiency of the analysis even when including these detailed models.\n\nA critical issue in the analysis of power grids is the large size of the network (typically millions of nodes in a state-of-the-art microprocessor). Simulating all the non-linear devices in the chip together with the power grid is computationally infeasible. To make the size manageable, the simulation is done in two steps. First, the non-linear devices are simulated assuming perfect supply voltages and the currents drawn by the devices are measured. Next, these devices are modeled as independent time-varying current sources for simulating the power grid and the voltage drops at the transistors are measured. Since voltage drops are typically less than 10% of the power supply voltage, the error incurred by ignoring the interaction between the device currents and the supply voltage is small. By doing these two steps, the power grid analysis problem reduces to solving a linear network which is still quite large. To further reduce the network size, we can exploit the hierarchy in the power distribution models.\n\nNote that the circuit currents are not independent due to signal correlations between blocks. This is addressed by deriving the inputs for individual blocks of the chip from the results of logic simulation using a common set of chip-wide input patterns. An important issue in power grid analysis is to determine what these input patterns should be. For IR-drop analysis, patterns that produce maximum instantaneous currents are required, whereas for electromigration purposes, patterns producing large sustained (average) currents are of interest.\n\nPower grid analysis can be classified into \"input vector dependent\" methods and \"vectorless\" methods. The input vector pattern dependent methods employ search techniques to find a set of input patterns which cause the worst drop in the grid. A number of methods have been proposed in literature which use genetic algorithms or other search techniques to find vectors or a pattern of vectors that maximize the total current drawn from the supply network. Input vector-pattern dependent approaches are computationally intensive and are limited to circuit blocks rather than full-chip analysis. Furthermore, these approaches are inherently optimistic, underestimating the voltage drop and thus letting some of the supply noise problems go unnoticed. The vectorless approaches, on the other hand, aim to compute an upper bound on the worst-case drop in an efficient manner. These approaches have the advantage of being fast and conservative, but are sometimes too conservative, leading to overdesign.\n\nMost of the literature on power network analysis deals with the issue of computing the worst voltage drops in the power network. Electromigration is an equally serious concern, but is attacked with almost identical methods. Instead of the voltage at each node, EM analysis solves for current in each branch, and instead of a voltage limit, there is a current limit per wire, depending on its layer and width.\n\nOther IC applications may use only a portions of the flows mentioned here. A gate array or field programmable gate array (FPGA) designer, for example, will only do the design stages, since the detailed usage of these parts is not known when the power supply must be designed. Likewise, a user of FPGAs or gate arrays will only use the analysis portion, as the design is already fixed.\n\n\n"}
{"id": "1162543", "url": "https://en.wikipedia.org/wiki?curid=1162543", "title": "Quantum efficiency", "text": "Quantum efficiency\n\nThe term quantum efficiency (QE) may apply to incident photon to converted electron (IPCE) ratio, of a photosensitive device or it may refer to the TMR effect of a Magnetic Tunnel Junction.\n\nThis article deals with the term as a measurement of a device's electrical sensitivity to light. In a charge-coupled device (CCD) it is the percentage of photons hitting the device's photoreactive surface that produce charge carriers. It is measured in electrons per photon or amps per watt. Since the energy of a photon is inversely proportional to its wavelength, QE is often measured over a range of different wavelengths to characterize a device's efficiency at each photon energy level. The QE for photons with energy below the band gap is zero. Photographic film typically has a QE of much less than 10%, while CCDs can have a QE of well over 90% at some wavelengths.\n\nA solar cell's quantum efficiency value indicates the amount of current that the cell will produce when irradiated by photons of a particular wavelength. If the cell's quantum efficiency is integrated over the whole solar electromagnetic spectrum, one can evaluate the amount of current that the cell will produce when exposed to sunlight. The ratio between this energy-production value and the highest possible energy-production value for the cell (i.e., if the QE were 100% over the whole spectrum) gives the cell's overall energy conversion efficiency value. Note that in the event of multiple exciton generation (MEG), quantum efficiencies of greater than 100% may be achieved since the incident photons have more than twice the band gap energy and can create two or more electron-hole pairs per incident photon.\n\nTwo types of quantum efficiency of a solar cell are often considered:\n\n\nThe IQE is always larger than the EQE. A low IQE indicates that the active layer of the solar cell is unable to make good use of the photons. To measure the IQE, one first measures the EQE of the solar device, then measures its transmission and reflection, and combines these data to infer the IQE.\n\nThe external quantum efficiency therefore depends on both the absorption of light and the collection of charges. Once a photon has been absorbed and has generated an electron-hole pair, these charges must be separated and collected at the junction. A \"good\" material avoids charge recombination. Charge recombination causes a drop in the external quantum efficiency.\n\nThe ideal quantum efficiency graph has a square shape, where the QE value is fairly constant across the entire spectrum of wavelengths measured. However, the QE for most solar cells is reduced because of the effects of recombination, where charge carriers are not able to move into an external circuit. The same mechanisms that affect the collection probability also affect the QE. For example, modifying the front surface can affect carriers generated near the surface. And because high-energy (blue) light is absorbed very close to the surface, considerable recombination at the front surface will affect the \"blue\" portion of the QE. Similarly, lower energy (green) light is absorbed in the bulk of a solar cell, and a low diffusion length will affect the collection probability from the solar cell bulk, reducing the QE in the green portion of the spectrum. Generally, solar cells on the market today do not produce much electricity from ultraviolet and infrared light (<400 nm and >1100 nm wavelengths, respectively); these wavelengths of light are either filtered out or are absorbed by the cell, thus heating the cell. That heat is wasted energy, and could damage the cell.\n\nQuantum efficiency of Image Sensors :\nQuantum efficiency (QE) is the fraction of photon flux that contributes to the photocurrent in a photodetector or\na pixel. Quantum efficiency is one of the most important parameters used to evaluate the quality of a detector and is often called the spectral response to reflect its wavelength dependence. It is defined as the number of signal electrons created per incident photon. In some cases it can exceed 100% (i.e. when more than one electron is created per incident photon).\n\nEQE mapping :\nConventional measurement of the EQE will give the efficiency of the overall device. However it is often useful to have a map of the EQE over large area of the device. This mapping provides an efficient way to visualize the homogeneity and/or the defects in the sample. It was realized by researchers from the Institute of Researcher and Development on Photovoltaic Energy (IRDEP) who calculated the EQE mapping from electroluminescence measurements taken with an hyperspectral imager.\n\nSpectral responsivity is a similar measurement, but it has different units: amperes per watt (A/W); (i.e. how much current comes out of the device per incoming photon of a given energy and wavelength). Both the quantum efficiency and the responsivity are functions of the photons' wavelength (indicated by the subscript λ).\n\nTo convert from responsivity (\"R\", in A/W) to QE (on a scale 0 to 1):\n\nwhere \"λ\" is the wavelength in nm, \"h\" is the Planck constant, \"c\" is the speed of light in a vacuum, and \"e\" is the elementary charge.\n\nwhere formula_5 = number of electrons produced, formula_6 = number of photons absorbed.\n\nAssuming each photon absorbed in the depletion layer produces a viable electron-hole pair, and all other photons do not,\n\nwhere \"t\" is the measurement time (in seconds), \nformula_9 = incident optical power in watts, \nformula_10 = optical power absorbed in depletion layer, also in watts.\n\n"}
{"id": "5758972", "url": "https://en.wikipedia.org/wiki?curid=5758972", "title": "Radio Technical Commission for Aeronautics", "text": "Radio Technical Commission for Aeronautics\n\nRTCA, Inc., formerly known as (Radio Technical Commission for Aeronautics), is a United States volunteer organization that develops technical guidance for use by government regulatory authorities and by industry. It was founded in 1935, and was re-incorporated in 1991 as a private not-for-profit corporation. It has over 200 committees and overall acts as an advisory body to the FAA. In 1948 Special Committee 31 recommended that a common air traffic control system be developed for all aircraft flown in the United States. \n\nRequirements for membership include a fee that is based on information in the application for membership, and an interest in aviation. RTCA is sponsored as a Federal Advisory Committee by the United States Department of Transportation (DOT) Federal Aviation Administration (FAA). Guidance documents are developed and drafted by Special Committee (SC) and are based on a consensus developed within the SC charged with responsibility for the given document. Despite the loosely defined requirements of membership in RTCA, the guidance documents are based on expert technical opinion.\n\nRTCA's objectives include but are not limited to:\n\nAlthough RTCA is sponsored by the FAA, RTCA is not an agency of the United States government. Hence the documents it publishes are treated as guidelines, not as requirements.\n\nSpecial Committee 31 was formed by members of the RTCA to predict the future needs of the United States' air traffic control system. This report recommended a common air traffic control system be developed that would serve the needs of both military and civilian aircraft. They released a final report of their recommendations on May 12, 1948; this report made several requirements of future air traffic control systems (as listed in ): \n\n\nTo meet these requirements the report recommended installation of airport surveillance radar and installation of VHF omnidirectional range (VOR)/distance measuring equipment (DME) on aircraft. In addition the report made several other recommendations such as installing transponders to aircraft to provide altitude and identification information to ground-based radar and to install precision approach radar to improve the capability of aircraft to land in poor weather conditions.\n\nIn 1948 the Air Navigation Development Board (ANDB) was formed to oversee the implementation of the ATC system described in SC-31. There were immediate problems such as budgetary restraints (from the ongoing war) and a concurrent effort by the military to pursue an incompatible system (Tactical Air Navigation; TACAN). In general though, the SC-31 was instrumental in laying the groundwork for the next-generation air traffic system.\n\n\nRTCA SC-167; EUROCAE WG-12. Domain, Aviation. Abbreviation. DO-178B; ED-12B. DO-178B,"}
{"id": "535191", "url": "https://en.wikipedia.org/wiki?curid=535191", "title": "Real-time clock", "text": "Real-time clock\n\nA real-time clock (RTC) is a computer clock (most often in the form of an integrated circuit) that keeps track of the current time.\n\nAlthough the term often refers to the devices in personal computers, servers and embedded systems, RTCs are present in almost any electronic device which needs to keep accurate time. A common RTC used in single-board computers is the Maxim Integrated DS1307.\n\nThe term \"real-time clock\" is used to avoid confusion with ordinary hardware clocks which are only signals that govern digital electronics, and do not count time in human units. RTC should not be confused with real-time computing, which shares its three-letter acronym but does not directly relate to time of day.\n\nAlthough keeping time can be done without an RTC, using one has benefits:\n\nA GPS receiver can shorten its startup time by comparing the current time, according to its RTC, with the time at which it last had a valid signal. If it has been less than a few hours, then the previous ephemeris is still usable.\n\nRTCs often have an alternate source of power, so they can continue to keep time while the primary source of power is off or unavailable. This alternate source of power is normally a lithium battery in older systems, but some newer systems use a supercapacitor, because they are rechargeable and can be soldered. The alternate power source can also supply power to battery backed RAM.\n\nMost RTCs use a crystal oscillator, but some have the option of using the power line frequency. In many cases, the oscillator's frequency is 32.768 kHz. This is the same frequency used in quartz clocks and watches, and for the same reasons, namely that the frequency is exactly 2 cycles per second, is a convenient rate to use with simple binary counter circuits.\n\nMany commercial RTC ICs are accurate to less than 5 parts per million. In practical terms, this is good enough to perform celestial navigation, the classic task of a chronometer. In 2011, Chip-scale atomic clocks were invented. Although more expensive, they keep time within 100 nanoseconds.\n\nMany integrated circuit manufacturers make RTCs, including Epson, Intersil, IDT, Maxim, NXP Semiconductors, Texas Instruments, STMicroelectronics and Ricoh.\n\nThe RTC was introduced to PC compatibles by the IBM PC/AT in 1984, which used a Motorola MC146818 RTC. Later, Dallas Semiconductor made compatible RTCs, which were often used in older personal computers, and are easily found on motherboards because of their distinctive black battery cap and silkscreened logo.\n\nIn newer systems, the RTC is integrated into the southbridge chip.\n\nSome microcontrollers have a real-time clock built in, generally only the ones with many other features and peripherals.\n\nSome modern computers receive clock information by digital radio and use it to promote time-standards. There are two common methods: Most cell phone protocols (e.g. LTE) directly provide the current local time. If an internet radio is available, a computer may use the network time protocol. Computers used as local time servers occasionally use GPS or ultra-low frequency radio transmissions broadcast by a national standards organization (i.e. a radio clock).\n\nSome older computer designs such as Novas and PDP-8s used a real-time clock that was notable for its high accuracy, simplicity, flexibility and low cost. The computer's power supply produces a pulse at logic voltages for either each half-wave or each zero crossing of AC mains. A wire carries the pulse to an interrupt. The interrupt handler software counts cycles, seconds, etc. In this way, it can provide an entire clock and calendar.\n\nThe clock also usually formed the basis of computers' software timing chains; e.g. it was usually the timer used to switch tasks in an operating system. Counting timers used in modern computers provide similar features at lower precision, and may trace their requirements to this type of clock. (e.g. in the PDP-8, the mains-based clock, model DK8EA, came first, and was later followed by a crystal-based clock, DK8EC.) \n\nA software-based clock must be set each time its computer is turned on. Originally this was done by computer operators. When the Internet became commonplace, network time protocols were used to automatically set clocks of this type.\n\nIn Europe, North America and some other grids, this RTC works because the frequency of the AC mains is adjusted to have a long-term frequency accuracy as good as the national standard clocks. That is, in those grids this RTC is superior to quartz clocks and less costly. \n\nThis design of RTC is not practical in portable computers or grids (e.g. in South Asia) that do not regulate the frequency of AC mains. Also it might be thought inconvenient without Internet access to set the clock.\n\nSome motherboards are made without real time clocks. The real time clock is omitted either out of the desire to save money (as in the Raspberry Pi system architecture) or because real time clocks may not be needed at all (as in the Arduino system architecture).\n\n"}
{"id": "19397618", "url": "https://en.wikipedia.org/wiki?curid=19397618", "title": "Runway Awareness and Advisory System", "text": "Runway Awareness and Advisory System\n\nThe Runway Awareness and Advisory System (RAAS) is an electronic detection system that notifies aircraft flight crews on the ground of their position relative to their allocated runway.\n\n RAAS functions by providing audible alerts to confirm runway identification, and also provides an aural alarm if it detects undue acceleration (indicating an attempt to take off) while the aircraft is on any taxiway instead of a designated runway. Its function is possible by a software enhancement to the aircraft's terrain awareness and warning system (TAWS) or enhanced ground proximity warning system (EGPWS). The system was developed by Honeywell.\n\nAlaska Airlines announced in September 2008 that its entire airline fleet of Boeing 737s will be equipped with RAAS by the end of September. That will be the first airline fleet to be completely equipped with this system. In 2015 the largest European low-cost carrier Ryanair announced it would equip their entire fleet by the end of 2016 with RAAS as an investment in its commitment to safety.\n\n"}
{"id": "48706354", "url": "https://en.wikipedia.org/wiki?curid=48706354", "title": "S-procedure", "text": "S-procedure\n\nThe S-procedure or S-lemma is a mathematical result that gives conditions under which a particular quadratic inequality is a consequence of another quadratic inequality. The S-procedure was developed independently in a number of different contexts and has applications in control theory, linear algebra and mathematical optimization.\n\nLet F and F be symmetric matrices, g and g be vectors and h and h be real numbers. Assume that there is some x such that the strict inequality formula_1 holds. Then the implication \nholds if and only if there exists some nonnegative number λ such that \nis positive semidefinite.\n"}
{"id": "32038483", "url": "https://en.wikipedia.org/wiki?curid=32038483", "title": "Scan tool (automotive)", "text": "Scan tool (automotive)\n\nAn automotive scan tool (scanner) is an electronic tool used to interface with, diagnose and, sometimes, reprogram vehicle control modules.\n\nThere are many types from just as many manufacturers, one of the most familiar being the Snap-On \"brick\", or MT2500/MTG2500. Snap-On, OTC/SPX, Autel, Launch, Vetronix/Bosch and a number of other companies produce various types of scan tools, from simple code readers to highly capable bi-directional computers with programming capabilities.\nThe scan tool is connected to the vehicle's data link connector (DLC) and, depending on the particular tool, may only read out diagnostic trouble codes or DTC's (this would be considered a \"code reader\") or may have more capabilities. Actual scan tools will display live data stream (inputs and outputs), have bi-directional controls (the ability to make the controllers do things outside of normal operations) and may even be able to calibrate/program modules within certain parameters. However, a typical scan tool does not have the ability to fully reprogram modules because it requires a pass-through device and specific software.\n\n"}
{"id": "41735", "url": "https://en.wikipedia.org/wiki?curid=41735", "title": "Squelch", "text": "Squelch\n\nIn telecommunications, squelch is a circuit function that acts to suppress the audio (or video) output of a receiver in the absence of a sufficiently strong desired input signal. Squelch is widely used in two-way radios and radio scanners to suppress the sound of channel noise when the radio is not receiving a transmission. Squelch can be 'opened', which allows all signals entering the receiver to be heard. This can be useful when trying to hear distant or otherwise weak signals (also known as DXing).\n\nA carrier squelch or noise squelch is the most simple variant of all. It operates strictly on the signal strength, such as when a television mutes the audio or blanks the video on \"empty\" channels, or when a walkie-talkie mutes the audio when no signal is present. In some designs, the squelch threshold is preset. For example, television squelch settings are usually preset. Receivers in base stations, or repeaters at remote mountain top sites, are usually not adjustable remotely from the control point.\n\nIn two-way radios (also known as radiotelephones), the received signal level required to unsquelch (un-mute) the receiver may be fixed or adjustable with a knob or a sequence of button presses. Typically the operator will adjust the control until noise is heard, and then adjust in the opposite direction until the noise is squelched. At this point, a weak signal will unsquelch the receiver and be heard by the operator. Further adjustment will increase the level of signal required to unsquelch the receiver. \n\nA typical FM two-way radio carrier squelch circuit is noise-operated. To minimize the effects of voice audio on squelch operation, the audio from the receiver's detector is passed through a high-pass filter, typically passing 4,000 Hz (4 kHz) and above, leaving only high frequency noise. The squelch control adjusts the gain of an amplifier which varies the level of the noise coming out of the filter. This noise is rectified, producing a DC voltage when noise is present. The presence of continuous noise on an idle channel creates a DC voltage which turns the receiver audio off. When a signal with little or no noise is received, the noise-derived voltage is reduced and the receiver audio is unmuted. Some applications have the receiver tied to other equipment that uses the audio muting control voltage, as a \"signal present\" indication; for example, in a repeater the act of the receiver unmuting will switch on the transmitter.\n\nTone squelch, or another form of selective calling, is sometimes used to solve interference problems. Where more than one user is on the same channel (\"co-channel\" users), selective calling addresses a subset of all receivers. Instead of turning on the receiver audio for any signal, the audio turns on only in the presence of the correct selective calling code. This is akin to the use of a lock on a door. A carrier squelch is unlocked and will let any signal in. Selective calling locks out all signals except ones with the correct key to the lock (the correct code).\n\nIn non-critical uses, selective calling can also be used to hide the presence of interfering signals such as receiver-produced intermodulation. Receivers with poor specifications—such as inexpensive police scanners or low-cost mobile radios—cannot reject the strong signals present in urban environments. The interference will still be present, and will still degrade system performance, but by using selective calling the user will not have to hear the noises produced by receiving the interference.\n\nFour different techniques are commonly used. Selective calling can be regarded as a form of in-band signaling.\n\nCTCSS (Continuous Tone-Coded Squelch System) continuously superimposes any one of about 50 low-pitch audio tones on the transmitted signal, ranging from 67 to 254 Hz. The original tone set was 10, then 32 tones, and has been expanded even further over the years. CTCSS is often called \"PL tone\" (for \"Private Line\", a trademark of Motorola), or simply \"tone squelch\". General Electric's implementation of CTCSS is called \"Channel Guard\" (or \"CG\"). RCA Corporation used the name \"Quiet Channel\", or \"QC\". There are many other company-specific names used by radio vendors to describe compatible options. Any CTCSS system that has compatible tones is interchangeable. Old and new radios with CTCSS and radios across manufacturers are compatible.\n\nSelcall (Selective Calling) transmits a burst of up to five in-band audio tones at the beginning of each transmission. This feature (sometimes called \"tone burst\") is common in European systems. Early systems used one tone (commonly called \"Tone Burst\"). Several tones were used, the most common being 1,750Hz, which is still used in European amateur radio repeater systems. The addressing scheme provided by one tone was not enough, so a two-tone system was devised—one tone followed by a second tone (sometimes called a \"1+1\" system). Motorola later marketed a system called \"Quik-Call\" that used two simultaneous tones followed by two more simultaneous tones (sometimes called a \"2+2\" system) that was heavily used by fire department dispatch systems in the USA. Later selective call systems used paging system technology that made use of a burst of five sequential tones.\n\nDCS (Digital-Coded Squelch), generically known as \"CDCSS\" (Continuous Digital-Coded Squelch System), was designed as the digital replacement for CTCSS. In the same way that a single CTCSS tone would be used on an entire group of radios, the same DCS code is used in a group of radios. DCS is also referred to as \"Digital Private Line\" (or \"DPL\"), another trademark of Motorola, and likewise, General Electric's implementation of DCS is referred to as \"Digital Channel Guard\" (or \"DCG\"). DCS is also called \"DTCS\" (Digital Tone Code Squelch) by Icom, and other names by other manufacturers. Radios with DCS options are generally compatible, provided the radio's encoder-decoder will use the same code as radios in the existing system.\n\nDCS adds a 134.4 bps (sub-audible) bitstream to the transmitted audio. The code word is a 23-bit Golay (23,12) code which has the ability to detect and correct errors of 3 or fewer bits. The word consists of 12 data bits followed by 11 check bits. The last 3 data bits are a fixed '001', this leaves 9 code bits (512 possibilities) which are conventionally represented as a 3-digit octal number. Note that the first bit transmitted is the LSB, so the code is \"backwards\" from the transmitted bit order. Only 84 of the 512 possible codes are available, to prevent falsing due to alignment collisions.\n\nXTCSS is the newest signalling technique, and provides 99 codes with the added advantage of \"silent operation\". XTCSS-fitted radios are purposed to enjoy more privacy and flexibility of operation. XTCSS is implemented as a combination of CTCSS and in-band signalling.\n\nSquelch was invented first and is still in wide use in two-way radio, especially in the amateur radio world. Squelch of any kind is used to indicate loss of signal, which is used to keep commercial and amateur radio repeaters from continually transmitting. Since a carrier squelch receiver cannot tell a valid carrier from a spurious signal (noise, etc.), CTCSS is often used as well, as it avoids false keyups. Use of CTCSS is especially helpful on congested frequencies or on frequency bands prone to skip and during band openings.\n\nIt is a bad idea to use any coded squelch system to hide interference issues in systems with life-safety or public-safety uses such as police, fire, search and rescue or ambulance company dispatching. Adding tone or digital squelch to a radio system does not solve interference issues, it just covers them up. The presence of interfering signals should be corrected rather than masked. Interfering signals masked by tone squelch will produce apparently random missed messages. The intermittent nature of interfering signals will make the problem difficult to reproduce and troubleshoot. Users will not understand why they cannot hear a call, and will lose confidence in their radio system.\n\nProfessional wireless microphones use squelch to avoid reproducing noise when the receiver does not receive enough signal from the microphone. Most professional models have adjustable squelch, usually set with a screwdriver adjustment or front-panel control on the receiver.\n\n"}
{"id": "44460166", "url": "https://en.wikipedia.org/wiki?curid=44460166", "title": "Staggered tuning", "text": "Staggered tuning\n\nStaggered tuning is a technique used in the design of multi-stage tuned amplifiers whereby each stage is tuned to a slightly different frequency. In comparison to synchronous tuning (where each stage is tuned identically) it produces a wider bandwidth at the expense of reduced gain. It also produces a sharper transition from the passband to the stopband. Both staggered tuning and synchronous tuning circuits are easier to tune and manufacture than many other filter types.\n\nThe function of stagger-tuned circuits can be expressed as a rational function and hence they can be designed to any of the major filter responses such as Butterworth and Chebyshev. The poles of the circuit are easy to manipulate to achieve the desired response because of the amplifier buffering between stages.\n\nApplications include television IF amplifiers (mostly 20th century receivers) and wireless LAN.\n\nStaggered tuning improves the bandwidth of a multi-stage tuned amplifier at the expense of the overall gain. Staggered tuning also increases the steepness of passband skirts and hence improves selectivity.\n\nThe value of staggered tuning is best explained by first looking at the shortcomings of tuning every stage identically. This method is called synchronous tuning. Each stage of the amplifier will reduce the bandwidth. In an amplifier with multiple identical stages, the of the response after the first stage will become the points of the second stage. Each successive stage will add a further to what was the band edge of the first stage. Thus the bandwidth becomes progressively narrower with each additional stage.\n\nAs an example, a four-stage amplifier will have its points at the points of an individual stage. The fractional bandwidth of an LC circuit is given by,\n\nThe bandwidth is thus reduced by a factor of formula_2. In terms of the number of stages formula_3. Thus, the four stage synchronously tuned amplifier will have a bandwidth of only 19% of a single stage. Even in a two-stage amplifier the bandwidth is reduced to 41% of the original. Staggered tuning allows the bandwidth to be widened at the expense of overall gain. The overall gain is reduced because when any one stage is at resonance (and thus maximum gain) the others are not, unlike synchronous tuning where all stages are at maximum gain at the same frequency. A two-stage stagger-tuned amplifier will have a gain less than a synchronously tuned amplifier.\n\nEven in a design that is intended to be synchronously tuned, some staggered tuning effect is inevitable because of the practical impossibility of keeping all tuned circuits perfectly in step and because of feedback effects. This can be a problem in very narrow band applications where essentially only one spot frequency is of interest, such as a local oscillator feed or a wave trap. The overall gain of a synchronously tuned amplifier will always be less than the theoretical maximum because of this.\n\nBoth synchronously tuned and stagger-tuned schemes have a number of advantages over schemes that place all the tuning components in a single aggregated filter circuit separate from the amplifier such as ladder networks or coupled resonators. One advantage is that they are easy to tune. Each resonator is buffered from the others by the amplifier stages so have little effect on each other. The resonators in aggregated circuits, on the other hand, will all interact with each other, particularly their nearest neighbours. Another advantage is that the components need not be close to ideal. Every LC resonator is directly working into a resistor which lowers the \"Q\" anyway so any losses in the L and C components can be absorbed into this resistor in the design. Aggregated designs usually require high \"Q\" resonators. Also, stagger-tuned circuits have resonator components with values that are quite close to each other and in synchronously tuned circuits they can be identical. The spread of component values is thus less in stagger-tuned circuits than in aggregated circuits.\n\nTuned amplifiers such as the one illustrated at the beginning of this article can be more generically depicted as a chain of transconductance amplifiers each loaded with a tuned circuit.\n\nThe gain \"A\"(\"s\"), of one stage of this amplifier is given by;\n\nThis can be written in a more generic form, that is, not assuming that the resonators are the LC type, with the following substitutions,\n\nResulting in,\n\nThe gain expression can be given as a function of (angular) frequency by making the substitution where \"i\" is the imaginary unit and \"ω\" is the angular frequency\n\nThe frequency at the band edges, \"ω\", can be found from this expression by equating the value of the gain at the band edge to the magnitude of the expression,\n\nSolving this for \"ω\" and taking the difference between the two positive solutions finds the bandwidth Δ\"ω\",\n\nand the fractional bandwidth \"B\",\n\nThe overall response of the amplifier is given by the product of the individual stages,\n\nIt is desirable to be able to design the filter from a standard low-pass prototype filter of the required specification. Frequently, a smooth Butterworth response will be chosen but other polynomial functions can be used that allow ripple in the response. A popular choice for a polynomial with ripple is the Chebyshev response for its steep skirt. For the purpose of transformation, the stage gain expression can be rewritten in the more suggestive form,\n\nThis can be transformed into a low-pass prototype filter with the transform\n\nThis can be done straightforwardly for the complete filter in the case of synchronously tuned amplifiers where every stage has the same \"ω\" but for a stagger-tuned amplifier there is no simple analytical solution to the transform. Stagger-tuned designs can be approached instead by calculating the poles of a low-pass prototype of the desired form (e.g. Butterworth) and then transforming those poles to a band-pass response. The poles so calculated can then be used to define the tuned circuits of the individual stages.\n\nThe stage gain can be rewritten in terms of the poles by factorising the denominator;\n\nand the overall response is,\n\nFrom the band-pass to low-pass transform given above, an expression can be found for the poles in terms of the poles of the low-pass prototype, \"q\",\n\nEach pole in the prototype transforms to a complex conjugate pair of poles in the band-pass and corresponds to one stage of the amplifier. This expression is greatly simplified if the cutoff frequency of the prototype, \"ω\"', is set to the final filter bandwidth \"ω\"/\"Q\".\n\nIn the case of a narrowband design which can be used to make a further simplification with the approximation,\n\nThese poles can be inserted into the stage gain expression in terms of poles. By comparing with the stage gain expression in terms of component values, those component values can then be calculated.\n\nStaggered tuning is of most benefit in wideband applications. It was formerly commonly used in television receiver IF amplifiers. However, SAW filters are more likely to be used in that role nowadays. Staggered tuning has advantages in VLSI for radio applications such as wireless LAN. The low spread of component values make it much easier to implement in integrated circuits than traditional ladder networks.\n\n\n"}
{"id": "5270659", "url": "https://en.wikipedia.org/wiki?curid=5270659", "title": "Tie press", "text": "Tie press\n\nA tie press is a device, based solely on pressure, to flatten neckties. Its use is necessitated by ties usually being of silk or some other textile ill-suited to the heat of ironing.\n\nTie presses usually operate based on two separate wooden boards which are clamped together with spring-loaded levers. A cardboard cut-out is usually included to retain the shape of the tie during pressing.\n\nTie presses are particularly useful for bow ties, due to the creasing and thus deformative nature of the bow tie knot, which involves crushing the ends to produce the 'bow' effect. In time, this crushing affects the appearance of the finished knot. This is particularly the case with bow ties with rectangular ends, rather than the 'bow' shaped ends in some bow ties, though both suffer from crushing to some degree or another. Four-in-hand ties, naturally, are also creased, but rarely to the same extent and, as such, usually require less regular pressing.\n"}
{"id": "9755758", "url": "https://en.wikipedia.org/wiki?curid=9755758", "title": "Victorian decorative arts", "text": "Victorian decorative arts\n\nVictorian decorative arts refers to the style of decorative arts during the Victorian era. Victorian design is widely viewed as having indulged in a grand excess of ornament. The Victorian era is known for its interpretation and eclectic revival of historic styles mixed with the introduction of middle east and Asian influences in furniture, fittings, and interior decoration. The Arts and Crafts movement, the aesthetic movement, Anglo-Japanese style, and Art Nouveau style have their beginnings in the late Victorian era and gothic period. \n\nInterior decoration and interior design of the Victorian era are noted for orderliness and ornamentation. A house from this period was idealistically divided in rooms, with public and private space carefully separated. The parlour was the most important room in a home and was the showcase for the homeowners where guests were entertained. A bare room was considered to be in poor taste, so every surface was filled with objects that reflected the owner's interests and aspirations. The dining room was the second-most important room in the house. The sideboard was most often the focal point of the dining room and very ornately decorated.\n\nThe choice of paint color on the walls in Victorian homes was said to be based on the use of the room. Hallways that were in the entry hall and the stair halls were painted a somber gray so as not to compete with the surrounding rooms. Most people marbleized the walls or the woodwork. Also on walls it was common to score into wet plaster to make it resemble blocks of stone. Finishes that were either marbleized or grained were frequently found on doors and woodwork. \"Graining\" was meant to imitate woods of higher quality that were more difficult to work. There were specific rules for interior color choice and placement. The theory of “harmony by analogy” was to use the colors that lay next to each other on the color wheel. And the second was the “harmony by contrast” that was to use the colors that were opposite of one another on the color wheel. There was a favored tripartite wall that included a dado or wainscoting at the bottom, a field in the middle and a frieze or cornice at the top. This was popular into the 20th century. Frederick Walton who created linoleum in 1863 created the process for embossing semi-liquid linseed oil, backed with waterproofed paper or canvas. It was called Lincrusta and was applied much like wallpaper. This process made it easy to then go over the oil and make it resemble wood or different types of leather. On the ceilings that were 8–14 feet the color was tinted three shades lighter than the color that was on the walls and usually had a high quality of ornamentation because decorated ceilings were favored.\n\nThere was not one dominant style of furniture in the Victorian period. Designers rather used and modified many styles taken from various time periods in history like Gothic, Tudor, Elizabethan, English Rococo, Neoclassical and others. The Gothic and Rococo revival style were the most common styles to be seen in furniture during this time in history.\nWallpaper and wallcoverings became accessible for increasing numbers of householders with their wide range of designs and varying costs. This was due to the introduction of mass production techniques and, in England, the repeal in 1836 of the Wallpaper tax introduced in 1712.\n\nWallpaper was often made in elaborate floral patterns with primary colors (red, blue, and yellow) in the backgrounds and overprinted with colours of cream and tan. This was followed by Gothic art inspired papers in earth tones with stylized leaf and floral patterns. William Morris was one of the most influential designers of wallpaper and fabrics during the latter half of the Victorian period. Morris was inspired and used Medieval and Gothic tapestries in his work. Embossed paper were used on ceilings and friezes.\n\n\nVictorian Furniture\n"}
