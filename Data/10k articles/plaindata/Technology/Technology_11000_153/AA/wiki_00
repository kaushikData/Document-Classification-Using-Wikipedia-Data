{"id": "50065824", "url": "https://en.wikipedia.org/wiki?curid=50065824", "title": "3Com 3c509", "text": "3Com 3c509\n\n3Com 3c509 is a line of Ethernet IEEE 802.3 network cards for the ISA, EISA, MCA and PCMCIA computer buses. It was designed by 3Com, and put on the market in 1994.\n\nThe 3Com 3c5x9 family of network controllers have different interface combinations of computer bus like ISA, EISA, MCA and PCMCIA. And for network connection uses 10BASE-2, AUI and 10BASE-T.\n\nB = On ISA and PCMCIA adapter numbers indicates that these adapters are part of the second generation of the Parallel Tasking™ EtherLink III technology.\nThe DIP-28 (U1) EPROM for network booting may be 8, 16 or 32 kByte size. This means EPROMs of type 64, 128, 256 kbit (2^10) are compatible, like the 27C256.\n\nBoot ROM address is located between 0xC0000 - 0xDE000.\n\nThe Etherlink III 3C509B-Combo is registered with the FCC ID DF63C509B. The main components on the card is Y1: crystal oscillator 20 MHz, U50: coaxial transceiver interface DP8392, U4: main controller 3Com 9513S (or 9545S etc.), U6: 70 ns CMOS static RAM, U1: DIP-28 27C256 style EPROM for boot code, U3: 1024 bit 5V CMOS Serial EEPROM (configuration).\n\nLabel:\n\nBarcode:\n\nR = Resistor\nC = Capacitor\nL = Inductance\nQ = Transistor\nCR = Transistor\nFL = Transformer\nT = Transformer\nU = Integrated circuit\nJ = Jumper or connector\nVR\nF\n\nFL70: Pulse transformer\n\nY1: 20 MHz crystal\n\nU50:\n\nT50: Pulse transformer, pinout: 2x8\n\nx00: Pulse transformer\n\nU4: Plastic package 33x33 pins\nAnother chip with the same function:\n\nU6: 8192 x 8-bit 70 ns CMOS static RAM\nAnother chip with the same function:\n\nU1: Boot ROM\n\nU3: 256 Bit/1K 5.0V CMOS Serial EEPROM\n\nQ41: N-Channel Logic level Power MOSFET 60V, 11A, 107 mΩ \n\nVR41: 3-Terminal 0.5 A Negative Voltage Regulator (-5V) in D2PAK\n\nASSY 03-0021-004 REV-B has written on it: U.S. Patents: \n\nConnector for the computer bus: ISA 16-bit\n\nConnections for networking: 10BASE-T (8P8C), AUI (DA-15), 10BASE2 (BNC)\n\nSome of the possible ISA I/O bases are 0x280, 0x300, 0x310, 0x320, 0x330, 0x340, 0x350. And IRQ 5, 7, 9, 10, 11, 12. The driver for OpenBSD, NetBSD and FreeBSD For Linux it is \"eth\".\n\n3c509B-C from 1996 specify the use of with a priority date of 1992-07-28.\n\nThe patent describes a method where a data transfer counter triggers a threshold logic that generates an early indication or interrupt signal before the transfer is completed. The adapter also writes timing information into status registers such that a device driver can optimize for any latency.\n\n\n\n"}
{"id": "34341266", "url": "https://en.wikipedia.org/wiki?curid=34341266", "title": "Aerodynamic and Propulsion Test Unit (Arnold AFB, TN)", "text": "Aerodynamic and Propulsion Test Unit (Arnold AFB, TN)\n\nAEDC Aerodynamic and Propulsion Test Unit (APTU) is a blowdown hypersonic wind tunnel driven by a combustion air heater (CAH). The facility is owned by the United States Air Force and operated by Aerospace Testing Alliance.\n\nThe AEDC Aerodynamic and Propulsion Test Unit started out as a vitiated air heater (VAH) conducting over 275 experiments for the development of many different aerodynamic and aerothermal systems. Upgrades to the facility started in 2002 in order to provide ground-test capability for supersonic and hypersonic systems up to flight speeds of Mach 8.\n\nThe facility was designed to provide ground-based simulations of supersonic and hypersonic flight conditions. The combustion air heater can provide total pressures from 200 psia to 2,800 psia (13.6 atm to 190.5 atm) and a total temperatures from 1,200°R to 4,700°R (667 K to 2,611 K). Five nozzles ranging from Mach 3.2 to Mach 7.1 are currently available.\n\n\n"}
{"id": "57840089", "url": "https://en.wikipedia.org/wiki?curid=57840089", "title": "Albedometer", "text": "Albedometer\n\nAn albedometer is an instrument used to measure the albedo (reflecting radiation) of a surface. An albedometer is mostly used to measure the reflectance of earths surface. Often it consists of two pyranometers: one facing up towards the sky and one facing down towards the surface. From the ratio of incoming and reflecting radiation the albedo can be calculated.\nThe measurement of surface albedo of earths surface happens by using two pyranometers. The upfacing pyranometer measures the incoming global solar radiation. The downward facing pyranometer measures the reflected global solar radiation. The ratio of the reflected to the global radiation is the solar albedo and depends on the properties of the surface and the directional distribution of the incoming solar radiation. Typical values range from 4% for asphalt to 90% for fresh snow.\n\n\n"}
{"id": "2393", "url": "https://en.wikipedia.org/wiki?curid=2393", "title": "Analog television", "text": "Analog television\n\nAnalog television or analogue television is the original television technology that uses analog signals to transmit video and audio. In an analog television broadcast, the brightness, colors and sound are represented by rapid variations of either the amplitude, frequency or phase of the signal.\n\nAnalog signals vary over a continuous range of possible values which means that electronic noise and interference becomes reproduced by the receiver. Thus with analog, a moderately weak signal becomes snowy and subject to interference. In contrast, a moderately weak digital signal and a very strong digital signal transmit equal picture quality. Analog television may be wireless or can be distributed over a cable network using cable converters.\n\nAll broadcast television systems used analog signals before the arrival of digital television (DTV). Motivated by the lower bandwidth requirements of compressed digital signals, since the 2000s a digital television transition is proceeding in most countries of the world, with different deadlines for cessation of analog broadcasts.\n\nThe earliest systems of analog television were mechanical television systems, which used spinning disks with patterns of holes punched into the disc to scan an image. A similar disk reconstructed the image at the receiver. Synchronization of the receiver disc rotation was handled through sync pulses broadcast with the image information. However these mechanical systems were slow, the images were dim and flickered severely, and the image resolution very low. Camera systems used similar spinning discs and required intensely bright illumination of the subject for the light detector to work.\n\nAnalog television did not really begin as an industry until the development of the cathode-ray tube (CRT), which uses a focused electron beam to trace lines across a phosphor coated surface. The electron beam could be swept across the screen much faster than any mechanical disc system, allowing for more closely spaced scan lines and much higher image resolution. Also far less maintenance was required of an all-electronic system compared to a spinning disc system. All-electronic systems became popular with households after the Second World War.\n\nBroadcasters of analog television encode their signal using different systems. The official systems of transmission are named: A, B, C, D, E, F, G, H, I, K, K1, L, M and N. These systems determine the number of scan lines, frame rate, channel width, video bandwidth, video-audio separation, and so on.\n\nThe colors in those systems are encoded with one of three color coding schemes: NTSC, PAL, or SECAM, and then use RF modulation to modulate this signal onto a very high frequency (VHF) or ultra high frequency (UHF) carrier. Each frame of a television image is composed of lines drawn on the screen. The lines are of varying brightness; the whole set of lines is drawn quickly enough that the human eye perceives it as one image. The next sequential frame is displayed, allowing the depiction of motion. The analog television signal contains timing and synchronization information, so that the receiver can reconstruct a two-dimensional moving image from a one-dimensional time-varying signal.\n\nThe first commercial television systems were black-and-white; the beginning of color television was in the 1950s.\n\nA practical television system needs to take luminance, chrominance (in a color system), synchronization (horizontal and vertical), and audio signals, and broadcast them over a radio transmission. The transmission system must include a means of television channel selection.\n\nAnalog broadcast television systems come in a variety of frame rates and resolutions. Further differences exist in the frequency and modulation of the audio carrier. The monochrome combinations still existing in the 1950s are standardized by the International Telecommunication Union (ITU) as capital letters A through N. When color television was introduced, the hue and saturation information was added to the monochrome signals in a way that black and white televisions ignore. In this way backwards compatibility was achieved. That concept is true for all analog television standards.\n\nThere were three standards for the way the additional color information can be encoded and transmitted. The first was the American NTSC (National Television Systems Committee) color television system. The European/Australian PAL (Phase Alternation Line rate) and the French-former Soviet Union SECAM (Séquentiel Couleur Avec Mémoire) standard were developed later and attempt to cure certain defects of the NTSC system. PAL's color encoding is similar to the NTSC systems. SECAM, though, uses a different modulation approach than PAL or NTSC.\n\nIn principle, all three color encoding systems can be combined with any scan line/frame rate combination. Therefore, in order to describe a given signal completely, it's necessary to quote the color system and the broadcast standard as a capital letter. For example, the United States, Canada, Mexico and South Korea use NTSC-M (many of these transitioned or transitioning to digital), Japan uses NTSC-J (discontinued in 2012, when Japan transitioned to digital (ISDB)), the UK uses PAL-I (discontinued in 2012, when UK transitioned to digital (DVB-T)), France uses SECAM-L (discontinued in 2011, when France transitioned to digital (DVB-T)), much of Western Europe and Australia use PAL-B/G (Many of these transitioned or transitioning to DVB-T as digital television standards), most of Eastern Europe uses SECAM-D/K or PAL-D/K and so on.\n\nHowever, not all of these possible combinations actually exist. NTSC is currently only used with system M, even though there were experiments with NTSC-A (405 line) in the UK and NTSC-N (625 line) in part of South America. PAL is used with a variety of 625-line standards (B,G,D,K,I,N) but also with the North American 525-line standard, accordingly named PAL-M. Likewise, SECAM is used with a variety of 625-line standards.\n\nFor this reason many people refer to any 625/25 type signal as \"PAL\" and to any 525/30 signal as \"NTSC\", even when referring to digital signals; for example, on DVD-Video, which does not contain any analog color encoding, and thus no PAL or NTSC signals at all. Even though this usage is common, it is misleading, as that is not the original meaning of the terms PAL/SECAM/NTSC.\n\nAlthough a number of different broadcast television systems were in use worldwide, the same principles of operation apply.\n\nIn many countries, over-the-air broadcast television of analog audio and analog video signals has been discontinued, to allow the re-use of the television broadcast radio spectrum for other services such as datacasting and subchannels.\n\nA cathode-ray tube (CRT) television displays an image by scanning a beam of electrons across the screen in a pattern of horizontal lines known as a raster. At the end of each line the beam returns to the start of the next line; the end of the last line is a link that returns to the top of the screen. As it passes each point the intensity of the beam is varied, varying the luminance of that point. A color television system is identical except that an additional signal known as chrominance controls the color of the spot.\n\nRaster scanning is shown in a slightly simplified form below.\n\nWhen analog television was developed, no affordable technology for storing any video signals existed; the luminance signal has to be generated and transmitted at the same time at which it is displayed on the CRT. It is therefore essential to keep the raster scanning in the camera (or other device for producing the signal) in exact synchronization with the scanning in the television.\n\nThe physics of the CRT require that a finite time interval be allowed for the spot to move back to the start of the next line (\"horizontal retrace\") or the start of the screen (\"vertical retrace\"). The timing of the luminance signal must allow for this.\n\nThe human eye has a characteristic called Phi phenomenon. Quickly displaying successive scan images will allow the apparent illusion of smooth motion. Flickering of the image can be partially solved using a long persistence phosphor coating on the CRT, so that successive images fade slowly. However, slow phosphor has the negative side-effect of causing image smearing and blurring when there is a large amount of rapid on-screen motion occurring.\n\nThe maximum frame rate depends on the bandwidth of the electronics and the transmission system, and the number of horizontal scan lines in the image. A frame rate of 25 or 30 hertz is a satisfactory compromise, while the process of interlacing two video fields of the picture per frame is used to build the image. This process doubles the apparent number of video frames per second and further reduces flicker and other defects in transmission.\n\nPlasma screens and LCD screens have been used in analog television sets. These types of display screens use lower voltages than older CRT displays. Many dual system television receivers, equipped to receive both analog transmissions and digital transmissions have analog tuner receiving capability and must use a television antenna.\n\nThe television system for each country will specify a number of television channels within the UHF or VHF frequency ranges. A channel actually consists of two signals: the picture information is transmitted using amplitude modulation on one frequency, and the sound is transmitted with frequency modulation at a frequency at a fixed offset (typically 4.5 to 6 MHz) from the picture signal.\n\nThe channel frequencies chosen represent a compromise between allowing enough bandwidth for video (and hence satisfactory picture resolution), and allowing enough channels to be packed into the available frequency band. In practice a technique called vestigial sideband is used to reduce the channel spacing, which would be nearly twice the video bandwidth if pure AM was used.\n\nSignal reception is invariably done via a superheterodyne receiver: the first stage is a \"tuner\" which selects a television channel and frequency-shifts it to a fixed intermediate frequency (IF). The signal amplifier performs amplification to the IF stages from the microvolt range to fractions of a volt.\n\nAt this point the IF signal consists of a video carrier signal at one frequency and the sound carrier at a fixed offset. A demodulator recovers the video signal. Also at the output of the same demodulator is a new frequency modulated sound carrier at the offset frequency. In some sets made before 1948, this was filtered out, and the sound IF of about 22 MHz was sent to an FM demodulator to recover the basic sound signal. In newer sets, this new carrier at the offset frequency was allowed to remain as \"intercarrier sound\", and it was sent to an FM demodulator to recover the basic sound signal. One particular advantage of intercarrier sound is that when the front panel fine tuning knob is adjusted, the sound carrier frequency does not change with the tuning, but stays at the above-mentioned offset frequency. Consequently, it is easier to tune the picture without losing the sound.\n\nSo the FM sound carrier is then demodulated, amplified, and used to drive a loudspeaker. Until the advent of the NICAM and MTS systems, television sound transmissions were invariably monophonic.\n\nThe video carrier is demodulated to give a composite video signal; this contains luminance, chrominance and synchronization signals; this is identical to the video signal format used by analog video devices such as VCRs or CCTV cameras. Note that the RF signal modulation is inverted compared to the conventional AM: the minimum video signal level corresponds to maximum carrier amplitude, and vice versa. To ensure good linearity (fidelity), consistent with affordable manufacturing costs of transmitters and receivers, the video carrier is never shut off altogether. When intercarrier sound was invented later in 1948, not completely shutting off the carrier had the side effect of allowing intercarrier sound to be economically implemented.\n\nEach line of the displayed image is transmitted using a signal as shown above. The same basic format (with minor differences mainly related to timing and the encoding of color) is used for PAL, NTSC and SECAM television systems. A monochrome signal is identical to a color one, with the exception that the elements shown in color in the diagram (the color burst, and the chrominance signal) are not present.\nThe \"front porch\" is a brief (about 1.5 microsecond) period inserted between the end of each transmitted line of picture and the leading edge of the next line sync pulse. Its purpose was to allow voltage levels to stabilise in older televisions, preventing interference between picture lines. The \"front porch\" is the first component of the horizontal blanking interval which also contains the horizontal sync pulse and the \"back porch\".\n\nThe \"back porch\" is the portion of each scan line between the end (rising edge) of the horizontal sync pulse and the start of active video. It is used to restore the black level (300 mV) reference in analog video. In signal processing terms, it compensates for the fall time and settling time following the sync pulse.\n\nIn color television systems such as PAL and NTSC, this period also includes the colorburst signal. In the SECAM system it contains the reference subcarrier for each consecutive color difference signal in order to set the zero-color reference.\n\nIn some professional systems, particularly satellite links between locations, the audio is embedded within the back porch of the video signal, to save the cost of renting a second channel.\n\nThe luminance component of a composite video signal varies between 0 V and approximately 0.7 V above the \"black\" level. In the NTSC system, there is a \"blanking\" signal level used during the front porch and back porch, and a \"black\" signal level 75 mV above it; in PAL and SECAM these are identical.\n\nIn a monochrome receiver the luminance signal is amplified to drive the control grid in the electron gun of the CRT. This changes the intensity of the electron beam and therefore the brightness of the spot being scanned. Brightness and contrast controls determine the DC shift and amplification, respectively.\n\nA color signal conveys picture information for each of the red, green, and blue components of an image (see the article on color space for more information). However, these are not simply transmitted as three separate signals, because: such a signal would not be compatible with monochrome receivers (an important consideration when color broadcasting was first introduced). It would also occupy three times the bandwidth of existing television, requiring a decrease in the number of television channels available. Furthermore, typical problems with signal transmission (such as differing received signal levels between different colors) would produce unpleasant side effects.\n\nInstead, the RGB signals are converted into YUV form, where the Y signal represents the lightness and darkness (luminance) of the colors in the image. Because the rendering of colors in this way is the goal of both black and white (monochrome) film and black and white (monochrome) television systems, the Y signal is ideal for transmission as the luminance signal. This ensures a monochrome receiver will display a correct picture in black and white, where a given color is reproduced by a shade of gray that correctly reflects how light or dark the original color is.\n\nThe U and V signals are \"color difference\" signals. The U signal is the difference between the B signal and the Y signal, also known as B minus Y (B-Y), and the V signal is the difference between the R signal and the Y signal, also known as R minus Y (R-Y). The U signal then represents how \"purplish-blue\" or its complementary color \"yellowish-green\" the color is, and the V signal how \"purplish-red\" or its complementary \"greenish-cyan\" it is. The advantage of this scheme is that the U and V signals are zero when the picture has no color content. Since the human eye is more sensitive to errors in luminance than in color, the U and V signals can be transmitted in a relatively lossy (specifically: bandwidth-limited) way with acceptable results.\n\nIn the receiver, a single demodulator can extract an additive combination of U plus V. An example is the X demodulator used in the X/Z demodulation system. In that same system, a second demodulator, the Z demodulator, also extracts an additive combination of U plus V, but in a different ratio. The X and Z color difference signals are further matrixed into three color difference signals, (R-Y), (B-Y), and (G-Y). The combinations of usually two, but sometimes three demodulators were:\n\nIn the end, further matrixing of the above color-difference signals c through f yielded the three color-difference signals, (R-Y), (B-Y), and (G-Y).\n\nThe R,G,B signals in the receiver needed for the display device (CRT, Plasma display or LCD display) are electronically derived by matrixing as follows: R is the additive combination of (R-Y) with Y, G is the additive combination of (G-Y) with Y, and B is the additive combination of (B-Y) with Y. All of this is accomplished electronically. It can be seen that in the combining process, the low resolution portion of the Y signals cancel out, leaving R,G, and B signals able to render a low-resolution image in full color. However, the higher resolution portions of the Y signals do not cancel out, and so are equally present in R, G, and B, producing the higher definition (higher resolution) image detail in monochrome, although it appears to the human eye as a full-color and full resolution picture.\n\nIn the NTSC and PAL color systems, U and V are transmitted by using quadrature amplitude modulation of a subcarrier. This kind of modulation applies two independent signals to one subcarrier, with the idea that both signals will be recovered independently at the receive end. Before transmission, the subcarrier itself, is removed from the active (visible) portion of the video, and moved, in the form of a burst, to the horizontal blanking portion, which is not directly visible on screen. (More about the burst below.)\n\nFor NTSC, the subcarrier is a 3.58 MHz sine wave. For the PAL system it is a 4.43 MHz sine wave. After the above-mentioned quadrature amplitude modulation of the subcarrier, subcarrier sidebands are produced, and the subcarrier itself is filtered out of the visible portion of the video, since it is the subcarrier sidebands that carry all of the U and V information, and the subcarrier itself carries no information.\n\nThe resulting subcarrier sidebands is also known as \"chroma\" or \"chrominance\". Physically, this chrominance signal is a 3.58 MHz(NTSC) or 4.43 MHz(PAL) sine wave which, in response to changing U and V values, changes phase as compared to the subcarrier, and also changes amplitude.\n\nAs it turns out, the chroma amplitude (when considered together with the Y signal) represents the approximate saturation of a color, and the chroma phase against the subcarrier as reference, approximately represents the hue of the color. For particular test colors found in the test color bar pattern, exact amplitudes and phases are sometimes defined for test and trouble shooting purposes only.\n\nAlthough, in response to changing U and V values, the chroma sinewave changes phase with respect to the subcarrier, it's not correct to say that the subcarrier is simply \"phase modulated\". That is because a single sine wave U test signal with QAM produces only one pair of sidebands, whereas real phase modulation under the same test conditions would produce multiple sets of sidebands occupying more frequency spectrum.\n\nIn NTSC, the chrominance sine wave has the same average frequency as the subcarrier frequency. But a spectrum analyzer instrument shows that, for transmitted chrominance, the frequency component at the subcarrier frequency is actually zero energy, verifying that the subcarrier was indeed removed before transmission.\n\nThese sideband frequencies are within the luminance signal band, which is why they are called \"subcarrier\" sidebands instead of simply \"carrier\" sidebands. Their exact frequencies were chosen such that (for NTSC), they are midway between two harmonics of the frame repetition rate, thus ensuring that the majority of the power of the luminance signal does not overlap with the power of the chrominance signal.\n\nIn the British PAL (D) system, the actual chrominance center frequency, with equal lower and upper sidebands, is 4.43361875 MHz, a direct multiple of the scan rate frequency. This frequency was chosen to minimize the chrominance beat interference pattern that would be visible in areas of high color saturation in the transmitted picture.\n\nAt certain times, the chrominance signal represents only the U signal, and 70 nanoseconds (NTSC) later, the chrominance signal represents only the V signal. (This is the nature of the quadrature amplitude modulation process that created the chrominance signal.) About 70 nanoseconds later still, -U, and another 70 nanoseconds, -V.\n\nSo to extract U, a synchronous demodulator is utilized, which uses the subcarrier to briefly gate (sample) the chroma every 280 nanoseconds, so that the output is only a train of discrete pulses, each having an amplitude that is the same as the original U signal at the corresponding time. In effect, these pulses are discrete-time analog samples of the U signal. The pulses are then low-pass filtered so that the original analog continuous-time U signal is recovered. For V, a 90 degree shifted subcarrier briefly gates the chroma signal every 280 nanoseconds, and the rest of the process is identical to that used for the U signal.\n\nGating at any other time than those times mentioned above will yield an additive mixture of any two of U, V, -U, or -V. One of these \"off-axis\" (that is, off the U and V axis) gating methods is called I/Q demodulation. Another much more popular \"off-axis\" scheme was the X/Z demodulation system. Further matrixing recovered the original U and V signals. This scheme was actually the most popular demodulator scheme throughout the 60's.\n\nThe above process uses the subcarrier. But as previously mentioned, it was deleted before transmission, and only the chroma is transmitted. Therefore, the receiver must reconstitute the subcarrier. For this purpose, a short burst of subcarrier, known as the color burst, is transmitted during the back porch (re-trace blanking period) of each scan line. A subcarrier oscillator in the receiver locks onto this signal (see phase-locked loop) to achieve a phase reference, resulting in the oscillator producing the reconstituted subcarrier.\n\nNTSC uses this process unmodified. Unfortunately, this often results in poor color reproduction due to phase errors in the received signal, caused sometimes by multipath, but mostly by poor implementation at the studio end. With the advent of solid state receivers, cable TV, and digital studio equipment for conversion to an over-the-air analog signal, these NTSC problems have been largely fixed, leaving operator error at the studio end as the sole color rendition weakness of the NTSC system. In any case, the PAL D (delay) system mostly corrects these kind of errors by reversing the phase of the signal on each successive line, and the averaging the results over pairs of lines. This process is achieved by the use of a 1H (where H = horizontal scan frequency) duration delay line. (A typical circuit used with this device converts the low frequency color signal to ultrasound and back again). Phase shift errors between successive lines are therefore cancelled out and the wanted signal amplitude is increased when the two in-phase (coincident) signals are re-combined.\n\nNTSC is more spectrum efficient than PAL, giving more picture detail for a given bandwidth. This is because sophisticated comb filters in receivers are more effective with NTSC's 4 field color phase cadence compared to PAL's 8 field cadence. However, in the end, the larger channel width of most PAL systems in Europe still give their PAL systems the edge in transmitting more picture detail.\n\nIn the SECAM television system, U and V are transmitted on \"alternate\" lines, using simple frequency modulation of two different color subcarriers.\n\nIn some analog color CRT displays, starting in 1956, the brightness control signal (luminance) is fed to the cathode connections of the electron guns, and the color difference signals (chrominance signals) are fed to the control grids connections. This simple CRT matrix mixing technique was replaced in later solid state designs of signal processing with the original matrixing method used in the 1954 and 1955 color TV receivers.\n\nSynchronizing pulses added to the video signal at the end of every scan line and video frame ensure that the sweep oscillators in the receiver remain locked in step with the transmitted signal, so that the image can be reconstructed on the receiver screen.\n\nA \"sync separator\" circuit detects the sync voltage levels and sorts the pulses into horizontal and vertical sync. (see section below – Other technical information, for extra detail.)\n\nThe horizontal synchronization pulse (\"horizontal sync\", or \"HSync\"), separates the scan lines. The horizontal sync signal is a single short pulse which indicates the start of every line. The rest of the scan line follows, with the signal ranging from 0.3 V (black) to 1 V (white), until the next horizontal or vertical synchronization pulse.\n\nThe format of the horizontal sync pulse varies. In the 525-line NTSC system it is a 4.85 µs-long pulse at 0 V. In the 625-line PAL system the pulse is 4.7 µs synchronization pulse at 0 V . This is lower than the amplitude of any video signal (\"blacker than black\") so it can be detected by the level-sensitive \"sync stripper\" circuit of the receiver.\n\nVertical synchronization (also called vertical sync or VSync) separates the video fields. In PAL and NTSC, the vertical sync pulse occurs within the vertical blanking interval. The vertical sync pulses are made by prolonging the length of HSYNC pulses through almost the entire length of the scan line.\n\nThe \"vertical sync\" signal is a series of much longer pulses, indicating the start of a new field. The sync pulses occupy the whole of line interval of a number of lines at the beginning and end of a scan; no picture information is transmitted during vertical retrace. The pulse sequence is designed to allow horizontal sync to continue during vertical retrace; it also indicates whether each field represents even or odd lines in interlaced systems (depending on whether it begins at the start of a horizontal line, or midway through).\n\nThe format of such a signal in 525-line NTSC is:\n\nEach pre- or post- equalizing pulse consists in half a scan line of black signal: 2 µs at 0 V, followed by 30 µs at 0.3 V.\n\nEach long sync pulse consists in an equalizing pulse with timings inverted: 30 µs at 0 V, followed by 2 µs at 0.3 V.\n\nIn video production and computer graphics, changes to the image are often kept in step with the vertical synchronization pulse to avoid visible discontinuity of the image. Since the frame buffer of a computer graphics display imitates the dynamics of a cathode-ray display, if it is updated with a new image while the image is being transmitted to the display, the display shows a mishmash of both frames, producing a page tearing artifact partway down the image.\n\nVertical synchronization eliminates this by timing frame buffer fills to coincide with the vertical blanking interval, thus ensuring that only whole frames are seen on-screen. Software such as video games and computer-aided design (CAD) packages often allow vertical synchronization as an option, because it delays the image update until the vertical blanking interval. This produces a small penalty in latency, because the program has to wait until the video controller has finished transmitting the image to the display before continuing. Triple buffering reduces this latency significantly.\n\nTwo timing intervals are defined – the \"front porch\" between the end of displayed video and the start of the sync pulse, and the \"back porch\" after the sync pulse and before displayed video. These and the sync pulse itself are called the \"horizontal blanking\" (or \"retrace\") \"interval\" and represent the time that the electron beam in the CRT is returning to the start of the next display line.\n\nThe lack of precision timing components in early television receivers meant that the timebase circuits occasionally needed manual adjustment.\nIf their free-run frequencies were too far from the actual line and field rates, the circuits would not be able to follow the incoming sync signals.\nLoss of horizontal synchronization usually resulted in an unwatchable picture; loss of vertical synchronization would produce an image rolling up or down the screen.\n\nThe adjustment took the form of \"horizontal hold\" and \"vertical hold\" controls, usually on the front panel along with other common controls. These adjusted the free-run frequencies of the corresponding timebase oscillators.\n\nBy the early 1980s the efficacy of the synchronization circuits, plus the inherent stability of the sets' oscillators, had been improved to the point where these controls were no longer necessary.\n\nA typical analog monochrome television receiver is based around the block diagram shown below:\n\nImage synchronization is achieved by transmitting negative-going pulses; in a composite video signal of 1 volt amplitude, these are approximately 0.3 V below the \"black level\". The \"horizontal sync\" signal is a single short pulse which indicates the start of every line. Two timing intervals are defined – the \"front porch\" between the end of displayed video and the start of the sync pulse, and the \"back porch\" after the sync pulse and before displayed video. These and the sync pulse itself are called the \"horizontal blanking\" (or \"retrace\") \"interval\" and represent the time that the electron beam in the CRT is returning to the start of the next display line.\n\nThe \"vertical sync\" signal is a series of much longer pulses, indicating the start of a new field. The sync pulses occupy the whole of line interval of a number of lines at the beginning and end of a scan; no picture information is transmitted during vertical retrace. The pulse sequence is designed to allow horizontal sync to continue during vertical retrace; it also indicates whether each field represents even or odd lines in interlaced systems (depending on whether it begins at the start of a horizontal line, or midway through).\n\nIn the television receiver, a \"sync separator\" circuit detects the sync voltage levels and sorts the pulses into horizontal and vertical sync.\n\nLoss of horizontal synchronization usually resulted in an unwatchable picture; loss of vertical synchronization would produce an image rolling up or down the screen.\n\nCounting sync pulses, a video line selector picks a selected line from a TV signal, used for teletext, on-screen displays, station identification logos as well as in the industry when cameras were used as a sensor.\n\nIn an analog receiver with a CRT display sync pulses are fed to horizontal and vertical \"timebase\" circuits (commonly called \"sweep circuits\" in the United States), each consisting of an oscillator and an amplifier. These generate modified sawtooth and parabola current waveforms to scan the electron beam in a linear way. The waveform shapes are necessary to make up for the distance variations from the electron beam source and the screen surface. The oscillators are designed to free-run at frequencies very close to the field and line rates, but the sync pulses cause them to reset at the beginning of each scan line or field, resulting in the necessary synchronization of the beam sweep with the originating signal. The output waveforms from the timebase amplifiers are fed to the horizontal and vertical \"deflection coils\" wrapped around the CRT tube. These coils produce magnetic fields proportional to the changing current, and these deflect the electron beam across the screen.\n\nIn the 1950s, the power for these circuits was derived directly from the mains supply.\nA simple circuit consisted of a series voltage dropper resistance and a rectifier valve (tube) or semiconductor diode. This avoided the cost of a large high voltage mains supply (50 or 60 Hz) transformer. This type of circuit was used for thermionic valve (vacuum tube) technology. It was inefficient and produced a lot of heat which led to premature failures in the circuitry.\n\nIn the 1960s, semiconductor technology was introduced into timebase circuits. During the late 1960s in the UK, synchronous (with the scan line rate) power generation was introduced into solid state receiver designs. These had very complex circuits in which faults were difficult to trace, but had very efficient use of power.\n\nIn the early 1970s AC mains (50 or 60 Hz), and line timebase (15,625 Hz), thyristor based switching circuits were introduced. In the UK use of the simple (50 Hz) types of power circuits were discontinued. The reason for design changes arose from the electricity supply contamination problems arising from EMI, and supply loading issues due to energy being taken from only the positive half cycle of the mains supply waveform.\n\nMost of the receiver's circuitry (at least in transistor- or IC-based designs) operates from a comparatively low-voltage DC power supply. However, the anode connection for a cathode-ray tube requires a very high voltage (typically 10–30 kV) for correct operation.\n\nThis voltage is not directly produced by the main power supply circuitry; instead the receiver makes use of the circuitry used for horizontal scanning. Direct current (DC), is switched though the line output transformer, and alternating current (AC) is induced into the scan coils. At the end of each horizontal scan line the magnetic field, which has built up in both transformer and scan coils by the current, is a source of latent electromagnetic energy. This stored collapsing magnetic field energy can be captured. The reverse flow, short duration, (about 10% of the line scan time) current from both the line output transformer and the horizontal scan coil is discharged again into the primary winding of the flyback transformer by the use of a rectifier which blocks this negative reverse emf. A small value capacitor is connected across the scan switching device. This tunes the circuit inductances to resonate at a much higher frequency. This slows down (lengthens) the flyback time from the extremely rapid decay rate that would result if they were electrically isolated during this short period. One of the secondary windings on the flyback transformer then feeds this brief high voltage pulse to a Cockcroft–Walton generator design voltage multiplier. This produces the required EHT supply. A flyback converter is a power supply circuit operating on similar principles.\n\nA typical modern design incorporates the flyback transformer and rectifier circuitry into a single unit with a captive output lead, (known as a diode split line output transformer or an Integrated High Voltage Transformer (IHVT)), so that all high-voltage parts are enclosed. Earlier designs used a separate line output transformer and a well insulated high voltage multiplier unit. The high frequency (15 kHz or so) of the horizontal scanning allows reasonably small components to be used.\n\nThe first country to make a wholesale switch to digital over-the-air (terrestrial television) broadcasting was Luxembourg in 2006, followed later in 2006 by the Netherlands; in 2007 by Finland, Andorra, Sweden and Switzerland; in 2008 by Belgium (Flanders) and Germany; in 2009 by the United States (high power stations), southern Canada, the Isle of Man, Norway, and Denmark. In 2010, Belgium (Wallonia), Spain, Wales, Latvia, Estonia, the Channel Islands, San Marino, Croatia and Slovenia; in 2011 Israel, Austria, Monaco, Cyprus, Japan (excluding Miyagi, Iwate, and Fukushima prefectures), Malta and France; in 2012 the Czech Republic, Arab World, Taiwan, Portugal, Japan (including Miyagi, Iwate, and Fukushima prefectures), Serbia, Italy, Canada, Mauritius, the United Kingdom, the Republic of Ireland, Lithuania, Slovakia, Gibraltar, and South Korea; in 2013, the Republic of Macedonia, Poland, Bulgaria, Hungary, Australia, and New Zealand, completed the transition. The United Kingdom made the transition to digital television between 2008 and 2012, with the exception of Barrow-in-Furness, which made the switch over in 2007. The first digital TV-only area in the United Kingdom was Ferryside in Carmarthenshire.\n\nThe Digital television transition in the United States for high-powered transmission was completed on 12 June 2009, the date that the Federal Communications Commission (FCC) set. Almost two million households could no longer watch television because they had not prepared for the transition. The switchover had been delayed by the DTV Delay Act. While the majority of the viewers of over-the-air broadcast television in the U.S. watch full-power stations (which number about 1800), there are three other categories of television stations in the U.S.: low-power broadcasting stations, class A stations, and television translator stations. Class A low-power stations were required to convert to digital by 21 September 2015 and all others were required to convert to digital before 13 July 2021. In broadcasting, whatever happens in the United States also influences southern Canada and northern Mexico because those areas are covered by television stations in the U.S.\n\nIn Japan, the switch to digital began in northeastern Ishikawa Prefecture on 24 July 2010 and ended in 43 of the country's 47 prefectures (including the rest of Ishikawa) on 24 July 2011, but in Fukushima, Iwate, and Miyagi prefectures, the conversion was delayed to 31 March 2012, due to complications from the 2011 Tōhoku earthquake and tsunami and its related nuclear accidents. \n\nIn Canada, most of the larger cities turned off analog broadcasts on 31 August 2011. \n\nChina is scheduled to end analog broadcasting between 2015 and 2018, due to the large size of the country.\n\nBrazil switched to digital television on 2 December 2007 in its major cities. It is now estimated that Brazil will end analog broadcasting in 2023.\n\nIn Malaysia, the Malaysian Communications & Multimedia Commission (MCMC) advertised for tender bids to be submitted in the third quarter of 2009 for the 470 through 742 MHz UHF allocation, to enable Malaysia's broadcast system to move into DTV. The new broadcast band allocation would result in Malaysia's having to build an infrastructure for all broadcasters, using a single digital terrestrial transmission/television broadcast (DTTB) channel. Large portions of Malaysia are covered by television broadcasts from Singapore, Thailand, Brunei, and Indonesia (from Borneo and Batam).\n\nIn Singapore, digital television under DVB-T2 began on 16 December 2013. The switchover has been delayed many times until 31 December 2018.\n\nIn the Philippines, the National Telecommunications Commission required all broadcasting companies to end analog broadcasting on December 31, 2015 at 11:59 p.m. Due to delay of the release of the implementing rules and regulations for digital television broadcast, the target date was moved to 2020. Full digital broadcast is expected in 2021.\n\n"}
{"id": "30535348", "url": "https://en.wikipedia.org/wiki?curid=30535348", "title": "Autometric", "text": "Autometric\n\nAutometric Inc. was a company spun out of Paramount Pictures to work with early satellite imagery.\n\nEarly successes at Autometric included the invention of the Chromatron which was subsequently sold to Sony and used prior to developing the Trinitron.\n\nThe company created image analysis products for representing imagery of the earth, cataloging, and image analysis. Autometric developed early orthophotographic hardware and methodology. The company was acquired by Raytheon and transitioned its product focus to military and intelligence projects. It was called Autometric Operation of Raytheon. In a few years, Autometric split off from Raytheon becoming independent. After a series of management blunders and near bankruptcies, Boeing Inc. purchased the company at the end of 2000 as part of the Future Imagery Architecture. \nThe Future Imagery Architecture became the most spectacular and expensive failure in the 50-year history of American spy satellite projects. Since the F.I.A. debacle, the National Reconnaissance Office has banned Boeing from bidding on new spy satellite contracts. Autometric's nominally successful products, before they were shelved by Boeing, included Wings Mission Rehearsal, Edge Whole Earth, Spatial Query Server and DataMaster. Due to high turnover, most of the technology made its way to Google and other companies in the form of disgruntled technical talent.\n\nWings and Edge were direct predecessors to Google Earth. Bob Cowling, the Director of Product Engineering at Autometric, was the primary developer of Wings. Wings was a mission rehearsal product draping imagery over terrain. With advances learned from that experience, Bob built a globe which led to the formation of Edge Whole Earth. Edge was used to develop graphics for National Geographic, CBS Evening News, and the motion picture \"Shadow Conspiracy.\"\n\nThe success of Edge led to several smaller spin offs, including Edge Development Option. Many of the engineers working on Edge Development Option, went to work for Keyhole, Inc. helping to create what is now Google Earth.\n"}
{"id": "41610115", "url": "https://en.wikipedia.org/wiki?curid=41610115", "title": "Ayrton shunt", "text": "Ayrton shunt\n\nAyrton shunt or universal shunt is a high-resistance shunt used in galvanometers to increase their range without changing the damping.\nThe circuit is named after its inventor William E. Ayrton. Multirange ammeters that use this technique are more accurate than those using a make-before-break switch. Also it will eliminate the possibility of having a meter without a shunt which is a serious concern in make-before-break switches.\n\nThe selector switch changes the amount of resistance in parallel with R (meter resistance). The voltage drop across parallel branches is always equal. When all resistances are placed in parallel with R maximum sensitivity of ammeter is reached.\n\nAyrton shunt is rarely used for currents above 10 amperes.\n\n"}
{"id": "422948", "url": "https://en.wikipedia.org/wiki?curid=422948", "title": "Barn raising", "text": "Barn raising\n\nA barn raising, also historically called a raising bee or rearing in the U.K., is a collective action of a community, in which a barn for one of the members is built or rebuilt collectively by members of the community. Barn raising was particularly common in 18th- and 19th-century rural North America. A barn was a necessary structure for any farmer, for example for storage of cereals and hay and keeping of animals. Yet a barn was also a large and costly structure, the assembly of which required more labor than a typical family could provide. Barn raising addressed the need by enlisting members of the community, unpaid, to assist in the building of their neighbors' barns. Because each member was entitled to recruit others for help, the favor would eventually return to each participant.\n\nThe tradition of \"barn raising\" continues, more or less unchanged, in some Amish and Old Order Mennonite communities, particularly in Ohio, Indiana, Pennsylvania, and some rural parts of Canada. The practice continues outside of these religious communities, albeit less frequently than in the 19th century. Most frames today are raised using a crane and small crew.\n\nA large amount of preparation is done before the one to two days a barn raising requires. Lumber and hardware are laid in, plans are made, ground is cleared, and tradesmen are hired. Materials are purchased or traded for by the family who will own the barn once it is complete.\n\nGenerally, participation is mandatory for community members. These participants are not paid. All able-bodied members of the community are expected to attend. Failure to attend a barn raising without the best of reasons leads to censure within the community. Some specialists brought in from other communities for direction or joinery may be paid, however.\n\nOne or more people with prior experience or with specific skills are chosen to lead the project. Older people who have participated in many barn raisings are crew chiefs. On the whole, the affair is well organized. At most barn raisings, the community members have raised barns before and approach the task with experience both in the individual tasks and the necessary organization. Young people participating physically for the first time have watched many barn raisings and know what is expected of them.\n\nOnly certain specialists are permitted to work on the more critical jobs, such as the joinery and dowling of the beams. (Post and beam construction is the traditional method of construction in barn raisings.) There is competition for these jobs, and they are sought after. Workers are differentiated by age and gender: men construct the barn, women provide water and food, the youngest children watch, and older boys are assigned to fetch parts and tools.\n\nMost barn raisings were accomplished in June and July when the mostly agrarian society members had time between planting season and harvest season. Timber for the framing was mostly produced in the winter by the farmer and his crew hewing logs to the correct shape with axes or felling the trees and bringing them to a sawmill.\n\nAn ancient tradition is to place a bough, wreath and/or flag at the high point of the frame after the last piece is in place. This celebration is called topping out and historically the master carpenter may also make a speech and a toast.\n\nIn earlier American rural life, communities raised barns because many hands were required. In areas that were sparsely settled or on the edge of the frontier, it was not possible to hire carpenters or other tradesmen to build a barn. The harsher winters gave more urgency to the matter of barn construction than was present in the relatively milder climate in much of Europe. Similar conditions have given rise to similar institutions, such as the Finnish one of 'talkoot'.\n\nBarn raisings occurred in a social framework with a good deal of interdependence. Members of rural communities often shared family bonds going back generations. They traded with each other, buying and selling land, labor, seed, cattle, and the like. They worshipped and celebrated together, because cities were too far away to visit with any frequency by horse and wagon. Despite traditions of independence, self-sufficiency, and refusal to incur debt to one another, community barn raisings were a part of life.\n\nChurches were considered as important to communities of the 18th and 19th centuries as barns. In like fashion, they were often constructed using unpaid community labor. There were important differences. Churches were not constructed with the same degree of urgency, and were most often built of native stone in some regions — a more durable material than the wood of which barns were made, and more time-consuming to lay. Barns, once completed, belonged to an individual family, while churches belonged to the community.\n\nBarn raising as a method of providing construction labor had become rare by the close of the 19th century. By that time, most frontier communities already had barns and those that did not were constructing them using hired labor. Mennonite and Amish communities carried on the tradition, however, and continue to do so to this day.\n\nGroup construction by volunteers enjoyed something of a resurgence during the 1970s, when houses, sheds, and barn-shaped structures were constructed for a variety of purposes. Echoes of the tradition can still be found in other community building projects, such as house building and renovation carried out by Habitat for Humanity.\n\n"}
{"id": "47703639", "url": "https://en.wikipedia.org/wiki?curid=47703639", "title": "BeatBuddy", "text": "BeatBuddy\n\nThe BeatBuddy is a digital effects pedal designed for guitar and other instruments, manufactured by Miami-based Singular Sound. The BeatBuddy is the first guitar pedal drum machine, and provides a drum machine with hands-free control. The pedal uses recordings of non-quantized drums, as recorded in a studio.\n\nThe pedal was first engineered and manufactured in 2014, after crowdfunding via Indiegogo, while the pedal itself has garnered numerous awards from \"Guitar Player\", \"Guitar World\", and NAMM.. The pedal is foot-controlled to start, stop, fill, and transition, with rotary knobs for adjusting volume, tempo and drum set. An additional accessory dual footswitch may be plugged into the BeatBuddy to provide control of accent hits, pause/unpause, as well as tap tempo and hands free content navigation.\n\n"}
{"id": "13837917", "url": "https://en.wikipedia.org/wiki?curid=13837917", "title": "Bose International Planning and Architecture", "text": "Bose International Planning and Architecture\n\nBose International Planning and Architecture is a group of professionals that provide design services to international clients. BI was founded in 1976 by internationally recognized architect Oru Bose AIA in Florida and for 26 years has been providing specialized design consultancy to clients all over the world.\n\nThe organization is owned and managed by two Partners: Oru Bose and Marek Tryzybowicz.\n\n"}
{"id": "330282", "url": "https://en.wikipedia.org/wiki?curid=330282", "title": "Camcorder", "text": "Camcorder\n\nA camcorder is an electronic device originally combining a video camera and a videocassette recorder. \n\nThe earliest camcorders were tape-based, recording analog signals onto videotape cassettes. In 2006, digital recording became the norm, with tape replaced by storage media such as mini-HD, microDVD, internal flash memory and SD cards.\n\nMore recent devices capable of recording video are camera phones and digital cameras primarily intended for still pictures; the term \"camcorder\" may be used to describe a portable, self-contained device, with video capture and recording its primary function, often having advanced functions over more common cameras.\n\n Video cameras originally designed for television broadcast were large and heavy, mounted on special pedestals and wired to remote recorders in separate rooms. As technology improved, out-of-studio video recording was possible with compact video cameras and portable video recorders; a detachable recording unit could be carried to a shooting location. Although the camera itself was compact, the need for a separate recorder made on-location shooting a two-person job. Specialized videocassette recorders were introduced by JVC (VHS) and Sony (U-matic, with Betamax) releasing a model for mobile work. Portable recorders meant that recorded video footage could be aired on the early-evening news, since it was no longer necessary to develop film.\n\nIn 1983, Sony released the first camcorder, the Betacam system, for professional use. A key component was a single camera-recorder unit, eliminating a cable between the camera and recorder and increasing the camera operator's freedom. The Betacam used the same cassette format ( tape) as the Betamax, but with a different, incompatible recording format. It became standard equipment for broadcast news.\n\nSony released the first consumer camcorder in 1983, the Betamovie BMC-100P. It used a Betamax cassette and rested on the operator's shoulder, due to a design not permitting a single-handed grip. That year, JVC released the first VHS-C camcorder. Kodak announced a new camcorder format in 1984, the 8 mm video format. Sony introduced its compact 8 mm Video8 format in 1985. That year, Panasonic, RCA and Hitachi began producing camcorders using a full-size VHS cassette with a three-hour capacity. These shoulder-mount camcorders were used by videophiles, industrial videographers and college TV studios. Full-size Super-VHS (S-VHS) camcorders were released in 1987, providing an inexpensive way to collect news segments or other videographies. Sony upgraded Video8, releasing the Hi8 in competition with S-VHS.\n\nDigital technology emerged with the Sony D1, a device which recorded uncompressed data and required a large amount of bandwidth for its time. In 1992 Ampex introduced DCT, the first digital video format with data compression using the discrete cosine transform algorithm present in most commercial digital video formats. In 1995 Sony, JVC, Panasonic and other video-camera manufacturers launched DV, which became a \"de facto\" standard for home video production, independent filmmaking and citizen journalism. That year, Ikegami introduced Editcam (the first tapeless video recording system).\n\nCamcorders using DVD media were popular at the turn of the 21st century due to the convenience of being able to drop a disc into the family DVD player; however, DVD capability, due to the limitations of the format, is largely limited to consumer-level equipment targeted at people who are not likely to spend any great amount of effort video editing their video footage.\n\nPanasonic launched DVCPRO HD in 2000, expanding the DV codec to support high definition (HD). The format was intended for professional camcorders, and used full-size DVCPRO cassettes. In 2003 Sony, JVC, Canon and Sharp introduced HDV as the first affordable HD video format, due to its use of inexpensive MiniDV cassettes.\n\nSony introduced the XDCAM tapeless video format in 2003, introducing the Professional Disc (PFD). Panasonic followed in 2004 with its P2 solid state memory cards as a recording medium for DVCPRO-HD video. In 2006 Panasonic and Sony introduced AVCHD as an inexpensive, tapeless, high-definition video format. AVCHD camcorders are produced by Sony, Panasonic, Canon, JVC and Hitachi. \n\nIn 2010, after the success of James Cameron's 2009 3D film \"Avatar\", full 1080p HD 3D camcorders entered the market. With the proliferation of file-based digital formats, the relationship between recording media and recording format has declined; video can be recorded onto different media. With tapeless formats, recording media are storage for digital files.\nIn 2011 Panasonic released a camcorder capable of shooting in 3D, the HDC-SDT750. It is a 2D camcorder which can shoot in HD; 3D is achieved by a detachable conversion lens. Sony released a 3D camcorder, the HDR-TD10. The Sony's 3D lens is built in, but it can shoot 2D video. Panasonic has also released 2D camcorders with an optional 3D conversion lens. The HDC-SD90, HDC-SD900, HDC-TM900 and HDC-HS900 are sold as \"3D-ready\": 2D camcorders, with optional 3D capability at a later date.\n\nIn CES (January) 2014, Sony announced the first consumer/low-end professional (\"prosumer\") camcorder Sony FDR-AX100 with a 1\" 20.9MP sensor able to shoot 4K video in 3840x2160 pixels 30fps or 24fps in the XAVC-S format; in standard HD the camcorder can also deliver 60fps. When using the traditional format AVCHD, the camcorder supports 5.1 surround sound from its built-in microphone, this is however not supported in the XAVC-S format. The camera also has a 3-step ND filter switch for maintaining a shallow depth of field or a softer appearance to motion. For one hour video shooting in 4K the camera needs about 32 GB to accommodate a data transfer rate of 50 Mbit/s. The camera's MSRP in the US is USD $2,000.\n\nIn early 2014 Sony released the FDR-AX100 which represents the next generation of camcorders. It is capable of shooting in 4K resolution. It currently has a price tag of £1,699 and 4K camcorders are not expected to come into the mainstream market for at least another eight to ten years as most current Blu-ray players are not capable of playing 4K video. Virtually all mainstream TVs are not 4K ready either with the only 4K TVs available being very expensive at £2,500 or over. The only means of archiving 4K video is the 100 GB Blu-ray Disc XL but the discs are very expensive.\n\nHowever, in 2015, consumer UHD (3840x2160) camcorders below USD $1000 have become available. Sony released the FDRAX33, and Panasonic has released the HC-WX970K and the HC-VX870.\n\nIn September 2014 Panasonic announced and claimed 4K Ultra HD Camcorder HC-X1000E as the first conventional camcorder design that can capture up to 60fps at 150 Mbit/s or alternatively standard HD recording at up to 200 Mbit/s in ALL-I mode with MP4, MOV and AVCHD formats all offered depending on the resolution and frame rate. With use 1/2.3\" small sensor as commonly is used by bridge cameras, the camcorder has 20x optical zoom in a compact body with dual XLR audio inputs, Internal ND filters and separate control rings for focus, iris and zoom. In HD capture, the camcorder get benefit to reduce noises of small sensor by in-camera downscaling of the 4K image to HD.\n\nAs of January 2017, the only major manufacturer to announce new consumer camcorders at CES (Consumer Electronic Show) in Las Vegas was Canon with its entry-level HD models. Panasonic only announced details regarding their Mirrorless Micro Four Thirds Digital Camera called the LUMIX GH5, capable of shooting 4K in 60p. This is the first time in decades that Panasonic & Sony haven't announced new traditional camcorders at CES, & instead carried over 2016's models, such as Sony's FDR-AX53. This is due to there being far less demand in the market for traditional camcorders as more & more consumers prefer to record video with their 4K-capable smartphones, DSLRs, and action cameras from GoPro, Xiaomi, Sony, Nikon, and many others.\n\nCamcorders have three major components: lens, imager and recorder. The lens gathers light, focusing it on the imager. The imager (usually a CCD or CMOS sensor; earlier models used vidicon tubes) converts incident light into an electrical signal. The recorder converts the electrical signal to video, encoding it in a storable form. The lens and imager comprise the \"camera\" section.\n\nThe lens is the first component of the light path. Camcorder optics generally have one or more of the following controls:\n\nIn consumer units these adjustments are often automatically controlled by the camcorder, but can be adjusted manually if desired. Professional-grade units offer user control of all major optical functions.\n\nThe imager converts light into an electrical signal. The camera lens projects an image onto the imager surface, exposing the photosensitive array to light. This light exposure is converted into an electrical charge. At the end of the timed exposure, the imager converts the accumulated charge into a continuous analog voltage at the imager's output terminals. After the conversion is complete, the photosites reset to start the exposure of the next video frame.\n\nThe recorder writes the video signal onto a recording medium, such as magnetic videotape. Since the record function involves many signal-processing steps, some distortion and noise historically appeared on the stored video; playback of the stored signal did not have the exact characteristics and detail as a live video feed. All camcorders have a recorder-controlling section, allowing the user to switch the recorder into playback mode for reviewing recorded footage, and an image-control section controlling exposure, focus and color balance.\n\nThe image recorded need not be limited to what appeared in the viewfinder. For documenting events (as in law enforcement), the field of view overlays the time and date of the recording along the top and bottom of the image. The police car or constable badge number to which the recorder was given, the car's speed at the time of recording, compass direction and geographical coordinates may also be seen.\n\nCamcorders are often classified by their storage device; VHS, VHS-C, Betamax, Video8 are examples of late 20th century videotape-based camcorders which record video in analog form. Digital video camcorder formats include Digital8, MiniDV, DVD, hard disk drive, direct to disk recording and solid-state, semiconductor flash memory. While all these formats record video in digital form, Digital8, MiniDV, DVD and hard-disk drives have no longer been manufactured in consumer camcorders since 2006.\n\nIn the earliest analog camcorders the imaging device is vacuum-tube technology, in which the charge of a light-sensitive target was directly proportional to the amount of light striking it; the Vidicon is an example of such an imaging tube. Newer analog, and digital camcorders use a solid-state charge-coupled imaging device (CCD) or a CMOS imager. Both are analog detectors, using photodiodes to pass a current proportional to the light striking them. The current is then digitised before being electronically scanned and fed to the imager's output. The main difference between the two devices is how the scanning is done. In the CCD the diodes are sampled simultaneously, and the scan passes the digitised data from one register to the next. In CMOS devices, the diodes are sampled directly by the scanning logic.\n\nDigital video storage retains higher-quality video than analog storage, especially on the prosumer and strictly consumer levels. MiniDV storage allows full-resolution video (720x576 for PAL, 720x480 for NTSC), unlike analog consumer-video standards. Digital video does not experience colour bleeding, jitter, or fade.\n\nUnlike analog formats, digital formats do not experience generation loss during dubbing; however, they are more prone to complete loss. Although digital information can theoretically be stored indefinitely without deterioration, some digital formats (like MiniDV) place tracks only about 10 micrometers apart (compared with 19–58 μm for VHS). A digital recording is more vulnerable to wrinkles or stretches in the tape which could erase data, but tracking and error-correction code on the tape compensates for most defects. On analog media, similar damage registers as \"noise\" in the video, leaving a deteriorated (but watchable) video. DVDs may develop DVD rot, losing large chunks of data. An analog recording may be \"usable\" after its storage media deteriorates severely, but slight media degradation in digital recordings may trigger an \"all or nothing\" failure; the digital recording will be unplayable without extensive restoration.\n\nOlder digital camcorders record video onto tape digitally, microdrives, hard drives, and small DVD-RAM or DVD-Rs. Newer machines since 2006 record video onto flash memory devices and internal solid-state drives in MPEG-1, MPEG-2 or MPEG-4 format. Because these codecs use inter-frame compression, frame-specific editing requires frame regeneration, additional processing and may lose picture information. Codecs storing each frame individually, easing frame-specific scene editing, are common in professional use.\n\nOther digital consumer camcorders record in DV or HDV format on tape, transferring content over FireWire or USB 2.0 to a computer where large files (for DV, 1GB for 4 to 4.6 minutes in PAL/NTSC resolutions) can be edited, converted and recorded back to tape. The transfer is done in real time, so the transfer of a 60-minute tape requires one hour to transfer and about 13GB of disk space for the raw footage (plus space for rendered files and other media).\n\nA tapeless camcorder is a camcorder that does not use video tape for the digital recording of video productions as 20th century ones did. Tapeless camcorders record video as digital computer files onto data storage devices such as optical discs, hard disk drives and solid-state flash memory cards.\n\nInexpensive pocket video cameras use flash memory cards, while some more expensive camcorders use solid-state drives or SSD; similar flash technology is used on semi-pro and high-end professional video cameras for ultrafast transfer of high-definition television (HDTV) content.\n\nMost consumer-level tapeless camcorders use MPEG-2, MPEG-4 or its derivatives as video coding formats. They are normally capable of still-image capture to JPEG format additionally.\n\nConsumer-grade tapeless camcorders include a USB port to transfer video onto a computer. Professional models include other options like Serial digital interface (SDI) or HDMI. Some tapeless camcorders are equipped with a Firewire (IEEE-1394) port to ensure compatibility with magnetic tape-based DV and HDV formats.\n\nSince the consumer market favors ease of use, portability and price, most consumer-grade camcorders emphasize handling and automation over audio and video performance. Most devices with camcorder capability are camera phones or compact digital cameras, in which video is a secondary capability. Some pocket cameras, mobile phones and camcorders are shock-, dust- and waterproof.\n\nThis market has followed an evolutionary path driven by miniaturization and cost reduction enabled by progress in design and manufacture. Miniaturization reduces the imager's ability to gather light; designers have balanced improvements in sensor sensitivity with size reduction, shrinking the camera imager and optics while maintaining relatively noise-free video in daylight. Indoor or dim-light shooting is generally noisy, and in such conditions artificial lighting is recommended. Mechanical controls cannot shrink below a certain size, and manual camera operation has given way to camera-controlled automation for every shooting parameter (including focus, aperture, shutter speed and color balance). The few models with manual override are menu-driven. Outputs include USB 2.0, Composite and S-Video and IEEE 1394/Firewire (for MiniDV models).\n\nThe high end of the consumer market emphasizes user control and advanced shooting modes. More-expensive consumer camcorders offer manual exposure control, HDMI output and external audio input, progressive-scan frame rates (24fps, 25fps, 30fps) and higher-quality lenses than basic models. To maximize low-light capability, color reproduction and frame resolution, multi-CCD/CMOS camcorders mimic the 3-element imager design of professional equipment. Field tests have shown that most consumer camcorders (regardless of price) produce noisy video in low light.\n\nBefore the 21st century, video editing required two recorders and a desktop video workstation to control them. A typical home personal computer can hold several hours of standard-definition video, and is fast enough to edit footage without additional upgrades. Most consumer camcorders are sold with basic video editing software, so users can create their own DVDs or share edited footage online.\n\nSince 2006, nearly all camcorders sold are digital. Tape-based (MiniDV/HDV) camcorders are no longer popular, since tapeless models (with an SD card or internal SSD) cost almost the same but offer greater convenience; video captured on an SD card can be transferred to a computer faster than digital tape. None of the consumer-class camcorders announced at the 2006 International Consumer Electronics Show recorded on tape.\n\nVideo-capture capability is not confined to camcorders. Cellphones, digital single-lens reflex and compact digicams, laptops and personal media players offer video-capture capability, but most multipurpose devices offer less video-capture functionality than an equivalent camcorder. Most lack manual adjustments, audio input, autofocus and zoom. Few capture in standard TV-video formats (480p60, 720p60, 1080i30), recording in either non-TV resolutions (320x240, 640x480) or slower frame rates (15 or 30 fps).\n\nA multipurpose device used as a camcorder offers inferior handling, audio and video performance, which limits its utility for extended or adverse shooting situations. The camera phone developed video capability during the early 21st century, reducing sales of low-end camcorders.\n\nDSLR cameras with high-definition video were also introduced early in the 21st century. Although they still have the handling and usability deficiencies of other multipurpose devices, HDSLR video offers the shallow depth-of-field and interchangeable lenses lacking in consumer camcorders. Professional video cameras with these capabilities are more expensive than the most expensive video-capable DSLR. In video applications where the DSLR's operational deficiencies can be mitigated, DSLRs such as the Canon 5D Mark II provide depth-of-field and optical-perspective control.\n\nCombo-cameras combine full-feature still cameras and camcorders in a single unit. The Sanyo Xacti HD1 was the first such unit, combining the features of a 5.1 megapixel still camera with a 720p video recorder with improved handling and utility. Canon and Sony have introduced camcorders with still-photo performance approaching that of a digicam, and Panasonic has introduced a DSLR body with video features approaching that of a camcorder. Hitachi has introduced the DZHV 584E/EW, with 1080p resolution and a touch screen.\n\nThe Flip Video was a series of tapeless camcorders introduced by Pure Digital Technologies in 2006. Slightly larger than a smartphone, the Flip Video was a basic camcorder with record, zoom, playback and browse buttons and a USB jack for uploading video. The original models recorded at a 640x480-pixel resolution; later models featured HD recording at 1280x720 pixels. The Mino was a smaller Flip Video, with the same features as the standard model. The Mino was the smallest of all camcorders, slightly wider than a MiniDV cassette and smaller than most smartphones on the market. In fact the Mino was small enough to fit inside the shell of a VHS cassette. Later HD models featured larger screens. In 2011, the Flip Video (more recently manufactured by Cisco) was discontinued.\n\nInterchangeable-lens camcorders can capture HD video with DSLR lenses and an adapter.\n\nIn 2011, Sony launched its HDR-PJ range of HD camcorders: the HDR-PJ10, 30 and 50. Known as Handycams, they were the first camcorders to incorporate a small image projector on the side of the unit. This feature allows a group of viewers to watch video without a television, a full-size projector or a computer. These camcorders were a huge success and Sony subsequently released further models in this range. Sony's current 2014 line up comprises the HDR-PJ240, HDR-PJ330 (entry level models), HDR-PJ530 (mid-range model) and the HDR-PJ810 (top of the range). Specifications vary by model.\n\nCamcorders are used by nearly all electronic media, from electronic-news organizations to current-affairs TV productions. In remote locations, camcorders are useful for initial video acquisition; the video is subsequently transmitted electronically to a studio or production center for broadcast. Scheduled events (such as press conferences), where a video infrastructure is readily available or can be deployed in advance, are still covered by studio-type video cameras \"tethered\" to production trucks.\n\nCamcorders often cover weddings, birthdays, graduations, children's growth and other personal events. The rise of the consumer camcorder during the mid- to late 1980s led to the creation of TV shows such as \"America's Funniest Home Videos\", which showcases homemade video footage.\n\nPolitical protesters use camcorders to film what they believe unjust. Animal rights protesters who break into factory farms and animal testing labs use camcorders to film the conditions in which the animals are living. Anti-hunting protesters film fox hunts. People investigating political crimes use surveillance cameras for evidence-gathering. Activist videos often appear on Indymedia.\n\nPolice use camcorders to film riots, protests and crowds at sporting events. The film can be used to spot troublemakers, who can then be prosecuted. In countries such as the United States, the use of compact dashcams in police cars allows the police to retain a record of activity in front of the car (such as interaction with a stopped motorist).\n\nCamcorders are used in the production of low-budget TV shows if the production crew does not have access to more expensive equipment. Movies have been shot entirely on consumer camcorder equipment (such as \"The Blair Witch Project\", \"28 Days Later\" and \"Paranormal Activity\"). Academic filmmaking programs have also switched from 16mm film to digital video in early 2010s, due to the reduced expense and ease of editing of digital media and the increasing scarcity of film stock and equipment. Some camcorder manufacturers cater to this market; Canon and Panasonic support 24p (24 fps, progressive scan—the same frame rate as cinema film) video in some high-end models for easy film conversion.\n\nSchools in the developed world increasingly use digital media and digital education. Students use camcorders to record video diaries, make short films and develop multi-media projects across subject boundaries. Teacher evaluation involves a teacher's classroom lessons being recorded for review by officials, especially for questions of teacher tenure.\n\nStudent camcorder-created material and other digital technology are used in new-teacher preparation courses. The University of Oxford Department of Education PGCE programme and NYU's Steinhardt School's Department of Teaching and Learning MAT programme are examples.\n\nThe USC Rossier School of Education goes further, insisting that all students purchase their own camcorder (or similar) as a prerequisite to their MAT education programs (many of which are delivered online). These programs employ a modified version of Adobe Connect to deliver the courses. Recordings of MAT student work are posted on USC's web portal for evaluation by faculty as if they were present in class. Camcorders have allowed USC to decentralize its teacher preparation from Southern California to most American states and abroad; this has increased the number of teachers it can train.\n\nThe following list covers consumer equipment only (for other formats, see videotape):\n\n\nSince most manufacturers focus their support on Windows and Mac users, users of other operating systems have difficulty finding support for their devices. However, open-source products such as Kdenlive, Cinelerra and Kino (written for the Linux operating system) allow editing of most popular digital formats on alternative operating systems and can be used in conjunction with OBS for online broadcast solutions; software to edit DV streams is available on most platforms.\n\nThe issue of digital-camcorder forensics to recover data (e.g. video files with timestamps) has been addressed.\n\n\n"}
{"id": "10321734", "url": "https://en.wikipedia.org/wiki?curid=10321734", "title": "Canadian Telework Association", "text": "Canadian Telework Association\n\nThe Canadian Telework Association (CTA) is an organization promoting telework and telecommuting in Canada. It was founded in 1997, and since then, it has grown to include over 1000 members, most of which are individuals, corporations, and academic institutions. The association does not accept funding or donations and does not charge fees for membership.\n\nThe founder of this association has since retired and the websites have been closed.\n\n\n"}
{"id": "4247774", "url": "https://en.wikipedia.org/wiki?curid=4247774", "title": "Clifton nanolitre osmometer", "text": "Clifton nanolitre osmometer\n\nCooling of the stage is achieved with the use of Peltier devices inside the cooling stage.\n\nThe Clifton nanolitre osmometer is especially well suited for determining the antifreeze activity or thermal hysteresis of a solution, which is a difference in the melting and freezing point of a solution. This phenomenon arises when biological antifreeze proteins are present in a solution. Solutions that do not contain antifreeze proteins generally have identical freezing and melting points.\n\nA melting and freezing point determination consists of the following steps:\n\n"}
{"id": "38374753", "url": "https://en.wikipedia.org/wiki?curid=38374753", "title": "Comité Européen des groupements de constructeurs du machinisme agricole", "text": "Comité Européen des groupements de constructeurs du machinisme agricole\n\nComité Européen des groupements de constructeurs du machinisme agricole (CEMA aisbl) is the European association representing the agricultural machinery industry in Europe.\n\nIt was established during the first General Assembly in London on 10 July 1959. The secretariat was then based in Paris and stayed there until it moved to Brussels in 2006. This was mainly done to be closer to European decision-making. In Brussels it has currently an office near the European Quartier. The organisation is listed in the Transparency Register of the European Commission.\n\nThe main activities of CEMA consist of providing advice and services to the national member associations on various relevant topics for the agricultural machinery industry. Examples of those topics are tractor type approval, harmonisation of road requirements and engine emissions. Furthermore, it provides a common European industry view on those topics. The interests of the agricultural machinery industry are defended and promoted towards the European Institutions.\nSeveral events are organised by CEMA. E.g. every other year CEMA organises a two-day summit in Brussels in corporation with CECE, the Committee for European Construction Equipment. The last Summit in 2013 was held under the theme: \"Towards a competitive industrial production for Europe\".\n\nCEMA is a network of national organisations that come together every year with industry at the General Assembly. Every other year, the CEMA Board is elected and that Board chooses a Technical Board. Specific technical topics are dealt with in Project Teams (PTs) consisting of representatives of national associations and industry. The coordination and contact with other organisations is mainly taken care of by the Secretariat in Brussels. The current president of CEMA is Richard Markwell.\n\n\n"}
{"id": "20957023", "url": "https://en.wikipedia.org/wiki?curid=20957023", "title": "Cottonelle", "text": "Cottonelle\n\nCottonelle is a brand of toilet paper produced by Kimberly-Clark. The company has made several different toilet paper types such as regular, Cottonelle Double, (Two-ply) Cottonelle Ultra, Cottonelle Aloe & E, Cottonelle Kids, and Cottonelle Extra Strength, and are currently sold in the United States and Australia under the Kleenex brand.\n\nCottonelle's mascot was originally a woman. At the time, the commercials usually consisted of how soft the roll was by showing a cotton ball and comparing it to the product itself.\n\nIn early 2008, Cottonelle devised an extensive advertising campaign featuring a large \"Comfort Haven Bus\" decorated to resemble a dog. According to Ad Rants, the bus would travel cross-country to \"offer visitors access to \"relaxation stations\" where people can see first-hand—and hopefully in privacy—how soft and comforting Cottonelle can be.\"\n\nAt one point, Cottonelle featured a program called Puppy Points. On a package of Cottonelle toilet paper would be a label with a certain amount of points. The label had to be cut off and saved. The Cottonelle website, showed a list of Cottonelle related items that you could receive in exchange for puppy points, including a Cottonelle bath robe, a Cottonelle picture frame, hand bag, slippers, etc. On July 31, 2008, puppy points were discontinued and could no longer be redeemed for merchandise.\n\nCottonelle is marketed as Andrex in the UK.\n\nCottonelle was introduced in Canada shortly after its 1972 introduction in the United States. Due to corporate changes in Canada, during 2004 Scott Paper Limited (which became Kruger Products) transitioned the highly recognisable Canadian branding to the new name of Cashmere.\n\n"}
{"id": "41406522", "url": "https://en.wikipedia.org/wiki?curid=41406522", "title": "Did You Know Gaming?", "text": "Did You Know Gaming?\n\nDid You Know Gaming? (abbreviated DYKG) is a video game–focused blog which launched in May 2012. The site features video content focusing on video game related trivia and facts, hosted on video game entertainment website NormalBoots.com, covering various franchises.\n\nSince the website's launch, it has been featured on numerous major news and gaming outlets including \"Huffington Post\", MCV, \"Game Informer\", MTV, Nerdist, and the \"Houston Press\".\n\nIn 2017, \"Did You Know Gaming?\" opened a second channel, called DidYouKnowGaming? 2.\n\nThe site launched on May 14, 2012, by Shane Gill who came up with the idea for a trivia focused website based on gaming after being inspired by a number of Facebook trivia groups. By July 2012, the official Facebook page had reached nearly 20,000 fans in under eight weeks. As of November 2018, the official YouTube channel has currently over 2,200,000 subscribers and over 440 million views.\n\nOn January 25, 2014, Did You Know Gaming partnered with a relaunched \"Normal Boots\", a collaborative website for hosting gaming themed content created by Jon Jafari and Austin Hargrave. A spin-off series' has also been created on the channel \"The Film Theorists\" called \"Did You Know Movies\".\n\nOn July 7, 2017, Did You Know Gaming? announced a new channel, featuring content created by Dazz, creator of The VG Resource websites and the Region Locked series, alongside Greg who also works on Region Locked. The channel features a more off-the-cuff style primarily focused around providing additional content to compliment the main channel. The channel hosted its own version of two of the channels main shows, \"Did You Know Gaming? Extra\" and \"Region Locked Light\". As of November 2017, the second channel has over 160,000 subscribers and more than 7.3 million views.\n\nOn November 21, 2017, they announced that Region Locked Light would be cancelled and that Did You Know Gaming? Extra would be moved to the main channel. They, as of the date, had almost no plans for the channel. One of their ideas was using the channel to promote smaller trivia and gaming channels.\n\nThe site releases videos presenting trivia based on different franchises. Series which have been covered are \"Star Fox\", \"Pikmin\", \"Super Smash Bros.\" \"Metroid\" and many more. One specific series called \"Easter Egg Hunting\" looks at secrets and Easter Eggs found in games based on a particular show or in a specific game such as \"South Park\", \"Doctor Who\" and \"Metal Gear Solid\". Each video is narrated by a number of popular internet personalities including Jon Jafari of \"JonTron\", Arin Hanson, and Smooth McGroove.\n\nVGFacts is a sister website of \"Did You Know Gaming?\" which launched in March 2013 and also features gaming related trivia. Created in partnership with \"The Spriters Resource\", the site features trivia covering thousands of games, series and consoles as well as articles discussing various topics, which even includes contributions from game publisher Konami.\n\nEach episode covers a specific franchise or game and is often narrated by a popular internet personality. There is the main \"Did You Know Gaming?\" series and other spin-offs such as \"VGFacts\" and \"Region Locked\".\n\nDid You Know Gaming? Extra (first on their second channel), usually talks about a specific theme (such as censorship or love in video games) and a random piece of trivia in the end. On November 21, 2017, they announced that the series was moving to their main channel.\nIn Region Locked, they talk about games that didn't get an international release or didn't release in all regions.\nRegion Locked Light was originally a spin-off of Region Locked. On November 21, 2017, they announced that Region Locked Light would be cancelled to focus on more, bigger Region Locked episodes.\nThe site has received generally positive reception from critics.\n\nCEO of Destructoid, Hamza Aziz, has praised the site saying \"The Did You Know Gaming series is a pretty wonderful look at the lesser known facts of your favorite videogames.\"\n\nSteve Napierski, author of the webcomic series \"Dueling Analogs\", has posted about \"Did You Know Gaming?\" multiple times saying that \"Not sure if you’ve noticed from the amount of Did You Know Gaming? videos I have reposted on Dueling Analogs, but I am definitely a fan of them.\"\n\n"}
{"id": "24644328", "url": "https://en.wikipedia.org/wiki?curid=24644328", "title": "Engineering, Science, and Management War Training", "text": "Engineering, Science, and Management War Training\n\nThe Engineering, Science, and Management War Training program (ESMWT) was one of the largest and most productive educational activities in America's history. It was perhaps only second to the G.I. Bill (officially the Servicemen's Readjustment Act of 1944) in its scope and productivity.\n\nSometimes referred to as an \"experiment in streamlined higher education\", this government-sponsored program provided, without charge, college-grade courses for large numbers of Americans to fill urgently needed technical and scientific civilian positions just prior to and during World War II. College-grade was officially defined as \"work of an academic standard customarily demanded of engineering-school students.\"\n\nWith successive designations of Engineering Defense Training (EDT), Engineering, Science, and Management Defense Training (ESMDT), and ESMWT, the program was operated by the U.S. Office of Education from October 1940 through June 1945, with 227 colleges and universities providing about 68,000 courses for close to 1,800,000 students at a total cost of some $60 million ($940 million today's dollars).\n\nIn mid-1940, before America officially entered World War II, it was brought to the attention of U.S. Congress that the civilian effort supporting the expected conflict would require far more engineers than were then available or could be produced through normal programs at colleges and universities. As part of the budget preparation, Congress tasked John W. Studebaker, then U.S. Commissioner of Education and head of the Office of Education (predecessor of the United States Department of Education), to develop a program to help alleviate this crisis.\n\nStudebaker asked Andrey A. Potter, Dean of Engineering at Purdue University, to assist in developing this program and in preparing a proposal to Congress. A National Advisory Committee – composed of academic leaders and industry officials – first met on September 20–21, 1940. On October 9, a bill authoring the Engineering Defense Training (EDT) program with initial funding of $9 million (equivalent to $138 million in 2008) was passed by Congress and quickly signed by President Franklin D. Roosevelt.\n\nThe bill authorized the Office of Education to contract with engineering schools throughout the U.S. to offer \"intensive courses of college grade, designed to meet the shortage of engineers in activities essential to national defense.\" Roy A. Seaton, Dean of Engineering at Kansas State College, was appointed to direct the program. Twenty-two regional advisors, each a prominent engineering educator, served without pay as coordinators. \n\nCourses under the EDT program began on December 9, 1940. During the remainder of the Fiscal Year ending June 30, 1941, about 120,800 individuals enrolled in 2,260 courses offered by 144 institutions. \n\nIn July 1941, the Labor-Federal Security Appropriations Act, authorized the addition of chemistry, physics, and production-related courses to form an integrated ESMDT program. Funding was increased to $17.5 million ($270 million equivalent in 2008), and Dean George W. Case from the University of New Hampshire, replaced Seaton as the program director. During the next year, enrollment increased to nearly 450,000 men and women in about 7,800 courses offered by 196 colleges and universities. \n\nAfter the Japanese attack on Pearl Harbor (December 7, 1941) and the entry of the U.S. into World War II, the War-Time Commission was formed within the Department of Education to be responsible for this and other special training activities. The program was again renamed, becoming ESMWT, and continued as such through June 1945. The annual enrollment in ESMWT averaged nearly 600,000 in some 13,000 courses. Ultimately, 215 colleges and universities participated, with the offerings given in more than 1,000 towns and cities.\n\nThe primary objective of the program was to prepare persons for professional positions in defense activities. The authorization required the courses to be of \"an academic standard customarily demanded of engineering-school students\", distinguishing them from offerings of trade or technical schools. They were limited to engineering, physics, chemistry, and management subjects directly needed in defense activities\n\nIt is noted that many of the ESMWT courses, although of \"college grade\" at that time, might not be recognized as such in engineering and science programs today. At most colleges and universities in the early 1940s, engineering education greatly emphasized practical applications as opposed to theoretical analysis. It was somewhat the same in many undergraduate physics and chemistry programs.\n\nAlthough there might be progressively higher-level courses in a subject, each ESMWT course was to be complete in itself and designed to convey some stated, applicable knowledge set. While prerequisites were listed for courses, these were mainly to indicate the background needed to understand the subject matter, and the pursuit of a course was not limited to persons having these prerequisites.\n\nThe participating colleges and universities were fully responsible for determining the local needs and developing courses to meet these needs. The relationship of these courses to the regular courses in a degree program was also under the responsibility of the offering institution.\n\nRegular faculty members taught most of the courses. Although requiring considerable preparation, this was welcome work for most professors; between the draft and volunteering, college enrollment was a small fraction of normal. Some courses required instructors to have a highly specialized knowledge, and practitioners from industry were then used.\n\nThe matter of granting academic course credit for the special courses was an issue from the beginning. The Office of Education recommended against academic credit, reasioning that since the courses were paid by the government, this might \"constitute a Federal subsidy to the participating college\", a forbidden practice in that day. It was recommended, however, that knowledge of a subject gained through these courses be recognized, by examination or otherwise, for students in regular college programs, allowing them to excused from that subject, with or without credit. \n\nThe Engineers’ Council for Professional Development (ECPD), then the accrediting body for engineering programs and predecessor of the Accreditation Board for Engineering and Technology (ABET), also had reservations concerning academic credit. Following much debate, it was left to each school to make any credit-related decisions and, in the end, most decided against directly granting academic credit, primarily citing the imbalance of applications over theory in most ESMWT courses.\n\nAfter being prepared by the offering institution, course outlines, schedules, and other information were submitted to the ESMWT Washington staff for approval, and the cognizant school was required to permanently maintain the same type of records on presented courses as those kept in regular academic programs.\n\nBy law, admission to ESMWT courses was open to students without regard to age, sex, or race. With this, and without enforcement of prerequisites, there was a great diversity of students in the various offerings. It was not uncommon to find students in their early teens using the ESMWT program to accelerate their entry into engineering and science jobs. On the other end of the age spectrum, many of the students were in mid-life, preparing for a change of work field or refreshing their earlier education.\n\nPerhaps the greatest diversity was in the number of female students. Prior to the 1940s, there were few women working in industry science and even less in engineering. The entry of women into defense jobs is well known through \"Rosie the Riveter\", but those in wartime engineering and related work are not often recognized; however, thousands entered professional work after being prepared through this program.\n\nUnder the initial EDT, women constituted less than 1 percent. With the addition of science and management courses, there were about 9 percent under ESMDT. After the start of the war and ESMWT, the numbers participating in the program increased to 22 percent. Many engineering schools that had earlier been all-male, first accepted women when they began the program. Conversely, a number of previously female colleges – including Bryn Mawr College, Skidmore College, and Wellesley College – participated in the program and had their first male students. (Some of these schools later reverted to all-female).\n\nThroughout the program, about 75 percent of the enrollments were in engineering subjects. After a series of courses in a particular subject area, many of the students, especially those having an earlier background in science and/or mathematics, were placed in regular engineering positions with industry or government agencies. These were sometimes referred to as \"instant engineers.\" A 1950 survey of practicing engineers found that there were many more than the sum of those in 1940 plus those graduating from engineering colleges in the 1940-50 decade; much of this difference was attributed to persons who gained this classification through the ESMWT program.\n\nMany schools considered graduation from ESMWT courses to have great significance. As an example, at North Carolina State University the Chancellor often personally awarded certificates to the graduates. \nThe Office of Education requirements were that students of any race would be admitted to ESMDT courses. Unfortunately, African-Americans were not well represented either in the participating institutions or as students. Howard University did have a relatively large program for the District of Columbia area; this was led by Herman R. Branson of the Physics Department who had a distinguished post-war career in academia.\n\nFrom June 23 to August 29, 1941, electrical engineering student J. Presper Eckert assisted in teaching a 10-week ESMDT course in electronics at the University of Pennsylvania's Moore School of Electrical Engineering. The course had 30 students, 16 of whom held Ph.D. degrees. Among the students was John W. Mauchly. Eckert and Mauchly then teamed in developing the Electronic Numerical Integrator And Computer (ENIAC), America's first large-scale digital computer.\n\nCourses in industrial chemistry and metallurgy were in great demand. Harry J. Sweeney, then Chief Metallurgist for the giant Republic Steel, stated, \"I don’t know what we would have done without the ESMDT courses; about 75 percent of our new professionals were trained through this program.\"\n\nAircraft played a vital role in World War II. During 1940, Cornell University started courses in aircraft structures and stress analysis at Buffalo, New York, for Bell Aircraft and Curtis Wright. One of these, starting December 9, was the first EDT-sponsored course in the U.S. By the end of the first year, more than 800 students were attending these classes.\n\nIn 1941, the University of Texas started a series of ESMDT courses in aeronautical engineering. These were so successful that the next year a full Department of Aeronautical Engineering was formed. For the aircraft industries of Los Angeles, Maurice J. Zucrow of University of California, Los Angeles, taught ESMWT courses on jet propulsion and gas turbines, introducing hundreds of aeronautical engineers to this critical technology of post-war propulsion.\n\nAcross the nation, some of the most attended courses were in electronics and radio communications. Massachusetts Institute of Technology, Harvard University, Tufts College, Northeastern University, Boston University, and Boston College joined forces to offer the ESMDT/ESMWT program; Dean Edward L. Moreland of MIT served as the coordinator. Much of it was in support of the Radiation Laboratory at MIT where microwave radar was being developed and the electronics firms in the northeast area.\n\nRutgers University was one of the few participants that included labor-management courses in its ESMWT offerings. These brought workers to the Rutgers campus in unprecedented numbers. The coordinator, Norman C. Miller stated, \"The spirit of patriotism, sacrifice, and co-operation that imbued both labor and management in service to the war effort was clearly evident in these courses.\" This effort carried into the post-war era with the establishment of the School of Industrial and Labor Relations at Rutgers,\n\n"}
{"id": "37765984", "url": "https://en.wikipedia.org/wiki?curid=37765984", "title": "Flattop (critical assembly)", "text": "Flattop (critical assembly)\n\nFlattop is a benchmark critical assembly that is used to study the nuclear characteristics of uranium-233, uranium-235, and plutonium-239 in spherical geometries surrounded by a relatively thick natural uranium reflector.\n\nFlattop assemblies are used to measure neutron activation and reactivity coefficients. Since the neutron energies gradually decrease in the reflector, experiments may be run in various energy spectra based on the location in which they are placed.\n\nFlattop is a natural-uranium-reflected, benchmarked, fixed-geometry critical assembly machine that can accommodate plutonium or uranium cores. The fast neutron spectrum is used to provide benchmarked neutronic measurements in spherical geometry with different fissile driver materials. Key missions for Flattop include fundamental reactor physics studies, sample irradiation for radiochemical research, actinide minimum critical mass studies, detector calibration, and training. The U-233 core is no longer usable because of its high gamma-ray activity.\n\nThe experiment was originally located at the Los Alamos National Laboratory Critical Experiments Facility (LACEF) located at the Los Alamos Pajarito Site, otherwise known as Technical Area 18. In 2005 the Pajarito Site started to shut down and nuclear material was moved to the National Criticality Experiments Research Center (NCERC) which is located at the Nevada National Security Site. However, NCERC continues to be operated by the Los Alamos National Laboratory. The core capabilities at NCERC include Flattop along with three other critical assemblies, Comet, Planet, and Godiva-IV and a significant inventory of nuclear material items available for experimental use. NCERC critical operations commenced in 2011 and continue to be operational today. \n\nIn 2012, Flattop was used for key demonstration of the use of nuclear power for space applications. The Demonstration Using Flattop Fission, or DUFF, test was planned by Los Alamos National Laboratory to use Flattop as a nuclear heat source. A team from the NASA Glenn Research Center in partnership with the LANL reactor design team designed, built, and tested a heat pipe and power conversion system to couple to Flattop with the end goal of demonstrating electrical power production using technology applicable to space application.\n\nFlattop consists of a hemispherical fixed reflector and two movable quarter-spheres of reflector that can close down on the central core. One movable reflector is controlled by hydraulic pressure, while the other is actuated by a motor.\n"}
{"id": "4870833", "url": "https://en.wikipedia.org/wiki?curid=4870833", "title": "Gnits standards", "text": "Gnits standards\n\nThe Gnits standards are a collection of standards and recommendations for programming, maintaining, and distributing software. They are published by a group of GNU project maintainers who call themselves \"Gnits\", which is short for \"GNU nit-pickers\". As such, they represent advice, not Free Software Foundation or GNU policy, but parts of the Gnits' standards have seen widespread adoption among free software programmers in general.\n\nThe Gnits standards are extensions to, refinements of, and annotations for the GNU Standards. However, they are in no way normative in GNU; GNU maintainers are not required to follow them. Nevertheless, maintainers and programmers often find in Gnits standards good ideas on the way to follow GNU Standards themselves, as well as tentative, non-official explanations about why some GNU standards were decided the way they are. There are very few discrepancies between Gnits and GNU standards, and they are always well noted as such.\n\nThe standards address aspects of software architecture, program behaviour, human–computer interaction, C programming, documentation, and software releases.\n\nAs of 2008, the Gnits standards carry a notice that they are moribund and no longer actively maintained, and points readers to the manuals of Gnulib, Autoconf, and Automake, which are said to cover many of the same topics.\n\n\n"}
{"id": "34644256", "url": "https://en.wikipedia.org/wiki?curid=34644256", "title": "Grip (percussion)", "text": "Grip (percussion)\n\nIn percussion, grip refers to the manner in which the player holds the percussion mallet or mallets, whether drum sticks or other mallets.\n\nFor some instruments, such as triangles and large gongs, only one mallet or beater is normally used, held either in one hand, or in both hands for larger beaters; For others such as snare drums often two beaters are used, one in each hand. More rarely, more than one beater may be held in one hand, for example when four mallets are used on a vibraphone, or when a kit drummer performs a cymbal roll by holding two soft sticks in one hand while keeping a rhythm with the other.\n\nWhen two identical beaters are used, one in each hand, there are two main varieties of grip:\n\nTraditional grip was developed to conveniently play a snare drum while marching, and was documented and popularised by Sanford A. Moeller in \"The Art of Snare Drumming\" (1925). It was the standard grip for kit drummers in the first half of the twentieth century and remains popular, and the standard grip for most snare drummers.\n\nMatched grip is used for most percussion instruments when two beaters are used.\n\nThere are three main varieties, distinguished by the means of moving the beaters and the angle of the palms to facilitate this action:\n\nSingle-beater grips are common for:\n\nUnmatched grips are common for:\n\nMatched grips are common for:\n\n"}
{"id": "2627964", "url": "https://en.wikipedia.org/wiki?curid=2627964", "title": "History of the Actor model", "text": "History of the Actor model\n\nIn computer science, the Actor model, first published in 1973, is a mathematical model of concurrent computation.\n\nA fundamental challenge in defining the Actor model is that it did not provide for global states so that a computational step could not be defined as going from one global state to the next global state as had been done in all previous models of computation.\n\nIn 1963 in the field of Artificial Intelligence, John McCarthy introduced situation variables in logic in the Situational Calculus. In McCarthy and Hayes 1969, a situation is defined as \"the complete state of the universe at an instant of time.\" In this respect, the situations of McCarthy are not suitable for use in the Actor model since it has no global states.\n\nFrom the definition of an Actor, it can be seen that numerous events take place: local decisions, creating Actors, sending messages, receiving messages, and designating how to respond to the next message received. Partial orderings on such events have been axiomatized in the Actor model and their relationship to physics explored (see Actor model theory).\n\nAccording to Hewitt (2006), the Actor model is based on physics in contrast with other models of computation that were based on mathematical logic, set theory, algebra, \"etc.\" Physics influenced the Actor model in many ways, especially quantum physics and relativistic physics. One issue is what can be observed about Actor systems. The question does not have an obvious answer because it poses both theoretical and observational challenges similar to those that had arisen in constructing the foundations of quantum physics. In concrete terms for Actor systems, typically we cannot observe the details by which the arrival order of messages for an Actor is determined (see Indeterminacy in concurrent computation). Attempting to do so affects the results and can even push the indeterminacy elsewhere. \"e.g.\", see metastability in electronics. Instead of observing the insides of arbitration processes of Actor computations, we await the outcomes.\n\nThe Actor model builds on previous models of computation.\n\nThe lambda calculus of Alonzo Church can be viewed as the earliest message passing programming language (see Hewitt, Bishop, and Steiger 1973; Abelson and Sussman 1985). For example, the lambda expression below implements a tree data structure when supplied with parameters for a leftSubTree and rightSubTree. When such a tree is given a parameter message \"getLeft\", it returns leftSubTree and likewise when given the message \"getRight\" it returns rightSubTree.\n\nHowever, the semantics of the lambda calculus were expressed using variable substitution in which the values of parameters were substituted into the body of an invoked lambda expression. The substitution model is unsuitable for concurrency because it does not allow the capability of sharing of changing resources. Inspired by the lambda calculus, the interpreter for the programming language Lisp made use of a data structure called an environment so that the values of parameters did not have to be substituted into the body of an invoked lambda expression. This allowed for sharing of the effects of updating shared data structures but did not provide for concurrency.\n\nSimula 67 pioneered using message passing for computation, motivated by discrete event simulation applications. These applications had become large and unmodular in previous simulation languages. At each time step, a large central program would have to go through and update the state of each simulation object that changed depending on the state of whichever simulation objects it interacted with on that step. Kristen Nygaard and Ole-Johan Dahl developed the idea (first described in an IFIP workshop in 1967) of having methods on each object that would update its own local state based on messages from other objects. In addition they introduced a class structure for objects with inheritance. Their innovations considerably improved the modularity of programs.\n\nHowever, Simula used coroutine control structure instead of true concurrency.\n\nAlan Kay was influenced by message passing in the pattern-directed invocation of Planner in developing Smalltalk-71. Hewitt was intrigued by Smalltalk-71 but was put off by the complexity of communication that included invocations with many fields including \"global\", \"sender\", \"receiver\", \"reply-style\", \"status\", \"reply\", \"operator selector\", \"etc.\"\n\nIn 1972 Kay visited MIT and discussed some of his ideas for Smalltalk-72 building on the Logo work of Seymour Papert and the \"little person\" model of computation used for teaching children to program. However, the message passing of Smalltalk-72 was quite complex. Code in the language was viewed by the interpreter as simply a stream of tokens. As Dan Ingalls later described it:\n\nThus the message-passing model in Smalltalk-72 was closely tied to a particular machine model and programming-language syntax that did not lend itself to concurrency. Also, although the system was bootstrapped on itself, the language constructs were not formally defined as objects that respond to Eval messages (see discussion below). This led some to believe that a new mathematical model of concurrent computation based on message passing should be simpler than Smalltalk-72.\n\nSubsequent versions of the Smalltalk language largely followed the path of using the virtual methods of Simula in the message-passing structure of programs. However Smalltalk-72 made primitives such as integers, floating point numbers, \"etc.\" into objects. The authors of Simula had considered making such primitives into objects but refrained largely for efficiency reasons. Java at first used the expedient of having both primitive and object versions of integers, floating point numbers, \"etc.\" The C# programming language (and later versions of Java, starting with Java 1.5) adopted the less elegant solution of using \"boxing\" and \"unboxing\", a variant of which had been used earlier in some Lisp implementations.\n\nThe Smalltalk system went on to become very influential, innovating in bitmap displays, personal computing, the class browser interface, and many other ways. For details see Kay's \"The Early History of Smalltalk\". Meanwhile, the Actor efforts at MIT remained focused on developing the science and engineering of higher level concurrency. (See the paper by Jean-Pierre Briot for ideas that were developed later on how to incorporate some kinds of Actor concurrency into later versions of Smalltalk.)\n\nPrior to the development of the Actor model, Petri nets were widely used to model nondeterministic computation. However, they were widely acknowledged to have an important limitation: they modeled control flow but not data flow. Consequently, they were not readily composable, thereby limiting their modularity. Hewitt pointed out another difficulty with Petri nets: simultaneous action. \"I.e.\", the atomic step of computation in Petri nets is a transition in which tokens \"simultaneously\" disappear from the input places of a transition and appear in the output places. The physical basis of using a primitive with this kind of simultaneity seemed questionable to him. Despite these apparent difficulties, Petri nets continue to be a popular approach to modelling concurrency, and are still the subject of active research.\n\nPrior to the Actor model, concurrency was defined in low-level machine terms of threads, locks and buffers(channels). It certainly is the case that implementations of the Actor model typically make use of these hardware capabilities. However, there is no reason that the model could not be implemented directly in hardware without exposing any hardware threads and locks. Also, there is no necessary relationship between the number of Actors, threads, and locks that might be involved in a computation. Implementations of the Actor model are free to make use of threads and locks in any way that is compatible with the laws for Actors.\n\nAn important challenge in defining the Actor model was to abstract away implementation details.\n\nFor example, consider the following question: \"Does each Actor have a queue in which its communications are stored until received by the Actor to be processed?\" Carl Hewitt argued against including such queues as an integral part of the Actor model. One consideration was that such queues could themselves be modeled as Actors that received messages to enqueue and dequeue the communications. Another consideration was that some Actors would not use such queues in their actual implementation. \"E.g.,\" an Actor might have a network of arbiters instead. Of course, there is a mathematical abstraction which is the \"sequence\" of communications that have been received by an Actor. But this sequence emerged only as the Actor operated. In fact the ordering of this sequence can be indeterminate (see Indeterminacy in concurrent computation).\n\nAnother example of abstracting away implementation detail was the question of interpretation: \"Should interpretation be an integral part of the Actor model?\" The idea of interpretation is that an Actor would be defined by how its program script processed eval messages. (In this way Actors would be defined in a manner analogous to Lisp which was \"defined\" by a meta-circular interpreter procedure named eval written in Lisp.) Hewitt argued against making interpretation integral to the Actor model. One consideration was that to process the eval messages, the program script of an Actor would itself have a program script (which in turn would have ...)! Another consideration was that some Actors would not use interpretation in their actual interpretation. \"E.g.,\" an Actor might be implemented in hardware instead. Of course there is nothing wrong with interpretation \"per se\". Also implementing interpreters using eval messages is more modular and extensible than the monolithic interpreter approach of Lisp.\n\nNevertheless, progress developing the model was steady. In 1975, Irene Greif published the first operational model in her dissertation.\n\nGerald Sussman and Guy Steele then took an interest in Actors and published a paper on their Scheme interpreter in which they concluded \"we discovered that the 'actors' and the lambda expressions were identical in implementation.\" According to Hewitt, the lambda calculus is capable of expressing some kinds of parallelism but, in general, \"not\" the concurrency expressed in the Actor model. On the other hand, the Actor model is capable of expressing all of the parallelism in the lambda calculus.\n\nTwo years after Greif published her operational model, Carl Hewitt and Henry Baker published the Laws for Actors.\n\nUsing the laws of the Actor model, Hewitt and Baker proved that any Actor that behaves like a function is continuous in the sense defined by Dana Scott (see denotational semantics).\n\nAki Yonezawa published his specification and verification techniques for Actors. Russ Atkinson and Carl Hewitt published a paper on specification and proof techniques for serializers providing an efficient solution to encapsulating shared resources for concurrency control.\n\nFinally eight years after the first Actor publication, Will Clinger (building on the work of Irene Greif 1975, Gordon Plotkin 1976, Michael Smyth 1978, Henry Baker 1978, Francez, Hoare, Lehmann, and de Roever 1979, and Milne and Milnor 1979) published the first satisfactory mathematical denotational model incorporating unbounded nondeterminism using domain theory in his dissertation in 1981 (see Clinger's model). Subsequently, Hewitt [2006] augmented the diagrams with arrival times to construct a technically simpler denotational model that is easier to understand. See History of denotational semantics.\n\n\n"}
{"id": "4103373", "url": "https://en.wikipedia.org/wiki?curid=4103373", "title": "International Association of Public Transport", "text": "International Association of Public Transport\n\nThe International Association of Public Transport (UITP, from the ) is a non-profit advocacy organization for public transport authorities and operators, policy decision-makers, scientific institutes and the public transport supply and service industry. The association was founded on August 17, 1885 by King Leopold II in Brussels, Belgium to support the Belgian tram and steel industries. UITP supports a holistic approach to urban mobility and advocates for public transport development and sustainable mobility.\n\nUITP represents an international network of 1,600 member companies located in 96 countries and covers all modes of public transport – metro, light rail, regional and suburban railways, bus, and waterborne transport. It also represents collective transport in a broader sense.\n\nUITP's network counts one main and EU office in Brussels and fifteen regional and liaison offices worldwide (Abidjan, Astana, Bangalore, Casablanca, Dubai, Hong Kong, Istanbul, Johannesburg, Moscow, New York, Rome, São Paulo, Shenzhen, Singapore and Tehran). The General Secretariat in Brussels is managed by Mohamed Mezghani, who has been working for more than 25 years in public transport and urban mobility related fields. He\nhas been the Deputy Secretary General of the Association since January 2014 until his election in 2017; Pere Calvet Tordera is the association's President.\n\n\n\n"}
{"id": "49366750", "url": "https://en.wikipedia.org/wiki?curid=49366750", "title": "International Society for Technology in Education", "text": "International Society for Technology in Education\n\nThe International Society for Technology in Education (ISTE) is a nonprofit organization that serves educators interested in the use of technology in education. ISTE serves more than 100,000 education stakeholders throughout the world through individual and organizational membership and support services. ISTE provides educational technology resources to support professional learning for educators and education leaders, including the \"ISTE Conference & Expo\"—a worldwide comprehensive ed tech event, and the widely adopted \"ISTE Standards for learning, teaching and leading with technology\". ISTE also provides a suite of professional learning resources to members, including webinars, online courses, consulting services, books, and peer-reviewed journals and publications.\n\nISTE is probably best known for its annual conference (called the ISTE Conference & Expo). The annual conference serves as a forum for exploring and exchanging ideas about education technology with educators from around the world. The event attracts more than 24,000 educators and education leaders, and includes keynote speakers, hundreds of sessions, and a massive expo where vendors can show off the latest ed tech products and services. Recent conferences have been held in Chicago, IL (2018), San Antonio, TX (2017), Denver, CO (2016) Philadelphia, PA (2015) and Atlanta, GA (2014).\n\nThe 2019 conference will be held in Philadelphia, Pennsylvania, June 23–26, 2019. See ISTE Conference 2019 for more information.\n\nThe ISTE Standards (formerly \"National Educational Technology Standards\", NETS) are a framework for implementing digital strategies in education to positively impact learning, teaching and leading. Along with the standards themselves, ISTE offers information and resources to support understanding and implementation of the standards at a variety of levels. See ISTE Standards.\n\nISTE actively advocates for education technology at the local and national levels to advance the global transformation of education through the application of technology to education. We work with educators and policy makers at all levels to try to ensure that all learners have equal access to tools, connectivity and skills needed for success in using technology. See ISTE Advocacy.\n\nISTE membership is extended to individuals, affiliates (organizations, like school districts and state technology organizations), and corporate members interested in the use and application of technology in Education.\n\nIn addition to an individual membership of over 20,000, ISTE has several corporate members, including:\n\nAdobe, Apple, Best Buy, BrainPOP, Canon U.S.A., CDW-G, Cisco Systems, Dell, Google, LEGO Education, Microsoft, Pearson, PowerSchool, SMART Technologies, Summit Learning, the Verizon Foundation, and more\n\nThe International Council for Computers in Education (ICCE) was founded in 1979, with David Moursund as executive officer and editor-in-chief of the organization's organ \"The Computing Teacher\". In 1989 ICCE changed its name to the present name, International Society for Technology in Education (ISTE). Shortly after, in 1990, The \"Computing Teacher\" was retitled \"Learning and Leading with Technology\".\n\n"}
{"id": "55776277", "url": "https://en.wikipedia.org/wiki?curid=55776277", "title": "Jihan Wu", "text": "Jihan Wu\n\nJihan Wu () is a co-founder of Bitmain (with Micree Zhan), and a prominent supporter of Bitcoin Cash. He was ranked one of the ten most influential figures in the blockchain world by Coindesk in 2017. \n\nIn 2018 he was ranked number 3 in Fortune's The Ledger 40 under 40 for transforming business at the leading edge of finance and technology. His net worth in 2018 was US$$2.39 billion.\n\nWu lives in China and has an economics degree from Peking University.\n"}
{"id": "54449949", "url": "https://en.wikipedia.org/wiki?curid=54449949", "title": "Kingmax", "text": "Kingmax\n\nKingmax is a Taiwan based corporate group and its brand. The principal company of the group is Kingmax Semiconductor Inc. was established in 1989, headquartered in Hsinchu, Taiwan, and the group manufactures and offers computer hardware and electronics products all over the world.\n\nOriginally, in 1989, Kingmax Semiconductor Inc. was established . The Group has several manufacturing facilities in Taiwan, China, and Hong Kong. The group manufactures and offers flash memory products (SD cards, USB flash drives and Solid state drives), Hard disk drives, DRAM, Card readers, USB adapters, and other electronics products all over the world. The business type and scope is same as ADATA, Silicon Power and Transcend Information, these are also the companies in Taiwan. In 2011, Kingmax was known that introduced the first 64GB microSD card in the world . In 2015, Kingpak Technology Inc. was merged into International Branding Marketing Inc. (English name is still Kingpak Technology Inc.) . And then, it was the first company of the group that changed status from private to public listed on the Taiwan OTC Market (6238.TWO). In 2017, Kingmax was also known that AirQ Check, the portable air quality checker for checking PM2.5 etc., received Taiwan Excellence Award .<br>\n\nIn the aspect of business-to-business, as the supplier of computer hardware, Kingmax has contributed to offer the various products to major computer companies. The group has offered TinyBGA to IBM, hp, Sun Microsystems, Compaq, Dell, NEC, Acer, Asus\n, CMOS image sensor to ON Semiconductor, etc. . The group also has offered flash memory products (SD cards etc.) and DRAM to Lexar (Micron Technology) and Elixir (Nanya Technology) as OEM .\n\n\n\n"}
{"id": "7370764", "url": "https://en.wikipedia.org/wiki?curid=7370764", "title": "Kitchen garden", "text": "Kitchen garden\n\n\"Kailyard redirects here. For the grouping of Scottish literature see Kailyard school\"\n\nThe traditional kitchen garden, also known as a potager (in French, \"jardin potager\") or in Scotland a kailyaird, is a space separate from the rest of the residential garden – the ornamental plants and lawn areas. Most vegetable gardens are still miniature versions of old family farm plots, but the kitchen garden is different not only in its history, but also its design. \n\nThe kitchen garden may serve as the central feature of an ornamental, all-season landscape, or it may be little more than a humble vegetable plot. It is a source of herbs, vegetables and fruits, but it is often also a structured garden space with a design based on repetitive geometric patterns. \n\nThe kitchen garden has year-round visual appeal and can incorporate permanent perennials or woody shrub plantings around (or among) the annuals.\n\nA potager is a French term for an ornamental vegetable or kitchen garden. The historical design precedent is from the Gardens of the French Renaissance and Baroque Garden à la française eras. Often flowers (edible and non-edible) and herbs are planted with the vegetables to enhance the garden's beauty. The goal is to make the function of providing food aesthetically joyful.\n\nPlants are chosen as much for their functionality as for their color and form. Many are trained to grow upward. A well-designed potager can provide food as well as cut flowers and herbs for the home with very little maintenance. Potagers can disguise their function of providing for a home in a wide array of forms—from the carefree style of the cottage garden to the formality of a knot garden.\n\nA vegetable garden (also known as a vegetable patch or vegetable plot) is a garden that exists to grow vegetables and other plants useful for human consumption, in contrast to a flower garden that exists for aesthetic purposes. It is a small-scale form of vegetable growing. A vegetable garden typically includes a compost heap, and several plots or divided areas of land, intended to grow one or two types of plant in each plot. Plots may also be divided into rows with an assortment of vegetables grown in the different rows. It is usually located to the rear of a property in the back garden or back yard. Many families have home kitchen and vegetable gardens that they use to produce food. In World War II, many people had a \"victory garden\" which provided food and thus freed resources for the war effort.\n\nWith worsening economic conditions and increased interest in organic and sustainable living, many people are turning to vegetable gardening as a supplement to their family's diet. Food grown in the back yard consumes little if any fuel for shipping or maintenance, and the grower can be sure of what exactly was used to grow it. Organic horticulture, or organic gardening, has become increasingly popular for the modern home gardener.\n\nThe herb garden is often a separate space in the garden, devoted to growing a specific group of plants known as herbs. These gardens may be informal patches of plants, or they may be carefully designed, even to the point of arranging and clipping the plants to form specific patterns, as in a knot garden.\n\nHerb gardens may be purely functional or they may include a blend of functional and ornamental plants. The herbs are usually used to flavour food in cooking, though they may also be used in other ways, such as discouraging pests, providing pleasant scents, or serving medicinal purposes (such as a physic garden), among others.\n\nA kitchen garden can be created by planting different herbs in pots or containers, with the added benefit of mobility. Although not all herbs thrive in pots or containers, some herbs do better than others. Mint, a fragrant yet invasive herb, is an example of an herb that is advisable to keep in a container or it will take over the whole garden. \n\nCulinary herbs in temperate climates are to a large extent still the same as in the medieval period. Herbs often have multiple uses. For example, mint may be used for cooking, tea, and pest control. Among the many uses of herbs are:\n\n\nA witches' garden is an herb garden specifically designed and used for the cultivation of herbs, for culinary, medicinal and/or spiritual purposes. Herbal baths, the making of incense, tied in bundles for rituals or prayers, or placed in charms are just some of the ways herbs can be used for spiritual purposes.\n\nHerb gardens developed from the general gardens of the ancient classical world, which were used for growing vegetables, flowers, fruits and medicines. For centuries \"wise women\" and \"healers\" understood the uses of herbs for the purposes of healing and magic. During the medieval period, monks and nuns acquired this medical knowledge and grew the necessary herbs in specialised gardens.\n\nTypical plants in a witches' garden are: rosemary, sage, parsley, mint, catnip, henbane, marjoram, thyme, rue, angelica, bay, oregano, dill, aloe, arnica, chives, and basil. Basil is especially common in these gardens as a strong protection herb. It is said, \"Where basil grows, no evil goes!\" and \"Where basil is, no evil lives!\"\n\n\n\n"}
{"id": "43132637", "url": "https://en.wikipedia.org/wiki?curid=43132637", "title": "List of Young Achievers Award winners", "text": "List of Young Achievers Award winners\n\nThe Young Achievers Awards is a national competition held annually in Uganda which selects and promotes the best practice and excellence in youth creativity. The following is the list of Young Achievers Award winners.\n\nMedia and Society\n\nFilm and Television\nQ\nMusic\n\nArts, Fashion and Culture\n\nLeadership and governance \n\nSports\n\nEnvironment \n\nBusiness and Trade\n\nICT\n\nMedia and Journalism award\nOutstanding performing Arts\n\nEricsson Innovation excellence Award\n\nFarming and Agro-Processing\n\nLeadership and Social Enterprise\n\nSports Personality Award\n\nHeroes Award\n\nLifetime Achievement Award\n\nStar Hall of Fame\n\nMedia and Journalism award\nCreative Arts (Fashion)\n\nCreative Arts (Music)\n\nSocial Entrepreneurship Award\n\nInnovations & ICT Award\n\nFarming and Agro-Processing\n\nBusiness Award\n\nSports Personality Award\n\nHeroes Award\n\nLifetime Achievement Award\n\nStar Hall of Fame\n"}
{"id": "22861361", "url": "https://en.wikipedia.org/wiki?curid=22861361", "title": "Lunar regolith simulant", "text": "Lunar regolith simulant\n\nA lunar regolith simulant is a terrestrial material synthesized in order to approximate the chemical, mechanical, or engineering properties of, and the mineralogy and particle size distributions of, lunar regolith. Lunar regolith simulants are used by researchers who wish to research the materials handling, excavation, transportation, and uses of lunar regolith. Samples of actual lunar regolith are too scarce, and too small, for such research.\n\nIn the run-up to the Apollo program, crushed terrestrial rocks were first used to simulate the anticipated soils that astronauts would encounter on the lunar surface. In some cases the properties of these early simulants were substantially different from actual lunar soil, and the issues associated with the pervasive, fine-grained sharp dust grains on the Moon came as a surprise. \n\nAfter Apollo and particularly during the development of the Constellation program, there was a large proliferation of lunar simulants produced by different organizations and researchers. Many of these were given three-letter acronyms to distinguish them (e.g., MLS-1, JSC-1), and numbers to designate subsequent versions. These simulants were broadly divided into highlands or mare soils, and were usually produced by crushing and sieving analogous terrestrial rocks (anorthosite for highlands, basalt for mare). Returned Apollo and Luna samples were used as reference materials in order to target specific properties such as elemental chemistry or particle size distribution. Many of these simulants were criticized by prominent lunar scientist Larry Taylor for a lack of quality control and wasted money on features like nanophase iron that had no documented purpose.\n\nJSC-1 (Johnson Space Center Number One) was a lunar regolith simulant that was developed in 1994 by NASA and the Johnson Space Center. Its developers intended it to approximate the lunar soil of the maria. It was sourced from a basaltic ash with a high glass content.\n\nIn 2005, NASA contracted with Orbital Technologies Corporation (ORBITEC) for a second batch of simulant in three grades:\n\n\nNASA received 14 metric tons of JSC-1A, and one ton each of AF and AC in 2006. Another 15 tons of JSC-1A and 100 kg of JSC-1F were produced by ORBITEC for commercial sale, but ORBITEC is no longer selling simulants and was acquired by the Sierra Nevada Corporation. An 8-ton sand box of commercial JSC‐1A is available for daily rental from the NASA Solar System Exploration Research Virtual Institute (SSERVI).\n\nJSC-1A can geopolymerize in an alkaline solutions resulting in a hard, rock-like, material. Tests show that the maximum compressive and flexural strength of the 'lunar' geopolymer is comparable to that of conventional cements. \nJSC-1 and JSC-1A are now no longer available outside of NASA centers.\n\nTwo lunar highlands simulants, the NU-LHT (lunar highlands type) series and OB-1 (olivine-bytownite) were developed and produced in anticipation of the Constellation activities. Both of these simulants are sourced mostly from rare anorthosite deposits on the Earth. For NU-LHT the anorthosite came from the Stillwater complex, and for OB-1 it came from the Shawmere Anorthosite in Ontario. Neither of these simulants were widely distributed.\n\nMost of the previously developed lunar simulants are no longer being produced or distributed outside of NASA. Multiple companies have tried to sell regolith simulants for profit, including Zybek Advanced Products, ORBITEC, and Deep Space Industries. None of these efforts have seen much success. NASA is unable to sell simulants, or distribute unlimited amounts for free; however, NASA can award set amounts of simulant to grant winners. \n\nSeveral lunar simulants have been developed recently and are either being sold commercially or are available for rent inside large regolith bins. These include the Lunar Highlands Simulant (LHS-1) and Lunar Mare Simulant (LMS-1) produced and distributed by the not-for-profit Exolith Lab run out of the University of Central Florida, and a series of simulants produced by Off Planet Research that are available for rent.\n\n\n"}
{"id": "33070424", "url": "https://en.wikipedia.org/wiki?curid=33070424", "title": "Manufacturing Grocers' Employees' Federation of Australia", "text": "Manufacturing Grocers' Employees' Federation of Australia\n\nManufacturing Grocers' Employees' Federation of Australia (M.G.U.) was an Australian trade union existing between 1906 and 1988. The union was first established as the Federated Candle, Soap, Soda & Starch Employees' Union of Australia, before changing its name in 1914. The union represented workers employed in manufacturing grocers' sundries and non-edible grocery products, particularly in the southern states of South Australia and Victoria. In 1988 the union amalgamated with the Federated Millers and Mill Employees' Union to form the Federated Millers and Manufacturing Grocers Employees' Association of Australia, which in turn merged with a number of unions to form the National Union of Workers.\n\nThe Manufacturing Grocers' Union undertook a wide variety of activities to pursue the interests of its members. The union made representations to government and the public in favour of protectionist measures to ensure the competitiveness of products manufactured in Australia with international imports. The M.G.U. also publicised the poor working conditions of its members, including problems such as noise, dust and physical fatigue. The South Australian trade union leader and later politician Theo Nicholls served as part-time secretary of the union in South Australia, and was active in its organisation.\n\nThe Manufacturing Grocers' Union underwent several bitter industrial disputes during its history, including a number of strikes. The first major strike the union was involved in was in April 1916, when the M.G.U., along with a number of other unions, participated in a sympathy strike with the Storemen and Packers' Union over the dismissal of several men at a Parsons' Brothers factory in Melbourne. The strike lasted for several months and led to shortages of several products in Victoria, due to blockade of goods by the employers. The strike eventually collapsed in June 1916, with all strikers returning to work.\n\nThe Manufacturing Grocers' Union held another major strike in 1948 at the match factories of Bryant and May, in Richmond, Melbourne, over an increase in the required daily output of matches. The dispute was resolved when all parties agreed to allow the Commonwealth Statistician determine a fair daily output per worker.\n\nThe Manufacturing Grocers' Union made efforts to establish a branch of the union in New South Wales, the most populous state of Australia, but this attempt was thwarted by the Australian Workers' Union, who represented the relevant workers at the time.\n\nIn 1988 the Manufacturing Grocers' Union amalgamated with the Federated Millers and Mill Employees' Union, another small union representing workers in food processing. The resulting body, the Federated Millers and Manufacturing Grocers Employees' Association of Australasia, was short-lived and subsequently amalgamated with the National Union of Storeworkers, Packers, Rubber and Allied Workers to form the National Union of Workers.\n\n"}
{"id": "50278739", "url": "https://en.wikipedia.org/wiki?curid=50278739", "title": "MicroPython", "text": "MicroPython\n\nMicroPython is a software implementation of the Python 3 programming language, written in C, that is optimized to run on a microcontroller. MicroPython is a full Python compiler and runtime that runs on the micro-controller hardware. The user is presented with an interactive prompt (the REPL) to execute supported commands immediately. Included are a selection of core Python libraries; MicroPython includes modules which give the programmer access to low-level hardware.\n\nMicroPython was originally created by the Australian programmer and physicist Damien George, after a successful Kickstarter backed campaign in 2013. While the original Kickstart campaign released MicroPython with a pyboard microcontroller, MicroPython supports a number of ARM based architectures. MicroPython has since been run on Arduino platform based products, ESP8266, ESP32, and Internet of things hardware. In 2016 a version of MicroPython for the BBC Micro Bit was created as part of the Python Software Foundation's contribution to the Micro Bit partnership with the BBC.\n\nThe source code for the project is available on GitHub.\n\n"}
{"id": "43233668", "url": "https://en.wikipedia.org/wiki?curid=43233668", "title": "Muzzley", "text": "Muzzley\n\nMuzzley is an app to control your Internet of Things devices. The app, available for iOS, Android and Windows 10 allows you to control these devices. They provide third party devices integrations through their API's and SDK's for manufacturers to directly embed into their micro controllers.\n\nMuzzley was founded in March 2012 by Domingos Bruges and Eduardo Pinheiro.\n\nLater that year, they started beta testing and raised their first round of seed investment.\n\nIn 2013, the Muzzley team moved to the United States. They also expanded their team and partnered with Intel.\n\nIn February 2014, Muzzley received $2.5m in Seed venture capital funding, led by Portugal Ventures. The investment was also backed by Espirito Santo Ventures and Plug and Play Tech Center.\n\nThey also nominated Jon Castor as the Chair of the Board and released the second major release of the app, Muzzley V2.0.\n\n\n"}
{"id": "22711136", "url": "https://en.wikipedia.org/wiki?curid=22711136", "title": "Neoparium", "text": "Neoparium\n\nNeoparium, also known as Neopariés, is a glass material made in Japan by Nippon Electric Glass. Described as \"crystalized glass ceramic,\" it was developed as an architectural cladding material for use in harsh environments. Typical units are 5/8\" thick in a number of opaque colors. Panels can be fabricated with curves.\n\nThe material was most notably used to replace failing marble cladding on the BMA Tower in Kansas City, Missouri, where material replacement was reviewed for compliance with National Register of Historic Places status. Another use was cladding Marco Polo House, an office building built in 1987 in the Victorian district of Battersea, London.\n\n"}
{"id": "1165697", "url": "https://en.wikipedia.org/wiki?curid=1165697", "title": "Non-recurring engineering", "text": "Non-recurring engineering\n\nNon-recurring engineering (NRE) refers to the one-time cost to research, design, develop and test a new product or product enhancement. When budgeting for a new product, NRE must be considered to analyze if a new product will be profitable. Even though a company will pay for NRE on a project only once, NRE costs can be prohibitively high and the product will need to sell well enough to produce a return on the initial investment. NRE is unlike production costs, which must be paid constantly to maintain production of a product. It is a form of fixed cost in economics terms. Once a system is designed any number of units can be manufactured without increasing NRE cost.\n\nIn a project-type (manufacturing) company, large parts (possibly all) of the project represent NRE. In this case the NRE costs are likely to be included in the first project's costs, this can also be called research and development (R&D). If the firm cannot recover these costs, it must consider funding part of these from reserves, possibly take a project loss, in the hope that the investment can be recovered from further profit on future projects.\n\nThe concept of full product NRE as described above may lead readers to believe that NRE expenses are necessarily high. However, Focused NRE wherein small amounts of NRE money can yield large returns by making existing product changes is an option to consider as well. A small adjustment to an existing assembly may be considered, in order to use a less expensive or improved subcomponent or to replace a subcomponent which is no longer available. In the world of embedded firmware, NRE may be invested in code development to fix problems or to add features where the costs to implement are a very small percentages of an immediate return. Chrysler found such a way to repair a transmission problem by investing trivial NRE dollars into computer firmware to fix a mechanical problem to save some tens of millions of dollars in mechanical repairs to transmissions in the field. \n\nMastery of NRE concepts as financial investments are a most excellent loss control tool and must be considered a key part of manufacturing profit enhancement.\n\n"}
{"id": "41692369", "url": "https://en.wikipedia.org/wiki?curid=41692369", "title": "OpenPOWER Foundation", "text": "OpenPOWER Foundation\n\nThe OpenPOWER Foundation is a collaboration around Power Architecture products initiated by IBM and announced as the \"OpenPOWER Consortium\" on August 6, 2013. IBM is opening up technology surrounding their Power Architecture offerings, such as processor specifications, firmware and software with a liberal license, and will be using a collaborative development model with their partners.\n\nThe goal stated is to enable the server vendor ecosystem to build their own customized server, networking and storage hardware for future data centers and cloud computing.\n\nPower.org is still the governing body around the Power Architecture instruction set but specific implementations are now free to use under a liberal license granted by IBM. Processors based on IBM's IP can now be fabricated on any foundry and mixed with other hardware products of the integrator's choice.\n\nIBM is using the word open to describe this project in three ways:\n\n\nIBM is looking to offer the POWER8 chip technology and other future iterations under the OpenPOWER initiative but they are also making previous designs available for licensing. Partners are required to contribute intellectual property to the OpenPOWER Foundation to be able to gain high level status.\n\nThe POWER8 processor architecture incorporates facilities to integrate it more easily into custom designs. The generic memory controllers are designed to evolve with future technologies, and the new CAPI (Coherent Accelerator Processor Interface) expansion bus is built to integrate easily with external coprocessors like GPUs, ASICs and FPGAs. \n\nNvidia is contributing their fast interconnect technology, NVLink, that will enable tight coupling of Nvidia's Pascal based graphics processors into future POWER processors.\n\nThe OpenPOWER initiative will include firmware, the KVM hypervisor and little endian Linux operating system. The foundation has a site on GitHub for the software they are releasing as open source. As of July 2014, they have released firmware to boot Linux.\n\nSUSE included support for Power8 in their enterprise Linux distribution SUSE Linux Enterprise Server version 12 (release 27 October 2014).\n\nCanonical Ltd. has working firmware and general Linux support via their Ubuntu Server version 16.04 LTS.\n\nFreeBSD has also been reported to have preliminary support for the architecture.\n\nGoogle, Tyan, Nvidia and Mellanox are all founding members of the OpenPOWER Foundation. Nvidia is looking to merge their graphics cores and Mellanox to integrate their high performance interconnects with Power cores. Tyan is said to be working on servers using POWER8 and Google sees using POWER processors in its data centers as a future possibility.\nAltera announced support for OpenPOWER in November 2013 with their FPGA offerings and OpenCL software.\n\nOn January 19, 2014, the Suzhou PowerCore Technology Company and the Research Institute of Jiangsu Industrial Technology announced that they will join the OpenPOWER Foundation and license POWER8 technologies to promote and help build systems around and design custom made processors for use in big data and cloud computing applications.\n\nOn February 12, 2014, Samsung Electronics joined.\n\nAs of March 2014, members also include Altera, Fusion-io, Hynix, Micron, Servergy and Xilinx.\n\nAs of April 2014, Canonical, Chuanghe Mobile, Emulex, Hitachi, Inspur, Jülich Research Centre, Oregon State University, Teamsun, Unisource Technology Inc and ZTE are listed as members at various levels.\n\nAs of December 2014, Rackspace, Avnet, Lawrence Livermore National Laboratory, Sandia National Laboratories, Tsinghua University, Nallatech, Bull and Qlogic have joined, to bring it to a total of about 80 members.\n\nAt the first annual \"OpenPOWER Summit 2015\", the organization announced that there were 113 members, including Wistron, Cirrascale and PMC-Sierra.\n\nAs of Fall 2016 the OpenPOWER foundation has over 250 members.\n\n\n"}
{"id": "24605779", "url": "https://en.wikipedia.org/wiki?curid=24605779", "title": "Plotter (instrument)", "text": "Plotter (instrument)\n\nIn navigational instruments, a plotter is an instrument which marks the position of a vehicle on a map or chart.\n\nSeveral types of plotters exist. These include manual and electronic plotters:\n\n\n\n"}
{"id": "435852", "url": "https://en.wikipedia.org/wiki?curid=435852", "title": "Register renaming", "text": "Register renaming\n\nIn computer architecture, register renaming is a technique that eliminates the false data dependencies arising from the reuse of architectural registers by successive instructions that do not have any real data dependencies between them. The elimination of these false data dependencies reveals more instruction-level parallelism in an instruction stream, which can be exploited by various and complementary techniques such as superscalar and out-of-order execution for better performance.\n\nIn a register machine, programs are composed of instructions which operate on values. The instructions must name these values in order to distinguish them from one another. A typical instruction might say, add X and Y and put the result in Z. In this instruction, X, Y, and Z are the names of storage locations.\n\nIn order to have a compact instruction encoding, most processor instruction sets have a small set of special locations which can be directly named. For example, the x86 instruction set architecture has 8 integer registers, x86-64 has 16, many RISCs have 32, and IA-64 has 128. In smaller processors, the names of these locations correspond directly to elements of a register file.\n\nDifferent instructions may take different amounts of time; for example, a processor may be able to execute hundreds of instructions while a single load from the main memory is in progress. Shorter instructions executed while the load is outstanding will finish first, thus the instructions are finishing out of the original program order. Out-of-order execution has been used in most recent high-performance CPUs to achieve some of their speed gains.\n\nConsider this piece of code running on an out-of-order CPU:\n\nInstructions 4, 5, and 6 are independent of instructions 1, 2, and 3, but the processor cannot finish 4 until 3 is done, otherwise instruction 3 would write the wrong value. This restriction can be eliminated by changing the names of some of the registers:\n\nNow instructions 4, 5, and 6 can be executed in parallel with instructions 1, 2, and 3, so that the program can be executed faster.\n\nWhen possible, the compiler would detect the distinct instructions and try to assign them to a different register. However, there is a finite number of register names that can be used in the assembly code. Many high performance CPUs have more physical registers than may be named directly in the instruction set, so they rename registers in hardware to achieve additional parallelism.\n\nWhen more than one instruction references a particular location for an operand, either by reading it (as an input) or by writing to it (as an output), executing those instructions in an order different from the original program order can lead to three kinds of data hazards:\n\nInstead of delaying the write until all reads are completed, two copies of the location can be maintained, the old value and the new value. Reads that precede, in program order, the write of the new value can be provided with the old value, even while other reads that follow the write are provided with the new value. The false dependency is broken and additional opportunities for out-of-order execution are created. When all reads that need the old value have been satisfied, it can be discarded. This is the essential concept behind register renaming.\n\nAnything that is read and written can be renamed. While the general-purpose and floating-point registers are discussed the most, flag and status registers or even individual status bits are commonly renamed as well.\n\nMemory locations can also be renamed, although it is not commonly done to the extent practiced in register renaming. The Transmeta Crusoe processor's gated store buffer is a form of memory renaming.\n\nIf programs refrained from reusing registers immediately, there would be no need for register renaming. Some instruction sets (e.g., IA-64) specify very large numbers of registers for specifically this reason. However, there are limitations to this approach:\n\nCode size increases are important because when the program code is larger, the instruction cache misses more often and the processor stalls waiting for new instructions.\n\nMachine language programs specify reads and writes to a limited set of registers specified by the instruction set architecture (ISA). For instance, the Alpha ISA specifies 32 integer registers, each 64 bits wide, and 32 floating-point registers, each 64 bits wide. These are the \"architectural\" registers. Programs written for processors running the Alpha instruction set will specify operations reading and writing those 64 registers. If a programmer stops the program in a debugger, they can observe the contents of these 64 registers (and a few status registers) to determine the progress of the machine.\n\nOne particular processor which implements this ISA, the Alpha 21264, has 80 integer and 72 floating-point \"physical\" registers. There are, on an Alpha 21264 chip, 80 physically separate locations which can store the results of integer operations, and 72 locations which can store the results of floating point operations. (In fact, there are even more locations than that, but those extra locations are not germane to the register renaming operation.)\n\nThe following text describes two styles of register renaming, which are distinguished by the circuit which holds the data ready for an execution unit.\n\nIn all renaming schemes, the machine converts the architectural registers referenced in the instruction stream into tags. Where the architectural registers might be specified by 3 to 5 bits, the tags are usually a 6 to 8 bit number. The rename file must have a read port for every input of every instruction renamed every cycle, and a write port for every output of every instruction renamed every cycle. Because the size of a register file generally grows as the square of the number of ports, the rename file is usually physically large and consumes significant power.\n\nIn the \"tag-indexed register file\" style, there is one large register file for data values, containing one register for every tag. For example, if the machine has 80 physical registers, then it would use 7 bit tags. 48 of the possible tag values in this case are unused.\n\nIn this style, when an instruction is issued to an execution unit, the tags of the source registers are sent to the physical register file, where the values corresponding to those tags are read and sent to the execution unit.\n\nIn the \"reservation station\" style, there are many small associative register files, usually one at the inputs to each execution unit. Each operand of each instruction in an issue queue has a place for a value in one of these register files.\n\nIn this style, when an instruction is issued to an execution unit, the register file entries corresponding to the issue queue entry are read and forwarded to the execution unit.\n\n\nReorder buffers can be data-less or data-ful.\n\nIn Willamette's ROB, the ROB entries point to registers in the physical register file (PRF), and also contain other book keeping. This was also the first Out of Order design done by Andy Glew, at Illinois with HaRRM.\n\nP6's ROB, the ROB entries contain data; there is no separate PRF. Data values from the ROB are copied from the ROB to the RRF at retirement.\n\nOne small detail: if there is temporal locality in ROB entries (i.e., if instructions close together in the von Neumann instruction sequence write back close together in time, it may be possible to perform write combining on ROB entries and so have fewer ports than a separate ROB/PRF would). It is not clear if it makes a difference, since a PRF should be banked.\n\nROBs usually don't have associative logic, and certainly none of the ROBs designed by Andy Glew have CAMs. Keith Diefendorff insisted that ROBs have complex associative logic for many years. The first ROB proposal may have had CAMs.\n\nThis is the renaming style used in the MIPS R10000, the Alpha 21264, and in the FP section of the AMD Athlon.\n\nIn the renaming stage, every architectural register referenced (for read or write) is looked up in an architecturally-indexed remap file. This file returns a tag and a ready bit. The tag is non-ready if there is a queued instruction which will write to it that has not yet executed. For read operands, this tag takes the place of the architectural register in the instruction. For every register write, a new tag is pulled from a free tag FIFO, and a new mapping is written into the remap file, so that future instructions reading the architectural register will refer to this new tag. The tag is marked as unready, because the instruction has not yet executed. The previous physical register allocated for that architectural register is saved with the instruction in the reorder buffer, which is a FIFO that holds the instructions in program order between the decode and graduation stages.\n\nThe instructions are then placed in various issue queues. As instructions are executed, the tags for their results are broadcast, and the issue queues match these tags against the tags of their non-ready source operands. A match means that the operand is ready. The remap file also matches these tags, so that it can mark the corresponding physical registers as ready. When all the operands of an instruction in an issue queue are ready, that instruction is ready to issue. The issue queues pick ready instructions to send to the various functional units each cycle. Non-ready instructions stay in the issue queues. This unordered removal of instructions from the issue queues can make them large and power-consuming.\n\nIssued instructions read from a tag-indexed physical register file (bypassing just-broadcast operands) and then execute. Execution results are written to tag-indexed physical register file, as well as broadcast to the bypass network preceding each functional unit. Graduation puts the previous tag for the written architectural register into the free queue so that it can be reused for a newly decoded instruction.\n\nAn exception or branch misprediction causes the remap file to back up to the remap state at last valid instruction via combination of state snapshots and cycling through the previous tags in the in-order pre-graduation queue. Since this mechanism is required, and since it can recover any remap state (not just the state before the instruction currently being graduated), branch mispredictions can be handled before the branch reaches graduation, potentially hiding the branch misprediction latency.\n\nThis is the style used in the integer section of the AMD K7 and K8 designs.\n\nIn the renaming stage, every architectural register referenced for reads is looked up in both the architecturally-indexed future file and the rename file. The future file read gives the value of that register, if there is no outstanding instruction yet to write to it (i.e., it's ready). When the instruction is placed in an issue queue, the values read from the future file are written into the corresponding entries in the reservation stations. Register writes in the instruction cause a new, non-ready tag to be written into the rename file. The tag number is usually serially allocated in instruction order—no free tag FIFO is necessary.\n\nJust as with the tag-indexed scheme, the issue queues wait for non-ready operands to see matching tag broadcasts. Unlike the tag-indexed scheme, matching tags cause the corresponding broadcast value to be written into the issue queue entry's reservation station.\n\nIssued instructions read their arguments from the reservation station, bypass just-broadcast operands, and then execute. As mentioned earlier, the reservation station register files are usually small, with perhaps eight entries.\n\nExecution results are written to the reorder buffer, to the reservation stations (if the issue queue entry has a matching tag), and to the future file if this is the last instruction to target that architectural register (in which case register is marked ready).\n\nGraduation copies the value from the reorder buffer into the architectural register file. The sole use of the architectural register file is to recover from exceptions and branch mispredictions.\n\nExceptions and branch mispredictions, recognised at graduation, cause the architectural file to be copied to the future file, and all registers marked as ready in the rename file. There is usually no way to reconstruct the state of the future file for some instruction intermediate between decode and graduation, so there is usually no way to do early recovery from branch mispredictions.\n\nIn both schemes, instructions are inserted in-order into the issue queues, but are removed out-of-order. If the queues do not collapse empty slots, then they will either have many unused entries, or require some sort of variable priority encoding for when multiple instructions are simultaneously ready to go. Queues that collapse holes have simpler priority encoding, but require simple but large circuitry to advance instructions through the queue.\n\nReservation stations have better latency from rename to execute, because the rename stage finds the register values directly, rather than finding the physical register number, and then using that to find the value. This latency shows up as a component of the branch misprediction latency.\n\nReservation stations also have better latency from instruction issue to execution, because each local register file is smaller than the large central file of the tag-indexed scheme. Tag generation and exception processing are also simpler in the reservation station scheme, as discussed below.\n\nThe physical register files used by reservation stations usually collapse unused entries in parallel with the issue queue they serve, which makes these register files larger in aggregate, and consume more power, and more complicated than the simpler register files used in a tag-indexed scheme. Worse yet, every entry in each reservation station can be written by every result bus, so that a reservation-station machine with, e.g., 8 issue queue entries per functional unit will typically have 9 times as many bypass networks as an equivalent tag-indexed machine. Consequently, result forwarding consumes much more power and area than in a tag-indexed design.\n\nFurthermore, the reservation station scheme has four places (Future File, Reservation Station, Reorder Buffer and Architectural File) where a result value can be stored, whereas the tag-indexed scheme has just one (the physical register file). Because the results from the functional units, broadcast to all these storage locations, must reach a much larger number of locations in the machine than in the tag-indexed scheme, this function consumes more power, area, and time. Still, in machines equipped with very accurate branch prediction schemes and if execute latencies are a major concern, reservation stations can work remarkably well.\n\nThe IBM System/360 Model 91 was an early machine that supported out-of-order execution of instructions; it used the Tomasulo algorithm, which uses register renaming.\n\nThe POWER1 is the first microprocessor that used register renaming and out-of-order execution in 1990.\n\nThe original R10000 design had neither collapsing issue queues nor variable priority encoding, and suffered starvation problems as a result—the oldest instruction in the queue would sometimes not be issued until both instruction decode stopped completely for lack of rename registers, and every other instruction had been issued. Later revisions of the design starting with the R12000 used a partially variable priority encoder to mitigate this problem.\n\nEarly out-of-order machines did not separate the renaming and ROB/PRF storage functions. For that matter, some of the earliest, such as Sohi's RUU or the Metaflow DCAF, combined scheduling, renaming, and storage all in the same structure.\n\nMost modern machines do renaming by RAM indexing a map table with the logical register number. E.g., P6 did this; future files do this, and have data storage in the same structure.\n\nHowever, earlier machines used content-addressable memory (CAM) in the renamer. E.g., the HPSM RAT, or Register Alias Table, essentially used a CAM on the logical register number in combination with different versions of the register.\n\nIn many ways, the story of out-of-order microarchitecture has been how these CAMs have been progressively eliminated. Small CAMs are useful; large CAMs are impractical.\n\nThe P6 microarchitecture was the first microarchitecture by Intel to implement both out-of-order execution and register renaming. The P6 microarchitecture was used in Pentium Pro, Pentium II, Pentium III, Pentium M, Core, and Core 2 microprocessors. The Cyrix M1, released on October 2, 1995, was the first x86 processor to use register renaming and out-of-order execution. Other x86 processors (such as NexGen Nx686 and AMD K5) released in 1996 also featured register renaming and out-of-order execution of RISC μ-operations (rather than native x86 instructions).\n\n"}
{"id": "3009173", "url": "https://en.wikipedia.org/wiki?curid=3009173", "title": "Remote Solution", "text": "Remote Solution\n\nRemote Solution Co., Ltd. is an electronics manufacturer located in Gimcheon, South Korea. The company was founded in 1994 as Hango Electronics Co., Ltd. and assumed its current name in 2002.\n\nAlthough the company's primary products are handheld remote control devices, it is best known for its \"Personal Jukebox\" digital audio player. Produced from 2000 until about 2003, the player was developed by and licensed to HanGo Electronics by Compaq. The PJB-100 was the first hard drive-based DAP and was first sold by Remote Solution in 1999.\n"}
{"id": "29069615", "url": "https://en.wikipedia.org/wiki?curid=29069615", "title": "Resistance Database Initiative", "text": "Resistance Database Initiative\n\nHIV Resistance Response Database Initiative (RDI) is a not-for-profit organisation established in 2002 with the mission of improving the clinical management of HIV infection through the application of bioinformatics to HIV drug resistance and treatment outcome data. The RDI has the following specific goals:\n\n\nThe RDI consists of a small executive group based in the UK, an international advisory group of leading HIV/AIDS scientists and clinicians, and an extensive global network of collaborators and data contributors.\n\nHuman immunodeficiency virus (HIV) is the virus that causes acquired immunodeficiency syndrome (AIDS), a condition in which the immune system begins to fail, leading to life-threatening opportunistic infections.\n\nThere are approximately 25 HIV ‘antiretroviral’ drugs that have been approved for the treatment of HIV infection, from six different classes, based on the point in the HIV life-cycle at which they act.\n\nThey are used in combination; typically 3 or more drugs from 2 or more different classes, a form of therapy known as highly active antiretroviral therapy or HAART. The aim of therapy is suppression of the virus to very low, ideally undetectable, levels in the blood this prevents the virus from depleting the immune cells that it preferentially attacks (CD4 cells) and prevents or delays illness and death.\n\nDespite the expanding availability of these drugs and the impact of their use, treatments continue to fail, often due to the development of resistance. During drug therapy, low-level virus replication still occurs, particularly when a patient misses a dose. HIV makes errors in copying its genetic material and, if a mutation makes the virus resistant to one or more of the drugs, it may begin to replicate more successfully in the presence of that drug and undermine the effect of the treatment. If this happens then the treatment needs to be changed to re-establish control over the virus.\n\nIn well-resourced healthcare settings, when treatment fails a resistance test may be run to predict to which drugs the patient’s virus is resistant. The type of test in most common use is the genotype test, which detects mutations in the viral genetic code. This information is then typically interpreted using rules equating individual mutations with resistance against individual drugs. However, there are many different interpretation systems available that do not always agree, the systems only provide categorical results (resistant, sensitive or intermediate) and they do not necessarily relate well to how a patient will respond to a combination of drugs in the clinic.\n\nThe RDI was established in 2002 to pioneer a new approach: to develop computational models using the genotype and a wide range of other clinically relevant data collected from thousands of patients treated with HAART all over the world and to use these models to predict how an individual patient will respond to different combinations of drugs. The RDI’s goal was to make available a free treatment-response prediction tool over the Internet.\n\nKey to the success of this approach is the collection of large amounts of data with which to train the models and the use of data from as wide and heterogeneous range of sources as possible to maximise the generalisability of the models’ predictions. In order to achieve this, the RDI set out to involve as many clinics worldwide as possible and to be the single repository for the data required, in an attempt to avoid unnecessary duplication of effort and competition.\n\nAs of October 2013, the RDI has collected data from approximately 110,000 patients from dozens of clinics in more than 30 countries. It is probably the largest database of its kind in the world. The data includes demographic information for the patient, and multiple determinations of the amount of virus in the patient’s bloodstream, CD4 cells counts (a white blood cell critical to the function of the immune system that HIV targets and destroys), genetic code of the patients virus, and details of the drugs that have been used to treat the patient.\n\nThe RDI has used these data to conduct extensive research in order to develop the most accurate system possible for the prediction of treatment response. This research involved the development and comparison of different computational modelling methods including artificial neural networks, support vector machines, random forests and logistical regression.\n\nThe predictions of the RDI’s models have historically correlated well with the actual changes in virus load of patients in the clinic, typically achieving a correlation co-efficient of 0.8 or more.\n\nIn October 2010, following clinical testing in two multinational studies, the RDI made its experimental HIV Treatment Response Prediction System, HIV-TRePS available over the Internet. In January 2011, two clinical studies were published indicating that use of the HIV-TRePS system could lead to clinical and economic benefits. The studies, conducted by expert HIV physicians in the USA, Canada and Italy, showed that use of the system was associated with changes of treatment decision to combinations involving fewer drugs overall, which were predicted to result in better virological responses, suggesting that use of the system could potentially improve patient outcomes and reduce the overall number and cost of drugs used.\n\nRecent models have predicted whether a combination treatment will reduce the level of virus in the patient’s bloodstream to undetectable levels with an accuracy of approximately 80%, significantly better than just using a genotype with rules-based interpretation\n\nAs clinics in resource-limited settings are often unable to afford genotyping, the RDI has developed models that predicted treatment response without the need for a genotype, with only a small loss of accuracy. In July 2011, the RDI made these models available as part of the HIV-TRePS system. This version is aimed particularly at resource-limited settings where genotyping is often not routinely available. The most recent of these models, trained with the largest dataset so far, achieved 80% accuracy, which is comparable to models that use a genotype in their predictions and significantly more accurate than genotyping with rules-based interpretation itself.\n\nHIV-TRePS is now in use in 70 countries as a tool to predict virological response to therapy and avoid treatment failure.\n\nThe system has been expanded to enable physicians to include their local drug costs in the modelling. A recent study of data from an Indian cohort demonstrated that the system was able to identify combinations of three locally available drugs with a higher probability of success than the regimen prescribed in the clinic, including those cases where the treatment used in the clinic failed. Moreover, in all these cases some of the alternatives were less costly than the regimen used in the clinic, suggesting that the system could be not only help avoid treatment failure but also reduce costs.\n\n\n\nCohorts: Peter Reiss and Ard van Sighem (ATHENA, the Netherlands); Julio Montaner and Richard Harrigan (BC Center for Excellence in HIV & AIDS, Canada); Tobias Rinke de Wit, Raph Hamers and Kim Sigaloff (PASER-M cohort, The Netherlands); Brian Agan, Vincent Marconi and Scott Wegner (US Department of Defense); Wataru Sugiura (National Institute of Health, Japan); Maurizio Zazzi (MASTER, Italy); Adrian Streinu-Cercel National Institute of Infectious Diseases Prof.Dr. Matei Balş, Bucharest, Romania; Gerardo Alvarez-Uria (VFHCS, India).\nClinics: Jose Gatell and Elisa Lazzari (University Hospital, Barcelona, Spain); Brian Gazzard, Mark Nelson, Anton Pozniak and Sundhiya Mandalia (Chelsea and Westminster Hospital, London, UK); Lidia Ruiz and Bonaventura Clotet (Fundacion Irsi Caixa, Badelona, Spain); Schlomo Staszewski (Hospital of the Johann Wolfgang Goethe-University, Frankfurt, Germany); Carlo Torti (University of Brescia); Cliff Lane and Julie Metcalf (National Institutes of Health Clinic, Rockville, USA); Maria-Jesus Perez-Elias (Instituto Ramón y Cajal de Investigación Sanitaria, Madrid, Spain); Andrew Carr, Richard Norris and Karl Hesse (Immunology B Ambulatory Care Service, St. Vincent’s Hospital, Sydney, NSW, Australia); Dr Emanuel Vlahakis (Taylor’s Square Private Clinic, Darlinghurst, NSW, Australia); Hugo Tempelman and Roos Barth (Ndlovu Care Group, Elandsdoorn, South Africa), Carl Morrow and Robin Wood (Desmond Tutu HIV Centre, University of Cape Town, South Africa); Luminita Ene (“Dr. Victor Babes” Hospital for Infectious and Tropical Diseases, Bucharest, Romania); Gordana Dragovic (University of Belgrade, Belgrade, Serbia).\nClinical trials: Sean Emery and David Cooper (CREST); Carlo Torti (GenPherex); John Baxter (GART, MDR); Laura Monno and Carlo Torti (PhenGen); Jose Gatell and Bonventura Clotet (HAVANA); Gaston Picchio and Marie-Pierre deBethune (DUET 1 & 2 and POWER 3); Maria-Jesus Perez-Elias (RealVirfen).\n\n"}
{"id": "50845364", "url": "https://en.wikipedia.org/wiki?curid=50845364", "title": "Respirator assigned protection factors", "text": "Respirator assigned protection factors\n\nThe respiratory protective devices (RPD) can protect workers only if their protective properties are adequate to the conditions in the workplace. Therefore, specialists have developed criteria for the selection of proper, adequate respirators, including the Assigned Protection Factors (APF) - the decrease of the concentration of harmful substances in the inhaled air, which (is expected) to be provided with timely and proper use of a certified respirator of certain types (\"design\") by taught and trained workers (after individual selection with a tight-fitting mask and fit testing), when the employer performs an effective respiratory protective device programme.\n\nThe imperfection of technological processes, machines and other equipment can lead to air contamination with harmful substances in the workplace. Protecting of the workers' health in this situation may be achieved with different ways, listed below in order of decreasing of their effectiveness: \nIf the use of these methods is impossible, or if their use did not reduce the concentration of harmful substances to a safe value, workers must use respirators. These respirators must be sufficiently effective, and they should correspond to known or expected conditions at the workplace. However, it is the least effective method of protection; and the reasons for the decrease of their efficiency often are: non-usage of the respirators in the contaminated atmosphere; leakage of unfiltered air through the gaps between the mask and face; and delayed replacement of gas cartridges.\n\nDifferent terms may be used to describe the protective properties of respirators:\nThe term \"Protection Factor PF\" has been used in the U.S., and the term \"Penetration\" was used in the soviet literature from the 1960s.\n\nIn the first half of the 20th century, experts carried out measurements protective properties of respirators in the laboratories. They used different control substances (argon, halogenated hydrocarbon vapour, aerosols of sodium chloride and oil mist, fluorophores, dioctyl phthalate, and others, and they measured their concentrations under the mask, and outside masks (simultaneously). The ratio of the measured concentrations is an indicator of the protective properties of different types of respirators. These measurements showed that if the efficiency of filters is enough high, then the gaps between the mask and the face become the main way of penetration of air contaminations under the mask. \n\nThe shape and size of these gaps is not constant, and depends on many factors (the degree of fit the mask to the face - by shape and size; the correct donning the mask; the mask slippage on the face during the work due to execution of different movements; the design of the mask). The respirator's PF may change dozens of times during of several minutes; and the two average PF (\"that were measured for the same worker in one day; for example - before and after the lunch break\") can differ by more than 12 000 times.\n\nExperts believed that the measurement of protection factors in the laboratory allows them to correctly evaluate, predict the RPD efficiency in the workplace conditions. But after the detection of cases of excessive harmful exposure on employees who used high quality respirators with HEPA particle filters in the nuclear industry of the USA, the experts changed their opinion. Studies have been carried out to measure the protection factors for the various types of respirators - not only in the laboratories, but also at the workplaces. Dozens of such field studies have shown that the performance of serviceable respiratory protective equipment at the workplaces may be significantly less than in laboratory conditions. Therefore, the usage of laboratory results to assess the real efficiency is incorrect; and can lead to a wrong choice of such respirators that can not reliably protect workers.\n\nThe experts used the results of measurements at the laboratories and at the workplaces to develop more completely terminology for description of the respirators' performance; and this terminology has been applied officially, and in the preparation of research results for publication. Specialists began to use different terms to describe the protection factors, which were measured at workplaces with continuous use of respirators; and measured in the workplace when the workers used of respirators intermittently; measured not in the workplace while fit testing; measured in the laboratories under the simulation workplace's conditions; and for the protection factors, that can be expected (\"in most cases\") when the workers properly used the respirators at the workplace.\n\nA significant difference between respirator performance in the laboratories compared to the efficiency at the workplaces not allowed to use the laboratory results to predict the degree of protection offered in practice. And instability of respirators' protective properties (for the same RPD design, and in the same usage conditions) prevented evaluate their efficiency. For solving these problems, scientists Donald Campbell and Steven Lenhart suggested to use the results of measurements of Workplace PF values for development of Assigned (\"expected in practice\") PF values (APF) - as the lower 95% confidence interval of WPF values. The results of measurements of WPF has been used in the development of APF by ANSI (for the recommended standard, that is not mandatory). The same was made during the development of the APF by OSHA (in the development of the standard, that is mandatory for the employer).\n\nResults of measurements of WPF in the US and the UK became the basis for the development of APF for UK standard and for English version of EU standard.\nIn some cases, there was no information on the effectiveness for respirators of specific design (type) in the workplace. This is due to the fact that the measurement of workplace PF is very difficult, time consuming, and expensive work, which was carried out not very often. For these types of respirators experts used the results of WPF measurements of other types of respirators, which are similar. For example, the effectiveness of the Supplied Air Respirators (SARs, with hose) was considered similar to the efficiency of Powered Air Purifying Respirators (PAPRs), if they have the same facepieces and the same air supply mode. Finally, in the absence of this information, specialists could use the results of Simulated WPF measurements; or estimates of competent experts.\n\nMeasurement of workplace protection factors surprisingly revealed the low efficiency of some designs of respirators, and that results have led to a sharp tightening of the requirements for application limits for respirators of such designs.\n\nMeasuring WPF of Powered Air Purifying Respirators (PAPR) with helmets (that is not tight-fitted to the face) showed that the ingress of harmful substances in the inhaled air can be very high (PF = 28 and 42 for two models). It was a surprise, since earlier studies in the laboratory showed that the flow of clean filtered air from the inside to the outside of the helmet prevents ingress of harmful substances under the helmet (PF > 1000). Additional studies have confirmed the result of the first study: the minimum values of the workplace protection factors of 2 models of respirators were 31 and 23; and leakage of unfiltered air achieved 16% in some cases in wind tunnel at 2 m/s air velocity\n\nTherefore, the use of such RPD types was limited 25 PEL in the U.S., and 40 OEL in the UK. \nMeasurement of protection factors of negative pressure full face masks with high-efficiency filters in the laboratory revealed a risk of decrease in protective properties to a small values. Therefore, the use of such respirators has been limited to the values 50 or 100 PEL in the United States. However, the experts in the UK believed that the quality of their masks is higher than American masks, and were allowed to use up to 900 OEL. But the study showed that the value of the protection factor of > 900 has been achieved in practice infrequently. Minimum protection factors of 3 different models of full facepiece respirators were 11, 18 and 26. So, the new standards limit usage of these respirators up to 40 OEL in UK (after this study).\nFit testing of tight-fitting masks of negative-pressure respirators became widely used in US industry in 1980-s. At the beginning, it was thought that the half-mask fit quite well to the worker's face, if during a fit test the protection factor (fit factor) is not less than 10 (later, experts began to use \"safety factor\" = 10 during the fit test; threshold fit factor become 10 × 10 = 100). The widespread use of fit testing in the industry gives professionals optimism, and they allowed to the employers restrict the use of half mask respirators in accordance with the values of worker's personal fit factor (the maximum concentration of pollutants = personal Fit Factor × PEL), but not more than 100 × PEL. However, scientific studies have shown that although such test increases the effectiveness of protection, the risk of leakage of large amounts of unfiltered air is maintained. Furthermore, the studies have shown that non-filtered air under the mask is not uniformly mixed with the filtered air, which leads to large errors in the measurement of the in-facepiece concentration of contaminants, and subsequent calculations of fit factors - the latter is often much smaller than the \"measured\" value. So, specialists recommend not allowed usage negative pressure half mask respirators then harmful substances' concentrations exceeds 10 PEL. Therefore, OSHA standards require to restrict using of half-mask negative-pressure respirators up to 10 PEL after obtaining fit factor greater than or equal to 100 during the mask selection for the worker (they used a safety factor = 10).\n\nThe table lists the APF values for the most common respirator types (for US and UK). \n\n\"US particle filters N95 are similar to P2; and P100 (HEPA) are similar to P3; filtering materials in US N95 filtering facepieces are similar to FFP2.\"\n\nThe difference of the APF for air purifying negative pressure full-facepiece masks are not large. The difference between PAPR with helmets a few more. But measurements showed that the real effectiveness of RPD (at the workplace conditions) is strongly dependent on the conditions of their use, not only from the design, and this partly explains the difference in APF values. The APF for negative pressure half mask respirators are twofold. But this difference cannot be considered separately from recommendations for use of respirators. The use of half-face masks in the US is limited to 10 PEL for the \"worst case\" - work in the polluted atmosphere of 8 hours per day, 40 hours a week. But British experts took into account large experience of the use of negative pressure air purifying RPDs, and they concluded that to achieve continuous wear respirator 8 hours a day is impossible (because of the negative impact on the health of workers). For this reason, they recommend to the employer to give the job to the workers so that they work in the polluted atmosphere not during entire shift, but only a part of the shift. The remaining time the employee needs to work in a non-polluted atmosphere (without the respirator). The fact that the employee is in a non-polluted atmosphere some part of working time provide additional protection of his health, and therefore, the requirements to the efficiency of the respirator may be less stringent.\n\nThe development of the Assigned PF in the United States and Britain were based on measurements of the effectiveness of respirators in the workplace (after statistical processing). Also used opinions of experts, based on the similarity of the respirators with different designs (for example, powered air purifying filtering respirators (PAPR), and a similar supplied air respirators SAR) - provided that the mode and the quantity of air supply, and the facepieces (masks) were the same. Experts in the two countries often used the results of the same studies of WPF (because of their limited number). For example, British standard had been developed with usage of results of 1897 WPF measurements during 31 studies; and 23 of this 31 studies had been conducted in United States.\n\nTherefore, the values of the assigned PF in the US and in the UK are evidence-based; and they are very similar to each other.\n\nStudies of respirator's performance was carried out not very often, and almost all of these studies were conducted in USA (and UK). It is possible that the lack of information about the RPD efficiency in the workplaces, was the reason behind developing these assigned PF in several European countries, whose values differ significantly from the evidence-based values of APFs in the US and UK.\n\nMost European countries (except UK) did not conduct very complex and expensive studies on the effectiveness of respirators in the workplaces, or spent very little of such research. Therefore, it may be that some countries do not take full account of results of foreign researches (that showed a significant difference between the effectiveness of respirators in a laboratory environment; and in applying them in the workplaces). For example, after the study in 1990, the APF value of negative pressure full face masks was reduced from 900 to 40 (1997) in UK. But in other countries, similar research was not carried out; and a similar decrease did not occur.\n\nThe study showed that the three models of full face masks had a significant leakage of unfiltered air through the gaps between the mask and the face. The minimum values of the workplace protection factors (WPF) of each of the three negative pressure full face mask models were 11, 17 and 26. The maximum value of the WPF from one of the models did not exceed 500 no times at all. And for all results together, the WPF was not more than 100 in ~ 30% of the measurements. So, for this reason, the values of the APFs for this RPD type in Germany (400), Finland (500), Italy (400), and Sweden (500), may not fully take into account the lower this type respirators' performance at the workplace compared to the performance in the laboratory (during certification). The same was true for other RPD types and their APF.\n\nState standard in India points to the need to use the workplace protection factors for restricting the permissible use of respirators, but does not set any values of the APFs. The standard also recommends the use of those PFs, which are obtained during the certification (in the laboratories, but not at workplaces). These values greatly exceed the values used in the USA and in the UK.\n\nThe Ukrainian version of the EU standard EN 529 does not set any values of the APFs for the selection of respirator in this country. This document only listed the values of APFs in several European countries (for reference); and declares the inadmissibility of the use of laboratory efficiency for predicting the protective properties at the workplace.\n\nThe APFs are not developed in RF, in South Korea, as well as in many other countries, and selection of respirators is not regulated by its national legislation. This contributes to errors, and the usage of such respirator's types, which are not able to reliably protect the workers due to its design (even at high quality of specific certified models).\n\nUS law obliges the employer to accurately measure air pollution at workplaces. The results of such measurements are used to assess whether short-term inhalation of harmful substances lead to the death of the person, or irreversible and significant deterioration of his health (IDLH concentrations). If concentrations exceed the IDLH, the standard allows the use of only the most reliable respirators - SAR or self-contained breathing apparatus: with pressure-demand air supply in the full facepiece mask ( §(d)(2)).\n\nIf the concentration of harmful substance is less than IDLH, when one must determine the coefficient of air pollution for harmful substance (Hazard Factor), which is equal to the ratio of this concentration to the PEL (TLV, OEL) for the harmful substance. APF of selected respirator type must be equal or exceed the Hazard Factor\n\nIf there are several harmful substances in the workplace air, then the selected respirator must run the following requirement:\n\nC/(APF×PEL) + C/(APF×PEL ) + C/(APF×PEL ) + ... + Cn/(APF×PELn) ≤ 1\n\nwhere C, C ... and Cn - concentrations of harmful substances number 1, 2 ... n; and PEL is the maximum allowable concentration for corresponding harmful substances in the breathing zone.\n\nIf this requirement is not met, the employer needs to choose a different type of respirator, which has a greater APF value.\n\nIn all cases, if employer select respirator with tight-fitting facepiece (full face mask, elastomeric half-mask or quarter-mask, or filtering facepiece respirator), all employee must be fit tested (\"to prevent leakage unfiltered polluted air through gaps between their faces and the tight-fitting masks, which may not match to their faces\"). Appendix A provides a detailed description of this testing.\n\nValues of IDLH concentrations and detailed recommendations for the selection of respirators (and self-rescuers) are available in the NIOSH directory.\n\nISO is developing two international standard types, that governing the certification of respirators; and their selection and application\n\nThe standards governing the selection of respirators, uses the values of APF. But HSE spesialist criticue this documents. These standards are set those values of Assigned Protection Factors that differ from those established in the US and the UK (evidence-based); and these values are set not for specific RPD type, but for any RPD, that met approval requirements:\nThe report concluded - new ISO standards are used insufficiently substantiated APF values, and M. Clayton made recommendation: do not use these values in practice, and to continue work on APF justification for the different types of respirators.\n\n"}
{"id": "18852576", "url": "https://en.wikipedia.org/wiki?curid=18852576", "title": "Saab Information Display", "text": "Saab Information Display\n\nSaab Information Display (SID) later also called \"Saab Car Computer\" (SCC) is the name for various in-car computer systems found on most Saab automobiles beginning in 1985 with the Saab 9000 and followed in 1994 with the Saab 900 NG.\n\nSIDs typically provide functions useful to the driver such as multiple trip odometers, fuel efficiency, estimated range (\"distance to empty\" – DTE), current CD track or radio station, and also brief description of car system failures. SIDs designed prior to the full General Motors takeover of Saab in 2000 were mounted in the center console, usually just above the head unit and below climate control vents with the exception of 9000 models, possibly after the \"facelift\" in the early 1990s.\n\nThe 9000 featured the SID on the standard instrument cluster, with a car pictogram, showing open doors/tailgate/boot (trunk), lamp failures and low oil pressure in red. If there were no warnings, the white pictogram would disappear. To the right of this was the data display in orange with green background data. This constantly showed instantaneous fuel consumption on an orange moving bar display with other parameters to its left such as alternator voltage, battery cranking voltage during start, outside temperature, estimated fuel range and average fuel consumption, cycled through by pressing an INFO button on the clock on the dashboard. Also displayed were engine check warnings. In addition, the analogue electric clock module could be replaced by a digital unit that scrolled SAAB across the orange LED display on startup before showing the time. This display featured digital trip functions such as distance to destination, average speed, estimated arrival time and adjustable speed warning settings, cycled through and set by pressing several buttons and retaining the R (reset) and INFO button for the instrument cluster. When fitted, the automatic climate control (as opposed to simple air conditioning) showed the selected temperature on green LEDs on the ACC unit fitted in the centre console in degrees C or F, selected by a nearby slide switch. Heated rear window (repeated on the main instrument cluster), heated door mirrors, ventilation outlet selections (including rear side window demist) and ACC mode were also indicated on the ACC unit by green LEDs next to the selector buttons (orange for the electrically heated items). Audio functions were displayed on the head unit, also in the centre console, such as FM Radio Station Name where RDS was available (otherwise just the Radio Frequency) on a green LCD. CD track info was similarly displayed on the console mounted single CD player, where fitted, on its own green LCD. The 9000 SID was not noted for pixel failures since it appears not to be LCD based. Where a CD \"pack\" was fitted, it is assumed the track info would be displayed on the head unit. Reference: from personal ownership.\n\nOn 2003–2005 Saab 9-3 models, the SID was temporarily moved to the top of the dashboard just below the windshield, in an enclosed hump facing the driver.\n\nGM began moving SIDs in all Saabs to the electronic instrument cluster, just above the steering wheel, as stock GM navigation and/or audio head units replaced the Saab-designed units in the center console. The 2005 - 2006 Saab 9-2X did not feature an authentic SID, having a less feature-rich Subaru unit instead as it was basically a rebadged 2nd generation Subaru Impreza with some styling changes.\n\nSaab Information Displays from 1994–2003 commonly suffer from an electrical system failure that results in some or all of the on-screen liquid crystal display (LCD) pixel lines becoming invisible, rendering the SID difficult to read or, in some cases, entirely unusable. The problem is most often caused not by defective pixels, but by some or all of the LCD's flexible flat cable (ribbon cable) contacts separating from the main logic board.\n\nSeveral third party vendors offer a service to permanently fix broken SIDs, for a fee.\n\nThe frequency of the problem strongly suggests a design flaw, but Saab/GM has never issued a product recall.\n\n"}
{"id": "1155993", "url": "https://en.wikipedia.org/wiki?curid=1155993", "title": "Sabre (computer system)", "text": "Sabre (computer system)\n\nSabre Global Distribution System, owned by Sabre Holdings, is used by travel agents around the world with more than 400 airlines, 220,000 hotels, 42 car rental brands, 38 rail providers and 17 cruise lines. The Sabre GDS enables companies such as American Airlines to search, price, book, and ticket travel services provided by airlines, hotels, car rental companies, rail providers and tour operators.\n\nSabre Holdings aggregates airlines, hotels, online and offline travel agents and travel buyers. The company is organized into three business units:\n\n\nThe company is headquartered in Southlake, Texas, and has employees in various locations around the world.\n\nSabre Holdings' history starts with SABRE \"(Semi-automated Business Research Environment)\", a computer reservation system which was developed to automate the way American Airlines booked reservations.\n\nIn the 1950s, American Airlines was facing a serious challenge in its ability to quickly handle airline reservations in an era that witnessed high growth in passenger volumes in the airline industry. Before the introduction of SABRE, the airline's system for booking flights was entirely manual, having developed from the techniques originally developed at its Little Rock, Arkansas reservations center in the 1920s. In this manual system, a team of eight operators would sort through a rotating file with cards for every flight. When a seat was booked, the operators would place a mark on the side of the card, and knew visually whether it was full. This part of the process was not all that slow, at least when there were not that many planes, but the entire end-to-end task of looking for a flight, reserving a seat and then writing up the ticket could take up to three hours in some cases, and 90 minutes on average. The system also had limited room to scale. It was limited to about eight operators because that was the maximum that could fit around the file, so in order to handle more queries the only solution was to add more layers of hierarchy to filter down requests into batches.\n\nAmerican Airlines had already attacked the problem to some degree, and was in the process of introducing their new Magnetronic Reservisor, an electromechanical computer, in 1952 to replace the card files. This computer consisted of a single magnetic drum, each memory location holding the number of seats left on a particular flight. Using this system, a large number of operators could look up information simultaneously, so the ticket agents could be told over the phone whether a seat was available. On the downside, a staff member was still needed at each end of the phone line, and actually handling the ticket still took considerable effort and filing. Something much more highly automated was needed if American Airlines was going to enter the jet age, booking many times more seats.\n\nIt was during the testing phase of the Reservisor that a high-ranking IBM salesman, Blair Smith, was flying on an American Airlines flight from Los Angeles back to IBM in New York City in 1953. He found himself sitting next to American Airlines president C. R. Smith. Noting that they shared a family name, they began talking.\n\nJust prior to this chance meeting, IBM had been working with the United States Air Force on their Semi Automatic Ground Environment (SAGE) project. SAGE used a series of large computers to coordinate the message flow from radar sites to interceptors, dramatically reducing the time needed to direct an attack on an incoming bomber. The system used teleprinter machines located all around the world to feed information into the system, which then sent orders back out to teleprinters located at the fighter bases. It was one of the first online systems.\n\nIt was not lost on either man that the basic idea of the SAGE system was perfectly suited to American Airlines' booking needs. Teleprinters would be placed at American Airlines' ticketing offices to send in requests and receive responses directly, without the need for anyone on the other end of the phone. The number of available seats on the aircraft could be tracked automatically, and if a seat was available the ticket agent could be notified instantly. Booking simply took one more command, updating the availability and even printing out the ticket for them.\n\nOnly 30 days later IBM sent a research proposal to American Airlines, suggesting that they really study the problem and see if an \"electronic brain\" could actually help. They set up a team consisting of IBM engineers led by John Siegfried and a large number of American Airlines' staff led by Malcolm Perry, taken from booking, reservations and ticket sales, calling the effort the \"Semi-Automated Business Research Environment\", or SABRE.\n\nA formal development arrangement was signed in 1957, and the first experimental system went online in 1960, based on two IBM 7090 mainframes in a new data center located in Briarcliff Manor, New York. The system was a success. Up until this point, it had cost the astonishing sum of $40 million to develop and install (about $350 million in 2000 dollars). The SABRE system by IBM in the 1960s was specified to process a very large number of transactions, such as handling 83,000 daily phone calls. The system took over all booking functions in 1964, at which point the name had changed to the more familiar SABRE.\n\nIn 1972, the system was migrated to IBM System/360 systems in a new underground location in Tulsa, Oklahoma. Max Hopper joined American Airlines in 1972 as director of Sabre, and pioneered its use. Originally used only by American Airlines, the system was expanded to travel agents in 1976.\n\nWith SABRE up and running, IBM offered its expertise to other airlines, and soon developed Deltamatic for Delta Air Lines on the IBM 7074, and PANAMAC for Pan American World Airways using an IBM 7080. In 1968, they generalized their work into the PARS (Programmed Airline Reservation System), which ran on any member of the IBM System/360 family and thus could support any sized airline. The operating system component of PARS evolved into ACP (\"Airlines Control Program\"), and later to TPF (\"Transaction Processing Facility\"). Application programs were originally written in assembly language, later in SabreTalk, a proprietary dialect of PL/I, and now in C and C++.\n\nBy the 1980s, SABRE offered airline reservations through the CompuServe Information Service, and the Prodigy Internet Service GEnie under the Eaasy SABRE brand. This service was extended to America Online in the 1990s.\n\nAmerican and Sabre separated on March 15, 2000. Sabre had been a publicly traded corporation, Sabre Holdings, stock symbol TSG on the New York Stock Exchange until taken private in March 2007. The corporation introduced the new logo and changed from the all-caps acronym \"SABRE\" to the mixed-case \"Sabre Holdings\", when the new corporation was formed. The Travelocity website, introduced in 1996, was owned by Sabre Holdings. Travelocity was acquired by Expedia in January 2015. Sabre Holdings' three remaining business units, Sabre Travel Network, Sabre Airline Solutions and Sabre Hospitality, today serves as a global travel technology company. The system connects more than travel agents and millions of travelers with more than 400 airlines, 90,000 hotels, 30 car-rental companies, 200 tour operators, and dozens of railways, ferries and cruise lines.\n\nA 1982 study by American Airlines found that travel agents selected the flight appearing on the first line more than half the time. Ninety-two percent of the time, the selected flight was on the first screen. This provided a huge incentive for American to manipulate its ranking formula, or even corrupt the search algorithm outright, to favor American flights.\n\nAt first this was limited to juggling the relative importance of factors such as the length of the flight, how close the actual departure time was to the desired time, and whether the flight had a connection, but with each success American became bolder. In late 1981, New York Air added a flight from La Guardia to Detroit, challenging American in an important market. Before long, the new flights suddenly started appearing at the bottom of the screen. Its reservations dried up, and it was forced to cut back from eight Detroit flights a day to none.\n\nOn one occasion, Sabre deliberately withheld Continental's discount fares on 49 routes where American competed. A Sabre staffer had been directed to work on a program that would automatically suppress any discount fares loaded into the computer system.\n\nCongress investigated these practices and in 1983 Bob Crandall, president of American, was the most vocal supporter of the systems. \"The preferential display of our flights, and the corresponding increase in our market share, is the competitive \"raison d'être\" for having created the system in the first place,\" he told them. Unimpressed, in 1984 the United States government outlawed screen bias.\n\nEven after biases were eliminated, travel agents using the system leased and serviced by American were significantly more likely to choose American over other airlines. The same was true of United and its Apollo system. The airlines referred to this phenomenon as the \"halo\" effect.\n\nThe fairness rules were eliminated or allowed to expire in 2010. By then, none of the major distribution systems was majority owned by the airlines.\n\nIn 1987 Sabre's success of selling to European travel agents was inhibited by the refusal of big European carriers led by British Airways to grant the system ticketing authority for their flights even though Sabre had obtained IATA Billing and Settlement Plan (BSP) clearance for the UK in 1986. American brought High Court action which alleged that after the arrival of Sabre on its doorstep British Airways immediately offered financial incentives to travel agents who continued to use Travicom and would tie any override commissions to it. Travicom was created by Videcom, British Airways and British Caledonian and launched in 1976 as the world's first multi-access reservations system based on Videcom technology which eventually became part of Galileo UK. It connected 49 subscribing international airlines (including British Airways, British Caledonian, TWA, Pan American World Airways, Qantas, Singapore Airlines, Air France, Lufthansa, SAS, Air Canada, KLM, Alitalia, Cathay Pacific and JAL) to thousands of travel agents in the UK. It allowed agents and airlines to communicate via a common distribution language and network, handling 97% of UK airline business trade bookings by 1987.\n\nBritish Airways eventually bought out the stakes in Travicom held by Videcom and British Caledonian, to become the sole owner. Although Sabre's vice-president in London, David Schwarte, made representations to the U.S. Department of Transportation and the British Monopolies Commission, British Airways defended the use of Travicom as a truly non-discriminatory system in flight selection because an agent had access to some 50 carriers worldwide, including Sabre, for flight information.\n\n\n\n"}
{"id": "28645342", "url": "https://en.wikipedia.org/wiki?curid=28645342", "title": "Safir Office Machines Museum", "text": "Safir Office Machines Museum\n\nThe Safir Office Machines Museum is a private museum located in Tehran, Iran. It was founded in 2008 by Frashad Kamalkhani, the museum owner. It includes a collection of early office machines.\n\nThere are more than 80 unique and early devices such as typewriters, check writers, pencil sharpeners, duplicators, calculators, cash registers, telegraph equipment, telephones, etc., in the collection.\n\n\n"}
{"id": "706357", "url": "https://en.wikipedia.org/wiki?curid=706357", "title": "Seikosha", "text": "Seikosha\n\n"}
{"id": "43427948", "url": "https://en.wikipedia.org/wiki?curid=43427948", "title": "Superintelligence: Paths, Dangers, Strategies", "text": "Superintelligence: Paths, Dangers, Strategies\n\nSuperintelligence: Paths, Dangers, Strategies is a 2014 book by the Swedish philosopher Nick Bostrom from the University of Oxford. It argues that if machine brains surpass human brains in general intelligence, then this new superintelligence could replace humans as the dominant lifeform on Earth. Sufficiently intelligent machines could improve their own capabilities faster than human computer scientists, and the outcome could be an existential catastrophe for humans.\n\nBostrom's book has been translated into many languages and is available as an audiobook.\n\nIt is unknown whether human-level artificial intelligence will arrive in a matter of years, later this century, or not until future centuries. Regardless of the initial timescale, once human-level machine intelligence is developed, a \"superintelligent\" system that \"greatly exceeds the cognitive performance of humans in virtually all domains of interest\" would follow surprisingly quickly, possibly even instantaneously. Such a superintelligence would be difficult to control or restrain.\n\nWhile the ultimate goals of superintelligences can vary greatly, a functional superintelligence will spontaneously generate, as natural subgoals, \"instrumental goals\" such as self-preservation and goal-content integrity, cognitive enhancement, and resource acquisition. For example, an agent whose sole final goal is to solve the Riemann hypothesis (a famous unsolved, mathematical conjecture) could create, and act upon, a subgoal of transforming the entire Earth into some form of computronium (hypothetical \"programmable matter\") to assist in the calculation. The superintelligence would proactively resist any outside attempts to turn the superintelligence off or otherwise prevent its subgoal completion. In order to prevent such an existential catastrophe, it might be necessary to successfully solve the \"AI control problem\" for the first superintelligence. The solution might involve instilling the superintelligence with goals that are compatible with human survival and well-being. Solving the control problem is surprisingly difficult because most goals, when translated into machine-implementable code, lead to unforeseen and undesirable consequences.\n\nThe book ranked #17 on the \"New York Times\" list of best selling science books for August 2014. In the same month, business magnate Elon Musk made headlines by agreeing with the book that artificial intelligence is potentially more dangerous than nuclear weapons.\nBostrom’s work on superintelligence has also influenced Bill Gates’s concern for the existential risks facing humanity over the coming century. In a March 2015 interview with Baidu's CEO, Robin Li, Gates claimed he would \"highly recommend\" \"Superintelligence\".\n\nThe science editor of the \"Financial Times\" found that Bostrom’s writing \"sometimes veers into opaque language that betrays his background as a philosophy professor\" but convincingly demonstrates that the risk from superintelligence is large enough that society should start thinking now about ways to endow future machine intelligence with positive values.\nA review in \"The Guardian\" pointed out that \"even the most sophisticated machines created so far are intelligent in only a limited sense\" and that \"expectations that AI would soon overtake human intelligence were first dashed in the 1960s\", but finds common ground with Bostrom in advising that \"one would be ill-advised to dismiss the possibility altogether\".\n\nSome of Bostrom's colleagues suggest that nuclear war presents a greater threat to humanity than superintelligence, as does the future prospect of the weaponisation of nanotechnology and biotechnology. \"The Economist\" stated that \"Bostrom is forced to spend much of the book discussing speculations built upon plausible conjecture... but the book is nonetheless valuable. The implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect of actually doing so seems remote.\" Ronald Bailey wrote in the libertarian \"Reason\" that Bostrom makes a strong case that solving the AI control problem is the \"essential task of our age\". According to Tom Chivers of \"The Daily Telegraph\", the book is difficult to read, but nonetheless rewarding.\n"}
{"id": "27751228", "url": "https://en.wikipedia.org/wiki?curid=27751228", "title": "Tactical reconnaissance and counter-concealment-enabled radar", "text": "Tactical reconnaissance and counter-concealment-enabled radar\n\nThe United States Army tactical reconnaissance and counter-concealment-enabled radar – TRACER is a mid-range, long wavelength synthetic aperture radar (SAR) system that provides all-weather persistent surveillance developed by the United States Army Communications-Electronics Research, Development and Engineering Center's Intelligence and Information Warfare Directorate. The TRACER program began in April 2007 and has been integrated onto a C-12 Huron. In the fall of 2010, it is planned to test TRACER on NASAs unmanned Predator-B IKHANA, and later on Air Warrior.\n\nDue to its decreased size, weight and power consumption compared to the predecessor foliage-penetrating radar program, or FOPEN which has been in the field since the late-90s, TRACER operates on manned and unmanned platforms and produces images on board in less than five minutes. As a follow-on to FOPEN, the TRACER system can be tailored to specific missions by providing a variety of SAR images including strip maps and spotlight and circle images.\n"}
{"id": "16576231", "url": "https://en.wikipedia.org/wiki?curid=16576231", "title": "The Green Grid", "text": "The Green Grid\n\nThe Green Grid is a nonprofit, industry consortium of end-users, policy-makers, technology providers, facility architects, and utility companies collaborating to improve the resource efficiency of data centers. \nAt one time it had more than 175 member companies.\n\nAs business demands increase, so does the number of data center facilities which house a rising amount of IT equipment. Data center managers run into resource limits on electrical power, cooling, and space.\n\nAn initial announcement in April 2006 included members Advanced Micro Devices, Dell, Hewlett-Packard, IBM, and Sun Microsystems.\nThey were soon joined by Intel and Microsoft.\nBy February 26, 2007, APC by Schneider Electric, Rackable Systems, SprayCool (later part of Parker Hannifin), and VMware had joined the effort, and a meeting in April 2007 was announced.<br>\nIn March, 2011, the Green Grid proposed a new sustainability metric, Water Usage Effectiveness (WUE), which attempts to take into account the amount of water used by data centers in their cooling systems\n\nIn 2015, the Board of Directors had the following members:\n\nIn 2007, the Board of Directors had the following members:\n\n\n"}
{"id": "1158081", "url": "https://en.wikipedia.org/wiki?curid=1158081", "title": "Universal design", "text": "Universal design\n\nUniversal design is the design of buildings, products or environments to make them accessible to all people, regardless of age, disability or other factors.\n\nThe term \"universal design\" was coined by the architect Ronald Mace to describe the concept of designing all products and the built environment to be aesthetic and usable to the greatest extent possible by everyone, regardless of their age, ability, or status in life. However, it was the work of Selwyn Goldsmith, author of \"Designing for the Disabled\" (1963), who really pioneered the concept of free access for people with disabilities. His most significant achievement was the creation of the dropped curb – now a standard feature of the built environment.\n\nUniversal design emerged from slightly earlier barrier-free concepts, the broader accessibility movement, and adaptive and assistive technology and also seeks to blend aesthetics into these core considerations. As life expectancy rises and modern medicine increases the survival rate of those with significant injuries, illnesses, and birth defects, there is a growing interest in universal design. There are many industries in which universal design is having strong market penetration but there are many others in which it has not yet been adopted to any great extent. Universal design is also being applied to the design of technology, instruction, services, and other products and environments.\n\nCurb cuts or sidewalk ramps, essential for people in wheelchairs but also used by all, are a common example. Color-contrast dishware with steep sides that assists those with visual or dexterity problems are another. There are also cabinets with pull-out shelves, kitchen counters at several heights to accommodate different tasks and postures, and, amidst many of the world's public transit systems, low-floor buses that \"kneel\" (bring their front end to ground level to eliminate gap) and/or are equipped with ramps rather than on-board lifts.\n\nThe Center for Universal Design at North Carolina State University expounds the following principles: \n\nEach principle above is succinctly defined and contains a few brief guidelines that can be applied to design processes in any realm: physical or digital.\n\nThese principles are broader than those of accessible design and barrier-free design.\n\nIn 2012, The Center for Inclusive Design and Environmental Access at The University at Buffalo expanded definition of the principles of universal design to include social participation and health and wellness. Rooted in evidence based design, the 8 goals of universal design were also developed.\n\n\nThe first four goals are oriented to human performance: anthropometry, biomechanics, perception, cognition. Wellness bridges human performance and social participation. The last three goals addresses social participation outcomes. The definition and the goals are expanded upon in the textbook \"Universal Design: Creating Inclusive Environments.\" \n\n\nIn 1960, specifications for barrier free design were published. It was a compendium of over 11 years of disability ergonomic research. In 1961, the specifications became the first Barrier Free Design standard called the American National Standard, A1171.1 was published. It was the first standard to present the criteria for designing facilities and programs for the use of disabled individuals. The research started in 1949 at the University of Illinois Urbana-Champaign and continues to this day. The principal investigator is Dr. Timothy Nugent (his name is listed in the front of the 1961, 1971, 1980 standard). In 1949 Dr. Nugent also started the National Wheelchair Basketball Association. This ANSI A117.1 standard was adopted by the US federal government General Services Administration under 35 FR 4814 - 3/20/70, 39 FR 23214 - 6/27/74, 43 FR 16478 ABA- 4/19/78, 44 FR 39393 7/6/79, 46 FR 39436 8/3/81, in 1984 for UFAS and then in 1990 for ADA. The archived research documents are at the International Code Council (ICC) - ANSI A117.1 division. Dr. Nugent made presentations around the globe in the late 50's and 60's presenting the concept of independent functional participation for individuals with disabilities through program options and architectural design.\n\nAnother comprehensive publication by the Royal Institute of British Architects published three editions 1963, 1967, 1976 and 1997 of Designing for the Disabled by Selwyn Goldsmith UK. These publications contain valuable empirical data and studies of individuals with disabilities. Both standards are excellent resources for the designer and builder.\n\nDisability ergonomics should be taught to designers, engineers, non-profits executives to further the understanding of what makes an environment wholly tenable and functional for individuals with disabilities.\n\nIn October 2003, representatives from China, Japan, and South Korea met in Beijing and agreed to set up a committee to define common design standards for a wide range of products and services that are easy to understand and use. Their goal is to publish a standard in 2004 which covers, among other areas, standards on containers and wrappings of household goods (based on a proposal from experts in Japan), and standardization of signs for public facilities, a subject which was of particular interest to China as it prepared to host the 2008 Summer Olympics.\n\nThe International Organization for Standardization, the European Committee for Electrotechnical Standardization, and the International Electrotechnical Commission have developed:\n\nThe term Design for All (DfA) is used to describe a design philosophy targeting the use of products, services and systems by as many people as possible without the need for adaptation. \"Design for All is design for human diversity, social inclusion and equality\" (EIDD Stockholm Declaration, 2004). According to the European Commission, it \"encourages manufacturers and service providers to produce new technologies for everyone: technologies that are suitable for the elderly and people with disabilities, as much as the teenage techno wizard.\" The origin of Design for All lies in the field of barrier free accessibility for people with disabilities and the broader notion of universal design.\n\n\"Design for All\" has been highlighted in Europe by the European Commission in seeking a more user-friendly society in Europe. Design for All is about ensuring that environments, products, services and interfaces work for people of all ages and abilities in different situations and under various circumstances.\n\nDesign for All has become a mainstream issue because of the aging of the population and its increasingly multi-ethnic composition. It follows a market approach and can reach out to a broader market. Easy-to-use, accessible, affordable products and services improve the quality of life of all citizens. Design for All permits access to the built environment, access to services and user-friendly products which are not just a quality factor but a necessity for many aging or disabled persons. Including Design for All early in the design process is more cost-effective than making alterations after solutions are already in the market. This is best achieved by identifying and involving users (\"stakeholders\") in the decision-making processes that lead to drawing up the design brief and educating public and private sector decision-makers about the benefits to be gained from making coherent use of Design (for All) in a wide range of socio-economic situations.\n\nThe following examples of Designs for All were presented in the book \"Diseños para todos/Designs for All\" published in 2008 by Optimastudio with the support of Spain's Ministry of Education, Social Affairs and Sports (IMSERSO) and CEAPAT:\nOther useful items for those with mobility limitations:\n\nDesign for All criteria are aimed at ensuring that everyone can participate in the Information society. The European Union refers to this under the terms eInclusion and eAccessibility. A three-way approach is proposed: goods which can be accessed by nearly all potential users without modification or, failing that, products being easy to adapt according to different needs, or using standardized interfaces that can be accessed simply by using assistive technology. To this end, manufacturers and service providers, especially, but not exclusively, in the Information and Communication Technologies (ICT), produce new technologies, products, services and applications for everyone.\n\nIn Europe, people have joined in networks to promote and develop Design for All:\n\n\n building modification consists of modifying buildings or facilities so that they can be used by people who are disabled or have physical impairments. The term is used primarily in Japan and non-English speaking countries (e.g. German: Barrierefreiheit; Finnish: Esteettömyys), while in English-speaking countries, terms such as \"accessibility\" and \"handicapped accessible\" dominate in regular everyday use. An example of barrier-free design would be installing a ramp for wheelchairs alongside or in place of steps. In the case of new buildings, however, the idea of barrier free modification has largely been superseded by the concept of universal design, which seeks to design things from the outset to support easy access.\n\nFreeing a building of barriers means:\n\nBarrier free is also a term that applies to handicap accessibility in situations where legal codes such as the Americans with Disabilities Act of 1990 Guidelines don't make specifications.\n\nAn example of a country that has sought to implement barrier-free accessibility in housing estates is Singapore. Within five years, all public housing estates in the country, all of 7,800 blocks of apartments, have benefited from the program.\n\n\nU.S. Department of Education's National Institute on Disability and Rehabilitation Research (NIDRR) funds the Rehabilitation Engineering Research Center (RERC) on Universal Design in the Built Environment. The current recipient is the Center for Inclusive Design and Environmental Access at the University at Buffalo.\n\n"}
{"id": "13353520", "url": "https://en.wikipedia.org/wiki?curid=13353520", "title": "Urea nitrate", "text": "Urea nitrate\n\nUrea nitrate is a fertilizer-based high explosive that has been used in improvised explosive devices in Afghanistan, Pakistan, Iraq, and various other terrorist acts elsewhere in the world, like the 1993 World Trade Center bombings. It has a destructive power similar to better-known ammonium nitrate explosives, with a velocity of detonation between and .\n\nUrea nitrate is produced in one step by reaction of urea with nitric acid. This is an exothermic reaction, so steps must be taken to control the temperature.\n\nUrea nitrate explosions may be initiated using a blasting cap.\n\nUrea contains a carbonyl group. The more electronegative oxygen atom pulls electrons away from the carbon forming a greater electron density around the oxygen, giving the oxygen a partial negative charge and forming a polar bond. When nitric acid is presented, it ionizes. A hydrogen cation contributed by the acid is attracted to the oxygen and forms a covalent bond [electrophile H]. The electronegative NO ion then is attracted to the positive hydrogen ion. This forms an ionic bond and hence the compound urea nitrate.\n\nThe compound is favored by many amateur explosive enthusiasts as a principal explosive for use in larger charges. In this role it acts as a substitute for ammonium nitrate based explosives. This is due to the ease of acquiring the materials necessary to synthesize it, and its greater sensitivity to initiation compared to ammonium nitrate based explosives.\n\n"}
{"id": "53509742", "url": "https://en.wikipedia.org/wiki?curid=53509742", "title": "World of Knowledge", "text": "World of Knowledge\n\nWorld of Knowledge is a global Wikipedia inspired commercial project to enhance learning developed by company WOKcraft based on a community of users creating multiple choice questions for free public use. The abbreviation for the project is WOK and used to market quiz game mobile apps in a social network called the Knowledge Network.\n\nIn all WOK apps the connection to Wikipedia is used by in coexistence with the multiple choice questions and Wikipedia content of articles and images using Wikipedia live content.\n\nWOK develops mobile apps in a ecosystem that are connected with the same scoresystem and with Wikipedia articles using the open API from Wikimedia Commons.\n\nThe WOK project was started by the founder Erik Bolinder in 2009. During several years he planned to create a system for social network based on learning and knowledge. After extensive travel around the World a team was found in Cairo for developing the first WOK related product called Wokcraft.com, WOKquiz and WOKbattle in 2012-2013. All initial projects were cancelled and redeveloped in 2014 when the app Quiz King was launched.\n\nIn 2015 the app WOKwiki was developed and launched on the 15th Anniversary of Wikipedia. In 2017 the app was renamed to WikiMaster. In late 2016 the app WikiFlip was launched as the third app in the WOK ecosystem of quiz apps. All apps have been developed for Android and iOS mobile smartphones.\n\nAll apps in WOK are based on the database of multiple choice questions with four alternatives, one being factually correct and three alternatives being incorrect. No direct relation to Wikipedia articles on questions exists other than keywords that are the same as Wikipedia articles.\n\nIn the WikiMaster app a full Wikipedia access is to be found in 11 languages. Quizzes exist in 4 languages, English being the largest with 300000 questions indexed.\n\nWOKcraft has also developed an API that could be used in Wikipedia making all WOK questions available in Wikipedia without the need for access to apps from WOKcraft.\n\nCritics among the Wikipedia community project point to the commercial interests the WOK project based on which is not in line with the Wikipedia Foundation principles being a nonprofit organization.\n\nEnhancers to the WOK project point to the fact that learning is in coexistence with control questions in text books at the end of chapters and this would be useful to Wikipedia users if accessed and that WOK has been free to use without any advertising or subscription model just as Wikipedia. WOK has developed an API for the Wikipedia community to implement free public access to the questions in WOK so Wikipedia users could use questions in Wikipedia articles.\n\nThe crowd sourced system of content used by Wikipedia articles is similar in WOK. All questions in WOK can be traced to a log file of creators, date and content. Wokers correct questions that are incorrect in spelling or content and are not fact based with a reliable source. All users of the WOK community can create and send questions in the WikiMaster app. \nDuring 2017 WOK plans to enlarge the existing database of questions currently available in English, Arabic, Swedish and French to Hindu, Portuguese and Spanish.\n\nWikiMaster app was launched as WOKwiki on 15 January 2016 in Google Play and AppStore. In early 2017 the app changed name to WikiMaster. In WikiMaster you have access to Wikipedia in 11 languages. All Wikipedia articles can be found by a search function using the API provided by Wikipedia. On top of a Wikipedia article a function of quiz is added. If questions exist in WOK database and are indexed with same keyword or tag as the name of the Wikipedia article by any user the question is available in the Wikipedia article and a quiz exist to this Wikipedia article. If no questions are indexed, no quiz is available. Only the members of WOK add questions and tags. No bot or automatic procedure is used.\n\nOn all Wikipedia articles questions can be created with a five step by step procedure. A question created can be tagged by any other woker enhance the questions in other related articles on a broad and granular level. A question about a football player can be adequately tagged to the Wikipedia article Sport, Associated Football, a club in which the football player play in, a championship related to the question as well as the article about the football player him/herself and so on. The usefulness of WOK is therefore similar to Wikipedia over time in sense the more questions on top of Wikipedia articles, the more useful for the community finding relevant content to be quizzed in for educational purpose like learning a subject in education. Questions can have images attached from Wikimedia Commons. No other images can be uploaded to questions, thus enhance the interconnection with Wikipedia.\n\nAll quiz in WikiMaster can be played alone by the registered user or played with other wokers in a challenge. Challenges are independent of time for each woker to play. All WOKer get points called WOkbits based on correct questions, created questions, challenge wins and alerts. And notifications of events as common in social media apps. All WOKbits counts in a universal score-system in all apps. A WikiMaster can be reached on allocated WOKbits on article level, in a city, country or the World. And on different time periods.\n\nWikiFlip is an app developed by WOKcraft in the end of 2016 aiming to use the images uploaded to Wikipedia Commons and used as a part of questions in WOK added in WikiMaster. With over 80000 questions in database with images associated from Wikimedia Commons, WikiFlip user can search any Wikipedia article and find questions in WOK related and take them by a clic on the image.\n\nUsing Wikimedia Commons images uploaded by users in Wikipedia community and questions created by the WOK community WikiFlip benefit the learning process by many students in need for learning in a new way. WOK claim to have \"released the images captured by the Wikipedia text\". Any answered flips can be reviewed and tags associated with the question can be flipped or the Wikipedia article opened in WikiMaster.\n\nWOKers can reach as many correct questions flipped as possible in a row.\n\nQuiz King is the first app developed by WOK that is still in AppStore and Google Play in 2014. The quiz game is based on questions from the WOK community. Players play 5 rounds with four questions from a main category, chosen out of 22 categories. Each player chose a category and answer same questions in the round. The quicker correct answer is given, the more WOKbits is given.\n\n"}
{"id": "3045866", "url": "https://en.wikipedia.org/wiki?curid=3045866", "title": "Zytel", "text": "Zytel\n\nZytel is a trademark owned by DuPont and used for a number of different high strength, abrasion and impact resistant thermoplastic polyamide formulations of the family more commonly known as nylon. The Zytel product line is based mostly on nylon 66, but also includes grades based on nylon 6 as a matrix, long chain nylons such as nylon 610 (if based on at least one renewable monomer they are branded Zytel RS), and copolymers including a transparent resin called Zytel 330. Resins based on polyphthalamides are branded 'Zytel HTN'. The Zytel product range takes advantage of the fact that nylons are one of the most compatible polymers with modifiers and so offers grades with varying degrees of fiberglass, from 13% to 60%, (to increase stiffness and strength), rubber toughened resins, flame retarded grades. Nylon resins with mineral reinforcement are branded 'Minlon'.\nThe properties of Zytel will vary with the specific formulation. Formulation Zytel HTN 35% Glass Reinforced Resin, consisting of 35% glass fiber by weight, has a tensile strength of around 30kpsi and a flexural modulus of 1500kpsi under room temperature conditions. Zytel also offers good chemical resistance to common chemicals such as motor oil, transmission fluid, and methanol, and shows little thermal expansion. Other additives or treatments may be used to increase toughness, wear resistance, and temperature tolerance.\n\n\n"}
