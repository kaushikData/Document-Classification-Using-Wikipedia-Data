{"id": "3177762", "url": "https://en.wikipedia.org/wiki?curid=3177762", "title": "6b/8b encoding", "text": "6b/8b encoding\n\nIn telecommunications, 6b/8b is a line code that expands 6-bit codes to 8-bit symbols for the purposes of maintaining DC-balance in a communications system.\n\nEach 8-bit output symbol contains 4 zero bits and 4 one bits, so the code can, like a parity bit, detect all single-bit errors.\n\nThe number of binomial coefficient 8-bit patterns with 4 bits set is formula_1 = 70. Further excluding the patterns codice_1 and codice_2, this allows 68 coded patterns: 64 data codes, plus 4 additional control codes.\n\nThe 64 possible 6-bit input codes can be classified according to their disparity, the number of 1 bits minus the number of 0 bits:\nThe 6-bit input codes are mapped to 8-bit output symbols as follows:\nObviously, no data symbol contains more than four consecutive matching bits, and because the patterns codice_1 and codice_2 are excluded, no data symbol begins or ends with more than three identical bits.\nThus, the longest run of identical bits that will be produced is 6. (I.e. this is a (0,5) RLL code, with a worst-case running disparity of +3 to −3.)\n\nAny occurrence of 6 consecutive identical bits constitutes a comma sequence or sync mark or syncword; it identifies the symbol boundaries precisely.\nThose 6 bits straddle the inter-symbol boundary with exactly 3 of those identical bits at the end of one symbol, and 3 of those identical bits at the start of the following next symbol.\n\n\n"}
{"id": "5116225", "url": "https://en.wikipedia.org/wiki?curid=5116225", "title": "Advanced Mezzanine Card", "text": "Advanced Mezzanine Card\n\nAdvanced Mezzanine Cards are printed circuit boards (PCBs) that follow a specification of the PCI Industrial Computers Manufacturers Group (PICMG), with more than 100 companies participating. \n\nKnown as AdvancedMC or AMC, the official specification designation is AMC.\"x\" (see below). At the early days AMC is targeted to requirements for the next generation of \"carrier grade\" communications equipment. Nowadays it is used in nearly all kind of equipment demanding high flexibility, modularity and bandwidth. \n\nAMC are designed to work as standalone module, as hot pluggable module on any carrier card (base boards and system carrier boards in AdvancedTCA Systems) but also to plug as hot pluggable board into a backplane directly as defined by MicroTCA specification.\n\nThe AMC standard differs from other mezzanine card standards such as PCI Mezzanine Card (PMC), PCIexpress Mezzanine Card XMC and FMC – FPGA Mezzanine Card by the 0 degree instead of 90 degree orientation of its connector enabling hot plug of the AMC.\n\nAn AMC card can use proprietary LVDS-based signaling, or one of the following AMC specifications:\n\nThere are six types of AMC cards (\"Module\") available. A \"Full-size\" Module is the most common, allowing up to 23.25 mm high components (from centerline of PCB). A \"Mid-size\" Module allows component heights maxed at 11.65 to 14.01 mm (depending on board location). A \"Compact\" Module allows only 8.18 mm. \n\nPicture of PCB form factors (contacted PICMG to add photo)\n\nTo use AMCs in ATCA-systems a special carrier card known as \"hybrid\" or \"cutaway\" carrier is required to hold one Full-size Module or two Compact-size (see connectors below). Each height is paired with a width, \"single\" or \"double\", describing how many carrier slots the board fills. A double width card allows more component space, but does not provide any additional power or bandwidth because it only uses a single connector.\n\nThe pinout of the AMC connector on an ATCA-AMC carrier or Motherboard is fairly complex, with up to 170 traces. There are four different lengths the traces can be, which allows hot swapping by knowing in advance which traces will become active in which order upon insertion. To help reduce cost for mass production, a card may only require the traces on one side (pins 1 to 85). The possibility of using only half the pin locations, combined with various height combinations, results in four different connector types that are available on the carrier card:\nBay sizes:\nThe AdvancedMC card is considered powerful enough that there are situations where the processing functionality is the only requirement. The MicroTCA standard is targeted at supplying a COTS chassis that will allow AMC cards to function without any AdvancedTCA carrier card. The function of the ATCA carrier board and of the ATCA shelf manager are concentrated on one board, which is called the MicroTCA Carrier Hub (MCH). On July 6, 2006 MicroTCA R1.0 was approved. Since this approval, companies like Advantech, Kontron, N.A.T., Annapolis Micro Systems, VadaTech Inc. , have launched AMC and MCH products.\n\nVersions of MicroTCA with fewer AdvancedMC card slots are informally known as \"NanoTCA\" and \"PicoTCA\".\n\n"}
{"id": "10933021", "url": "https://en.wikipedia.org/wiki?curid=10933021", "title": "Air caster", "text": "Air caster\n\nAn air caster is a pneumatic lifting device used to move heavy loads on flat, non-porous surfaces. Its operation is similar to a hovercraft, as it uses a thin layer of air as a way to float a very small distance off the ground. Compressed air enters an airbag shaped like a torus, and when the bag is filled it creates an airtight seal with the ground, and forces more air into the center of torus, eventually causing the air to flow over the bag and to raise the load above the ground.\n\nThe compressed air is forced under the airbag, pushing it and the load less than a millimeter off the ground.\n\nBecause air casters are virtually friction-free, the force required to move them is very low, often around one to five lbs. per 1,000 lbs. of weight. Air casters are also omni-directional, meaning they move with equal ease in any direction.\n\nAir casters require a smooth, non-porous surface in order to maintain lift and operate properly. Cracks and other surface defects can interrupt the proper flow of air, causing the air cushion to dissipate and thus lose its ability to lift. This limits their application to fields like manufacturing, where such surfaces are abundant. Also necessary is connection to a system which provides compressed air, as well as a power supply.\n\nUsually, standard concrete warehouse or factory floors provide an adequate surface. Large cracks and expansion joints may need to be filled in. Overlay material can provide options for moving over rough surfaces. (Example: One can use an overlay material to move onto an area of broom finished concrete outside. The overlay material can be sheet metal or a heavy gauge plastic sheet material.)\n\nThe air caster (originally called the air bearing) was invented by Harry A Mackie in December 1961 as the “pendant air bearing load supporting device” for General Motors Corporation as an alternative to the overhead crane, which ran opposed to a rail, creating friction free movement. In November 1963, Mackie, on behalf of General Motors Corporation, filed a patent for an “air bearing support with automatic air flow regulation”. This was the first air caster designed to be used on the ground.\n\nIn January 1964, David R Snoeyenbos, while working for General Motors Corporation, revised the design of the air caster, and filed a patent for a “reversible air bearing support”. General Motors Corporation manufactured air casters, or “air bearings” for several years, then sold the rights to Snoeyenbos, who moved to Decatur, Illinois to found Airfloat. He launched his company from his brother’s manufacturing shop in 1964, and officially incorporated as the Airfloat Corp in 1967.\n\nToday, air casters are manufactured by several companies worldwide, such as Aerofilm Systems, Airfloat, Air Caster Corp, AeroGo, Hovair, and Solving.\n"}
{"id": "16730681", "url": "https://en.wikipedia.org/wiki?curid=16730681", "title": "Arthur Pollen", "text": "Arthur Pollen\n\nArthur Joseph Hungerford Pollen (13 September 1866 – 28 January 1937) was a writer on naval affairs in the early 1900s who recognised the need for a computer-based fire-control system. The system took twelve years to develop but included the world's first electrically powered mechanical analogue computer (called at the time the Argo Clock).\n\nPollen was born on 13 September 1866, the sixth son and eighth child of eight sons and two daughters born to John Hungerford Pollen and Maria Margaret Pollen. His father being a leading convert to Catholicism along with Cardinal Newman, Arthur was educated at the school which the latter founded in Birmingham, The Oratory School (1878–1884). He then went up to read Modern History at Trinity College, Oxford where he gained a second-class degree in 1888. In 1893 he was called to the bar at Lincoln's Inn. He then took an interest in parliamentary politics, standing as Radical candidate for the Walthamstow Division of Essex in the General Election of 1895 which he lost; his 4,523 votes to the 6,876 of his opponent, Edmund Widdrington Byrne, MP, QC. After this setback he continued to speak at Liberal Party events, but declined to stand in the by-election brought about by Byrne's resignation in 1897.\n\nOn 7 September 1898 he married Maud Beatrice, the only daughter of the leading Conservative politician Joseph Lawrence, (later Sir Joseph Lawrence, Bart.) who was also chairman of Linotype & Machinery Co. Ltd. With Maud he had one daughter, who died aged four, and two sons. In 1898 Pollen was made the managing-director of Linotype, which he ran successfully for the next decade.\n\nThrough a relative, Commander William Goodenough, Pollen saw a naval gunnery practice near Malta in 1900, and the accuracy was so poor that even at ranges of less than a mile the big guns could not reliably hit their targets. On the board of the Linotype Company at the time was Lord Kelvin, widely regarded as Britain's leading scientist. It was Kelvin who proposed using an analogue computer to solve the equations which arise from the relative motion of the ships engaged in the battle and the time delay in the flight of the shell to calculate the required trajectory and therefore the direction and elevation of the guns. Kelvin's brother James Thomson was responsible for producing a tidal analyser using a ball, disc and cylinder differential analyzer which was the original source of the suggested analogue computer.\n\nHowever first accurate data is needed of the target's position and relative motion. Pollen developed a plotting unit (or plotter) to capture this data. He added a gyroscope to allow for the yaw of the firing ship. Again this required substantial development of the, at the time, primitive gyroscope to provide continuous reliable correction. Pollen utilised the resources of Linotype for his work, specifically the services of a designer named Harold Isherwood. Trials were carried out in 1905 and 1906, which although completely unsuccessful showed promise. To further the development of fire-control Pollen set up in 1909 the Argo Company, and in 1911 took a holding in the firm of Thomas Cooke & Sons of York, who had been manufacturing components for his equipment.\n\nEarly on Pollen was encouraged in his efforts by the rapidly rising figure of Admiral Jackie Fisher, Admiral Arthur Knyvet Wilson and the Director of Naval Ordnance and Torpedoes (DNO), John Jellicoe. Many officers in the navy were naturally interested in the prospect of being able to have ranges calculated for them so that they could achieve a greater rate of hits against the enemy. In early 1906, while presenting his planned \"Aim Correction\" system to naval officers, he met for the second time a promising gunnery lieutenant, Frederic Dreyer. The nature of Pollen's work involved close cooperation with the Navy, and upon Dreyer's appointment as Assistant to the DNO with responsibility for fire-control he was invited to view Pollen's Linotype works at Broadheath, near Altrincham. Pollen had had a long relationship with Dreyer's predecessor, Lieutenant Harding.\n\nPollen continued his work, with tests carried out on Royal Navy warships intermittently. Equipment was repeatedly purchased, despite the misgivings of successive DNOs Reginald Bacon and Gordon Moore.\n\nDreyer began producing his own fire-control computer. The fundamental difference between the systems was that the Dreyer system was a \"dual rate\" approach that plotted ranges and bearings separately, versus time rather than using a single rangefinder to take both ranges and bearings to drive the drawing of a plan view of own and target ships, as in the Argo system. The systems differed in other particulars.\n\nThe single prototype Dreyer Fire Control Table (called simply \"the Original\") did not include a gyroscope, though the first adopted for service in 1912—the Mark III—did. Conversely, the automatic plotting of rangefinder readings on the Original Dreyer table and early service examples was later discarded in favour of manual plotting keyboards which were capable of plotting the data of multiple rangefinders. Dreyer played a key role within the Admiralty in deciding which system to use, and always chose his own.\n\nCertain aspects of the Dreyer Table Mk III were found by a subsequent Royal Commission to be similar to Pollen's work and £30,000 compensation was paid to Pollen in 1926 based on a theoretical number of his Argo units which might have been fitted in Royal Navy ships, most of which were never constructed. Pollen's supporters have argued that the poor performance of Naval gunnery at the Battle of Jutland and at Gallipoli was due to the shortcomings of the Dreyer system, but others cite the tactics of the commander of the British battlecruisers, Vice-Admiral David Beatty.\n\nAt the outbreak of World War I Pollen's relationship with the admiralty had broken down to the extent that he had been removed from the list of recognised Naval contractors. The war also interrupted negotiations which had begun with foreign navies aimed at selling the argo system to them. Some units had been purchased by Russia before the outbreak of war and some more were sold, but no other countries purchased units. Pollen was thus under-employed. He wrote to a friend, Alfred Spender, editor of the Westminster Gazette, asking whether he would be interested in articles about the naval war. Spender agreed. In April 1915 he accepted a post as a feature writer for Land and Water, making this a full-time employment rather just supplying one weekly article. His task was made easier by ongoing good relations with various naval officers he had met as part of his work on the Argo AC system. He remained in contact with James Masterton-Smith, Private secretary to the First Lord of the Admiralty, and with William Reginald Hall, who had been impressed by tests conducted on his ship, HMS Natal, and was appointed Director of Naval Intelligence shortly after the start of the war. Hall commented on the usefulness to him of a responsive journalist, should he need to present a story to the public. Pollen also embarked on a career as a lecturer, speaking at public meetings about naval affairs.\n\nAfter news of the Battle of Jutland began to break in Britain, based upon German reports of victory, Pollen attempted to redress the shortcomings of official reports issued by the admiralty, which as a terse statement of facts known to them, invited the worst possible interpretation of events. Writing for the 'Westminster Gazette' he attempted to stress the overarching result of the engagement, that the German fleet had fled in the presence of the British, and to spread this view amongst other journalists. This response to the news was in contrast to many other leading newspapers, which only belatedly began to regard the battle as a victory.\n\nPollen was generally supportive of the establishment and admiralty. However, on occasions he disagreed publicly with their handling of the war. Commentators, such as Winston Churchill had written articles observing that British supremacy at sea was secured by the dominance of the British fleet over the German, even though no battle had been fought. This was in accord with theories of sea power, such as advocated by Alfred Thayer Mahan. Pollen argued that the cost in this case of the unresolved conflict was that Germany retained significant control of the Baltic Sea, and tied up the larger part of the British fleet patrolling the North Sea. The fleet might otherwise have been used combatting submarine attacks.\n\nIn April 1917 Pollen wrote an article in \"Land and water\" critical of the way the war against submarine attacks was being conducted, and against failures in administrative organisation of the admiralty. This was followed by another article of 3 May arguing Britain had lost control of the seas in the face of mounting losses of merchant shipping to submarines, which drew the attention of the official censor and could not be published. Reaction to the censorship of a respected columnist inevitably led to greater publicity of the issue than if the article had been published, and its contents became known. The admiralty had resisted introducing a convoy system for merchant ships for some time, believing it was impractical and that too few naval vessels were available as escorts. By the end of April the admiralty had agreed to start forming ships into convoys, although it remains a source of debate to what extent this was a result of public pressure, the direct intervention of the prime minister, David Lloyd George or the natural result of convoy trials already being conducted. Although there was great concern at mounting shipping losses and shortages of materials and food, it also remains unclear to what extent the situation was sufficiently critical to have affected Britain's ability to conduct the war.\n\nIn June 1917 Pollen embarked on a visit to America to discuss possible sales of Argo systems. He was approached by John Buchan, Director of Propaganda in the British Foreign Office, asking that he could publicise the work of the navy while there and encourage the development of the American navy. Pollen sought to address a perception amongst Americans that the British fleet had failed to defeat the German fleet, or resolve the submarine issue, thus leaving American ships open to attack. It was felt that his position as a critic of British policy added to his credibility when defending it. The success of his visit led to an offer by the British government of a knighthood and salary, should he return to America and continue a propaganda campaign. Pollen declined, arguing that he would be received entirely differently if acting in an official capacity than as an independent commentator.\n\nPollen became extremely critical of the actions of the British Grand Fleet, in particular as commanded by Jellicoe. Ethel Beatty, wife of Admiral Beatty, made a point of becoming friendly with Pollen in May 1917. Pollen declared to her that he intended to have Jellicoe removed from his post as First Sea Lord within a month, and proceeded with a campaign seeking to impress his views upon anyone he could get to listen. Pollen later wrote a book about the Battle of Jutland which argued that any positive actions credited to Jellicoe must have been the work of Admiral Beatty (\"The Navy in Battle\", 1918). The book was described by the author of the official report on Jutland as a work which \"teems with inaccuracies\", and by Jellicoe's biographers as \"full of errors\", and \"almost unreadable\". Jellicoe wrote to a friend in 1921: \"It fell to me to turn down his inventions on more than one occasion\".\n\nAfter World War I the Argo company had effectively ceased trading, and interest in naval journalism declined. Pollen had continued as a part-time director of Linotype and now joined the board of The Birmingham Small Arms Company Limited (BSA). He became an active member of the Council of the Federation of British Industries and became its vice-president. He became chairman of the British Commonwealth Union and espoused the role of the entrepreneur in growth of industry, campaigning against the growth of socialism. In 1926 the post of managing director of Linotype became vacant, and he returned to it, hiring one of the first management consultants, T. Gerald Rose, to help reorganise the company. He was invited to become chairman of BSA, but declined, not having sufficient time to manage two companies. In 1936 he was part of a group of Catholics who acquired the Catholic magazine, The Tablet, and served as its chairman for a year while its fortunes were restored.\n\n"}
{"id": "7066429", "url": "https://en.wikipedia.org/wiki?curid=7066429", "title": "Breast prostheses", "text": "Breast prostheses\n\nBreast prostheses are breast forms intended to simulate breasts. There are a number of materials and designs; although, the most common construction is silicone gel in a plastic skin. Off-the shelf breast prostheses come in a range of shapes and sizes, while customised breast prostheses are moulded to fit an individual's chest by taking an impression of both breasts and sculpting a mirror image. Pals Breast Forms consist of a non silicone gel which can be trimmed to fit with scissors, then sent back for customization. Fabrication of customised prostheses is typically done by an anaplastologist who makes somato prostheses. The areola and nipple may be replicated by being moulded into the breast form or a separate nipple prosthesis may be attached to the breast prosthesis. Both custom made and off-the shelf breast prostheses come in varieties that are designed to either be held in a pocket in a specially designed mastectomy bra or attached to the skin and worn with a standard bra.\n\nBreast prostheses have a long history. In the 19th century they were made of rubber. On 22 September 1885, one Charles L. Morehouse received US patent 326915 for his \"Breast-Pad\", made of natural rubber and filled with air at normal pressure.\n\nBreast prostheses are most commonly used after a mastectomy, usually a consequence of cancer. They may act as an alternative to, or a stopgap measure until, plastic surgery to reconstruct a breast.\n\nMany pre or non-hormonal transwomen and males who cross-dress as females use breast prostheses in order to create the illusion of feminine breasts. They are sometimes combined with cleavage enhancement techniques when used with clothing with low necklines.\nFull frontal Cleavage Tops are also available, mainly marketed to the Transgender community. They incorporate a pair of breast prostheses in a one-piece skin coloured garment that is designed to provide the illusion of natural cleavage. Such garments have the disadvantage of having a visible top edge at the neck, which requires the wearing of a choker or similar necklace to hide the top edge of the garment. The edges of the breast prostheses are often distinguishable through the thin outer cover.\n\nNon-customised prostheses are made of different shapes to suit the extent of breast tissue removal or the shape of a crossdresser's chest. Asymmetric breast forms incorporate an extension towards the armpit to replicate the shape of the tail of Spence, while symmetric \"triangle\" or \"teardrop\" prostheses do not incorporate that extension. Customised prostheses will mirror the other breast.\n"}
{"id": "49768384", "url": "https://en.wikipedia.org/wiki?curid=49768384", "title": "Brine mining", "text": "Brine mining\n\nBrine mining is the extraction of useful materials (elements or compounds) which are naturally dissolved in brine. The brine may be seawater, other surface water, or groundwater. It differs from solution mining or in-situ leaching in that those methods inject water or chemicals to dissolve materials which are in a solid state; in brine mining, the materials are already dissolved.\n\nBrines are important sources of salt, iodine, lithium, magnesium, potassium, bromine, and other materials, and potentially important sources of a number of others.\n\nAround 500 BCE, the ancient Chinese dug hundreds of brine wells, some of which were over 100 meters (330 feet) in depth. Large brine deposits under the earth's surface were drilled by drilling boreholes. Bamboo towers were erected, similar in style to modern-day oil derricks. Bamboo was used for ropes, casing, and derricks since it was salt resistant. Iron wedges were hung from a bamboo cable tool attached to a lever on a platform constructed atop the tower. The derricks required two to three men jumping on and off the lever that moved the iron wedge pounded into the ground to dig a hole deep enough into the ground to hit the brine.\n\nCommercial brines include both surface water (seawater and saline lakes) and groundwater (shallow brine beneath saline or dry lakes, and deep brines in sedimentary basins). Brine brought to the surface by geothermal energy wells often contains high concentrations of minerals, but is not currently used for commercial mineral extraction.\n\nSeawater has been used as a source of sea salt since prehistoric times, and more recently of magnesium and bromine. Potassium is sometimes recovered from the bittern left after salt precipitation. The oceans are often described as an inexhaustible resource.\n\nThere are many saline lakes with salinity greater than seawater, making them attractive for mineral extraction. Examples are the Dead Sea and the Great Salt Lake. In addition, some saline lakes, such as Lake Natron in East Africa, have chemistry very different than seawater, making them potential sources of sodium carbonate.\n\nThe groundwater beneath saline or dry lakes often has brines with chemistry similar to that of the lakes or former lakes.\n\nThe chemistry of shallow brines used for mineral extraction is sometimes influenced by geothermal waters. This is true of a number of shallow brines in the western United States, such as at Searles Lake, California.\n\nGeothermal power plants often bring brine to the surface as part of the operation. This brine is usually re-injected into the ground, but some experiments have been made to extract minerals before re-injection. Brine brought to the surface by geothermal energy plants has been used in pilot plants as a source of colloidal silica (Wairkei, New Zealand, and Mammoth Lakes, California), and as a source of zinc (Salton Sea, California). Boron was recovered circa 1900 from geothermal steam at Larderello, Italy. Lithium recovery has also been investigated. But as of 2015, there is no sustained commercial-scale mineral recovery from geothermal brine.\n\nThe concentration of dissolved solids in deep connate water varies from much less than sea water to ten times the total dissolved solids of sea water. In general, total dissolved solids (TDS) concentrations increase with depth. Most deep groundwaters classified as brines (having total dissolved solids equal to or greater than that of seawater) are predominantly sodium chloride type. However, the predominance of chloride usually increases with increasing TDS, at the expense of sulfate. The ratio of calcium to sodium usually increases with depth.\n\nThe presence of groundwater with TDS higher than seawater is in some cases due to contact with salt beds. More often, however, the higher TDS of deep sediments is thought to be the result of the sediments acting as semi-permeable membranes. As the sediments compact under burial pressure, the dissolved species are less mobile than the water, resulting in higher TDS concentrations than seawater. Bivalent species such as calcium (Ca) are less mobile than univalent species such as sodium (Na), resulting in calcium enrichment. The ratio of potassium to sodium (K/Na) may increase or decrease with depth, thought to be the result of ion exchange with the sediments.\n\nMany brines contain more than one recovered product. For instance, the shallow brine beneath Searles Lake, California, is or has been a source of borax, potash, bromine, lithium, phosphate, soda ash, and sodium sulfate.\n\nSalt (sodium chloride) has been a valuable commodity since prehistoric times, and its extraction from sea water also goes back to prehistory. Salt is extracted from seawater in many countries around the world, but the majority of salt put on the market today is mined from solid evaporite deposits.\n\nSalt is produced as a byproduct of potash extraction from Dead Sea brine at one plant in Israel (Dead Sea Works), and another in Jordan (Arab Salt Works). The total salt precipitated in solar evaporation at the Dead Sea plants is tens of millions of tons annually, but very little of the salt is marketed. \n\nToday, salt from groundwater brines is generally a byproduct of the process of extracting other dissolved substances from brines and constitutes only a small part of world salt production. In the United States, salt is recovered from surface brine at the Great Salt Lake, Utah, and from a shallow subsurface brine at Searles Lake, California.\n\nIn 1997 about two-thirds of world sodium sulfate production was recovered from brine. Two plants in the US, at Searles Lake, California, and Seagraves, Texas, recovered sodium sulfate from shallow brines beneath dry lakes.\n\nSoda ash (sodium carbonate) is recovered from shallow subsurface brines at Searles Lake, California. Soda ash was formerly extracted at El Caracol, Ecatepec, in Mexico City, from the remnant of Lake Texcoco.\n\nBrines brought to the surface by geothermal energy production often contain concentrations of dissolved silica of about 500 parts per million. A number of geothermal plants have pilot-tested recovery of colloidal silica, including those at Wairakei, New Zealand, Mammoth Lakes, California, and the Salton Sea, California. To date, colloidal silica from brine has not achieved commercial production.\n\nPotash is recovered from surface brine of the Dead Sea, at plants in Israel and Jordan. In 2013 Dead Sea brine provided 9.2% of the world production of potash. As of 1996, the Dead Sea was estimated to contain 2.05 million tons of potassium chloride, the largest brine reserve of potassium other than the ocean.\n\nMost Recently Anson Resources have discovered Lithium in the paradox basin in Utah reading 142ppm.\n\nIn 2015 subsurface brines yielded about half of the world's lithium production. Whereas seawater contains about 0.17 mg/l, subsurface brines may contain up to 4,000 mg/l, more than four orders of magnitude greater than sea water. Typical commercial lithium concentrations are between 200 and 1,400 mg/l.\n\nThe largest operations are in the shallow brine beneath the Salar de Atacama dry lakebed in Chile, which as of 2015 yielded about a third of the world's supply. The brine operations are primarily for potassium; extraction of lithium as a byproduct began in 1997.\n\nThe shallow brine beneath the Salar de Uyuni in Bolivia is thought to contain the world's largest lithium resource, often estimated to be half or more of the world's resource. As of 2015, no commercial extraction has taken place, other than a pilot plant.\n\nCommercial deposits of shallow lithium brines beneath dry lakebeds have the following characteristics in common:\n\n\nIn 2010 Simbol Materials received a $3 million grant from the U.S. Department of Energy for a pilot project aimed at showing the financial feasibility of extracting high-quality lithium from geothermal brine. It uses brine from the 49.9 megawatt Featherstone geothermal power plant in California's Imperial Valley. Simbol passes the plant's extracted fluid through a series of membranes, filters and adsorption materials to extract lithium.\n\nIn 2016, MGX Minerals developed a proprietary design process (U.S. Provisional Patent #62/419,011) to potentially recover lithium and other valuable minerals from highly mineralized oilfield brine. The company has acquired development rights to over approximately 1.7 million acres of brine-bearing formations in Canada and Utah. According to MGX, the Saskatchewan Research Council, an independent laboratory, verified the MGX Minerals petrolithium extraction technology in April 2017.\n\nBoron is recovered from shallow brines beneath Searles Lake, California, by Searles Valley Minerals. Although boron is the primary product, potassium and other salts are also recovered as byproducts.\n\nThe brine beneath the Salar de Olaroz, Argentina, is a commercial source of boron, lithium, and potassium.\n\nCirca 1900, boron was recovered from geothermal steam at Larderello, Italy.\n\nBrines are a major source of iodine supply worldwide. Major deposits occur in Japan and the United States. Iodine is recovered from deep brines pumped to the surface as a byproduct of oil and natural gas production. Seawater contains about 0.06 mg/l iodine, while subsurface brines contain as much as 1,560 mg/l, more than five orders of magnitude greater than seawater. The source of the iodine is thought to be organic material in shales, which also form the source rock for the associated hydrocarbons.\n\nBy far the largest source of iodine from brine is Japan, where iodine-rich water is co-produced with natural gas. Iodine extraction began in 1934. In 2013 seven companies were reported to be extracting iodine. Japanese iodine brines are produced from mostly marine sediments ranging in age from Pliocene to Pleistocene. The main producing area is the Southern Kanto gas field on the east-central coast of Honshu. The iodine content of the brine can be as high as 160ppm.\n\nSince 1977, iodine has been extracted from brine in the Morrow Sandstone of Pennsylvanian age, at locations in the Anadarko Basin. of northwest Oklahoma. The brine occurs at depths of 6,000 to 10,000 feet, and contains about 300ppm iodine.\n\nAll the world's bromine production is derived from brine. The majority is recovered from Dead Sea brine at plants in Israel and Jordan, where bromine is a byproduct of potash recovery. Plants in the United States (\"see: Bromine production in the United States\"), China, Turkmenistan, and Ukraine, recover bromine from subsurface brines. In India and Japan, bromine is recovered as a byproduct of sea salt production.\n\nThe first commercial production of magnesium from seawater was recorded in 1923, when some solar salt plants around San Francisco Bay, California, extracted magnesium from the bitterns left after salt precipitation. \n\nThe Dow Chemical Company began producing magnesium on a small scale in 1916, from deep subsurface brine in the Michigan Basin. In 1933, Dow began using an ion exchange process to concentrate the magnesium in its brine. In 1941, prompted by the need for magnesium for aircraft during World War II, Dow started a large plant at Freeport, Texas, to extract magnesium from the sea. A number of other plants to extract magnesium from brine were built in the US, including one near the Freeport plant at Velasco. At the end of World War II, all shut down except the plant at Freeport, Texas, although the Velasco plant was reactivated during the Korean War.The magnesium plant at Freeport operated until 1998, when Dow announced that it would not rebuild the unit following hurricane damage.\n\nBecause metallic magnesium is extracted from brine by an electrolytic process, the economics are sensitive to the cost of electricity. Dow had located their facility on the Texas coast to take advantage of cheap natural gas for electrical generation. In 1951, Norsk Hydro started a magnesium-from-seawater plant at Heroya, Norway, supplied by inexpensive hydroelectricity. The two seawater magnesium plants, in Texas and Norway, provided more than half the world's primary magnesium through the 1950s and 1960s. \n\nAs of 2014, the only producer of primary magnesium metal in the United States was U.S. Magnesium LLC, which extracted the metal from surface brine of the Great Salt Lake, at its plant in Rowley, Utah.\n\nThe Dead Sea Works in Israel produces magnesium as a byproduct of potash extraction.\n\nStarting in 2002, CalEnergy extracted zinc from brines at its geothermal energy plants at the Salton Sea, California. At full production, the company hoped to produce 30,000 metric tons of 99.99% pure zinc per year, yielding about as much profit as the company made from geothermal energy. But the zinc recovery unit did not perform as anticipated, and zinc recovery halted in 2004.\n\nSome near-surface brines in the western United States contain anomalously high concentrations of dissolved tungsten. Should recovery ever prove economic, some brines could be significant sources of tungsten. For instance, brines beneath Searles Lake, California, with concentrations of about 56 mg/l tungsten (70 mg/l WO), contain about 8.5 million short tons of tungsten. Although 90% of the dissolved tungsten is technically recoverable by ion exchange resins, recovery is uneconomic.\n\nIn 2012 research for the US Department of Energy, building on Japanese research from the 1990s, tested a method for extracting uranium from seawater, which, they concluded, could extract uranium at a cost of US$660/kg. While this was still five times the cost of uranium from ore, the amount of uranium dissolved in seawater would be enough to provide nuclear fuel for thousands of years at current rates of consumption.\n\nAttempts to extract gold from seawater were common in the early 20th century. A number of people claimed to be able to economically recover gold from sea water, but they were all either mistaken or acted in an intentional deception. Prescott Jernegan ran a gold-from-seawater swindle in the United States in the 1890s. A British fraudster ran the same scam in England in the early 1900s. \n\nFritz Haber (the German inventor of the Haber process) did research on the extraction of gold from sea water in an effort to help pay Germany's reparations following World War I. Based on published values of 2 to 64ppb of gold in seawater, a commercially successful extraction seemed possible. After analysis of 4,000 water samples yielding an average of 0.004ppb, it became clear to Haber that the extraction would not be possible, and he stopped the project.\n"}
{"id": "32061397", "url": "https://en.wikipedia.org/wiki?curid=32061397", "title": "Building and Road Research Institute", "text": "Building and Road Research Institute\n\nThe Building and Road Research Institute (BRRI) is a research Institute under the Council for Scientific and Industrial Research of Ghana. It is based in Kumasi in the Ashanti Region of Ghana. BRRI is a research institute.\n\nThe institute was established in 1952 and was known as the West African Building Research Institute in Accra. It was made up of building engineers from Ghana and Nigeria. The institute had a name change in 1960 when the institute's members from Nigeria left to form the Nigerian Building and Road Research Institute. This was because Nigeria had gained independence from Britain. It became known as the Building Research Institute of the Ghana Academy of Arts and Sciences. The institute relocated to the campus of the Kwame Nkrumah University of Science and Technology in Kumasi in 1963. This was to allow the institute's members to lecture at the university due to university under-staffing.\n\nThe institute is made up of various professional groupings. They include architects, engineers, planners, quantity surveyors.\n\nIn 1964 the government of Ghana expanded the functions of the institution to include road research. The name of the institution was changed to include the new function to the Building and Road Research Institute.\n\nIn 1995, the institute moved from the KNUST campus to it present site at Fumesua in Kumasi. The new site which is about 20 kilometres from the institute's former location is called the Kumasi Science City. It houses research institutions in Kumasi. The other institutions include the Forestry Research Institute of Ghana (FORIG) and the Crops Research Institute (CRI).\n\nThe institute was established as a research and development organisation in the construction industry with the purposes of offering research and development products and services to the building and road sectors for the development of Ghana.\n\nA centre of excellence that offers a one stop service in the conduct of research, training and technology transfer in the construction and transportation sectors.\n\nTo promote the conduct of demand-driven and problem based research, provide training and technology transfer that links effectively to the socio-economic development of the country particularly the building, road and transport industry.\n\n\nThe institute works on developing alternative building materials that last longer and cost cheaper for the Ghanaian building industry.\n\nThe institute in the 1990s begun researching into Pozzolana cement, an alternative cement to the Portland cement for building. Pozzolana cement cost less than Portland cement. In May, 2007, BRRI and PMC Global Incorporated of America signed a contract for the commercial production of Pozzolana. The agreement included PMC offering 150,000 dollars to BRRI for expansion of the pilot plant for the production of the pozzolana at the institute and land acquisition for the building of a plant by PMC for the production process.\n\nBRRI has research collaboration with other state and foreign agencies. In 2007, BRRI and the Transport and Road Research Laboratory (TRRL) of the United Kingdom worked together on a program of national studies in Ghana for two years. The purpose of the collaboration was to research into developing building materials that would benefit both bodies. In 2009, the institutes and its Nigerian counterpart (NBRRI) signed a memorandum of understanding to research into building and road construction materials.\n\n"}
{"id": "503817", "url": "https://en.wikipedia.org/wiki?curid=503817", "title": "CIMD", "text": "CIMD\n\nComputer Interface to Message Distribution (CIMD) is a proprietary short message service centre protocol developed by Nokia for their SMSC (now: Nokia Networks).\n\nAn example CIMD exchange looks like the following:\n\nEach packet starts with STX (hex 02) and ends with ETX (hex 03). The content of the packet consists of fields separated by TAB (hex 09). Each field, in turn, consists of a \"parameter type\", a colon (:), and the \"parameter value\". Note that the last field must also be terminated with a TAB before the ETX.\n\nTwo-digit parameter types are \"operation codes\" and each message must have exactly one. The number after the operation code is the \"sequence number\" used to match an operation to its response. The response code (acknowledgement) of the message is equal to the operation code plus 50.\n\nIn the example above, the operation code 03 means submit message. Field 021 defines the destination address (telephone number), with field 033 is the user data (content) of the message. Response code 53 with a field 060 time stamp indicates that the message was accepted; if the message failed, the SMSC would reply with field 900 (error code) instead.\nA good number of supporting software to implement CIMD is available from\nNokia's Website to build CIMD client. You can fire SMS from message center with the help of CIMD client tools.\n\n\n\n"}
{"id": "43605359", "url": "https://en.wikipedia.org/wiki?curid=43605359", "title": "Charity Basaza Mulenga", "text": "Charity Basaza Mulenga\n\nCharity Basaza Mulenga is a Ugandan electrical engineer and academic administrator. She was the founding vice chancellor (2011 to 2016) of St. Augustine International University (SAIU), a private institution of higher education that the Uganda National Council for Higher Education accredited in 2011.\n\nShe was born circa 1979 in Kisoro District in the Western Region of Uganda.\n\nMulenga studied electrical engineering at Makerere University, the largest and oldest public university in Uganda, graduating with a Bachelor of Science degree in 2001. Her Master of Science degree in digital communication systems was awarded by the University of Loughborough in 2004. She also holds a Doctor of Philosophy degree in electrical and electronic engineering, awarded in 2010 by Loughborough University.\n\nIn 2001 Mulenga joined MTN Uganda as a switch planning engineer. In 2003, she left there to pursue a Master of Science degree in the United Kingdom on a British Council scholarship. She returned to Uganda in 2005 and joined the Faculty of Computing and Information Technology at Makerere University as a research coordinator. While there, she obtained another scholarship to pursue her doctorate. Her research area was antennas and electromagnetic modelling. During this period, she was appointed an assistant lecturer in the same faculty at Makerere. In 2009, she was appointed deputy vice chancellor at SAIU. Between 2011 and 2016, she served as vice chancellor at SAIU. She is a member of the University Council at SAIU.\n\n\n"}
{"id": "15199671", "url": "https://en.wikipedia.org/wiki?curid=15199671", "title": "Common-mode signal", "text": "Common-mode signal\n\nCommon-mode signal is the component of an analog signal which is present with one sign on all considered conductors. In telecommunication, common-mode signal on a transmission line is known as longitudinal voltage.\n\nIn electronics where the signal is transferred by differential voltage, the common-mode signal is a half-sum of voltages\n\nWhen referenced to the local common or ground, a common-mode signal appears on both lines of a two-wire cable, in-phase and with equal amplitudes. Technically, a common-mode voltage is one-half the vector sum of the voltages from each conductor of a balanced circuit to local ground or common. Such signals can arise from one or more of the following sources:\n\nNoise induced into a cable, or transmitted from a cable usually occurs in the common mode; i.e. the same signal tends to be picked up by both conductors in a two wire cable. Likewise, RF noise transmitted from a cable tends to emanate from both conductors. Elimination of common mode signals on cables entering or leaving electronic equipment is important to ensure electromagnetic compatibility. Unless the intention is to transmit or receive radio signals, an electronic designer will generally design electronic circuits to minimise or eliminate common mode effects.\n\n\nCommon mode filtering may also be used to prevent egress of noise for electromagnetic compatibility purposes.\nHigh frequency common mode signals, for example, RF noise from a computing circuit, may be blocked using a ferrite bead clamped to the outside of a cable. These are often observable on laptop computer power supplies near the jack socket, and good quality mouse or printer USB cables and HDMI cables.\n\nSwitch mode power supplies include common and differential mode filtering inductors to block the switching signal noise returning into mains wiring.\n\n"}
{"id": "828834", "url": "https://en.wikipedia.org/wiki?curid=828834", "title": "Copper extraction", "text": "Copper extraction\n\nCopper extraction refers to the methods used to obtaining copper from its ores. The conversion of copper consists of a series of chemical, physical, and electrochemical processes. Methods have evolved and vary with country depending on the ore source, local environmental regulations, and other factors.\n\nAs in all mining operations, the ore must usually be beneficiated (concentrated). The processing techniques depend on the nature of the ore. If the ore is primarily sulfide copper minerals (such as chalcopyrite), the ore is crushed and ground to liberate the valuable minerals from the waste ('gangue') minerals. It is then concentrated using mineral flotation. The concentrate is typically then sold to distant smelters, although some large mines have smelters located nearby. Such colocation of mines and smelters was more typical in the 19th and early 20th centuries, when smaller smelters could be economic. The sulfide concentrates are typically smelted in such furnaces as the Outokumpo or Inco flash furnace or the ISASMELT furnace to produce matte, which must be converted and refined to produce anode copper. Finally, the final refining process is electrolysis. For economic and environmental reasons, many of the byproducts of extraction are reclaimed. Sulfur dioxide gas, for example, is captured and turned into sulfuric acid — which can then be used in the extraction process or sold for such purposes as fertiliser manufacture.\n\nOxidised copper ores can be treated by hydrometallurgical extraction.\n\nThe earliest evidence of cold-hammering of native copper comes from the excavation at Çaÿonü Tepesi in eastern Anatolia, which dates between 7200 to 6600 BCE. Among the various items considered to be votive or amulets there was one that looked like a fishhook and one like an awl. Another find, at Shanidar Cave in Mergasur, Iraq, contained copper beads, dates to 8,700 BCE.\n\nThe world's oldest known copper mine, as opposed to usage of surface deposits, is at Timna Valley, Israel, since the fourth millennium BC, with smelting and surface deposit usage since the sixth to fifth millennium.\n\nPločnik archaeological site in southeastern Europe (Serbia) contains the oldest securely dated evidence of copper making at high temperature, from 5,000 BCE. The find in June 2010 extends for additional 500 years the earlier record of copper smelting from Rudna Glava (Serbia), dated to 5th millennium BCE.\n\nCopper smelting technology gave rise to the Copper Age, aka Chalcolithic Age, and then the Bronze Age. The Bronze Age would not have been possible without humans developing smelting technology.\n\nMost copper ores contain only a small percentage of copper metal bound up within valuable ore minerals, with the remainder of the ore being unwanted rock or gangue minerals, typically silicate minerals or oxide minerals for which there is often no value. In some cases, tailings have been retreated to recover lost value as the technology for recovering copper has improved. The average grade of copper ores in the 21st century is below 0.6% copper, with a proportion of economic ore minerals (including copper) being less than 2% of the total volume of the ore rock. A key objective in the metallurgical treatment of any ore is the separation of ore minerals from gangue minerals within the rock.\n\nThe first stage of any process within a metallurgical treatment circuit is accurate grinding or \"comminution\", where the rock is crushed to produce small particles (<100 μm) consisting of individual mineral phases. These particles are then separated to remove gangue (rocks residues), thereafter followed by a process of physical liberation of the ore minerals from the rock. The process of liberation of copper ores depends upon whether they are oxide or sulfide ores.\n\nSubsequent steps depend on the nature of the ore containing the copper and what will be extracted. For oxide ores, a hydrometallurgical liberation process is normally undertaken, which uses the soluble nature of the ore minerals to the advantage of the metallurgical treatment plant. For sulfide ores, both secondary (supergene) and primary (hypogene), froth flotation is used to physically separate ore from gangue. For special native copper bearing ore bodies or sections of ore bodies rich in supergene native copper, this mineral can be recovered by a simple gravity circuit.\n\nThe modern froth flotation process was independently invented the early 1900s in Australia by C.V Potter and around the same time by G. D. Delprat.\nAll primary sulfide ores of copper sulfides, and most concentrates of secondary copper sulfides (being chalcocite), are subjected to smelting. Some vat leach or pressure leach processes exist to solubilise chalcocite concentrates and produce copper cathode from the resulting leachate solution, but this is a minor part of the market.\n\nCarbonate concentrates are a relatively minor product produced from copper cementation plants, typically as the end-stage of a heap-leach operation. Such carbonate concentrates can be treated by a solvent extraction and electrowinning (SX-EW) plant or smelted.\n\nThe copper ore is crushed and ground to a size such that an acceptably high degree of liberation has occurred between the copper sulfide ore minerals and the gangue minerals. The ore is then wet, suspended in a slurry, and mixed with xanthates or other reagents, which render the sulfide particles hydrophobic. Typical reagents include potassium ethylxanthate and sodium ethylxanthate, but dithiophosphates and dithiocarbamates are also used.\n\nThe treated ore is introduced to a water-filled aeration tank containing surfactant such as methylisobutyl carbinol (MIBC). Air is constantly forced through the slurry and the air bubbles attach to the hydrophobic copper sulfide particles, which are conducted to the surface, where they form a froth and are skimmed off. These skimmings are generally subjected to a cleaner-scavenger cell to remove excess silicates and to remove other sulfide minerals that can deleteriously impact the concentrate quality (typically, galena), and the final concentrate sent for smelting. The rock which has not floated off in the flotation cell is either discarded as tailings or further processed to extract other metals such as lead (from galena) and zinc (from sphalerite), should they exist. To improve the process efficiency, lime is used to raise the pH of the water bath, causing the collector to ionize more and to preferentially bond to chalcopyrite (CuFeS) and avoid the pyrite (FeS). Iron exists in both primary zone minerals. Copper ores containing chalcopyrite can be concentrated to produce a concentrate with between 20% and 30% copper-in-concentrate (usually 27–29% copper); the remainder of the concentrate is iron and sulfur in the chalcopyrite, and unwanted impurities such as silicate gangue minerals or other sulfide minerals, typically minor amounts of pyrite, sphalerite or galena. Chalcocite concentrates typically grade between 37% and 40% copper-in-concentrate, as chalcocite has no iron within the mineral.\n\nSecondary sulfides – those formed by supergene secondary enrichment – are resistant (\"refractory\") to sulfuric leaching. These ores are a mixture of copper carbonate, sulfate, phosphate, and oxide minerals and secondary sulfide minerals, dominantly chalcocite but other minerals such as digenite can be important in some deposits.\n\nSupergene ores rich in sulfides may be concentrated using froth flotation. A typical concentrate of chalcocite can grade between 37% and 40% copper in sulfide, making them relatively cheap to smelt compared to chalcopyrite concentrates.\n\nSome supergene sulfide deposits can be leached using a bacterial oxidation heap leach process to oxidize the sulfides to sulfuric acid, which also allows for simultaneous leaching with sulfuric acid to produce a copper sulfate solution. As with oxide ores, solvent extraction and electrowinning technologies are used to recover the copper from the pregnant leach solution.\n\nSupergene sulfide ores rich in native copper minerals are refractory to treatment with sulfuric acid leaching on all practicable time scales, and the dense metal particles do not react with froth flotation media. Typically, if native copper is a minor part of a supergene profile it will not be recovered and will report to the tailings. When rich enough, native copper ore bodies may be treated to recover the contained copper via a gravity separation circuit where the density of the metal is used to liberate it from the lighter silicate minerals. Often, the nature of the gangue is important, as clay-rich native copper ores prove difficult to liberate.\n\nOxidised copper ore bodies may be treated via several processes, with hydrometallurgical processes used to treat oxide ores dominated by copper carbonate minerals such as azurite and malachite, and other soluble minerals such as silicates like chrysocolla, or sulfates such as atacamite and so on.\n\nSuch oxide ores are usually leached by sulfuric acid, usually in a heap leaching or dump leaching process to liberate the copper minerals into a solution of sulfuric acid laden with copper sulfate in solution. The copper sulfate solution (the pregnant leach solution) is then stripped of copper via a solvent extraction and electrowinning (SX-EW) plant, with the barred (denuded) sulfuric acid recycled back on to the heaps. Alternatively, the copper can be precipitated out of the pregnant solution by contacting it with scrap iron; a process called cementation. Cement copper is normally less pure than SX-EW copper. Commonly sulfuric acid is used as a leachant for copper oxide, although it is possible to use water, particularly for ores rich in ultra-soluble sulfate minerals.\n\nIn general, froth flotation is not used to concentrate copper oxide ores, as oxide minerals are not responsive to the froth flotation chemicals or process (i.e.; they do not bind to the kerosene-based chemicals). Copper oxide ores have occasionally been treated via froth flotation via sulfidation of the oxide minerals with certain chemicals which react with the oxide mineral particles to produce a thin rime of sulfide (usually chalcocite), which can then be activated by the froth flotation plant.\n\nUntil the latter half of the 20th century, smelting sulfide ores was almost the sole means of producing copper metal from mined ores (\"primary\" copper production). Davenport, et al, noted in 2002 that even then 80% of global primary copper production was from copper–iron–sulfur minerals and that the vast majority of these were treated by smelting.\n\nCopper was initially recovered from sulfide ores by directly smelting the ore in a furnace. The smelters were initially located near the mines to minimize the cost of transport. This avoided the prohibitive costs of transporting the waste minerals and the sulfur and iron present in the copper-containing minerals. However, as the concentration of copper in the ore bodies decreased, the energy costs of smelting the whole ore also became prohibitive, and it became necessary to concentrate the ores first.\n\nInitial concentration techniques included hand-sorting and gravity concentration. They resulted in high losses of copper. Consequently, the development of the froth flotation process was a major step forward in mineral processing. It made possible the development of the giant Bingham Canyon mine in Utah.\n\nIn the twentieth century, most ores were concentrated before smelting. Smelting was initially undertaken using sinter plants and blast furnaces, or with roasters and reverberatory furnaces. Roasting and reverberatory furnace smelting dominated primary copper production until the 1960s.\n\nThe roasting process is generally undertaken in combination with reverberatory furnaces. In the roaster, the copper concentrate is partially oxidised to produce \"calcine\" and sulfur dioxide gas. The stoichiometry of the reaction which occurs is:\n\nRoasting generally leaves more sulfur in the calcined product (15% in the case of the roaster at Mount Isa Mines) than a sinter plant leaves in the sintered product (about 7% in the case of the Electrolytic Refining and Smelting smelter).\n\nAs of 2005, roasting is no longer common in copper concentrate treatment, because its combination with reverberatory furnaces is not energy efficient and the SO concentration in the roaster offgas is too dilute for cost-effective capture. Direct smelting is now favored, e.g. using the following smelting technologies: flash smelting, Isasmelt, Noranda, Mitsubishi or El Teniente furnaces.\n\nThe initial melting of the material to be smelted is usually referred to as the \"smelting\" or \"matte smelting\" stage. It can be undertaken in a variety of furnaces, including the largely obsolete blast furnaces and reverberatory furnaces, as well as flash furnaces, Isasmelt furnaces, etc. The product of this smelting stage is a mixture of copper, iron and sulfur that is enriched in copper, and which is called \"matte\" or \"copper matte\". The term \"matte grade\" is normally used to refer to the copper content of the matte.\n\nThe purpose of the matte smelting stage is to eliminate as much of the unwanted iron, sulfur and \"gangue\" minerals (such as silica, magnesia, alumina and limestone) as possible, while minimizing the loss of copper. This is achieved by reacting iron sulfides with oxygen (in air or oxygen enriched air) to produce iron oxides (mainly as FeO, but with some magnetite (FeO)) and sulfur dioxide.\n\nCopper sulfide and iron oxide can mix, but when sufficient silica is added, a separate slag layer is formed. Adding silica also reduces the melting point (or, more properly, the liquidus temperature) of the slag, meaning that the smelting process can be operated at a lower temperature.\n\nThe slag forming reaction is:\n\nSlag is less dense than matte, so it forms a layer that floats on top of the matte.\n\nCopper can be lost from the matte in three ways: as cuprous oxide (CuO) dissolved in the slag, as sulfide copper dissolved in the slag or as tiny droplets (or \"prills\") of matte suspended in the slag.\n\nThe amount of copper lost as oxide copper increases as the oxygen potential of the slag increases. The oxygen potential generally increases as the copper content of the matte is increased. Thus the loss of copper as oxide increases as the copper content of the matte increases.\n\nOn the other hand, the solubility of sulfidic copper in slag decreases as the copper content of the matte increases beyond about 40%. Nagamori calculated that more than half the copper dissolved in slags from mattes containing less than 50% copper is sulfidic copper. Above this figure, oxidic copper begins to dominate.\n\nThe loss of copper as prills suspended in the slag depends on the size of the prills, the viscosity of the slag and the settling time available. Rosenqvist suggested that about half the copper losses to slag were due to suspended prills.\n\nThe mass of slag generated in the smelting stage depends on the iron content of the material fed into the smelting furnace and the target matte grade. The greater the iron content of the feed, the more iron that will need to be rejected to the slag for a given matte grade. Similarly, increasing the target matte grade requires the rejection of more iron and an increase in the slag volume.\n\nThus, the two factors that most affect the loss of copper to slag in the smelting stage are:\n\nThis means that there is a practical limit on how high the matte grade can be if the loss of copper to slag is to be minimized. Therefore, further stages of processing (converting and fire refining) are required.\n\nThe following subsections briefly describe some of the processes used in matte smelting.\n\nReverberatory furnaces are long furnaces can treat wet, dry or roasted concentrate. Most of the reverberatory furnaces used in the latter years treated roasted concentrate because putting dry feed materials into the reverberatory furnace is more energy efficient, and because the elimination of some of the sulfur in the roaster results in higher matte grades.\n\nThe reverberatory furnace feed is added to the furnace through feed holes along the sides of the furnace. Additional silica is normally added to help form the slag. The furnace is fired with burners using pulverized coal, fuel oil or natural gas and the solid charge is melted.\n\nReverberatory furnaces can additionally be fed with molten slag from the later converting stage to recover the contained copper and other materials with a high copper content.\n\nBecause the reverberatory furnace bath is quiescent, very little oxidation of the feed occurs (and thus very little sulfur is eliminated from the concentrate). It is essentially a melting process. Consequently, wet-charged reverberatory furnaces have less copper in their matte product than calcine-charged furnaces, and they also have lower copper losses to slag. Gill quotes a copper in slag value of 0.23% for a wet-charged reverberatory furnace vs 0.37% for a calcine-charged furnace.\n\nIn the case of calcine-charged furnaces, a significant portion of the sulfur has been eliminated during the roasting stage, and the calcine consists of a mixture of copper and iron oxides and sulfides. The reverberatory furnace acts to allow these species to approach chemical equilibrium at the furnace operating temperature (approximately 1600 °C at the burner end of the furnace and about 1200 °C at the flue end; the matte is about 1100 °C and the slag is about 1195 °C). In this equilibration process, oxygen associated with copper compounds exchanges with sulfur associated with iron compounds, increasing the iron oxide content of the furnace, and the iron oxides interact with silica and other oxide materials to form the slag.\n\nThe main equilibration reaction is:\n\nThe slag and the matte form distinct layers that can be removed from the furnace as separate streams. The slag layer is periodically allowed to flow through a hole in the wall of the furnace above the height of the matte layer. The matte is removed by draining it through a hole into ladles for it to be carried by crane to the converters. This draining process is known as \"tapping\" the furnace. The matte taphole is normally a hole through a water-cooled copper block that prevents erosion of the refractory bricks lining the furnace. When the removal of the matte or slag is complete, the hole is normally plugged with clay, which is removed when the furnace is ready to be tapped again.\n\nReverberatory furnaces were often used to treat molten converter slag to recover contained copper. This would be poured into the furnaces from ladles carried by cranes. However, the converter slag is high in magnetite and some of this magnetite would precipitate from the converter slag (due to its higher melting point), forming an accretion on the hearth of the reverberatory furnace and necessitating shut downs of the furnace to remove the accretion. This accretion formation limits the quantity of converter slag that can be treated in a reverberatory furnace.\n\nWhile reverberatory furnaces have very low copper losses to slag, they are not very energy-efficient and the low concentrations of sulfur dioxide in their offgases make its capture uneconomic. Consequently, smelter operators devoted a lot of money in the 1970s and 1980s to developing new, more efficient copper smelting processes. In addition, flash smelting technologies had been developed in earlier years and began to replace reverberatory furnaces. By 2002, 20 of the 30 reverberatory furnaces still operating in 1994 had been shut down.\n\nIn flash smelting, the concentrate is dispersed in an air or oxygen stream and the smelting reactions are largely completed while the mineral particles are still in flight. The reacted particles then settle in a bath at the bottom of the furnace, where they behave as does calcine in a reverberatory furnace. A slag layer forms on top of the matte layer, and they can separately be tapped from the furnace.\n\nThe matte, which is produced in the smelter, contains 30–70% copper (depending on the process used and the operating philosophy of the smelter), primarily as copper sulfide, as well as iron sulfide. The sulfur is removed at high temperature as sulfur dioxide by blowing air through molten matte:\n\nIn a parallel reaction the iron sulfide is converted to slag:\n\nThe purity of this product is 98%, it is known as \"blister\" because of the broken surface created by the escape of sulfur dioxide gas as blister copper \"pigs\" or ingots are cooled. By-products generated in the process are sulfur dioxide and slag. The sulfur dioxide is captured for use in earlier leaching processes.\n\nThe blister copper is put into an anode furnace, a furnace that refines the blister copper to anode-grade copper in two stages by removing most of the remaining sulfur and iron, and then removing oxygen introduced during the first stage. This second stage, often referred to as \"poling\" is done by blowing natural gas, or some other reducing agent, through the molten copper oxide. When this flame burns green, indicating the copper oxidation spectrum, the oxygen has mostly been burned off. This creates copper at about 99% pure. The anodes produced from this are fed to the electrorefinery.\n\nThe copper is refined by electrolysis. The anodes cast from processed blister copper are placed into an aqueous solution of 3–4% copper sulfate and 10–16% sulfuric acid. Cathodes are thin rolled sheets of highly pure copper or, more commonly these days, reusable stainless steel starting sheets (as in the IsaKidd process). A potential of only 0.2–0.4 volts is required for the process to commence. At the anode, copper and less noble metals dissolve. More noble metals such as silver, gold, selenium, and tellurium settle to the bottom of the cell as anode slime, which forms a salable byproduct. Copper(II) ions migrate through the electrolyte to the cathode. At the cathode, copper metal plates out, but less noble constituents such as arsenic and zinc remain in solution unless a higher voltage is used. The reactions are:\n\nAt the anode: Cu → Cu + 2e\n\nAt the cathode: Cu + 2e → Cu\n\nCopper concentrates produced by mines are sold to smelters and refiners who treat the ore and refine the copper and charge for this service via treatment charges (TCs) and refining charges (RCs). The TCs are charged in US$ per tonne of concentrate treated and RCs are charged in cents per pound treated, denominated in US dollars, with benchmark prices set annually by major Japanese smelters. The customer in this case can be a smelter, who on-sells blister copper ingots to a refiner, or a smelter-refiner which is vertically integrated.\n\nOne prevalent form of copper concentrate contains gold and silver, like the one produced by Bougainville Copper Limited from the Panguna mine from the early 1970s to the late 1980s.\n\nThe typical contract for a miner is denominated against the London Metal Exchange price, minus the TC-RCs and any applicable penalties or credits. Penalties may be assessed against copper concentrates according to the level of deleterious elements such as arsenic, bismuth, lead or tungsten. Because a large portion of copper sulfide ore bodies contain silver or gold in appreciable amounts, a credit can be paid to the miner for these metals if their concentration \"within the concentrate\" is above a certain amount. Usually the refiner or smelter charges the miner a fee based on the concentration; a typical contract will specify that a credit is due for every ounce of the metal in the concentrate above a certain concentration; below that, if it is recovered, the smelter will keep the metal and sell it to defray costs.\n\nCopper concentrate is traded either via spot contracts or under long term contracts as an intermediate product in its own right. Often the smelter sells the copper metal itself on behalf of the miner. The miner is paid the price at the time that the smelter-refiner makes the sale, not at the price on the date of delivery of the concentrate. Under a Quotational Pricing system, the price is agreed to be at a fixed date in the future, typically 90 days from time of delivery to the smelter.\n\nA-grade copper cathode is of 99.99% copper in sheets that are 1 cm thick, and approximately 1 meter square weighing approximately 200 pounds. It is a true commodity, deliverable to and tradeable upon the metal exchanges in New York City (COMEX), London (London Metals Exchange) and Shanghai (Shanghai Futures Exchange). Often copper cathode is traded upon the exchanges indirectly via warrants, options, or swap contracts such that the majority of copper is traded upon the LME/COMEX/SFE but delivery is achieved directly, logistically moving the physical copper, and transferring the copper sheet from the physical warehouses themselves.\n\nThe chemical specification for electrolytic grade copper is ASTM B 115-00 (a standard that specifies the purity and maximum electrical resistivity of the product).\n\n\n"}
{"id": "20120864", "url": "https://en.wikipedia.org/wiki?curid=20120864", "title": "Cranked eye bolt", "text": "Cranked eye bolt\n\nA cranked eye bolt is an eye bolt typically used as a structural tie down in building construction where the eye of the bolt must be fastened to a point that cannot be directly below where the shaft would otherwise be fastened. This often occurs where a bearer must be tied down to a post or column but the bearer cannot be directly fastened to the post of column.\n\nIt has a shaft which is \"cranked\", or bent twice: once off center, and a second time to bring the shaft back parallel to the original shaft.\n\nThe requirement for an offset tie down will occur when vermin proofing must be placed between the column or post, and a wooden bearer, for example to stop termites travelling up through a concrete or wooden post or column directly into the bearer and the rest of the building. The \"ant capping\", typically a 0.5mm to 0.8mm thick galvanised steel sheet, must be placed between the post and the bearer overlapping the perimeter of the post by approximately 20mm to 40mm or more. The cranked eye bolt is fastened to the post using a bolt though the eye, the crank in the shaft allowing the shaft to be positioned so that it does not impede the overlap of the \"ant capping\" up through the bearer.\n\nShould termite attack occur, the post of column can be replaced with no structural effect to the building.\n\nCranked eye bolts can also be used to tie the top plate of a house frame directly to house supports, using rod couplers and steel extension rods.\nCranked eye bolts used to be made by bending an \"eye\" into the end of a rod that was threaded at the other end.\n\nToday, cranked eye bolts are now typically made by welding a cranked and threaded rod to a heavy gauge steel washer.\n\nCranked eye bolts are made with different degrees of crank and lengths of shaft for flexibility.\n\n"}
{"id": "8559342", "url": "https://en.wikipedia.org/wiki?curid=8559342", "title": "Deal–Grove model", "text": "Deal–Grove model\n\nThe Deal–Grove model mathematically describes the growth of an oxide layer on the surface of a material. In particular, it is used to predict and interpret thermal oxidation of silicon in semiconductor device fabrication. The model was first published in 1965 by Bruce Deal and Andrew Grove, of Fairchild Semiconductor.\n\nThe model assumes that oxidation reaction occurs at the interface between the oxide layer and the substrate material, rather than between the oxide and the ambient gas. Thus, it considers three phenomena that the oxidizing species undergoes, in this order:\n\n\nThe model assumes that each of these stages proceeds at a rate proportional to the oxidant's concentration. In the first case, this means Henry's law; in the second, Fick's law of diffusion; in the third, a first-order reaction with respect to the oxidant. It also assumes steady state conditions, i.e. that transient effects do not appear.\n\nGiven these assumptions, the flux of oxidant through each of the three phases can be expressed in terms of concentrations, material properties, and temperature.\n\nBy setting the three fluxes equal to each other, each may be found. In turn, the growth rate may be found readily from the oxidant reaction flux.\n\nIn practice, the ambient gas (stage 1) does not limit the reaction rate, so this part of the equation is often dropped. This simplification yields a simple quadratic equation for the oxide thickness. For oxide growing on an initially bare substrate, the thickness \"X\" at time \"t\" is given by the following equation:\nwhere the constants formula_6 and formula_7 encapsulate the properties of the reaction and the oxide layer, respectively, and the constant formula_8 takes into account any initial oxide thickness present. These constants are given as:\nwhere formula_12, with formula_13 being the gas solubility parameter of the Henry's law and formula_14 is the partial pressure of the diffusing gas. formula_15 denotes the oxidant molecules/unit volume needed to produce a unit volume of the oxide.\n\nSolving the quadratic equation for \"X\" yields:\n\nTaking the short and long time limits of the above equation reveals two main modes of operation:\n\nBecause they appear in these equations, the quantities \"B\" and \"B/A\" are often called the \"quadratic\" and \"linear reaction rate constants\". They depend exponentially on temperature, like this:\n\nwhere formula_20 is the activation energy and formula_21 is the Boltzmann Constant in eV. formula_20 differs from one equation to the other. The following table lists the values of the four parameters for single-crystal silicon under conditions typically used in industry (low doping, atmospheric pressure). The linear rate constant depends on the orientation of the crystal (usually indicated by the Miller indices of the crystal plane facing the surface). The table gives values for <100> and <111> silicon.\n\nThe Deal–Grove model works very well for single-crystal silicon under most conditions. However, experimental data shows that very thin oxides (less than about 25 nanometres) grow much more quickly in formula_23 than the model predicts. In silicon nanostructures (e.g. Silicon Nanowires) this rapid growth is generally followed by diminishing oxidation kinetics in a process known as self-limiting oxidation, necessitating a modification of the Deal–Grove model.\n\nIf the oxide grown in a particular oxidation step will significantly exceed 25 nm, a simple adjustment accounts for the aberrant growth rate. The model yields accurate results for thick oxides if, instead of assuming zero initial thickness (or any initial thickness less than 25 nm), we assume that 25 nm of oxide exists before oxidation begins. However, for oxides near to or thinner than this threshold, more sophisticated models must be used.\n\nIn the 1980s, it became obvious that an update to the Deal-Grove Model would be necessary to model the aforementioned thin oxides (self-limiting cases). One such approach more accurately models thin oxides is the Massoud Model from 1985 [2]. The Massoud Model is analytical and based on parallel oxidation mechanisms. It changes the parameters of Deal-Grove Model to better model the initial oxide growth with the addition of rate enhancement terms. \n\nDeal-Grove also fails for polycrystalline silicon (\"poly-silicon\"). First, the random orientation of the crystal grains makes it difficult to choose a value for the linear rate constant. Second, oxidant molecules diffuse rapidly along grain boundaries, so that poly-silicon oxidizes more rapidly than single-crystal silicon.\n\nDopant atoms strain the silicon lattice, and make it easier for silicon atoms to bond with incoming oxygen. This effect may be neglected in many cases, but heavily doped silicon oxidizes significantly faster. The pressure of the ambient gas also affects oxidation rate.\n\n\nOnline Calculator including pressure, doping, and thin oxide effects: http://www.lelandstanfordjunior.com/thermaloxide.html\n"}
{"id": "50764202", "url": "https://en.wikipedia.org/wiki?curid=50764202", "title": "E-NAM", "text": "E-NAM\n\nNational Agriculture Market or eNAM is an online trading platform for agricultural commodities in India. The market facilitate farmers, traders and buyers with online trading in commodities. The market is helping in better price discovery and provide facilities for smooth marketing of their produce. The market transactions stood at ₹36,200 crores by January 2018, mostly intra-market. Over 90 commodities including staple food grains, vegetables and fruits are currently listed in its list of commodities available for trade. The eNAM markets are proving popular as the crops are weighed immediately and the stock is lifted on the same day and the payments are cleared online. In February 2018, some attractive features like MIS dashboard, BHIM and other mobile payments, enhanced features on the mobile app such as gate entry and payment through mobile phones and farmers database is helping adoption even more. The present trading is done mostly for intra-market, but in phases, it will be rolled out to trade in inter-market, inter-state, creating a unified national market for agricultural commodities.\n\nIt was launched by Ministry of Agriculture, Government of India. The electronic market pilot across India was launched on 14 April 2016 by Prime Minister of India, Narendra Modi. The Portal is managed by Small Farmers’ Agribusiness Consortium (SFAC) with technology provider, NFCL’s iKisan division.\nA similar project was initiated by the Congress government in the State of Karnataka, during UPA tenure and had been a great success. NDA government has rolled it out nationally.\n\nOn the eNAM platform, farmers can opt to trade directly on their own through mobile app or through registered commission agents.\n\nThe eNAM is linked with 585 markets (APMCs) in 16 states and 2 union territory, with over 45 lakh farmer membership in 15 states. The market is helping traders and exporters in procuring quality produce in bulk, at one place and ensure transparent financial transactions.\n\nThe Government plans to connect over 22,000 GrAMs, local farmers markets, with the platform. To provide better grading and assaying services, the Agriculture Department is looking at looping in AGMARK for better certification.\n\nThe trading is done online, with trading computers or through mobile app in all e-nam markets.\n\nAn eNAM mobile application is available on Android for farmers and traders to bid and complete a transact on the app, available in 8 languages.\n\nThe payment network RTGS/NEFT, debit card and internet banking was also integrated into the app. In 2017 mobile payment, Unified Payment Interface (UPI) facility through BHIM support was added in the app.\n\nThe mobile phone gate entry, integration of farmers database and e-learning module is available. The agents are mostly using the eNAM mobile App for trading on behalf of farmers.\n\nThe critical operation of gate entry from e-NAM mobile app helps facilitate the farmers to do advance gate entry on mobile app, which in turn will reduce a lot of time for farmers coming into the market and will bring huge efficiency and facilitate smooth arrival recording at the gate.\n\nA new feature has been introduced for farmers, where they can see the progress of their lot being traded and also real time bidding progress of price will be visible to farmers on their mobile app.\n\nThe MIS dashboard was introduced in February 2018, to give a greater insight into the performance of each market (mandi) in terms of arrival and trade in commodities.\n\nThe six States with the most eNAMs as of March 2018 are Uttar Pradesh – 100, Madhya Pradesh – 58, Haryana - 54, Maharashtra – 60, Gujarat – 49 and Telangana – 48.\n\n\nSmall Farmers’ Agribusiness Consortium (SFAC) is the lead promoter of NAM. SFAC is a registered society of Department of Agriculture, Cooperation & Farmers’ Welfare (DAC&FW) under Ministry of Agriculture and Farmer Welfare. SFAC through open tender selected Nagarjuna Fertilizers and Chemicals Ltd / Ikisan Division as Strategic Partner (SP) to develop, operate and maintain the NAM e-platform. SFAC implements NAM with the technical support of SP and budgetary grant support from DAC&FW. DAC&FW meets the expenses on software and its customization for the States and is providing it for free. DAC&FW is also giving a grant as one time fixed cost up to ₹30 lakhs per Mandi (other than to the private mandis) for installation of the e-market platform. Around 6500 APMCs operate throughout the country of which 585 district level mandis in States/UTs desirous of joining are planned to be linked by NAM. 470 mandis are planned to be integrated by March 2017 and the remaining by March 2018.\n\nThe Cabinet Committee on Economic Affairs had approved a Central Sector Scheme for Promotion of National Agricultural Market through Agri-Tech Infrastructure Fund (ATIF). The government has allocated ₹200 crores to the newly created ATIF. With this fund SFAC will implement NAM for three years from 2015-16 to 2017-18. Each market is given ₹30 lakhs by the department.\n\n\nBuyers like large retailers, processors or exporters will be able to source commodities from any mandi in India thereby reducing the inter-mediation cost.Their physical presence and dependence on intermediaries will not be needed.\n\nNAM will increase the number of traders and the competition among them increases. This translates into stable prices and availability to the consumers.\n\nThere will be reduction in book keeping and reporting system as it will be generated automatically. Monitoring and regulation of traders and commission agents becomes easy. Transparency in the process eliminates the scope of manipulation of tendering/auctioning process. Market allocation fee will increase due to accounting of all transactions taking place in the market. It will reduce the manpower requirements as the tendering/auctioning process is carried out electronically. For instance, the system declares the winner of lots within few seconds. It eliminates information asymmetry as all the activities of an APMC can be known directly from the website.\n\nNAM aims to improve the marketing aspect of the agriculture sector. With one license for the entire state and single point levy, an entire state becomes a market and the market fragmentation within the same state gets abolished. It will improve the supply chain of commodities and reduces wastages.\n\n\n"}
{"id": "48845429", "url": "https://en.wikipedia.org/wiki?curid=48845429", "title": "Enadenotucirev", "text": "Enadenotucirev\n\nEnadenotucirev is an investigational oncolytic virus that is entering clinical trials for various cancers.\n\nIt is an oncolytic A11/Ad3 Chimeric Group B Adenovirus, previously described as oncolytic vaccine ColoAd1.\n\nIt is intended to be modified with additional genes to treat different conditions.\n\nIn Jan 2015 the European Medicines Agency's (EMA) Committee for Orphan Medical Products (COMP) gave a positive review of enadenotucirev to treat platinum-resistant ovarian cancer. \n\n Two phase 1 trials have been registered: EVOLVE and OCTAVE (in ovarian cancer).\n\nIn 2014, positive results from EVOLVE were reported.\n\nOCTAVE started phase 1b in March 2016 and is due to complete around March 2017.\n\nThe \"Study of Pembrolizumab In Combination with Enadenotucirev\" (SPICE) study (not yet on clinicaltrials.gov) will assess if enadenotucirev can reverse resistance to checkpoint inhibitors for various tumour types, including metastatic colorectal cancer.\n\n"}
{"id": "45461344", "url": "https://en.wikipedia.org/wiki?curid=45461344", "title": "Eric Burhop", "text": "Eric Burhop\n\nEric Henry Stoneley Burhop, (31 January 191122 January 1980) was an Australian physicist and humanitarian.\n\nA graduate of the University of Melbourne, Burhop was awarded an 1851 Exhibition Scholarship to study at the Cavendish Laboratory under Lord Rutherford. Under the supervision of Mark Oliphant, he investigated nuclear fusion. He produced a non-relativistic theory of the Auger effect in 1935, followed by a relativistic treatment the following year. He later wrote a monograph on the subject. He returned to the University of Melbourne as a lecturer in 1936, and helped Professor Thomas Laby build up the physics department there.\n\nDuring the Second World War he worked in the Radiophysics Laboratory in Sydney, where he produced a laboratory model of a cavity magnetron. In September 1942, he returned to Melbourne as the officer in charge of the Radar Research Laboratory, where he continued the development of cavity magnetrons and reflex klystrons for radar sets. In May 1944, he became one of three Australian physicists who worked on the Manhattan Project, which created the first atomic bombs.\n\nIn early 1945, Harrie Massey offered Burhop a position as a lecturer in the Mathematics Department at University College, London. He fostered international cooperation in nuclear physics. As part of a five-nation study of K mesons and their interaction with atomic nuclei that went on for several years, his group produced a wealth of new results, including the first observation of a double lambda hypernucleus. He spent a year on secondment to CERN, as secretary of a committee that recommended the construction of the Intersecting Storage Rings and the Super Proton Synchrotron. In 1974 and 1975, an international team under his leadership carried out a successful search for the (charmed lambda baryon).\n\nEric Henry Stoneley Burhop was born in Hobart, Tasmania, on 31 January 1911, the third child of two Salvation Army officers, Henry Augustus Burhop and his wife Bertha née Head. He had two older sisters, Edna and Vera. His family was not wealthy, and they moved frequently owing to the nature of his parents' evangelical work. The family moved to Ballarat in 1923, where he attended Ballarat High School for most of his secondary education, receiving his leaving (Year 11) certificate in 1926. He transferred to Melbourne High School for his final year.\n\nBurhop won a scholarship, and entered the University of Melbourne in 1928. He initially studied civil engineering, but switched to science after two years, and majored in physics. In 1929, he was awarded a bursary that provided financial assistance. He graduated in 1931 with a bachelor of science BSc degree with first class honours in physics. He then earned a Bachelor of Arts (BA), also with first class honours, in mathematics in 1932, and a master of science in physics in 1933.\n\nFor a master's research problem, Professor Thomas Laby had Burhop investigate the probability K shell ionisation by electron impact by measuring the intensity of the resultant X-ray emissions. This aroused an interest in the Auger effect, a subject in which he would later become an authority. By contrast, his master's thesis on \"The Band Spectra of Diatomic Molecules\" had little influence on his later work.\n\nThe thesis was good enough, though, for Burhop to be awarded an 1851 Exhibition Scholarship to study at Cambridge University's Cavendish Laboratory under Lord Rutherford in 1933. The scholarship included a first-class ticket to London on the liner RMS \"Oronsay\". At this time the Cavendish Laboratory was one of the leading centres of physics in the world. In 1932, Cavendish laboratory scientists John Cockcroft and Ernest Walton split the atomic nucleus, James Chadwick discovered the neutron, and Patrick Blackett and Giuseppe Occhialini confirmed the existence of the positron. Burhop was supervised by a fellow Australian, Mark Oliphant. He was initially assigned a task of investigating the diffusion of positive alkali metal ions on hot metal surfaces, but soon switch to a more interesting subject, the measurement of the excitation function for the pairs of deuterons producing a triton through nuclear fusion:\nBurhop was able to observe the reaction at energies of less than 8 keV. He continued his investigations of X-rays and the Auger effect. He conducted investigation of the probabilities of the ionisation of the K and L shells of silver, using the Born approximation. In 1935, he produced a non-relativistic theory of the Auger effect . This followed by a relativistic treatment in 1936 with Harrie Massey, another Australian expatriate fellow at the Cavendish. His work on the Auger effect would culminate in a monograph, \"The Auger Effect and Other Radiationless Transitions\" (1952).\n\nBefore coming to Cambridge, Burhop had not engaged in political activities. At Cambridge he encountered political debate generated by the suffering caused by the Great Depression and the rise of fascism in Europe. While he rejected his parents' faith in favour of rationalist science, he did not reject their values, especially that of compassion for others, and there was no estrangement between them. At Cambridge he embraced socialism, and believed that scientists had a moral responsibility to society, one that included fighting fascism.\n\nBurhop returned to Australia in 1936, taking up a position as a lecturer at the University of Melbourne, where Laby was eager to build up the Physics Department by adding expertise in the latest developments in nuclear physics. He married his fiancée, Winifred Ida Stevens, on 23 December 1936 in a Salvation Army ceremony. They had a daughter and two sons. He completed his Cambridge doctor of philosophy (PhD) degree under Laby's supervision in 1938. It was in three parts: \"The ionization and reorganization of an atom in an inner shell, with special reference to the Dirac theory of the electron\"; \"Some problems in atomic disintegration\"; and \"Note on the migration of atoms on a surface\". Burhop established the first research program in the field in an Australian university, employing scientific equipment that he brought back from Britain. The centrepiece of the research effort was a 300 keV accelerator that produced a homogeneous neutron beam, which he commissioned in August 1939. He gave lectures on modern physics to the undergraduates, and on quantum mechanics to the postgraduates.\n\nAfter the outbreak of the Second World War in 1939, the Physics Department worked on the development of optical munitions, particularly aluminised mirrors for aerial photography. In February 1942, Oliphant persuaded Laby to release Burhop and Leslie Martin to work on microwave radar at the Radiophysics Laboratory in Sydney. Burhop and Martin produced a laboratory model of a cavity magnetron on 23 July 1942. Their magnetron was based on an overseas design, but made entirely from local components so that it could be manufactured in Australia. In September 1942 he returned to the University of Melbourne as the officer in charge of the Radar Research Laboratory, an outpost of the Radiophysics Laboratory, where his task was turning his cavity magnetrons and reflex klystrons from prototypes into production models. Eventually, over 2,000 radar sets were produced in Australia.\n\nIn January 1944, Oliphant had Sir David Rivett, the head of the Council for Scientific and Industrial Research, release Burhop to work on the Manhattan Project, the Allied effort to create atomic bombs. In May 1944, Burhop joined Oliphant's British Mission at the Ernest Lawrence's Radiation Laboratory at the University of California in Berkeley. He was one of three Australian physicists working on the Manhattan Project, all at Berkeley, the others being Oliphant and Massey. The Radiation Laboratory's task was to develop an electromagnetic isotope separation process. Burhop worked with David Bohm in Massey's Theoretical Group, studying the characteristics of electric discharges in magnetic fields, today known as Bohm diffusion. They also studied the ionisation of uranium compounds used as feed in the electromagnetic uranium enrichment process such as uranium tetrachloride (UCl) and uranium hexafluoride (UF). Burhop's work involved the occasional visit to the Manhattan Project's Y-12 electromagnetic faculty at Oak Ridge, Tennessee.\n\nIn early 1945, Massey offered Burhop a position as a lecturer at University College, London, in the Mathematics Department, of which Massey was the departmental head. He had to wait until he was released by the University of Melbourne, and did not reach London until after the war ended in August 1945. His wife and family, who had stayed in Australia while he was working in the United States, joined him in London some months later. Living and working conditions in London were much worse than in California or Australia. Wartime shortages persisted, and the college had suffered bombing damage, so the Mathematics Department were located in temporary quarters. He was promoted to reader in 1949. Massey became head of the Physics Department in 1950, and Burhop moved there too. He became a professor in 1960, and Dean of the Faculty of Science in 1967.\n\nBurhop listed \"furtherance of international scientific cooperation\" as one of his hobbies. He worked with the University of Edinburgh and the University of Padua to establish a high-altitude cloud chamber on Marmolada that commenced operation in 1953. In 1957, he collaborated with Occhialini and C. F. Powell on a five-nation study of K mesons and their interaction with atomic nuclei that went on for several years, and produced a wealth of new results, including the first observation of a double lambda hypernucleus. He spent the 1962–63 academic year on secondment to CERN, and was secretary of a committee chaired by Edoardo Amaldi that drew up its policy for accelerator development. The machines the committee recommended, the Intersecting Storage Rings and the Super Proton Synchrotron (SPS) were built, and became an important part of physics research in Europe for decades to come.\n\nWhen Burhop took charge of the Bubble Chamber Group at University College in 1967, he was quick to grasp the advantages of heavy liquid bubble chambers for studying neutrino interactions, and steered the group towards participation in joint European ventures, using the Gargamelle. The group's discovery of neutral currents in 1973 was a milestone on the road to the theoretical unification of electromagnetism with the weak force. In 1974 and 1975, with the help of Robert R. Wilson, the director of the Fermilab in the United States, an international team from Fermilab and seven European laboratories under Burhop's leadership carried out a search for a new particle, the existence of which Burhop had predicted in 1963. He had suggested that neutrino interactions could create short-lived (perhaps as low as 10 s) particles that could be detected with the use of nuclear emulsion. Experiment E247 at Fermilab successfully detected particles with a lifetime of the order of 10 s. A follow-up experiment WA17 with the SPS confirmed the existence of the (charmed lambda baryon), with a flight time of 7.3±0.1 x 10 s.\n\nThe Australian Security Intelligence Organisation (ASIO) opened a file on Burhop in 1948, believing him to be a secret member of the Communist Party of Australia, and an associate of Ian Milner, who was known through Venona intercepts to have passed secret documents to the Soviet Union. In July 1951, the British government cancelled his passport when he accepted an offer to travel to the Soviet Union. A new passport was issued after he gave the Foreign Secretary a written assurance that he would not seek to travel to the Soviet Union or other Iron Curtain countries.\n\nLike many scientists who had worked on the Manhattan Project, Burhop was concerned about the dangers of nuclear weapons, and addressed over 500 public meetings to raise awareness of the subject. He had been a founding member of the Australian Association of Scientific Workers in 1939, and after the war became chairman of the Atomic Science Committee of the Association of Scientific Workers, and a member of its Science Policy Committee, in Britain. In this capacity he helped organise the Pugwash Conferences on Science and World Affairs in 1957. He founded the British Society for Social Responsibility in Science in 1969. He was president of his local branch of the Association of University Teachers from 1970 to 1972, and of the World Federation of Scientific Workers from 1971 to 1980.\n\nOver the years Burhop received a number of honours and awards. He was elected a Fellow of the Royal Society in 1963, and delivered its Rutherford Memorial Lecture in 1979. He was elected a Foreign Member of the German Democratic Republic's Academy of Sciences in 1971. He received the Joliot-Curie Medal of Peace in 1965, the Lenin Peace Prize in 1972, and the Bulgarian Order of Saints Cyril and Methodius in 1973.\n\nBurhop retired in 1978. He died in Camden, London, from empyema as a result of stomach cancer on 22 January 1980. He was survived by his wife and children. His papers are in the University College, London, Special Collections.\n\n"}
{"id": "24590863", "url": "https://en.wikipedia.org/wiki?curid=24590863", "title": "Extended aeration", "text": "Extended aeration\n\nExtended aeration is a method of sewage treatment using modified activated sludge procedures. It is preferred for relatively small waste loads, where lower operating efficiency is offset by mechanical simplicity.\n\nMechanized sewage treatment typically includes settling in a primary clarifier, followed by biological treatment and a secondary clarifier. Both clarifiers produce waste sludge requiring sewage sludge treatment and disposal. Activated sludge agitates a portion of the secondary clarifier sludge in the primary clarifier effluent. Remaining secondary sludge and all primary sludge typically require digestion prior to disposal.\n\nExtended aeration agitates all incoming waste in the sludge from a single clarifier. The combined sludge starts with a higher concentration of inert solids than typical secondary sludge and the longer mixing time required for digestion of primary solids in addition to dissolved organics produces aged sludge requiring greater mixing energy input per unit of waste oxidized.\n\nExtended aeration is typically used in prefabricated \"package plants\" intended to minimize design costs for waste disposal from small communities, tourist facilities, or schools. In comparison to traditional activated sludge, longer mixing time with aged sludge offers a stable biological ecosystem better adapted for effectively treating waste load fluctuations from variable occupancy situations. Supplemental feeding with something like sugar is sometimes used to sustain sludge microbial populations during periods of low occupancy; but population response to variable food characteristics is unpredictable, and supplemental feeding increases waste sludge volumes. Sludge may be periodically removed by septic tank pumping trucks as sludge volume approaches storage capacity.\n\n\n"}
{"id": "222858", "url": "https://en.wikipedia.org/wiki?curid=222858", "title": "Financial cryptography", "text": "Financial cryptography\n\nFinancial cryptography is the use of cryptography in applications in which financial loss could result from subversion of the message system. Financial cryptography is distinguished from traditional cryptography in that for most of recorded history, cryptography has been used almost entirely for military and diplomatic purposes. \n\nFinancial cryptography includes the mechanisms and algorithms necessary for the protection of financial transfers, in addition to the creation of new forms of money. Proof of work and various auction protocols fall under the umbrella of Financial cryptography. Hashcash is being used to limit spam.\n\nFinancial cryptography has been seen to have a very broad scope of application. Ian Grigg sees financial cryptography in seven layers, being the combination of seven distinct disciplines: cryptography, software engineering, rights, accounting, governance, value, and financial applications. Business failures can often be traced to the absence of one or more of these disciplines, or to poor application of them. This views Financial cryptography as an appropriately cross-discipline subject. Indeed, inevitably so, given that finance and cryptography are each built upon multiple disciplines.\n\nCryptographers think the field originated from the work of Dr David Chaum who invented the blind signature. The blind signature is a special form of a cryptographic signature which allowed virtual coins to be signed without the signer seeing the actual coin. It permitted a form of digital token money that prevented traceability. This form is sometimes known as Digital currency. \n\nA system that was widely used during the 1970s-1990s and previously developed cryptographic mechanism is the Data Encryption Standard, which was used primarily for the protection of electronic funds transfers. However, it was the work of David Chaum that excited the cryptography community about the potential of encrypted messages as actual financial instruments. \n\nAs part of a business model, Financial cryptography followed the guide of cryptography and only the simplest ideas were adopted. Account money systems protected by SSL such as PayPal and e-gold were relatively successful, but more innovative mechanisms, including blinded token money, were not. \n\nFinancial cryptography is to some extent organized around the annual meeting of the \"International Financial Cryptography Association\", which is held each year in a different location.\n\n\n"}
{"id": "1331549", "url": "https://en.wikipedia.org/wiki?curid=1331549", "title": "Fly ash", "text": "Fly ash\n\nFly ash or flue ash, also known as pulverised fuel ash in the United Kingdom, is a coal combustion product that is composed of the particulates (fine particles of burned fuel) that are driven out of coal-fired boilers together with the flue gases. Ash that falls to the bottom of the boiler is called bottom ash. In modern coal-fired power plants, fly ash is generally captured by electrostatic precipitators or other particle filtration equipment before the flue gases reach the chimneys. Together with bottom ash removed from the bottom of the boiler, it is known as coal ash. Depending upon the source and composition of the coal being burned, the components of fly ash vary considerably, but all fly ash includes substantial amounts of silicon dioxide (SiO) (both amorphous and crystalline), aluminium oxide (AlO) and calcium oxide (CaO), the main mineral compounds in coal-bearing rock strata.\n\nThe minor constituents of fly ash depend upon the specific coal bed composition but may include one or more of the following elements or compounds found in trace concentrations (up to hundreds ppm): arsenic, beryllium, boron, cadmium, chromium, hexavalent chromium, cobalt, lead, manganese, mercury, molybdenum, selenium, strontium, thallium, and vanadium, along with very small concentrations of dioxins and PAH compounds. It also has unburnt carbon.\n\nIn the past, fly ash was generally released into the atmosphere, but air pollution control standards now require that it be captured prior to release by fitting pollution control equipment. In the United States, fly ash is generally stored at coal power plants or placed in landfills. About 43% is recycled, often used as a pozzolan to produce hydraulic cement or hydraulic plaster and a replacement or partial replacement for Portland cement in concrete production. Pozzolans ensure the setting of concrete and plaster and provide concrete with more protection from wet conditions and chemical attack.\n\nIn the case that fly (or bottom) ash is not produced from coal, for example when solid waste is incinerated in a waste-to-energy facility to produce electricity, the ash may contain higher levels of contaminants than coal ash. In that case the ash produced is often classified as hazardous waste.\n\nFly ash material solidifies while suspended in the exhaust gases and is collected by electrostatic precipitators or filter bags. Since the particles solidify rapidly while suspended in the exhaust gases, fly ash particles are generally spherical in shape and range in size from 0.5 µm to 300 µm. The major consequence of the rapid cooling is that few minerals have time to crystallize, and that mainly amorphous, quenched glass remains. Nevertheless, some refractory phases in the pulverized coal do not melt (entirely), and remain crystalline. In consequence, fly ash is a heterogeneous material. SiO, AlO, FeO and occasionally CaO are the main chemical components present in fly ashes. The mineralogy of fly ashes is very diverse. The main phases encountered are a glass phase, together with quartz, mullite and the iron oxides hematite, magnetite and/or maghemite. Other phases often identified are cristobalite, anhydrite, free lime, periclase, calcite, sylvite, halite, portlandite, rutile and anatase. The Ca-bearing minerals anorthite, gehlenite, akermanite and various calcium silicates and calcium aluminates identical to those found in Portland cement can be identified in Ca-rich fly ashes.\nThe mercury content can reach , but is generally included in the range 0.01 - 1 ppm for bituminous coal.\nThe concentrations of other trace elements vary as well according to the kind of coal combusted to form it. In fact, in the case of bituminous coal, with the notable exception of boron, trace element concentrations are generally similar to trace element concentrations in unpolluted soils.\n\nTwo classes of fly ash are defined by ASTM C618: Class F fly ash and Class C fly ash. The chief difference between these classes is the amount of calcium, silica, alumina, and iron content in the ash. The chemical properties of the fly ash are largely influenced by the chemical content of the coal burned (i.e., anthracite, bituminous, and lignite).\n\nNot all fly ashes meet ASTM C618 requirements, although depending on the application, this may not be necessary. Fly ash used as a cement replacement must meet strict construction standards, but no standard environmental regulations have been established in the United States. Seventy-five percent of the fly ash must have a fineness of 45 µm or less, and have a carbon content, measured by the loss on ignition (LOI), of less than 4%. In the US, LOI must be under 6%. The particle size distribution of raw fly ash tends to fluctuate constantly, due to changing performance of the coal mills and the boiler performance. This makes it necessary that, if fly ash is used in an optimal way to replace cement in concrete production, it must be processed using beneficiation methods like mechanical air classification. But if fly ash is used as a filler to replace sand in concrete production, unbeneficiated fly ash with higher LOI can be also used. Especially important is the ongoing quality verification. This is mainly expressed by quality control seals like the Bureau of Indian Standards mark or the DCL mark of the Dubai Municipality.\n\nThe burning of harder, older anthracite and bituminous coal typically produces Class F fly ash. This fly ash is pozzolanic in nature, and contains less than 7% lime (CaO). Possessing pozzolanic properties, the glassy silica and alumina of Class F fly ash requires a cementing agent, such as Portland cement, quicklime, or hydrated lime—mixed with water to react and produce cementitious compounds. Alternatively, adding a chemical activator such as sodium silicate (water glass) to a Class F ash can form a geopolymer.\n\nFly ash produced from the burning of younger lignite or sub-bituminous coal, in addition to having pozzolanic properties, also has some self-cementing properties. In the presence of water, Class C fly ash hardens and gets stronger over time. Class C fly ash generally contains more than 20% lime (CaO). Unlike Class F, self-cementing Class C fly ash does not require an activator. Alkali and sulfate () contents are generally higher in Class C fly ashes.\n\nAt least one US manufacturer has announced a fly ash brick containing up to 50% Class C fly ash. Testing shows the bricks meet or exceed the performance standards listed in ASTM C 216 for conventional clay brick. It is also within the allowable shrinkage limits for concrete brick in ASTM C 55, Standard Specification for Concrete Building Brick. It is estimated that the production method used in fly ash bricks will reduce the embodied energy of masonry construction by up to 90%. Bricks and pavers were expected to be available in commercial quantities before the end of 2009.\n\nIn the past, fly ash produced from coal combustion was simply entrained in flue gases and dispersed into the atmosphere. This created environmental and health concerns that prompted laws that have reduced fly ash emissions to less than 1% of ash produced. Worldwide, more than 65% of fly ash produced from coal power stations is disposed of in landfills and ash ponds, although companies such as Duke Energy are starting initiatives to excavate coal ash basins due to the negative environmental impact involved.\n\nThe recycling of fly ash has become an increasing concern in recent years due to increasing landfill costs and current interest in sustainable development. , coal-fired power plants in the US reported producing 71.1 million tons of fly ash, of which 29.1 million tons were reused in various applications. If the nearly 42 million tons of unused fly ash had been recycled, it would have reduced the need for approximately of landfill space. Other environmental benefits to recycling fly ash includes reducing the demand for virgin materials that would need quarrying and cheap substitution for materials such as Portland cement.\n\nAs of 2006, about 125 million tons of coal-combustion byproducts, including fly ash, were produced in the US each year, with about 43% of that amount used in commercial applications, according to the American Coal Ash Association Web site. As of early 2008, the United States Environmental Protection Agency (EPA) hoped that figure would increase to 50% as of 2011.\n\nThere is no US governmental registration or labelling of fly ash utilization in the different sectors of the economy - industry, infrastructures and agriculture. Fly ash utilization survey data, acknowledged as incomplete, are published annually by the American Coal Ash Association.\n\nCoal ash uses include (approximately in order of decreasing importance):\n\nOther applications include cosmetics, toothpaste, kitchen counter tops, floor and ceiling tiles, bowling balls, flotation devices, stucco, utensils, tool handles, picture frames, auto bodies and boat hulls, cellular concrete, geopolymers, roofing tiles, roofing granules, decking, fireplace mantles, cinder block, PVC pipe, Structural Insulated Panels, house siding and trim, running tracks, blasting grit, recycled plastic lumber, utility poles and crossarms, railway sleepers, highway sound barriers, marine pilings, doors, window frames, scaffolding, sign posts, crypts, columns, railroad ties, vinyl flooring, paving stones, shower stalls, garage doors, park benches, landscape timbers, planters, pallet blocks, molding, mail boxes, artificial reef, binding agent, paints and undercoatings, metal castings, and filler in wood and plastic products.\n\nOwing to its pozzolanic properties, fly ash is used as a replacement for Portland cement in concrete. The use of fly ash as a pozzolanic ingredient was recognized as early as 1914, although the earliest noteworthy study of its use was in 1937. Roman structures such as aqueducts or the Pantheon in Rome used volcanic ash or pozzolana (which possesses similar properties to fly ash) as pozzolan in their concrete. As pozzolan greatly improves the strength and durability of concrete, the use of ash is a key factor in their preservation.\n\nUse of fly ash as a partial replacement for Portland cement is particularly suitable but not limited to Class C fly ashes. Class \"F\" fly ashes can have volatile effects on the entrained air content of concrete, causing reduced resistance to freeze/thaw damage. Fly ash often replaces up to 30% by mass of Portland cement, but can be used in higher dosages in certain applications. In some cases, fly ash can add to the concrete's final strength and increase its chemical resistance and durability.\n\nFly ash can significantly improve the workability of concrete. Recently, techniques have been developed to replace partial cement with high-volume fly ash (50% cement replacement). For roller-compacted concrete (RCC)[used in dam construction], replacement values of 70% have been achieved with processed fly ash at the Ghatghar dam project in Maharashtra, India. Due to the spherical shape of fly ash particles, it can increase workability of cement while reducing water demand. Proponents of fly ash claim that replacing Portland cement with fly ash reduces the greenhouse gas \"footprint\" of concrete, as the production of one ton of Portland cement generates approximately one ton of CO, compared to no CO generated with fly ash. New fly ash production, i.e., the burning of coal, produces approximately 20 to 30 tons of CO per ton of fly ash. Since the worldwide production of Portland cement is expected to reach nearly 2 billion tons by 2010, replacement of any large portion of this cement by fly ash could significantly reduce carbon emissions associated with construction, as long as the comparison takes the production of fly ash as a given.\n\nFly ash properties are unusual among engineering materials. Unlike soils typically used for embankment construction, fly ash has a large uniformity coefficient and it consists of clay-sized particles. Engineering properties that affect the use of fly ash in embankments include grain size distribution, compaction characteristics, shear strength, compressibility, permeability, and frost susceptibility. Nearly all the types of fly ash used in embankments are Class F.\n\nSoil stabilization is the permanent physical and chemical alteration of soils to enhance their physical properties. Stabilization can increase the shear strength of a soil and/or control the shrink-swell properties of a soil, thus improving the load-bearing capacity of a sub-grade to support pavements and foundations. Stabilization can be used to treat a wide range of sub-grade materials from expansive clays to granular materials. Stabilization can be achieved with a variety of chemical additives including lime, fly ash, and Portland cement. Proper design and testing is an important component of any stabilization project. This allows for the establishment of design criteria, and determination of the proper chemical additive and admixture rate that achieves the desired engineering properties. Stabilization process benefits can include: Higher resistance (R) values, Reduction in plasticity, Lower permeability, Reduction of pavement thickness, Elimination of excavation - material hauling/handling - and base importation, Aids compaction, Provides \"all-weather\" access onto and within projects sites. Another form of soil treatment closely related to soil stabilization is soil modification, sometimes referred to as \"mud drying\" or soil conditioning. Although some stabilization inherently occurs in soil modification, the distinction is that soil modification is merely a means to reduce the moisture content of a soil to expedite construction, whereas stabilization can substantially increase the shear strength of a material such that it can be incorporated into the project's structural design. The determining factors associated with soil modification vs soil stabilization may be the existing moisture content, the end use of the soil structure and ultimately the cost benefit provided. Equipment for the stabilization and modification processes include: chemical additive spreaders, soil mixers (reclaimers), portable pneumatic storage containers, water trucks, deep lift compactors, motor graders.\n\nFly ash is also used as a component in the production of flowable fill (also called controlled low strength material, or CLSM), which is used as self-leveling, self-compact backfill material in lieu of compacted earth or granular fill. The strength of flowable fill mixes can range from 50 to 1,200 lbf/in² (0.3 to 8.3 MPa), depending on the design requirements of the project in question. Flowable fill includes mixtures of Portland cement and filler material, and can contain mineral admixtures. Fly ash can replace either the Portland cement or fine aggregate (in most cases, river sand) as a filler material. High fly ash content mixes contain nearly all fly ash, with a small percentage of Portland cement and enough water to make the mix flowable. Low fly ash content mixes contain a high percentage of filler material, and a low percentage of fly ash, Portland cement, and water. Class F fly ash is best suited for high fly ash content mixes, whereas Class C fly ash is almost always used in low fly ash content mixes.\n\nAsphalt concrete is a composite material consisting of an asphalt binder and mineral aggregate. Both Class F and Class C fly ash can typically be used as a mineral filler to fill the voids and provide contact points between larger aggregate particles in asphalt concrete mixes. This application is used in conjunction, or as a replacement for, other binders (such as Portland cement or hydrated lime). For use in asphalt pavement, the fly ash must meet mineral filler specifications outlined in ASTM D242. The hydrophobic nature of fly ash gives pavements better resistance to stripping. Fly ash has also been shown to increase the stiffness of the asphalt matrix, improving rutting resistance and increasing mix durability.\n\nMore recently, fly ash has been used as a component in geopolymers, where the reactivity of the fly ash glasses can be used to create a binder similar to a hydrated Portland cement in appearance, but with potentially superior properties, including reduced CO emissions, depending on the formulation.\n\nAnother application of using fly ash is in roller compacted concrete dams. Many dams in the US have been constructed with high fly ash contents. Fly ash lowers the heat of hydration allowing thicker placements to occur. Data for these can be found at the US Bureau of Reclamation. This has also been demonstrated in the Ghatghar Dam Project in India.\n\nThere are several techniques for manufacturing construction bricks from fly ash, producing a wide variety of products. One type of fly ash brick is manufactured by mixing fly ash with an equal amount of clay, then firing in a kiln at about This approach has the principal benefit of reducing the amount of clay required. Another type of fly ash brick is made by mixing soil, plaster of paris, fly ash and water, and allowing the mixture to dry. Because no heat is required, this technique reduces air pollution. More modern manufacturing processes use a greater proportion of fly ash, and a high pressure manufacturing technique, which produces high strength bricks with environmental benefits.\n\nIn the United Kingdom, fly ash has been used for over fifty years to make concrete building blocks. They are widely used for the inner skin of cavity walls. They are naturally more thermally insulating than blocks made with other aggregates. \n\nAsh bricks have been used in house construction in Windhoek, Namibia since the 1970s. There is, however, a problem with the bricks in that they tend to fail or produce unsightly pop-outs. This happens when the bricks come into contact with moisture and a chemical reaction occurs causing the bricks to expand. \n\nIn India, fly ash bricks are used for construction. Leading manufacturers use an industrial standard known as \"Pulverized fuel ash for lime-Pozzolana mixture\" using over 75% post-industrial recycled waste, and a compression process. This produces a strong product with good insulation properties and environmental benefits.\n\nHollow fly ash can be infiltrated by molten metal to form solid, alumina encased spheres. Fly ash can also be mixed with molten metal and cast to reduce overall weight and density, due to the low density of fly ash. Research is underway to incorporate fly ash into lead acid batteries in a lead calcium tin fly ash composite in an effort to reduce weight of the battery.\n\nFly ash, in view of its alkalinity and water absorption capacity, may be used in combination with other alkaline materials to transform sewage sludge into organic fertilizer or biofuel.\n\nFly ash, when treated with sodium hydroxide, appears to function well as a catalyst for converting polyethylene into substance similar to crude oil in a high-temperature process called pyrolysis.\n\nIn addition, fly ash, mainly class C, may be used in the stabilization/solidification process of hazardous wastes and contaminated soils. For example, the Rhenipal process uses fly ash as an admixture to stabilize sewage sludge and other toxic sludges. This process has been used since 1996 to stabilize large amounts of chromium(VI) contaminated leather sludges in Alcanena, Portugal.\n\nSince coal contains trace levels of trace elements (like e.g. arsenic, barium, beryllium, boron, cadmium, chromium, thallium, selenium, molybdenum and mercury), fly ash obtained after combustion of this coal contains enhanced concentrations of these elements, and therefore the potential of the ash to cause groundwater pollution needs to be evaluated. In the USA there are documented cases of groundwater pollution which followed ash disposal or utilization without the necessary protection means.\n\nIn 2014, residents living near the Buck Steam Station in Dukeville, North Carolina, were told that \"coal ash pits near their homes could be leaching dangerous materials into groundwater.\"\n\nWhen newly produced the dust is strongly alkaline; a pH as high as 11 is known, and >9 is normal. It leaches a solution dominated by sodium and sulfate, with enough boron (>15 mg l) to kill most plants, though coastal species often tolerate the salinity and boron to grow on young ash lagoons. In dry conditions these solutes rise to the surface to form a hard salt crust, impeding all plant growth, though hardy grass species such as \"Vulpia myuros\" can later colonise it. A \"waxy\" layer has been known to form in some locations which inhibits root penetration, however, mixing crushed rock into the top layer has been found to inhibit the formation of the \"waxy\" layer.\n\nAs the ash weathers, its salinity, boron level and pH all fall; the former two are largely removed from surface layers after 5 years outdoors, while pH declines towards 7 in a generally linear fashion at a rate of about 1 pH unit per 20 years. The floral succession approximates to that of a coastal dune system, without the wind-blown deposition, so salt-tolerant plants are replaced by an attractive sward of legumes and perennials before turning to birch/willow scrub woodland. A notable feature are the \"Dactylorhiza\" orchids which often form spectacular colonies 10–20 years post dumping, only to fade away again as the woodland thickens. Hydroseeding is often used to establish vegetation onto PFA due to the inhospitable conditions of most sites.\n\nWhere fly ash is stored in bulk, it is usually stored wet rather than dry to minimize fugitive dust. The resulting impoundments (ponds) are typically large and stable for long periods, but any breach of their dams or bunding is rapid and on a massive scale.\n\nIn December 2008, the collapse of an embankment at an impoundment for wet storage of fly ash at the Tennessee Valley Authority's Kingston Fossil Plant caused a major release of 5.4 million cubic yards of coal fly ash, damaging 3 homes and flowing into the Emory River. Cleanup costs may exceed $1.2 billion. This spill was followed a few weeks later by a smaller TVA-plant spill in Alabama, which contaminated Widows Creek and the Tennessee River.\n\nIn 2014, 39,000 tons of ash and 27 million gallons (100,000 cubic meters) of contaminated water spilled into the Dan River near Eden, NC from a closed North Carolina coal-fired power plant that is owned by Duke Energy. It is currently the third worst coal ash spill ever to happen in the United States.\n\nNew regulations published in the Federal Register on December 19, 2015, stipulate a comprehensive set of rules and guidelines for safe disposal and storage. Designed to prevent pond failures and protect groundwater, enhanced inspection, record keeping and monitoring is specified. Procedures for closure are also included and include capping, liners, and dewatering.\n\nFly ash contains trace concentrations of heavy metals and other substances that are known to be detrimental to health in sufficient quantities. Potentially toxic trace elements in coal include arsenic, beryllium, cadmium, barium, chromium, copper, lead, mercury, molybdenum, nickel, radium, selenium, thorium, uranium, vanadium, and zinc. Approximately 10% of the mass of coals burned in the United States consists of unburnable mineral material that becomes ash, so the concentration of most trace elements in coal ash is approximately 10 times the concentration in the original coal. A 1997 analysis by the United States Geological Survey (USGS) found that fly ash typically contained 10 to 30 ppm of uranium, comparable to the levels found in some granitic rocks, phosphate rock, and black shale.\n\nIn 2000, the United States EPA said that coal fly ash did not need to be regulated as a hazardous waste. Studies by the USGS and others of radioactive elements in coal ash have concluded that fly ash compares with common soils or rocks and should not be the source of alarm. However, community and environmental organizations have documented numerous environmental contamination and damage concerns.\n\nA revised risk assessment approach may change the way coal combustion wastes (CCW) are regulated, according to an August 2007 EPA notice in the Federal Register. In June 2008, the United States House of Representatives held an oversight hearing on the Federal government's role in addressing health and environmental risks of fly ash.\n\nCrystalline silica and lime along with toxic chemicals represent exposure risks to human health and the environment. Fly ash contains crystalline silica which is known to cause lung disease, in particular silicosis, if inhaled. Crystalline silica is listed by the IARC and US National Toxicology Program as a known human carcinogen.\n\nLime (CaO) reacts with water (HO) to form calcium hydroxide [Ca(OH)], giving fly ash a pH somewhere between 10 and 12, a medium to strong base. This can also cause lung damage if present in sufficient quantities.\n\nMaterial Safety Data Sheets recommend a number of safety precautions be taken when handling or working with fly ash. These include wearing protective goggles, respirators and disposable clothing and avoiding agitating the fly ash in order to minimize the amount which becomes airborne.\n\nThe National Academy of Sciences noted in 2007 that \"the presence of high contaminant levels in many CCR (coal combustion residue) leachates may create human health and ecological concerns\".\n\nAfter a long regulatory process, the EPA published a final ruling in December 2014, which establishes that coal fly ash is regulated on the federal level as \"non-hazardous\" waste according to the Resource Conservation and Recovery Act (RCRA). Coal Combustion Residuals (CCR's) are listed in the subtitle D (rather than under subtitle C dealing with hazardous waste, which was also considered).\n\nIn March 2018, as coal power producers complied with federal Obama-era groundwater monitoring rules at coal ash disposal sites, the Trump Administration announced plans to let states regulate the facilities similar to how they regulate municipal trash landfills. The United States EPA announced a series of proposals it said would save the utility industry up to $100 million annually in compliance costs.\n\n“Today’s coal ash proposal embodies EPA’s commitment to our state partners by providing them with the ability to incorporate flexibilities into their coal ash permit programs based on the needs of their states,” said EPA Administrator Scott Pruitt in a written statement on March 1.\n\nAccording to \"Daily Energy Insider\", \"The rules would apply to ash sites at more than 400 coal-fired power plants across the nation. The proposed rules involve the material that remains after power plants burn coal to create steam and drive turbine electric generators. The ash is called coal combustion residual, or CCR, in regulatory parlance.\"\n\n"}
{"id": "32464022", "url": "https://en.wikipedia.org/wiki?curid=32464022", "title": "Global Innovation Index", "text": "Global Innovation Index\n\nThe Global Innovation Index (GII) is an annual ranking of countries by their capacity for, and success in, innovation. It is published by Cornell University, INSEAD, and the World Intellectual Property Organization, in partnership with other organisations and institutions, and is based on both subjective and objective data derived from several sources, including the International Telecommunication Union, the World Bank and the World Economic Forum. The index was started in 2007 by INSEAD and \"World Business\", a British magazine. The GII is commonly used by corporate and government officials to compare countries by their level of innovation.\n\nThe GII is computed by taking a simple average of the scores in two sub-indices, the Innovation Input Index and Innovation Output Index, which are composed of five and two pillars respectively. Each of these pillars describe an attribute of innovation, and comprise up to five indicators, and their score is calculated by the weighted average method.\n\n\n"}
{"id": "620529", "url": "https://en.wikipedia.org/wiki?curid=620529", "title": "Grader", "text": "Grader\n\nA grader, also commonly referred to as a road grader or a motor grader, is a construction machine with a long blade used to create a flat surface during the grading process. Although the earliest models were towed behind horses or other powered equipment, most modern graders contain an engine so are known as \"motor graders\". Typical models have three axles, with the engine and cab situated above the rear axles at one end of the vehicle and a third axle at the front end of the vehicle, with the blade in between. Most motor graders drive the rear axles in tandem, but some also add front wheel drive to improve grading capability. Many graders also have optional attachments for the rear of the machine which can be ripper, scarifier, blade, or compactor. In certain countries, for example in Finland, almost every grader is equipped with a second blade that is placed in front of the front axle. For snowplowing and some dirt grading operations, a side blade can also be mounted. Some construction personnel refer to the entire machine as \"the blade\". Capacities range from a blade width of 2.50 to 7.30 m (8 to 24 ft) and engines from 93–373 kW (125–500 hp). Certain graders can operate multiple attachments, or be designed for specialized tasks like underground mining. \n\nIn civil engineering, the grader's purpose is to \"finish grade\" (to refine or set precisely). The angle, tilt (or pitch) and height of the grader's blade can be adjusted to achieve precision grading of a surface. The \"rough grading\" is performed by heavy equipment or engineering vehicles such as scrapers and bulldozers.\n\nGraders are commonly used in the construction and maintenance of dirt roads and gravel roads. In the construction of paved roads they are used to prepare the base course to create a wide flat surface upon which to place the road surface. Graders are also used to set native soil or gravel foundation pads to finish grade prior to the construction of large buildings. Graders can produce inclined surfaces, to give cant (camber or sideslope) to roads. In some countries they are used to produce drainage ditches with shallow V-shaped cross-sections on either side of highways.\n\nSteering for a motor grader is typically accomplished via a steering wheel or joystick controlling the angle of the front wheels, but many models also allow frame articulation between the front and rear axles, which allows a smaller turning radius in addition to allowing the operator to adjust articulation angle to aid in the efficiency of moving material. Other implement functions are typically hydraulically powered, and can be directly controlled by levers, or by joystick inputs or electronic switches controlling Electrohydraulic servo valves.\n\nA more recent innovation is the outfitting of graders with grade control technologies, such as those manufactured by Topcon Positioning Systems, Inc., Trimble Navigation, Leica Geosystems or Mikrofyn for precise grade control and (potentially) \"stakeless\" construction. Manufacturers such as Caterpillar have also began to integrate these technologies into their machines out of the factory. ()\n\nEarly graders were drawn by people and draft animals. The era of motorization by traction engines, steam tractors, motor trucks, and tractors saw such towed graders grow in size and productivity. The first self-propelled grader was made in 1920 by the Russell Grader Manufacturing Company, which called it the Russell Motor Hi-Way Patrol. These early graders were created by adding the grader blade as an attachment to a generalist tractor unit. After purchasing the company in 1928, Caterpillar went on to truly integrate the tractor and grader into one design—at the same time replacing crawler tracks with wheels to yield the first rubber-tire self-propelled grader, the Caterpillar Auto Patrol, released in 1931.\n\nIn addition to their use in road construction, graders may also be used to perform roughly equivalent work.\n\nIn some locales such as Northern Europe, Canada, and places in the United States, graders are often used in municipal and residential snow removal. In scrubland and grassland areas of Australia and Africa, graders are often an essential piece of equipment on ranches, large farms, and plantations to make dirt tracks where the absence of rocks and trees means bulldozers are not required. \n\n\n"}
{"id": "17176461", "url": "https://en.wikipedia.org/wiki?curid=17176461", "title": "Hexanite", "text": "Hexanite\n\nHexanit was a castable German military explosive developed early in the 20th century before the First World War for the Kaiserliche Marine, intended to augment supplies of trinitrotoluene (TNT), which were then in short supply. Hexanite is significantly more powerful than TNT on its own. The most common hexanite formula (by weight) was 60% TNT and 40% hexanitrodiphenylamine.\n\nTypically, hexanite was used in underwater naval weapons e.g. warheads for the G7a and G7e series torpedoes and the 300 kilogram main explosive charge in aluminium-cased buoyant, moored \"EMF\" magnetic mines capable of being laid by U-boats in 200, 300 or 500 metres of water. \n\nThis explosive is regarded as obsolete, so any hexanite-filled munitions encountered will be in the form of unexploded ordnance dating from the Second World War.\n\nThe Japanese used this in World War II as explosive compound type 97 & 98. \n\n"}
{"id": "35715231", "url": "https://en.wikipedia.org/wiki?curid=35715231", "title": "Human–animal breastfeeding", "text": "Human–animal breastfeeding\n\nHuman–animal breastfeeding has been practiced in many different cultures in many time periods. The practice of breastfeeding or suckling between humans and other species has gone in both directions: human females sometimes breastfeed young animals, and animals are used to suckle babies and children. Animals were used as substitute wet nurses for infants, particularly after the rise of syphilis increased the health risks of wet nursing. Goats and donkeys were widely used to feed abandoned babies in foundling hospitals in 18th- and 19th-century Europe. Breastfeeding animals has also been practised, whether for health reasons – such as to toughen the nipples and improve the flow of milk – or for religious and cultural purposes. A wide variety of animals have been used for this purpose, including puppies, kittens, piglets and monkeys.\n\nTerracotta feeding bottles surviving from the third millennium BC in Sumeria indicate that children who were not being breastfed were receiving animal milk, probably from cows. It is possible that some infants directly suckled lactating animals, which served as alternatives to wet nurses. Unless another lactating woman was available, a mother who lacked enough breast milk was likely to lose her child. To avert that possibility if a wet nurse was not available, an animal such as a donkey, cow, goat, sheep or dog could be employed. Suckling directly was preferable to milking an animal and giving the milk, as contamination by microbes during the milking process could lead to the infant contracting a deadly diarrheal disease. It was not until as late as the 1870s that stored animal milk became safe to drink due to the invention of pasteurisation and sterilisation.\n\nThe suckling of animals by infants was a repeated theme in classical mythology. Most famously, twin brothers Romulus and Remus (the former founded Rome) were portrayed as having been raised by a she-wolf which suckled the infants, as depicted in the iconic image of the Capitoline Wolf. The Greek god Zeus was said to have been brought up by Amalthea, portrayed variously as a goat who suckled the god or as a nymph who brought him up on the milk of her goat. Similarly, Telephus, the son of the demigod Heracles, was suckled by a deer. Several famous ancient historical figures were claimed to have been suckled by animals; Cyrus I of Persia was said to have been suckled by a dog, while mares supposedly suckled Croesus, Xerxes and Lysimachus. In reality, though, such stories probably owed more to myth-making about such prominent figures, as they were used as evidence of their future greatness.\nStories of abandoned children being brought up by animal mothers such as she-wolves and bears were widespread in Europe from the Middle Ages and into modern times. One real-life case was that of Peter the Wild Boy, found in northern Germany in 1724. His coarse, curly hair was attributed to his being (supposedly) suckled by a bear, based on the premise that characteristics of the animal foster mother had been transmitted to him via her milk. (It is now thought he had Pitt-Hopkins syndrome, a condition unidentified until 1978.)\n\nThe belief that animal characteristics could be transmitted via milk was widely held; the Swedish scientist Carl Linnaeus thought that being suckled by lionesses conferred great courage. Goats were thought to transmit a libidinous character and some preferred to employ donkeys as wet nurses instead, as they were thought to be more moral animals. In modern Egypt, though, donkeys were disfavoured as wet nurses as it was thought that a child suckled on donkeys' milk would acquire the animal's stupidity and obstinacy.\n\nThe teachings of the Quran discourage giving babies animal milk on the basis that animal traits might be transmitted along with it, though the Jewish Talmud permits children to suckle animals if the child's welfare dictates it. Human milk was thought to transmit character traits as well; in 19th century France a law was proposed to ban disreputable mothers from nursing their own children so that their immoral traits would not be transmitted via their milk.\n\nGoats have often been used to suckle human babies and infants. The Khoikhoi of southern Africa were reported to tie their babies to the bellies of female goats so that they could feed there. In the 18th and 19th centuries, goats were widely used in Europe as alternatives to human wet nurses, as they were easier to obtain, cheaper to use and safer, in that they were less prone to passing on diseases. This use of animals was already a well-established practice in rural France and Italy; Pierre Brouzet, the personal physician of Louis XV of France, wrote of how he had seen \"some peasants who have no other nurses but ewes, and these peasants were as strong and vigorous as others.\" In 1816, a German writer named Conrad A. Zweirlein overheard a conversation at a fashionable resort about the problems of wet nurses and responded by writing a book called \"The Goat as the Best and Most Agreeable Wet Nurse\", which popularised the use of the animals for many years.\n\nOne important use of goats for suckling concerned the feeding and attempted cure of babies born with congenital syphilis inherited from their mothers. Liquid compounds laced with mercury were fed to nanny goats – if they refused to drink them, honey was recommended as a way of disguising the metallic taste – or were ingested into the goat's bloodstream via a deliberately inflicted wound on the animal's leg that was covered with an ointment containing mercury. The mercury would accumulate in the goats' milk and was passed into the syphilitic babies when they suckled at the goats' teats. This method did have some effect of improving the infants' mortality rates, though the goats tended to die prematurely of mercury poisoning.\n\nIn France, homes for foundlings (abandoned babies) often kept large numbers of goats to feed the infants, as they were considered less problematic than lower-class wet nurses. In some institutions, nurses (nannies) carried the infants to the goats; elsewhere, the goats came to the infants. Alphonse Le Roy described how goats were used at the foundling hospital in Aix-en-Provence in 1775: \"The cribs are arranged in a large room in 2 ranks. Each goat which comes to feed enters bleating and goes to hunt the infant which has been given it, pushes back the covering with its horns and straddles the crib to give suck to the infant. Since that time they have raised very large numbers [of infants] in that hospital.\"\n\nIn 19th-century Ireland, foundlings from Dublin were \"sent to the mountains of Wicklow, to feed upon the goats' milk. As the children grew older, the goats came to know them, and became very tame; so that the infant sought the goat, and was suckled by it as he would have been by a human wet nurse. These children throve remarkably well.\" Donkeys were preferred in England; as one writer has put it, \"nothing was more picturesque than the spectacle of babies, held under the bellies of the donkeys in the stable adjoining the infants' ward, sucking contentedly the teats of the docile donkeys.\" In Brittany, attempts were made around 1900 to employ sows as wet nurses but foundered due to opposition to the use of pigs for this purpose.\n\nThe breastfeeding by humans of animals is a practice that is widely attested historically and continues to be practised today by some cultures. The reasons for this are varied: to feed young animals, to drain a woman's breasts, to promote lactation, to harden the nipples before a baby is born, to prevent conception, and so on. \n\nEnglish and German physicians between the 16th and 18th centuries recommended using puppies to \"draw\" the mother's breasts, and in 1799 the German Friedrich Benjamin Osiander reported that in Göttingen women suckled young dogs to dislodge nodules from their breasts. An example of the practice being used for health reasons comes from late 18th century England. When the writer Mary Wollstonecraft was dying of puerperal fever following the birth of her second daughter, the doctor ordered that puppies be applied to her breasts to draw off the milk, possibly with the intention of helping her womb to contract to expel the infected placenta that was slowly poisoning her. \n\nAnimals have widely been used to toughen the nipples and maintain the mother's milk supply. In Persia and Turkey puppies were used for this purpose. The same method was practised in the United States in the early 19th century; William Potts Dewees recommended in 1825 that from the eighth month of pregnancy, expectant mothers should regularly use a puppy to harden the nipples, improve breast secretion and prevent inflammation of the breasts. The practice seems to have fallen out of favour by 1847, as Dewees suggested using a nurse or some other skilled person to carry out this task rather than an animal. \n\nReligious and ceremonial reasons have also been a factor. Saint Veronica Giuliani (1660–1727), an Italian nun and mystic, was known for taking a lamb to bed with her and suckling it as a symbol of the Lamb of God. In far northern Japan, the Ainu people are noted for holding an annual bear festival at which a captured bear, raised and suckled by the women, is sacrificed. \n\nBears were also suckled by the Itelmens of the Kamchatka Peninsula of Russia but in their case for economic reasons, to benefit from the meat when the bear was grown and to obtain highly prized bear bile for use in traditional medicine.\n\nTribal peoples around the world have breastfed many types of animal. Travelers in Guyana observed native women breastfeeding a variety of animals, including monkeys, opossums, pacas, agoutis, peccaries and deer. Native Canadians and Americans often breastfed young dogs; an observer commented that the Pima people of Arizona \"withdrew their breasts sooner from their own infants than from young dogs.\"\n\nIn the present day, the act of breastfeeding animals has been used as a sometimes controversial artistic statement. The album art for \"Boys for Pele\" by Tori Amos includes a photograph of the singer breastfeeding a piglet. In Ireland, 22-year-old model and PETA member Agata Dembiecka became the focus of controversy in 2010 when a calendar issued by an animal rescue charity featured a photograph of her suckling a puppy.\n\n"}
{"id": "5914872", "url": "https://en.wikipedia.org/wiki?curid=5914872", "title": "Hung Pham", "text": "Hung Pham\n\nHung Pham or Hung Kim Pham (born Phạm Kim Hưng; October 2, 1963 in Saigon) is a former politician and information expert in Alberta, Canada. He formerly served as a member of Legislative Assembly of Alberta, Canada.\n\nBorn and growing up in Saigon, the capital of South Vietnam, Hung Pham was one of the boat people leaving the country after Vietnam War. Coming to Canada in 1980 at the age of 16 without English, teenager Hung Pham worked as a part-time janitor to make his living while going to high-school in Calgary.\n\nHung Pham completed his high-school education in 1982 and subsequently took the computer science university transfer program at Mount Royal College, where he received many awards and scholarships for outstanding academic achievements. Pham then transferred to University of Calgary in 1984 and graduated from the university in 1986 with two bachelor's degrees with Distinction: Bachelor of Science with Distinction in Computer Science and Bachelor of Science with Distinction in Pure Mathematics. He went on to take Master program at University of Calgary, specializing in Artificial Intelligence.\n\nStarting as a prominent Information Analyst with Canalta Data Services in 1988, Hung Pham joined The City of Calgary in 1989 as a Database Technical Analyst. He was promoted by The City of Calgary to be Senior Programmer Analyst in 1991.\n\nAs an active member of many non-profit organizations, Hung Pham helped Calgary Police Service with her multicultural and crime prevention programs. He was elected President of Calgary Vietnamese Canadian Association in 1990. At that position, Pham promoted programs to help low income workers. With his well-known speech delivered to the community on 1991 Tết celebration event, Pham publicly declared his programs to help cleaners in Calgary.\n\nElected to Legislative Assembly of Alberta (Progressive Conservative), Canada at the age of 29, Hung Pham was the youngest Member of Legislative Assembly (MLA) in Alberta Legislature at the time. He is also the very first Vietnamese ever elected to a state legislature outside of Vietnam in history.\n\nRe-elected four times later, Hung Pham has held many high-rank positions in Alberta Government. In 2006, he was promoted by Premier Ed Stelmach to be a member of Alberta Treasury Board, the legislative body responsible for all expenditures and budgets of Alberta (Premier Ed Stelmach is also a member of the board).\n\nIn January, 2008, Hung Pham announced that he would not seek re-election. \n\n"}
{"id": "15723416", "url": "https://en.wikipedia.org/wiki?curid=15723416", "title": "IEC 62264", "text": "IEC 62264\n\nIEC 62264 consists of the following parts detailed in separate IEC 62264 standard documents:\n\n"}
{"id": "22656356", "url": "https://en.wikipedia.org/wiki?curid=22656356", "title": "Industrial Heritage Trail", "text": "Industrial Heritage Trail\n\nThe Industrial Heritage Trail () links tourist attractions related to the industrial heritage in the Ruhr area in Germany. It is a part of the European Route of Industrial Heritage.\n\nThe trail network connects museums and exhibitions that present the industrial revolution during the last 750 years in the Ruhr area. It includes 400 km of road network and about 700 km of bicycle tracks. \n\nThere are 52 main attractions on the trail.\n\n\n"}
{"id": "1446729", "url": "https://en.wikipedia.org/wiki?curid=1446729", "title": "Institute of Biosciences and Technology", "text": "Institute of Biosciences and Technology\n\nThe Institute of Biosciences and Technology (IBT), a component of the Texas A&M Health Science Center represents the Texas A&M University System, one of the two main Texas state university systems, in the state's and world's largest medical center, the Texas Medical Center, in Houston, Texas. The institute provides a bridge between Texas A&M University System scientists and other institutions' researchers working in the Texas Medical Center and the biomedical and biotechnology research community in Houston. It emphasizes collaboration between member scientists and others working in all the fields of the biosciences and biotechnology. IBT encourages its scientists to transfer discoveries made in their laboratories to the clinic and marketplace.\n\nThe IBT was first established as part of the Texas A&M University System with the broad goal to expand biotechnology in the system. Plans for an institutional campus in Houston came to fruition in 1986 under Mr. David Eller (Chairman of the Texas A&M University System Board of Regents) and Dr. Richard Wainerdi (Chairman and CEO of the Texas Medical Center). Dr. Eugene G. Sander, head of the Department of Biochemistry and Biophysics in the Texas A&M College of Agriculture and Life Sciences (COALS) was named first director of the institute. The Robert A. Welch Foundation endowed a $1 million Robert A. Welch Chair in 1990.\n\nIn 1988 the institute administration was moved from the Texas A&M University System to the Texas A&M University under the leadership of Dr. Charles Arntzen, Vice Chancellor of Agriculture and Dean of the College of Life Sciences and Agriculture (COALS). Arntzen secured a grant from the United States Department of Agriculture for $12.5 million and other private donations including one from Texas oilman Albert B. Alkek to build the Albert B. Alkek Institute of Biosciences and Technology Building in the Texas Medical Center at a cost of $21.5 million. The 11-story building stands on the former site of the historic Shamrock Hotel. Under Arntzen’s leadership, Centers for Animal Genetics, Advanced Invertebrate Molecular Sciences and Biotechnology Policy and Ethics were established and funding secured for additional endowed chairs from the Allen, John S. Dunn, and Neva and Wesley West (see Wesley West) Foundations. In 1990, he recruited Dr. Robert D. Wells into the Welch Chair as Chairman of the Department of Biochemistry and Biophysics (COALS) and Director of the IBT. Under Wells directorship (1990–1994), Centers for Genome Research, Macromolecular Design, Extracellular Matrix Biology, Crop Biotechnology, Animal Biotechnology, Genome Informatics and Cancer Biology were established.\n\nDr. Fuller W. Bazer was Director from 1994-2001. Under the Bazer administration by agreement between the Texas A&M University President Ray Bowen and The University of Texas Health Science Center at Houston President Dr. David Low, the University of Texas' Institute of Molecular Medicine for Prevention of Human Disease (Hans Muller-Eberhard, Director) was established on two floors of the IBT building. During this period similar cooperative efforts between Texas A&M University and The University of Texas Health Science Center at Houston resulted in acceptance of IBT Texas A&M faculty into the UT Graduate School of Biomedical Sciences. The building became a hub for the Texas GigaPop and similar high speed, broad bandwidth internet activities of the Texas A&M University’s Academy for Advanced Telecommunications and Distance Learning. In 1997 the IBT became a college level unit of the newly established Texas A&M Health Science Center.\n\nDr. Richard Finnell served as director 2001-2005 under whose leadership the Texas Institute for Genomic Medicine (TIGM) was established. Dr. Robert Schwartz served as director for 2006-2009. In 2007 a self-study called for a strategic plan in which the IBT would be the founding basic research element of a broader Texas A&M Health Science Center Houston Campus representing all components of the Health Science Center in Houston. Dr. David Carlson, the Texas A&M Health Science Center Vice President of Research and Graduate Studies served as the interim IBT Director 2009-2011. In 2011 Dr. Cheryl Lyn Walker was appointed as the current Director of IBT.\n\nUnder its college level status within the Texas A&M Health Science Center, the IBT consists of thematic Centers (departments) led by a Center Director and tenured and tenure-track faculty, research track faculty and associate and adjunct faculty.\n\nThe approximately $10 million operating budget of the IBT consists of about 30% state funds, 50% federal grants and 20% private sources. Current Centers include Cancer and Stem Cell Biology (formerly Cancer Biology and Nutrition), Epigenetics and Disease Prevention, Environmental and Genetic Medicine, Infectious and Inflammatory Diseases (formerly Extracellular matrix Biology) and Translational Cancer Research.\n\nThe IBT Graduate Program consists of students from the Texas A&M Health Science Center Graduate School of Biomedical Sciences, other Texas A&M System programs, The University of Texas Health Science Center at Houston Graduate School of Biomedical Sciences and other programs in Texas Medical Center institutions in which faculty have joint graduate faculty appointments. Students do research in IBT laboratories while taking formal classroom instruction at other Texas Medical Center institutions.\n\nIn addition to the IBT, the Texas A&M Health Science Center Alkek Building hosts the Houston division of the Texas Institute for Genomic Medicine, representatives of the Texas A&M Health Science Center College of Medicine, School of Rural Public Health and Baylor College of Dentistry. Two floors of the building are dedicated to a partnership with the Department of Medicine of The University of Texas M. D. Anderson Cancer Center.\n\n"}
{"id": "6536977", "url": "https://en.wikipedia.org/wiki?curid=6536977", "title": "Isopropyl nitrate", "text": "Isopropyl nitrate\n\nIsopropyl nitrate (IPN, 2-propyl nitrate) is a colorless liquid monopropellant. It is used as a diesel cetane improver. IPN is a low-sensitivity explosive, with a detonation velocity of approximately 5400 m/s.\n\nIsopropyl nitrate is extremely flammable and burns with a practically invisible flame. This presents unique hazards in its handling. The flame is significantly less luminous than hydrogen or methanol flame and is only visible due to the turbulent hot air it generates.\n\nIsopropyl nitrate was previously used in a jet engine rapid starting system for military interceptor aircraft (a requirement for fast scrambling, as it provided simultaneous, near-instantaneous engine starting for twin engines) and was known as AVPIN. The exhaust fumes from an AVPIN monopropellant engine may themselves be explosive, if mixed with further air. \n\nEarly systems, as used on the Gloster Javelin, used a simple pressurising cartridge and had a poor reputation for misfires, including engine-destroying explosions. Nevertheless, Rolls Royce was able to demonstrate that the fuel is extremely stable and that a cannon shell could be fired through a container of it with no dramatic effect. Isopropyl nitrate was also sold by the Turbonique company under the brand name \"Thermolene\" as a fuel for their line of turbine-powered \"drag axles\" and superchargers, during the 1960s.\n\nIt is officially classed as safe to transport provided it is labelled as flammable. Although engine starters do not require an air supply for their basic operation, air was supplied to later designs, such as that of the English Electric Lightning, by an automatic scavenge pump simply to control these fumes. The history of AVPIN use on the Lightning demonstrated it was both safe and effective in use. The reduced availability of AVPIN is now a restriction on the continued operation of some preserved military aircraft, such as Lightnings in the UK and South Africa and some are thus being converted to electric starting.\n\nIt has also been used as a fuel for power supply and actuation in guided weapons, notably in the British Royal Navy.\n\n"}
{"id": "29199932", "url": "https://en.wikipedia.org/wiki?curid=29199932", "title": "LiteOS", "text": "LiteOS\n\nHuawei LiteOS is a lightweight real-time operating system. It is an open source operating system for IoT smart terminals. It supports ARM (M0/3/4/7, A7/17/53, ARM9/11), X86,RISC-V, Microcontrollers of different architectures, follow the BSD 3. It supports 50+ development boards. Huawei LiteOS is part of Huawei's \"1+2+1\" Internet of Things solution. It has launched a number of open source development kits and industry solutions. Huawei hopes LiteOS to create an Android-like IoT operating system through open source.\n\nHuawei LiteOS features lightweight, low-power, fast-response, multi-sensor collaboration, multi-protocol interconnect connectivity, enabling IoT terminals to quickly access the network. Huawei LiteOS will make intelligent hardware development easier. Thereby accelerating the realization of the interconnection of all things.\n\nThe latest version is V2.1, which was released in May 2018.\n\nOn May 20, 2015, at the Huawei Network Conference, Huawei proposed the \"1+2+1\" Internet of Things solution and release the IoT operating system named Huawei LiteOS.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "52673586", "url": "https://en.wikipedia.org/wiki?curid=52673586", "title": "Luma Home", "text": "Luma Home\n\nLuma is a Wi-Fi solutions company based in Atlanta, Georgia. The company sells Wi-Fi routers using mesh networking to project a consistent Wi-Fi signal throughout private homes as well as a corresponding mobile app for added layers of security and parental controls.\n\nLuma was founded in 2014 by serial entrepreneur Dr. Paul Judge and Mike Van Bruinisse, who had worked together at cybersecurity firms CipherTrust and Purewire. The idea emerged as a response to what was perceived by the founders as poor Wi-Fi consistency throughout private homes and the need for increased security as the Internet of Things (IoT) becomes more pervasive. Judge said, “Cybersecurity must be built into the home network routers directly because IoT devices are vulnerable and users do not have the time to be the CISO [chief information security officer] of their home.”\n\nTo date, the company has been given $22 million in seed and Series A funding, including $12.5 million from Amazon’s Alexa fund, as well as former Beats by Dre CMO Omar Johnson, Yahoo chairman Maynard Webb and San Francisco 49ers CEO Jed York. In November, 2016, Luma announced that it had acquired Alexa integration, allowing it to be controlled by voice commands to Amazon Alexa.\n\nIn January 2018, Luma Home was acquired by Newell Brands for a reported $10 million. The brand will operate under First Alert.\n\n"}
{"id": "5106169", "url": "https://en.wikipedia.org/wiki?curid=5106169", "title": "Manure-derived synthetic crude oil", "text": "Manure-derived synthetic crude oil\n\nManure-derived synthetic crude oil is a synthetic bio-oil chemically engineered (converted) from animal or human manure. Research into the production of manure-derived synthetic fuel began with pig manure in 1996 at the University of Illinois at Urbana-Champaign by the research team led by professors Yuanhui Zhang and Lance Schideman. They developed a method for converting raw pig manure into bio-oil through thermal depolymerization (thermochemical conversion). This process uses a thermochemical conversion reactor to apply heat and pressure for breaking down carbohydrate materials. As a result, bio-oil, methane and carbon dioxide are produced.\n\nWith further research, large-scale chemical processing in a refinery-style environment could help process millions of gallons of \"pig biocrude\" per day. However, this technology is still in its infancy and could produce only of oil per of manure. In 2006, preparations for a construction of a pilot plant started. It is developed by Snapshot Energy, a start-up firm.\n\nAccording to the tests conducted by the National Institute of Standards and Technology pig manure biocrude produced by current technology contains 15% water, sulfur and char waste containing heavy metals, which should be removed to improve the quality of oil.\n\n"}
{"id": "11268193", "url": "https://en.wikipedia.org/wiki?curid=11268193", "title": "Micro-opto-mechanical systems", "text": "Micro-opto-mechanical systems\n\nMicro-Opto-Mechanical Systems (MOMS) are a special class of Micro Electro-Mechanical Systems (MEMS) which use optical and mechanical, but not electrical components.\n\n"}
{"id": "4550348", "url": "https://en.wikipedia.org/wiki?curid=4550348", "title": "Nanophotonics", "text": "Nanophotonics\n\nNanophotonics or nano-optics is the study of the behavior of light on the nanometer scale, and of the interaction of nanometer-scale objects with light. It is a branch of optics, optical engineering, electrical engineering, and nanotechnology. It often (but not exclusively) involves metallic components, which can transport and focus light via surface plasmon polaritons.\n\nThe term \"nano-optics\", just like the term \"optics\", usually refers to situations involving ultraviolet, visible, and near-infrared light (free-space wavelengths from 300 to 1200 nanometers).\n\nNormal optical components, like lenses and microscopes, generally cannot normally focus light to nanometer (deep subwavelength) scales, because of the diffraction limit (Rayleigh criterion). Nevertheless, it is possible to squeeze light into a nanometer scale using other techniques like, for example, surface plasmons, localized surface plasmons around nanoscale metal objects, and the nanoscale apertures and nanoscale sharp tips used in near-field scanning optical microscopy (NSOM) and photoassisted scanning tunnelling microscopy.\n\nNanophotonics researchers pursue a very wide variety of goals, in fields ranging from biochemistry to electrical engineering. A few of these goals are summarized below.\n\nIf light can be squeezed into a small volume, it can be absorbed and detected by a small detector. Small photodetectors tend to have a variety of desirable properties including low noise, high speed, and low voltage and power.\n\nSmall lasers have various desirable properties for optical communication including low threshold current (which helps power efficiency) and fast modulation (which means more data transmission). Very small lasers require subwavelength optical cavities. An example is spasers, the surface plasmon version of lasers.\n\nIntegrated circuits are made using photolithography, i.e. exposure to light. In order to make very small transistors, the light needs to be focused into extremely sharp images. Using various techniques such as immersion lithography and phase-shifting photomasks, it has indeed been possible to make images much finer than the wavelength—for example, drawing 30 nm lines using 193 nm light. Plasmonic techniques have also been proposed for this application.\n\nHeat-assisted magnetic recording is a nanophotonic approach to increasing the amount of data that a magnetic disk drive can store. It requires a laser to heat a tiny, subwavelength area of the magnetic material before writing data. The magnetic write-head would have metal optical components to concentrate light at the right location.\n\nMiniaturization in optoelectronics, for example the miniaturization of transistors in integrated circuits, has improved their speed and cost. However, optoelectronic circuits can only be miniaturized if the optical components are shrunk along with the electronic components. This is relevant for on-chip optical communication (i.e. passing information from one part of a microchip to another by sending light through optical waveguides, instead of changing the voltage on a wire).\n\nSolar cells often work best when the light is absorbed very close to the surface, both because electrons near the surface have a better chance of being collected, and because the device can be made thinner, which reduces cost. Researchers have investigated a variety of nanophotonic techniques to intensify light in the optimal locations within a solar cell.\n\n\"Using nanophotonics to create high peak intensities\": If a given amount of light energy is squeezed into a smaller and smaller volume (\"hot-spot\"), the intensity in the hot-spot gets larger and larger. This is especially helpful in nonlinear optics; an example is surface enhanced Raman scattering. It also allows sensitive spectroscopy measurements of even single molecules located in the hot-spot, unlike traditional spectroscopy methods which take an average over millions or billions of molecules.\n\nOne goal of nanophotonics is to construct a so-called \"superlens\", which would use metamaterials (see below) or other techniques to create images that are more accurate than the diffraction limit (deep subwavelength).\n\nNear-field scanning optical microscope (NSOM or SNOM) is a quite different nanophotonic technique that accomplishes the same goal of taking images with resolution far smaller than the wavelength. It involves raster-scanning a very sharp tip or very small aperture over the surface to be imaged.\n\nNear-field microscopy refers more generally to any technique using the near-field (see below) to achieve nanoscale, subwavelength resolution. For example, dual polarization interferometry has picometer resolution in the vertical plane above the waveguide surface.\n\nMetals are an effective way to confine light to far below the wavelength. This was originally used in radio and microwave engineering, where metal antennas and waveguides may be hundreds of times smaller than the free-space wavelength. For a similar reason, visible light can be confined to the nano-scale via nano-sized metal structures, such as nano-sized structures, tips, gaps, etc. This effect is somewhat analogous to a lightning rod, where the field concentrates at the tip.\n\nThis effect is fundamentally based on the fact that the permittivity of the metal is very large and negative. At very high frequencies (near and above the plasma frequency, usually ultraviolet), the permittivity of a metal is not so large, and the metal stops being useful for concentrating fields.\n\nMany nano-optics designs look like common microwave or radiowave circuits, but shrunk down by a factor of 100,000 or more. After all, radiowaves, microwaves, and visible light are all electromagnetic radiation; they differ only in frequency. So other things equal, a microwave circuit shrunk down by a factor of 100,000 will behave the same way but at 100,000 times higher frequency. For example, researchers have made nano-optical Yagi-Uda antennas following essentially the same design as used for radio Yagi-Uda antennas. \n\nMetallic parallel-plate waveguides (striplines), lumped-constant circuit elements such as inductance and capacitance (at visible light frequencies, the values of the latter being of the order of femtohenries and attofarads, respectively), and impedance-matching of dipole antennas to transmission lines, all familiar techniques at microwave frequencies, are some current areas of nanophotonics development. That said, there are a number of very important differences between nano-optics and scaled-down microwave circuits. For example, at optical frequency, metals behave much less like ideal conductors, and also exhibit interesting plasmon-related effects like kinetic inductance and surface plasmon resonance. Likewise, optical fields interact with semiconductors in a fundamentally different way than microwaves do.\n\nIf you take the Fourier transform of an object, it consists of different spatial frequencies. The higher frequencies correspond to the very fine features and sharp edges.\n\nWhen light is emitted by such an object, the light with very high spatial frequency forms an evanescent wave, which only exists in the near field (very close to the object, within a wavelength or two) and disappears in the far field. This is the origin of the diffraction limit, which says that when a lens images an object, the subwavelength information is blurred out.\n\nNano-photonics is primarily concerned with the near-field evanescent waves. For example, a superlens (mentioned above) would prevent the decay of the evanescent wave, allowing higher-resolution imaging.\n\nMetamaterials are artificial materials engineered to have properties that may not be found in nature. They are created by fabricating an array of structures much smaller than a wavelength. The small (nano) size of the structures is important: That way, light interacts with them as if they made up a uniform, continuous medium, rather than scattering off the individual structures.\n\n\n"}
{"id": "848130", "url": "https://en.wikipedia.org/wiki?curid=848130", "title": "Norman architecture", "text": "Norman architecture\n\nThe term Norman architecture is used to categorise styles of Romanesque architecture developed by the Normans in the various lands under their dominion or influence in the 11th and 12th centuries. In particular the term is traditionally used for English Romanesque architecture. The Normans introduced large numbers of castles and fortifications including Norman keeps, and at the same time monasteries, abbeys, churches and cathedrals, in a style characterised by the usual Romanesque rounded arches (particularly over windows and doorways) and especially massive proportions compared to other regional variations of the style.\n\nThese Romanesque styles originated in Normandy and became widespread in north western Europe, particularly in England, which contributed considerable development and has the largest number of surviving examples. At about the same time a Norman dynasty ruled in Sicily, producing a distinctive variation incorporating Byzantine and Saracen influences which is also known as Norman architecture, or alternatively as Sicilian Romanesque. Ancient Rome's invention of the arch is the basis of all Norman architecture.\n\nThe term may have originated with eighteenth-century antiquarians, but its usage in a sequence of styles has been attributed to Thomas Rickman in his 1817 work \"An Attempt to Discriminate the Styles of English Architecture from the Conquest to the Reformation\" which used the labels \"Norman, Early English, Decorated, and Perpendicular\". The more inclusive term \"romanesque\" was used of the Romance languages in English by 1715, and was applied to architecture of the eleventh and twelfth centuries from 1819. Although Edward the Confessor built Westminster Abbey in Romanesque style (now all replaced by later rebuildings) just before the Conquest, which is still believed to be the earliest major Romanesque building in England, no significant remaining Romanesque architecture in Britain can clearly be shown to predate the Conquest, although historians believe that many surviving \"Norman\" elements in buildings, nearly all churches, may well in fact be Anglo-Saxon.\n\nThe Norman arch is a defining point of Norman architecture. Grand archways are designed to evoke feelings of awe and are very commonly seen as the entrance to large religious buildings such as cathedrals.\n\nViking invaders arrived at the mouth of the river Seine in 911, at a time when Franks were fighting on horseback and Frankish lords were building castles. Over the next century the population of the territory ceded to the Vikings, now called Normans, adopted these customs as well as Christianity and the \"langue d'oïl\". Norman barons built timber castles on earthen mounds, beginning the development of motte-and-bailey castles, and great stone churches in the Romanesque style of the Franks. By 950, they were building stone keeps. The Normans were among the most travelled peoples of Europe, exposing them to a wide variety of cultural influences which became incorporated in their art and architecture. They elaborated on the early Christian basilica plan. Originally longitudinal with side aisles and an apse they began to add in towers, as at the Church of Saint-Étienne]] at Caen, in 1067. This would eventually form a model for the larger English cathedrals some 20 years later.\n\nIn England, Norman nobles and bishops had influence before the Norman Conquest of 1066, and Norman influences affected late Anglo-Saxon architecture. Edward the Confessor was brought up in Normandy and in 1042 brought masons to work on the first Romanesque building in England, Westminster Abbey. In 1051 he brought in Norman knights who built \"motte\" castles as a defence against the Welsh. Following the invasion, Normans rapidly constructed motte-and-bailey castles along with churches, abbeys, and more elaborate fortifications such as Norman stone keeps.\n\nThe buildings show massive proportions in simple geometries using small bands of sculpture. Paying attention to the concentrated spaces of capitals and round doorways as well as the tympanum under an arch. The \"Norman arch\" is the rounded, often with mouldings carved or incised onto it for decoration. chevron patterns, frequently termed \"zig-zag mouldings\", were a frequent signature of the Normans. The cruciform churches often had deep chancels and a square crossing tower which has remained a feature of English ecclesiastical architecture. Hundreds of parish churches were built and the great English cathedrals were founded from 1083.\n\nAfter a fire damaged Canterbury Cathedral in 1174 Norman masons introduced the new Gothic architecture. Around 1191 Wells Cathedral and Lincoln Cathedral brought in the English Gothic style, and Norman became increasingly a modest style of provincial building.\n\n\nBibliography\n\n\n\nScotland also came under early Norman influence with Norman nobles at the court of King Macbeth around 1050. His successor Máel Coluim III overthrew him with English and Norman assistance, and his queen, Margaret, encouraged the church. The Benedictine order founded a monastery at Dunfermline. Her sixth and youngest son, who became King David, built St. Margaret's Chapel at the start of the 12th century.\n\nKirkliston Parish Church. With rare examples of late 12th century \"Norman Transitional\" architecture[3\n\nThe Normans first landed in Ireland in 1169. Within five years earthwork castles were springing up, and in a further five, work was beginning on some of the earliest of the great stone castles. For example, Hugh de Lacy built a Motte-and-bailey castle on the site of the present day Trim Castle, County Meath, which was attacked and burned in 1173 by the Irish king Ruaidrí Ua Conchobair. De Lacy, however, then constructed a stone castle in its place, which enclosed over three acres within its walls, and this could not be burned down by the Irish. The years between 1177 and 1310 saw the construction of some of the greatest of the Norman castles in Ireland. The Normans settled mostly in an area in the east of Ireland, later known as the Pale, and among other buildings they constructed were Swords Castle in Fingal (North County Dublin), Dublin Castle and Carrickfergus Castle in County Antrim.\n\nThe Normans began constructing castles, their trademark architectural piece, in Italy from an early date. William Iron Arm built one at an unidentified location (Stridula) in Calabria in 1045. After the death of Robert Guiscard in 1085, the Mezzogiorno (peninsular southern Italy) experienced a series of civil wars and fell under the control of increasingly weaker princes. Revolts characterised the region until well into the twelfth century and minor lords sought to resist ducal or royal power from within their own castles. In the Molise, the Normans embarked on their most extensive castle-building programme and introduced the \"opus gallicum\" technique to Italy. Their clever use of the local stone artisans, together with the vast riches amassed from their enslaved population, made such tremendous feats possible. Some as majestic as those of the ancient Roman structures they tried to emulate.\n\nBesides the encastellation of the countryside, the Normans erected several religious buildings which still survive. They edified the shrine at Monte Sant'Angelo and built a mausoleum to the Hauteville family at Venosa. They also built many new Latin monasteries, including the famous foundation of . Other examples of great importance are the portal of the Shrine of Mary Queen of Anglona and the ambulatory and radiating chapels of the Aversa Cathedral. In Salerno, however, remember Fruscione Palace.\n\nHere is a list of Norman architecture in the Mezzogiorno : \n\nSicily's Norman period lasted from circa 1070 until about 1200. The architecture was decorated in gilded mosaics such as that at the cathedral at Monreale. The Palatine Chapel in Palermo built in 1130 is perhaps the strongest example of this. The interior of the dome, (itself a Byzantine feature), is decorated in a mosaic depicting Christ Pantocrator accompanied by his angels.\n\nDuring Sicily's later Norman era early Gothic influences can be detected such as those in the cathedral at Messina consecrated in 1197. However, here the high Gothic campanile is of a later date and should not be confused with the early Gothic built during the Norman period; which featured pointed arches and windows rather than the flying buttresses and pinnacles later to manifest themselves in the Gothic era.\n\n\nAfter its Norman conquest in 1091, Malta saw the construction of several Norman pieces of architecture. Many have been demolished and rebuilt over the years (especially after the 1693 Sicily earthquake which destroyed many old Norman buildings), however some fortresses and houses still exist in Mdina and Vittoriosa.\n\nAs master masons developed the style and experimented with ways of overcoming the geometric difficulties of groin vaulted ceilings, they introduced features such as the pointed arch that were later characterised as being Gothic in style. Architectural historians and scholars consider that a style must be assessed as an integral whole rather than an aggregate of features, and while some include these developments within the Norman or Romanesque styles, others describe them as transitional or \"Norman–Gothic Transitional\". A few websites use the term \"Norman Gothic\", but it is unclear whether they refer to the transitional style or to the Norman style as a whole.\n\nNeo-Norman architecture is a type of Romanesque Revival architecture based on Norman Romanesque architecture. There is sometimes confusion, especially in North America, between this style and revivalist versions of vernacular or later architecture of Normandy, such as the \"Norman farmhouse style\" popular for larger houses.\n\nRomanesque Revival versions focus on the arch and capitals, and decorated doorways. There are two examples in Manchester: the former Stock Exchange building and a synagogue in Fallowfield.\n\n\n\n"}
{"id": "14077738", "url": "https://en.wikipedia.org/wiki?curid=14077738", "title": "Optical power meter", "text": "Optical power meter\n\nAn optical power meter (OPM) is a device used measure the power in an optical signal. The term usually refers to a device for testing average power in fiber optic systems. Other general purpose light power measuring devices are usually called radiometers, photometers, laser power meters (can be photodiode sensors or thermopile laser sensors), light meters or lux meters.\n\nA typical optical power meter consists of a calibrated sensor, measuring amplifier and display.\nThe sensor primarily consists of a photodiode selected for the appropriate range of wavelengths and power levels.\nOn the display unit, the measured optical power and set wavelength is displayed. Power meters are calibrated using a traceable calibration standard such as a NIST standard.\n\nA traditional optical power meter responds to a broad spectrum of light, however the calibration is wavelength dependent. This is not normally an issue, since the test wavelength is usually known, however it has a couple of drawbacks. Firstly, the user must set the meter to the correct test wavelength, and secondly if there are other spurious wavelengths present, then wrong readings will result.\n\nSometimes optical power meters are combined with a different test function such as an Optical Light Source (OLS) or Visual Fault Locator (VFL), or may be a sub-system in a much larger instrument. When combined with a light source, the instrument is usually called an Optical Loss Test Set.\n\nOptical Loss Test Sets (OLTS) are available in dedicated hand held instruments and platform-based modules to suit various network architectures and test requirements. They are used to measure optical power and power loss, and reflectance and reflected power loss. The products may also be used as optical sources or optical power meters, or to measure optical return loss or event reflectance.\n\nThree types of equipment can be used to measure optical power loss:\n\nThe major semiconductor sensor types are Silicon (Si), Germanium (Ge) and Indium Gallium Arsenide (InGaAs). Additionally, these may be used with attenuating elements for high optical power testing, or wavelength selective elements so they only respond to particular wavelengths. These all operate in a similar type of circuit, however in addition to their basic wavelength response characteristics, each one has some other particular characteristics:\n\nAn important part of an optical power meter sensor, is the fiber optic connector interface. Careful optical design is required to avoid significant accuracy problems when used with the wide variety of fiber types and connectors typically encountered.\n\nAnother important component, is the sensor input amplifier. This needs very careful design to avoid significant performance degradation over a wide range of conditions.\n\nA typical OPM measures accurately under most conditions from about 0 dBm (1 milli Watt) to about -50 dBm (10 nano Watt), although the display range may be larger. Above 0 dBm is considered \"high power\", and specially adapted units may measure up to nearly + 30 dBm ( 1 Watt). Below -50 dBm is \"low power\", and specially adapted units may measure as low as -110 dBm. Irrespective of power meter specifications, testing below about -50 dBm tends to be sensitive to stray ambient light leaking into fibers or connectors. So when testing at \"low power\", some sort of test range / linearity verification (easily done with attenuators) is advisable. At low power levels, optical signal measurements tend to become noisy, so meters may become very slow due to use of a significant amount of signal averaging.\n\nTo calculate dBm from power meter output :\nThe linear-to-dBm calculation method is:\ndBm = 10 log ( P1 / P2 )\nwhere P1 = measured power level ( e.g. in mWatts )\n\nOptical Power Meter calibration and accuracy is a contentious issue. The accuracy of most primary reference standards (e.g. Weight, Time, Length, Volt etc.) is known to a high accuracy, typically of the order of 1 part in a billion. However the optical power standards maintained by NIST, are only defined to about one part in a thousand. By the time this accuracy has been further degraded through successive links, instrument calibration accuracy is usually only a few %. The most accurate field optical power meters claim 1% calibration accuracy. Comparatively, this is orders of magnitude less accurate than a typical electrical voltmeter.\n\nFurther, the in-use accuracy achieved is usually significantly lower than the claimed calibration accuracy, by the time additional factors are taken into account. In typical field applications, factors may include: ambient temperature, optical connector type, wavelength variations, linearity variations, beam geometry variations, detector saturation.\n\nTherefore, achieving a good level of practical instrument accuracy and linearity is something that requires considerable design skill, and care in manufacturing.\n\nA class of laboratory power meters has an extended sensitivity, of the order of -110 dBm. This is achieved by using a very small detector and lens combination, and also a mechanical light chopper at typically 270 Hz, so the meter actually measures AC light. This eliminates unavoidable dc electrical drift effects. If the light chopping is synchronized with an appropriate synchronous (or \"lock-in\") amplifier, further sensitivity gains are achieved. In practice, such instruments usually achieve lower absolute accuracy due to the small detector diode, and for the same reason, may only be accurate when coupled with singlemode fiber. Occasionally such an instrument may have a cooled detector, though with the modern abandonment of Germanium sensors, and the introduction of InGaAs sensors, this is now increasingly uncommon.\n\nOptical power meters usually display time averaged power. So for pulse measurements, the signal duty cycle must be known to calculate the peak power value. However, the instantaneous peak power must be less than the maximum meter reading, or the detector may saturate, resulting in a wrong average readings. Also, at low pulse repetition rates, some meters with data or tone detection may produce improper or no readings.\nA class of \"high power\" meters has some type of optical attenuating element in front of the detector, typically allowing about a 20 dB increase in maximum power reading. Above this level, an entirely different class of \"laser power meter\" instrument is used, usually based on thermal detection.\n\n\nTypical test automation features usually apply to loss testing applications, and include:\n\nAn increasingly common special-purpose OPM, commonly called a \"PON Power Meter\" is designed to hook into a live PON (Passive Optical Network) circuit, and simultaneously test the optical power in different directions and wavelengths. This unit is essentially a triple power meter, with a collection of wavelength filters and optical couplers. Proper calibration is complicated by the varying duty cycle of the measured optical signals. It may have a simple pass/ fail display, to facilitate easy use by operators with little expertise.\n\nWavelength sensitivity of fiber optic power meter is a problem when used a photodiode for voltage current measurement. If the temperature sensitive measurement replaces voltage-current measurement by photodiode the wavelength sensitivity of an OPM can be reduced. Thus if the photodiode is reverse biased by a constant voltage source and supplied with constant current, when triggered by light the junction dissipates power. The temperature of the junction rises and the temperature rise measured by thermistor is directly proportional to the Optical power. Due to constant current supply the reflection of power to photodiode is nearly zero and the transition to and fro of electrons between valance band and conduction band is stable.\n\n\n"}
{"id": "3409656", "url": "https://en.wikipedia.org/wiki?curid=3409656", "title": "Optimized power control", "text": "Optimized power control\n\nOptimized Power Control, also known as Optimum Power Calibration, abbreviated OPC, is a function of an optical disc recorder. It checks the proper writing power and reflection of the media in use, calculating the optimum laser power and adjusting it for writing the particular session. More sophisticated is Active OPC or Running OPC. Active OPC monitors writing power and reflection of the media in use, calculating the optimum laser power and adjusting it in real-time, which, in theory, should result in a better quality burn.\n\n"}
{"id": "27598561", "url": "https://en.wikipedia.org/wiki?curid=27598561", "title": "Oscilloscope history", "text": "Oscilloscope history\n\nThis article discusses the history and development of oscilloscope technology. The modern day digital oscilloscope grew out of multiple developments of analog oscilloscopes, which in turn grew out of the older oscillograph. The oscillograph started as a hand drawn chart which was later slightly automated. This then grew into galvanometer driven recorders and photographic recorders. Eventually, the cathode ray tube came along and displaced the oscillograph, eventually taking over the majority of the market when advancements such as triggers were added to them. With the lowering costs of digital circuitry, the digital oscilloscope has taken over the majority of the modern oscilloscope sales, with almost no makers offering analog cathode ray tube oscilloscopes. However, the oscillograph lives on to a degree in pen chart recorders for electrical signals.\n\nThe earliest method of creating an image of a waveform was through a laborious and painstaking process of measuring the voltage or current of a spinning rotor at specific points around the axis of the rotor, and noting the measurements taken with a galvanometer. By slowly advancing around the rotor, a general standing wave can be drawn on graphing paper by recording the degrees of rotation and the meter strength at each position.\n\nThis process was first partially automated by Jules François Joubert with his \"step-by-step\" method of wave form measurement. This consisted of a special single-contact commutator attached to the shaft of a spinning rotor. The contact point could be moved around the rotor following a precise degree indicator scale and the output appearing on a galvanometer, to be hand-graphed by the technician. This process could only produce a very rough waveform approximation since it was formed over a period of several thousand wave cycles, but it was the first step in the science of waveform imaging.\nThe first automated oscillographs used a galvanometer to move a pen across a scroll or drum of paper, capturing wave patterns onto a continuously moving scroll. Due to the relatively high-frequency speed of the waveforms compared to the slow reaction time of the mechanical components, the waveform image was not drawn directly but instead built up over a period of time by combining small pieces of many different waveforms, to create an averaged shape.\n\nThe device known as the Hospitalier Ondograph was based on this method of wave form measurement. It automatically charged a capacitor from each 100th wave, and discharged the stored energy through a recording galvanometer, with each successive charge of the capacitor being taken from a point a little farther along the wave. (Such wave-form measurements were still averaged over many hundreds of wave cycles but were more accurate than hand-drawn oscillograms.)\nIn order to permit direct measurement of waveforms it was necessary for the recording device to use a very low-mass measurement system that can move with sufficient speed to match the motion of the actual waves being measured. This was done with the development of the \"moving-coil oscillograph\" by William Duddell which in modern times is also referred to as a mirror galvanometer. This reduced the measurement device to a small mirror that could move at high speeds to match the waveform.\n\nTo perform a waveform measurement, a photographic slide would be dropped past a window where the light beam emerges, or a continuous roll of motion picture film would be scrolled across the aperture to record the waveform over time. Although the measurements were much more precise than the built-up paper recorders, there was still room for improvement due to having to develop the exposed images before they could be examined.\n\nIn the 1920s, a tiny tilting mirror attached to a diaphragm at the apex of a horn provided good response up to a few kHz, perhaps even 10 kHz. A time base, unsynchronized, was provided by a spinning mirror polygon, and a collimated beam of light from an arc lamp projected the waveform onto the lab wall or a screen.\nEven earlier, audio applied to a diaphragm on the gas feed to a flame made the flame height vary, and a spinning mirror polygon gave an early glimpse of waveforms. \n\nMoving-paper oscillographs using UV-sensitive paper and advanced mirror galvanometers provided multi-channel recordings in the mid-20th century. Frequency response was into at least the low audio range.\n\nCathode ray tubes (CRTs) were developed in the late 19th century. At that time, the tubes were intended primarily to demonstrate and explore the physics of electrons (then known as cathode rays). Karl Ferdinand Braun invented the CRT oscilloscope as a physics curiosity in 1897, by applying an oscillating signal to electrically charged deflector plates in a phosphor-coated CRT. Braun tubes were laboratory apparatus, using a cold-cathode emitter and very high voltages (on the order of 20,000 to 30,000 volts). With only vertical deflection applied to the internal plates, the face of the tube was observed through a rotating mirror to provide a horizontal time base. In 1899 Jonathan Zenneck equipped the cathode ray tube with beam-forming plates and used a magnetic field for sweeping the trace.\n\nEarly cathode ray tubes had been applied experimentally to laboratory measurements as early as 1919 \nbut suffered from poor stability of the vacuum and the cathode emitters. The application of a thermionic emitter allowed operating voltage to be dropped to a few hundred volts. Western Electric introduced a commercial tube of this type, which relied on a small amount of gas within the tube to assist in focussing the electron beam.\n\nV. K. Zworykin described a permanently sealed, high-vacuum cathode ray tube with a thermionic emitter in 1931. This stable and reproducible component allowed General Radio to manufacture an oscilloscope that was usable outside a laboratory setting.\n\nThe first dual-beam oscilloscope was developed in the late 1930s by the British company A.C.Cossor (later acquired by Raytheon). The CRT was not a true double beam type but used a split beam made by placing a third plate between the vertical deflection plates. It was widely used during WWII for the development and servicing of radar equipment. Although extremely useful for examining the performance of pulse circuits it was not calibrated so could not be used as a measuring device. It was, however, useful in producing response curves of IF circuits and consequently a great aid in their accurate alignment.\n\nAllen B. Du Mont Labs. made moving-film cameras, in which continuous film motion provided the time base. Horizontal deflection was probably disabled, although a very slow sweep would have spread phosphor wear. CRTs with P11 phosphor were either standard or available.\n\nLong-persistence CRTs, sometimes used in oscilloscopes for displaying slowly changing signals or single-shot events, used a phosphor such as P7, which comprised a double layer. The inner layer fluoresced bright blue from the electron beam, and its light excited a phosphorescent \"outer\" layer, directly visible inside the envelope (bulb). The latter stored the light, and released it with a yellowish glow with decaying brightness over tens of seconds. This type of phosphor was also used in radar analog PPI CRT displays, which are a graphic decoration (rotating radial light bar) in some TV weather-report scenes.\n\nThe technology for the horizontal sweep, that portion of the oscilloscope that creates the horizontal time axis, has changed.\n\nEarly oscilloscopes used a synchronized sawtooth waveform generator to provide the time axis. The sawtooth would be made by charging a capacitor with a relatively constant current; that would create a rising voltage. The rising voltage would be fed to the horizontal deflection plates to create the sweep. The rising voltage would also be fed to a comparator; when the capacitor reached a certain level, the capacitor would be discharged, the trace would return to the left, and the capacitor (and the sweep) would start another traverse. The operator would adjust the charging current so the sawtooth generator would have a slightly longer period than a multiple of the vertical axis signal. For example, when looking at a 1 kHz sinewave (1 ms period), the operator might adjust the horizontal frequency to a little bit more than 5 ms. When the input signal was absent, the sweep would free run at that frequency.\n\nIf the input signal were present, the resulting display would not be stable at the horizontal sweep's free-running frequency because it was not a submultiple of the input (vertical axis) signal. To fix that, the sweep generator would be synchronized by adding a scaled version of the input the signal to the sweep generator's comparator. The added signal would cause the comparator to trip a little earlier and thus synchronize it to the input signal. The operator could adjust the synch level; for some designs, the operator could choose the polarity. The sweep generator would turn off (known as blanking) the beam during retrace.\n\nThe resulting horizontal sweep speed was uncalibrated because the sweep rate was adjusted by changing slope of the sawtooth generator. The time per division on the display depended upon the sweep's free-running frequency and a horizontal gain control.\n\nA synchronized sweep oscilloscope could not display a non-periodic signal because it could not synchronize the sweep generator to that signal. Horizontal circuits were often AC-coupled\n\nDuring World War II, a few oscilloscopes used for radar development (and a few laboratory oscilloscopes) had so-called driven sweeps. These sweep circuits remained dormant, with the CRT beam cut off, until a drive pulse from an external device unblanked the CRT and started a constant-speed horizontal trace; the calibrated speed permitted measurement of time intervals. When the sweep was complete, the sweep circuit blanked the CRT (turned off the beam), reset itself, and waited for the next drive pulse. The Dumont 248, a commercially available oscilloscope produced in 1945, had this feature.\n\nOscilloscopes became a much more useful tool in 1946 when Howard Vollum and Melvin Jack Murdock introduced the Tektronix Model 511 \"triggered-sweep\" oscilloscope. Howard Vollum had first seen this technology in use in Germany. The triggered sweep has a circuit that develops the driven sweep's drive pulse from the input signal.\n\nTriggering allows stationary display of a repeating waveform, as multiple repetitions of the waveform are drawn over exactly the same trace on the phosphor screen. A triggered sweep maintains the calibration of sweep speed, making it possible to measure properties of the waveform such as frequency, phase, rise time, and others, that would not otherwise be possible. Furthermore, triggering can occur at varying intervals, so there is no requirement that the input signal be periodic.\n\nTriggered-sweep oscilloscopes compare the vertical deflection signal (or rate of change of the signal) with an adjustable threshold, referred to as trigger level. As well, the trigger circuits also recognize the slope direction of the vertical signal when it crosses the threshold—whether the vertical signal is positive-going or negative-going at the crossing. This is called trigger polarity. When the vertical signal crosses the set trigger level and in the desired direction, the trigger circuit unblanks the CRT and starts an accurate linear sweep. After the completion of the horizontal sweep, the next sweep will occur when the signal once again crosses the threshold trigger.\n\nVariations in triggered-sweep oscilloscopes include models offered with CRTs using long-persistence phosphors, such as type P7. These oscilloscopes were used for applications where the horizontal trace speed was very slow, or there was a long delay between sweeps, to provide a persistent screen image. Oscilloscopes without triggered sweep could also be retro-fitted with triggered sweep using a solid-state circuit developed by Harry Garland and Roger Melen in 1971.\n\nAs oscilloscopes have become more powerful over time, enhanced triggering options allow capture and display of more complex waveforms. For example, \"trigger holdoff\" is a feature in most modern oscilloscopes that can be used to define a certain period following a trigger during which the oscilloscope will not trigger again. This makes it easier to establish a stable view of a waveform with multiple edges which would otherwise cause another trigger. \n\nVollum and Murdock went on to found Tektronix, the first manufacturer of calibrated oscilloscopes (which included a graticule on the screen and produced plots with calibrated scales on the axes of the screen). Later developments by Tektronix included the development of multiple-trace oscilloscopes for comparing signals either by time-multiplexing (via chopping or trace alternation) or by the presence of multiple electron guns in the tube. In 1963, Tektronix introduced the Direct View Bistable Storage Tube (DVBST), which allowed observing single pulse waveforms rather than (as previously) only repeating wave forms. Using micro-channel plates, a variety of secondary-emission electron multiplier inside the CRT and behind the faceplate, the most advanced analog oscilloscopes (for example, the Tek 7104 mainframe) could display a visible trace (or allow the photography) of a single-shot event even when running at extremely fast sweep speeds. This oscilloscope went to 1 GHz.\n\nIn vacuum-tube oscilloscopes made by Tektronix, the vertical amplifier's delay line was a long frame, L-shaped for space reasons, that carried several dozen discrete inductors and a corresponding number of low-capacitance adjustable (\"trimmer\") cylindrical capacitors. These oscilloscopes had plug-in vertical input channels. For adjusting the delay line capacitors, a high-pressure gas-filled mercury-wetted reed switch created extremely fast-rise pulses which went directly to the later stages of the vertical amplifier. With a fast sweep, any misadjustment created a dip or bump, and touching a capacitor made its local part of the waveform change. Adjusting the capacitor made its bump disappear. Eventually, a flat top resulted.\n\nVacuum-tube output stages in early wideband oscilloscopes used radio transmitting tubes, but they consumed a lot of power. Picofarads of capacitance to ground limited bandwidth. A better design, called a distributed amplifier, used multiple tubes, but their inputs (control grids) were connected along a tapped L-C delay line, so the tubes' input capacitances became part of the delay line. As well, their outputs (plates/anodes) were likewise connected to another tapped delay line, its output feeding the deflection plates. (This amplifier was push-pull, so there were four delay lines, two for input, and two for output.)\n\nThe first Digital Storage Oscilloscope (DSO) was invented by Nicolet Test Instrument of Madison, Wisconsin. It was a low speed ADC (1 MHz, 12 bit) used primarily for vibration and medical analysis. The first high speed DSO (100 MHz, 8 bit) was invented by Walter LeCroy (who founded the LeCroy Corporation, based in New York, USA) after producing high-speed digitizers for the research center CERN in Switzerland. LeCroy (since 2012 Teledyne LeCroy) remains one of the three largest manufacturers of oscilloscopes in the world.\n\nStarting in the 1980s, digital oscilloscopes became prevalent. Digital storage oscilloscopes use a fast analog-to-digital converter and memory chips to record and show a digital representation of a waveform, yielding much more flexibility for triggering, analysis, and display than is possible with a classic analog oscilloscope. Unlike its analog predecessor, the digital storage oscilloscope can show pre-trigger events, opening another dimension to the recording of rare or intermittent events and troubleshooting of electronic glitches. As of 2006 most new oscilloscopes (aside from education and a few niche markets) are digital.\n\nDigital scopes rely on effective use of the installed memory and trigger functions: not enough memory and the user will miss the events they want to examine; if the scope has a large memory but does not trigger as desired, the user will have difficulty finding the event.\n\nDSOs also led to the creation of hand-held digital oscilloscopes (pictured), useful for many test and field service applications. A hand held oscilloscope is usually a digital sampling oscilloscope, using a liquid crystal display for its display.\n\nDue to the recent rise in the prevalence of PCs, PC-based oscilloscopes have been becoming more common. Typically, a signal will be captured on external hardware (which includes an analog-to-digital converter and memory) and transmitted to the computer, where it is processed and displayed.\n\n"}
{"id": "29473303", "url": "https://en.wikipedia.org/wiki?curid=29473303", "title": "Peter H. Appel", "text": "Peter H. Appel\n\nPeter H. Appel (born 1964) was the administrator of the Research and Innovative Technology Administration (RITA). From 2009 to 2011 Appel was the Obama Administration point person on transportation technology issues and research. He left RITA in late 2011 for the private sector to work on emerging technologies in the transportation industry.\n\nMr. Appel has a Bachelor of Science degree in economics and computer science from Brandeis University and a Master of Science degree in transportation from the Massachusetts Institute of Technology.\n\nHe worked for the global management consulting firm of A.T. Kearney, Inc., where he led business improvement initiatives for clients in the private and public sectors, with a focus on Transportation and Infrastructure. Mr. Appel has worked in the transportation field since the late 1980s, and has supported organizations in the railroad, trucking, airline, and ocean shipping industries with growth strategy, supply chain improvement, post-merger integration, public-private partnerships, and other key business and policy issues. Previously, he served as the Special Assistant to the Administrator of the Federal Aviation Administration, and as Assistant Director for Pricing and Yield Management at Amtrak.\n\nHe was confirmed by the U.S. Senate as Administrator of the Research and Innovative Technology Administration (RITA) on April 29, 2009. During his tenure at RITA, Appel worked with Secretary Ray LaHood to advance key U.S. Department of Transportation (USDOT) initiatives by leveraging effective research and cross-modal coordination. These initiatives have included the Distracted Driving Summit, which brought key transportation researchers, advocates, decision makers and other leaders together to address this growing safety issue; the bolstering of USDOT Intelligent Transportation Systems (ITS) Program to best improve safety, efficiency, and environmental sustainability across all modes of surface transportation; and the establishment of the Department's Safety Council. Appel coordinated DOT's research programs associated with railroads, aviation, maritime transportation and vehicles. As administrator of RITA, he also coordinated DOT efforts on distracted driving and research gathering statistical data about the various transportation modes.\n"}
{"id": "28730241", "url": "https://en.wikipedia.org/wiki?curid=28730241", "title": "Printing in Tamil language", "text": "Printing in Tamil language\n\nThe introduction and early development of printing in South India is attributed to missionary propaganda and the endeavours of the British East India Company. Among the pioneers in this arena, maximum attention is claimed by the Jesuit missionaries, followed by the Protestant Fathers and Hindu Pandits. Once the immigrants realized the importance of the local language, they began to disseminate their religious teachings through that medium, in effect ushering in the vernacular print culture in India. The first Tamil booklet was printed in 1554 (February 11) in Lisbon - \"Cartilha em lingoa Tamul e Portugues\" in Romanized Tamil script by Vincente de Nazareth, Jorge Carvalho and Thoma da Cruz, all from the Paravar community of Tuticorin. it is also the first non-European language to find space in the modern printing culture in the world.\n\nThese developments took place at a time when other locations such as Madurai were still confined to the use of copper plates and stone inscriptions. This book was printed earlier than the first printed and dated books of Russia (1563), Africa (1624) and Greece (1821).\n\nThe appearance of Tamil in print, both in Roman transliteration and in its native script was the result of the convergence between colonial expansion and local politics, coupled with the efforts of a Portuguese Jesuit, Henrique Henriques who arrived on the Fishery Coast (Tuticorin) in 1547. During his stay Henriques produced five different books in the Tamil script and language, printed at various Jesuit settlements on the west coast. He also compiled a Tamil Grammar and a Tamil Dictionary, which, though never printed, were widely used by other Europeans. Graham Shaw speaks of Henriques as, “the first great European Scholar of any Indian language\" (Stuart Blackburn).\n\nAround 1575 Henriques was relieved of his missionary duties on the east coast and moved to Goa where he began to prepare his texts. Henriques was there assisted by Father Pero Luis, who entered the Jesuit order in 1562. The stage was finally set when Tamil types were cast in Goa by João Gonçalves (perfected by Father João de Faria in Kollam), with the assistance of Luis.\n\nIn 1577 the first of the Henriques’ five books, \"Doctrina Christam en Lingua Malauar Tamul (Thambiran Vanakkam)\" was printed in Goa. The book was the first book printed with Indian type. Although some scholars refuse to consider this as a historical fact, Graham Shaw seems convinced that it was printed. The second printed Tamil book was only 16 pages long, but a third Catechism of 127 pages, a Tamil translation of the popular Portuguese text by Marcos Jorge, was printed again with new type in Cochin on November 14, 1579. Three Catechisms were printed with three sets of type, at three different locations on the west coast over the following three years. Henriques’ two other books printed at Cochin were:\n\n\nIn the 17th century, Tamil books were printed at Ambalakad with type made in Rome. Only five in number and printed within a space of two years, these books might be called the second phase of Tamil printing. There were five books but only two texts. The first was Roberto De Nobili’s Catechism, \"Nanopatecam\", printed posthumously in three volumes: Volume 1 in 1677 followed by Volumes 2 and 3 in 1678. The second text was Antem De Proenca’s \"Tamil-Portuguese Dictionary\" of 1679.\n\nUnlike Henriques, Roberto de Nobili did not translate a Portuguese text into Tamil, instead he wrote his own manual, so that he might emphasize the hidden truths of the new faith.\n\nBartholomäus Ziegenbalg was the pioneer in the setup of a printing press at Madras. In South India the printing press had been established as early as 1578, but printing activities came to an end owing to a gradual decline in the religious zeal of successive generations of missionaries. Tamil printing stopped after 1612, as the numerous writings of Nobili and Manoel Martin lay unpublished in 1649 and 1660. There were some attempts to revive printing, but they proved short-lived. For instance, there is a reference to a Latin–Tamil grammar by Father Beschi, a Sanskrit scholar, having been printed at Ziegenbalg’s press.\n\nZiegenbalg explained in a number of letters that the books prepared in the Malabar language, to help in the propagation of the Christian faith, were initially written in Portuguese and then translated into the “Malabarick Language” with the help of Indian assistants. In the absence of a printing press the books that had been prepared up until then had to be transcribed by hand. This proved to be a slow, laborious and expensive process. With the objective of facilitating a wider and faster dissemination of Christian literature, Ziegenbalg in his letter of August 22, 1708, put forth a demand for a “Malabarick and Portuguese printing press”. In the mean time Ziegenbalg devoted considerable attention to collecting manuscripts of Indian literature, as this would help him to understand the old beliefs of the Hindus which he proposed to refute.\n\nIn a letter written in 1708, Ziegenbalg speaks of 26 sermons delivered by him at the church of Tranquebar and two vocabularies of Malabar language prepared by him. The first consisted of 26,000 words in common use, and had three columns, the first giving the word in Malabar characters, the second its transliteration and the third its meaning in German. The second contained words used in poetry. For this work Ziegenbalg was assisted by Indian scholars and poets who remained at his house for four months.\n\nZiegenbalg was keenly aware that to attain his object he needed a printing press. He made repeated demands for a press in his letters of April–June 1709. The Society for Promoting Christian Knowledge, set up in the 1690s, came forward to help under the recommendation of the Rev. A. W. Boehme (the German chaplain to Prince George of Denmark). In 1711 the society sent the mission some copies of the Bible in Portuguese as well as a printing press with pica types and other accessories along with a printer to operate it. The ship was held up by the French near Brazil, and the printer Jones Finck was arrested but later released. Finck soon succumbed to fever near the Cape of Good Hope. The printing press reached India in 1712 unaccompanied by its operator. The press, however, started functioning with the help of a German printer–cum–compositor.\n\nMalabar characters were obtained from Europe. A letter dated April 7, 1713, contains a list of 32 books in the Malabar language, original works as well as translations, and 22 books in Portuguese prepared by the missionaries. It is stated that the books in the Malabar language included a vocabulary written on paper and another written on palm leaves.\n\nAccording to a letter of January 3, 1714, the work of printing the New Testament in Tamil had already begun. Another letter of September 27, 1714, states that, \"The Four Evangelists and Acts of the Apostles\" was already printed. Reportedly, this is the oldest Tamil book printed at Tranquebar, a copy of which is available at the Serampore College Library. From 1715 onwards with the completion of the New Testament, printing activity in Tamil commenced in full swing. In 1715 Ziegenbalg wrote a concise grammar of the Malabar language for use by Europeans and had it printed by 1716. A copy of this book also exists at the Serampore College Library.\nZiegenbalg and his collaborators aimed at spreading their printed work all over India. Consequently, their marketing strategies cajoled them to produce almanacs which were quite scarce in the country. A \"Sheet Almanac\" was printed and sold on the coast of Coromandel as well as in Malabar and Bengal.\n\nParallel to printing efforts by the Protestant missionaries at Tranquebar or Tarangampãdi, the growth of the Jesuit missionary Constanzo Beschi (Viramãmunivar; Constantine Joseph Beschi 1680–1747) was equally significant in revolutionizing the face of Tamil print and literature. The difference in the Christian beliefs of the respective cults gave rise to rigorous disputes and theological debates, which on many occasions even led to violent conflicts resulting in injuries and death. These disputes were carried on by the Lutherans through “printed books and pamphlets\", whereas Beschi (due to lack of a Jesuit-owned printing press) mainly concentrated on writing influential pieces of literature. Although printing in Tamil was introduced by the Jesuits, by the eighteenth century the scenario had changed and the domain of the press came to be majority controlled and cultivated by the Protestants. Beschi's efforts in a place populated with thousands of Lutheran converts (mainly Tanjore and Travancore), grew to become an “alarming”, “arrogant” and “formidable” rival to the already sprawling missionary activities of the Protestant fathers. One particular reason for Beschi’s popularity was, as Blackburn observes, his “Romanish compromises with local customs”. In the books of Muttusami Pillai (Beschi’s Tamil biographer), he is frequently portrayed as a traditional Eastern or Oriental king, adorned with ornate jewellery and chandan on his forehead. Beschi was reportedly favoured by the local rulers, especially Chanda Sahib whom he had served diwan to, thereby making it easier for him to master the language. He was intelligent enough to adopt such means which would undoubtedly benefit him in ways more than one.\n\nContrary to this image, Beschi has also been examined as a magical Indian “poet-saint” with extraordinary literary skills and persuasion prowess. Beschi’s written works constituted the substructure of modern Tamil literary acculturation. According to sources, Beschi wrote more than twenty books :– dictionaries, epic poetry, prose collections, grammar, folklore. His major prose essay was \"Veta Vilakkam\" which ran to 250 pages. The first bilingual Tamil grammar printed in India is also credited to Beschi. He composed various interlingual dictionaries: \"Tamil-Latin\", \"Latin-Tamil-Portuguese\", and \"Tamil-French\" and most importantly the four-way lexicon \"Tamil-Tamil\" \"Catur-Agarati\" which comprised meanings, synonyms, rhymes, etc. This book was not printed before 1824. Although it cannot be assumed that his works were well accepted and appreciated by the Protestants, as Blackburn comments, the rival camp unbiasedly “admired Beschi’s literary skills - they printed one of his grammars and another of his books (\"Vetiyar Olukkam\", A Manual for Catechists) became standard reading for them by the nineteenth century…”. Beschi’s \"Parramarta Kuruvin Kattai\" or \"Guru Simpleton\" was the first printed book of Tamil folktale.\n\nBeschi’s \"Guru Simpleton\" (which occupies a status similar to The Arabian Nights or The Panchatantra in Tamil culture) is a blend of the oral tradition of Tamil folklore and the European story form, wrapped in the author’s imaginative faculty. Although Beschi had completed its composition (along with a preface) by 1776, the book was not published singularly until 1822 in London. Records show that Beschi wrote the Tamil version first and later translated it into Latin. Although Beschi claimed that the sole purpose of the book was to disseminate amusement and humour among both locals and missionaries, Blackburn mentions that the author was most probably yearning for something more than that – “this was a plea for a Jesuit patron, somewhere outside India, to underwrite the publication of his dictionary and folktale”, as print was a more reliable medium to “demonstrate correct spelling” than local scribes and copyists.\n\nIn the history of print in early nineteenth-century India there were an enormous number of books of oral literature, especially folktales published. Between 1800 and 1835 most printed books in Tamil(dictionaries and grammars aside) were collections of oral tales. Well known literary texts, such as \"Tirrukkural\" and \"Nalatiyar\", also appeared in print, but these classical texts were outnumbered by books of oral tales\". The first of these was \"Vikkiramatittan\" \"Katai\", a collection of folktales in the framework of a literary tale which appeared in 1804, followed by the \"Catamuka Ravanan Katai\" in 1808; the \"Mariyatai Raman Katai\" and \"Tamilariyum Mantai Katai\" in 1812; the \"Pururava Cakravarti Katai\" in 1819; the \"Katamantacari\", a collection of oral tales in 1820; the Tamil – English bilingual publication of \"Paramartta Kuruvin Katai\" (\"Guru Simpleton\") in 1822 (in London) ; a Tamil \"Pancatantra\" in 1826 ; the \"Katacintamani\", another collection of oral tales, in 1833; and translations of tales from English, French and Aesop by the 1850s. Some of these books are still available today.\n\nA number of early Tamil print publishing houses were set up by the pundits in the 1830s in Madras. These establishments played a significant role in the consolidation of the commercial printing world. They were also involved in public–politics, the anti–missionary movement in Georgetown, for instance. Pundits who were educated at the College Fort of St George and some who were not, used the text-making skills they learned from the Europeans in setting up of their own presses at Madras.\n\nThe rise of the pundit - presses saw growth during the 1830s with Kalvi Vilakkam, the joint venture of Charavanaperumal Aiyar and Vichakaperumal Aiyar in 1834. The press functioned till the 1850s producing more than 50 books. This was followed by the Sarasvati Press (1835) of Tiruvenkatachala Mutaliar, and Kalvi Kalanchiyam set up in 1839 by Umapati Mutaliar and his three brothers. These presses quickly became associated with movements in deflecting the missionaries as they started voicing the sentiments of certain sections of the Hindu community.\n\nArumuka Navalar spearheaded the Saivism cult both in Sri Lanka and in Tamil Nadu. He was the guardian of pure and pristine Saiva tradition. He established a number of schools for Tamil and Saivism and printing presses at Jaffna, Chidambaram and Madras. He was the most fluent Tamil speaker and writer of his generation. At the age of 27, Arumugam was conferred the title of “Navalar”, the eloquent.\n\nIn the context of printing, Arumuka Navalar or Arumuga Navalar was an editor of old Tamil texts. Among his editions the most important are Mantalapurutar’s lexicon \"cutamani nikantu\" with commentary (first printed in 1849), the standard medieval grammar \"Nannūl\" with a commentary (1851), the early devotional poem \"Tirumurukāṟṟuppaṭai\", Manikkavacakar’s great devotional poems \"Tiruvacakam\" and \"tirukkovaiyar\", the text of \"Tirukkuṛaḷ\" with Parimelazhagar’s detailed gloss in 1861.\n\nArumuga Navalar apparently introduced few novel features in the area of Tamil editing. He was probably the first to use punctuation marks like the semicolon, the question mark and the exclamation mark. He produced the first “split” complex sandhi forms to facilitate reading and comprehension.\n\nThe Calcutta School-Book Society was established under the patronage of the Marquis of Hastings in 1817. Soon after a similar society was set up in Madras. The association in South India soon languished, and for many years it virtually ceased to exist. It was revived around 1850, when prizes were offered for the best school book on specified subjects. Several new publications were thus secured of which, The History of India by H. Morris Esq., was very successful.\n\nThe publications of the Madras School Book Society being chiefly used in Government Schools such that religious sentiments were adapted accordingly. The committee of the Madras Tract Society issued some books with Christian elements intended specially for mission schools Classified catalogue of Tamil printed books,with introductory notices. Though reading books of the Madras School Book society were prepared with special reference to the government schools, the committee was not restricted to non–Christian publications. The Rev. A.R. Symonds suggested that the society should make an effort to provide wholesome and attractive literature. Prizes were also offered for the best translation of Robinson Crusoe.\n\nSheikh Mustafa from Beruwala who published his work 'Mizan Malai' Arwi poem book in 1868.\n\nMadras was the foremost seat of printing among the “colonial metropolises”. The Society for Promoting Christian Knowledge (SPCK) was set up at Vepery (situated just outside Madras) in 1726 by Benjamin Schultz. This new venture (Vepery mission) was just an extension of the Tranquebar mission. Earlier in 1712, a printing press enabled with Tamil and Telegu typefaces was provided by the SPCK for publishing activities at Tranquebar, on repeated appeals by Ziegenbalg. This press mostly dealt with smaller publications like \"A General Description Of Malabar Heathendom\", \"Four Gospels And Acts\", and \"Accursed Heathendom\" which were usually antagonistic to Hindu beliefs and principles. It also printed the translated version of the New Testament in 1715. When the English army under Sir Eyre Coote attacked the French colony of Pondicherry in 1761 they seized the printing press from the governor’s house along with its typefaces (which were a “prize catch” for them ) and the printer, Delon and transferred it to Madras. Nonetheless Johann Phillip Fabricius, a well-known Tamil scholar convinced Coote to hand over the press, only on agreement that the printing demands of Fort St. George would be given maximum importance. In 1762 itself, the SPCK press published a calendar and several Tamil books, “pre-dating the books printed in Calcutta and Bombay at least by a decade”.\n\nBy 1766, Vepery got its own press supplemented with its own print equipment. Therefore, the presses confiscated from Pondicherry were returned to Fort St George, which led to the establishment of the Government Press in Mount Road. The Vepery Press was renamed as the SPCK Press; Johann Philipp Fabricius being its managerial head, who composed and printed a Tamil book on Catechism (1766) with typefaces cut in Germany (Halle). By the next decade typecases were produced by the SPCK Press itself and they lasted until the 1870s. Books printed included Fabricius’s \"Translation of the New Testament\" (1772); his \"Dictionary of Tamil and English\", based on Ziegenbalg’s \"Malabar English Dictionary\"(1779) which came out 100 years after Antão da Proença’s \"Tamil-Portuguese Dictionary\" of 1679; and \"Oru Paratecyin Punyacaritram\" (a translation of Bunyan’s \"Pilgrims Progress\" (1793). This press was sold to the American Board of Commissioners for Foreign Missions (the American Board Mission or ABM) in Çintadaripet in the mid 19th century. When the ABM left India in 1886 the press was reacquired by the SPCK–Diocesan committee and renamed the Diocesan Press that still exists today, almost 250 years later, as the CLS Press.\n\n"}
{"id": "521507", "url": "https://en.wikipedia.org/wiki?curid=521507", "title": "Priority inversion", "text": "Priority inversion\n\nIn computer science, priority inversion is a scenario in scheduling in which a high priority task is indirectly preempted by a lower priority task effectively inverting the relative priorities of the two tasks.\n\nThis violates the priority model that high priority tasks can only be prevented from running by higher priority tasks and briefly by low priority tasks which will quickly complete their use of a resource shared by the high and low priority tasks.\n\nConsider two tasks \"H\" and \"L\", of high and low priority respectively, either of which can acquire exclusive use of a shared resource \"R\". If \"H\" attempts to acquire \"R\" after \"L\" has acquired it, then \"H\" becomes blocked until \"L\" relinquishes the resource. Sharing an exclusive-use resource (\"R\" in this case) in a well-designed system typically involves \"L\" relinquishing \"R\" promptly so that \"H\" (a higher priority task) does not stay blocked for excessive periods of time. Despite good design, however, it is possible that a third task \"M\" of medium priority (p(\"L\") < p(\"M\") < p(\"H\"), where p(\"x\") represents the priority for task (\"x\")) becomes runnable during \"L\"&apos;s use of \"R\". At this point, \"M\" being higher in priority than \"L\", preempts \"L\" (since \"M\" does not depend on \"R\"), causing \"L\" to not be able to relinquish \"R\" promptly, in turn causing \"H\"—the highest priority process—to be unable to run (that is, \"H\" suffers unexpected blockade indirectly caused by lower priority tasks like \"M\").\n\nIn some cases, priority inversion can occur without causing immediate harm—the delayed execution of the high priority task goes unnoticed, and eventually the low priority task releases the shared resource. However, there are also many situations in which priority inversion can cause serious problems. If the high priority task is left starved of the resources, it might lead to a system malfunction or the triggering of pre-defined corrective measures, such as a watchdog timer resetting the entire system. The trouble experienced by the Mars lander \"Mars Pathfinder\" is a classic example of problems caused by priority inversion in realtime systems.\n\nPriority inversion can also reduce the perceived performance of the system. Low priority tasks usually have a low priority because it is not important for them to finish promptly (for example, they might be a batch job or another non-interactive activity). Similarly, a high priority task has a high priority because it is more likely to be subject to strict time constraints—it may be providing data to an interactive user, or acting subject to realtime response guarantees. Because priority inversion results in the execution of a lower priority task blocking the high priority task, it can lead to reduced system responsiveness, or even the violation of response time guarantees.\n\nA similar problem called deadline interchange can occur within earliest deadline first scheduling (EDF).\n\nThe existence of this problem has been known since the 1970s. Lampson and Redell\n\n\n\n\n\n\n\n"}
{"id": "30502980", "url": "https://en.wikipedia.org/wiki?curid=30502980", "title": "Product layout", "text": "Product layout\n\nIn manufacturing engineering, a product layout refers to a production system where the work stations and equipment are located along the line of production, as with assembly lines.\n\nUsually, work units are moved along a line (not necessarily a geometric line, but a set of interconnected work stations) by a conveyor. Work is done in small amounts at each of the work stations on the line. To use the product layout, the total work to be performed must be dividable into small tasks that can be assigned to each of the workstations.\n\nBecause the work stations each do small amounts of work, the stations utilize specific techniques and equipment tailored to the individual job they are assigned. This can lead to a higher rate of production.\n\n\n"}
{"id": "1810870", "url": "https://en.wikipedia.org/wiki?curid=1810870", "title": "Reconciliation ecology", "text": "Reconciliation ecology\n\nReconciliation ecology is the branch of ecology which studies ways to encourage biodiversity in human-dominated ecosystems. Michael Rosenzweig first articulated the concept in his book \"Win-Win Ecology\", based on the theory that there is not enough area for all of earth’s biodiversity to be saved within designated nature preserves. Therefore, humans should increase biodiversity in human-dominated landscapes. By managing for biodiversity in ways that do not decrease human utility of the system, it is a \"win-win\" situation for both human use and native biodiversity. The science is based in the ecological foundation of human land-use trends and species-area relationships. It has many benefits beyond protection of biodiversity, and there are numerous examples of it around the globe. Aspects of reconciliation ecology can already be found in management legislation, but there are challenges in both public acceptance and ecological success of reconciliation attempts.\n\nTraditional conservation is based on \"reservation and restoration\"; reservation meaning setting pristine lands aside for the sole purpose of maintaining biodiversity, and restoration meaning returning human impacted ecosystems to their natural state. However, reconciliation ecologists argue that there is too great a proportion of land already impacted by humans for these techniques to succeed.\n\nWhile it is difficult to measure exactly how much land has been transformed by human use, estimates range from 39 to 50%. This includes agricultural land, pastureland, urban areas, and heavily harvested forest systems. An estimated 50% of arable land is already under cultivation. Land transformation has increased rapidly over the last fifty years, and is likely to continue to increase. Beyond direct transformation of land area, humans have impacted the global biogeochemical cycles, leading to human caused change in even the most remote areas. These include addition of nutrients such nitrogen and phosphorus, acid rain, ocean acidification, redistribution of water resources, and increased carbon dioxide in the atmosphere. Humans have also changed species compositions of many landscapes that they do not dominate directly by introducing new species or harvesting native species. This new assemblage of species has been compared to previous mass extinctions and speciation events caused by formation of land bridges and colliding of continents.\n\nThe need for reconciliation ecology was derived from patterns of species distribution and diversity. The most relevant of these patterns is the species-area curve which states that a larger geographic area will contain higher species diversity. This relationship has been supported by so large a body of research that some scholars consider it to be an ecological law.\n\nThere are two main reasons for the relationship between number of species and area, both of which can be used as an argument for conservation of larger areas. The habitat heterogeneity hypothesis claims that a larger geographic area will have a greater variety of habitat types, and therefore more species adapted to each unique habitat type. Setting aside a small area will not encompass enough habitat variety to contain a large variety of species. The equilibrium hypothesis draws from the theory of island biogeography as described by MacArthur and Wilson. Large areas have large populations, which are less likely to go extinct through stochastic processes. The theory assumes that speciation rates are constant with area, and a lower extinction rate coupled with higher speciation leads to more species.\n\nThe species-area relationship has often been applied to conservation, often quantitatively. The simplest and most commonly used formula was first published by Frank W. Preston. The number of species present in a given area increases in relationship to that area with the relationship S = cA where S is the number of species, A is the area, and c and z are constants which vary with the system under study. This equation has frequently been used for designing reserve size and placement (see SLOSS debate). The most common version of the equation used in reserve design is the formula for inter-island diversity, which has a z-value between 0.25-0.55, meaning protecting 5% of the available habitat will preserve 40% of the species present. However, inter-provincial species area relationships have z-values closer to 1, meaning protecting 5% of habitat will only protect 5% of species diversity.\n\nTaken together, proponents of reconciliation ecology see the species-area relationship and human domination of a large percentage of the earth's area as a sign that we will not be able to set aside enough land to protect all of life's biodiversity. There can be negative effects of setting land aside because it means the remaining land is used more intensely. For example, less land is required for crop production when high levels of inorganic fertilizer is applied, but these chemicals will affect nearby land set aside for natural ecosystems. The direct benefits of land transformation for the growing world population often make it ethically difficult to justify the tradeoff between biodiversity and human use. Reconciled ecosystems are ones in which humans dominate, but natural biodiversity is encouraged to persist within the human landscape. Ideally, this creates a more sustainable socio-ecological system and does not necessitate a trade off between biodiversity and human use.\n\nHow can understanding of species' natural history aid their effective conservation in human-dominated ecosystems? Humans often conduct activities that allow for the incorporation of other species, whether as a by-product or as a result of a focus on nature. Traditional natural history can only inform how best to do this to a certain degree, because landscapes have been changed so dramatically. However, there is much more to learn through direct study of species' ecology in human-dominated ecosystems, through what is known as focused natural history. Rosenzweig cites four examples: shrikes (Laniidae) thrived in altered landscapes when wooden fence post perches allowed them easy access to pouncing on prey, but inhospitable steel fence posts contributed to their decline. Replacing steel fence posts with wood fence posts reverses the shrikes' decline and allows humans to determine the reasons for the distribution and abundance of shrikes. Additionally, the cirl bunting (\"Emberiza cirlus\") thrived on farms when fields alternated between harvests and hay, but declined where farmers began to plant winter grain crops, natterjack toads (\"Bufo calamatus\") declined when reductions in sheep grazing ceased to alter ponds to their preferred shape and depth, and longleaf pine (\"Pinus palustris\") declined in the Southeastern United States when lack of wildfires prevented its return after timbering. Thus, applying focused natural history in human-dominated landscapes can contribute to conservation efforts.\n\nThe emerging concept of ecosystem services (coined by the Millennium Ecosystem Assessment in 2005) changed the way ecologists perceived so-called \"ordinary species\" : as abundant species represent the bulk of biomass and biological processes, even if they don't appear directly threatened their conservation constitutes as a major concern for maintaining these services on which rely both human societies and rarer species. Reconciliation ecology then proposes to take care of such species and to maintain (or restore) ecological processes in human-dominated ecosystems, hence creating ecological corridors and preserving a good functioning of biological cycles.\n\nReconciliation ecologists believe increasing biodiversity within human dominated landscapes will help to save global biodiversity. This is sometimes preferable to traditional conservation because it does not impair human use of the landscape and therefore may be more acceptable to stakeholders. However, not only will it encourage biodiversity in the areas where it takes place, but many scholars cite other benefits of including biodiversity in human landscapes on both global conservation activities and human well-being.\n\nIncreasing wildlife habitat in human-dominated systems not only increases \"in situ\" biodiversity, it also aids in conservation of surrounding protected areas by increasing connectivity between habitat patches. This may be especially important in agricultural systems where buffers, live fences, and other small habitat areas can serve as stops between major preserves. This concept forms the basis of the subdiscipline countryside biogeography which studies the potential of the matrix between preserves to provide habitat for species moving from preserve to preserve.\n\nPlacing importance on native ecosystems and biodiversity within human landscapes increases human exposure to natural areas, which has been shown to increase appreciation of nature. Studies have shown that students who participate in outdoor education programs show a greater understanding of their environment, greater willingness to act in order to save the environment, and even a greater enthusiasm for school and learning. Green spaces have also been shown connect urban dwellers of all ages with nature, even when dominated by invasive species. Reconnecting people with nature is especially important for conservation because there is a tendency for people to use the biodiversity present in the landscape they grew up in as a point of comparison for future trends (see Shifting baseline).\n\nThe results of reconciliation ecology can also improve human well-being. E. O. Wilson has hypothesized that humans have an innate desire to be close to nature (see Biophilia), and numerous studies have linked natural settings to decreased stress and faster recovery during hospital stays.\n\nMany examples of native plants and animals taking advantage of human dominated landscapes have been unintentional, but may be enhanced as part of reconciliation ecology. Others are intentional redesigns of human landscapes to better accommodate native biodiversity. These have been going on for many hundreds of years including examples within agricultural systems, urban and suburban systems, marine systems, and even industrial areas.\n\nWhile Rosenzweig formalized the concept, humans have been encouraging biodiversity within human landscapes for millennia. In the Trebon Biosphere Reserve of the Czech Republic, a system of human-engineered aquaculture ponds built in the 1500s not only provides a profitable harvest of fish, but also provides habitat for a hugely diverse wetland ecosystem. Many cities in Europe take pride in their local population of storks, which nest on roofs or in church towers that replace the trees they would naturally nest in. There are records of humans maintaining plants in pleasure gardens as early as ancient Mesopotamia, with an especially strong tradition of incorporating gardens into the architecture of human landscapes in China.\n\nAgroforestry provides many examples of reconciliation ecology at work. In tropical agroforestry systems, crops such as coffee or fruit trees are cultivated under a canopy of shade trees, providing habitat for tropical forest species outside of protected areas. For example, shade-grown coffee plantations typically have lower tree diversity than unmanaged forests, however they have much higher tree species diversity and richness than other agricultural methods. Agriculture that mimics nature, encourages natural forest species along with the crops, and also takes pressure off nearby uncultivated forest areas where people are allowed to collect forest products. The understory can also be managed with reconciliation ecology: allowing weeds to grow among crops (minimizing labor and preventing the invasion of noxious weed species) and leaving fallowlands alongside farmed areas can enhance understory plant richness with associated benefits for native insects and birds compared to other agricultural practices.\n\nThe oil palm (\"Elaeis guineensis\") provides another example of the potential of reconciliation ecology. It is one of the most important and rapidly expanding tropical crops, so lucrative because it is used in many products throughout the world. Unfortunately, oil-palm agriculture is one of the main drivers of forest conversion in Southeast Asia and is devastating for native biodiversity, perhaps even more so than logging. However, attempts are being made to foster the sustainability of this industry. As a monoculture, oil palm is subject to potentially devastating attacks from insect pests. Many companies are attempting an integrated pest management approach which encourages the planting of species that support predators and parasitoids of these insect pests, as well as an active native bird community. Experiments have shown that a functioning bird community, especially at higher densities, can serve to reduce insect herbivory on oil palms, promoting increased crop yields and profits. Thus, oil palm plantation managers can participate in reconciliation ecology by promoting local vegetation that is beneficial to insectivorous birds, including maintaining ground plants that serve as nesting sites, thereby protecting natural communities. Additionally, steps such as maintaining riparian buffer zones or natural forest patches can help to slow the loss of biodiversity within oil palm plantation landscapes. By engaging in these environmentally friendly practices, fewer chemicals and less effort are required to maintain both plantation productivity and ecosystem services.\n\nThere are many grazing practices that also encourage native biodiversity. In Rosenzweig’s book he uses the example of a rancher in Arizona who intentionally deepened his cattle ponds in order to save a population of threatened leopard frogs (\"Rana chiricahuensis\"), with no detriment to the use of those tanks for cattle, and a similar situation has occurred with the vulnerable California tiger salamander (\"Ambystoma californiense\") in the Central Valley of California. Research has shown that without cattle grazing, many of the remaining vernal pools would dry too early for the salamanders to complete their life cycle under global climate change predictions. In Central America, a large percentage of pastureland is fenced using live trees which are not only low maintenance for the farmer, but also provide habitat for birds, bats, and invertebrates which cannot persist in open pastureland. Another example from Rosenzweig involves encouraging loggerhead shrikes (\"Lanius ludovicianus\") to populate pastureland by placing perches around the pasture. These are all simple, low-cost ways to encourage biodiversity without negatively impacting the human uses of the landscape.\n\nUrban ecology can be included under the umbrella of reconciliation ecology and it tackles biodiversity in cities, the most extreme of human-dominated landscapes. Cities occupy less than 3% of global surface area, but are responsible for a majority of carbon emissions, residential water use, and wood use. Cities also have unique climatic conditions such as the urban heat island effect, which can greatly affect biodiversity. There is a growing trend among city managers to take biodiversity into account when planning city development, especially in rapidly growing cities. Cities often have surprisingly high plant biodiversity due to their normally high degree of habitat heterogeneity and high numbers of gardens and green spaces cultivated to include a large variety of species. However, these species are often not native, and a large part of the total urban biodiversity is usually made up of exotic species.\n\nBecause cities are so highly impacted by human activities, restoration to the pristine state is not possible, however there are modifications that can be made to increase habitat without negatively impacting human needs. In urban rivers, addition of large woods and floating islands to provide habitat, modifications to walls and other structures to mimic natural banks, and buffer areas to reduce pollutants can all increase biodiversity without reducing the flood control and water supply services. Urban green spaces can be re-designed to encourage natural ecosystems rather than manicured lawns, as is seen in the National Wildlife Federation’s Backyard Wildlife Habitat program. Peregrine falcons (\"Falco peregrinus\"), which were once endangered by pesticide use, are frequently seen nesting in tall urban buildings throughout North America, feeding chiefly on the introduced rock dove. The steep walls of buildings mimic the cliffs peregrines naturally nest in and the rock doves replace the native prey species that were driven out of urban areas.\n\nIn Florida, the Florida manatee (\"Trichechus manatus latirostris\") uses warm water discharged from power plants as a refuge when the temperature of the Gulf of Mexico drops. These warm areas replace the warm springs that manatees once naturally used in the winter. These springs have been drained or cut off from open water by human uses. American crocodiles (\"Crocodylus acutus\") have a similar habitat in the cooling canals of the Turkey Point power plant, where an estimated 10% of the total North American population of the species lives.\n\nWastewater treatment systems have shown potential for reconciliation ecology on numerous occasions. Man-made wetlands designed to remove nitrogen before runoff from agriculture enters the Everglades in Florida are used as breeding sites for a number of birds, including the endangered wood stork (\"Mycteria americana\"). Stormwater treatment ponds can provide important breeding habitat for amphibians, especially where natural wetlands have been drained by human development.\n\nCoral reefs have been intensively impacted by human use, including overfishing and mining of the reef itself. One reconciliation approach to this problem is building artificial reefs that not only provide valuable habitat for aquatic species, but also protect nearby islands from storms when the natural structure has been mined away. Even structures as simple as scrap metal and automobiles can be used as habitat, providing added benefits of freeing space in landfills.\n\nGovernmental intervention can aid in encouraging private landowners to create habitat or otherwise increase biodiversity on their land. The United States' Endangered Species Act requires landowners to halt any activities negatively affecting endangered species on their land, which is a disincentive for them to encourage endangered species to settle on their land in the first place. To help mediate this problem, the US Fish and Wildlife Service has instituted safe harbor agreements whereby the landowner engages in restoration on their land to encourage endangered species, and the government agrees not to place further regulation on their activities should they want to reverse the restoration at a later date. This practice has already led to an increase in aplomado falcons (\"Falco femoralis\") in Texas and red-cockaded woodpeckers (\"Picoides borealis\") in the Southeastern US.\n\nAnother example is the US Department of Agriculture’s Conservation Reserve Program (CRP). The CRP was originally put in place to protect soil from erosion, but also has major implications for conservation of biodiversity. In the program, landowners take their land out of agricultural production and plant trees, shrubs, and other permanent, erosion controlling vegetation. Unintended, but ecologically significant consequences of this were the reduction of runoff, improved water quality, creation of wildlife habitat, and possible carbon sequestration.\n\nWhile reconciliation ecology attempts to modify the human world to encourage biodiversity without negatively impacting human use, there are still difficulties in getting broad acceptance of the idea. For example, addition of large woods to urban river systems, which provides critical habitat structure for native fish and invertebrates may be seen as \"untidy\" and a sign of poor management by residents. Similarly, many suburban areas do not allow long, unkempt lawns that provide useful wildlife habitat because of perceived damage to property values. Many humans have negative feelings toward certain species, especially predators such as wolves, which are often based more on perceived risk than actual risk of loss or injury resulting from the animal.\nEven with cooperation of the human element of the equation, reconciliation ecology can not help every species. Some animals, such as several species of waterfowl, show strong avoidance behaviors toward humans and any form of human disturbance. No matter how nice an urban park is built, the proximity of humans will scare away some birds. Other species must maintain large territories, and barriers that abound in human habitats, such as roads, will stop them from coexisting with humans. These animals will require undisturbed land set aside for them.\n\nThere is hence a double social challenge for reconciliation ecology : making people's perception of biodiversity evolve, and then changing relating norms and policies so as to better consider biodiversity as a positive component in our habitat.\n\n\n"}
{"id": "681185", "url": "https://en.wikipedia.org/wiki?curid=681185", "title": "Stress concentration", "text": "Stress concentration\n\nA stress concentration (often called stress raisers or stress risers) is a location in an object where stress is concentrated. An object is stronger when force is evenly distributed over its area, so a reduction in area, e.g., caused by a crack, results in a localized increase in stress. A material can fail, via a propagating crack, when a concentrated stress exceeds the material's theoretical cohesive strength. The real fracture strength of a material is always lower than the theoretical value because most materials contain small cracks or contaminants (especially foreign particles) that concentrate stress. Fatigue cracks always start at stress raisers, so removing such defects increases the fatigue strength.\n\n1) Geometric discontinuities cause an object to experience a local increase in the intensity of a stress field. Examples of shapes that cause these concentrations are cracks, sharp corners, holes, and changes in the cross-sectional area of the object. High local stresses can cause objects to fail more quickly, so engineers must design the geometry to minimize stress concentrations.\n\n2) Due to discontinuities in applied loads.\n\n3) Material discontinuities which may occur while manufacturing.\n\nA counter-intuitive method of reducing one of the worst types of stress concentrations, a crack, is to drill a large hole at the end of the crack. The drilled hole, with its relatively large diameter, causes a smaller stress concentration than the sharp end of a crack. This is however, a temporary solution that must be corrected at the first opportune time. \n\nIt is important to systematically check for possible stress concentrations caused by cracks—there is a critical crack length of 2a for which, when this value is exceeded, the crack proceeds to definite catastrophic failure. This ultimate failure is definite since the crack will propagate on its own once the length is greater than 2a. (There is no additional energy required to increase the crack length so the crack will continue to enlarge until the material fails.) The origins of the value 2a can be understood through Griffith's theory of brittle fracture.\n\nAnother method used to decrease the stress concentration is by creating the fillet at the sharp edges. It gives smooth flow of stress streamlines. In a threaded component force flow line is bent as it passes from shank portion to threaded portion as a result stress concentration takes place. To reduce this a small undercut is taken between shank and threaded portion.\n\nThe term \"stress raiser\" is used in orthopedics; a focus point of stress on an implanted orthosis is very likely to be its point of failure.\n\nClassic cases of metal failures due to stress concentrations include metal fatigue at the corners of the windows of the De Havilland Comet aircraft and brittle fractures at the corners of hatches in Liberty ships in cold and stressful conditions in winter storms in the Atlantic Ocean.\n\nThe maximum stress felt near a crack occurs in the area of lowest radius of curvature. In an elliptical crack of length formula_1 and width formula_2, under an applied external stress formula_3, the stress at the ends of the major axes is given by Inglis' equation:\n\nwhere ρ is the radius of curvature of the crack tip. A stress concentration factor is the ratio of the highest stress (formula_5) to a reference stress (formula_3) of the gross cross-section. As the radius of curvature approaches zero, the maximum stress approaches infinity. Note that the stress concentration factor is a function of the geometry of a crack, and not of its size. These factors can be found in typical engineering reference materials to predict the stresses that could otherwise not be analyzed using strength of materials approaches. This is not to be confused with 'Stress Intensity Factor'.\n\nThere are experimental methods for measuring stress concentration factors including photoelastic stress analysis, brittle coatings or strain gauges. While all these approaches have been successful, all also have experimental, environmental, accuracy and/or measurement disadvantages.\n\nDuring the design phase, there are multiple approaches to estimating stress concentration factors. Several catalogs of stress concentration factors have been published. Perhaps most famous is \"Stress Concentration Design Factors\" by Peterson, first published in 1953. Finite element methods are commonly used in design today. Theoretical approaches, using elasticity or strength of material considerations, can lead to equations similar to the one shown above.\n\nThere may be small differences between the catalog, FEM and theoretical values calculated. Each method has advantages and disadvantages. Many catalog curves were derived from experimental data. FEM calculates the peak stresses directly and nominal stresses may be easily found by integrating stresses in the surrounding material. The result is that engineering judgment may have to be used when selecting which data applies to making a design decision. Many theoretical stress concentration factors have been derived for infinite or semi-infinite geometries which may not be analyzable and are not testable in a stress lab, but tackling a problem using two or more of these approaches will allow an engineer to achieve an accurate conclusion.\n\n\n\n"}
{"id": "8573930", "url": "https://en.wikipedia.org/wiki?curid=8573930", "title": "Stuart White", "text": "Stuart White\n\nStuart White is an Australian educator and sustainability advocate. He is a professor and the Director of the Institute for Sustainable Futures at the University of Technology, Sydney. White has researched sustainability for more than twenty years, specialising in least cost planning for utilities and resource use efficiency. In 1998, White was a member of the NSW Task Force on Water Conservation, and in 2001 was a member of the Expert Panel on Environmental Flows for the Hawkesbury Nepean. He has written widely on sustainable futures and is often quoted in the media. White has criticised the building of desalination plants, citing their high operational costs and energy usage. \n\n"}
{"id": "1127304", "url": "https://en.wikipedia.org/wiki?curid=1127304", "title": "Tactical air navigation system", "text": "Tactical air navigation system\n\nA tactical air navigation system, commonly referred to by the acronym TACAN, is a navigation system used by military aircraft. It provides the user with bearing and distance (slant-range or hypotenuse) to a ground or ship-borne station. It is a more accurate version of the VOR/DME system that provides bearing and range information for civil aviation. The DME portion of the TACAN system is available for civil use; at VORTAC facilities where a VOR is combined with a TACAN, civil aircraft can receive VOR/DME readings. Aircraft equipped with TACAN avionics can use this system for en route navigation as well as non-precision approaches to landing fields. The space shuttle is one such vehicle that was designed to use TACAN navigation but later upgraded with GPS as a replacement.\n\nThe typical TACAN onboard user panel has control switches for setting the channel (corresponding to the desired surface station's assigned frequency), the operation mode for either transmit/receive (T/R, to get both bearing and range) or receive only (REC, to get bearing but not range). Capability was later upgraded to include an air-to-air mode (A/A) where two airborne users can get relative slant-range information. Depending on the installation, Air-to-Air mode may provide range, closure (relative velocity of the other unit), and bearing, though an air-to-air bearing is noticeably less precise than a ground-to-air bearing. A TACAN only equipped aircraft cannot receive bearing information from a VOR only station.\n\nThe TACAN navigation system is an evolution of radio transponder navigation systems that date back to the British Oboe system of World War II. In the United States, many companies were involved with the development of TACAN for military aircraft. Hoffman Laboratories Div. of the Hoffman Electronics Corp.–Military Products Division (now NavCom Defense Electronics) was a leader in developing the present TACAN system in the US starting in the late 1950s.\n\nTACAN in general can be described as the military version of the VOR/DME system. It operates in the frequency band 960-1215 MHz. The bearing unit of TACAN is more accurate than a standard VOR since it makes use of a two-frequency principle, with 15 Hz and 135 Hz components, and because UHF transmissions are less prone to signal bending than VHF.\n\nThe distance measurement component of TACAN operates with the same specifications as civil DMEs. Therefore, to reduce the number of required stations, TACAN stations are frequently co-located with VOR facilities. These co-located stations are known as VORTACs. This is a station composed of a VOR for civil bearing information and a TACAN for military bearing information and military/civil distance measuring information. The TACAN transponder performs the function of a DME without the need for a separate co-located DME. Because the rotation of the antenna creates a large portion of the azimuth (bearing) signal, if the antenna fails, the azimuth component is no longer available and the TACAN downgrades to a DME only mode.\n\nTheoretically a TACAN should provide a 9-fold increase in accuracy compared to a VOR, but operational use has shown only an approximate 3-fold increase.\n\nAccuracy of the 135 Hz azimuth component is ±1° or ±63 m at 3.47 km. Accuracy of the DME portion must be or 3 percent of slant range distance, whichever is greater, per FAA 9840.1 1982. and FAA N8200.121\n\nTACAN stations can provide distance up to 390 nautical miles.\n\nModern TACANs are much more accurate. The requirement now is to have portable TACAN that is IFR certifiable, both station and portable systems. The latest modern version of TACAN has been tested and could be a feasible back-up to future Air traffic control systems and may even be integrated into systems for a seamless back up.\n\nPast TACANs have relied on high output power (up to 10,000 watts) to ensure good signal in space to overcome nulls present in antenna design and to provide their required 200 mile range. With the advancement of technology, antenna design has improved with higher gain antennas, much shallower nulls, and lighter construction. Now it's feasible to have a 200 nmi range with a 400 watt TACAN DME transmitter, making the TACAN package much smaller, more portable and more reliable (a decrease in power also reduces heat, which lengthens the life of electronics).\n\nOn the first Space Shuttle flight, Capcom Joseph P. Allen reported up to the crew that their TACANs had locked onto the Channel 111 signals at St Petersburg, FL at a range of 250 miles.\n\nTACAN is getting smaller: full TACAN coverage can now be provided in a system that can be carried on a single trailer weighing less than 4000 lbs, and set up by two people in less than an hour. TACAN Transceivers can now be as small as lunch boxes (with full coverage and range) and the antennas can be reduced from 800 pounds to less than 100 pounds.\n\nBecause the azimuth and range units are combined in one system it provides for simpler installation. Less space is required than a VOR because a VOR requires a large counterpoise and a fairly complex phased antenna system. A TACAN system theoretically might be placed on a building, a large truck, an airplane, or a ship, and be operational in a short period of time. An airborne TACAN receiver can be used in air-to-air mode, which allows two cooperating aircraft to find their relative bearings and distance.\n\nFor military usage a primary drawback is lack of the ability to control emissions (EMCON) and stealth. Naval TACAN operations are designed so an aircraft can find the ship and land. There is no encryption involved, an enemy can simply use the range and bearing provided to attack a ship equipped with a TACAN. Some TACANs have the ability to employ a \"Demand Only\" mode wherein they will only transmit when interrogated by an aircraft on-channel. It is likely that TACAN will be replaced with a differential GPS system similar to the Local Area Augmentation System called JPALS. The Joint Precision Approach and Landing System has a low probability of intercept to prevent enemy detection and an aircraft carrier version can be used for autoland operations.\n\nSome systems used in the United States modulate the transmitted signal by using a 900 RPM rotating antenna. Since this antenna is fairly large and must rotate 24 hours a day, it can cause reliability issues. Modern systems have antennas that use electronic rotation (instead of mechanical rotation) with no moving parts.\n\nLike all other forms of ground-based aircraft radio navigation currently used, it is likely that TACAN will eventually be replaced by some form of space-based navigational system such as GPS.\n\n\n"}
{"id": "946963", "url": "https://en.wikipedia.org/wiki?curid=946963", "title": "Telephone line", "text": "Telephone line\n\nA telephone line or telephone circuit (or just line or circuit within the industry) is a single-user circuit on a telephone communication system. This is the physical wire or other signaling medium connecting the user's telephone apparatus to the telecommunications network, and usually also implies a single telephone number for billing purposes reserved for that user. Telephone lines are used to deliver landline telephone service and Digital subscriber line (DSL) phone cable service to the premises. Telephone overhead lines are connected to the public switched telephone network.\n\nIn 1878, the Bell Telephone Company began to use two-wire circuits (called the local loop) from each user's telephone to end offices which performed any necessary electrical switching to allow voice signals to be transmitted to more distant telephones.\n\nThese wires were typically copper, although aluminium has also been used, and were carried in balanced pairs of open wire, separated by about 25 cm (10\") on poles above the ground, and later as twisted pair cables. Modern lines may run underground, and may carry analog or digital signals to the exchange, or may have a device that converts the analog signal to digital for transmission on a carrier system.\n\nOften the customer end of that wire pair is connected to a data access arrangement;\nthe telephone company end of that wire pair is connected to a telephone hybrid.\n\nIn most cases, two copper wires (tip and ring) for each telephone line run from a home or other small building to a local telephone exchange. There is a central junction box for the building where the wires that go to telephone jacks throughout the building and wires that go to the exchange meet and can be connected in different configurations depending upon the subscribed telephone service. The wires between the junction box and the exchange are known as the local loop, and the network of wires going to an exchange, the access network.\n\nThe vast majority of houses in the U.S. are wired with 6-position modular jacks with four conductors (6P4C) wired to the house's junction box with copper wires. Those wires may be connected back to two telephone overhead lines at the local telephone exchange, thus making those jacks RJ14 jacks. More often, only two of the wires are connected to the exchange as one telephone line, and the others are unconnected. In that case, the jacks in the house are RJ11.\n\nOlder houses often have 4-conductor telephone station cable in the walls color coded with Bell System colors: red, green, yellow, black as 2-pairs of 22 AWG (0.33 mm²) solid copper; \"line 1\" uses the red/green pair and \"line 2\" uses the yellow/black pair.\nInside the walls of the house—between the house's outside junction box and the interior wall jacks—the most common telephone cable in new houses is Category 5 cable—4 pairs of 24 AWG (0.205 mm²) solid copper.\n\nInside large buildings, and in the outdoor cables that run to the telephone company POP, many telephone lines are bundled together in a single cable using the 25-pair color code.\n"}
{"id": "38505581", "url": "https://en.wikipedia.org/wiki?curid=38505581", "title": "TeraView", "text": "TeraView\n\nTeraView Limited, or TeraView, is a company that designs terahertz imaging and spectroscopy instruments and equipment for measurement and evaluation of pharmaceutical tablets, nanomaterials, ceramics and composites, integrated circuit chips and more.\n\nTeraView was co-founded by Michael Pepper (CSO) and Dr Don Arnone (CEO) as a spin-out of Toshiba Research Europe in April 2001. The company was set up to exploit the intellectual property and expertise developed in sourcing and detecting terahertz radiation (1 THz= 33.3 cm), using semiconductor technologies. Leading industry proponents of the technology sit on its Advisory Board, and TeraView maintains close links with the Cavendish Laboratory at the University of Cambridge, which was one of the research universities which had an interest in Terahertz techniques. It is also where Professor Pepper, has held the position of Professor of Physics since 1987.\n\nTeraView has developed a number of instruments that harness the properties of terahertz radiation. Terahertz light has some interesting application. Many common materials and living tissues are semi-transparent and have ‘terahertz fingerprints’, permitting them to be imaged, identified, and analyzed. Moreover, the non-ionizing properties of terahertz radiation and the relatively low power levels used, indicate that it is safe.\n\nBy applying different software analysis packages, the same base technologies can be brought to bear to several applications.\n\nThe company's primary focus of investigation includes the development of terahertz light into a usefulspectroscopic and imaging technique. The ‘terahertz gap’ – where until recently bright sources of light and sensitive means of detection were difficult to access – encompasses frequencies invisible to the naked eye in the electromagnetic spectrum, lying between microwave and infrared in the range from 0.3 to 3THz. TeraView's existing instruments generate, detect and manipulate THz light and have been tested in a number of application areas.\n\nThe applications of terahertz radiation in the pharmaceutical industry include nondestructive estimation of critical quality attributes in pharmaceutical products such as crystalline structure, thickness and chemical composition analysis. TeraView has demonstrated that terahertz instruments can produce 3D coating thickness maps for multiple coating layers and structural features models allowing better understanding and control of product scale up and manufacture.\n\nDue in part to its ability to recognize spectral fingerprints, terahertz pulsed imaging can be applied to provide contrast between different types of soft tissue. Also, it is a sensitive means of detecting the degree of water content and markers of cancer and other diseases. Attempts have been made to apply Terahertz to image cancers like breast, cancer as well as other diseases in medicine, oral health care, and related areas. The company announced it has been cleared by the Medicines and Healthcare products Regulatory Agency (MHRA) to trial \"in-vivo\" terahertz spectroscopy for bio-medical research. The trials will be held in Guy's Hospital in London and aim to determine if the technology can be applied real-time for accurate removal of cancer tissue.\n\nTerahertz technology has the potential to safely, noninvasively and quickly image through different types of clothing and other concealment and confusion materials. It has been hypothesized that because THz light is absorbed by explosive materials at certain frequencies it may be possible to find unique 'terahertz fingerprints' that can be distinguished from clothing or other materials. This has never been proved in a practical sense. The company's technology has been used by the Naval Surface Warfare Command to test the presence of different types of plastic explosives through clothing, including PETN (Pentaerythritol tetranitrate).\n\nTHz spectroscopy can be used as a non-contact analytical method. The absorption coefficient and refractive index measured by terahertz pulsed spectroscopy can be used directly to obtain the high frequency-dependent complex conductivities of materials in the 0.1 – 3 THz (3 – 100 cm) region of the electromagnetic spectrum. The technology has been applied to some areas of solid state physics research such as semiconductors, high-temperature superconductors, terahertz metamaterials, carrier density dynamics, graphene, carbon nanotubes, magnetism and more.\n\nTerahertz light can be used as non-contact technique for analysis in material integrity studies. It has proved to be effective in nondestructive inspection of layers in paints and coatings, detecting structural defects in ceramic and composite materials and imaging the physical structure of paintings and manuscripts. The use of THz waves for non-destructive evaluation enables inspection of multi-layered structures and can identify abnormalities from foreign material inclusions, disbond and delamination, mechanical impact damage, heat damage, and water or hydraulic fluid ingression. The company's Chief Scientific Director, Sir Michael Pepper, explains that THz imaging can measure thickness across a substrate precisely and it can also obtain the density of the coating: \"The radiation is reflected each time there is a change in material. The time of arrival is measured and then various algorithms complete the picture by developing 3D fine feature images and precise material identifications\". Further research by the company and active collaboration with the University of Cambridge is aiming to develop a terahertz sensor that can be used to measure the quality of paint coatings on cars.\n\nTerahertz technology allows high resolution 3D imaging of semiconductor packages and integrated circuit devices. THz time-domain reflectometry (TDR) offers significant advantages in imaging resolution compared to existing fault isolation techniques and conventional millimetre wave systems. Working with Intel on the applications of THz technology for the semiconductor industry, TeraView developed a new technique which combines electro-optics and THz pulses in a non-destructive \"electro-optical terahertz pulsed reflectometry\" (EOTPR) which operates at up to 2 THz with resolution of 10 μm for improved fault isolation and failure analysis process-flow studies. \"\"The unique capabilities of terahartz TDR and its advantages over the conventional TDR have been recognized. With such revolutionary concept, innovative design and superior performance, EOTPR will become an essential tool for microelectronic package fault isolation and failure analysis.\" \" Yongming Cai, Zhiyong Wang, Rajen Dias, and Deepak Goyal, Intel Corporation.\n\n\n"}
{"id": "4622312", "url": "https://en.wikipedia.org/wiki?curid=4622312", "title": "Transient response", "text": "Transient response\n\nIn electrical engineering and mechanical engineering, a transient response is the response of a system to a change from an equilibrium or a steady state. The transient response is not necessarily tied to abrupt events but to any event that affects the equilibrium of the system. The impulse response and step response are transient responses to a specific input (an impulse and a step, respectively).\n\nThe response can be classified as one of three types of damping that describes the output in relation to the steady-state response.\n\n\nTransient response can be quantified with the following properties.\n"}
{"id": "100782", "url": "https://en.wikipedia.org/wiki?curid=100782", "title": "Wagonway", "text": "Wagonway\n\nWagonways (or Waggonways) consisted of the horses, equipment and tracks used for hauling wagons, which preceded steam-powered railways. The terms plateway, tramway and dramway were used. The advantage of wagonways was that far bigger loads could be transported with the same power.\n\nThe earliest evidence is of the 6 to 8.5 km long \"Diolkos\" paved trackway, which transported boats across the Isthmus of Corinth in Greece from around 600 BC. Wheeled vehicles pulled by men and animals ran in grooves in limestone, which provided the track element, preventing the wagons from leaving the intended route. The Diolkos was in use for over 650 years, until at least the 1st century AD. Paved trackways were later built in Roman Egypt.\n\nSuch an operation was illustrated in Germany in 1556 by Georgius Agricola (image right) in his work De re metallica. This line used \"Hund\" carts with unflanged wheels running on wooden planks and a vertical pin on the truck fitting into the gap between the planks to keep it going the right way. The miners called the wagons \"Hunde\" (\"dogs\") from the noise they made on the tracks.\nAround 1568, German miners working in the Mines Royal near Keswick used such a system. Archaeological work at the Mines Royal site at Caldbeck in the English Lake District confirmed the use of \"hunds\".\n\nIn 1604, Huntingdon Beaumont completed the Wollaton Wagonway, built to transport coal from the mines at Strelley to Wollaton Lane End, just west of Nottingham, England. Wagonways have been discovered between Broseley and Jackfield in Shropshire from 1605, used by James Clifford to transport coal from his mines in Broseley to the Severn River. It has been suggested that these are somewhat older than that at Wollaton.\n\nThe Middleton Railway in Leeds, which was built in 1758 as a wagonway, later became the world's first operational railway (other than funiculars), albeit in an upgraded form. In 1764, the first railway in the America was built in Lewiston, New York as a wagonway.\n\nWagonways improved coal transport by allowing one horse to deliver between of coal per run an approximate fourfold increase. Wagonways were usually designed to carry the fully loaded wagons downhill to a canal or boat dock and then return the empty wagons back to the mine.\n\nUntil the beginning of the Industrial Revolution, rails were made of wood, were a few inches wide and were fastened end to end, on logs of wood or \"sleepers\", placed crosswise at intervals of two or three feet. In time, it became common to cover them with a thin flat sheathing or \"plating\" of iron, in order to add to their life and reduce friction. This caused more wear on the wooden rollers of the wagons and towards the middle of the 18th century, led to the introduction of iron wheels. However, the iron sheathing was not strong enough to resist buckling under the passage of the loaded wagons, so rails made wholly of iron were invented.\nIn 1760 the Coalbrookdale Iron Works began to reinforce their wooden railed tramway with iron bars, which were found to facilitate passage and diminish expenses. As a result, in 1767, they began to make cast iron rails. These were probably long, with four projecting ears or lugs by to enable them to be fixed to the sleepers. The rails were wide and thick. Later, descriptions also refer to rails long and only wide.\n\nA later system involved \"L\" shaped iron rails or plates, each long and wide, having on the inner side an upright ledge or flange, high at the centre and tapering to at the ends, for the purpose of keeping the flat wheels on the track. Subsequently, to increase strength, a similar flange might be added below the rail. Wooden sleepers continued to be usedthe rails were secured by spikes passing through the extremitiesbut, circa 1793, stone blocks began to be used, an innovation associated with Benjamin Outram, although he was not the originator. This type of rail was known as the plate-rail, tramway-plate or way-plate, names that are preserved in the modern term \"platelayer\" applied to the workers who lay and maintain the permanent way. The wheels of flangeway wagons were plain, but they could not operate on ordinary roads as the narrow rims would dig into the surface.\n\nAnother form of rail, the edge rail, was first used by William Jessop on a line that was opened as part of the Charnwood Forest Canal between Loughborough and Nanpantan in Leicestershire in 1789. This line was originally designed as a plateway on the Outram system, but objections were laying raised to rails with upstanding ledges or flanges on the turnpike. This difficulty was overcome by paving or \"causewaying\" the road up to the level of the top of the flanges. In 1790, Jessop and his partner Outram began to manufacture edge-rails. Another example of the edge rail application was the Lake Lock Rail Road used primarily for coal transport. This was a public railway (charging a toll) and opened for traffic in 1798, making it the world's oldest public railway. The route started at Lake Lock, Stanley, on the Aire & Calder Navigation, running from Wakefield to Outwood, a distance of approximately . Edge-rails (with a side rack) were used on the nearby Middleton-Leeds rack railway (a length of this rail is on display in Leeds City Museum). The wheels of an edgeway have flanges, like modern railways and tramways. Causewaying is also done on modern level crossings and tramways.\n\nThese two systems of constructing iron railways continued to exist until the early 19th century. In most parts of England the plate-rail was preferred. Plate-rails were used on the Surrey Iron Railway (SIR), from Wandsworth to West Croydon. The SIR was sanctioned by Parliament in 1801 and finished in 1803. Like the Lake Lock Rail Road, the SIR was available to the public on payment of tolls; previous lines had all been private and reserved exclusively for the use of their owners. Since it was used by individual operators, vehicles would vary greatly in wheel spacing (gauge) and the plate rail coped better. In South Wales again, where in 1811 the railways were connected with canals, collieries, ironworks, and copper works, and had a total length of nearly , the plateway was almost universal. But in the North of England and in Scotland the edge-rail was held in greater favor, and soon its superiority was generally established. Wheels tended to bind against the flange of the plate rail and mud and stones would build up.\nThe manufacture of the rails themselves was gradually improved. By making them in longer lengths, the number of joints per mile was reduced. Joints were always the weakest part of the line. Another advance was the substitution of wrought iron for cast iron, though that material did not gain wide adoption until after the patent for an improved method of rolling rails was granted in 1820 to John Birkinshaw, of the Bedlington Ironworks. His rails were wedge-shaped in section, much wider at the top than at the bottom, with the intermediate portion or web thinner still. He recommended that they be made long, suggesting that several might be welded together end to end to form considerable lengths. They were supported on sleepers by chairs at intervals of , and were fish-bellied between the support points. As used by George Stephenson on the Stockton & Darlington, and Canterbury & Whitstable lines, they weighed . On the Liverpool and Manchester Railway they were usually long and weighed and were fastened by iron wedges to chairs weighing each. The chairs were in turn fixed to the sleepers by two iron spikes, half-round wooden cross sleepers employed on embankments and stone blocks square by deep in cuttings. The fish-bellied rails were found to break near the chairs and starting in 1834, they were gradually replaced with parallel rails weighing .\n\nIn 1804, Richard Trevithick, in the first recorded use of steam power on a railway, ran a high-pressure steam locomotive with smooth wheels on an 'L' section plateway near Merthyr Tydfil, but it was more expensive than horses. He made three trips from the iron mines at Penydarren to the Merthyr-Cardiff Canal and each time broke the rails that were designed for horse wagon loads. There was general doubt at the time that smooth wheels could obtain traction on smooth rails. This resulted in proposals using rack or other drive mechanisms.\nMr Blenkinsop of Middleton Colliery patented the use of cogged wheels in 1811 and in 1812, the Middleton Railway (edgeway, rack rail) successfully used twin cylinder steam locomotives made by Matthew Murray of Holbeck, Leeds. George Stephenson made his first steam locomotive in 1813 (patented 1815) for the Killingworth colliery, and found smooth wheels on smooth rails provided adequate grip. Although he later recounted that they called this locomotive 'My Lord' as it was financed by Lord Ravensworth, it seems that it was known at the time as Blücher. In 1814 William Stewart was engaged by Parkend Coal Co in the Forest of Dean for the construction of a steam locomotive, which when trialled was reported to be successful. Stewart did not receive his expected reward and the two parties parted on bad terms. Stewart was 'obliged to abandon the engine to that Company'. In 1821, a wagonway was proposed to connect the mines at West Durham, Darlington and the River Tees at Stockton, George Stephenson successfully argued that horse-drawn wagonways were obsolete and a steam-powered railway could carry 50 times as much coal. In 1825 he built the locomotive \"Locomotion\" for the Stockton and Darlington Railway in England's northeast, which became the world's first public steam railway in 1825, via both horse power and steam power on different runs.\n\nstationary steam engines for mining were generally available around the middle of the 18th century. Wagonways and steam-powered railways had steep uphill sections and would employ a cable powered by a stationary steam engine to work the inclined sections. British troops in Lewiston, New York used a cable wagonway to move supplies to bases before the American Revolutionary War. The Stockton and Darlington had two inclined sections powered by cable. The transition from a wagonway to a fully steam-powered railway was gradual. Railways up to the 1830s that were steam-powered often made runs with horses when the steam locomotives were unavailable. Even in the steam age, it was convenient to use horses in station yards to shunt wagons from one place to another. Horses do not need lengthy times to raise steam in the boiler, and can take shortcuts from one siding to another. At Hamley Bridge tenders were called for the supply of horses, in part because normal railway staff lacked horse handling skills. \n\nWooden rails continued to be used for temporary railroads into the early twentieth century. Some timber harvesting companies in the southeastern United States created pole roads using unmarketable logs, which were effectively free, to create tracks at a cost of between $100 and $500 per mile. Permanence was not an issue, as the lumberjacks moved on to other stands of timber as each area was cleared. At least one such pole road system reportedly extended some 20 miles.\n\nTypically the pole rails were logs of 8 to 12 inches (20 to 30 cm) diameter, laid parallel directly on the ground without cross-ties, and joined end-to-end with lap joints and wooden pegs. Rolling stock typically had wheels either with concave treads that hugged the top of the pole rails, or un-flanged wheels with separate guide wheels running against the side of each rail. Steam traction engines and some purpose-built locomotives were successfully used for hauling trains of logs. For example, ‘’Perdido’’ was built by Adams & Price Locomotive and Machinery Works of Nashville, Tennessee in 1885 for the Wallace, Sanford and Company sawmill at Williams Station, Alabama, where it hauled up to seven cars of 3 or 4 logs each. This was a geared engine (4.5 to 1 gear ratio), driving four individually-rotating concave-tread wheels on stationery axles via chain drives; powerful but running less than 5 miles per hour.\n\nAs steam power gradually replaced horse power, the term \"wagonway\" became obsolete and was superseded by the term \"railway\". In 2018, very few horse or cable freight railways, notable examples being the cable-hauled St Michael's Mount Tramway and the Reisszug - which has been in continuous operation since around 1500. A few passenger lines continue to operate, including the horse-hauled Douglas Bay Horse Tramway and the cable-hauled San Francisco cable cars.\n\n\n"}
{"id": "862179", "url": "https://en.wikipedia.org/wiki?curid=862179", "title": "Watchdog timer", "text": "Watchdog timer\n\nA watchdog timer (sometimes called a \"computer operating properly\" or \"COP\" timer, or simply a \"watchdog\") is an electronic timer that is used to detect and recover from computer malfunctions. During normal operation, the computer regularly resets the watchdog timer to prevent it from elapsing, or \"timing out\". If, due to a hardware fault or program error, the computer fails to reset the watchdog, the timer will elapse and generate a timeout signal. The timeout signal is used to initiate corrective action or actions. The corrective actions typically include placing the computer system in a safe state and restoring normal system operation.\n\nWatchdog timers are commonly found in embedded systems and other computer-controlled equipment where humans cannot easily access the equipment or would be unable to react to faults in a timely manner. In such systems, the computer cannot depend on a human to invoke a reboot if it hangs; it must be self-reliant. For example, remote embedded systems such as space probes are not physically accessible to human operators; these could become permanently disabled if they were unable to autonomously recover from faults. A watchdog timer is usually employed in cases like these. Watchdog timers may also be used when running untrusted code in a sandbox, to limit the CPU time available to the code and thus prevent some types of denial-of-service attacks.\n\nThe act of restarting a watchdog timer, commonly referred to as \"kicking\" the watchdog, is typically done by writing to a watchdog control port. Alternatively, in microcontrollers that have an integrated watchdog timer, the watchdog is sometimes kicked by executing a special machine language instruction. An example of this is the CLRWDT (clear watchdog timer) instruction found in the instruction set of some PIC microcontrollers.\n\nIn computers that are running operating systems, watchdog resets are usually invoked through a device driver. For example, in the Linux operating system, a user space program will kick the watchdog by interacting with the watchdog device driver, typically by writing a zero character to /dev/watchdog. The device driver, which serves to abstract the watchdog hardware from user space programs, is also used to configure the time-out period and start and stop the timer.\n\nWatchdog timers come in many configurations, and many allow their configurations to be altered. Microcontrollers often include an integrated, on-chip watchdog. In other computers the watchdog may reside in a nearby chip that connects directly to the CPU, or it may be located on an external expansion card in the computer's chassis. The watchdog and CPU may share a common clock signal, as shown in the block diagram below, or they may have independent clock signals.\n\nTwo or more timers are sometimes cascaded to form a \"multistage watchdog timer\", where each timer is referred to as a \"timer stage\", or simply a \"stage\". For example, the block diagram below shows a three-stage watchdog. In a multistage watchdog, only the first stage is kicked by the processor. Upon first stage timeout, a corrective action is initiated and the next stage in the cascade is started. As each subsequent stage times out, it triggers a corrective action and starts the next stage. Upon final stage timeout, a corrective action is initiated, but no other stage is started because the end of the cascade has been reached. Typically, single-stage watchdog timers are used to simply restart the computer, whereas multistage watchdog timers will sequentially trigger a series of corrective actions, with the final stage triggering a computer restart.\n\nWatchdog timers may have either fixed or programmable time intervals. Some watchdog timers allow the time interval to be programmed by selecting from among a few selectable, discrete values. In others, the interval can be programmed to arbitrary values. Typically, watchdog time intervals range from ten milliseconds to a minute or more. In a multistage watchdog, each timer may have its own, unique time interval.\n\nA watchdog timer may initiate any of several types of corrective action, including maskable interrupt, non-maskable interrupt, processor reset, fail-safe state activation, power cycling, or combinations of these. Depending on its architecture, the type of corrective action or actions that a watchdog can trigger may be fixed or programmable. Some computers (e.g., PC compatibles) require a pulsed signal to invoke a processor reset. In such cases, the watchdog typically triggers a processor reset by activating an internal or external pulse generator, which in turn creates the required reset pulses.\n\nIn embedded systems and control systems, watchdog timers are often used to activate fail-safe circuitry. When activated, the fail-safe circuitry forces all control outputs to safe states (e.g., turns off motors, heaters, and high-voltages) to prevent injuries and equipment damage while the fault persists. In a two-stage watchdog, the first timer is often used to activate fail-safe outputs and start the second timer stage; the second stage will reset the computer if the fault cannot be corrected before the timer elapses.\n\nWatchdog timers are sometimes used to trigger the recording of system state information—which may be useful during fault recovery—or debug information (which may be useful for determining the cause of the fault) onto a persistent medium. In such cases, a second timer—which is started when the first timer elapses—is typically used to reset the computer later, after allowing sufficient time for data recording to complete. This allows time for the information to be saved, but ensures that the computer will be reset even if the recording process fails.\n\nFor example, the above diagram shows a likely configuration for a two-stage watchdog timer. During normal operation the computer regularly kicks Stage1 to prevent a timeout. If the computer fails to kick Stage1 (e.g., due to a hardware fault or programming error), Stage1 will eventually timeout. This event will start the Stage2 timer and, simultaneously, notify the computer (by means of a non-maskable interrupt) that a reset is imminent. Until Stage2 times out, the computer may attempt to record state information, debug information, or both. The computer will be reset upon Stage2 timeout.\n\nA computer system is typically designed so that its watchdog timer will be kicked only if the computer deems the system functional. The computer determines whether the system is functional by conducting one or more fault detection tests and it will kick the watchdog only if all tests have passed. In computers that are running an operating system and multiple processes, a single, simple test may be insufficient to guarantee normal operation, as it could fail to detect a subtle fault condition and therefore allow the watchdog to be kicked even though a fault condition exists.\n\nFor example, in the case of the Linux operating system, a user-space watchdog daemon may simply kick the watchdog periodically without performing any tests. As long as the daemon runs normally, the system will be protected against serious system crashes such as a kernel panic. To detect less severe faults, the daemon can be configured to perform tests that cover resource availability (e.g., sufficient memory and file handles, reasonable CPU time), evidence of expected process activity (e.g., system daemons running, specific files being present or updated), overheating, and network activity, and system-specific test scripts or programs may also be run.\n\nUpon discovery of a failed test, the Linux watchdog daemon may attempt to perform a software-initiated restart, which can be preferable to a hardware reset as the file systems will be safely unmounted and fault information will be logged. However it is essential to have the insurance of the hardware timer as a software restart can fail under a number of fault conditions. In effect, this is a dual-stage watchdog with the software restart comprising the first stage and the hardware reset the second stage.\n\n\n"}
