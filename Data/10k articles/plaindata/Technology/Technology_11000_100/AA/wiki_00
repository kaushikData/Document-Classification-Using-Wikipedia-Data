{"id": "41075880", "url": "https://en.wikipedia.org/wiki?curid=41075880", "title": "ASBIS", "text": "ASBIS\n\nASBISC Enterprises PLC is a multinational corporate group that is engaged in distribution of IT-products (mobile devices, computer software and hardware) in Europe, Middle East and Africa (EMEA) emerging markets and is headquartered in Limassol (Cyprus).\n\nASBIS distributes a wide range of A-branded finished products and IT components to assemblers, system integrators, local brands, retail and wholesale companies. The company is an official distributor of world’s leading brands such as Intel, Advanced Micro Devices (\"AMD\"), Seagate Technology, Western Digital, Samsung, Microsoft, Toshiba, Dell, Acer, Hitachi, Gigabyte, Lenovo and Apple. The company generates a significant part of its revenue from the sales of IT and consumer electronics products under its own brands Prestigio and Canyon.\n\nThe company was founded in Belarus in 1990. In 1992 ASBIS signed its first distribution agreement with Seagate Technology and started selling data storage systems. By 1994 ASBIS established presence on Russian and Ukrainian IT markets.\n\nIn 1995 the company was incorporated in Cyprus and its headquarters moved to Limassol. By the year of 1997 ASBIS reached revenue of $100 million and was recognized by vendors as the fastest growing distribution company in the EMEA region.\n\nDuring 1998 ASBIS new subsidiaries were opened in the Czech Republic, Croatia, Yugoslavia, Hungary, Poland, Romania, Bulgaria and Slovenia. In 1999 company opened an office in Prague to serve subsidiaries and key customers in 10 countries of Central and Eastern Europe. In 2000 a distribution centre in Dubai was set up to serve Gulf and Central Asia regions.\n\nIn 2001 Intel recognized ASBIS as the fastest growing distributor of communication products in the emerging markets of Europe. Products of the company’s brand Canyon became available to more than 10,000 customers in 20 countries. In 2002 the company’s income reached $540 million and showed a 44% increase compared to 2001. The Greek Black Sea Fund and Alfa-bank became the portfolio investors of ASBIS.\n\nBy 2004 ASBIS had opened 37 offices in 25 countries, the company’s turnover reached $680 million, 4 distribution centers and 33 local warehouses were serving over 11,000 active customers. In 2006 ASBIS obtained a listing on the London Stock Exchange’s Alternative Investment Market. In 2007 the company completed a successful IPO at Warsaw Stock Exchange. 36.7% of the company shares were traded in 2011.\n\nIn 2009 while coping with the consequences of the global economic crisis ASBIS signed 25 new distribution agreements, including those with Apple for Georgia and 9 countries of the Commonwealth of Independent States (CIS). In 2011 ASBIS started to sell tablets and in 2012 smartphones under its own brand Prestigio.\n\nIn 2011 Prestigio navigation system was ranked first in sales in Russia with a market share 28.5%.\n\nIn 2013 Prestigio introduced the first two ranges of B2B products: MultiBoard (interactive flat panel displays with Windows PCs and supporting software for meeting rooms and education) and Digital Signage (public information screens). In 2016 MultiBoard range extended with the biggest ultra high definition display with interactive touch features - MultiBoard 98\" UHD.\n\nAt the present time, ASBIS distributes mobile gadgets, computer software and hardware on the markets of the following countries: Ukraine, Poland, Estonia, Romania, Croatia, Serbia, Hungary, Belarus, Bulgaria, Czech Republic, Lithuania, Slovenia, United Arab Emirates, Slovakia, Netherlands, Ireland, Cyprus, Russia, Morocco, Turkey, Latvia, Bosnia and Herzegovina, Italy, Kazakhstan, Germany, and Saudi Arabia.\n\nThe company’s headquarters are located in Cyprus and coordinate the work of 3 logistics centers in the Czech Republic, United Arab Emirates, and China. 47 local offices in 29 countries enable the company to supply more than 32,000 customers in 86 countries. In 2012 the company’s sales were shared between the following markets: 40.41% – the CIS, 34.82% – Central and Eastern Europe, 13.20% – Middle East, and 8.31% – Western Europe.\n\nIn 2011, 25% of ASBIS income accounted for Prestigio electronics sales, and 75% for the distribution of electronics of leading global computer brands such as HP, Dell, and Lenovo. ASBIS is the exclusive distributor of Apple products in all CIS countries except Russia and Ukraine.\n\nIn 2012 the company’s revenue from the sales of private labels Prestigio and Canyon grew to a historical record of $157.590 million and as a result the own brand contribution to the total revenue was 9.03%.\n\nIn 2002, ASBIS started the process of developing its own online purchasing platform IT4Profit. The platform is used for electronic trading with B2B customers as well as for electronic data interchange between the company and its subsidiaries. In 2011 dealings through IT4Profit online platform represented 55% of the company’s revenue.\n\nIn April, 2013 ASBIS in cooperation with Intel started to develop a new smartphone under the brand Prestigio. Smartphone Prestigio MultiPhone PAP5430 with the processor Intel Atom Z2420 went on sale in August 2013.\nOn October 24, 2013 Prestigio and MediaTek introduced the new models of tablets based on MediaTek MT8389 processor.\n\nIn 2013 the company’s revenue from the sales of private labels Prestigio and Canyon grew to a historical record of $468.988 million, and as a result of the own brand contribution, the total revenue growth was 24.42%.\n\nYuri Antoshkin held the position of General Manager, Prestigio EMEA, from 2006 to October 2014. Over the last decade the company's sales increased almost 30 times followed by the established cooperation with the largest Russian electronic retailers (Euroset, Eldorado, M.video and others). The Prestigio brand has become the leader in the Russian market of GPS navigation devices and has entered the top 3 of the tablet computers market in Russia.\n\nIn 2011 the company’s revenue increased by 3.28% to reach $1.48 billion. Net income showed a significant growth and increased by 335% to reach $5.66 million (up from $1.3 million in 2010).\n\nOperating income grew by 66% to reach $15.66 million (up from $9.44 million in 2010). EBITDA increased by 50.7% to reach $18.7 million (up from $12.4 million in 2010).\n\nAccording to the company’s operations in 2012 revenues grew by 17.73% to reach $1.744 billion. Operating income reached $20.22 million and net income increased by 59.84% to reach $9.047 million. EBITDA grew by 23.11% to reach $23.02 million.\n\nIn 2013 the company’s revenues increased by 10.06% to reach $1.92 billion. Operating income reached $31.939 million and net income grew by 40.51% to reach $12.712 million. EBITDA increased by 50.95% to reach $34.84 million.\n\n"}
{"id": "14205946", "url": "https://en.wikipedia.org/wiki?curid=14205946", "title": "Algae fuel", "text": "Algae fuel\n\nAlgae fuel, algal biofuel, or algal oil is an alternative to liquid fossil fuels that uses algae as its source of energy-rich oils. Also, algae fuels are an alternative to commonly known biofuel sources, such as corn and sugarcane.<ref name=\"doi10.1016/j.copbio.2010.03.005\"></ref> Several companies and government agencies are funding efforts to reduce capital and operating costs and make algae fuel production commercially viable. Like fossil fuel, algae fuel releases when burnt, but unlike fossil fuel, algae fuel and other biofuels only release recently removed from the atmosphere via photosynthesis as the algae or plant grew. The energy crisis and the world food crisis have ignited interest in algaculture (farming algae) for making biodiesel and other biofuels using land unsuitable for agriculture. Among algal fuels' attractive characteristics are that they can be grown with minimal impact on fresh water resources, can be produced using saline and wastewater, have a high flash point, and are biodegradable and relatively harmless to the environment if spilled. Algae cost more per unit mass than other second-generation biofuel crops due to high capital and operating costs, but are claimed to yield between 10 and 100 times more fuel per unit area. The United States Department of Energy estimates that if algae fuel replaced all the petroleum fuel in the United States, it would require , which is only 0.42% of the U.S. map, or about half of the land area of Maine. This is less than the area of corn harvested in the United States in 2000.\n\nThe head of the Algal Biomass Organization stated in 2010 that algae fuel could reach price parity with oil in 2018 if granted production tax credits. However, in 2013, Exxon Mobil Chairman and CEO Rex Tillerson said that after committing to spend up to $600 million over 10 years on development in a joint venture with J. Craig Venter's Synthetic Genomics in 2009, Exxon pulled back after four years (and $100 million) when it realized that algae fuel is \"probably further\" than 25 years away from commercial viability. On the other hand, Solazyme, Sapphire Energy, and Algenol, among others have begun commercial sale of algal biofuel in 2012 and 2013, and 2015, respectively. By 2017, most efforts had been abandoned or changed to other applications, with only a few remaining.\n\nIn 1942 Harder and Von Witsch were the first to propose that microalgae be grown as a source of lipids for food or fuel. Following World War II, research began in the US, Germany, Japan, England, and Israel on culturing techniques and engineering systems for growing microalgae on larger scales, particularly species in the genus \"Chlorella\". Meanwhile, H. G. Aach showed that \"Chlorella pyrenoidosa\" could be induced via nitrogen starvation to accumulate as much as 70% of its dry weight as lipids.<ref name=\"doi10.1007/BF00410827\"></ref> Since the need for alternative transportation fuel had subsided after World War II, research at this time focused on culturing algae as a food source or, in some cases, for wastewater treatment.<ref name=\"doi10.1007/978-94-007-5479-9_1\"></ref>\n\nInterest in the application of algae for biofuels was rekindled during the oil embargo and oil price surges of the 1970s, leading the US Department of Energy to initiate the Aquatic Species Program in 1978. The Aquatic Species Program spent $25 million over 18 years with the goal of developing liquid transportation fuel from algae that would be price competitive with petroleum-derived fuels. The research program focused on the cultivation of microalgae in open outdoor ponds, systems which are low in cost but vulnerable to environmental disturbances like temperature swings and biological invasions. 3,000 algal strains were collected from around the country and screened for desirable properties such as high productivity, lipid content, and thermal tolerance, and the most promising strains were included in the SERI microalgae collection at the Solar Energy Research Institute (SERI) in Golden, Colorado and used for further research. Among the program's most significant findings were that rapid growth and high lipid production were \"mutually exclusive\", since the former required high nutrients and the latter required low nutrients. The final report suggested that genetic engineering may be necessary to be able to overcome this and other natural limitations of algal strains, and that the ideal species might vary with place and season. Although it was successfully demonstrated that large-scale production of algae for fuel in outdoor ponds was feasible, the program failed to do so at a cost that would be competitive with petroleum, especially as oil prices sank in the 1990s. Even in the best case scenario, it was estimated that unextracted algal oil would cost $59–186 per barrel, while petroleum cost less than $20 per barrel in 1995. Therefore, under budget pressure in 1996, the Aquatic Species Program was abandoned.\n\nOther contributions to algal biofuels research have come indirectly from projects focusing on different applications of algal cultures. For example, in the 1990s Japan's Research Institute of Innovative Technology for the Earth (RITE) implemented a research program with the goal of developing systems to fix using microalgae. Although the goal was not energy production, several studies produced by RITE demonstrated that algae could be grown using flue gas from power plants as a source, an important development for algal biofuel research. Other work focusing on harvesting hydrogen gas, methane, or ethanol from algae, as well as nutritional supplements and pharmaceutical compounds, has also helped inform research on biofuel production from algae.\n\nFollowing the disbanding of the Aquatic Species Program in 1996, there was a relative lull in algal biofuel research. Still, various projects were funded in the US by the Department of Energy, Department of Defense, National Science Foundation, Department of Agriculture, National Laboratories, state funding, and private funding, as well as in other countries. More recently, rising oil prices in the 2000s spurred a revival of interest in algal biofuels and US federal funding has increased, numerous research projects are being funded in Australia, New Zealand, Europe, the Middle East, and other parts of the world, and a wave of private companies has entered the field (see Companies). In November 2012, Solazyme and Propel Fuels made the first retail sales of algae-derived fuel, and in March 2013 Sapphire Energy began commercial sales of algal biofuel to Tesoro.\n\nAlgal oil is used as a source of fatty acid supplementation in food products, as it contains mono- and polyunsaturated fats, in particular EPA and DHA. Its DHA content is roughly equivalent to that of salmon based fish oil.\n\nAlgae can be converted into various types of fuels, depending on the technique and the part of the cells used. The lipid, or oily part of the algae biomass can be extracted and converted into biodiesel through a process similar to that used for any other vegetable oil, or converted in a refinery into \"drop-in\" replacements for petroleum-based fuels. Alternatively or following lipid extraction, the carbohydrate content of algae can be fermented into bioethanol or butanol fuel.\n\nBiodiesel is a diesel fuel derived from animal or plant lipids (oils and fats). Studies have shown that some species of algae can produce 60% or more of their dry weight in the form of oil. Because the cells grow in aqueous suspension, where they have more efficient access to water, and dissolved nutrients, microalgae are capable of producing large amounts of biomass and usable oil in either high rate algal ponds or photobioreactors. This oil can then be turned into biodiesel which could be sold for use in automobiles. Regional production of microalgae and processing into biofuels will provide economic benefits to rural communities.\n\nAs they do not have to produce structural compounds such as cellulose for leaves, stems, or roots, and because they can be grown floating in a rich nutritional medium, microalgae can have faster growth rates than terrestrial crops. Also, they can convert a much higher fraction of their biomass to oil than conventional crops, e.g. 60% versus 2-3% for soybeans. The per unit area yield of oil from algae is estimated to be from 58,700 to 136,900 L/ha/year, depending on lipid content, which is 10 to 23 times as high as the next highest yielding crop, oil palm, at 5 950 L/ha/year.<ref name=\"doi10.1016/j.rser.2012.01.003\"></ref>\n\nThe U.S. Department of Energy's Aquatic Species Program, 1978–1996, focused on biodiesel from microalgae. The final report suggested that biodiesel could be the only viable method by which to produce enough fuel to replace current world diesel usage. If algae-derived biodiesel were to replace the annual global production of 1.1bn tons of conventional diesel then a land mass of 57.3 million hectares would be required, which would be highly favorable compared to other biofuels.\n\nButanol can be made from algae or diatoms using only a solar powered biorefinery. This fuel has an energy density 10% less than gasoline, and greater than that of either ethanol or methanol. In most gasoline engines, butanol can be used in place of gasoline with no modifications. In several tests, butanol consumption is similar to that of gasoline, and when blended with gasoline, provides better performance and corrosion resistance than that of ethanol or E85.\n\nThe green waste left over from the algae oil extraction can be used to produce butanol. In addition, it has been shown that macroalgae (seaweeds) can be fermented by bacteria of genus \"Clostridia\" to butanol and other solvents.\n\nBiogasoline is gasoline produced from biomass. Like traditionally produced gasoline, it contains between 6 (hexane) and 12 (dodecane) carbon atoms per molecule and can be used in internal-combustion engines.\n\nMethane, the main constituent of natural gas can be produced from algae in various methods, namely gasification, pyrolysis and anaerobic digestion. In gasification and pyrolysis methods methane is extracted under high temperature and pressure. Anaerobic digestion is a straightforward method involved in decomposition of algae into simple components then transforming it into fatty acids using microbes like acidogenic bacteria followed by removing any solid particles and finally adding methanogenic bacteria to release a gas mixture containing methane. A number of studies have successfully shown that biomass from microalgae can be converted into biogas via anaerobic digestion. Therefore, in order to improve the overall energy balance of microalgae cultivation operations, it has been proposed to recover the energy contained in waste biomass via anaerobic digestion to methane for generating electricity.\n\nThe Algenol system which is being commercialized by BioFields in Puerto Libertad, Sonora, Mexico utilizes seawater and industrial exhaust to produce ethanol. Porphyridium cruentum also have shown to be potentially suitable for ethanol production due to its capacity for accumulating large amount of carbohydrates.\n\nAlgae can be used to produce 'green diesel' (also known as renewable diesel, hydrotreating vegetable oil or hydrogen-derived renewable diesel) through a hydrotreating refinery process that breaks molecules down into shorter hydrocarbon chains used in diesel engines. It has the same chemical properties as petroleum-based diesel meaning that it does not require new engines, pipelines or infrastructure to distribute and use. It has yet to be produced at a cost that is competitive with petroleum. While hydrotreating is currently the most common pathway to produce fuel-like hydrocarbons via decarboxylation/decarbonylation, there is an alternative process offering a number of important advantages over hydrotreating. In this regard, the work of Crocker et al. and Lercher et al. is particularly noteworthy. For oil refining, research is underway for catalytic conversion of renewable fuels by decarboxylation. As the oxygen is present in crude oil at rather low levels, of the order of 0.5%, deoxygenation in petroleum refining is not of much concern, and no catalysts are specifically formulated for oxygenates hydrotreating. Hence, one of the critical technical challenges to make the hydrodeoxygenation of algae oil process economically feasible is related to the research and development of effective catalysts.\n\nTrials of using algae as biofuel were carried out by Lufthansa, and Virgin Airlines as early as 2008, although there is little evidence that using algae is a reasonable source for jet biofuels. By 2015, cultivation of fatty acid methyl esters and alkenones from the algae, \"Isochrysis\", was under research as a possible jet biofuel feedstock.\n\nAs of 2017, there was little progress in producing jet fuel from algae, with a forecast that only 3 to 5% of fuel needs could be provided from algae by 2050. Further, algae companies that formed in the early 21st century as a base for an algae biofuel industry have either closed or changed their business development toward other commodities, such as cosmetics, animal feed, or specialty oil products.\n\nResearch into algae for the mass-production of oil focuses mainly on microalgae (organisms capable of photosynthesis that are less than 0.4 mm in diameter, including the diatoms and cyanobacteria) as opposed to macroalgae, such as seaweed. The preference for microalgae has come about due largely to their less complex structure, fast growth rates, and high oil-content (for some species). However, some research is being done into using seaweeds for biofuels, probably due to the high availability of this resource.\n\n\nThe amount of oil each strain of algae produces varies widely. Note the following microalgae and their various oil yields:\n\n\nIn addition, due to its high growth-rate, \"Ulva\" has been investigated as a fuel for use in the \"SOFT cycle\", (SOFT stands for Solar Oxygen Fuel Turbine), a closed-cycle power-generation system suitable for use in arid, subtropical regions.\n\nOther species used include \"Clostridium saccharoperbutylacetonicum\", \"Sargassum\", \"Gracilaria\", \"Prymnesium parvum\", and \"Euglena gracilis\"\n\nLight is what algae primarily need for growth as it is the most limiting factor. Many companies are investing for developing systems and technologies for providing artificial light. One of them is OriginOil that has developed a Helix BioReactorTM that features a rotating vertical shaft with low-energy lights arranged in a helix pattern. Water temperature also influences the metabolic and reproductive rates of algae. Although most algae grow at low rate when the water temperature gets lower, the biomass of algal communities can get large due to the absence of grazing organisms. The modest increases in water current velocity may also affect rates of algae growth since the rate of nutrient uptake and boundary layer diffusion increases with current velocity.\n\nOther than light and water, phosphorus, nitrogen, and certain micronutrients are also useful and essential in growing algae. Nitrogen and phosphorus are the two most significant nutrients required for algal productivity, but other nutrients such as carbon and silica are additionally required. Of the nutrients required, phosphorus is one of the most essential ones as it is used in numerous metabolic processes. The microalgae \"D. tertiolecta\" was analyzed to see which nutrient affects its growth the most. The concentrations of phosphorus (P), iron (Fe), cobalt (Co), zinc (Zn), manganese (Mn) and molybdenum (Mo), magnesium (Mg), calcium (Ca), silicon (Si) and sulfur (S) concentrations were measured daily using inductively coupled plasma (ICP) analysis. Among all these elements being measured, phosphorus resulted in the most dramatic decrease, with a reduction of 84% over the course of the culture. This result indicates that phosphorus, in the form of phosphate, is required in high amounts by all organisms for metabolism.\n\nThere are two enrichment media that have been extensively used to grow most species of algae: Walne medium and the Guillard's F/ medium. These commercially available nutrient solutions may reduce time for preparing all the nutrients required to grow algae. However, due to their complexity in the process of generation and high cost, they are not used for large-scale culture operations. Therefore, enrichment media used for mass production of algae contain only the most important nutrients with agriculture-grade fertilizers rather than laboratory-grade fertilizers.\n\nAlgae grow much faster than food crops, and can produce hundreds of times more oil per unit area than conventional crops such as rapeseed, palms, soybeans, or jatropha. As algae have a harvesting cycle of 1–10 days, their cultivation permits several harvests in a very short time-frame, a strategy differing from that associated with annual crops. In addition, algae can be grown on land unsuitable for terrestrial crops, including arid land and land with excessively saline soil, minimizing competition with agriculture.<ref name=\"doi10.1007/s12155-008-9008-8\"></ref> Most research on algae cultivation has focused on growing algae in clean but expensive photobioreactors, or in open ponds, which are cheap to maintain but prone to contamination.\n\nThe lack of equipment and structures needed to begin growing algae in large quantities has inhibited widespread mass-production of algae for biofuel production. Maximum use of existing agriculture processes and hardware is the goal.\n\nClosed systems (not exposed to open air) avoid the problem of contamination by other\norganisms blown in by the air. The problem for a closed system is finding a cheap source of sterile .\nSeveral experimenters have found the from a smokestack works well for growing algae.\nFor reasons of economy, some experts think that algae farming for biofuels will have to be done as part of cogeneration, where it can make use of waste heat and help soak up pollution.\n\nMost companies pursuing algae as a source of biofuels pump nutrient-rich water through plastic or borosilicate glass tubes (called \"bioreactors\" ) that are exposed to sunlight (and so-called photobioreactors or PBR).\n\nRunning a PBR is more difficult than using an open pond, and costlier, but may provide a higher level of control and productivity. In addition, a photobioreactor can be integrated into a closed loop cogeneration system much more easily than ponds or other methods.\n\nOpen pond systems consist of simple in ground ponds, which are often mixed by a paddle wheel. These systems have low power requirements, operating costs, and capital costs when compared to closed loop photobioreactor systems.. Nearly all commercial algae producers for high value algal products utilize open pond systems.\n\nThe Algae scrubber is a system designed primarily for cleaning nutrients and pollutants out of water using algal turfs. ATS mimics the algal turfs of a natural coral reef by taking in nutrient rich water from waste streams or natural water sources, and pulsing it over a sloped surface. This surface is coated with a rough plastic membrane or a screen, which allows naturally occurring algal spores to settle and colonize the surface. Once the algae has been established, it can be harvested every 5–15 days, and can produce 18 metric tons of algal biomass per hectare per year. In contrast to other methods, which focus primarily on a single high yielding species of algae, this method focuses on naturally occurring polycultures of algae. As such, the lipid content of the algae in an ATS system is usually lower, which makes it more suitable for a fermented fuel product, such as ethanol, methane, or butanol. Conversely, the harvested algae could be treated with a hydrothermal liquefaction process, which would make possible biodiesel, gasoline, and jet fuel production.\n\nThere are three major advantages of ATS over other systems. The first advantage is documented higher productivity over open pond systems. The second is lower operating and fuel production costs. The third is the elimination of contamination issues due to the reliance on naturally occurring algae species. The projected costs for energy production in an ATS system are $0.75/kg, compared to a photobioreactor which would cost $3.50/kg. Furthermore, due to the fact that the primary purpose of ATS is removing nutrients and pollutants out of water, and these costs have been shown to be lower than other methods of nutrient removal, this may incentivize the use of this technology for nutrient removal as the primary function, with biofuel production as an added benefit.\n\nAfter harvesting the algae, the biomass is typically processed in a series of steps, which can differ based on the species and desired product; this is an active area of research and also is the bottleneck of this technology: the cost of extraction is higher than those obtained. One of the solutions is to use filter feeders to \"eat\" them. Improved animals can provide both foods and fuels. An alternative method to extract the algae is to grow the algae with specific types of fungi. This causes bio-flocculation of the algae which allows for easier extraction.\n\nOften, the algae is dehydrated, and then a solvent such as hexane is used to extract energy-rich compounds like triglycerides from the dried material. Then, the extracted compounds can be processed into fuel using standard industrial procedures. For example, the extracted triglycerides are reacted with methanol to create biodiesel via transesterification. The unique composition of fatty acids of each species influences the quality of the resulting biodiesel and thus must be taken into account when selecting algal species for feedstock.\n\nAn alternative approach called Hydrothermal liquefaction employs a continuous process that subjects harvested wet algae to high temperatures and pressures— and .\n\nProducts include crude oil, which can be further refined into aviation fuel, gasoline, or diesel fuel using one or many upgrading processes. The test process converted between 50 and 70 percent of the algae's carbon into fuel. Other outputs include clean water, fuel gas and nutrients such as nitrogen, phosphorus, and potassium.\n\nNutrients like nitrogen (N), phosphorus (P), and potassium (K), are important for plant growth and are essential parts of fertilizer. Silica and iron, as well as several trace elements, may also be considered important marine nutrients as the lack of one can limit the growth of, or productivity in, an area.\n\nBubbling through algal cultivation systems can greatly increase productivity and yield (up to a saturation point). Typically, about 1.8 tonnes of will be utilised per tonne of algal biomass (dry) produced, though this varies with algae species. The Glenturret Distillery in Perthshire, UK – home to The Famous Grouse Whisky – percolate made during the whisky distillation through a microalgae bioreactor. Each tonne of microalgae absorbs two tonnes of . Scottish Bioenergy, who run the project, sell the microalgae as high value, protein-rich food for fisheries. In the future, they will use the algae residues to produce renewable energy through anaerobic digestion.\n\nNitrogen is a valuable substrate that can be utilized in algal growth. Various sources of nitrogen can be used as a nutrient for algae, with varying capacities. Nitrate was found to be the preferred source of nitrogen, in regards to amount of biomass grown. Urea is a readily available source that shows comparable results, making it an economical substitute for nitrogen source in large scale culturing of algae. Despite the clear increase in growth in comparison to a nitrogen-less medium, it has been shown that alterations in nitrogen levels affect lipid content within the algal cells. In one study nitrogen deprivation for 72 hours caused the total fatty acid content (on a per cell basis) to increase by 2.4-fold. 65% of the total fatty acids were esterified to triacylglycerides in oil bodies, when compared to the initial culture, indicating that the algal cells utilized de novo synthesis of fatty acids. It is vital for the lipid content in algal cells to be of high enough quantity, while maintaining adequate cell division times, so parameters that can maximize both are under investigation.\n\nA possible nutrient source is waste water from the treatment of sewage, agricultural, or flood plain run-off, all currently major pollutants and health risks. However, this waste water cannot feed algae directly and must first be processed by bacteria, through anaerobic digestion. If waste water is not processed before it reaches the algae, it will contaminate the algae in the reactor, and at the very least, kill much of the desired algae strain. In biogas facilities, organic waste is often converted to a mixture of carbon dioxide, methane, and organic fertilizer. Organic fertilizer that comes out of the digester is liquid, and nearly suitable for algae growth, but it must first be cleaned and sterilized.\n\nThe utilization of wastewater and ocean water instead of freshwater is strongly advocated due to the continuing depletion of freshwater resources. However, heavy metals, trace metals, and other contaminants in wastewater can decrease the ability of cells to produce lipids biosynthetically and also impact various other workings in the machinery of cells. The same is true for ocean water, but the contaminants are found in different concentrations. Thus, agricultural-grade fertilizer is the preferred source of nutrients, but heavy metals are again a problem, especially for strains of algae that are susceptible to these metals. In open pond systems the use of strains of algae that can deal with high concentrations of heavy metals could prevent other organisms from infesting these systems. In some instances it has even been shown that strains of algae can remove over 90% of nickel and zinc from industrial wastewater in relatively short periods of time.\n\nIn comparison with terrestrial-based biofuel crops such as corn or soybeans, microalgal production results in a much less significant land footprint due to the higher oil productivity from the microalgae than all other oil crops. Algae can also be grown on marginal lands useless for ordinary crops and with low conservation value, and can use water from salt aquifers that is not useful for agriculture or drinking. Algae can also grow on the surface of the ocean in bags or floating screens. Thus microalgae could provide a source of clean energy with little impact on the provisioning of adequate food and water or the conservation of biodiversity.<ref name=\"doi10.1111/j.1523-1739.2007.00879.x\"></ref> Algae cultivation also requires no external subsidies of insecticides or herbicides, removing any risk of generating associated pesticide waste streams. In addition, algal biofuels are much less toxic, and degrade far more readily than petroleum-based fuels. However, due to the flammable nature of any combustible fuel, there is potential for some environmental hazards if ignited or spilled, as may occur in a train derailment or a pipeline leak. This hazard is reduced compared to fossil fuels, due to the ability for algal biofuels to be produced in a much more localized manner, and due to the lower toxicity overall, but the hazard is still there nonetheless. Therefore, algal biofuels should be treated in a similar manner to petroleum fuels in transportation and use, with sufficient safety measures in place at all times.\n\nStudies have determined that replacing fossil fuels with renewable energy sources, such as biofuels, have the capability of reducing emissions by up to 80%. An algae-based system could capture approximately 80% of the emitted from a power plant when sunlight is available. Although this will later be released into the atmosphere when the fuel is burned, this would have entered the atmosphere regardless. The possibility of reducing total emissions therefore lies in the prevention of the release of from fossil fuels. Furthermore, compared to fuels like diesel and petroleum, and even compared to other sources of biofuels, the production and combustion of algal biofuel does not produce any sulfur oxides or nitrous oxides, and produces a reduced amount of carbon monoxide, unburned hydrocarbons, and reduced emission of other harmful pollutants. Since terrestrial plant sources of biofuel production simply do not have the production capacity to meet current energy requirements, microalgae may be one of the only options to approach complete replacement of fossil fuels.\n\nMicroalgae production also includes the ability to use saline waste or waste streams as an energy source. This opens a new strategy to produce biofuel in conjunction with waste water treatment, while being able to produce clean water as a byproduct. When used in a microalgal bioreactor, harvested microalgae will capture significant quantities of organic compounds as well as heavy metal contaminants absorbed from wastewater streams that would otherwise be directly discharged into surface and ground-water. Moreover, this process also allows the recovery of phosphorus from waste, which is an essential but scarce element in nature – the reserves of which are estimated to have depleted in the last 50 years. Another possibility is the use of algae production systems to clean up non-point source pollution, in a system known as an algal turf scrubber (ATS). This has been demonstrated to reduce nitrogen and phosphorus levels in rivers and other large bodies of water affected by eutrophication, and systems are being built that will be capable of processing up to 110 million liters of water per day. ATS can also be used for treating point source pollution, such as the waste water mentioned above, or in treating livestock effluent.\n\nNearly all research in algal biofuels has focused on culturing single species, or monocultures, of microalgae. However, ecological theory and empirical studies have demonstrated that plant and algae polycultures, i.e. groups of multiple species, tend to produce larger yields than monocultures. Experiments have also shown that more diverse aquatic microbial communities tend to be more stable through time than less diverse communities. Recent studies found that polycultures of microalgae produced significantly higher lipid yields than monocultures. Polycultures also tend to be more resistant to pest and disease outbreaks, as well as invasion by other plants or algae. Thus culturing microalgae in polyculture may not only increase yields and stability of yields of biofuel, but also reduce the environmental impact of an algal biofuel industry.\n\nThere is clearly a demand for sustainable biofuel production, but whether a particular biofuel will be used ultimately depends not on sustainability but cost efficiency. Therefore, research is focusing on cutting the cost of algal biofuel production to the point where it can compete with conventional petroleum. The production of several products from algae has been mentioned as the most important factor for making algae production economically viable. Other factors are the improving of the solar energy to biomass conversion efficiency (currently 3%, but 5 to 7% is theoretically attainable)and making the oil extraction from the algae easier.\n\nIn a 2007 report a formula was derived estimating the cost of algal oil in order for it to be a viable substitute to petroleum diesel:\n\nwhere: C is the price of microalgal oil in dollars per gallon and C is the price of crude oil in dollars per barrel. This equation assumes that algal oil has roughly 80% of the caloric energy value of crude petroleum.\n\nWith current technology available, it is estimated that the cost of producing microalgal biomass is $2.95/kg for photobioreactors and $3.80/kg for open-ponds. These estimates assume that carbon dioxide is available at no cost. If the annual biomass production capacity is increased to 10,000 tonnes, the cost of production per kilogram reduces to roughly $0.47 and $0.60, respectively. Assuming that the biomass contains 30% oil by weight, the cost of biomass for providing a liter of oil would be approximately $1.40 ($5.30/gal) and $1.81 ($6.85/gal) for photobioreactors and raceways, respectively. Oil recovered from the lower cost biomass produced in photobioreactors is estimated to cost $2.80/L, assuming the recovery process contributes 50% to the cost of the final recovered oil. If existing algae projects can achieve biodiesel production price targets of less than $1 per gallon, the United States may realize its goal of replacing up to 20% of transport fuels by 2020 by using environmentally and economically sustainable fuels from algae production.\n\nWhereas technical problems, such as harvesting, are being addressed successfully by the industry, the high up-front investment of algae-to-biofuels facilities is seen by many as a major obstacle to the success of this technology. Only few studies on the economic viability are publicly available, and must often rely on the little data (often only engineering estimates) available in the public domain. Dmitrov examined the GreenFuel's photobioreactor and estimated that algae oil would only be competitive at an oil price of $800 per barrel. A study by Alabi et al. examined raceways, photobioreactors and anaerobic fermenters to make biofuels from algae and found that photobioreactors are too expensive to make biofuels. Raceways might be cost-effective in warm climates with very low labor costs, and fermenters may become cost-effective subsequent to significant process improvements. The group found that capital cost, labor cost and operational costs (fertilizer, electricity, etc.) by themselves are too high for algae biofuels to be cost-competitive with conventional fuels. Similar results were found by others, suggesting that unless new, cheaper ways of harnessing algae for biofuels production are found, their great technical potential may never become economically accessible. Recently, Rodrigo E. Teixeira demonstrated a new reaction and proposed a process for harvesting and extracting raw materials for biofuel and chemical production that requires a fraction of the energy of current methods, while extracting all cell constituents.\n\nMany of the byproducts produced in the processing of microalgae can be used in various applications, many of which have a longer history of production than algal biofuel. Some of the products not used in the production of biofuel include natural dyes and pigments, antioxidants, and other high-value bio-active compounds. These chemicals and excess biomass have found numerous use in other industries. For example, the dyes and oils have found a place in cosmetics, commonly as thickening and water-binding agents. Discoveries within the pharmaceutical industry include antibiotics and antifungals derived from microalgae, as well as natural health products, which have been growing in popularity over the past few decades. For instance \"Spirulina\" contains numerous polyunsaturated fats (Omega 3 and 6), amino acids, and vitamins, as well as pigments that may be beneficial, such as beta-carotene and chlorophyll.\n\nOne of the main advantages that using microalgae as the feedstock when compared to more traditional crops is that it can be grown much more easily. Algae can be grown in land that would not be considered suitable for the growth of the regularly used crops. In addition to this, wastewater that would normally hinder plant growth has been shown to be very effective in growing algae. Because of this, algae can be grown without taking up arable land that would otherwise be used for producing food crops, and the better resources can be reserved for normal crop production. Microalgae also require fewer resources to grow and little attention is needed, allowing the growth and cultivation of algae to be a very passive process.\n\nMany traditional feedstocks for biodiesel, such as corn and palm, are also used as feed for livestock on farms, as well as a valuable source of food for humans. Because of this, using them as biofuel reduces the amount of food available for both, resulting in an increased cost for both the food and the fuel produced. Using algae as a source of biodiesel can alleviate this problem in a number of ways. First, algae is not used as a primary food source for humans, meaning that it can be used solely for fuel and there would be little impact in the food industry. Second, many of the waste-product extracts produced during the processing of algae for biofuel can be used as a sufficient animal feed. This is an effective way to minimize waste and a much cheaper alternative to the more traditional corn- or grain-based feeds.\n\nGrowing algae as a source of biofuel has also been shown to have numerous environmental benefits, and has presented itself as a much more environmentally friendly alternative to current biofuels. For one, it is able to utilize run-off, water contaminated with fertilizers and other nutrients that are a by-product of farming, as its primary source of water and nutrients. Because of this, it prevents this contaminated water from mixing with the lakes and rivers that currently supply our drinking water. In addition to this, the ammonia, nitrates, and phosphates that would normally render the water unsafe actually serve as excellent nutrients for the algae, meaning that fewer resources are needed to grow the algae. Many algae species used in biodiesel production are excellent bio-fixers, meaning they are able to remove carbon dioxide from the atmosphere to use as a form of energy for themselves. Because of this, they have found use in industry as a way to treat flue gases and reduce GHG emissions.\n\nAlgae biodiesel is still a fairly new technology. Despite the fact that research began over 30 years ago, it was put on hold during the mid-1990s, mainly due to a lack of funding and a relatively low petroleum cost. For the next few years algae biofuels saw little attention; it was not until the gas peak of the early 2000s that it eventually had a revitalization in the search for alternative fuel sources. While the technology exists to harvest and convert algae into a usable source of biodiesel, it still hasn't been implemented into a large enough scale to support the current energy needs. Further research will be required to make the production of algae biofuels more efficient, and at this point it is currently being held back by lobbyists in support of alternative biofuels, like those produced from corn and grain. In 2013, Exxon Mobil Chairman and CEO Rex Tillerson said that after originally committing to spending up to $600 million on development in a joint venture with J. Craig Venter's Synthetic Genomics, algae is \"probably further\" than \"25 years away\" from commercial viability, although Solazyme and Sapphire Energy already began small-scale commercial sales in 2012 and 2013, respectively. By 2017, most efforts had been abandoned or changed to other applications, with only a few remaining.\n\nThe biodiesel produced from the processing of microalgae differs from other forms of biodiesel in the content of polyunsaturated fats. Polyunsaturated fats are known for their ability to retain fluidity at lower temperatures. While this may seem like an advantage in production during the colder temperatures of the winter, the polyunsaturated fats result in lower stability during regular seasonal temperatures.\n\nThe National Renewable Energy Laboratory (NREL) is the U.S. Department of Energy's primary national laboratory for renewable energy and energy efficiency research and development. This program is involved in the production of renewable energies and energy efficiency. One of its most current divisions is the biomass program which is involved in biomass characterization, biochemical and thermochemical conversion technologies in conjunction with biomass process engineering and analysis. The program aims at producing energy efficient, cost-effective and environmentally friendly technologies that support rural economies, reduce the nations dependency in oil and improve air quality.\n\nAt the Woods Hole Oceanographic Institution and the Harbor Branch Oceanographic Institution the wastewater from domestic and industrial sources contain rich organic compounds that are being used to accelerate the growth of algae. The Department of Biological and Agricultural Engineering at University of Georgia is exploring microalgal biomass production using industrial wastewater. Algaewheel, based in Indianapolis, Indiana, presented a proposal to build a facility in Cedar Lake, Indiana that uses algae to treat municipal wastewater, using the sludge byproduct to produce biofuel. A similar approach is being followed by Algae Systems, a company based in Daphne, Alabama.\n\nSapphire Energy (San Diego) has produced green crude from algae.\n\nSolazyme (South San Francisco, California) has produced a fuel suitable for powering jet aircraft from algae.\n\nThe Marine Research station in Ketch Harbour, Nova Scotia, has been involved in growing algae for 50 years. The National Research Council (Canada) (NRC) and National Byproducts Program have provided $5 million to fund this project. The aim of the program has been to build a 50 000 litre cultivation pilot plant at the Ketch harbor facility. The station has been involved in assessing how best to grow algae for biofuel and is involved in investigating the utilization of numerous algae species in regions of North America. NRC has joined forces with the United States Department of Energy, the National Renewable Energy Laboratory in Colorado and Sandia National Laboratories in New Mexico.\n\nUniversities in the United Kingdom which are working on producing oil from algae include: University of Manchester, University of Sheffield, University of Glasgow, University of Brighton, University of Cambridge, University College London, Imperial College London, Cranfield University and Newcastle University. In Spain, it is also relevant the research carried out by the CSIC´s Instituto de Bioquímica Vegetal y Fotosíntesis (Microalgae Biotechnology Group, Seville).\n\nThe European Algae Biomass Association (EABA) is the European association representing both research and industry in the field of algae technologies, currently with 79 members. The association is headquartered in Florence, Italy.\nThe general objective of the EABA is to promote mutual interchange and cooperation in the field of biomass production and use, including biofuels uses and all other utilisations. It aims at creating, developing and maintaining solidarity and links between its Members and at defending their interests at European and international level. Its main target is to act as a catalyst for fostering synergies among scientists, industrialists and decision makers to promote the development of research, technology and industrial capacities in the field of Algae.\n\nCMCL innovations and the University of Cambridge are carrying out a detailed design study of a C-FAST (Carbon negative Fuels derived from Algal and Solar Technologies) plant. The main objective is to design a pilot plant which can demonstrate production of hydrocarbon fuels (including diesel and gasoline) as sustainable carbon-negative energy carriers and raw materials for the chemical commodity industry. This project will report in June 2013.\n\nUkraine plans to produce biofuel using a special type of algae.\n\nThe European Commission's Algae Cluster Project, funded through the Seventh Framework Programme, is made up of three algae biofuel projects, each looking to design and build a different algae biofuel facility covering 10ha of land. The projects are BIOFAT, All-Gas and InteSusAl.\n\nSince various fuels and chemicals can be produced from algae, it has been suggested to investigate the feasibility of various production processes( conventional extraction/separation, hydrothermal liquefaction, gasification and pyrolysis) for application in an integrated algal biorefinery.\n\nReliance industries in collaboration with Algenol, USA commissioned a pilot project to produce algal bio-oil in the year 2014. Spirulina which is an alga rich in proteins content has been commercially cultivated in India. Algae is used in India for treating the sewage in open/natural oxidation ponds This reduces the Biological Oxygen Demand (BOD) of the sewage and also provides algal biomass which can be converted to fuel.\n\nThe Algae Biomass Organization (ABO) is a non-profit organization whose mission is \"to promote the development of viable commercial markets for renewable and sustainable commodities derived from algae\".\n\nThe National Algae Association (NAA) is a non-profit organization of algae researchers, algae production companies and the investment community who share the goal of commercializing algae oil as an alternative feedstock for the biofuels markets. The NAA gives its members a forum to efficiently evaluate various algae technologies for potential early stage company opportunities.\n\nPond Biofuels Inc. in Ontario, Canada has a functioning pilot plant where algae is grown directly off of smokestack emissions from a cement plant, and dried using waste heat. In May 2013, Pond Biofuels announced a partnership with the National Research Council of Canada and Canadian Natural Resources Limited to construct a demonstration-scale algal biorefinery at an oil sands site near Bonnyville, Alberta.\n\nOcean Nutrition Canada in Halifax, Nova Scotia, Canada has found a new strain of algae that appears capable of producing oil at a rate 60 times greater than other types of algae being used for the generation of biofuels.\n\nVG Energy, a subsidiary of Viral Genetics Incorporated, claims to have discovered a new method of increasing algal lipid production by disrupting the metabolic pathways that would otherwise divert photosynthetic energy towards carbohydrate production. Using these techniques, the company states that lipid production could be increased several-fold, potentially making algal biofuels cost-competitive with existing fossil fuels.\n\nAlgae production from the warm water discharge of a nuclear power plant has been piloted by Patrick C. Kangas at Peach Bottom Nuclear Power Station, owned by Exelon Corporation. This process takes advantage of the relatively high temperature water to sustain algae growth even during winter months.\n\nCompanies such as Sapphire Energy and Bio Solar Cells are using genetic engineering to make algae fuel production more efficient. According to Klein Lankhorst of Bio Solar Cells, genetic engineering could vastly improve algae fuel efficiency as algae can be modified to only build short carbon chains instead of long chains of carbohydrates. Sapphire Energy also uses chemically induced mutations to produce algae suitable for use as a crop.\n\nSome commercial interests into large-scale algal-cultivation systems are looking to tie into existing infrastructures, such as cement factories, coal power plants, or sewage treatment facilities. This approach changes wastes into resources to provide the raw materials, and nutrients, for the system.\n\nA feasibility study using marine microalgae in a photobioreactor is being done by The International Research Consortium on Continental Margins at the Jacobs University Bremen.\n\nThe Department of Environmental Science at Ateneo de Manila University in the Philippines, is working on producing biofuel from a local species of algae.\n\nGenetic engineering algae has been used to increase lipid production or growth rates. Current research in genetic engineering includes either the introduction or removal of enzymes. In 2007 Oswald et al. introduced a monoterpene synthase from sweet basil into Saccharomyces cerevisiae, a strain of yeast. This particular monoterpene synthase causes the de novo synthesis of large amounts of geraniol, while also secreting it into the medium. Geraniol is a primary component in rose oil, palmarosa oil, and citronella oil as well as essential oils, making it a viable source of triacylglycerides for biodiesel production.\n\nThe enzyme ADP-glucose pyrophosphorylase is vital in starch production, but has no connection to lipid synthesis. Removal of this enzyme resulted in the sta6 mutant, which showed increased lipid content. After 18 hours of growth in nitrogen deficient medium the sta6 mutants had on average 17 ng triacylglycerides/1000 cells, compared to 10 ng/1000 cells in WT cells. This increase in lipid production was attributed to reallocation of intracellular resources, as the algae diverted energy from starch production.\n\nIn 2013 researchers used a \"knock-down\" of fat-reducing enzymes (multifunctional lipase/phospholipase/acyltransferase) to increase lipids (oils) without compromising growth. The study also introduced an efficient screening process. Antisense-expressing knockdown strains 1A6 and 1B1 contained 2.4- and 3.3-fold higher lipid content during exponential growth, and 4.1- and 3.2-fold higher lipid content after 40 h of silicon starvation.\n\nNumerous Funding programs have been created with aims of promoting the use of Renewable Energy. In Canada, the ecoAgriculture biofuels capital initiative (ecoABC) provides $25 million per project to assist farmers in constructing and expanding a renewable fuel production facility. The program has $186 million set aside for these projects. The sustainable development (SDTC) program has also applied $500 millions over 8 years to assist with the construction of next-generation renewable fuels. In addition, over the last 2 years $10 million has been made available for renewable fuel research and analysis\n\nIn Europe, the Seventh Framework Programme (FP7) is the main instrument for funding research. Similarly, the NER 300 is an unofficial, independent portal dedicated to renewable energy and grid integration projects. Another program includes the Horizon 2020 program which will start 1 January, and will bring together the framework program and other EC innovation and research funding into a new integrated funding system\n\nThe American NBB's Feedstock Development program is addressing production of algae on the horizon to expand available material for biodiesel in a sustainable manner.\n\nNumerous policies have been put in place since the 1975 oil crisis in order to promote the use of Renewable Fuels in the United States, Canada and Europe. In Canada, these included the implementation of excise taxes exempting propane and natural gas which was extended to ethanol made from biomass and methanol in 1992. The federal government also announced their renewable fuels strategy in 2006 which proposed four components: increasing availability of renewable fuels through regulation, supporting the expansion of Canadian production of renewable fuels, assisting farmers to seize new opportunities in this sector and accelerating the commercialization of new technologies. These mandates were quickly followed by the Canadian provinces:\n\nBC introduced a 5% ethanol and 5% renewable diesel requirement which was effective by January 2010. It also introduced a low carbon fuel requirement for 2012 to 2020.\n\nAlberta introduced a 5% ethanol and 2% renewable diesel requirement implemented April 2011. The province also introduced a minimum 25% GHG emission reduction requirement for qualifying renewable fuels.\n\nSaskatchewan implemented a 2% renewable diesel requirement in 2009.\n\nAdditionally, in 2006, the Canadian Federal Government announced its commitment to using its purchasing power to encourage the biofuel industry. Section three of the 2006 alternative fuels act stated that when it is economically feasible to do so-75% per cent of all federal bodies and crown corporation will be motor vehicles.\n\nThe National Research Council of Canada has established research on Algal Carbon Conversion as one of its flagship programs. As part of this program, the NRC made an announcement in May 2013 that they are partnering with Canadian Natural Resources Limited and Pond Biofuels to construct a demonstration-scale algal biorefinery near Bonnyville, Alberta.\n\nPolicies in the United States have included a decrease in the subsidies provided by the federal and state governments to the oil industry which have usually included $2.84 billion. This is more than what is actually set aside for the biofuel industry. The measure was discussed at the G20 in Pittsburgh where leaders agreed that \"inefficient fossil fuel subsidies encourage wasteful consumption, reduce our energy security, impede investment in clean sources and undermine efforts to deal with the threat of climate change\". If this commitment is followed through and subsidies are removed, a fairer market in which algae biofuels can compete will be created. In 2010, the U.S. House of Representatives passed a legislation seeking to give algae-based biofuels parity with cellulose biofuels in federal tax credit programs. The algae-based renewable fuel promotion act (HR 4168) was implemented to give biofuel projects access to a $1.01 per gal production tax credit and 50% bonus depreciation for biofuel plant property. The U.S Government also introduced the domestic Fuel for Enhancing National Security Act implemented in 2011. This policy constitutes an amendment to the Federal property and administrative services act of 1949 and federal defense provisions in order to extend to 15 the number of years that the Department of Defense (DOD) multiyear contract may be entered into the case of the purchase of advanced biofuel. Federal and DOD programs are usually limited to a 5-year period\n\nThe European Union (EU) has also responded by quadrupling the credits for second-generation algae biofuels which was established as an amendment to the Biofuels and Fuel Quality Directives\n\nWith algal biofuel being a relatively new alternative to conventional petroleum products, it leaves numerous opportunities for drastic advances in all aspects of the technology. Producing algae biofuel is not yet a cost-effective replacement for gasoline, but alterations to current methodologies can change this. The two most common targets for advancements are the growth medium (open pond vs. photobioreactor) and methods to remove the intracellular components of the algae. Below are companies that are currently innovating algal biofuel technologies.\n\nFounded in 2006, Algenol Biofuels is a global, industrial biotechnology company that is commercializing its patented algae technology for production of ethanol and other fuels. Based in Southwest Florida, Algenol's patented technology enables the production of the four most important fuels (ethanol, gasoline, jet, and diesel fuel) using proprietary algae, sunlight, carbon dioxide and saltwater for around $1.27 per gallon and at production levels of 8 000 total gallons of liquid fuel per acre per year. Algenol's technology produces high yields and relies on patented photobioreactors and proprietary downstream techniques for low-cost fuel production using carbon dioxide from industrial sources. The company originally intended on producing commercially by 2014, but was set back when Florida Governor Rick Scott signed a bill in 2013 eliminating the state's mandate of a minimum of 10% ethanol in commercial gasoline. This caused Algenol CEO Paul Woods to scrap a plan for a US $500 million plant to produce commercial amounts of algae biofuels and pursue other job sites. Currently, Algenol is a partner of the US Department of Energy's Bioenergy Technologies Office, and in 2015 began smaller-scale commercial sales of E15 and E85 ethanol blends to Protec Fuel, a Florida-based fuel distributor.\n\nBlue Marble Production is a Seattle-based company that is dedicated to removing algae from algae-infested water. This in turn cleans up the environment and allows this company to produce biofuel. Rather than just focusing on the mass production of algae, this company focuses on what to do with the byproducts. This company recycles almost 100% of its water via reverse osmosis, saving about 26 000 gallons of water every month. This water is then pumped back into their system. The gas produced as a byproduct of algae will also be recycled by being placed into a photobioreactor system that holds multiple strains of algae. Whatever gas remains is then made into pyrolysis oil by thermochemical processes. Not only does this company seek to produce biofuel, but it also wishes to use algae for a variety of other purposes such as fertilizer, food flavoring, anti-inflammatory, and anti-cancer drugs.\n\nSolazyme is one of a handful of companies which is supported by oil companies such as Chevron. Additionally, this company is also backed by Imperium Renewables, Blue Crest Capital Finance, and The Roda Group. Solazyme has developed a way to use up to 80% percent of dry algae as oil. This process requires the algae to grow in a dark fermentation vessel and be fed by carbon substrates within their growth media. The effect is the production of triglycerides that are almost identical to vegetable oil. Solazyme's production method is said to produce more oil than those algae cultivated photosynthetically or made to produce ethanol. Oil refineries can then take this algal oil and turn it into biodiesel, renewable diesel or jet fuels.\n\nPart of Solazyme's testing, in collaboration with Maersk Line and the US Navy, placed 30 tons of Soladiesel(RD) algae fuel into the 98,000-tonne, 300-metre container ship Maersk Kalmar. This fuel was used at blends from 7% to 100% in an auxiliary engine on a month-long trip from Bremerhaven, Germany to Pipavav, India in Dec 2011. In Jul 2012, The US Navy used 700 000 gallons of HRD76 biodiesel in three ships of the USS Nimitz \"Green Strike Group\" during the 2012 RIMPAC exercise in Hawaii. The Nimitz also used 200 000 gallons of HRJ5 jet biofuel. The 50/50 biofuel blends were provided by Solazyme and Dynamic Fuels.\n\nSapphire Energy is a leader in the algal biofuel industry backed by the Wellcome Trust, Bill Gates' Cascade Investment, Monsanto, and other large donors. After experimenting with production of various algae fuels beginning in 2007, the company now focuses on producing what it calls \"green crude\" from algae in open raceway ponds. After receiving more than $100 million in federal funds in 2012, Sapphire built the first commercial demonstration algae fuel facility in New Mexico and has continuously produced biofuel since completion of the facility in that year. In 2013, Sapphire began commercial sales of algal biofuel to Tesoro, making it one of the first companies, along with Solazyme, to sell algae fuel on the market.\n\nDiversified Technologies Inc. has created a patent pending pre-treatment option to reduce costs of oil extraction from algae. This technology, called Pulsed Electric Field (PEF) technology, is a low cost, low energy process that applies high voltage electric pulses to a slurry of algae. The electric pulses enable the algal cell walls to be ruptured easily, increasing the availability of all cell contents (Lipids, proteins and carbohydrates), allowing the separation into specific components downstream. This alternative method to intracellular extraction has shown the capability to be both integrated in-line as well as scalable into high yield assemblies. The Pulse Electric Field subjects the algae to short, intense bursts of electromagnetic radiation in a treatment chamber, electroporating the cell walls. The formation of holes in the cell wall allows the contents within to flow into the surrounding solution for further separation. PEF technology only requires 1-10 microsecond pulses, enabling a high-throughput approach to algal extraction.\n\nPreliminary calculations have shown that utilization of PEF technology would only account for $0.10 per gallon of algae derived biofuel produced. In comparison, conventional drying and solvent-based extractions account for $1.75 per gallon. This inconsistency between costs can be attributed to the fact that algal drying generally accounts for 75% of the extraction process. Although a relatively new technology, PEF has been successfully used in both food decomtamination processes as well as waste water treatments.\n\nOrigin Oils Inc. has been researching a revolutionary method called the Helix Bioreactor, altering the common closed-loop growth system. This system utilizes low energy lights in a helical pattern, enabling each algal cell to obtain the required amount of light. Sunlight can only penetrate a few inches through algal cells, making light a limiting reagent in open-pond algae farms. Each lighting element in the bioreactor is specially altered to emit specific wavelengths of light, as a full spectrum of light is not beneficial to algae growth. In fact, ultraviolet irradiation is actually detrimental as it inhibits photosynthesis, photoreduction, and the 520 nm light-dark absorbance change of algae.\n\nThis bioreactor also addresses another key issue in algal cell growth; introducing CO and nutrients to the algae without disrupting or over-aerating the algae. Origin Oils Inc. combats this issues through the creation of their Quantum Fracturing technology. This process takes the CO and other nutrients, fractures them at extremely high pressures and then deliver the micron sized bubbles to the algae. This allows the nutrients to be delivered at a much lower pressure, maintaining the integrity of the cells.\n\nProviron is a Belgian microalgae company that also operates in the United States. The company has been working on a new type of reactor (using flat plates) which reduces the cost of algae cultivation. At AlgaePARC similar research is being conducted using 4 grow systems (1 open pond system and 3 types of closed systems). According to René Wijffels the current systems do not yet allow algae fuel to be produced competitively. However using new (closed) systems, and by scaling up the production it would be possible to reduce costs by 10X, up to a price of 0,4 € per kg of algae. Currently, Proviron focuses primarily on alternative uses of algae cultures, such as environmentally-conscious plastics, esterification processes, and de-icing processes.\n\nGenifuel Corporation has licensed the high temperature/pressure fuel extraction process and has been working with the team at the lab since 2008. The company intends to team with some industrial partners to create a pilot plant using this process to make biofuel in industrial quantities. Genifuel process combines hydrothermal liquefaction with catalytic hydrothermal gasification in reactor running at 350 Degrees Celsius (662 Degrees Fahrenheit) and pressure of 20 684.2719 kPa (3 000 PSI).\n\nQMAB is an Iran-based biofuels company operating solely on the Iranian island of Qeshm in the Strait of Hormuz. QMAB's original pilot plant has been operating since 2009, and has a 25,000 Litre capacity. In 2014, QMAB released BAYA Biofuel, a biofuel deriving from the algae Nannochloropsis, and has since specified that its unique strain is up to 68% lipids by dry weight volume.\nDevelopment of the farm mainly focuses on 2 phases, production of nutraceutical products and green crude oil to produce biofuel. The main product of their microalgae culture is crude oil, which can be fractioned into the same kinds of fuels and chemical compounds.\n\n\n"}
{"id": "24989675", "url": "https://en.wikipedia.org/wiki?curid=24989675", "title": "Alsos Digital Library for Nuclear Issues", "text": "Alsos Digital Library for Nuclear Issues\n\nThe Alsos Digital Library for Nuclear Issues is a searchable collection of vetted annotations and bibliographic information for resources including books, articles, films, CD-ROMs, and websites pertaining to nuclear topics. Part of the United States' National Science Digital Library (NSDL) and located at Washington and Lee University, the digital library provides a balanced selection of high quality resources across many disciplines to a broad audience including students, scholars, professionals, and the general public. Originally focused on the history of the Manhattan Project, the library has expanded to include the far-reaching consequences of the discovery of atomic energy, including topics such as nuclear power, and nuclear proliferation.\n\nPublicly available since 2001, the Alsos library has been headed since its inception by chemist Dr. Frank Settle. The name Alsos honors the Allied Alsos Mission during World War II to discover the extent to which Germany had progressed with its atomic bomb project. Funding has been provided by the National Science Foundation and H. F. Lenfest. Undergraduate students have built and provided the content for the library, supervised by a computer science professor and an editor.\n\nThe Alsos Digital Library for Nuclear Issues contains over 3,500 annotations, which have been reviewed by members of its National Advisory Board of experts in many disciplines. Its entries are both searchable as full-text and hierarchically browsable in six major categories: issues, science, warfare, people, places, and disciplines. Bibliographic resources that are available on the Internet have links provided; library locations of non-electronic resources nearest to the user are also available via WorldCat.\n\nIn order to integrate its extensive bibliographic information with web-based content on nuclear issues, the Alsos team has worked in partnership with other high quality websites to create a gateway to nuclear resources online, Nuclear Pathways. That website provides federated searches of the Alsos Digital Library for Nuclear Issues, Atomic Archive, Nuclear Files, and the nuclear chemistry component of ChemCases; resources include bibliographies, biographies, time lines, policy analyses, explanations of the history and science of nuclear weapons, nuclear chemistry lessons, study guides, syllabi, and extensive collections of historical primary source documents, photographs, audio, and film clips.\n\n"}
{"id": "428508", "url": "https://en.wikipedia.org/wiki?curid=428508", "title": "American Chemical Society", "text": "American Chemical Society\n\nThe American Chemical Society (ACS) is a scientific society based in the United States that supports scientific inquiry in the field of chemistry. Founded in 1876 at New York University, the ACS currently has more than 158,000 members at all degree levels and in all fields of chemistry, chemical engineering, and related fields. It is the world's largest scientific society by membership. The ACS is a 501(c)(3) non-profit organization and holds a congressional charter under Title 36 of the United States Code. Its headquarters are located in Washington, D.C., and it has a large concentration of staff in Columbus, Ohio.\n\nThe ACS is a leading source of scientific information through its peer-reviewed scientific journals, national conferences, and the Chemical Abstracts Service. Its publications division produces 51 scholarly journals including the prestigious \"Journal of the American Chemical Society\", as well as the weekly trade magazine \"Chemical & Engineering News\". The ACS holds national meetings twice a year covering the complete field of chemistry and also holds smaller conferences concentrating on specific chemical fields or geographic regions. The primary source of income of the ACS is the Chemical Abstracts Service, a provider of chemical databases worldwide.\n\nThe organization also publishes textbooks, administers several national chemistry awards, provides grants for scientific research, and supports various educational and outreach activities.\n\nIn 1874, a group of American chemists gathered at the Joseph Priestley House to mark the 100th anniversary of Priestley's discovery of oxygen. Although there was an American scientific society at that time (the American Association for the Advancement of Science, founded in 1848), the growth of chemistry in the U.S. prompted those assembled to consider founding a new society that would focus more directly on theoretical and applied chemistry. Two years later, on April 6, 1876, during a meeting of chemists at the University of the City of New York (now New York University) the American Chemical Society was founded. The society received its charter of incorporation from the State of New York in 1877.\n\nCharles F. Chandler, a professor of chemistry at Columbia University who was instrumental in organizing the society said that such a body would “prove a powerful and healthy stimulus to original research, … would awaken and develop much talent now wasting in isolation, … [bring] members of the association into closer union, and ensure a better appreciation of our science and its students on the part of the general public.”\n\nAlthough Chandler was a likely choice to become the society's first president because of his role in organizing the society, New York University chemistry professor John William Draper was elected as the first president of the society because of his national reputation. Draper was a photochemist and pioneering photographer who had produced one of the first photographic portraits in 1840. Chandler would later serve as president in 1881 and 1889.\n\nIn the ACS logo, originally designed in the early 20th century by Tiffany's Jewelers and used since 1909, a stylized symbol of a kaliapparat is used.\n\nThe \"Journal of the American Chemical Society\" was founded in 1879 to publish original chemical research. It was the first journal published by ACS and is still the society's flagship peer-reviewed publication. In 1907, Chemical Abstracts was established as a separate journal (it previously appeared within JACS), which later became the Chemical Abstracts Service, a division of ACS that provides chemical information to researchers and others worldwide. \"Chemical & Engineering News\" is a weekly trade magazine that has been published by ACS since 1923.\n\nThe society adopted a new constitution aimed at nationalizing the organization in 1890. In 1905, the American Chemical Society moved from New York City to Washington, D.C. ACS was reincorporated under a congressional charter in 1937. It was granted by the U.S. Congress and signed by president Franklin D. Roosevelt. ACS's headquarters moved to its current location in downtown Washington in 1941.\n\nNotable Presidents of the American Chemical Society\n\nACS first established technical divisions in 1908 to foster the exchange of information among scientists who work in particular fields of chemistry or professional interests. Divisional activities include organizing technical sessions at ACS meetings, publishing books and resources, administering awards and lectureships, and conducting other events. The original five divisions were 1) organic chemistry, 2) industrial chemists and chemical engineers, 3) agricultural and food chemistry, 4) fertilizer chemistry, and 5) physical and inorganic chemistry.\n\nAs of 2016, there are 32 technical divisions of ACS.\nThis is the largest division of the Society. It marked its 100th Anniversary in 2008. The first Chair of the Division was Edward Curtis Franklin. The Organic Division played a part in establishing Organic Syntheses, Inc. and Organic Reactions, Inc. and it maintains close ties to both organizations.\n\nThe Division's best known activities include organizing symposia (talks and poster sessions) at the biannual ACS National Meetings, for the purpose of recognizing promising Assistant Professors, talented young researchers, outstanding technical contributions from junior-level chemists, in the field of organic chemistry. The symposia also honor national award winners, including the Arthur C. Cope Award, Cope Scholar Award, James Flack Norris Award in Physical Organic Chemistry, Herbert C. Brown Award for Creative Research in Synthetic Methods.\n\nThe Division helps to organize symposia at the international meeting called Pacifichem and it organizes the biennial National Organic Chemistry Symposium (NOS) which highlights recent advances in organic chemistry and hosts the Roger Adams Award address. The Division also organizes corporate sponsorships to provide fellowships for Ph.D. students and undergraduates. It also organizes the Graduate Research Symposium and manages award and travel grant programs for undergraduates.\n\nLocal sections were authorized in 1890 and are autonomous units of the American Chemical Society. They elect their own officers and select representatives to the national ACS organization. Local sections also provide professional development opportunities for members, organize community outreach events, offer awards, and conduct other business. The Rhode Island Section was the first local section of ACS, organized in 1891. There are currently 186 local sections of the American Chemical Society in all 50 states, the District of Columbia, and Puerto Rico.\n\nInternational Chemical Sciences Chapters allow ACS members outside of the U.S. to organize locally for professional and scientific exchange. There are currently 16 International Chemical Sciences Chapters.\n\nACS states that it offers teacher training to support the professional development of science teachers so they can better present chemistry in the classroom, foster the scientific curiosity of our nation’s youth and encourage future generations to pursue scientific careers. As of 2009, Clifford and Kathryn Hach donated $33 million to ACS, to continue the work of the Hach Scientific Foundation in supporting high school chemistry teaching.\n\nThe American Chemical Society sponsors the United States National Chemistry Olympiad (USNCO), a contest used to select the four-member team that represents the United States at the International Chemistry Olympiad (IChO).\n\nThe ACS Division of Chemical Education provides standardized tests for various subfields of chemistry. The two most commonly used tests are the undergraduate-level tests for general and organic chemistry. Each of these tests consists of 70 multiple-choice questions, and gives students 110 minutes to complete the exam.\n\nThe ACS also approves certified undergraduate programs in chemistry. A student who completes the required laboratory and course work—sometimes in excess of what a particular college may require for its Bachelor's degree—is considered by the Society to be well trained for professional work.\n\nThe ACS also coordinates National Chemistry Week as part of its educational outreach. Since 1977, each year has celebrated a theme, such as \"Chemistry colors our world\" (2015) and \"Energy: Now and forever!\" (2013).\n\nThe Green Chemistry Institute (GCI) supports the \"implementation of green chemistry and engineering throughout the global chemistry enterprise.\" The GCI organizes an annual conference, the Green Chemistry and Engineering Conference, provides research grants, administers awards, and provides information and support for green chemistry practices to educators, researchers, and industry.\n\nThe GCI was founded in 1997 as an independent non-profit organization, by chemists Joe Breen and Dennis Hjeresen in cooperation with the Environmental Protection Agency. In 2001, the GCI became a part of the American Chemical Society.\n\nThe Petroleum Research Fund (PRF) is an endowment fund administered by the ACS that supports advanced education and fundamental research in the petroleum and fossil fuel fields at non-profit institutions. Several categories of grants are offered for various career levels and institutions. The fund awarded more than $25 million in grants in 2007.\n\nThe PRF traces its origins to the acquisition of the Universal Oil Products laboratory by a consortium of oil companies in 1931. The companies established a trust fund, The Petroleum Research Fund, in 1944 in order to prevent antitrust litigation tied to their UOP assets. The ACS was named the beneficiary of the trust. The first grants from the PRF were awarded in 1954. In 2000, the trust was transferred to the ACS. The ACS established The American Chemical Society Petroleum Research Fund and the previous trust was dissolved. The PRF trust was valued at $144.7 million in December 2014.\n\nThe ACS International Activities is the birthplace of the ACS International Center, an online resource for scientists and engineers looking to study abroad or explore an international career or internship. The site houses information on hundreds of scholarships and grants related to all levels of experience to promote scientific mobility of researchers and practitioners in STEM fields.\n\nThe American Chemical Society grants membership to undergraduates as student members provided they can pay the $25 yearly dues. Any university may start its own ACS Student Chapter and receive benefits of undergraduate participation in regional conferences and discounts on ACS publications.\n\nThe American Chemical Society administers 64 national awards, medals and prizes based on scientific contributions at various career levels that promote achievement across the chemical sciences. The ACS national awards program began in 1922 with the establishment of the Priestley Medal, the highest award offered by the ACS, which is given for distinguished services to chemistry. The 2017 winner of the Priestley Medal is Tobin J. Marks of Northwestern University.\n\nAdditional awards are offered by divisions, local sections and other bodies of ACS. The William H. Nichols Medal Award was the first ACS award to honor outstanding researchers in the field of chemistry. It was established in 1903 by the ACS New York Section and is named for William H. Nichols, an American chemist and businessman and one of the original founders of ACS. Of the over 100 Nichols Medalists, 16 have subsequently been awarded the Nobel Prize in Chemistry. The Willard Gibbs Award, granted by the ACS Chicago Section, was established in 1910 in honor of Josiah Willard Gibbs, the Yale University professor who formulated the phase rule.\n\nThe Georgia Local Section of ACS has awarded the Herty Medal since 1933 recognizing outstanding chemists who have significantly contributed to their chosen fields. All chemists in academic, government, or industrial laboratories who have been residing in the southeastern United States for at least 10 years are eligible. (For this purpose Southeastern United States is defined as the union of the following states: Virginia, West Virginia, Kentucky, Tennessee, Mississippi, Louisiana, Alabama, Georgia, Florida, North Carolina, and South Carolina.)\n\nThe New York Section of ACS also gives Leadership Awards. The Leadership Awards are the highest honors given by the Chemical Marketing and Economic Group of ACS NY since December 6, 2012. They are presented to leaders of industry, investments, and other sectors, for their contributions to science, technology, engineering and mathematics (STEM) initiatives. Honorees include Andrew N. Liveris (Dow Chemical), P. Roy Vagelos (Regeneron, Merck), Thomas M. Connelly (DuPont) and Juan Pablo del Valle (Mexichem).\n\nThe ACS also administers regional awards presented annually at regional meetings. This includes the E. Ann Nalley Regional Award for Volunteer Service to the American Chemical Society, Regional Awards for Excellence in High School Teaching, and the Stanley C. Israel Regional Award for Advancing Diversity in the Chemical Sciences.\n\n ACS Publications is the publishing division of the ACS. It is a nonprofit academic publisher of scientific journals covering various fields of chemistry and related sciences. As of 2017, ACS Publications published the following 54 peer-reviewed journals:\n\n\nIn addition to academic journals, ACS Publications also publishes \"Chemical & Engineering News\", a weekly trade magazine covering news in the chemical profession, \"inChemistry\", a magazine for undergraduate students, and \"ChemMatters\", a magazine for high school students and teachers.\n\nIn debates about free access to scientific information, the ACS has been described as \"in an interesting dilemma, with some of its representatives pushing for open access and others hating the very thought.\" The ACS has generally opposed legislation that would mandate free access to scientific journal articles and chemical information, however it has recently launched new open access journals and provided authors with open access publishing options.\n\nThe mid-2000s saw a debate between some research funders (including the federal government), which argued that research they funded should be presented freely to the public, and some publishers (including the ACS), which argued that the costs of peer-review and publishing justified their subscription prices. In 2006, Congress debated legislation that would have instructed the National Institutes of Health (NIH) to require all investigators it funded to submit copies of final, peer-reviewed journal articles to PubMed Central, a free-access digital repository it operates, within 12 months of publication. At the time the American Association of Publishers (of which ACS is a member) hired a public relations firm to counter the open access movement. In spite of publishers' opposition, the PubMed Central legislation was passed in December 2007 and became effective in 2008.\n\nAs the open access issue has continued to evolve, so too has the ACS's position. In response to a 2013 White House Office of Science and Technology Policy directive that instructed federal agencies to provide greater access to federally funded research, the ACS joined other scholarly publishers in establishing the Clearinghouse for the Open Research of the United States (Chorus) to allow free access to published articles. The ACS has also introduced several open access publishing options for its journals, including providing authors the option to pay an upfront fee to enable free online access to their articles. In 2015 the ACS launched the first fully open access journal in the society's history, \"ACS Central Science\". The ACS states that the journal offers the same peer-review standards as its subscription journals, but without publishing charges to either authors or readers. A second open access title, \"ACS Omega\", an interdisciplinary mega journal, launched in 2016.\n\nIn 2005, the ACS was criticized for opposing the creation of PubChem, which is an open access chemical database developed by the NIH's National Center for Biotechnology Information. The ACS raised concerns that the publicly supported PubChem database would duplicate and unfairly compete with their existing fee-based Chemical Abstracts Service and argued that the database should only present data created by the Molecular Libraries Screening Center initiative of the NIH.\n\nThe ACS lobbied members of the United States Congress to rein in PubChem and hired outside lobbying firms to try to persuade congressional members, the NIH, and the Office of Management and Budget (OMB) against establishing a publicly funded database. The ACS was unsuccessful, and as of 2012 PubChem is the world's largest free chemical database.\n\nAs a major provider of chemistry related information, ACS has been involved with several legal cases over the past few decades that involve access to its databases, trademark rights, and intellectual property. These include \"Dialog\" \"v. American Chemical Society\", a suit claiming antitrust violations in access to ACS databases, settled out of court in 1993; \"American Chemical Society v. Google\", a suit claiming trademark violation, settled out of court in 2006; and \"American Chemical Society v. Leadscope\", a suit alleging stolen trade secrets, concluded in 2012 with ACS losing its trade secrets claim and Leadscope losing its counterclaim of defamation.\n\nIn 2004, a group of ACS members criticized the compensation of former executive director and chief executive officer John Crum, whose total salary, expenses, and bonuses for 2002 was reported to be $767,834. The ACS defended the figure, saying that it was in line with that of comparable organizations, including for-profit publishers.\n\n, two employees were reported to have a total compensation exceeding $900,000, while 694 had a compensation exceeding $100,000.\n\n\n\n\n"}
{"id": "3294795", "url": "https://en.wikipedia.org/wiki?curid=3294795", "title": "Angle notation", "text": "Angle notation\n\nAngle notation or phasor notation is a notation used in electronics.  formula_1 can represent either the vector  formula_2  or the complex number  formula_3, with formula_4, both of which have magnitudes of 1.  A vector whose polar coordinates are magnitude  formula_5  and angle  formula_6  is written  formula_7    To convert between polar and rectangular forms, see Converting between polar and Cartesian coordinates.\n\nIn electronics and electrical engineering, there may also be an implied conversion from degrees to radians. For example  formula_8  would be assumed to be  formula_9  which is the vector  formula_10  or the number  formula_11\n\n"}
{"id": "9344216", "url": "https://en.wikipedia.org/wiki?curid=9344216", "title": "Autocloning", "text": "Autocloning\n\nAutocloning is a proprietary method of fabricating 2D and 3D photonic crystals for free space applications.\n\n"}
{"id": "3711378", "url": "https://en.wikipedia.org/wiki?curid=3711378", "title": "BROACH warhead", "text": "BROACH warhead\n\nThe BROACH warhead is a multi-stage warhead developed by Team BROACH; BAE Systems Global Combat Systems Munitions, Thales Missile Electronics and QinetiQ. BROACH stands for \"Bomb Royal Ordnance Augmented CHarge\".\n\nDevelopment of BROACH began in 1991 when Team BROACH consisted of British Aerospace RO Defence, Thomson-Thorn Missile Electronics and DERA. The two stage warhead is made up from an initial shaped charge, which cuts a passage through armour, concrete, earth, etc., allowing a larger following warhead to penetrate inside the target. The weapon is designed to allow a cruise missile to achieve the degree of hard-target penetration formerly only possible by the use of laser-guided gravity bombs.\n\n"}
{"id": "44892368", "url": "https://en.wikipedia.org/wiki?curid=44892368", "title": "Belgrade printing house", "text": "Belgrade printing house\n\nThe Belgrade printing house was a printing house established by count () Radiša Dmitrović in Belgrade, Ottoman Serbia (today the capital of Serbia). It was the first printing house in Belgrade. After Dmitrović's death, the printing house was taken over by Trojan Gundulić, who organized publishing of the first and only book of this printing house, the Gospel, printed and edited in 1552 by Hieromonk Mardarije.\n\nCount Radiša Dmitrović, a Serb nobleman and native of Herzegovina, bought the printing press and types and employed Hieromonk Mardarije as editor and printer. Some earlier sources speculated that the Belgrade in question was actually Berat in Albania or some other Belgrade on the Balkans. Dmitrović died before the first book was printed in his printing house. According to some sources, he died before the printing press he bought was even delivered to him, while some other sources say he died during the printing of the first book.\n\nAfter Dmitrovićs death, the printing press was received by Trojan Gundulić, a member of the noble Gondola family, who lived in the large colony of Ragusans in Belgrade as an illegitimate son of a Ragusan nobleman. Gundulić moved the press to his house, which was located in the part of the Belgrade called Zerek, near the Bayrakli mosque, which is today Gospodar Jevremova Street. Gundulić personally sold books in Belgrade. Gundulić did not find the printing business profitable, however, and did not continue with it.\n\nHieromonk Mardarije moved the printing press from Belgrade to the Mrkšina crkva monastery and established the Mrkšina crkva printing house there. According to some sources, it was Mardarije who inspired first Dmitrović and then Gundulić to invest in the printing business and who organized all activities during the set-up of the printing house in Belgrade.\n\n"}
{"id": "23469224", "url": "https://en.wikipedia.org/wiki?curid=23469224", "title": "Berkeley IRAM project", "text": "Berkeley IRAM project\n\nA 1996–2004 research project in the Computer Science Division of the University of California, Berkeley, the Berkeley IRAM project explored computer architecture enabled by the wide bandwidth between memory and processor made possible when both are designed on the same integrated circuit (chip). Since it was envisioned that such a chip would consist primarily of random-access memory (RAM), with a smaller part needed for the central processing unit (CPU), the research team used the term \"Intelligent RAM\" (or IRAM) to describe a chip with this architecture. Like the J–Machine project at MIT, the primary objective of the research was to avoid the Von Neumann bottleneck which occurs when the connection between memory and CPU is a relatively narrow memory bus between separate integrated circuits.\n\nWith strong competitive pressures, the technology employed for each component of a computer system—principally CPU, memory, and offline storage—is typically selected to minimize the cost needed to attain a given level of performance. Though both microprocessor and memory are implemented as integrated circuits, the prevailing technology used for each differs; microprocessor technology optimizes speed and memory technology optimizes density. For this reason, the integration of memory and processor in the same chip has (for the most part) been limited to static random-access memory (SRAM), which may be implemented using circuit technology optimized for logic performance, rather than the denser and lower-cost dynamic random-access memory (DRAM), which is not. Microprocessor access to off-chip memory costs time and power, however, significantly limiting processor performance. For this reason computer architecture employing a hierarchy of memory systems has developed, in which static memory is integrated with the microprocessor for temporary, easily accessible storage (or cache) of data which is also retained off-chip in DRAM. Since the on-chip cache memory is redundant, its presence adds to cost and power. The purpose of the IRAM research project was to find if (in some computing applications) a better trade-off between cost and performance could be achieved with an architecture in which DRAM was integrated on-chip with the processor, thus eliminating the need for a redundant static memory cache—even though the technology used was not optimum for DRAM implementation.\n\nWhile it is fair to say that Berkeley IRAM did not achieve the recognition that Berkeley RISC received, the IRAM project was nevertheless influential. \nAlthough initial IRAM proposals focused on trade-offs between CPU and DRAM, IRAM research came to concentrate on vector instruction sets.\nIts publications were early advocates of the incorporation of vector processing and vector instruction sets into microprocessors, and several commercial microprocessors, such as the Intel AVX, subsequently adopted vector processing instruction set extensions.\n\n\n"}
{"id": "6092100", "url": "https://en.wikipedia.org/wiki?curid=6092100", "title": "Boeing X-53 Active Aeroelastic Wing", "text": "Boeing X-53 Active Aeroelastic Wing\n\nThe X-53 Active Aeroelastic Wing (AAW) development program is a completed American research project that was undertaken jointly by the Air Force Research Laboratory (AFRL), Boeing Phantom Works and NASA's Dryden Flight Research Center, where the technology was flight tested on a modified McDonnell Douglas F/A-18 Hornet. Active Aeroelastic Wing Technology is a technology that integrates wing aerodynamics, controls, and structure to harness and control wing aeroelastic twist at high speeds and dynamic pressures. By using multiple leading and trailing edge controls like \"aerodynamic tabs\", subtle amounts of aeroelastic twist can be controlled to provide large amounts of wing control power, while minimizing maneuver air loads at high wing strain conditions or aerodynamic drag at low wing strain conditions. The flight program which first proved the use of AAW technology in full scale was the X-53 Active Aeroelastic Wing program.\n\nGerry Miller and Jan Tulinius led the development of the initial concept during wind tunnel testing in the mid 1980s under Air Force contract. The designation \"X-52\" was skipped in sequence to avoid confusion with Boeing's B-52 Stratofortress bomber. Ed Pendleton served as the Air Force's program manager.\nActive Aeroelastic Wing (AAW) Technology is multidisciplinary in that it integrates air vehicle aerodynamics, active controls, and structural aeroelastic behavior to maximize air vehicle performance. The concept uses wing aeroelastic flexibility for a net benefit and enables the use of high aspect ratio, thin, swept wings that are aeroelastically deformed into shapes for optimum performance. This makes it possible to achieve the multi-point aerodynamic performance required of future fighter, bomber, and transport aircraft.\n\nAAW Technology employs wing aeroelastic flexibility for a net benefit through use of multiple leading and trailing edge control surfaces activated by a digital flight control system. At higher dynamic pressures, AAW control surfaces are used as \"tabs\" which are deflected into the air stream to produce favorable wing twist instead of the reduced control generally associated with “aileron reversal” caused by trailing edge surfaces. The energy of the air stream is employed to twist the wing with less control surface motion; the wing itself creates the control forces.\n\nAAW's use of favorable wing twist response at high speeds, is viewed as a return to the wing-warping concept used by the Wright Brothers. Active aeroelastic wing technology enables a higher aspect ratio, more aerodynamically efficient wing which can be used to reduce induced drag at low wing strain conditions and, at higher strain conditions, reduce maneuver air loads. Overall benefits of AAW technology to future systems include substantially increased control power, reduced aerodynamic drag, reduced aircraft structural weight, and increased design latitude in terms of wing span, sweep, and thickness.\n\nThe pre-production version of the F/A-18 was an ideal host aircraft for proving AAW technology, a relatively high wing aspect ratio for a fighter, with adequate strength, but no additional stiffness was added for static aeroelastic issues. The X-53 F/A-18 was modified to allow two leading edge control surfaces to work in concert with its two trailing edge surfaces to control wing aeroelastic twist and provide excellent high speed rolling performance.\n\nAn aircraft maneuvers by deploying flight control surfaces into the airflow, which changes the air force on the surface. For instance, the ailerons on the wings used to roll an aircraft work by increasing or decreasing the lift of the outer portion of one wing, while doing the opposite on the other wing. This imbalance in lift forces causes the aircraft to rotate around its longitudinal axis.\n\nIt is the latter effect that is considered to be detrimental. When the aileron is deployed, the airflow is deflected at a point well behind the center of lift. This creates a torque around the span axis of the wing, which \"flattens\" it in relation to the airflow and reduces the effectiveness of the aileron input. Although this effect is quite small at low speeds, at higher speeds the amount of airflow over the surface can be considerable, enough to cause the entire wing to flex, an effect known as aeroelasticity.\n\nSince the control force of the aileron is a function of speed, and an aircraft has to be maneuverable at landing and takeoff speeds, there is a lower limit to the size of the control surfaces that can be used. In some cases, like the Supermarine Spitfire and Mitsubishi Zero, this meant that at faster speeds the forces on the controls were extremely high. In the case of the Spitfire, these forces acted on a very thin wing that was highly elastic, and in dives the wing would twist so much that the lift curve was opposed to the motion of the aileron, reversing the direction of the roll. This problem required several rounds of upgrades to correct.\n\nThe same problems affect modern aircraft as well, but today's wings are engineered with greater stiffness, to reduce the problem. Once the plane is built and flown, the flight control software is then adjusted to correct for the loss of control due to aeroelasticity. However, this simply hides the problem; the aeroelasticity still exists and is affecting the overall control authority. To correct this the flight controller uses more control input to offset any loss of effectiveness, which increases the spanwise torque loads on the aircraft's wing.\n\nAAW developed from the simple observation that the aeroelasticity can be offset by the deployment of other control surfaces on the wing. In particular, almost all modern aircraft use some form of slat along the wing's leading edge to provide more lift during certain portions of flight. By deploying the slats at the same time as the ailerons, the torque can be equalled out on either side of the spars, eliminating the twisting, which improves the control authority of the ailerons. This means that less aileron input is needed to produce a given motion, which, in turn, will reduce aileron drag and its associated negative control aspects. Better yet, the wing is already designed to be extremely strong in the lift component, eliminating the torque requires lift, converting the undesired torque into an acceptable lift component.\n\nBut if one can use the controls to eliminate the twisting and its negative effects on control input, the next step is to deliberately introduce a twisting component to \"improve\" the control authority. When applied correctly, the wing will twist less and in an opposite direction to a conventional wing during maneuvering. So this change, which can be accomplished in software, benefits overall performance.\n\nTo test the AAW theory, NASA and the USAF agreed to fund development of a single demonstrator, based on the F/A-18. Work started by taking an existing F/A-18 airframe modified with a preproduction wing, and added an outboard leading edge flap drive system and an updated flight control computer. Active aeroelastic wing control laws were developed to flex the wing, and flight instrumentation was used to accurately measure the aeroelastic performance of the wing planform. Flight software was then modified for flight testing, and the aircraft first flew in modified form in November 2002. The aircraft successfully proved the viability of the concept in full scale during roll maneuver testing in 2004–2005. The test aircraft was re-designated X-53 on August 16, 2006, per memo by USAF Deputy Chief of Staff, Strategic Plans and Programs.\n\n\n\n"}
{"id": "1249261", "url": "https://en.wikipedia.org/wiki?curid=1249261", "title": "Bulk micromachining", "text": "Bulk micromachining\n\nBulk micromachining is a process used to produce micromachinery or microelectromechanical systems (MEMS).\n\nUnlike surface micromachining, which uses a succession of thin film deposition and selective etching, bulk micromachining defines structures by selectively etching inside a substrate. Whereas surface micromachining creates structures \"on top\" of a substrate, bulk micromachining produces structures \"inside\" a substrate.\n\nUsually, silicon wafers are used as substrates for bulk micromachining, as they can be anisotropically wet etched, forming highly regular structures. Wet etching typically uses alkaline liquid solvents, such as potassium hydroxide (KOH) or tetramethylammonium hydroxide (TMAH) to dissolve silicon which has been left exposed by the photolithography masking step. These alkali solvents dissolve the silicon in a highly anisotropic way, with some crystallographic orientations dissolving up to 1000 times faster than others. Such an approach is often used with very specific crystallographic orientations in the raw silicon to produce V-shaped grooves. The surface of these grooves can be atomically smooth if the etch is carried out correctly, and the dimensions and angles can be precisely defined. Pressure sensors are usually created by bulk micromachining technique.\n\nBulk micromachining starts with a silicon wafer or other substrates which is selectively etched, using photolithography to transfer a pattern from a mask to the surface. Like surface micromachining, bulk micromachining can be performed with wet or dry etches, although the most common etch in silicon is the anisotropic wet etch. This etch takes advantage of the fact that silicon has a crystal structure, which means its atoms are all arranged periodically in lines and planes. Certain planes have weaker bonds and are more susceptible to etching. The etch results in pits that have angled walls, with the angle being a function of the crystal orientation of the substrate. This type of etching is inexpensive and is generally used in early, low-budget research.\n\n \n"}
{"id": "35205408", "url": "https://en.wikipedia.org/wiki?curid=35205408", "title": "Conservation and restoration of ceramic objects", "text": "Conservation and restoration of ceramic objects\n\nConservation and restoration of ceramic objects is a process dedicated to the preservation and protection of objects of historical and personal value made from ceramic. Typically this activity of conservation-restoration is undertaken by a conservator-restorer, especially when dealing with object of cultural heritage. Ceramics are created from a production of coatings of inorganic, nonmetallic materials using heating and cooling to create a glaze. Typically the coatings are permanent and sustainable for utilitarian and decorative purposes. The cleaning, handling, storage, and in general treatment of ceramics is consistent with that of glass because they are made of similar oxygen-rich components such as silicates. In conservation ceramics are broken down into three groups: unfired clay, earthenware or terracotta, and stoneware and porcelain.\n\nIt is in the nature of all the materials used for construction to eventually degrade and deteriorate. Degradation of an object occurs as a result of the interaction between the environment or with the materials that form the object however, in the case of ceramics, environmental factors are the major cause. There are several ways in which ceramics break down physically and chemically.\n\nAdditionally the type of ceramic will affect how it will break down. Unfired clay, like mud and clay adobe, is clay that is fired under 1000 °C or 1832 °F. This type of clay is water-soluble and unstable. Earthenware is clay that has been fired between 1000-1200 °C or 1832°-2192 °F. The firing makes the clay water insoluble but does not allow the formation of an extensive glassy or vitreous within the body. Although water-insoluble, the porous body of earthenware allows water to penetrate. A glaze can be applied that will protect the vessel from water. Due to its porosity, earthenware is susceptible to moisture and creates problems including cracks, breaks and mold growth. Porcelain and stoneware is fired at the highest temperatures between 1200-1400 °C or 2192-2552 °F. Porcelain clay mixtures are fired to create a non-porous and very hard surface. However, the materials also create a very brittle surface which increases the potential for chips, cracks and breaks.\n\nDue to their fragility, damage to ceramics typically comes from mishandling and packing. However, other factors such as vandalism, frost, mold, and other similar occurrences can also inflect harm.\n\nAlso known as inherent vice, the intrinsic instability of the fabric and components of an objects can lead to its own physical degradation. This is difficult to prevent because it occurs within the fabric of the material and therefore is a natural occurrence. Deterioration of an object can happen even before the object is used. How the piece is created can instil manufacturing defects in the piece. This means that objects can be damaged even before they are used. This would include a body that contains inadequate qualities of (filler (materials)). A second typical defect is from poor design and construction. An example of this would be a ceramic piece with a handle that is too thin to support the weight of the cup. A third manufacturing defect includes careless firing. A ceramic piece that has been fired too rapidly or allowed to dry too fast will crack or break.\n\nWith its delicate nature, ceramics that have been used over a period of time will sustain cracks, nicks, and blemishes. Additionally in a museum environment, damages can occur from packing storing, and handling of objects.\n\nDamage can occur when ceramics are exposed to freezing temperatures and frost. The problem occurs when ice crystals form inside of the pores of the ceramic piece. The frost inside of the pores will exert pressure onto the fabric of the pottery and cause the material to crack and break.\n\nOpposite of frost is heat and humidity. When the humidity is high molds can begin to form on ceramic, particularly ones in which there is no glaze. Spores of molds are found throughout the atmosphere and will attached to any organic residues nearby, such as ceramics. Earthenware ceramics are frequently affected due to their porosity and lack of glaze.\n\nChemical degradation of objects occurs not in the physical structure of the object but rather in at the chemical or compound level. Compounds begin to breakdown into more simpler compounds and is often an undesired reaction. The degradation of the chemical component of an object will hinder or weaken the stability of the object when exposed to environmental factors such as water, air, pollution, heat, humidity, and the like.\n\nWater can dissolve or deform ceramics that have been low fired, i.e. temperatures around 600 °C. Ceramic fired in high temperatures may also be susceptible to water if their mineral particles are soluble in water, for example Gypsum or calcite. Additionally the different compounds in water can flux and react in different ways to different ceramics. In naturally occurring water, carbon dioxide is dissolved and can create a chemical reaction with minerals in clay bodies that may form calcium bicarbonate which is very soluble. Stagnant water is less damaging because the carbon dioxide is not exhausted.\n\nA common degradation issue in ceramics involves soluble salts. Soluble salts can either enter the clay body from the environment, for example from being buried underground for decades, or they are already naturally occurring due to the components of the materials or clay used. Non-archaeological objects, such as modern dishware, can acquire salts from normal use such as storing salt. Soluble salts respond to changes in humidity both high and low. In high humidity salts become soluble and in low humidity they crystallize. The changing from soluble to crystallization and back damages the surface of the ceramic because salt crystals are larger than liquid salt and therefore will shrink and expand the ceramic body. A white haze on the surface is the first indication of soluble salts, which is the salt crystallizing. Over time, the physical component of the body will crumble until it is completely destroyed.\n\nIn the realm of conservation there are two distinct practices: non-interventive and active conservation. Non-interventive types of conservation are used to control the surrounding environment such as light, humidity, and temperature. Active conservation is when a conservator practices treatments to alleviate physical problems in the object such as fading, chipping, or breaks.\n\nAlthough ceramics are utilitarian, some pieces are made to be artwork and therefore displayed. Displaying an object improperly can cause damages either physical or chemically from the environment. One of the most common causes for damages is a ceramic piece falling over or off a shelf. To prevent this issue, many historic houses will line storage and display shelves with a thin layer of ethafoam (polyethylene foam) or bubble wrap.\n\nCeramics are very delicate in nature and damages can occur even when they are stored away. The most common way in which ceramics become damaged is when they are stacked one inside the other. Unless this is part of the original design, this will typically cause nicks, cracks, or breaks. Some ceramics, depending on their provenance, survive better in different temperature and humidity conditions. Pottery that has been buried, such as from an archaeological site, adheres better to storage at a constant low humidity. Low humidity will help to keep any salts from efflorescing which can mar the surface as well as remove the surface glaze.\n\nIn general ceramics are typically inert and are not sensitive to elevated light levels. However, extreme changes in temperature and humidity can cause chemical and physical damage. Typically museums strive to store ceramics, as well as many other material types, in a stable temperature of 68 °F with ± 3°. Additionally relative humidity should be stabilized at 50% also with a ±5%. Storing objects near windows, heaters, fireplaces, and exterior walls can create an unstable environment with temperature and humidity fluctuation and increase potential for damages.\n\nSome storing materials can be harmful to ceramic objects. Wool felt attracts and harbors insects including moths and silverfish which can be potentially very harmful to other collection material types. Polyurethane foam deteriorates over time which leaves a by-product that are sticky and acidic.\n\nOne of the cardinal rules in object handling is to treat every object as if it is fragile and easily breakable. Museum technicians, curators, and conservators are trained to prepare a moving plan before an object is even touched that way minimal damage can occur when handling or moving an object. In the museum field is it a known fact that a vessel, or any object, should be held and handled by its strongest part, such as the base, and with both hands. Areas such as the handle or neck of a vessel tend to be the weakest points and may break if picked up by these components.\n\nDamages also occur to ceramics from previous restoration. Although the intent was to repair the object for use or display, some dated practices are now known to increase damages either physically, from rivets or staples, or chemically, from old used adhesives that off-gas.\n\nOverpaint is a technique that is used to cover imperfection on the surface of a ceramic piece. Differences can be seen to the naked eye due to discoloration, being matched poorly, and change in texture or gloss. subtle difference can also been seen by restorers by using lighting and magnification. Overpaint and surface coatings can be removed either mechanically or with the use of solvents.\n\nMechanical removal of overpaint include physical techniques to remove the coating from the surface. On a glazed surface a sharp needle or scalpel can be used. If mechanical removal is not possible without damaging the surface then solvents can be used instead. The archetype solvents typically used are water, white spirit, industrial methylated spirits (denatured alcohol), acetone, and Dichloromethane which is usually found in the form of a commercial paint stripper. The appropriate solvent works by being applied to the ceramic surface by a cotton wool swab and is rolled on the surface rather than being wiped. Wiping the solvent on the surface will push the paint into the surface rather than lift it off.\n\nFill materials are used to fill in missing parts or breaks in a ceramic piece in order to stabilize the piece. A wide range of materials and techniques have been used to restore losses in ceramics. Today the most common filling materials are made from calcium-sulphate-based fillers or synthetic resins based on epoxy, acrylic, or polyester resin. These new resins are stronger and do not harm the object. Removing previous filling materials, either mechanically or chemically, and replacing them with new fillers can help keep the piece strong and stable.\n\nFillers can be removed physically by mechanical ways, depending on the filler material type. Cement mortar can be chiselled away with a hammer and chisel gradually. Plaster is easily removed through mechanical methods such as chiselling and chipping away with sharp implements. Saws, drills, and other mechanical methods can be used to remove the bulk of protruding materials, however scratches, chips, and breaks can occur.\n\nA second option to removing filler material is chemically. Typically, chemical removal is used once the bulk of filler material is left and only a small portion is left.\n\nUnlike adhesives, fills tend to be easier to remove and dismantle from ceramics. Plaster of Paris is one example of a fill that comes apart easily with warm.\n\nThe selection for the proper solvent is based on the identification of the adhesive itself. Every adhesive has a particular solvent that work best to break down its chemical composition. Color, hardness, and other physical properties will allow for identification of the adhesive. The adhesive can be soften once exposed, either in a liquid of vapor form, of the solvent for some time. The length of time depends on the solubility of the adhesive and the thickness of the joint. Porous bodies, low-fired clays, are sometimes pre-soaked in water to prevent the adhesive from being drawn back into the body once it joins with the removal solution. If the adhesive that is being removed is part of the support for the object, then supports, such as tissue paper or propping up the object, will be used to make sure the object does not sustain damages once the adhesive is removed. Sometimes if the adhesive is not softened enough if will remove part of the surface on the ceramic if pressure is applied. The information on solvents for specific adhesives are found below, under each adhesive section.\n\nRemoval of surface dirt and deposits is beneficial for the health and longevity of an object because it will prevent the dirt from becoming drawn into the body. Dust and grease may be held on the surface loosely by electrostatic forces or weak chemical bonds and are easily removed. Some deposits, such as calcium salts, can be strongly attached to a ceramic surface, especially if the surface is unglazed. There are two main methods in which ceramics are cleaned and treated: mechanically and chemically.\n\nNot all ceramic pieces are dry when they need cleaning. Some ceramics, such as those that are excavated archaeologically, will be damp or wet in nature. Conservators tend to remove the surface dirt before the object is completely dry. This is done because it is easier to do before the dirt hardens and because as it dried the dirt may shrink and cause physical damage to the ceramic surface. Some ceramics are kept damp until treatment can be completed.\n\nMechanical methods include dusting, picking and cutting, and abrading. Mechanical cleaning is typically much easier to control than chemical treatments and there is no danger of dirt being drawn into a solution and then absorbed by the ceramic. The danger of mechanical cleaning is the potential for the surface to break or become scratched with a tool. Dusting is used when dirt is not strongly adhered to the surface of the ceramic and is carried out by either a brush or a soft cloth. Large ceramic vessels are cleaned with delicate vacuum cleaner with a soft, muslin-covered head. Picking and cutting is used when there are hardened dirt, encrustations, or old restoration materials closely adheres to the surface. Needles, sharp scalpels, other custom made tools, usually made from wood, and electric vibrotools are used. The dangers with these tools are the increase potential for scratches, gouges, cracking, and breaking of the object due to pressure.\n\nAbrading is the process in which surface deposits are removed using abrasives. Abrasives come in both solid and cream forms. Solid forms of abrasives include glass fiber brush or a rubber burr on a dental drill. Cream forms are usually attached to paper or film. Polishing creams are commonly used to remove thin layers of insoluble surface deposits such calcium. These creams can also remove surface dirt and marks made by tools. The best creams of ceramic do not have oil, grease, or bleach as additives and are used only on glazed ceramics.\n\nChemical methods for cleaning ceramics involve water, solvents, acids, and alkalis. Prolonged soaking in water may be used as a conservation method. The goal is to either remove stains from the surface or to remove the soluble salts in the clay body.\n\nThe repair and restoration of ceramics has occurred since ceramics were invented including fillings, adhesives, reinforcements, and even patch work. The history of ceramic repair is vast and ranges from different methods and methodologies. For example, in 16th century China, people would repair broken ceramics by using pieces from other objects to disguise the patch. A sixteenth-century manuscript describes the process of patching broken ceramics:\nToday there are new advances in ceramic restoration including consolidation, bonding, adhesives, dowels, rivets, and fillers.\n\nConsolidation is the process in which the fabric of the ceramic is strengthened by introducing a material into the fabric that will bind together. The most common ceramics that need consolidation are excavated pieces because they tend to have lost their bonding fabrics due to leeching or have absorbed soluble salts. A consolidant works in two ways: it either links to the particles in the ceramic chemically or it may form a support system mechanically without reacting with the fabric itself. Chemical consolidants that are used in modern conservation include isocyanates, silanes, siloxnes, and methyl methacrylates, however the consolidants that create a mechanical support system are used more frequently.\n\nA chemical compound that adheres or bonds items together, such as pieces of ceramic. In ceramic conservation there are several different types that range from natural to man-made adhesives. Conservators characterise the best adhesive as one which can be undone.\n\nAnimal glue is an adhesive that contains various animal parts such as bone, skin, or fish and is widely used. It is a soft adhesive and can appear white, but usually has a pale yellow or brown appearance. Animal glue is very soft and can easily be broken down and removed with warm water and steam. Although easily reversible, the relative ease with which the glue breaks down makes it a less strong bonding method.\n\nA widely used old adhesive that is orange or very dark brown in appearance. Once dried, the adhesive is very hard and becomes increasingly more brittle over time. Shellac does not break down easily with commercially available products. Additionally, the resin has naturally-occurring dyes that can stain ceramic pink or black. The solvent that works best on this resin is Industrial methylated spirit or (IMA). Shellac is prepared by dissolving flakes of shellac in hot alcohol. The properties of shellac make it vulnerable to climatic conditions and inclined to deteriorate over time. Damage can even occur to shellac under the hot light of photography.\n\nThis type of resin is typically used post-1930s and is an indication of modern conservation work. Generally epoxy is very hard but unlike shellac is not brittle. The color of epoxy resin can range from yellow/green to a dark yellow/brown. Yellowing of the resin is an indication of aging. Warm to hot water or acetone are known to be the solvents of this adhesive.\n\nRubber cements are solutions of synthetic or natural rubber products in solvents, with or without resins and gums. Vulcanizers, accelerators, and stabilizers are considered problematic due to the nature of their compounds. One example is the additive of sulfur which is harmful to some types of material, including silver, because it can cause discoloration. Rubber adhesives can be confused with epoxy resins due to their similar appearance. However, unlike epoxy resins, rubber adhesives will stretch when pulled. Nitromors or Polystrippa solvent brands are used as a solvent but warm water can also loosen the bond.\n\nVinyl acetate polymers include polyvinyl acetate, polyvinyl alcohol and polyvinyl acetal; all come from reaction products of vinyl acetate. Some forms of acetates are known to be acidic and will do damage to an object with direct contact. Additionally, polyvinyl acetate mixtures tend to degrade in storage and release acetic acid, which in some cases can corrode lead. This compound's coloring ranges from clear/white to a soft yellow. As it ages, it will change to a deeper yellow. It can have a similar appearance to rubber adhesives but the difference is that PVA turns white when comes into contact with water. Warm water and acetone are typically used as solvents.\n\nThere are early and modern forms of this adhesive. While both tend to tinge with yellow as they age, the early form tends to become more brittle than the modern version, which contains a plasticizer to make the compound more stable. As with many adhesives, acetone is generally used as a solvent, however IMS can also be used.\n\nB-72 is a thermoplastic resin that was created by Rohm and Haas for use as a surface coating and as a vehicle for Flexographic ink. However B-72 is now being used more as an adhesive specifically for ceramic and glass. One of the major advantages of B-72 as a consolidate is that it is stronger and harder than polyvinyl acetate without being extremely brittle. This adhesive is more flexible than many of the other typically-used adhesives and tolerates stress and strain on a join that most others can not. One major drawback to using B-72 is the difficulty of applying the acrylic resin as an adhesive, as is difficulty in manipulating the substance as a workable agent. The most suitable solvent for B-72 is acetone.\n\nUnlike cellulose nitrate, B-72 does not need additives like plasticizers to stabilize its durability. Fumed colloidal silica is a chemical that can be added to help with the workability of the resin. Additionally research shows that the silica will better distribute stress and strain that occurs during evaporation of the solvent and during the setting of the adhesive film.\n\nDowels and rivets are physical ways in which ceramics can be reinforced and strengthen beneath the surface. Dowels are cylindrical rods that consist of wood, metal, or plastic. They are drilled into the ceramic piece and usually are set in the hole with an adhesive that is used to repair the ceramic piece. Removing dowels can be hard because they lie under the surface and are usually hidden. Conservators will cut through dowels with a piercing saw and soften the area with a solvent, like acetone to remove two pieces of ceramic from one another.\n\nRiveting is a process in which holes are drilled in the surface of the ceramic, but does not go completely through the piece. The rivets are angled toward the joint and provide additional structural support. There are two methods to removing rivets: the 'cut' and 'pull'. The 'cut' method consists of cutting the rivets through the middle with a file and then pulled out. The 'pull' method involves placing a thin blade under the rivet and pushing out any plaster packing. This method uses leverage to pull the rivet from the ceramic piece.\n\nFillers are used to replace gaps and losses from ceramic materials for either aesthetic reasons or for support. There are several different filler materials used in ceramics including plaster of Paris and other commercially available putties and fillers.\n\nPlaster of Paris is a material that consists of calcium sulphate hemihydrate power and is produced by heating gypsum to 120 °C. The chemical formula is as follows: :CaSO·2HO + \"Heat\" → CaSO·½HO + 1½ HO (released as steam). When mixed with water, an exothermic reaction occurs and forms a hard white filling similar to density of fired ceramics. Different grades of plasters are available and vary based on their particle size, setting time, density, expansion, and color.\n\nA thermoplastic synthetic wax resin mixture developed by John W Burke and Steve Colton in 1997 can be used to compensate losses in objects from translucent materials such as alabaster, marble, calcite, diorite, and anhydrite. The mixture consists of polyvinyl acetate (PVAC) AYAC, ethylene acrylic acid (EAA) copolymers A-C 540, and 580, antioxidants Irganox 1076 or 1035, dry pigments, marble powder, and other additives which were all melted together. This wax resin is a better substitute to wax-resins because wax collects dust and dirt and make the fill noticeable. Polyester resin and epoxies are toxic and noxious. The wax-resin is fast and easy to use, making it a possible new alternative to fill materials in the conservation field. The wax-resin works best on losses that allow for large contact with the original, primed surface and on losses that are thicker than 1/16in. Shallow losses and small gaps are more difficult due to the ease in which the fill is pulled out.\n\nIn France, conservators specialized in earthenware and glassware are trained at the Institut National du Patrimoine (The National Institute of Cultural Heritage). Their mission is to intervene when heritage resources are threatened or deteriorated for several reasons. The conservator prevents works of art from disappearing or loses its purpose whilst analyzing the complex stage of its material history and the cause of alteration.\n\n\n"}
{"id": "49251426", "url": "https://en.wikipedia.org/wiki?curid=49251426", "title": "Cooper Harris", "text": "Cooper Harris\n\nCooper Harris is an American entrepreneur and actress. She is currently the CEO & founder of Klickly, a payments platform, based in Venice, California. Harris is also the co-founder of Manventures, a high-end travel adventure company, that takes vetted groups of individuals on various missions and experiences. Prior to her foray into entrepreneurship, Harris had a career as an actress on such TV shows like Young and the Restless and As the World Turns, among others.\n\nCooper was also the Director of Tech for Collective Summit and produced the first-ever Innovation Summit and Social-Impact Hackathon during Sundance. She is one of the first females to win the Los Angeles AT&T Hackathon.\n\nEarly on in her childhood, Cooper Harris attended an arts-based school system, Waldorf schooling. She attended the Emerson Waldorf School in Chapel Hill, North Carolina. Cooper Harris trained at the University of North Carolina School of the Arts. She was among the six women to matriculate from her graduating class.\n\nUpon her move to New York City Harris was cast in the soap opera, As the World Turns. After her role on the TV Harris stepped in to play a leading role in an off-Broadway play at the Cherry Lane Theater. Shortly after the show closed, Harris moved to Los Angeles and into a role in a Paramount film called Dough Boys, which was her entrée into the Hollywood entertainment scene.\n\nDuring her stint on Young and the Restless, Cooper became interested in entrepreneurship and started attending hackathons. Having grown up with a brother who was a computer engineer and a father who was a serial entrepreneur, Harris stated that she was interested in a field that would allow both creativity and the ability to control her own destiny.\n\nHarris won the AT&T Hackathon in Los Angeles, and shortly thereafter build the first Innovation and Tech Summit during the Sundance film festival, called Collective. As part of this initiative, she co-founded Hackdance, a social impact hackathon that brought celebrities together to create apps or platforms that would change the world for the better. Cooper then went on to judge and produce the South by Southwest Hackathon, a hackathon for PBS, and the BritWeek children’s hackathon.\n\nIn her first startup, a digital production company, she created and sold her own shows & content to companies such as Post It Notes, Kimberly Clark, DailyMotion & studio production giant, Endemol.\n\nIn February 2015, Cooper raised money to fund her own company, Klickly. Klickly is an “innovative payments company” based in Venice, CA. In the past year, both Harris and the company have gained attention for innovation and have received mentions in Forbes, Inc. Mashable, Women 2.0 among others.\n\nPreviously, as the Executive Director of the Oasis Foundation, Cooper oversaw initiatives to foster innovation & entrepreneurship. She is also a mentor at Network For Teaching Entrepreneurship (NFTE), which teaches kids entrepreneurship in schools. Cooper speaks on being a Woman in Tech and the intersection of Entertainment & Tech at venues such as the CES Conference, USC, SXSW, Silicon Beach Fest, iMedia Summit, General Assembly, Wonder Woman Tech Summit, Sundance, and more, and produced a media-based Hackathon for PBS.\n\nCooper is on the Board of the non-profit organization Hidden Voices.\n\n\n"}
{"id": "27564525", "url": "https://en.wikipedia.org/wiki?curid=27564525", "title": "Crack arrestor", "text": "Crack arrestor\n\nIn materials science and material fatigue, a crack arrestor or rip-stop doubler is a structurally strong ring or strip of material which serves to contain stress cracking that could lead to catastrophic failure of a device.\n\nThe crack arrestor can be as simple as a thickened region of metal, or may be constructed of a laminated or woven material that can withstand deformation without failure.\n\nCrack arrestors were used to reinforce the airplane hull of the de Havilland Comet following a series of catastrophic accidents related to structural design problems that were previously unknown.\n\n"}
{"id": "44791207", "url": "https://en.wikipedia.org/wiki?curid=44791207", "title": "Digital Beijing Building", "text": "Digital Beijing Building\n\nThe Digital Beijing Building () is located northwest of the intersection of Beichen West and Anxiang North roads, on Olympic Green, in the Chaoyang District of Beijing, China. It is a block-shaped building erected to serve as a data center during the 2008 Summer Olympics. Since then it has served as both a museum devoted to the use of computing in the Olympics, and exhibition space for digital technology companies.\n\nIt was the only major facility on Olympic Green not to be an event venue for the games, and the only major Olympic facility designed by a Chinese architect. That architect, Pei Zhu, was interested in the connections between traditional Chinese design and digital technology. He produced a sustainable building that resembles a circuit board when viewed from either side and a bar code when viewed from either end, in the process using some new materials for the first time in China. It has been both praised for its avoidance of kitsch and criticized as resembling Orwell's Ministry of Truth. At the 2008 World Architecture Festival it was shortlisted in its category.\n\nDigital Beijing is located on the northwest corner of the intersection of Beichen West, Anxiang North and Huizhong roads in the Olympic Green neighborhood of Beijing's Chaoyang District, a generally level area north of the Forbidden City on the city's central axis. Guihua Third Street is to the north, and Tianchen West Road is to the west. To the southeast is the Beijing National Aquatics Center, colloquially known as the Watercube, the venue for swimming and diving at the Olympics and now an indoor water park; the distinctive Beijing National Stadium, or Birds' Nest, is to the east of the Aquatics Center. East of the building is the Beijing National Indoor Stadium, where gymnastics and several other events were held. Another important Olympic venue, the China National Convention Center, takes up several blocks to the northeast.\n\nDirectly to the south are several blocks of lightly planted open space buffering the Aquatics Center from Beichen West. On the north a parking lot separates Digital Beijing from National Stadium Road and the InterContinental Beichen Beijing Hotel high-rise on the other side. The buildings of Science Park Nanli are across Beichen to the west, and the Pangu Seven-Star Hotel on the southwest corner of the intersection anchors the buildings of Pangu Plaza, which continue to the south along the west side of Beichen West.\n\nWater surrounds the building's on all sides but the east. It consists of four large narrow slabs 11 stories () high but of varying thickness, with the easternmost thickest of all, with gaps between them, with the gap between the eastern slab and the others being wider. All are connected by pedestrian bridges at various heights; those nearer each other have larger bridges, with the two on the west having a two-story glass hyphen. They are faced in a dark stone quarried in northern China, with some inserts of aluminum made to look like stone.\n\nAlthough there is a smooth glass curtain wall (made of a low-energy glass with low thermal conductivity, to save energy with many windows on the east elevation and the interior facades overlooking the gaps, there are no windows on the west face. Instead, it is decorated with irregularly spaced vertical grooves of differing width and that take diagonal turns at different points along their descent and then straighten out again shortly afterwards. At night a series of green LEDs blink in a descending fashion down the east facade. The flat roof has a rainwater collection system.\n\nInside the rooms and hallways are floored in a translucent fiber-reinforced plastic (FRP). Images can be, and are, projected on the undersides of interior pedestrian bridges. There is 98,000 m (1.05 million sq ft) of space, including two underground levels. Lighting is provided by an LED system that uses 60% less energy than other forms.\n\nOnce Beijing was awarded the 2008 Summer Olympics in 2001, planning began for the Games, and the Olympic Green area in the city's Chaoyang District that would host the Olympic Village and many major event venues. One theme of the Olympics was to be the \"Digital Olympics\", using more new information technology than had ever before been used in the Olympics. Early in 2002, the city's Municipal Informatization Office called for a \"landmark building\" to use as the main data center during the Games and for other, related purposes afterwards.\n\nA contest was held, and the design by Pei Zhu, then with the Chinese firm Urbanus but in the process of setting up his own Pei Zhu Studio, was chosen from among eight competitors in 2004. It was the only major facility among the 31 new buildings to have been designed by a Chinese architect. It was also the only Olympic Green building that was not an Olympic event venue.\n\n\"If the industrial revolution resulted in modernism, contemporary architecture needs to explore what will form out of the current revolution of information,\" Zhu wrote later. \"Conceptually, Digital Beijing was developed through reconsideration and reflection on the role of Chinese architecture in the modern information era. [It] helps to develop a new aesthetic, an architectural language that is thoroughly contemporary, but retains a Chinese texture and sensibility.\"\n\nTo that end, the building he designed echoes the tools it was designed to house. From the ends it resembles a bar code, rising from the water. Zhu explains:\n\nThe east and west facades offer a contrast between the windowless western side, continuing to emphasize the building's contents and purpose by being decorated to resemble a circuit board, and the open, glassy easy. This contrast of extremes, according to Zhu, is what the building draws from Chinese culture, specifically the \"hutong\", the narrow mazes of alleyways where much of Beijing's traditional street life and community took place. \"On the exterior transparency is very rare, where internally the true interaction between nature, open space and building is most evident.\" He concludes \"Through the perspective of Chinese philosophy, everything including the advancement of technology has an intimate connection with the natural realm. Aesthetically this perspective continues a dialogue between the past and the future.\"\n\nConstruction began in 2005. During that time Zhu and the builders were to take advantage of two newly developed materials. The first, a translucent fiber reinforced plastic, had first been developed for a hotel Zhu was building elsewhere in Beijing as a substitute for jade, which proved too expensive to use in the quantity he had wanted it. Since it held images projected in it well, he also decided to use it for the flooring inside Digital Beijing, where it could become a \"digital carpet\", similar to the \"urban carpet\" Zaha Hadid created for the Rosenthal Center for Contemporary Art in Cincinnati, Ohio. It was also strong enough to support the interior footbridges, so Zhu used it there too. On the exterior, a local maker of beverage cans developed the aluminum sheets for the building's facades that gleam in places yet still look like stone from far away.\n\nThe building was completed and opened on 3 November 2007. It served as the main data center during the Summer Games the following year, as intended (a backup was built in an undisclosed location). Since then it has served, also as planned, as a museum of the Digital Olympics and exhibition space, both concentrated in the public area of the building on the east, where the interior is visible. \"Digital Beijing accepts this transformation with a capacity for constant renovation, sprinting along side the pace of our time,\" Zhu wrote.\n\nThe first public reaction from an architecture critic was positive. \"If China has set out to impress the world with the 2008 Olympics,\" \"The Guardian\"'s Jonathan Glancey wrote in February 2008, several months before the Games and two months after the building's completion, \"the stadium and its attendant buildings—the Aquatics Centre and Digital Beijing (the Olympics 'command post')—have set a heady precedent.\" He contrasted them with the tepid reaction to building designs already unveiled for the 2012 Summer Olympics in London.\n\nGlancey likened the four upright slabs to \"upright 1960s IBM computers\" and praised Zhu's \"creative use of unexpected materials\". Unlike most other prominent contemporary Chinese architects, Zhu had, the critic noted, enjoyed the luxury of time in his work. \"The result is delightful.\"\n\nHowever, a few months later, Tom Dyckhoff of \"The Times\" was not so impressed. He was the first Western journalist allowed to tour Olympic Green, most of which was still under construction and surrounded with a steel fence under military guard to block it from public view. While he had unmitigated praise for the Birds' Nest, he considered the rest of the buildings \"a flop\". He described Digital Beijing as \"chees[y] ... Four gloomy stone slabs, divided by glass atria, do an excellent Orwellian Ministry of Truth impression.\" He did allow that it was \"slightly less spirit-crushing inside\".\n\nDyckhoff's was the only prominent negative reaction. Two months after the Olympics, at that year's World Architecture Festival, Digital Beijing was shortlisted in its category. The next year, former \"New York Times\" critic Paul Goldberger wrote in his book \"Building Up and Tearing Down: Reflection on the Age of Architecture\" that while, like the adjacent Water Cube, Digital Beijing \"steers dangerously close to a kitschy conceit\", it, too, succeeds. \"The finished building has a dignity that is surprising ... an austerity that is the opposite of kitsch.\"\n\nIn 2011 Harvard professor Peter G. Rowe wrote at length about Digital Beijing in \"Emergent Architectural Territories in East Asian Cities\". \"Although hardly the fault of architecture under such a presumption,\", he wrote of Zhu's stated aesthetic intentions for the building, \"this sort of symbol may seem somewhat at odds with the dispersed and uniquitous character of today's digital media.\" Nevertheless, he continued, that may have been part of the building's point.\n\n"}
{"id": "27990947", "url": "https://en.wikipedia.org/wiki?curid=27990947", "title": "Digital Humanities conference", "text": "Digital Humanities conference\n\nThe Digital Humanities conference is an academic conference for the field of digital humanities. It is hosted by Alliance of Digital Humanities Organizations and has been held annually since 1989.\n\nThe first joint conference was held in 1989, at the University of Toronto—but that was the 16th annual meeting of ALLC, and the ninth annual meeting of the ACH-sponsored International Conference on Computers and the Humanities (ICCH).\n\nThe Chronicle of Higher Education has called the conference \"highly competitive\" but \"worth the price of admission,\" praising its participants' focus on best practices, the intellectual community it has fostered, and the tendency of its organizers to sponsor attendance of early-career scholars (important given the relative expense of attending it, as compared to other academic conferences). \n\nAn analysis of the Digital Humanities conference abstracts between 2004 and 2014 highlights some trends evident in the evolution of the conference (such as the increasing rate of new authors entering the field, and the continuing disproportional predominance of authors from North America represented in the abstracts). An extended study (2000-2015) offer a feminist and critical engagement of Digital Humanities conferences with solutions for a more inclusive culture. Scott Weingart has also published detailed analyses of submissions to Digital Humanities 2013, 2014, 2015, and 2016 on his blog.\n\n"}
{"id": "9971", "url": "https://en.wikipedia.org/wiki?curid=9971", "title": "Eden Project", "text": "Eden Project\n\nThe Eden Project () is a popular visitor attraction in Cornwall, England, UK. Inside the two biomes are plants that are collected from many diverse climates and environments. The project is located in a reclaimed china clay pit, located from the town of St Blazey and from the larger town of St Austell.\n\nThe complex is dominated by two huge enclosures consisting of adjoining domes that house thousands of plant species, and each enclosure emulates a natural biome. The biomes consist of hundreds of hexagonal and pentagonal, inflated, plastic cells supported by steel frames. The largest of the two biomes simulates a rainforest environment and the second, a Mediterranean environment. The attraction also has an outside botanical garden which is home to many plants and wildlife native to Cornwall and the UK in general; it also has many plants that provide an important and interesting backstory, for example, those with a prehistoric heritage.\n\nThe project was conceived by Tim Smit and designed by architect Nicholas Grimshaw and engineering firm Anthony Hunt and Associates (now part of Sinclair Knight Merz). Davis Langdon carried out the project management, Sir Robert McAlpine and Alfred McAlpine did the construction, MERO designed and built the biomes, and Arup was the services engineer, economic consultant, environmental engineer and transportation engineer. Land use consultants led the masterplan and landscape design. The project took 2½ years to construct and opened to the public on 17 March 2001.\n\nOnce into the attraction, there is a meandering path with views of the two biomes, planted landscapes, including vegetable gardens, and sculptures that include a giant bee and previously The WEEE Man (removed in 2016), a towering figure made from old electrical appliances and was meant to represent the average electrical waste used by one person in a lifetime.\n\nAt the bottom of the pit are two covered biomes:\n\nThe Tropical Biome, covers and measures high, wide, and long. It is used for tropical plants, such as fruiting banana plants, coffee, rubber and giant bamboo, and is kept at a tropical temperature and moisture level.\nThe Mediterranean Biome covers and measures high, wide, and long. It houses familiar warm temperate and arid plants such as olives and grape vines and various sculptures.\n\nThe Outdoor Gardens represent the temperate regions of the world with plants such as tea, lavender, hops, hemp and sunflowers, as well as local plant species.\n\nThe covered biomes are constructed from a tubular steel (hex-tri-hex) with mostly hexagonal external cladding panels made from the thermoplastic ETFE. Glass was avoided due to its weight and potential dangers. The cladding panels themselves are created from several layers of thin UV-transparent ETFE film, which are sealed around their perimeter and inflated to create a large cushion. The resulting cushion acts as a thermal blanket to the structure. The ETFE material is resistant to most stains, which simply wash off in the rain. If required, cleaning can be performed by abseilers. Although the ETFE is susceptible to punctures, these can be easily fixed with ETFE tape. The structure is completely self-supporting, with no internal supports, and takes the form of a geodesic structure. The panels vary in size up to across, with the largest at the top of the structure.\n\nThe ETFE technology was supplied and installed by the firm Vector Foiltec, which is also responsible for ongoing maintenance of the cladding. The steel spaceframe and cladding package (with Vector Foiltec as ETFE subcontractor) was designed, supplied and installed by MERO (UK) PLC, who also jointly developed the overall scheme geometry with the architect, Nicholas Grimshaw & Partners.\n\nThe entire build project was managed by McAlpine Joint Venture.\n\nThe Core is the latest addition to the site and opened in September 2005. It provides the Eden Project with an education facility, incorporating classrooms and exhibition spaces designed to help communicate Eden's central message about the relationship between people and plants. Accordingly, the building has taken its inspiration from plants, most noticeable in the form of the soaring timber roof, which gives the building its distinctive shape.\n\nGrimshaw developed the geometry of the copper-clad roof in collaboration with a sculptor, Peter Randall-Page, and Mike Purvis of structural engineers SKM Anthony Hunts. It is derived from phyllotaxis, which is the mathematical basis for nearly all plant growth; the \"opposing spirals\" found in many plants such as the seeds in a sunflower's head, pine cones and pineapples. The copper was obtained from traceable sources, and the Eden Project is working with Rio Tinto Group to explore the possibility of encouraging further traceable supply routes for metals, which would enable users to avoid metals mined unethically. The services and acoustic, mechanical and electrical engineering design was carried out by Buro Happold.\n\nThe Core is also home to art exhibitions throughout the year. A permanent installation entitled \"Seed\", by Peter Randall-Page, occupies the anteroom. \"Seed\" is a large, 70 tonne egg-shaped stone installation standing some tall and displaying a complex pattern of protrusions that are based upon the geometric and mathematical principles that underlie plant growth.\n\nThe domes provide diverse growing conditions, and many plants are on display.\n\nThe Eden Project includes environmental education focusing on the interdependence of plants and people; plants are labelled with their medicinal uses. The massive amounts of water required to create the humid conditions of the Tropical Biome, and to serve the toilet facilities, are all sanitised rain water that would otherwise collect at the bottom of the quarry. The only mains water used is for hand washing and for cooking. The complex also uses Green Tariff Electricity – the energy comes from one of the many wind turbines in Cornwall, which were among the first in Europe.\n\nControversially, one of the companies the Eden Project currently partners with is the British mining company Rio Tinto Group.\n\nIn December 2010 the Eden Project received permission to build a geothermal electricity plant which will generate approx 4MWe, enough to supply Eden and about 5000 households.\n\nThe clay pit in which the project is sited was in use for over 160 years. In 1981, the pit was used by the BBC as the planet surface of Magrathea in the 1981 TV series of \"the Hitchhiker's Guide to the Galaxy\". By the mid-1990s the pit was all but exhausted.\n\nThe initial idea for the project dates back to 1996, with construction beginning in 1998. The work was hampered by torrential rain in the first few months of the project, and parts of the pit flooded as it sits below the water table.\n\nThe first part of the Eden Project, the visitor centre, opened to the public in May 2000. The first plants began arriving in September of that year, and the full site opened on 17 March 2001.\n\nThe Eden Project was used as a filming location for the 2002 James Bond film, \"Die Another Day\". On 2 July 2005 The Eden Project hosted the \"Africa Calling\" concert of the Live 8 concert series. It has also provided some plants for the British Museum's Africa garden.\n\nIn 2005, the Project launched \"A Time of Gifts\" for the winter months, November to February. This features an ice rink covering the lake, with a small café/bar attached, as well as a Christmas market. Cornish choirs regularly perform in the biomes.\n\nOn 6 December 2007, the Eden Project invited people all over Cornwall to try to break the world record for the biggest ever pub quiz as part of its campaign to bring £50 million of lottery funds to Cornwall.\n\nIn December 2007, the project failed in its bid for £50 million of funding, after the Big Lottery Fund popular vote,\nwhen it received just 12.07% of the votes, the lowest for the four projects being considered. Eden wanted the money for Edge, a proposed desert biome that was going to look at people and plants living on the edge today and the solutions that they have come up with to the challenge of living within limits.\n\nIn December 2009, much of the project, including both greenhouses, became available to navigate through Google Street View.\n\nThe Eden Trust revealed a trading loss of £1.3 million for 2012-13, on a turnover of £25.4 million. The Eden Project had posted a surplus of £136,000 for the previous year. In 2014 Eden accounts showed a surplus of £2 million.\n\nThe World Pasty Championships have been held at the Eden Project since 2012, an international competition to find the best Cornish pasties and other pasty-type savoury snacks.\nThe Eden Project is said to have contributed over £1 billion to the Cornish economy.\n\nThe Eden Project received 1,024,156 visitors in 2017. \n\nSince 2002, the Project has hosted a series of musical performances, called the Eden Sessions. Artists have included Amy Winehouse, James Morrison, Muse, Lily Allen, Snow Patrol, Pulp, Brian Wilson and The Magic Numbers. 2008's summer headliners were: The Verve, Kaiser Chiefs, and KT Tunstall. Oasis were also set to play in the summer of 2008, but the concert was postponed because Noel Gallagher was unable to perform after breaking three ribs in a stage invasion incident several weeks before. The concert was instead played in the summer of 2009. 2010 saw performances from artists including Mika, Jack Johnson, Mojave 3, Doves, Paolo Nutini, Mumford & Sons, and Martha Wainwright.\n\nThe 2011 sessions were headlined by The Flaming Lips, Primal Scream, Pendulum, Fleet Foxes and Brandon Flowers with support from The Horrors, The Go! Team, OK Go, Villagers, and The Bees.\n\nThe 2012 Eden sessions were headlined by: Tim Minchin, Example, Frank Turner, Chase & Status, Plan B, Blink-182, Noah and the Whale, and The Vaccines.\n\nThe 2013 Eden Sessions were headlined by: Kaiser Chiefs, Jessie J, Eddie Izzard, Sigur Rós, and The xx.\n\nThe 2014 Eden Sessions were headlined by: Dizzee Rascal, Skrillex, Pixar in Concert, Ellie Goulding and Elbow.\n\nThe 2015 Eden Sessions were headlined by: Paolo Nutini, Elton John, Paloma Faith, Motörhead, The Stranglers, Spandau Ballet and Ben Howard.\n\nThe 2016 Eden Sessions were headlined by: Lionel Richie, Tom Jones, PJ Harvey, Manic Street Preachers and Jess Glynne.\n\nThe 2017 Eden Sessions were headlined by: Bastille (band), Madness (band), Royal Blood (band), Blondie (band), Van Morrison, Bryan Adams, and Foals (band).\n\n\n\n\n"}
{"id": "17914692", "url": "https://en.wikipedia.org/wiki?curid=17914692", "title": "European Food Information Resource Network", "text": "European Food Information Resource Network\n\nEuroFIR (European Food Information Resource) is a non-profit international Association, which supports use of existing food composition data and future resources through cooperation and harmonization of data quality, functionality and global standards.\n\nThe purpose of the Association is the development, management, publication and exploitation of food composition data, and the promotion of international cooperation and harmonization through improved data quality, food composition database searchability and standards, for example, with the European Committee for Standardization (CEN) on the standard for food data. Other work includes that on ethnic and traditional foods and critical evaluation of data on nutrients \n\n(*) Member organisations, which are also national food composition database compilers.\n\n(*) Member organisations, which are also national food composition database compilers.\n\nList of EuroFIR AISBL key people, members and collaborators on the EuroFIR website.\n\n"}
{"id": "10931772", "url": "https://en.wikipedia.org/wiki?curid=10931772", "title": "FETI", "text": "FETI\n\nIn mathematics, in particular numerical analysis, the FETI method (finite element tearing and interconnect) is an iterative substructuring method for solving systems of linear equations from the finite element method for the solution of elliptic partial differential equations, in particular in computational mechanics In each iteration, FETI requires the solution of a Neumann problem in each substructure and the solution of a coarse problem. The simplest version of FETI with no preconditioner (or only a diagonal preconditioner) in the substructure is scalable with the number of substructures but the condition number grows polynomially with the number of elements per substructure. FETI with a (more expensive) preconditioner consisting of the solution of a Dirichlet problem in each substructure is scalable with the number of substructures and its condition number grows only polylogarithmically with the number of elements per substructure. The coarse space in FETI consists of the nullspace on each substructure.\n\n\n"}
{"id": "10337466", "url": "https://en.wikipedia.org/wiki?curid=10337466", "title": "Field Ration Eating Device", "text": "Field Ration Eating Device\n\nThe Field Ration Eating Device (F.R.E.D. or FRED) is a small device which is a combination of a can opener, a bottle opener and a spoon. It is issued to the Australian Defence Force in its CR1M ration packs. It is also known widely as the \"Fucking Ridiculous Eating Device\".\n\nThe can opener is very similar in design to the US military P-38 can opener.\n"}
{"id": "4204003", "url": "https://en.wikipedia.org/wiki?curid=4204003", "title": "Franz–Keldysh effect", "text": "Franz–Keldysh effect\n\nThe Franz–Keldysh effect is a change in optical absorption by a semiconductor when an electric field is applied. The effect is named after the German physicist Walter Franz and Russian physicist Leonid Keldysh (nephew of Mstislav Keldysh).\n\nKarl W. Böer observed first the shift of the optical absorption edge with electric fields during the discovery of high-field domains and named this the Franz-effect. A few months later, when the English translation of the Keldysh paper became available, he corrected this to the Franz–Keldysh effect.\n\nAs originally conceived, the Franz–Keldysh effect is the result of wavefunctions \"leaking\" into the band gap. When an electric field is applied, the electron and hole wavefunctions become Airy functions rather than plane waves. The Airy function includes a \"tail\" which extends into the classically forbidden band gap. According to Fermi's Golden Rule, the more overlap there is between the wavefunctions of a free electron and a hole, the stronger the optical absorption will be. The Airy tails slightly overlap even if the electron and hole are at slightly different potentials (slightly different physical locations along the field). The absorption spectrum now includes a tail at energies below the band gap and some oscillations above it. This explanation does, however, omit the effects of excitons, which may dominate optical properties near the band gap.\n\nThe Franz–Keldysh effect occurs in uniform, bulk semiconductors, unlike the quantum-confined Stark effect, which requires a quantum well. Both are used for Electro-absorption modulators. The Franz–Keldysh effect usually requires hundreds of volts, limiting its usefulness with conventional electronics – although this is not the case for commercially available Franz–Keldysh-effect electro-absorption modulators that use a waveguide geometry to guide the optical carrier.\n\nThe absorption coefficient is related to the dielectric constant (especially the complex term). From Maxwell's equation, we can easily find out the relation,\n\nWe will consider the direct transition of an electron from the valence band to the conduction band induced by the incident light in a perfect crystal and try to take into account of the change of absorption coefficient for each Hamiltonian with a probable interaction like electron-photon, electron-hole, external field. These approach follows from. We put the 1st purpose on the theoretical background of Franz–Keldysh effect and third-derivative modulation spectroscopy.\n\nformula_2 (A: vector potential, V(r): periodic potential)\n\nformula_3\n\nNeglecting the square term formula_4 and using the relation formula_5 within the Coulomb gauge formula_6, we obtain\n\nformula_7\n\nThen using the Bloch function formula_8 (j= v, c that mean valence band, conduction band)\n\nthe transition probability can be obtained such that\n\nformula_9\nformula_10(formula_11 means wave vector of light)\nformula_12\n\nPower dissipation of the electromagnetic waves per unit time and unit volume gives rise to following equation\n\nformula_13\n\nFrom the relation between the electric field and the vector potential, formula_14, we may put formula_15\n\nAnd finally we can get the imaginary part of the dielectric constant and surely the absorption coefficient.\nformula_16\n\nAn electron in the valence band(wave vector k) is excited by photon absorption into the conduction band(the wave vector at the band is formula_17) and leaves a hole in the valence band(the wave vector of the hole is formula_18). In this case, we include the electron-hole interaction.(formula_19)\n\nThinking about the direct transition, formula_20 is almost same. But Assume the slight difference of the momentum due to the photon absorption is not ignored and the bound state- electron-hole pair is very weak and the effective mass approximation is valid for the treatment. Then we can make up the following procedure, the wave function and wave vectors of the electron and hole\n\nformula_21 (i, j are the band indices, and r, r, k, k are the coordinates and wave vectors of the electron and hole respectively)\n\nAnd we can take a total wave vector K such that\n\nformula_22\nformula_23\n\nThen, Bloch functions of the electron and hole can be constructed with the phase term formula_24\nformula_25\n\nIf V slowly over the distance of the integral, the term can be treated like following.\n\nformula_26\n\nhere we assume that the conduction and valence bands are parabolic with scalar masses and that at the top of the valence band formula_27, i.e.\nformula_28 (formula_29 is the energy gap)\n\nNow, The Fourier transform of formula_30 and above (*), the effective mass equation for the exciton may be written as\n\nformula_31\n\nformula_32\n\nthen the solution of eq is given by\n\nformula_33\nformula_34\n\nformula_35 is called the envelope function of an exciton. The ground state of the exciton is given in analogy to the hydrogen atom.\n\nthen, the dielectric function is\n\nformula_36\n\ndetailed calculation is in.\n\nFranz–Keldysh effect means an electron in a valence band can be allowed to be excited into a conduction band by absorbing a photon with its energy below the band gap. Now we're thinking about the effective mass equation for the relative motion of electron hole pair when the external field is applied to a crystal. But we are not to take a mutual potential of electron-hole pair into the Hamiltonian.\n\nWhen the Coulomb interaction is neglected, the effective mass equation is\n\nformula_37.\n\nAnd the equation can be expressed,\n\nformula_38( where formula_39 is the value in the direction of the principal axis of the reduced effective mass tensor)\n\nUsing change of variables:\n\nformula_40\n\nthen the solution is\n\nformula_41\n\nwhere formula_42\n\nFor example, formula_43 the solution is given by\n\nformula_44\n\nThe dielectric constant can be obtained inserting this equation to the (**) (above block), and changing the summation with respect to λ to formula_45\n\nThe integral with respect to formula_46 is given by the joint density of states for the two-D band. (the Joint density of states is nothing but the meaning of DOS of both electron and hole at the same time.)\n\nformula_47\n\nwhere formula_48\n\nformula_49\n\nThen we put formula_50\n\nAnd think about the case we find formula_51, thus formula_52 with the asymptotic solution for the Airy function in this limit.\n\nFinally,formula_53\n\nTherefore, the dielectric function for the incident photon energy below the band gap exist! These results indicate that absorption occurs for an incident photon.\n\n\n"}
{"id": "25313777", "url": "https://en.wikipedia.org/wiki?curid=25313777", "title": "Fujitsu Computer Products of America", "text": "Fujitsu Computer Products of America\n\nFujitsu Computer Products of America, Inc. (FCPA) Fujitsu Computer Products of America, Inc. is a subsidiary of Fujitsu Limited, the worlds third largest IT products and services provider. FCPA designs, develops, and manufactures innovative computer products for the global marketplace. Current product and service offerings include high-performance hard disk drives, scanners and scanner maintenance, palm vein recognition technology, and 10Gb Ethernet switches and degaussers. FCPA is headquartered in Sunnyvale, California, United States. The company is responsible for design and development, distribution, sales and marketing, finance and administration, and engineering and technical support for the Fujitsu document imaging scanner business and computing and storage products.\n\nThe company claims to have a \"55 percent market share in the U.S. of the 20-to-49-pages-per-minute, high-performance scanner market.\" \n\n\n\"Fujitsu sold its Enterprise Hard Disk Drive business to Toshiba as of October 1st, 2009.\"\n"}
{"id": "50680", "url": "https://en.wikipedia.org/wiki?curid=50680", "title": "Funicular", "text": "Funicular\n\nA funicular () is one of the modes of transportation which uses a cable traction for movement on steep inclined slopes.\n\nA funicular railway employs a pair of passenger vehicles which are pulled on a slope by the same cable which loops over a pulley wheel at the upper end of a track. The vehicles are permanently attached to the ends of the cable and counterbalance each other. They move synchronously: while one vehicle is ascending, the other one is descending the track. These particularities distinguish funiculars from other types of cable railways. For example, a funicular is distinguished from an inclined elevator by the presence of two vehicles which counterbalance each other.\n\nThe name \"funicular\" itself is derived from the Latin word \"\", the diminutive of , which translates as \"rope\".\n\nThe basic idea of funicular operation is that two cars are always attached to each other by a cable, which runs through a pulley at the top of the slope. Counterbalancing of the two cars, with one going up and one going down, minimizes the energy needed to lift the car going up. Winching is normally done by an electric drive that turns the pulley. Sheave wheels guide the cable to and from the drive mechanism and the slope cars.\n\nEarly funiculars used two parallel straight tracks, four rails, with separate station platforms for each vehicle. The tracks are laid with sufficient space between them for the two cars to pass at the midpoint. Three-rail arrangement was also used to overcome the half-way passing problem.\n\nThe wheels of the cars are usually single-flanged, as on standard railway vehicles. Examples of this type of track layout are the Duquesne Incline in Pittsburgh, Pennsylvania, and most cliff railways in the UK.\n\nThe Swiss engineer Carl Roman Abt invented the method that allows cars to be used with a two-rail configuration: the outboard wheels have flanges on both sides, which keeps them aligned with the outer rail, thus holding each car in position, whereas the inboard wheels are unflanged and ride on top of the opposite rail, thereby easily crossing over the rails (and cable) at the passing track.\n\nTwo-rail configurations of this type avoid the need for switches and crossings, since the cars have the flanged wheels on opposite sides and will automatically follow different tracks, and in general, significantly reduce costs.\n\nIn layouts using three rails, the middle rail is shared by both cars. The three-rail layout is wider than the two-rail layout, but the passing section is simpler to build. If a rack for braking is used, that rack can be mounted higher in a three-rail layout, making it less sensitive to choking in snowy conditions.\n\nSome four-rail funiculars have the upper and lower sections interlaced and a single platform at each station. The Hill Train at Legoland, Windsor, is an example of this configuration.\n\nThe track layout can also be changed during the renovation of a funicular, and often four-rail layouts have been rebuilt as two- or three-rail layouts; e.g., the Wellington Cable Car in New Zealand was rebuilt with two rails.\n\nThe cars can be attached to a second cable running through a pulley at the bottom of the incline in case the gravity force acting on the vehicles is too low to operate them on the slope. One of the pulleys must be designed as a tensioning wheel to avoid slack in the ropes. In this case, the winching can also be done at the lower end of the incline. This practice is used for funiculars with gradients below 6%, funiculars using sledges instead of cars, or any other case where it is not ensured that the descending car is always able to pull out the cable from the pulley in the station on the top of the incline.\nAnother reason for a bottom cable is that the cable supporting the lower car at the extent of its travel will potentially weigh several tons, whereas that supporting the upper car weighs virtually nothing. The lower cable adds an equal amount of cable weight to the upper car while deducting the same weight from the lower, thereby keeping the cars in equilibrium.\n\nA few funiculars have been built using water tanks under the floor of each car that are filled or emptied until just sufficient imbalance is achieved to allow movement. The car at the top of the hill is loaded with water until it is heavier than the car at the bottom, causing it to descend the hill and pulling up the other car. The water is drained at the bottom, and the process repeats with the cars exchanging roles. The movement is controlled by a brakeman.\n\nThe Giessbachbahn in the Swiss canton of Berne, opened in 1879, originally was powered by water ballast. Later on it was converted to electrical power. The Bom Jesus funicular built in 1882 near Braga, Portugal is another example.\n\nThe in Fribourg, Switzerland, is of a particular interest as for counterbalancing it utilizes waste water, coming from a sewage plant at the upper part of the city.\n\nFunicular railways operating in urban areas date from the 1860s. The first line of the Funiculars of Lyon (Funiculaires de Lyon) opened in 1862, followed by other lines in 1878, 1891 and 1900. The Budapest Castle Hill Funicular was built in 1868–69, with the first test run on 23 October 1869. In Istanbul, Turkey, the Tünel has been in continuous operation since 1875 and is both the first underground funicular and the second-oldest underground railway. The oldest funicular railway operating in Britain dates from 1875 and is in Scarborough, North Yorkshire.\n\nUntil the end of the 1870s, the four-rail parallel-track funicular was the normal configuration. Carl Roman Abt developed the Abt Switch allowing the two-rail layout, which was used for the first time in 1879 when the Giessbach Funicular opened in Switzerland. \n\nIn the United States, the first funicular to use a two-rail layout was the Telegraph Hill Railroad in San Francisco, which was in operation from 1884 until 1886. The Mount Lowe Railway in Altadena, California, was the first mountain railway in the United States to use the three-rail layout. Three- and two-rail layouts considerably reduced the space required for building a funicular, reducing grading costs on mountain slopes and property costs for urban funiculars. These layouts enabled a funicular boom in the latter half of the 19th century.\n\nIn 1880 the funicular of Mount Vesuvius inspired the song \"Funiculì, Funiculà\", composed by Luigi Denza and lyrics by Peppino Turco. Unfortunately, this funicular was wrecked repeatedly by volcanic eruptions and abandoned after the eruption of 1944.\n\nAn inclined elevator is not a funicular since it has only one car carrying payload on the slope.\n\nFor example, despite its name the Montmartre Funicular in Paris after a reconstruction in 1991 is technically a \"double inclined elevator\" since each of its two cabins have its own cable traction with own counterweight and they operate independently from each other.\n\nAccording to the Guinness World Records the smallest public funicular in the world is the Fisherman's Walk Cliff Railway in Bournemouth, England with its length of .\n\nStoosbahn in Switzerland with its maximum gradient of 110% is the steepest funicular in the world.\n\nValparaiso in Chile, used to have up to 30 \"funicular elevators\" (), the oldest dating from 1883. Only 15 of them have remained in the city. Nearly half of them are in operation, although the authorities are making some efforts to restore the others remained into a working condition.\n\nThe Carmelit in Haifa, Israel, with its 6 stations and a tunnel 1.8 km (1.1 mi) long is claimed by the Guinness World Records as the \"least extensive metro\" in the world. The Carmelit itself is never claimed to be a metro system. Technically it is an underground funicular. \n\nThe Dresden Schwebebahn is the only suspended funicular in the world, hanging from an elevated rail. \n\n\n"}
{"id": "4035183", "url": "https://en.wikipedia.org/wiki?curid=4035183", "title": "Garbage (computer science)", "text": "Garbage (computer science)\n\nIn computer science, garbage includes objects, data, or other regions of the memory of a computer system (or other system resources), which will not be used in any future computation by the system, or by a program running on it. As computer systems all have finite amounts of memory, it is frequently necessary to \"deallocate\" garbage and return it to the heap, or memory pool, so the underlying memory can be reused.\n\nGarbage is generally classified into two types: semantic garbage that is any object or data never accessed by a running program for any combination of program inputs, and syntactic garbage that refers to objects or data within a program's memory space but unreachable from the program's root set. Objects and/or data which are not garbage are said to be \"live\".\n\nCasually stated, syntactic garbage is data that \"cannot\" be reached, while semantic garbage is data that \"will not\" be reached. More precisely, syntactic garbage is data that is unreachable due to the reference graph (there is no path to it), which can be determined by many algorithms, as discussed in tracing garbage collector and only requires analyzing the data, not the code. Semantic garbage is data that will not be accessed, either because it is unreachable (hence also syntactic garbage), or reachable but will not be accessed; this latter requires analysis of the code, and is in general an undecidable problem.\n\nSyntactic garbage is a (usually strict) subset of semantic garbage as it is entirely possible for an object to hold a reference to another object without the latter object being used.\n\nIn the following simple stack implementation in Java, elements popped from the stack become semantic garbage once there are no outside references to them:\n\nThis is because there is still a reference to the object from codice_1, but the object will never be accessed again through this reference, since codice_1 is private to the class and the codice_3 method only returns references to elements it has not already popped (once codice_4 is decremented, that element will never be accessed again by this class). However, this requires analysis of the code of the class, which is undecidable in general.\n\nIf a later codice_5 call re-grows the stack to the previous size, overwriting this last reference, then the object will become syntactic garbage, since it is unreachable, and will be eligible for garbage collection.\n\nAn example of the automatic collection of syntactic garbage, by reference counting garbage collection, can be produced using the Python command-line interpreter:\n\nIn this session, an object is created, its location in the memory is displayed, and the only reference to the object is then destroyed—there is no way to ever use the object again from this point on, as there are no references to it. This becomes apparent when we try to access the original reference:\n\nAs it is impossible to refer to the object, it has become useless: the object is garbage. Since Python uses garbage collection, it automatically deallocates the memory that was used for the object so that it may be used again:\n\nNote that the Bar instance now resides at the memory location 0x54f30; at the same place as where our previous object, the Foo instance, was located. Since the Foo instance was destroyed, freeing up the memory used to contain it, the interpreter creates the Bar object at the same memory location as before, making good use of the available resources.\n\nGarbage consumes heap memory, and thus one wishes to collect it (to minimize memory use, and allow faster memory allocation and prevent out-of-memory errors by reducing heap fragmentation and memory use).\n\nHowever, collecting garbage takes time, and if done manually, requires coding overhead. Further, collecting garbage destroys objects and thus can cause calls to finalizers, executing potentially arbitrary code at an arbitrary point in the program's execution. Incorrect garbage collection (deallocating memory that is not garbage), primarily due to errors in manual garbage collection (rather than errors in garbage collectors), results in memory safety violations (often security holes) due to use of dangling pointers.\n\nSyntactic garbage can be collected automatically, and garbage collectors have been extensively studied and developed. Semantic garbage cannot be automatically collected in general, and thus cause memory leaks even in garbage-collected languages. Detecting and eliminating semantic garbage is typically done using a specialized debugging tool called a heap profiler, which allows one to see what objects are live and how they are reachable, enabling one to remove the unintended reference.\n\nThe problem of managing the deallocation of garbage is a well-known one in computer science. Several approaches are taken:\n\n\n"}
{"id": "5830550", "url": "https://en.wikipedia.org/wiki?curid=5830550", "title": "Heidemarie Stefanyshyn-Piper", "text": "Heidemarie Stefanyshyn-Piper\n\nHeidemarie Martha Stefanyshyn-Piper (born February 7, 1963) is an American Naval officer, engineer, and a former NASA astronaut. She has achieved the rank of Captain in the United States Navy. She is also a qualified and experienced salvage officer. Her major salvage projects include de-stranding the tanker \"Exxon Houston\" off the coast of Barbers Point, on the island of Oahu, Hawaii, and developing the plan for the Peruvian Navy salvage of the Peruvian submarine .\n\nStefanyshyn-Piper has received numerous honors and awards, such as the Meritorious Service Medal, two Navy Commendation Medals, and two Navy Achievement Medals. She has flown on two Space Shuttle missions, STS-115 and STS-126, during which she completed five spacewalks totaling 33 hours and 42 minutes. As of 2018, she ranks 39th on the all-time list of space walkers by duration.\n\nStefanyshyn-Piper was born in St. Paul, Minnesota, United States, of Ukrainian-American heritage. Her father, Michael (Mykhailo) Stefanyshyn, now deceased, was born in the Halychyna region of Ukraine, and sent to work in Germany during World War II. After the end of the war, he married a German woman and they both immigrated to the U.S. As of 2008, Stefanyshyn-Piper's mother, Adelheid Stefanyshyn, still lived in St. Paul. Stefanyshyn-Piper was raised in the Ukrainian cultural community of Minneapolis–Saint Paul, is a member of Plast – a Ukrainian scouting organization, and speaks Ukrainian.\n\nStefanyshyn-Piper graduated in 1980 from what was then the all-girls Derham Hall High School in St. Paul, Minnesota, and holds Bachelor of Science (1984) and Master of Science (1985) degrees in mechanical engineering from the Massachusetts Institute of Technology (MIT). She is a licensed ham radio operator with Technician License KD5TVR.\n\nStefanyshyn-Piper married Glenn A. Piper, and they have one son, Michael, named after Piper's grandfather. Stefanyshyn-Piper hyphenated her surname after marriage to serve as a reminder of her family roots.\n\nStefanyshyn-Piper received her commission from the Naval ROTC Program at MIT in June 1985. She completed training at the Naval Diving and Salvage Training Center in Panama City, Florida as a Navy Basic Diving Officer and Salvage Officer. During her Salvage tour, she participated in the de-stranding of the tanker \"Exxon Houston\" off the coast of Barbers Point in Hawaii. Stefanyshyn-Piper is currently a Captain in the United States Navy.\n\nDuring her military career she was awarded: the Defense Superior Service Medal, two Legion of Merit medals, the Defense Meritorious Service Medal, the Meritorious Service Medal, two Navy Commendation Medals, two Navy Achievement Medals, and other service medals.\n\nSelected as an astronaut candidate by NASA in April 1996, Stefanyshyn-Piper reported to the Johnson Space Center in August 1996. After two years of training and evaluation, she qualified for flight assignment as a mission specialist. Initially assigned to astronaut support duties for launch and landing, she has also served as lead Astronaut Office Representative for Payloads and in the Astronaut Office EVA branch.\n\nStefanyshyn-Piper flew her first mission on STS-115, aboard Space Shuttle \"Atlantis\" (launched September 9, 2006, and returned September 21), as a Mission Specialist and became only the 8th woman to perform a spacewalk (out of 180 total spacewalkers). Stefanyshyn-Piper participated in two of the mission's three EVAs for a total of 12 hours, 8 minutes made her the second most experienced female spacewalker. She also became the first Minnesota woman to go into space.\n\nDuring her pre-flight interview, she described her philosophy about human exploration of space:\n\nPiper fainted twice during the STS-115 welcome home ceremony.\n\nStefanyshyn-Piper was commander of the 12th expedition of NASA Extreme Environment Mission Operations (NEEMO), a NASA program for studying human survival in the Aquarius underwater laboratory in preparation for future space exploration.\n\nStefanyshyn-Piper flew as a Mission Specialist on STS-126, aboard Space Shuttle \"Endeavour\" (launched November 14, 2008, and returned November 30), during which she participated in and was \"Lead Spacewalker\" on three of four spacewalks. The mission ended when \"Endeavour\" landed successfully at Edwards Air Force Base, California. Following Stefanyshyn-Piper's third spacewalk during STS-126, her fifth overall, her total time in EVA became 33 hours, 42 minutes, putting her in twenty-fifth place for total time in EVA.\n\nDuring the first EVA of STS-126 on November 18, 2008, as Stefanyshyn-Piper was preparing to begin work on the Solar Alpha Rotary Joint, she noticed a significant amount of grease in her tool bag. \"I think we had a grease gun explode in the large bag, because there's grease in the bag,\" Stefanyshyn-Piper reported to Kimbrough, who was working inside the shuttle to help coordinate the EVA. Mission Control managers instructed Stefanyshyn-Piper to clean up the grease using a dry wipe, and while she was doing the cleanup, she accidentally pushed aside the bag. \"I guess one of my crew lock bags was not transferred and it's loose,\" Stefanyshyn-Piper told Kimbrough. The bag floated aft and starboard of the station, and did not pose a risk to the station or orbiter. The bag and its contents entered Low Earth Orbit as space debris, where it eventually burned-up as it entered the Earth's atmosphere west of Mexico on August 3, 2009. When in orbit, it was visible from the ground using a telescope.\n\nAfter taking an inventory of the items inside the lost bag, managers on the ground determined that Bowen had all those items in his bag, and the two could share equipment. While it extended the EVA duration slightly, the major objectives were not changed. The estimated value of the equipment lost is US$100,000.\n\nDuring the Mission Status Briefing, lead International Space Station Flight Director Ginger Kerrick said that there was no way to know what caused the bag to come loose. \"We don't know that this incident occurred because they forgot to tether something. We don't know if perhaps the hook just came loose inside the bag,\" Kerrick said. \"You've got to remember, we are working with humans here and we are prone to human error. We do the best we can, and we learn from our mistakes.\" Said Stefanyshyn-Piper of the incident, \"that definitely was not the high point of the EVA. It was very disheartening to watch it float away.\"\n\nIn July 2009, Stefanyshyn-Piper retired from NASA's Astronaut Corps to return to her Navy duties.\n\nFellow astronaut Steven Lindsey, Chief of the Astronaut Office at the Johnson Space Center in Houston, stated on her retirement: \"Heide has been an outstanding astronaut, contributing significantly to the Space Shuttle and Space Station programs. In particular, her superb leadership as lead spacewalker during the STS-126 mission resulted in restoring full power generation capability to the International Space Station. We wish her the best of luck back in the Navy – she will be missed.\"\nShe was awarded two NASA Space Flight Medals, and the NASA Exceptional Service Medal.\n\nIn August 2009, Stefanyshyn-Piper reported to the Naval Sea Systems Command as the Chief Technology Officer.\n\nOn May 20, 2011 Captain Stefanyshyn-Piper became commander of the Carderock Division of the Naval Surface Warfare Center in Maryland. In July 2013, Captain Stefanyshyn-Piper assumed command of Southwest Regional Maintenance Center in San Diego, California, responsible for all non-nuclear ship maintenance in the San Diego area. She retired from the U.S. Navy after 30 years of active duty service on July 1, 2015.\n\nShe and her husband currently reside in Oak Harbor in Washington state.\n\n"}
{"id": "9719835", "url": "https://en.wikipedia.org/wiki?curid=9719835", "title": "History of cycling", "text": "History of cycling\n\nCycling quickly became an activity after bicycles were introduced in the 19th century and remains popular with more than a billion people worldwide used for recreation, transportation and sport.\n\nThe first documented cycling race was a 1,200 metre race held on May 31, 1868 at the Park of Saint-Cloud, Paris. It was won by expatriate Englishman James Moore who rode a bicycle with solid rubber tires. The first cycle race covering a distance between two cities was Paris–Rouen, also won by James Moore, who rode the 123 kilometres dividing both cities in 10 hours and 40 minutes.\n\nThe oldest established bicycle racing club in the United States is the St. Louis Cycling Club. Operating continuously since 1887, the club has sponsored races and timed distance events since its inception. Its members have included numerous national champions and Olympic team members.\n\nCycling as recreation became organized shortly after racing did. In its early days, cycling brought the sexes together in an unchaperoned way, particularly after the 1880s when cycling became more accessible owing to the invention of the Rover Safety bicycle. Public cries of alarm at the prospect of moral chaos arose from this and from the evolution of women’s cycling attire, which grew progressively less enveloping and restrictive.\n\nOn 4 March 1915 the society for the construction of cycle paths in the Gooi and Eemland region in the Netherlands was founded. It is the last private “Cycle Path Society” that still exists today. Some people thought the increasing amount of motor traffic in the early 20th century was so dangerous for people cycling, especially those who rode as a leisure activity, that they wanted separate cycling infrastructure to be built. The routes would also not be connected to a route for motor traffic and mainly for recreation - so not the shortest routes, but the nicest routes.\n\nToday we see a resurgence of recreational cycling. We can see this with many companies meeting the demand for this trend and specialising in retro/vintage style bikes that originated in the Netherlands.\n\nPeople have been riding bicycles to work since the initial bicycle heyday of the 1890s. According to the website Bike to Work, this practice continued in the United States until the 1920s, when biking experienced a sharp drop, in part due to the growth of suburbs and the popularity of the car. In Europe, cycling to work continued to be common until the end of the 1950s.\n\nToday many people ride bikes to work for a variety of reasons including fitness, environmental concerns, convenience, frugality, and enjoyment. According to the US Census Bureau’s 2008 American Community Survey(ACS), on September 22, 2009, 0.55 percent of Americans use a bicycle as the primary means of getting to work. Some places of employment offer amenities to bike commuters, such as showers, changing rooms, indoor bike racks and other secure bike parking.\n\nMany cyclists wanted to use their machines to travel; some of them went around the world. Annie Londonderry did so in the 1880s, taking 15 months. Six Indian men cycled 71000 km around the world in the 1920s.\n\nWith four key aspects (steering, safety, comfort and speed) improved over the penny-farthing, bicycles became very popular among elites and the middle classes in Europe and North America in the middle and late 1890s. It was the first bicycle that was suitable for women, and as such became the \"freedom machine\" (as American feminist Susan B. Anthony called it), giving women \"a feeling of freedom and self-reliance\".\n\nThe Svea Velocipede with vertical pedal arrangement and locking hubs was introduced in 1892 by the Swedish engineers Fredrik Ljungström and Birger Ljungström. It attracted attention at the World Fair and was produced in a few thousand units.\n\nBicycle historians often call this period the \"golden age\" or \"bicycle craze.\" By the start of the 20th century, cycling had become an important means of transportation, and in the United States an increasingly popular form of recreation. Bicycling clubs for men and women spread across the U.S. and across European countries. Chicago immigrant Adolph Schoeninger with his Western Wheel Works became the \"Ford of the Bicycle\" (ten years before Henry Ford) by copying Pope's mass production methods and by introducing stamping to the production process in place of machining, significantly reducing production costs, and thus prices. His \"Crescent\" bicycles thus became affordable for working people, and massive exports from the United States lowered prices in Europe. The Panic of 1893 wiped out many American manufacturers who had not followed the lead of Pope and Schoeninger, in the same way as the Great Depression would ruin car makers who did not follow Ford.\n\nThe impact of the bicycle on female emancipation should not be underestimated. The safety bicycle gave women unprecedented mobility, contributing to their larger participation in the lives of Western nations. As bicycles became safer and cheaper, more women had access to the personal freedom they embodied, and so the bicycle came to symbolise the New Woman of the late nineteenth century, especially in Britain and the United States. Feminists and suffragists recognised its transformative power. Susan B. Anthony said, \"Let me tell you what I think of bicycling. I think it has done more to emancipate women than anything else in the world. It gives women a feeling of freedom and self-reliance. I stand and rejoice every time I see a woman ride by on a wheel...the picture of free, untrammeled womanhood.\" In 1895 Frances Willard, the tightly laced president of the Women’s Christian Temperance Union, wrote a book called \"How I Learned to Ride the Bicycle\" (described in \"Bicycling\" magazine as \"the greatest book ever written on learning to ride\"), in which she praised the bicycle she learned to ride late in life, and which she named \"Gladys\", for its \"gladdening effect\" on her health and political optimism. Willard used a cycling metaphor to urge other suffragists to action, proclaiming, \"I would not waste my life in friction when it could be turned into momentum.\" Elizabeth Robins Pennell started cycling in the 1870s in Philadelphia, and from the 1880s onwards brought out a series of travelogues about her cycling journeys around Europe, from \"A Canterbury Pilgrimage\" to \"Over the Alps on a Bicycle\". In 1895 Annie Londonderry became the first woman to bicycle around the world.\n\nThe backlash against the New (bicycling) Woman was demonstrated when the male undergraduates of Cambridge University chose to show their opposition to the admission of women as full members of the university by hanging a woman in effigy in the main town square—tellingly, a woman on a bicycle—as late as 1897.\n\nSince women could not cycle in the then-current fashions for voluminous and restrictive dress, the bicycle craze fed into a movement for so-called rational dress, which helped liberate women from corsets and ankle-length skirts and other encumbering garments, substituting the then-shocking bloomers.\n\n"}
{"id": "237495", "url": "https://en.wikipedia.org/wiki?curid=237495", "title": "Information system", "text": "Information system\n\nInformation systems (IS) are formal, sociotechnical, organizational systems designed to collect, process, store, and distribute information.\nIn a sociotechnical perspective Information Systems are composed by four components: technology, process, people and organizational structure. \n\nA computer information system is a system that a branch of Science composed of people and computers that processes or interprets information.\nThe term is also sometimes used in more restricted senses to refer to only the software used to run a computerized database or to refer to only a computer system.\n\nInformation Systems is an academic study of systems with a specific reference to information and the complementary networks of hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks.\n\nAny specific information system aims to support operations, management and decision-making. An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes.\n\nSome authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes.\n\nAlter argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information.\n\nAs such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action.\n\nInformation systems are the primary focus of study for organizational informatics.\n\nSilver et al. (1995) provided two views on IS that includes software, hardware, data, people, and procedures. Zheng provided another system view of information system which also adds processes and essential system elements like environment, boundary, purpose, and interactions.\nThe Association for Computing Machinery defines \"Information systems specialists [as] focus[ing] on integrating information technology solutions and business processes to meet the information needs of businesses and other enterprises.\"\n\nThere are various types of information systems, for example: transaction processing systems, decision support systems, knowledge management systems, learning management systems, database management systems, and office information systems. Critical to most information systems are information technologies, which are typically designed to enable humans to perform tasks for which the human brain is not well suited, such as: handling large amounts of information, performing complex calculations, and controlling many simultaneous processes.\n\nInformation technologies are a very important and malleable resource available to executives. Many companies have created a position of chief information officer (CIO) that sits on the executive board with the chief executive officer (CEO), chief financial officer (CFO), chief operating officer (COO), and chief technical officer (CTO). The CTO may also serve as CIO, and vice versa. The chief information security officer (CISO) focuses on information security management.\n\nThe six components that must come together in order to produce an information system are: (Information systems are organizational procedures and do not need a computer or software, this data is erroneous) (IE, an accounting system in the 1400s using ledger and ink utilizes an information system)\n\nData is the bridge between hardware and people. This means that the data we collect is only data until we involve people. At that point, data is now information.\n\nThe \"classic\" view of Information systems found in the textbooks in the 1980s was a pyramid of systems that reflected the hierarchy of the organization, usually transaction processing systems at the bottom of the pyramid, followed by management information systems, decision support systems, and ending with executive information systems at the top. Although the pyramid model remains useful since it was first formulated, a number of new technologies have been developed and new categories of information systems have emerged, some of which no longer fit easily into the original pyramid model.\n\nSome examples of such systems are:\n\nA computer(-based) information system is essentially an IS using computer technology to carry out some or all of its planned tasks. The basic components of computer-based information systems are:\n\nThe first four components (hardware, software, database, and network) make up what is known as the information technology platform.\nInformation technology workers could then use these components to create information systems that watch over safety measures, risk and the management of data. These actions are known as information technology services.\n\nCertain information systems support parts of organizations, others support entire organizations, and still others, support groups of organizations. Recall that each department or functional area within an organization has its own collection of application programs or information systems. These functional area information systems (FAIS) are supporting pillars for more general IS namely, business intelligence systems and dashboards . As the name suggest, each FAIS support a particular function within the organization, e.g.: accounting IS, finance IS, production-operation management (POM) IS, marketing IS, and human resources IS. In finance and accounting, managers use IT systems to forecast revenues and business activity, to determine the best sources and uses of funds, and to perform audits to ensure that the organization is fundamentally sound and that all financial reports and documents are accurate. Other types of organizational information systems are FAIS, Transaction processing systems, enterprise resource planning, office automation system, management information system, decision support system, expert system, executive dashboard, supply chain management system, and electronic commerce system. Dashboards are a special form of IS that support all managers of the organization. They provide rapid access to timely information and direct access to structured information in the form of reports. Expert systems attempt to duplicate the work of human experts by applying reasoning capabilities, knowledge, and expertise within a specific domain.\n\nInformation technology departments in larger organizations tend to strongly influence the development, use, and application of information technology in the organizations.\nA series of methodologies and processes can be used to develop and use an information system. Many developers use a systems engineering approach such as the system development life cycle (SDLC), to systematically develop an information system in stages. The stages of the system development lifecycle are planning, system analysis and requirements, system design, development, integration and testing, implementation and operations and maintenance. \nRecent research aims at enabling and measuring the ongoing, collective development of such systems within an organization by the entirety of human actors themselves.\nAn information system can be developed in house (within the organization) or outsourced. This can be accomplished by outsourcing certain components or the entire system. A specific case is the geographical distribution of the development team (offshoring, global information system).\n\nA computer-based information system, following a definition of Langefors, is a technologically implemented medium for:\nGeographic information systems, land information systems, and disaster information systems are examples of emerging information systems, but they can be broadly considered as spatial information systems.\nSystem development is done in stages which include:\n\nThe field of study called \"information systems\" encompasses a variety of topics including systems analysis and design, computer networking, information security, database management and decision support systems. \"Information management\" deals with the practical and theoretical problems of collecting and analyzing information in a business function area including business productivity tools, applications programming and implementation, electronic commerce, digital media production, data mining, and decision support. \"Communications and networking\" deals with the telecommunication technologies.\nInformation systems bridges business and computer science using the theoretical foundations of information and computation to study various business models and related algorithmic processes on building the IT systems within a computer science discipline. Computer information system(s) (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society, whereas IS emphasizes functionality over design.\n\nSeveral IS scholars have debated the nature and foundations of Information Systems which have its roots in other reference disciplines such as Computer Science, Engineering, Mathematics, Management Science, Cybernetics, and others. Information systems also can be defined as a collection of hardware, software, data, people and procedures that work together to produce quality information.\n\nSimilar to computer science, other disciplines can be seen as both related and foundation disciplines of IS. The domain of study of IS involves the study of theories and practices related to the social and technological phenomena, which determine the development, use, and effects of information systems in organization and society. But, while there may be considerable overlap of the disciplines at the boundaries, the disciplines are still differentiated by the focus, purpose, and orientation of their activities.\n\nIn a broad scope, the term \"Information Systems\" is a scientific field of study that addresses the range of strategic, managerial, and operational activities involved in the gathering, processing, storing, distributing, and use of information and its associated technologies in society and organizations. The term information systems is also used to describe an organizational function that applies IS knowledge in industry, government agencies, and not-for-profit organizations. \"Information Systems\" often refers to the interaction between algorithmic processes and technology. This interaction can occur within or across organizational boundaries. An information system is the technology an organization uses and also the way in which the organizations interact with the technology and the way in which the technology works with the organization’s business processes. Information systems are distinct from information technology (IT) in that an information system has an information technology component that interacts with the processes' components.\n\nOne problem with that approach is that it prevents the IS field from being interested in non-organizational use of ICT, such as in social networking, computer gaming, mobile personal usage, etc. A different way of differentiating the IS field from its neighbours is to ask, \"Which aspects of reality are most meaningful in the IS field and other fields?\" This approach, based on philosophy, helps to define not just the focus, purpose and orientation, but also the dignity, destiny and, responsibility of the field among other fields.\n\"International Journal of Information Management\", 30, 13-20.\n\nInformation Systems workers enter a number of different careers:\n\nThere is a wide variety of career paths in the information systems discipline. \"Workers with specialized technical knowledge and strong communications skills will have the best prospects. Workers with management skills and an understanding of business practices and principles will have excellent opportunities, as companies are increasingly looking to technology to drive their revenue.\"\n\nInformation technology is important to the operation of contemporary businesses, it offers many employment opportunities. The information systems field includes the people in organizations who design and build information systems, the people who use those systems, and the people responsible for managing those systems.\nThe demand for traditional IT staff such as programmers, business analysts, systems analysts, and designer is significant. Many well-paid jobs exist in areas of Information technology. At the top of the list is the chief information officer (CIO).\n\nThe CIO is the executive who is in charge of the IS function. In most organizations, the CIO works with the chief executive officer (CEO), the chief financial officer (CFO), and other senior executives. Therefore, he or she actively participates in the organization's strategic planning process.\n\nInformation systems research is generally interdisciplinary concerned with the study of the effects of information systems on the behaviour of individuals, groups, and organizations. Hevner et al. (2004) categorized research in IS into two scientific paradigms including \"behavioural science\" which is to develop and verify theories that explain or predict human or organizational behavior and\" design science\" which extends the boundaries of human and organizational capabilities by creating new and innovative artifacts.\n\nSalvatore March and Gerald Smith proposed a framework for researching different aspects of Information Technology including outputs of the research (research outputs) and activities to carry out this research (research activities). They identified research outputs as follows:\n\n\nAlso research activities including:\n\nAlthough Information Systems as a discipline has been evolving for over 30 years now, the core focus or identity of IS research is still subject to debate among scholars. There are two main views around this debate: a narrow view focusing on the IT artifact as the core subject matter of IS research, and a broad view that focuses on the interplay between social and technical aspects of IT that is embedded into a dynamic evolving context. A third view calls on IS scholars to pay balanced attention to both the IT artifact and its context.\n\nSince the study of information systems is an applied field, industry practitioners expect information systems research to generate findings that are immediately applicable in practice. This is not always the case however, as information systems researchers often explore behavioral issues in much more depth than practitioners would expect them to do. This may render information systems research results difficult to understand, and has led to criticism.\n\nIn the last ten years, the business trend is represented by the considerable increasing of Information Systems Function (ISF) role, especially with regard the enterprise strategies and operations supporting. It became a key-factor to increase productivity and to support new value creation. To study an information system itself, rather than its effects, information systems models are used, such as EATPUT.\n\nThe international body of Information Systems researchers, the Association for Information Systems (AIS), and its Senior Scholars Forum Subcommittee on Journals (23 April 2007), proposed a 'basket' of journals that the AIS deems as 'excellent', and nominated: \"Management Information Systems Quarterly\" (MISQ), \"Information Systems Research\" (ISR), \"Journal of the Association for Information Systems\" (JAIS), \"Journal of Management Information Systems\" (JMIS), \"European Journal of Information Systems\" (EJIS), and \"Information Systems Journal\" (ISJ).\n\nA number of annual are run in various parts of the world, the majority of which are peer reviewed. The AIS directly runs the International Conference on Information Systems (ICIS) and the Americas Conference on Information Systems (AMCIS), while AIS affiliated conferences include the Pacific Asia Conference on Information Systems (PACIS), European Conference on Information Systems (ECIS), the Mediterranean Conference on Information Systems (MCIS), the International Conference on Information Resources Management (Conf-IRM) and the Wuhan International Conference on E-Business (WHICEB). AIS chapter conferences include Australasian Conference on Information Systems (ACIS), Information Systems Research Conference in Scandinavia (IRIS), Information Systems International Conference (ISICO), Conference of the Italian Chapter of AIS (itAIS), Annual Mid-Western AIS Conference (MWAIS) and Annual Conference of the Southern AIS (SAIS). EDSIG, which is the special interest group on education of the AITP, organizes the Conference on Information Systems and Computing Education and the Conference on Information Systems Applied Research which are both held annually in November.\n\n\n\n\n\n"}
{"id": "39262126", "url": "https://en.wikipedia.org/wiki?curid=39262126", "title": "Interspiro DCSC", "text": "Interspiro DCSC\n\nThe Interspiro DCSC is a semi-closed circuit nitrox rebreather manufactured by Interspiro of Sweden for military applications.\nInterspiro was formerly a division of AGA and has been manufacturing self-contained breathing apparatus for diving, firefighting and rescue applications since the 1950s.\n\nThe first Interspiro rebreather was the ACSC - the alternating closed and semi-closed circuit rebreather which was developed and marketed in the 1980s. In the 1990s this design was developed further to become the DCSC, also intended for mine countermeasures.\n\nGas supply is carried in a 5l 200bar aluminium cylinder mounted horizontally at the bottom of the unit with the valve to the diver's left. The reserve valve and bypass valve are also on the left.\n\nThe fairing case holding the components is clipped to the tubular harness frame and can be released by pulling a knob on the lower right.\n\nThe scrubber is a radial flow cylindrical design, with inward flow. It carries a 2.5 kg charge of absorbent.\n\nThe counterlung is a wedge shaped bellows, hinged on the lower edge, and the angle between the top and bottom covers is proportional to the internal volume. The change in top plate angle, as the diver breathes, controls the gas addition mechanism.\n\nThe top plate of the bellows is ballasted, so that the lifting force of the air inside is balanced by the weights: when the diver is trimmed horizontally, face down, the weights create a slight positive pressure relative to ambient. This compensates for the depth difference between the counterlung and the diver's lungs, reducing the effort required to breathe. When the diver is upright the effect of the weights is cancelled as the weight is carried by the hinge, and when the diver is horizontal, face up, the weight causes a slight negative pressure in the bellows, which compensates for the increased hydrostatic pressure on the counterlung compared with the lungs.\n\nThe dump valve for the loop and also functions as a drain for water. The counterlung is in the exhalation side of the loop. Water from condensate and leakage is trapped in the bellows before it can reach the scrubber, and can be discharged through the exhaust valve for the loop, which is mounted on the lower plate of the bellows.\n\nVolume of the bellows is about 4.5 litres, and total loop volume is about 7 litres.\n\nGas circulation: Exhalation hose to the right, inhalation from the left.\n\nApproved operating depth range is from 0 to 57m. Nitrox 28% is used for depths below about 30m. and 46% for shallower depths.\n\n\nThe DCSC is an active addition semi-closed circuit rebreather, but has more in common with the passive addition systems, in that the amount of feed gas supplied is a function of the breathing rate of the diver. Unlike most passive addition rebreathers, the gas feed mass flow rate is independent of depth, and unlike most active addition systems, it is not constant mass flow.\n\nThe Interspiro DCSC is the only rebreather using this gas mixture control principle that has been marketed. \nThe principle of operation is to add a mass of oxygen that is proportional to the volume of each breath. This approach is based on the assumption that the volumetric breathing rate of a diver is directly proportional to metabolic oxygen consumption, which experimental evidence indicates is close enough to work.\nThe fresh gas addition is made by controlling the pressure in a dosage chamber proportional to the counterlung bellows volume. The dosage chamber is filled with fresh gas to a pressure proportional to bellows volume, with the highest pressure when the bellows is in the empty position. When the bellows fills during exhalation, the gas is released from the dosage chamber into the breathing circuit, proportional to the volume in the bellows during exhalation, and is fully released when the bellows is full. Excess gas is dumped to the environment through the overpressure valve after the bellows is full.\n\nThe result is the addition of a mass of gas proportional to ventilation volume.\n\nThe volume of the dosage chamber is matched to a specific supply gas mixture, and is changed when the gas is changed. The DCSC uses two standard mixtures of nitrox: 28% and 46%, and has two corresponding dosage chambers.\n\nThe DCSC controls the feed gas pressure in the dosage chamber by changes of bellows angle, which is proportional to the change in volume in the loop. A mechanical linkage connects the bellows cover plate to an oscillating cam which controls loading of the diaphragm spring. The spring force controls a diaphragm in the dosage regulator which actuates the inlet and outlet valves.\n\nExhalation will increase of bellows angle and will increase loading on the control spring, pushing the dosage inlet valve open and allowing gas to flow into the dosage chamber until the increased pressure lifts the diaphragm and closes the valve again.\n\nInhalation will decrease the bellows angle, which reduces the spring loading, and the internal pressure in the dosage chamber will lift the diaphragm against the spring, opening the dosage outlet valve and allowing the gas to flow into the breathing circuit until the pressure in the dosage chamber is matched by the spring force, and the diaphragm is pushed back against the outlet valve to close it.\n\nThe feed gas is supplied by a depth compensated first stage regulator which takes gas from the cylinder and reduces the pressure to 3 bar above ambient pressure.\nA linkage connected to the bellows rotates a cam against the control spring in the dosage regulator, to adjust the spring force on the dosage regulator diaphragm.\n\nIf the gas supply to the dosage mechanism were to fail without warning, the gas addition would stop and the diver would use up the oxygen in the loop gas until it became hypoxic and the diver lost consciousness. To prevent this, there is a controllable flow restriction in the inhalation side of the loop, which is operated by pressure from the supply gas in the dosage mechanism. This is open when there is suitable operating pressure in the dosage mechanism, but if this falls, the flow warning system imposes a restriction to the inhalation gas flow, similar to the effect of a low supply pressure on an open circuit demand valve, which warns the diver that there is a feed gas supply failure. The diver can then activate the reserve mechanism on the cylinder valve, which allows the last 25 bar from the cylinder to be used, which will de-activate the warning restriction. If the gas supply remains inadequate, the diver must take other action, such as bailing out to an independent open circuit gas supply.\n\nThe gas calculation differs from other semi-closed circuit rebreathers.\nA diver with a constant workload during aerobic working conditions will use an approximately constant amount of oxygen formula_1 as a fraction of the respiratory minute volume formula_2. This ratio of minute ventilation and oxygen uptake is the extraction ratio formula_3, and usually falls in the range of 17 to 25 with a normal value of about 20 for healthy humans. Values as low as 10 and as high as 30 have been measured. Variations may be caused by the diet of the diver and the dead space of the diver and equipment, raised levels of carbon dioxide, or raised work of breathing and tolerance to carbon dioxide.\n\nTherefore, the respiratory minute volume may be expressed as a function of the extraction ratio and oxygen uptake:\n\nThe volume of gas in the breathing circuit can be described as approximately constant, and the fresh gas addition must balance the sum of the dumped volume, the metabolically removed oxygen, and the volume change due to depth change. (metabolic carbon dioxide added to the mixture is removed by the scrubber and therefore does not affect the equation)\n\nOxygen partial pressure in the DCSC is controlled by the flow rate of feed gas through the dosage regulator and the oxygen consumption of the diver. Dump rate is equal to feed rate minus oxygen consumption for this case.\n\nThe change in the fraction of oxygen formula_6 in the breathing circuit may be described by the following equation:\n\nWhere:\n\nThis leads to the differential equation:\n\nWith solution:\n\nWhich comprises a steady state and a transient term.\n\nThe steady state term is sufficient for most calculations:\n\nThe steady state oxygen fraction in the breathing circuit, formula_14, can be calculated from the formula:\n\nWhere:\nin a consistent system of units.\n\nAs oxygen consumption is an independent variable, a fixed feed rate will give a range of possible oxygen fractions for any given depth. In the interests of safety, the range can be determined by calculating oxygen fraction for maximum and minimum oxygen consumption as well as the expected rate.\n\nFeed gas flow is a function of respiratory minute volume at surface pressure and the dosage ratio based on the dosage chamber volume. The values for dosage ratio are 60% for the large chamber and 30% for the small chamber.\n\nSubstitution of the first equation into this yields:\n\nThis may be substituted into the steady state term to give:\n\nWhich simplifies to:\n\nThis shows that there is no dependency depth or on oxygen uptake, and since the dosage ratio is constant once the gas has been selected, it is clear that the variations remaining are due to variations in the extraction ratio. This means that the DCSC has theoretically the most stable oxygen fraction of the semi-closed rebreathers and is a reasonable approximation of open circuit for decompression purposes. The unit has been used by the Swedish armed forces for over 15 years with a good safety record. However a large decompression stress when using air tables for decompression on dives using a 28% nitrox supply gas has been indicated by the presence of high venous gas emboli (VGE) scores post-dive. Oxygen fraction in the loop was not monitored during these tests.\n\nThe reserve valve is activated at about 25 bar. A 5-litre cylinder at 200 bar will provide about (200-25)*5 litres = 875 free gas at 1 bar available for the dive. A RMV of 30 l/min for a diver working moderately hard, using the 28% nitrox with a dosage ratio of 0,6 will use the gas in 875/(30*0.6) = 48 min. The 46% nitrox with a dosage ratio of 0.3 will last 875/(30*0.3) = 97 min. A 15 l/min RMV for light work will double these times.\n\nThe scrubber capacity is 2.5 kg of soda lime. If a conservative value of 100 litres CO per kg is used, the capacity of the scrubber will be 2.5*100 = 250 litres CO. At an extraction rate of 1/20 and a dosage rate of 0.3, some 875/0.3*1/20 = 146 litres of carbon dioxide may be produced by the diver, showing that endurance is not limited by the scrubber.\n\n"}
{"id": "26619546", "url": "https://en.wikipedia.org/wiki?curid=26619546", "title": "LN-3 inertial navigation system", "text": "LN-3 inertial navigation system\n\nThe LN-3 inertial navigation system is an inertial navigation system (INS) that was developed in the 1960s by Litton Industries. It equipped the Lockheed F-104 Starfighter versions used as strike aircraft in European forces. An inertial navigation system is a system which continually determines the position of a vehicle from measurements made entirely within the vehicle using sensitive instruments. These instruments are accelerometers which detect and measure vehicle accelerations, and gyroscopes which act to hold the accelerometers in proper orientation.\n\nThe development of Litton's first INS was the result of a collusive act by the engineer, Max Lipscomb of the Wright Air Force Base in Ohio and Dr. Henry E. Singleton, Head of the newly formed Guidance and Control Dept. of Litton Industries at Beverly Hills, California.\n\nLipscomb's department was not permitted to engage in development of navigation systems, but was permitted to engage in other aircraft avionics such as pitch, roll, and yaw indicators. Singleton proposed to provide a system that would provide highly accurate pitch, roll, and yaw indicators. The system would be a north seeking stable platform controlled by gyroscopes and accelerometers. Such a system would automatically provide velocities in the East-West and North-South direction. And later, by providing integrators for these two axes, one would then have a full-fledged Inertial Navigation System.\n\nIn about mid-1956 a contract for approximately $300,000 was awarded by Wright Air Force Base to Litton Industries for the development of such \"Aircraft Attitude System.\" Singleton appointed Sidney Shapiro as Project Engineer for this program. The system was completed and ready for flight test by the end of 1958.\n\nMr. Shapiro selected Paul Mantz, a partner in Tallmantz Aviation, to supply the aircraft, principally because of Mantz's extensive experience with the movie industry. They had done their work on several Cinerama travelogs. Mantz's people had also recently finished work on the picture \"North by Northwest\" starring Cary Grant in which there was considerable stunt flying. Shapiro's idea was to photograph the ground periodically and at the same instant to photograph the Inertial Navigation System's output. In that way no possibility of finger pointing was possible since none of Shapiro's people were involved in the data taking. So the two extra integrators were installed and the system was ready for test by early 1959.\n\nBy 1959 things had gone well enough that Shapiro was able to obtain three successive flights in which the accuracies were substantially better than one mile an hour. On the basis of these results, Litton Industries was awarded a contract to provide 2000 systems for the F104 NATO Fighter Aircraft.\n\nThe Cold War missile race spurred the development of smaller, lighter and more accurate inertial systems. Independent of its environment, the inertial system provides velocity and position information accurately and instantaneously for all manoeuvres, as well as being an accurate attitude and heading reference. The LN3-2A was the first inertial navigation system small and light and accurate enough to be fitted in a high performance fighter.\n\nThe early F-104's, model A through F, did not have an Inertial Navigator. It was the development of the F-104G, around 1959, for the European Air Forces with tactical bomber/strike capabilities, that brought the LN-3 into the aircraft. The LN-3 gave the F-104G the capability to navigate at low level in adverse weather and to drop a nuclear weapon at a range of 1,000 km with the best possible precision; this being vital to the F-104G program.\n\nThe LN-3 is a full 3-degrees-of-freedom, 4-gimbal inertial navigator, covering the flight performance envelope of the F-104G which ranged from 0 to 70,000 feet altitude; 0 to Mach 2+ speed, and accelerations from −5 to +9 g.\n\nThe functional description of the LN3-2A requires some knowledge of some basic principles of inertial navigation to understand their application to the LN3-2A. The principal component of the system is the stable platform to which are mounted three accelerometers and two gyros. This stable platform is mounted in a system of platform gimbals. The acceleration of the airplane in any plane or direction is measured by the accelerometers and integrated in the computer to obtain velocity. Velocities in turn are integrated to obtain distance. With a known reference point representing initial position of the airplane with respect to earth, this data can be converted to distance and heading traveled, and distance and bearing to destination.\n\nThe following characteristics of the \"platform\" are described:\n\n\nThe LN3-2A \"computer\" controls the platform, computes navigational information and provides special AC and DC voltages required for equipment operation.\n\nThe functions of the computer are:\n\n\nBefore starting the Inertial navigator, the pilot has to enter the coordinates of the starting point in the \"Align Control\" panel in the right-hand console of the F-104G.\nThe first selection in the starting sequence is to rotate the mode selector switch of the \"Inertial Navigation Control\" Panel from \"Off\" to \"Standby\".\n\nIn this mode the platform and component oven are brought up to operating temperature; indicated by the \"heat\" light on the IN Control Panel, which takes several minutes depending on outside and system temperatures.\n\nAll at operating temperature the system may be switched to \"\"Align\", allowing the machine to commence operation. The computer is powered up and nulls its velocity shafts; the gyros are powered by 115 V and 400 Hz and revving up; the platform is levelled in pitch, inner and outer roll relative to the aircraft using the gimbal synchrotransmitters; and the azimuth axis is driven to the grid north direction using the magnetic heading sensor. This phase of Alignment takes 1 minute and is called coarse align.\n\nAfter this 1 minute the system switches to the \"fine align phase\", during which the gyro spin motor power is brought down to 95 V and 375 Hz to avoid any magnetic interference with any other aircraft system using 400 Hz. The levelling of the platform is taken over by the X and Y accelerometers sensing even the smallest component of gravity which is an indication of not being precisely level. The levelling of the stable element is achieved by torquing the respective gyro torquers which makes the gimbal motors to follow up and level the stable element. The distance shafts are set to zero; the gyros are at operational speed and the computer is continuously feeding the gyros, and thereby the stable element, with corrections for local earth rotation. This is called the \"levelling phase of fine align\".\n\nLeveling ends automatically when the computer decides that the platform stable element is exactly locally level, which may take a few minutes. If level, the final phase of alignment is switched on; \"gyrocompassing\".\nThe stable element is exactly level and Schuler-tuned but the gyros are not yet aligned with the earth rotation axis. Therefore, the stable element tends to turn off-level, which is sensed by the Y accelerometer which signal is fed to the gyro torquer to rotate the azimuth axis of the stable element. This process continues for a few minutes until the correction signal is getting smaller and can be kept almost zero for 50 seconds, which gives confidence that the system is level and aligned. This is visible for the pilot because the green Nav light flashes.\n\nThe system is now ready for use and the pilot selects \"Nav\"\" on the IN Control Panel, and all circuitry that was involved in the various alignment phases is switched to the \"navigate mode\".\n\nOther possible modes are \"Compass only\" which may be selected after a LN3 in-flight failure, and \"Alert Align\" to shorten the alignment phase. After the last flight but before shutting down aircraft power the precise heading of the running LN3 is stored and can be used at starting up the next time, if the aircraft is not moved.\n\nSpecified navigation accuracy for the LN-3 is a 50% circular-error probability of two nautical miles after one hour's operation, which is equivalent to a 98% c.e.p. of four nautical miles. Until the −9 version of the LN-3-2A came into service (~1963) results were outside these limits by a fair margin, but since then it has been greatly exceeded in a number of groups of flights.\n\nDuring manufacturer's development flying at Palmdale, some 1167 flights were made up to October 1961, and the c.e.p. of the LN-3 and PHI-4 combined was a mile or so outside specification. From October 1961 to January 1962 a further 123 flights at Palmdale were assessed, following incorporation of the −9 modifications, and the c.e.p. came almost up to specification.\n\nAt Edwards AFB, during Category 2 testing, and at Palmdale during the \"avionics marriage\" period, mean time between failures of pre-9 systems was considerably below the 200 hr specified, but the target has been exceeded since then.\nLitton Systems Inc., or Litton Industries, the Guidance and Control Systems Division at Beverly Hills CA, were one of the major producers of inertial systems in the USA in the 50's and 60's, and have made a series of systems for a number of American aircraft.\n\nThe Genesis of inertial navigation systems is explained in the following reference.\n\n\nThe gimballed platform of the LN3-2A is the Litton P200 platform; the Gyro is the G200 Gyro; and the accelerometer is the A200 accelerometer. (and Litton doc)\nThe G-200 Gyro is commonly used in the LN-2, LN-3 and the LN-12 systems.\n\nManufacturers designation of the F-104G system is LN3-2A. Mark the difference in notation LN-3 and LN3-2A with the position of the dividing dash \"-\" .\nThe designation LN3-2A leaves room for a LN3-1, not known to author. Any additional information about the early Litton's is welcome!\n\nThe Litton LN-3 was one of the first inertial navigators on a production aircraft, but other systems, either inertial navigators or inertial measurement units, of other brands and for various applications with comparable technology existed.\n\nThe Autonetics Radar Enhanced Inertial Navigation System (REINS) of the North American A-5 Vigilante was more or less comparable to the LN-3/PHI-4. This system was derived from the XN-6 system developed for the SM-64 Navaho, the N5G system for the AGM-28 Hound Dog and the N2C/N2J/N3A/N3B system for the XB-70, and was related to the N6A-1 navigation system used in the USS Nautilus (SSN-571) and the N10 inertial guidance system for the LGM-30 Minuteman. Note that the Boeing history claims the REINS to be the first inertial navigation in a production airplane.\n\nNortronics had developed and produced Astro-inertial guidance/navigation systems for the SM-62 Snark. The system developed for the GAM-87 Skybolt was later adapted for use in the Lockheed SR-71 Blackbird and mostly referred to as NAS-14 and/or NAS-21.\n\nThe UGM-27 Polaris missile was equipped with a MIT-developed inertial system, which later evolved to the Delco produced IMU of the Apollo PGNCS.\n\nThe Saturn V was equipped with a MSFC-developed ST-124-M3 inertial platform which was a further development of the PGM-19 Jupiter's ST-90.\n\nThe Convair B-58 Hustler was equipped by AN/ASQ-42 Dopler-inertial system, made by Sperry Corporation.\n\nThe LN-3 system was designed to constantly monitor critical parameters, and warn the pilot in case of a malfunction. Depending on the problem the pilot could switch-off the system, or continue in a dead reckoning mode. In case of serious self-diagnosed problems the system would auto shut-down.\n\nFlight line maintenance of the LN-3, like systemchecks and fault isolation, was performed using specific test equipment :\n\nAt base (nav)shop level the platform, computer and adapter units were tested and repaired using the following test equipment :\n\n\nFor repairs beyond the capabilities of base level, the RNlAF Electronics Depot (DELM, at Rhenen) was equipped with specific testequipment and tooling to handle the (higher) depot level repairs of the LN-3 system.\n\nThe main test stations in use were:\n\n\nThe repair of the system's sensors, gyros and accelerometers, was performed by Litton. The RNlAF had its sensors repaired by Litton Canada, which also provided all necessary spare parts.\nOther European users relied on German or Italian subsidiaries/licensees as LITEF at Freiburg and Hamburg.\n\nExhibit of the LN3-2A system (without Alert Align Unit) in a vitrine. The platform gimbals can be rotated by the visitor with a remote control box.\n\nDisplay of a complete system, running as new. On request explication and demonstration of the system is given.\n\n\n"}
{"id": "41782533", "url": "https://en.wikipedia.org/wiki?curid=41782533", "title": "List of Earth flybys", "text": "List of Earth flybys\n\nList of Earth flybys is a list of cases where spacecraft incidentally performed Earth flybys, typically for a gravity assist to another body.\n"}
{"id": "39243863", "url": "https://en.wikipedia.org/wiki?curid=39243863", "title": "List of Women in Technology International Hall of Fame inductees", "text": "List of Women in Technology International Hall of Fame inductees\n\nThe Women in Technology International Hall of Fame was established in 1996 by Women in Technology International (WITI) to honor women who contribute to the fields of science and technology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "43738567", "url": "https://en.wikipedia.org/wiki?curid=43738567", "title": "List of ovens", "text": "List of ovens\n\nThis is a list of ovens. An oven is a thermally insulated chamber used for the heating, baking or drying of a substance, and most commonly used for cooking. Kilns and furnaces are special-purpose ovens, used in pottery and metalworking, respectively.\n\nBaking is a food cooking method that uses prolonged dry heat by convection, rather than by thermal radiation, normally in an oven, but also in hot ashes, or on hot stones. Bread is a commonly baked food.\n\nAn earth oven, or cooking pit, is one of the most simple and long-used cooking structures. At its simplest, an earth oven is a pit in the ground used to trap heat and bake, smoke, or steam food. Earth ovens have been used in many places and cultures in the past, and the presence of such cooking pits is a key sign of human settlement often sought by archaeologists. They remain a common tool for cooking large quantities of food where no equipment is available.\nIndustrial ovens are heated chambers used for a variety of industrial applications, including drying, curing, or baking components, parts or final products. Industrial ovens can be used for large or small volume applications, in batches or continuously with a conveyor line, and a variety of temperature ranges, sizes and configurations.\nA kiln is a thermally insulated chamber, a type of oven, that produces temperatures sufficient to complete some process, such as hardening, drying, or chemical changes. Various industries and trades use kilns to harden objects made from clay into pottery, bricks etc. Various industries use rotary kilns for pyroprocessing—to calcinate ores, produce cement, lime, and many other materials.\n"}
{"id": "24931940", "url": "https://en.wikipedia.org/wiki?curid=24931940", "title": "Ministry of Higher Education and Scientific Research (UAE)", "text": "Ministry of Higher Education and Scientific Research (UAE)\n\nThe Ministry of Higher Education and Scientific Research (MOHESR) is a ministry of the government in the United Arab Emirates (UAE). Established in 1976, the Ministry has a number of departments, including the Commission for Academic Accreditation (CAA), which provides institutional licensure and degree accreditationCAA for private universities and their academic programmes in the UAE. It houses NAPO (the National Admissions and Placement Office) which provides admissions and placement services for the federal institutions of higher education, including United Arab Emirates University, Higher Colleges of Technology, and Zayed University, as well as the CEPA (Common Educational Proficiency Assessment) which assesses the English and Math skills of MOHESR applicants to higher education. The Ministry handles steps in the certificate attestation process, provides equivalency services for degrees and qualifications received outside of the UAE, and provides government scholarships for UAE nationals who wish to study overseas.\n\nSheikh Hamdan bin Mubarak Al Nahyan is the Minister of Higher Education and Scientific Research.\n\nMinistry of Higher Education and Scientific Research website \n"}
{"id": "24441890", "url": "https://en.wikipedia.org/wiki?curid=24441890", "title": "Ministry of Power and Renewable Energy", "text": "Ministry of Power and Renewable Energy\n\nThe Ministry of Power and Renewable Energy (Sinhala: විදුලිබල හා පුණර්ජනනීය බලශක්ති අමාත්‍යාංශය \"Vidulibala hā Punarjananeeya Balashakthi Amathyanshaya\"; Tamil: மின்வலு மற்றும் புதுப்பிக்கத்தக்க வலுசக்தி அமைச்சு) is a cabinet ministry of the Government of Sri Lanka responsible for power and renewable energy. The ministry is responsible for formulating and implementing national policy on power and renewable energy and other subjects which come under its purview. \n\nThe current Minister and Deputy Minister of Power and Renewable Energy are Ranjith Siyambalapitiya and Ajith Perera respectively. The ministry's secretary is Dr. B. M. S. Batagoda.\n\n\n"}
{"id": "56333950", "url": "https://en.wikipedia.org/wiki?curid=56333950", "title": "Nintendo Labo", "text": "Nintendo Labo\n\nNintendo Labo is released as individual Labo Kits, each containing a set of pre-made cardboard cut-outs and other materials, used to make one or more \"Toy-Con\", and a Nintendo Switch game card, which contains interactive instructions on how to assemble the Toy-Con and software that the Toy-Con can interact with. Once each Toy-Con is constructed, players insert the main Nintendo Switch display and/or one or both of the Joy-Con controllers according to the instructions. Each Toy-Con functions differently in the ways it interacts with either the Joy-Con or the main display. For example, the piano Toy-Con's keystrokes are read by the Right Joy-Con controller's infrared sensor to identify notes being played, while robotic Toy-Con move using HD Rumble from the Joy-Con controllers, which are controlled via the touchscreen. Players may freely decorate the cardboard parts using coloring pens, tape, and other materials, while more experienced users can invent new ways to play with each Toy-Con. The game software provides instructions on how the Toy-Con works with the Switch, such as describing the fundamentals of infrared sensing.\n\nThe Nintendo Labo software comes with a feature called \"Toy-Con Garage\", which allows users to create and program their own Toy-Con using simple programming commands, either starting with the available Labo kits, or with their own materials. Toy-Con Garage is based upon creating simple commands by connecting input and output nodes. When an input is performed, it will trigger the connected output event. Additional middle nodes can be added to modify the input. For example, an input node can be a specific button press or a controller movement, while the middle node can set a required number of presses in order for the output to occur. Output nodes range from vibrating the Joy-Con to lighting up the Console's display. Toy-Con Garage provides multiple options for customizing each node, such as adjusting the sensitivity and direction of the control stick as an input node. Multiple input-output commands can be used in combination to create more complex creations.\n\nLabo was announced on January 17, 2018. According to Nintendo of America president Reggie Fils-Aimé, \"Labo is unlike anything we've done before\", and was developed to extend the age audience for the Switch. Nintendo said the product was \"specifically crafted for kids and those who are kids at heart.\" The tagline for Labo is \"Make, Play, Discover\"; the discovery part arises from how the user of the Toy-Con can understand the fundamentals of physics, engineering, and programming that make the Toy-Con work through the act of making and playing with them. The product was not originally intended to be educational, though one of its goals was to \"explain how the technology behind the Toy-Con creations works\".\nThe concept of Labo came from Nintendo when they asked their employees to come up with ways that the Switch's Joy-Con could be used; out of many potential ideas, the idea of building cardboard toys around the controllers held promise. According to Shinya Takahashi, the use of cardboard as part of playthings is a common practice among Japanese children, and as they started prototyping ideas, they found the \"trial and error\" process of putting together the cardboard toys was \"extremely fun\". As the Labo concept was developed, they found it fit well within Nintendo's overall philosophy on innovating new ways to have fun, and had potential to introduce the Switch to more than just game enthusiasts.\n\nNintendo Labo developers Kawamoto, Sakaguchi, and Ogasawara stated in an interview that their goal was to use the unique features of the Nintendo Switch in such a way that no other system could emulate. The Joy-Con were treated as a \"bundle of sensors\" that could be used in a multitude of ways by attaching them to different hardware pieces, focusing on the gyro motion-sensor, IR motion camera, and HD Rumble features. The team was tasked with exploring Joy-Con attachment and game prototypes during three weeks of brainstorming sessions, called \"prototype parties.\" When asked about the choice of building material, the developers responded by saying that cardboard was chosen over plastic because the 3D printer they were using for prototyping was unable to keep up with their pace of testing; cardboard would also allow for the player to create, repair, and customize the project themselves. The initial construction designs were first thought as being simple to construct, but later proved to be complex and challenging when it was decided that no cutting or gluing would be required to assemble the kits. Improvements were made to the prototypes based on feedback from consumer and developer testing; this prompted the creation of the interactive instruction software, as well as favoring simplicity over appearance. During development testing—prior to the cardboard designs being finalized—photographs were taken and compiled into booklets to be used as temporary instruction manuals; these booklets ranged from 1,000 pages for simpler models to 3,000 pages for complex ones. While the cardboard Toy-Con are sturdy, Nintendo recognized that the cardboard can suffer wear and tear with time, and sells replacement sheets for individual Toy-Con through its online store. When asked about the durability of Labo during an interview with CNET, Yoshiyasu Ogasawara stated, \"We tested their resilience to the same action through hundreds and thousands of repetitions, so we expect them to last a long time under normal use.\"\n\nStarting in October 2018, Nintendo worked with the Institute of Play to bring Nintendo Labo into elementary schools, with an initial goal to reach 2,000 students by the end of the 2018-2019 school year. Nintendo is providing the Switch and Labo kits, while the Institute is developing the lesson plans for teachers.\n\nTwo Labo Kits, Variety Kit and Robot Kit, were announced for launch in North America, Australia, and Japan on April 20, 2018, and in Europe on April 27, 2018. An accessory set containing stencils, stickers, and tape are available separately. Replacement packs for individual parts and Toy-Con are available for purchase on Nintendo's online store, while free templates for the cardboard cut-outs are also available for download. While Nintendo did not confirm any additional Toy-Con kits at launch, journalists observed that other Toy-Con configurations were shown in the announce trailer, suggesting that additional kits may be announced at a later date.\n\nThe Variety Kit contains kits for five individual Toy-Con:\n\nThe kit comes with a game cartridge that contains interactive instructions of how to assemble each Toy-Con, and at least one software package to use the Toy-Con. Some Toy-Con have multiple programs; for example, the motorbike handles allow the player to race along a track in stunt bikes, and gives the player the ability to create new track layouts, or to use any object detected through the IR sensor to create a track based on that object. Similar IR sensing abilities allows the player to create new fish to catch in the fishing rod Toy-Con, or to create new waveforms to use on the piano Toy-Con.\n\nThe Robot Kit includes parts to make a mecha suit that includes a visor which holds the left Joy-Con for motion sensing and a backpack that holds the right Joy-Con to read hand and feet swings. This allows the player to rampage through a virtual world presented on the screen. The software includes multiple game-modes: Robot, VS, Challenge, Robo-Studio, and the Hangar. Robot mode gives the player the ability to control the giant robot as it attacks targets across a cityscape, to control the robot as it flies over a city, and to transform in a tank. The two-player VS mode allows players to fight with their giant robots, though both players need a separate Robot Toy-Con for this mode. In Challenge Mode, the player can complete missions to unlock special abilities that can be used in the game's other modes. The Robo-Studio mode allows the player to insert the Console into the Toy-Con and play sound effects based on the player's movement, while the Hanger allows the player to customize the color and appearance of their virtual robot.\n\nJournalists noted similarities between this Kit and \"Project Giant Robot\", a software title for the Wii U that had players use the motion sensing of the Wii U GamePad to control a robot and rampage through a city. \"Project Giant Robot\" was teased during E3 2014 and believed to be tied to \"Star Fox Zero\", but was ultimately cancelled by Nintendo. Labo developers stated in an interview that the original prototype was a ground-based tank with interactive floor pedals, but risked being crushed by the user and did not utilize the potential of the Joy-Con's gyro sensor; to solve these issues, the prototype was modified to be worn as a \"Carry-Con\" on the user's back.\n\nNintendo announced its first post-release Labo Kit in July 2018, which was released worldwide on September 14, 2018. The Vehicle Kit includes the cardboard parts to make three steering consoles, one for a car, one for a plane, and one for a submarine, each with a slot for a \"key\" that is built around a Toy-Con. The associated game allows players to control cars, planes, and submarines in game, switching between these modes by moving their Toy-Con key between units, and supports co-operative play with a second person using another Toy-Con. The kit also includes cutouts for a pedal Toy-Con to power each of the vehicles, two key Toy-Cons, a spray can Toy-Con, and a stand for the console to sit atop the car Toy-Con. Included with the spray can Toy-Con are several \"extra parts\" made to aid in the Paint Studio, the mode made for it.\n\nA free update to \"Mario Kart 8 Deluxe\" in June 2018 allowed players to use the motorbike Toy-Con, from the Variety Kit, to control their racer in the game. In August 2018, Nintendo announced that the car steering wheel Toy-Con from the upcoming Vehicle Kit will also be compatible with \"Mario Kart 8 Deluxe\". After unveiling a tech demo in April 2018, Rayark Games announced their title \"Deemo\" will gain limited support for the piano Toy-Con, from the Variety Kit, in October 2018, marking as the first third-party title to support Nintendo Labo.\n\nOn the day after its announcement in January 2018, Nintendo's stock price jumped around 2.4%, representing about to their value. Analysis suggested that Labo was the type of unorthodox product that only Nintendo could develop and market, showing a further return to their more financially-successful period about a decade prior, leading to the rising stock price.\n\nNintendo Labo was received with praise for its unique take on video gaming and unconventional method of play, and its ability to encourage creativity and learning, especially in children. Reviewers primarily praised the enjoyable building experience and easy to follow step-by-step instructions; the rotatable camera and fast-forward/rewind features were appreciated, as well as the sense of humor that the instructions contain. Critics were initially concerned with the sturdiness of the cardboard, but were impressed with the durability of the assembled Toy-Con, also noting that the software contains tutorials with tips on repairing broken Toy-Con. Reviewers were mixed on the enjoyment and limited replay value of the software's gameplay, with Andrew Webster from \"The Verge\" saying \"the games are perhaps the least interesting part of Labo.\" However, critics highlighted the seemingly unlimited options provided by Toy-Con Garage, and the possibility for the community to develop and share new creations. \"The Verge\" and CNET found Labo to be a clever utilization of the Joy-Con controllers' motion sensors, HD Rumble, and IR sensing abilities.\n\nLabo was featured in a May 2018 episode of \"The Tonight Show Starring Jimmy Fallon\", where Jimmy Fallon, Ariana Grande, and The Roots used various kits and Switches in a performance to premiere Grande's single, \"No Tears Left to Cry\".\n\nSome players have found ways to recreate the cardboard Toy-Con functionality with more sturdy versions using Lego bricks, otherwise retaining all the functionality of the original toys.\n\nIn Japan, \"Variety Kit\" sold 90,410 copies within the first week, placing it first on the all-format sales chart, while \"Robot Kit\" sold 28,629 copies, placing it third. As of August 2018, \"Variety Kit\" has sold 222,514 units in Japan, and all kits consolidated have sold 1.39 million units worldwide.\n"}
{"id": "16085364", "url": "https://en.wikipedia.org/wiki?curid=16085364", "title": "Organisms involved in water purification", "text": "Organisms involved in water purification\n\nMost organisms involved in water purification originate from the waste, wastewater or water stream itself or arrive as resting spore of some form from the atmosphere. In a very few cases, mostly associated with constructed wetlands, specific organisms are planted to maximise the efficiency of the process.\n\nBiota are an essential component of most sewage treatment processes and many water purification systems. Most of the organisms involved are derived from the waste, wastewater or water stream itself or from the atmosphere or soil water. However some processes, especially those involved in removing very low concentrations of contaminants, may use engineered eco-systems created by the introduction of specific plants and sometimes animals. Some full scale sewage treatment plants also use constructed wetlands to provide treatment\n\nParasites, bacteria and viruses may be injurious to the health of people or livestock ingesting the polluted water. These pathogens may have originated from sewage or from domestic or wild bird or mammal feces. Pathogens may be killed by ingestion by larger organisms, oxidation, infection by phages or irradiation by ultraviolet sunlight unless that sunlight is blocked by plants or suspended solids.\n\nParticles of soil or organic matter may be suspended in the water. Such materials may give the water a cloudy or turbid appearance. The anoxic decomposition of some organic materials may give rise to obnoxious or unpleasant smells as sulphur containing compounds are released.\n\nCompounds containing nitrogen, potassium or phosphorus may encourage growth of aquatic plants and thus increase the available energy in the local food-web. this can lead to increased concentrations of suspended organic material. In some cases specific micro-nutrients may be required to allow the available nutrients to be fully utilised by living organisms. In other cases, the presence of specific chemical species may produce toxic effects limiting growth and abundance of living matter.\n\nMany dissolved or suspended metal salts exert harmful effects in the environment sometimes at very low concentrations. Some aquatic plants are able to remove very low metal concentrations, with the metals ending up bound to clay or other mineral particles.\n\nSaprophytic bacteria and fungi can convert organic matter into living cell mass, carbon dioxide, water and a range of metabolic by-products. These saprophytic organisms may then be predated upon by protozoa, rotifers and, in cleaner waters, Bryozoa which consume suspended organic particles including viruses and pathogenic bacteria. Clarity of the water may begin to improve as the protozoa are subsequently consumed by rotifers and cladocera. Purifying bacteria, protozoa, and rotifers must either be mixed throughout the water or have the water circulated past them to be effective. Sewage treatment plants mix these organisms as activated sludge or circulate water past organisms living on trickling filters or rotating biological contactors.\n\nAquatic vegetation may provide similar surface habitat for purifying bacteria, protozoa, and rotifers in a pond or marsh setting; although water circulation is often less effective. Plants and algae have the additional advantage of removing nutrients from the water; but some of those nutrients will be returned to the water when the plants die unless the plants are removed from the water. Because of the complex chemistry of Phosphorus much of this element is in an unavailable form unless decomposition creates anoxic conditions which render the phosphorus available for re-uptake. Plants also provide shade, a refuge for fish, and oxygen for aerobic bacteria. In addition, fish can limit pests such as mosquitoes. Fish and waterfowl feces return waste to the water, and their feeding habits may increase turbidity. Cyanobacteria have the disadvantageous ability to add nutrients from the air to the water being purified and to generate toxins in some cases.\n\nThe choice of organism depends on the local climate different species and other factors. Indigenous species usually tend to be better adapted to the local environment.\n\nThe choice of plants in engineered wet-lands or managed lagoons is dependent on the purification requirements of the system and this may involve plantings of varying plant species at a range of depths to achieve the required goal.\n\nPlants purify water by consuming excess nutrients and by providing surfaces upon which a wide range of other purifying organisms can live. They also are effective oxygenators in sunlight. They also have the ability to translocate chemicals between their submerged foliage and their root systems and this is of significance in engineered wet-lands designed to de-toxify waste waters. Plants that have been used in temperate climates include \"Nymphea alba\",\"Phragmites australis\", \"Sparganium erectum\", \"Iris pseudacorus\", \"Schoenoplectus lacustris\" and \"Carex acutiformis\".\n\nWhere oxygenation is a critical requirement \"Stratiotes aloides\", \"Hydrocharis morsus-ranae\", \"Acorus calamus\", \"Myriophyllum\" species and \"Elodea\" have been used.\n\"Hydrocharis morsus-ranae\" and \"Nuphar lutea\" have been used where shade and cover are required\n\nFish are frequently the top level predators in a managed treatment eco-system and in some case may simply be a mono-culture of herbivorous species. Management of multi-species fisheries requires careful management and may involve a range of fish species including bottom-feeders and predatory species to limit population growth of the herbivorous fish\n\nRotifers are microscopic complex organisms and are filter feeders removing fine particulate matter from water. They occur naturally in aerobic lagoons, activated sludge processes, in trickling filters and in final settlement tanks and are a significant factor in removing suspended bacterial cells and algae from the water column.\n\nAnnelid worms are essential to the effective operation of trickling filters helping to remove excess bio-mass and enhancing natural sloughing of the bio-film. Supernumerary worms are very commonly found in the drainage troughs around trickling filters and in the final settlement sludge. Annelids also play a key role in lagoon treatment systems and in the effective working or engineered wet-lands. In this environment worms are a principal force in mixing in the upper few centimetres of the sediment layer exposing organic material to both oxidative and anoxic environments aiding the complete breakdown of most organics. They are also a key ingredient in the food-chain transferring energy upwards to fish and aquatic birds.\n\nThe range of protozoan species found is very wide but may include species of the following genera:\n\n\n\n"}
{"id": "7427890", "url": "https://en.wikipedia.org/wiki?curid=7427890", "title": "Peek Freans", "text": "Peek Freans\n\nPeek Freans is the name of a former biscuit making company based in Bermondsey, London, which is now a global brand of biscuits and related confectionery owned by various food businesses. Owned but not marketed in the UK and Europe by United Biscuits, in the United States and Canada the brand is owned by Mondelēz International, whilst in Pakistan the brand is owned by English Biscuit Manufacturers.\n\nJames Peek (1800–1879) was one of three brothers born in Dodbrooke, Devon, to a well-off family. In 1821 the three brothers founded a tea importation company, established as Peek Brothers and Co., in the East End of London. By the 1840s, the company was importing £5M of tea per annum.\n\nIn 1824, Peek married Elizabeth Masters (1799–1867). The couple had eight children. By 1857, two of his late-teenage sons had announced that they were not going to join the family tea import business. Peek wanted them in a complementary trade and proposed that they start a biscuit business. After founding the business, the two sons quickly decided on a different course (one died in his early 20s; the other emigrated to North America). As a consequence Peek needed someone to run the biscuit business. One of his nieces, Hannah Peek, had recently married George Hender Frean, a miller and ship biscuit maker in Devon, so Peek wrote to Frean asking him to manage the new biscuit business.\n\nThe partners registered their business in 1857 as Peek, Frean & Co. Ltd, based in a disused sugar refinery on Mill Street in Dockhead, South East \nLondon, in the west of Bermondsey. With a quickly expanding business, in 1860 Peek engaged his friend James Carr, the apprenticed son of the Carlisle-based Scottish milling and biscuit making family, Carr's.\n\nFrom 1861, the company started exporting biscuits to Australia, but outgrew their premises from 1870 after agreeing to fulfil an order from the French Army for of biscuits for the ration packs supplied to soldiers fighting the Franco-Prussian War. After hostilities ended, the French Government ordered a further /11 million sweet Pearl biscuits in celebration of the end of the Siege of Paris, and further flour supplies for Paris in 1871 and 1872, with financing undertaken by their bankers the Rothschilds. The consequential consumer demands of emigrating French expatriate soldiers, allowed the company to start exporting directly to Ontario, Canada from the mid-1870s.\n\nIn 1865 Peek agreed with Carr that the business needed bigger premises. In exchange for a stake in the business, Carr gave the company of market gardens he had recently bought on Clements Road and Drummond Road, Bermondsey. Commissioning a new integrated factory, its resultant scale and sweet-emanating smell resulted in Bermondsey gaining the nickname \"Biscuit Town\". The opening of the factory coincided in 1866 with James Peek stepping down from the business, installing his son-in-law Thomas Stone in his place. \n\nOn 23 April 1873 the old Dockhead factory burnt down in a spectacular fire, which brought the Prince of Wales out on a London Fire Brigade horse-drawn water pump to view the resulting explosions.\n\nJames Peek died aged 79 at his home in Watcome near Torquay, Devon. After George Frean's son James Frean retired in 1887, his family had nothing more to do with running the business. Peek's nephew Francis Hedley Peek became the first chairman of the now publicly listed company in 1901, but on his death in 1904 again the Peek family had nothing more to do with managing the business. James Carr's family remained actively associated with the business for several more generations.\n\nIn 1906, the Peek, Frean and Co. factory in Bermondsey was the subject of one of the earliest documentary films shot by Cricks and Sharp. This was in part to celebrate an expansion of the company's cake business, which later made the wedding cakes for both Queen Elizabeth II and the Wedding of Charles, Prince of Wales, and Lady Diana Spencer. In 1949, when James Carr's relative Rupert Carr established a new biscuit factory in York, he named the new street on which it stood \"Bermondsey Road\".\n\nIn 1924, the company established their first factory outside the UK, in Dum Dum, India. In 1949 they establishment their first bakery in Canada, located on Bermondsey Road in East York, Ontario, which still today produces Peek Freans branded products..\n\nAfter 126 years, the London factory was closed by then owner BSN on Wednesday 26 May 1989. Left derelict for a long period, the former premises were eventually redeveloped into what today is called the Tower Bridge Business Complex. In late 2011, a tinned Christmas pudding was discovered at the back of a kitchen cupboard in Poole, Dorset. Donated to the museum at Portsmouth Historic Dockyard, it was a \"Peek, Frean & Co's Teetotal Plum Pudding—London, High Class Ingredients Only\" from the 1900s. It was one of a thousand puddings sent on behalf of Agnes \"Aggie\" Weston, superintendent of the Royal Naval Temperance Society – hence its recipe being teetotal), (alcohol-free) – to British troops during the Boer War.\n\nFrom the outset of its establishment, the company produced what were then the established form of biscuits in the Commonwealth countries, a hard, square, pin-pricked (known as \"docker-holes\", introduced by the baker to stop the biscuit expanding like a bread) dry style, suitable for storage on ships in passage due to its longevity. However, Carr brought his knowledge of both the Scottish cake-like tradition (i.e.: shortbread), and experience during his apprenticeship of Dutch sweet and soft cookies. With James Peek still viewing the business as a complementary and co-marketing opportunity to the families tea company, they began introducing sweetened product lines:\n\nLike many good employers of the Victorian age, the company developed an enlightened matriarch-like approach, giving many innovative benefits to its employees. At its Biscuit Town factory, much like a mini-town, as well as having: an on-site bank, post office and fire station; employees and their families had free-to-use access to on-site medical, dental and optical services. The original contracted hours were 68 across a Monday-Saturday double-shift pattern, but these were reduced from 1868 without a reduction in pay, noted as highly beneficial by Henry Mayhew. The directors wanted to ensure that the workers didn't indulge in \"virtuous pursuits\", and so formed the first of the company paid-for societies, included: a cricket club (1868); musical society (1907); and athletic and dramatic societies (both 1908). Post-World War I, the company set up a tribunal, through which workers could freely express and debate their concerns. This resulted in the company giving its employees a pension plan, plus a week's paid holiday per year.\n\nIn 1921, Peek Frean entered into an amalgamation agreement with rival biscuit firm Huntley and Palmers, resulting in the creation of a holding company, Associated Biscuit Manufacturers Ltd (ABM). However, both biscuit firms retained their own brands and premises. Jacob's joined the conglomerate from 1961. English Biscuit Manufacturers (EBM) was established in Pakistan as a local joint-venture production company from 1965, which still owns the various brand rights in the country.\n\nDuring the course of its life, the firm's brand name changed from Peek, Frean and Co. to Peek Frean (in the early twentieth century) and then to Peek Freans (by the 1970s, the name having been used in the possessive case on products for many years).\n\nThe company was broken apart from 1982, after Nabisco bought ABM. In 1985, Nabisco was acquired by the foods division of R. J. Reynolds Tobacco Company, resulting in the creation of conglomerate RJR Nabisco. After RJR Nabisco was bought in a leveraged buyout by Kohlberg Kravis Roberts, to pay down the resultant debt pile, various assets were sold off. This included dividing the former Peek Freans company; the North America division was sold to Kraft Foods, the European mainly-UK division was sold to the European food conglomerate BSN (now known as Groupe Danone), and many of the international subsidiaries were sold off locally to in-country investors, e.g. EBM. As part of its cost cutting, BSN ended use and marketing of the brand in the UK, which allowed it to shut the factory in Bermondsey. In September 2004 United Biscuits bought what was by then known as the Jacob's Biscuit Group for £240M from Groupe Danone. \n\nIn 2017, the Peek Frean trademark was acquired by the great-great-great-grandson of Francis Peek, the first Chairman of Peek Frean and the son of James Peek.\n\n\n"}
{"id": "41733230", "url": "https://en.wikipedia.org/wiki?curid=41733230", "title": "Petróczy-Kármán-Žurovec", "text": "Petróczy-Kármán-Žurovec\n\nPetróczy, Kármán and Žurovec were Hungarian and Czech engineers who worked on helicopter development immediately before and during World War I. Between them they produced two experimental prototypes, the PKZ-1 and PKZ-2, intended to replace the dangerous hydrogen-filled observation balloons then in use. As such, these craft were tethered on long cables and were not intended to fly freely. After the war other engineers, notably Oszkár von Asboth, further developed the design.\n\nIn 1916, the aviator Oberstlt István Petróczy proposed an electrically driven rotorcraft to replace the dangerously flammable observation balloon. His original concept was for the electric motor to be supplied by a dynamo driven by an internal combustion engine.\n\nAustro-Daimler were at that time developing a lightweight electric motor for aircraft use, but would take several years to develop one able to handle the electrical power required. A major problem was in providing high-quality insulation for the motor windings, which could get very hot in use.\n\nMeanwhile, tests on the large propellers then available showed them to be too inefficient and so a research programme into efficient large propellers, for use as rotors, was begun at Fischamend airfield. Dr. Theodore von Kármán was the director of the research group at Fischamend and Ensign Vilém Žurovec was an engineer there.\n\nModel tests showed that proposed designs with a single tether were unstable. Initially four tethers were used to provide stability, but this was later reduced to three.\n\nBy 1917 the technology appeared ready and two rotary-wing aircraft, the PKZ-1 and PKZ-2, were built under separate projects. Both types hovered briefly while tethered, though even with the tethers they were barely controllable and required skilful handling of the tethers. At the time these were referred to as \"schraubenfesselflieger\" or SFF (propeller-driven captive aircraft). The PKZ designations were applied later in a postwar article by Karman.\n\nA third design, for a small unmanned version powered by a single Gnome rotary piston engine, was constructed in 1918. It was intended to carry meteorological (weather) instruments or radio antennas aloft, but it is not known whether it ever flew.\n\nThe PKZ-1 was designed by Karman and Žurovec, and built by MAG in Budapest under Karman's direction. It had four radiating arms with a 3.9 m four-bladed rotor or propeller on top of each, geared in pairs such that each pair spun in the opposite direction. The rotors were driven from a single Austro-Daimler electric motor located centrally, beneath the observer's cockpit. The 195 kg motor produced at 6,000 rpm, limited by the heat resistance of the insulation around the windings – in other respects it was capable of producing . to A ground-based generator fed direct current (DC) through the tethering cables to the motor. Landing gear comprised four inflated rubber-fabric cushions, one under the end of each arm.\n\nThe finished craft was taken to Fischamend for flight testing. In a short series of four test flights in March 1918 the craft was able to lift three men. The wiring insulation in the motor burned out on the fourth flight and was not repaired.\n\nThe PKZ-2 was developed by Žurovec in parallel with the PKZ-1, but wholly independently. Zurovec acknowledged the support only of Petróczy, although later reports have mistakenly attributed the entire design to Kármán. The aircraft was built by Dr Liptak & Co AG under Žurovek's direction. It had three radiating arms, each housing a rotary piston engine. These engines were coupled together to drive a central pair of co-axial, contra-rotating two-bladed wooden propellers or rotors, of 6 m diameter, mounted above the airframe. A round cockpit for the aircrew was fixed centrally, on top of the rotor mast. Landing gear again comprised rubber-fabric cushions, one large central one and three smaller ones at the end of each arm.\n\nThe PKZ-2 began flight trials on 2 April 1918. Initially fitted with three Gnome rotary engines of , these were found insufficient to maintain safety at any altitude and were replaced by le Rhone rotary engines of . In this form the PKZ-2 could rise to a height of over 50 m and hover for up to half an hour, although it was unstable and remained tethered on long cables. To maintain stability and control, the tethers had to remain in tension, as if the engines faltered the tethers could slacken and control would be lost.\n\nOn 10 June the aircraft was demonstrated to Air Service officials. The le Rhone engines were not reliable and Žurovec had misgivings about the demonstration. These turned out to be justified when the engines faltered. The tether handlers panicked and this resulted in a crash-landing, damaging the craft and breaking the rotors.\nAfter the war the Italians confiscated the aircraft and took it back to Italy.\n\nOszkár von Asboth was one of the researchers at Fischamend. In 1917 he commissioned Ufag to make a full-scale prototype to his own design, but it was destroyed in a fire before completion. After the war he went on to build and fly several more helicopters. One was constructed in 1920 but later destroyed on the order of the Allied Control Commission. Subsequent examples based on the PKZ-2 design were designated AH-1 to AH-4 and made over 150 successful flights between 1928 and 1930. Von Asboth later set up companies in Britain and France to promote his twin-rotor designs, and at least one machine was built by the French company.\n\n"}
{"id": "33535504", "url": "https://en.wikipedia.org/wiki?curid=33535504", "title": "Pilot direction indicator", "text": "Pilot direction indicator\n\nA pilot direction indicator, or PDI, is an aircraft instrument used by bombardiers to indicate heading changes to the pilot in order to direct them to the proper location to drop bombs. The PDI is used in aircraft where the pilot and bombardier are physically separated and cannot easily see each other.\n\nPDI's typically consist of a dial that is installed in the pilot's instrument set on the main console, with an arrow pointer than can be moved to indicate how far and in what direction to correct the heading. The bombardier typically has a switch to move the pointer to the right or left, and a repeater dial so he can see the setting.\n\nThe Norden bombsight was originally designed with the idea of automatically directing a PDI and thereby simplifying the bombardier's task.\n\n"}
{"id": "10368163", "url": "https://en.wikipedia.org/wiki?curid=10368163", "title": "Production packer", "text": "Production packer\n\nA production packer is a standard component of the completion hardware of oil or gas wells used to provide a seal between the outside of the production tubing and the inside of the casing, liner, or wellbore wall.\n\nBased on their primary use, packers can be divided into two main categories: production packers and service packers. Production packers are those that remain in the well during well production. Service packers are used temporarily during well service activities such as cement squeezing, acidizing, fracturing and well testing.\n\nIt is usually run in close to the bottom end of the production tubing and set at a point above the top perforations or sand screens. In wells with multiple reservoir zones, packers are used to isolate the perforations for each zone. In these situations, a sliding sleeve would be used to select which zone to produce. Packers may also be used to protect the casing from pressure and produced fluids, isolate sections of corroded casing, casing leaks or squeezed perforations, and isolate or temporarily abandon producing zones. In water-flooding developments in which water is injected into the reservoir, packers are used in injection wells to isolate the zones into which the water must be injected.\n\nThere are occasions in which running a packer may not be desirable. For example, high volume wells that are produced both up the tubing and annulus will not include a packer. Rod pumped wells are not normally run with packers because the associated gas is produced up the annulus. In general, well completions may not incorporate a packer when the annular space is used as a production conduit.\n\nA production packer is designed to grip and seal against the casing ID. Gripping is accomplished with metal wedges called \"slips.\" These components have sharpened, carburized teeth that dig into the metal of the casing. Sealing is accomplished with large, cylindrical rubber elements. In situations where the sealed pressure is very high (above 5,000 psi), metal rings are used on either side of the elements to prevent the rubber from extruding.\n\nA packer is run in the casing on production tubing or wireline. Once the desired depth is reached, the slips and element must be expanded out to contact the casing. Axial loads are applied to push the slips up a ramp and to compress the element, causing it to expand outward. The axial loads are applied either hydraulically, mechanically, or with a slow burning chemical charge.\n\nMost packers are \"permanent\" and require milling in order to remove them from the casing. The main advantages of permanent packers are lower cost and greater sealing and gripping capabilities.\n\nIn situations where a packer must be easily removed from the well, such as secondary recoveries, re-completions, or to change out the production tubing, a retrievable packer must be used. To unset the tool, either a metal ring is sheared or a sleeve is shifted to disengage connecting components. Retrievable packers have a more complicated design and generally lower sealing and gripping capabilities, but after removal and subsequent servicing, they can be reused.\n\n\nThere are three types of packers: mechanical and hydraulic set and permanent. All packers fall into one or a combination of these.\n\nMechanical set packers are set by some form of tubing movement, usually a rotation or upward /downward motion.\n\nOthers can be weight set—the tubing weight can be used to compress and expand the sealing element. By a simple up string pull the packer is released. It is used best in shallow low pressure wells that are straight. It is not designed to withstand pressure differences unless a hydraulic hold down is incorporated.\n\nTension-set packers are set by pulling a tension on the tubing, slacking off releases the packer. Good for shallow wells with moderate pressure differences. The lower pressure helps to increase the setting force on the packer. Used in a stimulation well.\n\nRotation-set packer – Tubing rotation is used to set the packer to mechanically lock it in; a left-hand turn engages and a right-hand turn retrieves it.\n\nHydraulic-set packers use fluid pressure to drive the cone behind the slips. Once set they remain set by the use of either entrapped pressure or a mechanical lock. They are released by picking up the tubing. They are good for used in deviated or crooked holes where tubing movement is restricted or unwanted. The tubing can be hung in neutral tension.\n\nInflatable packers - use fluid pressure to inflate a long cylindrical tube of reinforced rubber to set the packer. Frequently used for open hole testing in exploration wells and for cement assurance in production wells. Also used in wells where the packer must pass through a restriction and then set at a much larger diameter in casing or open holes. Many variations for specific applications are available including those capable of withstanding high pressure differentials.\n\nPermanent packers are run and set on an electric wireline, drill pipe or tubing. Opposed slips are positioned to lock it in compression. Once set this packer is resistant to motion for either direction. Wireline uses an electric current to detonate an explosive charge to set the packer. A release stud then frees the assembly form the packer. Tubing can be used by applying rotation or a pull or a combination of both. They are good in wells that have high pressure differentials or large tubing load variations and can be set precisely. They can be set the deepest.\n\nCement packers – In this case the tubing is cemented in place inside the casing or open hole. This type of packer is cheap.\n\nTemperature and pressure can affect how the tubing and the packer behave as this could cause changes in the packer and tubing expansion rates. If the packer allows free motion then the tubing can elongate or shorten. If not the tensile and compressive forces can develop within.\n\n\n"}
{"id": "12033314", "url": "https://en.wikipedia.org/wiki?curid=12033314", "title": "Proto-industrialization", "text": "Proto-industrialization\n\nProto-industrialization (also spelled proto-industrialisation) was a possible phase in the development of modern industrial economies that preceded, and created conditions for, the establishment of fully industrial societies. Proto-industrialisation generally refers to the phase before industrialisation. Proto-industrialization was marked by the increasing involvement of agrarian families in market-oriented craft production, mainly through the putting-out system organized by merchant capitalists. It was an effective method of production which was controlled by merchants and had links to developing European consumerism. However, the phase was not observed across Europe, and nor did it always smoothly transition into the Industrial Revolution proper.\n\nThe term was coined by F. F. Mendels in 1972,\nthough a UNESCO colloquium on the 10th anniversary of the deaths of Einstein and Teilhard de Chardin and published in 1971 discusses \"the basis for proto-industrialization\".\n\nThe applicability of proto-industrialization in Europe has since been challenged. M.J. Daunton, for example, argues that proto-industrialisation \"excludes too much\" to fully explain the expansion of industry: not only do proponents of proto-industrialisation ignore the vital town-based industries in pre-industrial economies, but also ignores \"rural and urban industry based upon non-domestic organisation\", referring to how mines, mills, forges and furnaces fit into the agrarian economy.\n\nInitially using surplus labor available during slow periods of the agricultural seasons, proto-industrialization led to specialization - not only in industrial production but also in commercial agricultural production. This allowed reciprocal trade favored by regional economies of scale. It resulted in accumulation of capital and in the acquisition of entrepreneurial skills by merchant capitalists, which facilitated the development of large-scale, and capital-intensive production methods in the full industrialization phase that followed.\n\nProto-industrialization sparked social changes in traditional agrarian societies that would become more marked during full industrialization, such as greater independence of women and children, who gained a means of income separate from the family subsistence farm. During this phase of industrialisation, machines were not used. They were not even invented at that time. People could only use their hands or any hand-made material to produce required goods.\n\nThe term proto-industrialization has also been used in reference to Mughal India during the 17th–18th centuries, when the Indian subcontinent experienced a growth in manufacturing industries and its economy had similar conditions to 18th-century Western Europe prior to the Industrial Revolution.\n\n"}
{"id": "144615", "url": "https://en.wikipedia.org/wiki?curid=144615", "title": "Psychological warfare", "text": "Psychological warfare\n\nPsychological warfare (PSYWAR), or the basic aspects of modern psychological operations (PSYOP), have been known by many other names or terms, including MISO, Psy Ops, political warfare, \"Hearts and Minds\", and propaganda. The term is used \"to denote any action which is practiced mainly by psychological methods with the aim of evoking a planned psychological reaction in other people\". Various techniques are used, and are aimed at influencing a target audience's value system, belief system, emotions, motives, reasoning, or behavior. It is used to induce confessions or reinforce attitudes and behaviors favorable to the originator's objectives, and are sometimes combined with black operations or false flag tactics. It is also used to destroy the morale of enemies through tactics that aim to depress troops' psychological states. Target audiences can be governments, organizations, groups, and individuals, and is not just limited to soldiers. Civilians of foreign territories can also be targeted by technology and media so as to cause an effect in the government of their country.\n\nIn \"\", Jacques Ellul discusses psychological warfare as a common peace policy practice between nations as a form of indirect aggression. This type of propaganda drains the public opinion of an opposing regime by stripping away its power on public opinion. This form of aggression is hard to defend against because no international court of justice is capable of protecting against psychological aggression since it cannot be legally adjudicated. \"Here the propagandists is [sic] dealing with a foreign adversary whose morale he seeks to destroy by psychological means so that the opponent begins to doubt the validity of his beliefs and actions.\"\n\nSince prehistoric times, warlords and chiefs have recognised the importance of weakening morale of opponents.\n\nIn the Battle of Pelusium (525 BC) between the Persian Empire and ancient Egypt, the Persian forces used cats and other animals as a psychological tactic against the Egyptians, who avoided harming cats due to religious belief and spells.\n\nCurrying favour with supporters was the other side of psychological warfare, and an early practitioner of such this was Alexander the Great, who successfully conquered large parts of Europe and the Middle East and held on to his territorial gains by co-opting local elites into the Greek administration and culture. Alexander left some of his men behind in each conquered city to introduce Greek culture and oppress dissident views. His soldiers were paid dowries to marry locals in an effort to encourage assimilation.\n\nGenghis Khan, leader of the Mongolian Empire in the 13th century AD employed less subtle techniques. Defeating the will of the enemy before having to attack and reaching a consented settlement was preferable to facing his wrath. The Mongol generals demanded submission to the Khan, and threatened the initially captured villages with complete destruction if they refused to surrender. If they had to fight to take the settlement, the Mongol generals fulfilled their threats and massacred the survivors. Tales of the encroaching horde spread to the next villages and created an aura of insecurity that undermined the possibility of future resistance.\n\nThe Khan also employed tactics that made his numbers seem greater than they actually were. During night operations he ordered each soldier to light three torches at dusk to give the illusion of an overwhelming army and deceive and intimidate enemy scouts. He also sometimes had objects tied to the tails of his horses, so that riding on open and dry fields raised a cloud of dust that gave the enemy the impression of great numbers. His soldiers used arrows specially notched to whistle as they flew through the air, creating a terrifying noise.\n\nAnother tactic favoured by the Mongols was catapulting severed human heads over city walls to frighten the inhabitants and spread disease in the besieged city's closed confines. This was especially used by the later Turko-Mongol chieftain.\n\nThe Muslim caliph Omar, in his battles against the Byzantine Empire, sent small reinforcements in the form of a continuous stream, giving the impression that a large force would accumulate eventually if not swiftly dealt with.\n\nIn the 6th century BCE Greek Bias of Priene successfully resisted the Lydian king Alyattes by fattening up a pair of mules and driving them out of the besieged city. When Alyattes' envoy was then sent to Priene, Bias had piles of sand covered with corn to give the impression of plentiful resources.\n\nThis ruse appears to have been well known in medieval Europe: defenders in castles or towns under siege would throw food from the walls to show besiegers that provisions were plentiful. A famous example occurs in the 8th-century legend of Lady Carcas, who supposedly persuaded the Franks to abandon a five-year siege by this means and gave her name to Carcassonne as a result.\n\nThe start of modern psychological operations in war is generally dated to the World War I. By that point, Western societies were increasingly educated and urbanized, and mass media was available in the form of large circulation newspapers and posters. It was also possible to transmit propaganda to the enemy via the use of airborne leaflets or through explosive delivery systems like modified artillery or mortar rounds.\n\nAt the start of the war, the belligerents, especially the British and Germans, began distributing propaganda, both domestically and on the Western front. The British had several advantages that allowed them to succeed in the battle for world opinion; they had one of the world's most reputable news systems, with much experience in international and cross-cultural communication, and they controlled much of the undersea cable system then in operation. These capabilities were easily transitioned to the task of warfare.\n\nThe British also had a diplomatic service that kept up good relations with many nations around the world, in contrast to the reputation of the German services. While German attempts to foment revolution in parts of the British Empire, such as Ireland and India, were ineffective, extensive experience in the Middle East allowed the British to successfully induce the Arabs to revolt against the Ottoman Empire.\n\nIn August 1914, David Lloyd George appointed Charles Masterman MP, to head a Propaganda Agency at Wellington House. A distinguished body of literary talent was enlisted for the task, with its members including Arthur Conan Doyle, Ford Madox Ford, G. K. Chesterton, Thomas Hardy, Rudyard Kipling and H. G. Wells. Over 1,160 pamphlets were published during the war and distributed to neutral countries, and eventually, to Germany. One of the first significant publications, the \"Report on Alleged German Outrages\" of 1915, had a great effect on general opinion across the world. The pamphlet documented atrocities, both actual and alleged, committed by the German army against Belgian civilians. A Dutch illustrator, Louis Raemaekers, provided the highly emotional drawings which appeared in the pamphlet.\n\nIn 1917, the bureau was subsumed into the new Department of Information and branched out into telegraph communications, radio, newspapers, magazines and the cinema. In 1918, Viscount Northcliffe was appointed Director of Propaganda in Enemy Countries. The department was split between propaganda against Germany organized by H.G Wells and against the Austro-Hungarian Empire supervised by Wickham Steed and Robert William Seton-Watson; the attempts of the latter focused on the lack of ethnic cohesion in the Empire and stoked the grievances of minorities such as the Croats and Slovenes. It had a significant effect on the final collapse of the Austro-Hungarian Army at the Battle of Vittorio Veneto.\n\nAerial leaflets were dropped over German trenches containing postcards from prisoners of war detailing their humane conditions, surrender notices and general propaganda against the Kaiser and the German generals. By the end of the war, MI7b had distributed almost 26 million leaflets. The Germans began shooting the leaflet-dropping pilots, prompting the British to develop unmanned leaflet balloons that drifted across no-man's land. At least one in seven of these leaflets were not handed in by the soldiers to their superiors, despite severe penalties for that offence. Even General Hindenburg admitted that \"Unsuspectingly, many thousands consumed the poison\", and POWs admitted to being disillusioned by the propaganda leaflets that depicted the use of German troops as mere cannon fodder. In 1915, the British began airdropping a regular leaflet newspaper \"Le Courrier de l'Air\" for civilians in German-occupied France and Belgium.\nAt the start of the war, the French government took control of the media to suppress negative coverage. Only in 1916, with the establishment of the Maison de la Presse, did they begin to use similar tactics for the purpose of psychological warfare. One of its sections was the \"Service de la Propagande aérienne\" (Aerial Propaganda Service), headed by Professor Tonnelat and Jean-Jacques Waltz, an Alsatian artist code-named \"Hansi\". The French tended to distribute leaflets of images only, although the full publication of US President Woodrow Wilson's Fourteen Points, which had been heavily edited in the German newspapers, was distributed via airborne leaflets by the French.\n\nThe Central Powers were slow to use these techniques; however, at the start of the war the Germans succeeded in inducing the Sultan of the Ottoman Empire to declare 'holy war', or Jihad, against the Western infidels. They also attempted to foment rebellion against the British Empire in places as far afield as Ireland, Afghanistan, and India. The Germans' greatest success was in giving the Russian revolutionary, Lenin, free transit on a sealed train from Switzerland to Finland after the overthrow of the Tsar. This soon paid off when the Bolshevik Revolution took Russia out of the war.\n\nAdolf Hitler was greatly influenced by the psychological tactics of warfare the British had employed during WWI, and attributed the defeat of Germany to the effects this propaganda had on the soldiers. He became committed to the use of mass propaganda to influence the minds of the German population in the decades to come. By calling his movement The Third Reich, he was able to convince many civilians that his cause was not just a fad, but the way of their future. Joseph Goebbels was appointed as Propaganda Minister when Hitler came to power in 1933, and he portrayed Hitler as a messianic figure for the redemption of Germany. Hitler also coupled this with the resonating projections of his orations for effect.\n\nGermany's \"Fall Grün\" plan of invasion of Czechoslovakia had a large part dealing with psychological warfare aimed both at the Czechoslovak civilians and government as well as, crucially, at Czechoslovak allies. It became successful to the point that Germany gained support of UK and France through appeasement to occupy Czechoslovakia without having to fight an all-out war, sustaining only minimum losses in covert war before the Munich Agreement.\n\nAt the start of the Second World War, the British set up the Political Warfare Executive to produce and distribute propaganda. Through the use of powerful transmitters, broadcasts could be made across Europe. Sefton Delmer managed a successful black propaganda campaign through several radio stations which were designed to be popular with German troops while at the same time introducing news material that would weaken their morale under a veneer of authenticity. British Prime Minister Winston Churchill made use of radio broadcasts for propaganda against the Germans.\nDuring World War II, the British made extensive use of deception – developing many new techniques and theories. The main protagonists at this time were 'A' Force, set up in 1940 under Dudley Clarke, and the London Controlling Section, chartered in 1942 under the control of John Bevan. Clarke pioneered many of the strategies of military deception. His ideas for combining fictional orders of battle, visual deception and double agents helped define Allied deception strategy during the war, for which he has been referred to as \"the greatest British deceiver of WW2\".\n\nDuring the lead up to the Allied invasion of Normandy, many new tactics in psychological warfare were devised. The plan for Operation Bodyguard set out a general strategy to mislead German high command as to the exact date and location of the invasion. Planning began in 1943 under the auspices of the London Controlling Section (LCS). A draft strategy, referred to as Plan Jael, was presented to Allied high command at the Tehran Conference. Operation Fortitude was intended to convince the Germans of a greater Allied military strength than existed, through fictional field armies, faked operations to prepare the ground for invasion and leaked information about the Allied order of battle and war plans.\n\nElaborate naval deceptions (Operations \"Glimmer\", \"Taxable\" and \"Big Drum\") were undertaken in the English Channel. Small ships and aircraft simulated invasion fleets lying off Pas de Calais, Cap d'Antifer and the western flank of the real invasion force. At the same time Operation \"Titanic\" involved the RAF dropping fake paratroopers to the east and west of the Normandy landings.\nThe deceptions were implemented with the use of double agents, radio traffic and visual deception. The British \"Double Cross\" anti-espionage operation had proven very successful from the outset of the war, and the LCS was able to use double agents to send back misleading information about Allied invasion plans. The use of visual deception, including mock tanks and other military hardware had been developed during the North Africa campaign. Mock hardware was created for \"Bodyguard\"; in particular, dummy landing craft were stockpiled to give the impression that the invasion would take place near Calais.\n\nThe Operation was a strategic success and the Normandy landings caught German defences unaware. Subsequent deception led Hitler into delaying reinforcement from the Calais region for nearly seven weeks.\n\nThe United States ran an extensive program of psychological warfare during the Vietnam War. The Phoenix Program had the dual aim of assassinating National Liberation Front of South Vietnam (NLF or Viet Cong) personnel and terrorizing any potential sympathizers or passive supporters. Chieu Hoi program of the South Vietnam government promoted NLF defections.\n\nWhen members of the PRG were assassinated, CIA and Special Forces operatives placed playing cards in the mouth of the deceased as a calling card. During the Phoenix Program, over 19,000 NLF supporters were killed. The United States also used tapes of distorted human sounds and played them during the night making the Vietnamese soldiers think that the dead were back for revenge.\n\nThe CIA made extensive use of Contra soldiers to destabilize the Sandinista government in Nicaragua. The CIA used psychological warfare techniques against the Panamanians by delivering unlicensed TV broadcasts. The United States government has used propaganda broadcasts against the Cuban government through TV Marti, based in Miami, Florida. However, the Cuban government has been successful at jamming the signal of TV Marti.\n\nIn the Iraq War, the United States used the shock and awe campaign to psychologically maim and break the will of the Iraqi Army to fight.\n\nIn cyberspace, social media has enabled the use of disinformation on a wide scale. Analysts have found evidence of doctored or misleading photographs spread by social media in the Syrian Civil War and 2014 Russian military intervention in Ukraine, possibly with state involvement. Military and governments have engaged in psychological operations (PSYOPS) and informational warfare on social networking platforms to regulate foreign propaganda, which includes countries like the US, Russia, and China.\n\nMost modern uses of the term psychological warfare, refers to the following military methods:\n\nMost of these techniques were developed during World War II or earlier, and have been used to some degree in every conflict since. Daniel Lerner was in the OSS (the predecessor to the American CIA) and in his book, attempts to analyze how effective the various strategies were. He concludes that there is little evidence that any of them were dramatically successful, except perhaps surrender instructions over loudspeakers when victory was imminent. It should be noted, though, that measuring the success or failure of psychological warfare is very hard, as the conditions are very far from being a controlled experiment.\n\nLerner also divides psychological warfare operations into three categories:\n\nLerner points out that grey and black operations ultimately have a heavy cost, in that the target population sooner or later recognizes them as propaganda and discredits the source. He writes, \"This is one of the few dogmas advanced by Sykewarriors that is likely to endure as an axiom of propaganda: Credibility is a condition of persuasion. Before you can make a man do as you say, you must make him believe what you say.\" Consistent with this idea, the Allied strategy in World War II was predominantly one of truth (with certain exceptions).\n\nAccording to U.S. military analysts, attacking the enemy’s mind is an important element of the People's Republic of China's military strategy. This type of warfare is rooted in the Chinese Stratagems outlined by Sun Tzu in \"The Art of War\" and \"Thirty-Six Stratagems\". In its dealings with its rivals, China is expected to utilize Marxism to mobilize communist loyalists, as well as flex its economic and military muscle to persuade other nations to act in China's interests. The Chinese government also tries to control the media to keep a tight hold on propaganda efforts for its people.\n\nIn the German Bundeswehr, the Zentrum Operative Information and its subordinate Batallion für Operative Information 950 are responsible for the PSYOP efforts (called Operative Information in German). Both the center and the battalion are subordinate to the new \"Streitkräftebasis\" (Joint Services Support Command, SKB) and together consist of about 1,200 soldiers specialising in modern communication and media technologies. One project of the German PSYOP forces is the radio station \"Stimme der Freiheit\" (Sada-e Azadi, Voice of Freedom), heard by thousands of Afghans. Another is the publication of various newspapers and magazines in Kosovo and Afghanistan, where German soldiers serve with NATO.\n\nThe British were one of the first major military powers to use psychological warfare in the First and Second World Wars. In current the British Armed Forces, PSYOPS are handled by the tri-service 15 Psychological Operations Group. (See also MI5 and Secret Intelligence Service). The Psychological Operations Group comprises over 150 personnel, approximately 75 from the regular Armed Services and 75 from the Reserves. The Group supports deployed commanders in the provision of psychological operations in operational and tactical environments.\n\nThe Group was established immediately after the 1991 Gulf War, has since grown significantly in size to meet operational requirements, and from 2015 it will be one of the sub-units of the 77th Brigade, formerly called the Security Assistance Group. Stephen Jolly, the MOD's Director of Defence Communications and former Chair of the UK's National Security Communications Committee (2013–15), is thought to be the most senior serving psyops officer within British Defence.\n\nIn June 2015, NSA files published by Glenn Greenwald revealed details of the JTRIG group at British intelligence agency GCHQ covertly manipulating online communities. This is in line with JTRIG's goal: to \"destroy, deny, degrade [and] disrupt\" enemies by \"discrediting\" them, planting misinformation and shutting down their communications.\n\nThe term psychological warfare is believed to have migrated from Germany to the United States in 1941. During World War II, the United States Joint Chiefs of Staff defined psychological warfare broadly, stating \"Psychological warfare employs \"any\" weapon to influence the mind of the enemy. The weapons are psychological only in the effect they produce and not because of the weapons themselves.\" The U.S. Department of Defense currently defines psychological warfare as:\n\"The planned use of propaganda and other psychological actions having the primary purpose of influencing the opinions, emotions, attitudes, and behavior of hostile foreign groups in such a way as to support the achievement of national objectives.\"\n\nThis definition indicates that a critical element of the U.S. psychological operations capabilities includes propaganda and by extension counterpropaganda. Joint Publication 3-53 establishes specific policy to use public affairs mediums to counterpropaganda from foreign origins.\n\nThe purpose of United States psychological operations is to induce or reinforce attitudes and behaviors favorable to US objectives. The Special Activities Division (SAD) is a division of the Central Intelligence Agency's National Clandestine Service, responsible for Covert Action and \"Special Activities\". These special activities include covert political influence (which includes psychological operations) and paramilitary operations. SAD's political influence group is the only US unit allowed to conduct these operations covertly and is considered the primary unit in this area.\n\nDedicated psychological operations units exist in the United States Army. The United States Navy also plans and executes limited PSYOP missions. United States PSYOP units and soldiers of all branches of the military are prohibited by law from targeting U.S. citizens with PSYOP within the borders of the United States (Executive Order S-1233, DOD Directive S-3321.1, and National Security Decision Directive 130). While United States Army PSYOP units may offer non-PSYOP support to domestic military missions, they can only target foreign audiences.\n\nA U.S. Army field manual released in January 2013 states that \"Inform and Influence Activities\" are critical for describing, directing, and leading military operations. Several Army Division leadership staff are assigned to “planning, integration and synchronization of designated information-related capabilities.\"\n\n\nNATO\n\nUK\n\nUS specific:\n\nWorld War II:\n\nUSSR\n\nRelated:\n\n"}
{"id": "22521226", "url": "https://en.wikipedia.org/wiki?curid=22521226", "title": "RESOL", "text": "RESOL\n\nRESOL – Elektronische Regelungen GmbH is a German solar thermal technology company based in Hattingen. It is, according to a market survey of Sun & Wind Energy magazine (issue 2/2007), the worldwide market leader in solar thermal control technology with a market share of almost 50%. Among the products are solar thermal controllers, pump stations and a range of additional modules and accessories. 100% of the company is owned by its founder and General Manager, Mr. Rudolf Pfeil.\n\nRESOL was founded in Hattingen in 1977, where the founder produced the first solar controllers in his own private flat. A few years later, the enterprise moved to the neighbouring town of Sprockhövel, but moved back to Hattingen into a new building in 1998. In 2007, another new building was added, and currently (April 2009) an annexe to the first plant is being built. It will be inaugurated in summer, increasing the total plant and office space to 6,600 m².\n\nToday (2009), RESOL employs 160 people; in 2008, a 29 Mio. Euro turnover was generated.\n\nIn the 32 years since its founding, RESOL has grown from a one-man-company into a market leading enterprise which is active all over the world. Especially during the last few years, the growth rate increased enormously, leading to a record growth rate of nearly 80% in 2008.\n\nCrucial for this development was the implementation of an intensive process management, which was started in 2005. With the help of a consultancy that specialises in the implementation of synchronous production systems on the basis of the Toyota Production System, the assembly lines were turned from a manufactory into a flexible, efficient industrial production. Process chains have been streamlined, workplace layouts changed radically and the cycle times have been shortened considerably.\n\nThe investment and the effort paid off – since the implementation of the synchronous production, the turnover generated by RESOL increased by more than 400%.\n\nResearch and development, production as well as administration take place in the RESOL headquarters in Hattingen, Germany.\n\nRESOL has subsidiaries in France and in Spain as well as agencies in the UK, Sweden and Bulgaria. Authorised distributors sell RESOL products in Poland, the Czech Republic, Russia, the Netherlands, Portugal, Jordan, Iran, the United Arab Emirates, the US, Canada and Japan.\n\nDue to several strategic partnerships with companies that distribute RESOL products under their own brands as OEM clients, controllers made by RESOL can be found in more than 50 countries worldwide.\n\nFor outstanding commitment to the generation and usage of solar energy, RESOL was awarded with the title „Solar-Unternehmen 2000” (solar company 2000) by the „Solar-Unternehmen 2001+” initiative.\n\nIn 2001, the FlowCon Pro pump station was presented with the IF Design Award, awarded by the iF International Forum Design GmbH. In the year 2008, the award again came to Hattingen, this time for the FlowCon D pump station.\n\nThe FlowCon Pro pump station also won the ISH Design Plus Award.\n\nSeveral RESOL products have been presented with the Red dot design award of the Design Zentrum Nordrhein Westfalen, for example the solar controllers RS600 MIDI and DeltaSol A as well as the FlowCon S pump station.\n\nAt Intersolar trade fair in Munich, May 2009, the newly developed RESOL FlowCon Sensor pump station was presented with the Intersolar Award in the solar thermal category.\n\nwww.sunwindenergy.com\n\n"}
{"id": "13276879", "url": "https://en.wikipedia.org/wiki?curid=13276879", "title": "Racetrack memory", "text": "Racetrack memory\n\nRacetrack memory or domain-wall memory (DWM) is an experimental non-volatile memory device under development at IBM's Almaden Research Center by a team led by physicist Stuart Parkin. In early 2008, a 3-bit version was successfully demonstrated. If it were to be developed successfully, racetrack would offer storage density higher than comparable solid-state memory devices like flash memory and similar to conventional disk drives, with higher read/write performance.\n\nRacetrack memory uses a spin-coherent electric current to move magnetic domains along a nanoscopic permalloy wire about 200 nm across and 100 nm thick. As current is passed through the wire, the domains pass by magnetic read/write heads positioned near the wire, which alter the domains to record patterns of bits. A racetrack memory device is made up of many such wires and read/write elements. In general operational concept, racetrack memory is similar to the earlier bubble memory of the 1960s and 1970s. Delay line memory, such as mercury delay lines of the 1940s and 1950s, are a still-earlier form of similar technology, as used in the UNIVAC and EDSAC computers. Like bubble memory, racetrack memory uses electrical currents to \"push\" a sequence of magnetic domains through a substrate and past read/write elements. Improvements in magnetic detection capabilities, based on the development of spintronic magnetoresistive sensors, allow the use of much smaller magnetic domains to provide far higher bit densities.\nIn production, it was expected that the wires could be scaled down to around 50 nm. There were two arrangements considered for racetrack memory. The simplest was a series of flat wires arranged in a grid with read and write heads arranged nearby. A more widely studied arrangement used U-shaped wires arranged vertically over a grid of read/write heads on an underlying substrate. This would allow the wires to be much longer without increasing its 2D area, although the need to move individual domains further along the wires before they reach the read/write heads results in slower random access times. Both arrangements offered about the same throughput performance. The primary concern in terms of construction was practical; whether or not the three dimensional vertical arrangement would be feasible to mass-produce.\n\nProjections in 2008 suggested that racetrack memory would offer performance on the order of 20-32 ns to read or write a random bit. This compared to about 10,000,000 ns for a hard drive, or 20-30 ns for conventional DRAM. The primary authors discussed ways to improve the access times with the use of a \"reservoir\" to about 9.5 ns. Aggregate throughput, with or without the reservoir, would be on the order of 250-670 Mbit/s for racetrack memory, compared to 12800 Mbit/s for a single DDR3 DRAM, 1000 Mbit/s for high-performance hard drives, and 1000 to 4000 Mbit/s for flash memory devices. The only current technology that offered a clear latency benefit over racetrack memory was SRAM, on the order of 0.2 ns, but at a higher cost. larger feature size \"F\" of about 45 nm (as of 2011) with a cell area of about 140 F.\n\nRacetrack memory is one among several emerging technologies that aim to replace conventional memories such as DRAM and Flash, and potentially offer a universal memory device applicable to a wide variety of roles. Other contenders included magnetoresistive random-access memory (MRAM), phase-change memory (PCRAM) and ferroelectric RAM (FeRAM). Most of these technologies offer densities similar to flash memory, in most cases worse, and their primary advantage is the lack of write-endurance limits like those in flash memory. Field-MRAM offers excellent performance as high as 3 ns access time, but requires a large 25-40 F² cell size. It might see use as an SRAM replacement, but not as a mass storage device. The highest densities from any of these devices is offered by PCRAM, with a cell size of about 5.8 F², similar to flash memory, as well as fairly good performance around 50 ns. Nevertheless, none of these can come close to competing with racetrack memory in overall terms, especially density. For example, 50 ns allows about five bits to be operated in a racetrack memory device, resulting in an effective cell size of 20/5=4 F², easily exceeding the performance-density product of PCM. On the other hand, without sacrificing bit density, the same 20 F² area could fit 2.5 2-bit 8 F² alternative memory cells (such as resistive RAM (RRAM) or spin-torque transfer MRAM), each of which individually operating much faster (~10 ns).\n\nIn most cases, memory devices store one bit in any given location, so they are typically compared in terms of \"cell size\", a cell storing one bit. Cell size itself is given in units of F², where \"F\" is the feature size design rule, representing usually the metal line width. Flash and racetrack both store multiple bits per cell, but the comparison can still be made. For instance, hard drives appeared to be reaching theoretical limits around 650 nm²/bit, defined primarily by the capability to read and write to specific areas of the magnetic surface. DRAM has a cell size of about 6 F², SRAM is much less dense at 120 F². NAND flash memory is currently the densest form of non-volatile memory in widespread use, with a cell size of about 4.5 F², but storing three bits per cell for an effective size of 1.5 F². NOR flash memory is slightly less dense, at an effective 4.75 F², accounting for 2-bit operation on a 9.5 F² cell size. In the vertical orientation (U-shaped) racetrack, nearly 10-20 bits are stored per cell, which itself would have a physical size of at least about 20 F². In addition, bits at different positions on the \"track\" would take different times (from ~10 to ~1000 ns, or 10 ns/bit) to be accessed by the read/write sensor, because the \"track\" would move the domains at a fixed rate of ~100 m/s past the read/write sensor. There are software utilities for modeling single and multi-bit racetrack memory designs.\n\nOne limitation of the early experimental devices was that the magnetic domains could be pushed only slowly through the wires, requiring current pulses on the orders of microseconds to move them successfully. This was unexpected, and led to performance equal roughly to that of hard drives, as much as 1000 times slower than predicted. Recent research has traced this problem to microscopic imperfections in the crystal structure of the wires which led to the domains becoming \"stuck\" at these imperfections. Using an X-ray microscope to directly image the boundaries between the domains, their research found that domain walls would be moved by pulses as short as a few nanoseconds when these imperfections were absent. This corresponds to a macroscopic performance of about 110 m/s.\n\nThe voltage required to drive the domains along the racetrack would be proportional to the length of the wire. The current density must be sufficiently high to push the domain walls (as in electromigration). A difficulty for racetrack technology arises from the need for high current density (>10 A/cm²); a 30 nm x 100 nm cross-section would require >3 mA. The resulting power draw becomes higher than that required for other memories, e.g., spin-transfer torque memory (STT-RAM) or flash memory.\n\nAnother challenge associated with Racetrack memory is the stochastic nature in which the domain walls move, i.e., they move and stop at random positions. There have been attempts to overcome this challenge by producing notches at the edges of the nanowire . Researchers have also proposed stepped nanowires to pin the domain walls precisely. Recently researchers have proposed non-geometrical approaches such as local modulation of magnetic properties through composition modification. Techniques such as annealing induced diffusion and ion-implantation are used.\n\n\n"}
{"id": "3123513", "url": "https://en.wikipedia.org/wiki?curid=3123513", "title": "Rice transplanter", "text": "Rice transplanter\n\nA rice transplanter is a specialized transplanter fitted to transplant rice seedlings onto paddy field. Mainly two types of rice transplanter i.e., riding type and walking type. Riding type is power driven and can usually transplant six lines in one pass. On the other hand, walking type is manually driven and can usually transplant four lines in one pass.\n\nAlthough rice is grown in areas other than Asia, rice transplanters are used mainly in East, Southeast, and South Asia. This is because rice can be grown without transplanting, by simply sowing seeds on field, and farmers outside Asia prefer this fuss-free way at the expense of reduced yield.\n\nA common rice transplanter comprises:\n\nMachine transplanting using rice transplanters requires considerably less time and labor than manual transplanting. It increases the approximate area that a person can plant from 700 to 10,000 square metres per day.\n\nHowever, rice transplanters are considerably expensive for almost all Asian small-hold farmers. Rice transplanters are popular in industrialized countries where labor cost is high, for example in South Korea. It is now also becoming more popular in South Asian countries because, at transplanting time, labour shortage is at peak levels.\n\nRice transplanters were first developed in Japan in the 1960s, whereas the earliest attempt to mechanize rice transplanting dates back to late 19th century. In Japan, development and spread of rice transplanters progressed rapidly during the 1970s and 1980s.\n\n"}
{"id": "42505492", "url": "https://en.wikipedia.org/wiki?curid=42505492", "title": "Software diagnosis", "text": "Software diagnosis\n\nSoftware diagnosis (also: \"software diagnostics\") refers to concepts, techniques, and tools that allow for obtaining findings, conclusions, and evaluations about software systems and their implementation, composition, behavior, and evolution. It serves as means to monitor, steer, observe and optimize software development, software maintenance, and software re-engineering in the sense of a business intelligence approach specific to software systems. It is generally based on the automatic extraction, analysis, and visualization of corresponding information sources of the software system. It can also be manually done and not automatic.\n\nSoftware diagnosis supports all branches of software engineering, in particular project management, quality management, risk management as well as implementation and test. Its main strength is to support all stakeholders of software projects (in particular during software maintenance and for software re-engineering tasks) and to provide effective communication means for software development projects. For example, software diagnosis facilitates \"bridging an essential information gap between management and development, improve awareness, and serve as early risk detection instrument\". Software diagnosis includes assessment methods for \"perfective maintenance\" that, for example, apply \"visual analysis techniques to combine multiple indicators for low maintainability, including code complexity and entanglement with other parts of the system, and recent changes applied to the code\".\n\nIn contrast to manifold approaches and techniques in software engineering, software diagnosis does not depend on programming languages, modeling techniques, software development processes or the specific techniques used in the various stages of the software development process. Instead, software diagnosis aims at analyzing and evaluating the software system in its as-is state and based on system-generated information to bybass any subjective or potentially outdated information sources (e.g., initial software models). For it, software diagnosis combines and relates sources of information that are typically not directly linked. Examples: \n\nThe core principle of software diagnosis is to automatically extract information from all available information sources of a given software projects such as source code base, project repository, code metrics, execution traces, test results, etc. To combine information, software-specific data mining, analysis, and visualization techniques are applied. Its strength results, among various reasons, from integrating decoupled information spaces in the scope of a typical software project, for example development and developer activities (recorded by the repository) and code and quality metrics (derived by analyzing source code) or key performance indicators (KPIs).\n\nExamples of software diagnosis tools include software maps and software metrics.\n\nSoftware diagnosis—in contrast to many approaches in software engineering—does not assume that developer capabilities, development methods, programming or modeling languages are right or wrong (or better or worse compared to each other): Software diagnosis aims at giving insight into a given software system and its status \"regardless of the methods, languages, or models\" used to create and maintain the system.\n\n\n"}
{"id": "46411064", "url": "https://en.wikipedia.org/wiki?curid=46411064", "title": "Staged Detonation", "text": "Staged Detonation\n\nStaged detonation is an overall principle used for many explosive devices. A general description of the principle is that one explosive compound with one set of properties is detonated, and the detonation wave from this is transfers into another explosive compound with different set of properties.\n\nThe principle is most often used for detonating insensitive explosive compounds by using stages of less and less sensitive explosive compounds. A simple example is the common industrial detonator, where a very small amount of a sensitive explosive detonates, causing the less sensitive main charge of the detonator to detonate. It may involve additional steps where successively less sensitive compounds trigger a detonation wave in each other, with stages including various types of booster charges.\n\nStaged detonation may be used to initiate cheap and insensitive mining explosives, but the principle has other uses. In certain applications there is a need to modify the shape of a detonation wave travelling through an explosive material. One example of this can be found in explosive lenses. Here the properties of different explosive compounds are used to initiate each stage in the detonation with a specific relative delay, creating a detonation wave with a non conventional shape. That shape can for example be a spherical wave travelling in towards the centre of that sphere, or any other shape required. This principle is used in many areas in high energy research, as well as in nuclear weapons.\n\nIt does not necessarily involve different explosive compounds. Because the properties of an explosive compound is in some degree related to its density, which is a result of with what pressure it is compressed during manufacture, the stages in a staged detonation may also come from each section having been pressed to a different density, giving it varying properties.\n\nThe term Staged detonation may also refer to a method where several explosive charges are set to detonate in series with a given delay between them. This method called Delayed detonation. This method is commonly used in industrial demolition activities such as mining.\n\nThe use of a primary and secondary explosive was the first practical use of the staged detonation principle, and was revolutionary to the safety of the mining industry. After this staged detonation is used in everything from explosive lenses in nuclear weapons (where the different properties of each explosive create a perfectly spherical detonation wave moving in towards origo) to mining where each blasting operation requires several stages to detonate each main charge, not only the primary and secondary explosive used in the simplest staged detonation.\n"}
{"id": "9775613", "url": "https://en.wikipedia.org/wiki?curid=9775613", "title": "Superadobe", "text": "Superadobe\n\nSuperadobe is a form of earthbag construction that was developed by Iranian architect Nader Khalili. The technique uses layered long fabric tubes or bags filled with adobe to form a compression structure. The resulting beehive shaped structures employs corbelled arches, corbelled domes, and vaults to create sturdy single and double-curved shells. It has received growing interest for the past two decades in the natural building and sustainability movements.\n\nAlthough it is not known exactly how long, Earth bag shelters have been used for decades, primarily as implements of refuge in times of war. Military infantrymen have used sand filled sacks to create bunkers and barriers for protection prior to World War I. In the last century other earthbag buildings have undergone extensive research and are slowly beginning to gain worldwide recognition as a plausible solution to provide affordable housing. \nGerman architect Frei Otto is said to have experimented with earth bags, as is more recently Gernot Minke. It was Nader Khalili who popularized earthbag construction. Initially in 1984 in response to a NASA call for housing designs for future human settlements on the Moon and on Mars he proposed to use moon dust to fill the plastic Superadobe tubes and velcro together the layers (instead of barbed wire). He came to term his particular technique of earthbag construction \"Superadobe\". Some projects have been done using bags as low-tech foundations for straw-bale construction. They can be covered in a waterproof membrane to keep the straw dry.\nIn 1995 15 refugee shelters were built in Iran, by Nader Khalili and the United Nations Development Programme (UNDP) and the United Nations High Commissioner for Refugees (UNHCR) in response to refugees from the Persian Gulf War. According to Khalili the cluster of 15 domes that was built could have been repeated by the thousands. The government dismantled the camp a few years later.\nSince then, the Superadobe Method has been put to use in Canada, Mexico, Brazil, Belize, Costa Rica, Chile, Iran, India, Russia, Mali, and Thailand, as well as in the U.S.\nWhile Superadobe constructions have generally been limited to approximately 4 meters in diameter, larger structures have been created by grouping several \"beehives\" together to form a network of domes. There is a 32' (10m) dome being constructed in the St. Ignacio area of Belize, which when finished will be the centre dome of an eco-resort complex.\n\nSuperadobe's earthbag technique lends itself to a wide range of materials. Polypropylene tubing is ideal, although burlap is also sufficient. Likewise, while sand, cement or lime are preferred, virtually any fill material (e.g. gravel, crushed volcanic rock or rice hulls) will work. \n\nAfter materials are gathered and the dimensions of the building are decided upon, a circular foundation trench is dug, approximately 1 foot deep and 8-14 feet in diameter, giving room for at least two layers of earthbags to be laid down underground. A chain is anchored to the ground in the center of the circle and used like a pair of compasses to trace the shape of the base. Another chain is fastened just outside the dome wall: this is the fixed or height guide and provides an interior measurement for the layers as they corbel higher, ensuring the accuracy of each new layer as it is laid and tamped. \n\nBetween layers of tamped, filled tubes, loop of barbed wire functions as mortar and holds the structure together. Window voids can be placed in several ways: either by rolling the filled tube back on itself around a circular plug (forming an arched header) or by sawing out a Gothic or pointed arch void after the filler material has set. \n\nOnce the corbelled dome is complete, it can be covered in several different kinds of exterior treatments, both for aesthetic reasons and to protect the structure from environmental damage such as that from ultraviolet radiation. Like the materials for the construction itself, there are multiple choices. While CalEarth names plaster as the most common finishing option, soil and living grass have also been used. Khalili has also used a mix of earth and plaster, further covered by a \"reptile\" layer of cement and earth balls that strengthen the finish by redirecting stress.\n\nAccording to Khalili's website, in an emergency, impermanent shelters can be built with unskilled labor, using only dirt with no cement or lime, and for the sake of speed of construction windows can be punched out later due to the strength of the compressive nature of the dome/beehive. Superadobe is not an exact art and similar materials may be substituted if the most ideal ones are not readily available. Ordinary sand bags can also be used to form the dome if no Superadobe tubes can be procured; this in fact was how the original design was developed. \n\nIn an interview with an AIA (American Institute of Architects) representative, Nader Khalili, super adobe’s founder and figurehead, said this about the emergency shelter aspects of Superadobe: \n\n\n\n"}
{"id": "664585", "url": "https://en.wikipedia.org/wiki?curid=664585", "title": "Upholstery regulator", "text": "Upholstery regulator\n\nAn upholstery regulator is an upholstery tool which smooths irregularities in the stuffing beneath the coverings. \n\nWhilst it looks similar to a needle it is heavier; like needles the regulator comes in various gauges and lengths. It is used to poke through the various layers to adjust the stuffing before the final cover is put in place. \n\nA related tool is the stuffing iron, which is used to push the stuffing into the curves and corners of a piece of furniture; it has a narrow piece of steel with one toothed edge to grab loose stuffing and place it in the hard to reach spots.\n"}
{"id": "47066830", "url": "https://en.wikipedia.org/wiki?curid=47066830", "title": "Vermiponics", "text": "Vermiponics\n\nVermiponics is a soil-less growing technique that combines hydroponics with vermiculture by utilizing diluted wormbin leachate (\"worm tea\") as the nutrient solution as opposed to the use of fish waste (as used in aquaponics) or the addition of manufactured chemicals to provide the nutrients.\n\nIn 2008, Saginaw Valley State University in Michigan was using vermiponics as part of its efforts to reduce waste. Aquaponics growers have noted previously that redworms can be added to aquaponic grow beds successfully and serve useful functions in aquaponic systems. This has led several people to experiment with nutrient solutions that are based on wormbin leachate alone, for instance in a Central Queensland University trial.\nThe effectiveness of vermiponics compared to hydroponics and aquaponics has not been thoroughly studied however a paper from the University of Arizona has found that using wormtea has beneficial effects on root protrusion in lettuce seedlings when compared to inorganic fertilizer.\n\nAs hydroponics is based on feeding plants inorganic fertilizer and as many aquaponic growers use commercial fish feed, it has been suggested that vermiponics is a more sustainable method of food production as worm castings can be used from local food waste rather than mined fertilizer or sea caught fish.\n"}
