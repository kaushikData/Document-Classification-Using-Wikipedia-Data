{"id": "27509530", "url": "https://en.wikipedia.org/wiki?curid=27509530", "title": "2010 KQ", "text": "2010 KQ\n\n2010 KQ is a small asteroid-like object that has been discovered in an orbit about the Sun that is so similar to the Earth's orbit that scientists strongly suspect it to be a rocket stage that escaped years ago from the Earth–Moon system. The object was discovered on May 16, 2010 by Richard Kowalski at the Catalina Sky Survey, and has subsequently been observed by many observers, including Bill Ryan (Magdalena Ridge Observatory) and Peter Birtwhistle (England). It was given the asteroid designation 2010 KQ by the Minor Planet Center in Cambridge, Massachusetts, who identified its orbit as being very similar to that of the Earth. Orbit refinements by JPL's Paul Chodas and amateur astronomer Bill Gray have shown that this object was very close to the Earth in early 1975, but the trajectory is not known with enough accuracy to associate the object with any particular launch. Nevertheless, scientists do not expect that a natural object could remain in this type of orbit for very long because of its relatively high impact probability with the Earth. In fact, an analysis carried out by Paul Chodas suggests that 2010 KQ has a 6% chance of impacting the Earth over a 30-year period starting in 2036.\n\nNear-infrared spectral measurements of this object carried out by S.J. Bus (University of Hawaii) using the NASA IRTF telescope on Mauna Kea, Hawaii, indicate that its spectral characteristics do not match those of any of the known asteroid types, and in fact are similar to those of a rocket body. The object's absolute magnitude (28.9) also suggests that it is only a few meters in size, about the size of an upper stage. Additional observations over the coming months should allow scientists to discern how strongly solar radiation pressure affects the object's motion, a result that could help distinguish a solid, rocky asteroid from a lighter man-made object.\n\nAstronomer Richard Miles believes 2010 KQ may be the 4th stage of the Russian Proton rocket from the Luna 23 mission, launched October 28, 1974.\n\nEven in the unlikely event that this object is headed for impact with the Earth, whether it is an asteroid or rocket body, it is so small that it would disintegrate in the atmosphere and not cause harm on the ground.\n\n\n"}
{"id": "35971170", "url": "https://en.wikipedia.org/wiki?curid=35971170", "title": "5D BIM", "text": "5D BIM\n\n5D BIM, an acronym for 5D Building Information Modeling, is a term used in the CAD and construction industries, and refers to the intelligent linking of individual 3D CAD components or assemblies with schedule (time - 4D BIM) constraints\n\n5D = 3D + SCHEDULE + COST\n\nThe creation of 5D models enables the various participants (from architects, designers, contractors to owners) of a construction project to visualize the progress of construction activities and its related costs over time. This BIM-centric project management technique has potential to improve management and delivery of projects of any size or complexity.\n\nIn June 2016, McKinsey, a consultancy, identified 5D BIM technology as one of five big ideas poised to disrupt construction.It defined 5D BIM as \"a five-dimensional representation of the physical and functional characteristics of any project. It considers a project’s cost and schedule in addition to the standard spatial design parameters in 3-D.\" \n\nIn Germany and Europe at large, 5D BIM technology is prevalent among large general contractors.\n\n"}
{"id": "1242", "url": "https://en.wikipedia.org/wiki?curid=1242", "title": "Ada (programming language)", "text": "Ada (programming language)\n\nAda is a structured, statically typed, imperative, and object-oriented high-level computer programming language, extended from Pascal and other languages. It has built-in language support for design-by-contract, extremely strong typing, explicit concurrency, tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international standard; the current version (known as Ada 2012) is defined by ISO/IEC 8652:2012.\n\nAda was originally designed by a team led by Jean Ichbiah of CII Honeywell Bull under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages used by the DoD at that time. Ada was named after Ada Lovelace (1815–1852), who has been credited as the first computer programmer.\n\nAda was originally targeted at embedded and real-time systems. The Ada 95 revision, designed by S. Tucker Taft of Intermetrics between 1992 and 1995, improved support for systems, numerical, financial, and object-oriented programming (OOP).\n\nFeatures of Ada include: strong typing, modularity mechanisms (packages), run-time checking, parallel processing (tasks, synchronous message passing, protected objects, and nondeterministic select statements), exception handling, and generics. Ada 95 added support for object-oriented programming, including dynamic dispatch.\n\nThe syntax of Ada minimizes choices of ways to perform basic operations, and prefers English keywords (such as \"or else\" and \"and then\") to symbols (such as \"||\" and \"&&\"). Ada uses the basic arithmetical operators \"+\", \"-\", \"*\", and \"/\", but avoids using other symbols. Code blocks are delimited by words such as \"declare\", \"begin\", and \"end\", where the \"end\" (in most cases) is followed by the identifier of the block it closes (e.g., \"if … end if\", \"loop … end loop\"). In the case of conditional blocks this avoids a \"dangling else\" that could pair with the wrong nested if-expression in other languages like C or Java.\n\nAda is designed for development of very large software systems. Ada packages can be compiled separately. Ada package specifications (the package interface) can also be compiled separately without the implementation to check for consistency. This makes it possible to detect problems early during the design phase, before implementation starts.\n\nA large number of compile-time checks are supported to help avoid bugs that would not be detectable until run-time in some other languages or would require explicit checks to be added to the source code. For example, the syntax requires explicitly named closing of blocks to prevent errors due to mismatched end tokens. The adherence to strong typing allows detection of many common software errors (wrong parameters, range violations, invalid references, mismatched types, etc.) either during compile-time, or otherwise during run-time. As concurrency is part of the language specification, the compiler can in some cases detect potential deadlocks. Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful suggestions on how to fix the error.\n\nAda also supports run-time checks to protect against access to unallocated memory, buffer overflow errors, range violations, off-by-one errors, array access errors, and other detectable bugs. These checks can be disabled in the interest of runtime efficiency, but can often be compiled efficiently. It also includes facilities to help program verification. For these reasons, Ada is widely used in critical systems, where any anomaly might lead to very serious consequences, e.g., accidental death, injury or severe financial loss. Examples of systems where Ada is used include avionics, ATC, railways, banking, military and space technology.\n\nAda's dynamic memory management is high-level and type-safe. Ada does not have generic or untyped pointers; nor does it implicitly declare any pointer type. Instead, all dynamic memory allocation and deallocation must take place through explicitly declared \"access types\".\nEach access type has an associated \"storage pool\" that handles the low-level details of memory management; the programmer can either use the default storage pool or define new ones (this is particularly relevant for Non-Uniform Memory Access). It is even possible to declare several different access types that all designate the same type but use different storage pools.\nAlso, the language provides for \"accessibility checks\", both at compile time and at run time, that ensures that an \"access value\" cannot outlive the type of the object it points to.\n\nThough the semantics of the language allow automatic garbage collection of inaccessible objects, most implementations do not support it by default, as it would cause unpredictable behaviour in real-time systems. Ada does support a limited form of region-based memory management; also, creative use of storage pools can provide for a limited form of automatic garbage collection, since destroying a storage pool also destroys all the objects in the pool.\n\nA double-dash (\"--\"), resembling an em dash, denotes comment text. Comments stop at end of line, to prevent unclosed comments from accidentally voiding whole sections of source code. Disabling a whole block of code now requires the prefixing of each line (or column) individually with \"--\". While clearly denoting disabled code with a column of repeated \"--\" down the page this renders the experimental dis/re-enablement of large blocks a more drawn out process.\n\nThe semicolon (\";\") is a statement terminator, and the null or no-operation statement is codice_1. A single codice_2 without a statement to terminate is not allowed.\n\nUnlike most ISO standards, the Ada language definition (known as the \"Ada Reference Manual\" or \"ARM\", or sometimes the \"Language Reference Manual\" or \"LRM\") is free content. Thus, it is a common reference for Ada programmers and not just programmers implementing Ada compilers. Apart from the reference manual, there is also an extensive rationale document which explains the language design and the use of various language constructs. This document is also widely used by programmers. When the language was revised, a new rationale document was written.\n\nOne notable free software tool that is used by many Ada programmers to aid them in writing Ada source code is the GNAT Programming Studio.\n\nIn the 1970s, the US Department of Defense (DoD) was concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and the UK Ministry of Defence requirements. After many iterations beginning with an original Straw man proposal the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996.\n\nThe HOLWG working group crafted the Steelman language requirements, a series of documents stating the requirements they felt a programming language should satisfy. Many existing languages were formally reviewed, but the team concluded in 1977 that no existing language met the specifications.\n\nRequests for proposals for a new programming language were issued and four contractors were hired to develop their proposals under the names of Red (Intermetrics led by Benjamin Brosgol), Green (CII Honeywell Bull, led by Jean Ichbiah), Blue (SofTech, led by John Goodenough) and Yellow (SRI International, led by Jay Spitzen). In April 1978, after public scrutiny, the Red and Green proposals passed to the next phase. In May 1979, the Green proposal, designed by Jean Ichbiah at CII Honeywell Bull, was chosen and given the name Ada—after Augusta Ada, Countess of Lovelace. This proposal was influenced by the programming language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual\nwas published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was approved on December 10, 1980 (Ada Lovelace's birthday), and\ngiven the number MIL-STD-1815 in honor of Ada Lovelace's birth year. In 1981, C. A. R. Hoare took advantage of his Turing Award speech to criticize Ada for being overly complex and hence unreliable, but subsequently seemed to recant in the foreword he wrote for an Ada textbook.\n\nAda attracted much attention from the programming community as a whole during its early days. Its backers and others predicted that it might become a dominant language for general purpose programming and not just defense-related work. Ichbiah publicly stated that within ten years, only two programming languages would remain, Ada and Lisp. Early Ada compilers struggled to implement the large, complex language, and both compile-time and run-time performance tended to be slow and tools primitive. Compiler vendors expended most of their efforts in passing the massive, language-conformance-testing, government-required \"ACVC\" validation suite that was required in another novel feature of the Ada language effort.\n\nThe first validated Ada implementation was the NYU Ada/Ed translator, certified on April 11, 1983. NYU Ada/Ed is implemented in the high-level set language SETL. A number of commercial companies began offering Ada compilers and associated development tools, including Alsys, TeleSoft, DDC-I, Advanced Computer Techniques, Tartan Laboratories, TLD Systems, Verdix, and others.\nIn 1991, the US Department of Defense began to require the use of Ada (the \"Ada mandate\") for all software, though exceptions to this rule were often granted. The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace COTS technology. Similar requirements existed in other NATO countries: Ada was required for NATO systems involving command and control and other functions, and Ada was the mandated or preferred language for defense-related applications in countries such as Sweden, Germany, and Canada.\n\nBy the late 1980s and early 1990s, Ada compilers had improved in performance, but there were still barriers to full exploitation of Ada's abilities, including a tasking model that was different from what most real-time programmers were used to.\n\nBecause of Ada's safety-critical support features, it is now used not only for military applications, but also in commercial projects where a software bug can have severe consequences, e.g., avionics and air traffic control, commercial rockets such as the Ariane 4 and 5, satellites and other space systems, railway transport and banking.\nFor example, the Airplane Information Management System, the fly-by-wire system software in the Boeing 777, was written in Ada. Developed by Honeywell Air Transport Systems in collaboration with consultants from DDC-I, it became arguably the best-known of any Ada project, civilian or military. The Canadian Automated Air Traffic System was written in 1 million lines of Ada (SLOC count). It featured advanced distributed processing, a distributed Ada database, and object-oriented design. Ada is also used in other air traffic systems, e.g., the UK’s next-generation Interim Future Area Control Tools Support (iFACTS) air traffic control system is designed and implemented using SPARK Ada.\nIt is also used in the French TVM in-cab signalling system on the TGV high-speed rail system, and the metro suburban trains in Paris, London, Hong Kong and New York City.\n\nThe language became an ANSI standard in 1983 (ANSI/MIL-STD 1815A), and after translation in French and without any further changes in English became\nan ISO standard in 1987 (ISO-8652:1987). This version of the language is commonly known as Ada 83, from the date of its adoption by ANSI, but is sometimes referred to also as Ada 87, from the date of its adoption by ISO.\n\nAda 95, the joint ISO/ANSI standard (ISO-8652:1995) was published in February 1995, making Ada 95 the first ISO standard object-oriented programming language. To help with the standard revision and future acceptance, the US Air Force funded the development of the GNAT Compiler. Presently, the GNAT Compiler is part of the GNU Compiler Collection.\n\nWork has continued on improving and updating the technical content of the Ada programming language. A Technical Corrigendum to Ada 95 was published in October 2001, and a major Amendment, ISO/IEC 8652:1995/Amd 1:2007 was published on March 9, 2007. At the Ada-Europe 2012 conference in Stockholm, the Ada Resource Association (ARA) and Ada-Europe announced the completion of the design of the latest version of the Ada programming language and the submission of the reference manual to the International Organization for Standardization (ISO) for approval. ISO/IEC 8652:2012 was published in December 2012.\n\nOther related standards include ISO 8651-3:1988 \"Information processing systems—Computer graphics—Graphical Kernel System (GKS) language bindings—Part 3: Ada\".\n\nAda is an ALGOL-like programming language featuring control structures with reserved words such as \"if\", \"then\", \"else\", \"while\", \"for\", and so on. However, Ada also has many data structuring facilities and other abstractions which were not included in the original ALGOL 60, such as type definitions, records, pointers, enumerations. Such constructs were in part inherited from or inspired by Pascal.\n\nA common example of a language's syntax is the Hello world program:\n\nwith Ada.Text_IO; use Ada.Text_IO;\nprocedure Hello is\nbegin\nend Hello;\n\nThis program can be compiled by using the freely available open source compiler GNAT, by executing\ngnatmake hello.adb\n\nAda's type system is not based on a set of predefined primitive types but allows users to declare their own types. This declaration in turn is not based on the internal representation of the type but on describing the goal which should be achieved. This allows the compiler to determine a suitable memory size for the type, and to check for violations of the type definition at compile time and run time (i.e., range violations, buffer overruns, type consistency, etc.). Ada supports numerical types defined by a range, modulo types, aggregate types (records and arrays), and enumeration types. Access types define a reference to an instance of a specified type; untyped pointers are not permitted.\nSpecial types provided by the language are task types and protected types.\n\nFor example, a date might be represented as:\ntype Day_type is range 1 .. 31;\ntype Month_type is range 1 .. 12;\ntype Year_type is range 1800 .. 2100;\ntype Hours is mod 24;\ntype Weekday is (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday);\n\ntype Date is\nTypes can be refined by declaring subtypes:\n\nsubtype Working_Hours is Hours range 0 .. 12; -- at most 12 Hours to work a day\nsubtype Working_Day is Weekday range Monday .. Friday; -- Days to work\n\nWork_Load: constant array(Working_Day) of Working_Hours -- implicit type declaration\n\nTypes can have modifiers such as \"limited, abstract, private\" etc. Private types can only be accessed and limited types can only be modified or copied within the scope of the package that defines them.\nAda 95 adds additional features for object-oriented extension of types.\n\nAda is a structured programming language, meaning that the flow of control is structured into standard statements. All standard constructs and deep level early exit are supported so the use of the also supported 'go to' commands is seldom needed.\n-- while a is not equal to b, loop.\nwhile a /= b loop\nend loop;\n\nif a > b then\nelse\nend if;\n\nfor i in 1 .. 10 loop\nend loop;\n\nloop\nend loop;\n\ncase i is\nend case;\n\nfor aWeekday in Weekday'Range loop -- loop over an enumeration\nend loop;\nAmong the parts of an Ada program are packages, procedures and functions.\n\nExample:\nPackage specification (example.ads)\n\npackage Example is\nend Example;\n\nPackage body (example.adb)\n\nwith Ada.Text_IO;\npackage body Example is\n\n-- package initialization executed when the package is elaborated\nbegin\nend Example;\n\nThis program can be compiled, e.g., by using the freely available open source compiler GNAT, by executing\ngnatmake -z example.adb\n\nPackages, procedures and functions can nest to any depth and each can also be the logical outermost block.\n\nEach package, procedure or function can have its own declarations of constants, types, variables, and other procedures, functions and packages, which can be declared in any order.\n\nAda has language support for task-based concurrency. The fundamental concurrent unit in Ada is a \"task\", which is a built-in limited type. Tasks are specified in two parts – the task declaration defines the task interface (similar to a type declaration), the task body specifies the implementation of the task.\nDepending on the implementation, Ada tasks are either mapped to operating system threads or processes, or are scheduled internally by the Ada runtime.\n\nTasks can have entries for synchronisation (a form of synchronous message passing). Task entries are declared in the task specification. Each task entry can have one or more \"accept\" statements within the task body. If the control flow of the task reaches an accept statement, the task is blocked until the corresponding entry is called by another task (similarly, a calling task is blocked until the called task reaches the corresponding accept statement). Task entries can have parameters similar to procedures, allowing tasks to synchronously exchange data. In conjunction with \"select\" statements it is possible to define \"guards\" on accept statements (similar to Dijkstra's guarded commands).\n\nAda also offers \"protected objects\" for mutual exclusion. Protected objects are a monitor-like construct, but use guards instead of conditional variables for signaling (similar to conditional critical regions). Protected objects combine the data encapsulation and safe mutual exclusion from monitors, and entry guards from conditional critical regions. The main advantage over classical monitors is that conditional variables are not required for signaling, avoiding potential deadlocks due to incorrect locking semantics. Like tasks, the protected object is a built-in limited type, and it also has a declaration part and a body.\n\nA protected object consists of encapsulated private data (which can only be accessed from within the protected object), and procedures, functions and entries which are guaranteed to be mutually exclusive (with the only exception of functions, which are required to be side effect free and can therefore run concurrently with other functions). A task calling a protected object is blocked if another task is currently executing inside the same protected object, and released when this other task leaves the protected object. Blocked tasks are queued on the protected object ordered by time of arrival.\n\nProtected object entries are similar to procedures, but additionally have \"guards\". If a guard evaluates to false, a calling task is blocked and added to the queue of that entry; now another task can be admitted to the protected object, as no task is currently executing inside the protected object. Guards are re-evaluated whenever a task leaves the protected object, as this is the only time when the evaluation of guards can have changed.\n\nCalls to entries can be \"requeued\" to other entries with the same signature. A task that is requeued is blocked and added to the queue of the target entry; this means that the protected object is released and allows admission of another task.\n\nThe \"select\" statement in Ada can be used to implement non-blocking entry calls and accepts, non-deterministic selection of entries (also with guards), time-outs and aborts.\n\nThe following example illustrates some concepts of concurrent programming in Ada.\n\nwith Ada.Text_IO; use Ada.Text_IO;\n\nprocedure Traffic is\n\nbegin\nend Traffic;\nA pragma is a compiler directive that conveys information to the compiler to allow specific manipulation of compiled output. Certain pragmas are built into the language while others are implementation-specific.\n\nExamples of common usage of compiler pragmas would be to disable certain features, such as run-time type checking or array subscript boundary checking, or to instruct the compiler to insert object code in lieu of a function call (as C/C++ does with inline functions).\n\n\n\n\n\n"}
{"id": "4064391", "url": "https://en.wikipedia.org/wiki?curid=4064391", "title": "Advanced Digital Information Corporation", "text": "Advanced Digital Information Corporation\n\nAdvanced Digital Information Corporation (ADIC) was an American manufacturer of tape libraries and storage management software which is now part of Quantum Corp. Their product line included both hardware, such as the \"Scalar\" line of robotic tape libraries, and software, such as the StorNext File System and the StorNext Storage Manager, a Hierarchical Storage Management system. Partners and resellers included Apple, Dell, EMC, Fujitsu-Siemens, HP, IBM and Sun.\n\nADIC was acquired by Quantum in August 2006.\n"}
{"id": "4379279", "url": "https://en.wikipedia.org/wiki?curid=4379279", "title": "Alag", "text": "Alag\n\nAlag () is a 2006 Bollywood film starring Akshay Kapoor and Dia Mirza and directed by Ashu Trikha and produced by Subi Samuel.\n\nWidower Hemant Rastogi lives in scenic Mahabaleshwar, seemingly alone. One night he has a heart attack and passes away. When the police search his residence they find Tejas, Hemant's only son, in the basement of the house. Tejas has spent his entire life in the basement and as a result of this is extremely sensitive to sunlight. The Police ask Purva Rana, head of P.R. Institute (an institute for the rehabilitation of young criminals) to look after Tejas, whose only experience of other people up until this point has been with his father and the books he provided for him. Tejas starts showing signs of Telekenesis, and is shunned by the other boys in the institute, resulting in the near-fatal accident of a security guard and the death of a fellow student. Tejas redeems himself in the eyes of Purva's wealthy father, Pushkar, when he is able to wake his wife, Gayatri Rana, from a coma-like condition. It looks like Tejas has been accepted into the Rana household, but he is subsequently harassed by doctors and scientists wishing to perform experiments on him. When Tejas and Pushkar both refuse to be part of these experiments, Tejas is abducted and held in a glass chamber by Dr. Richard Dyer, who wants to control his mind for his own benefit. Purva realises that Tejas has been abducted, and is fatally injured by Dr. Dyer in a rescue attempt. At the sight of this, Tejas' anger causes his powers to surge, shattering the glass cage, killing Dr. Dyer and electrocuting Purva. Tejas brings her back to life by shocking her with his hands. Purva and Tejas driving away into the sunset.\n\n\n"}
{"id": "17366982", "url": "https://en.wikipedia.org/wiki?curid=17366982", "title": "Ammonium dinitramide", "text": "Ammonium dinitramide\n\nAmmonium dinitramide (ADN) is the ammonium salt of dinitraminic acid. ADN decomposes under heat to leave only nitrogen, oxygen, and water. The ions are the ammonium ion NH and the dinitramide N(NO).\n\nIt makes an excellent solid rocket oxidizer with a slightly higher specific impulse than ammonium perchlorate and more importantly, does not leave hydrogen chloride fumes. This property is also of military interest because halogen free smoke is harder to detect. It decomposes into low molecular mass gases so it contributes to higher performance without creating excessive temperatures if used in gun or rocket propellants. The salt is prone to detonation under high temperatures and shock more so than the perchlorate.\n\nThe EURENCO Bofors company produced LMP-103S as a 1-to-1 substitute for hydrazine by dissolving 65% ammonium dinitramide, NHN(NO), in 35% water solution of methanol and ammonia. LMP-103S has 6% higher specific impulse and 30% higher impulse density than hydrazine monopropellant. Additionally, hydrazine is highly toxic and carcinogenic, while LMP-103S is only moderately toxic. LMP-103S is UN Class 1.4S allowing for transport on commercial aircraft, and was demonstrated on the Prisma satellite in 2010. Special handling is not required. LMP-103S could replace hydrazine as the most commonly used monopropellant.\n\nAmmonium dinitramid was invented in 1971 in Zelinskiy Institute of Organic Chemistry in the USSR. Initially all information related to this compound was classified because of its use as a rocket propellant, particularly in Topol-M intercontinental ballistic missiles. In 1989 ammonium dinitramid was independently discovered at SRI International. SRI obtained US and international patents for ADN in the mid-1990s, at which time scientists from the former Soviet Union revealed they had discovered ADN 18 years earlier.\n\nIn laboratory ammonium dinitramid can be prepared by nitration of sulfaminic acid or its salt under low temperatures. The process is performed under red light, since the compound is decomposed by higher energy photons. The details of the synthesis remain classified.\nOther sources report ammonium synthesis from ammonium nitrate, anhydrous nitric acid, and fuming sulfuric acid containing 20% free sulfur trioxide. A base other than ammonia must be added before the acid dinitramide decomposes. The final product is obtained by fractional crystallization.\n\n"}
{"id": "2073134", "url": "https://en.wikipedia.org/wiki?curid=2073134", "title": "BancNet", "text": "BancNet\n\nBancNet (also spelled Bancnet) is a Philippine-based interbank network connecting the ATM networks of local and offshore banks, and the largest interbank network in the Philippines in terms of the number of member banks and annual transactions.\n\nBancNet is also the exclusive gateway of China's UnionPay, allowing access to the nearly 1 billion ATM cardholders from China. BancNet is allied with global payment brand JCB International. Through this alliance, JCB cardholders can now do cash advances at participating BancNet member ATMs nationwide.\nBancnet interconnects with international card networks Diners Club, Discover Card, KFTC, MasterCard, and VISA\nBancNet serves more than 41 million ATM cardholders of its 114 members and affiliates with over 12,000 ATMs and more than 5,000 POS terminals.\n\nIn 2008, Expressnet outsourced its ATM operations to BancNet. On January 30, 2015, BancNet and MegaLink announced their merger and will retain itself as its brand.\n\nBancNet was founded on July 17, 1990 as the Philippines' second ATM consortium when the ATMs of eight banks, PCI Bank (later Equitable PCI Bank, now Banco de Oro), Security Bank, Chinabank, RCBC, Allied Bank(now part of PNB), Metrobank, International Exchange Bank (now part of UnionBank) and CityTrust Banking Corp. (now part of BPI) formed BancNet. Other members have since joined.\n\nIn 1994, BancNet introduced the a point-of-sale system to serve the retail payment requirements of cardholders. In 1997, BancNet started offering website hosting, email and surfing services to member banks at affordable rates.\n\nIn 2002, BancNet started its online banking and payment gateway system.\n\nIt was during the anniversary business forum of 2002 that BancNet introduced Interbank Funds Transfer, reportedly the first consortium-run automated transfer facility in the region. The product allows real-time, online transfer of money among the members of BancNet using either the payment gateway, the ATM or a cardholder's cellular phone.\n\nA partnership with Globe Telecom, the second largest telecoms company in the Philippines, in 2006 allowed BancNet to expand ATM-like functions to the mobile phones of cardholders. This was followed by a similar agreement in early 2007 with Smart Communications, the dominant telecoms company, for mobile banking.\n\nA Memorandum of Agreement was signed in 2007 with Nationlink, admitting the latter as the first network alliance member of BancNet. This allows all the rural bank members of Nationlink to enjoy the convenience of electronic banking on 8,000 ATMs and more than 10,000 Point-of-Sale terminals nationwide.\n\n\n\n\n"}
{"id": "1725842", "url": "https://en.wikipedia.org/wiki?curid=1725842", "title": "Blickensderfer typewriter", "text": "Blickensderfer typewriter\n\nThe Blickensderfer Typewriter was invented by George Canfield Blickensderfer (1850–1917) and patented on August 4, 1891. Two models were initially unveiled to the public at the 1893 World's Columbian Exposition in Chicago, the Model 1 and the Model 5. His machines were originally intended to compete with larger Remington, Hammond and Yost typewriters, and were the first truly portable, full-keyboard typewriters. The design also enabled the typist to see the typed work at a time when most typewriters were understrike machines that concealed the writing. When Blickensderfer unveiled his small Model 5 at the 1893 World's Fair, a stripped-down version of his larger more complex Model 1 machine, these revolutionary features attracted huge crowds and a full order book – many of them from Britain, Germany and France, whose business machine markets were more highly developed than the United States. \nThe Blickensderfer typewriters were initially manufactured in a rented factory on Garden Street in Stamford, Connecticut. By 1896, due to strong foreign demand in particular for his machines, Blickensderfer opened a new and modern factory on Atlantic Street in Stamford. According to an article published by the Stamford Historical Society \" Blickensderfer's typewriter...became the world's best seller, and the company became one of the world's largest typewriter manufacturers\". The factory employed about 200 people and produced about 10,000 typewriters per year at its peak (1903-1907) until the factory closed in 1919. The first commercially successful model was the No. 5, which sold for $35, compared to the benchmark machines of the day which cost $100 or more. Each new model 5 came in a simple wooden carrying case with an extra typewheel, a dozen ink rolls and a tool kit.\n\nGeorge Blickensderfer's invention dramatically reduced the complexity of the typewriter design. A typical Blick contained only 250 parts, compared to the 2,500 parts of a standard typewriter. It was much smaller, lighter and cheaper than others. Some of the first aluminum typewriters, marketed as the Blickensderfer 6 and the Blick Featherweight, were made by Blickensderfer, as was the world's first fully electric typewriter, the Blickensderfer Electric.\n\nInstead of the common mechanism with letters on the end of individual type bars connected to the keys, the first Blickensderfer prototype used a cylindrical typewheel with four rows of characters; lower case, upper case, italics and short words embossed on it. It was later modified to three rows of characters: lower case, upper case and a row of numbers and symbols. Depressing a key caused the typewheel to turn so the correct letter was positioned over the paper. As the wheel turned it moved downward, contacting an ink roller prior to striking the paper. This allowed for greater speed in typing as there were no keys to become jammed or stuck together. The interchangeable typewheel principle is very similar to the IBM Selectric design introduced almost 70 years later in 1961. Like the Selectric, one could easily change the typeface or the font style on a Blickensderfer simply by changing the typewheel.\n\nBlickensderfers were also notable for their unique keyboard layout developed by George Blickensderfer after careful analysis of the English language. The home, or lowest row of keys, contained the most commonly used letters. Blickensderfer determined that 70% of the most commonly used letters and 85% of words contained the letters DHIATENSOR. This positioning allowed the typist to keep his hands on the home row as much as possible, minimizing extraneous hand movement and increasing efficiency. The QWERTY keyboard introduced on the Sholes & Glidden typewriter in 1874 was designed for purely mechanical reasons and the chances of the keys striking each other and jamming was more limited with this configuration. Because the Blickensderfer used the typewheel, the \"scientific\" keyboard layout could be used for maximum typing efficiency.\n\nBlickensderfer typewriters were sold in France under the Dactyle name. The machines were sold in Canada by the Creelman Brothers Company of Georgetown, Ontario. They were also sold and marketed in Great Britain, Germany, New Zealand and Russia. The keyboards and type wheels were available in numerous languages, including French, Spanish, German and Polish. Some machines, called the Oriental, were adapted so the carriage moved from left to right which accommodated Hebrew and Arabic languages. \nThe company saw much of their European business decline with the onset of World War I, and production and sales were particularly hard hit when the United States joined the war in 1917. As part of the war effort Blickensderfer converted much of his factory to produce munitions for the war, including a machine gun belt feed for the French government and 50 caliber machine gun mounts for aircraft. In the same year a fatal blow was delivered to the company when George Blickensderfer died on August 15, 1917, after a short illness. With the brilliant chief engineer and designer gone, the company could not continue, and the heirs sold the company in 1919. After several unsuccessful years under different ownership, the Remington Typewriter Company acquired the assets, including tools, parts, drawings and intellectual rights from the bankrupt L.R. Roberts Company in 1926. Remington attempted to introduce a modified Blick 5 called the Rem-Blick to the market, but by the end of 1928 the model was discontinued.\n\nThe first widely successful production model was the Blickensderfer 5, introduced at the 1893 World's Columbian Exposition in Chicago were. Production of the model 5 did not seriously get underway until 1895-1896. The 5 was the first portable, full keyboard, typewriter and came with the DHIATENSOR keyboard as standard or with a QWERTY keyboard available on request. Some of the earliest Blick 5's were sold in France as the Dactyle . The Blick 5 was the simplest of all Blickensderfer models and more were produced than any other Blickensderfer machine. About 74,000 or 37% of the 200,000 Blick typewriters produced, were No. 5's and their production continued until 1913, when other models became more popular.\n\nIn 1910 George Blickensderfer introduced the Blickensderfer 6, which was cast in aluminum and was essentially a lighter version of the Blickensderfer 5 which used cast iron. The aluminum version also appeared as the Blick Featherweight, which was an improved Model 6 with a backspace mechanism and the ink arm support was modified to an attractive curved folding design. These typewriters weighed only 5 pounds and were widely marketed as the Five-Pound Secretary.\n\nThe Model 7, first introduced in 1897, became the deluxe version of the basic No. 5 and was designed to please a greater cross section of the market. The biggest and most obvious difference was the wraparound space bar, which gave the machine a more solid and distinctive look. Blickensderfer also improved the paper scale, added black composite carriage knobs at both ends of the palten, added adjustable margin stops and a bell mechanism designed to be struck by a clapper attached to the inker arm indicating when the carriage was approaching the end of the typed line. The Blick 7 was also mounted on an oak base fitted with an attractive bentwood laminated oak case. About 63,000 Blick No. 7's were produced from 1897 until 1916.\n\nThe Blick Electric was a revolutionary machine and was far ahead of its time when it was first introduced at the Pan-American Exhibition in Buffalo in 1901 following its patent in August 1900 (patent no.656,085). It had all the familiar characteristics of the manual models, plus a QWERTY or a DHIATENSOR keyboard and all the advantages of later electric typewriters, including a light key touch, even typing, and automatic carriage return and line spacing. The machine was powered by an Emerson electric motor mounted on the rear and switched on by turning a Yale key on the side. The motor ran on 104 Volt 60 Hz AC electric current, which was not yet widely standard at the time. (See War of Currents)\n\nIt is believed that the Blick Electric was produced from 1901 to 1919. Although a technological and engineering success, it was a commercial failure as at the time many homes and businesses were not wired for electricity. Electricity was primarily used for lighting and therefore not widely available during the daytime.\nThe Blick Electrics were advertised and marketed in both France and Britain, although it is not known how many machines were ultimately sold. \nVery few machines are known to exist today, and they are found with both straight fronts and QWERTY keyboards or curved fronts and DHIATENSOR keyboards.\n\nThe Blickensderfer 8 introduced in 1908 was the first Blick to boast a tabulator system (see Tab stop), even though tabulators had been around for some time. This model was a considerable success and more were sold in 1908 than any other model. This machine was more massive and sturdier looking than the 7 with a two-piece typehead casting and a popular backspace mechanism. The tabulator used large nickel-plated levers placed on top of the machine, making it easy to operate. \nProduction peaked in 1910 and declined until production ended in 1917. About 20,000 Blick 8's were produced, accounting for about 10% of all Blick models sold.\n\nThe Model 9, believed to be introduced in mid-1910, was similar in appearance to the Model 8, but lacked the nickel plated tabulator keys and also featured a folding ink roller arm support similar to the Blick Featherweight. Only about 10,000 machines were produced, ending in 1919.\n\nOn August 4, 1891, after several prototypes, Blickensderfer patented The Blickensderfer Type Writing Machine. The central component was the use of the cylindrical shaped interchangeable type element which he initially patented on July 15, 1890. An earlier variation to Blickensderfer's typewheel was introduced on the Crandall 1 typewriter in 1883 but Blickensderfer's genius was in the use of the typewheel combined with the simplicity and the small size of his machine. This invention was unlike anything else on the market and was considered a mechanical marvel that took typewriter engineering to a new level. Striking a key turned the wheel to the appropriate orientation while inking the typeface as it tipped downward past the ink roller to strike the paper. Holding the Cap or Fig keys shifted the typewheel along its axis to use either the middle row, for capital letters, or the lower row, for special characters and numbers.\n\nIndustrial production in the 1890s was not highly automated and assembly was done completely by hand. This allowed for easy design changes and improvements to the production process. Blickensderfer continued to improve his machines and a number of new patents were filed for the Blick 5 up to June 1, 1897, when the machine largely remained the same for the balance of its production life. Blickensderfer continued to invent new machines until his death, including the Blick Electric and various standard machines with the Blick 9 being his last major new model.\n\nThe typewheel, like the later typeball of the IBM Selectric typewriter, was the central component of all the Blick machines. It could be easily removed, allowing users to select a large variety of fonts and typeface. Later in the companies production history the basic machines keyboards could easily be adapted to other languages at the Blickensderfer factory simply by changing the removable keytop covers and adding the corresponding language typewheel.\n\nOne unique feature of the Blickensderfer typewriter was the scientific or DHIATENSOR keyboard. The DHIATENSOR layout is shown below (with alphanumeric characters only): Blickensderfer analyzed the English language and proposed a unique and more efficient keyboard based on his research. He determined that the ten most frequently used letter were A,D,E,H,I,N,O,R,S and T and were used in about 70% of written text and in about 85% of all words. The middle row contained letters that occurred 24% of the time and the top row about 6%. Blickensderfer offered the scientific keyboard in the hopes that his more efficient system would replace the clumsy but widely used QWERTY or universal keyboard developed by Sholes and Glidden in 1874. Unfortunately for Blickensderfer, no other typewriter manufacturer adopted the scientific keyboard and Blickensderfer had no choice but to offer the universal keyboard on all his machines as an option. Toward the end of the company's existence, most machines were sold with the universal keyboard. There were modifications to the layout made for Spanish, French, German, and Polish.\n\nIn 1936 Prof August Dvorak proposed a similar keyboard layout with a more logical design to the QWERTY keyboard. His theory was similar to Blickensderfer's as he tried to minimize the distance then typists fingers travelled. He proposed the same common letters as Blickensderfer in the \"middle row\" of his keyboard with the exception of the R which he replaced with a punctuation key. The Dvorak keyboard continues to have a small but loyal following to this day and is available on most major operating systems.\n\n\n"}
{"id": "48180931", "url": "https://en.wikipedia.org/wiki?curid=48180931", "title": "Burrowing vehicle", "text": "Burrowing vehicle\n\nA burrowing vehicle is a (manned or unmanned) vehicle that would travel through the earth/rock as it moves, while underground. The Afeka College of Engineering is testing an autonomous drone vehicle which could burrow underground and destroy hidden bunkers or underground areas/dwellings.\n\nIn 2010, the Weapons and Capabilities Division (RD-CXW) of the Defense Threat Reduction Agency (DTRA) submitted a request for information (RFI) regarding technologies suitable for use in the Robotic Underground Munition (RUM). According to the RFI, \"the RUM would be a one-time use, air-delivered, highly mobile vehicle having certain characteristics similar to an unmanned ground vehicle.\" The design is to be able to \"avoid, traverse, neutralize or defeat natural and man-made obstacles\", all of which would be encountered in an attempt to break fortified structures. According to \"Popular Science\", \"One of DTRA's missions is keeping weapons of mass destruction in check, so it's probably safe to assume the main objective here is to be able to locate and, if necessary, destroy underground weapons caches or development sites.\" \n"}
{"id": "6875545", "url": "https://en.wikipedia.org/wiki?curid=6875545", "title": "Business Software Association of Australia", "text": "Business Software Association of Australia\n\nThe Business Software Association of Australia (BSAA, founded in 1989) was an industry association in Australia of commercial software producers and corporations that advocated software copyright compliancy and assisted the litigation of copyright infringement through support and funding. The BSAA typically retained the law firm Mallesons Stephen Jacques (a particularly notable Australian law firm) to investigate and litigate its interests.\n\nThe association published a range of auditing tools and provided information on software copyright and license compliancy.\n\nThe association had been particularly more active in the defense of copyright infringement since January 2006 when Australian Copyright law was changed to make the running of an enterprise with infringing software a criminal offense.\n\nAs of January 2007, the BSAA is now the Australian branch of the Business Software Alliance.\n\nThe full members of the BSAA were Adobe Systems, Microsoft, Computer Associates, Symantec, Autodesk and Apple Computer.\n\nLike most industry associations that support litigation of copyright matters, the BSAA was occasionally mentioned as being draconian, although it is considerably more measured in action it takes than its counterparts in the UK and United States. \n\nAssociations such as the BSAA are also sometimes considered as an example of a 'good hand, bad hand' tactic as its members can actively pursue their interests through the association and draw criticism to the association rather than themselves as members.\n"}
{"id": "33963660", "url": "https://en.wikipedia.org/wiki?curid=33963660", "title": "Chicken eyeglasses", "text": "Chicken eyeglasses\n\nChicken eyeglasses, also known as chickens specs, chicken goggles, generically as pick guards and under other names, were small eyeglasses made for chickens intended to prevent feather pecking and cannibalism. They differ from blinders as they allowed the bird to see forward whereas blinders do not. One variety used rose-colored lenses as the coloring was thought to prevent a chicken wearing them from recognizing blood on other chickens which may increase the tendency for abnormal injurious behavior. They were mass-produced and sold throughout the United States as early as the beginning of the 20th century.\n\nChicken eyeglasses were often made from celluloid or aluminum and typically consisted of \"two oval panels that fit over the upper beak of the chicken. A pin is put through the nostril to-hold the oval pieces in place.\" Different designs were produced that attached to the chicken's head in different ways. Some were held in place by a strap, some by small hooks into the nares (nostrils) and some by piercing the bone septum between the nostrils with a cotter pin. Due to the piercing of tissue, this last type of design is illegal in some countries. \n\nSome versions of the devices had lenses that were semi- or fully transparent whereas others were tinted, often red- or rose-colored. Other designs were blinders which are opaque and completely prevent forward vision. The intended purposes of chicken eyeglasses were to prevent aggressive pecking, cannibalism and feather pecking.\n\nChicken eyeglasses are an alternative to beak trimming, which is the removal of approximately one-third of the beak by a cold or heated blade, or an infrared beam, usually when chicks are 1-day old. This is often effective in reducing pecking injuries, but causes pain and has significant effects on chicken welfare.\n\nRed-tinted lenses were considered to be effective in reducing internecine pecking because they disguise the color of blood. As summed up in a 1953 article in Indiana's \"National Road Traveler\" newspaper, \"The deep rose-colored plastic lenses make it impossible for the cannibal [chicken] to see blood on the other chickens, although permitting it to see the grain on the ground.\"\n\nElmer Haas of the National Band & Tag Company, a major producer of rose-colored chicken eyeglasses, whose grandfather had \"devised wire frames for chickens in 1902\", indicated that he believed the purported blood-masking effect of the rose coloring was a myth: \"the firm added the rose colored glasses because it indulged the chicken owners ... [c]hickens are color blind\". (In fact, chickens, like other birds, have good color vision.) The firm had added the rose-colored feature to its glasses in 1939 under the brand name \"Anti-Pix\". This variety of eyeglasses was more complicated than others because the red lenses were fixed to a hinge at the top of the frame. This meant that as the hen lowered its head to feed, the lens swung out giving the hen an unobstructed view of the ground. When the hen raised her head, as she would during aggression, the lens would swing down giving the hen a red tinted perception of the environment. \n\nRose-colored contact lenses, rather than eyeglasses, have also been proposed to reduce cannibalism in chickens.\n\nA form of chicken eyeglasses was first patented in 1903 by Andrew Jackson Jr. of Munich, Tennessee, as an \"Eye-protector for chickens\". In the U.S. they were available through the mail order company Sears-Roebuck, or through chicken feed stores for a few cents. The eyeglasses are no longer produced by the National Band & Tag Company, but are sought as collector's items. \n\nUsing chicken eyeglasses was still practiced in 1973, evident by a report in Illinois' \"The Hawk-Eye\" newspaper that a farmer had 8,000 chickens fitted with the rose-colored variety. One inventor of a form of the glasses proposed legislation in Kansas to require \"all\" chickens in the state to be fitted with glasses, but his campaign was unsuccessful.\n\nOn January 16, 1955, Sam Nadler of the National Farm Equipment Company of Brooklyn appeared on CBS' popular primetime television show, \"What's My Line?\" The show was in the format of a guessing game, in which a panel attempted to determine the line (occupation) of contestants. Show officials listed Mr. Nadler's occupation for the audience as \"sells 'eyeglasses' for chickens\". After the panel was unsuccessful in guessing his occupation, Mr. Nadler's identity was revealed and he stated that his company sold 2–3 million pairs of chicken eyeglasses per year. \"What's My Line?\"s director, Frank Heller, said in 1958 that the show's \"most unusual occupation\" over its then eight-season run was \"...the gentleman who makes eye glasses for chickens.\"\n\n\n"}
{"id": "8116008", "url": "https://en.wikipedia.org/wiki?curid=8116008", "title": "Cohn process", "text": "Cohn process\n\nThe Cohn process, developed by Edwin J. Cohn, is a series of purification steps with the purpose of extracting albumin from blood plasma. The process is based on the differential solubility of albumin and other plasma proteins based on pH, ethanol concentration, temperature, ionic strength, and protein concentration. Albumin has the highest solubility and lowest isoelectric point of all the major plasma proteins. This makes it the final product to be precipitated, or separated from its solution in a solid form. Albumin was an excellent substitute for human plasma in World War Two. When administered to wounded soldiers or other patients with blood loss, it helped expand the volume of blood and led to speedier recovery. Cohn's method was gentle enough that isolated albumin protein retained its biological activity.\n\nDuring the operations, the ethanol concentration change from zero initially to 40%. The pH decreases from neutral at 7 to more acidic at 4.8 over the course of the fractionation. The temperature starts at room temperature and decreases to −5 degrees Celsius. Initially, the blood is frozen. There are five major fractions. Each fraction ends with a specific precipitate. These precipitates are the separate fractions.\n\nFractions I, II, and III are precipitated out at earlier stages. The conditions of the earlier stages are 8% ethanol, pH 7.2, −3 °C, and 5.1% protein for Fraction I; 25% ethanol, pH of 6.9, −5 °C, and 3% protein. The albumin remains in the supernatant fraction during the solid/liquid extraction under these conditions. Fraction IV has several unwanted proteins that need to be removed. In order to do this, the conditions are varied in order to precipitate the proteins out. The conditions to precipitate these proteins are raising the ethanol concentration from 18 to 40% and raising the pH from 5.2 to 5.8. Finally, albumin is located in fraction V. The precipitation of albumin is done by reducing the pH to 4.8, which is near the pI of the protein, and maintaining the ethanol concentration to be 40%, with a protein concentration of 1%. Thus, only 1% of the original plasma remains in the fifth fraction.\n\nHowever, albumin is lost at each process stage, with roughly 20% of the albumin lost through precipitation stages before fraction V. In order to purify the albumin, there is an extraction with water, and adjustment to 10% ethanol, pH of 4.5 at −3 °C. Any precipitate formed here is done so by filtration and is an impurity. These precipitates are discarded. Reprecipitation, or repetition of the precipitation step in order to improve purity, is done so by raising ethanol concentration back to 40% from the extraction stage. The pH is 5.2 and it is conducted at −5 °C. Several variations of Cohn fraction were created to account for lower cost and higher yield. Generally, if the yield is high, the purity is lowered, to roughly 85-90%. \nCohn was able to start the Plasma Fractionation Laboratory after he was given massive funding from the government agencies and the private pharmaceutical companies. This led to the fractionation of human plasma. Human plasma proved to have several useful components other than albumin. Human blood plasma fractionation yielded human serum albumin, Serum Gamma Globulin, Fibrinogen, Thrombin, and Blood Group Globulins. The fibrinogen and thrombin fractions were further combined during the War into additional products, including liquid Fibrin Sealant, solid Fibrin Foam and a Fibrin Film.\nGamma Globulins are found in Fractions II and III and proved to be essential in treating measles for soldiers. Gamma globulin also was useful in treatment for polio, but did not have much effect in treating mumps or scarlet fever. Most importantly, the gamma globulins were useful in modifying and preventing infectious hepatitis during the Second World War. It eventually became a treatment for children exposed to this type of hepatitis.\n\nLiquid fibrin sealant was used in treating burn victims, including some from the attack at Pearl Harbor, to attach skin grafts with an increased success rate. It was also found helpful at re-connecting or anastomosing severed nerves. Fibrin foam and thrombin were used to control blood vessel oozing especially in liver injuries and near tumors. It also minimized bleeding from large veins as well as dealing with blood vessel malformations within the brain. Fibrin film was used to stop bleeding in various surgical applications, including neurosurgery. However, it was not useful in controlling arterial bleeding. The first fibrinogen/fibrin based product capable of stopping arterial hemorrhage was the \"Fibrin Sealant Bandage\" or \"Hemostatic Dressing (HD)\" invented by Martin MacPhee at the American Red Cross in the early 1990s, and tested in collaboration with the U.S. Army.\n\nThe Gerlough method, developed in 1955 improved process economics by reducing the consumption of ethanol. Instead of 40% in certain steps, Gerlough used 20% ethanol for precipitation. This is especially used for Fractions II and III. In addition, Gerlough combined the two fractions with IV into one step to reduce the number of fractionations required. While this method proved less expensive, it was not adopted by industry because of this combination of fractions II, III, and IV, for fear of mixing and high impurities.\n\nThe Hink method developed in 1957. This method gave higher yields through recovery of some of the plasma proteins discarded in the Fractions of IV. The improved yields, however, balanced by the lower purities obtained, within the 85% range.\n\nThe Mulford method, akin to the Hink, used the fractions II and III supernatant as the last step before finishing and heat treatment. The method combined fractions IV and V, but in this case, the albumin would not be as pure, although the yields may be higher.\n\nAnother variation was developed by Kistler and Nitschmann, to provide a purer form of albumin, even though offset by lower yields. Similar to Gerlough, the Precipitate A, which is equivalent to Cohn’s Fraction II and III, was done at a lower ethanol concentration of 19%, but the pH, in this case, was also lower to 5.85. Also similar to Gerlough and Mulford, the fraction IV was combined and precipitated at conditions of 40% ethanol, pH of 5.85, and temperature of −8 degrees C. The albumin, which is recovered in fraction V, is recovered in Precipitate C at a pH adjustment to 4.8. Similar to the Cohn Process, the albumin is purified by extraction into water followed by precipitation of the impurities at 10% ethanol, pH 4.6, and −3 degrees C. Akin to the Cohn Process, the precipitate formed here is filtered out and discarded. Then Precipitate C (fraction V) is reprecipitated at pH 5.2 and stored as a paste at −40 degrees C. This process has been more widely accepted because it separates the fractions and makes each stage independent of each other.\n\nAnother variation involved a heat ethanol fractionation. It was originally developed to inactivate the hepatitis virus. In this process, recovery of high yield, high purity albumin is the most important goal, while the other plasma proteins are neglected. In order to make sure the albumin does not denature in the heat, there are stabilizers such as sodium octanoate, which allow the albumin to tolerate higher temperatures for long periods. In heat ethanol, the plasma is heat treated at 68 degrees C with sodium octanoate with 9% ethanol at pH of 6.5. This results in improved albumin recovery with yields of 90%, and purities of 100%. It is not nearly as expensive as cold ethanol procedures such as the Cohn Process. One drawback is the presences of new antigens due to possible heat denaturation of the albumin. In addition, the other plasma proteins have practical uses and to neglect them would not be worth it. Finally, the expensive heat treatment vessels offset the lower cost compared to the cold ethanol format that do not need it. For these reasons, several companies haven not adopted this method even though it has the most impressive results. However, one prominent organization that uses it is the German Red Cross.\n\nThe latest variation was developed by Hao in 1979. This method is significantly simplified compared to the Cohn Process. Its goal is to create high albumin yields as long as albumin is the sole product. Through a two-stage process, impurities are precipitated directly from fractions II and III supernatant at 42% ethanol, pH 5.8, temperature −5 degrees C, 1.2% protein, and 0.09 ionic strength. Fraction V is precipitated at pH 4.8. Fractions I, II, III, and IV are coprecipitated at 40% ethanol, with pH of 5.4 to 7.0, and temperature −3 to −7 degrees C. Fraction V is then precipitated at pH 4.8 and −10 degrees C. The high yields are due to a combination of a simplified process, with lower losses due to coprecipitation, and use of filtration. Higher purities were also achieved at 98% because of the higher ethanol levels, but the yields were lowered with the high purity.\n\nMore recent methods involve the use of chromatography.\n\nThe Cohn process was a major development in the field of blood fractionation. It has several practical uses in treating diseases such as hepatitis and polio. It was most useful during the Second World War where soldiers recovered at a faster rate because of the transfusions with albumin. The Cohn Process has been modified over the years as seen above. In addition, it has influenced other processes with the blood fractionation industry. This has led to new forms of fractionation such as chromatographic plasma fractionation in ion exchange and albumin finishing processes. In general, the Cohn Process and its variations have given a huge boost to and serve as a foundation for the fractionation industry to this day.\n\nHowever, the process has not been studied well because it is archaic. Most importantly, it has never been modernized by manufacturing companies. In addition, the conventional process can be environmentally unfriendly because ethanol is a highly flammable substance. It is unsanitary because of the open vessels and tanks; thus, the possibility of contamination is high. The cold ethanol format may be too gentle to kill off certain viruses that require heat inactivation. Since this process remains unchanged for so long, several built-in inefficiencies and inconsistencies affect the economics of the process for pharmaceutical and manufacturing companies. One exception to this was the application in Scotland of continuous-flow processing instead of batch processing. This process was devised at the Protein Fractionation Centre (PFC), the plasma fractionation facility of the Scottish National Blood Transfusion Service (SNBTS). This process involved in-line monitoring and control of pH and temperature, with flow control of plasma and ethanol streams using precision gear pumps, all under computerised feedback control . As a result, Cohn Fractions I+II+III, IV and V were produced in a few hours, rather than over many days. The continuous-flow preparation of cryoprecipitate was subsequently integrated into the process upstream of Cohn Fractionation.\n\nNevertheless, this process still serves as a major foundation for the blood industry in general and its influence can be seen as it is referred to in the development of newer methods. Although it has its drawbacks depending on the variation, the Cohn Process’ main advantage is its practical uses and its utility within pharmacological and medical industries.\n"}
{"id": "1174172", "url": "https://en.wikipedia.org/wiki?curid=1174172", "title": "Current loop", "text": "Current loop\n\nIn electrical signalling an analog current loop is used where a device must be monitored or controlled remotely over a pair of conductors. Only one current level can be present at any time.\n\nA major application of current loops is the industry de facto standard 4–20 mA current loop for process control applications, where they are extensively used to carry signals from process instrumentation to PID controllers, SCADA systems, and programmable logic controllers (PLCs). They are also used to transmit controller outputs to the modulating field devices such as control valves. These loops have the advantages of simplicity and noise immunity, and have a large international user and equipment supplier base. Some 4–20 mA field devices can be powered by the current loop itself, removing the need for separate power supplies, and the \"smart\" HART protocol uses the loop for communications between field devices and controllers. Various automation protocols may replace analog current loops, but 4–20 mA is still a principal industrial standard.\n\nIn industrial process control, analog 4–20 mA current loops are commonly used for electronic signalling, with the two values of 4 & 20 mA representing 0–100% of the range of measurement or control. These loops are used both for carrying sensor information from field instrumentation, and carrying control signals to the process modulating devices, such as a valve.\n\nThe key advantages of the current loop are:\n\n\nField instrumentation measurements are such as pressure, temperature, level, flow, pH or other process variables. A current loop can also be used to control a valve positioner or other output actuator. Since input terminals of instruments may have one side of the current loop input tied to the chassis ground (earth), analog isolators may be required when connecting several instruments in series.\n\nThe relationship between current value and process variable measurement is set by calibration, which assigns different ranges of engineering units to the span between 4 and 20 mA. The mapping between engineering units and current can be inverted, so that 4 mA represents the maximum and 20 mA the minimum.\n\nDepending on the source of current for the loop, devices may be classified as \"active\" (supplying or \"sourcing\" power) or \"passive\" (relying on or \"sinking\" loop power). For example, a chart recorder may provide loop power to a pressure transmitter. The pressure transmitter modulates the current on the loop to send the signal to the strip chart recorder, but does not in itself supply power to the loop and so is passive. Another loop may contain two passive chart recorders, a passive pressure transmitter, and a 24 V battery. (The battery is the active device). Note that a \"4-wire\" instrument has a power supply input separate from the current loop.\n\nPanel mount displays and chart recorders are commonly termed 'indicator devices' or 'process monitors'. Several passive indicator devices may be connected in series, but a loop must have only one transmitter device and only one power source (active device).\n\nThe 4–20 mA convention was born in the 1950s out of the earlier highly successful 3–15 psi pneumatic control signal standard, when electronics became cheap and reliable enough to emulate the older standard electrically. The 3–15 psi standard had the same features of being able to power some remote devices, and have a \"live\" zero. However the 4–20 mA standard was better suited to the electronic controllers then being developed.\n\nThe transition was gradual and has extended into the 21st century, due to the huge installed base of 3–15 psi devices. As the operation of pneumatic valves over motorised valves has many cost and reliability advantages, pneumatic actuation is still an industry standard. To allow the construction of hybrid systems, where the 4–20 mA is generated by the controller, but allows the use of pneumatic valves, a range of current to pressure (I to P) converters are available from manufacturers. These are usually local to the control valve and convert 4–20 mA to 3–15 psi (or 0.2–1.0 bar). This signal is then fed to the valve actuator or more commonly, a pneumatic positioner. The positioner is a dedicated controller which has a mechanical linkage to the actuator movement. This ensures that problems of friction are overcome and the valve control element moves to the desired position. It also allows the use of higher air pressures for valve actuation.\n\nWith the development of cheap industrial micro-processors, \"smart\" valve positioners have become available since the mid-1980s and are very popular for new installations. These include an I to P converter, plus valve position and condition monitoring. These latter are fed back over the current loop to the controller, using such as the HART protocol.\n\nAnalog current loops were historically occasionally carried between buildings by dry pairs in telephone cables leased from the local telephone company. 4–20 mA loops were more common in the days of analog telephony. These circuits require end-to-end direct current (DC) continuity, and unless a dedicated wire pair was hardwired, their use ceased with the introduction of semiconductor switching. DC continuity is not available over a microwave radio, optical fibre, or a multiplexed telephone circuit connection. \nBasic DC circuit theory shows that the current is the same all along the line. It was common to see 4–20 mA circuits that had loop lengths in miles or circuits working over telephone cable pairs that were longer than ten thousand feet end-to-end. There are still legacy systems in place using this technology. In Bell System circuits, voltages up to 125 VDC were employed.\n\nDiscrete control functions can be represented by discrete levels of current sent over a loop. This would allow multiple control functions to be operated over a single pair of wires. Currents required for a specific function vary from one application or manufacturer to another. There is no specific current that is tied to a single meaning. It is almost universal that 0 mA indicates the circuit has failed. In the case of a fire alarm, 6 mA could be normal, 15 mA could mean a fire has been detected, and 0 mA would produce a trouble indication, telling the monitoring site the alarm circuit had failed. Some devices, such as two-way radio remote control consoles, can reverse the polarity of currents and can multiplex audio onto a DC current.\n\nThese devices can be employed for any remote control need a designer might imagine. For example, a current loop could actuate an evacuation siren or command synchronized traffic signals.\n\nCurrent loop circuits are one possible way used to control radio base stations at distant sites. The two-way radio industry calls this type of remote control DC remote. This name comes from the need for DC circuit continuity between the control point and the radio base station. A current loop remote control saves the cost of extra pairs of wires between the operating point and the radio transceiver. Some equipment, such as the Motorola MSF-5000 base station, uses currents below 4 mA for some functions. An alternative type, the tone remote, is more complex but requires only an audio path between control point and base station.\n\nFor example, a taxi dispatch base station might be physically located on the rooftop of an eight-story building. The taxi company office might be in the basement of a different building nearby. The office would have a remote control unit that would operate the taxi company base station over a current loop circuit. The circuit would normally be over a telephone line or similar wiring. Control function currents come from the remote control console at the dispatch office end of a circuit. In two-way radio use, an idle circuit would normally have no current present.\n\nIn two-way radio use, radio manufacturers use different currents for specific functions. Polarities are changed to get more possible functions over a single circuit. For example, imagine one possible scheme where the presence of these currents cause the base station to change state:\n\n\nThis circuit is polarity-sensitive. If a telephone company cable splicer accidentally reversed the conductors, selecting channel 2 would lock the transmitter on.\n\nEach current level could close a set of contacts, or operate solid-state logic, at the other end of the circuit. That contact closure caused a change of state on the controlled device. Some remote control equipment could have options set to allow compatibility between manufacturers. That is, a base station that was configured to transmit with a +18 mA current could have options changed to (instead) make it transmit when +6 mA was present.\n\nIn two-way radio use, AC signals were also present on the circuit pair. If the base station were idle, receive audio would be sent over the line from the base station to the dispatch office. In the presence of a transmit command current, the remote control console would send audio to be transmitted. The voice of the user in the dispatch office would be modulated and superimposed over the DC current that caused the transmitter to operate.\n\n\n\n"}
{"id": "3575651", "url": "https://en.wikipedia.org/wiki?curid=3575651", "title": "Data cleansing", "text": "Data cleansing\n\nData cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.\n\nAfter cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.\nThe actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code) or fuzzy (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve activities like, harmonization of data, and standardization of data. For example, harmonization of short codes (st, rd, etc.) to actual words (street, road, etcetera). Standardization of data is a means of changing a reference data set to a new standard, ex, use of standard codes.\n\nAdministratively, incorrect or inconsistent data can lead to false conclusions and misdirected investments on both public and private scales. For instance, the government may want to analyze population census figures to decide which regions require further spending and investment on infrastructure and services. In this case, it will be important to have access to reliable data to avoid erroneous fiscal decisions. In the business world, incorrect data can be costly. Many companies use customer information databases that record data like contact information, addresses, and preferences. For instance, if the addresses are inconsistent, the company will suffer the cost of resending mail or even losing customers. The profession of forensic accounting and fraud investigating uses data cleansing in preparing its data and is typically done before data is sent to a data warehouse for further investigation. There are packages available so you can cleanse/wash address data while you enter it into your system. This is normally done via an application programming interface (API).\n\nHigh-quality data needs to pass a set of quality criteria. Those include:\n\n\nThe term integrity encompasses accuracy, consistency and some aspects of validation (see also data integrity) but is rarely used by itself in data-cleansing contexts because it is insufficiently specific. (For example, \"referential integrity\" is a term used to refer to the enforcement of foreign-key constraints above.)\n\n\nGood quality source data has to do with “Data Quality Culture” and must be initiated at the top of the organization. It is not just a matter of implementing strong validation checks on input screens, because almost no matter how strong these checks are, they can often still be circumvented by the users. There is a nine-step guide for organizations that wish to improve data quality:\n\nOthers include:\n\nThe essential job of this system is to find a suitable balance between fixing dirty data and maintaining the data as close as possible to the original data from the source production system. This is a challenge for the Extract, transform, load architect. The system should offer an architecture that can cleanse data, record quality events and measure/control quality of data in the data warehouse. A good start is to perform a thorough data profiling analysis that will help define to the required complexity of the data cleansing system and also give an idea of the current data quality in the source system(s).\n\nThere are lots of data cleansing tools like Trifacta, OpenRefine, Paxata, Alteryx, and others. It's also common to use libraries like Pandas (software) for Python (programming language), or Dplyr for R (programming language). \n\nOne example of a data cleansing for distributed systems under Apache Spark is called Optimus, an OpenSource framework for laptop or cluster allowing pre-processing, cleansing, and exploratory data analysis. It includes several data wrangling tools.\n\nPart of the data cleansing system is a set of diagnostic filters known as quality screens. They each implement a test in the data flow that, if it fails records an error in the Error Event Schema. Quality screens are divided into three categories:\n\nWhen a quality screen records an error, it can either stop the dataflow process, send the faulty data somewhere else than the target system or tag the data.\nThe latter option is considered the best solution because the first option requires, that someone has to manually deal with the issue each time it occurs and the second implies that data are missing from the target system (integrity) and it is often unclear what should happen to these data.\n\nMost data cleansing tools have limitations in usability:\n\nThe Error Event schema holds records of all error events thrown by the quality screens. It consists of an Error Event Fact table with foreign keys to three dimension tables that represent date (when), batch job (where) and screen (who produced error). It also holds information about exactly when the error occurred and the severity of the error. In addition there is an Error Event Detail Fact table with a foreign key to the main table that contains detailed information about in which table, record and field the error occurred and the error condition.\n\n\n\n\n"}
{"id": "47362877", "url": "https://en.wikipedia.org/wiki?curid=47362877", "title": "Dream 2047", "text": "Dream 2047\n\nDream 2047 is monthly popular science magazine published by Vigyan Prasar, an autonomous institution under Department of Science and Technology, Government of India. The magazine has over fifty thousand subscribers. It is sent free to schools, colleges and individuals interested in science and technology communication.\n\nA few past issues of \"Dream 2047\" \nAugust 2015 \nJuly 2015 \nJune 2015 \nMay 2015 \nApril 2015 \nMarch 2015 \n"}
{"id": "1564394", "url": "https://en.wikipedia.org/wiki?curid=1564394", "title": "Electromagnetic shielding", "text": "Electromagnetic shielding\n\nElectromagnetic shielding is the practice of reducing the electromagnetic field in a space by blocking the field with barriers made of conductive or magnetic materials. Shielding is typically applied to enclosures to isolate electrical devices from their surroundings, and to cables to isolate wires from the environment through which the cable runs. Electromagnetic shielding that blocks radio frequency electromagnetic radiation is also known as RF shielding.\n\nThe shielding can reduce the coupling of radio waves, electromagnetic fields and electrostatic fields. A conductive enclosure used to block electrostatic fields is also known as a Faraday cage. The amount of reduction depends very much upon the material used, its thickness, the size of the shielded volume and the frequency of the fields of interest and the size, shape and orientation of apertures in a shield to an incident electromagnetic field.\n\nTypical materials used for electromagnetic shielding include sheet metal, metal screen, and metal foam. Any holes in the shield or mesh must be significantly smaller than the wavelength of the radiation that is being kept out, or the enclosure will not effectively approximate an unbroken conducting surface.\n\nAnother commonly used shielding method, especially with electronic goods housed in plastic enclosures, is to coat the inside of the enclosure with a metallic ink or similar material. The ink consists of a carrier material loaded with a suitable metal, typically copper or nickel, in the form of very small particulates. It is sprayed on to the enclosure and, once dry, produces a continuous conductive layer of metal, which can be electrically connected to the chassis ground of the equipment, thus providing effective shielding.\n\nElectromagnetic shielding is the process of lowering the electromagnetic field in an area by barricading it with conductive or magnetic material. Copper is used for radio frequency (RF) shielding because it absorbs radio and electromagnetic waves. Properly designed and constructed copper RF shielding enclosures satisfy most RF shielding needs, from computer and electrical switching rooms to hospital CAT-scan and MRI facilities.\n\nOne example is a shielded cable, which has electromagnetic shielding in the form of a wire mesh surrounding an inner core conductor. The shielding impedes the escape of any signal from the core conductor, and also prevents signals from being added to the core conductor.\nSome cables have two separate coaxial screens, one connected at both ends, the other at one end only, to maximize shielding of both electromagnetic and electrostatic fields.\n\nThe door of a microwave oven has a screen built into the window. From the perspective of microwaves (with wavelengths of 12 cm) this screen finishes a Faraday cage formed by the oven's metal housing. Visible light, with wavelengths ranging between 400 nm and 700 nm, passes easily through the screen holes.\n\nRF shielding is also used to prevent access to data stored on RFID chips embedded in various devices, such as biometric passports.\n\nNATO specifies electromagnetic shielding for computers and keyboards to prevent passive monitoring of keyboard emissions that would allow passwords to be captured; consumer keyboards do not offer this protection primarily because of the prohibitive cost.\n\nRF shielding is also used to protect medical and laboratory equipment to provide protection against interfering signals, including AM, FM, TV, emergency services, dispatch, pagers, ESMR, cellular, and PCS. It can also be used to protect the equipment at the AM, FM or TV broadcast facilities.\n\nElectromagnetic radiation consists of coupled electric and magnetic fields. The electric field produces forces on the charge carriers (i.e., electrons) within the conductor. As soon as an electric field is applied to the surface of an ideal conductor, it induces a current that causes displacement of charge inside the conductor that cancels the applied field inside, at which point the current stops.\n\nSimilarly, \"varying\" magnetic fields generate eddy currents that act to cancel the applied magnetic field. (The conductor does not respond to static magnetic fields unless the conductor is moving relative to the magnetic field.) The result is that electromagnetic radiation is reflected from the surface of the conductor: internal fields stay inside, and external fields stay outside.\n\nSeveral factors serve to limit the shielding capability of real RF shields. One is that, due to the electrical resistance of the conductor, the excited field does not completely cancel the incident field. Also, most conductors exhibit a ferromagnetic response to low-frequency magnetic fields, so that such fields are not fully attenuated by the conductor. Any holes in the shield force current to flow around them, so that fields passing through the holes do not excite opposing electromagnetic fields. These effects reduce the field-reflecting capability of the shield.\n\nIn the case of high-frequency electromagnetic radiation, the above-mentioned adjustments take a non-negligible amount of time, yet any such radiation energy, as far as it is not reflected, is absorbed by the skin (unless it is extremely thin), so in this case there is no electromagnetic field inside either. This is one aspect of a greater phenomenon called the skin effect. A measure of the depth to which radiation can penetrate the shield is the so-called skin depth.\n\nEquipment sometimes requires isolation from external magnetic fields. For static or slowly varying magnetic fields (below about 100 kHz) the Faraday shielding described above is ineffective. In these cases shields made of high magnetic permeability metal alloys can be used, such as sheets of permalloy and mu-metal or with nanocrystalline grain structure ferromagnetic metal coatings. These materials don't block the magnetic field, as with electric shielding, but rather draw the field into themselves, providing a path for the magnetic field lines around the shielded volume. The best shape for magnetic shields is thus a closed container surrounding the shielded volume. The effectiveness of this type of shielding depends on the material's permeability, which generally drops off at both very low magnetic field strengths and at high field strengths where the material becomes saturated. So to achieve low residual fields, magnetic shields often consist of several enclosures one inside the other, each of which successively reduces the field inside it.\n\nBecause of the above limitations of passive shielding, an alternative used with static or low-frequency fields is active shielding; using a field created by electromagnets to cancel the ambient field within a volume. Solenoids and Helmholtz coils are types of coils that can be used for this purpose.\n\nAdditionally, superconducting materials can expel magnetic fields via the Meissner effect.\n\nSuppose that we have a spherical shell of a (linear and isotropic) diamagnetic material with permeability formula_1, with inner radius formula_2 and outer radius formula_3. We then put this object in a constant magnetic field:\nSince there are no currents in this problem except for possible bound currents on the boundaries of the diamagnetic material, then we can define a magnetic scalar potential that satisfies Laplace's equation:\nwhere\nIn this particular problem there is azimuthal symmetry so we can write down that the solution to Laplace's equation in spherical coordinates is:\nAfter matching the boundary conditions\nat the boundaries (where formula_11 is a unit vector that is normal to the surface pointing from side 1 to side 2), then we find that the magnetic field inside the cavity in the spherical shell is:\nwhere formula_13 is an attenuation coefficient that depends on the thickness of the diamagnetic material and the magnetic permeability of the material:\nThis coefficient describes the effectiveness of this material in shielding the external magnetic field from the cavity that it surrounds. Notice that this coefficient appropriately goes to 1 (no shielding) in the limit that formula_15. In the limit that formula_16 this coefficient goes to 0 (perfect shielding), then the attenuation coefficient takes on the simpler form:\nwhich shows that the magnetic field decreases like formula_18.\n\nNOTE: In the above relations, formula_1 is relative permeability \"µ\", which is the ratio of the permeability of a specific medium to the permeability of free space \"µ\":\n\nwhere \"µ\" = 4\"π\" × 10 N A.\n\n\n"}
{"id": "22638874", "url": "https://en.wikipedia.org/wiki?curid=22638874", "title": "Fairmat", "text": "Fairmat\n\nFairmat is a free-of-charge multi-platform software that allows to model financial contracts (e.g. a derivative contract) or projects with many contingencies (e.g. a Real Options model) by decomposing it into basic parts. Complex structures and dependencies are modelled using a graphical interface. Virtually any pay-off function and asset class( from interest rate derivatives to equity-linked notes) can be described using a simple algebraic language.\n\nFairmat is available for Linux, Microsoft Windows, Mac OS X and Ubuntu.\n\n\nThe following plug-ins are released under the LGPL license:\n\n-The Hull and White one and two factors models.\n\n-The Pelsser squared gaussian model plug-in.\n\n-The Heston stochastic volatility model plug-in.\n\n-The Dupire local volatility model plug-in.\n\n-The Variance Gamma model plug-in.\n\nIntegration plug-ins for data from the European Central Bank , Yahoo! Finance , and MEFF \n\n- Quantum random generator support: the plug-in uses a web service provided by the university of Berlin. For more details see .\n\nAmong the other, the following plug-in are free:\n\n- The IAS 39 Hedge Accounting plug-in allows users to generate IAS 39 accounting reports for derivatives .\n\n- The Geometric Brownian Motion plug-in implements the calibration of the Geometric Brownian motion model using different techniques .\n\n- The Economic Scenarios Generator plug-in generates market consistent risk-neutral and real-world economic scenarios for \nseveral asset classes such as zero coupon bonds (ZCB), Inflation Rates, defaultable bonds / credit spreads and baskets of equities and indices \n\nFrom version 1.4 Fairmat supports an on-demand data pricing service offered by the same producers.\n"}
{"id": "1248428", "url": "https://en.wikipedia.org/wiki?curid=1248428", "title": "Game Boy Advance Video", "text": "Game Boy Advance Video\n\nGame Boy Advance Video is a format for putting full color, full-motion videos onto Game Boy Advance ROM cartridges. These videos are playable using the Game Boy Advance system's screen and sound hardware. These video cartridges were manufactured by Majesco Entertainment, except for the \"Pokémon\" Game Boy Advance Video cartridges, which were published by Nintendo. The cartridges themselves were developed by 4Kids Entertainment's subsidiary 4Kids Technology, Inc. The video cartridges are colored white for easy identification and are sold as \"Game Boy Advance Video Paks\". The Game Boy Advance Video game paks offer the same 240×160 resolution as standard Game Boy Advance games.\nGame Boy Advance Video Paks first became available in North America in May 2004. In June 2004, Majesco had expanded its Game Boy Advance Video licenses into other categories. In November 2004, Majesco started to sell GBA Video Paks featuring several Disney Channel animated series, including \"Brandy & Mr. Whiskers\", \"Kim Possible\", \"\", and \"The Proud Family\". In November 2005, Majesco began to sell GBA Video Paks featuring full-length animated movies like \"Shrek 2\" and \"Shark Tale\". A special GBA Video Pak containing the movies \"Shrek\" and \"Shark Tale\" combined into one cartridge was released later, costing approximately US$29.99 MSRP as of April 2007. As of April 2007, the retail price of original GBA Video Paks was lowered to US$9.95.\n\nGame Boy Advance Video Paks are viewable only on Game Boy Advance, Game Boy Advance SP, Game Boy Micro, Nintendo DS, and Nintendo DS Lite systems, as the owners of copyright in the television shows requested that Majesco prevent people from using the GameCube's Game Boy Player accessory to play and record the shows onto VHS tapes or DVDs. However, the low resolution and mono sound would result in a low-quality video output on a TV regardless. Unlike Sony's PlayStation 2 and Microsoft's Xbox video game consoles, the Nintendo GameCube cannot output Macrovision gain-control copy distortion signals. The GBA Video Paks perform a check when inserted into the Game Boy Player, using the same logo authentication method used by Game Boy Advance games that support controller rumble, and will freeze with the message \"Not designed for Game Boy Player\" if they detect the Game Boy Player in use.\n\nBecause of the low capacity of Game Boy Advance cartridges (ranging from 4 to 32 MB) and the length of the video content (generally feature-length movies and episodes), GBA Video Paks are heavily compressed, with visual artifacts marring nearly every frame. The image quality has a similar appearance to early Cinepak compression, and the \"quilting\" and color bleeding effect found in other compressed video formats is also present. Also, in cases where certain videos are available both as a 45-minute two-part episodes or a 22-minute edited version, the 22-minute version is used.\n\nGame Boy Advance Video Paks were the feature prize in Vol. 183 of \"Nintendo Power\" Magazine, as part of its players poll sweepstakes, in which five grand prize winners would receive a Game Boy Advance SP and twenty GBA Video Paks. Most GBA Video Paks cost US$9.95 and feature 40 to 45 minutes of video content. GBA Video Movie Paks cost US$19.99 and feature up to a 90-minute movie.\n\nSome GBA Video Movie Paks came packaged with headphones.\n\nThe following titles and episodes were released in the Game Boy Advance Video:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following Game Boy Advance Video titles were planned but never saw a public release:\n\n\n"}
{"id": "41717312", "url": "https://en.wikipedia.org/wiki?curid=41717312", "title": "Georg Sigl", "text": "Georg Sigl\n\nGeorg Sigl (13 January 1811, Breitenfurt bei Wien - 9 May 1887, Vienna) was an Austrian mechanical engineer and entrepreneur.\n\nSigl studied to be a locksmith, but later moved to Berlin where, in 1844, he established a small factory for the construction of printing presses. In 1846, he founded a second factory in Vienna.\n\nIn 1851, he moved the company to Währinger Straße (then on the outskirts of the city), where he manufactured steam locomotives. In the early 1870s, he produced the first compaction-free, two-stroke locomotive engines, designed by automotive pioneer Siegfried Marcus. In 1861, he leased the Wiener Neustädter Lokomotivfabrik from the Creditanstalt and, by 1867, was its owner. It became the largest factory of its kind in the Empire and, by 1870, had produced its 1000th locomotive.\n\nIn addition to the locomotive trade, his factories also produced oil presses, marine engines, architectural support structures and equipment for amusement park rides. In 1872, he introduced the \"Straßenlokomotive\" (road locomotive), that looked like a steam roller and was put to use as a tow truck.\n\nDuring the Panic of 1873, Sigl lost all of his holdings except his original Vienna factory. The Wiener Neustädter Lokomotivfabrik became a public company.\n\n\n\n"}
{"id": "4948354", "url": "https://en.wikipedia.org/wiki?curid=4948354", "title": "Hai Karate", "text": "Hai Karate\n\nHai Karate was a budget aftershave sold in the United States and the United Kingdom from the 1960s through to the 1980s. It was reintroduced in the United Kingdom under official licence in late 2014 by Healthpoint Ltd.\n\nThe fragrance was originally developed by the Leeming division of Pfizer and launched in 1967. As well as the original Hai Karate fragrance, versions named Oriental Lime and Oriental Spice were soon introduced. It competed successfully with such other brands as Aqua Velva, Old Spice, Jaguar, English Leather, British Sterling, Dante, and Brut before fading away in the 1980s.\n\nBrought back in 2014, Hai Karate has been re-produced according to the original formulation, although in a different bottle and re-worked packaging. Available for sale in the UK and online.\n\nHai Karate is best remembered today for its television adverts and its marketing plan, with a small self-defence instruction booklet sold with each bottle to help wearers fend off women. In the UK spots, a stereotypical nerd covers himself in Hai Karate and is promptly seduced by a female passer-by played by British starlet Valerie Leon; similar ads ran in the US as well. All of the spots contained the catch phrase \"Be careful how you use it\".\n\n\n\n"}
{"id": "1489315", "url": "https://en.wikipedia.org/wiki?curid=1489315", "title": "Homodyne detection", "text": "Homodyne detection\n\nHomodyne detection is a method of extracting information encoded as modulation of the phase and/or frequency of an oscillating signal, by comparing that signal with a standard oscillation that would be identical to the signal if it carried null information. \"Homodyne\" signifies a single frequency, in contrast to the dual frequencies employed in heterodyne detection.\n\nWhen applied to processing of the reflected signal in remote sensing for topography, homodyne detection lacks the ability of heterodyne detection to determine the size of a static discontinuity in elevation between two locations. (If there is a path between the two locations with smoothly changing elevation, then homodyne detection may in principle be able to track the signal phase along the path if sampling is dense enough.) Homodyne detection is more readily applicable to velocity sensing.\n\nIn optical interferometry, homodyne signifies that \"the reference radiation\" (i.e. the local oscillator) \"is derived from the same source as the signal before the modulating process\". For example, in a laser scattering measurement, the laser beam is split into two parts. One is the local oscillator and the other is sent to the system to be probed. The scattered light is then mixed with the local oscillator on the detector. This arrangement has the advantage of being insensitive to fluctuations in the frequency of the laser. Usually the scattered beam will be weak, in which case the (nearly) steady component of the detector output is a good measure of the instantaneous local oscillator intensity and therefore can be used to compensate for any fluctuations in the intensity of the laser.\n\nIn radio technology, the distinction is not the source of the local oscillator, but the frequency used. In heterodyne detection, the local oscillator is frequency-shifted, while in homodyne detection it has the same frequency as the radiation to be detected. See direct conversion receiver.\n\nLock-in amplifiers are homodyne detectors integrated into measurement equipment or packaged as stand-alone laboratory equipment for sensitive detection and highly selective filtering of weak or noisy signals. Homodyne/lock-in detection has been one of the most commonly used signal processing methods across a wide range of experimental disciplines for decades.\n\nHomodyne and heterodyne techniques are commonly used in thermoreflectance techniques.\n\nIn the processing of signals in some applications of magnetic resonance imaging, homodyne detection can offer advantages over magnitude detection. The homodyne technique can suppress excessive noise and undesired quadrature components (90° out-of-phase), and provide stable access to information that may be encoded into the phase or polarity of images.\n\nAn encrypted secure communication system can be based on quantum key distribution (QKD). An efficient receiver scheme for implementing QKD is balanced homodyne detection (BHD) using a positive-intrinsic-negative (PIN) diode.\n\nHomodyne detection was one of the key techniques in demonstrating Quantum Entanglement.\n\n"}
{"id": "5649261", "url": "https://en.wikipedia.org/wiki?curid=5649261", "title": "Japan–North Korea Pyongyang Declaration", "text": "Japan–North Korea Pyongyang Declaration\n\nThe Japan-North Korea Pyongyang Declaration, signed in 2002, was the result of the first Japan-North Korea summit meeting. It was an attempt to resolve the uneasy diplomatic relationship that existed between the two nations, provided for economic assistance to North Korea (including humanitarian aid), low-interest long-term loans, and discussed the future of nuclear missile development.\n\nNorth Korea agreed to extend its moratorium on missile tests, in place since 1999. However, this may have been breached. (See North Korea and weapons of mass destruction).\n\n\n"}
{"id": "46232640", "url": "https://en.wikipedia.org/wiki?curid=46232640", "title": "Jongla", "text": "Jongla\n\nJongla is a Finnish start-up company, specialising mobile messaging apps. In June 2016, Jongla announced that it wants to bridge the gap between social networking services and messaging apps. Jongla is targeting especially emerging markets like Africa, Southeast Asia and South America, where they are seeing the best traction. Jongla app is available on Android, iOS and Windows Phone platforms.\n\nIn June, 2016 Jongla introduced their 3rd generation app, the Jongla - Social Messenger, which introduced feature updates, brand upgrade and a new app UI. In Social Messenger, Jongla introduced the community of nearby Jongla users and an added ability to engage with user profiles with a choice of reaction like thumbs-up, smile or heart.\n\nJongla claims to be the world’s lightest instant messaging app. The company backs up their claim with app package size comparisons. In June 2016, their APK (Android Application Package) size was 3.5MB, being one tenth of that compared to their competitor apps like Whatsapp, Messenger and Viber.\n\nJongla has the basic messaging functions like private and group chats and sharing text, stickers, images, locations and videos. Also, anyone can join a Jongla conversation via web application called Jongla Out. Jongla is one of the few messaging apps offering voice messages with special filters which is an integrated push-to-talk voice messaging feature with access to a range of funny voice filters that alter sender’s voice.\n\nJongla has been selected as a winner of the Red Herring’s Top 100 Global award 2013. The company has been featured in articles by Forbes, CNBC Africa, Mobile Industry Review and The Guardian Nigeria.\n\nJongla is a Finland-based company founded by Arto Boman and headquartered in Helsinki, Finland. Jongla CSO is Riku Salminen and the company is owned by a group of private investors including JSH Capital Oy, Ingman Finance Oy, and Holdington Ltd Oy. Chairman of the board is Henry Sjöman accompanied with board members Arto Boman and Simo Makkonen.\n\n"}
{"id": "43748083", "url": "https://en.wikipedia.org/wiki?curid=43748083", "title": "List of AASHTO standards", "text": "List of AASHTO standards\n\nThis is a list of AASHTO standards, current and withdrawn.\n"}
{"id": "17224530", "url": "https://en.wikipedia.org/wiki?curid=17224530", "title": "Louis XIII style", "text": "Louis XIII style\n\nThe Louis XIII style or Louis Treize was a fashion in French art and architecture, especially affecting the visual and decorative arts. Its distinctness as a period in the history of French art has much to do with the regency under which Louis XIII began his reign (1610–1643). His mother and regent, Marie de' Medici, imported mannerism from her homeland of Italy and the influence of Italian art was to be strongly felt for several decades. \n\nLouis XIII-style painting was influenced from the north, through Flemish and Dutch Baroque, and from the south, through Italian mannerism and early Baroque. Schools developed around Caravaggio and Peter Paul Rubens. Among the French painters who blended Italian mannerism with a love of genre scenes were Georges de La Tour, Simon Vouet, and the Le Nain brothers. The influence of the painters on subsequent generations, however, was minimised by the rise of classicism under Nicolas Poussin and his followers. \n\nLouis XIII architecture was equally influenced by Italian styles. The greatest French architect of the era, Salomon de Brosse, designed the Palais du Luxembourg for Marie de' Medici. De Brosse began a tradition of classicism in architecture that was continued by Jacques Lemercier, who completed the Palais and whose own most famous work of the Louis XIII period is the chapel of the Sorbonne (1635). Under the next generation of architects, French Baroque would take an even greater classical shift. \n\nFurniture of the period was typically large and austere. \n\n"}
{"id": "16107313", "url": "https://en.wikipedia.org/wiki?curid=16107313", "title": "MIL-DTL-5015", "text": "MIL-DTL-5015\n\nMIL-DTL-5015, a military specification, covers electrical circular connectors with solder or removable crimp contacts (both front and rear release). These connectors are for use in electronic, electrical power, and control circuits and are used in large numbers for defense, civil, and industrial applications due to their versatility, reliability, and ease of supply.\n\nIn the early 1930s, Cannon (now ITT Interconnect Solutions) was contracted by Douglas Aircraft Company to develop electrical circular connectors for use on the DC-1 and on the subsequent DC-2 and DC-3 aircraft platforms. During the late 1930s—with World War II on the horizon—Cannon began volume production of multi-contact circular connectors that were used by virtually every aircraft builder in the United States. Cannon's Type \"AN\" (Army-Navy) series set the standard for modern circular connectors procured under MIL-C-5015—now MIL-DTL-5015—military specifications.\n\nThe military specification MIL-DTL-5015 was superseded by SAE-AS50151 in December 2009 pursuant to the general United States Department of Defense goal to reduce the number of military standards in favor of industry technical standards supported by Standards Developing Organizations (SDO) such as the ISO 9000 series for quality assurance and the SAE for technical specifications.\n\nMIL-DTL-5015 circular connectors are used primarily for their ease of engagement and disengagement, ability to house different types of contacts, a wide range of allowable contact voltages and currents, and their rugged mechanical performance under a variety of environmental conditions. Their primary disadvantage is loss of panel space when used in arrays when compared to rectangular connector housings.\n\nThere are 19 shell sizes—from size-8 to size-40—with about 160 approved insert arrangements incorporating 1-to-85 contacts. The circular connectors are available with a choice of wall mount, box mount, cable connecting and jam-nut receptacles, as well as standard or self-locking plugs.\n\nSimilar connectors with different contact arrangements and bayonet coupling rings are also available to military specifications. Supplemental military specifications (e.g., MS Drawings, Detail Specifications, and Performance Specifications) are available to assist users with the proper selection of connectors and related components.\n\nCommercial Off-The-Shelf (COTS) derivative circular connectors exist that intermate with the military versions designed to MIL-C-5015/MIL-DTL-5015. Both COTS and military-style connectors are sometimes called \"Type AN\" connectors, \"MS connectors\" \"Cannon connectors\", or \"Amphenol connectors\". Amphenol Corporation, Glenair Inc., and ITT Interconnect Solutions (Cannon) are notable manufacturers of this type of connectors. Amphenol Corporation (Industrial Global Operations) provides a \"5015 series\". 5015 series connectors are medium to heavy weight cylindrical connectors with 5 shell styles, 19 shell sizes and 5 service classes. The 5015 offers 305 contact arrangements from 1-to-104 circuits. ITT Corporation (ITT Interconnect Solutions) business portfolio includes the brands Cannon, VEAM, and BIW, and is the foremost manufacturer of Type \"AN\", MS and MS-type connectors.\n"}
{"id": "204912", "url": "https://en.wikipedia.org/wiki?curid=204912", "title": "Magnetic refrigeration", "text": "Magnetic refrigeration\n\nMagnetic refrigeration is a cooling technology based on the magnetocaloric effect. This technique can be used to attain extremely low temperatures, as well as the ranges used in common refrigerators.<ref name=\"doi10.1088/0022-327/38/23/R01\"></ref>\n\nThe effect was first observed by a German physicist Warburg (1881) Subsequently by French physicist P. Weiss and Swiss physicist A. Piccard in 1917. The fundamental principle was suggested by P. Debye (1926) and W. Giauque (1927). The first working magnetic refrigerators were constructed by several groups beginning in 1933. Magnetic refrigeration was the first method developed for cooling below about 0.3K (a temperature attainable by refrigeration, that is pumping on the vapors).\n\nThe magnetocaloric effect (MCE, from \"magnet\" and \"calorie\") is a magneto-thermodynamic phenomenon in which a temperature change of a suitable material is caused by exposing the material to a changing magnetic field. This is also known by low temperature physicists as \"adiabatic demagnetization\". In that part of the refrigeration process, a decrease in the strength of an externally applied magnetic field allows the magnetic domains of a magnetocaloric material to become disoriented from the magnetic field by the agitating action of the thermal energy (phonons) present in the material. If the material is isolated so that no energy is allowed to (re)migrate into the material during this time, (i.e., an adiabatic process) the temperature drops as the domains absorb the thermal energy to perform their reorientation. The randomization of the domains occurs in a similar fashion to the randomization at the curie temperature of a ferromagnetic material, except that magnetic dipoles overcome a decreasing external magnetic field while energy remains constant, instead of magnetic domains being disrupted from internal ferromagnetism as energy is added.\n\nOne of the most notable examples of the magnetocaloric effect is in the chemical element gadolinium and some of its alloys. Gadolinium's temperature increases when it enters certain magnetic fields. When it leaves the magnetic field, the temperature drops. The effect is considerably stronger for the gadolinium alloy (). Praseodymium alloyed with nickel () has such a strong magnetocaloric effect that it has allowed scientists to approach to within one milliKelvin, one thousandth of a degree of absolute zero.\n\nThe magnetocaloric effect can be quantified with the equation below:\n\nformula_1\n\nwhere T is the temperature, H is the applied magnetic field, C is the heat capacity of the working magnet (refrigerant) and M is the magnetization of the refrigerant.\n\nFrom the equation we can see that magnetocaloric effect can be enhanced by:\n\n\nThe cycle is performed as a refrigeration cycle that is analogous to the Carnot refrigeration cycle, but with increases and decreases in magnetic field strength instead of increases and decreases in pressure. It can be described at a starting point whereby the chosen working substance is introduced into a magnetic field, i.e., the magnetic flux density is increased. The working material is the refrigerant, and starts in thermal equilibrium with the refrigerated environment.\n\n\nOnce the refrigerant and refrigerated environment are in thermal equilibrium, the cycle can restart.\n\nThe basic operating principle of an adiabatic demagnetization refrigerator (ADR) is the use of a strong magnetic field to control the entropy of a sample of material, often called the \"refrigerant\". Magnetic field constrains the orientation of magnetic dipoles in the refrigerant. The stronger the magnetic field, the more aligned the dipoles are, corresponding to lower entropy and heat capacity because the material has (effectively) lost some of its internal degrees of freedom. If the refrigerant is kept at a constant temperature through thermal contact with a heat sink (usually liquid helium) while the magnetic field is switched on, the refrigerant must lose some energy because it is equilibrated with the heat sink. When the magnetic field is subsequently switched off, the heat capacity of the refrigerant rises again because the degrees of freedom associated with orientation of the dipoles are once again liberated, pulling their share of equipartitioned energy from the motion of the molecules, thereby lowering the overall temperature of a system with decreased energy. Since the system is now insulated when the magnetic field is switched off, the process is adiabatic, i.e., the system can no longer exchange energy with its surroundings (the heat sink), and its temperature decreases below its initial value, that of the heat sink.\n\nThe operation of a standard ADR proceeds roughly as follows. First, a strong magnetic field is applied to the refrigerant, forcing its various magnetic dipoles to align and putting these degrees of freedom of the refrigerant into a state of lowered entropy. The heat sink then absorbs the heat released by the refrigerant due to its loss of entropy. Thermal contact with the heat sink is then broken so that the system is insulated, and the magnetic field is switched off, increasing the heat capacity of the refrigerant, thus decreasing its temperature below the temperature of the heat sink. In practice, the magnetic field is decreased slowly in order to provide continuous cooling and keep the sample at an approximately constant low temperature. Once the field falls to zero or to some low limiting value determined by the properties of the refrigerant, the cooling power of the ADR vanishes, and heat leaks will cause the refrigerant to warm up.\n\nThe magnetocaloric effect (MCE) is an intrinsic property of a magnetic solid. This thermal response of a solid to the application or removal of magnetic fields is maximized when the solid is near its magnetic ordering temperature. Thus, the materials considered for magnetic refrigeration devices should be magnetic materials with a magnetic phase transition temperature near the temperature region of interest.<ref name=\"doi10.1002/aenm.201200167\"></ref> For refrigerators that could be used in the home, this temperature is room temperature. The temperature change can be further increased when the order-parameter of the phase transition changes strongly within the temperature range of interest.\n\nThe magnitudes of the magnetic entropy and the adiabatic temperature changes are strongly dependent upon the magnetic ordering process. The magnitude is generally small in antiferromagnets, ferrimagnets and spin glass systems but can be much larger for ferromagnets that undergo a magnetic phase transition. First order phase transitions are characterized by a discontinuity in the magnetization changes with temperature, resulting in a latent heat. Second order phase transitions do not have this latent heat associated with the phase transition.\n\nIn the late 1990s Pecharksy and Gschneidner reported a magnetic entropy change in that was about 50% larger than that reported for Gd metal, which had the largest known magnetic entropy change at the time. This giant magnetocaloric effect (GMCE) occurred at 270K, which is lower than that of Gd (294K). Since the MCE occurs below room temperature these materials would not be suitable for refrigerators operating at room temperature.<ref name=\"doi10.1038/NMAT3951\"></ref> Since then other alloys have also demonstrated the giant magnetocaloric effect. These include , and alloys. Gadolinium and its alloys undergo second-order phase transitions that have no magnetic or thermal hysteresis. However, the use of rare earth elements makes these materials very expensive.\n\nCurrent research has been used to describe alloys with a significant magnetocaloric effect in terms of a thermodynamic system. Literature says that Gd5(Si2Ge2) for example may be described as a thermodynamic system provided it satisfies the condition of being “a quantity of matter or region in space chosen for study”. Such systems have become relevant to modern research in thermodynamics because they serve as plausible materials for the creation of high performance thermoelectric materials.\n\nThe development of this technology is very material-dependent and will likely not replace vapor-compression refrigeration without significantly improved materials that are cheap, abundant, and exhibit much larger magnetocaloric effects over a larger range of temperatures. Such materials need to show significant temperature changes under a field of two tesla or less, so that permanent magnets can be used for the production of the magnetic field.\n\nThe original proposed refrigerant was a paramagnetic salt, such as cerium magnesium nitrate. The active magnetic dipoles in this case are those of the electron shells of the paramagnetic atoms.\n\nIn a paramagnetic salt ADR, the heat sink is usually provided by a pumped (about 1.2 K) or (about 0.3 K) cryostat. An easily attainable 1 T magnetic field is generally required for initial magnetization. The minimum temperature attainable is determined by the self-magnetization tendencies of the refrigerant salt, but temperatures from 1 to 100 mK are accessible. Dilution refrigerators had for many years supplanted paramagnetic salt ADRs, but interest in space-based and simple to use lab-ADRs has remained, due to the complexity and unreliability of the dilution refrigerator\n\nEventually paramagnetic salts become either diamagnetic or ferromagnetic, limiting the lowest temperature that can be reached using this method.\n\nOne variant of adiabatic demagnetization that continues to find substantial research application is nuclear demagnetization refrigeration (NDR). NDR follows the same principles, but in this case the cooling power arises from the magnetic dipoles of the nuclei of the refrigerant atoms, rather than their electron configurations. Since these dipoles are of much smaller magnitude, they are less prone to self-alignment and have lower intrinsic minimum fields. This allows NDR to cool the nuclear spin system to very low temperatures, often 1 µK or below. Unfortunately, the small magnitudes of nuclear magnetic dipoles also makes them less inclined to align to external fields. Magnetic fields of 3 teslas or greater are often needed for the initial magnetization step of NDR.\n\nIn NDR systems, the initial heat sink must sit at very low temperatures (10–100 mK). This precooling is often provided by the mixing chamber of a dilution refrigerator or a paramagnetic salt.\n\nResearch and a demonstration proof of concept device in 2001 succeeded in applying commercial-grade materials and permanent magnets at room temperatures to construct a magnetocaloric refrigerator\n\nOn August 20, 2007, the Risø National Laboratory (Denmark) at the Technical University of Denmark, claimed to have reached a milestone in their magnetic cooling research when they reported a temperature span of 8.7 K. They hoped to introduce the first commercial applications of the technology by 2010.\n\nAs of 2013 this technology had proven commercially viable only for ultra-low temperature cryogenic applications available for decades. Magnetocaloric refrigeration systems are composed of pumps, motors, secondary fluids, heat exchangers of different types, magnets and magnetic materials. These processes are greatly affected by irreversibilities and should be adequately considered.\nAt year-end, Cooltech Applications announced that its first commercial refrigeration equipment would enter the market in 2014. Cooltech Applications launched their first commercially available magnetic refrigeration system on 20 June 2016.\nAt the 2015 Consumer Electronics Show in Las Vegas, a consortium of Haier, Astronautics Corporation of America and BASF presented the first cooling appliance. BASF claim of their technology a 35% improvement over using compressors\n\nThermal and magnetic hysteresis problems remain to be solved for first-order phase transition materials that exhibit the GMCE.\n\nOne potential application is in spacecraft.\n\nVapor-compression refrigeration units typically achieve performance coefficients of 60% of that of a theoretical ideal Carnot cycle, much higher than current MR technology. Small domestic refrigerators are however much less efficient.\n\nIn 2014 giant anisotropic behaviour of the magnetocaloric effect was found in at 10 K. The anisotropy of the\nmagnetic entropy change gives rise to a large rotating MCE offering the possibility to build simplified, compact, and efficient magnetic cooling systems by rotating it in a constant magnetic field.\n\nIn 2015 Aprea \"et al.\" presented a new refrigeration concept, GeoThermag, which is a combination of magnetic refrigeration technology with that of low-temperature geothermal energy. To demonstrate the applicability of the GeoThermag technology, they developed a pilot system that consists of a 100-m deep geothermal probe; inside the probe, water flows and is used directly as a regenerating fluid for a magnetic refrigerator operating with gadolinium. The GeoThermag system showed the ability to produce cold water even at 281.8 K in the presence of a heat load of 60 W. In addition, the system has shown the existence of an optimal frequency f AMR, 0.26 Hz, for which it was possible to produce cold water at 287.9 K with a thermal load equal to 190 W with a COP of 2.20. Observing the temperature of the cold water that was obtained in the tests, the GeoThermag system showed a good ability to feed the cooling radiant floors and a reduced capacity for feeding the fan coil systems.\n\nThe effect was discovered first observed by a German physicist Warburg (1881) Subsequently by French physicist P. Weiss and Swiss physicist A. Piccard in 1917.\n\nMajor advances first appeared in the late 1920s when cooling via adiabatic demagnetization was independently proposed by chemistry Nobel Laureates Peter Debye in 1926 and William F. Giauque in 1927.\n\nIt was first demonstrated experimentally by Giauque and his colleague D. P. MacDougall in 1933 for cryogenic purposes when they reached 0.25 K. Between 1933 and 1997, advances in MCE cooling occurred.\n\nIn 1997, the first near room-temperature proof of concept magnetic refrigerator was demonstrated by Karl A. Gschneidner, Jr. by the Iowa State University at Ames Laboratory. This event attracted interest from scientists and companies worldwide who started developing new kinds of room temperature materials and magnetic refrigerator designs.\n\nA major breakthrough came 2002 when a group at the University of Amsterdam demonstrated the giant magnetocaloric effect in MnFe(P,As) alloys that are based on abundant materials.\n\nRefrigerators based on the magnetocaloric effect have been demonstrated in laboratories, using magnetic fields starting at 0.6 T up to 10 T. Magnetic fields above 2 T are difficult to produce with permanent magnets and are produced by a superconducting magnet (1 T is about 20,000 times the Earth's magnetic field).\n\nRecent research has focused on near room temperature. Constructed examples of room temperature magnetic refrigerators include:\n\nIn one example, Prof. Karl A. Gschneidner, Jr. unveiled a proof of concept magnetic refrigerator near room temperature on February 20, 1997. He also announced the discovery of the GMCE in on June 9, 1997. Since then, hundreds of peer-reviewed articles have been written describing materials exhibiting magnetocaloric effects.\n\n\n\n\n\n"}
{"id": "11748713", "url": "https://en.wikipedia.org/wiki?curid=11748713", "title": "Mechanical rectifier", "text": "Mechanical rectifier\n\nA mechanical rectifier is a device for converting alternating current (AC) to direct current (DC) by means of mechanically operated switches. The best-known type is the commutator, which is an integral part of a DC dynamo, but before solid-state devices became available, independent mechanical rectifiers were used for certain applications. Before the invention of semiconductors, rectification at high currents involved serious losses.\n\nThere were various vacuum/gas devices, such as the mercury arc rectifiers, thyratrons, ignitrons, and vacuum diodes. Solid-state technology was in its infancy, represented by copper oxide and selenium rectifiers. All of these gave excessive forward voltage drop at high currents. One answer was mechanically opening and closing contacts, if this could be done quickly and cleanly enough.\n\nThis was the reverse of a vibrator inverter. An electromagnet, powered by DC through contacts it operated (like a buzzer) (or fed with AC), caused a spring to vibrate and the spring operated change-over contacts which converted the AC to DC. This arrangement was only suitable for low-power applications, e.g. auto radios and was also found in some motorcycle electrical systems, where it was combined with a voltage regulator.\n\nThis operated on the same principle as the vibrator type but the change-over contacts were operated by a synchronous motor. It was suitable for high-power applications, e.g. electrolysis cells and electrostatic precipitators.\n\nA mechanical rectifier was patented in 1895 (US patent 547043) by William Joseph Still. The details are obscure but it appears from the diagram to be similar to a third-brush dynamo.\n\nThe machine shown in the reference was designed by Read and Gimson et al., at British Thomson-Houston (BTH) Rugby, Warwickshire, England, in the early 1950s. It is a three-phase mechanical rectifier working at 220 volts and 15,000 amperes, and its application was the powering of huge banks of electrolysis cells.\n\nThe central shaft was rotated by synchronous motor, driving an eccentric with a throw of about 2mm. (0.077 inch) Push-rods from this operated the contacts. The timing was critical, and was adjusted by rotating the position of the eccentric on its shaft, and by sliding wedges between the eccentric and push-rods.\n\nCrucial to this system were the \"commutating reactors\", inductors that ensured the contacts closed when the voltage across them was small, and opened when the current was small. Without these, contact wear would have been intolerably heavy. These were series inductors that operated for most of the cycle with saturated cores. When the current decreased below that for saturation, their inductances reduced the current considerably. Contact switching was timed to occur while their cores were un-saturated.\n\nIn the USA, similar rectifiers were made by the I-T-E circuit breaker company.\n\nThis machinery was undoubtedly successful; its efficiency was determined to be 97.25%. Contact life was never fully determined but considerably exceeded 2000 hours. However, the rapid development of the silicon diode made it ultimately redundant.\n"}
{"id": "22748103", "url": "https://en.wikipedia.org/wiki?curid=22748103", "title": "Microwave cavity", "text": "Microwave cavity\n\nA microwave cavity or \"radio frequency (RF) cavity\" is a special type of resonator, consisting of a closed (or largely closed) metal structure that confines electromagnetic fields in the microwave region of the spectrum. The structure is either hollow or filled with dielectric material. The microwaves bounce back and forth between the walls of the cavity. At the cavity's resonant frequencies they reinforce to form standing waves in the cavity. Therefore, the cavity functions similarly to an organ pipe or sound box in a musical instrument, oscillating preferentially at a series of frequencies, its resonant frequencies. Thus it can act as a bandpass filter, allowing microwaves of a particular frequency to pass while blocking microwaves at nearby frequencies. \n\nA microwave cavity acts similarly to a resonant circuit with extremely low loss at its frequency of operation, resulting in quality factors (Q factors) up to the order of 10, compared to 10 for circuits made with separate inductors and capacitors at the same frequency. They are used in place of resonant circuits at microwave frequencies, since at these frequencies discrete resonant circuits cannot be built because the values of inductance and capacitance needed are too low. They are used in oscillators and transmitters to create microwave signals, and as filters to separate a signal at a given frequency from other signals, in equipment such as radar equipment, microwave relay stations, satellite communications, and microwave ovens.\n\nRF cavities can also manipulate charged particles passing through them by application of acceleration voltage and are thus used in particle accelerators and microwave vacuum tubes such as klystrons and magnetrons.\n\nMost resonant cavities are made from closed (or short-circuited) sections of waveguide or high-permittivity dielectric material (see dielectric resonator). Electric and magnetic energy is stored in the cavity and the only losses are due to finite conductivity of cavity walls and dielectric losses of material filling the cavity. Every cavity has numerous resonant frequencies that correspond to electromagnetic field modes satisfying necessary boundary conditions on the walls of the cavity. Because of these boundary conditions that must be satisfied at resonance (tangential electric fields must be zero at cavity walls), it follows that cavity length must be an integer multiple of half-wavelength at resonance. Hence, a resonant cavity can be thought of as a waveguide equivalent of short circuited half-wavelength transmission line resonator. Q factor of a resonant cavity can be calculated using cavity perturbation theory and expressions for stored electric and magnetic energy.\n\nThe electromagnetic fields in the cavity are excited via external coupling. An external power source is usually coupled to the cavity by a small aperture, a small wire probe or a loop. External coupling structure has an effect on cavity performance and needs to be considered in the overall analysis.\n\nThe resonant frequencies of a cavity can be calculated from its dimensions.\n\nResonance frequencies of a rectangular microwave cavity for any formula_1 or formula_2 resonant mode can be found by imposing boundary conditions on electromagnetic field expressions. This frequency is given by\n\n\\cdot k_{mnl}\\\\\n\nwhere formula_3 is the wavenumber, with formula_4, formula_5, formula_6 being the mode numbers and formula_7, formula_8, formula_9 being the corresponding dimensions; c is the speed of light in vacuum; and formula_10 and formula_11 are relative permeability and permittivity of the cavity filling respectively.\nThe field solutions of a cylindrical cavity of length formula_12 and radius formula_13 follow from the solutions of a cylindrical waveguide with additional electric boundary conditions at the position of the enclosing plates. The resonance frequencies are different for TE and TM modes.\n\n\n\nHere, formula_16 denotes the formula_5-th zero of the formula_4-th Bessel function, and formula_19 denotes the formula_5-th zero of the \"derivative\" of the formula_4-th Bessel function.\n\nThe quality factor formula_22 of a cavity can be decomposed into three parts, representing different power loss mechanisms.\n\n\nTotal Q factor of the cavity can be found as\nwhere k is the wavenumber, formula_26 is the intrinsic impedance of the dielectric, formula_27 is the surface resistivity of the cavity walls, formula_10 and formula_11 are relative permeability and permittivity respectively and formula_30 is the loss tangent of the dielectric.\n\nMicrowave resonant cavities can be represented and thought of as simple LC circuits. For a microwave cavity, the stored electric energy is equal to the stored magnetic energy at resonance as is the case for a resonant LC circuit. In terms of inductance and capacitance, the resonant frequency for a given formula_31 mode can be written as\n\n}\\\\\n\nwhere V is the cavity volume, formula_3 is the mode wavenumber and formula_33 and formula_34 are permittivity and permeability respectively.\n\nTo better understand the utility of resonant cavities at microwave frequencies, it is useful to note that the losses of conventional inductors and capacitors start to increase with frequency in the VHF range. Similarly, for frequencies above one gigahertz, Q factor values for transmission-line resonators start to decrease with frequency. Because of their low losses and high Q factors, cavity resonators are preferred over conventional LC and transmission-line resonators at high frequencies.\n\nConventional inductors are usually wound from wire in the shape of a helix with no core. Skin effect causes the high frequency resistance of inductors to be many times their direct current resistance. In addition, capacitance between turns causes dielectric losses in the insulation which coats the wires. These effects make the high frequency resistance greater and decrease the Q factor.\n\nConventional capacitors use air, mica, ceramic or perhaps teflon for a dielectric. Even with a low loss dielectric, capacitors are also subject to skin effect losses in their leads and plates. Both effects increase their equivalent series resistance and reduce their Q.\n\nEven if the Q factor of VHF inductors and capacitors is high enough to be useful, their parasitic properties can significantly affect their performance in this frequency range. The shunt capacitance of an inductor may be more significant than its desirable series inductance. The series inductance of a capacitor may be more significant than its desirable shunt capacitance. As a result, in the VHF or microwave regions, a capacitor may appear to be an inductor and an inductor may appear to be a capacitor. These phenomena are better known as parasitic inductance and parasitic capacitance.\n\nDielectric loss of air is extremely low for high-frequency electric or magnetic fields. Air-filled microwave cavities confine electric and magnetic fields to the air spaces between their walls. Electric losses in such cavities are almost exclusively due to currents flowing in cavity walls. While losses from wall currents are small, cavities are frequently plated with silver to increase their electrical conductivity and reduce these losses even further. Copper cavities frequently oxidize, which increases their loss. Silver or gold plating prevents oxidation and reduces electrical losses in cavity walls. Even though gold is not quite as good a conductor as copper, it still prevents oxidation and the resulting deterioration of Q factor over time. However, because of its high cost, it is used only in the most demanding applications.\n\nSome satellite resonators are silver-plated and covered with a gold flash layer. The current then mostly flows in the high-conductivity silver layer, while the gold flash layer protects the silver layer from oxidizing.\n"}
{"id": "51181110", "url": "https://en.wikipedia.org/wiki?curid=51181110", "title": "Mixing paddle", "text": "Mixing paddle\n\nA mixing paddle is a shaped device, typically mounted on a shaft, which can be inserted on the shaft end into a motorised drive, for the purpose of mixing liquids, solids or both. Whilst mounted in fixed blending equipment, the paddle may also be referred to as an agitator.\n\n\n\n"}
{"id": "34761780", "url": "https://en.wikipedia.org/wiki?curid=34761780", "title": "Nuclear power in space", "text": "Nuclear power in space\n\nNuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mössbauer spectrometer. One common type is a radioisotope thermoelectric generator, which has been used on many space probes and on manned lunar missions, and another is small fission reactors for Earth observation satellites such as the TOPAZ nuclear reactor. A radioisotope heater unit provides heat from radioactive decay of a material and can potentially produce heat for decades.\n\nRussia has sent about 40 reactors into space and its TOPAZ-II reactor can produce 10 kilowatts. The Romashka reactor family uses uranium and direct thermoelectric conversion to electricity, rather than using a heated fluid to drive a turbine. The United States tested a nuclear reactor in space for 43 days in 1965. While not yet tested in space, the test of the Demonstration Using Flattop Fission (DUFF) on September 13, 2012 was the first test of a nuclear reactor power system for space since then.\n\nExamples of nuclear power for space propulsion systems include nuclear electric rocket (nuclear electric propulsion), radioisotope rocket, and radioisotope electric propulsion (REP). One of the more explored is the nuclear thermal rocket, which was tested in the NERVA program. See also (category link). Nuclear pulse propulsion was the subject of Project Orion (nuclear propulsion)\n\nWhile solar power is much more commonly used, nuclear power offers great advantages in many areas. Solar cells, although efficient, can only supply energy to spacecraft in orbits where the solar flux is sufficiently high, such as low Earth orbit and interplanetary destinations close enough to the Sun. Unlike solar cells, nuclear power systems function independently of sunlight, which is necessary for deep space exploration. Nuclear reactors are especially beneficial in space because of their lower weight-to-capacity ratio than solar cells. Therefore, nuclear power systems take up much less space than solar power systems. Compact spacecraft are easier to orient and direct in space when precision is needed. Estimates of nuclear power, which can power both life support and propulsion systems, suggest that use of these systems can effectively reduce both cost and flight time.\n\nSelected applications and/or technologies for space include\n\n\nFor more than fifty years, radioisotope thermoelectric generators (RTGs) have been the United States’ main nuclear power source in space. RTGs offer many benefits; they are relatively safe and maintenance-free, are resilient under harsh conditions, and can operate for decades. RTGs are particularly desirable for use in parts of space where solar power is not a viable power source. Dozens of RTGs have been implemented to power 25 different US spacecraft, some of which have been operating for more than 20 years. Over 40 radioisotope thermoelectric generators have been used globally (principally US and USSR) on space missions.<ref name=\"nasa/doe2010\">\n</ref>\n\nThe advanced Stirling radioisotope generator (ASRG, a model of Stirling radioisotope generator (SRG)) produces roughly four times the electric power of an RTG per unit of nuclear fuel, but it is not yet ready to be implemented on an actual mission. NASA plans to utilize two ASRGs to explore Titan in the distant future.\n\nRadioisotope power generators include:\n\nRadioisotope heater units (RHUs) are also used on spacecraft to warm scientific instruments to the proper temperature so they operate efficiently. A larger model of RHU called the General Purpose Heat Source (GPHS) is used to power RTGs and the ASRG.\n\nExtremely slow-decaying radioisotopes have been proposed for use on realistic interstellar probes with multi-decade lifetimes.\n\nAnother direction for development is an RTG assisted by subcritical reactions.\n\nFission power systems may be utilized to power a spacecraft’s heating or propulsion systems.\n\nIn terms of heating requirements, when spacecraft require more than 100 kW for power, fission systems are much more cost effective than RTGs. Over the past few decades, several fission reactors have been proposed, but these fission systems haven’t been utilized in US space projects as prominently as radioisotope systems have. The Soviet Union, however, launched 31 BES-5 low power fission reactors in their RORSAT satellites utilizing thermoelectric converters between 1967 and 1988. Shortly after, the Soviet Union developed TOPAZ reactors, which utilize thermionic converters instead. \nIn 2008, NASA announced plans to utilize a small fission power system to be used on the surface of the moon and Mars, and began testing \"key\" technologies for it to come to fruition.\n\nNuclear thermal propulsion systems (NTR) are based on the heating power of a fission reactor, offering a more efficient propulsion system for thrust in launches and landings than one powered by chemical reactions. Current research focuses more on nuclear electric systems as the power source for providing thrust to propel spacecraft that are already in space. \n\nOther space fission reactors for powering space vehicles include the SAFE-400 reactor and the HOMER-15. In 2020, Roscosmos (the Russian Federal Space Agency) plans to launch a spacecraft utilizing nuclear-powered propulsion systems (developed at the Keldysh Research Center), which includes a small gas-cooled fission reactor with 1 MWe.\n\n, more than 30 small fission power system nuclear reactors have been sent into space in the Soviet RORSAT satellites, with only one—SNAP-10A—by the US.\n\nProposed fission power system spacecraft and exploration systems have included SP-100, JIMO nuclear electric propulsion, and Fission Surface Power.\n\nA number of Micro nuclear reactor types have been developed or are in development for space applications:\n\nIn 2002, NASA announced an initiative towards developing nuclear systems, which later came to be known as Project Prometheus. A major part of the Prometheus Project was to develop the Stirling Radioisotope Generator and the Multi-Mission Thermoelectric Generator, both types of RTGs. The project also aimed to produce a safe and long-lasting space fission reactor system for a spacecraft’s power and propulsion, replacing the long-used RTGs. Budget constraints resulted in the effective halting of the project, but Project Prometheus has had success in testing new systems. After its creation, scientists successfully tested a High Power Electric Propulsion (HiPEP) ion engine, which offered substantial advantages in fuel efficiency, thruster lifetime, and thruster efficiency over other power sources.\n\nExamples of nuclear powered spacecraft:\nSee also Radioisotope thermoelectric generator#Nuclear power systems in space\n\n\n"}
{"id": "5820187", "url": "https://en.wikipedia.org/wiki?curid=5820187", "title": "PT Mi-Ba-II mine", "text": "PT Mi-Ba-II mine\n\nThe PT Mi-Ba-II is a large Bakerlite cased Czechoslovakian anti-tank blast mine. The mine is unusual in that it has two plunger fuses instead of a pressure plate. The plunger fuses give the mine resistance to overpressure, also the plastic body makes it difficult to detect. \n\nThe mine is no longer produced, and it is found in Africa.\n\n\n"}
{"id": "57997388", "url": "https://en.wikipedia.org/wiki?curid=57997388", "title": "Power loss factor", "text": "Power loss factor\n\nThe power loss factor β describes the loss of electrical power in CHP systems with a variable power-to-heat ratio when an increasing heat flow is extracted from the main thermodynamic electricity generating process in order to provide useful heat. Usually, the power loss factor refers to extraction steam turbines in thermal power stations, which conduct a part of the steam in a heating condenser for the production of useful heat, instead of the low pressure part of the steam turbine where is could perform mechanical work.\n\nformula_1\n\nThe picture on the right shows in the left part the principle of steam extraction. After the intermediate-pressure section of the turbine, i.e. before the low-pressure section, steam is diverted and flows into the heating condenser, where it transfers heat to the heating circuit (temperature level T about 100 °C) and liquefies. The remaining steam works in the low-pressure section of the turbine and is then liquefied in the condenser at approx. 30 °C. Then it is fed via the condensate pump to the feedwater circuit. The partial steam flow, which goes into the heating condenser at high temperature can no longer work in the low-pressure section and is responsible for the loss of power.\n\nThe right-hand side of the picture shows the associated T-s diagram (see Rankine cycle) for an operating state in which half of the waste heat is used for heating purposes. To the left of the red sqare, the white area below the red line corresponds to the waste heat (q), which is released via the condenser to the environment (ambient temperature level T). The entire red area corresponds to the useful heat (q), the upper hatched part of this area corresponds to the power loss in the low pressure stage.\n\nModern cogeneration plants have power loss ratios of about 1/5 to 1/9 when delivering heat in the range of 80 °C-120 °C. That means in exchange of one kWh of electrical energy ca. 5 up to 9 kWh of useful heat are obtained.\n\nBased on the equivalence of power loss and gain of heat, the power loss method assigns CO emissions and primary energy from the fuel to the useful heat and the electrical energy.\n"}
{"id": "8640428", "url": "https://en.wikipedia.org/wiki?curid=8640428", "title": "Product fit analysis", "text": "Product fit analysis\n\nA Product fit analysis (PFA) is a form of requirements analysis of the gap between an IT product's functionality and required functions.\n\nIt is a document which consists of all the business requirements which are mapped to the product or application.\n\nRequirements are specifically mentioned and the application is designed accordingly.\n\nPFA document is designed covering all the functionality required by the business and how it is addressed in the application.\nIt covers all the data inputs, data processing and data outputs.\n\n"}
{"id": "7612776", "url": "https://en.wikipedia.org/wiki?curid=7612776", "title": "SDEP", "text": "SDEP\n\nThe SDEP (Street events Data Exchange Protocol) comprises an XML data schema and web service WSDL for exchanging information about streetworks, roadworks, and street events between systems.\n\nElgin was funded by the UK NeSDS Government e-Standards Programme to conduct a consultation and convene meetings to define the requirements of a common data exchange protocol for streetworks registers and other systems handling street events data. SDEP was developed to allow the open exchange of such data between back office systems used by local authorities to manage their highway networks in order to enable e-Government and streetworks co-ordination.\n\nThe SDEP consultation group comprised ELGIN (Chair), Mayrise Ltd., Symology Ltd., Pitney Bowes Inc., Exor Corporation (Bentley Systems), Office of the Deputy Prime Minister and Transport for London, with the National Traffic Control Centre in an observing capacity.\n\n"}
{"id": "49141682", "url": "https://en.wikipedia.org/wiki?curid=49141682", "title": "Scale (analytical tool)", "text": "Scale (analytical tool)\n\nIn the study of complex systems and hierarchy theory, the concept of scale refers to the combination of (1) the level of analysis (for example, analyzing the whole or a specific component of the system); and (2) the level of observation (for example, observing a system as an external viewer or as an internal participant). The scale of analysis encompasses both the analytical choice of how to observe a given system or object of study, and the role of the observer in determining the identity of the system. This analytical tool is central to multi-scale analysis (see for example, MuSIASEM, land-use analysis).\n\nFor example, on at the scale of analysis of a given population of zebras, the number of predators (e.g. lions) determines the number of preys that survives after hunting, while at the scale of analysis of the ecosystem, the availability of preys determines how many predators can survive in a given area. The semantic categories of \"prey\" and \"predator\" are not given, but are defined by the observer.\n\n"}
{"id": "51495856", "url": "https://en.wikipedia.org/wiki?curid=51495856", "title": "Scottish Union of Bakers and Allied Workers", "text": "Scottish Union of Bakers and Allied Workers\n\nThe Scottish Union of Bakers and Allied Workers was a trade union representing bakers and confectioners in Scotland.\n\nThe union was founded in 1888, when it was known as the Operative Bakers' National Federal Union of Scotland. By the following year, it had more than 3,000 members, but a decision to undertake a national strike led many new members to resign. Membership fell below 2,000 before gradually increasing, rising above 5,000 by 1910, and to around 7,500 by 1923.\n\nDuring World War I, the union changed its name to the Operative Bakers and Confectioners of Scotland National Federal Union, then to the Scottish Union of Bakers and Confectioners in 1923, and to the Scottish Union of Bakers, Confectioners and Bakery Workers in 1927. From 1926, the union accepted women as members, and sought to become an industrial union, including unskilled workers in the industry.\n\nIn 1949, the union again changed its name, to the lengthy Scottish Union of Bakers, Confectioners, Biscuit Bakers and Bakery Workers, shortening this in 1955 to the \"Scottish Union of Bakers and Allied Workers\".\n\nMembership of the union peaked in 1971 at 11,713, but fell below 9,000 as the decade progressed. As a result, it decided to merge into the Union of Shop, Distributive and Allied Workers in 1978.\n\nThe union sponsored Norman Hogg, its national organiser, as a Labour Party candidate in a Parliamentary election.\n"}
{"id": "851297", "url": "https://en.wikipedia.org/wiki?curid=851297", "title": "Seed drill", "text": "Seed drill\n\nA seed drill is a device that sows the seeds for crops by metering out the individual seeds, positioning them in the soil, and covering them to a certain average depth. This makes sure the seed will be placed evenly.\n\nThe seed drill sows the seeds at equal distances and proper depth, ensuring that the seeds get covered with soil and are saved from being eaten by birds and being blown by the wind. This allows plants to get sufficient sunlight, nutrients, and water from the soil. Before the introduction of the seed drill, a common practice was to plant seeds by hand. Besides being wasteful, planting was usually imprecise and led to a poor distribution of seeds, leading to low productivity. The use of a seed drill can improve the ratio of crop yield (seeds harvested per seed planted) by as much as nine times. \n\nSome machines for metering out seeds for planting are called planters. The concepts evolved from ancient Chinese practice and later evolved into mechanisms that pick up seeds from a bin and deposit them down a tube. \n\nSeed drills of earlier centuries included single-tube seed drills in Sumer and multi-tube seed drills in China, and later a seed drill by Jethro Tull that was influential in the growth of farming technology in recent centuries. Even for a century after Tull, hand sowing of grain remained common.\n\nIn older methods of planting, a field is initially prepared with a plow to a series of linear cuts known as \"furrows\". The field is then seeded by throwing the seeds over the field, a method known as \"manual broadcasting\". The seeds may not be sown to the right depth nor the proper distance from one another. Seeds that land in the furrows have better protection from the elements, and natural erosion or manual raking will cover them while leaving some exposed. The result is a field planted roughly in rows, but having a large number of plants outside the furrow lanes.\n\nThere are several downsides to this approach. The most obvious is that seeds that land outside the furrows will not have the growth shown by the plants sown in the furrow since they are too shallow on the soil. Because of this, they are lost to the elements. Many of the seeds remain on the surface where they are vulnerable to being eaten by birds or carried away on the wind. Surface seeds commonly never germinate at all or germinate prematurely, only to be killed by frost.\n\nSince the furrows represent only a portion of the field's area, and broadcasting distributes seeds fairly evenly, this results in considerable wastage of seeds. Less obvious are the effects of overseeding; all crops grow best at a certain density, which varies depending on the soil and weather conditions. Additional seeding above this limit will actually reduce crop yields, in spite of more plants being sown, as there will be competition among the plants for the minerals, water, and the soil available. Another reason is that the mineral resources of the soil will also deplete at a much faster rate, thereby directly affecting the growth of the plants.\n\nThe invention of the seed drill dramatically improved germination. The seed drill employed a series of runners spaced at the same distance as the plowed furrows. These runners, or drills, opened the furrow to a uniform depth before the seed was dropped. Behind the drills were a series of presses, metal discs which cut down the sides of the trench into which the seeds had been planted, covering them over.\n\nThis innovation permitted farmers to have precise control over the depth at which seeds were planted. This greater measure of control meant that fewer seeds germinated early or late and that seeds were able to take optimum advantage of available soil moisture in a prepared seedbed. The result was that farmers were able to use less seed and at the same time experience larger yields than under the broadcast methods.\n\nWhile the Babylonians used primitive seed drills around 1400 BCE, the invention never reached Europe. Multi-tube iron seed drills were invented by the Chinese in the 2nd century BCE. This multi-tube seed drill has been credited with giving China an efficient food production system that allowed it to support its large population for millennia. This multi-tube seed drill may have been introduced into Europe following contacts with China. In the Indian subcontinent, the seed drill was in widespread use among peasants by the time of the Mughal Empire in the 16th century.\n\nThe first known European seed drill was attributed to Camillo Torello and patented by the Venetian Senate in 1566. A seed drill was described in detail by Tadeo Cavalina of Bologna in 1602. In England, the seed drill was further refined by Jethro Tull in 1701 in the Agricultural Revolution. However, seed drills of this and successive types were both expensive and unreliable, as well as fragile. Seed drills would not come into widespread use in Europe until the mid to late 19th century, when manufacturing advances such as machine tools, die forging and metal stamping allowed large scale precision manufacturing of metal parts. \n\nEarly drills were small enough to be pulled by a single horse, and many of these remained in use into the 1930s. The availability of steam, and later gasoline tractors, however, saw the development of larger and more efficient drills that allowed farmers to seed ever larger tracts in a single day.\n\nRecent improvements to drills allow seed-drilling without prior tilling. This means that soils subject to erosion or moisture loss are protected until the seed germinates and grows enough to keep the soil in place. This also helps prevent soil loss by avoiding erosion after tilling. The development of the press drill was one of the major innovations in pre-1900 farming technology.\n\n\"Drilling\" is the term used for the mechanized sowing of an agricultural crop. Traditionally, a seed drill used to consist of a hopper filled with seeds arranged above a series of tubes that can be set at selected distances from each other to allow optimum growth of the resulting plants. Seeds are spaced out using fluted paddles which rotate using a geared drive from one of the drill's land wheels—seed rate is altered by changing gear ratios. Most modern drills use air to convey seed in plastic tubes from the seed hopper to the coulters—it is an arrangement which allows seed drills to be much wider than the seed hopper—as much as 12 m wide in some cases. The seed is metered mechanically into an air stream created by a hydraulically powered onboard fan and conveyed initially to a distribution head which sub-divides the seed into the pipes taking the seed to the individual colters.\n\nThe seed drill allows farmers to sow seeds in well-spaced rows at specific depths at a specific seed rate; each tube creates a hole of a specific depth, drops in one or more seeds, and covers it over. This invention gives farmers much greater control over the depth that the seed is planted and the ability to cover the seeds without back-tracking. The result is an increased rate of germination, and a much-improved crop yield (up to eight times).\n\nThe use of a seed drill also facilitates weed control. Broadcast seeding results in a random array of growing crops, making it difficult to control weeds using any method other than hand weeding. A field planted using a seed drill is much more uniform, typically in rows, allowing weeding with the hoe during the growing season. Weeding by hand is laborious and inefficient. Poor weeding reduces crop yield, so this benefit is extremely significant.\n\nThe ground would have to be plowed and harrowed. The plow would dig up the earth and the harrow would smooth the soil and break up any clumps. The drill would be set for the size of the seed used. Then the grain would be put in the hopper on top and then follow along behind it while the seed drill spaced and planted the seed. This system is still used today but it has been modified and updated so a farmer can plant many rows of seed at the same time.\n\nA seed drill can be pulled across the field using bullocks or a tractor. Seeds sown using a seed drill are distributed evenly and placed at the correct depth in the soil.\n\n\n"}
{"id": "1875863", "url": "https://en.wikipedia.org/wiki?curid=1875863", "title": "Shutter lag", "text": "Shutter lag\n\nIn photography, shutter lag is the delay between triggering the shutter and when the photograph is actually recorded. This is a common problem in the photography of fast-moving objects or people in motion. The term narrowly refers only to shutter effects, but more broadly refers to all lag between when the shutter button is pressed and when the photo is taken, including metering and focus lag.\n\nIn film cameras, the delay is caused by the mechanism inside the camera that opens the shutter, exposing the film. Because the process is mechanical, however, and relatively brief, shutter lag in film cameras is often only noticeable (and of any concern) to professionals. SLRs have slightly longer shutter lag than rangefinders, because of the need to lift the mirror. Point and shoot film cameras often have significant shutter lag.\n\nShutter lag is much more of a problem with digital cameras. Here, the delay results from the charging of the CCD and relatively slow transmission of its capture data to the circuitry of the camera for processing and storage. Recent improvements in technology, however, such as the speed, bandwidth and power consumption of processor chips and memory, as well as CCD technology, have made shutter lag less of a problem. As of 2007, the greatest advancements have been limited mostly to professional, \"prosumer,\" and high-end consumer-grade digital cameras. Inexpensive (most \"point-and-shoot\") digital cameras, however, have even reduced the average shutter lag to half seconds, and higher-end \"point-and-shoot\" cameras have reduced this down to a quarter second or less.\n\nHowever, what many people consider shutter lag is in fact the time the camera takes to meter (set the exposure) and auto-focus, which is lag of a different cause but similar effect.\n\nThese causes of lag can be eliminated by pre-setting the exposure and focus, by either manually setting the exposure and focus, or by pre-exposing and pre-focusing. Pre-exposing and pre-focusing mean \"using automatic exposure and autofocus, then fixing the settings so they do not change\"; this can often be done by holding the shutter release halfway down, or by using a separate \"AE / AF lock\" button (useful if taking multiple photographs that are not in a burst), and means the subsequent photographs will be taken faster. These techniques can be combined – one can manually set the exposure and then use AF lock or conversely.\n\n"}
{"id": "2877265", "url": "https://en.wikipedia.org/wiki?curid=2877265", "title": "Stuart Parkin", "text": "Stuart Parkin\n\nStuart Stephen Papworth Parkin (born 9 December 1955) is an experimental physicist, IBM Fellow and manager of the magnetoelectronics group at the IBM Almaden Research Center in San Jose, California. He is also a consulting professor in the Department of Applied Physics at Stanford University and director of the IBM-Stanford Spintronic Science and Applications Center, which was formed in 2004.\n\nHe is a pioneer in the science and application of spintronic materials, and has made discoveries into the behaviour of thin-film magnetic structures that were critical in enabling recent increases in the data density and capacity of computer hard-disk drives. For these discoveries, he was awarded the 2014 Millennium Technology Prize.\nSince 1 April 2014, Parkin is a director at the Max Planck Institute of Microstructure Physics in Halle and a professor at the Institute of Physics at the Martin-Luther-University Halle-Wittenberg.\n\nA native of Watford, England, Parkin received his B.A. (1977) and was elected a Research Fellow (1979) at Trinity College, Cambridge, England, and was awarded his PhD (1980) at the Cavendish Laboratory, also in Cambridge. He joined IBM in 1982 as a World Trade Post-doctoral Fellow, becoming a permanent member of the staff the following year. In 1999 he was named an IBM Fellow, IBM's highest technical honour.\n\nIn 2007 Parkin was named a Distinguished Visiting Professor at the National University of Singapore, a Visiting Chair Professor at the National Taiwan University, and an Honorary Visiting Professor at University College London, The United Kingdom. In 2008, he was elected to the National Academy of Sciences. The Materials Research Network Dresden granted him the Dresden Barkhausen Award in 2009. Parkin has been awarded honorary doctorates by the University of Aachen, Germany and the Eindhoven University of Technology, The Netherlands.\n\nIn 1989 Stuart Parkin discovered the phenomenon of oscillatory interlayer coupling in magnetic multilayers, by which magnetic layers are magnetically coupled via an intervening non-magnetic metallic spacer layer. Parkin found that the sign of the exchange coupling oscillates from ferromagnetic to antiferromagnetic with an oscillation period of just a few atomic layers. Remarkably, Parkin discovered this phenomenon in thin film magnetic heterostructures that he prepared in a simple home-made sputtering system. Parkin, moreover, showed that this phenomenon is displayed by almost all metalllic transition elements. In what is often referred to as \"Parkin's Periodic Table\", Parkin showed that the strength of this oscillatory interlayer exchange interaction varied systematically across the Periodic Table of the elements. Parkin made numerous other fundamental discoveries which continued the development of the field of \"spintronics\" of which he is recognised as a prolific scientist. \n\nLater Parkin improved magnetic tunnelling junctions, a device invented in the 1970s by julliere, and revolutionized by Jagadeesh Moodera of MIT. This element can create a high performance magnetic random access memory in 1995. MRAM promises unique attributes of high speed, high density and non-volatility. The development by Parkin in 2001 of giant tunnelling magnetoresistance in magnetic tunnel junctions using highly textured MgO tunnel barriers has made MRAM even more promising. IBM developed the first MRAM prototype in 1999 and is currently developing a 16 Mbit chip.\n\nMost recently, Parkin has proposed and is working on a novel storage class memory device, The Magnetic Racetrack memory, which could replace both hard disk drives and many forms of conventional solid state memory. His research interests also include spin transistors and spin-logic devices that may enable a new generation of low-power electronics.\n\nParkin's research interests include organic superconductors, high-temperature superconductors, and, most recently, magnetic thin film structures and spintronic materials and devices for advanced sensor, memory, and logic applications. He is a Fellow of the Royal Society, the American Physical Society, the Materials Research Society, the Institute of Physics (London), the Institute of Electrical and Electronics Engineers, the American Association for the Advancement of Science, and the Gutenberg Research College (GRC).\n\nParkin has authored approximately 400 papers and has around 90 issued patents. He is also the chief editor of Spin, one of World Scientific's newest journals, which publishes articles in spin electronics.\n\nParkin is the recipient of numerous honours, including the Gutenberg Research Award (2008), a Humboldt Research Award (2004), the 1999–2000 American Institute of Physics Prize for Industrial Applications of Physics, the European Physical Society's Europhysics Prize (1997), the American Physical Society's International New Materials Prize (1994), the MRS Outstanding Young Investigator Award (1991) and the Charles Vernon Boys Prize from the Institute of Physics, London (1991). In 2001, he was named the first \"Innovator of the Year\" by \"R&D Magazine\" and in October 2007 was received the \"No Boundaries\" Award for Innovation from \"The Economist\".\n\nIn April 2014, Parkin was awarded the Millennium Technology Prize for his work on spintronic materials, \"leading to a prodigious growth in the capacity to store digital information\".\n\nIn March 2016, Parkin was elected a Corresponding Fellow of the Royal Society of Edinburgh, Scotland's national academy of science and letters.\n\n"}
{"id": "23610187", "url": "https://en.wikipedia.org/wiki?curid=23610187", "title": "Stubble-mulching", "text": "Stubble-mulching\n\nStubble-mulching refers to leaving the stubble (agriculture) or crop residue essentially in place on the land as a surface cover during a fallow period. Stubble-mulching can prevent soil erosion from wind or water and conserve soil moisture.\n"}
{"id": "39881017", "url": "https://en.wikipedia.org/wiki?curid=39881017", "title": "TEAM2 cluster", "text": "TEAM2 cluster\n\nTEAM2 cluster is a French business cluster for environmental technologies and circular economy. It is located in Northern France in Urban Community of Lille Métropole and at Loos-en-Gohelle. It is supported by Université Lille Nord de France, Veolia Environnement, Suez Environnement and 200 SMEs.\n\n\n\n"}
{"id": "13362980", "url": "https://en.wikipedia.org/wiki?curid=13362980", "title": "The Engineering Pathway", "text": "The Engineering Pathway\n\nEngineering Pathway is a portal to high-quality teaching and learning resources in applied science and math, engineering, computer science/information technology and engineering technology, for use by K-12 and university educators and students. It is the engineering education \"wing\" of the National Science Digital Library (NSDL).\n\nThe Engineering Pathway uses ABET accreditation criteria to tag educational resources with this criteria and link teaching resources to research on outcomes assessment. It has also established community groups for each ABET accredited disciplines in Engineering and Computing.\n\n"}
{"id": "56226862", "url": "https://en.wikipedia.org/wiki?curid=56226862", "title": "Timeline of the gunpowder age in Korea", "text": "Timeline of the gunpowder age in Korea\n\nThis is a timeline of the history of gunpowder and related topics such as weapons, warfare, and industrial applications in Korea.\n\n\n"}
{"id": "725139", "url": "https://en.wikipedia.org/wiki?curid=725139", "title": "Undulation of the geoid", "text": "Undulation of the geoid\n\nUndulation of the geoid is the height of the geoid relative to a given ellipsoid of reference. In maps and common use the height over the mean sea level (such as orthometric height) is used to indicate the height of elevations while the ellipsoidal height results from the GPS system. \n\nThe process of the undulation is not standardised, as different countries use different mean sea levels as reference but most commonly refers to the EGM96 geoid. Calculating the undulation factor is mathematically challenging. This is why many handheld GPS receivers have built-in undulation lookup tables to determine the height above sea level.\n\nThe deviation formula_1 between the ellipsoidal height formula_2 and the orthometric height formula_3 can be calculated by\n\nLikewise, the deviation formula_5 between the ellipsoidal height formula_2 and the normal height formula_7 can be calculated by\n\nGeoid undulations display uncertainties which can be estimated by using several methods, i.e. least-squares collocation (LSC), fuzzy logic, artificial neutral networks, radial basis functions (RBF), and geostatistical techniques. Geostatistical approach has been defined as the most improved technique in prediction of geoid undulation.\n\n"}
{"id": "11144386", "url": "https://en.wikipedia.org/wiki?curid=11144386", "title": "Uzel (computer)", "text": "Uzel (computer)\n\nUzel was the Soviet Union's first digital computer used on submarines, to assist in tracking multiple targets and calculate torpedo solutions. Uzel's design team was headed by two American defectors to the Soviet Union, Alfred Sarant (a.k.a. Philip Staros) and Joel Barr (a.k.a. Joseph Berg). An upgraded version of the Uzel computer is still in use on the Kilo class submarine today.\n"}
{"id": "11226479", "url": "https://en.wikipedia.org/wiki?curid=11226479", "title": "Wells turbine", "text": "Wells turbine\n\nThe Wells turbine is a low-pressure air turbine that rotates continuously in one direction independent of the direction of the air flow. Its blades feature a symmetrical airfoil with its plane of symmetry in the plane of rotation and perpendicular to the air stream.\n\nIt was developed for use in Oscillating Water Column wave power plants, in which a rising and falling water surface moving in an air compression chamber produces an oscillating air current. The use of this bidirectional turbine avoids the need to rectify the air stream by delicate and expensive check valve systems.\n\nIts efficiency is lower than that of a turbine with constant air stream direction and asymmetric airfoil. One reason for the lower efficiency is that symmetric airfoils have a higher drag coefficient than asymmetric ones, even under optimal conditions. Also, in the Wells turbine, the symmetric airfoil runs partly under high angle of attack (i.e., low blade speed / air speed ratio), which occurs during the air velocity maxima of the oscillating flow. A high angle of attack causes a condition known as \"stall\" in which the airfoil loses lift. The efficiency of the Wells turbine in oscillating flow reaches values between 0.4 and 0.7.\n\nThis simple but ingenious device was developed by Prof. Alan Arthur Wells of Queen's University Belfast in the late 1970s.\n\nAnother solution of the problem of stream direction independent turbine is the Darrieus wind turbine (Darrieus rotor). \n\n\n"}
{"id": "9155444", "url": "https://en.wikipedia.org/wiki?curid=9155444", "title": "William Heick", "text": "William Heick\n\nWilliam Heick (October 6, 1916 – September 13, 2012) was a San Francisco-based photographer and filmmaker. He is best known for his ethnographic photographs and documentary films of North American Indian cultures. W.R. Heick served as producer-director and chief cinematographer for the Anthropology Department of the University of California, Berkeley on their National Science Foundation supported American Indian Film Project. His photographs capture the life and culture of Native Americans from the Kwakiutl, Kashaya Pomo, Hupa, Navajo, Blackfoot and Sioux. He filmed a number of award winning films in this series along with the documentaries \"Pomo Shaman\" and \"Sucking Doctor\", a Pomo doctoring ceremony considered by anthropologists to be one of the most complete and outstanding films of an aboriginal ceremony made to date.\n\nHis fine art photography has been exhibited at the San Francisco Museum of Modern Art, the California Palace of the Legion of Honor, the DeYoung Museum, the Seattle Museum of Art, the Henry Gallery (University of Washington), the Phoebe A. Hearst Museum of Anthropology, and the University Art Gallery (Cal State at Chico) among others. His photographs have been selected for the permanent collections of the Museum of Modern Art, New York, the Smithsonian Institution, the High Museum of Art, Atlanta, the Santa Barbara Museum of Art, and the Monterey Peninsula Museum of Art.\n\nIn a published \"Art Scene\" review Monterey landscape artist and art critic Rick Deregon wrote: \"The special qualities of W.R. Heicks's images come from the simple relationship between the photographer and subject. With no agenda other than to capture the decisive inspirational moment and to illustrate the human parade Mr. Heick's work transcends straight journalism and aspires to an art of nobility and compassion.\"\n\nWilliam Heick's career in photography began as a naval intelligence photographer during World War Two in the Pacific. After the war he studied photography at California School of Fine Arts (now the San Francisco Art Institute) under such notable teachers as Ansel Adams and Minor White. He became lifelong friends with Imogen Cunningham and Dorothea Lange and regards these two photographers as the primary influences on his photographic work.\n\nWilliam Heick filmed two documentaries about Pacific Northwest Indian tribes, \"Blunden Harbour\" (1951) and \"Dances of the Kwakiutl\" (1951).\n\nW.R. Heick worked through most of the 1950s and 1960s as producer-director, assistant historian and cinematographer for the worldwide engineering firm of Bechtel Corporation. While with Bechtel he wrote and filmed documentaries of their major projects with special emphasis on ethnic and social consideration in remote areas of the Arctic, South America, Africa, Greenland, Europe, The Middle East, Australia, Indonesia and the islands of New Guinea and Bougainville. From 1956 to 1964 Heick was involved with C. Cameron Macauley in the American Indian Film Project, a project to document Native American cultures through film and sound recordings, working closely with Alfred Kroeber and Samuel Barrett.\n\nWilliam Heick produced two documentaries for the Quakers. \"Beauty for Ashes\" documents the Quaker's project to rebuild 40 churches that had been burned by nightriders during Mississippi's racial strife in the turbulent 1960s. \"Voyage of the Phoenix\" documents the controversial voyage of the yacht \"Phoenix,\" which sailed through the American battle fleet during the Vietnam War to deliver medical supplies to North Vietnam when the bombing of that beleaguered country was at its peak.\n\nIn the late 1960s and early 1970s W.R. Heick served as cinematographer on three feature films, all for the director/artist Fredric Hobbs: \"Troika\" (1969, co-directed by Gordon Mueller), \"Alabama's Ghost\" (1973), and \"The Godmonster of Indian Flat\" (1973).\n\nDuring the mid-1970s, working as an independent producer with Gordon Mueller, W.R. Heick produced the \"Indonesian Dance Series.\" This series, funded with grants from Caltex Pacific Indonesia and Pertamina, documents fourteen traditional dance performances from the islands of Java, Bali, Sumatra and Kalimantan.\n\nW.R. Heick's later films include \"The Other China\", a four-part mini-series filmed on location in Taiwan in 1988 documenting the social and cultural fabric of Taiwan.\n\n\n"}
{"id": "45203418", "url": "https://en.wikipedia.org/wiki?curid=45203418", "title": "Xputer", "text": "Xputer\n\nThe Xputer is a design for a reconfigurable computer, proposed by computer scientist Reiner Hartenstein. Hartenstein uses various terms to describe the various innovations in the design, including config-ware, flow-ware, morph-ware, and \"anti-machine\".\n\nThe Xputer represents a move away from the traditional Von Neumann computer architecture, to a coarse-grained \"soft Arithmetic logic unit (ALU)\" architecture. Parallelism is achieved by configurable elements known as \"reconfigurable datapath arrays\" (rDPA), organized in a two-dimensional array of ALU's similar to the KressArray.\n\nThe Xputer architecture is data-stream-based, and is the counterpart of the instruction-based von Neumann computer architecture.\n\nThe Xputer architecture was one of the first coarse-grained reconfigurable architectures, and consists of a reconfigurable datapath array (rDPA) organized as a two-dimensional array of ALUs (rDPU). The bus-width between ALU's were 32-bit in the first version of the Xputer.\n\nThe ALUs (also known as rDPUs) are used for computing a single mathematical operation, such as addition, subtraction or multiplication, and can also be used purely for routing.\n\nALUs are mesh-connected via three types of connections, and data-flow along these connections are managed by an address generation unit.\n\nPrograms for the Xputer are written in the C language, and compiled for usage on the Xputer using the CoDeX compiler written by the author. The CoDeX compiler maps suitable portions of the C program onto the Xputer's rDPA fabric. The remainder of the program is executed on the host system, such as a personal computer.\n\nA reconfigurable datapath array (rDPA) is a semiconductor device containing reconfigurable data path units and programmable interconnects, first proposed by Rainer Kress in 1993, at the University of Kaiserslautern.\n\nInstead of FPGAs (field-programmable gate arrays) having single bit configurable logic blocks (CLBs), rDPAs have multiple bits wide (for instance, 32 bit path width) reconfigurable datapath units (rDPUs).\n\nEach rDPU can be configured to perform an individual function. These rDPUs and interconnects can be programmed after the manufacturing process by the customer/designer (hence the term \"reconfigurable\") so that the rDPA can perform whatever complex computation is needed. Because rDPUs are multiple bits wide (for instance, 32 bits), we talk about coarse-grained reconfigurability - in contrast to FPGAs with single-bit wide configurable logic blocks, called fine-gained reconfigurable.\n\nrDPAs are structurally programmed from \"config-ware\" source code, compiled into pipe-networks to be mapped onto the rDPA. rDPAs are not instruction-stream-driven and have no instruction fetch at run time. rDPUs do not have a program counter.\n"}
