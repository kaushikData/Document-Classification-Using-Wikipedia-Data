{"id": "40413203", "url": "https://en.wikipedia.org/wiki?curid=40413203", "title": "222 Second Street", "text": "222 Second Street\n\n222 Second Street is a office skyscraper in the South of Market District of San Francisco, California. It is under lease by social networking company LinkedIn (headquartered in nearby Sunnyvale).\n\nDeveloped by Tishman Speyer and designed by Thomas Phifer, the high rise was planned to provide of office space, of ground floor retail, and of open space accessible to the public, at the southern corner of Second and Howard Streets. Construction began in August 2013, still without a tenant on hand.\n\nIn April 2014, LinkedIn announced it was leasing the building for an undisclosed sum, to accommodate up to 2,500 of its employees, with the lease covering 10 years. The goal was to join all of its San Francisco based staff (1,250 as of January 2016) in one building, bring sales and marketing employees together with the research and development team.\n\nThe building was topped-out in August 2014 and opened in March 2016, with LinkedIn staff moving in in stages until 2017.\n\nThe ground floor is open to the public during work hours, as a privately owned public space. It features three large artworks by Frank Stella, in accordance with the developers' public art proposal to the city Planning Commission, with the purchase price of $1 million matching 1% of the total \"construction hard costs\".\n\nThe San Francisco Chronicle's architecture critic John King characterized the building as \"severe yet sleek\" and expressed appreciation for the arrangement of the \"panes of overlapping glass 6 feet wide and 13 feet high [that] cover a form that begins as a squat 16-story rectangle and concludes as a 10-story square. On the lower four stories the shingled pattern fans to the right; the fifth floor panels are flat, side by side, and then the shingles resume in reverse, flipping tightly to the left. The upper floors reverse the pattern yet again.\" However, while acknowledging their appeal in certain light situations (\"a large-scale shuffle of vivid reflections\"), King criticized their dark color (evoking a \"dull gloom\" on cloudy days). And he chastised the building - \"designed and built by New Yorkers\" - as being aesthetically out of place on Second Street, \"an alien presence in a well-established setting where other recent buildings have done their best to add to the ambiance\".\n\n\n"}
{"id": "13393765", "url": "https://en.wikipedia.org/wiki?curid=13393765", "title": "ARINC 826", "text": "ARINC 826\n\nARINC 826 is a protocol for avionic data loading over the Controller Area Network (CAN) as internationally standardized in ISO 11898-1. It allows Loadable Software Aircraft Parts to be loaded in a verifiable and secure manner to avionics Line Replaceable Units (LRUs) and Line Replaceable Modules (LRMs) using CAN.\n\nBased on a subset of ARINC 615A features (the avionic data loading protocol for data loading over Ethernet), ARINC 826 provides basic features for avionics data loading.\n"}
{"id": "17828579", "url": "https://en.wikipedia.org/wiki?curid=17828579", "title": "Acoustic board", "text": "Acoustic board\n\nAn acoustic board is a special kind of board made of sound absorbing materials. Its job is to provide sound insulation. Between two outer walls sound absorbing material is inserted and the wall is porous. Thus, when sound passes through an acoustic board, the intensity of sound is decreased. The loss of sound energy is balanced by producing heat energy.\n\nThey are used in auditoriums, halls, seminar rooms, libraries, courts and wherever sound insulation is needed. Acoustic boards are also used in speaker boxes.\n\n"}
{"id": "5765010", "url": "https://en.wikipedia.org/wiki?curid=5765010", "title": "Advanced silicon etching", "text": "Advanced silicon etching\n\nAdvanced silicon etching (ASE) is a deep reactive-ion etching (DRIE) technique to rapidly etch deep and high aspect ratio structures in silicon.\nASE was pioneered by Surface Technology Systems Plc. (STS) in 1994 in the UK. STS has continued to develop this process with even greater etch rates while maintaining side wall roughness and selectivity.\nSTS developed the switched process originally invented by Dr. Larmer at Bosch, Stuttgart.\nASE consists in combining the fast etch rates achieved in an isotropic Si etch (usually making use of an SF plasma) with a deposition or passivation process (usually utilising a CF plasma condensation process) by alternating the two process steps. This approach achieves the fastest etch rates while maintaining the ability to etch anisotropically, typically vertically in Microelectromechanical Systems (microelectromechanical systems (MEMS)) applications.\nThe ASE HRM is an evolution of the previous generations of ICP design, now incorporating a decoupled plasma source (patent pending). This decoupled source generates very high density plasma which is allowed to diffuse into a separate process chamber. Through careful chamber design, the excess ions that are detrimental to process control are reduced, leaving a uniform distribution of fluorine free-radicals at a higher density than that available from the conventional ICP sources. The higher fluorine free-radical density facilitates increased etch rates, typically over three times the etch rates achieved with the original Bosch process. Also, as a result of the reduction in the effect of localised depletion of these species, improved uniformity for many applications can be achieved.\n\n"}
{"id": "7694953", "url": "https://en.wikipedia.org/wiki?curid=7694953", "title": "AeA", "text": "AeA\n\nThe AeA (formerly the American Electronics Association) was a nationwide non-profit trade association that represented all segments of the technology industry. It lobbied governments at the state, federal, and international levels; provided access to capital and business opportunities; and offered select business services and networking programs.\n\nIn 2008 the AeA merged with the Information Technology Association of America (ITAA) to form TechAmerica.\n\nAeA was founded in 1943 by David Packard and 25 of Hewlett-Packard's suppliers to help lobby for government contracts. It was originally named the West Coast Electronic Manufacturers Association (WCEMA). In 1969, WCEMA changed its name to the Western Electronic Manufacturers Association (WEMA) to reflect the growing membership outside California. In 1977, the association once again changed its name to the American Electronics Association, in an effort to more accurately represent its 750 members nationwide. A final name change occurred in 2001, as the American Electronics Association was shortened to AeA with the tagline \"Advancing the Business of Technology.\"\n\nAeA had 18 offices across the United States, and had two international offices in Brussels and Beijing. AeA had nearly 2,500 corporate members (and the 1.8 million employees they represent nationwide) at the time of the merger. The membership was drawn from a wide range of high tech sectors, including the aerospace/defense, business related services, computers, medical equipment, semiconductors/electronic components, software, and telecommunications industries. \n\nSince 1959, AeA has awarded an annual Medal of Achievement to a recipient selected for contributions and advances within the high-tech industry, their community, and humankind.\n\nAeA also produced an annual \"Cyberstates\" report which quantifies the high-tech industry on a state-by-state basis in the United States. s\n\nOn September 11, 2008, The Boards of Directors of AeA and the Information Technology Association of America (ITAA) announced that they are in discussions to merge the trade associations’ memberships and programs.\n\nOn December 9, 2008, the Boards of Directors of AeA and ITAA announced that they have each approved the merger of the two trade associations' memberships and programs. The combined associations became TechAmerica (The Technology Association of America) on January 1, 2009. The merger, gave rise to a stronger voice for the technology industry by bringing together the largest number of tech companies throughout the United States.\n\nPeter J. Boni is the Chairperson of the AeA Board of Directors and the President and CEO of Safeguard Scientifics, Inc.\n\nAs President and Chief Executive Officer of Safeguard, Peter J. Boni is responsible for developing and executing Safeguard’s corporate strategy.\n\nIn addition to acting as CEO for several publicly traded and privately held companies, he has served as a chairman, a Fortune 500 corporate executive, a NYSE Fortune 1000 president, a management consultant, board member, investor and advisor to institutional investors in both early- and later-stage hardware, software and technology-enabled services firms. After his CEO experience, Peter served as an Operating Partner at Advent International, a leading global private equity firm managing $10 billion.\n\nSelect companies represented on the Board include Agilent, Citrix, Intel, Microsoft, Motorola, Symantec, and Xerox. All board members can be found at http://www.aeanet.org/BoardofDirectors\n\nPresident & Chief Executive Officer<br>\nChristopher Hansen\n\nFinancial Operations<br>\nSamuel J. Block<br>\nVice President/Controller\n\nGovernment Affairs<br>\nRobert J. Mulligan<br>\nSenior Vice President International\n\nLegal<br>\nBenjamin Aderson<br>\nAssociation Counsel and Secretary\n\nOperations<br>\nMatthew Kazmierczak<br>\nSenior Vice President, Operations\n\nServices<br>\nElaine Sanders<br>\nSenior Vice President for Financial Conferences, Executive Education, and Affinity Programs\n\nEric Meyer<br>\nSenior Vice President for Insurance Programs\n\n\n\n\n\n\nAeA's main offices were located in Washington, DC and in Silicon Valley, CA. AeA had a total network of 18 offices across the country and two overseas. These offices were located in:\n\n\nInternational Offices\n\n"}
{"id": "8032763", "url": "https://en.wikipedia.org/wiki?curid=8032763", "title": "Amplidyne", "text": "Amplidyne\n\nAn amplidyne is an electromechanical amplifier invented prior to World War II by Ernst Alexanderson. It consists of an electric motor driving a DC generator. The signal to be amplified is applied to the generator's field winding, and its output voltage is an amplified copy of the field current. The amplidyne is used in industry in high power servo and control systems, to amplify low power control signals to control powerful electric motors, for example. It is now mostly obsolete.\n\nAn amplidyne is an electric motor which turns a generator on the same shaft. Unlike an ordinary motor-generator, the purpose of an amplidyne is not to generate a steady voltage but to generate a voltage proportional to an input current, to amplify the input. The motor provides the power, turning the generator at a constant speed, and the signal to be amplified is applied to the generator's field winding. The higher the current applied to the winding, the stronger the magnetic field and thus the higher the output voltage of the generator. So the output voltage of the generator is an amplified copy of the current waveform applied to the field winding. In a typical generator the load brushes are positioned perpendicular to the magnetic field flux. To convert a generator to an amplidyne, what would normally be the load brushes are connected together and the output is taken from another set of brushes that are parallel with the field. The perpendicular brushes are now called the 'quadrature' brushes. This simple change can increase the gain by a factor of 10,000 or more.\n\nThe amplidyne's frequency response is limited to low frequencies, it cannot even handle audio frequencies, so its use is limited to amplifying low frequency control signals in industrial processes.\n\nHistorically, amplidynes were one of the first amplifiers to generate very high power (tens of kilowatts), allowing precise feedback control of heavy machinery. Vacuum tubes of reasonable size were unable to deliver enough power to control large motors, but vacuum tube circuits driving the input of an amplidyne could be used to boost small signals up to the power needed to drive large motors. Early (World War II era) gun tracking and radar systems used this approach.\n\nAmplidynes are now obsolete technology, replaced by modern power semiconductor electronic devices such as MOSFETs and IGBTs which can produce output power in the kilowatt range.\n\nThe amplidyne was first used in the US Navy in servo systems to control the electric motors rotating naval gun mounts, to aim the gun at a target. The system \"(diagram right)\" is a feedback control system in which a feedback signal from a sensor representing the current position of the gun is compared with the control signal which represents the desired position, and the difference is amplified by the amplidyne generator to turn the gun mount motor. The components are:\n\nChapter 10 of the U.S. Navy manual \"Naval Ordnance and Gunnery, Volume 1\" (1957) explains the operation of the amplidyne:\n\nSpecifically, the phase of the control transformer's output (in phase with the synchro power source, or opposite phase)\nprovided the polarity of the error signal. A phase-sensitive demodulator, with the synchro AC power as its reference, created the DC error signal of the required polarity.\n\nAmplidynes were initially used for electric elevators and to point naval guns, and antiaircraft artillery radar such as SCR-584 in 1942.\nLater used to control processes in steelworks.\n\nUsed to remotely operate the control rods in early nuclear submarine designs (S3G Triton).\n\nDiesel-electric locomotive control systems. Early ALCO road-switcher locomotives used this technology.\n\n"}
{"id": "3500286", "url": "https://en.wikipedia.org/wiki?curid=3500286", "title": "Building envelope", "text": "Building envelope\n\nA building envelope is the physical separator between the conditioned and unconditioned environment of a building including the resistance to air, water, heat, light, and noise transfer.\n\nThe building envelope is all of the elements of the outer shell that maintain a dry, heated, or cooled indoor environment and facilitate its climate control. Building envelope design is a specialized area of architectural and engineering practice that draws from all areas of building science and indoor climate control.\n\nThe many functions of the building envelope can be separated into three categories:\n\nThe control function is at the core of good performance, and in practice focuses, in order of importance, on rain control, air control, heat control, and vapor control.\n\nControl of rain is most fundamental, and there are numerous strategies to this end, namely, perfect barriers, drained screens, and mass / storage systems.\n\nOne of the main purposes of a roof is to resist water. Two broad categories of roofs are flat and pitched. Flat roofs actually slope up to 10° or 15° but are built to resist standing water. Pitched roofs are designed to shed water but not resist standing water which can occur during wind-driven rain or ice damming. Typically residential, pitched roofs are covered with an underlayment material beneath the roof covering material as a second line of defense. Domestic roof construction may also be ventilated to help remove moisture from leakage and condensation.\n\nWalls do not get as severe water exposure as roofs but still leak water. Types of wall systems with regard to water penetration are \"barrier\", \"drainage\" and \"surface-sealed walls\". Barrier walls are designed to allow water to be absorbed but not penetrate the wall, and include concrete and some masonry walls. Drainage walls allow water that leaks into the wall to drain out such as cavity walls. Drainage walls may also be ventilated to aid drying such as rainscreen and pressure equalization wall systems. Sealed-surface walls do not allow any water penetration at the exterior surface of the siding material. Generally most materials will not remain sealed over the long term and this system is very limited, but ordinary residential construction often treats walls as sealed-surface systems relying on the siding and an underlayment layer sometimes called housewrap.\n\nMoisture can enter basements through the walls or floor. Basement waterproofing and drainage keep the walls dry and a moisture barrier is needed under the floor.\n\nControl of air flow is important to ensure indoor air quality, control energy consumption, avoid condensation (and thus help ensure durability), and to provide comfort. Control of air movement includes flow through the enclosure (the assembly of materials that perform this function is termed the air barrier system) or through components of the building envelope (interstitial) itself, as well as into and out of the interior space, (which can affect building insulation performance greatly). Hence, air control includes the control of windwashing (cold air passing through insulation) and convective loops which are air movements within a wall or ceiling that may result in 10% to 20% of the heat loss alone.\n\nThe physical components of the envelope include the foundation, roof, walls, doors, windows, ceiling, and their related barriers and insulation. The dimensions, performance and compatibility of materials, fabrication process and details, connections and interactions are the main factors that determine the effectiveness and durability of the building enclosure system.\n\nCommon measures of the effectiveness of a building envelope include physical protection from weather and climate (comfort), indoor air quality (hygiene and public health), durability and energy efficiency. In order to achieve these objectives, all building enclosure systems must include a solid structure, a drainage plane, an air barrier, a thermal barrier, and may include a vapor barrier. Moisture control (e.g. damp proofing) is essential in all climates, but cold climates and hot-humid climates are especially demanding.\n\nThe \"thermal envelope\", or heat flow control layer, is part of a building envelope but may be in a different location such as in a ceiling. The difference can be illustrated by understanding that an insulated attic floor is the primary thermal control layer between the inside of the house and the exterior while the entire roof (from the surface of the roofing material to the interior paint finish on the ceiling) comprises the building envelope.\n\nBuilding envelope thermography involves using an infrared camera to view temperature anomalies on the interior and exterior surfaces of the structure. Analysis of infrared images can be useful in identifying moisture issues from water intrusion, or interstitial condensation. Other types of anomalies that can be detected are thermal bridging, continuity of insulation and air leakage, however this requires a temperature differential between the inside and outside ambient temperatures.\n\n\n"}
{"id": "50099171", "url": "https://en.wikipedia.org/wiki?curid=50099171", "title": "Butterfly roof", "text": "Butterfly roof\n\nA butterfly roof (sometimes called a V roof or Aysha roof) is a form of roof characterised by an inversion of a standard roof form, with two roof surfaces sloping down from opposing edges to a valley near the middle of the roof. It is so called because its shape resembles a butterfly's wings. Butterfly roofs are commonly associated in the USA with 20th century mid-century modern architecture. They were also commonly used in Georgian and Victorian terraced house architecture of British cities, where they are alternatively termed \"London\" roofs. The form has no gutter as rainwater can run off the roof in no more than two locations, at either end of the valley, often into a scupper or downspout. The form may be symmetrical, with the valley located in the center, or asymmetrical with an off-center valley. The valley itself may be flat, with a central roof cricket diverting water towards the valley ends, or sloping if the entire roof form is tilted towards one end of the valley. The roof also allows for higher perimeter walls, with clerestory windows allowing light penetration without impacting privacy.\n\nThe modern butterfly roof is commonly credited to be the creation of William Krisel and Dan Palmer in the late 1950s in Palm Springs, California. It has been estimated that starting in 1957, they created nearly 2,000 houses in a series of developments that were popularly known as the Alexander Tract, which has been described by historian Alan Hess as \"the largest Modernist housing subdivision in the United States.\" Krisel confirms that while his work popularized the form, he was not its originator. The timeline of the emergence of the butterfly roof:\n\n\n"}
{"id": "2359530", "url": "https://en.wikipedia.org/wiki?curid=2359530", "title": "Buttock augmentation", "text": "Buttock augmentation\n\nGluteoplasty (Greek \"gloutόs\", rump + \"plassein\", to shape) denotes the plastic surgery and the liposuction procedures for the correction of the congenital, traumatic, and acquired defects and deformities of the buttocks and the anatomy of the gluteal region; and for the aesthetic enhancement (by augmentation or by reduction) of the contour of the buttocks.\n\nThe corrective procedures for buttcock augmentation and buttcock repair include the surgical emplacement of a gluteal implant (buttock prosthesis); liposculpture (fat transfer and liposuction); and body contouring (surgery and liposculpture) to resolve the patient’s particular defect or deformity of the gluteal region. Moreover, in the praxis of sexual reassignment surgery, the prosthetic and liposculpture augmentation of the buttocks can be performed on transsexual and transgender women to enhance the anatomic curvature of the gluteal region in order to establish the markedly feminine buttocks and hips that project more (to the rear and to the side) than do masculine hips.\n\nThe functional purpose of the buttocks musculature is to establish a stable gait (balanced walk) for the man or the woman who requires the surgical correction of either a defect or a deformity of the gluteal region; therefore, the restoration of anatomic functionality is the therapeutic consideration that determines which gluteoplasty procedure will effectively correct the damaged muscles of the buttocks. The applicable techniques for surgical and correction include the surgical emplacement of gluteal implants; autologous tissue-flaps; the excision (cutting and removal) of damaged tissues; lipoinjection augmentation; and liposuction reduction — to resolve the defect or deformity caused by a traumatic injury (blunt, penetrating, blast) to the buttocks muscles (gluteus maximus, gluteus medius, gluteus minimus), and any deformation of the anatomic contour of the buttocks. Likewise, the corrective techniques apply to resolving the sagging skin of the body, and the muscle and bone deformities presented by the formerly obese patient, after a massive weight loss (MWL) bariatric surgery procedure; and for resolving congenital defects and congenital deformities of the gluteal region.\n\n\nAnatomically, the mass of each buttock principally comprises two (2) muscles — the gluteus maximus muscle and the gluteus medius muscle — which are covered by a layer of adipose body fat. The upper aspects of the buttocks end at the iliac crest (the upper edges of the wings of the ilium, and the upper lateral margins of the greater pelvis), and the lower aspects of the buttocks end at the horizontal gluteal crease, where the buttocks anatomy joins the rear, upper portion of the thighs. The gluteus maximus muscle has two (2) points of insertion: (i) the one-third (1/3) superior portion of the (coarse line) linea aspera of the thigh bone (femur), and (ii) the superior portion of the iliotibial tract (a long, fibrous reinforcement of the deep fascia lata of the thigh). The left and the right gluteus maximus muscles (the butt cheeks) are vertically divided by the intergluteal cleft (the butt-crack) which contains the anus.\n\nThe gluteus maximus muscle is a large and very thick muscle (6–7 cm) located on the sacrum, which is the large, triangular bone located at the base of the vertebral column, and at the upper- and back-part of the pelvic cavity, where it is inserted (like a wedge) between the two hip bones. The upper part of the sacrum is connected to the final lumbar vertebra (L5), and to the bottom of the coccyx (tailbone). At its origin, the gluteus maximus muscle extends to include parts of the iliac bone, the sacrum, the coccyx, the sacrosciatic ligament, and the tuberosity of the ischium.\n\nLike every pelvic-area muscle, the gluteus maximus muscle originates from the pelvis; nonetheless, it is the sole pelvic muscle not inserted to the trochanter (head of the femur), and is approximately aligned to the femur and the fascia lata (the deep fascia of the thigh); the tissues of the gluteus maximus muscle cover only the rear, lateral face of the trochanter, and there form a bursa (purse) that faces the interior of the thigh.\n\nThe motor innervation of the gluteus maximus muscle is performed by the inferior gluteal nerve (a branch nerve of the sacral plexus) and extends from the pelvis to the gluteal region, then traverses the greater sciatic foramen (opening) from behind and to the middle to then join the sciatic nerve. The inferior gluteal nerve divides into three (3) collateral branches: (i) the gluteus branch, (ii) the perineal branch, and (iii) the femoral branch. The first ramification — the gluteus branch — is a branch nerve that is very close to the emergence of the inferior gluteal nerve to the area, next to the inferior border of the pyramidalis muscle. As it arises, the inferior gluteal nerve then divides into four (4) or more fillets (bands of nerve fibres) that travel (in a crow’s-foot configuration) between the gluteus maximus muscle and its (front) anterior fascia; the thickest nerve-bands are the superior-most and the inferior-most fillets. The superior-most fillet runs almost vertically, near the sacrum, and innervates the superior portion of the gluteus muscle; the inferior-most fillet, which has the greatest calibre, travels very close and parallel to the sacrotuberous ligament; the inferior-most fillet provides fine-gauge branch-nerve ramifications that innervate the gluteus muscle through its anterior (front) face.\n\nIn surgical and body contouring praxis, the plastic surgeon creates the implant-pocket — either for the gluteal prosthesis or for the injections of autologous fat — by undermining the gluteus maximus muscle with a dissection technique that avoids the sacrum, the sacrotuberous ligament, and the tuberosity of the ischium; which, if accidentally cut, might isolate the posterior (back) portion of the muscle and lead to denervation, the loss of nerve function and of innervation.\n\nThe superior gluteal artery, the inferior gluteal artery, the superior gluteal veins, and the inferior gluteal veins irrigate the gluteus maximus muscle with arterial and venous blood. The vascularization, the entrance of the blood vessels to the muscle tissues, occurs at the anterior (front) face of the muscle, very close to the sacrum. As the arteries and the veins enter the mass of the gluteal muscle, they divide into narrower blood-vessel ramifications (configured like the horizontal branches of a tree), most of which travel parallel to the muscle fibres.\n\nIn surgical and body contouring praxis, the plastic surgeon effects the implant-pocket undermining of the gluteus maximus muscle by carefully separating the muscle fibres to avoid severing the pertinent blood vessels, which would interfere with the blood irrigation of the muscle tissue. Therefore, to create an implant-pocket, either for a gluteal prosthesis or for lipoinjection, a low-angle muscle-dissection is performed in order to avoid the risk of severing any major branch — superior or inferior — of the gluteal artery, which travels very close to the sacrum and to the sacrotuberous ligament.\n\nThe resolution of the defects and deformities of the muscles of the gluteal region (the buttocks and the thighs) of the human body cannot be realized with medical therapy; thus, for example, a treatment with cellulite-diminishing cream is ineffective for correcting the corresponding physical faults respectively presented by the man and by the woman patient.\n\nWhile the resolution of the defects and deformities of the gluteal region can be realized surgically, the assessment of the degree of severity of the injury organizes treatment therapies into three types: (i) buttocks augmentation, (ii) buttocks reduction, and (iii) contour irregularity treatments that combine surgery and liposculpture (liposuction and fat-injection).\n\nThe augmentation of the buttocks is realized with a gluteal implant, which is emplaced under each gluteus maximus muscle; the insertion of the buttock prosthesis is through a midline incision (5–8-cm-wide) over the tailbone (coccyx). Augmentation with a gluteal implant is the method most effective for enlarging the buttocks of the man or of the woman whose body possesses few stores of excess adipose fat in the lower portion of the trunk, the buttocks and thighs, the anatomic regions where the human body usually stores excess body fat. Post-operatively, because of the cutting (incising) into the flesh of the tailbone muscles, the full healing of the augmented tissues can be approximately 6–8 months, in the course of which the gluteal-muscle tissues relax, and the settled buttocks prostheses are integrated to the gluteal region. The implantation procedure can be performed upon a patient who is either sedated or anaesthetized, either under general anaesthesia or under local anaesthesia. The usual operating-room time for a buttocks augmentation procedure is approximately 2 hours. The procedure can be managed either as an overnight in-patient treatment or as a hospital outpatient treatment. Given the nature of the surgical incisions to the gluteus maximus muscles, the therapeutic management of post-surgical pain (at the surgical-wound sites) and normal tissue-healing usually require a 4-6-week convalescence, after which the patient resumes his or her normal-life activities.\n\nThe augmentation and contouring of the buttocks with autologous-fat transfer (lipoinjection) therapy is realized with the excess adipose-fat tissue harvested from the abdomen, flanks, and thighs of the patient. In 1987 Dr. Eduardo Krulig, a Venezuelan Plastic Surgeon describes the technique, using the name \"Lipoinjection\" for the first time, mentioning the regions of the body where the technique is useful. The gentle liposuction applied to harvest the autologous fat minimally disturbs the local tissues, especially the connective-tissue layer between the skin and the immediate subcutaneous muscle tissues. Then, the harvested fat is injected to the pertinent body area of the gluteal region, through a fine-gauge cannula inserted through a small incision, which produces a short and narrow scar. Lipoinjection contouring and augmentation with the patient’s own body fat avoids the possibility of tissue rejection, and is physically less invasive than buttocks-implant surgery. Therefore, depending upon the health of the patient, the convalescence period allows him or her to resume daily, normal-life activities at 2-days post-operative, and the full spectrum of physical activity at 2-weeks post-operative. Furthermore, the liposuction harvesting of the patient’s excess body fat improves the aesthetic appearance of the body fat donor-sites. Nonetheless, physiologically, the human body’s normal, health-management chemistry does resorb (break down and eliminate) some of the injected adipose-fat tissue, and so might diminish the augmentation. According to the degree of diminishment of the volume and contour caused by the fat-resorption, the patient might require additional sessions of fat-transfer therapy to achieve the desired size, shape, and contour of the buttocks.\n\nThe augmentation of the buttocks, by rearranging and enhancing the pertinent muscle and fat tissues of the gluteal region, is realized with a combined gluteoplasty procedure of surgery (subcutaneous dermal-fat flaps) and liposculpture (fat-suction, fat-injection). Therapeutically, such a combined correction-and-enhancement procedure is a realistic and feasible lower-body-lift treatment for the man and for the woman patient who has undergone massive weight loss (MWL) in the course of resolving obesity with bariatric surgery. In the case of the man or woman who presents under-projected, flat buttocks (gluteal hypoplasia), and a degree of gluteal-muscle ptosis (prolapsation, falling forward), wherein neither gluteal-implant surgery nor lipoinjection would be adequate to restoring the natural anatomic contour of the gluteal region, the application of a combined treatment of autologous dermal-fat flap surgery and lipoinjection can achieve the required functional correction and aesthetic contour.\n\nThe methods for reducing the size of the buttocks include the varieties of liposuction, such as lipectomy (with and without ultrasonic enhancement) to reduce excess body fat, and superficial liposculpture, to reshape, refine, and re-establish the natural contour of the body. The usual buttocks-reduction treatment is lipectomy with applied tumescence and anaesthesia, wherein the body fat is harvested by aspiration (suction) through a small-gauge cannula (2–4 mm) that is inserted through a small incision, either to the intergluteal sulcus (the butt-crack), or to the upper area of the gluteus maximus muscle proper.\n\nUltrasonically-assisted liposuction can quickly remove a large volume of body fat for the correction of a notable occurrence of lipodystrophy, a deposit of adipose fat to the buttocks and related anatomic areas. The ultrasonic liposuction machine liquefies the excess fat tissue, and so more readily facilitates its removal with conventional suction-lipectomy. The quick fat-harvesting allowed by the ultrasonic lipectomy technique has eliminated the larger (long and wide) surgical incisions that once were required for removing a large volume of adipose tissue. Nonetheless, because of the sensitivity of the gluteal-region tissues, the skin of the pertinent donor-site is cooled in order to prevent ultrasonic heat damage caused by the liquefying and removal of the excess adipose fat.\n\nReshaping the buttocks with liposculpture is performed with a small cannula (2 mm) specifically for contouring superficial body fat, the configuration of which (number of open ports) is determined by the type and the degree of gluteal correction to be realized. To sculpt rounded contours to square-shaped buttocks muscles, superficial liposculpture allows the plastic surgeon to control the injection-rate of the fat-volume. Moreover, superficial liposuction can be combined with other treatment methods for contouring the gluteal region to achieve the required functional, anatomic correction, and the aesthetic enhancement sought by the patient, such as reshaping the lateral area of the buttocks into an athletic form. The study \"Contouring the Gluteal Region with Tumescent Liposculpture\" (2011) indicated that effective, gluteal-region contouring is best achieved by tailoring the liposuction-reduction and the lipoinjection-augmentation techniques to the anatomic topography of the body areas to be corrected. Furthermore, the study \"Contouring of the Gluteal Region in Women: Enhancement and Augmentation\" (2011) indicated that natural contours of the buttocks and the thighs are effectively achieved with a combined gluteoplasty of selective liposuction and lipoinjection, which reduces the need for aggressive surgical procedures, decreases the risk of medical complications, abbreviates wound-recovery-time, and lessens post-operative scarring. Combined with any buttocks-correction method, superficial liposculpture facilitates the treatment of contour irregularities, the surgical revision of scars, and the correction of gluteal-region contour depressions.\n\nTo meet the functional requirements and the aesthetic expectations (body image) of the patient, the plastic surgeon establishes a realistic and feasible surgery plan by which to correct the anatomic contour deficiencies of the gluteal region. The surgeon and the patient determine the location of the surgical-wound scars, and determine the best operative position, to allow the proper exposure of the pertinent anatomy to be corrected. Because the surgical procedure requires the tumescence and anaesthesia of the gluteal-region area to be corrected, the physician and the anaesthesiologist determine the volumes of the anaesthetic and tumescent fluids to be administered to the patient during the procedure, and so avoid the risks of drugs overdose and toxicity.\n\nOnce the patient is atop the operating table, the surgeon positions him or her to best expose the pertinent gluteal-region area that is to be corrected or contoured, or both; the usual operative position is the prone (face down) position, but the patient can also be positioned laterally (on his or her side). The surgical correction plan can be delineated and marked to the patient’s body when he or she is awake (before sedation or anaesthesia) or it can be delineated when the patient is on the operating table (already sedated or anaesthetized). In operative praxis, the second option allows the plastic surgeon greater freedom to properly manipulate the patient into the position best suited for performing the body-contouring surgery.\n\nOnce the patient is in the operative position, the surgeon begins the liposuction correction by making incisions to the marks of the surgical-correction plan, and then infiltrates (injects) a solution of anaesthesia- and tumescence-inducing drugs, usually a combination of lidocaine and epinephrine. The volume of the anaesthetic-tumescent solution is gradually infiltrated to the pertinent gluteal area, in order to avoid the nerves and the deeper anatomic structures of the gluteus maximus muscle. The particular anatomic features to be contoured determine the types of cannula (gauge, size, grade) used to effect and control the harvesting of excess adipose fat from the patient’s body.\n\nFor a lipoinjection augmentation, the surgeon first dissects and prepares the augmentation-pocket to which will be injected the autologous fat-tissue. The surgical creation (muscle dissection) of the augmentation-pocket avoids the gluteal innervation (superior gluteal nerve and inferior gluteal nerve) and the vascular system (venous and arterial) of the gluteus maximus muscle. Afterwards, the surgeon sutures the dissection-incision and secures it with adhesive tape to ensure that the augmentation-pocket remains open, as dissected, ready to receive the injections of adipose fat. For the revision of scars, with surgery and injections of autologous fat, or with allopathic synthetic fillers, the surgeon applies subcuticular closures to the incision wounds, which then are bandaged.\n\nAfter completing the surgical corrections and the lipoinjection contouring of the pertinent area(s) of the gluteal region, the surgeon thoroughly examines the patient to ensure his or her general recovery from the operation; and examines each surgical incision to ascertain that it is correctly sutured and taped, in order to facilitate the uneventful healing of the gluteus-muscle tissues, without medical complications. The patient is advised to avoid exercise and strenuous physical activity until 3-weeks post-operative; how to properly care for surgical-incision wounds; and how to wear a compression garment that will keep in place the surgically corrected tissues, and so ensure their healing as a whole anatomic unit of the gluteal region.\n\nThe physician advises the patient who has undergone a surgical contouring of the buttocks with gluteal implants, that, although immediate results can be observed, the final, corrected body contour usually is observed at 6-months post-operative, and at 1-year post-operative, depending upon the tissue-healing capabilities of the patient’s body. The liposculpture patient usually requires approximately 6 months, and occasionally 1 year before producing the final, corrected body contour. For both procedures, at approximately 1-month post-operative, marked aesthetic improvement is noticeable in the corrected body areas, as is the elimination of the initial, post-operative weight gain caused by the body’s retention of the infiltrated, anaesthetic and tumescent, fluids. The patient is advised to wear a compression garment to contain swelling and to immobilize the corrected tissues, so that they heal as one anatomic unit of the gluteal region. Moreover, throughout the convalescence, to facilitate shrinking the skin to the new, corrected body contour, and to resolve unevenness, wrinkles to the skin, and localized swelling, the continual application of massage and (occasional) ultrasound treatments can facilitate the diminishment of the post-operative conditions.\n\nThe surgical and liposculpture contouring of the human body presents possible medical complications such as: the psychological — unmet body image expectations of aesthetic improvement; the physical — uneven contour, local and general; the physiologic — toxic reactions to the anaesthesic and the tumescent drugs; and the nervous — paresthesia, localized areas of perduring numbness in the corrected portion(s) of the gluteal region. The medical complications possible to a surgical buttocks augmentation procedure, the submuscular emplacement of a gluteal implant, include infection, surgical-wound dehiscence that exposes the implant, revision surgery, rupture of the implant, seroma (a pocket of clear serous fluid), capsular contracture, asymmetry of the corrected area, shifting of the implant, surgical over-correction, injury to the sciatic nerve, and paresthesia (tingling skin). The medical complications possible to a liposclupture buttocks augmentation include the bodily resorption of some of the injected adipose fat, asymmetric contour of the corrected body area, an irregular contour to the body, seroma, abscess (pus enclosed by inflamed tissue), cellulitis (subcutaneous connective-tissue inflammation), and paresthesia.\n\nLike most medical procedures, buttock augmentation come with risks some of which can be life threatening. A total of 413 Mexican plastic surgeons reported 64 deaths related to liposuction, with 13 deaths caused by gluteal lipoinjection. In Colombia, nine deaths were documented. Of the 13 deaths in Mexico, eight (61.6 percent) occurred during lipoinjection, whereas the remaining five (38.4 percent) occurred within the first 24 hours. In Colombia, six deaths (77.7 percent) occurred during surgery and three occurred (22.2 percent) immediately after surgery. Secondary lymphoedema of the lower extremities has been reported as an unusual side effect of liquid silicone injection on the hips and buttock while thromboembolism, implant displacement and explosion has also been listed as some of the dangers. The surgical and liposculpture contouring of the human body presents possible medical complications such as: the psychological — unmet body image expectations of aesthetic improvement; the physical — uneven contour, local and general; the physiologic — toxic reactions to the anaesthesic and the tumescent drugs; and the nervous — paresthesia, localized areas of perduring numbness in the corrected portion(s) of the gluteal region. The medical complications possible to a surgical buttocks augmentation procedure, the submuscular emplacement of a gluteal implant, include infection, surgical-wound dehiscence that exposes the implant, revision surgery, rupture of the implant, seroma (a pocket of clear serous fluid), capsular contracture, asymmetry of the corrected area, shifting of the implant, surgical over-correction, injury to the sciatic nerve, and paresthesia (tingling skin). The medical complications possible to a liposclupture buttocks augmentation include the bodily resorption of some of the injected adipose fat, asymmetric contour of the corrected body area, an irregular contour to the body, seroma, abscess (pus enclosed by inflamed tissue), cellulitis (subcutaneous connective-tissue inflammation), and paresthesia.\n\nIn the surgical praxis of body contouring therapy, the patient’s body-image expectations can be different from the contoured body that is the outcome of the performed surgical operation. Such unmet aesthetic expectations can be avoided at the pre-operative consultation stage, whereby, with informed consent, the physician and the patient jointly establish a realistic and feasible surgery plan to achieve a mutually satisfactory corrective outcome (functional and aesthetic) of the operation to the gluteal region, the buttock- and thigh-areas.\n\nContour problems of the corrected gluteal region can be prevented with the operational use of small-gauge cannulas (ca. 2.0 mm) specifically for superficial liposuction; and with the application of cross-pattern harvesting of the excess body fat, to avoid removing too much adipose fat tissue, which might disfigure the contour of the patient’s fat-donor area. The possible contour problems that might arise from ultrasonic liposuction are skin burns and hypertrophic scarring, which might occur if the fat-donor area skin is not cooled and protected during the fat harvest. To that end, the infusion of a tumescence-inducing solution to the fat-donor area(s) assists in cooling the patient’s skin during the ultrasonic lipo-harvesting; likewise, the application of moist towels, a skin protector, and the constant cooling-fluid infiltration of the cannula in an integrated sheath.\n\nThe infiltration of a solution of anaesthesia- and tumescence-inducing drugs can present medical complications such as a fluid overload of the tissues, the inadequate replacement of the infiltrated solution, and the partitioning (separation) of a single infiltration into several pools, which then are removed by suction lipectomy. Moreover, during anaesthesia, maintaining the patient’s stable blood pressure can be difficult, which increases the possibility of bleeding, and the possibility that anaesthetic toxicity can occur if excessive doses are administered by infiltration; the symptoms are manifested as central nervous system (CNS) occurrences of drug-induced anxiety, apprehension, restlessness, nervousness, disorientation, confusion, dizziness, blurred vision, tremors, nausea, vomiting, shivering, and seizures; likewise, as manifestations of drowsiness, unconsciousness, respiratory depression, and respiratory arrest. Furthermore, the toxicity symptoms of a tumescence-inducing drug (e.g. epinephrine) might cause such CNS symptoms, for which reason the operative application of a tumescent drug is limited throughout the operation.\n\nPost-operatively, local areas of numbness (paresthesia) might occur in the contoured portion(s) of the gluteal region, and might perdure for a long time after the surgery. Hence, the patient is advised to facilitate the re-sensitizing of the numb area(s) with applications of gentle massage, to prevent the development of a neuroma complication, and to alleviate pain. Nonetheless, depending upon the tissue-healing capabilities of the patient, he or she can recover in full at 2-years post-operative.\n\nThe outcome of a buttocks-contouring procedure depends upon the specific defect or deformity that can be effectively corrected with liposculpture, ultrasonic or not. Nonetheless, depressed scars and deep morphological defects are difficult to correct because of the curvature of the buttocks as an anatomic unit, and because of the scar-contracting elements of the tissues across the gluteal curvature. In such a case, although the injection of (autologous or artificial) tissue fillers to correct the defect or the deformity might be impermanent — it usually will remedy the functional and aesthetic shortcoming(s) required by the patient, which is the therapeutic purpose of gluteoplasty.\n"}
{"id": "8523144", "url": "https://en.wikipedia.org/wiki?curid=8523144", "title": "Conditional text", "text": "Conditional text\n\nConditional text is content within a document that is meant to appear in some renditions of the document, but not other renditions. \n\nFor example, a writer can produce Macintosh and Windows versions of the same software manual by marking Macintosh-specific content as \"Macintosh only\" and Windows-specific content as \"Windows only.\" Everything that is not marked for one platform or the other, appears in the manuals for both platforms.\n\n"}
{"id": "5627634", "url": "https://en.wikipedia.org/wiki?curid=5627634", "title": "Continua Health Alliance", "text": "Continua Health Alliance\n\nContinua Health Alliance is an international non-profit, open industry group of nearly 240 healthcare providers, communications, medical, and fitness device companies. \nContinua Health Alliance members aim to develop a system to deliver personal and individual healthcare. Continua was a founding member of Personal Connected Health Alliance which was launched in February 2014 with other founding members mHealth SUMMIT and HIMSS.\n\nContinua Health Alliance is an international not-for-profit industry organization enabling end-to-end, plug-and-play connectivity of devices and services for personal health management and healthcare delivery. Its mission is to empower information-driven health management and facilitate the incorporation of health and wellness into the day-to-day lives of consumers. Continua is a pioneer in establishing industry standards and security for connected health technologies such as smart phones, gateways and remote monitoring devices. Its activities include a certification and brand support program, events and collaborations to support technology and clinical innovation, as well as outreach to employers, payers, governments and care providers. With nearly 220 member companies reaching across the globe, Continua comprises technology, medical device and healthcare industry leaders and service providers dedicated to making personal connected health a reality.\n\nContinua Health Alliance is working toward establishing systems of interoperable telehealth devices and services in three major categories: chronic disease management, aging independently, and health and physical fitness.\n\nContinua Health Alliance version 1 design guidelines are based on proven connectivity technical standards and include Bluetooth for wireless and USB for wired device connection. The group released the guidelines to the public in June 2009.\n\nThe group is establishing a product certification program using its recognizable logo, the Continua Certified Logo program, signifying that the product is interoperable with other Continua-certified products. Products made under Continua Health Alliance guidelines will provide consumers with increased assurance of interoperability between devices, enabling them to more easily share information with caregivers and service providers.\n\nThrough collaborations with government agencies and other regulatory bodies, Continua works to provide guidelines for the effective management of diverse products and services from a global network of vendors. Continua Health Alliance products make use of the ISO/IEEE 11073 Personal Health Data (PHD) Standards.\n\nContinua design guidelines are not available to the public without signing a Non-disclosure agreement. Continua's guidelines help technology developers build end-to-end, plug-and-play systems more efficiently and cost effectively.\n\nContinua Health Alliance was founded on June 6, 2006\n\nContinua Health alliance performed its first public demonstration of interoperability on October 27, 2008 at the Partners Center for Connected Health 5th Annual Connected Health Symposium in Boston.\n\nContinua Health Alliance certified its first product, the Nonin 2500 PalmSAT handheld pulse oximeter with USB, on January 26, 2009.\n\nBy the end of December 2014 there are more than 100 certified products.\n\nContinua selected Bluetooth Low Energy and ZigBee wireless protocols as the wireless standards for its Version 2 Design Guidelines which have been released. Bluetooth Low Energy is to be used for low-power mobile devices. ZigBee will be used for networked low-power sensors such as those enabling independent living.\n\nBeginning in 2012, Continua invites non-members to request a copy of its Design Guidelines after signing a non-disclosure agreement.\n\nContinua has working groups and operations in the U.S., EU, Japan, India and China.\n\nContinua Health Alliance currently has nearly 220 member companies.\n\nContinua's Board of Directors is currently composed of the following companies:\n\n\nThe Organisation is primarily staffed by volunteers from the member organisations that are organised into working groups that address the goals of the alliance. Below the board of directors sit the following main working groups:\n\n\n\nThe Continua Alliance website contains a full listing of member organisations, a directory of qualified products, and a clear statement of their mission.\n\n\n"}
{"id": "25853528", "url": "https://en.wikipedia.org/wiki?curid=25853528", "title": "Course Hero", "text": "Course Hero\n\nCourse Hero is an education technology website company based in Redwood City, California, which operates an online learning platform for students to access course-specific study resources contributed by a community of students and educators. The crowdsourced learning platform contains practice problems, study guides, infographics, class notes, step-by-step explanations, essays, lab reports, videos, user-submitted questions paired with answers from tutors, and original materials created and uploaded by educators. Users either buy a subscription or upload original documents to receive unlocks that are used to view and download full Course Hero documents.\n\nCourse Hero was founded by Andrew Grauer at Cornell University in 2006 for college students to share lectures, class notes, exams and assignments that usually went ignored. He believed that information is valuable and can be even more useful if properly indexed and accessible. The full website was launched in 2008 and the company is based in Redwood City, California.\n\nIn November 2014, the company raised $15 million in Series A Funding, with investors that included GSV Capital and IDG Capital. Seed investors SV Angel and Maveron also participated.\n\nIn 2012 it was asserted that Course Hero provided access to over 7 million uploaded study documents. Students pay a monthly subscription to have immediate Premier Access to Course Hero or they can upload 40 documents to receive free access for a month. When a user has uploaded 40 documents, they can download up to 300 documents from Course Hero. However, it takes about three days to get Premier Access after submitting documents. User can search for documents by content, university or course subject. A philanthropic initiative called the Course Hero Knowledge Drive was introduced in September 2010 in which one book is donated to Books for Africa for every 10 study documents uploaded to the website. Since its inception, the Course Hero Knowledge Drive has donated over 200,000 books to students and schools abroad.\n\nCourse Hero offers 24/7 access to online tutors. They can ask any question about a subject and a tutor will respond within 3 days. This access is charged per use via \"credits\" for Premier Users, but basic subscribers have to pay per question.\n\nOn April 17, 2012, Course Hero launched 22 free online courses in three \"learning paths\": Entrepreneurship, Business, and Web Programming. These courses use aggregated educational content from the web and consistently test students until they master their subject. Each course breaks down into roughly 6 sections, teaching a combination of videos and articles. On August 7, 2012, Course Hero added a further 18 free skill-based courses to their catalog. Course Hero also rewards students who complete 5 or more in either three offered learning paths. Prizes include the opportunity to pitch a business plan to SV Angel and $5,000 or a chance to get a job at Course Hero.\n\nThis is where educators can openly share knowledge and content through Course Hero. You can search through a library of video lectures by university, subject or instructor. This part of the website also offers a free directory of digital courses that are available on the Internet.\n\nThe documents uploaded for sale are frequently the intellectual property of instructors, not of the students who post them/sell them. This includes exams and their keys, quizzes and their keys, study guides written by instructors. To protect the rights of the copyright holders, the Digital Millennium Copyright Act requires Course Hero to expeditiously remove content when it is flagged as infringing its copyright. However, the process to remove copyrighted material can be seen as overly burdensome and may be a subtle way to discourage people from following through on such claims.\n\nAs Course Hero allows students to post previous homework and exam solutions from their classes, the website is often cited as an aid to student cheating. There is some debate about whether faculty who reuse the same homework/exams bear some of the blame, but websites like Course Hero are viewed as not necessarily blameless in the controversy.\n"}
{"id": "35011497", "url": "https://en.wikipedia.org/wiki?curid=35011497", "title": "Cruise ship ID card", "text": "Cruise ship ID card\n\nA cruise ship ID card is a plastic card the size of a credit card that serves several functions for passengers on a cruise ship. Cruise ship ID cards are scanned at the entrances to the ship and at various points throughout the ship, either via magnetic strip, RFID readers or bar code in order to identify the passenger, allow entry to and exit off the ship, allow entry into certain areas of the ship, including the passenger's cabin, bill purchases to the passenger, and various other functions.\n\nThe information printed on the ship card may include:\n\nIt is now rare for a picture of the passenger to be displayed on the card, but it is frequently scanned in the presence of a computer that will display a picture of the passenger on the screen.\n\nFunctions of a ship card may include:\n"}
{"id": "8949", "url": "https://en.wikipedia.org/wiki?curid=8949", "title": "Decca Navigator System", "text": "Decca Navigator System\n\nThe Decca Navigator System was a hyperbolic radio navigation system which allowed ships and aircraft to determine their position by receiving radio signals from fixed navigational beacons. The system used phase comparison of two low frequency signals between 70 and 129 kHz, as opposed to pulse timing systems like Gee and LORAN. This made it much easier to implement the receivers using 1940s electronics, eliminating the need for a cathode ray tube.\n\nThe system was invented in the US, but development was carried out by Decca in the UK. It was first deployed by the Royal Navy during World War II when the Allied forces needed a system which could be used to achieve accurate landings and was not known to the Germans and thus free of jamming. After the war, it was extensively developed around the UK and later used in many areas around the world. Decca's primary use was for ship navigation in coastal waters, offering much better accuracy than the competing LORAN system. Fishing vessels were major post-war users, but it was also used on aircraft, including a very early (1949) application of moving map displays. The system was deployed extensively in the North Sea and was used by helicopters operating to oil platforms.\n\nThe opening of the more accurate Loran-C system to civilian use in 1974 offered stiff competition, but Decca was well established by this time and continued operations into the 1990s. Decca was eventually replaced, along with Loran and similar systems, by the GPS during the 1990s. The Decca system in Europe was shut down in the spring of 2000, and the last worldwide chain, in Japan, in 2001.\n\nThe Decca Navigator System consisted of a number of land-based radio beacons organised into \"chains\". Each chain consisted of a master station and three (occasionally two) secondary stations, termed Red, Green and Purple. Ideally, the secondaries would be positioned at the vertices of an equilateral triangle with the master at the centre. The baseline length, that is, the master-secondary distance, was typically .\n\nEach station transmitted a continuous wave signal that, by comparing the phase difference of the signals from the master and one of the secondaries, resulted in a set of hyperbolic \"lines of position\" called a \"pattern\". As there were three secondaries there were three patterns, termed Red, Green and Purple. The patterns were drawn on nautical charts as a set of hyperbolic lines in the appropriate colour. Receivers identified which hyperbola they were on and a position could be plotted at the intersection of the hyperbola from different patterns, usually by using the pair with the angle of cut closest to orthogonal as possible.\n\nWhen two stations transmit at the phase-locked frequency, the difference in phase between the two signals is constant along a hyperbolic path. Of course, if two stations transmit on the same frequency, it is practically impossible for the receiver to separate them; so instead of all stations transmitting at the same frequency, each chain was allocated a nominal frequency, 1f, and each station in the chain transmitted at a harmonic of this base frequency, as follows:\n\nThe frequencies given are those for Chain 5B, known as the English Chain, but all chains used similar frequencies between 70 kHz and 129 kHz.\n\nDecca receivers multiplied the signals received from the Master and each Slave by different values to arrive at a common frequency (least common multiple, LCM) for each Master/Slave pair, as follows:\n\nIt was phase comparison at this common frequency that resulted in the hyperbolic lines of position. The interval between two adjacent hyperbolas on which the signals are in phase was called a \"lane\". Since the wavelength of the common frequency was small compared with the distance between the Master and Slave stations there were many possible lines of position for a given phase difference, and so a unique position could not be arrived at by this method.\n\nOther receivers, typically for aeronautical applications, divided the transmitted frequencies down to the basic frequency (1f) for phase comparison, rather than multiplying them up to the LCM frequency.\n\nEarly Decca receivers were fitted with three rotating \"Decometers\" that indicated the phase difference for each pattern. Each Decometer drove a second indicator that counted the number of lanes traversed – each 360 degrees of phase difference was one lane traversed. In this way, assuming the point of departure was known, a more or less distinct location could be identified.\n\nThe lanes were grouped into \"zones\", with 18 green, 24 red, or 30 purple lanes in each zone. This meant that on the baseline (the straight line between the Master and its Slave) the zone width was the same for all three patterns of a given chain. Typical lane and zone widths on the baseline are shown in the table below (for chain 5B):\n\nThe lanes were numbered 0 to 23 for red, 30 to 47 for green and 50 to 79 for purple. The zones were labelled A to J, repeating after J. A Decca position coordinate could thus be written: Red I 16.30; Green D 35.80. Later receivers incorporated a microprocessor and displayed a position in latitude and longitude.\n\n\"Multipulse\" provided an automatic method of lane and zone identification by using the same phase comparison techniques described above on lower frequency signals.\n\nThe nominally continuous wave transmissions were in fact divided into a 20-second cycle, with each station in turn simultaneously transmitting all four Decca frequencies (5f, 6f, 8f and 9f) in a phase-coherent relationship for a brief period of 0.45 seconds each cycle. This transmission, known as Multipulse, allowed the receiver to extract the 1f frequency and so to identify which lane the receiver was in (to a resolution of a zone).\n\nAs well as transmitting the Decca frequencies of 5f, 6f, 8f and 9f, an 8.2f signal, known as Orange, was also transmitted. The beat frequency between the 8.0f (Red) and 8.2f (Orange) signals allowed a 0.2f signal to be derived and so resulted in a hyperbolic pattern in which one cycle (360°) of phase difference equates to 5 zones.\n\nAssuming that one’s position was known to this accuracy, this gave an effectively unique position.\n\nDuring daylight, ranges of around could be obtained, reducing at night to 200 to , depending on propagation conditions.\n\nThe accuracy depended on:\n\nBy day these errors could range from a few meters on the baseline up to a nautical mile at the edge of coverage. At night, skywave errors were greater and, on receivers without multipulse capabilities, it was not unusual for the position to jump a lane, sometimes without the navigator knowing.\n\nAlthough in the days of differential GPS this range and accuracy may appear poor, in its day the Decca system was one of the few, if not the only, position fixing system available to many mariners. Since the need for an accurate position is less when the vessel is further from land, the reduced accuracy at long ranges was not a great problem.\n\nIn 1936 William J. O'Brien, an engineer, contracted tuberculosis which put his career on hold for a period of two years. During this period he had the idea of position fixing by means of phase comparison of continuous wave transmissions. This was not the first such system, but O'Brien apparently developed his version without knowledge of the others, and made several advancements in the art that would prove useful. He initially imagined the system being used for aircraft testing, specifically the accurate calculation of ground speed. Some experiments were carried out in California in 1938, selecting frequencies with harmonic \"beats\" that would allow for station identification in a network of transmitters. Both the US Army and Navy considered the idea too complicated and work ended in 1939.\n\nO’Brien's friend, Harvey F. Schwarz, was chief engineer of the Decca Record company in England. In 1939 O’Brien sent him details of the system so it could be put forward to the British military. Initially Robert Watson-Watt reviewed the system but he did not follow it up, deeming it too easily jammed (and likely due to the existing work on the Gee system, being carried out by Watt's group). However, in October 1941 the British Admiralty Signal Establishment (ASE) became interested in the system, which was then classified as \"Admiralty Outfit QM\". O’Brien brought the Californian equipment to the UK and conducted the first marine trials between Anglesey and the Isle of Man, at frequencies of 305/610 kHz, on 16 September 1942.\n\nFurther trials were conducted in the northern Irish Sea in April 1943 at 70/130 kHz. It was decided that the original frequencies were not ideal, and a new system using a 14 kHz inter-signal spacing was selected. This led to the common 5, 6, 8 and 9\"f\" frequencies, used throughout the life of the Decca system. 7\"f\" was reserved for a Loran-C-like extension, but never developed. A follow-up test was carried out in the Irish Sea in January 1944 to test a wide variety of upgrades and production equipment. By this time the competing Gee system was known to the Admiralty and the two systems were tested head-to-head under the code names QM and QH. QM was found to have better sea-level range and accuracy, which led to its adoption.\n\nA three-station trial was held in conjunction with a large-scale assault and landing exercise in the Moray Firth in February/March 1944. The success of the trials and the relative ease of use and accuracy of the system resulted in Decca receiving an order for 27 \"Admiralty Outfit QM\" receivers. The receiver consisted of an electronics unit with two dials and was known to its operators as the \"Blue Gasmeter Job\". A Decca chain was set up, consisting of a master station at Chichester and slaves at Swanage and Beachy Head. A fourth decoy transmitter was located in the Thames Estuary as part of the deception that the invasion would be focussed on the Calais area.\n\n21 minesweepers and other vessels were fitted with \"Admiralty Outfit QM\" and, on 5 June 1944, 17 of these ships used it to accurately navigate across the English Channel and to sweep the minefields in the planned areas. The swept areas were marked with buoys in preparation for the Normandy Landings.\n\nAfter the initial ship tests, Decca conducted tests in cars, driving in the Kingston By-Pass area to verify receiver accuracy. In the car installation, it was found possible to navigate within an individual traffic lane. The company entertained high hopes that the system could be used in aircraft, to permit much more precise navigation in the critical airspace around airports and urban centres where traffic density was highest.\n\nAfter the end of World War II the Decca Navigator Co. Ltd. was formed (1945) and the system expanded rapidly, particularly in areas of British influence; at its peak it was deployed in many of the world's major shipping areas. More than 15,000 receiving sets were in use aboard ships in 1970. There were 4 chains around England, 1 in Ireland and 2 in Scotland, 12 in Scandinavia (5 each in Norway and Sweden and 1 each in Denmark and Finland), a further 4 elsewhere in northern Europe and 2 in Spain.\n\nIn the late 1950s an experimental Decca chain was set up in the United States, in the New York area, to be used for navigating the Vertol 107 helicopters of New York Airways. These helicopters were operating from the principal local airports—Idlewild Airport on Long Island, Newark Airport in New Jersey, LaGuardia Airport in the Borough of Queens, nearer to Manhattan, and a site on the top of the (then) PanAm Building on Park Avenue. Use of Decca was essential because its signals could be received down to sea level, were not subject to the line-of-sight limitations of VOR/DME and did not suffer the slant-range errors that create problems with VOR/DME close to the transmitters. The Decca installations in the New York Airways helicopters included the unique Decca 'roller map' displays that enabled the pilot to see his or her position at a glance, a concept infeasible with VOR/DME.\n\nThis chain installation was considered highly controversial at the time, for political reasons. This led to the U.S. Coast Guard, under instructions from the Treasury Department to which it reported, banning the use of Decca receivers in ships entering New York harbour for fear that the system might create a de facto standard (as it had become in other areas of the world). It also served to protect the marketing interests of the Hoffman Electronics division of ITT, a principal supplier of VOR/DME systems, that Decca might have been poised to usurp.\n\nThis situation was exacerbated by the workload problems of the Air Traffic Controllers Association (ATCA), under its executive director Francis McDermott, whose members were forced to use radar data on aircraft positions, relaying those positions by radio to the aircraft from their control locations. An example of the problem, cited by experts, was the collision of a Douglas DC8 and a Lockheed Constellation over Staten Island, New York, that—according to some experts—could have been avoided if the aircraft had been Decca-equipped and could not only have determined their positions more precisely but would not have suffered from the rho-theta position errors inherent in VOR/DME.\n\nOther chains were established in Japan (6 chains); Namibia and South Africa (5 chains); India and Bangladesh (4 chains); Canada (4 chains around Newfoundland and Nova Scotia); North-West Australia (2 chains); the Persian Gulf (1 chain with stations in Qatar and the United Arab Emirates and a second chain in the north of the Gulf with stations in Iran) and the Bahamas (1 chain). Four chains were planned for Nigeria but only two were built and these did not enter into public service. Two chains in Vietnam were used during the Vietnam War for helicopter navigation, with limited success.\nDuring the Cold War period, following WWII, the R.A.F. established a confidential chain in Germany. The Master station was in Bad Iburg near Osnabrück and there were two Slaves. The purpose of this chain was to provide accurate air navigation for the corridor between Western Germany and Berlin in the event that a mass evacuation of allied personnel may be required. In order to maintain secrecy, frequencies were changed at irregular intervals.\n\nThe headquarters of Decca Navigator were at New Malden, Surrey, just off the Kingston by-pass. There was a Decca School, at Brixham, Devon, where employees were sent on courses from time to time. Racal, the UK weapons and communications company, acquired Decca in 1980. Merging Decca's radar assets with their own, Racal began selling off the other portions of the company, including avionics and Decca Navigator.\n\nA significant amount of income from the Decca system was due to the receivers being leased to users, not sold outright. This guaranteed predictable annual income. When the patents on the original technology lapsed in the early 1980s, new receivers were quickly built by a number of companies. In particular, Aktieselskabet Dansk Philips ('Danish Philips', \"ap\") introduced receivers that could be purchased outright, and were much smaller and easier to use than the current Decca counterparts. The \"ap\" versions directly output the longitude and latitude to two decimals (originally in datum ED50 only) instead of using the \"deco meter\" displays, offering accuracy better than ±9.3 m, much better than the Decca units. This also eliminated the need for the special charts printed with Decca lanes and zones.\n\nDecca sued ap for infringement and, in the ensuing court battle, Decca lost the monopoly. That signalled the beginning of the end for the company. Income dwindled and eventually, the UK Ministry of Transport stepped in, having the lighthouse authorities take responsibility for operating the system in the early 1990s.\n\nA ruling from the European Union forced the UK government to withdraw funding. The general lighthouse authority ceased Decca transmissions at midnight on 31 March 2000. The Irish chain provided by Bórd Iascaigh Mhara continued transmitting until 19 May 2000. Japan continued operating their Hokkaidō chain until March 2001, the last Decca chain in operation.\n\nIn the immediate post-war era, Decca began studying a long-range system like Decca, but using much lower frequencies to enable reception of skywaves at long distances. In February 1946 the company proposed a system with two main stations located at Shannon Airport in Ireland and Gander International Airport in Newfoundland (today part of Canada). Together, these stations would provide navigation over the main great circle route between London and New York. A third station in Bermuda would provide general ranging information to measure progress along the main track.\n\nWork on this concept continued, and in 1951 a modified version was presented that offered navigation over very wide areas. This was known as Delrac, short for \"Decca Long Range Area Cover\". A further development, including features of the General Post Office's POPI system, was introduced in 1954, proposing 28 stations that provided worldwide coverage. The system was predicted to offer accuracy at range 95% of the time. Further development was ended in favour of the Dectra system.\n\nIn the early 1960s the Radio Technical Commission for Aeronautics (RTCA), as part of a wider ICAO effort, began the process of introducing a standard long-range radio navigation system for aviation use. Decca proposed a system that could offer both high-accuracy at short ranges and trans-Atlantic navigation with less accuracy, using a single receiver. The system was known as Dectra, short for \"Decca Track\".\n\nUnlike the Delrac system, Dectra was essentially the normal Decca Navigator system with the modification of several existing transmitter sites. These were located at the East Newfoundland and Scottish chains, which were equipped with larger antennas and high-power transmitters, broadcasting 20 times as much energy as normal chain stations. Given that the length of the chain baselines did not change, and were relatively short, at long distance the signal offered almost no accuracy. Instead, Dectra operated as a track system; aircraft would navigate by keeping themselves within the signal defined by a particular Decca lane.\n\nThe main advantage of Dectra compared to other systems being proposed for the RTCA solution was that it could be used for both medium-range navigation over land, as well as long-range navigation over the Altlantic. In comparison, the VOR/DME system that ultimately won the competition offered navigation over perhaps a 200 mile radius, and could not offer a solution to the long-distance problem. Additionally, as the Decca system provided an X and Y location, as opposed to the angle-and-range VOR/DME, Decca proposed offering it with their Decca Flight Log moving map display to further improve ease of navigation. In spite of these advantages, the RTCA ultimately chose VOR/DME for two primary reasons; VOR offered coverage over about the same range as Decca, about 200 miles, but did so with a single transmitter instead of Decca's four, and Decca's frequencies proved susceptible to interference from static due to lightning, while VOR's higher frequencies were not quite as sensitive.\n\nDecca continued to propose that Dectra be used for the long-range role. In 1967 they installed another transmitter in Iceland to provide ranging along the Scotland-Newfoundland track, with a second proposed to be installed on the Azores. They also installed Dectra receivers with Omnitrac computers and a lightweight version of the Flight Log on a number of commercial airliners, notably a BOAC Vickers VC10. The Omnitrac could take inputs from Decca (and Dectra), Loran-C, VOR/DME, an air data computer and doppler radars and combine them all to produce a lat/long output along with bearing, distance-to-go, bearing and an autopilot coupling. Their efforts to standardize this were eventually abandoned as inertial navigation systems began to be installed for these needs.\n\nA more accurate system named Hi-Fix was developed using signalling in the 1.6 MHz range. It was used for specialised applications such as precision measurements involved with oil-drilling and by the Royal Navy for detailed mapping and surveying of coasts and harbours. The Hi-Fix equipment was leased for a period with temporary chains established to provide coverage of the area required, Hi-Fix was commercialised by Racal Survey in the early 1980s. An experimental chain was installed with coverage of central London and receivers placed in London buses and other vehicles to demonstrate an early vehicle location and tracking system. Each vehicle would report its location automatically via a conventional VHF two-way radio link, the data added to a voice channel.\n\nAnother application was developed by the Bendix Pacific division of Bendix Corporation, with offices in North Hollywood, California, but not deployed: PFNS—Personal Field Navigation System—that would enable individual soldiers to ascertain their geographic position, long before this capability was made possible by the satellite-based GPS (Global Positioning System).\n\nA further application of the Decca system was implemented by the U.S. Navy in the late 1950s and early 1960s for use in the Tongue of the Ocean/Eleuthera Sound area near The Bahamas, separating the islands of Andros and New Providence. The application was for sonar studies made possible by the unique characteristics of the ocean floor.\n\nAn interesting characteristic of the Decca VLF signal discovered on BOAC, later British Airways, test flights to Moscow, was that the carrier switching could not be detected even though the carrier could be received with sufficient strength to provide navigation. Such testing, involving civilian aircraft, is quite common and may well not be in the knowledge of a pilot.\n\nThe 'low frequency' signalling of the Decca system also permitted its use on submarines. One 'enhancement' of the Decca system was to offer the potential of keying the signal, using Morse code, to signal the onset of nuclear war. This option was never taken up by the UK government. Messages were clandestinely sent, however, between Decca stations thereby bypassing international telephone calls, especially in non-UK chains.\n\n\n\n\n"}
{"id": "53267005", "url": "https://en.wikipedia.org/wiki?curid=53267005", "title": "Design for Verification", "text": "Design for Verification\n\nDesign for Verification (DfV) is a set of engineering guidelines to aid designers in ensuring right first time manufacturing and assembly of large-scale components. The guidelines were developed as a tool to inform and direct designers during early stage design phases to trade off estimated measurement uncertainty against tolerance, cost, assembly, measurability and product requirements.\n\nIncreased competition in the aerospace market has placed additional demands on aerospace manufacturers to reduce costs, increase product flexibility and improve manufacturing efficiency. There is a knowledge gap within the sphere of digital to physical dimensional verification and on how to successfully achieve dimensional specifications within real-world assembly factories that are subject to varying environmental conditions. The DfV framework is an engineering principle to be used within low rate and high value and complexity manufacturing industries to aid in achieving high productivity in assembly via the effective dimensional verification of large volume structures, during final assembly. The DfV framework has been developed to enable engineers to design and plan the effective dimensional verification of large volume, complex structures in order to reduce failure rates and end-product costs, improve process integrity and efficiency, optimise metrology processes, decrease tooling redundancy and increase product quality and conformance to specification. The theoretical elements of the DfV methods were published in 2016, together with their testing using industrial case studies of representative complexity. The industrial tests published on ScienceDirect proved that by using the new Design for Verification methods alongside the traditional ‘Design for X’ toolbox, the resultant process achieved improved tolerance analysis and synthesis, optimized large volume metrology and assembly processes and more cost effective tool and jig design.\n\n"}
{"id": "22800551", "url": "https://en.wikipedia.org/wiki?curid=22800551", "title": "Distributed Oceanographic Data Systems", "text": "Distributed Oceanographic Data Systems\n\nThe Distributed Oceanographic Data Systems, or DODS, is a type of server that allows sharing data with remote users or between DODS servers. It is developed by the National Oceanic and Atmospheric Administration, and is based upon the OPeNDAP data transport architecture.\n"}
{"id": "32986709", "url": "https://en.wikipedia.org/wiki?curid=32986709", "title": "DockNET", "text": "DockNET\n\nDockNET is an intelligent control system for cargo handling developed by Portsystem in Sweden.\n\nThe control system was created for being able to analyze, communicate, supervise, direct, record and to document everything that happens in and around a loading door hole in a modern cargo terminal. Cargo terminals often suffer damage to loading houses and vehicles incurred in connection with drive into and away from the loading house. This is usually not just mistake out of the driver, but lack of communication between driver and loading dock. DockNET solves this lack of communication and secures while running the work environment and cargo handling. The result is increased efficiency where technology makes everything measurable around the loading space.\n\n\n"}
{"id": "54953193", "url": "https://en.wikipedia.org/wiki?curid=54953193", "title": "Doppler Labs", "text": "Doppler Labs\n\nDoppler Labs was a San Francisco-based audio technology company. Founded in 2013 in New York City, the company's mission was to make computing more immersive and human. The company designed and manufactured in-ear computing technology, including earplugs and wireless smart earbuds.\n\nDoppler Labs was co-founded by Noah Kraft and Fritz Lanman. Kraft had previously worked in the entertainment industry, while Lanman was an executive at Microsoft and a prominent angel investor.\n\nIn July 2015, Doppler raised $17 million in Series B funding bringing the company's total funding to over $50 million. The round was led by The Chernin Group, Wildcat Capital Management, and Acequia Capital and included luminary investors like Henry Kravis, David Geffen, Blake Krikorian, Dan Gilbert, David Bonderman and Barry Sternlicht.\n\nDoppler Labs first product was DUBS Acoustic Filters, high-tech ear plugs designed that use a proprietary 17-piece physical acoustic filter system to reduce the sound pressure at different frequencies while maintaining acoustical fidelity. In July 2016, Doppler Labs Labs launched Here Active Listening at the Coachella Valley Music and Arts Festival and in 2017 launched its flagship product Here One, a pair of wireless smart earbuds that allow users to selectively filter ambient sound, stream music, and amplify speech. It can also be used to take phone calls and selectively filter certain sounds, such as background noise. Here One has been called the world’s first in-ear computer.\n\nIn March 2017, Doppler Labs sued Bose for trademark infringement of their Here Buds trademark.\n\nThe company supported the Over-the-Counter (OTC) Hearing Aid Act of 2017, which passed both houses of Congress.\n\nOn November 1, 2017, Doppler Labs announced that the company would be winding down operations, and officially closed on December 1. The company cited problems raising additional Series C funding as the reason for the company shutting down. Wired wrote that the company unsuccessfully explored options to stay afloat including partnership, investment, and acquisition from companies such as Microsoft, Apple, Google, Amazon, and Facebook. It was preparing to launch its next product, Here Two, in 2018.\n\nBefore voice assistants or true wireless technology were prevalent, Doppler Labs envisioned that computing would move onto the body and into the ear and that voice would become a more primary interface for how humans interact with technology. With Apple’s removal of the headphone jack, the launch of the AirPods, and the prevalence of Alexa, the smart earbud category that Doppler helped create was expected to become a $40 billion industry by 2020.\n\nThe company develops and distributes the Here One Wireless Smart Earbuds, a pair of wireless 3-in-1 earbuds that combine premium audio streaming, smart noise cancellation, and speech enhancement into a single form factor, as well as DUBS Acoustic Filters high-tech earplug. The company previously developed and released the predecessor to Here One called Here Active Listening, which was originally launched on Kickstarter.\n\nDoppler Labs has announced and demonstrated future product features, including real-time language translation and its “machine hearing” system.\n\nIn addition to its pre-existing partnerships with the Tao Group, Coachella, Bonnaroo and Outside Lands, in November 2016, Doppler Labs announced seven new partnerships with The New York Philharmonic, the Cleveland Cavaliers, the Fine Arts Museums of San Francisco, JetBlue, Gimlet Media, MADE Fashion and The New York Mets to bring Here One technology to sporting events, museums, concert halls, and other live environments.\n\nIn December 2016, they also partnered with the Global Citizen Festival to launch #HereTogether, a movement aimed at bringing greater global awareness around efforts to prevent hearing loss and to promote innovation in hearing accessibility As part of this initiative, Doppler Labs announced its Hearing Bill of Rights in April 2017\n\n"}
{"id": "1493171", "url": "https://en.wikipedia.org/wiki?curid=1493171", "title": "Dove (toiletries)", "text": "Dove (toiletries)\n\nDove is a personal care brand owned by Unilever originating in the United Kingdom. Dove products are manufactured in Argentina, Australia, Brazil, Canada, China, Egypt, Germany, India, Indonesia, Israel, Ireland, Japan, Mexico, Netherlands, Pakistan, Philippines, Poland, South Africa, Thailand, Turkey, and United States.\n\nThe products are sold in more than 80 countries and are offered for both women, men and babies. Dove's logo is a silhouette profile of the brand's namesake bird. Vincent Lamberti was granted the original patents related to the manufacturing of Dove in the 1950s, while he worked for the Lever brothers.\n\nProducts include: antiperspirants/deodorants, body washes, beauty bars, lotions/moisturizers, hair care, and facial care products. Dove is primarily made from synthetic surfactants, vegetable oils (such as palm kernel) and salts of animal fats (tallow). In some countries, Dove is derived from tallow, and for this reason it is not considered vegan, unlike vegetable oil based soaps.\n\nUnilever launched a men's toiletries range in January 2010, branded \"Dove Men + Care\". In 2012, Steve Bell of Macon, Georgia won the Dove Men+Care Hair \"King of the Castle Home Upgrade\" contest, receiving a home upgrade and consultation with Jonathan Scott of \"Property Brothers\".\n\nIn 2004, Dove began its Campaign for Real Beauty, followed by the creation of the Dove Self-Esteem Fund in 2006, by Geyner Andres Gaona and \"Amy\". The campaign has been criticized as hypocritical in light of the highly sexualized images of women presented in the advertising of Axe, which like Dove is produced by Unilever.\n\nIn May 2011, Dove prompted criticism and accusations of racism after publishing an ad for their body wash showing three women with different skin tones side by side in front of a \"before and after\" image of cracked and smooth skin, with a black woman below the \"before\" and a white woman below the \"after\".\n\nIn 2017, a 3-second video for Dove body lotion posted on their U.S. Facebook page prompted criticism and accusations of racism. The video clip showed a black woman removing her T-shirt to reveal a white woman, who then lifts her own T-shirt to reveal an Asian woman. The full 30 second TV commercial version included seven women of different races and ages. The ad sparked criticism, leading Dove to remove the ad, saying it “deeply regret[ed] the offence it caused.” Dove further stated that the \"video was intended to convey that Dove body wash is for every woman and be a celebration of diversity...\" The black woman in the ad, Lola Ogunyemi, said the ad had been misinterpreted and defended Dove.\n\n"}
{"id": "17728747", "url": "https://en.wikipedia.org/wiki?curid=17728747", "title": "Electronic Commerce Directive 2000", "text": "Electronic Commerce Directive 2000\n\nThe Electronic Commerce Directive 2000/31/EC is a European Union Directive of the European Parliament and of the Council from 8 June 2000. It regulates certain legal aspects of information society services in the Internal Market, in particular electronic commerce and mere conduit.\n\nThe Electronic Commerce Directive, adopted in 2000, sets up an Internal Market framework for electronic commerce. Its aim is to provide legal certainty for business and consumers. It establishes harmonised rules on issues such as the transparency and information requirements for online service providers, commercial communications, electronic contracts and limitations of liability of intermediary service providers.\n\nThe E-Commerce Directive makes several provisions on the liability of any service normally provided for remuneration, at a distance, by electronic means and at the individual request of a recipient of services (intermediary).\n\nWhere an information society service is provided that consists of the transmission in a communication network of information provided by a recipient of the service, or the provision of access to a communication network, Member States shall ensure that the service provider is not liable for the information transmitted, on condition that the provider:\nThe acts of transmission and of provision of access include the automatic, intermediate and transient storage of the information transmitted in so far as this takes place for the sole purpose of carrying out the transmission in the communication network, and provided that the information is not stored for any period longer than is reasonably necessary for the transmission.\n\nWhere an information society service is provided that consists of the transmission in a communication network of information provided by a recipient of the service, Member States shall ensure that the service provider is not liable for the automatic, intermediate and temporary storage of that information, performed for the sole purpose of making more efficient the information's onward transmission to other recipients of the service upon their request, on condition that:\n\nWhere an information society service is provided that consists of the storage of information provided by a recipient of the service, Member States shall ensure that the service provider is not liable for the information stored at the request of a recipient of the service, on condition that:\n\nArticle 14 forms the basis for notice and take down procedures by online hosts under EU law.\n\n\n"}
{"id": "44718002", "url": "https://en.wikipedia.org/wiki?curid=44718002", "title": "Emospark", "text": "Emospark\n\nEmoSPARK is an artificial intelligence console created in London, United Kingdom by Patrick Levy-Rosenthal. The device uses facial recognition and language analysis to evaluate human emotion and convey responsive content according to the emotion. The console measures 90 mm x 90 mm x 90 mm and is cube shaped. It operates on an \"Emotional Processing Unit\", an emotion chip developed by Emoshape Inc. that enables the system to create emotional profile graphs of its surroundings. The emotional processing unit is a patent pending technology that is said to create synthesised emotional responses in machines. EmoSPARK was funded through an Indiegogo campaign which aimed to raise $200,000.\n\nEmoSPARK was created by French inventor Patrick Levy-Rosenthal, as an emotionally intelligent artificial life unit for the home that can interact with people. It is powered by Android and can communicate with users through typed input from a computer, tablet, smartphone or TV as well as through spoken commands.\n\nThe EmoSpark's features are categorized into two types: functional and emotional. EmoSPARK is said to have the ability to perform practical software-based tasks. Through the smartphone interface, it is able to gauge a person’s emotions and is reported to have a conversational library of over 2 million sentences. The face-tracking technology identifies users likes and dislikes to categorize their emotional responses to stimuli such as videos and music. The device has an emotional spectrum that is composed of eight emotions which are \"surprise, sadness, joy, trust, fear, disgust, anger and anticipation\".\n\nEmoSPARK monitors a person's facial expressions and emotions through images from an external camera which are then processed through an emotion text analysis and content analysis. The New Scientist reported that EmoSPARK had the ability to work on the best way to cheer up its users, emotionally.\n\nEmoSPARK is able to connect to Facebook and YouTube to present users with content designed to improve their mood or to Wikipedia for collaborative knowledge that can be shared when users ask questions of it. Through Android OS, EmoSPARK is able to be customized with Google Play store apps.\n\nThe cube is capable of learning the user’s emotions and responses to types of music or content then uses it in the future for similar emotions. It is also able to emulate the emotions that it has observed and learned which are in the spectrum of primary emotions. The cube is expected to develop its own personality based on the communications it has had with the people using it.\n\nThe Emotion Chip (EPU) used in the cube is created by the US company Emoshape Inc founded by Levy-Rosenthal. EmoShape Ltd (UK) was the company that developed EmoSPARK cube. Patrick Levy-Rosenthal also received the IST Prize in 2005 from the European Council for Applied Science, Technology and Engineering.\n\n"}
{"id": "34200049", "url": "https://en.wikipedia.org/wiki?curid=34200049", "title": "EyeEm", "text": "EyeEm\n\nEyeEm (originally Eye'em), pronounced \"I am\", is a technology company with a global photography community and marketplace that was co-founded by Florian Meissner, Ramzi Rizk, Gen Sadakane, and Lorenz Aschoff in Berlin. Eyeem.com and the EyeEm mobile app is a place for photographers of any ability to share, interact and learn more about photography. The technology company uses artificial intelligence to find the best images to license to brands, agencies or individuals looking for authentic imagery. As of August 2016, the community consists of over 18 million photographers and more than 70M photos.\n\nIn early 2010, photo enthusiasts Florian Meissner, Ramzi Rizk, Gen Sadakane and Lorenz Aschoff came together to discuss mobile photography. By March, they decided to host one of the world's first mobile photography competitions and host an exhibition. The winners and runners-up were part of an exhibition that took place in Berlin in June 22 of the same year.\n\nDuring the beta period, EyeEm was an iPhone only app and was called EYE'EM, and managed to have more than 5,000 users from 79 countries. In early 2011, Meissner, Rizk, Sadakane, and Aschoff decided to work on the idea full-time and launched the first version of the EyeEm app on Android and iOS in August 2011 following the beta test period.\n\nIn March 2014, EyeEm partnered with Getty Images to distribute photographs taken by EyeEm users. By June 2014, EyeEm had over 10 million users.\n\nIn March 2015, EyeEm reached 13 million users and launched Market, its own online marketplace where users can sell their own images. A month later, in April 2015, the technology company raised $18M in Series B funding. The round was led by Valar Ventures and includes existing investors Earlybird Ventures, Passion Capital, Wellington Partners, Atlantic Labs and Open Ocean Capital. As part of the new round, Valar took a board seat at the company. In June 2015, EyeEm launched the Discover Feed, a manually curated feed, in an effort to showcase the “most beautiful images from featured photographers and albums.” In September 2015, EyeEm launched EyeEm Vision (previously called EyeVision), a deep learning computer vision framework that recognizes concepts and ranks aesthetics of images.\n\nIn January 2016, EyeEm announced a new partnership with Alamy as another distribution partner besides Getty. In May 2016, EyeEm launched The Roll, an iOS app that analyzes the camera roll and uses computer vision to tag the images and rank the photos by how good they are. The app made headlines at Apple's worldwide developer conference (WWDC 2016), where it was announced that The Roll will be one of the first apps to deeply integrate with Siri, \"by combining speech recognition with EyeEm’s aesthetic rank technology\". In July 2016, EyeEm reaches 18M users and opens their Web Upload to all users so that it is as easy as possible for Flickr users to upload their pictures to EyeEm Market and join their photography based community.\n\nEyeEm's image recognition technology uses artificial intelligence to tag and rank images based on an aesthetic score assigned to each photo. When users upload a photo via the Web Upload tool, this technology is applied to determine the discoverability of each photo, and suggest keywords. The company applied this to an iOS app called The Roll, which launched in May 2016 and organizes the images on your camera roll.\n\nThe company has raised $24 million to date in funding from Valar Ventures, Earlybird Venture Capital, Passion Capital, Wellington Partners, Atlantic Labs and OpenOcean. In 2018, it was revealed that the company raised an additional $10 million in an internal round from existing investors, with new investors Cipio Partners joining the round. \nEyeEm was incorporated in Berlin, Germany, in February 2011 by founders Florian Meissner, Lorenz Aschoff, Gen Sadakane, and Ramzi Rizk.\n\n\n"}
{"id": "14597216", "url": "https://en.wikipedia.org/wiki?curid=14597216", "title": "Festuca arundinacea", "text": "Festuca arundinacea\n\nFestuca arundinacea (syn., Schedonorus arundinaceus and Lolium arundinaceum) is a species of grass commonly known as tall fescue. It is a cool-season perennial C species of bunchgrass native to Europe. It is an important forage grass throughout Europe, and many cultivars have been used in agriculture. It is also an ornamental grass in gardens, and a phytoremediation plant.\n\nThe predominant cultivar found in British pastures is S170, an endophyte-free variety. In its native European environment, tall fescue is found in damp grasslands, river banks, and in coastal seashore locations. Its distribution is a factor of climatic, edaphic, or other environmental attributes.\n\nTall fescue was introduced into the United States in the late 19th century, but it did not establish itself as a widely used perennial forage until the 1940s. As in Europe, tall fescue has become an important, well-adapted cool season forage grass for agriculture in the US with many cultivars. In addition to forage, it has become an important grass for turf and soil conservation. Tall fescue is the most heat tolerant of the major cool season grasses. Tall fescue has a deep root system compared to other cool season grasses. This non-native grass is well adapted to the \"transition zone\" Mid Atlantic and Southeastern United States and now occupies over .\n\nTall fescue has become an invasive species and noxious weed in native California grasslands and habitats, such as the California coastal prairie plant community.\n\nThe dominant cultivar grown in the United States is Kentucky 31. In 1931 E. N. Fergus, a professor of agronomy at the University of Kentucky, collected seed from a population on a hillside in Menifee County, Kentucky although formal cultivar release did not happen until 1943. Fergus heard about this \"wonder grass\" while judging a sorghum syrup competition in a nearby town. He wanted to see this grass because it was green, lush, and growing well on a sloped hillside during a drought. While visiting the site he was impressed and took seed samples with him. With this seed he conducted variety trials, initiated seed increase nurseries, and lauded its performance. It was released as Kentucky 31 in 1943 and today it dominates grasslands in the humid southeastern US. In 1943, Fergus and others recognized this tall fescue cultivar as being vigorous, widely adaptable, able to withstand poor soil conditions, resistant to pests and drought. It is used primarily in pastures and low maintenance situations.\n\nBreeders have created numerous cultivars that are dark green with desirable narrower blades than the light green coarse bladed K-31. Tall fescue is the grass on the South Lawn of the White House.\n\nTall fescue is a long-lived perennial bunchgrass species. Photosynthesis occurs throughout the leaves, which form bunches and are thick and wide with prominent veins running parallel the entire length of the blade. The blades have a \"toothed\" edge which can be felt if fingers are run down the edge of the leaf blade. The underside of the leaf may be shiny. Emerging leaves are rolled in the bud with no prominent ligule. Note that most grasses are folded not rolled, which make this a key identification feature on tall fescue. The auricles are usually blunt but occasionally may be more clawlike. The culm is round in cross-section. Typically, this species of grass has a long growing season and ranges between 2 and tall in seedhead stage.\n\nTall fescue spreads through tillering and seed transmission — not by stolons or rhizomes, which are common in many grass species. However, tall fescue may have numerous sterile shoots that extend the width of each bunch. There are approximately 227,000 seeds per pound.\n\nTypically found across the mid-Atlantic and Southeast US, tall fescue performs best in soils with pH values between 5.5 and 7. Growth may occur year-round if conditions are adequate, but typically growth ceases when soil temperature falls below .\n\n\"Festuca arundinacea\" was first described by the German naturalist Johann Christian Daniel von Schreber in 1771. It was later moved to the genus \"Schedonorus\" by the Belgian botanist Barthélemy Charles Joseph Dumortier in 1824 and again to the genus \"Lolium\" under the name \"Lolium arundinaceum\" by Stephen J. Darbyshire in 1993. The genus \"Schedonorus\" was resurrected in 1998 and the name \"Schedonorus arundinaceus\" (Schreb.) Dumort. was conserved against the earlier name \"Schedonorus arundinaceus\" Roem. & Schult. Best known by the name \"Festuca arundinacea\", there is disagreement by taxonomists whether \"Festuca\" subgenus \"Schedonorus\" is allied more with the genus \"Lolium\" or best elevated to genus rank on its own.\n\nTall fescue can be found growing in most soils of the southeast including marginal, acidic, and poorly drained soils and in areas of low fertility, and where stresses occur due to drought and overgrazing. These beneficial attributes are now known to be a result of a symbiotic association with the fungus \"Neotyphodium coenophialum\".\n\nThis association between tall fescue and the fungal endophyte is a mutualistic symbiotic relationship (both symbionts derive benefits from it). The fungus remains completely intercellular, growing between the cells of the aboveground parts of its grass host. The fungus is asexual, and is transmitted to new generations of tall fescue only through seed, a mode known as vertical transmission. Thus in nature, the fungus does not live outside the plant. Viability of the fungus in seeds is limited; typically, after a year or two of seed storage the fungal endophyte mycelium has died, and seeds germinated will result in plants that are endophyte-free.\n\nThe tall fescue–endophyte symbiosis confers a competitive advantage to the plant. Endophyte-infected tall fescue compared to endophyte-free tall fescue deters herbivory by insects and mammals, bestows drought resistance, and disease resistance. In return for shelter, seed transmission, and nutrients the endophyte produces secondary metabolites. These metabolites, namely alkaloids, are responsible for increased plant fitness. Alkaloids in endophytic tall fescue include 1-aminopyrrolizidines (lolines), ergot alkaloids (clavines, lysergic acids, and derivative alkaloids), and the pyrrolopyrazine, peramine.\n\nThe lolines are the most abundant alkaloids, with concentrations 1000 higher than those of ergot alkaloids. Endophyte-free grasses do not produce lolines, and, as shown for the closely related endophyte commonly occurring in meadow fescue, \"Neotyphodium uncinatum\", the endophyte can produce lolines in axenic laboratory culture. However, although \"N. coenophialum\" possesses all the genes for loline biosynthesis, it does not produce lolines in culture. So in the tall fescue symbiosis, only the interaction of the host and endophyte produces the lolines. Lolines have been shown to deter insect herbivory, and may cause various other responses in higher organisms. Despite their lower concentrations, ergot alkaloids appear to significantly affect animal growth. Ergots cause changes in normal homeostatic mechanisms in animals that result in toxicity manifested through reduced weight gains, elevated core temperatures, restricted blood flow, reduced milk production and reproductive problems. Peramine, like the ergot alkaloids, is found in much lower concentrations in the host compared with loline alkaloids. Its activity has been shown to be primarily insecticidal, and has not been linked to toxicity in mammals or other herbivores.\n\nHorses are especially prone to reproductive problems associated with tall fescue, often resulting in death of the foal, mare, or both. Horses which are pregnant may be strongly affected by alkaloids produced by the tall fescue symbiont. Broodmares that forage on infected fescue may have prolonged gestation, foaling difficulty, thickened placenta, or impaired lactation. In addition, the foals may be born weakened or dead. To moderate toxicosis, it is recommended that pregnant mares should be taken off infected tall fescue pasture for 60–90 days before foaling as late gestation problems are most common.\n\nFescue toxicity in cattle appears as roughening of the coat in the summer and intolerance to heat. Cattle that graze on tall fescue are more likely to stay in the shade or wade in the water in hot weather. In the winter, a condition known as \"fescue foot\" might afflict cattle. This results from vasoconstriction of the blood vessels especially in the extremities, and causes a gangrenous condition. Untreated, the hoof might slough off. Additionally, cattle may experience decreased weight gains and poor milk production when heavily grazing infected tall fescue pasture. To deter toxicosis cattle should be given alternative feed to dilute their infected tall fescue intake.\n\nCarbon cycling in terrestrial ecosystems is a major focus of research. Terrestrial carbon sequestration is the process of removing carbon dioxide from the atmosphere via photosynthesis and storing this carbon in either plant or soil carbon pools. Increases in soil organic carbon help aggregate the soil, increase infiltration, reduce erosion, increase soil fertility, and act as long lived pools of soil carbon. Many studies have suggested that long term endophyte-infected tall fescue plots increase soil carbon storage in the soil by limiting the microbial and macrofaunal activity to break down endophyte infected organic matter input and by increasing inputs of carbon via plant production. While the long term studies tend to show an increase in carbon storage, the short term studies do not. However, short term studies have shown that the endophyte association results in higher above- and belowground plant biomass production compared to uninfected plants, as well as a decrease in certain microbial communities. Site-specific characteristics, such as management and climate, need to be further understood to realize the ecological role and potential benefits of tall fescue and the endophyte association as it relates to carbon sequestration.\n\nNew cultivars are being bred and tested every year. A major focus of research is producing endophyte-infected tall fescue cultivars that have no detrimental effects to livestock while keeping the endophytic effects of reduced insect herbivory, disease resistance, drought tolerance, and extended growing season. Novel endophytes, also referred to as \"friendly\" endophytes, are symbiotic fungi that are associated with tall fescue, but do not produce target alkaloids in toxic concentrations. A widely used and tested novel endophyte is called MaxQ and is grown in the tall fescue grass host Georgia-Jesup. This cultivar of tall fescue-novel endophyte combination produces ergot alkaloids at near zero levels while maintaining the concentration of other alkaloids.\n\n\n"}
{"id": "7192395", "url": "https://en.wikipedia.org/wiki?curid=7192395", "title": "FirstDefender", "text": "FirstDefender\n\nFirstDefender is a handheld liquid and solid chemical identification instrument which uses a method of analysis called Raman spectroscopy. It is designed for use by first responders, homeland security, law enforcers and forensic chemistry personnel for immediate identification of unknown solids, liquids, and mixtures. This includes narcotics, explosives, white powders, chemical weapons, WMDs and toxic industrial chemicals. The detector identifies substances even through the walls of their containers. It is used by some airport officials in the United States to detect liquid explosives.\n\nhttp://www.ahuracorp.com/first_defender.html\n"}
{"id": "16210493", "url": "https://en.wikipedia.org/wiki?curid=16210493", "title": "Heat spreader", "text": "Heat spreader\n\nA heat spreader conducts heat between a heat source and a heat sink or heat exchanger. Most commonly a plate or block of material having high thermal conductivity, such as copper, aluminum, or diamond. (A heat pipe performs a similar function, but it uses moving fluids inside a sealed case.) By definition, it \"spreads out\" heat, so that the heat exchanger(s) may be more fully utilized. This has the potential to increase the heat capacity of the total assembly, but the additional thermal junctions limit total thermal capacity. The high conduction properties of the spreader will make it more effective to function as an air heat exchanger, as opposed to the original (presumably smaller) source. The low heat conduction of air in convection is matched by the higher surface area of the spreader, and heat is transferred more effectively.\n\nA heat spreader is generally used when the heat source tends to have a high heat-flux density, (high heat flow per unit area), and for whatever reason, heat can not be conducted away effectively by the heat exchanger. For instance, this may be because it is air-cooled, giving it a lower heat transfer coefficient than if it were liquid-cooled. A high enough heat exchanger transfer coefficient is sufficient to avoid the need for a heat spreader.\n\nThe use of a heat spreader is an important part of an economically optimal design for transferring heat from high to low heat flux media. Examples include:\n\nDiamond has a very high thermal conductivity. Synthetic diamond is used as submounts for high-power integrated circuits and laser diodes.\n\nComposite materials can be used, such as the metal matrix composites (MMCs) copper–tungsten, AlSiC (silicon carbide in aluminium matrix), Dymalloy (diamond in copper-silver alloy matrix), and E-Material (beryllium oxide in beryllium matrix). Such materials are often used as substrates for chips, as their thermal expansion coefficient can be matched to ceramics and semiconductors.\n\n"}
{"id": "12953321", "url": "https://en.wikipedia.org/wiki?curid=12953321", "title": "IEEE Journal of Quantum Electronics", "text": "IEEE Journal of Quantum Electronics\n\nThe IEEE Journal of Quantum Electronics is a peer-reviewed scientific journal covering optical, electrical, and electronics engineering, and some applied aspects of lasers, physical optics, and quantum electronics. It is published by the IEEE Photonics Society and was established in 1965. The editor-in-chief is Prof. Hon Ki Tsang (The Chinese University of Hong Kong). According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.852.\n\nThe journal is abstracted and indexed in:\n\n"}
{"id": "13246418", "url": "https://en.wikipedia.org/wiki?curid=13246418", "title": "Industrial fan", "text": "Industrial fan\n\nIndustrial fans and blowers are machines whose primary function is to provide and accommodate a large flow of air or gas to various parts of a building or other structures. This is achieved by rotating a number of blades, connected to a hub and shaft, and driven by a motor or turbine. The flow rates of these mechanical fans range from approximately to per minute. A blower is another name for a fan that operates where the resistance to the flow is primarily on the downstream side of the fan.\n\nThere are many uses for the continuous flow of air or gas that industrial fans generate, including combustion, ventilation, aeration, particulate transport, exhaust, cooling, air-cleaning, and drying, to name a few. The industries served include electrical power production, pollution control, metal manufacturing and processing, cement production, mining, petrochemical, food processing, cryogenics, and clean rooms.\n\nMost industrial fans may be categorized into one of two general types: centrifugal fans and axial fans.\n\nThe centrifugal design uses the centrifugal force generated by a rotating disk, with blades mounted at right angles to the disk, to impart movement to the air or gas and increase its pressure. The assembly of the hub, disk and blades is known as the fan wheel, and often includes other components with aerodynamic or structural functions. The centrifugal fan wheel is typically contained within a scroll-shaped fan housing, resembling the shell of the nautilus sea creature with a central hole. The air or gas inside the spinning fan is thrown off the outside of the wheel, to an outlet at the housing's largest diameter. This simultaneously draws more air or gas into the wheel through the central hole . Inlet and outlet ducting are often attached to the fan's housing, to supply and/or exhaust the air or gas to the industry's requirements.\n\nThere are many varieties of centrifugal fans, which may have fan wheels that range from less than 3 cm to over 16 feet (5 m) in diameter.\n\nThe axial design uses axial forces to achieve the movement of the air or gas, spinning a central hub with blades extending radially from its outer diameter. The fluid is moved parallel to the fan wheel's shaft, or axis of rotation. The axial fan wheel is often contained within a short section of cylindrical ductwork, to which inlet and outlet ducting can be connected.\n\nAxial fan types have fan wheels with diameters that usually range from less than a foot (0.3 meters) to over 30 feet (9 m), although axial cooling tower fan wheels may exceed 82 feet (25 m) in diameter.\n\nIn general, axial fans are used where the principal requirement is for a large volume of flow, and the centrifugal design where both flow and higher pressures are required.\n\nThere are several paths to determining a fan design for an application.\n\nFor industries where the application requirements do not vary greatly and applicable fan designs have diameters of around 4 feet (1.2 meters) or less, a standard or pre-engineered design might be selected.\n\nWhen the application involves more complex specifications or a larger fan, then a design based on an existing model configuration will often satisfy the requirements. Many model configurations already cover the range of current industry processes. An appropriate model from the fan company's catalogue is selected, and the company's engineers apply design rules to calculate the dimensions and select options and material for the desired performance, strength and operating environment.\n\nSome applications require a dedicated, custom configuration for a fan design to satisfy all specifications.\n\nAll industrial fan designs must be accurately engineered to meet performance specifications while maintaining structural integrity. For each application, there are specific flow and pressure requirements. Depending on the application, the fan may be subject to high rotating speeds, an operating environment with corrosive chemicals or abrasive air streams, and extreme temperatures. Larger fans and higher speeds produce greater forces on the rotating structures; for safety and reliability, the design must eliminate excessive stresses and excitable resonant frequencies. Computer modeling programs for computational fluid dynamics (CFD) and finite element analysis (FEA) are often employed in the design process, in addition to laboratory scale model testing. Even after the fan is built the verification might continue, using fan performance testing for flow and pressure, strain gage testing for stresses and tests to record the fan's resonant frequencies.\n\nFan types and their subtypes are industry standard, recognized by all major fan producers. \n\nAny of these fan subtypes can be built with long-lasting erosion-resistant liners.\n\nAirfoil (Air foil) – Used for a wide range of applications in many industries, fans with hollow, airfoil-profiled blades are designed for use in airstreams where high efficiency and quiet operation are required. They are used extensively for continuous service at ambient and elevated temperatures in forced and induced draft applications in the metals, chemical, power generation, paper, rock products, glass, resource recovery, incineration and other industries throughout the world.\n\nBackward curve – These fans have efficiencies nearly as high as the airfoil design. An advantage is that their single-thickness, curved plate blades prevent the possibility of dust particle buildup inside the blade, as may occur with perforated airfoil blades. The robust design allows high tip-speed operation, and therefore this fan is often used in high-pressure applications.\n\nBackward inclined – These fans have simple flat blades, backwardly inclined to match the velocity pattern of the air passing through the fan wheel for high-efficiency operation. These fans are typically used in high-volume, relatively low-pressure, clean air applications.\n\nRadial blade – The flat blades of this type are arranged in a radial pattern. These rugged fans offer high pressure capability with average efficiency. They are often fitted with erosion-resistant liners to extend rotor life. The housing design is compact to minimize the floor space requirement.\n\nRadial tipped – These fans have wheels that are backward curved, but in a way slightly different from backward curved fans. Backward curved fans have wheels whose blades curve outward, while radial-tip fans' blades are curved inward and radial at their tips (hence the name \"radial tip\"), while still in a backwardly-curved configuration. Their curvature can also be thought of as radial at the tips but gradually sloping toward the direction of rotation. This rugged design is used in high-volume flow rate applications when the pressure requirement is rather high and erosion resistance is necessary. It offers medium efficiencies. A common application is the dirty side of a baghouse or precipitator. The design is more compact than airfoil, backward curved or backward inclined fans.\n\nPaddle-wheel – This is an open impeller design without shrouds. Although the efficiency is not high, this fan is well suited for applications with extremely high dust loading. It can be offered with field-replaceable blade liners from ceramic tiles or tungsten carbide. This fan may also be used in high-temperature applications.\n\nForward-curve – This \"squirrel cage\" impeller generates the highest volume flow rate (for a given tip speed) of all the centrifugal fans. Therefore, it often has the advantage of offering the smallest physical package available for a given application. This type of fan is commonly used in high-temperature furnaces. However, these fans can only be used for conveying air with low dust loading because they are the most sensitive to particle build-up, but also due to the large number of blades that forward-curve wheels require.\n\nIndustrial exhausters – This is a relatively inexpensive, medium-duty, steeply inclined flat-bladed fan for exhausting gases, conveying chips, etc.\n\nPre-engineered fans (PE) – A series of fans of varying blade shapes that are usually available in only standard sizes. Because they are pre-engineered these fans may be available with relatively short delivery times. Often, pre-engineered rotors with various blade shapes may be installed into a common housing. These are often available in a wide range of volume and pressure requirements to meet the needs of many applications.\n\nPressure blowers – These are high-pressure, low-volume blowers used in combustion air applications in furnaces or to provide “blow-off” air for clearing and/or drying applications.\n\nSurgeless blowers – These high-pressure, low-volume blowers have a reduced tendency for “surging” (periodic variation of flow rate) even at severely reduced fan speeds. This allows extreme turndown (low-flow) without significant pulsation.\n\nMechanical vapor recovery blowers -These specially designed centrifugal fans are designed to increase temperature and pressure of saturated steam in a closed-loop system.\n\nAcid gas blowers - These very heavy construction blowers are suitable for inlet pressures from full vacuum to 100 psig. Materials are selected for corrosion resistance to the gases and particulate handled.\n\nSpecialty process gas blowers - These blowers are for high pressure petrochemical processes.\n\nHigh-temperature axial fans – These are high-volume fans designed to operate against low flow resistance in industrial convection furnaces. They may be of either single-direction or bi-directional designs. Extremely rugged, they are most often used in high-temperature furnace (up to 1800 degF) application.\n\nTube axial fans – These are axial fan units with fan wheels located in cylindrical tubes, without inlet or outlet dampers.\n\nVaneaxial fans – These axial flow fans have a higher pressure capability due to the presence of static vanes.\n\nVariable pitch axial fans – The blades on these axial fans are manually adjustable to permit the blade angle to be changed. This allows operation over a much wider range of volume/pressure relationships. The blades are adjusted periodically to optimize efficiency by matching the blade pitch to the varying conditions for the application. These fans are often used in mining applications.\n\nVariable pitch on-the-fly axial fans – These are similar to “Variable Pitch Axial Fans” except they include an internal mechanism that allows the blade pitch to be adjusted while the fan rotor is in motion. These versatile fans offer high-efficiency operation at many different points of operation. This instantaneous blade adjustment capability is an advantage that is possible with axial fans only.\n\nCooling fans - (also referred to as \"cooling tower fans\") - These are axial fans, typically with large diameters, for low pressures and large volumes of airflow. Applications are in wet mechanical cooling towers, air-cooled steam condensers, air-cooled heat exchangers, radiators, or similar air-cooled applications.\n\nMixed-flow fans - The gas flow patterns these fans produce resemble a combination of axial and centrifugal patterns, although the fan wheels often appear similar to centrifugal wheels. There are various types of mixed-flow fans, including gas-tight high-pressure fans and blowers.\n\nThere are several means of controlling the flow rate of a fan, e.g., temporarily reducing the air or gas flow rate; these can be applied to both centrifugal and axial fans.\n\nSpeed Variation - All of the fan types described above can be used in conjunction with a variable speed driver. This might be an adjustable frequency AC controller, a DC motor and drive, a steam turbine driver, or a hydraulic variable speed drive unit (\"fluid drive\"). Flow control by means of variable speed is typically smoother and more efficient than by means of damper control. Significant power savings (with reduced cost of operation) are possible if variable speed fan drives are used for applications that require reduced flow operation for a significant portion of the system operating life.\n\nIndustrial Dampers - These devices also allow fan volumetric flow control during operation, by means of panels so as to direct gas flow or restrict the inlet or outlet areas.\n\nThere is a variety of dampers available:\n\nLouvered Inlet Box Dampers\nRadial Inlet Dampers\nVariable Inlet Vane (VIV) Dampers\nVortex Dampers\nDischarge Dampers<BR>\n\n\n"}
{"id": "39773873", "url": "https://en.wikipedia.org/wiki?curid=39773873", "title": "Industry 4.0", "text": "Industry 4.0\n\nIndustry 4.0 is a name given to the trend of automation and data exchange in manufacturing technologies. It includes cyber-physical systems, the Internet of things, cloud computing and cognitive computing. Industry 4.0 is commonly referred to as the fourth industrial revolution.\n\nIndustry 4.0 fosters what has been called a \"smart factory\". Within modular structured smart factories, cyber-physical systems monitor physical processes, create a virtual copy of the physical world and make decentralized decisions. Over the Internet of Things, cyber-physical systems communicate and cooperate with each other and with humans in real-time both internally and across organizational services offered and used by participants of the value chain.\n\nThe term \"Industry 4.0\", shortened to I4.0 or simply I4, originates from a project in the high-tech strategy of the German government, which promotes the computerization of manufacturing.\n\nThe term \"Industry 4.0\" was revived in 2011 at the Hannover Fair. In October 2012 the Working Group on Industry 4.0 presented a set of Industry 4.0 implementation recommendations to the German federal government. The Industry 4.0 workgroup members are recognized as the founding fathers and driving force behind Industry 4.0.\n\nOn 8 April 2013 at the Hannover Fair, the final report of the Working Group Industry 4.0 was presented.. This working group was headed by Siegfried Dais (Robert Bosch GmbH) and Henning Kagermann (German Academy of Science and Engineering).\n\nAs Industry 4.0 principles have been applied by companies they have sometimes been re-branded, for example the aerospace parts manufacturer Meggitt PLC has branded its own Industry 4.0 research project M4. \n\nThere are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios.\n\nCurrent usage of the term has been criticised as essentially meaningless, in particular on the grounds that technological innovation is continuous and the concept of a \"revolution\" in technology innovation is based on a lack of knowledge of the details.\n\nThe characteristics given for the German government's Industry 4.0 strategy are: the strong customization of products under the conditions of highly flexible (mass-) production. The required automation technology is improved by the introduction of methods of self-optimization, self-configuration, self-diagnosis, cognition and intelligent support of workers in their increasingly complex work. The largest project in Industry 4.0 as of July 2013 is the BMBF leading-edge cluster \"Intelligent Technical Systems Ostwestfalen-Lippe (it's OWL)\". Another major project is the BMBF project RES-COM, as well as the Cluster of Excellence \"Integrative Production Technology for High-Wage Countries\". In 2015, the European Commission started the international Horizon 2020 research project CREMA (Providing Cloud-based Rapid Elastic Manufacturing based on the XaaS and Cloud model) as a major initiative to foster the Industry 4.0 topic.\n\nIn June 2013, consultancy firm McKinsey released an interview featuring an expert discussion between executives at Robert Bosch - Siegfried Dais (Partner of the Robert Bosch Industrietreuhand KG) and Heinz Derenbach (CEO of Bosch Software Innovations GmbH) - and McKinsey experts. This interview addressed the prevalence of the Internet of Things in manufacturing and the consequent technology-driven changes which promise to trigger a new industrial revolution. At Bosch, and generally in Germany, this phenomenon is referred to as Industry 4.0. The basic principle of Industry 4.0 is that by connecting machines, work pieces and systems, businesses are creating intelligent networks along the entire value chain that can control each other autonomously.\n\nSome examples for Industry 4.0 are machines which can predict failures and trigger maintenance processes autonomously or self-organized logistics which react to unexpected changes in production.\n\nAccording to Dais, \"it is highly likely that the world of production will become more and more networked until everything is interlinked with everything else\". While this sounds like a fair assumption and the driving force behind the Internet of Things, it also means that the complexity of production and supplier networks will grow enormously. Networks and processes have so far been limited to one factory. But in an Industry 4.0 scenario, these boundaries of individual factories will most likely no longer exist. Instead, they will be lifted in order to interconnect multiple factories or even geographical regions.\n\nThere are differences between a typical traditional factory and an Industry 4.0 factory. In the current industry environment, providing high-end quality service or product with the least cost is the key to success and industrial factories are trying to achieve as much performance as possible to increase their profit as well as their reputation. In this way, various data sources are available to provide worthwhile information about different aspects of the factory. In this stage, the utilization of data for understanding current operating conditions and detecting faults and failures is an important topic to research. e.g. in production, there are various commercial tools available to provide overall equipment effectiveness (OEE) information to factory management in order to highlight the root causes of problems and possible faults in the system. In contrast, in an Industry 4.0 factory, in addition to condition monitoring and fault diagnosis, components and systems are able to gain self-awareness and self-predictiveness, which will provide management with more insight on the status of the factory. Furthermore, peer-to-peer comparison and fusion of health information from various components provides a precise health prediction in component and system levels and force factory management to trigger required maintenance at the best possible time to reach just-in-time maintenance and gain near-zero downtime.\n\nDuring EDP Open Innovation conducted in Oct 2018 at Lisbon, Portugal, Industry 4.0 conceptualization was extended by Sensfix B.V. a Dutch company with introduction of M2S terminology. It essentially is characterizing upcoming service industry to cater to millions of machines, managed by the machines themselves, fortunately using Artificial intelligence developed by humans!. \n\nChallenges in implementation of Industry 4.0:\n\nModern information and communication technologies like cyber-physical system, big data analytics and cloud computing, will help early detection of defects and production failures, thus enabling their prevention and increasing productivity, quality, and agility benefits that have significant competitive value.\n\nBig data analytics consists of 6Cs in the integrated Industry 4.0 and cyber physical systems environment.\nThe 6C system comprises:\nIn this scenario and in order to provide useful insight to the factory management, data has to be processed with advanced tools (analytics and algorithms) to generate meaningful information. Considering the presence of visible and invisible issues in an industrial factory, the information generation algorithm has to be capable of detecting and addressing invisible issues such as machine degradation, component wear, etc. in the factory floor.\n\nProponents of the term claim Industry 4.0 will affect many areas, most notably:\n\nThe aerospace industry has sometimes been characterized as \"too low volume for extensive automation\" however Industry 4.0 principles have been investigated by several aerospace companies, technologies have been developed to improve productivity where the upfront cost of automation cannot be justified, one example of this is the aerospace parts manufacturer Meggitt PLC's project, M4. \nThe discussion of how the shift to Industry 4.0, especially digitalization, will affect the labour market is being discussed in Germany under the topic of Work 4.0.\n\n\n"}
{"id": "14934", "url": "https://en.wikipedia.org/wiki?curid=14934", "title": "International Organization for Standardization", "text": "International Organization for Standardization\n\nThe International Organization for Standardization (ISO ) is an international standard-setting body composed of representatives from various national standards organizations.\n\nFounded on 23 February 1947, the organization promotes worldwide proprietary, industrial and commercial standards. It is headquartered in Geneva, Switzerland, and works in 162 countries.\n\nIt was one of the first organizations granted general consultative status with the United Nations Economic and Social Council.\n\nThe International Organization for Standardization is an independent, non-governmental organization, the members of which are the standards organizations of the 168 member countries. It is the world's largest developer of voluntary international standards and facilitates world trade by providing common standards between nations. Over twenty thousand standards have been set covering everything from manufactured products and technology to food safety, agriculture and healthcare.\n\nUse of the standards aids in the creation of products and services that are safe, reliable and of good quality. The standards help businesses increase productivity while minimizing errors and waste. By enabling products from different markets to be directly compared, they facilitate companies in entering new markets and assist in the development of global trade on a fair basis. The standards also serve to safeguard consumers and the end-users of products and services, ensuring that certified products conform to the minimum standards set internationally.\n\nThe three official languages of the ISO are English, French, and Russian.\n\nThe name of the organization in French is ', and in Russian, ('). \"ISO\" is not an acronym. The organization adopted \"ISO\" as its abbreviated name in reference to the Greek word \"\" (, meaning \"equal\"), as its name in the three official languages would have different acronyms. During the founding meetings of the new organization, the Greek word explanation was not invoked, so this meaning may have been made public later.\n\nISO gives this explanation of the name: \"Because 'International Organization for Standardization' would have different acronyms in different languages (IOS in English, OIN in French), our founders decided to give it the short form \"ISO\". \"ISO\" is derived from the Greek \"\", meaning equal. Whatever the country, whatever the language, the short form of our name is always \"ISO\".\"\n\nBoth the name \"ISO\" and the ISO logo are registered trademarks, and their use is restricted.\n\nThe organization today known as ISO began in 1926 as the International Federation of the National Standardizing Associations (ISA). It was suspended in 1942 during World War II, but after the war ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization; the new organization officially began operations in February 1947.\n\nISO is a voluntary organization whose members are recognized authorities on standards, each one representing one country. Members meet annually at a General Assembly to discuss ISO's strategic objectives. The organization is coordinated by a Central Secretariat based in Geneva.\n\nA Council with a rotating membership of 20 member bodies provides guidance and governance, including setting the Central Secretariat's annual budget.\n\nThe Technical Management Board is responsible for over 250 technical committees, who develop the ISO standards.\n\nISO has formed two joint committees with the International Electrotechnical Commission (IEC) to develop standards and terminology in the areas of electrical and electronic related technologies.\n\nISO/IEC Joint Technical Committee 1 (JTC 1) was created in 1987 to \"[d]evelop, maintain, promote and facilitate IT standards\", where IT refers to information technology.\n\nISO/IEC Joint Technical Committee 2 (JTC 2) was created in 2009 for the purpose of \"[s]tandardization in the field of energy efficiency and renewable energy sources\".\n\nISO has 162 national members.\n\nISO has three membership categories:\n\nParticipating members are called \"P\" members, as opposed to observing members, who are called \"O\" members.\n\nISO is funded by a combination of: \n\nISO's main products are international standards. ISO also publishes technical reports, technical specifications, publicly available specifications, technical corrigenda, and guides.\n\nInternational standards\n\nTechnical reports\n\nTechnical and publicly available specifications\n\nTechnical corrigenda\n\nISO guides\nThese are meta-standards covering \"matters related to international standardization\". They are named using the format \"ISO[/IEC] Guide N:yyyy: Title\".For example:\n\nISO documents are copyrighted and ISO charges for most copies. It does not, however, charge for most draft copies of documents in electronic format. Although they are useful, care must be taken using these drafts as there is the possibility of substantial change before they become finalized as standards. Some standards by ISO and its official U.S. representative (and, via the U.S. National Committee, the International Electrotechnical Commission) are made freely available.\n\nA standard published by ISO/IEC is the last stage of a long process that commonly starts with the proposal of new work within a committee. Here are some abbreviations used for marking a standard with its status:\n\nAbbreviations used for amendments:\n\nOther abbreviations:\nInternational Standards are developed by ISO technical committees (TC) and subcommittees (SC) by a process with six steps:\n\nThe TC/SC may set up working groups (WG) of experts for the preparation of a working drafts. Subcommittees may have several working groups, which can have several Sub Groups (SG).\n\nIt is possible to omit certain stages, if there is a document with a certain degree of maturity at the start of a standardization project, for example a standard developed by another organization. ISO/IEC directives allow also the so-called \"Fast-track procedure\". In this procedure a document is submitted directly for approval as a draft International Standard (DIS) to the ISO member bodies or as a final draft International Standard (FDIS) if the document was developed by an international standardizing body recognized by the ISO Council.\n\nThe first step—a proposal of work (New Proposal) is approved at the relevant subcommittee or technical committee (e.g., SC29 and JTC1 respectively in the case of Moving Picture Experts Group – ISO/IEC JTC1/SC29/WG11). A working group (WG) of experts is set up by the TC/SC for the preparation of a working draft. When the scope of a new work is sufficiently clarified, some of the working groups (e.g., MPEG) usually make open request for proposals—known as a \"call for proposals\". The first document that is produced for example for audio and video coding standards is called a verification model (VM) (previously also called a \"simulation and test model\"). When a sufficient confidence in the stability of the standard under development is reached, a working draft (WD) is produced. This is in the form of a standard but is kept internal to working group for revision. When a working draft is sufficiently solid and the working group is satisfied that it has developed the best technical solution to the problem being addressed, it becomes committee draft (CD). If it is required, it is then sent to the P-members of the TC/SC (national bodies) for ballot.\n\nThe CD becomes final committee draft (FCD) if the number of positive votes is above the quorum. Successive committee drafts may be considered until consensus is reached on the technical content. When it is reached, the text is finalized for submission as a draft International Standard (DIS). The text is then submitted to national bodies for voting and comment within a period of five months. It is approved for submission as a final draft International Standard (FDIS) if a two-thirds majority of the P-members of the TC/SC are in favour and not more than one-quarter of the total number of votes cast are negative. ISO will then hold a ballot with National Bodies where no technical changes are allowed (yes/no ballot), within a period of two months. It is approved as an International Standard (IS) if a two-thirds majority of the P-members of the TC/SC is in favour and not more than one-quarter of the total number of votes cast are negative. After approval, only minor editorial changes are introduced into the final text. The final text is sent to the ISO Central Secretariat, which publishes it as the International Standard.\n\nThe fact that many of the ISO-created standards are ubiquitous has led, on occasion, to common use of \"ISO\" to describe the actual product that conforms to a standard. Some examples of this are:\n\nWith the exception of a small number of isolated standards, ISO standards are normally not available free of charge, but for a purchase fee, which has been seen by some as too expensive for small open source projects.\n\nThe ISO/IEC JTC1 fast-track procedures (\"Fast-track\" as used by OOXML and \"PAS\" as used by OpenDocument) have garnered criticism in relation to the standardization of Office Open XML (ISO/IEC 29500). Martin Bryan, outgoing Convenor of ISO/IEC JTC1/SC34 WG1, is quoted as saying:\nI would recommend my successor that it is perhaps time to pass WG1’s outstanding standards over to OASIS, where they can get approval in less than a year and then do a PAS submission to ISO, which will get a lot more attention and be approved much faster than standards currently can be within WG1.\nThe disparity of rules for PAS, Fast-Track and ISO committee generated standards is fast making ISO a laughing stock in IT circles. The days of open standards development are fast disappearing. Instead we are getting 'standardization by corporation'.\nComputer security entrepreneur and Ubuntu investor, Mark Shuttleworth, commented on the Standardization of Office Open XML process by saying \"I think it de-values the confidence people have in the standards setting process,\" and Shuttleworth alleged that ISO did not carry out its responsibility. He also noted that Microsoft had intensely lobbied many countries that traditionally had not participated in ISO and stacked technical committees with Microsoft employees, solution providers and resellers sympathetic to Office Open XML.\nWhen you have a process built on trust and when that trust is abused, ISO should halt the process... ISO is an engineering old boys club and these things are boring so you have to have a lot of passion … then suddenly you have an investment of a lot of money and lobbying and you get artificial results. The process is not set up to deal with intensive corporate lobbying and so you end up with something being a standard that is not clear.\n\n\n\n\n\n"}
{"id": "5380820", "url": "https://en.wikipedia.org/wiki?curid=5380820", "title": "Investment-specific technological progress", "text": "Investment-specific technological progress\n\nInvestment-specific technological progress refers to progress that requires investment in new equipment and structures embodying the latest technology in order to realize its benefits.\n\nTo model how something is produced, think of a box that in one end takes in inputs such as labor (employees) and capital (equipment, buildings, etc.) and in another end spits out the final good. With this picture in mind now one can ask, how does technological progress affect production? One way of thinking is that technological progress affects \"specific\" inputs (arrows going in) such as equipment and buildings. To realize the benefits of such technological change for production, these inputs must be purchased. So for example, the advent of the microchip (an important technological improvement in computers) will affect the production of Ford cars only if Ford Motor Co.'s assembly plants (the red box) invest in computers with microchips (instead of computers with punched cards) and use them (they are one of the arrows going in the box) in the production of Mustangs (the arrow coming out). As the name suggests, this is \"investment-specific\" technological progress---it requires investing in new machines or buildings which contain or \"embody\" the latest technology. Notice that the term \"investment\" can be general: not only must a firm buy the new technology to reap its benefits, but it also must invest in training its workers and managers to be able to use this new technology (Greenwood & Jovanovic 2001) .\n\nIdentifying \"investment-specific\" technological progress is important, because knowing what type of technological progress is operating in an economy will determine how someone (should) want his or her tax dollars to be spent and how he or she may want to invest his or her savings (Gort et al. 1999). If \"investment-specific\" technological change is the main source of progress, then one would want his or her dollars spent on helping firms buy new equipment and renovate their plants, because these investments will improve production and hence what you consume. Furthermore, one may want to help pay for current employee training in using new technologies (to keep them up to date) or subsidize the education of new employees (who will enter the job market knowing how to use the new technology). So, the type of technological progress will also matter for unemployment and education issues. Finally, if technological progress is \"investment-specific\" you may want to direct your money towards the research and development (R & D) of new technologies (like quantum computers or alternative energy sources) (Krusell 1998).\n\nMore generally, why is any type of technological progress important? Technological change has made our lives easier. Because of technological progress, people can work less, make more money and enjoy more leisure time (Greenwood & Vandenbroucke 2006). Women have been able to break away from the traditional \"housewife\" role, join the labor-force in greater numbers (Greenwood et al. 2005) and become less economically dependent on men (Greenwood & Guner 2009). Finally, technological progress has been shown to affect the fall in child labor starting around 1900 (Greenwood & Seshadri 2005). Figure 1 illustrates this last point: in 1900 child labor's share of the paid labor force began to fall.\n\nAn example of investment-specific technological progress is the microwave oven. The idea of the microwave came to be by accident: in 1946 an engineer noticed that a candy bar in his pocket had melted while working on something completely unrelated to cooking (Gallawa 2005). The development of this good, from melting the candy bar to the home appliance we know today, took time and the investment of resources to make a microwave small and cheap. The first microwave oven cost between 2000 and 3000 dollars and was housed in refrigerator-sized cabinets (Gallawa 2005)! Today, almost any college student can enjoy a 3-minute microwaveable meal in the smallest dorm room. But a microwave's uses do not stop at the dorm room. Many industries have found microwave heating advantageous: it has been used to dry cork, ceramics, paper, leather, and so on (Gallawa 2005). However, for either college students or firms to reap the benefits of quick warming, they must first \"invest\" in a microwave oven (that \"embodies\" the technological advance). To realize the benefits of investment-specific technological progress you must first invest in a technology that embodies it.\n\nWhile measuring technological progress is not easy, economists have found indirect ways of estimating it. If \"'investment-specific'\" technological progress makes producing goods easier, then the price of the goods affected (relative to the price of other goods) should decrease. In particular, \"investment-specific\" technological advance has affected the prices of two inputs into the production process: equipment and structures. Think of equipment as machines (like computers) and structures as buildings. If there is technological progress in the production (or creation) of these goods, then one would expect the price of them to fall or the value of them to rise relative to older versions of the same good.\n\nFigure 2 (the pink line) shows how the price of new producer durables (such as equipment) in the US relative to the price of new consumer nondurables (like clothing) has consistently declined over the past fifty years (Gort et al. 1999). To calculate the relative price of producer durables divide the price that firms pay (for the durable inputs of production) by the price that a regular consumer pays (for things like jeans). We use relative prices so we can say how many units of equipment can be bought instead (or in terms) of buying one unit of consumer goods. Figure 3 (the pink line) says that over time, firms have been able to buy more and more units of equipment instead of one unit of consumption, especially when we take into account that the quality of equipment being acquired has increased (a computer today is much faster than a computer five years ago and we should take that into account when comparing their prices). When changes in quality are not taken into account (which is wrong) it looks like the price of equipment has not decreased as much (see the black line in Figure 2).\n\nMeasuring the price of structures is more complicated than measuring the price of equipment, but economists have again been able to get an idea of how much progress there has been in structures (such as buildings). One approach is that if newer buildings were constructed or designed using newer technologies then they should be worth more than older buildings (because they embody the new technology (Gort et al. 1999). In particular, they should rent for more. As Figure 3 shows, this is true. Renting a square foot in a new building is much more expensive than renting a square foot in a building forty years old. So it must be the case that you are paying for a nicer, more functional and maybe even safer building.\nFigures 2 and 3 suggest that investment-specific technological change is operating in the US. The annual rate of technological progress in equipment and structures has been estimated to be about 3.2% and 1%, respectively (Gort et al. 1999) (Greenwood et al. 1997).\n\nIn the second section it was mentioned that \"investment-specific\" technological change is important since it will affect production (both in quality and size). An important question then is, just how much \"bang for your buck\" do you get with \"investment-specific\" technological change? The answer is quite astounding; economists have found that 37% of growth in US output (production) is due to technological progress in equipment and 15% is due to technological progress in structures (Gort et al. 1999) (Greenwood et al. 1997). All in all, more than half (37% + 15% = 52%) of the growth of the US economy is due to \"investment-specific\" technological change (Gort et al. 1999) (Greenwood et al. 1997).\n\n"}
{"id": "41127149", "url": "https://en.wikipedia.org/wiki?curid=41127149", "title": "Jameco Electronics", "text": "Jameco Electronics\n\nJameco Electronics is an American electronic component and tool distributor. It was founded in 1974 by Dennis Farrey in Silicon Valley, San Francisco, California and is currently headquartered in Belmont, California, where it also operates a brick and mortar store.\n\nJameco is family owned, with James Farrey, the son of founder and first CEO David Farrey becoming the company's second CEO in 2010. He had worked within the company since 1983, and in 2009 was named COO.\n\nA smaller operation relative to its competitors in the industry, the company nonetheless enjoys significant popularity in the electronic hobbyist community and in 2011 they launched a \"Club Jameco\" program in which hobbyists can create kits, and, if they are popular, sell them to other hobbyists. If a kit is chosen for a production run by the company, the creator can earn a small royalty on its sales. \n\nIn 2015, the company conducted a study among its customers, dubbed 'The Great American Electronics Hobbyist Census'. The study reported on the growth of the electronic hobbyist community, attributed by participants to the growing affordability and availability of electronic components online. The study also found that half of those interviewed had no professional experience with the industry. It also observed a proportion of only 2% female respondents identifying as hobbyists, even though 19% of engineering students are female.\n\n"}
{"id": "41788976", "url": "https://en.wikipedia.org/wiki?curid=41788976", "title": "Korangi Creek Industrial Park", "text": "Korangi Creek Industrial Park\n\nKorangi Creek Industrial Park is located in Karachi, Sindh, Pakistan. Korangi Creek Industrial Park is built over an area of 250 acres, this industrial zone is located in Korangi Creek Cantonment.\n\n"}
{"id": "40147167", "url": "https://en.wikipedia.org/wiki?curid=40147167", "title": "Krisztina Holly", "text": "Krisztina Holly\n\nKrisztina Holly, known by her colleagues as 'Z' is a Hungarian American innovator, entrepreneur, and adventurer. She is the host of The Art of Manufacturing podcast and the Founder and Chief Instigator of MAKE IT IN LA, which was launched from her term as Entrepreneur-in-Residence for LA Mayor Garcetti. She is best known as the creator of the first TEDx, the founding executive director of the Deshpande Center for Technological Innovation at the Massachusetts Institute of Technology and the Vice Provost for Innovation and founding executive director of the Stevens Center for Innovation at the University of Southern California. She was founder or key team member of various technology startups including Stylus Innovation, Direct Hit Technologies, and Jeeves Solutions and was a prominent mountain bike advocate in New England for a decade. She is a founding donor and board member of the River LA, serves on the board of TTI/Vanguard, and has been an advisor to nearly two dozen other companies and organizations, including the Obama Administration as an inaugural member of the National Advisory Council on Innovation and Entrepreneurship, and a global agenda council member of the World Economic Forum advising in the areas of entrepreneurship and manufacturing. She is married and resides in Los Angeles.\n\nHolly was born to Hungarian parents and has written about being greatly influenced by their stories of escaping their home country in 1956 and being refugees in America. From a young age she thought she would be an entrepreneur.\nShe attended Massachusetts Institute of Technology and received bachelor's and master's degrees in mechanical engineering. She, Michael\n\nHolly created TEDxUSC, the first ever TEDx event in 2009, which so far inspired over 15,000 similar events globally. In her role as curator and host for TEDxUSC over four years, she discovered and coached more than 60 presenters whose videos have garnered 12 million views online. From 2006-2012 she was the vice provost for innovation at USC and founding executive director the USC Stevens Center for Innovation. From 2002-2006 she was the founding executive director of the MIT Deshpande Center. While at MIT and USC, she helped expand the innovation ecosystems in Boston and LA and the centers helped spin out 39 startups based on university research.\n\nEarly on Holly was co-founder of computer telephony pioneer, Stylus Innovation (acquired by Artisoft) and subsequently joined other tech and media startups including Direct Hit Technologies (acquired by Ask Jeeves) and Jeeves Solutions. Between startups Holly spent nearly three years in documentary production.\n\nHolly started her career as an undergraduate researcher at the MIT Media Lab, working on the team that created the world's first full-color computer generated reflection hologram. Later, she developed a robotic weld-seam-tracking program for the space shuttle main engine and built a head-eye robot for the MIT AI Lab.\n\nHolly was named Champion of Free Enterprise by Forbes in 2009. She has been a contributor to Forbes, The Economist, Businessweek, Huffington Post, CNN.com, NASA ASK, Strategy+Business, World Economic Forum, Science Progress, Innovation Management, and Mountain Bike Magazine. She has also served as a board member or advisor for numerous organizations and is a member of YPO. In 2013, Krisztina served as a mentor for Unreasonable at Sea, a technology business accelerator for social entrepreneurs seeking to scale their ventures in international markets, founded by Unreasonable Group, Semester at Sea, and Stanford’s Hasso Plattner Institute of Design.\n\nHolly is a licensed skydiver and SCUBA instructor, and an avid backcountry/telemark skier, adventure traveler, mountain biker, shark diver, and authentic food aficionado.\n\n"}
{"id": "9644994", "url": "https://en.wikipedia.org/wiki?curid=9644994", "title": "Landfill liner", "text": "Landfill liner\n\nA landfill liner, or composite liner, is intended to be a low permeable barrier, which is laid down under engineered landfill sites. Until it deteriorates, the liner retards migration of leachate, and its toxic constituents, into underlying aquifers or nearby rivers, causing spoliation of the local water.\n\nModern landfills generally require a layer of compacted clay with a minimum required thickness and a maximum allowable hydraulic conductivity, overlaid by a high-density polyethylene geomembrane.\n\nThe United States Environmental Protection Agency has stated that the barriers \"will ultimately fail,\" while the site remains a threat for \"thousands of years,\" suggesting that modern landfill designs delay but do not prevent ground and surface water pollution.\n\nChipped or waste tires are used to support and insulate the liner.\n\nThere are certain levels of harmfulness in which the different types of trash have; therefore, there are different types of liner systems which are required for these different types of disposal sites. The first type is single liner-systems. These systems usually are put within landfills which mostly hold construction rubble. These landfills are not meant to hold the disposal of harmful liquid wastes such as paint, tar, or any other type of liquid garbage that can easily seep through a single liner system. The second type is double-liner systems. These systems are usually found in municipal solid waste landfills as well all hazardous waste landfills. The first part is constructed to collect the leachate while the second layer is engineered to be a leak-detection system to ensure that no contaminates leak into the ground and contaminate everything.\n\nComposite liners are required to be used in municipal solid waste systems for landfills and use a double liner system which is composed of a leachate system which is a liquid that collects solids from the substance this is passed through it. The leachate system is surrounded in a by a type of solid drainage layer such as gravel which is enclosed by a geomembrane and compressed clay, also known as a geosynthetic clay liner. This geosynthetic clay liner is usually made of sodium bentonite which is compacted in between two thick pieces of geotextile. The next material surrounding the composite liner would be a leak detection system composed of another material like gravel with an additional geomembrane or complex liner. The geomembranes within the composite liner consist of a high-density polyethylene which provide an effective minimization for flow and deliver and helpful barrier which is used on inorganic contaminants. It can be used as a substitute for sand or gravel and also has a very high transmissivity and low storage. The lower surface helps provide an effective leak test once correctly installed. It is also a low permeable vapor and liquid barrier. The geosynthetic clay liners are manufactured by factories and the purpose for it being made of sodium bentonite is that they regulate the movement of liquids in gases within the waste. The geocomposites which are a combination of the geomembranes and geosynthetic liner material also include a layer of bentonite between the middle of the layers of geotextile; however, airspace is allowed to be implemented. It is then topped off with a final cover.\n\nThe main role a composite liner has for a municipal solid waste system for landfills is that is reduces the amount of leakage through small seep holes that sometimes form in the geomembrane part of the composite liner. The protection layer part serves as a preventer from these holes from forming inside the geomembrane which would allow the waste to leak through the entire liner(awkward sentence consider rewording). It also takes away the pressure and stress which can cause cracking and holes from forming in the membrane as well. An effective liner in a landfill system should be able to control water in terms of movement and protection on the environment. It should be able to regulate the flow away from the waste area and withhold the waste contents as it enters the actual landfill. Due to the effectiveness on how landfills are placed on top of slopes in order for the water to stream downhill and in an emergency, into the actual landfill. Water moves through the landfill and downward through the composite liner. The main purpose for all of this is so that the movement is lateral which lessens the probability for slope catastrophe and the waste leaking down and freely contaminating whatever is in its path. The final cover functions as a way to keep the water out of the contaminate and to control the runoff from entering the system. This helps prevent plants and animals from being harmed by the waste contaminated water,leachate. Using gravity and pumps the leachate is able to be pushed to a sump where it is removed by a pump. When developing composite liners it is extremely important to take in risk factors such as earthquakes and other slope failure problems that could occur. \nComposite liners are used in municipal solid waste (MSW) landfills to reduce water pollution. A composite liner is made of a geomembrane along with a geosynthetic clay liner. Composite-liner systems are better at reducing leachate migration into the subsoil than either a clay liner or a single geomembrane layer.\n\nThe primary forms of mechanical degradation associated with geomembranes result from insufficient tensile strength, tear resistance, impact resistance, puncture resistance, and susceptibility to environmental stress cracking (ESC). The ideal method of assessing the amount of liner degradation would be by examining field samples over their service life. Due to the lengths of time required for field sampling tests, various laboratory-accelerated ageing tests have been developed to measure the important mechanical properties.\n\nTensile strength represents the ability for a geomembrane to resist tensile stress. Geomembranes are most commonly tested for tensile strength using one of three methods; the uniaxial tensile test described in ASTM D639-94, the wide-strip tensile test described in ASTM D4885-88, and the multiaxial tension test described in ASTM D5617-94. The difference in these three methods lies in the boundaries imposed into the test specimens. Uniaxial tests do not provide lateral restraint during testing and thus tests the sample under uniaxial stress conditions. During the wide-strip test the sample is restrained laterally while the middle portion is unrestrained. The multiaxial tensile test provides a plane stress boundary condition at the edges of the sample. A typical range of tensile strengths in the machine direction are from 225 to 245 lb/in for 60-mil HDPE to 280 to 325 lb/in for 80-mil HDPE.\n\nTear resistance of a geomembrane becomes important when it is exposed to high winds or handling stress during installation. There are various ASTM methods for measuring tear resistance of geomembranes, with most common reports using ASTM D1004. Typical tear resistances show a value of 40 to 45 lb for 60-mil HDPE and 50 to 60 lb for 80-mil HDPE.\n\nImpact resistance provides an assessment of the effects of impacts from falling objects which can either tear or weaken the geomembrane. As with the previous mechanical properties, there are various ASTM methods for assessment. Significantly higher impact resistances are realized when geotextiles are placed above or below the geomembrane. Thicker geomembranes also display higher impact resistances.\n\nPuncture resistance of a geomembrane is important due to the heterogeneous material above and below a typical liner. Rough surfaces, such as stones or other sharp objects, may puncture a membrane if it does not have sufficient puncture resistance. Various methods beyond standard ASTM tests are available; one such method, the critical cone height test, measures the maximum height of a cone on which a compressed geomembrane, which is subjected to increasing pressure, does not fail. HDPE samples typically have a critical cone height of around 1 cm.\n\nEnvironmental stress cracking is defined as external or internal cracking in plastic induced by applied tensile stress less than its short-term tensile strength. ESC is a fairly common observation in HDPE geomembranes and thus needs to be evaluated carefully. Proper polymeric properties, such as molecular weight, orientation, and distribution, aid in ESC resistance. ASTM D5397 [standard test method for evaluation of stress crack resistance of polyolefin geomembranes using notched constant tensile load (NCTL)] provides the necessary procedure for measuring the ESC resistance of most HDPE geomembranes. The current recommended transition time for an acceptable HDPE geomembrane is around 100 h.\n\n"}
{"id": "1852679", "url": "https://en.wikipedia.org/wiki?curid=1852679", "title": "MOPP", "text": "MOPP\n\nMOPP (an acronym for \"Mission Oriented Protective Posture\"; pronounced \"mop\") is protective gear used by U.S. military personnel in a toxic environment, e.g., during a chemical, biological, radiological, or nuclear (CBRN) strike:\n\n\nEach MOPP level corresponds to an increasing level of protection. The readiness level will usually be dictated by the in-theatre commander.\n\n\n"}
{"id": "27290335", "url": "https://en.wikipedia.org/wiki?curid=27290335", "title": "Ministry of Higher Education and Scientific Research (Jordan)", "text": "Ministry of Higher Education and Scientific Research (Jordan)\n\nThe Ministry of Higher Education and Scientific Research (Arabic: وزارة التعليم العالي والبحث العلمي) is the government body that is responsible for maintaining and implementing government policies in higher education in Jordan. The ministry provides the support, services and organization aimed at guiding higher education sector in Jordan.\n\nThe Council of Higher Education was established in 1982 in response to the increased demand of regulating and planning the policies of higher education. In 1985, the council was renamed to the Ministry of Higher Education.\n\nThe ministry was annulled and was merged within the Ministry of Education in 1998, but was re-established in 2001 and was renamed as the Ministry of Higher Education and Scientific Research.\n\nThe ministry following its re-establishment in 2001, was given a new responsibility of guiding the scientific research sector of higher education in Jordan. In 2005, the Scientific Research Committee was formed. Since 2006, the ministry issues several internationally peer-reviewed journals in cooperation with several Jordanian universities:\n\n\n"}
{"id": "21765899", "url": "https://en.wikipedia.org/wiki?curid=21765899", "title": "Murray Goldberg", "text": "Murray Goldberg\n\nMurray Goldberg (born October 1962) is a noted Canadian educational technologist and a faculty member in the Department of Computer Science at the University of British Columbia in Vancouver, Canada. Goldberg is best known for being the founder of the elearning companies WebCT, Brainify, Silicon Chalk, AssociCom, and Marine Learning Systems. Goldberg was born in Calgary, Alberta, Canada and raised in Edmonton until he moved to British Columbia to attend the University of Victoria in 1980. Murray graduated from UVic in 1985 and then went on to earn an MSc from the University of British Columbia. In 2004 he was awarded an honorary Ph.D. from the Southern Cross University. Murray serves as director for various companies, sits on the board of trustees of Harvey Mudd College in Claremont California, is a mentor at the GSV Labs tech incubator in Redwood City California, and is a frequent consultant and speaker on the future of eLearning. Murray is also the chair of the British Columbia chapter of the Manning Innovation Awards.\n\nMurray began as a faculty member in the Department of Computer Science at UBC in 1993 and was awarded tenure there in 1998. In 1994, his first year as a faculty member at UBC, Goldberg won the Killiam Teaching Prize at UBC. \nGoldberg took a leave of absence from UBC in 1998 to lead WebCT full-time.\n\nIn 1995, as a UBC faculty member, Goldberg was researching the effectiveness of Web-Based learning environments. \nGoldberg found that the experience of building the courses for this experimentation was sufficiently time consuming and expensive that he decided to create a platform to enable the simple and rapid creation of web-based learning environments, WebCT.\nWebCT was widely accepted as a catalyst in the worldwide boom of on-line learning that accelerated beginning in 1997. By November, 2000 WebCT was purportedly serving 6 million students in 57 countries, and by late 2001, 10 million students in 80 countries at 2250 universities and colleges.\nIn 1999, Goldberg sold most of his stake in WebCT to Boston-Based Universal Learning Technology. The combined company took on the WebCT name. Goldberg remained as president of the Canadian division of WebCT until 2002 when he left to co-found Silicon Chalk.\n\nIn 2002 Goldberg co-founded Silicon Chalk and served as the president and CEO. Silicon Chalk created software for use in laptop or computer enabled higher education classrooms. The software facilitated the recording of classroom presentations, student note taking, polling, file sharing, and other features. The company never achieved widespread success, though did have users in 70 universities and colleges when it was sold to Horizon Wimba\n\nIn 2007 Goldberg began work on Brainify(website), an academic social bookmarking and networking site for university and college students and professors. Brainify was launched in January 2009 and Goldberg acts as the President and CEO. Members from 250 institutions had joined within the first 20 days of its launch.\n\nIn June, 2010, Goldberg launched AssociCom (website), a professional networking and discovery platform with an emphasis on enabling member discovery, learning and connection within professional associations and societies.\n\nAlso in 2010, Murray founded the company Marine Learning Systems (website). Their product, MarineLMS, is a learning management system for training in the maritime industry, with BC Ferries as their first customer.\n\n\n"}
{"id": "21252095", "url": "https://en.wikipedia.org/wiki?curid=21252095", "title": "Peer-Allocated Instant Response", "text": "Peer-Allocated Instant Response\n\nPeer-Allocated Instant Response (PAIR) is a project in the Netherlands that aims to match students with best-suited peer candidates for online support. It was launched in 2006 by the Open University of the Netherlands, an online university, and was supported by Fontys University and SURF, the national organization in the Netherlands that coordinates ICT in higher education.\n\nThe system works as follows: Online students requiring help type their question into an IM-like client; they are then automatically and instantaneously paired with another student who is currently online. When the conversation is over, both parties rate the experience. The currently used allocation algorithm does not perform a semantic analysis of the question; it allocates peer tutors based on their position in the curriculum relative to the help seeker, their past workload and past ratings.\n\nA simulation of the allocation algorithm was carried out and two 8-week pilot projects were conducted at the Open University and the Fontys University.\n"}
{"id": "32402084", "url": "https://en.wikipedia.org/wiki?curid=32402084", "title": "Polysilicon depletion effect", "text": "Polysilicon depletion effect\n\nPolysilicon depletion effect is the phenomenon in which unwanted variation of threshold voltage of the MOSFET devices using polysilicon as gate material is observed, leading to unpredicted behaviour of the electronic circuit. Polycrystalline silicon, also called polysilicon, is a material consisting of small silicon crystals. It differs from single-crystal silicon, used for electronics and solar cells, and from amorphous silicon, used for thin film devices and solar cells.\n\nThe gate contact may be of polysilicon or metal, previously polysilicon was chosen over metal because the interfacing between polysilicon and gate oxide (SiO) was favourable. But the conductivity of the poly-silicon layer is very low and because of this low conductivity, the charge accumulation is low, leading to a delay in channel formation and thus unwanted delays in circuits. The poly layer is doped with N-type or P-type impurity to make it behave like a perfect conductor and reduce the delay.\n\nIn \"figure 1(a)\" it is observed that the free majority carriers are scattered throughout the structure because of the absence of an external electric field. When a positive field is applied on the gate, the scattered carriers arrange themselves like \"figure 1(b)\", the electrons move closer toward the gate terminal but due to the open circuit configuration they don't start to flow. As a result of the separation of charges a depletion region is formed on the polysilicon-oxide interface, which has a direct effect on the channel formation in MOSFET.\n\nIn an NMOS with n+ Polysilicon gate, the poly depletion effect aids in the channel formation by the combined effect of the \"(+)ve\" field of donor ions (N) and the externally applied \"(+)ve\" field at gate terminal. Basically the accumulation of the \"(+)ve\" charged Donor ions (N) on the polysilicon enhances the Formation of the inversion channel and when V > V an inversion layer is formed, which can be seen in the figure 1(b) where the inversion channel is formed of acceptor ions (N)(minority carriers).\nPolysilicon depletion can vary laterally across a transistor depending on the fabrication process, which can lead to significant transistor variability in certain transistor dimensions.\n\nFor the above reason as the devices go down on the scaling (32-28nm nodes) poly gates are being replaced by metal gates. The following technology is known as High-k Dielectric Metal Gate (HKMG) integration. Recently, Intel also released a press-kit regarding their fabrication procedures of different nodes, which showed the use of Metal gate technology.\n\nDoped polysilicon was preferred earlier as gate material in MOS devices. Polysilicons were used as their work function matched with the Si substrate (which results in the low threshold voltage of MOSFET). Metal gates were re-introduced at the time when SiO dielectrics are being replaced by high-k dielectrics like Hafnium oxide as gate oxide in the mainstream CMOS technology. Also at the interface with gate dielectric, Polysilicon forms an SiO layer. Moreover, there remains a high probability for Fermi level pinning to occur. So the effect with doped poly is an undesired reduction of threshold voltage that wasn't taken into account during circuit simulation. In order to avoid this kind of variation in v of the MOSFET, at present metal gate is preferred over Polysilicon.\n"}
{"id": "8705978", "url": "https://en.wikipedia.org/wiki?curid=8705978", "title": "Remington Products", "text": "Remington Products\n\nRemington Products, commonly known as simply Remington, is a worldwide personal care corporation which manufactures razors (shavers), epilators, and haircare products for both men and women. It is a subsidiary of Spectrum Brands and Oak Hill Capital.\n\nThe origins of the \"Remington\" name date back to the formation of E. Remington and Sons, a firearms maker founded in 1816. E. Remington & Sons made occasional forays into products other than firearms, such as sewing machines and farm implements — but its most significant side venture was when inventor Christopher Sholes persuaded the firearms company to help him develop the typewriter with the QWERTY keyboard, which is still the standard today. In 1886, E. Remington & Sons sold the typewriter company, which became Remington Typewriter Company. This in turn merged with the Rand Kardex Corporation in 1927 to become Remington Rand. Remington Rand branched out into making adding machines, filing cabinets, punched card tabulating machines, and other office equipment to become a leading office equipment company.\n\nThe origin of Remington personal care products dates back to 1937 when Remington Rand began to branch out to electric shavers, starting with the Remington Model E.\n\nIn 1950, Remington Rand bought the pioneering Eckert-Mauchly Computer Company. In 1955, it merged with Sperry Corporation, developer of the automatic pilot, amongst other devices. The combined company became the Sperry Rand Corporation and continued to market shavers under the Remington brand. In 1979, Sperry Rand sold off a number of its divisions, including the consumer products. Victor Kiam bought the electric shaver company in a leveraged buyout.\n\nVictor Kiam's Remington Products Company became very profitable, branching out into other personal care small appliances, buying Clairol's personal care appliance business in 1994. Kiam sold controlling interest in Remington to Ike Perlmutter prior to the Clairol acquisition that same year. Remington changed hands again on June 1996 when Perlmutter and Kiam sold controlling interest in the company to Vestar Capital Partners. Victor Kiam died in 2001. In 2003, the Kiam family and Vestar sold Remington to the battery company Rayovac. Rayovac changed its name to Spectrum Brands and markets Remington brand men's and women's electric shavers, hair clippers, beard and moustache trimmers, nose and ear hair trimmers, foot massagers, make-up mirrors, heated hair rollers, blow dryers, and curling irons. Remington also sells flat irons.\n\nAir Plates, ProLuxe, Rose Pearl, Keratin Radiance, Wet2Straight, Shine therapy, Colour protect, Ceramic 210, Ceramic Slim 230, Pro ceramic Extra, Protect \nSleek and Smooth \n\nRemington Products' product line include Lady Remingtons, electric shavers for women.\n\nThe president of Remington shavers in 1978: \"When my wife bought me a Remington shaver, I was so impressed I bought the company\".\nVictor Kiam would often appear in television advertisements for Remington shavers and say: \"Shaves as close as a blade, or your money back!\"\n\n"}
{"id": "6126365", "url": "https://en.wikipedia.org/wiki?curid=6126365", "title": "Room 641A", "text": "Room 641A\n\nRoom 641A is a telecommunication interception facility operated by AT&T for the U.S. National Security Agency that commenced operations in 2003 and was exposed in 2006.\n\nRoom 641A is located in the SBC Communications building at 611 Folsom Street, San Francisco, three floors of which were occupied by AT&T before SBC purchased AT&T. The room was referred to in internal AT&T documents as the \"SG3 [Study Group 3] Secure Room.\" It is fed by fiber optic lines from beam splitters installed in fiber optic trunks carrying Internet backbone traffic and, as analyzed by J. Scott Marcus, a former CTO for GTE and a former adviser to the FCC, has access to all Internet traffic that passes through the building, and therefore \"the capability to enable surveillance and analysis of internet content on a massive scale, including both overseas and purely domestic traffic.\"\n\nThe room measures about and contains several racks of equipment, including a Narus STA 6400, a device designed to intercept and analyze Internet communications at very high speeds.\n\nThe existence of the room was revealed by former AT&T technician Mark Klein and was the subject of a 2006 class action lawsuit by the Electronic Frontier Foundation against AT&T. Klein claims he was told that similar black rooms are operated at other facilities around the country.\n\nRoom 641A and the controversies surrounding it were subjects of an episode of \"Frontline\", the current affairs documentary program on PBS. It was originally broadcast on May 15, 2007. It was also featured on PBS's \"NOW\" on March 14, 2008. The room was also covered in the PBS \"Nova\" episode \"The Spy Factory\".\n\nThe Electronic Frontier Foundation (EFF) filed a class-action lawsuit against AT&T on January 31, 2006, accusing the telecommunication company of violating the law and the privacy of its customers by collaborating with the National Security Agency (NSA) in a massive, illegal program to wiretap and data-mine Americans' communications. On July 20, 2006, a federal judge denied the government's and AT&T's motions to dismiss the case, chiefly on the ground of the States Secrets Privilege, allowing the lawsuit to go forward. On August 15, 2007, the case was heard by the Ninth Circuit Court of Appeals and was dismissed on December 29, 2011 based on a retroactive grant of immunity by Congress for telecommunications companies that cooperated with the government. The U.S. Supreme Court declined to hear the case. A different case by the EFF was filed on September 18, 2008, titled \"Jewel v. NSA\".\n\n\n"}
{"id": "1953334", "url": "https://en.wikipedia.org/wiki?curid=1953334", "title": "Rustication (architecture)", "text": "Rustication (architecture)\n\nIn classical architecture rustication is a range of masonry techniques giving visible surfaces a finish that contrasts in texture with the smoothly finished, squared-block masonry surfaces called ashlar. The visible face of each individual block is cut back around the edges to make its size and placing very clear. In addition the central part of the face of each block may be given a deliberately rough or patterned surface.\n\nRusticated masonry is usually \"dressed\", or squared off neatly, on all sides of the stones except the face that will be visible when the stone is put in place. This is given wide joints that emphasize the edges of each block, by angling the edges (\"channel-jointed\"), or dropping them back a little. The main part of the exposed face may be worked flat and smooth or left with, or worked, to give a more or less rough or patterned surface. Rustication is often used to give visual weight to the ground floor in contrast to smooth ashlar above. Though intended to convey a \"rustic\" simplicity, the finish is highly artificial, and the faces of the stones often carefully worked to achieve an appearance of a coarse finish.\n\nRustication was used in ancient times, but became especially popular in the revived classical styles of Italian Renaissance architecture and that of subsequent periods, above all in the lower floors of secular buildings. It remains in use in some modern architecture.\n\nSimilar finishes are very common in medieval architecture, especially in castles, walls and similar buildings, but here it merely arises from an unwillingness to spend the extra money required for ashlar masonry in a particular building, and lacks the deliberate emphasis on the joints between blocks. Though it often achieves a decorative effect, this is something of a by-product, and the exploitation for architectural effect within a single building of contrasts between rusticated and ashlar surfaces is rarely seen. In some buildings, such as the Palazzo Vecchio in Florence (begun 1298) something other than cost-saving is at play, and this may be the association of the technique with the display of power and strength, from its use in military architecture. Rough finishes on stone are also very common in architecture outside the European tradition, but these too would generally not be called rustication. For example, the bases of Japanese castles and other fortifications usually use rough stone, often very attractively.\n\nAlthough rustication is known from a few buildings of Greek and Roman antiquity, for example Rome's Porta Maggiore, the method first became popular during the Renaissance, when the stone work of lower floors and sometimes entire facades of buildings were finished in this manner. It was generally used for secular buildings, and has always remained uncommon in churches, perhaps through a lingering association with the architecture of military power; there are exceptions, such as St Giles in the Fields, London (1730–34).\n\nProbably the earliest and most influential example is the Palazzo Medici Riccardi in Florence, built between 1444 and 1484, with two contrasting rusticated finishes. The ground floor has an irregular and genuinely rugged appearance, with a variation in the degree to which parts of the faces of blocks project from the wall that is rarely equalled later. Above, the rustication is merely to emphasize the individual blocks, and the faces are all smooth and even. Also in Florence, Palazzo Strozzi, begun 1489, with large oblong rounded cushions, and the front of the Pitti Palace, begun 1458, rusticated their whole facades in the same style. These facades only used the classical orders in mullions and aedicules, with arched forms in rustication the main relief from the massive flat walls. The Palazzo Rucellai, probably of the 1460s, begins to classicize such facades, using smooth-faced rustication throughout, except for the pilasters at each level.\n\nIn Rome, Donato Bramante's Palazzo Caprini (\"House of Raphael\", by 1510, now destroyed) provided a standard model for the integration of rustication with the orders. Here the obvious strength of a blind arched arcade with emphatic voussoirs on the rusticated ground storey (in fact using stucco) gave reassuring support to the upper storey's paired Doric columns standing on rusticated piers, set against a smooth wall. The first major Renaissance building in Spain, the Palace of Charles V in Granada (1527), had a deeply rusticated ground floor facade with regular rounded cushions.\n\nThe technique was enthusiatically taken up by the next generation of Mannerist architects, with Giulio Romano in the lead. Most early examples of this \"rustic\" style are therefore built for sophisticated patrons in the leading centres of taste. Giulio's Palazzo Stati Maccarani in Rome and Palazzo Te in Mantua expand the voussoirs still further, and the courtyard in Mantua plays games with the technique, with some blocks ashlar, other projecting further than the rest, and larger blocks placed higher than smaller ones. The Mannerist architectural writer Sebastiano Serlio and others of his generation enjoyed the play between rusticated and finished architectural elements. In the woodcut of a doorway from Serlio's 1537 treatise, the banded rustication of the wall is carried right across the attached column and the moldings of the doorway surround, binding together all the elements. \nThe Italians brought in to expand the Palace of Fontainebleau introduced the technique to France. Its spread to Germany and England took longer, but by about the end of the 16th century it had reached all parts of Europe. In his Banqueting House in London (1619), Inigo Jones gave a lightly rusticated surface texture to emphasize the blocks on both storeys, and to unify them behind his orders of pilasters and columns. \nDuring the 18th century, following the Palladian revival, rustication was widely used on the ground floors of large buildings, as its contrived appearance of simplicity and solidity contrasted well to the carved ornamental stonework and columns of the floors above: \"Rustication became almost obligatory in all 18th- and 19th-century public buildings in Europe and the USA\". A ground floor with rustication, especially in an English mansion such as Kedleston Hall, is sometimes referred to as the \"rustic floor\", in order to distinguish it from the piano nobile above. As well as uses emphasizing the horizontal, rustication is often used in relatively narrow vertical bands, on the quoins at corners or elsewhere. Rustication may also be confined to the surrounds of arches, doors or windows, especially at the top. In these and other situations where rustication stops horizontally, the edge is usually made up of vertically alternating long and short blocks. Rustication therefore often reverses the patterns of medieval and later vernacular architecture, where roughly dressed wall surfaces often contrast with ashlar quoins and frames to openings.\nArchitectural books by authors such as James Gibbs and William Chambers set out detailed recommendations for the proportions of the blocks in relation to columns in the same facade, and the proportion of the block that a widened joint should occupy, though their prescriptions differ, and were not always followed by architects.\n\nTypically, rustication after 1700 is highly regular, with the front faces of blocks flat even when worked in patterns, as opposed to the real unevenness often seen in the 16th-century examples. Often the Palazzo Medici Riccardi model is followed; the ground floor has heavy rustication with textured faces, while above there is smooth-faced \"V\" rustication. Though such horizontal zones of rustication are the most common, vertical zones can often be used as highlights, as in the illustration from Catania above, or the Cour Napoleon in the Louvre Palace. The Baroque garden front of the Pitti Palace achieves a striking effect, not often copied, by using extensive \"blocking\", both rounded and rectangular, on the shafts of its columns and pilasters.\n\nCanton Viaduct, a blind arcade cavity wall railroad viaduct built in 1834-35 in Canton, Massachusetts is an example of modest effects of rustication in industrial architecture; the effect is pleasing, and the cost was probably reduced. Massive effects of contrasting rustications typify the \"Richardsonian Romanesque\" style exemplified in the 1870s and 80s by the American architect H. H. Richardson. The technique is still sometimes used in architecture of a broadly Modernist character, especially in city centre streets where it helps modern buildings blend with older ones with rustication.\n\nAlthough essentially a technique for stone masonry, rustication can be imitated in brick and stucco, which began as early as Bramante's Palazzo Caprini and was common in smaller houses in Georgian architecture, and also in wood (see below), which is mainly found in British America.\n\nThe most common variation of rustication is the smooth-faced, where the external face of the block is smooth, as in ashlar, and differs from that only by the cutting in at the joints; this became increasingly popular, and is now the most commonly seen type. If deeply cut-back edges are worked only to the horizontal joints, with the appearance of the vertical joints being minimised, the resulting effect is known as banded rustication, mostly seen on the lowest levels of very large buildings like the Palace of Versailles or the main Foreign Office building in London. As at Versailles, the bands may de \"elbowed\", dipping diagonally around arches to join up with and emphasize the voussoirs. Banded rustication is mostly seen with smooth-faced stones, and has remained popular in Stripped Classicism and other contemporary versions of classical styles. In this style, the bands are sometimes several feet apart, making it apparent that stone facings are being used.\n\nWhen the stone is left with a rough external surface, rough shapes may be drilled or chiselled in the somewhat smoothed face in a technique called \"vermiculation\" (vermiculate rustication or vermicular rustication), so called from the Latin \"vermiculus\" meaning \"little worm\", because the shapes resemble worms, worm-casts or worm tracks in mud or wet sand. Carved vermiculation requires a good deal of careful mason's work, and is mostly used over limited areas to highlight them. Disparities between individual blocks are often seen, presumably as different carvers interpreted their patterns slightly differently, or had different levels of skill. The small Turner Mausoleum at Kirkleatham by James Gibbs (1740) has an unusually large area vermiculated, over half of the main level. When the shapes join up to form a network, the style is called \"reticulated\".\n\nOften, especially from the Baroque onwards, the roughly flattened central areas of stones are indented in regular, but not too regular, patterns called \"pecked\" or \"picked-work\", and various other ways of patterning them may be found. In garden architecture, where water was to flow over or near the surface, a vertically oriented pattern evoking hanging pond-weed or algae, or icicles (\"frost-work\") is sometimes used. Also associated with gardens is \"cyclopian\" rustication, where the blocks are very large and irregular, as though placed by giants, and \"rock-work\", where surfaces are built up of rough rocks not placed in regular courses at all. This last goes beyond rustication, and is found in fountains and follies, and later rockeries for planting.\n\nIn prismatic rustication the blocks are dressed at an angle near each edge, giving a prism-like shape. Where the faces rise to a single point, this is often known by terms using \"diamond\", and is covered below. They may also, usually in blocks that are oblong rather than square, rise to a ridge in the centre. Both types are illustrated, with several others, by Serlio.\n\nVarious types of other patterns in masonry surfaces are sometimes called rustication. These include \"diamond point\" or \"diamond rustication\" where the face of each stone is a low pyramid facing out. This covered the whole facade of the Palazzo dei Diamanti in Ferrara, completed in 1503, and most of that of the Palace of Facets in the Moscow Kremlin, designed by Italians and completed in 1492, with rectangular \"diamonds\". These \"diamond palaces\" influenced other whole walls of diamonds at Casa dos Bicos in Portugal (after 1523, alternate stones only), and Crichton Castle in Scotland (c. 1585; all stones, with flat edges between pyramids).\n\nThe round towers at the Castello Sforzesco in Milan almost use diamonds, but their points are smoothed over. The illustration at right, from Catania in Sicily, alternates rows of three square \"diamond\" blocks with two oblong blocks, where the faces rise to a ridge rather than a point, showing both the main forms of \"prismatic rustication\".\n\nThe sharply pointed styles have really nothing to do with classical rustication, and are instead a development of styles of raised decoration of masonry that were popular in late Gothic architecture, especially the Iberian Manueline (or Portuguese late Gothic) and its equivalent in Spain, known as Isabelline Gothic. When not figurative these are known as bossage. These are probably a development of Mudéjar styles of patterning walls. In the spectacular late 15th-century gateway to the Palacio de Jabalquinto in Baeza, Andalucia, small widely spaced pyramids cover one of the many zones with fancy carved elements, projecting from a wall otherwise in ashlar.\n\nLater, in Baroque architecture, relatively small areas of diamond rustication were reintegrated into architecture in the classical tradition, and were popular as highlights, especially in Sicily and southern Italy and Eastern and Central Europe. The large Černín Palace in Prague (1660s) repeats the Kremlin formula of a broad zone of diamonds across the middle height of the facade, though like the towers in Milan these do not come to a point.\n\nThe appearance of rustication, creating a rough, unfinished stone-like surface, can be worked on a wooden exterior. This process became popular in 18th century New England to translate the features of Palladian architecture to the house-carpenter's idiom: in Virginia Monticello and Mount Vernon both made use of this technique. Mount Vernon in particular makes extensive use of feigned rustication and sanded paint and the original finished surfaces of several original planks still survive.\n\nRustication of a wooden exterior consists of three basic steps. First, the wood is cut, sanded and prepared with beveled grooves that make each plank appear as if it were a series of stone blocks. Second, the wood is painted with a thick coat of paint. Third, while the paint is still wet, sand is thrown or air blasted onto the planks until no more sand will stick. After the paint dries the plank is ready for use.\n\nIn Central Europe, especially the Czech Republic, feigned rustication in sgraffito (decoration by scraping away one colour of coating on an exterior to show another beneath) is a feature from the Late Renaissance onwards, continuing into the 20th century. Often \"prismatic\" or \"diamond\" rustication is imitated.\n\n\n"}
{"id": "32250999", "url": "https://en.wikipedia.org/wiki?curid=32250999", "title": "Scientific and Technical Centre for Building", "text": "Scientific and Technical Centre for Building\n\nThe \"Centre Scientifique et Technique du Bâtiment\" (Scientific and Technical Center for Building) CSTB, is the French national organisation providing research and innovation, consultancy, testing, training and certification services in the construction industry.\n\nIt was founded in 1947 after the Second World War to support the reconstruction effort.\n\nThe mission of the CSTB is to ensure the quality and safety of buildings, and support innovation from the idea to the market. It brings together multidisciplinary skills to develop and share essential scientific and technical knowledge, and to provide stakeholders with answers to the challenges of their professional practice.\n\nThe CSTB focuses on four key activities: research and consulting, assessment, certification and dissemination of knowledge. Its field of expertise covers construction products, buildings, and their integration into neighborhoods and cities.\nTheir principal test facilities are as follows: \n\n"}
{"id": "45555495", "url": "https://en.wikipedia.org/wiki?curid=45555495", "title": "Sukhinder Singh Cassidy", "text": "Sukhinder Singh Cassidy\n\nSukhinder Singh Cassidy (born Sukhinder Singh in 1970) is a technology executive and entrepreneur. She has worked at companies including Google, Amazon and News Corp, Yodlee (YODL), and Polyvore. In 2011, she founded JOYUS, the video shopping platform for women, and served as CEO then Chairman until 2017. Singh Cassidy is also Founder of theBoardlist.\n\nSukhinder Cassidy was born in 1970 in Dar es Salaam, Tanzania to parents of Sikh descent. Her family moved to Ontario, Canada, when she was two years old. She grew up in St. Catharines, in Canada's Niagara Region. Sukhinder Cassidy graduated from the University of Western Ontario and earned her H.B.A. from the Richard Ivey School of Business in 1992.\n\nSukhinder Cassidy was raised in an entrepreneurial family. Both of her parents were doctors and ran a medical practice for thirty years.\n\nSukhinder Cassidy started her career in investment banking at Merrill Lynch in New York. She moved to the Merrill Lynch London office 1994. She then worked as an analyst for British Sky Broadcasting.\n\nIn 1998, Sukhinder Cassidy moved to Silicon Valley and joined ecommerce startup Junglee (company) as Head of Business Development. Junglee was acquired by Amazon in 1998. Following the acquisition, Sukhinder Cassidy joined Amazon, where she led merchant business development for the first generation of Amazon marketplace. According to Bloomberg Business, she helped create \"one of the company's first programs designed to deliver shoppers to merchants that carried what the buyers were looking for.\"\n\nSukhinder Cassidy co-founded financial services platform Yodlee in 1999 with five engineering co-founders, including Schwark Satyavolu. and served as SVP of Sales and Business Development from 1999 to 2003. In 2014, Yodlee went public, trading under the ticker YODL.\n\nIn 2003, Sukhinder Cassidy joined Google as the first General Manager for Google Local & Maps, and Head of Content Acquisition for Books, Library, Scholar, Shopping and Video. There she launched Google Local and Maps, with product manager Bret Taylor and a team of engineers. In 2004, she became head of Google's international operations in Asia Pacific (APAC) and Latin America (LATAM), becoming VP in 2005 and then President of APAC and LATAM in 2008. Singh-Cassidy is credited with building Google's presence across 103 countries in Asia Pacific and Latin America.\n\nIn 2009, Sukhinder Cassidy left Google to become CEO-in-Residence at venture capital firm Accel Partners. Singh Cassidy was named CEO of Polyvore in 2010.\n\nIn October 2010, Sukhinder Cassidy had the idea for a new platform aimed at converging content and commerce through online video as a way to increase consumer engagement with products and drive direct purchases via the video. She founded JOYUS in January, 2011 in San Francisco, CA. Joyus.com launched to the public in August, 2011. Singh Cassidy raised $7.9 million in seed funding from Accel Partners, Harrison Metal, Joel Hyatt, Venky Harinarayan & Anand Rajaraman. In 2012, JOYUS raised a second round of funding totaling $11.5 million from Interwest and Time Warner, as well as existing investors.\n\nJOYUS was ranked one of the \"Hot 100\" e-retailing websites by Internet Retailer in 2012; and one of the \"Top 25 Ecommerce Companies to Watch\" by Brand Innovators in 2013. JOYUS was a winner of the 2013 Brightcove Innovation Awards and a winner of the 2013 L’Oreal Next Generation Digital Awards. JOYUS was featured by Apple as one of the best Lifestyle shopping apps for the 2014 holiday season.\n\nJOYUS was acquired by StackCommerce in September, 2017.\n\nIn May 2015, Sukhinder Cassidy published an open letter, titled \"Tech Women Choose Possibility\", challenging the tech community to increase the rate of progress for women in the industry by leveraging its wealth of existing female talent. The letter was co-signed by 59 female entrepreneurs and investors. Singh Cassidy based the letter on her own experiences as a tech entrepreneur and research she conducted on 230 female founders and CEOs of tech companies.\n\nSukhinder Cassidy launched theBoardlist, the first initiative of the #ChoosePossibility Project, on July 15, 2015. theBoardlist is an online marketplace that connects CEOs who are looking for board candidates with women who are peer-endorsed for private and public tech company boards. When it launched, theBoardlist included the names of over 600 women who had been endorsed by 50 investors and CEOs in the tech industry, from companies including Accel Partners, Greylock Partners, Twitter, Lyft and Box.\n\nOn October 20, 2015, theBoardlist announced that it had facilitated its first placement of a woman to the board of a private tech company.\n\nAs of late 2017, theBoardlist had attracted over 1,400 executives to nominate over 2,100 women for board service, and influenced over 100 board placements.\n\nIn April 2018, Singh Cassidy was named the president of StubHub.\n\nSukhinder Cassidy is married to Simon Cassidy, a fellow Canadian and former hedge-fund manager who runs an independent investment firm. The couple has three children and lives in the San Francisco Bay area.\n\n\nSukhinder Cassidy serves as a public board director at TripAdvisor (TRIP), Ericsson (ERIC), and Urban Outfitters (URBN). She previously served on the boards of J. Crew Group, Inc.(JCG), StitchFix, and as an advisor to Twitter.\n\n\n\nSingh Cassidy is a select angel investor whose investments include:\n\n"}
{"id": "3328462", "url": "https://en.wikipedia.org/wiki?curid=3328462", "title": "Technical Standard Order", "text": "Technical Standard Order\n\nA Technical Standard Order (TSO) is a minimum performance standard issued by the United States Federal Aviation Administration for specified materials, parts, processes, and appliances used on civil aircraft. Articles with TSO design approval are eligible for use on the United States type certified products by following a much lighter process than similar non-TSO approved part, provided the TSO standard meets the aircraft requirements. The TSO authorization (also called TSOA) or a letter of TSO Design Approval does not necessarily convey approval for installation. See 14 CFR Part 21 Subpart O. \n\nSimilar standards are maintained by other aviation authorities (for example ETSO by EASA for European Union), with limited reciprocal equivalence on a per-country basis \n\n\n"}
{"id": "13324152", "url": "https://en.wikipedia.org/wiki?curid=13324152", "title": "Tygon tubing", "text": "Tygon tubing\n\nTygon is a brand name for a \"family\" of flexible polymer tubing consisting of a \"variety\" of materials to be used \"across a range of specialized fluid transfer requirements\". The specific composition of each type is a trade secret. Some variants have multiple layers of different materials. Tygon is a registered trademark of Saint-Gobain Corporation. It is an invented word, owned and used by Saint-Gobain and originated in the late 1930s. Tygon products are produced in three countries, but sold throughout the world. Tygon tubing is used in many markets, including food and beverage, chemical processing, industrial, laboratory, medical, pharmaceutical, and semiconductor processing. There are many formulations of clear, flexible, Tygon tubing. The chemical resistance and physical properties vary among the different formulations, but the tubing generally is intended to be \"so resistant to chemical attack that it will handle practically any chemical\", whether liquid, gas, or slurry. While largely non-reactive, Tygon has been reported to liberate carbon monoxide and is listed among carbon monoxide-releasing molecules. \n\n\"Tygon B-44-3\", \"Tygon B-44-4X\", \"Tygon B-44-4X I.B.,\" and \"Tygon Silver\" (antimicrobial) were widely used in the food and beverage industry, in particular in: beverage dispensing, dairy processing, soft-serve dispensing, vitamin and flavor concentrate systems, cosmetic production, and water purification systems. These formulations each meet U.S. Food and Drug Administration 3-A and NSF International 51 criteria but they do not comply with European Directives (European Directive 2002/72/EC of 6 August 2002 relating to plastic materials and articles intended to come into contact with foodstuffs as modified in particular by Directive 2007/19/EC of 2 April 2007).\n\nSeveral formulations of Tygon are USP class VI approved and can be used in either surgical procedures or pharmaceutical processing.\n\n\"Tygon Medical/Surgical Tubing S-50-HL\" — Characterized to the latest ISO 10993 standards and U.S. Food and Drug Administration (FDA) guidelines for biocompatibility. This material is non-toxic, non-hemolytic, and non-pyrogenic. This formulation is used in minimally invasive devices, dialysis equipment, for bypass procedures, and chemotherapy drug delivery.\n\n\"Tygon Medical Tubing S-54-HL\" was introduced in 1964 for use in medical applications. This material can be used in catheters, for intravenous or intra-arterial infusion and other surgical uses. Tygon S-54-HL can also be fabricated into cannulae or protective sheath products using thermoforming and flaring techniques.\n\n\"Tygon LFL\" (Long Flex Life) pump tubing is non-toxic clear tubing with broad chemical resistance. It is often used in product filtration and fermentation and surfactant delivery.\n\n\"Tygon 2275 High Purity Tubing\" is a plasticizer-free material that is often used in sterile filling and dispensing systems and diagnostic equipment. This formulation is also considered to have low absorption/adsorption properties, which minimizes the risk of fluid alteration.\n\n\"Tygon 2275 I.B. High-Purity Pressure Tubing\" is plasticizer-free and is reinforced with a braid for use with elevated working pressures.\n\nMany formulations of Tygon can be used in peristaltic pumps, including the following:\n\n\"Tygon R-3603 Laboratory Tubing\" is commonly used in chemical laboratories. It is often used in incubators and as a replacement for rubber tubing for Bunsen burners. This material is produced in vacuum sizes and can withstand a full vacuum at room temperature.\n\n\"Tygon R-1000 Ultra-Soft Tubing\" is used in general laboratory applications. It is the softest of the Tygon formulations with a durometer hardness of Shore A 40 (ASTM Method D2240-02). Because of the low durometer of this material it is often used in low-torque peristaltic pumps.\n\n\"Tygon LFL\" (Long Flex Life) Pump Tubing, \"Tygon 3350\", \"Tygon S-50-HL Medical/Surgical\" Tubing, \"Tygon 2275 High Purity Tubing\", and \"Tygon 2001\" Tubing are also used in peristaltic pump applications.\n\nTygon tubing is available in Plasticizer-free/non-DEHP (non-Phthalate)-formulations. These formulations have a high degree of chemical resistance and do not release any hazardous material when properly incinerated. \"Tygon 2275 High Purity tubing\", \"Tygon Ultra Chemical Resistant Tubing 2075\", and \"Tygon Plasticizer Free Tubing 2001\" are all plasticizer-free. \"ND-100 series\" products are non-DEHP and use a non-Phthalate plasticizer.\n\n\"Tygon Silver\" Tubing has a plasticizer-free inner bore and a silver-based compound on the inner surface to decrease bacterial growth and protect against microbes.\n\nThere are several formulations of Tygon that are used in industrial applications.\n\n\n"}
{"id": "56351412", "url": "https://en.wikipedia.org/wiki?curid=56351412", "title": "Uzawa's theorem", "text": "Uzawa's theorem\n\nUzawa's theorem, also known as the steady state growth theorem, is a theorem in economic growth theory concerning the form that technological change can take in the Solow–Swan and Ramsey–Cass–Koopmans growth models. It was first proved by Japanese economist Hirofumi Uzawa.\n\nOne general version of the theorem consists of two parts. The first states that, under the normal assumptions of the Solow and Neoclassical models, if (after some time T) capital, investment, consumption, and output are increasing at constant exponential rates, these rates must be equivalent. Building on this result, the second part asserts that, within such a balanced growth path, the production function, formula_1 (where formula_2 is technology, formula_3 is capital, and formula_4 is labor), can be rewritten such that technological change affects output solely as a scalar on labor (i.e. formula_5) a property known as labor-augmenting or Harrod-neutral technological change. \n\nUzawa's theorem demonstrates a significant limitation of the commonly used Neoclassical and Solow models. Imposing the assumption of balanced growth within such models requires that technological change by labor-augmenting. By contraposition, any production function for which it is not possible to represent the effect of technology as a scalar on labor cannot produce a balanced growth path.\n\nThroughout this page, a dot over a variable will denote its derivative with respect to time (i.e. formula_6). Also, the growth rate of a variable formula_7 will be denoted formula_8.\n\nUzawa's theorem \n\nModel with aggregate production function formula_9, where formula_10 and formula_11 represents technology at time t (where formula_12 is an arbitrary subset of formula_13 for some natural number formula_14). Assume that formula_15 exhibits constant returns to scale in formula_3 and formula_4. The growth in capital at time t is given by\n\nformula_18\n\nwhere formula_19 is the depreciation rate and formula_20 is consumption at time t. \n\nSuppose that population grows at a constant rate, formula_21, and that there exists some time formula_22 such that for all formula_23, formula_24, formula_25, and formula_26. Then \n\n1. formula_27; and\n\n2. For any formula_28 , there exists a function formula_29 that is homogeneous of degree 1 in its two arguments, such that the aggregate production function can be represented as \"formula_30\", where formula_31 and formula_32.\n\nFor any constant formula_33, formula_34.\n\nProof: Observe that for any formula_35, formula_36. Therefore,\nformula_37.\n\nWe first show that the growth rate of investment formula_38 must equal the growth rate of capital formula_39 (i.e. formula_40) \n\nThe resource constraint at time formula_41 implies \nBy definition of formula_43, formula_44 for all formula_23 . Therefore, the previous equation implies\nfor all formula_23. The left-hand side is a constant, while the right-hand side grows at formula_48 (by Lemma 1). Therefore, formula_49 and thus\nFrom national income accounting for a closed economy, final goods in the economy must either be consumed or invested, thus for all formula_41\nDifferentiating with respect to time yields\nDividing both sides by formula_54 yields\nSince formula_57 and formula_58 are constants, formula_59 is a constant. Therefore, the growth rate of formula_59 is zero. By Lemma 1, it implies that \nSimilarly, formula_62. Therefore, formula_63. \n\nNext we show that for any formula_23, the production function can be represented as one with labor-augmenting technology.\n\nThe production function at time formula_65 is \nThe constant return to scale property of production (formula_15 is homogeneous of degree one in formula_3 and formula_4) implies that for any formula_23, multiplying both sides of the previous equation by formula_71 yields\nNote that formula_73 because formula_74(refer to solution to differential equations for proof of this step). Thus, the above equation can be rewritten as\nFor any formula_23, define \nand\nCombining the two equations yields \nBy construction, formula_81 is also homogeneous of degree one in its two arguments.\n\nMoreover, by Lemma 1, the growth rate of formula_82 is given by\n"}
{"id": "42742734", "url": "https://en.wikipedia.org/wiki?curid=42742734", "title": "Welfare of farmed insects", "text": "Welfare of farmed insects\n\nThe welfare of farmed insects concerns treatment of insects raised for animal feed, pet food, human consumption (entomophagy), and other purposes like honey and silk.\n\nScientists remain uncertain about the existence and degree of pain in invertebrates, including insects. Nonetheless, insect welfare is being taken increasingly seriously in laboratory settings. Vincent Wigglesworth suggested a precautionary approach of anaesthetizing insects during potentially painful procedures. John Cooper has written about techniques for \"Anesthesia, analgesia, and euthanasia of invertebrates\" including insects. Neil A. C. Bennie and colleagues proposed a method for chemical euthanasia of insects and other terrestrial arthropods.\n\nSome authors have begun extending discussions of insect welfare beyond the laboratory to the domain of raising insects for food. A new Dutch Animal Act that went into effect on 1 January 2013 creates a regulatory framework for farm-animal welfare based on the five freedoms, and the law specifically lists a number of insect species as \"production animals\" whose wellbeing needs to be respected. Dutch politician Marianne Thieme asked a series of questions suggesting concern that insect farming would multiply the number of animals farmed and killed for human consumption. Robert Nathan Allen of the pro-entomophagy organization Little Herds feels that the welfare of insects is important, though he believes well managed farms can maintain high standards of care. Some entomophagy suppliers highlight the importance of humane insect treatment. For instance, World Ento uses the name \"Good Karma Killing\" to describe its process of freezing insects into a statis state. A 2013 FAO report on \"Edible insects\" includes a section encouraging high standards of welfare in entomophagy operations, despite uncertainty about whether insects can suffer.\n\nOthers feel that considering the wellbeing of farmed insects is going too far. Rhys Southan suggests that even most vegans do not care a lot about insects, but that \"Insects are to animal rights what Larry Flynt is to the First Amendment – you have to uphold their rights even if you don’t want to, or the whole thing falls apart.\" He goes on to propose satirical slogans that insect-rights activists might use against entomophagy. Wesley J. Smith rejects a proposal that \"We have to begin to ponder the welfare of insects we consume,\" adding: \"See how crazy things get once human exceptionalism is rejected?\" Smith cites a parody website, \"The Society for the Prevention of Cruelty to Insects.\" The site features an article feigning concern for the insects eaten alive by contestants on \"Survivor\".\n\nBecause there is little standardized protocol for insect rearing, most farmers learn the best production methods by trial and error. For instance, if they kill an insect colony by setting the temperature too high, they avoid doing that in the future. Most breeders keep their techniques secret in order to avoid having them copied by other farms.\n\nBecause there are so many types of insects, it is not feasible to devise a single protocol for their treatment. Case-by-case understanding is required.\n\nWhen viruses infect an insect-rearing facility, they spread rapidly and kill most of the insects.\n\nHumans can spread diseases to farmed insects. Thus, sanitation is very important, and only farm staff should have access to the insects. For instance, the company Van de Ven had a pathogen outbreak that killed all of its \"Zophobas morio\" worms, and the breeders hypothesized that the disease may have been brought by human visitors.\n\nGiving insects a heat gradient may sometimes help prevent disease because behavioral thermoregulation can suppress pathogens.\n\nWalter Jansen's Jagran company raises housefly larvae for use as animal feed. Humidity needs to be carefully controlled to avoid dehydrating or drowning the insects.\n\nInsects are poikilothermic, but maintaining an adequate temperature range remains important. For example, mealworms thrive best when living close together, but this can lead to overheating if temperature is not controlled.\n\nSome insects like locusts begin eating each other when they become overcrowded or malnourished. Adequate space and nutrition are important to prevent this.\n\nEntomophagy is common in many developing countries, such as Thailand and Mexico. Usually killing is done without euthanasia. For instance:\n\nIn Silveiras, Brazil, residents pluck the wings off ants and then either fry them or dip them in chocolate. In Thailand, crickets are gathered fresh in the morning and then fried.\n\nLittle research has been done on humane methods of killing insects for consumption.\n\nThe most common killing methods used by entomophagy companies in the Netherlands are freezing and dry-freezing (i.e., freezing and reducing pressure in order to extract water from the insects).\n\nProtix Biosystems kills its black soldier flies by shredding, since its end product is a powder. Death takes less than a second. Tarique Arsiwalla at Protix said shredding makes sense because Western consumers are more likely to accept powdered insects than whole insects.\n\nThe Jagran company has tried asphyxiation, cooling, freeze-drying, boiling, and shredding. Managing Director Walter Jansen believes that shredding is most humane.\n\nThe Kreca company kills its animal-feed insects by putting them into a fridge or freeze-drying them. Insects destined for human consumption are first sterilized in hot water and then are refrigerated or freeze-dried.\n\nFAO's \"Edible insects\" report suggests: \"Insect-killing methods that would reduce suffering include freezing or instantaneous techniques such as shredding.\"\n\nWhile freezing is sometimes said to be a humane way to kill certain arthropods, others dispute this. According to \"AVMA Guidelines for the Euthanasia of Animals,\" freezing is \"not considered to be humane\" when not preceded by another form of anesthesia. The British and Irish Association of Zoos and Aquariums (BIAZA) Terrestrial Invertebrate Working Group (TIWG) reports on a survey conducted by Mark Bushell of BIAZA institutions. He found that refrigeration and freezing were the most common methods \"of euthanasia of invertebrates although research has suggested that this is probably one of the least ethical options.\" That said, freezing is a worst-case method if chemical or instantaneous physical destruction is not possible.\n\nSome \"how to\" guides for eating insects make no mention of freezing or other euthanasia methods. For example, Miles Olson recommends\nand so on.\n\nThe website Insects Are Food suggests refrigerating insects to slow them down without killing them, prior to boiling or otherwise cooking them.\n\nOther guides recommend freezing first. Timothy Ferriss recounts what he observed when roasted his insects without freezing them first: \"Suffice it to say, merely sedated crickets make horrible noises if you roast them, and the visual is far, far worse. Do yourself a favor and freeze them.\"\n\nSometimes insects are not killed by farming companies but are sold live, for consumption by fish and pets. 95% of the Kreca company's insects are sold live. Of the 1500 kg per week of mealworms produced by the Van de Ven company, most are sold as live feed.\n\nMany suppliers of insects for reptiles offer live bugs and worms. Monitor lizards are typically fed live insects and may not eat pre-killed ones. Amphibians typically require live insects—wild-caught, home-grown, or bought at a pet store—although some like axolotls can be fed chunks of meat. It's generally hard to convert reptiles and amphibians that eat insects to pre-killed prey, though some pet owners can feed dead insects by moving or dangling them. Bearded dragons can be fed dead crickets by hiding them in other food, dangling them with tongs, squirting them with water, or vibrating a bowl.\n\nPet spiders, praying mantids, and other insectivorous bugs typically require live food. Hedgehogs can be fed live, freeze-dried, or canned insects.\n\nLive worms and insects are commonly used as fishing bait, with the result that they are either eaten alive by fish or drowned.\n\nMany vegans avoid honey and silk because these require insect farming, even though the insects are not eaten. Silk production involves boiling silk worms alive in their cocoons.\n\nThe red pigment carmine is produced from powdered bodies of scale insects, so some vegans avoid it.\n\nShellac is produced from a resin secreted by the lac bug on specific trees in Asia. In addition to its use in industry, shellac is incorporated into some fruits, coffee beans, and candies as confectioner's glaze. Some vegans avoid confectioner's glaze because lac bugs may be killed during shellac production. Lac used to produce red dye may be even more injurious to lac bugs because while shellac comes from lac-bug secretions, lac dye's color comes from the insect bodies themselves.\n\n"}
{"id": "36526751", "url": "https://en.wikipedia.org/wiki?curid=36526751", "title": "Wide span vehicle", "text": "Wide span vehicle\n\nWide span vehicle is a special type of farming vehicle that is much wider than a typical vehicle. Wide span vehicles can measure up to wide or wider. The impetus behind wide span vehicles in farming is to reduce the amount of soil that becomes damaged (\"e.g.\", from compaction) from farm vehicles. Controlled traffic farming (CTF) is also aimed at solving this problem and both CTF and wide span vehicles can be used together to further improve land use.\n\nThe many benefits of using wide span vehicles in farming include improved soil quality, less land lost to wheelways, increase in farm profits, less fuel use, reduced chemical use from more precise application, and increased types of land available for organic farming.\n\nDavid Dowler was a pioneer of wide span vehicle farming and created a first prototype gantry, called P1, in January 1978. Dowler went on to build and apply for patents on many wide span-related machines.\n"}
