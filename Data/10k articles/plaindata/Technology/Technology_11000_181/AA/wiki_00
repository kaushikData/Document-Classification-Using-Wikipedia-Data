{"id": "3247336", "url": "https://en.wikipedia.org/wiki?curid=3247336", "title": "1-bit DAC", "text": "1-bit DAC\n\nA Bitstream or 1-bit DAC is a consumer electronics marketing term describing an oversampling digital-to-analog converter (DAC) with an \"actual\" 1-bit DAC (that is, a simple \"on/off\" switch) in a delta-sigma loop operating at multiples of the sampling frequency. The combination is equivalent to a DAC with a larger number of bits (usually 16-20).\nThe advantages of this type of converter are high linearity combined with low cost, owed to the fact that most of the processing takes place in the digital domain and requirements for the analog anti-aliasing filter after the output can be relaxed. For these reasons, this design is very popular in digital consumer electronics (CD/DVD players, set-top boxes and the like).\n\n"}
{"id": "3247710", "url": "https://en.wikipedia.org/wiki?curid=3247710", "title": "AN/ALE-50 towed decoy system", "text": "AN/ALE-50 towed decoy system\n\nThe AN/ALE-50 towed decoy system was developed by Raytheon to protect multiple US military aircraft from radar-guided missiles. The AN/ALE-50 towed decoy system is an anti-missile countermeasures decoy system used on multiple U.S. Air Force, Navy, and Marine Corps aircraft, and by certain non-United States air forces. The system is manufactured by Raytheon Space and Airborne Systems at its facility in Goleta, California. The ALE-50 system consists of a launcher and launch controller installed on the aircraft (usually on a wing pylon), and one or more expendable towed decoys. Each decoy is delivered in a sealed canister and has a ten-year shelf life.\n\nWhen deployed, the decoy is towed behind the host aircraft, protecting the aircraft and its crew against RF-guided missiles by luring the missile toward the decoy and away from the intended target. In both flight tests and actual combat, the ALE-50 has successfully countered numerous live firings of both surface-to-air and air-to-air missiles. U.S. military pilots have nicknamed the decoy \"Little Buddy\".\n\nThe ALE-50 was first deployed in 1995, but is also used on the F/A-18E/F Super Hornet and the B-1B Lancer. The ALE-50 has also been integrated into the next-generation ALQ-184(V)9 ECM pod, creating an integrated threat-protection system that can be carried on a larger number of platforms.\n\nThe ALE-50 expendable decoys’ estimated value is $22,000 each. The latest production run of 1,048 units will be delivering through October 2010. \n\nThe ALE-50 towed decoy has provided combat-proven aircraft protection against RF missile threats in Kosovo, Afghanistan, and Iraq. Featuring low acquisition and life-cycle cost, the system adaptability enables installation and operation on virtually any airborne platform. The ALE-50 towed decoy is currently operational on the F-16, F/A-18E/F, and B-1B aircraft.\n"}
{"id": "21987776", "url": "https://en.wikipedia.org/wiki?curid=21987776", "title": "Accessaphone", "text": "Accessaphone\n\naccessaphone, first introduced in 2005 is CTI (Computer Telephony Integration) enabled software that provides better user access to desk and soft phones via the keyboard, mouse and/or voice commands. The application is specifically useful for users with various vision and mobility abilities.\n\nAs an example, when using an enterprise phone from Cisco, NEC or Tadiran Telecom, functions like Dial, Hold and Transfer can be executed via keyboard hot keys D, H and T (preceded with the ALT key) respectively. accessaphone uses Microsoft Windows Telephony Application Programming Interface (TAPI) to communicate with the Enterprise Phone System.\n\nOne of the ways in which accessaphone makes phone systems accessible and Section 508 compliant is by providing Audible Caller ID. This is a requirement on the Voluntary Product Accessibility Template. This is specifically useful for users with vision loss as the identification of an incoming call gets announced with the use of the software. The United States Access Board - the agency who is responsible for Section 508 - deployed accessaphone along with a Cisco phone system for their employees who required certain access to specific phone features like Audible Caller ID or those employees with physical impairments who needed voice access to the phone.\n\nIn 2009, The American Foundation for the Blind did a case study and determined that accessaphone was compliant with Section 508 and did in fact enhance Voice over IP telephony equipment for users needing certain access to that equipment.\n\n"}
{"id": "99431", "url": "https://en.wikipedia.org/wiki?curid=99431", "title": "Advanced Encryption Standard process", "text": "Advanced Encryption Standard process\n\nThe Advanced Encryption Standard (AES), the symmetric block cipher ratified as a standard by National Institute of Standards and Technology of the United States (NIST), was chosen using a process lasting from 1997 to 2000 that was markedly more open and transparent than its predecessor, the Data Encryption Standard (DES). This process won praise from the open cryptographic community, and helped to increase confidence in the security of the winning algorithm from those who were suspicious of backdoors in the predecessor, DES.\n\nA new standard was needed primarily because DES has a relatively small 56-bit key which was becoming vulnerable to brute-force attacks. In addition, the DES was designed primarily for hardware and is relatively slow when implemented in software. While Triple-DES avoids the problem of a small key size, it is very slow even in hardware, it is unsuitable for limited-resource platforms, and it may be affected by potential security issues connected with the (today comparatively small) block size of 64 bits.\n\nOn January 2, 1997, NIST announced that they wished to choose a successor to DES to be known as AES. Like DES, this was to be \"an unclassified, publicly disclosed encryption algorithm capable of protecting sensitive government information well into the next century.\" However, rather than simply publishing a successor, NIST asked for input from interested parties on how the successor should be chosen. Interest from the open cryptographic community was immediately intense, and NIST received a great many submissions during the three-month comment period.\n\nThe result of this feedback was a call for new algorithms on September 12, 1997. The algorithms were all to be block ciphers, supporting a block size of 128 bits and key sizes of 128, 192, and 256 bits. Such ciphers were rare at the time of the announcement; the best known was probably Square.\n\nIn the nine months that followed, fifteen different designs were created and submitted from several different countries. They were, in alphabetical order: CAST-256, CRYPTON, DEAL, DFC, E2, FROG, HPC, LOKI97, MAGENTA, MARS, RC6, Rijndael, SAFER+, Serpent, and Twofish.\n\nIn the ensuing debate, many advantages and disadvantages of the different candidates were investigated by cryptographers; they were assessed not only on security, but also on performance in a variety of settings (PCs of various architectures, smart cards, hardware implementations) and on their feasibility in limited environments (smart cards with very limited memory, low gate count implementations, FPGAs).\n\nSome designs fell due to cryptanalysis that ranged from minor flaws to significant attacks, while others lost favour due to poor performance in various environments or through having little to offer over other candidates. NIST held two conferences to discuss the submissions (AES1, August 1998 and AES2, March 1999), and in August 1999 they announced that they were narrowing the field from fifteen to five: MARS, RC6, Rijndael, Serpent, and Twofish. All five algorithms, commonly referred to as \"AES finalists\", were designed by cryptographers considered well-known and respected in the community.\nThe AES2 conference votes were as follows:\n\nA further round of intense analysis and cryptanalysis followed, culminating in the AES3 conference in April 2000, at which a representative of each of the final five teams made a presentation arguing why their design should be chosen as the AES.\n\nOn October 2, 2000, NIST announced that Rijndael had been selected as the proposed AES and started the process of making it the official standard by publishing an announcement in the Federal Register on February 28, 2001 for the draft FIPS to solicit comments. On November 26, 2001, NIST announced that AES was approved as FIPS PUB 197.\n\nNIST won praises from the cryptographic community for the openness and care with which they ran the standards process. Bruce Schneier, one of the authors of the losing Twofish algorithm, wrote after the competition was over that \"I have nothing but good things to say about NIST and the AES process.\"\n\n\n"}
{"id": "39634010", "url": "https://en.wikipedia.org/wiki?curid=39634010", "title": "Aqua Horological Tintinnabulator", "text": "Aqua Horological Tintinnabulator\n\nThe Aqua Horological Tintinnabulator (also known as the Victoria Centre Clock or the Emett Clock or The Time Fountain is a 'water-powered' clock. From 1973 to 2010 it was installed on the ground floor at the Victoria Centre in Nottingham, England. In 2015 it was reinstalled in the shopping centre on the first floor.\n\nIt was commissioned by Capital and Counties in 1970 and designed and built by kinetic sculptor Rowland Emett. A photograph shows a pencilled note on a whitewashed beam in Emett's barn:\"26th August 1970 (1/2 Closing Day) construction of the fountain started\". Installation commenced late 1972 (the year the Victoria Centre opened) and was completed before 20 February 1973. The foundation stone reads \"THE VICTORIA CENTRE TIME FOUNTAIN FEBRUARY 20TH 1973 BY EMETT.\" In its original design, this clock played Rameau's Gigue en Rondeau II from the E-minor suite of his \"Pièces de Clavecin\" when striking the hour and half hour. The clock parts were designed by Thwaites & Reed.\n\nIt is 23ft high and has become an icon of Nottingham.\n\nBetween its installation in 1973 and 2010, around £250,000 in coins had been thrown into the fountain.\n\nIn November 2011, local engineer Pete Dexter contacted the Centre management to ask why the clock appeared to be defunct. He was allowed to investigate, leading to his design and build of a new electrical control system for the animation. He also re-instated the original music, replacing an audio cassette tape system with one based on an audio CD.\nIn February 2014, the clock was dismantled by Pete Dexter and technicians from intu Victoria Centre and taken to Croxall and other locations near Tamworth where Pete Dexter and the Rowland Emett Society carried out some initial refurbishment of the parts. In particular the frame was restored to its original configuration by adding many sections of tube, some of which were to raise the height by around 400mm. This height had been lost through two occasions where the legs buried in concrete had been sliced off to move the clock. The frame colour was restored to aircraft grey-green based on a paint chip discovered during dismantling. Pete Dexter designed and built a mechanical drive for the cobweb wheel, based on original photographs. The clock was assembled as a dry installation in Millennium Point, Birmingham. This formed part of a wider exhibition in Birmingham of the works of Rowland Emett.\nAfter around three months, it was dismantled and put back into storage until December 2014, when the parts were transported back to Nottingham. Between January and June 2015, Pete Dexter and staff at the Victoria Centre carried out significant further renovation.\nThe clock was reassembled on the upper mall north end of the Victoria Centre and officially restarted on 17 June 2015. It sits in a purpose-built basin equipped with a newly designed water system, reinstating a fountain of twelve water jets, as had existed originally.\n\nIt is topped by four silver-white clock faces with golden hands and numerals. The animated sculpture underneath consists of a sunflower with 36 copper petals, partially obscuring an orchestra of six jewelled players; three squirrels and three birds. At fifteen-minute intervals, a bell strikes followed by a performance; the petals are lowered to reveal the players, the entire orchestra rotates and each player spins on its vertical axis. The music for harpsichord \"Gigue en Rondeau II\" plays for around 75 seconds after which the petals are raised and the animation ceases. A two-metre diameter cobweb wheel carrying jewelled butterflies and frogs rotates continuously during the daytime as does the 'Top Feature' which consists of three arms with jewelled pendants: one is a flying fish ridden by a squirrel, the second is a squirrel pushing a pram with a bird's nest and birds above, the third is a peacock rowing a boat with a squirrel on the rudder. In the middle of the sculpture is a floral bouquet sitting on a large copper leaf, adorned by jewelled birds. The clock originally struck and performed on the hour and half hour but was modified at some point to perform every fifteen minutes.\nMotive power is provided by four 230 Volt AC Parvalux geared motors (cobweb wheel, Top Feature, petals and orchestra) and two 230 Volt AC Crouzet synchronous motors (going and strike trains). The bell solenoid is powered by 240VDC pulses. Electrical control of the motors is via 12 Volt DC relays.\n"}
{"id": "2331819", "url": "https://en.wikipedia.org/wiki?curid=2331819", "title": "Beaker (Muppet)", "text": "Beaker (Muppet)\n\nBeaker is a Muppet character from \"The Muppet Show\". He is the shy, long-suffering assistant of Dr. Bunsen Honeydew, and is likewise named after a piece of laboratory equipment.\n\nDuring the first season of \"The Muppet Show\", Dr. Honeydew presented the Muppet Labs segments by himself; Beaker was added as his lab assistant from the second season on. Beaker has bulging eyes, a shock of red hair, and a drawbridge mouth which serves as a frown. He was originally puppeteered and voiced by Richard Hunt until Hunt's death in 1992, when the role was taken over primarily by Steve Whitmire. After Whitmire was fired in 2016, David Rudman took over the character.\n\nBeaker is a magnet for disaster; he routinely experiences mishaps such as being blown up, electrocuted, eaten by large monsters, or afflicted with awkward side effects caused by Dr. Bunsen Honeydew's experiments. Beaker communicates in a nervous, high-pitched squeak that sounds like \"Mee-mee-mee mee\". In books and merchandise, the sound is spelled \"Meep\". In \"The Muppet Movie\" he appeared to say something other than \"mee\" or \"meep\" (he \"meeps\" Honeydew's previous line \"sadly temporary\"). Although he can say normal words at times, such as \" Bye-bye.\" His tone or expression helps to communicate his meaning. \n\nBeaker rapidly became a favorite with audiences, who both sympathized with and enjoyed laughing at his humorous sufferings. Occasionally, Beaker was able to take revenge, particularly in a segment when he inadvertently made numerous copies of himself and spent the rest of the episode chasing Dr. Honeydew around the theater. In the 2008 TV special, \"\", Beaker is even more fortunate when he tests a wish machine and gets the company of model Petra Němcová, and not only refuses Honeydew's order to send her back, but then also successfully teleports away with her to apparently enjoy his first wish for the rest of the story...\n\nBeaker has also appeared as a musical performer, singing \"Danny Boy,\" \"Carol of the Bells,\" and \"Habanera\" with the Swedish Chef and Animal, and \"Feelings\" and \"Dust in the wind\" solo. He also sang \"Ode to Joy\" with his clones who were accidentally formed by Dr. Honeydew's copying machine. The \"Danny Boy\" performance was marked by the Chef's singing in his trademark gibberish and Animal's inability to remember anything but the first three words. For \"Feelings,\" Animal had to shush the increasingly unruly crowd (who tormented the Muppets throughout the episode) so Beaker could finish: \"QUIET!!...Thank you.\" In the 2011 film \"The Muppets\", Beaker sang a comedic a cappella version of Nirvana's \"Smells Like Teen Spirit\" as part of a barbershop quartet with Sam the Eagle, Rowlf the Dog and Link Hogthrob. Because of Disney's designation of the Muppets franchise as being for family audiences, Beaker was given a crucial role replacing the song's more questionable lines like \"a mulatto\" and \"my libido\" with \"mee-mee-mee-mo.\"\n\nThe two scientists were later incorporated into the \"Muppet Babies\" animated series. Howie Mandel (during early seasons) and Dave Coulier voiced Bunsen, and Frank Welker provided Beaker's squeaky meeps. Beaker was performed by Kevin Clash in \"The Muppet Show Live\". An animated Beaker was also voiced by Richard Hunt, his usual performer during that period, when he appeared in the short lived \"Little Muppet Monsters\" series.\n\nBeaker's \"meep\" sound has become a well-known catchphrase, and is referenced in various media.\n\nIn a 2004 Internet poll sponsored by the BBC and the British Association for the Advancement of Science, Beaker and Dr. Bunsen Honeydew were voted Britain's favourite cinematic scientists. They beat Mr. Spock, their closest rival, by a margin of two to one, winning a third of the total votes.\n\nBeaker features in an episode of WWE Raw, assisting Santino Marella ringside in his match with Jack Swagger by providing him with a special energy drink formulated by Dr. Honeydew. In the same episode, Beaker is revealed to be distantly related to WWE wrestler Sheamus.\n\nUK politicians \nDanny Alexander\nand Ed Miliband\nhave both been disparagingly likened to Beaker in terms of their appearance.\n\n\n"}
{"id": "6301158", "url": "https://en.wikipedia.org/wiki?curid=6301158", "title": "Buoyancy aid", "text": "Buoyancy aid\n\nBuoyancy aids are a specialist form of \"personal flotation device\" (PFD) used most commonly by kayakers, canoeists and dinghy sailors. They are designed as a flotation aid, rather than a life-saving device and have several key differences to other PFD's and lifejackets. Regardless of the specification of buoyancy aids, they do not provide (nor are they intended to) the same high level of protection as lifejackets. Therefore, they should not be used as a substitute for a life-jacket, particularly where children are concerned.\n\nCanoeing and kayaking buoyancy aids are designed with mobility in mind. A buoyancy aid that does not fit properly can restrict a paddler's (kayaker's) range of movement, which could cause them to tire or prevent them from paddling properly. They typically have front and back foam buoyancy, with none or very little around the sides to allow for better arm movements.\n\nAll canoeing/kayaking buoyanith a foam core, instead of being inflatable like some life jackets. This removes the possibility of them bursting or not being activated in the case of an incapacitated paddler. The foam used is typically closed cell PVC (polyvinyl chloride), although some manufacturers are now starting to use less toxic and more recyclable materials. Older designs used vertically aligned ribs of foam all around the body, but more modern designs typically feature front and rear slabs of foam buoyancy, with the sides left clear to allow unrestricted rotation and arm movement. Most buoyancy aids are one of three basic designs:\n\n\nAll buoyancy aids include some form of strap for tightening the buoyancy aid, preventing it from coming off in the water. Many white water designs feature multiple straps on the shoulders and waist to ensure the buoyancy aid can not be swept off in fast water. They may also include pockets for storing equipment and a range of safety and rescue features. Some lower quality ones only offer a belt, and these are often poor fitting and may be designed for generic water sports rather than specifically canoeing/kayaking.\n\nThere is a large variety of designs to fit every figure and purpose. It is important to have a buoyancy aid that fits comfortably, allowing freedom of movement. It is also important that it is suitable for the chosen discipline and the grade of water being paddled. Each discipline has different requirements and although one buoyancy aid can be used for multiple disciplines, there are several factors to consider which type to choose.\n\nThese are designed with high maneuverability as a key feature, often at the expense of buoyancy. Minimalistic designs which tend to hug the body tightly and are well cut around the arms aim to allow the wearer complete freedom of motion (something important to both Slalom and Polo paddlers as well as playboaters). These vests may not be fully suitable for other purposes, such as whitewater paddling where additional buoyancy is required due to the higher flow of water. Canoe polo rules specify that the buoyancy aids must have protective buoyancy at the sides of the garment, resulting in a garment that has more overall coverage\n\nBuoyancy aids for sea and long distance touring often feature several large pockets for storing essential equipment, and many also accommodate hydration packs. They have to be comfortable to wear whilst paddling for long distances, and so typically have very low cut sides to allow the arms free movement. More recently they are being designed more and more like whitewater vests, with low cut fronts to allow the paddler to lean forward easily.\n\nWhitewater buoyancy aids are designed to provide enough buoyancy in fast whitewater rivers, should the paddler capsize and leave the kayak. They are often more bulky than Slalom/Polo vests, but are usually cut short at the front to allow the paddler to lean further forward despite the thicker foam and allow good trunk rotation for quick turns. They always have shoulder and side straps to ensure they are not swept off in fast rapids.\n\nThey typically feature one or two pockets to store basic essentials such as a rescue knife, carabiner and tow-line. Many often also feature harnesses for use in rescues, however these are often used without proper training and can become more of a danger to both the rescuer and the swimmer if used incorrectly.\n\nAs an example, WW harnesses require a significant load to release effectively, (usually provided by the force of the water, when a rescuer in the river is held by rope from the bank). Without this load the harness may not release. So it is not a good idea to use these for low load situations, (e.g. towing kayaks), and in an incident you really need to get the rescuer to release from the harness, and not allow the person on the bank to release their end as the rescuer would end up swimming down river with 30+ metres of slack rope on their back. Training is vital\n\nBuoyancy aids come with a set of specifications detailing their sizing, weight range, and the standards they adhere to. This is often in a table printed onto the inside of the buoyancy aid, or a label on the bag/box. Most companies give the specifications for each of their models on all of their buoyancy aids, to simplify the manufacturing process, so it is important to check the model and size range of your particular buoyancy aid.\n\nBuoyancy aids should detail:\nFor a buoyancy aid to be sold in the European Union, it must be C.E. tested and approved.\n\n\nBuoyancy aids with providing only 50 N of force should only be used by swimmers in sheltered waters when help is close at hand. They are not guaranteed to turn a person from a face-down position in the water.\n\nThe foam used in buoyancy aids does degrade with age. Manufacturers add additional foam to ensure that their buoyancy aids will still provide enough force even after years of use. A rough life expectancy is three years of use, although buoyancy aids exposed to polluted water may degrade faster than normally expected.\nIn practice Buoyancy aids far exceed a 3-year service life expectancy. they can be tested for buoyancy using a weight (for adult sizes this is about 5.5 kg)The test is simple, attach weight, squeeze out air, drop in water and force it under until bubbles stop rising, then let it go. and if the BA/PFD surfaces it's OK, many websites publish guidance on this as the actual weight required is different in some situations. (In the UK the Scouts produce useful guidance).\nThe other issues are damage, stitching and wear. BA/PFDs need regular and routine inspection of any belts, stitching and other parts. this wear and damage will serve as valuable indicators on when to retire your PFD/BA\n\nMany white water buoyancy aids, as well as some sea kayak ones, feature a safety harness. These fit around the buoyancy aid below the arms and can be used for Live Bait Rescues (where a rescuer swims out into the river on a line to rescue a swimmer) or for anchoring a belayer onto the bank. They feature quick-release buckles to quickly remove the harness, and these are often (but not always) designed to release automatically past a certain load/pressure. Often harnesses have a metal ring at the back for attaching a rope with a screwgate carabiner. Use of a non-locking carabiner can allow the carabiner to accidentally clip onto other straps on the buoyancy aid during use, thus preventing the harness from being released, and as such they should not be used for attaching to a harness.\n\nMany harnesses feature a metal friction plate which stops the belt from slipping under high-load situations, however these can also prevent the harness coming off when swimming and as such present more of a hazard than an aid. It is often recommended that the belt be removed from within the friction plate and only threaded back through if the situation requires an anchored belay.\n\nImprovised harnesses (ropes around the waist, belts, or attaching ropes directly onto the buoyancy aid itself) can present a serious danger to the wearer, as they can not be released or may not be strong enough to withstand the load required.\n\nIn recent years pet buoyancy aids have become quite popular. A web search or visit to a pet supply store will reveal many types and brands of pet buoyancy aids including full life jackets, vests, and flotation collars. The purpose, design, and composition of these buoyancy aids are very similar to the human version with the major difference being that pet buoyancy aids are not usually required by law, also they are generally not certified by the Coast Guard or any other government department.\n\nAncient instances of the life jacket can be traced back to simple blocks of wood or cork used by Norwegian seamen. The modern life jacket is generally credited to one Captain Ward, a Royal National Lifeboat Institution inspector in the United Kingdom, who, in 1854, created a cork vest to be worn by lifeboat crews for both weather protection and buoyancy.\n"}
{"id": "4541", "url": "https://en.wikipedia.org/wiki?curid=4541", "title": "Burnt-in timecode", "text": "Burnt-in timecode\n\nBurnt-in timecode (often abbreviated to BITC by analogy to VITC) is a human-readable on-screen version of the timecode information for a piece of material superimposed on a video image. BITC is sometimes used in conjunction with \"real\" machine-readable timecode, but more often used in copies of original material on to a non-broadcast format such as VHS, so that the VHS copies can be traced back to their master tape and the original time codes easily located. \n\nMany professional VTRs can \"burn\" (overlay) the tape timecode onto one of their outputs. This output (which usually also displays the setup menu or on-screen display) is known as the \"super out\" or \"monitor out\". The \"character\" switch or menu item turns this behaviour on or off. The \"character\" function is also used to display the timecode on the preview monitors in linear editing suites.\n\nVideotapes that are recorded with timecode numbers overlaid on the video are referred to as \"window dubs\", named after the \"window\" that displays the burnt-in timecode on-screen.\n\nWhen editing was done using magnetic tapes that were subject to damage from excessive wear, it was common to use a window dub as a working copy for the majority of the editing process. Editing decisions would be made using a window dub, and no specialized equipment was needed to write down an edit decision list which would then be replicated from the high-quality masters.\n\nTimecode can also be superimposed on video using a dedicated overlay device, often called a \"window dub inserter\". This inputs a video signal and its separate timecode audio signal, reads the timecode, superimposes the timecode display over the video, and outputs the combined display (usually via composite), all in real time. Stand-alone timecode generator / readers often have the window dub function built-in. \n\nSome consumer cameras, in particular DV cameras, can \"burn\" (overlay) the tape timecode onto the composite output. This output typically is semi-transparent and may include other tape information. It is usually activated by turning on the 'display' info in one of the camera's sub-menus. While not as 'professional' as an overlay as created by a professional VCRs, it is a cheap alternative that is just as accurate.\n\nTimecode is stored in the metadata areas of captured DV AVI files, and some software is able to \"burn\" (overlay) this into the video frames. For example, DVMP Pro is able to \"burn\" timecode or other items of DV metadata (such as date and time, iris, shutter speed, gain, white balance mode, etc.) into DV AVI files.\n\nOCR techniques can be used to read BITC in situations where other forms of timecode are not available.\n\n"}
{"id": "674969", "url": "https://en.wikipedia.org/wiki?curid=674969", "title": "Cassone", "text": "Cassone\n\nA cassone (plural \"cassoni\") or marriage chest is a rich and showy Italian type of chest, which may be inlaid or carved, prepared with gesso ground then painted and gilded. \"Pastiglia\" was decoration in low relief carved or moulded in gesso, and was very widely used. The cassone (\"large chest\") was one of the trophy furnishings of rich merchants and aristocrats in Italian culture, from the Late Middle Ages onward. The cassone was the most important piece of furniture of that time. It was given to a bride and placed in the bridal suite. It would be given to the bride during the wedding, and it was the bride's parents' contribution to the wedding.\n\nThere are in fact a variety of different terms used in contemporary records for chests, and the attempts by modern scholars to distinguish between them remain speculative, and all decorated chests are today usually called \"cassoni\", which was probably not the case at the time. For example, a \"forziere\" probably denoted a decorated chest with a lock. \n\nSince a cassone contained the personal goods of the bride, it was a natural vehicle for painted decoration commemorating the marriage in heraldry and, when figural painted panels began to be included in the decor from the early \"quattrocento\", flattering allegory. The side panels offered a flat surface for a suitable painting, with subjects drawn from courtly romance or, much less often, religious subjects. By the 15th century subjects from classical mythology or history became the most popular. Great Florentine artists of the 15th century were called upon to decorate \"cassoni\", though as Vasari complains, by his time in the 16th century, artists thought such work beneath them. Some Tuscan artists in Siena and Florence specialized in such cassone panels, which were preserved as autonomous works of art by 19th century collectors and dealers, who sometimes discarded the cassone itself. From the late 1850s, neo-Renaissance cassoni were confected for dealers like William Blundell Spence, Stefano Bardini or Elia Volpi in order to present surviving cassone panels to clients in a more \"authentic\" and glamorous presentation. \n\nA typical place for such a cassone was in a chamber at the foot of a bed that was enclosed in curtains. Such a situation is a familiar setting for depictions of the Annunciation or the Visitation of St. Anne to the Virgin Mary. A cassone was largely immovable. In a culture where chairs were reserved for important personages, often pillows scattered upon the floor of a chamber provided informal seating, and a cassone could provide both a backrest and a table surface. The symbolic \"humility\" that modern scholars read into Annunciations where the Virgin sits reading upon the floor, perhaps underestimates this familiar mode of seating.\n\nAt the end of the 15th century, a new classicising style arose, and early Renaissance cassoni of central and northern Italy were carved and partly gilded, and given classical décor, with panels flanked by fluted corner pilasters, under friezes and cornices, or with sculptural panels in high or low relief. Some early to mid-sixteenth-century cassoni drew their inspiration from Roman sarcophagi (\"illustration, right\"). By the mid-sixteenth century Giorgio Vasari could remark on the old-fashioned cassoni with painted scenes, examples of which could be seen in the palazzi of Florentine families.\n\nA cassone that has been provided with a high panelled back and sometimes a footrest, for both hieratic and practical reasons, becomes a \"cassapanca\" (\"chest-bench\"). \"Cassapanche\" were immovably fixed in the main public room of a palazzo, the \"sala\" or \"salone.\" They were part of the \"immobili\" (\"unmoveables\"), perhaps even more than the removable glazed window casements, and might be left in place, even if the palazzo passed to another family.\n\n\n"}
{"id": "51623139", "url": "https://en.wikipedia.org/wiki?curid=51623139", "title": "Clinatec", "text": "Clinatec\n\nClinatec is a biomedical research center based at the Polygone Scientifique in Grenoble. Doctors, biologists and experts work side-by-side at the 6,000 m² facility. Around a hundred researchers and employees work at the center. When it opened at the end of 2011, it was hailed as the first center of its kind in the world. With six hospital rooms, cutting-edge medical imaging equipment and an operating suite, Clinatec was developed by the Research Division of the CEA (French Alternative Energies and Atomic Energy Commission), Grenoble-Alpes University Hospital (CHU), Inserm and the Université Grenoble Alpes. The primary focus is on cancer, neurodegenerative diseases and disability.\n\nProfessor Alim Louis Benabid and Jean Therme first met back in 2006.\n\nAlim Louis Benabid is a neurosurgeon. Together with Professor Pierre Pollak, he developed a new treatment for Parkinson's disease, deep brain stimulation. His work received recognition in 2014 with the Lasker Award and in 2016 with the European Inventor Award.\n\nJean Therme is Director of Technological Research at CEA Grenoble. He has worked tirelessly to make Grenoble a key center of expertise in electronics and , and to encourage close collaboration between the worlds of research and industry.\n\nBoth men were convinced that millions of lives could be transformed by merging medical research and technology R&D. They agreed that the way to achieve this would be to bring together doctors, researchers, biologists, engineers, robotics engineers, mathematicians and knowledge engineers to work at a single site.\n\nIn 2010–11, the 6,000 m² building was built and equipped.\n\nIn 2013, the first patient was admitted to the center, in relation to the \"Protool\" clinical trial coordinated by Professor Berger. For the first time, it was possible to use non-invasive procedures to explore regions of the brain that had hitherto been inaccessible. The exciting prospects opened up by this technology, the result of the coupling of technology developed at with the clinical approach taken by researchers at Grenoble-Alpes University Hospital (CHU), Inserm and the Université Grenoble Alpes, led to setting up the MedPrint start-up, which won an I-lab award in 2015.\n\nIn 2014, Clinatec launched a campaign to raise 30 million euros in order to develop a number of projects, including: \n\nClinatec is the outcome of a solid partnership between the CEA (French Alternative Energies and Atomic Energy Commission), Grenoble-Alpes University Hospital (CHU), Inserm and the Université Grenoble Alpes.\n\nThe Clinatec endowment fund was set up in 2014. A sponsorship campaign has been launched, aiming to raise 30 million euros by 2018.\n\nProfessor Alim Louis Benabid, Clinatec's founder and Chairman of the Board, member of the French Academy of Sciences, is one of the joint winners of the prestigious Albert-Lasker Award for clinical research awarded by the Albert and Mary Lasker Foundation in New York. Professors Alim-Louis Benabid and Mahlon R. Delong of Emory University received awards for their contributions to the development of deep brain stimulation. The technique consists in stimulating the subthalamic nucleus to inhibit tremors and restore motor function in patients suffering from Parkinson's disease and from complications caused by Levodopa, a drug widely used in treating the disease.\n\nSet up in 2013 by Mark Zuckerberg and Priscilla Chan of Facebook, Sergey Brin, founder of Google, and Yuri Milner and Anne Wojcicki, the founders of 23andMe, the Breakthrough Prize in Life Sciences is awarded to researchers whose work extends human life expectancy.\nProfessor Alim Louis Benabid was awarded the prize in 2015 for the development of deep brain stimulation, a technique which has revolutionized the treatment of Parkinson’s disease.\n\nOn June 9, 2016, Professor Alim Louis Benabid received the European Inventor Award 2016 for his work on deep brain stimulation. The technique, now used all over the world, has radically transformed the lives of more than 150,000 people suffering from Parkinson's disease, and significantly improved their quality of life.\n\nIn July 2012, Philippe Pozzo di Borgo, rendered quadriplegic following an accident and the man whose story was the inspiration for the French film Intouchables, visited Clinatec with a group of local journalists. He expressed his confidence in and admiration for this research program in a report aired on regional news channel, France 3 Alpes\n\n\n"}
{"id": "19857803", "url": "https://en.wikipedia.org/wiki?curid=19857803", "title": "Clyde Broadcast", "text": "Clyde Broadcast\n\nClyde Broadcast is a brand associated with the manufacture and supply of studio equipment for the radio broadcast industry. The name dates back to 'Clyde Electronics' who manufactured a series of analog mixers known as Alpha, Beta, Prima and Presenter. \n\nIn 1997 Clyde Broadcast Products Limited was formed as a UK-based radio studio equipment manufacturer. Clyde Broadcast also designed and installed radio broadcast and production facilities for the education sector under the name Synergy School Radio.\n\nThe brand 'Clyde Broadcast' and 'Synergy School Radio' were purchased by Clyde Broadcast technology Ltd in September 2018 as part of the sale of intellectual property and fixed assets of Clyde Broadcast Products Ltd by Keith V Anderson of MLM Solutions.\n\n"}
{"id": "7878457", "url": "https://en.wikipedia.org/wiki?curid=7878457", "title": "Computer", "text": "Computer\n\nA computer is a device that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming. Modern computers have the ability to follow generalized sets of operations, called \"programs.\" These programs enable computers to perform an extremely wide range of tasks.\n\nComputers are used as control systems for a wide variety of industrial and consumer devices. This includes simple special purpose devices like microwave ovens and remote controls, factory devices such as industrial robots and computer-aided design, and also general purpose devices like personal computers and mobile devices such as smartphones.\n\nEarly computers were only conceived as calculating devices. Since ancient times, simple manual devices like the abacus aided people in doing calculations. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The speed, power, and versatility of computers have been increasing dramatically ever since then.\n\nConventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.\n\nAccording to the \"Oxford English Dictionary\", the first known use of the word \"computer\" was in 1613 in a book called \"The Yong Mans Gleanings\" by English writer Richard Braithwait: \"I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number.\" This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. Originally, women were often hired as \"human computers\" because they could be paid less than their male counterparts. By 1943, most human computers were women. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.\n\nThe \"Online Etymology Dictionary\" gives the first attested use of \"computer\" in the \"1640s, [meaning] \"one who calculates,\"; this is an \"... agent noun from compute (v.)\". The \"Online Etymology Dictionary\" states that the use of the term to mean \"calculating machine\" (of any type) is from 1897.\" The \"Online Etymology Dictionary\" indicates that the \"modern use\" of the term, to mean \"programmable digital electronic computer\" dates from \"... 1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine\".\n\nDevices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example.\n\nThe abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.\nThe Antikythera mechanism is believed to be the earliest mechanical analog \"computer\", according to Derek J. de Solla Price. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to . Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.\n\nMany mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century. The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235. Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe, an early fixed-wired knowledge processing machine with a gear train and gear-wheels, .\n\nThe sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.\n\nThe planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.\nThe slide rule was invented around 1620–1630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.\n\nIn the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically \"programmed\" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.\n\nThe tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.\n\nThe differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.\n\nCharles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the \"father of the computer\", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.\n\nThe machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the \"mill\") in 1888. He gave a successful demonstration of its use in computing tables in 1906.\n\nDuring the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers. The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.\n\nThe art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (control systems) and aircraft (slide rule).\n\nBy 1938, the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.\nEarly digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.\n\nIn 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz. Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was Turing complete.\n\nPurely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes. In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942, the first \"automatic electronic digital computer\". This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.\nDuring World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes which were often run by women. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus. He spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.\n\nColossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both 5 times faster and simpler to operate than Mark I, greatly speeding the decoding process.\nThe U.S.-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a \"program\" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the \"ENIAC girls\".\n\nIt combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.\n\nThe principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper, \"On Computable Numbers\". Turing proposed a simple device that he called \"Universal Computing machine\" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.\n\nEarly computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine. With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945, Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report \"Proposed Electronic Calculator\" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his \"First Draft of a Report on the EDVAC\" in 1945.\n\nThe Manchester Baby was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948. It was designed as a testbed for the Williams tube, the first random-access digital storage device. Although the computer was considered \"small and primitive\" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. Grace Hopper was the first person to develop a compiler for programming language.\n\nThe Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer. Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam. In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 and ran the world's first regular routine office computer job.\n\nThe bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the \"second generation\" of computers.\nCompared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.\n\nAt the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell.\n\nThe next great advance in computing power came with the advent of the integrated circuit.\nThe idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.\n\nThe first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as \"a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated\". Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.\n\nThis new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term \"microprocessor\", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.\n\nThe first mobile computers were heavy and ran from mains power. The 50lb IBM 5100 was an early example. Later portables such as the Osborne 1 and Compaq Portable were considerably lighter, but still needed to be plugged in. The first laptops, such as the Grid Compass, removed this requirement by incorporating batteries – and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s. The same developments allowed manufacturers to integrate computing resources into cellular phones.\n\nThese smartphones and tablets run on a variety of operating systems and soon became the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.\n\nComputers are typically classified based on their uses:\n\n\n\nThe term \"hardware\" covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and \"mice\" input devices are all hardware.\n\nA general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires.\nInside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a \"1\", and when off it represents a \"0\" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.\n\nWhen unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:\n\nThe means through which computer gives output are known as output devices. Some examples of output devices are:\n\nThe control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer. Control systems in advanced computers may change the order of execution of some instructions to improve performance.\n\nA key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.\n\nThe control system's function is as follows—note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:\n\n\nSince the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as \"jumps\" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).\n\nThe sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.\n\nThe control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid-1970s CPUs have typically been constructed on a single integrated circuit called a \"microprocessor\".\n\nThe ALU is capable of performing two classes of operations: arithmetic and logic. The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) while others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (\"is 64 greater than 65?\"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.\n\nSuperscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously. Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.\n\nA computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered \"address\" and can store a single number. The computer can be instructed to \"put the number 123 into the cell numbered 1357\" or to \"add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595.\" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.\n\nIn almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (2 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.\n\nThe CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.\n\nComputer main memory comes in two principal varieties:\nRAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.\n\nIn more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.\n\nI/O is the means by which a computer exchanges information with the outside world. Devices that provide input or output to the computer are called peripherals. On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.\nI/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics. Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry.\n\nWhile a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn. One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running \"at the same time\". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed \"time-sharing\" since each program is allocated a \"slice\" of time in turn.\n\nBefore the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a \"time slice\" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.\n\nSome computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.\n\nSupercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general purpose computers. They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called \"embarrassingly parallel\" tasks.\n\n\"Software\" refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. It is often divided into system software and application software]] Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called \"firmware\".\n\nThere are thousands of different programming languages—some intended to be general purpose, others useful only for highly specialized applications.\n\nThe defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.\n\nThis section applies to most common RAM machine–based computers.\n\nIn most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called \"jump\" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that \"remembers\" the location it jumped from and another instruction to return to the instruction following that jump instruction.\n\nProgram execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.\n\nComparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:\nOnce told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.\n\nIn most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program, architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.\n\nWhile it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers, it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.\n\nProgramming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.\n\nMachine languages and the assembly languages that represent them (collectively termed \"low-level programming languages\") tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC.\n\nAlthough considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually \"compiled\" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler. High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.\n\nFourth-generation languages (4GL) are less procedural than 3G languages. The benefit of 4GL is that they provide ways to obtain information without requiring the direct help of a programmer.\n\nProgram design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies.\nThe task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.\n\nErrors in computer programs are called \"bugs\". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to \"hang\", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.\nAdmiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term \"bugs\" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.\n\nFirmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.\n\nComputers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre. In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET. The technologies that made the Arpanet possible spread and evolved.\n\nIn time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. \"Wireless\" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.\nA computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word \"computer\" is synonymous with a personal electronic computer, the modern definition of a computer is literally: \"\"A device that computes\", especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.\" Any device which \"processes information\" qualifies as a computer, especially if the processing is purposeful.\n\nHistorically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example. More realistically, modern computers are made out of transistors made of photolithographed semiconductors.\n\nThere is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.\n\nThere are many types of computer architectures:\n\nOf all these abstract machines, a quantum computer holds the most promise for revolutionizing computing. Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.\n\nA computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning. Artificial intelligence based products generally fall into two major categories: rule based systems and pattern recognition systems. Rule based systems attempt to represent the rules used by human experts and tend to be expensive to develop. Pattern based systems use data about a problem to generate conclusions. Examples of pattern based systems include voice recognition, font recognition, translation and the emerging field of on-line marketing.\n\nAs the use of computers has spread throughout society, there are an increasing number of careers involving computers.\nThe need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.\n\n\n"}
{"id": "44339868", "url": "https://en.wikipedia.org/wiki?curid=44339868", "title": "Contraves Cora", "text": "Contraves Cora\n\nThe Cora was a digital fire control system designed by Peter Toth and produced by the Swiss company Contraves.\n\nPeter Toth started the design in 1957, and the system was fielded for anti-aircraft fire direction with the Swiss Army in the 60s.\n\nOne copy of the system was used at the EPFL for cartography, and was put on display during Expo64. The unit was rediscovered in storage in 2011, and is now on display at the Musée Bolo, in the Computer Science department of the EPFL.\n\n"}
{"id": "8827593", "url": "https://en.wikipedia.org/wiki?curid=8827593", "title": "Definity (film recorder)", "text": "Definity (film recorder)\n\nDefinity is a digital film recorder for motion picture applications, produced by CCG Digital Image Technology, the successor of Agfa-Gevaert's film recorder division.\n\nLaunched at NAB in Las Vegas in 2004, Definity marked a departure from previous, more analogue technologies of recording digital sequences onto motion picture film as it utilizes a monochrome high-resolution LCD panel. Before Definity's launch, LCD technology was commonly regarded as unsuitable for film recording purposes, despite its great potential: Main problems were limitations in bit depth, temporary image retention (TIR) and insufficiently saturated colors.\n\nDefinity's imaging unit exploits the sub-pixel structure of the LCD in order to record anamorphic \"CinemaScope\" material without loss of horizontal image information: When instead of the standard square pixels the device utilizes its native anamorphic pixels (with an aspect of 3:1), flat widescreen material with a typical aspect ratio of 2.36:1 is horizontal compressed on the LCD, thus arriving at a squeezed, 1.18:1 image required on-film without having to jettison vital image information.\n\nSince 2010 CCG launched a new full LED backlight system to increase the contrast and Density range by using 4320 separate RGB LEDs, increasing speed and Contrast ratio.\n\n\n"}
{"id": "43955598", "url": "https://en.wikipedia.org/wiki?curid=43955598", "title": "Door control unit", "text": "Door control unit\n\nIn the field of automotive electronics, Door Control Unit (DCU) is a generic term for an embedded system that controls a number of electrical systems associated with an advanced motor vehicle. An advanced motor vehicle consists of a number of ECUs (Electrical Control Units), and the Door Control Unit (DCU) is an important one among them.\n\nThe Door Control Unit is responsible for controlling and monitoring various electronic accessories in a vehicle's door. Since most of the vehicles have more than one door, generally DCUs are present in each door separately. A DCU associated with the driver's door has some additional functionalities. This additional features are the result of complex functions like locking, driver door switch pad, child lock switches, etc., which are associated with the driver's door. In most of the cases driver door module acts as a master and others act as slaves in communication protocols.\n\nIn some advanced motor vehicles, luxury features like puddle lamps and BLIS (Blind spot Indicator System) are also supported by DCUs.\n"}
{"id": "11918162", "url": "https://en.wikipedia.org/wiki?curid=11918162", "title": "Environmental engineering law", "text": "Environmental engineering law\n\nEnvironmental engineering law is a profession that requires an expertise in both environmental engineering and law. This field includes professionals with both a legal and environmental engineering education. This dual educational requirement is typically satisfied through an ABET accredited degree in environmental engineering and an ABA accredited law degree. Likewise, this profession requires both licensure in professional environmental engineering and admittance to one bar.\n\nEnvironmental engineering law is the professional application of law, science and engineering principles to improve the environment (air, water, and/or land resources), to provide healthy water, air, and land for human habitation and for other organisms, and to remediate polluted sites. Environmental engineering lawyers seek to promote the advancement of technical engineering knowledge in the legal profession and to enhance informed legal analysis of complex environmental matters.\n\nEnvironmental engineering law professionals offer a sound knowledge base in the fields of both environmental engineering and law to address complex environmental problems which demand both professional technical practice and legal expertise. Areas of practice are continually expanding, but frequently include complex land transactions, such as: \n\n\n"}
{"id": "34231060", "url": "https://en.wikipedia.org/wiki?curid=34231060", "title": "Enyo (software)", "text": "Enyo (software)\n\nEnyo is an open source JavaScript framework for cross-platform mobile, desktop, TV and web applications emphasizing object-oriented encapsulation and modularity. Initially developed by Palm, which was later acquired by Hewlett-Packard and then released under an Apache 2.0 license. It is sponsored by LG Electronics and Hewlett-Packard.\n\n\"Bootplate\" is a simplified way of creating an app, providing a skeleton of the program's folder tree. The Bootplate template provides a complete starter project that supports source control and cross-platform deployment out of the box. It can be used to facilitate both the creation of a new project and the preparation for its eventual deployment.\n\n\nThe following projects are built with Enyo:\n\n\nPartial list of Enyo apps can be found on Enyo Apps. Some developers can be found on Enyo Developer Directory.\n\nThis is an example of a 'Hello world program' in Enyo\nIn general, Enyo can run across all relatively modern, standards-based web environments, but because of the variety of them there are three priority tiers. At 2015 some platforms supported are:\nPackaged Apps: iOS7, iOS6 (PhoneGap), Android 4+ (PhoneGap), Windows 8.1 Store App and Windows Phone 8 (PhoneGap), Blackberry 10 (PhoneGap), Chrome Web Store App, LG webOS.\n\nDesktop Browsers: Chrome (latest), Safari (latest MAC), Firefox (latest), IE11 IE10, IE9, IE8. (Win).\n\nMobile Browsers: iOS7, iOS6, Android 4+ Chrome, Kindle Fire and HD, Blackberry 10, IE11 (Windows 8.1),IE10 (Windows Phone 8).\nPackaged Apps: iOS5, iOS4, Android 2.3,Firefox OS (pre-release), Tizen OS (pre-release), Windows 8 Store App, Windows (Intel AppUp).\n\nDesktop Browsers: Opera, Chrome >10, Firefox >4, Safari >5.\n\nMobile Browsers: iOS5, iOS4, Android 4+ Firefox, webOS 3.0.5, webOS 2.2, BlackBerry 6-7, BlackBerry Playbook and others.\n\nMobile Browsers: Windows Phone 7.5.\n\nDesktop Browsers: IE8\n\nMobile Browsers: Windows Phone 7, BlackBerry 6, Symbian, Opera Mini\n\n\n"}
{"id": "23452313", "url": "https://en.wikipedia.org/wiki?curid=23452313", "title": "Fa (brand)", "text": "Fa (brand)\n\nFa is an international brand for personal care products. It is a subsidiary of German company Henkel AG. Fa products include skin care lotions, creams and gels, as well as shower gels, bubble baths, soaps and deodorants.\n\nFrom \"fabulous\" and \"thread soap\".\n\nThe first Fa product, a new bar soap, talc powders was launched in 1954 by Henkel-subsidiary Dreiring. In 1975, Henkel's first Fa-shower gel was introduced. Fa products were since then joined by new series of bar soaps, liquid soaps, shower gels, bubble baths, roll-on deodorants, stick deodorants and deosprays. As of today, Fa is marketed in some 120 countries.\n\n"}
{"id": "2075960", "url": "https://en.wikipedia.org/wiki?curid=2075960", "title": "Ferroelectric RAM", "text": "Ferroelectric RAM\n\nFerroelectric RAM (FeRAM, F-RAM or FRAM) is a random-access memory similar in construction to DRAM but using a ferroelectric layer instead of a dielectric layer to achieve non-volatility. FeRAM is one of a growing number of alternative non-volatile random-access memory technologies that offer the same functionality as flash memory.\n\nFeRAM's advantages over flash include: lower power usage, faster write performance and a much greater maximum read/write endurance (about 10 to 10 cycles). FeRAMs have data retention times of more than 10 years at +85 °C (up to many decades at lower temperatures).\nMarket disadvantages of FeRAM are much lower storage densities than flash devices, storage capacity limitations and higher cost. FeRAM read process is destructive, necessitating a write-after-read architecture (standard DRAM capacitor cells require write-after-read as well).\n\nFerroelectric RAM was proposed by MIT graduate student Dudley Allen Buck in his master's thesis, \"Ferroelectrics for Digital Information Storage and Switching,\" published in 1952. In 1955 Bell Telephone Laboratories was experimenting with ferroelectric-crystal memories.\nDevelopment of FeRAM began in the late 1980s. Work was done in 1991 at NASA's Jet Propulsion Laboratory on improving methods of read out, including a novel method of non-destructive readout using pulses of UV radiation. Much of the current FeRAM technology was developed by Ramtron, a fabless semiconductor company. One major licensee is Fujitsu, who operates what is probably the largest semiconductor foundry production line with FeRAM capability. Since 1999 they have been using this line to produce standalone FeRAMs, as well as specialized chips (e.g. chips for smart cards) with embedded FeRAMs. Fujitsu produced devices for Ramtron until 2010. Since 2010 Ramtron's fabricators have been TI (Texas Instruments) and IBM. Since at least 2001 Texas Instruments has collaborated with Ramtron to develop FeRAM test chips in a modified 130 nm process. In the fall of 2005, Ramtron reported that they were evaluating prototype samples of an 8-megabit FeRAM manufactured using Texas Instruments' FeRAM process. Fujitsu and Seiko-Epson were in 2005 collaborating in the development of a 180 nm FeRAM process. In 2012 Ramtron was acquired by Cypress Semiconductor.\nFeRAM research projects have also been reported at Samsung, Matsushita, Oki, Toshiba, Infineon, Hynix, Symetrix, Cambridge University, University of Toronto, and the Interuniversity Microelectronics Centre (IMEC, Belgium).\n\nConventional DRAM consists of a grid of small capacitors and their associated wiring and signaling transistors. Each storage element, a \"cell\", consists of one capacitor and one transistor, a so-called \"1T-1C\" device. DRAM cells scale directly with the size of the semiconductor fabrication process being used to make it. For instance, on the 90 nm process used by most memory providers to make DDR2 DRAM, the cell size is 0.22 μm², which includes the capacitor, transistor, wiring, and some amount of \"blank space\" between the various parts — it appears 35% utilization is typical, leaving 65% of the space wasted.\n\nDRAM data is stored as the presence or lack of an electrical charge in the capacitor, with the lack of charge in general representing \"0\". Writing is accomplished by activating the associated control transistor, draining the cell to write a \"0\", or sending current into it from a supply line if the new value should be \"1\". Reading is similar in nature; the transistor is again activated, draining the charge to a \"sense amplifier\". If a pulse of charge is noticed in the amplifier, the cell held a charge and thus reads \"1\"; the lack of such a pulse indicates a \"0\". Note that this process is \"destructive\", once the cell has been read. If it did hold a \"1,\" it must be re-charged to that value again. Since a cell loses its charge after some time due to leak currents, it must be actively refreshed at intervals.\n\nThe 1T-1C storage cell design in an FeRAM is similar in construction to the storage cell in widely used DRAM in that both cell types include one capacitor and one access transistor. In a DRAM cell capacitor, a linear dielectric is used, whereas in an FeRAM cell capacitor the dielectric structure includes ferroelectric material, typically lead zirconate titanate (PZT).\n\nA ferroelectric material has a nonlinear relationship between the applied electric field and the apparent stored charge. Specifically, the ferroelectric characteristic has the form of a hysteresis loop, which is very similar in shape to the hysteresis loop of ferromagnetic materials. The dielectric constant of a ferroelectric is typically much higher than that of a linear dielectric because of the effects of semi-permanent electric dipoles formed in the crystal structure of the ferroelectric material. When an external electric field is applied across a dielectric, the dipoles tend to align themselves with the field direction, produced by small shifts in the positions of atoms and shifts in the distributions of electronic charge in the crystal structure. After the charge is removed, the dipoles retain their polarization state. Binary \"0\"s and \"1\"s are stored as one of two possible electric polarizations in each data storage cell. For example, in the figure a \"1\" is encoded using the negative remnant polarization \"-Pr\", and a \"0\" is encoded using the positive remnant polarization \"+Pr\".\n\nIn terms of operation, FeRAM is similar to DRAM. Writing is accomplished by applying a field across the ferroelectric layer by charging the plates on either side of it, forcing the atoms inside into the \"up\" or \"down\" orientation (depending on the polarity of the charge), thereby storing a \"1\" or \"0\". Reading, however, is somewhat different than in DRAM. The transistor forces the cell into a particular state, say \"0\". If the cell already held a \"0\", nothing will happen in the output lines. If the cell held a \"1\", the re-orientation of the atoms in the film will cause a brief pulse of current in the output as they push electrons out of the metal on the \"down\" side. The presence of this pulse means the cell held a \"1\". Since this process overwrites the cell, reading FeRAM is a destructive process, and requires the cell to be re-written if it was changed.\n\nIn general, the operation of FeRAM is similar to ferrite core memory, one of the primary forms of computer memory in the 1960s. However, compared to core memory, FeRAM requires far less power to flip the state of the polarity and does so much faster.\n\nThe main determinant of a memory system's cost is the density of the components used to make it up. Smaller components, and fewer of them, means that more cells can be packed onto a single chip, which in turn means more can be produced at once from a single silicon wafer. This improves yield, which is directly related to cost.\n\nThe lower limit to this scaling process is an important point of comparison. In general, the technology that scales to the smallest cell size will end up being the least expensive per bit. In terms of construction, FeRAM and DRAM are similar, and can in general be built on similar lines at similar sizes. In both cases, the lower limit seems to be defined by the amount of charge needed to trigger the sense amplifiers. For DRAM, this appears to be a problem at around 55 nm, at which point the charge stored in the capacitor is too small to be detected. It is not clear as to whether FeRAM can scale to the same size, as the charge density of the PZT layer may not be the same as the metal plates in a normal capacitor.\n\nAn additional limitation on size is that materials tend to stop being ferroelectric when they are too small. (This effect is related to the ferroelectric's \"depolarization field\".) There is ongoing research on addressing the problem of stabilizing ferroelectric materials; one approach, for example, uses molecular adsorbates.\n\nTo date, the commercial FeRAM devices have been produced at 350 nm and 130 nm. Early models required two FeRAM cells per bit, leading to very low densities, but this limitation has since been removed.\n\nThe key advantage to FeRAM over DRAM is what happens \"between\" the read and write cycles. In DRAM, the charge deposited on the metal plates leaks across the insulating layer and the control transistor, and disappears. In order for a DRAM to store data for anything other than a very short time, every cell must be periodically read and then re-written, a process known as \"refresh\". Each cell must be refreshed many times every second (~65 ms) and this requires a continuous supply of power.\n\nIn contrast, FeRAM only requires power when actually reading or writing a cell. The vast majority of power used in DRAM is used for refresh, so it seems reasonable to suggest that the benchmark quoted by STT-MRAM researchers is useful here too, indicating power usage about 99% lower than DRAM. The destructive read aspect of FeRAM may put it at a disadvantage compared to MRAM, however.\n\nAnother non-volatile memory type is flash RAM, and like FeRAM it does not require a refresh process. Flash works by pushing electrons across a high-quality insulating barrier where they get \"stuck\" on one terminal of a transistor. This process requires high voltages, which are built up in a charge pump over time. This means that FeRAM could be expected to be lower power than flash, at least for writing, as the write power in FeRAM is only marginally higher than reading. For a \"mostly-read\" device the difference might be slight, but for devices with more balanced read and write the difference could be expected to be much higher.\n\nDRAM performance is limited by the rate at which the charge stored in the cells can be drained (for reading) or stored (for writing). In general, this ends up being defined by the capability of the control transistors, the capacitance of the lines carrying power to the cells, and the heat that power generates.\n\nFeRAM is based on the physical movement of atoms in response to an external field, which happens to be extremely fast, settling in about 1 ns. In theory, this means that FeRAM could be much faster than DRAM. However, since power has to flow into the cell for reading and writing, the electrical and switching delays would likely be similar to DRAM overall. It does seem reasonable to suggest that FeRAM would require less charge than DRAM, because DRAMs need to \"hold\" the charge, whereas FeRAM would have been written to before the charge would have drained. However, there is a delay in writing because the charge has to flow through the control transistor, which limits current somewhat.\n\nIn comparison to flash, the advantages are much more obvious. Whereas the read operation is likely to be similar in performance, the charge pump used for writing requires a considerable time to \"build up\" current, a process that FeRAM does not need. Flash memories commonly need a millisecond or more to complete a write, whereas current FeRAMs may complete a write in less than 150 ns.\n\nOn the other hand, FeRAM has its own reliability issues, including imprint and fatigue. Imprint is the preferential polarization state from previous writes to that state, and fatigue is increase of minimum writing voltage due to loss of polarization after extensive cycling.\n\nThe theoretical performance of FeRAM is not entirely clear. Existing 350 nm devices have read times on the order of 50–60 ns. Although slow compared to modern DRAMs, which can be found with times on the order of 2 ns, common 350 nm DRAMs operated with a read time of about 35 ns, so FeRAM performance appears to be comparable given the same fabrication technology.\n\nFeRAM remains a relatively small part of the overall semiconductor market. In 2005, worldwide semiconductor sales were US $235 billion (according to the Gartner Group), with the flash memory market accounting for US $18.6 billion (according to IC Insights). The 2005 annual sales of Ramtron, perhaps the largest FeRAM vendor, were reported to be US $32.7 million. The much larger sales of flash memory compared to the alternative NVRAMs support a much larger research and development effort. Flash memory is produced using semiconductor linewidths of 30 nm at Samsung (2007) while FeRAMs are produced in linewidths of 350 nm at Fujitsu and 130 nm at Texas Instruments (2007). Flash memory cells can store multiple bits per cell (currently 3 in the highest density NAND flash devices), and the number of bits per flash cell is projected to increase to 4 or even to 8 as a result of innovations in flash cell design. As a consequence, the areal bit densities of flash memory are much higher than those of FeRAM, and thus the cost per bit of flash memory is orders of magnitude lower than that of FeRAM.\n\nThe density of FeRAM arrays might be increased by improvements in FeRAM foundry process technology and cell structures, such as the development of vertical capacitor structures (in the same way as DRAM) to reduce the area of the cell footprint. However, reducing the cell size may cause the data signal to become too weak to be detectable. In 2005, Ramtron reported significant sales of its FeRAM products in a variety of sectors including (but not limited to) electricity meters, automotive (e.g. black boxes, smart air bags), business machines (e.g. printers, RAID disk controllers), instrumentation, medical equipment, industrial microcontrollers, and radio frequency identification tags. The other emerging NVRAMs, such as MRAM, may seek to enter similar niche markets in competition with FeRAM.\n\nTexas Instruments proved it to be possible to embed FeRAM cells using two additional masking steps during conventional CMOS semiconductor manufacture. Flash typically requires nine masks. This makes possible for example, the integration of FeRAM onto microcontrollers, where a simplified process would reduce costs. However, the materials used to make FeRAMs are not commonly used in CMOS integrated circuit manufacturing. Both the PZT ferroelectric layer and the noble metals used for electrodes raise CMOS process compatibility and contamination issues. Texas Instruments has incorporated an amount of FRAM memory into its MSP430 microcontrollers in its new FRAM series.\n\n\n"}
{"id": "1661475", "url": "https://en.wikipedia.org/wiki?curid=1661475", "title": "Green computing", "text": "Green computing\n\nGreen computing, green ICT as per International Federation of Global & Green ICT \"IFGICT\", green IT, or ICT sustainability, is the study and practice of environmentally sustainable computing or IT.\n\nThe goals of green computing are similar to green chemistry: reduce the use of hazardous materials, maximize energy efficiency during the product's lifetime, the recyclability or biodegradability of defunct products and factory waste. Green computing is important for all classes of systems, ranging from handheld systems to large-scale data centers.\n\nMany corporate IT departments have green computing initiatives to reduce the environmental effect of their IT operations.\n\nIn 1992, the U.S. Environmental Protection Agency launched Energy Star, a voluntary labeling program that is designed to promote and recognize the energy efficiency in monitors, climate control equipment, and other technologies. This resulted in the widespread adoption of sleep mode among consumer electronics. Concurrently, the Swedish organization TCO Development launched the TCO Certification program to promote low magnetic and electrical emissions from CRT-based computer displays; this program was later expanded to include criteria on energy consumption, ergonomics, and the use of hazardous materials in construction.\n\nThe Organisation for Economic Co-operation and Development (OECD) has published a survey of over 90 government and industry initiatives on \"Green ICTs\", i.e. information and communication technologies, the environment and climate change. The report concludes that initiatives tend to concentrate on the greening ICTs themselves rather than on their actual implementation to tackle global warming and environmental degradation. In general, only 20% of initiatives have measurable targets, with government programs tending to include targets more frequently than business associations.\n\nMany governmental agencies have continued to implement standards and regulations that encourage green computing. The Energy Star program was revised in October 2006 to include stricter efficiency requirements for computer equipment, along with a tiered ranking system for approved products.\n\nBy 2008, 26 US states established statewide recycling programs for obsolete computers and consumer electronics equipment. The statutes either impose an \"advance recovery fee\" for each unit sold at retail or require the manufacturers to reclaim the equipment at disposal.\n\nIn 2010, the American Recovery and Reinvestment Act (ARRA) was signed into legislation by President Obama. The bill allocated over $90 billion to be invested in green initiatives (renewable energy, smart grids, energy efficiency, etc.) In January 2010, the U.S. Energy Department granted $47 million of the ARRA money towards projects that aim to improve the energy efficiency of data centers. The projects provided research to optimize data center hardware and software, improve power supply chain, and data center cooling technologies.\n\n\nModern IT systems rely upon a complicated mix of people, networks, and hardware; as such, a green computing initiative must cover all of these areas as well. A solution may also need to address end user satisfaction, management restructuring, regulatory compliance, and return on investment (ROI). There are also considerable fiscal motivations for companies to take control of their own power consumption; \"of the power management tools available, one of the most powerful may still be simple, plain, common sense.\"\n\nGartner maintains that the PC manufacturing process accounts for 70% of the natural resources used in the life cycle of a PC. More recently, Fujitsu released a Life Cycle Assessment (LCA) of a desktop that show that manufacturing and end of life accounts for the majority of this desktop's ecological footprint. Therefore, the biggest contribution to green computing usually is to prolong the equipment's lifetime. Another report from Gartner recommends to \"Look for product longevity, including upgradability and modularity.\" For instance, manufacturing a new PC makes a far bigger ecological footprint than manufacturing a new RAM module to upgrade an existing one.\n\nData center facilities are heavy consumers of energy, accounting for between 1.1% and 1.5% of the world’s total energy use in 2010 [1]. The U.S. Department of Energy estimates that data center facilities consume up to 100 to 200 times more energy than standard office buildings.<ref name=\"https://energy.gov\">“Best Practices Guide for Energy-Efficient Data Center Design”, prepared by the National Renewable Energy Laboratory for the U.S. Department of Energy, Federal Energy Management Program, March 2011. </ref>\n\nEnergy efficient data center design should address all of the energy use aspects included in a data center: from the IT equipment to the HVAC(Heating, ventilation and air conditioning) equipment to the actual location, configuration and construction of the building.\n\nThe U.S. Department of Energy specifies five primary areas on which to focus energy efficient data center design best practices:\n\n\nAdditional energy efficient design opportunities specified by the U.S. Department of Energy include on-site electrical generation and recycling of waste heat.\n\nEnergy efficient data center design should help to better utilize a data center’s space, and increase performance and efficiency.\n\nIn 2018, three new US Patents make use of facilities design to simultaneously cool and produce electrical power by use of internal and external waste heat. The three patents use silo design for stimulating use internal waste heat, while the recirculation of the air cooling the silo's computing racks. US Patent 9,510,486, uses the recirculating air for power generation, while sister patent, US Patent 9,907,213, forces the recirculation of the same air, and sister patent, US Patent 10,020,436, uses thermal differences in temperature resulting in negative power usage effectiveness. Negative power usage effectiveness, makes use of extreme differences between temperatures at times running the computing facilities, that they would run only from external sources other than the power use for computing. \n\nThe efficiency of algorithms affects the amount of computer resources required for any given computing function and there are many efficiency trade-offs in writing programs. Algorithm changes, such as switching from a slow (e.g. linear) search algorithm to a fast (e.g. hashed or indexed) search algorithm can reduce resource usage for a given task from substantial to close to zero. In 2009, a study by a physicist at Harvard estimated that the average Google search released 7 grams of carbon dioxide (CO₂). However, Google disputed this figure, arguing instead that a typical search produced only 0.2 grams of CO₂.\n\nAlgorithms can also be used to route data to data centers where electricity is less expensive. Researchers from MIT, Carnegie Mellon University, and Akamai have tested an energy allocation algorithm that successfully routes traffic to the location with the cheapest energy costs. The researchers project up to a 40 percent savings on energy costs if their proposed algorithm were to be deployed. However, this approach does not actually reduce the amount of energy being used; it reduces only the cost to the company using it. Nonetheless, a similar strategy could be used to direct traffic to rely on energy that is produced in a more environmentally friendly or efficient way. A similar approach has also been used to cut energy usage by routing traffic away from data centers experiencing warm weather; this allows computers to be shut down to avoid using air conditioning.\n\nLarger server centers are sometimes located where energy and land are inexpensive and readily available. Local availability of renewable energy, climate that allows outside air to be used for cooling, or locating them where the heat they produce may be used for other purposes could be factors in green siting decisions.\n\nApproaches to actually reduce the energy consumption of network devices by proper network/device management techniques are surveyed in. The authors grouped the approaches into 4 main strategies, namely (i) Adaptive Link Rate (ALR), (ii) Interface Proxying, (iii) Energy Aware Infrastructure, and (iv) Max Energy Aware Applications.\n\nComputer virtualization refers to the abstraction of computer resources, such as the process of running two or more logical computer systems on one set of physical hardware. The concept originated with the IBM mainframe operating systems of the 1960s, but was commercialized for x86-compatible computers only in the 1990s. With virtualization, a system administrator could combine several physical systems into virtual machines on one single, powerful system, thereby unplugging the original hardware and reducing power and cooling consumption. Virtualization can assist in distributing work so that servers are either busy or put in a low-power sleep state. Several commercial companies and open-source projects now offer software packages to enable a transition to virtual computing. Intel Corporation and AMD have also built proprietary virtualization enhancements to the x86 instruction set into each of their CPU product lines, in order to facilitate virtual computing.\n\nNew virtual technologies, such as Linux Containers can also be used to reduce energy consumption. These technologies make a ore efficient use of resources, thus reducing energy consumption by design. Also, the consolidation of virtualized technologies is more efficient than the one done in virtual machines, so more services can be deployed in the same physical machine, reducing the amount of hardware needed. \n\nTerminal servers have also been used in green computing. When using the system, users at a terminal connect to a central server; all of the actual computing is done on the server, but the end user experiences the operating system on the terminal. These can be combined with thin clients, which use up to 1/8 the amount of energy of a normal workstation, resulting in a decrease of energy costs and consumption. There has been an increase in using terminal services with thin clients to create virtual labs. Examples of terminal server software include Terminal Services for Windows and the Linux Terminal Server Project (LTSP) for the Linux operating system. Software-based remote desktop clients such as Windows Remote Desktop and RealVNC can provide similar thin-client functions when run on low power, commodity hardware that connects to a server.\n\nThe Advanced Configuration and Power Interface (ACPI), an open industry standard, allows an operating system to directly control the power-saving aspects of its underlying hardware. This allows a system to automatically turn off components such as monitors and hard drives after set periods of inactivity. In addition, a system may hibernate, when most components (including the CPU and the system RAM) are turned off. ACPI is a successor to an earlier Intel-Microsoft standard called Advanced Power Management, which allows a computer's BIOS to control power management functions.\n\nSome programs allow the user to manually adjust the voltages supplied to the CPU, which reduces both the amount of heat produced and electricity consumed. This process is called undervolting. Some CPUs can automatically undervolt the processor, depending on the workload; this technology is called \"SpeedStep\" on Intel processors, \"PowerNow!\"/\"Cool'n'Quiet\" on AMD chips, LongHaul on VIA CPUs, and LongRun with Transmeta processors.\n\nData centers, which have been criticized for their extraordinarily high energy demand, are a primary focus for proponents of green computing. According to a Greenpeace study, data centers represent 21% of the electricity consumed by the IT sector, which is about 382 billion kWh a year.\n\nData centers can potentially improve their energy and space efficiency through techniques such as storage consolidation and virtualization. Many organizations are aiming to eliminate underutilized servers, which results in lower energy usage. The first step toward this aim will be training of data center administrators. The U.S. federal government has set a minimum 10% reduction target for data center energy usage by 2011. With the aid of a self-styled ultraefficient evaporative cooling technology, Google Inc. has been able to reduce its energy consumption to 50% of that of the industry average.\n\nMicrosoft Windows, has included limited PC power management features since Windows 95. These initially provided for stand-by (suspend-to-RAM) and a monitor low power state. Further iterations of Windows added hibernate (suspend-to-disk) and support for the ACPI standard. Windows 2000 was the first NT-based operating system to include power management. This required major changes to the underlying operating system architecture and a new hardware driver model. Windows 2000 also introduced Group Policy, a technology that allowed administrators to centrally configure most Windows features. However, power management was not one of those features. This is probably because the power management settings design relied upon a connected set of per-user and per-machine binary registry values, effectively leaving it up to each user to configure their own power management settings.\n\nThis approach, which is not compatible with Windows Group Policy, was repeated in Windows XP. The reasons for this design decision by Microsoft are not known, and it has resulted in heavy criticism. Microsoft significantly improved this in Windows Vista by redesigning the power management system to allow basic configuration by Group Policy. The support offered is limited to a single per-computer policy. The most recent release, Windows 7 retains these limitations but does include refinements for timer coalescing, processor power management, and display panel brightness. The most significant change in Windows 7 is in the user experience. The prominence of the default High Performance power plan has been reduced with the aim of encouraging users to save power.\n\nThere is a significant market in third-party PC power management software offering features beyond those present in the Windows operating system. available. Most products offer Active Directory integration and per-user/per-machine settings with the more advanced offering multiple power plans, scheduled power plans, anti-insomnia features and enterprise power usage reporting. Notable vendors include 1E NightWatchman, Data Synergy PowerMAN (Software), Faronics Power Save, Verdiem SURVEYOR and EnviProt Auto Shutdown Manager\n\nLinux systems started to provide laptop-optimized power-management in 2005, with power-management options being mainstream since 2009.\n\nDesktop computer power supplies are in general 70–75% efficient, dissipating the remaining energy as heat. A certification program called 80 Plus certifies PSUs that are at least 80% efficient; typically these models are drop-in replacements for older, less efficient PSUs of the same form factor. As of July 20, 2007, all new Energy Star 4.0-certified desktop PSUs must be at least 80% efficient.\n\nSmaller form factor (e.g., 2.5 inch) hard disk drives often consume less power per gigabyte than physically larger drives. Unlike hard disk drives, solid-state drives store data in flash memory or DRAM. With no moving parts, power consumption may be reduced somewhat for low-capacity flash-based devices.\n\nIn a recent case study, Fusion-io, manufacturer of solid state storage devices, managed to reduce the energy use and operating costs of MySpace data centers by 80% while increasing performance speeds beyond that which had been attainable via multiple hard disk drives in Raid 0. In response, MySpace was able to retire several of their servers.\n\nAs hard drive prices have fallen, storage farms have tended to increase in capacity to make more data available online. This includes archival and backup data that would formerly have been saved on tape or other offline storage. The increase in online storage has increased power consumption. Reducing the power consumed by large storage arrays, while still providing the benefits of online storage, is a subject of ongoing research.\n\nA fast GPU may be the largest power consumer in a computer.\n\nEnergy-efficient display options include:\n\nUnlike other display technologies, electronic paper does not use any power while displaying an image. CRT monitors typically use more power than LCD monitors. They also contain significant amounts of lead. LCD monitors typically use a cold-cathode fluorescent bulb to provide light for the display. Some newer displays use an array of light-emitting diodes (LEDs) in place of the fluorescent bulb, which reduces the amount of electricity used by the display. Fluorescent back-lights also contain mercury, whereas LED back-lights do not.\n\nRecycling computing equipment can keep harmful materials such as lead, mercury, and hexavalent chromium out of landfills, and can also replace equipment that otherwise would need to be manufactured, saving further energy and emissions. Computer systems that have outlived their particular function can be re-purposed, or donated to various charities and non-profit organizations. However, many charities have recently imposed minimum system requirements for donated equipment. Additionally, parts from outdated systems may be salvaged and recycled through certain retail outlets and municipal or private recycling centers. Computing supplies, such as printer cartridges, paper, and batteries may be recycled as well.\n\nA drawback to many of these schemes is that computers gathered through recycling drives are often shipped to developing countries where environmental standards are less strict than in North America and Europe. The Silicon Valley Toxics Coalition estimates that 80% of the post-consumer e-waste collected for recycling is shipped abroad to countries such as China and Pakistan.\n\nIn 2011, the collection rate of e-waste is still very low, even in the most ecology-responsible countries like France. In this country, e-waste collection is still at a 14% annual rate between electronic equipment sold and e-waste collected for 2006 to 2009.\n\nThe recycling of old computers raises an important privacy issue. The old storage devices still hold private information, such as emails, passwords, and credit card numbers, which can be recovered simply by someone's using software available freely on the Internet. Deletion of a file does not actually remove the file from the hard drive. Before recycling a computer, users should remove the hard drive, or hard drives if there is more than one, and physically destroy it or store it somewhere safe. There are some authorized hardware recycling companies to whom the computer may be given for recycling, and they typically sign a non-disclosure agreement.\n\nCloud computing addresses two major ICT challenges related to Green computing – energy usage and resource consumption. Virtualization, Dynamic provisioning environment, multi-tenancy, green data center approaches are enabling cloud computing to lower carbon emissions and energy usage up to a great extent. Large enterprises and small businesses can reduce their direct energy consumption and carbon emissions by up to 30% and 90% respectively by moving certain on-premises applications into the cloud. One common example includes Online shopping that helps people purchase products and services over the Internet without requiring them to drive and waste fuel to reach out to the physical shop, which, in turn, reduces greenhouse gas emission related to travel.\n\nNew technologies such as Edge and Fog computing are a solution to reducing energy consumption. These technologies allow redistributing computation near the use, thus reducing energy costs in the network. Furthermore, having smaller data centers, the energy used in operations such as refrigerating and maintenance gets largely reduced. \n\nTeleconferencing and telepresence technologies are often implemented in green computing initiatives. The advantages are many; increased worker satisfaction, reduction of greenhouse gas emissions related to travel, and increased profit margins as a result of lower overhead costs for office space, heat, lighting, etc. The savings are significant; the average annual energy consumption for U.S. office buildings is over 23 kilowatt hours per square foot, with heat, air conditioning and lighting accounting for 70% of all energy consumed. Other related initiatives, such as Hoteling, reduce the square footage per employee as workers reserve space only when they need it. Many types of jobs, such as sales, consulting, and field service, integrate well with this technique.\n\nVoice over IP (VoIP) reduces the telephony wiring infrastructure by sharing the existing Ethernet copper. VoIP and phone extension mobility also made hot desking more practical.\n\nThe information and communication technologies (ICTs) energy consumption, in the USA and worldwide, has been estimated respectively at 9.4% and 5.3% of the total electricity produced. The energy consumption of ICTs is today significant even when compared with other industries. Some study tried to identify the key energy indices that allow a relevant comparison between different devices (network elements). This analysis was focused on how to optimise device and network consumption for carrier telecommunication by itself. The target was to allow an immediate perception of the relationship between the network technology and the environmental effect. These studies are at the start and the gap to fill in this sector is still huge and further research will be necessary.\n\nThe inaugural Green500 list was announced on November 15, 2007 at SC|07. As a complement to the TOP500, the unveiling of the Green500 ushered in a new era where supercomputers can be compared by performance-per-watt.\n\nThe TSUBAME-KFC-GSIC Center by Tokyo Institute of Technology, Made in Japan was with a great advantage to the second, the Top 1 Supercomputer in the World with 4,503.17 MFLOPS/W and 27.78 Total Power (kW)++\n\nToday a new supercomputer, L-CSC from the GSI Helmholtz Center, Made in Germany emerged as the most energy-efficient (or greenest) supercomputer in the world. The L-CSC cluster was the first and only supercomputer on the list to surpass 5 gigaflops/watt (billions of operations per second per watt). L-CSC is a heterogeneous supercomputer that is powered by Dual Intel Xeon E5-260 and GPU accelerators, namely AMD FirePro™ S9150 GPUs. It marks the first time that a supercomputer using AMD GPUs has held the top spot. Each server has a memory of 256 gigabytes. Connected, the server via an Infiniband FDR network.\n\nDegree and postgraduate programs that provide training in a range of information technology concentrations along with sustainable strategies in an effort to educate students how to build and maintain systems while reducing its harm to the environment. The Australian National University (ANU) offers \"ICT Sustainability\" as part of its information technology and engineering masters programs. Athabasca University offer a similar course \"Green ICT Strategies\", adapted from the ANU course notes by Tom Worthington. In the UK, Leeds Beckett University offers an MSc Sustainable Computing program in both full and part-time access modes.\n\nSome certifications demonstrate that an individual has specific green computing knowledge, including:\n\nThere are a lot of blogs and other user created references that can be used to gain more insights on green computing strategies, technologies and business benefits. A lot of students in Management and Engineering courses have helped in raising higher awareness about green computing.\n\nSince 2010, Greenpeace has maintained a list of ratings of prominent technology companies in several countries based on how clean the energy used by that company is, ranging from A (the best) to F (the worst). This Rating has been certified by Dr. Jordan Kennedy from Cambridge University and his husband Professor Cory Richards. These men have done many years of research towards Green ICT.\n\n"}
{"id": "2530196", "url": "https://en.wikipedia.org/wiki?curid=2530196", "title": "In a basket", "text": "In a basket\n\nFood served in a basket, a basket platter or a basket with fries is a sandwich or other main-dish that is served on top of a basket of an accompanying foodstuff, usually french fries. The \"basket\" is usually either made of plastic and lined with paper, or is simply a disposable paperboard box or tray. Sometimes the basket contains other side dishes as well, such as a container of coleslaw or a pickle. This term is common in the Midwestern U.S..\n\nIn the United Kingdom chicken in a basket, fried chicken on a bed of chips, was a popular dish in pubs and modest restaurants from the late 1960s through the 1970s. \n\nIn the 1970s the dish became so ubiquitous in UK venues offering evening entertainment that the locations became known to musicians and entertainers as the \"chicken-in-a-basket circuit\".\n"}
{"id": "51136320", "url": "https://en.wikipedia.org/wiki?curid=51136320", "title": "Intelligence engine", "text": "Intelligence engine\n\nAn intelligence engine is a type of enterprise information management that combines business rule management, predictive, and prescriptive analytics to form a unified information-access platform that provides real-time intelligence through search technologies, dashboards and/or existing business infrastructure. Intelligence Engines are process and/or business problem specific, resulting in industry and/or function-specific marketing trademarks associated with them. They can be differentiated from enterprise resource planning (ERP) software in that intelligence engines include organization-level business rules and proactive decision management functionality.\n\nThe first intelligence engine application appears to have been introduced in 2001 by Sonus Networks, Inc. in their patent US6961334 B1. Applied to the field of telecommunications systems, the intelligence engine was composed of a database queried by a data distributor layer, received by a telephony management layer and acted upon by a facility management command & control layer. This combined standalone business intelligence tools like a data warehouse, reporting and querying software and a decision support system.\n\nThe concept was reinforced in 2002 in patent application US20030236689 A1 which applied predictive quantitative models to data and used rules to correlate context data at different stages of the business process with business process outcomes to be presented to end users.\n\nLogRhythm Inc. advanced the concept in 2010 by adding event managers to the end of the intelligence engine's process to determine reporting, remediation and other outcomes.\n\nIn 2016, professional service company KPMG continued to advance the concept by commercializing intelligence engines with the introduction of Third Party Intelligence, which is differentiated from past intelligence engines in its increased use of embedded intellectual property, diversity of global data inputs and focus on predictive analytics to mitigate risk and yield cost savings.\n\nAs a system that combines human intelligence, data inputs, automated decision-making and unified information access, intelligence engines are an advancement in business intelligence tools because they: \n\n\n"}
{"id": "52981810", "url": "https://en.wikipedia.org/wiki?curid=52981810", "title": "International Society for Explosive Engineers", "text": "International Society for Explosive Engineers\n\nThe International Society for Explosive Engineers (ISEE) is a tax-exempt professional body founded in 1974 to advance the science and art of explosives engineering.. Headquartered in Cleveland, Ohio, it is the primary international organization for explosive engineers.\n\nISEE was founded in Pittsburgh in 1974, when a small group of explosive engineers came together to discuss how to advance the science and art of explosive engineering. Since then the ISEE has grown to over 4000 members with 43 local chapters.\n\nThe ISEE has 2 primary peer reviewed publications, and an industry handbook.\n\n\n\nEach year more than 1500 blasters, manufacturers, government officials and industry leaders, come together for an annual conference. The Blasters Weekend is a preconference event targeted at field personnel focusing on more practical knowledge and skills. The main conference is 3 days of technical papers and presentations. Both conference events qualify for continuing education units (CEUs) and/or professional development hours (PDHs) for most states licensing programs.\n\nIndustry awards and designations presented at the annual national conference.\n\n\nExplosives Engineering\n"}
{"id": "53032028", "url": "https://en.wikipedia.org/wiki?curid=53032028", "title": "Israeli ceramics", "text": "Israeli ceramics\n\nIsraeli ceramics are ceramics designed either in Palestine or Israel from the beginning of the 20th century. In additional to traditional pottery, in Israel there are artists whose works were created in an industrial environment. Until the late 1970s there existed in Israel a local tradition that emphasized the local values of nature as an expression of Zionist identity. From the 1980s artistic expressions that sought to undercut this tradition began to appear in the works of Israeli artists, who combined ceramics with other artistic media and with personal, critical agendas.\n\nAt the beginning of the 20th century the Palestinian tradition of designing pottery from local materials dominated in Israel. The pottery was primary functional, intended primarily for the use of the local population in the Land of Israel. Other vessels were imported from neighboring areas.\n\nPots were thrown on potter's wheels mostly in urban areas or in pottery villages. It was a craft traditionally worked by men. In the census carried out on the streets of the land of Israel in 1928 during the British Mandate, 77 pottery villages (of individuals or groups) were listed, while in the 1931 census, 211 different pottery villages were listed. Many of the pottery villages were centralized, based on geographical proximity of the potters' families. Examples of this can be found in the pottery workshops held in the pottery villages in Rashia al Fakhar (Tel Faher) at the foot of Mount Hermon. Or in Hebron, at the various workshops of the Alfahori family. These pots were fired at a low temperature in traditional kilns that burned wood, charcoal, or animal droppings. Different workshops were held in Gaza as well, where they produced unique black pottery (فخار اسود), produced by adding organic materials, such as barley husks, to the kiln or by reduction firing. The smoke from the process of the burning of these materials within the kiln lent the pottery its characteristic black color.\n\nIn addition, a tradition existed of producing pots made of clay mixed with straw or gravel for cooking and other utilitarian uses by the local population. This work was carried out by hand by women, and the pots were fired in improvised kilns, in kitchen ovens, or sometimes not fired at all. Pottery of this type, which was produced in the Samaria area and in Ramallah, for example, was typically decorated with color made from rusted iron that originated in the Jordan Valley.\n\nAs the century progressed, this tradition began dying out as a result of industrialization, and in addition, from the 1980s competing pottery from other countries began to be imported. Until 1989, for example, 11 different workshops were active in Hebron. However, by 2007 there remained only 8 potters there. In 1983 The Eretz Israel Museum mounted an exhibition displaying the pottery of the Lebanese village of Rashia al Fakhar.\n\nIn 1919 the British Mandate government invited a group of Armenian potters, survivors of the Armenian Genocide, to repair the ceramic tiles of the Dome of the Rock. This experiment indicated the British interest in traditional art of the Arts and Crafts movement. Armenian ceramic art can be traced back to the 15th century, to the Turkish cities of İznik and Kütahya, but the combination of the ancient art of the Land of Israel and Christian motifs created a unique artistic synthesis.\n\nThe outstanding artist of the early years was David Ohannessian, who specialized in the design of ceramic decorative art in buildings and monuments, many of which were commissioned by the British Mandate government. The workshop that he founded – \"The Dome of the Rock Tiles\" – produced not only monumental works, but also utilitarian and decorative pottery. Among the most important works Ohannessian produced in Jerusalem were tiles for the American Colony Hotel (1923), the fountain house in St. John's Hospital, the tiled dome in the Rockefeller Museum garden, etc. Among the motifs that appear in his decorations are the Cypresses trees, tulips, and grapevines typical of traditional Seljuk and Ottoman decorative art. Ohannessian introduced some unique Jerusalem iconography to his works, figures derived from Armenian medieval illuminations, historic designs and forms from European renaissance majolica, and bird motifs drawn from an Armenian Jerusalem 5th-6th century mosaic floor, discovered during an archeological dig in the Holy City during the 1890s, to name a few.\n\nThe workshop that Ohannessian founded in Jerusalem followed the tradition of ceramic making as he had practiced and overseen it in his atelier in Kutahya, in Ottoman Anatolia, which had flourished prior to World War I and his deportation to the Syrian desert. According to traditional methods, the processes of clay and glaze mixing, wheel throwing, design, painting, and firing were performed by a variety of artisans, all supervised by Ohannessian, some of whom specialized in design and others who were experts at the potter's wheel. Ohannessian was skilled himself in all facets of the art and trained a variety of apprentices, as he had done in Anatolia. He maintained particular expertise in painting ceramics and design. Upon the founding of his workshop on the Via Dolorosa, he cooperated with Near East Relief, to train a number of Armenian orphans of the genocide, giving them the skills to engage in the art on a professional basis. Some of his workshop's output, especially the monumental architectural tile works, were signed by the chief artist. Numerous other vases and plates identified their respective makers through small glazed initials on the bottom, usually painted in ochre. Ohannessian's workshop used a stone wood-burning kiln, built to his specifications in 1919. The model implemented in this workshop was used for other Armenian workshops founded subsequently in Jerusalem.\n\nThe artists Megardish Karakashian and Nishan Balian, who left Ohannessian's workshop in 1922, founded a workshop together called \"Palestine Pottery,\" where they developed a line of design with figurative images that were alien to traditional Turkish ceramic art. For example, the two of them combined designs from ancient mosaics discovered in the Land of Israel, such as the \"Birds Mosaic\" (Jerusalem) or the mosaic from Hisham's Palace in Jericho. Often these images were imbued with Christian theological interpretations. The joint workshop functioned continually until 1964, when Stefan Karakashian and Marie Balian, heirs of the founders, founded two separate workshops that made use of images from the past as well as new images that they created.\n\nWithin the framework of the \"Beazlel School of Art and Craft\", a ceramics studio was founded in 1924, with Jacob Eisenberg at its head. By 1917 Boris Schatz was considering opening a department for the design of cast decorative items, as well as a department of painting on \"porcelain.\" This craft was already being taught at Bezalel using ready-made porcelain brought in from outside Palestine for use at Bezalel. Schatz saw in the activity of the factory for bricks and roofing tiles that operated on the grounds of the \"Schneller Orphanage\" in Jerusalem from approximately 1895, proof of the practicality of such a local industry, which would make use of local materials brought from Motza.\n\nEisenberg, who was a student at Bezalel from 1913 to 1919, after his graduation went to study in Vienna, at the School for Arts and Crafts, where he took a continuing education course in ceramic design and production. The department separated the design of the ceramics, mostly taught by Bezalel instructors and particularly by Ze'ev Raban, from the practical production of the pieces. Of the objects produced in this department, the best known are the wall tiles and decorations from the 1920s and 1930s. These works include the tiles on the walls of the Ahad HaAm School, the Bialik House, and the Lederberg House in Tel Aviv, and in the synagogue of Moshav Zekanim.\n\nThe style of tile design was influenced by Art Art Nouveau and by the \"Jugendstil style. This style is expressed in the flatness of the area described and in the richly decorated borders. With regard to ideas, \"Bezalel tiles\" expressed a tendency toward transcendentalism, seen in their borders cast with images taken from Jewish tradition and Zionist content.\n\nIn her article \"Techno Tools: The Logical Ones\" (2011), Shlomit Bauman maintained that contemporary Israeli ceramics is characterized by a disconnect from the Palestinian tradition and by \"a lack of a local ceramics industry that would permit a dialog of understanding between local ceramic artists.\"\n\nAt the same time as the Armenians and the Arabs living in the Palestine worked within independent traditions, Jewish artists had to create a synthesis between European art and art in the Land of Israel under the conditions that existed there in the early 20th century. This can be seen both in the design of the models and in the work techniques, which tended to be mechanized. In addition, while local pottery depended on family-led workshops and on cooperative activity by the artists, the Jewish potter saw himself both as an artist and as an expression of the language of art.\n\nChava Samuel, who Immigrated to the Palestine in 1932, founded \"Hayozer\" [The Creator], the first ceramics workshop in the Jewish community in Jerusalem. \"Kad VeSefel\" [Jug and Cup], the ceramics workshop founded in 1934 in Rishon LeZion with Paula Ahronson, produced a variety of utilitarian pots and decorative pottery, using a combination of potter's wheel and ceramic casting. The style of the pots was, for the most part functional, influenced both by the spirit of modernism and the European Bauhaus style. Mira Libes, a pupil in the workshop of Samuel and Ahronson, described the pottery produced in the workshop as the direct result of \"create pottery that is simple, functional, and beautiful,\" a philosophy intended also to improve public taste, which was deemed at that time indescribably bad. The pottery produced was generally influenced by the \"Bauhaus\" style, which Paula had studied, and the simple and beautiful decorative-colorful style of Eva.\"[18]\n\nThe motifs of the decorations on Samuel's pottery were also influenced by the archeology of the Land of Israel, as well as by Oriental art, under whose influence she produced \"Eastern\" images and images from the Jewish world. As opposed to the figures from the Jewish world created by the artists of the \"Bezalel\" school, Samuel's imaged lacked the religious dimension. The images that remained, for the most part looked like images from folklore. The decorative style of Samuel's pottery focused on individual images, drawn primarily freehand and glazed.\n\nIn contrast to the pottery of Samuel and Ahronson, the works of Hedwig Grossman displayed an attempt to formulate a Land of Israel \"localness\" in their ceramic design. Grossman made Aliyah to the Land of Israel in 1933 after studying pottery in Germany. During her first years in Palestine, Grossman already began to carry out soil surveys to determine how local materials were used in pottery production. In addition, Grossman researched how pottery was made in the Land of Israel in ancient times and what where the work methods of Arab and Armenian potters throughout the Land. In her work Grossman emphasized the use of materials from the Land of Israel. Some of her work was even influenced by local archaeological findings. Her techniques for working the material included basic geometric decoration, using local non-glaze slips (engobes) in assorted colors.\n\nAn echo of Grossman's views can be seen in the 1940s, when Jacob Lev, who served from 1939 as the head of the Sculpture Department in the \"New Bezalel,\" began to offer classes in pottery in the department. Most of the pots that were made in this institution were not fired in a kiln and so did not survive, but in his article \"The Pretty Pot\" (1941), he emphasizes the modernist approach to design of the pot and the relationship between its parts in the \"Bauhaus\" spirit. However photographs of the pots show the influence of the architecture of the Land of Israel in the choice of the types of pots, as well as in the avoidance of decoration in the rough texture of their design.\n\nThe works of Hedwig Harag Zunz, who arrived in the Land of Israel at the beginning of the 1940s, also represent an attempt at creating pottery with a Land of Israel \"localness.\" In Zunz's works this was expressed primarily in her choice of local materials. Most of her work was produced using a potter's wheel, but she also created pottery with an architectural bent. In spite of her consistent use of local materials, Zunz's works differ from the archaeological direction of Hedwig Grossman's works or the oriental decoration of Eva Samuel's. The shape of her pottery was influenced by European Modernism in its lack of decoration and in its organic tendency toward the use of the glossy surface slips of Terra sigillata or ceramic glazes. In addition to her independent works, Harag Zunz also produced technical academic research and participated in various industrial projects.\n\nIn the 1950s and 1960s there was an upswing in ceramic activity in Israel, and steps were taken toward institutionalizing ceramics as a branch of art. Alongside traditional pottery and the newly established \"Studio Pottery\", other forms of media began to develop, including ceramic sculpture and industrial initiatives, a small-scale industry that took its spiritual roots from European Modernism. In general, ceramic artists, like many other Israeli artists, strove toward \"localness\", with its identification with the land, the landscape, and the archaeology of the Land of Israel.\n\nFrom the very beginning of the ceramics industry in Israel, ceramic objects were produced in casting molds. However, the development of the Zionist industry gave impetus to speedy industrialization.[24] The main area in which the ceramics industry developed was Haifa, where in 1938 \"Naaman\" where cast porcelain objects were produced, was founded.[25] In the exhibition \"Applied Art and the Prescription for Industry in the Land of Israel,\" held in the Bezalel Museum in Jerusalem in 1947, glazed pots were displayed by Hanna Harag Zunz as an example of cooperation among various elements of the industry.[26] In 1955 in the Haifa Museum of Art an exhibition entitled \"The Ceramic Society\" was mounted, in which objects and the processes of their production in the local ceramics industry were displayed.\n\nThe 1950s saw the beginning of practical cooperation between the ceramics industry and the designers and artists in the young state, with an eye to advancing the industry and raising its prestige. Similar initiatives took place in the 1960s in Israel Ceramic and Silicate Institute in the Technion, and in Bezalel, where they began testing ceramic materials for design.\n\nAharon Kahana - who with his wife founded \"Beit Hayotzer\" [Artisan's Workshop] (Haifa), which was so large he employed three assistant potters—succeeded in defining a widespread style that combined modernist abstraction with decorative folk motifs. This combination succeeded in \"returning art to the people.\" and \"making use of many shapes, which had been unacceptable in the eyes of the viewers, as decoration for ceramic creations.\n\nAmong the better known ceramic production plants were \"Kol-Keramic\" (Haifa), \"Carnet\" (Netanya Ceramics),\"Ceramit\" (Netanya) \"Beit Halachmi\" (Tel Aviv), \"Bror Hayil (Bror Hayil), \"Keramaklin\" (Nes Tziona), \"Palkeramic\" (Haifa), and \"Beit Hayotzer\" (Ramat Gan/Petah Tikva), the ceramics workshop of Kfar Menachem, etc. At the same time, the two most important factories were \"Lapid Pottery\" (1951) and \"Harsa Ceramics (1956),\" which combined technology with handmade techniques.\n\nIn 1952 Elisabeth Cohen came to work as a designer for Lapid Pottery. Elisabeth Cohen had studied with Hedwig Grossman and founded the \"art\" department in the company. By 1959 this department employed 20 people. In the beginning the company produced utilitarian pots cast from the materials used to cast toilets, but little by little both the methods and the designs of the company became more sophisticated. In addition to Cohen other designers, including Maud Friedland, came to work for the company. Another art department produced ceramics in the Harsa Ceramics company from 1956-1966. The department was founded by Nehemia Azaz, who designed his pots using local colors and materials. At its peak, the department employed 30 workers. After Azaz left the department, it was run by Pnina Zamir Amir.\n\nThe articles made in both companies were modernist in style and were characterized by the use of geometric styles and primarily abstract decoration. These designs were influenced by the tradition of modernist design, from designs created in Europe during this same period, and therefore by what was perceived as the expression of Land of Israel \"localness.\" Azaz, for example, stated that his forms were influenced by the desert landscape.\n\nThe use of porcelain by ceramics companies was concentrated for the most part in the \"Naaman\" and \"Lapid\" factories, both of which produced primarily utilitarian utensils. At the same time it is known that the industry tried to encourage local design. Shimon Badar, for example, who was the first designer for \"Naaman,\" also served as a lecturer on ceramic technology at the Ceramic Department of Bezalel. An example of a large scale technological–artistic project is the covering of the entrance to Asia House (1977-1979) in tiles produced in the porcelain molds next to \"Naaman\" and designed by the artist Pinchas Eshet.\n\nThe 1950s and 1960s were the most important years of ceramic activity in the field of Israeli art. For the first time ceramic artists began to be trained in many private workshops and a variety of academic institutions. In addition, decorative arts –including ceramic art – were no longer perceived as a foreign element in the young Israeli culture. To a large extent these forms of media were encouraged by various government agencies. Finally, an examination of the creations of the artists demonstrates the inculcation of a local artistic \"tradition\" that chose to cut itself off from the Arab ceramic tradition, the Armenian tradition, and even that of Bezalel, in favor of \"modernism\" and \"localness,\" which determined the central direction of Israeli ceramics until the 1980s.\n\nAlong with Hedwig Grossman, who taught most young ceramicists in her Jerusalem studio and in Ein Hod, and from 1964 in Givatayim, Hanna Harag Zunz also taught them in the Oranim Seminar (today Oranim Academic College), Paula Ahronson at Wizo in Tel Aviv, and Gedula Ogen at Havat HaNoar HaTzioni (Israel Goldstein Youth Village). In 1957 the department of ceramics at The Arts Institute at Tel-Hai College opened. In 1958 the Department of Ceramics and Glass Design at Bezalel Academy of Arts and Design was founded, next to the Schneller Orphanage. The department was headed by David Wachtel, who had worked previously at the \"Palkeramics\" factory. His teaching faculty included David Calderon, Pnina Amir-Zamir, and Maud Friedland. In 1962 Gedula Ogen was appointed head of the department. Between 1961 and 1963 the number of students in the department grew from 26 to 50, and the amount of work space in the department greatly increased.[38] Another teaching institution was the \"Visual Arts Center\" in Beersheba.\n\nFrom the standpoint of design, the ceramicists continued the tradition of Harag Zunz and particularly the tradition of Grossman, in everything connected to the spirit of European Modernism and in the aspiration toward the expression of what was local in their works. Among the most significant followers of Grossman are Amnon Israeli and Gedula Ogen. From the 1960s both of them took the inspiration for their work from archaeology and the Land of Israel landscape, expressed in works that were unglazed and coated in slip or exposed to an open fire during their firing. Other prominent ceramic artists were Yehudit Meir and Yocheved Marx, who worked together in Beersheba from 1958, using local materials from the earth of the Negev and fired their works at very high temperatures (1250-1300 degrees C), which was atypical for Israeli ceramicists of that period. The works they created spanned the field from decorative to practical.\n\nThe opportunities for exhibiting and selling ceramics during that period were limited and led to the production, for the most part, of practical ceramic ware. Along with exhibitions at \"Mikra-Studio\" gallery (Tel Aviv, 1946-1956), which combined emphasizing artistic values with exhibiting female ceramic artists, and \"Atelier 97\" (Tel Aviv), ceramics were sold at the Maskit stores (Tel Aviv, 1959- ?), which emphasized folkloristic elements. In addition there were a number of exhibitions of ceramic art at the Tel Aviv Museum of Art and at the Bezalel Museum and at a variety of private stores. The overwhelming majority of ceramic artists were not members of a professional artists' union, and if they were, like Grossman and Samuel, within that framework they exhibited primarily paintings and prints. In 1966 in Tel Aviv the \"Clay Museum\" opened as part of the Eretz Israel Museum, exhibiting ancient ceramics next to exhibitions of Israeli artists who exhibited in the open courtyard of the pavilion. In 1968 the Ceramic Artists Association of Israel was founded, headed by Joseph Blumenthal.\n\nIn 1963 the book Art in Israel was published, under the editorship of Benjamin Tammuz. The book devoted an entire chapter to art that John Cheney compiled. In this chapter, which was accompanied by a number of photographs, Cheney surveyed such areas of art as design, jewelry making, and ceramics, which he juxtaposed to media such as drawing, sculpture, and architecture. The presentation of ceramics and the rest of the arts as equal to other artistic media, bears witness to the importance of art during those years and its promotion by exhibitions and competitions organized by institutions such as the Israel Packaging Institute and the Israel Export Institution, etc. On 24 September 1970, an exhibition opened in the Tel Aviv Museum of Art that surveyed the field of ceramics in Israel and was the ultimate expression of this trend. The exhibition – \"Israel Ceramics 70\" – displayed 284 works of 64 artists, along with a detailed catalog that included photographs of the works as well. In addition the works produced by another five factories (\"Beit Hayotser,\" \"Lapid,\" \"Naaman,\" \"Palkeramic,\" and \"Carnet.\"\n\nIn 1957, in close proximity to one another, Itche Mambush and his wife Aviva Margalit opened pottery workshops in Ein Hod Artists Village. The village was founded in 1953 by Marcel Janco, as a village for the arts and artists. Around the workshops a group of resident artists and artists who lived nearby began to add ceramics to one of the media in which they worked. Some of these artists were identified with \"New Horizons,\" that is, Janco, Yehezkel Streichman, Pinchas Abramovich, and Aharon Kahana, who worked alongside artists for whom the decorative style was an integral part of their art, such as Louise Schatz, Bezalel Schatz, Jean David, Genia Berger, etc.\n\nIn their works we see a colorful, expressive approach with an emphasis on glazes not typical of the \"material\" tradition in local ceramics. In addition to a small amount of ceramic sculpture, these artists produced many paintings using glazing techniques, and murals using a variety of glazed ceramic tiles. Some of the artists worked the materials themselves, while others produced detailed sketches for items to be implemented in different workshops, such as the ones in Ein Hod or the one in Kfar Menachem.\nThese decorative walls were part of the trend towards combining art with local architecture, and were used by other artists along with other techniques such as sgrafitto and mosaics. One of the best known artistic event in this field took place in 1954, when a group of ceramic wall paintings by Aharon Kahana, Jean David, and Gedula Ogen were put in place on the Givat Ram campus of The Hebrew University of Jerusalem. In 1958 sketches of these walls were even presented in the exhibition \"Ten Years of Architecture in Israel\" in the Tel Aviv Museum. The peak of this creativity was in 1966, when an international hands-on seminar in ceramics took place in Ein Hod, at Berger's initiative.\n\nIn the second half of the 1960s the influence of different kinds of media began to appear in the works of Israeli ceramicists. In particular the influence of Israeli sculptors could be seen. Along with the continuing use of \"architectural\" images and the trend toward \"localness,\" it could be seen in the use of images of the land and desert landscapes, a divergence from the boundaries of traditional ceramics. The aspiration toward copying nature into works of art forced the artists into seeking new artistic expressions.\n\nThe status of Land of Israel architecture as the expression of Zionist nationalism attracted not only the broad public, but also many ceramicists, to it. Many of them utilized the skills they had acquired in the restoration of pottery, which was found in great numbers in the archeological digs of the 1950s. It was natural, therefore, that architectural images appeared in the works of Israeli artists. A whole group of artists such as David Ben Shaul, Agi Yoeli, David Calderon, Moshe Shek, and Shelly Harari, in addition to their ceramic works, produced during this period human sculptures or animal images inspired by figurines and statuettes from antiquity. Their works included a pioneering Zionist element that expressed an attempt to connect the young Israeli state with its historical and biblical past. At the same time, Israeli ceramicists showed a tendency toward modernism, which emphasized the geometric character of the shape and the work and showed the influence of European modernism.\n\nMost of the characteristics of sculpture can be found in works that originated on potter's wheels during those years. A central element in the widespread distribution of works of this genre was the teaching of Gedula Ogen at Bezalel. Ogen developed a style using the potter's wheel to achieve the volume needed for large scale relief sculpture and figurative sculpture. In her works she continued Grossman's style of design in her emphasis on the naturally colorful quality of the material. Most of her work dealt with images from nature and with the relationship between it and culture and art.\n\nThe main reason for the shift in style during the 1960s that deviated from the previous tradition by opening the field of Israeli ceramics to varied new points of view, was the arrival and influence of ceramics producers and professional ceramic artists, the practical implementation of which could be seen in the works of David Morris, Magdalena Hefetz, Sonia Natra and others who studied and became specialists in the United States and Europe.\n\nAn article from 1972 describes the creation of one of these sculptures by Rina Peleg, a graduate of Bezalel, who \"leaning heavily on the potter's wheel and round motions, creates the body of her sculpture. Only after this does she knead and shape the finished form with her hands, creating its decoration and ornamentation.\" However she soon began to request that the artists \"jettison the jug\" and the decorative dimension of traditional ceramics in favor of freestanding art. Artists such as Neta Avraham, Shelly Harari, Nora and Naomi, Jean Meir, Maud Friedland, and others, deviated from the tradition of \"sculpting with the potter's wheel\" in order to try out different techniques, particularly freestanding sculpture. A central motif was the imitation of formations found in local landscapes and their conversion into objects of sculpture.\n\nThe Israeli landscape, especially the desert landscape, was perceived as isolated from modern westernized society, and served as an inspiration to artists in creating the form, texture, and color of artistic objects. In this way these artists joined the general trend among many other Israeli artists who created in their works images of an archaic utopia, many of them even under the late influence of \"Canaanite\" art. The objects they created tended towards abstract plant structures or towards the building of objects that suggested archaic cultural structures.\n\nAs ceramics drew closer to the field of art, however, foreign influences also creeped into the Israeli tradition. Siona Shimshi, for example, during these years returned from studies in the United States, where she had been exposed to postmodern art, such as Pop Art. Her ceramic works combined these influences with biographical works and psychological images. For example, in her works such as \"Puppet Theater\"\n\nStarting in the 1960s, large scale sculptures began to appear in Israeli art, such as the bronze sculpture \"Sheep of the Negev\" by Itzhak Danziger, Assemblage art works, such as those of Igael Tumarkin and Yehiel Shemi, or works making use of various techniques such as installations or the use of non-artistic materials, such as those created by the artists of the group \"Ten Plus,\" while at the same time ceramic works continued to be limited by the size of the kilns in which they were fired. However, apparently it was the influence of conceptual art, which arrived in Israel in the 1970s, that cut off ceramic activity from Israeli art. This is particularly noticeable against the background of \"earth\" and \"place\" imagery among the Israeli artistic vanguard of this period.\n\nIn the 1980s a fundamental change occurred in the way Israeli ceramic art was perceived. The primary cause of the change was the disruption of the local tradition in Israeli ceramics. Some of the reasons can be discerned in the perception of a break which Israeli society felt in a delayed reaction to the Yom Kippur War, and in reaction to the First Intifada. In addition the influence of postmodern culture began to trickle into the local culture.\n\nEven though postmodernism had once been described as the expression of \"the personal, [with a tendency towards the] hand-made, metaphorical, figurative, [and] decorative\", traits that came naturally to Israeli ceramic artists and their art, paradoxically the distance between them and mainstream Israeli art widened. At the same time, in painting, sculpture, and photography objections began to be heard against the status of the object as a work of art, whereas until the 1990s the field of Israeli ceramics had primarily adopted the attitude of \"identity politics,\" as the central tenet of postmodernism. In place of historical and archeological relationships, a series of personal relationships began to appear, most of them expressivist, which deviated from the traditional Israeli ceramic tradition that had existed up to this time.\n\nThis sidelining of ceramics, and especially of pottery, from the main museums, led to the establishment of a number of independent cooperative galleries in Jerusalem and Tel Aviv, for the display and merchandising of ceramics, such as \"Ruach Cadim\" (1987), the \"Altogether 8\" group (1990), and \"Shlush Shloshim (1992). In 1991 Aharon Kahana's house in Ramat Gan was opened as a ceramics gallery.\n\nIn spite of the marginal place the medium occupied, postmodern expressions in ceramics did have a certain impact because of the works of Gideon Ofrat, one of the most important of the artists who translated postmodern thinking into an aesthetic in the 1980s. In 1985 Ofrat curated the exhibition \"Toward a Myth Without God\" in the Jerusalem Artists House. In the exhibition catalog, Ofrat described ceramics as an expression of metaphysics, and pottery and pots as a means of direct connection to nature. A connection that Ofrat described as a dialectic of \"proceeding toward a double eternity – an eternity of absorbing living and an eternity of absorbing dying\" connected to the practicality and the symbolism of the pottery. In this Ofrat reflected the phenomology of Martin Heidegger and Derrida, who emphasized earth and object (and the pitcher as a characteristic expression of this) as an intersection between life and death.\n\nMany of the artists sought to probe the personal and the biographical through archetypal images, and thus express the symbolic dimension of art. From the point of view of form, it was manifested by developing means of sculptural expression in a more figurative direction and with a tendency toward ignoring the individual object in favor of the sculptural array. Siona Shimshi, for example, developed her ceramic sculpture as a series of archetypal human forms made of bare material. Nora and Naomi began to communicate with the sculptural field when they started to make use of motifs of journeys or nature images. The works of Moshe Shek are somewhere on the spectrum between ceramics and sculpture, expressed in a sort of postmodern incarnation of the Israeli ceramic tradition. The enormous pieces he created, which he called \"containers\" (1986), were decorated with simple designs that emphasized the natural colors of the material, and these sculptures displayed stylized imaginary animals made of clay cylinders that emanated echoes of \"Canaanite\" sculpture. Other artists, such as David Morris and Mark Yudell, used images specifically in order to create a private mythological world, which reflected their personal experiences, along with a dollop of humor and irony.\n\nAnother aspect of the preoccupation with the biographical can be seen in the infiltration of religious motifs into Israeli art in general, and into Israeli ceramic art in particular. Marie Balian, for example, an Armenian artist, from the 1970s geared her art towards monumental wall paintings made of glazed tiles. Her images were drawn from the Armenian ceramic tradition, with changes that were expressed in the design of the composition, which was transformed into the fantastic.\n\nA similar trend can be seen in the work of Jewish ceramicists, whose Judaica works were no stranger to their culture of the material. Abraham and Pnina Gofer, for example, in the 1970s and 1980s displayed Seder plates made using casting molds, with images done in the ethnographic style, decorated with colorful glazes. In contrast, Meira Una, in the mid-1980s created a series of Chanukah lamps that combined images from Orthodox Judaism with architectural images. Her work is typified by the use of untreated material (without clay slips or glazes), with scribbles or engraved writing, and with no polishing or finishing of the form. All of these works emphasized the status of the personal and the preoccupation with ars poetica in ceramics and art.\n\nAt the same time, the most prominent ceramic artist who made use of Jewish motifs came from the field of art and painting. In the mid-1980s Moshe Gershuni began to create what became known as \"Jewish ceramics.\" Gershuni made use of ready made plates that he bought second hand. On these he drew with ceramic paints for low-fire pottery. The images that appeared on his pottery combined excerpts from prayers, swastikas, and Magen Davids, which also can be seen in his paintings from these years. Exhibitions of these works in 1988 at Bezalel and in 1990 at the Tel Aviv Museum of Art generated wide publicity.\n\nClear signs of the coming transition from pottery to ceramic sculpture could be seen during the second half of the 1980s and the first half of the 1990s. A large number of ceramic artists began to adopt ideas from painting and sculpture in their art. These ideas, in the eyes of these artists, conferred a new legitimacy on pottery and on the objects they created, both as an expression of their personal feelings and as an expression of ars poetica.\n\nAn example of the renewed relationship between sculpture and pottery can be seen in the works of artists on the border between traditional pottery and contemporary art. Rayah Redlich, for example, created large scale pots, which she presented as a renewed esthetic and cultural examination, and on which she used techniques of printing and painting \"on the shards.\" Many of these works were displayed on pedestals that were part of the exhibit. And Daniel Davis used his potter's wheel to create large scale pots whose \"shards\" were then joined together in bundles by steel strips. A group of about 70 of them appeared as part of \"A Woman of the Pots,\" a performance and film created by his wife, Adina Bar-On, in 1990. Besides being used for musical percussion instruments, the pots pointed to themes like death and Judaism. Another group that dealt with the imagery of the material and of pottery by means of a representative medium was the Zik Group, whose works include a pot as a central image. In the installation \"Anarzik\" (1997), for example, the group created, on the structure of a potter's wheel, a complex pot that underwent a transformation of its technology and its form before the eyes of the public.\n\nAnother prominent artist was Lidia Zavadsky, who even served as the head of the Department of Ceramics and Glass Design at Bezalel Academy of Arts and Design in Jerusalem. Like other artists, Zavadsky inserted sculpture into her work as an integral part of her postmodern approach. Between the years 1992-1994 Zavadsky produced a series of pot-sculptures that turned out to be the highlight of her artistic output. The pots were monumental in size, glazed in intense colors, and a combination of work on the potter's wheel and sculpture. In spite of her themes of corporeality and death, the form and the internal relationships were the central artistic concern of Zavadsky's work. Through them the artist grappled with European and Asian pottery traditions. These works received more artistic recognition than any ceramic works up to that time and were displayed as well in the Israel Museum.\n\nOnly at the end of the 20th century and the beginning of the 21st did ceramic artists begin to make more consistent use of installation art within ceramic art. Artists such as Talia Tokatly, Hadass Rosenberg Nir, Shlomit Bauman, Yael Atzmony, Maya Muchawsky Parnas, and others created installations that combined different media, such as sculpture, ceramics, video, etc. In their works we see a combining of elements of their personal identity through the disassembly and assembly of objects saturated with political or personal history.\n\nAn opposing philosophy to this sculptural trend was the return to the pottery tradition of which one expression was the increasing adoption of porcelain as the material used on potter's wheels and in casting beginning in the 1990s. This happened both because this material began to arrive in Israel on a regular basis, and because artists began to be trained in the use of this material within the academic framework as well as privately. Among the artists who began to use porcelain on potter's wheels was Irit Abba. The use of this material sharpened her concentration on the form of the pot and allowed her to develop a deconstructionist concept of the pot, of her perception of pottery, and of her view of European culture. She emphasized the materiality of the pots and jugs she produced in the early 2000s by using slotting and chinking on their surface, by combining the porcelain with \"paper clay,\" Egyptian paste, and shavings, and by using intense colors. In other artists as well, such as Esther Beck and Shulamit Teiblum-Millar, we can see this renewed interest in pottery and in the materials of ceramics to emphasize the work processes used in the creation of artistic objects.\n\nIn spite of massive training in the technology of design, the disappearance of the ceramics industry in Israel pushed artists, and particularly designers, to the creation of individual objects or of objects in small series, and sometimes in large ones, using hand casting techniques. The most prominent of these artists are designers such as Ami Derech, Dov Ganchrow, and Jonathan Hopp.\n"}
{"id": "36887200", "url": "https://en.wikipedia.org/wiki?curid=36887200", "title": "Lesson Planet", "text": "Lesson Planet\n\nEducation Planet dba Lesson Planet, is a for-profit education company based in Santa Barbara, California. Lesson Planet provides teacher-reviewed resources for use by teachers and parents. Its products are designed to supplement traditional and non-traditional education from kindergarten through the 12th grade. The firm was founded in 1999 by James Hurley, and is based in Santa Barbara, California.\nThe Lesson Planet division of the firm consists of an education-focused website and search engine, providing links to teacher-reviewed resources that include lesson plans, worksheets, presentations, education-articles and education videos. The reviews are prepared by the firm's staff of certified classroom teachers. The website is searchable by grade and subject. \nThe site offers both paid and free digital content to educators. Current partners providing content include Virtual Nerd and other digital content providers.\n\nLesson Planet's architecture makes it easy for teachers to find teacher-reviewed, online classroom-oriented resources.\nThe Teacher Content Management System lets teachers find reviews for lesson plans, worksheets, educational videos, PowerPoint presentations and education articles.\n\nEducation Planet was named 2012 Codie award finalist for \" Best Education Reference Solution\" and won a 2007 Technology & Learning Award of Excellence for \"Teacher Resources.\"\nDistrict Administration Magazine \n\n\n"}
{"id": "2050957", "url": "https://en.wikipedia.org/wiki?curid=2050957", "title": "Limbing", "text": "Limbing\n\nLimbing (also known as \"chasing\") in logging is the process of removing branches from the trunk of a fallen tree. Options for cutting off the branches include chain saws, harvesters, stroke delimbers and others. Limbing can happen at the stump in log/tree length systems and cut-to-length systems or at the landing in whole-tree logging.\n\nDifferent definitions for it include:\n\nWhen the tree is lying on the ground, branches may be storing enormous potential energy through mechanical strain. When a branch is cut, often with a chain saw, this energy can be released suddenly and the branch can jump dangerously. In addition, a branch may be supporting the tree, and the tree can fall or roll when the branch is cut. For these reasons, delimbing is a skilled operation requiring careful safety planning.\n\nIn British English, limbing can be synonymous with snedding. Alternatively, limbing can be used to describe the operation on larger branches, and snedding on smaller.\n\n"}
{"id": "56521678", "url": "https://en.wikipedia.org/wiki?curid=56521678", "title": "List of Henry vacuum cleaners", "text": "List of Henry vacuum cleaners\n\nHenry is a series of canister vacuums released in 1981 and made by Numatic most notable for their human-like, often considered cute faces on the body/head. They come in a variety of colors, names, sizes, and parts.\n\nThis is a list of current Henry models (currently being manufactured) in the main series, meaning they say \"Henry\" or another name on the cap and have a face on the body. Some models do not say a name and instead say \"Numatic\" on the cap, and even sometimes other words such as \"Micro-filter.\" There are a total of 14 current main series Henry vacuums. Some models have been out for numerous years, meaning many of these models' looks have been changed over time, but the parts, power, and bag size have stayed the same.\n\nThis is a list of current Henry models (currently being manufactured) but instead of saying a name on the cap, say \"Numatic.\" These vacuums are often very similar to main series Henry vacuums but opt to not say the name for more industrial reasons.\n\nNumatic has made a total of 2 mops in Numatic's line of Henry products. These two mops are the only mops Numatic has ever made and are still in production currently.\n\nThese Henry models have either been discontinued or put into a lower production state by Numatic. These can still be bought online or in-stores if retailers still have them and/or are getting them from the shorter amount being made by Numatic.\n"}
{"id": "4752677", "url": "https://en.wikipedia.org/wiki?curid=4752677", "title": "List of Korean War weapons", "text": "List of Korean War weapons\n\nThis is a list of Korean War weapons.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM62 Fragmentation hand grenade\n\n\n\n\n\n\n\n\n\nUSA\nCommonwealth\nCommunist states\n\n\n\n\n\n\nAttack Aircraft\n\nFighter Aircraft\n\nBomber Aircraft\n\nUtility/Transport Aircraft\n\nLiaison/Observation/Reconnaissance Aircraft\n\nPatrol/Search & Rescue Aircraft\n\nTanker Aircraft\n\nRepublic Of Korea\nAttack Aircraft\n\nFighter Aircraft\n\nTransport Aircraft\n\nLiaison/Observation/Reconnaissance Aircraft\n\nAttack Aircraft\n\nFighter Aircraft\n\nLiaison/Observation/Reconnaissance Aircraft\n\nPatrol/Search & Rescue Aircraft\n\nTransport Aircraft\n\nFighter Aircraft\n\nAttack Aircraft\n\nBomber Aircraft\nTransport Aircraft\n\n\n\n\n"}
{"id": "23849259", "url": "https://en.wikipedia.org/wiki?curid=23849259", "title": "List of first-person shooter engines", "text": "List of first-person shooter engines\n\nThis is a sortable list of first-person shooter engines.\n\nSome features may be integrated into engines. For instance for trees and foliage a special \"engine\" is available, SpeedTree, that does just that (or could be integrated into general engines). The Euphoria character's 3D animating engine can be used independently but is integrated in the Rockstar Advanced Game Engine and the game \"Grand Theft Auto IV\".\n\n"}
{"id": "12564389", "url": "https://en.wikipedia.org/wiki?curid=12564389", "title": "Marsh funnel", "text": "Marsh funnel\n\nThe Marsh funnel is a simple device for measuring viscosity by observing the time it takes a known volume of liquid to flow from a cone through a short tube. It is standardized for use by mud engineers to check the quality of drilling mud. Other cones with different geometries and orifice arrangements are called \"flow cones\", but have the same operating principle.\n\nIn use, the funnel is held vertically with the end of the tube closed by a finger. The liquid to be measured is poured through the mesh to remove any particles which might block the tube. When the fluid level reaches the mesh, the amount inside is equal to the rated volume. To take the measurement, the finger is released as a stopclock is started, and the liquid is allowed to run into a measuring container. The time in seconds is recorded as a measure of the viscosity.\nBased on a method published in 1931 by H.N.Marsh, a Marsh cone is a flow cone with an aspect ratio of 2:1 and a working volume of at least a litre. A Marsh funnel is a Marsh cone with a particular orifice and a working volume of 1.5 litres. It consists of a cone 6 inches (152 mm) across and 12 inches in height (305 mm) to the apex of which is fixed a tube 2 inches (50.8 mm) long and 3/16 inch (4.76 mm) internal diameter. A 10-mesh screen is fixed near the top across half the cone.\n\nIn American practice (and most of the oil industry) the volume collected is a quart. If water is used, the time should be 26 +/- 0.5 seconds. If the time is less than this the tube is probably enlarged by erosion, if more it may be blocked or damaged, and the funnel should be replaced. In some companies, and Europe in particular, the volume collected is a litre, for which the water funnel time should be 28 seconds. Marsh himself collected 0.50 litre, for which the time was 18.5 seconds.\n\nThe Marsh funnel time is often referred to as the Marsh funnel viscosity, and represented by the abbreviation FV. The unit (seconds) is often omitted. Formally, the volume should also be stated.\nThe (quart) Marsh funnel time for typical drilling muds is 34 to 50 seconds, though mud mixtures to cope with some geological conditions may have a time of 100 or more seconds.\n\nWhile the most common use is for drilling muds, which are non-Newtonian fluids, the Marsh funnel is not a rheometer, because it only provides one measurement under one flow condition. However the effective viscosity can be determined from following simple formula.\n\nμ = ρ (t - 25)\n\nwhere μ = effective viscosity in centipoise ρ = density in g/cm³t = quart funnel time in seconds\n\nFor example, a mud of funnel time 40 seconds and density 1.1 g/cm³ has an effective viscosity of about 16.5 cP. For the range of times of typical muds above, the shear rate in the Marsh funnel is about 2000 .\n\nThe term Marsh cone is also used in the concrete and oil industries. European standard EN445 and American standard C939 for measuring the flow properties of cement grout mixtures specify a funnel similar to the Marsh cone. Some manufacturers supply devices which they call Marsh cones, with removable tubes with size ranges from 5 to 15 mm. These can be used for quality control by selecting a tube which gives a convenient time, say 30 to 60 seconds.\n\n"}
{"id": "26712372", "url": "https://en.wikipedia.org/wiki?curid=26712372", "title": "Mary Engle Pennington", "text": "Mary Engle Pennington\n\nMary Engle Pennington (October 8, 1872 – December 27, 1952) was an American bacteriological chemist and refrigeration engineer.\n\nMary Engle Pennington was born in Nashville, Tennessee; her parents were Henry and Sarah B. (Malony) Pennington. Shortly after her birth, her parents moved to Philadelphia, Pennsylvania, to be closer to Sarah Pennington's Quaker relatives. Mary Pennington demonstrated an early interest in chemistry. She entered the University of Pennsylvania in 1890 and completed the requirements for a B.S. degree in chemistry with minors in botany and zoology in 1892. However, since the University of Pennsylvania did not grant degrees to women at this time, she was given a certificate of proficiency instead of a degree.\n\nPennington received her Ph.D. from the University of Pennsylvania in 1895, and was a university fellow in botany there in 1895–96. She was a fellow in physiological chemistry at Yale in 1897–99, where she did research in physiological chemistry with Mendel. In 1898, she accepted a position with the Women's Medical College of Pennsylvania as Director of their Clinical Laboratory. She also served as a research worker in the department of hygiene at the University of Pennsylvania from 1898 to 1901, and was a bacteriologist with the Philadelphia Bureau of Health. In her position with the Bureau of Health, she was instrumental in improving sanitation standards for the handling of milk and milk products.\n\nIn 1905, Pennington began working for the U.S. Department of Agriculture as a bacteriological chemist. Her director at the Bureau of Chemistry, Harvey W. Wiley, encouraged her to apply for a position as chief of the newly created Food Research Laboratory, which had been established to enforce the Pure Food and Drug Act of 1906. She accepted the position in 1907. One of her major accomplishments was the development of standards for the safe processing of chickens raised for human consumption. She also served as head of an investigation of refrigerated boxcar design, and served on Herbert Hoover's War Food Administration during World War I.\n\nPennington's involvement with refrigerated boxcar design at the Food Research Laboratory led to an interest in the entire process of transporting and storing perishable food, including both refrigerated transport and home refrigeration. In 1919, Pennington accepted a position with a private firm, American Balsa, which manufactured insulation for refrigeration units. She left the firm in 1922 to start her own consulting business, which she ran until her retirement in 1952. She founded the Household Refrigeration Bureau in 1923 to educate consumers in safe practices in domestic refrigeration. Much of her work in the 1920s was supported by the National Association of Ice Industries (NAII), an association of independent icemakers and distributors who delivered ice to the home for use in iceboxes, before the widespread availability of electric refrigerators. With NAII support, she published pamphlets on home food safety, including \"The Care of the Child's Food in the Home\" (1925) and \"Cold is the Absence of Heat\" (1927).\n\nShe contributed to many scientific and medical journals and was a member of the American Chemical Society and the Society of Biological Chemists. She was a fellow of the American Association for the Advancement of Science, and a member of the Philadelphia Pathological Society, Sigma XI, and the Kappa Kappa Gamma Sorority.\n\nMary Engle Pennington was the recipient of the Garvan-Olin Medal, the highest award given to women in the American Chemical Society. She is also an inductee of both the National Women's Hall of Fame and the ASHRAE Hall of Fame. In 2018, she was inducted into the National Inventors Hall of Fame.\n\n\n"}
{"id": "9268695", "url": "https://en.wikipedia.org/wiki?curid=9268695", "title": "Ministry of Information and Communication Technology", "text": "Ministry of Information and Communication Technology\n\nMinistry of Information and Communication Technology or Communications Technology may refer to:\n\n"}
{"id": "10874176", "url": "https://en.wikipedia.org/wiki?curid=10874176", "title": "Mullion wall", "text": "Mullion wall\n\nA mullion wall is a structural system in which the load of the floor slab is taken by prefabricated panels around the perimeter. Visually the effect is similar to the stone mullioned windows of Perpendicular Gothic or Elizabethan architecture.\n\nThe technology was devised by George Grenfell Baines and the engineer Felix Samuely in order to cope with material shortages at the Thomas Linacre School, Wigan (1952) and refined at the Shell Offices, Stanlow (1956), the Derby Colleges of Technology and Art (1956–64). and Manchester University Humanities Building (1961–67) \n\nA similar concept to the mullion wall was adopted by Eero Saarinen at the US Embassy, London (1955–60) and by Minoru Yamasaki at World Trade Center, New York (1966–73).\n\n"}
{"id": "22580", "url": "https://en.wikipedia.org/wiki?curid=22580", "title": "OSGi", "text": "OSGi\n\nThe OSGi Alliance, formerly known as the Open Services Gateway initiative, is an open standards organization founded in March 1999 that originally specified and continues to maintain the OSGi standard.\n\nThe OSGi specification describes a modular system and a service platform for the Java programming language that implements a complete and dynamic component model, something that does not exist in standalone Java/VM environments. Applications or components, coming in the form of bundles for deployment, can be remotely installed, started, stopped, updated, and uninstalled without requiring a reboot; management of Java packages/classes is specified in great detail. Application life cycle management is implemented via APIs that allow for remote downloading of management policies. The service registry allows bundles to detect the addition of new services, or the removal of services, and adapt accordingly.\n\nThe OSGi specifications have evolved beyond the original focus of service gateways, and are now used in applications ranging from mobile phones to the open-source Eclipse IDE. Other application areas include automobiles, industrial automation, building automation, PDAs, grid computing, entertainment, fleet management and application servers.\n\nThe OSGi specification is developed by the members in an open process and made available to the public free of charge under the OSGi Specification License. The OSGi Alliance has a compliance program that is open to members only. As of November 2010, there are seven certified OSGi framework implementations. A separate page lists both certified and non-certified OSGi Specification Implementations, which include OSGi frameworks and other OSGi specifications.\n\nOSGi is a Java framework for developing and deploying modular software programs and libraries. Each bundle is a tightly coupled, dynamically loadable collection of classes, jars, and configuration files that explicitly declare their external dependencies (if any).\n\nThe framework is conceptually divided into the following areas:\n\nA bundle is a group of Java classes and additional resources equipped with a detailed manifest codice_1 file on all its contents, as well as additional services needed to give the included group of Java classes more sophisticated behaviors, to the extent of deeming the entire aggregate a component.\n\nBelow is an example of a typical codice_1 file with OSGi Headers:\n\nThe meaning of the contents in the example is as follows:\n\n\nA Life Cycle layer adds bundles that can be dynamically installed, started, stopped, updated and uninstalled. Bundles rely on the module layer for class loading but add an API to manage the modules in run time. The life cycle layer introduces dynamics that are normally not part of an application. Extensive dependency mechanisms are used to assure the correct operation of the environment. Life cycle operations are fully protected with the security architecture.\nBelow is an example of a typical Java class implementing the codice_3 interface:\nThe OSGi Alliance has specified many services. Services are specified by a Java interface. Bundles can implement this interface and register the service with the Service Registry. Clients of the service can find it in the registry, or react to it when it appears or disappears.\n\nThe table below shows a description of OSGi System Services:\n\nThe table below shows a description of OSGi Protocol Services:\n\nThe table below shows a description of OSGi Miscellaneous Services:\nThe OSGi Alliance was founded by Ericsson, IBM, Motorola, Sun Microsystems and others in March 1999. Before incorporating as a nonprofit corporation, it was called the Connected Alliance.\n\nAmong its members are () more than 35 companies from quite different business areas, for example Adobe Systems, Deutsche Telekom, Hitachi, IBM, Liferay, Makewave (formerly Gatespace Telematics), NEC, NTT, Oracle, Orange S.A., ProSyst, Salesforce.com, Siemens, Software AG and TIBCO Software.\n\nThe Alliance has a board of directors that provides the organization's overall governance. OSGi officers have various roles and responsibilities in supporting the alliance. Technical work is conducted within Expert Groups (EGs) chartered by the board of directors, and non-technical work is conducted in various working groups and committees. The technical work conducted within Expert Groups include developing specifications, reference implementations, and compliance tests. These Expert Groups have produced five major releases of the OSGi specifications ().\n\nDedicated Expert Groups exist for the enterprise, mobile, vehicle and the core platform areas.\n\nThe Enterprise Expert Group (EEG) is the newest EG and is addressing Enterprise / Server-side applications.\nIn November 2007 the Residential Expert Group (REG) started to work on specifications to remotely manage residential/home-gateways.\nIn October 2003, Nokia, Motorola, IBM, ProSyst and other OSGi members formed a Mobile Expert Group (MEG) that will specify a MIDP-based service platform for the next generation of smart mobile phones, addressing some of the needs that CLDC cannot manage - other than CDC. MEG became part of OSGi as with R4.\n\n\n\n\n\n\n"}
{"id": "53571464", "url": "https://en.wikipedia.org/wiki?curid=53571464", "title": "Orphaned and abandoned wells in the United States", "text": "Orphaned and abandoned wells in the United States\n\nThough different jurisdictions have varying criteria for what exactly qualifies as an abandoned oil well, generally speaking an oil well is considered abandoned when it has been permanently taken out of production. Similarly, orphaned wells may have different legal definitions across different jurisdictions, but can be thought of as wells whose legal owner it is not possible to determine. State legislatures in the United States have specific definitions based on local needs and priorities. For example, the section on abandoned wells in Texas' Natural Resource Code defines an \"inactive well\" as \"an unplugged well that has had no reported production, disposal, injection, or other permitted activity for a period of greater than 12 months.\" Pennsylvania's definition of abandoned well includes not producing for 12 months, \"considered dry and not equipped for production within 60 days after drilling, redrilling or deepening, and from which the equipment needed to extract resources or produce energy has been removed.\" Ohio legislation defines \"idle and orphaned wells\" based on whether or not a well bond has been forfeited or the money to plug it is unavailable. It defines a \"temporary inactive well status\" as not having produced for two (non-horizontal wells) or eight (horizontal wells) statutorily defined reporting periods or one that has produced \"less than 100,000 cubic feet of natural gas or 15 barrels of crude oil.\"\n\nOrphaned and abandoned wells can cause environmental damage by leaking pollutants into the atmosphere or water supplies. Important determinants of how much orphaned and abandoned wells impact the environment include the techniques used and precautions taken when first drilling the well, whether it is a gas well, oil well, or combined oil and gas well, and if and how the well was sealed.\n\nIf wells are not properly sealed when orphaned or abandoned, there can allow oil and gas to contaminate groundwater. It is also possible for orphaned and abandoned wells to be significant emitters of methane into the atmosphere. Furthermore, brine present in wells dug into shale formations can contain some radioactive and toxic substances that contaminate groundwater if the well leaks. Plugging wells can reduce the risk of explosions and protect groundwater, but does not always prevent methane emissions. The costs to mitigate the impact of orphaned and abandoned wells varies, but may include removing all equipment from the site, restoring the land and topsoil, and planting local species, in addition to plugging the well itself. For example, plugging a well and restoring the surrounding land costs an average of $100,000 for wells in the Marcellus Shale.\n\nOne problem with studying the impacts of orphaned and abandoned wells is that data about them can be scarce and incomplete. In the United States, it is possible for wells to have been orphaned or abandoned for over a century, and information about them, if it exists at all, can be difficult to find.\n\nOne way to encourage well owners not to abandon or orphan wells and to make sure wells are safely abandoned is to use well bonds. These are bonds paid by well operators to a surety company and are held by an obligee (state or federal entity) until the well has been satisfactorily plugged and the land surface restored. A significant challenge of making well bonds an effective policy tool is to set their price to a point that does not make market entry prohibitively expensive, but also does not incentivize well operators to forfeit the bond instead of undertaking the abandonment requirements specified in local law.\n\nAnother way to encourage well owners not to abandon or orphan wells is to retrofit oil and gas wells to produce geothermal energy. One benefit of this approach is that it is less expensive to retrofit an abandoned well to produce geothermal energy than it is to drill a new oil or gas well. It also saves the cost of exploring sites for geothermal fields. Avoiding new exploration and drilling avoids the environmental impacts of these activities. However, geothermal fluids can contain environmentally hazardous chemicals such as hydrogen sulfide, ammonia, methane, arsenic, mercury, and lead.\n\nA third option is to mandate that well operators establish reclamation trusts which would be used to pay reclamation costs if the operator does not perform the necessary plugging and land restoration within a given time period after abandoning the well. This policy option has been used to mitigate the environmental impact of mines in the United States as part of a combined command-and-control and market incentive policy response to environmental protection. One risk attached to this policy option is that if wells become economically unproductive before the period planned for in the trust agreement, the abandoned well could become a liability held by the relevant government authority.\n"}
{"id": "6061779", "url": "https://en.wikipedia.org/wiki?curid=6061779", "title": "Polish Enigma double", "text": "Polish Enigma double\n\nA Polish Enigma \"double\" was a machine produced by the Polish Cipher Bureau that replicated the German Enigma rotor cipher machine. The Enigma double was one result of Marian Rejewski's remarkable achievement of determining the wiring of the Enigma's rotors and reflectors.\n\nThe Polish Cipher Bureau recognized that the Germans were using a new cipher. The Germans had mistakenly shipped a cipher machine to Poland; their attempts to recover a shipment raised the suspicions of Polish customs, and the Polish Cipher Bureau learned that the Germans were using an Enigma machine. The Bureau purchased a commercial Enigma machine, and it attempted but failed to break the cipher.\n\nIn December 1932, the Polish Cipher Bureau tasked Marian Rejewski with breaking the Enigma cipher machine. A French spy had obtained some material about the Enigma, and the French had provided the material to the Polish Cipher Bureau. By that time, the commercial Enigma had been extended to use a plugboard. Rejewski made rapid progress and was able to determine the wiring of the military Enigma. The Bureau modified its commercial Enigma rotors, reflector, and internal wiring to match the military Enigma. The commercial Enigma did not have a plugboard, but the plugboard could be simulated by relabeling the keys and the lamps. The result was the first Enigma double.\n\nIn February 1933, the Polish Cipher Bureau ordered fifteen \"doubles\" of the military Enigma machine from the AVA Radio Manufacturing Company, in Warsaw. Ultimately, about seventy such functional replicas were produced.\n\nIn August 1939, following the tripartite meeting of the French, British and Polish cryptanalysts held near Warsaw on 25 and 26 July, two Enigma replicas were passed to Poland's allies, one being sent to Paris and one to London. Until then, German military Enigma traffic had defeated the British and French, and they had faced the disturbing prospect that German communications would remain \"black\" to them for the duration of the coming war.\n\nAfter Germany invaded Poland in September 1939 and key Polish Cipher Bureau personnel had been evacuated to France, the Cipher Bureau resumed its interrupted work at \"PC Bruno\", outside Paris. The Poles had only three replica Enigma machines to work with, and these were wearing out from round-the-clock use. French Army intelligence officer Gustave Bertrand ordered parts for forty machines from a French precision-mechanics firm. Manufacture proceeded sluggishly, however, and it was only after the fall of France and the opening of underground work in southern France's Free Zone in October 1940 that four machines were finally assembled.\n\n\n"}
{"id": "46707528", "url": "https://en.wikipedia.org/wiki?curid=46707528", "title": "Rachel Jacobs", "text": "Rachel Jacobs\n\nRachel Jacobs (October 3, 1975 – May 12, 2015) was an American social entrepreneur and CEO of a tech company. She was killed at age 39 in the 2015 Philadelphia train derailment while commuting between her home in New York and the Philadelphia offices of ApprenNet, the educational technology company she had recently joined as CEO.\n\nJacobs grew up in Huntington Woods, Michigan, the daughter of Gilda Jacobs, a former Michigan state senator. She was a 1993 graduate of Berkley High School, a 1997 graduate of Swarthmore College, and a 2002 graduate of Columbia Business School. Jacobs moved to New York City in 2000.\n\nJacobs was CEO of ApprenNet, a video-learning tech company which was cofounded by Karl Okamoto, a law professor at Drexel University. The company \"provides tools for instructors to create video-based learning exercises.\" Before joining ApprenNet, Jacobs worked for the education-technology firm Ascend Learning where she was vice president of business innovation. According to Okamoto, the two met because ApprenNet was doing business with Ascend and Jacobs \"was our customer before she became our colleague.\"\n\nIn a career \"The Washington Post\" described as \"moving from one big job to the next,\" Jacobs' first job out of business school was as a manager at the Pragma Corporation, based in Kyrgyzstan, where she helped the government develop IT strategies. She next worked for the Eurasia Group, a political risk consultancy. In 2007, Jacobs joined McGraw Hill, where she \"led the expansion of McGraw-Hill's career-learning business into China, India and the Middle East.\"\n\nJacobs was hired to lead ApprenNet, which is backed by the National Science Foundation, in an expansion from its original focus on educating lawyers, into a phase to apply its online teaching technology to training health-care professionals, college level instruction and training for K-12 teachers.\n\nFollowing Jacobs's death, ApprenNet merged with Handsfree Learning of California.\n\nIn 2009, Jacobs organized 635 Mile Road, a non-profit organization of former Detroit-area residents \"dedicated to improving the flow of funds, ideas and energy between native Detroiters.\" By the end of 2010, 635 Mile Road became Detroit Nation. The organization soon had chapters in chapters in New York, Seattle, Chicago, and other cities dedicated to helping Detroit natives who continue supporting the region after moving away. The group offers free consulting to Detroit-based, grassroots entrepreneurs and artists. Through Detroit Nation, Jacobs helped arrange the Detroit Symphony Orchestra's first Carnegie Hall concert in 17 years. By the time the group held a Detroit ex-pats meet-up in their hometown in 2014, the group had 10,000 members online. Detroit Nation raised money for Detroit charities with fund-raising events held by expats in Seattle, Chicago, New York and other cities, but, as Jacobs explained to an interviewer in 2011, the ex-pats also provide \"human capital... helping organizations to better integrate social networking tools, develop marketing materials, or structure the organization and bring in larger donors.\"\n\nIn 2014, Jacobs was one of 150 business leaders invited to attend the first annual Detroit Homecoming. Interviewed during the Homecoming, Jacobs told The Detroit News that, \"Detroit doesn't need ideas. It has phenomenal ideas. It needs doers... My challenge to expats is who will raise their hand and be a doer in Detroit?\"\n\nShe was married to Todd Waldman; the couple had a son, two years old at the time of her death. Jacobs and Waldman, who works for Navigant Consulting, had been considering whether to move the family to Philadelphia.\n\nMedia attention focused on Jacobs in the hours after the crash because she was known to have been on the train but had not been identified among the injured and the dead. ApprenNet co-founder and COO Emily Foote went to the crash scene to try to locate Jacobs by showing her photograph to survivors and rescue workers.\n\nHundreds of people attended memorial services held at the Greenwich Village campus of Hebrew Union College on Saturday, May 16, 2015. An estimated 1,500 people attended the funeral held in suburban Detroit on May 18, 2015.\n\nAccording to CNN, Jacobs' family have set up two memorial funds in her honor, including the \"Rachel Jacobs Detroit Nation Fund\" to benefit Detroit Nation and a scholarship fund at Columbia Business School to benefit social entrepreneurs.\n\nFormer campers and counselors who had spent summers with Rachel at Tamarack Camps, located in the state of Michigan, came together and raised money to dedicate the Rachel Jacobs Tikkun Olam Leadership Award. Rachel often spoke to others about how her camp experiences help shape her identity during her formative years. Beginning in 2016, this award will go to one summer staff member each year who embodies Rachel's values for healing or repairing the world (the Hebrew translation of Tikkun Olam).\n\n\"Daily News\" columnist Mike Lupica demanded a full investigation of Amtrak safety from the National Transportation Safety Board to prevent future tragedies in which a two-year-old child \"grows up without a mother.\" Assigned to ride on the first train through Philadelphia after the derailment, columnist Ronnie Polaneczky wrote for \"The Philadelphia Inquirer\" that, \"The next time I sigh that I can't afford to fix the roof or haven't time to help a friend move across the country, I will try to remind myself that Rachel Jacobs would have given anything to still be here to indulge such petty worries.\"\n\n"}
{"id": "226769", "url": "https://en.wikipedia.org/wiki?curid=226769", "title": "Roaming", "text": "Roaming\n\nRoaming is a wireless telecommunication term typically used with mobile devices (like mobile phones). It refers to the mobile phone being used outside the range of its home network and connects to another available cell network.\n\nIn more technical terms, roaming refers to \"the ability for a cellular customer to automatically make and receive voice calls, send and receive data, or access other services, including home data services, when travelling outside the geographical coverage area of the home network, by means of using a visited network\". For example: should a subscriber travel beyond their cell phone company's transmitter range, their cell phone would automatically hop onto another phone company's service, if available.\n\nThe process is supported by the Telecommunication processes of mobility management, authentication, authorization and accounting billing procedures (known as AAA or 'triple A').\n\nRoaming is divided into \"SIM-based roaming\" and \"username/password-based roaming\", whereby the technical term \"roaming\" also encompasses roaming between networks of different network standards, e.g. WLAN (wireless local area network) or GSM. Device equipment and functionality, such as SIM card capability, antenna and network interfaces, and power management, determine the access possibilities.\n\nUsing the example of WLAN/GSM roaming, the following scenarios can be differentiated (cf. GSM Association Permanent Reference Document AA.39):\n\nAlthough these user/network scenarios focus on roaming from GSM network operator's networks, clearly roaming can be bi-directional, i.e. from public WLAN operators to GSM networks. Traditional roaming in networks of the same standard, e.g. from a WLAN to a WLAN or a GSM network to a GSM network, has already been described above and is likewise defined by the foreignness of the network based on the type of subscriber entry in the home subscriber register.\n\nIn the case of session continuity, seamless access to these services across different access types is provided.\n\n\"Home network\" refers to the network the subscriber is registered with. \n\n\"Visitor network\" refers to the network a subscriber roams temporarily and is outside the bounds of the \"home network\".\n\nThe legal roaming business aspects negotiated between the roaming partners for billing of the services obtained are usually stipulated in so called \"roaming agreements.\" The GSM Association broadly outlines the content of such roaming agreements in standardized form for its members. For the legal aspects of authentication, authorization and billing of the visiting subscriber, the roaming agreements typically can comprise minimal safety standards, as e.g. location update procedures or financial security or warranty procedures.\n\nThe details of the roaming process differ among types of cellular networks, but in general, the process resembles the following:\n\nLocation updating is the mechanism that is used to determine the location of an MS in the idle state (connected to the network, but with no active call).\n\nIt occurs for example when a call is made to a roaming cell phone.\n\nSignaling process:\n\nIn order that a subscriber is able to register on to a visited network, a roaming agreement needs to be in place between the visited network and the home network. This agreement is established after a series of testing processes called IREG (International Roaming Expert Group) and TADIG (Transferred Account Data Interchange Group). While the IREG testing is to test the proper functioning of the established communication links, the TADIG testing is to check the billability of the calls.\n\nThe usage by a subscriber in a visited network is captured in a file called the TAP (Transferred Account Procedure) for GSM / CIBER (Cellular Intercarrier Billing Exchange Record) for CDMA, AMPS etc... file and is transferred to the home network. A TAP/CIBER file contains details of the calls made by the subscriber viz. location, calling party, called party, time of call and duration, etc. The TAP/CIBER files are rated as per the tariffs charged by the visited operator. The home operator then bills these calls to its subscribers and may charge a mark-up/tax applicable locally.\nAs recently many carriers launched own retail rate plans and bundles for Roaming, TAP records are generally used for wholesale Inter-Operators settlements only\n\nRoaming fees are charged on a per-minute basis for wireless voice service, per text message and per megabyte per second for data service, and they are typically determined by the service provider's pricing plan.\n\nSeveral carriers in both the United States and India have eliminated these fees in their nationwide pricing plans. All of the major carriers now offer pricing plans that allow consumers to purchase nationwide roaming-free minutes. However, carriers define \"nationwide\" in different ways. For example, some carriers define \"nationwide\" as anywhere in the U.S., whereas others define it as anywhere within the carrier's network.\n\nIn the UK, the main network providers generally send text alerts to advise users that they will now be charged international rates so it is clear when this will apply. UK data roaming charges abroad vary depending on the nature of the phone agreement (either pay as you go or monthly contracts). Some carriers, including T-Mobile and Virgin Mobile, do not allow pay as you go customers to use international roaming without pre-purchase of an international \"add on\" or \"bolt on.\"\n\nAn operator intending to provide roaming services to visitors publishes the tariffs that would be charged in their network at least sixty days prior to its implementation under normal situations. The visited operator tariffs may include tax, discounts etc. and would be based on duration in case of voice calls. For data calls, the charging may be based on the data volume sent and received. Some operators also charge a separate fee for call setup i.e. for the establishment of a call. This charge is called a \"flagfall\" charge.\n\nIn the European Union, the regulation on roaming charges has been in force since 30 June 2007, forcing service providers to lower their roaming fees across the 28-member bloc. It later also included EEA member states. The regulation sets a price cap of €0.39 (€0.49 in 2007, €0.46 in 2008, €0.43 in 2009) per minute for outgoing calls, and €0.15 (€0.24 in 2007, €0.22 in 2008, €0.19 in 2009) per minute for incoming calls - excluding tax. If the Commission is satisfied that competition will continue to keep prices at this level, or lower, the regulation will expire in mid-2012. Since mid-2009 there is also an €0.11 (excluding tax) maximum price for SMS text message included into this regulation.\n\nOn 11 June 2013, the European Commission voted to end mobile roaming charges. It is expected to come into force on 1 July 2014.\n\nIf everything goes according to the plan, as of December 15, 2015, there will be no roaming charges within the European Union. While the European Commission (EC) believes that ending roaming charges will stimulate entrepreneurship and trade, mobile operators have their doubts about the changes.\n\nHere is a list of operators that started to renounce roaming charges before 15 December 2015:\n\nOn 15 June 2017 a bill dubbed \"Roam like at Home\" signed by the European Parliament and Commission earlier in May of the same year came into force that abolished all roaming charges within the EU, Iceland, Liechtenstein and Norway.\n\nCountries that do not share a supra-national authority have also begun examining the provision of international roaming services. In April 2011, Singapore and Malaysia announced that they had agreed with operators to reduce voice and SMS rates for roaming between their two countries. In August 2012, Australia and New Zealand published a draft report proposing coordinated action on roaming services. This was followed by a final report in February 2013 recommending that the two countries equip their telecommunications regulators with an extended palette of regulatory remedies, when they investigate international roaming. The Australian and New Zealand prime ministers subsequently announced that they would introduce legislation to effect the recommendations of the final report.\n\nThis type refers to the ability of moving from one region to another region inside national coverage of the mobile operator (\"internal roaming\"). Initially, operators may have provided commercial offers restricted to a region (sometimes to a town). Due to the success of GSM and the decrease in cost, regional roaming is rarely offered to clients except in nations with wide geographic areas like the USA, Russia, India, etc., in which there are a number of regional operators.\n\nIn Russia even country-wide operators charge different tariffs depending on whether the users are within or outside of their \"home region\". A number of legislative attempts to remove the \"internal roaming\" failed due to opposition from operators. Following the annexation of Crimea in 2014 the Russian operators are facing significant criticism as they do not offer their services inside Crimea directly, even though formally it's recognized as a regular federal subject inside Russia.\n\nThis type refers to the ability to move from one mobile operator to another in the same country. For example, a subscriber of T-Mobile USA who is allowed to roam on AT&T Mobility's service would have national roaming rights.\nFor commercial and license reasons, this type of roaming is not allowed unless under very specific circumstances and under regulatory scrutiny. This has often taken place when a new company is assigned a mobile telephony license, to create a more competitive market by allowing the new entrant to offer coverage comparable to that of established operators (by requiring the existing operators to allow roaming while the new entrant has time to build up its own network).\n\nIn a country like India, where the number of regional operators is high and the country is divided into \"telecom circles,\" this type of roaming is common. Following the launch of the Pebble Network in the UK on 15 July 2015, national roaming has been possible across the major UK networks at no additional cost using a Pebble Network SIM card.\n\nThis type of roaming refers to the ability to move to a foreign service provider's network. It is, consequently, of particular interest to international tourists and business travellers. Broadly speaking, international roaming is easiest using the GSM standard, as it is used by over 80% of the world's mobile operators. However, even then, there may be problems, since countries have allocated different frequency bands for GSM communications (there are two groups of countries: most GSM countries use 900/1800 MHz, but the United States and some other countries in the Americas have allocated 850/1900 MHz): for a phone to work in a country with a different frequency allocation, it must support one or both of that country's frequencies, and thus be tri or quad band. If international roaming allows the traveller to stay connected during their trip, it can also generate significant costs for users. In fact, the use of mobile networks outside its original country can lead to significant billing by its original mobile data operator.\n\nThis type refers to roaming between two standards. This term is now widely used in mobile communications where especially CDMA customers want to use their phone in areas where there is no CDMA network or there is no roaming agreement in place to support roaming on the used standard. In Europe there are hardly any CDMA networks. Most CDMA customers originate from the Americas or the Far East. In order to enable them to roam in Europe inter-standard roaming is the solution. The CDMA customers arriving in Europe can register on the available GSM networks.\n\nSince mobile communication technologies have evolved independently across continents, there is significant challenge in achieving seamless roaming across these technologies. Typically, these technologies were implemented in accordance with technological standards laid down by different industry bodies and hence the name. A number of the standards making industry bodies have come together to define and achieve interoperability between the technologies as a means to achieve inter-standards roaming. This is currently an ongoing effort.\n\n\"Mobile signature Roaming\" allows an access point to get a Mobile Signature from any end-user, even if the AP and the end-user have not contracted a commercial relationship with the same MSSP. Otherwise, an AP would have to build commercial terms with as many MSSPs as possible, and this might be a cost burden. This means that a Mobile Signature transaction issued by an Application Provider should be able to reach the appropriate MSSP, and this should be transparent for the AP.\n\nNetwork elements belonging to the same Operator but located in different areas (a typical situation where assignment of local licenses is a common practice) pair depends on the switch and its location. Hence, software changes and a greater processing capability are required, but furthermore this situation could introduce the fairly new concept of roaming on a per MSC basis instead of per Operator basis. But this is actually a burden, so it is avoided.\n\nThis type refers to customers who purchase service with a mobile phone operator intending to permanently roaming, or be off-network. This becomes possible because of the increasing popularity and availability of \"free roaming\" service plan, where there is no cost difference between on and off network usage. The benefits of getting service from a mobile phone operator that is not local to a user can include cheaper rates, or features and phones that are not available on their local mobile phone operator, or to get to a particular mobile phone operator's network to get free calls to other customers of that mobile phone operator through a free unlimited mobile to mobile feature. \n\nMost mobile phone operators will require the customer's living or billing address be inside their coverage area or less often inside the government issued radio frequency license of the mobile phone operator, this is usually determined by a computer estimate because it is impossible to guarantee coverage. If a potential customer's address is not within the requirements of that mobile phone operator, they will be denied service. In order to permanently roam customers may use a false address and online billing, or a relative or friend's address which is in the required area, and a 3rd party billing option.\n\nMost mobile phone operator discourage or prohibit permanent roaming since they must pay per minute rates to the network operator their customer is roaming onto to. This is because they can not pass that extra cost onto customers (\"free roaming\").\n\nRoaming calls within a local tariff area, when at least one of the phones belong outside that area. Usually implemented with \"trombone routing\" also known as \"tromboning\".\n\n\n"}
{"id": "16796181", "url": "https://en.wikipedia.org/wiki?curid=16796181", "title": "Stall torque", "text": "Stall torque\n\nStall torque is the torque produced by a mechanical device whose output rotational speed is zero. It may also mean the torque load that causes the output rotational speed of a device to become zero, i.e., to cause stalling. Electric motors, steam engines and hydrodynamic transmissions are all capable of developing torque when stalled.\n\nElectric motors continue to provide torque when stalled. However, electric motors left in a stalled condition are prone to overheating and possible damage since the current flowing is maximum under these conditions.\n\nThe maximum torque an electric motor can produce in the long term when stalled without causing damage is called the maximum continuous stall torque\n\nA hydrodynamic torque multiplier (torque converter) produces stall torque when the load prevents the turbine (output stage) from rotating while the pump (input stage) is being driven. In most cases, damage due to overheating occurs if the stall condition persists for any significant length of time. \n\nIn the case of a petrol (gasoline) or Diesel engine, the stall torque may refer to the torque load that causes the engine to stall. The actual amount of torque is dependent on engine RPM and throttle opening.\n"}
{"id": "38273988", "url": "https://en.wikipedia.org/wiki?curid=38273988", "title": "Sukarne", "text": "Sukarne\n\nSuKarne is a Mexican multinational corporation based in Culiacán, Mexico, that operates in the food protein industry. It is part of a family of companies under Grupo SuKarne.\n\nThe company annually exports the largest percentage of beef, pork, and chicken in Mexico, with at least 76% of the market. In 2010, the company had estimated world-wide beef sales of US$883 million, after an export growth of 60% in just one year.\n\nThe company makes a wide variety of animal-based and prepared products at its food processing plants. Currently, Grupo SuKarne generates more than 7,000 direct jobs, serves more than 40,000 customers, and maintains operations with companies in 18 countries. Its chain of production includes more than 80,000 agricultural and livestock providers.\n\nSuKarne is also one of the largest North American marketers of value-added chicken, beef, and pork to retail grocers, broad-line food service distributors, and national fast-food and full-service restaurant chains, fresh beef and pork, frozen and fully cooked chicken, and case-ready beef and pork. It supplies many of the largest players in the food industry, including franchise restaurants, Walmart, HEB, and small restaurant businesses.\n\nSuKarne's current chairman is Jesus Vizcarra Calderon .\n\nAs a young man, the current chairman of SuKarne, Jesus Vizcarra Calderon joined his family business, Corrales Vizcarra, an auxiliary cattle marketing and channel. In 1980, he was responsible for the general management. By 1985, Jesus took the chair of the company and founded SuKarne.\n\nIn October 2011, the 'Latin Business Association' recognized Calderon for his career in SuKarne and his strong push to bring health services to the poor people through a large, successful program called \"Salud Digna\".\n\nAccording to \"Expansion Magazine\", he ranks 67th among the 100 most important businessmen in Mexico.\n\nIn 2012, SuKarne announced it is investing $110 million to build the largest beef production and processing complex in Durango, Mexico. The complex is expected to create about 1,200 direct jobs and 6,000 indirect jobs, in addition to 680 jobs for initial construction.\n\n\n"}
{"id": "7542014", "url": "https://en.wikipedia.org/wiki?curid=7542014", "title": "Sun Modular Datacenter", "text": "Sun Modular Datacenter\n\nSun Modular Datacenter (Sun MD, known in the prototype phase as Project Blackbox) is a portable data center built into a standard 20-foot intermodal container (shipping container) manufactured and marketed by Sun Microsystems (acquired in 2010 by Oracle Corporation). An external chiller and power were required for the operation of a Sun MD. A data center of up to 280 servers could be rapidly deployed by shipping the container in a regular way to locations that might not be suitable for a building or another structure, and connecting it to the required infrastructure. Sun stated that the system could be made operational for 1% of the cost of building a traditional data center.\n\nOn 14 July 2007, the SLAC National Accelerator Laboratory (SLAC) deployed a Sun MD containing 252 Sun Fire X2200 compute nodes as a compute farm. Other customers include Radboud University.\n\nIn 2009, the Internet Archive migrated its digital archive onto Sun Modular Datacenter.\n\nThe prototype was first announced as \"Project Blackbox\" in October 2006; the official product was announced in January 2008.\n\nA Project Blackbox with 1088 Advanced Micro Devices Opteron processors ranked #412 on the June 2007 TOP500 list.\n\nIn late 2003, employees of the Internet Archive wrote a paper proposing \"an outdoor petabyte JBOD NAS box\" of sufficient capacity to store the then-current Archive in a 40' shipping container. The first implementation of the concept have been realized using Sun Microsystems' Modular Datacenters in March 2009.\n\n\n"}
{"id": "14464294", "url": "https://en.wikipedia.org/wiki?curid=14464294", "title": "Targeted reinnervation", "text": "Targeted reinnervation\n\nTargeted reinnervation enables amputees to control motorized prosthetic devices and to regain sensory feedback. The method was developed by Dr. Todd Kuiken at Northwestern University and Rehabilitation Institute of Chicago and Dr. Gregory Dumanian at Northwestern University Division of Plastic Surgery.\n\nTargeted reinnervation has an efferent and an afferent component. Targeted muscle reinnervation is a method by which a spare muscle (the target muscle) of an amputated patient is denervated (its original nerves cut and/or de-activated), then reinnervated with residual nerves of the amputated limb. The resultant EMG signals of the targeted muscle now represent the motor commands to the missing limb, and are used to drive a motorized prosthetic device.\n\nTargeted sensory reinnervation is a method by which skin near or over the targeted muscle is denervated, then reinnervated with afferent fibers of the remaining hand nerves. Therefore, when this piece of skin is touched, it provides the amputee with a sense of the missing arm or hand being touched.\n\nSeveral methods exist that seek to achieve advanced control of motorized neural prosthetics. Chronic brain implants record neuronal signals from the motor cortex, while methods such as EEG and fMRI obtain motor commands non-invasively. The recorded signals are decoded into electrical signals, and input into assistive devices or motorized prosthetics. Traditional myoelectric prostheses utilize surface EMG signals from the remains of the amputated limb. For example, a patient may flex a shoulder muscle in order to generate EMG signals that may be used to send “bend elbow” command to the prosthesis. However, there are shortcomings to all of these methods. Chronic implants fail over a period of time because neuronal signal degrade due to tissue immune response to foreign bodies. EEG and fMRI do not obtain as strong signals as direct electrode implant. Traditional myoelectric prostheses are unable to provide multiple control signals simultaneously, thus only one action can be performed at a time. They are also unnatural to use because the users have to use muscles (such as shoulder) that are not normally involved with lower arm functions to control lower arm functions (such as opening and closing hands). The solution to these problems could include a completely different concept of neural interface.\n\nTargeted reinnervation does not require any implants. Therefore, it does not have the issue of tissue foreign body response as chronic brain implant technology does. The targeted muscle acts as a natural amplifier for the neuronal signals produced by the transferred residual nerves. This is an advantage over technologies like EEG and fMRI that utilize weaker signals. With targeted reinnervation, multiple yet independent EMG signals can be produced, thus multiple functions of the artificial limb can be controlled simultaneously. For example, the patient would be able to perform actions such as throwing a ball relatively gracefully, exhibiting simultaneous control of elbow and hand. The control is also intuitive to the patient because the EMG signals are generated by transferred residual limb nerves, unlike traditional myoelectric prosthetics where EMG signals have to be generated by muscles normally not involved in arm or wrist functions. Also, existing commercially available myoelectric prostheses, such as powered wrists, elbows can be used. There is no need to develop specific prostheses for targeted reinnervation. By means of nerve transfer, targeted reinnervation can also provide sensory feedback, which has not been achieved by any other form of prosthetics aforementioned.\n\nThe goal of targeted muscle reinnervation is to transfer multiple nerves into separate regions of the targeted muscle, record multiple yet independent signals from the muscle regions, and to use the EMG signals to control a motorized prosthesis sophisticated enough to process multiple control signals.\n\nThe requirement to transplant multiple nerves into a muscle region originated from a hypothesis that hyper-reinnervation, by which an excessive amount of motor neurons transferred to a muscle, can increase the reinnervation of muscle fibers hence improving the recovery of paralyzed muscles. The hypothesis was tested on rat skeletal muscles and the result indicated that hyper-reinnervated muscles recovered more muscle mass and strength and more number of motor units were formed.\n\nThe first surgical patient was a bilateral shoulder disarticulation amputee. Both arms were entirely amputated at the shoulder level, with only the shoulder blades remaining. The pectoral muscles were chosen targets because they were close to the shoulder, and they were also biologically non-functional due to detachment from the amputated arm. The pectoral muscles were first denervated by cutting the original nerves that innervate them. The proximal ends of the original nerves were ligated to prevent them from reinnervating the pectoral muscle. Then the remnant arm nerves (brachial plexus) were transferred into the pectoral muscles. The musculocutaneous nerve was transferred to the clavicular head of the pectoralis major muscle; the median nerve was transferred to the upper sternal of the pectoralis major muscle; the radial nerve was transferred to the lower sternal head of the pectoralis major muscle. The pectoralis minor muscle was translocated from under the pectorialis major muscle to the lateral chest wall, so that its EMG signals would not interfere with those of the pectoralis major muscle, and it is also a fourth muscle target. The ulnar nerve was then transferred to the moved pectoralis minor muscle. The musculocutaneous, median, radial, and ulnar nerves (brachial plexus) were sewn onto the distal ends of the original pectoral muscle nerve fascicles and onto the muscle itself. Subcutaneous fat over the pectoral muscle was removed so that the electrodes can be as close to the muscle as possible to obtain optimal EMG signals.\n\nAbout 3 months after surgery, the patient had the first twitch in pectoral muscle when he attempted to bend his phantom elbow. Five months past surgery, he was able to contract four regions of pectoralis major muscle by attempting different moves. For example, when the patient attempted to bend his elbow, the muscle region beneath the clavicle contracted strongly. This was an indication of successful musculocutaneous nerve transfer because musculocutaneous nerve innervates biceps. The patient was then soon put to a training session and a testing session. During training session, the patient was sitting in an upright position and shown each of the 27 normal movements (such as shoulder adduction/abduction, hand open/close, elbow flexion/extension etc.) on a video. After each demonstration, the patient followed the movement 10 times, exerting a moderate force, held for 2.5 seconds. The patient was given 5 seconds of rest after each attempt. During the test session, the patient performed 5 sets of the 27 movements in random order. He was first shown a video of a movement, then asked to follow the repeated video of the same movement simultaneously after 2 seconds.\n\nA BioSemi Active II system (produced by BioSemi, Amsterdam, Netherlands) and a 127-channel electrode array were used to record monopolar EMG signals while patient was attempting movements during training and testing sessions. One hundred and fifteen electrodes were used to record EMG from the pectoral muscle; two electrodes were used to record from each of the deltoid, latissimus dorsi, supraspinatus, upper trapezius, middle trapezius and lower trapezius muscles. The electrodes were placed at a distance of 15mm from each other. To remove the artifact caused by body movement, the EMG signals were preliminarily filtered with a fifth order butterworth high-pass filter set at 5 Hz.\n\nThe major contaminant of the EMG signal was the ECG artifact. To remove ECG noise, an ECG template was constructed by averaging ECG complexes recorded when muscles were relaxed. The time between each ECG complex was used to calculate a representative inter-spike-interval. Detection of ECG spikes was calculated from the correlations between EMG and the ECG template. A threshold was set so that signals exceeding the threshold were marked as possible ECG spikes. The inter-spike-intervals of the possible spikes were then compared to the previously calculated representative inter-spike-interval to determine whether the possible spikes were to be accepted as ECG artifacts.\n\nAnother major task of processing the EMG signals is to eliminate crosstalk from other muscles. First, positions of and distance between electrodes are empirically determined to obtain strongest EMG thus least crosstalk. Setting a threshold above background noise and crosstalk from other muscles also helps eliminating crosstalk. Smaller muscle size and subcutaneous fat facilitate crosstalk. With a minimal level of less than 3mm subcutaneous fat, crosstalk is expected to be minimal in an area of 2–3 cm diameter.\n\nAfter surgery, the patient was fitted with his pre-surgery body-powered prosthesis on the right side and an experimental myoelectric prosthesis consisted of a Griefer terminal device, a power wrist rotator, a Boston digital arm, and an LTI-Collier Shoulder joint on the left side. Three strongest EMG signals were chosen from the successful nerve transfers: the musculocutaneous nerve, the median nerve and the radial nerve. The EMG resulting from contraction of muscle reinnvervated by median nerve was used to control hand closing movement; the EMG from musculocutaneous nerve was used to control elbow bending; the EMG from radial nerve was used to control wrist rotation and flexion.\n\nThe performances of these two prostheses were compared with a box-and-blocks test, where the patient was allowed 2 minutes to move one-inch cubes from one box to another, over a short wall. The result was quantified by the total number of blocks moved. To test the terminal device (“hand”), elbow and wrist rotator, the patient was administered a clothes-pin test, where he was asked to pick up clothes pins from a horizontal bar, rotate them, then put them on a higher vertical bar. The time used to move 3 clothes-pins was recorded. Both tests were repeated 3 times. The quantified results showed that the myoelectric prosthesis performed 246% better (moved 2.46 times more blocks) in box-and-blocks test, and 26.3% better (used 26.3% less time in moving clothes pins) in clothes-pin test.\n\nAn experimental six-motor prosthesis was also constructed. The most striking feature of targeted reinnervation compared to traditional myoelectric prosthetics is its ability to provide multiple signals to control multiple functions simultaneously. Although current myoelectric prostheses can be used directly, they are designed and aimed at traditional myoelectric control. Thus, the only commercially available prosthesis only has powered terminal device (often a hook), wrist rotation and powered elbow. To fully utilize the multiple signals provided by targeted reinnervation, an experimental prosthesis was constructed with added power components: a TouchEMAS shoulder, a humeral rotator, and a hand capable of opening and closing with wrist flexion/extension function. The elbow and hand functions were driven by four nerve transfer signals, and the humeral rotation was driven by EMG from latissumus dorsi and deltoids. With this six-motor prosthesis, the patient could control multiple joints at the same time and perform new tasks that could not be accomplished with other prostheses, such as reaching out to pick up objects and putting on a hat.\n\nTargeted sensory reinnervation was discovered by accident. While receiving an alcohol rub on his chest after the surgery, the patient described a sensation of being touched on the pinky. The explanation for this phenomenon is that, since his subcutaneous fat was removed during surgery, his chest skin was denervated. Thus, the afferent nerve fibers regenerated through the pectoral muscle, reinnervating the skin over the muscle. Since then, areas of the pectoral muscle have been mapped to parts of arm and hand according to patient’s description of touch sensations he felt. When touched in a specific region on the pectoral muscle, the patient would describe where in the phantom limb he felt as being touched. For example, when touched in a region immediately above the nipple, he felt as if his anterior forearm was being touched.\n\nWith this discovery, the team set out to perform nerve transfer surgery specifically aimed to reinnervate sensory feedback. A piece of skin near or over the targeted muscle was denervated, thus the afferent nerve fibers were allowed to reinnervate the skin.\nIn a case of a woman patient with left arm amputation at the humeral neck, the supraclavicular sensory nerve was cut, the proximal end was ligated to prevent regeneration and reinnervation, and the distal end was coapted end-to-side to the ulnar nerve. The intercostobrachial cutaneous nerve was treated with the same method, with the distal end coapted to the median nerve.\n\nThis technique has been dubbed “transfer sensation”, and it has the potential of providing useful sensory feedback, such as pressure sensing, to help the patient judge the amount of force to be exerted.\n\nAfter surgery, the patient was asked to identify the chest areas with most prominent sensation of individual digits, which were then mapped onto a diagram. The characteristic of the sensory reinnervation was quantified. Light touch is quantified by a threshold determined with Semmes-Weinstein monofilaments (a sensation measurement instrument). A Neurotip neurometer was used to determine the sensibility of sharpness and dullness at 20 sites distributed throughout the targeted muscle (the chest). A tuning fork was pressed against the points on the chest to assess patient’s ability to detect vibration. A TSA II NeuroSensory Analyzer was used to assess temperature thresholds at two points on the chest. The patient’s other (normal) pectoral muscle, normal arm and hand are used as controls.\n\nThe patient was able to perceive all modalities of cutaneous sensation. However, instead of normal pressure sensing, she perceived tingling in response to touch on the targeted chest skin. The lowest threshold above which light touch could be sensed in the target muscle was 0•4 g, while the control chest muscle had a light-touch threshold of 0•16 g; the thresholds were under 4 g at most points in the area while the control chest had a threshold of 0.4 g at its counterpart locations. The control chest demonstrated a much lower threshold hence higher sensibility. The patient was able to discern increasing, graded pressure. She felt more tingling as the test pressure increased. The patient also demonstrated perception of temperature. The mean threshold for coldness perception was 29•1 °C in the target muscle, and 29•9 °C in the control chest muscle. The mean threshold for warmth perception was 35•2 °C in the target muscle, and 34•7 °C in the control chest muscle. The patient was able to discern between sharp and dull stimuli and detect vibration at 19 of the 20 points selected for testing. All the above sensations perceived by the patient were described by the patient as occurring in her phantom hand.\n\nWith extraordinary successes came certain risks and failures. The general risks of the surgery, in addition to standard risks of surgery, include permanent paralysis of the target muscle, recurrence of phantom limb pain, and development of painful neuromas.\n\nWith the first patient, the ulnar nerve transfer was not successful. The muscle region was not reinnervated as expected, but instead turned bluish after mobilization, possibly due to a congestion of vascular supply.\n\nWith the left-arm amputation woman mentioned above, her phantom limb pain returned after surgery. Though at a lesser degree and resolved within 4 weeks, it still presented a serious risk because it is unclear whether it will resolve in other future patients.\nAdditionally, surgery was unsuccessful with a patient because severe nerve injuries were not detectable until during the surgery.\n\nIt also remains in speculation whether the transferred nerves would survive permanently.\n\nThe team has now moved onto a trial with transhumeral amputees (amputation above the elbow), with the hope that median nerve transfer in transradial amputation could potentially provide thumb control. With all previous patients being upper limb amputees, the team also hopes to move on to lower limb amputees eventually.\n\nThe nerves could also be further split to provide even more independent signals, so that more functions can be controlled simultaneously and more degrees of freedom can be gained in prosthesis control. This could also prompt the production of more sophisticated prosthetic devices with more degrees of freedom, such as the six-motor experimental prosthesis mentioned above.\n\nTargeted reinnervation could also utilize implantable electrodes to record more localized signals from the target muscle, so that crosstalk can be further mitigated.\n\nMuch work is still to be done to translocate the sensory feedback from the reinnervated target muscle to the actual prosthesis, or to construct prostheses that are capable of providing appropriate stimuli to the reinnervated target muscle according to the external stimuli received, so that the sensory feedback of the arm comes from its native physical position.\n\nBeginning in 2016, the Applied Physics Laboratory at Johns Hopkins began working with a patient having undergone both targeted muscle reinnervation and osseointegration of a titanium port to test and perfect their design for the Modular Prosthetic Limb funded by DARPA\n"}
{"id": "1057906", "url": "https://en.wikipedia.org/wiki?curid=1057906", "title": "Terra (company)", "text": "Terra (company)\n\nTerra Networks, S.A. is a Spanish Internet multinational company with headquarters in Spain and offices in Brazil, Chile, Colombia, Mexico, the United States and Peru. Part of Telefónica Group (the former Spanish public telephone monopoly), Terra operates as a web portal or internet access provider in the U.S., Spain, and 16 Latin American countries. Terra was publicly traded on NASDAQ under the symbol TRLY and on the Spanish stock market under the symbol TRR.\n\nTerra was founded in 1999 by Juan Villalonga, Telefónica's president between 1996 and 2000, and quickly turned into a major internet player through the aggressive acquisition of several local startups in Spain and the main Latin American markets: (Spain), (Brazil), Mexico, Gauchonet, Donde (Argentina) and Chevere (Venezuela).\n\nTerra has also created several vertical portals, like Invertia, a successful finance portal, and Educaterra (e-learning). It also has had or has stakes in other internet ventures: Uno-e (online banking), Rumbo (travel, in partnership with Amadeus), Atrea (real estate, in partnership with Spanish bank BBVA), Azeler (car selling, also with BBVA), and Maptel (online maps).\n\nIn November 1999, still during the period known as the \"Internet bubble\", Terra had a high-profile IPO both in the U.S. and Spain, and its shares skyrocketed from an initial price of 11.81 euros up to 157.65 euros in just 3 months. After that, the price fell sharply until it reached 2.75 euros in October 2004. This process sparked a lot of public controversy in Spain, where thousands of small investors acquired shares of Terra during the boom. Despite this problematic image, Terra managed to hold leading positions both as a web portal and as an ISP provider in several countries, and specially in Spain.\n\nDuring 2003 and 2004 Terra expanded aggressively into the paid content business, mainly in Spain, Brazil, and Chile, launching ventures with Disney and Spanish football clubs Real Madrid and FC Barcelona. It also started several entertainment services, including an online multiplayer gaming platform (Terra Games) and a digital music service (Terra Música Premium) similar to Apple Computer's iTunes.\n\nIn April 2000, Terra surprised the internet market with the acquisition of Lycos, the U.S. portal, in a stock swap valued at US$12.5 billion. By that time, Lycos was the third most visited portal in the U.S., according to Nielsen//NetRatings, and had a strong presence in key European and Asian markets. Specialists expected that the combination of Lycos with Terra's dominance in Latin America would create a powerful company, able even to challenge giants like AOL, Yahoo!. Lycos CEO Bob Davis was moved to the position of CEO of the combined company, from where he stepped down in January 2001, being replaced by then Chairman Joaquim Agut.\n\nPart of the deal was also German media giant Bertelsmann, owner of a stake in Lycos Europe. In exchange for keeping the control over Lycos Europe, Bertelsmann agreed to spend US$1 billion worth in advertising at Terra Lycos through a five-year period. That spending was crucial for Terra to survive the times of the internet crash, when several Latin American-based internet companies like Quepasa, Starmedia or El Sitio lost cash up to the point of filing for bankruptcy or being taken over by bigger companies.\n\nIn 2003 Bertelsmann executed an option to get itself out of the agreement, transferring to Terra's parent company Telefónica the obligation to keep the ad spending. Soon after that, Telefónica decided to get more control over Terra and launched an offer for shares of Terra still floating on the stock market. Although it granted Telefónica control over more than 70% of Terra's stock, the move was not successful enough to let Telefónica take Terra out of the public, as it was allegedly its objective.\n\nIn October 2004, following Telefónica's decision to re-focus their businesses, Terra sold Lycos to South Korean Internet portal company Daum Communications for US$105 million. Kim Faura was Terra's last chairman. Joaquim Agut was the previous one, and now he is chairman of Endemol.\n\nIn February 2005, Telefónica announced its intention of taking full control of Terra by giving Telefónica' shares in exchange for Terra's remaining shares in the stock market. After this plan was approved by both Telefónica and Terra shareholders meetings, Terra's shares were finally excluded from the market on July 15, 2005.\n\nFrom that moment, Terra Networks S.A. was merged into Telefónica, S.A. and, therefore, disappeared from a legal point of view. A small portion of the former corporate headquarters became \"Terra Networks Asociadas, S.L.U.\" (a new company) and local Terra operations (and assets) were transferred to local fixed-line Telefónica companies.\n\nTerra is the largest Latin American online media company, ranked as the 31st most popular internet destination in the world. Offering Entertainment, News and Sports to the 100 million people who visit its portals monthly, Terra was named by Fast Company in 2011 as one of the most innovative company in the music area, \"for a multipronged, and profitable, music-focused content model\".\n\nWith 100 million unique visitors per month, Terra is present in 17 Latin American countries as well as in USA. It has offices in cities such as São Paulo and Porto Alegre (BRA), Buenos Aires (ARG), Santiago (CHI), Lima (PER), Bogota (COL), Mexico DF (MEX), Miami and New York (USA). Among Terra's most successful products and channels are Sonora (music) and Terra TV (premium video content) and the Planeta Terra annual music festival.\n"}
{"id": "30273493", "url": "https://en.wikipedia.org/wiki?curid=30273493", "title": "That Should Not Be: Our Children Will Accuse Us", "text": "That Should Not Be: Our Children Will Accuse Us\n\nThat Should Not Be: Our Children Will Accuse Us (Original title: \"Nos enfants nous accuseront\", US title: \"Food Beware: The French Organic Revolution\") is a 2008 French documentary film directed by Jean-Paul Jaud.\n\nThe documentary is about food poisoning by toxins from agricultural chemicals such as pesticides, herbicides, fertilizers, etc.\n\nThe film tells the story of an initiative in Barjac, a commune located in the Gard department in southern France, that decided to introduce organic produce into the town's school cafeteria. The film depicts without concessions the environmental tragedy which threatens the young generation: the poisoning of our country sides by agricultural pesticides (76 000 tons of pesticides used each year in France) and the harm caused to public health and safety.\n\n"}
{"id": "25619879", "url": "https://en.wikipedia.org/wiki?curid=25619879", "title": "The Mirrored Heavens", "text": "The Mirrored Heavens\n\nThe Mirrored Heavens is a science fiction novel by David J. Williams. This is the author's debut novel, and the first volume in his Autumn Rain trilogy, which continues with \"The Burning Skies\" and \"The Machinery Of Light\". The story begins in the year 2110 where global political power is balanced between the United States and the Eurasian Coalition (a primarily Chinese and Russian alliance). These two powers jointly constructed a space elevator, which is destroyed by a terrorist attack before it can become operational.\n\nThe story follows the United States intelligence agents who are attempting to catch those responsible.\n\n\n"}
{"id": "6999722", "url": "https://en.wikipedia.org/wiki?curid=6999722", "title": "Traffic barrier", "text": "Traffic barrier\n\nTraffic barriers (sometimes called Armco barriers, also known in North America as guardrails or guard rails and in Britain as crash barriers) keep vehicles within their roadway and prevent them from colliding with dangerous obstacles such as boulders, sign supports, trees, bridge abutments, buildings, walls, and large storm drains, or from traversing steep (non-recoverable) slopes or entering deep water. They are also installed within medians of divided highways to prevent errant vehicles from entering the opposing carriageway of traffic and help to reduce head-on collisions. Some of these barriers, designed to be struck from either side, are called median barriers. Traffic barriers can also be used to protect vulnerable areas like school yards, pedestrian zones, and fuel tanks from errant vehicles.\n\nWhile barriers are normally designed to minimize injury to vehicle occupants, injuries do occur in collisions with traffic barriers. They should only be installed where a collision with the barrier is likely to be less severe than a collision with the hazard behind it. Where possible, it is preferable to remove, relocate or modify a hazard, rather than shield it with a barrier.\n\nTo make sure they are safe and effective, traffic barriers undergo extensive simulated and full scale crash testing before they are approved for general use. While crash testing cannot replicate every potential manner of impact, testing programs are designed to determine the performance limits of traffic barriers and provide an adequate level of protection to road users.\n\nRoadside hazards must be assessed for the danger they pose to traveling motorists based on size, shape, rigidity, and distance from the edge of travelway. For instance, small roadside signs and some large signs (ground-mounted breakaway post) often do not merit roadside protection as the barrier itself may pose a greater threat to general health and well-being of the public than the obstacle it intends to protect. In many regions of the world, the concept of clear zone is taken into account when examining the distance of an obstacle or hazard from the edge of travelway.\n\nClear zone, also known as clear recovery area or horizontal clearance is defined (through study) as a lateral distance in which a motorist on a recoverable slope may travel outside of the travelway and return their vehicle safely to the roadway. This distance is commonly determined as the 85th percentile in a study comparable to the method of determining speed limits on roadways through speed studies and varies based on the classification of a roadway. In order to provide for adequate safety in roadside conditions, hazardous elements such as fixed obstacles or steep slopes can be placed outside of the clear zone in order to reduce or eliminate the need for roadside protection.\n\nWhen barrier is needed, careful calculations are completed to determine length of need. The calculations take into account the speed and volume of traffic volume using the road, the distance from the edge of travelway to the hazard, and the distance or offset from the edge of travelway to the barrier.\n\nTraffic barriers are categorized in two ways: by the function they serve, and by how much they deflect when a vehicle crashes into them.\n\nRoadside barriers are used to protect traffic from roadside obstacles or hazards, such as slopes steep enough to cause rollover crashes, fixed objects like bridge piers, and bodies of water. Roadside barriers can also be used with medians, to prevent vehicles from colliding with hazards within the median.\n\nMedian barriers are used to prevent vehicles from crossing over a median and striking an oncoming vehicle in a head-on crash. Unlike roadside barriers, they must be designed to be struck from either side.\n\nBridge barrier is designed to restrain vehicles from crashing off the side of a bridge and falling onto the roadway, river or railroad below. It is usually higher than roadside barrier, to prevent trucks, buses, pedestrians and cyclists from vaulting or rolling over the barrier and falling over the side of the structure. Bridge rails are usually multi-rail tubular steel barriers or reinforced concrete parapets and barriers.\n\nWork zone barriers are used to protect traffic from hazards in work zones. Their distinguishing feature is they can be relocated as conditions change in the road works. Two common types are used: temporary concrete barrier and water-filled barrier. The latter is composed of steel-reinforced plastic boxes that are put in place where needed, linked together to form a longitudinal barrier, then ballasted with water. These have an advantage in that they can be assembled without heavy lifting equipment, but they cannot be used in freezing weather.\n\nBarriers are divided into three groups, based on the amount they deflect when struck by a vehicle and the mechanism the barrier uses to resist the impact forces. In the United States, traffic barriers are tested and classified according to the AASHTO Manual for Assessing Safety Hardware (MASH) standards, which recently superseded Federal Highway Administration NCHRP Report 350. Barrier deflections listed below are results from crash tests with a pickup truck traveling , colliding with the rail at a 25 degree angle.\n\nFlexible barriers include cable barriers and weak post corrugated guide rail systems. These are referred to as flexible barriers because they will deflect when struck by a typical passenger car or light truck. Impact energy is dissipated through tension in the rail elements, deformation of the rail elements, posts, soil and vehicle bodywork, and friction between the rail and vehicle.\n\nSemi-rigid barriers include box beam guide rail, heavy post blocked out corrugated guide rail and thrie-beam guide rail. Thrie-beam is similar to corrugated rail, but it has three ridges instead of two. They deflect : more than rigid barriers, but less than flexible barriers. Impact energy is dissipated through deformation of the rail elements, posts, soil and vehicle bodywork, and friction between the rail and vehicle. Box beam systems also spread the impact force over a number of posts due to the stiffness of the steel tube.\n\nRigid barriers are usually constructed of reinforced concrete. A permanent concrete barrier will only deflect a negligible amount when struck by a vehicle. Instead, the shape of a concrete barrier is designed to redirect a vehicle into a path parallel to the barrier. This means they can be used to protect traffic from hazards very close behind the barrier, and generally require very little maintenance. Impact energy is dissipated through redirection and deformation of the vehicle itself. Jersey barriers and F-shape barriers also lift the vehicle as the tires ride up on the angled lower section. For low-speed or low-angle impacts on these barriers, that may be sufficient to redirect the vehicle without damaging the bodywork. The disadvantage is there is a higher likelihood of rollover with a small car than the single slope or step barriers. Impact forces are resisted by a combination of the rigidity and mass of the barrier. Deflection is usually negligible.\n\nAn early concrete barrier design was developed by the New Jersey State Highway Department. This led to the term Jersey barrier being used as a generic term, although technically it applies to a specific shape of concrete barrier. Other types include constant-slope barriers, concrete step barriers, and F-shape barriers.\n\nConcrete barriers usually have smooth finishes. At some impact angles, coarse finishes allow the drive wheel of front wheel drive vehicles to climb the barrier, potentially causing the vehicle to roll over. However, along parkways and other areas where aesthetics are considered important, reinforced concrete walls with stone veneers or faux stone finishes are sometimes used. These barrier walls usually have vertical faces to prevent vehicles from climbing the barrier.\n\nEarly traffic barrier designs often paid little attention to the ends of the barriers, so they either ended abruptly in blunt ends, or sometimes featured some flaring of the edges away from the side of the barrier facing traffic. Vehicles that struck blunt ends at the wrong angle could stop too suddenly or have steel rail sections penetrate into the passenger compartment, resulting in severe injuries or fatalities. As a result, a new style of barrier terminals were developed in the 1960s in which the installers were directed to twist the guardrail 90 degrees and bring its end down so that it would lie flat at ground level (so-called \"turned-down\" terminals). While this innovation prevented the rail from penetrating the vehicle, it could also vault a vehicle into the air or cause it to roll over, since the rising and twisting guardrail formed a ramp. These crashes often led to vehicles flying at high speed into the very objects which guardrails or barriers were supposed to protect them from in the first place.\n\nTo address the vaulting and rollover crashes, energy-absorbing terminals were developed. The first generation of these terminals in the 1970s were breakaway cable terminals, in which the rail curves back on itself and is connected to a cable that runs between the first and second posts (which are often breakaway posts). The second generation, in the 1990s and 2000s, feature a large steel impact head that engages the frame or bumper of the vehicle. The impact head is driven back along the guide rail, dissipating the vehicle's kinetic energy by bending or tearing the steel in the guide rail sections. A guide rail may also be terminated by curving it back to the point that the terminal is unlikely to be hit end-on, or, if possible, by embedding the end in a hillside or cut slope.\nAn alternative to energy absorbing barrier terminals are impact attenuators. These are used for wider hazards that can't be effectively protected with a one-sided traffic barrier.\n\nRecycled tyres have been proposed for highway crash barriers, but most governments prefer sand-filled crash barriers because they have excellent absorption characteristics and are easier to erect and dismantle.\n\nA Fitch Barrier is a type of impact attenuator consisting of a sand-filled plastic barrels, usually yellow colored with a black lid. Fitch barriers are often found in a triangular arrangement at the end of a guard rail between a highway and an exit lane (the area known as the gore), along the most probable line of impact. The barriers in front contain the least sand, with each successive barrel containing more, so that when a vehicle collides with the barrels they shatter, the kinetic energy is dissipated by scattering the sand and the vehicle decelerates smoothly instead of violently striking a solid obstruction, reducing the risk of injury to the occupants. Fitch barriers are widely popular due to their effectiveness, low cost, and ease of setup and repair or replacement.\n\nTypes of end treatments:\n\n\n"}
{"id": "19987516", "url": "https://en.wikipedia.org/wiki?curid=19987516", "title": "Transocean Marianas", "text": "Transocean Marianas\n\nTransocean Marianas is an Earl & Wright Sedco 700 design semi-submersible drilling unit capable of operating in harsh environments and water depths up to using an , 15,000 psi blowout preventer (BOP), and a outside diameter (OD) marine riser.\n\n\"Transocean Marianas\" currently operates offshore drilling activities in Namibia under contract from HRT Participações em Petróleo S.A. Throughout her career, she has operated under several names: Tharos from 1979 to 1994, Polyportia from 1994 to 1996, and P. Portia from 1996 to 1998.\n\nOn October 7, 2009, it began drilling on the Macondo well in the Gulf of Mexico. On November 9, 2009. it was damaged by Hurricane Ida. It was subsequently replaced by the \"Deepwater Horizon\", which was responsible for the Deepwater Horizon oil spill when that rig exploded on April 20, 2010.\n\nIn 1988, as Tharos, the vessel operated as a large firefighting, construction, diving support and accommodation vessel equipped with a helicopter and a hospital ship with 22 beds, that rescued survivors from the \"Piper Alpha\" oil platform disaster. The vessel carried out firefighting on the night of the disaster and supported the fire fighting and well kill operations that led to the extinguishing of the blaze. However, flaws in the design of the vessel's extensible rescue arm prevented it from rescuing crew from the platform, while errors in the operation of the water cannon delayed their use. Divers from \"Tharos\" recovered many bodies from the seabed and from the galley area of \"Piper Alpha\". \"Tharos\"'s role in the early hours of the disaster was shown in the 1990 Scottish Television series \"Rescue\".\n\nOn July 6, 2011, the \"Transocean Marianas\" was evacuated due to it taking on water off the coast of Ghana.\n\n"}
{"id": "50757890", "url": "https://en.wikipedia.org/wiki?curid=50757890", "title": "True DC", "text": "True DC\n\nTRUE DC refers to a type of Switch Disconnect (Isolator) used in Solar Photovoltaic installations, in accordance with EN 60364-7-712. Pioneered by UK based IMO Precision Controls Ltd, and later adopted by other manufacturers in this marketplace such as Santon and ABB, the isolator design incorporates a user independent switching action so as the handle is moved, it interacts with a spring mechanism which, upon reaching a set point, causes the contacts to \"SNAP\" over, thereby ensuring a very fast break/make action. This mechanism means that the disconnection of the load circuits and the suppression of the electrical arc, produced by a constant DC load, is normally extinguished in a maximum of 5ms using the specific pole suppression chambers incorporated into the design.\n\nMany alternative solutions, particularly those based upon an AC Switch Disconnect design which use bridge contacts, have been modified and rated for DC operation. These types of product have a switching speed that is directly linked to the operator speed, therefore slow operation of the handle results in slow contact separation of the contacts which can produce arcing times of 100ms or more. Additionally in these switches, the contact surface is also the surface upon which electrical arcs tend to form; therefore any surface damage or sooting caused by arcing is likely to have a detrimental effect on the isolators contact resistance and its longevity.\n\nTRUE DC Solar Isolators use a rotary knife contact mechanism so when the unit is operated, the handle movement gives a double make/break per contact set. As DC load switching creates electrical arcing, the design is such that this only occurs on the corners of the switching parts meaning that the main contact is made on an area where no arcing has occurred. The rotary contact mechanism methodology used in TRUE DC solar isolators means that when the isolator is operated, a self-cleaning action occurs on the arcing points and contact surfaces thereby producing good high-vibration resistant contact integrity, with reduced contact resistance. This contact system ensures that power loss per pole is kept as low as possible, and consistent over the life of the product, unlike conventional style isolators where entrapment of contaminants, and then subsequent compression on lateral operation, can lead to variable and increasing contact resistance, and hence per pole losses.\n\nThe overall design of a TRUE DC solar isolator is satisfactory for use in installations classified as either DC-21A, DC-21B or DC-22A, and so suitable for a high number of \"off load\" operations (without current) and also a high number of operating cycles \"on load\" (with current).\n\nA further advantage of the TRUE DC mechanism is that in the event of the supply to earth failure, the high short circuit current pulls the contacts together, thereby giving high short circuit withstand current of up to 2400Amps (product dependent). Residential photovoltaic installations are typically 1000VDC, however the majority of the TRUE DC isolators available on the market today already have the capability to operate up to 1500VDC.\n\nIn the move towards safer installations of PV systems, whether it be in a domestic or industrial environment, consideration has to often be given to the materials and the risk of fire hazard that they pose. Ratings referred to under the UL 94 category are deemed generally acceptable for compliance with this requirement as this cover tests for flammability of polymeric materials used for parts in devices and appliances. Although there are 12 flame classifications specified in UL 94, there are 6 which relate to materials commonly used in manufacturing enclosures, structural parts and insulators found in consumer electronic products. These are 5VA, 5VB, V-0, V-1, V-2 and HB.\n\nWith the advent of more worldwide installations and the requirements laid down in many country’s national wiring publications for the use of DC switches in PV installations, TRUE DC Solar Isolators must also be assessed and tested under the latest UL standard UL508I which has been specifically written to cover the use of “Manual Disconnect Switches intended for use in Photovoltaic Systems”.\n\nThis UL508I standard specifically covers switches rated up to 1500 V that are intended for use in an ambient temperatures of -20 °C to +60 °C, and that are suitable for use on the load side of PV branch protection devices.\n"}
{"id": "57716737", "url": "https://en.wikipedia.org/wiki?curid=57716737", "title": "Ventilative cooling", "text": "Ventilative cooling\n\nVentilative cooling is the use of natural or mechanical ventilation to cool indoor spaces. The use of outside air reduces the cooling load and the energy consumption of these systems, while maintaining high quality indoor conditions. Ventilative cooling strategies are applied in a wide range of buildings and may even be critical to realize renovated or new high efficient buildings and zero-energy buildings (ZEBs) . Ventilation is present in buildings mainly for air quality reasons. It can be used additionally to remove both excess heat gains, as well as increase the velocity of the air and thereby widen the thermal comfort range. Ventilative cooling is assessed by long-term evaluation indices .Ventilative cooling is dependent on the availability of appropriate external conditions and on the thermal physical characteristics of the building.\n\nIn the last years, overheating in buildings has been a challenge not only during the design stage but also during the operation. The reasons are: \nIn many post-occupancy comfort studies overheating is a frequently reported problem not only during the summer months but also during the transitions periods, also in temperate climates.\n\nThe effectiveness of ventilative cooling has been investigated by many researchers and has been documented in many post occupancy assessments reports .The system cooling effectiveness (natural or mechanical ventilation) depends on the air flow rate that can be established, the thermal capacity of the construction and the heat transfer of the elements. During cold periods the cooling power of outdoor air is large. The risk of draughts is also important. During summer and transition months outdoor air cooling power might not be enough to compensate overheating indoors during daytime and application of ventilative cooling will be limited only during the night period. The night ventilation may remove effectively accumulated heat gains (internal and solar) during daytime in the building constructions .\nFor the assessment of the cooling potential of the location simplified methods have been developed . These methods use mainly building characteristics information, comfort range indices and local climate data. In most of the simplified methods the thermal inertia is ignored.\n\nThe critical limitations for ventilative cooling are:\n\nVentilative cooling requirements in regulations are complex. Energy performance calculations in many countries worldwide do not explicitly consider ventilative cooling. The available tools used for energy performance calculations are not suited to model the impact and effectiveness of ventilative cooling, especially through annual and monthly calculations .\n\nA large number of buildings using ventilative cooling strategies have already been built around the world . Ventilative cooling can be found not only in traditional, pre-air-condition architecture, but also in temporary European and international low energy buildings. For these buildings passive strategies are priority. When passive strategies are not enough to achieve comfort, active strategies are applied. In most cases for the summer period and the transition months, automatically controlled natural ventilation is used. During the heating season, mechanical ventilation with heat recovery is used for indoor air quality reasons. Most of the buildings present high thermal mass. User behavior is crucial element for successful performance of the method.\n\nBuilding components of ventilative cooling are applied on all three levels of climate-sensitive building design, i.e. site design, architectural design and technical interventions . A grouping of these components follows :\n\n\nControl strategies in ventilative cooling solutions have to control the magnitude and the direction, of air flows in space and time . Effective control strategies ensure high indoor comfort levels and minimum energy consumption. Strategies in a lot of cases include temperature and CO2 monitoring . In many buildings in which occupants had learned how to operate the systems, energy use reduction was achieved. Main control parameters are operative (air and radiant) temperature (both peak, actual or average), occupancy, carbon dioxide concentration and humidity levels . Automation is more effective than personal control . Manual control or manual override of automatic control are very important as it affects user acceptance and appreciation of the indoor climate positively (also cost).The third option is that operation of facades is left to personal control of the inhabitants, but the building automation system gives active feedback and specific advises.\n\nBuilding design is characterized by different detailed design levels. In order to support the decision-making process towards ventilative cooling solutions, airflow models with different resolution are used.\nDepending on the detail resolution required, airflow models can be grouped into two categories:\n\nExisting literature includes reviews of available methods for airflow modelling. \n\nAnnex 62 ‘ventilative cooling’ is a research project of the ‘Energy in Buildings and Communities Programme (EBC)’ of the International Energy Agency (IEA), with a four-year working phase (2014–2018).\nThe main goal is to make ventilative cooling an attractive and energy efficient cooling solution to avoid overheating of both new and renovated buildings. The results from the Annex will facilitate better possibilities for prediction and estimation of heat removal and overheating risk – for both design purposes and for energy performance calculation. The documented performance of ventilative cooling systems through analysis of case studies will promote the use of this technology in future high performance and conventional buildings .\nTo fulfil the main goal the Annex has the following targets for the research and development work:\nThe Annex 62 research work is divided in three subtasks.\n\n"}
{"id": "12839080", "url": "https://en.wikipedia.org/wiki?curid=12839080", "title": "Wireless Public Key Infrastructure", "text": "Wireless Public Key Infrastructure\n\nWireless Public Key Infrastructure (WPKI) is a technology that provides public key infrastructure functionality using a mobile Secure Element such as a SIM card. It can be used for example for two-factor authentication.\n"}
