{"id": "977096", "url": "https://en.wikipedia.org/wiki?curid=977096", "title": "Advanced Electronics Company Limited", "text": "Advanced Electronics Company Limited\n\nAdvanced Electronics Company (AEC) is a limited liability company in Saudi Arabia that was established in 1988 under the directives of the government of Saudi Arabia. AEC is specialized in advanced electronics research and manufacturing for defense and communication among others. The goal of AEC is to create local capabilities in strategic areas like advanced manufacturing technologies, communication systems, and product support. In 2006 it signed an agreement with Dell to manufacture personal computers as part of the Saudi governments programme to make PCs affordable to its population.\n\n"}
{"id": "4196082", "url": "https://en.wikipedia.org/wiki?curid=4196082", "title": "Arc converter", "text": "Arc converter\n\nThe arc converter, sometimes called the arc transmitter, or Poulsen arc after Danish engineer Valdemar Poulsen who invented it in 1903, was a variety of spark transmitter used in early wireless telegraphy. The arc converter used an electric arc to convert direct current electricity into radio frequency alternating current. It was used as a radio transmitter from 1903 until the 1920s when it was replaced by vacuum tube transmitters. One of the first transmitters that could generate continuous sinusoidal waves, it was one of the first technologies used to transmit sound (amplitude modulation) by radio. It is on the list of IEEE Milestones as a historic achievement in electrical engineering.\n\nElihu Thomson discovered that a carbon arc shunted with a series tuned circuit would \"sing\". This \"singing arc\" was probably limited to audio frequencies. Bureau of Standards credits William Duddell with the shunt resonant circuit around 1900.\n\nThe English engineer William Duddell discovered how to make a resonant circuit using a carbon arc lamp. Duddell's \"musical arc\" operated at audio frequencies, and Duddell himself concluded that it was impossible to make the arc oscillate at radio frequencies.\n\nValdemar Poulsen succeeded in raising the efficiency and frequency to the desired level. Poulsen's arc could generate frequencies of up to 200 kilohertz and was patented in 1903.\n\nAfter a few years of development the arc technology was transferred to Germany and Great Britain in 1906 by Poulsen, his collaborator Peder Oluf Pedersen and their financial backers. In 1909 the American patents as well as a few arc converters were bought by Cyril Frank Elwell. The subsequent development in Europe and the United States was rather different, since in Europe there were severe difficulties for many years implementing the Poulsen technology, whereas in the United States an extended commercial radiotelegraph system was soon established with the Federal Telegraph Company. Later the US Navy also adopted the Poulsen system. Only the arc converter with passive frequency conversion was suitable for portable and maritime use. This made it the most important mobile radio system for about a decade until it was superseded by vacuum tube systems.\n\nIn 1922, the Bureau of Standards stated, \"the arc is the most widely used transmitting apparatus for high-power, long-distance work. It is estimated that the arc is now responsible for 80 per cent of all the energy actually radiated into space for radio purposes during a given time, leaving amateur stations out of consideration.\"\n\nThis new, more-refined method for generating continuous-wave radio signals was initially developed by Danish inventor Valdemar Poulsen. The spark-gap transmitters in use at that time produced damped wave which wasted a large portion of their radiated power transmitting strong harmonics on multiple frequencies that filled the RF spectrum with interference. Poulsen’s arc converter produced undamped or continuous waves (CW) on a single frequency.\n\nThere are three types for an arc oscillator:\n\n\n\nContinuous or ‘undamped’ waves (CW) were an important feature, since the use of damped waves from spark-gap transmitters resulted in lower transmitter efficiency and communications effectiveness, while polluting the RF spectrum with interference.\nThe Poulsen arc converter had a tuned circuit connected across the arc. The arc converter consisted of a chamber in which the arc burned in hydrogen gas between a carbon cathode and a water-cooled copper anode. Above and below this chamber there were two series field coils surrounding and energizing the two poles of the magnetic circuit. These poles projected into the chamber, one on each side of the arc to provide a magnetic field.\n\nIt was most successful when operated in the frequency range of a few kilohertz to a few tens of kilohertz. The antenna tuning had to be selective enough to suppress the arc converter's harmonics.\n\nSince the arc took some time to strike and operate in a stable fashion, normal on-off keying could not be used. Instead, a form of frequency shift keying was employed. In this \"compensation-wave method\", the arc operated continuously, and the key altered the frequency of the arc by one to five percent. The signal at the unwanted frequency was called the \"compensation-wave\". In arc transmitters up to 70 kW, the key typically shorted out a few turns in the antenna coil. For larger arcs, the arc output would be transformer coupled to the antenna inductor, and the key would short out a few bottom turns of the grounded secondary. Therefore, the \"mark\" (key closed) was sent at one frequency, and the \"space\" (key open) at another frequency. If these frequencies were far enough apart, and the receiving station's receiver had adequate selectivity, the receiving station would hear standard CW when tuned to the \"mark\" frequency.\n\nThe compensation wave method used a lot of spectrum bandwidth. It not only transmitted on the two intended frequencies, but also the harmonics of those frequencies. Arc converters are rich in harmonics. Sometime around 1921, the Preliminary International Communications Conference prohibited the compensation wave method because it caused too much interference.\n\nThe need for the emission of signals at two different frequencies was eliminated by the development of \"uniwave methods\". In one uniwave method, called the \"ignition method\", keying would start and stop the arc. The arc chamber would have a \"striker\" rod that shorted out the two electrodes through a resistor and extinguished the arc. The key would energize an electromagnet that would move the striker and reignite the arc. For this method to work, the arc chamber had to be hot. The method was feasible for arc converters up to about 5 kW.\n\nThe second uniwave method is the \"absorption method\", and it involves two tuned circuits and a single-pole, double-throw, make-before-break key. When the key is down, the arc is connected to the tuned antenna coil and antenna. When the key is up, the arc is connected to a tuned dummy antenna called the \"back shunt\". The back shunt was a second tuned circuit consisting of an inductor, a capacitor, and load resistor in series. This second circuit is tuned to roughly the same frequency as the transmitted frequency; it keeps the arc running, and it absorbs the transmitter power. The absorption method is apparently due to W. A. Eaton.\n\nThe design of switching circuit for the absorption method is significant. It is switching a high voltage arc, so the switch's contacts must have some form of arc suppression. Eaton had the telegraph key drive electromagnets that operated a relay. That relay used four sets of switch contacts in series for each of the two paths (one to the antenna and one to the back shunt). Each relay contact was bridged by a resistor. Consequently, the switch was never completely open, but there was a lot of attenuation.\n\n\n\n\n"}
{"id": "14352325", "url": "https://en.wikipedia.org/wiki?curid=14352325", "title": "Architectural Engineering Institute", "text": "Architectural Engineering Institute\n\nThe Architectural Engineering Institute (AEI) is the professional organization for architectural engineers. It is managed as a semi-autonomous division of the American Society of Civil Engineers (ASCE), like the Structural Engineering Institute. ASCE and ASHRAE are the delegated societies for the architectural engineering's ABET accreditation process. However, AEI has successfully developed, with NCEES, the architectural engineering professional engineering registration examination which was first offered in 2003. AEI's postal address is 1801 Alexander Bell Drive, Reston, VA, 20191-4400 USA.\n\n\n"}
{"id": "3806498", "url": "https://en.wikipedia.org/wiki?curid=3806498", "title": "Argentum album", "text": "Argentum album\n\nArgentum album (Latin for \"white money\" or \"silver coin\"), mentioned in Domesday, signifies bullion, or silver uncoined. In those ancient days, such passed as money from one to another in payment.\n\n\"Sumitur pro ipso hoc metallo pensili non signato.\" Spelm.\n"}
{"id": "31261994", "url": "https://en.wikipedia.org/wiki?curid=31261994", "title": "Arrastra", "text": "Arrastra\n\nAn Arrastra (or Arastra) is a primitive mill for grinding and pulverizing (typically) gold or silver ore. The simplest form of the arrastra is two or more flat-bottomed drag stones placed in a circular pit paved with flat stones, and connected to a center post by a long arm. With a horse, mule or human providing power at the other end of the arm, the stones were dragged slowly around in a circle, crushing the ore. Some arrastras were powered by a water wheel; a few were powered by steam or gasoline engines, and even electricity.\n\nArrastras were widely used throughout the Mediterranean region since Phoenician times. The Spanish introduced the arrastra to the New World in the 16th century. The word \"arrastra\" comes from the Spanish language \"arrastre,\" meaning to drag along the ground. Arrastras were suitable for use in small or remote mines, since they could be built from local materials and required little investment capital.\n\nFor gold ore, the gold was typically recovered by amalgamation with quicksilver. The miner would add clean mercury to the ground ore, continue grinding, rinse out the fines, then add more ore and repeat the process. At cleanup, the gold amalgam was carefully recovered from the low places and crevices in the arrastra floor. The amalgam was then heated in a distillation retort to recover the gold, and the mercury was saved for reuse.\n\nFor silver ore, the patio process, invented in Mexico in 1554, was generally used to recover the silver from ore ground in the arrastra.\n\n"}
{"id": "5773520", "url": "https://en.wikipedia.org/wiki?curid=5773520", "title": "AskMeNow", "text": "AskMeNow\n\nAskMeNow Inc. was an American public corporation, specializing in mobile search and mobile advertising. The Irvine, California based company officially launched in November 2005 and ceased operations in late 2008. AskMeNow's primary offering was a consumer mobile search product based on proprietary technology that offered a natural language based interaction and dynamic content provision platform.\n\nAskMeNow signed a partnership agreement with the Wikimedia Foundation to bring natural language processing to English Wikipedia. The beta product, AskWiki, integrated some of the semantic web and natural language features of AskMeNow into English Wikipedia searches. The AskWiki engine was able to parse natural language statements and return specific answers rather than just relevant articles. The contract between AskMeNow and Wikimedia was rescinded after Jimmy Wales announced his own for profit search engine named Wikia.\n\nAskMeNow partnered with a variety of content publishers to offer mobile access to these publishers’ content. These partnerships included National Hockey League, Merriam-Webster, Encyclopædia Britannica, and Guinness World Records.\n\nAskMeNow had established distribution partnerships with Alltel in the US and Rogers Wireless and Bell Mobility in Canada.\n\n"}
{"id": "1119820", "url": "https://en.wikipedia.org/wiki?curid=1119820", "title": "Black match", "text": "Black match\n\nIn pyrotechnics, black match is a type of crude fuse, constructed of cotton string fibers intimately coated with a dried black powder slurry.\n\nWhen black match is confined in a paper tube, called quick match or piped match, the flame front propagates much more quickly, many feet per second.\n\nQuick match is often used in model rockets in the United Kingdom to ignite multiple engines/motors; it is however largely unavailable in the USA due to ambiguous explosives laws.\n\n"}
{"id": "18307910", "url": "https://en.wikipedia.org/wiki?curid=18307910", "title": "Brown waste", "text": "Brown waste\n\nBrown waste is any biodegradable waste that is predominantly carbon based. The term includes such items as grass cuttings, dry leaves, twigs, hay, paper, sawdust, corn cobs, cardboard, pine needles or cones, etc.\nCarbon is necessary to composting, which uses a combination of green waste and brown waste to promote the microbial processes involved in the decomposition process. The composting of brown waste sustainably returns the carbon to the carbon cycle.\n\n"}
{"id": "47614607", "url": "https://en.wikipedia.org/wiki?curid=47614607", "title": "Canadian Patents and Development Limited", "text": "Canadian Patents and Development Limited\n\nCanadian Patents and Development Limited (CPDL) was a Canadian agency tasked with promoting the commercialization of inventions and discoveries arising from government departments and agencies, as well as those disclosed to it by universities and others publicly funded organizations. The National Research Council of Canada (NRC) founded CPDL on October 24, 1947, as a subsidiary Crown Corporation under part 1 of the Canadian Companies Act (now Canadian Corporations Act). As a subsidiary of the NRC, CPDL was charged with handling the assessment, patenting, development, and licensing of the intellectual property developed by the scientific workers of the NRC. Soon after its incorporation, CPDL began making its services available to Canadian universities and other publicly financed organizations. The number of Canadian agencies and departments reporting inventions to CPDL increased substantially in 1954 with the enactment of the Public Servants' Inventions Act, which made CPDL eligible to accept and manage the inventions arising from all federal departments and agencies. Despite its broad mandate and many agreements, CPDL was noted by university administrators as possessing inadequate resources to effectively manage inventions for all of Canada's universities, while the industry consensus \"was that CPDL's work was under-publicized, under-supported, undersold and under-followed-up.\" On February 20, 1990, the Minister of Finance announced the planned dissolution of CPDL as part of a larger government commitment to reducing the size of government and improving the efficiency of public services. A few months later, the Crown Corporation Dissolution or Transfer Authorization Bill (Bill C-73) was introduced to parliament to facilitate the closure of several crown corporations and the transfer of their responsibilities. The bill authorized the Minister of Industry, Science, and Technology to dissolve CPDL, and made government departments and agencies responsible for managing their own intellectual property. Following the Crown Corporation Dissolution or Transfer Authorization Bill, all CPDL agreements with Canadian universities were terminated, and all patented faculty inventions held by CPDL were transferred back to each respective university. On August 1, 1993 CPDL ceased all operations.\n\nKretz, A. (2014). Inventions for Industry: A history of Canadian Patents and Development Limited and the commercialization of university research in Canada. Scientia Canadensis: Journal of the History of Canadian Science, Technology, and Medicine, 36(2), 1-36.\n"}
{"id": "8886432", "url": "https://en.wikipedia.org/wiki?curid=8886432", "title": "Constant-force spring", "text": "Constant-force spring\n\nAn ideal constant-force spring is a spring for which the force it exerts over its range of motion is a constant, that is, it does not obey Hooke's law. In reality, \"constant-force springs\" do not provide a truly constant force and are constructed from materials which do obey Hooke's law.\nGenerally constant-force springs are constructed as a rolled ribbon of spring steel such that the spring is in a rolled up form when relaxed.\n\nThe approximation of \"constant force\" comes from a long travel and a pre-loaded rest position, so that the initial force does not start from zero but a finite value. For relatively small variations around this initial position the force is approximately constant. \n"}
{"id": "15711468", "url": "https://en.wikipedia.org/wiki?curid=15711468", "title": "Craig Fleisher", "text": "Craig Fleisher\n\nCraig S. Fleisher is a scholar, \nadvisor and author who has written or edited several important books in the fields of public affairs, business insight and competitive intelligence and analysis. Before becoming Dean of the Business School at the College of Coastal Georgia, he was awarded two endowed research chair positions while a Professor of Business (Strategy & Environment) at the Odette School of Business, University of Windsor, Ontario, Canada. His research addresses phenomena in the areas of business and competitive intelligence, corporate public affairs (government relations and lobbying), and performance management and measurement. Since 2011 Dr. Fleisher has served as the Chief Learning Officer of Aurora WDC, a 20+ year old professional services firm (PSF) headquartered in Madison, Wisconsin, USA.\n\nFleisher received his PhD in Business from the Katz Graduate School of Business, University of Pittsburgh, his MBA in Human Resource Management and Marketing from the Owen Graduate School of Management, Vanderbilt University, and BSBA in Finance, Insurance and Real Estate from the University of Florida. He also held positions in mortgage banking management, real estate appraisal, and management consulting at various points in his career. After getting his PhD, he took an appointment at the University of Calgary as Associate Professor in the Policy and Environment area, spent nearly a decade in various professorial and program leadership roles at the School of Business and Economics at Wilfrid Laurier University in Waterloo, Ontario, and was at the University of Windsor in endowed chair position for nearly a decade, following a couple of years in decanal positions at the University of New Brunswick. He serves as an Adjunct Professor in Business Information Management at Tampere University of Technology (Finland), Adjunct Professor at the Wisconsin School of Business, University of Wisconsin in Madison, imember of the graduate faculty for the Executive MScom at the Università della Svizzera italiana in Lugano, Switzerland, has also served as a visiting professor or scholar at the University of Sydney (Australia), De Montfort University (Leicester, UK), University of Western Sydney¨and University of Waikato (Hamilton, New Zealand), among others.\n\nHe is considered an expert in the business or competitive intelligence and analysis area based on the number of cited pieces (see Google Scholar for the most current citation counts of his work) he has written or the level of sales his writings have achieved, as well as in the area of corporate public affairs where his books have been positively reviewed or named as best-sellers by leading associations and groups in the field. He was globally recognized for his lifetime contributions to the field of competitive intelligence by being named the Meritorious award winner and Fellow of the Society of Competitive Intelligence Professionals (Alexandria, Virginia, USA), and his efforts in graduate teaching led him to be named one of Canada's top MBA professors by \"Canadian Business Magazine\" in 2006. He has been interviewed about his expertise and research by leading media outlets for magazines, narrow-cast web publications, newspaper, radio and television in a variety of countries, most notably Australia, Canada, and the USA.\n\nFleisher's academic research primarily focuses on two distinct areas of business activity, business and competitive intelligence, and corporate public affairs. Fleisher's dissertation was in the area of measuring public affairs performance, and his research on public affairs benchmarking, performance measurement, and management is among the most cited according to Google Scholar in this field. His two books on these topics, published in 1995 and 1998 by the Public Affairs Council which is headquartered in Washington, DC were recognized as the Council's best-sellers at their time. His other major contributions in the field of corporate and public affairs are the two scholarly handbooks \"The SAGE Handbook of International Corporate and Public Affairs\" in 2017 and the \"Handbook of Public Affairs\" (Sage, 2005) with his colleague Dr. Phil Harris from University of Chester, United Kingdom.\n\nSince 2000, Fleisher is well known for his work in the area of business insights and competitive intelligence, particularly in pushing for the professionalization of the field. He published 3 edited volumes with his Wilfrid Laurier University Marketing professorial colleague David Blenkhorn, those being \"Managing Frontiers in Competitive Intelligence\" in 2001 (Quorum Books, an imprint of the Greenwood Publishing Group), \"Controversies in Competitive Intelligence\" published in 2003 and \"Global Business and Competitive Intelligence\" in 2005 by Praeger Publishers.\n\nThe only Canadian Winner of the international Society of Competitive Intelligence Professionals Meritorious Award for his lifetime contributions to the field of competitive intelligence, he may be best known for his two books looking at analysis methods used by consultants and practitioners in applied strategy. This included \"Strategic and Competitive Analysis\" with his colleague and fellow SCIP Meritorious Award winner Babette Bensoussan (Prentice Hall, 2003) and \"Business and Competitive Analysis\" also with Bensoussan, (2Ed., Pearson PLC, 2015, and FT Press, 2007), and \"Analysis without Paralysis\" (w/Bensoussan, 2nd Ed., FT Press, 2015). These books provided background and application information for over 50 separate methods of understanding business phenomena and competition, ranging from popular methods like SWOT analysis, financial ratio & statement analysis, critical success factor analysis, issue analysis, and S-curves to less popular but no less important methods including business model analysis, linchpin analysis, and alternative competing hypotheses. These books also address competitor analysis.\n\nIn addition to his work in academia, Fleisher has served as President of the Board of Directors of the Society of Competitive Intelligence Professionals (Alexandria, Virginia, USA), inaugural chair of the Competitive Intelligence Foundation (Washington, DC) - an organization that he also founded and helped launch with then SCIP Executive Director Alexander Graham, President of the Canadian Council for Public Affairs Advancement, Editor of the \"Journal of Competitive Intelligence and Management\", Associate Editor of the \"Journal of Public Affairs\" published by John Wiley & Sons, and on the editorial boards of the \"International Journal of Technology Intelligence and Planning\" (Inderscience Publishers), \"Asia Pacific Public Relations Journal\", \"South African Journal of Information Management\", \"Competitive Intelligence Review\", \"Public Affairs\", and SCIP Online. He is also a long-time contributor to the Academy of Management's Social Issues in Management division, a founding member of the International Association of Business and Society, a life member of the International Association of Business Communicators, an academic board member of the European Centre for Public Affairs, and a 2007 Canadian award-winning faculty advisor for the Golden Key International Honour Society.\n\nFleisher also speaks globally on his research, having been to over two score different countries in a variety of roles ranging from key-noting conferences and meetings to serving as an authoritative panel member. He regularly shares his research and expertise to corporate groups, executive audiences, professional and trade associations, as well as government departments and officials.\n\nFleisher is the author of ten books and more than one hundred peer-reviewed academic articles in various journals or academic serials. The following bibliography lists a few of his recent publications:\n\n"}
{"id": "30837479", "url": "https://en.wikipedia.org/wiki?curid=30837479", "title": "Digital distribution in video games", "text": "Digital distribution in video games\n\nIn the video game industry, digital distribution is the process of delivering video game content as digital information, without the exchange or purchase of new physical media. This process has existed since the early 1980s, but it was only with network advancements in bandwidth capabilities in the early 2000s that digital distribution became more prominent as a method of selling games. Currently, the process is dominated by online distribution over broadband internet.\n\nTo facilitate the sale of games, various video game publishers and console manufacturers have created their own platforms for digital distribution. These platforms, such as Steam, Origin, and Xbox Live Marketplace, provide centralized services to purchase and download digital content for either specific video game consoles or PCs. Some platforms may also serve as digital rights management systems, limiting the use of purchased items to one account.\n\nDigital distribution of video games is becoming increasingly common, with major publishers and retailers paying more attention to digital sales, including Steam, PlayStation Store, Amazon.com, GAME, GameStop, and others. According to study conducted by SuperData Research, the volume of digital distribution of video games worldwide was $6.2 billion per month in February 2016, and reached $7.7 billion per month in April 2017.\n\nBefore internet connections became widespread, there were few services for digital distribution of games, and physical media was the dominant method of delivering video games. One of the first examples of digital distribution in video games was GameLine, which operated during the early 1980s. The service allowed Atari 2600 owners to use a specialized cartridge to connect through a phone line to a central server and rent a video game for 5–10 days. The GameLine service was terminated during the video game crash of 1983.\n\nOnly a few digital distribution services for consoles would appear in the 90s. Among them were Sega's Sega Meganet and Sega Channel, released in 1990 and 1994 respectively, providing Sega Genesis owners with access to games on demand and other services. Similar peripherals and services would be released for the Super Famicom (Satellaview) and the Nintendo 64 (Randnet) in Japan.\n\nOn PCs, digital distribution was more prevalent. In the late 1980s and early 1990s, prior to the widespread adoption of the internet, it was common for software developers to upload demos and shareware to Bulletin Board Systems. In most cases, demos or shareware releases would contain an advertisement for the full game with ordering instructions for a physical copy of the full game or software. Some developers instead used a licensing system where 'full versions' could be unlocked from the downloaded software with the purchase of a key, thereby making this method the first true digital distribution method for PC Software. Notable examples include the Software Creations BBS and ExecPC BBS, both of which continue to exist today - albeit in a very different form. Bulletin Board systems however were not interconnected, and developers would have to upload their software to each site. Additionally, BBSs required users to place a telephone call with a modem to reach their system. For many users this meant incurring long distance charges. These factors contributed to a sharp decline in BBS usage in the early 1990s, coinciding with the rise of inexpensive internet providers.\n\nIn the mid 1990s, with the rise of the internet, early individual examples for digital distribution under usage of this new medium emerged, although there were no significant services for it. For instance, in 1997 the video game producer Cavedog distributed regularly additional content for the Real-time strategy computer game Total Annihilation as internet downloads via their website.\n\nAlso, users used the internet to distribute their own content. Without access to the retail infrastructure that would allow them to distribute this content through physical media, user-created content such as game modifications, maps or fan patches could only be distributed online.\n\nBy this time, internet connections were fast and numerous enough such that digital distribution of games and other related content became viable.\n\nThe proliferation of internet-enabled consoles allowed also additional buyable content that could be added onto full retail games, such as maps, in-game clothing, and gameplay. This type of content, called DLC (Downloadable content), become prevalent for consoles in the 2000s.\n\nAn early innovator of the digital distribution idea on the PC was Stardock. In 2001 Stardock released the Stardock Central to digitally distribute and sell its own PC titles, followed by a service called \"Drengin.net\" with a yearly subscription pay model in summer 2003. In 2004, the subscription model was substituted by \"TotalGaming.net\" which allowed individual purchases or pay an upfront fee for tokens which allowed them to purchase games at a discount. In 2008, Stardock announced Impulse a third-generation digital distribution platform, which included independent third-party games and major publisher titles. The platform was sold to GameStop in May 2011.\n\nThe period between 2004 and now saw the rise of many digital distribution services on PC, such as Amazon Digital Services, GameStop, Games for Windows – Live, Origin, Direct2Drive, GOG.com, GamersGate and several more. The offered properties and policies differs significantly between the digital distribution services: e.g. while most of the digital distributors don't allow reselling of bought games, \"Green Man Gaming\" allows this.\n\nIn 2004 the Valve Corporation released the Steam platform for Windows computers (later expanded to Mac OS and Linux) as a means to distribute Valve-developed video games. Steam has the speciality that customers don't buy games but instead get the right to use games, which might be revoked when a violation of the End-user license agreement is seen by Valve or when a customer doesn't accept changes in the End-user license agreement. Steam began later to sell titles from independent developers and major distributors and has since become the largest PC digital distributor. By 2011, Steam has approximately 50-70% of the market for downloadable PC games, with a userbase of about 40 million accounts.\n\nIn 2008, the website \"gog.com\" (formerly called \"Good Old Games\") was started, specialized in the distribution of older, classical PC games. While all the other DD services allow various forms of DRM (or even have them embedded) \"gog.com\" has a strict non-DRM policy. Desura was launched in 2010. The service was notable for having a strong support of the modding community and also has an open source client, called \"Desurium\". Origin, a new version of the Electronic Arts online store, was released in 2011 in order to compete with Steam and other digital distribution platforms on the PC.\n\nDigital distribution is the dominant method of delivering content on mobile platforms such as iOS devices and Android phones. Lower barriers to entry has allowed more developers to create and distribute games on these platforms, with the mobile gaming industry growing considerably as a result.\n\nToday, each of the current main consoles (Nintendo Switch, Xbox One, and PlayStation 4) has its own digital distribution platform to sell games exclusive to digital formats and digital versions of retail games. These are the Nintendo eShop, Xbox Live Marketplace, and PlayStation Store, respectively. Microsoft's Xbox Live Marketplace, Nintendo's eShop, and Sony's PlayStation Store all sell full retail games, along with other products, such as DLC.\n\nThe main advantages of digital distribution over the previously dominant retail distribution of video games include significantly reduced production, deployment, and storage costs.\n\nDigital distribution also offers new structural possibilities for the whole video game industry, which, prior to the emergence of digital media as a relevant means of distribution, was usually built around the relationship of the video game developer, who produced the game, and the video game publisher, who financed and organized the distribution and sale. The heightened production costs in the early 2000s made many video game publishers avoid risks and led to the rejection of many smaller-scale game development projects. Gabe Newell, co-founder of Valve, the developer and intellectual property rights owner of Steam, described the disadvantages of physical retail distribution for smaller game developers as such:\n\nSince the 2000s, when digital distribution saw its first meaningful surge in popularity, an increasing number of niche market titles have been made available and become commercially successful, including (but not limited to) remakes of classic games. The new possibilities of digital distribution stimulated the creation of game titles from small video game producers like independent game developers and modders (e.g. Garry's Mod), which before were not commercially feasible.\n\nThe increasing prevalence of digital distribution has allowed independent game developers to sell and distribute their games without having to negotiate deals with publishers. No longer required to rely on conventional physical retail sales, independent developers have seen success through the sale of games that normally would not be accepted by publishers for distribution. The PC and mobile platforms are the most prominent in regards to independent game distribution, with services such as GOG.com, GamersGate, Steam and the iOS App Store providing ways to sell games with minimal to no distribution costs. Some digital distribution platforms exist specifically for indie game distribution, such as the Xbox Live Indie Games.\n\nCompared to physically distributed games, digital games like those offered on the Steam digital distribution service cannot be lost or destroyed, and can be redownloaded at any time. Services like Steam, Origin, and Xbox Live do not offer ways to sell used games once they are no longer desired, even though some services like Steam do have family sharing options. This is also somewhat countered by frequent sales offered by these digital distributors, often allowing major savings by selling at prices below what a retailer is able to offer.\n\nIn contrast to physically distributed games, digital games can be easily purchased immediately, without leaving home. This way, gamers have immediate access to the games.\n\n\n\n"}
{"id": "31620373", "url": "https://en.wikipedia.org/wiki?curid=31620373", "title": "Dundee Cell Products", "text": "Dundee Cell Products\n\nDundee Cell Products (DCP) is a biotechnology company headquartered in Dundee, Scotland, United Kingdom.\nThe company is a bioreagents and life sciences services company which commercialises research tools for biochemistry, molecular biology and cell biology research, and provides services in these areas to the life sciences community. The company’s key business activities include research and development of new innovative products and services, commercialization of products in-licensed from academic institutions, distribution of life sciences research products from commercial partners and supply of contract research services to both academic and pharmaceutical/biotechnology companies customers.\n\nDundee Cell Products Ltd was founded in 2006 as a spinout of the internationally renowned School of Life Sciences of the University of Dundee by Dr Paul Ajuh (now with Gemini Biosciences ) and Prof. Angus Lamond (Lamond Lab). Since spinout the company has maintained close interactions with researchers at the institution and a strong relationship with the University. In 2007 the company secured equity investment funding from a syndicate of angel investors to finance and expand its activities with follow up investment secured in 2009 from angel investors and the Scottish Co-investment Fund (SCF). The investment was secured to support the business’s growth plans. Over the last four years DCP has enjoyed significant growth in its products portfolio and turnover.\n\nIn the past three years, the company has developed and commercialized several new products e.g. SILAC ready to use media and has been involved in proteomics research collaborations with scientists in both academic institutions and pharmaceutical companies. Some of these projects have been designed to investigate the effects of virus infections on cellular proteomes in order to elucidate the mechanisms and cellular pathways involved in virus–host factor interactions. DCP has also collaborated in a phosphoproteomics project designed to study the molecular mechanisms in the pathogenesis of transmissible spongiform encephalopathies (TSEs) or prion disease. Transmissible spongiform encephalopathies are fatal diseases associated with the conversion of the cellular prion protein to an abnormal prion protein leading to damage of brain and neural tissue. Examples of prion diseases include bovine spongiform encephalopathy (BSE, also known as \"mad cow disease\") in cattle and Creutzfeldt–Jakob disease (CJD) in humans. There is currently no treatment for prion diseases, which are usually fatal to the host. \n\nCurrent R&D projects within the company have been focused on developing applications for SILAC quantitative proteomics in predictive toxicology. These projects are designed to provide data that will be used in developing predictive models for drug toxicity early on during the drug discovery and development process.\n\nDCP initially developed antibodies, mammalian cell fractions for precursor mRNA splicing research and tissue culture media specifically formulated for SILAC quantitative proteomics. The first products commercialized by the company came from the research interests of the founding scientists who saw an opportunity in the market for developing and commercializing high quality research products and services in the research area to other colleagues. The company has subsequently expanded its products and services portfolio into many other areas of life sciences research to better cover the needs of its customers.\n\nDundee Cell Products commercializes a broad range of life sciences research products (many of which have been developed in-house) including cell culture media for quantitative proteomics, mammalian cell fractions e.g. nuclei, nucleoli, mitochondria and cell fractions for quantitative proteomics, antibodies, recombinant proteins, fluorescent cell markers and primary mammalian cells. The company’s SILAC ready to use media have been specially formulated to facilitate the use of SILAC technology by non-specialists in proteomics and scientists who are interested in the application of unbiased high throughput quantitative proteomics approaches in their R&D activities.\n\nAs well as offering research products, the company’s portfolio has grown to include research services designed to accelerate the research activities of scientists in both the academic and biotech/pharmaceutical industry sectors. Some of the research services that DCP offers include gene synthesis, DNA cloning, recombinant protein expression and purification, stable cell line development, custom monoclonal and polyclonal antibody development and various proteomics services i.e. both quantitative (e.g. SILAQ, Tandem Mass Tags, iTRAQ, Isotope-coded affinity tag (ICAT), label-free) and qualitative. \nIn collaboration with scientists at the University of Dundee, the company is developing novel and innovative products and services in the areas of proteomics, molecular biology and cell biology focused on the needs of research scientists in academic institutions as well as those in biotechnology and pharmaceutical companies.\n"}
{"id": "5619624", "url": "https://en.wikipedia.org/wiki?curid=5619624", "title": "Edith Clarke", "text": "Edith Clarke\n\nEdith Clarke (February 10, 1883 – October 29, 1959) was the first female electrical engineer and the first female professor of electrical engineering at the University of Texas at Austin. She specialized in electrical power system analysis and wrote \"Circuit Analysis of A-C Power Systems\".\n\nEdith Clarke was born February 10, 1883, in Howard County, Maryland to John Ridgely Clarke and Susan Dorsey Owings, one of nine children. After being orphaned at age 12, she was raised by her older sister. She used her inheritance to study mathematics and astronomy at Vassar College, where she graduated in 1908.\n\nAfter college, Clarke taught mathematics and physics at a private school in San Francisco and at Marshall College. She then spent some time studying civil engineering at the University of Wisconsin–Madison, but left to become a \"computer\" at AT&T in 1912. She computed for George Campbell, who applied mathematical methods to the problems of long-distance electrical transmissions. While at AT&T, she studied electrical engineering at Columbia University by night.\n\nIn 1918, Clarke enrolled at the Massachusetts Institute of Technology, and the following year she became the first woman to earn an M.S. in electrical engineering from MIT.\n\nUnable to find work as an engineer, she went to work for General Electric as a supervisor of computers in the Turbine Engineering Department. During this time, she invented the Clarke calculator, in 1921, a simple graphical device that solved equations involving electric current, voltage and impedance in power transmission lines. The device could solve line equations involving hyperbolic functions ten times faster than previous methods. She filed a patent for the calculator in 1921 and it was granted in 1925.\nIn 1921, still unable to obtain a position as an engineer, she left GE to teach physics at the Constantinople Women's College in Turkey. The next year, she was re-hired by GE as an electrical engineer in the Central Station Engineering Department. Clarke retired from General Electric in 1945.\n\nHer background in mathematics helped her achieve fame in her field. On February 8, 1926, as the first woman to deliver a paper at the American Institute of Electrical Engineers' annual meeting, she showed the use of hyperbolic functions for calculating the maximum power that a line could carry without instability. Two of her later papers won awards from the AIEE: the Best Regional Paper Prize in 1932 and the Best National Paper Prize in 1941.\n\nIn 1943, Edith Clarke wrote an influential textbook in the field of power engineering, \"Circuit Analysis of A-C Power Systems\", based on her notes for lectures to GE engineers.\n\nIn 1947, she joined the faculty of the Electrical Engineering Department at the University of Texas at Austin, making her the first female professor of Electrical Engineering in the country. She taught for ten years and retired in 1957.\n\nIn an interview with the Daily Texan on March 14, 1948, Clarke observed: \"There is no demand for women engineers, as such, as there are for women doctors; but there's always a demand for anyone who can do a good piece of work.\"\n\nEdith Clarke was the first female engineer to achieve professional standing in Tau Beta Pi.\nIn 1948, Clarke was the first female Fellow of the American Institute of Electrical Engineers. \nIn 1954, she received the Society of Women Engineers Achievement Award.\n\nIn 2015, Clarke was posthumously inducted into the National Inventors Hall of Fame.\n\n"}
{"id": "32741783", "url": "https://en.wikipedia.org/wiki?curid=32741783", "title": "Emtech", "text": "Emtech\n\nThe Emtech (short for \"Emerging Technologies\") conference, produced by the Massachusetts Institute of Technology's Technology Review magazine, is an annual conference highlighting invention and new developments in engineering and technology. Started in 1999, the 2011 conference is planned for October 18-19 at MIT.\n\nIn addition to two days of presentations, the conference highlights the winners of the annual TR35 award, recognizing the world's top 35 innovators under the age of 35. Some of the most famous winners of the award include Larry Page and Sergey Brin (creators of Google), Mark Zuckerberg (creator of Facebook), Jack Dorsey (creator of Twitter), and Konstantin Novoselov, who later won the Nobel Prize in Physics.\n"}
{"id": "37624856", "url": "https://en.wikipedia.org/wiki?curid=37624856", "title": "Examination table", "text": "Examination table\n\nAn examination table (or exam table) is used to support patients during medical examinations. During these exams, doctors in offices (UK: surgeries), clinics and hospitals use an adjusting mechanism to manipulate and position the table to allow patient support, closer examination of a portion or the entire patient, and the ability to move the patient on and off the table safely. Examination tables often have rolls of paper in which patients sit on, protecting the table. The paper is normally discarded after each patient uses the table.\n\nExamination tables have included electric motors since the 1970s. These are fitted underneath the tabletop and power cables generally detach to prevent a tripping hazard. The ability to transfer power forward and backwards using a reversible electric motor means greater mobility of the examination table.\n\n\n"}
{"id": "52641508", "url": "https://en.wikipedia.org/wiki?curid=52641508", "title": "Exergy (software)", "text": "Exergy (software)\n\nExergy is software developed by SilverBridge Holdings, a South African developer of business software for the financial services market in Africa. SilverBridge Holdings listed on the AltX on 27 November 2006.\n\nThe software is designed for life insurance companies to assist them in the administration of their policies.\n\nIn 2012, SilverBridge entered into a partnership with Net2Africa with the purpose of expanding the services offering of Exergy.\n\nExergy celebrated its 10th anniversary in 2014. In the same year, the software saw the introduction of Exergy KnowledgeBase, designed to provide a way of reducing risk in IT projects.\n\nAt the company's 2016 results, it was reported that Exergy is being used by businesses across Africa including in Angola, Botswana, Kenya, Malawi, Mauritius, and Ghana.\n\nIn November 2016, it was reported that SilverBridge has done an implementation at GetSure Zimbabwe to help the company migrate its business to a cloud computing system. The solution was based on the Exergy system.\n"}
{"id": "26050070", "url": "https://en.wikipedia.org/wiki?curid=26050070", "title": "GOPPAR", "text": "GOPPAR\n\nGOPPAR is the abbreviation for gross operating profit per available room, a key performance indicator for the hotel industry.\n\nIt gives greater insight in the actual performance of a hotel than the most commonly used RevPAR as it not only considers revenues generated, but also factors in operational costs related with such revenues.\n\nGOPPAR is the total revenue of the hotel less expenses incurred earning that revenue, divided by the available rooms.\n\nGOPPAR does not take into consideration the revenue mix of the hotel, so while it does not allow an accurate evaluation of the room revenue generated it demonstrates the profitability and value of the property as a whole.\n\n\n"}
{"id": "10795829", "url": "https://en.wikipedia.org/wiki?curid=10795829", "title": "Gil Shwed", "text": "Gil Shwed\n\nGil Shwed (Hebrew: גיל שויד; born in 1968 in Jerusalem) is an Israeli software engineer, inventor and entrepreneur. He is the co-founder and CEO of Check Point Software Technologies Ltd, one of Israel's largest technology companies and the world's largest pure-play cybersecurity company.\n\nShwed was born in Jerusalem in 1968. He started programming at the age of 13, and two years later began studying computer sciences at Hebrew University in Jerusalem, while he was still in high-school.\n\nDuring his military service he was part of the Intelligence Corps' Unit 8200. After his military service had ended, Shwed joined the Israeli startup company Optrotech (currently Orbotech), where he worked as a Software Developer.\n\nIn 1993, Shwed founded Check Point together with Shlomo Kramer, Shwed’s friend from the military unit, and Marius Nacht with whom he worked at Optrotech. \n\nThat year, Shwed invented and patented stateful inspection, which served as the basis for the first version of the company’s renowned FireWall-1, released in 1994. Stateful Inspection is still widely used in network firewalls today.\n\nShwed is the chief executive officer and director of Check Point. He previously served as president of Check Point and chairman of the board. As of 2018, Shwed is the leading shareholder in Checkpoint, owning 19.1% of the company, with an estimated worth of $3.4 billion USD. The second largest shareholder in Check Point is US investment company Massachusetts Financial Services Company with a 7.7% stake worth $1.3 billion at present..\n\nShwed and Check Point emphasize the \"fifth generation\" of cybersecurity, addressing the underlying issues behind such vulnerabilities as the WannaCry and NotPetya security breaches in 2017. He has stated that enterprise businesses are \"two generations behind\" in their security thinking, and describes the industry as being at an \"inflection point.\"\n\nHe is currently a member of the Board of Trustees of Tel Aviv University and the Chairman of the Board of Trustees of the Youth University of Tel Aviv University. He is also a member of the Board of Directors of Yeholot Association Founded by the Rashi Foundation whose charter is, among other things, to reduce dropout rates in high schools.\n\nIn 2002, Shwed appeared on the front cover of Forbes’ Billionaires issue. \n\nHe received numerous prestigious accolades for his individual achievements and industry contributions, including an honorary Doctor of Science from the Technion – Israel Institute of Technology, which he received in June 2005.\n\nHe was also acknowledged by the World Economic Forum's Global Leader for Tomorrow as one of the world's 100 top young leaders, due to his commitment to public affairs and leadership in areas beyond immediate professional interests. In addition, Shwed received the Academy of Achievement's Golden Plate Award for his innovative contribution to business and technology.\n\nShwed received the Israel-America Chambers of Commerce Industry Award on behalf of Check Point Software Technologies, for companies who demonstrate entrepreneurial abilities and excellence in the field of advanced technologies. \n\nIn 2010, Shwed was recognized as the Ernst & Young Entrepreneur of the Year in Israel.\n\nGlobes, one of Israel's largest financial outlets honored Shwed as their \"Person of the Year\" in 2014.In September 2017, Shwed was ranked 12th in TheMarker Magazine list.\n\nIn 2018, Shwed was the recipient of the first-ever Israel Prize in technology.\n\n"}
{"id": "2193225", "url": "https://en.wikipedia.org/wiki?curid=2193225", "title": "Headphone amplifier", "text": "Headphone amplifier\n\nA headphone amplifier is a low-powered audio amplifier designed particularly to drive headphones worn on or in the ears, instead of loudspeakers in speaker enclosures. Most commonly, headphone amplifiers are found embedded in electronic devices that have a headphone jack, such as integrated amplifiers, portable music players (e.g., iPods), and televisions. However, standalone units are used, especially in audiophile markets and in professional audio applications, such as music studios. Headphone amplifiers are available in consumer-grade models used by hi-fi enthusiasts and audiophiles and professional audio models, which are used in recording studios.\n\nConsumer headphone amplifiers are commercially available separate devices, sold to a niche audiophile market of hi-fi enthusiasts. These devices allow for higher possible volumes and superior current capacity compared to the smaller, less expensive headphone amplifiers that are used in most audio players. In the case of the extremely high-end electrostatic headphones, such as the Stax SR-007, a specialized electrostatic headphone amplifier or transformer step-up box and power amplifier is required to use the headphones, as only a dedicated electrostatic headphone amplifier or transformer can provide the voltage levels necessary to drive the headphones. Most headphone amplifiers provide power between 10 mW and 2 W depending on the specific headphone being used and the design of the amplifier. Certain high power designs can provide up to 6W of power into low impedance loads, although the benefit of such power output with headphones is unclear, as the few orthodynamic headphones that have sufficiently low sensitivities to function with such power levels will reach dangerously high volume levels with such amplifiers.\n\nEffectively, a headphone amplifier is a small power amplifier that can be connected to a standard headphone jack or the line output of an audio source. Electrically, a headphone amplifier can be thought of as an amplifier that presents a very high input impedance (ideally infinite) and presents a lower output impedance (ideally zero) and larger range of output voltages (ideally infinite). This allows headphones of a low sensitivity to be driven louder as a result of the extra voltage provided by the amplifier. There are potential fidelity gains if headphones are driven with lower distortion than using a headphone amplifier integrated into a general purpose audio product. In practice, this most often occurs when using low impedance headphones with consumer electronics with insufficiently low output impedance (see impedance discussion below).\n\nMost headphone amplifiers support a higher voltage output and therefore higher power (volume) levels. Whereas most portable electronics are powered by a 1.8, 2.5 or 3.3 Vpp supply, many headphone amplifiers use 10, 18 or 24 Vpp supplies, allowing 5-20 dB higher volume. If a pair of headphones is too quiet, adding an amplifier that can output higher voltage/power will increase its volume.\n\nMany headphone amplifiers have an output impedance in the range of 0.5 - 50 Ohms. The 1996 IEC 61938 standard recommended an output impedance of 120 Ohms, but in practice this is rarely used and not recommended with modern headphones. High output impedance can result in frequency response fluctuations, due to varying load impedance at different frequencies. In 2008 Stereophile Magazine published an article that showed that a 120-Ohm output impedance could cause a 5-dB error in frequency response with certain types of headphones. However, the author of the article also states: \"The ramifications for subjective assessment of headphones are more troublesome because it is usually unclear what assumptions the manufacturer has made regarding source impedance.\" \n\nMore importantly, low output impedance can reduce distortion by improving the control that the source has over the transducer. This is often expressed as damping factor, with higher damping factors greatly reducing distortion. One company shows a 45 dB improvement in THD+N at 30 Hz for their low-impedance amplifier compared to a 30-ohm amplifier. For example, a 32 Ω headphone driven by a headphone amp with a <1 Ω output impedance would have a damping factor of >32, whereas the same headphone driven with an iPod touch 3G (7 Ω output impedance) would have a damping factor of just 4.6. If the 120 ohms recommendation is applied, the damping factor would be an unacceptably low 0.26 and consequently distortion would be significantly higher. Conversely, the same iPod touch driving a pair of 120 ohm headphones would have a respectable damping factor of 17.1, and would most likely not benefit from the addition of a lower impedance headphone amplifier.\n\nIn addition to output impedance, other specifications are relevant to choosing a headphone amplifier — THD, frequency response, IMD, output power, minimum load impedance, and other measurements are also significant. However, most of these will be improved by lowering output impedance and hence improving damping factor.\n\nFor those who are electronically inclined, the low-power and fairly simple nature of the headphone amplifier has made it a popular DIY project. There are many designs for headphone amplifiers posted on the Internet varying considerably in complexity and cost. A key example is the simple opamp-based CMoy design, one of the most popular headphone amplifier designs available. The simplicity of the CMoy makes it an easy build, while it can be made small enough to fit inside a tin of breath mints (including batteries). On the other hand, it is often built using op-amps that are not designed to drive loads as low as headphones, leading to poor performance and audible differences between op-amps that would not exist in a good design.\n\nCrossfeeding blends the left and right stereo channels slightly, reducing the extreme channel separation which is characteristic of headphone listening in older stereo recordings, and is known to cause headaches in a small fraction of listeners. Crossfeed also improves the soundstage characteristics and makes the music sound more natural, as if one was listening to a pair of speakers. While some swear by crossfeed, many prefer amplifiers without it. The introduction of digital signal processing (DSP) technology led a number of manufacturers to introduce amplifiers with 'headphone virtualization' features. In principle, the DSP chips allow the two-driver headphone to simulate a full Dolby 5.1 (or more) surround system. When the sounds from the two headphone drivers mix, they create the phase difference the brain uses to locate the source of a sound. Through most headphones, because the right and left channels do not combine as they do with crossfeed, the illusion of sound directionality is created.\n\nIn pro-audio terminology, a headphone amplifier is a device that allows multiple headsets to be connected to one or more audio sources (typically balanced audio sources) at the same time to monitor sounds during a recording session, either singing or playing from the \"live room\" or recorded tracks. Headphone amps enable singers and musicians to be able to hear other musicians who are playing in isolation booths. They also enable audio engineers and record producers to monitor a live performance or live tracking.\n\nHeadphone amps with sub-mixing capabilities allow the listener to adjust, mix and monitor audio signals coming from multiple sources at the same time. This kind of headphone amp is often utilized during recording sessions to sub-mix playback of individual stem-mixes or instruments coming from a mixing board or a playback device. In many cases the listeners have their own sets of controls allowing them to adjust various aspects of the mix and individual and global parameters such as channel level, global loudness, bass and treble.\n\nDistribution headphone amplifiers are specialized headphone amps allowing a single signal to be fed to multiple headsets or multiple groups of multiple headsets at the same time. Many distribution headphone amps, like the one shown here, can be cascaded by connecting the audio input of one of the amps to the cascading output, marked \"THRU\", of another amp.\n\nThere are also various other combinations of pro-audio headphone amps with simultaneous sub-mixing and distribution capabilities.\n\n\n"}
{"id": "20296404", "url": "https://en.wikipedia.org/wiki?curid=20296404", "title": "Henry Doetsch", "text": "Henry Doetsch\n\nHenry Doetsch (31 January 1839 - 25 May 1894) was a German born industrialist who lived in London.\n\nHe was born Heinrich Moritz Doetsch the 31 January 1839 at the \"Burghof\", the 17th century manor of his family in Kärlich (Mülheim-Kärlich, Weißenthurm, Rheinland). Because of a lung weakness, Henry (Enrique) Doetsch went (after a short time in Liverpool) as a young man of 23 years to Sevilla in Spain, where he entered with the help of his cousin, Moritz Willmar-Doetsch from Frankfurt/Main, as volunteer a British company. A few years later he was founder of the »Firma Sundheim y Doetsch« in Huelva (1865) with his compagnon Guillermo Sundheim (1840 - 1903). In 1873 Doetsch was the director of the Rio Tinto Company Limited, (RTCL), (Minas de Río Tinto), in London and Huelva. Together with Hugh Matheson, chairman of the Rio Tinto Company, and Guillermo (Wilhelm) Sundheim he built up a railway line in Andalucia: The »Compañía del Ferrocarril de Zafra a Huelva«. Doetsch held a patent in Canada for an extraction process to remove copper from its ores, registered in September 1879, so he was involved in mining and production of copper in Río Tinto by the so-called »Doetsch process«. But the use of Teladas poisoned the air and many residents became ill or died. Henry Doetsch fell out with Hugh Matheson on the issue of supply of the Miners (Sundheim had left the RTCL in 1876). During an uprising of the miners in 1888 more than 100 civilians were killed by Spanish soldiers.\nDoetsch left Spain in autumn 1884, after he had invited his old friend »Fred« Burnaby to spend a month at Huelva. A few weeks later Burnaby gave up all his engagements, and fared forth for the Soudan. \nThe three times offered title of a »Marqués de Río Tinto« by the Spanish King Alfonso XII refused Enrique Doetsch. He was awarded the grand cross of the Orden de Isabel la Católica. For many summertimes, Doetsch has been a traveling companion of the Infanta Maria de la Paz and the Infanta Eulalia of Spain. \nHenry Doetsch was succeeded at the Rio Tinto Group by his nephew Carlos Doetsch (1870 - 1951), who married in 1902 Justita Sundheim de la Cueva, daughter of Guillermo Sundheim and Justa de la Cueva y Camporedondo. The couple with the children Mercedes and Jorge lived in Huelva and Madrid.\n\nIn London Henry Doetsch lived as a respected financier at 7 New Burlington Street. A generous host and notorious bachelor, Doetsch opened his home and Art Gallery to an interested audience. Still in Spain Henry Doetsch started to collect pictures by old masters. Staying in contact with Bernard Berenson, masterpieces by great painters were gathered in the collection: Titian, Veronese, Guido Reni, Dolci, Lorenzo Lotto, Palma Vecchio, Pontormo, Bronzino, Andrea del Sarto, van Dyck, Rubens, Breughel, Rembrandt, Frans Hals, Wouwerman, Clouet and Holbein. Some of the paintings came from important galleries: Orléans-Galliera, Palazzo Vendramin (Henri Duc de Bordeaux), The Collection King Charles I, Pinacotece Lochis, or from privat collections as those of the Duke of Roxburghe, the Marquis of Donegall or Conte Alborghetti. \nDoetsch's evenings at New Burlington Street were famous: Among his guests the Prince of Wales (Edward VII), Lord Rosebery and his friend Colonel Burnaby. Doetsch was a lover of black Havana cigars; he died of pneumonia on 25 May 1894 in London.\n\nAfter Doetsch's death in 1894, on 22nd, 24th and 25 June 1895, his pictures of Old Masters were auctioned by Christie's (Messrs. Christie, Manson and Woods), at their offices and sale rooms at 8 King Street, St. James's Square, and the sale appears to have been an enormous success. Some fine examples of paintings are now owned by H. M. the Queen and the House of Hesse at Kronberg. Henry Doetsch's career appears to have mirrored that of his countryman, Ludwig Mond, but Dr. Mond is remembered through the company that bears his name (Brunner-Mond) and the Mond Room at the National Gallery in London; Doetsch is largely forgotten. An Illustrated Catalogue of the Doetsch-Collection was prepared for Victoria, Empress Frederick (1895).\n\nJ. P. Richter, Ph. Dr., Catalogue of the Highly Important Collection of Pictures by Old Masters of Henry Doetsch, Esq., deceased, late of 7 New Burlington Street, which (by Order of the Executers) will be sold by Auction by Messrs. Christie, Manson & Woods. 8 King Street, St. James Square (London 1895).\n\nHenry Lucy, Faces and Places. London 1892. (The Project Gutenberg eBook. Prepared by Ruth Golding)\n\nJohannes Rein, Geographische und naturwissenschaftliche Abhandlungen. Zur vierhundertjährigen Feier der Entdeckung Amerikas: Natur und hervorragende Erzeugnisse Spaniens: Bergbau in der Provinz Huelva (Río Tinto). Leipzig, Engelmann, 1892\n\nStraßburger Post: Heinrich Doetsch in London verstorben, 27. Mai 1894\n\nKölnische Zeitung: Coblenz: Über den verstorbenen Herrn Heinrich Doetsch, 29. Mai 1894\n\nKölnische Zeitung: Coblenz: Ein Ereignis in London, 25. Mai 1895\n\nMeryle Secrest, Being Bernard Berenson. A Biography. New York 1979.\n\nMaría Asunción Díaz Zamorano: Huelva. La construcción de una ciudad. Excelentísimo Ayuntamiento de Huelva, Huelva, 1999.\n\nJuan Cobos Wilkins, El corazón de la tierra. Novela. Plaza & Janés 2007.\n\nEl corazón de la tierra. Película de Antonio Cuadri 2007; (»The heart of the earth«, cinefilm, Spain/Great Britain 2007)\n\nMaría Antonia Peña Guerrero, Adolfo Sundheim Lindemann y el triángulo Barranquilla-Bremen-Huelva. Universidad de Sevilla-Huelva 2009.\n\nMichael Schroeder, »Onkel Henry«: Zum 100ten Todestag von Henry Doetsch (1839–1894), I 1994 (II Ortenberg/Hessen 2012)\n"}
{"id": "55465", "url": "https://en.wikipedia.org/wiki?curid=55465", "title": "High-occupancy vehicle lane", "text": "High-occupancy vehicle lane\n\nA high-occupancy vehicle lane (also known as an HOV lane, carpool lane, diamond lane, 2+ lane, and transit lane or T2 or T3 lanes) is a restricted traffic lane reserved for the exclusive use of vehicles with a driver and one or more passengers, including carpools, vanpools, and transit buses. These restrictions may be only imposed during at peak travel times or may apply at all times. The normal minimum occupancy level is 2 or 3 occupants. Many jurisdictions exempt other vehicles, including motorcycles, charter buses, emergency and law enforcement vehicles, low-emission and other green vehicles, and/or single-occupancy vehicles paying a toll. HOV lanes are normally created to increase average vehicle occupancy and persons traveling with the goal of reducing traffic congestion and air pollution, although their effectiveness is questionable.\n\nRegional and corporate-sponsored vanpools, carpools, and rideshare communities give commuters a way to increase occupancy. For places without such services, online rideshare communities can serve a similar purpose. Slugging lines are common in some places, where solo drivers pick up a passenger to share the ride and allow them to use the HOV lane. High-occupancy toll lanes (HOT lanes), which allow solo driver vehicles to use HOV lanes on payment of a fee which varies depending on demand, have also been introduced in the United States and Canada.\n\nThe introduction of HOV lanes in the United States progressed slowly during the 1970s and early 1980s. Major growth occurred from the mid-1980s to the late 1990s. The first freeway HOV lane in the United States was implemented in the Henry G. Shirley Memorial Highway in Northern Virginia, between Washington, DC, and the Capital Beltway, and was opened in 1969 as a bus-only lane. The busway was opened in December 1973 to carpools with four or more occupants, becoming the first instance in which buses and carpools officially shared a HOV lane over a considerable distance.\n\nIn 2005, the two lanes of this HOV 3+ facility carried during the morning peak hour (6:30 am to 9:30 am) a total of 31,700 people in 8,600 vehicles (3.7 persons/veh), while the three or four general-purpose lanes carried 23,500 people in 21,300 vehicles (1.1 persons/veh). Average travel time in the HOV facility was 29 minutes, and 64 minutes in the general traffic lanes. As of 2012, the I-95/I-395 HOV facility is long and extends from Washington, D.C., to Dumfries, Virginia, and has two reversible lanes separated from the regular lanes by barriers, with access through elevated on- and off-ramps. Three or more people in a vehicle (HOV 3+) are required to travel on the facility during rush hours on weekdays.\n\nThe second freeway HOV facility was the contraflow bus lane on the Lincoln Tunnel Approach and Helix in Hudson County, New Jersey, opened in 1970. According to the Federal Highway Administration (FHWA), the Lincoln Tunnel XBL is the country's HOV facility with the highest number of peak hour persons among HOV facilities with utilization data available, with 23,500 persons in the morning peak, and 62,000 passengers during the four-hour morning peak.\n\nThe first permanent HOV facility in California was the bypass lane at the San Francisco–Oakland Bay Bridge toll plaza, opened to the public in April 1970. The El Monte Busway (I-10 / San Bernardino Freeway) in Los Angeles was initially only available for buses when it opened in 1973. Three-person carpools were allowed to use the bus lane for three months in 1974 due to a strike by bus operators, and then permanently at a 3+ HOV from 1976. It is one of the most efficient HOV facilities in North America and is currently being converted into a high-occupancy toll lane operation to allow low-occupancy vehicles to bid for excess capacity on the lane in the Metro ExpressLanes project.\n\nBeginning in the 1970s, the Urban Mass Transit Administration recognized the advantages of exclusive bus lanes and encouraged their funding. In the 1970s the FHWA began to allow state highway agencies to spend federal funds on HOV lanes. As a result of the 1973 Arab Oil Embargo, interest in ridesharing picked up, and states began experimenting with HOV lanes. In order to reduce crude oil consumption, the 1974 Emergency Highway Energy Conservation Act mandated maximum speed limits of on public highways and became the first instance when the U.S. federal government provided funding for ridesharing and states were allowed to spend their highway funds on rideshare demonstration projects. The 1978 Surface Transportation Assistance Act made funding for rideshare initiatives permanent.\n\nAlso during the early 1970s, ridesharing was recommended for the first time as a tool to mitigate air quality problems. The 1970 Clean Air Act Amendments established the National Ambient Air Quality Standards and gave the Environmental Protection Agency (EPA) substantial authority to regulate air quality attainment. A final control plan for the Los Angeles Basin was issued in 1973, and one of its main provisions was a two-phase conversion of of freeway and arterial roadway lanes to bus/carpool lanes and the development of a regional computerized carpool matching system. However, it took until 1985 before any HOV project was constructed in Los Angeles County, and by 1993 there were only of HOV lanes countywide.\n\nA significant policy shift took place in October 1990, when a memorandum from the FHWA administrator stated that \"FHWA strongly supports the objective of HOV preferential facilities and encourages the proper application of HOV technology.\"\" Regional administrators were directed to promote HOV lanes and related facilities. Also in the early 1990s, two laws reinforced the U.S. commitment to HOV lane construction. The Clean Air Act Amendments of 1990 included HOV lanes as one of the transportation control measures that could be included in state implementation plans to attain federal air quality standards. The 1990 amendments also deny the administrator of the EPA the authority to block FHWA from funding 24-hour HOV lanes as part of the sanctions for a state's failure to comply with the Clean Air Act, if the secretary of transportation wishes to approve the FHWA funds.\n\nOn the other hand, the Intermodal Surface Transportation Efficiency Act (ISTEA) of 1991 encouraged the construction of HOV lanes, which were made eligible for Congestion Mitigation and Air Quality (CMAQ) funds in regions not attaining federal air quality standards. CMAQ funds may be spent on new HOV lane construction, even if the HOV designation holds only at peak travel times or in the peak direction. ISTEA also provided that under the Interstate Maintenance Program, only HOV projects would receive the 90% federal matching ratio formerly available for the addition of general purpose lanes. And ISTEA permitted state authorities to define a high occupancy vehicle as having a minimum of two occupants (HOV 2+).\n\nAs of 2009, California was the state with the most HOV facilities in the country, with 88, followed by Minnesota with 83 facilities, Washington with 41, Texas with 35, and Virginia with 21. The only active U.S. facility with two HOV lanes in each direction is I-110 between Adams Boulevard and SR 91 in Los Angeles. By 2006, HOV lanes in California were operating at two-thirds of their capacity, and these HOV facilities carried on average 2,518 persons per hour during peak hours, substantially more people than the congested general-traffic lanes.\n\nAs of October 2016, the longest continuous HOV facility in the U.S. is on I-15 in Utah, extending approximately from Spanish Fork to Layton with a single HOV lane each direction for a total of of HOV lanes. While the Utah facility is the longest, the I-495 Capital Beltway in the Washington, D.C., Metropolitan Area extends but has two HOV lanes in each direction for a total of of HOV lanes. \n\nAs of 2012, there are some 126 HOV facilities on freeways in 27 metropolitan areas in the United States, which includes over 1,000 corridor miles (1,600 km).\n\nThe first HOV facilities in Canada were opened in Greater Vancouver and Toronto in the early 1990s, followed shortly by facilities in Ottawa, Gatineau, Montreal, and later Calgary. As of 2010 there were about of highway HOV lanes in 11 locations in British Columbia, Ontario, and Quebec, and over of arterial HOV lanes in 24 locations in Greater Vancouver, Calgary, Toronto, Ottawa, and Gatineau. The Ontario Ministry of Transportation (MTO) estimated in 2006 that commuters in Toronto using the HOV facilities on Highways 403 and 404 were saving 14–17 minutes per trip compared to their travel time before the HOV lanes opened. The MTO also estimated that almost 40% of commuters were carpooling on Highway 403 eastbound in the morning peak hour, compared to 14% in 2003, and 37% of commuters were carpooling on Highway 403 westbound in the afternoon peak hour, compared to 22% in 2003. The average rush hour speed on the HOV lanes is , compared to in general-traffic lanes on Highway 403.\nTemporary HOV lanes were added to selections of 400-series highways in the Greater Toronto Area for the 2015 Pan American Games and 2015 Parapan American Games.\n\nAs of 2012, there are a few HOV lanes in operation in Europe. The main reason for this is that, in general, European cities have better public transport services and fewer high-capacity multi-lane urban motorways than do the U.S. and Canada. However, at around 1.3 persons per vehicle, average car occupancy is relatively low in most European cities. The emphasis in Europe has been on providing bus lanes and on-street bus priority measures. The first HOV lane in Europe was opened in the Netherlands in October 1993 and operated until August 1994. Its facility was a barrier-separated HOV 3+ on the A1 near Amsterdam. The facility did not attract enough users to overcome public criticism and was converted to a reversible lane open to general traffic after the judge in a legal test case ruled that Dutch traffic law lacked the concept of a car pool and thus that the principle of equality was violated.\n\nSpain was the next European country to introduce HOV lanes, when median reversible HOV lanes were opened in Madrid's N-VI National Highway in 1995. This facility is Europe's oldest HOV facility that is still in operation. The first HOV facility in the United Kingdom opened in Leeds in 1998. The facility was implemented on A647 road near Leeds as an experimental scheme, but it became permanent. The HOV facility is long and operates as a HOV 2+ facility. A HOV 3+ facility opened in Linz, Austria, in 1999. Sweden opened its first HOV lane in Stockholm in 2000, an HOV 3+ facility. The first HOV lane in Norway was implemented in May 2001 as an HOV 3+ on Elgeseter Street, an undivided four-lane arterial road in Trondheim. This facility was followed by HOV lanes in Oslo and Kristiansand.\n\nThe first HOV lane (known as a Transit Lane T2 or T3) in Australia opened in February 1992, located on the Eastern Freeway in Melbourne travelling inbound. In May 2005, T2 Transit lanes were opened on Hoddle Street in Melbourne. As of 2012, there were also T2 and T3 facilities in Canberra, Sydney and Brisbane.\n\nIn Auckland, New Zealand, there are several short HOV 2+ and 3+ lanes throughout the region, commonly known as T2 and T3 lanes. There is a T2 transit lane in Tamaki Drive, in a short stretch between Glendowie and downtown Auckland. There are also T2 priority lanes on Auckland's Northern, Southern, Northwestern, and Southwestern Motorways. These priority lanes are left-side on-ramp lanes heading towards the motorway, where vehicles with two or more people can bypass the ramp meter signal. Priority lanes can also be used by trucks, buses, and motorcycles, and the priority lanes can be used by carpoolers at any time. Eleven lanes were opened to electric vehicles in a one-year trial from September 2017. There are also several short T2 and T3 facilities in North Shore City operating during rush hours.\n\nIn Jakarta, HOV 3+ is known as \"Three in One\" (\"Tiga dalam satu\") and was first implemented by governor Sutiyoso. HOV 3+ is implemented on weekdays in existing roads of Sisingamangraja Road (fast and slow lane), Jenderal Sudirman Road (fast and slow lane), MH. Thamrin Road (fast and slow lane), Medan Merdeka Barat Road, Majapahit Road, and sections of Jalan Jenderal Gatot Subroto. The policy was originally implemented only between 7:00 am and 10:00 am. Since the introduction of Jakarta's bus rapid transit in December 2003, the policy was extended to 7:00 am – 10:00 am and 4:00 pm – 7:00 pm. In September 2004, the evening time was changed to 4:30 pm – 7:00 pm.\n\nIn Shenzhen, HOV 2+ has been implemented on Binhai Avenue since 25 April 2016. The policy was then extended to 7:30 am – 9:30 am and 5:30 pm – 9:30 pm.\n\nIn Chengdu, from January 23, 2017, HOV 2+ has been implemented on Kehua Road South, Kehua Road Middle, and Tianfu Avenue Section 1 and 2, during 7:00 am-9: 00 am and 5:00 pm-7: 00 pm.\n\nIn Dalian, an expressway (Northeast Expressway, or Dongbei Expressway) linking old town and new town had one lane in both outbound and inbound directions set to HOV 2+. Starting from September 20, 2017, commuters can opt to drive in HOV lane on Northeast Expressway during the morning peak hours of 06:30-08:30, and evening peak hours of 16:30-19:00. A fine of CNY100 (about USD15) will be enforced for first violators. For a second violation, the fine will double.\n\nHOV lanes may be either a single traffic lane within the main roadway with distinctive markings or a separate roadway with one or more traffic lanes either parallel to the general lanes or grade-separated, above or below the general lanes. For example, Interstate 110 in California has four HOV lanes on an upper deck.\n\nHOV bypass lanes to allow carpool traffic to bypass areas of regular congestion in many places and an HOV lane may operate as a reversible lane, working in the direction of the dominant traffic flow in both the morning and the afternoon. All lanes of a section of the Interstate 66 in the suburbs of Washington, D.C., are treated as an HOV during the rush hour in the primary direction of flow.\n\nThe traffic speed differential between HOV and general-purpose lanes creates a potentially dangerous situation if the HOV lanes are not separated by a barrier. A Texas Transportation Institute study found that HOV lanes lacking barrier separations caused a 50% increase in injury crashes.\n\nA business access and transit (BAT) lane is a type of HOV lane that allows for all traffic to enter the lane for a short distance in order to access other streets and business entrances.\n\nBecause some HOV lanes were not utilized to their full capacity, users of low- or single-occupancy vehicles may be permitted to use an HOV lane if they pay a toll. This scheme is known as high-occupancy toll lane (or HOT lanes), and it has been introduced mainly in the United States. The first practical implementation was California's formerly private toll 91 Express Lanes, in Orange County, California, in 1995, followed in 1996 by Interstate 15 north of San Diego. According to the Texas A&M Transportation Institute, by 2012 there were 294 corridor-miles of HOT/Express lanes and 163 corridor-miles of HOT/Express lanes under construction in the United States.\n\nSolo drivers are permitted to use the HOV lanes upon payment of a fee that varies based on demand. Tolls change throughout the day according to real-time traffic conditions, which is intended to manage the number of cars in the lanes to maintain good journey times.\n\nProponents claim that all motorists benefit from HOT lanes, even those who choose not to use them. This argument applies only to projects that increase the total number of lanes. Proponents also claim that HOT lanes provide an incentive to use transit and ridesharing. There has been controversy over this concept, and HOT schemes have been called \"Lexus\" lanes, as critics see this new pricing scheme as a perk for the rich.\n\nHOT tolls are collected by manned toll booths, automatic number plate recognition, or electronic toll collection systems. Some systems use RFID transmitters to monitor entry and exiting of the lane and charge drivers depending on demand. Typically, tolls increase as traffic density and congestion within the tolled lanes increase, a policy known as congestion pricing. The goal of this pricing scheme is to minimize traffic congestion within the lanes.\n\nQualification for HOV status varies by scheme, but the following vehicles may be included:\n\nNew York City HOV lane codes prior to 2008 did not allow motorcycles leading to ticketing of motorcycle drivers and complaints from the American Motorcyclist Association, but have since been revised to comply with the federal regulations listed above.\n\nIn some jurisdictions such as Ontario, Canada, taxicabs and airport limousines are allowed to use HOV lanes even when no passenger is present because that vehicle \"will be able return to duty faster after dropping off a fare or arrive sooner to pick up a fare, thereby moving more people to their destinations in fewer vehicles\".\n\nIn Virginia, commuters form sluglines where drivers pick up one or more passengers from a designated \"slug lines\" to drive on HOV lanes along interstate 95/395; the driver pulls over near the sluglines and shouts out his or her destination, and people in the line going to that destination enter the car on a first-come, first-served basis.\n\nFines are usually imposed on drivers of non-qualifying vehicles who use the lanes.\n\nFollowing the introduction the HOVs, some drivers placed inflatable dolls in the passenger seat, a practice that persists today, even though it is now illegal. Cameras that can distinguish between humans and mannequins or dolls were tested in the United Kingdom in 2005.\n\nIn the United States, law enforcement officials have documented a variety of methods used by drivers in attempts to circumvent HOV occupancy rules:\n\nIn March 2015, a motorist tried to use a cardboard cutout of actor Jonathan Goldsmith to access an HOV lane in Fife, Washington. The officer noted that other drivers had used sleeping bags in earlier attempts to access the HOV lane.\n\nIn January 2013, a motorist tried to claim that the Articles of Incorporation of his business, which had been placed unbuckled on the passenger’s seat, constituted a person, citing the principle of corporate personhood and California's state Vehicle Code, which defines a person as \"natural persons and corporations\". This argument was rejected in traffic court, where the presiding judge commented, \"Common sense says carrying a sheaf of papers in the front seat does not relieve traffic congestion.\"\n\nIn February 2010, a 61-year-old woman tried to pass off a life-sized mannequin as a passenger in order to use the HOV lane in New York State. A police officer on a routine HOV patrol became suspicious when he noticed that the so-called passenger was wearing sunglasses and using the visor on a cloudy morning. When the officer approached the vehicle, he discovered that the \"passenger\" was, in fact, a mannequin wearing lipstick, designer shades, a full-length wig, and a blue sweater. The driver was issued a traffic ticket for using the HOV lane without a human passenger, which carries a fine of $135 and two points on a driver's license.\n\nIn early 2006, an Arizona woman asserted that she had been improperly ticketed for using the HOV lane because the unborn child she was carrying in her womb justified her use of the lane, while noting that Arizona traffic laws do not define what constitutes a person. However, a judge subsequently ruled that to qualify as an \"individual\" under Arizona traffic laws, the individual must occupy a \"separate and distinct\" space in a vehicle. Likewise, in California, in order to use HOV lanes, there must be two (or, if posted, three) separate individuals occupying seats in a vehicle, and an unborn child does not count towards this requirement.\n\nIn 2009 and 2010 it was found that non-compliance rates on HOV lanes in Brisbane, Australia, were approaching 90%. Enhanced enforcement led to increased compliance, average bus journey times dropped by up to 19%, and total person throughput increased by 12%. In 2006 it was claimed that many vehicles had only one occupant.\n\nAccording to 2009 data from the U.S. census, 76% drive to work alone and only 10% rideshare; for suburban commuters working in a city, the solo driving rate is 82%.\n\nSome underused HOV lanes in several states have been converted to high-occupancy toll lanes (HOT), which offer solo drivers access to HOV lanes after paying a toll.\n\nHOV lanes are also an effective way to manage traffic after natural disasters, as seen in New York City after Hurricane Sandy in October 2012. At the time Mayor Bloomberg banned passenger cars with fewer than three occupants from entering Manhattan. The restriction affected all bridges and tunnels entering the city except the George Washington Bridge.\n\nCritics have argued that HOV lanes are underused. It is unclear whether HOV lanes are sufficiently used to compensate for delays in the other mixed-use lanes.\n\nLas Vegas, Nevada has been considered a city where its HOV lanes (located on US-95) are considered useless and underused due to it being far from a \"nine-to-five\" city, but were built because federal funding was earmarked for the project, and further criticism was made during the 2009 recession in the building of the HOV flyover exit to and from Summerlin Parkway, a freeway that still does not have HOV lanes.\n\nThe situations have caused social problems in Indonesia, where some people become \"car jockey\", people who make their living by offering drivers to fill their car in order to meet the occupancy limit. Reportedly the situation caused people stay in unemployment for doing so, increased congestion and let parents profit from their babies.\n\nIn 1995, six people died in a head-on collision when the gates of a reversible lane were not set correctly on the Interstate 279 in Pittsburgh, Pennsylvania.\n\n\n"}
{"id": "46186906", "url": "https://en.wikipedia.org/wiki?curid=46186906", "title": "Infrastructure and economics", "text": "Infrastructure and economics\n\nThis article delineates the relationship between infrastructure and various economic issues.\n\nInfrastructure may be owned and managed by governments or by private companies, such as sole public utility or railway companies. Generally, most roads, major ports and airports, water distribution systems and sewage networks are publicly owned, whereas most energy and telecommunications networks are privately owned. Publicly owned infrastructure may be paid for from taxes, tolls, or metered user fees, whereas private infrastructure is generally paid for by metered user fees. Major investment projects are generally financed by the issuance of long-term bonds.\n\nHence, government owned and operated infrastructure may be developed and operated in the private sector or in public-private partnerships, in addition to in the public sector. In the United States, public spending on infrastructure has varied between 2.3% and 3.6% of GDP since 1950. Many financial institutions invest in infrastructure.\n\nInfrastructure debt is a complex investment category reserved for highly sophisticated institutional investors who can gauge jurisdiction-specific risk parameters, assess a project’s long-term viability, understand transaction risks, conduct due diligence, negotiate (multi)creditors’ agreements, make timely decisions on consents and waivers, and analyze loan performance over time.\n\nResearch conducted by the suggests that most UK and European pensions wishing to gain a degree of exposure to infrastructure debt have done so indirectly, through investments made in infrastructure funds managed by specialised Canadian, US and Australian funds.\n\nOn November 29, 2011, the British government unveiled an unprecedented plan to encourage large-scale pension investments in new roads, hospitals, airports, etc. across the UK. The plan is aimed at enticing 20 billion pounds ($30.97 billion) of investment in domestic infrastructure projects.\n\nPension and sovereign wealth funds are major direct investors in infrastructure. Most pension funds have long-dated liabilities, with matching long-term investments. These large institutional investors need to protect the long-term value of their investments from inflationary debasement of currency and market fluctuations, and provide recurrent cash flows to pay for retiree benefits in the short-medium term: from that perspective, think-tanks such as the have argued that infrastructure is an ideal asset class that provides tangible advantages such as long duration (facilitating cash flow matching with long-term liabilities), protection against inflation and statistical diversification (low correlation with ‘traditional’ listed assets such as equity and fixed income investments), thus reducing overall portfolio volatility. Furthermore, in order to facilitate the investment of institutional investors in developing countries' infrastructure markets, it is necessary to design risk-allocation mechanisms more carefully, given the higher risks of developing countries' markets.\n\nThe notion of supranational and public co-investment in infrastructure projects jointly with private institutional asset owners has gained traction amongst IMF, World Bank and European Commission policy makers in recent years notably in the last months of 2014/early 2015: Annual Meetings of the International Monetary Fund and the World Bank Group (October 2014) and adoption of the €315 bn European Commission Investment Plan for Europe (December 2014).\n\nSome experts have warned against the risk of \"infrastructure nationalism\", insisting that steady investment flows from foreign pension and sovereign funds were key for the long-term success of the asset class- notably in large European jurisdictions such as France and the UK \n\nAn interesting comparison between privatisation versus government-sponsored public works involves high-speed rail (HSR) projects in East Asia. In 1998, the Taiwan government awarded the Taiwan High Speed Rail Corporation, a private organisation, to construct the 345 km line from Taipei to Kaohsiung in a 35-year concession contract. Conversely, in 2004 the South Korean government charged the Korean High Speed Rail Construction Authority, a public entity, to construct its high-speed rail line, 412 km from Seoul to Busan, in two phases. While different implementation strategies, Taiwan successfully delivered the HSR project in terms of project management (time, cost, and quality), whereas South Korea successfully delivered its HSR project in terms of product success (meeting owners' and users' needs, particularly in ridership). Additionally, South Korea successfully created a technology transfer of high-speed rail technology from French engineers, essentially creating an industry of HSR manufacturing capable of exporting knowledge, equipment, and parts worldwide.\n\nThe method of \"infrastructure asset management\" is based upon the definition of a Standard of service (SoS) that describes how an asset will perform in objective and measurable terms. The SoS includes the definition of a minimum condition grade, which is established by considering the consequences of a failure of the infrastructure asset.\n\nThe key components of infrastructure asset management are:\n\nAfter completing asset management, official conclusions are made. The American Society of Civil Engineers gave the United States a \"D+\" on its 2017 infrastructure report card.\n\nMost infrastructure is designed by civil engineers or architects. Generally road and rail transport networks, as well as water and waste management infrastructure are designed by civil engineers, electrical power and lighting networks are designed by power engineers and electrical engineers, and telecommunications, computing and monitoring networks are designed by systems engineers.\n\nIn the case of urban infrastructure, the general layout of roads, sidewalks and public places may sometimes be developed at a conceptual level by urban planners or architects, although the detailed design will still be performed by civil engineers. Depending upon the height of the building, it may be designed by an architect or for tall buildings,a structural engineer, and if an industrial or processing plant is required, the structures and foundation work will still be done by civil engineers, but the process equipment and piping may be designed by industrial engineer or a process engineer.\n\nIn terms of engineering tasks, the design and construction management process usually follows these steps:\n\nIn general, infrastructure is planned by urban planners or civil engineers at a high level for transportation, water/waste water, electrical, urban zones, parks and other public and private systems. These plans typically analyze policy decisions and impacts of trade offs for alternatives. In addition, planners may lead or assist with environmental review that are commonly required to construct infrastructure. Colloquially this process is referred to as Infrastructure Planning. These activities are usually performed in preparation for preliminary engineering or conceptual design that is led by civil engineers or architects.\n\nPreliminary studies may also be performed and may include steps such as:\n\n\n\n\n\nFile:BBI 2010-07-23 5.JPG|thumb|right|The Berlin Brandenburg Airport under construction.\n\nInvestment in infrastructure is part of the capital accumulation required for economic development and may affect socioeconomic measures of welfare. The causality of infrastructure and economic growth has always been in debate. Generally, infrastructure plays a critical role in expanding national production capacity, which leads to increase in a country's wealth. In developing nations, expansions in electric grids, roadways, and railways show marked growth in economic development. However, the relationship does not remain in advanced nations who witness more and more lower rates of return on such infrastructure investments.\n\nNevertheless, infrastructure yields indirect benefits through the supply chain, land values, small business growth, consumer sales, and social benefits of community development and access to opportunity. The American Society of Civil Engineers cite the many transformative projects that have shaped the growth of the United States including the Transcontinental Railroad that connected major cities from the Atlantic to Pacific coast; the Panama Canal that revolutionised shipment in connected the two oceans in the Western hemisphere; the Interstate Highway System that spawned the mobility of the masses; and still others that include the Hoover Dam, Trans-Alaskan pipeline, and many bridges (the Golden Gate, Brooklyn, and San Francisco–Oakland Bay Bridge). All these efforts are testimony to the infrastructure and economic development correlation.\n\nEuropean and Asian development economists have also argued that the existence of modern rail infrastructure is a significant indicator of a country’s economic advancement: this perspective is illustrated notably through the Basic Rail Transportation Infrastructure Index (known as BRTI Index) \n\nDuring the Great Depression of the 1930s, many governments undertook public works projects in order to create jobs and stimulate the economy. The economist John Maynard Keynes provided a theoretical justification for this policy in \"The General Theory of Employment, Interest and Money\", published in 1936. Following the global financial crisis of 2008–2009, some again proposed investing in infrastructure as a means of stimulating the economy (see the American Recovery and Reinvestment Act of 2009).\n\nWhile infrastructure development may initially be damaging to the natural environment, justifying the need to assess environmental impacts, it may contribute in mitigating the \"perfect storm\" of environmental and energy sustainability, particularly in the role transportation plays in modern society. Offshore wind power in England and Denmark may cause issues to local ecosystems but are incubators to clean energy technology for the surrounding regions. Ethanol production may overuse available farmland in Brazil but have propelled the country to energy independence. High-speed rail may cause noise and wide swathes of rights-of-way through countrysides and urban communities but have helped China, Spain, France, Germany, Japan, and other nations deal with concurrent issues of economic competitiveness, climate change, energy use, and built environment sustainability.\n\n\n"}
{"id": "3799057", "url": "https://en.wikipedia.org/wiki?curid=3799057", "title": "KO PROPO", "text": "KO PROPO\n\nKO PROPO is a brand of radio control equipment and humanoid robot (KHR-1) by Kondo Kagaku, established in Tokyo, Japan in 1945.\n\nIn 1982 KO PROPO introduced the Expert EX-1, reported to have been the first to integrate a pistol grip into a transmitter device with a gun trigger to act as the throttle, which later became a popular fixture in radio controlled transmitters.\n\n"}
{"id": "21227568", "url": "https://en.wikipedia.org/wiki?curid=21227568", "title": "LiquidHD", "text": "LiquidHD\n\nLiquidHD technology was an architecture and a set of protocols for networking consumer electronics devices. It was designed to let consumers link their HDTVs, home theater components, PCs, gaming consoles, and mobile devices into local entertainment networks, where they could view high-definition digital content from any networked source device on any compliant display.\nSilicon Image promoted LiquidHD in 2009 and sponsored a website for it that year.\n\nThe LiquidHD architecture allowed network connections to be made via Ethernet, coaxial cable (i.e., MoCA or G.hn), 802.11 wireless, powerline communication (such as G.hn or other specifications such as HomePlug, HD-PLC or Universal Powerline Association), phone lines (HomePNA or G.hn) or HDMI, depending on the usage scenario and what connections were already available. Once devices were linked in a LiquidHD network, users could control them via a remote user interface, enabling sharing of source devices and facilitating activities such as pausing a program in one room and resuming it in another.\n\nLiquidHD included protocols for:\n\n\nIt was announced January 8, 2009 at the Consumer Electronics Show in Las Vegas.\nBy September 2009 analysts observed that no major customers had adopted the technology.\nAfter announcing declining sales in the Great Recession, the company replaced its chief executive in September 2009 as it was criticized for spreading itself over too many new initiatives.\nSilicon Image mentioned the technology on its web site in 2010, but quietly dropped it in early 2011 under new chief executive Camillo Martino.\n\nDigital Living Network Alliance\n\n"}
{"id": "5127900", "url": "https://en.wikipedia.org/wiki?curid=5127900", "title": "MSXML", "text": "MSXML\n\nMicrosoft XML Core Services (MSXML), now legacy, was a set of services that allowed applications written in JScript, VBScript, and Microsoft development tools to build Windows-native XML-based applications. It supports XML 1.0, DOM, SAX, an XSLT 1.0 processor, XML schema support including XSD and XDR, as well as other XML-related technologies.\n\nAll MSXML products are similar in that they are exposed programmatically as OLE Automation (a subset of COM) components. Developers can program against MSXML components from C, C++ or from Active Scripting languages such as JScript and VBScript. Managed .NET Interop with MSXML COM components is not supported nor recommended.\n\nAs with all COM components, an MSXML object is programmatically instantiated by CLSID or ProgID. Each version of MSXML exposes its own set of CLSID's and ProgIDs. For example, to create an MSXML 6.0 DOMDocument object, which exposes the codice_1, codice_2, and codice_3 COM interfaces, the ProgID \"MSXML2.DOMDocument.6.0\" must be used.\n\nMSXML also supports version-independent ProgIDs. Version-independent ProgIDs do not have a version number associated with them. For example, \"Microsoft.XMLHTTP\". These ProgIDs were first introduced in MSXML 1.0, however are currently mapped to MSXML 3.0 objects and the msxml3.dll.\n\nDifferent versions of MSXML support slightly different sets of functionality. For example, while MSXML 3.0 supports only XDR schemas, it does not support XSD schemas. MSXML 4.0, MSXML 5.0, and MSXML 6.0 support XSD schemas. However, MSXML 6.0 does not support XDR schemas. Support for XML Digital Signatures is provided only by MSXML 5.0. For new XML-related software development, Microsoft recommends using MSXML 6.0 or its lightweight cousin, \"XmlLite\", for native code-only projects.\n\nMSXML is a collection of distinct products, released and supported by Microsoft. The product versions can be enumerated as follows: More information on each version is also available at Microsoft Downloads website.\n\n\n\n\n"}
{"id": "1190694", "url": "https://en.wikipedia.org/wiki?curid=1190694", "title": "Ministry of Information (United Kingdom)", "text": "Ministry of Information (United Kingdom)\n\nThe Ministry of Information (MOI), headed by the Minister of Information, was a United Kingdom government department created briefly at the end of the First World War and again during the Second World War. Located in Senate House at the University of London during the 1940s, it was the central government department responsible for publicity and propaganda.\n\nIn the Great War, several different agencies had been responsible for propaganda, except for a brief period when there had been a Department of Information (1917) and a Ministry of Information (1918).\n\nColour key (for political parties):\n\nThe Ministry of Information (MOI) was formed on 4 September 1939, the day after Britain's declaration of war, and the first Minister was sworn into Office on 5 September 1939.\n\nThe Ministry's function was \"To promote the national case to the public at home and abroad in time of war\" by issuing \"National Propaganda\" and controlling news and information. It was initially responsible for censorship, issuing official news, home publicity and overseas publicity in Allied and neutral countries. These functions were matched by a responsibility for monitoring public opinion through a network of Regional Information Offices. Responsibility for publicity in enemy territories was organised by Department EH (later part of the Special Operations Executive).\n\nSecret planning for a Ministry of Information (MOI) had started in October 1935 under the auspices of the Committee of Imperial Defence (CID). Draft proposals were accepted on 27 July 1936 and Sir Stephen Tallents was appointed as Director General Designate. Tallents drew together a small group of planners from existing government departments, public bodies and specialist outside organisations.\n\nThe MOI's planners sought to combine experience gained during the First World War with new communications technology. Their work reflected an increasing concern that a future war would exert huge strain on the civilian population and a belief that government propaganda would be needed to maintain morale. However it was hindered by competing visions for the Ministry, a requirement for secrecy which disrupted the making of key appointments, and the reluctance of many government departments to give up their public relations divisions to central control.\n\nThe shadow Ministry of Information came into being briefly between 26 September and 3 October 1938 after the Nazi annexation of the Sudetenland (German occupation of Czechoslovakia) heightened international tensions. The seventy one officials who were assembled in temporary accommodation had responsibility for censoring press reports surrounding the Munich Agreement.\n\nThe week-long experiment was not regarded as a success. Instead it highlighted the extent to which questions over appointments, accommodation, links to the media and the relationship with other government departments had been left unresolved. The confusion had been made worse by tensions between the shadow MOI and the Foreign Office News Department. This tension spilled over into the Committee for Imperial Defence which considered proposals to abandon plans for the Ministry of Information.\n\nTallents left his post as Director General Designate on 2 January 1939 (with responsibility for the MOI assumed by the Home Office). Planning efforts would increase again after Nazi troops moved into Prague on 15 March 1939 (German occupation of Czechoslovakia). The British Prime Minister Neville Chamberlain publicly announced his government's intentions for the MOI in a parliamentary speech on 15 June 1939. Home Office officials were privately given until 31 December 1939 to complete their plans.\n\nThe Ministry of Information (MOI) was formed on 4 September 1939, the day after Britain's declaration of war, and Lord Macmillan was sworn in as its first Minister on 5 September 1939. The MOI's headquarters were housed within the University of London's Senate House and would remain in place until the end of the war.\n\nThe MOI was initially organised in four groups. A \"Press Relations\" group was responsible for both the issue of news and censorship. A \"Publicity Users\" group (split into \"foreign\" and \"home\" sections) was responsible for propaganda policy. A \"Publicity Producers\" group (split according to media) was responsible for design and production. These were overseen by a \"Co-ordination and Intelligence\" group responsible for administration. This structure had only been finalised in May–June 1940 and senior officials were often unsure about their responsibilities.\n\nThe press reacted negatively to the MOI. Initial confusion between the MOI and service departments led to accusations that the MOI was delaying access to the news, and a newspaper campaign against censorship was started. Other commentators pointed to the ministry's large staff and satirised it as ineffective and out of touch. The MOI's first publicity campaign also misfired with a poster bearing the message \"Your Courage, Your Cheerfulness, Your Resolution, Will Bring Us Victory\" criticised for appearing class-bound. These factors led to political scrutiny and resulted in the removal of the Press Relations Group on 9 October 1939 (which became an autonomous Press and Censorship Bureau) and an announcement on 25 October 1939 that the MOI's staff was to be cut by a third.\n\nLord Macmillan was replaced as Minister by Sir John Reith on 5 January 1940. Reith sought to improve the MOI's governance, expanded its network of Regional Information Offices and introduced a Home intelligence division. He also sought to secure the reintegration of the Press and Censorship Division in the belief that the decision to separate this function had been \"obviously and monstrously ridiculous and wrong\". These changes were announced by Neville Chamberlain on 24 April 1940 but were not fully operational until June 1940.\n\nNeville Chamberlain's resignation and replacement by Winston Churchill on 10 May 1940 resulted in Reith's sacking and the appointment of Duff Cooper. Cooper was closely allied to Churchill and had openly criticised the previous government's approach.\n\nNazi advances in Western Europe encouraged the Ministry of Information (MOI) to increasingly focus on domestic propaganda after May 1940. A Home Publicity Emergency Committee was set up to issue public instructions about air raids, parachute raids and what to do in the event of an invasion. Concurrent campaigns extolled the public to \"Avoid Rumour\" and join a \"Silent Column\" so that they would avoid passing information to spies. However, as the situation stabilised, these campaigns were increasingly criticised for treating the public with contempt.\n\nThe MOI was subject to further criticism in July 1940 when it was accused of using \"Gestapo\" tactics to spy on the British public. Popular newspapers seized on reports about MOI-sponsored opinion polling and denounced the department's staff as \"Cooper's Snoopers\". Home intelligence reports recorded complaints from those who felt that the British state was \"becoming dangerously akin to the one we are fighting\". The cost of such activities was also queried in Parliament.\n\nCooper was forced to confront further questions regarding the news and was dogged by accusations that he wanted to enforce a system of compulsory censorship. His failed attempt to increase the MOI's control over public relations divisions in other departments also drew him into conflict with Churchill. After two months of internal wrangling, it was reported that Cooper was to be replaced as Minister by Brendan Bracken on 17 July 1941.\n\nThe Ministry of Information (MOI) settled down during Brendan Bracken's tenure. Supported by Winston Churchill and the press, Bracken remained in office until after Victory in Europe Day.\n\nOne of Bracken's main achievements was to promote a closer working relationship with other government departments. Plans to increase control over public relations divisions in other departments were duly dropped and closer co-operation sought in their place. Bracken's experience as a newspaper proprietor encouraged a similar approach with regard to the press and he maintained that the MOI should not impinge upon the right to free speech.\n\nBracken was keen to promote a more limited role in the field of domestic propaganda. He insisted that the MOI should stop \"lecturing\" the public and publicly questioned its ability to \"stimulate British Morale\". The MOI was instead encouraged to provide background information to particular events whilst providing technical support to other departments. Bracken also insisted that the MOI should be dissolved at the end of the war with Germany and that its activities made it inappropriate to peacetime conditions.\n\nThe MOI was dissolved in March 1946, with its residual functions passing to the Central Office of Information (COI), a central organisation providing common and specialist information services.\n\nCampaigns carried out included themes such as the following:\n\nFor home publicity, the Ministry dealt with the planning of general government or interdepartmental information, and provided common services for public relations activities of other government departments. The Home Publicity division undertook three main types of campaigns: those requested by other government departments, specific regional campaigns, and those it initiated itself. Before undertaking a campaign, the MOI would ensure that propaganda was not being used as a substitute for other activities, including legislation.\n\nThe General Production Division (GPD), one of the few divisions to remain in place throughout the war, undertook technical work under Edwin Embleton. The GPD often produced work in as little as a week or a fortnight, when normal commercial practice was three months. Artists were not in a reserved occupation and were liable for call up for military service along with everyone else. Many were recalled from the services to work for the Ministry in 1942, a year in which £4 million was spent on publicity, approximately a third more than in 1941. £120,000 of this was spent on posters, art and exhibitions. Of the many officially employed war artists, several, including Eric Kennington, Paul Nash and William Rothenstein, were war artists during both World Wars. Many extra designs were prepared to cope with short lead-times and the changing events of war. Through the Home intelligence Division, the MOI collected reactions to general wartime morale and, in some cases, specifically to publicity produced.\n\nDylan Thomas, frustrated at being declared unfit to join the armed forces, contacted Sir Kenneth Clark, director of the films division of the Ministry of Information, and offered his services. Although not directly employed by the MOI, he scripted at least five films in 1942 with titles such as \"This Is Colour\" (about dye); \"New Towns for Old\"; \"These Are the Men\"; \"Our Country\" (a sentimental tour of Britain), and \"The Art of Conversation\".\n\nColour key (for political parties):\n\nA major 4 year Arts and Humanities Research Council (AHRC) funded research project on the \"Communications History of the Ministry of Information\" began in January 2014. The project is led by Professor Simon Eliot of the School of Advanced Study at the University of London and Paul Vetch of the Department of Digital Humanities at King's College London. A project website called MOI Digital was launched on 24 June 2014.\n\nHenry Irving and Judith Townend have drawn parallels between information censorship in Britain during World War II and contemporary restrictions in reporting trials that relate to terrorism offences, most notably the case of \"R v Incedal and Rarmoul-Bouhadjar\" (2014).\n\n"}
{"id": "28931066", "url": "https://en.wikipedia.org/wiki?curid=28931066", "title": "National Agency for Computer Security", "text": "National Agency for Computer Security\n\nThe National Agency for Computer Security is the Tunisian national computer security agency. It was founded in 2004 and it is based in Tunis, Tunisia. Its Director General is Ali GHRIB.\n"}
{"id": "11438281", "url": "https://en.wikipedia.org/wiki?curid=11438281", "title": "Optical rectification", "text": "Optical rectification\n\nElectro-optic rectification (EOR), also referred to as optical rectification, is a non-linear optical process that consists of the generation of a quasi-DC polarization in a non-linear medium at the passage of an intense optical beam. For typical intensities, optical rectification is a second-order phenomenon which is based on the inverse process of the electro-optic effect. It was reported for the first time in 1962, when radiation from a ruby laser was transmitted through potassium dihydrogen phosphate (KDP) and potassium dideuterium phosphate (KDP) crystals.\n\nOptical rectification can be intuitively explained in terms of the symmetry properties of the non-linear medium: in the presence of a preferred internal direction, the polarization will not reverse its sign at the same time as the driving field. If the latter is represented by a sinusoidal wave, then an average DC polarization will be generated.\n\nOptical rectification is analogous to the electric rectification effect produced by diodes, wherein an AC signal can be converted (\"rectified\") to DC. However, it is \"not\" the same thing. A diode can turn a sinusoidal electric field into a DC current, while optical rectification can turn a sinusoidal electric field into a DC polarization, but not a DC current. On the other hand, a \"changing\" polarization is a kind of current. Therefore, if the incident light is getting more and more intense, optical rectification causes a DC current, while if the light is getting less and less intense, optical rectification causes a DC current in the opposite direction. But again, if the light intensity is constant, optical rectification cannot cause a DC current.\n\nWhen the applied electric field is delivered by a femtosecond-pulse-width laser, the spectral bandwidth associated with such short pulses is very large. The mixing of different frequency components produces a beating polarization, which results in the emission of electromagnetic waves in the terahertz region. The EOR effect is somewhat similar to a classical electrodynamic emission of radiation by an accelerating/decelerating charge, except that here the charges are in a bound dipole form and the THz generation depends on the second order susceptibility of the nonlinear optical medium. A popular material for generating radiation in the 0.5–3 THz range (1 mm wavelength) is zinc telluride.\n\nOptical rectification also occurs on metal surfaces by similar effect as surface second harmonic generation. The effect is however influenced e. g. by nonequilibrium electron excitation and generally it manifests in a more complicated way.\n\nSimilar to other nonlinear optical processes, optical rectification is also reported to become enhanced when surface plasmons are excited on a metal surface.\n\nTogether with carrier acceleration in semiconductors and polymers, optical rectification is one of the main mechanisms for the generation of terahertz radiation using lasers. This is different from other processes of terahertz generation such as polaritonics where a polar lattice vibration is thought to generate the terahertz radiation.\n\n"}
{"id": "22458313", "url": "https://en.wikipedia.org/wiki?curid=22458313", "title": "Philosophy of computer science", "text": "Philosophy of computer science\n\nThe philosophy of computer science is concerned with the philosophical questions that arise with the study of computer science, which is understood to mean not just programming but the whole study of concepts and methods that assist in the development and maintenance of computer systems. There is still no common understanding of the content, aim, focus, or topic of the philosophy of computer science, despite some attempts to develop a philosophy of computer science like the philosophy of physics or the philosophy of mathematics.\n\nThe philosophy of computer science as such deals with the meta-activity that is associated with the development of the concepts and methodologies that implement and analyze the computational systems.\n\n\n\n"}
{"id": "2036050", "url": "https://en.wikipedia.org/wiki?curid=2036050", "title": "Professional Lighting Designers' Association", "text": "Professional Lighting Designers' Association\n\nThe Professional Lighting Designers' Association (PLDA), formerly known as European Lighting Designers' Association (ELDA+), is an international association of lighting designers whose work primarily involves them in architectural lighting design. Based in Gütersloh, Germany, the organisation was originally created as a European entity. However, since its founding in 1994, the organisation has expanded beyond the boundaries of Europe and has many members outside of Europe in Asia and North America. In the year 2014, PLDA was liquidated as a result of insolvency.\n\nThe Professional Lighting Designers' Association PLDA was founded as the European Lighting Designers' Association ELDA+ in 1994 in Frankfurt/D.\nThe founders were\n\nPLDA is a voluntary federation of lighting designers and lighting consultants who are active on an international scale. Their purpose is to increase the reputation of the profession and to establish the profession as such in its own right.\n\nThere are different membership categories which one can apply to - membership fees are organized accordingly.\n\nVoting members are Fellow members, Professional members and Associate members. The Fellow members are members who are deemed to have excelled in contributing to the Architectural Lighting Design profession and to PLDA. Professional members are those who own an independent practice or work in an independent practice, and have a certain number of years of experience in lighting design. The same applies to Associate members, except their number of years' experience is less.\n\nNon-voting members are Design members (independent, but with little or no experience as an independent designer), Student members and Affiliate members.\nAffiliate members are those who are not full-time lighting designers, but are very interested in lighting design or need it for their profession.\n\nPLDA collaborates with other lighting associations, including CIE, IES and other groupings of lighting designers around the world.\nWith the Institution of Lighting Engineers, PLDA has a partnership agreement, in order to foster deeper understanding between the lighting profession and engineering practitioners.\n\nThe official magazine of the organization is the Professional Lighting Design, which is published by the VIA-Verlag. Every member receives the magazine automatically. The VIA-Verlag maintains very close ties with PLDA by, for example, organizing many events for the association.\n\nThe Professional Lighting Designers' Association believes that education is the key to establishing Lighting Design as a serious profession. The association thus dedicates a large portion of its time to the organisation and support of education programmes at university level. PLDA has an Educators' network. International educators meet on a regular basis. PLDA works closely with many universities around the world, for example at KTH in Stockholm/S, the Hochschule Wismar/D, Fachhochschule Hildesheim/D, Parson's New School of Design in NYC/USA and Bachelor and Masters programmes in many European countries.\n\nThe Workshop Programme with up to four major workshops a year gives students and young designers the opportunity to work in teams under the guidance of a professional lighting designer to develop and implement a lighting design concept on a real project. Some universities have these workshops in their curriculum.\n\nThe Professional Lighting Designers' Association stages the Light Focus conference once a year, usually in conjunction with a major lighting fair event, e.g. Light+Building in Frankfurt/D or Euroluce in Milan/I. The sessions cover topics of interest and relevance to all those involved in architectural lighting design and bring to light a variety of professional themes.\n\nThe Professional Lighting Designers' Association stages the Vox Juventa conference for young designers once a year. The winning speaker receives a money prize.\n\nPLDA, hosts a number of practical workshops, which are organized by the event management of VIA publishing, in which various lighting projects are carried out in areas of a selected town or city. In this way, the general public is educated and shown what lighting design means and that it can help to improve the urban nightscape.\n\nThe town of Alingsås, Sweden is the most regular partner. In 2009 PLDA and Alingsås celebrated their tenth year of working together.\n\nOthers have been carried out in Jyväskylä/FIN, Winterthur/CH, Birmingham/UK, Lüdenscheid/D, Liverpool/UK or Stavanger/N and Notodden/N.\n\nPolicy making for PLDA is handled the association's elected council:\n\n\n\n"}
{"id": "4898962", "url": "https://en.wikipedia.org/wiki?curid=4898962", "title": "Quantum point contact", "text": "Quantum point contact\n\nA quantum point contact (QPC) is a narrow constriction between two wide electrically conducting regions, of a width comparable to the electronic wavelength (nano- to micrometer). Quantum point contacts were first reported in 1988 by a Dutch group (Van Wees \"et al.\" ) and, independently, by a British group (Wharam \"et al.\" ). They are based on earlier work by the British group which showed how split gates could be used to convert a two-dimensional electron gas into one-dimension, first in silicon (Dean and Pepper ) and then in gallium arsenide (Thornton \"et al.\", Berggren \"et al.\" )\n\nThere are several different ways of fabricating a QPC. It can be realized in a break-junction by pulling apart a piece of conductor until it breaks. The breaking point forms the point contact. In a more controlled way, quantum point contacts are formed in a two-dimensional electron gas (2DEG), e.g. in GaAs/AlGaAs heterostructures. By applying a voltage to suitably shaped gate electrodes, the electron gas can be locally depleted and many different types of conducting regions can be created in the plane of the 2DEG, among them quantum dots and quantum point contacts. Another means of creating a QPC is by positioning the tip of a scanning tunneling microscope close to the surface of a conductor.\n\nGeometrically, a quantum point contact is a constriction in the transverse direction which presents a resistance to the motion of electrons. Applying a voltage formula_1 across the point contact induces a current to flow, the magnitude of this current is given by formula_2, where formula_3 is the conductance of the contact. This formula resembles Ohm's law for macroscopic resistors. However, there is a fundamental difference here resulting from the small system size which requires a quantum mechanical analysis.\n\nAt low temperatures and voltages, unscattered and untrapped electrons contributing to the current have a certain energy/momentum/wavelength called Fermi energy/momentum/wavelength. Much like in a waveguide, the transverse confinement in the quantum point contact results in a \"quantization\" of the transverse motion—the transverse motion cannot vary continuously, but has to be one of a series of discrete modes. The waveguide analogy is applicable as long as coherence is not lost through scattering, e.g., by a defect or trapping site. The electron wave can only pass through the constriction if it interferes constructively, which for a given width of constriction, only happens for a certain number of modes formula_4. The current carried by such a quantum state is the product of the velocity times the electron density. These two quantities by themselves differ from one mode to the other, but their product is mode independent. As a consequence, each state contributes the same amount formula_5 per spin direction to the total conductance formula_6.\n\nThis is a fundamental result; the conductance does not take on arbitrary values but is quantized in multiples of the conductance quantum formula_7, which is expressed through the electron charge formula_8 and the Planck constant formula_9. The integer number formula_4 is determined by the width of the point contact and roughly equals the width divided by half the electron wavelength. As a function of the width of the point contact (or gate voltage in the case of GaAs/AlGaAs heterostructure devices), the conductance shows a staircase behavior as more and more modes (or channels) contribute to the electron transport. The step-height is given by formula_11.\n\nAn external magnetic field applied to the quantum point contact lifts the spin degeneracy and leads to half-integer steps in the conductance. In addition, the number formula_4 of modes that contribute becomes smaller. For large magnetic fields, formula_4 is independent of the width of the constriction, given by the theory of the quantum Hall effect. An interesting feature, not yet fully understood, is a plateau at formula_14, the so-called 0.7-structure.\n\nApart from studying fundamentals of charge transport in mesoscopic conductors, quantum point contacts can be used as extremely sensitive charge detectors. Since the conductance through the contact strongly depends on the size of the constriction, any potential fluctuation (for instance, created by other electrons) in the vicinity will influence the current through the QPC. It is \npossible to detect single electrons with such a scheme. In view of quantum computation in solid-state systems, QPCs can be used as readout devices for the state of a quantum bit (qubit) \n\n"}
{"id": "53816446", "url": "https://en.wikipedia.org/wiki?curid=53816446", "title": "Rebecca Bace", "text": "Rebecca Bace\n\nRebecca \"Becky\" Gurley Bace (1955 - 2017) was an American computer security expert and pioneer in intrusion detection. She spent 12 years at the US National Security Agency where she created the Computer Misuse and Anomaly Detection\n(CMAD) research program. She was known as the \"den mother of computer security\". She was also influential in the early stages of intelligence community venture capital and was a major player in Silicon Valley investments in cyber security technology.\n\nBace grew up in rural Alabama as one of seven children and was diagnosed with epilepsy in adolescence. Her mother was a war bride from Japan following World War II and her father was a self-educated teamster from Alabama. Due to prevailing attitudes about the illness and about women, her neurologist suggested that she stay home and collect disability following high school. She credited a local librarian and family friend, Bertha Nel Allen, for the encouragement to apply for college and scholarships. She won scholarships from charitable foundations set up by Betty Crocker and Jimmy Hoffa in her senior year of high school, and in 1973 she was accepted to the University of Alabama at Birmingham as the only woman in engineering. Because of financial hardship and frequent employment interruptions, she took eight years of classes at various schools to earn her degree. Bace first became interested in computing during her freshman year working with punch cards programming Fortran and COBOL on an IBM mainframe and got her first engineering job while teaching at an engineering lab. She was approached by a couple of Xerox technicians who needed to fill affirmative action requirements, and accepted a job as a specialist repairing copier machines. Of the experience she stated that she faced significant gender and racial bias, and that \"sometimes customers would raise a ruckus for having to deal with [her] because they believed they had been given \"second best\" when [she] showed up, even though [she] was better educated than most of the men.\"\n\nAfter graduation in 1984, Bace started working at the NSA, and while searching for a flexible job to allow her to care for her autistic son who was later diagnosed with leukemia, she took an assignment in 1989 in the National Computer Security Center. The NCSC was chartered as part of the NSA expressly to deal with computer security issues for the Department of Defense and the intelligence community. Bace served as program manager for intrusion detection research, specifically on transferring research into the relatively new commercial security products market. She played a pivotal role in the apprehension of Kevin Mitnick, proving that trace back and capture were possible beyond the theoretical context. She also provided some of the seed funding for computer security labs at UC Davis and Purdue University.\n\nFollowing the death of her son, Bace went to serve as the deputy security officer at Los Alamos National Laboratory in the Computing, Information and Communications Division. She left Los Alamos in 1998 and started Infidel, Inc., a security consulting company. In 2002, she signed on as a venture capital consultant at Trident Capital in Silicon Valley. Bace also briefly served as Technical VP of the Cyber Security Practice for In-Q-Tel, the investment arm of the US Intelligence Community, and before her death she served as chief strategist for the Center for Forensics, Information Technology, and Security (CFITS) at the University of South Alabama. As a venture capitalist, she provided expert advice to a generation of security startups, including Qualys, Sygate, iRobot, Arxan Technologies, HyTrust, and Neohapsis.\n\n\n"}
{"id": "24020232", "url": "https://en.wikipedia.org/wiki?curid=24020232", "title": "Revenue Technology Services", "text": "Revenue Technology Services\n\nRevenue Technology Solutions began as a division Control Data Corporation. It developed a yield management system for Republic Airlines on a mainframe in 1982. Revenue Technology Services Corporation was spun off as an independent company through a purchase by YMS, Inc. in 1991.\n\nRevenue Technology Services provides global revenue management and profit optimization software and consulting services. The verticals supported are airlines, cargo, coach, cruise/ferry lines, and railroads. Delivered products include revenue management for passenger and cargo, pricing management and business intelligence / analytics.\n"}
{"id": "32551529", "url": "https://en.wikipedia.org/wiki?curid=32551529", "title": "Samuel Eddy Barrett", "text": "Samuel Eddy Barrett\n\nSamuel Eddy Barrett (1834–1912) was a Chicago industrialist and Major in the American Civil War. He was born in 1834 in Boston, Massachusetts. He moved to Chicago, Illinois and in 1857 founded the manufacturing firm Barrett, Powell & Arnold, which later became S.E. Barrett Manufacturing Company. He volunteered in the 1st Illinois Artillery Regiment, commanding its Company B. He was promoted to Major in 1863. Barrett married Alice D. Brush in 1868 and had six children. He worked with S.E. Barrett Manufacturing Company (later Barrett Manufacturing Company) until his death, while simultaneously acting as the head of the American Coal Products Company starting in 1903. Barrett died in 1912.\n\n"}
{"id": "24694516", "url": "https://en.wikipedia.org/wiki?curid=24694516", "title": "Serial Port Memory Technology", "text": "Serial Port Memory Technology\n\nThe \"SPMT Consortium\" is a coalition of companies involved in designing and manufacturing mobile devices, integrated circuits, and semiconductor IP. The organization developed the SPMT specification, which is a SerDes memory interface primarily for commodity DRAM and mobile markets.\n\nThe SPMT Consortium was founded in 2009 by ARM Holdings, Hynix Semiconductor, Inc., LG Electronics, Samsung Electronics Co., Ltd. and Silicon Image, Inc. The Consortium is managed by SPMT, LLC, the entity responsible for licensing the SPMT memory interface specification. SPMT is a memory specification for dynamic random access memory (DRAM) that is based on SerDes rather than a standard parallel interface.\n\n\nThe SPMT Consortium is divided into three levels of membership: Promoters (Founders), Contributors, and Members.\n\n\n\n\n"}
{"id": "11824035", "url": "https://en.wikipedia.org/wiki?curid=11824035", "title": "Signal-to-interference ratio", "text": "Signal-to-interference ratio\n\nThe signal-to-interference ratio (SIR or S/I), also known as the carrier-to-interference ratio (CIR or C/I), is the quotient between the average received modulated carrier power \"S\" or \"C\" and the average received co-channel interference power \"I\", i.e. cross-talk, from other transmitters than the useful signal. \n\nThe CIR resembles the carrier-to-noise ratio (CNR or \"C/N\"), which is the signal-to-noise ratio (SNR or \"S/N\") of a modulated signal before demodulation. A distinction is that interfering radio transmitters contributing to \"I\" may be controlled by radio resource management, while \"N\" involves noise power from other sources, typically additive white gaussian noise (AWGN).\n\nThe CIR ratio is studied in interference limited systems, i.e. where \"I\" dominates over \"N\", typically in cellular radio systems and broadcasting systems where frequency channels are reused in view to achieve high level of area coverage. The \"C/N\" is studied in noise limited systems. If both situations can occur, the carrier-to-noise-and-interference ratio (CNIR or C/(N+I)) may be studied.\n\n"}
{"id": "2788153", "url": "https://en.wikipedia.org/wiki?curid=2788153", "title": "Software build", "text": "Software build\n\nIn the field of software development, the term build is similar to that of any other field. That is, the construction of something that has an observable and tangible result. Historically, build has often referred either to the process of converting source code files into standalone software artifact(s) that can be run on a computer, or the result of doing so. However, this is not the case with technologies such as Perl, Ruby or Python which are examples of interpreted languages.\n\nBuilding software is an end-to-end process that involves many distinct functions. Some of these functions are described below.\n\nThe version control function carries out activities such as workspace creation and updating, baselining and reporting. It creates an environment for the build process to run in and captures metadata about the inputs and outputs of the build process to ensure repeatability and reliability.\n\nTools such as Git, AccuRev or StarTeam help with these tasks by offering tools to tag specific points in history as being important, and more.\n\nAlso known as static program analysis/static code analysis this function is responsible for checking developers have adhered to the seven axes of code quality: comments, unit tests, duplication, complexity, coding rules, potential bugs and architecture & design.\n\nEnsuring a project has high-quality code results in fewer bugs and influences nonfunctional requirements such as maintainability, extensibility and readability, which have a direct impact on the ROI for your business.\n\nThis is only a small feature of managing the build process. The compilation function turns source files into directly executable or intermediate objects. Not every project will require this function.\n\nWhile for simple programs the process consists of a single file being compiled, for complex software the source code may consist of many files and may be combined in different ways to produce many different versions.\n\nThe process of building a computer program is usually managed by a build tool, a program that coordinates and controls other programs. Examples of such a program are make, Gradle, Meister by OpenMake Software, Ant, Maven, Rake, SCons and Phing. The build utility typically needs to compile the various files, in the correct order. If the source code in a particular file has not changed then it may not need to be recompiled (may not rather than need not because it may itself depend on other files that have changed). Sophisticated build utilities and linkers attempt to refrain from recompiling code that does not need it, to shorten the time required to complete the build. A more complex process may involve other programs producing code or data as part of the build process.\n\n"}
{"id": "370753", "url": "https://en.wikipedia.org/wiki?curid=370753", "title": "Stereoscope", "text": "Stereoscope\n\nA stereoscope is a device for viewing a stereoscopic pair of separate images, depicting left-eye and right-eye views of the same scene, as a single three-dimensional image.\n\nA typical stereoscope provides each eye with a lens that makes the image seen through it appear larger and more distant and usually also shifts its apparent horizontal position, so that for a person with normal binocular depth perception the edges of the two images seemingly fuse into one \"stereo window\". In current practice, the images are prepared so that the scene appears to be beyond this virtual window, through which objects are sometimes allowed to protrude, but this was not always the custom. A divider or other view-limiting feature is usually provided to prevent each eye from being distracted by also seeing the image intended for the other eye.\nMost people can, with practice and some effort, view stereoscopic image pairs in 3D without the aid of a stereoscope, but the physiological depth cues resulting from the unnatural combination of eye convergence and focus required will be unlike those experienced when actually viewing the scene in reality, making an accurate simulation of the natural viewing experience impossible and tending to cause eye strain and fatigue.\n\nAlthough more recent devices such as Realist-format 3D slide viewers and the View-Master are also stereoscopes, the word is now most commonly associated with viewers designed for the standard-format stereo cards that enjoyed several waves of popularity from the 1850s to the 1930s as a home entertainment medium.\n\nDevices such as polarized, anaglyph and shutter glasses which are used to view two actually superimposed or intermingled images, rather than two physically separate images, are not categorized as stereoscopes.\n\nThe earliest type of stereoscope was invented by Sir Charles Wheatstone in 1838. It used a pair of mirrors at 45 degree angles to the user's eyes, each reflecting a picture located off to the side. It demonstrated the importance of binocular depth perception by showing that when two pictures simulating left-eye and right-eye views of the same object are presented so that each eye sees only the image designed for it, but apparently in the same location, the brain will fuse the two and accept them as a view of one solid three-dimensional object. Wheatstone's stereoscope was introduced in the year before the first practical photographic process became available, so drawings were used. This type of stereoscope has the advantage that the two pictures can be very large if desired.\n\nContrary to a common assertion, David Brewster did not invent the stereoscope, as he himself was often at pains to make clear. A rival of Wheatstone, Brewster credited the invention of the device to a Mr. Elliot, a \"Teacher of Mathematics\" from Edinburgh, who, according to Brewster, conceived of the idea as early as 1823 and, in 1839, constructed \"a simple stereoscope without lenses or mirrors\", consisting of a wooden box 18 inches long, 7 inches wide and 4 inches high, which was used to view drawn landscape transparencies, since photography had yet to be invented. Brewster's personal contribution was the suggestion to use lenses for uniting the dissimilar pictures in 1849; and accordingly the lenticular stereoscope (lens based) may fairly be said to be his invention. This allowed a reduction in size, creating hand-held devices, which became known as Brewster Stereoscopes, much admired by Queen Victoria when they were demonstrated at the Great Exhibition of 1851.\n\nBrewster was unable to find in Britain an instrument maker capable of working with his design, so he took it to France, where the stereoscope was improved by Jules Duboscq who made stereoscopes and stereoscopic daguerreotypes, and a famous picture of Queen Victoria that was displayed at The Great Exhibition. Almost overnight a 3D industry developed and 250,000 stereoscopes were produced and a great number of \"stereoviews\", \"stereo cards\", \"stereo pairs\" or \"stereographs\" were sold in a short time. Stereographers were sent throughout the world to capture views for the new medium and feed the demand for 3D. Cards were printed with these views often with explanatory text when the cards were looked at through the double-lensed viewer, sometimes also called a \"stereopticon\", a common misnomer.\n\nIn 1861 Oliver Wendell Holmes created and deliberately did not patent a handheld, streamlined, much more economical viewer than had been available before. The stereoscope, which dates from the 1850s, consisted of two prismatic lenses and a wooden stand to hold the stereo card. This type of stereoscope remained in production for a century and there are still companies making them in limited production currently.\n\nIn the mid-20th century the View-Master stereoscope (patented 1939), with its rotating cardboard disks containing image pairs, was popular first for 'virtual tourism' and then as a toy. In 2010, Hasbro started producing a stereoscope designed to hold an iPhone or iPod Touch, called the My3D. In 2014, Google released the template for a papercraft stereoscope called Google Cardboard. Apps on the mobile phone substitute for stereo cards; these apps can also sense rotation and expand the stereoscope's capacity into that of a full-fledged virtual reality device. The underlying technology is otherwise unchanged from earlier stereoscopes.\n\nSeveral fine arts photographers and graphic artists have and continue to produce original artwork to be viewed using stereoscopes.\n\nA simple stereoscope is limited in the size of the image that may be used. A more complex stereoscope uses a pair of horizontal periscope-like devices, allowing the use of larger images that can present more detailed information in a wider field of view. The stereoscope is essentially an instrument in which two photographs of the same object, taken from slightly different angles, are simultaneously presented, one to each eye. This recreates the way which in natural vision, each eye is seeing the object from a slightly different angle, since they are separated by several inches, which is what gives humans natural depth perception. Each picture is focused by a separate lens, and by showing each eye a photograph taken several inches apart from each other and focused on the same point, it recreates the natural effect of seeing things in three dimensions.\n\nA moving image extension of the stereoscope has a large vertically mounted drum containing a wheel upon which are mounted a series of stereographic cards which form a moving picture. The cards are restrained by a gate and when sufficient force is available to bend the card it slips past the gate and into view, obscuring the preceding picture. These coin-enabled devices were found in arcades in the late 19th and early 20th century and were operated by the viewer using a hand crank. These devices can still be seen and operated in some museums specializing in arcade equipment.\n\nThe stereoscope offers several advantages:\n\nA stereo transparency viewer is a type of stereoscope that offers similar advantages, e.g. the View-Master.\n\nDisadvantages of stereo cards, slides or any other hard copy or print are that the two images are likely to receive differing wear, scratches and other decay. This results in stereo artifacts when the images are viewed. These artifacts compete in the mind resulting in a distraction from the 3D effect, eye strain and headaches.\n\n\n"}
{"id": "28714", "url": "https://en.wikipedia.org/wiki?curid=28714", "title": "Stethoscope", "text": "Stethoscope\n\nThe stethoscope is an acoustic medical device for auscultation, or listening to the internal sounds of an animal or human body. It typically has a small disc-shaped resonator that is placed against the chest, and two tubes connected to earpieces. It is often used to listen to lung and heart sounds. It is also used to listen to intestines and blood flow in arteries and veins. In combination with a sphygmomanometer, it is commonly used for measurements of blood pressure. Less commonly, \"mechanic's stethoscopes\", equipped with rod shaped chestpieces, are used to listen to internal sounds made by machines (for example, sounds and vibrations emitted by worn ball bearings), such as diagnosing a malfunctioning automobile engine by listening to the sounds of its internal parts. Stethoscopes can also be used to check scientific vacuum chambers for leaks, and for various other small-scale acoustic monitoring tasks. A stethoscope that intensifies auscultatory sounds is called phonendoscope.\n\nThe stethoscope was invented in France in 1816 by René Laennec at the Necker-Enfants Malades Hospital in Paris. It consisted of a wooden tube and was monaural. Laennec invented the stethoscope because he was uncomfortable placing his ear on women's chests to hear heart sounds. He observed that a rolled piece of paper, placed between the patient's chest and his ear, could amplify heart sounds without requiring physical contact. Laennec's device was similar to the common ear trumpet, a historical form of hearing aid; indeed, his invention was almost indistinguishable in structure and function from the trumpet, which was commonly called a \"microphone\". Laennec called his device the \"stethoscope\" (\"stetho-\" + \"-scope\", \"chest scope\"), and he called its use \"mediate auscultation\", because it was auscultation with a tool intermediate between the patient's body and the physician's ear. (Today the word \"auscultation\" denotes all such listening, mediate or not.) The first flexible stethoscope of any sort may have been a binaural instrument with articulated joints not very clearly described in 1829. In 1840, Golding Bird described a stethoscope he had been using with a flexible tube. Bird was the first to publish a description of such a stethoscope but he noted in his paper the prior existence of an earlier design (which he thought was of little utility) which he described as the snake ear trumpet. Bird's stethoscope had a single earpiece.\n\nIn 1851, Irish physician Arthur Leared invented a binaural stethoscope and, in 1852, George Philip Cammann perfected the design of the stethoscope instrument (that used both ears) for commercial production, which has become the standard ever since. Cammann also wrote a major treatise on diagnosis by auscultation, which the refined binaural stethoscope made possible. By 1873, there were descriptions of a differential stethoscope that could connect to slightly different locations to create a slight stereo effect, though this did not become a standard tool in clinical practice.\n\nSomerville Scott Alison described his invention of the stethophone at the Royal Society in 1858; the stethophone had two separate bells, allowing the user to hear and compare sounds derived from two discrete locations. This was used to do definitive studies on binaural hearing and auditory processing that advanced knowledge of sound localization and eventually lead to an understanding of binaural fusion.\n\nThe medical historian Jacalyn Duffin has argued that the invention of the stethoscope marked a major step in the redefinition of disease from being a bundle of symptoms, to the current sense of a disease as a problem with an anatomical system even if there are no noticeable symptoms. This re-conceptualization occurred in part, Duffin argues, because prior to stethoscopes, there were no non-lethal instruments for exploring internal anatomy.\n\nRappaport and Sprague designed a new stethoscope in the 1940s, which became the standard by which other stethoscopes are measured, consisting of two sides, one of which is used for the respiratory system, the other for the cardiovascular system. The Rappaport-Sprague was later made by Hewlett-Packard. HP's medical products division was spun off as part of Agilent Technologies, Inc., where it became Agilent Healthcare. Agilent Healthcare was purchased by Philips which became Philips Medical Systems, before the walnut-boxed, $300, original Rappaport-Sprague stethoscope was finally abandoned ca. 2004, along with Philips' brand (manufactured by Andromed, of Montreal, Canada) electronic stethoscope model. The Rappaport-Sprague model stethoscope was heavy and short () with an antiquated appearance recognizable by their two large independent latex rubber tubes connecting an exposed leaf-spring-joined pair of opposing F-shaped chrome-plated brass binaural ear tubes with a dual-head chest piece.\n\nSeveral other minor refinements were made to stethoscopes until, in the early 1960s, David Littmann, a Harvard Medical School professor, created a new stethoscope that was lighter than previous models and had improved acoustics. In the late 1970s, 3M-Littmann introduced the tunable diaphragm: a very hard (G-10) glass-epoxy resin diaphragm member with an overmolded silicone flexible acoustic surround which permitted increased excursion of the diaphragm member in a Z-axis with respect to the plane of the sound collecting area. The left shift to a lower resonant frequency increases the volume of some low frequency sounds due to the longer waves propagated by the increased excursion of the hard diaphragm member suspended in the concentric accountic surround. Conversely, restricting excursion of the diaphragm by pressing the stethoscope diaphragm surface firmly against the anatomical area overlying the physiological sounds of interest, the acoustic surround could also be used to dampen excursion of the diaphragm in response to \"z\"-axis pressure against a concentric fret. This raises the frequency bias by shortening the wavelength to auscultate a higher range of physiological sounds.\n\nIn 1999, Richard Deslauriers patented the first external noise reducing stethoscope, the DRG Puretone. It featured two parallel lumens containing two steel coils which dissipated infiltrating noise as inaudible heat energy. The steel coil \"insulation\" added .30 lb to each stethoscope. In 2005, DRG's diagnostics division was acquired by TRIMLINE Medical Products.\n\nStethoscopes are often considered as a symbol of healthcare professionals, as various healthcare providers are often seen or depicted with stethoscopes hanging around their necks. A 2012 research paper claimed that the stethoscope, when compared to other medical equipment, had the highest positive impact on the perceived trustworthiness of the practitioner seen with it.\n\nThe advent of practical, widespread portable ultrasonography (point-of-care ultrasonography) in the late 1990s to early 2000s led some physicians to ask how soon it would be before stethoscopes would become obsolete. Others answered that they thought the relationship of the various tools (stethoscopes and digital devices) would change but that it would be a long time before stethoscopes were obsolete. A decade later, in 2016, the same two sides of the coin were still recognized. One cardiologist said, \"the stethoscope is dead\", but a pediatrician said, \"We are not at the place, and probably won't be for a very long time\", where stethoscopes were obsolete. One consideration is that it depends on the segment of health care (emergency medical services, nursing, medicine) and the specialty. \"Stethoscopes retain their value for listening to lungs and bowels for clues of disease, experts agree.\" But for the cardiovascular system, \"auscultation is superfluous\", one cardiologist said. Thus, it could be that cardiology in the secondary and tertiary care settings may abandon the stethoscope many years before primary care, pediatrics, and physical therapy do.\n\nAcoustic stethoscopes are familiar to most people, and operate on the transmission of sound from the chest piece, via air-filled hollow tubes, to the listener's ears. The chestpiece usually consists of two sides that can be placed against the patient for sensing sound: a diaphragm (plastic disc) or bell (hollow cup). If the diaphragm is placed on the patient, body sounds vibrate the diaphragm, creating acoustic pressure waves which travel up the tubing to the listener's ears. If the bell is placed on the patient, the vibrations of the skin directly produce acoustic pressure waves traveling up to the listener's ears. The bell transmits low frequency sounds, while the diaphragm transmits higher frequency sounds. This two-sided stethoscope was invented by Rappaport and Sprague in the early part of the 20th century.\n\nOne problem with acoustic stethoscopes was that the sound level was extremely low. This problem was surmounted in 1999 with the invention of the stratified continuous (inner) lumen, and the kinetic acoustic mechanism in 2002.\n\nAn electronic stethoscope (or stethophone) overcomes the low sound levels by electronically amplifying body sounds. However, amplification of stethoscope contact artifacts, and component cutoffs (frequency response thresholds of electronic stethoscope microphones, pre-amps, amps, and speakers) limit electronically amplified stethoscopes' overall utility by amplifying mid-range sounds, while simultaneously attenuating high- and low- frequency range sounds. Currently, a number of companies offer electronic stethoscopes.\nElectronic stethoscopes require conversion of acoustic sound waves to electrical signals which can then be amplified and processed for optimal listening. Unlike acoustic stethoscopes, which are all based on the same physics, transducers in electronic stethoscopes vary widely. The simplest and least effective method of sound detection is achieved by placing a microphone in the chestpiece. This method suffers from ambient noise interference and has fallen out of favor. Another method, used in Welch-Allyn's Meditron stethoscope, comprises placement of a piezoelectric crystal at the head of a metal shaft, the bottom of the shaft making contact with a diaphragm. 3M also uses a piezo-electric crystal placed within foam behind a thick rubber-like diaphragm. The Thinklabs' Rhythm 32 uses an electromagnetic diaphragm with a conductive inner surface to form a capacitive sensor. This diaphragm responds to sound waves, with changes in an electric field replacing changes in air pressure. The Eko Core enables wireless transmission of heart sounds to a smartphone or tablet.\n\nBecause the sounds are transmitted electronically, an electronic stethoscope can be a wireless device, can be a recording device, and can provide noise reduction, signal enhancement, and both visual and audio output. Around 2001, Stethographics introduced PC-based software which enabled a phonocardiograph, graphic representation of cardiologic and pulmonologic sounds to be generated, and interpreted according to related algorithms. All of these features are helpful for purposes of telemedicine (remote diagnosis) and teaching.\n\nElectronic stethoscopes are also used with computer-aided auscultation programs to analyze the recorded heart sounds pathological or innocent heart murmurs.\n\nSome electronic stethoscopes feature direct audio output that can be used with an external recording device, such as a laptop or MP3 recorder. The same connection can be used to listen to the previously recorded auscultation through the stethoscope headphones, allowing for more detailed study for general research as well as evaluation and consultation regarding a particular patient's condition and telemedicine, or remote diagnosis.\n\nThere are some smartphone apps that can use the phone as a stethoscope. At least one uses the phone's own microphone to amplify sound, produce a visualization, and e-mail the results. These apps may be used for training purposes or as novelties, but have not yet gained acceptance for professional medical use.\n\nThe first stethoscope that could work with a smartphone application was introduced in 2015 \n\nA \"fetal stethoscope\" or \"fetoscope\" is an acoustic stethoscope shaped like a listening trumpet. It is placed against the abdomen of a pregnant woman to listen to the heart sounds of the fetus. The fetal stethoscope is also known as a Pinard horn after French obstetrician Adolphe Pinard (1844–1934).\n\nA Doppler stethoscope is an electronic device that measures the Doppler effect of ultrasound waves reflected from organs within the body. Motion is detected by the change in frequency, due to the Doppler effect, of the reflected waves. Hence the Doppler stethoscope is particularly suited to deal with moving objects such as a beating heart.\nIt was recently demonstrated that continuous Doppler enables the auscultation of valvular movements and blood flow sounds that are undetected during cardiac examination with a stethoscope in adults. The Doppler auscultation presented a sensitivity of 84% for the detection of aortic regurgitations while classic stethoscope auscultation presented a sensitivity of 58%. Moreover, Doppler auscultation was superior in the detection of impaired ventricular relaxation. Since the physics of Doppler auscultation and classic auscultation are different, it has been suggested that both methods could complement each other.\nA military noise-immune Doppler based stethoscope has recently been developed for auscultation of patients in loud sound environments (up to 110 dB).\n\nA 3D-printed stethoscope is an open-source medical device meant for auscultation and manufactured via means of 3D printing. The 3D stethoscope was developed by Dr. Tarek Loubani and a team of medical and technology specialists. The 3D-stethoscope was developed as part of the Glia project, and its design is open source from the outset. The stethoscope gained widespread media coverage in Summer 2015.\n\nThe need for a 3D-stethoscope was borne out of a lack of stethoscopes and other vital medical equipment because of the blockade of the Gaza Strip, where Loubani, a Palestinian-Canadian, worked as an emergency physician during the 2012 conflict in Gaza. The 1960s-era \"Littmann Cardiology 3\" stethoscope became the basis for the 3D-printed stethoscope developed by Loubani.\n\nStethoscopes usually have rubber earpieces, which aid comfort and create a seal with the ear, improving the acoustic function of the device. Stethoscopes can be modified by replacing the standard earpieces with moulded versions, which improve comfort and transmission of sound. Moulded earpieces can be cast by an audiologist or made by the stethoscope user from a kit.\n\n\n"}
{"id": "7044318", "url": "https://en.wikipedia.org/wiki?curid=7044318", "title": "Terminal controller", "text": "Terminal controller\n\nA Terminal controller is a device that collects traffic from a set of terminals and directs them to a concentrator.\n"}
{"id": "1230648", "url": "https://en.wikipedia.org/wiki?curid=1230648", "title": "The Prize: The Epic Quest for Oil, Money, and Power", "text": "The Prize: The Epic Quest for Oil, Money, and Power\n\nThe Prize: The Epic Quest for Oil, Money, and Power is Daniel Yergin's history of the global petroleum industry from the 1850s through 1990. \"The Prize\" became a bestseller, helped by its release date in December 1990, four months after the invasion of Kuwait ordered by Saddam Hussein and one month before the U.S.-led coalition began the Gulf War to oust Iraqi troops from that country. The book eventually went on to win a Pulitzer Prize.\n\n\"The Prize\" has been called the \"definitive\" history of the oil industry, even a \"bible\".\n\nIn 1992 \"The Prize\" won the Pulitzer Prize for General Non-Fiction; it has been translated into fourteen languages. Now out of print in hardcover, \"The Prize\" was published in a paperback edition () that was released at the end of 1992, and is currently in print. \"The Prize\" is often cited as essential background reading for students of the history of petroleum. Prof. Joseph R. Rudolph Jr. said in \"Library Journal\", for example:\n\nWritten by one of the foremost U.S. authorities on energy, it is a major work in the field, replete with enough insight to satisfy the scholar and sufficient concern with the drama and colorful personalities in the history of oil to capture the interest of the general public. Though lengthy, the book never drags in developing its themes: the relationship of oil to the rise of modern capitalism; the intertwining relations between oil, politics, and international power; and the relationship between oil and society in what Yergin calls today's age of \"Hydrocarbon Man\".\nTen years in the making, \"The Prize\" draws on extensive research carried out by the author and his staff, including Sue Lena Thompson, Robert Laubacher, and Geoffrey Lumsden. Daniel Yergin has excellent connections with the oil industry, and is the Chairman of a private energy consulting firm called Cambridge Energy Research Associates, Global Energy Analyst for NBC and CNBC, member of the board of the United States Energy Association and of the U.S.-Russia Business Council. Yergin's history has 61 pages of notes and a bibliography of 26 pages that lists as sources not only 700 books, articles, and dissertations, 60 government documents, 28 \"data sources\", more than 34 manuscript collections, fifteen government archives, eight oral histories, and four oil company archives (Amoco, Chevron, Gulf, and Royal Dutch Shell), but also 80 personal interviews with key individuals like James Schlesinger and Armand Hammer.\n\n\"The Prize\" was the basis for a eight-hour documentary television series titled \"The Prize: The Epic Quest for Oil, Money, & Power\", narrated by Donald Sutherland and broadcast by PBS in 1992–1993. The series is said to have been seen by 20 million people in the United States.\n\nThe book is also available as an abridged audiobook, read by Bob Jamieson with a run time of 2 hours and 53 minutes.\n\nIn 2011 Yergin's \"\" was published by Penguin Press. \"The Quest\" is considered the sequel to \"The Prize\".\n\nThe name of the book is taken from a quote by Winston Churchill in 1912, when he was First Lord of the Admiralty, long before becoming Prime Minister of the United Kingdom. He was arguing for the conversion of British warships from coal to fuel oil, but noted the geopolitical ramifications of tying Britain's fortunes to oil.\n\nAs Yergin quotes Churchill:\n\nTo build any large additional number of oil-burning ships meant basing our naval supremacy upon oil. But oil was not found in appreciable quantities in our islands. If we required it we must carry it by sea in peace or war from distant countries. We had, on the other hand, the finest supply of the best steam coal in the world, safe in our mines under our own land. To commit the Navy irrevocably to oil was indeed to \"take arms against a sea of troubles.\" Yet, if the difficulties and risk could be surmounted, \"we should be able to raise the whole power and efficiency of the Navy to a definitely higher level; better ships, better crews, higher economies, more intense forms of war power\"—in a word, \"mastery itself was the prize of the venture.\" [Emphasis added]\n\n"}
{"id": "48786797", "url": "https://en.wikipedia.org/wiki?curid=48786797", "title": "Urban Engines", "text": "Urban Engines\n\nUrban Engines is a data analytics startup based in Silicon Valley, acquired by Google in September 2016, that uses data to give insight into cities and how people move around in them. Founder and CEO Shiva Shivakumar says the company is mapping the “Internet of Moving Things,” as quoted by the New York Times, which is a new way of mapping objects in motion. The company’s goal is to improve mobility in cities.\n\nUrban Engines has developed a new type of database, which maps objects in motion, called a “Space/Time Engine”, and currently has deals with cities like, Washington D.C., São Paulo and Singapore as well as delivery and logistics companies.\n\nThe company also released a commuter app with “mixed-mode routing,” which evaluates different modes of transportation – walking, driving, public transit, and Uber – to give users the quickest routes. The app is built on Urban Engines own proprietary mapping system.\n\nThe company was founded by Shiva Shivakumar and Balaji Prabhakar. Shiva was previously VP of Engineering and a Distinguished Entrepreneur at Google (2001-2010), who helped build Adsense, Search Appliances and Cloud Apps, and Balaji is a Stanford professor and director of the Stanford Center for Societal Networks, a prominent research initiative to make societal networks smarter, more scalable and more efficient.\n\nUrban Engines has raised an undisclosed amount of money led by Google Ventures. Andreessen Horowitz, SV Angel, Greylock, Samsung Ventures, early Google investor Ram Shriram and Google Chairman Eric Schmidt also participated.\n"}
{"id": "18749313", "url": "https://en.wikipedia.org/wiki?curid=18749313", "title": "Video sculpture", "text": "Video sculpture\n\nA video sculpture is a type of video installation that integrates video into an object, environment, site or performance. The nature of video sculpture is that it utilizes the material of video in an innovative way in space and time, different from the standard traditional narrative screening where the video has a beginning and end.\n\nIn one definition video sculpture involves one or more monitors or projections that spectators move among or stand in front of. Video sculptures formed of more than one screen or projection may broadcast a single program or may simultaneously broadcast different interconnected sequences on several channels. The screens used in the sculpture can be arranged in many different ways. For example, they can be suspended from a ceiling, aligned and stacked to make a video wall or even randomly stacked on top of each other. Video sculpture is a medium that offers performing artists a chance to have a more permanent artistic forum.\n\nVideo sculpture includes projection mapping on objects and environments. This has become more accessible and popular due to software advancements in the last five years.\n\nIn the late 1950s and early 1960s, artists Wolf Vostell and Edward Kienholz began experimenting with televisions by using them in their happenings and assemblages respectively. In March 1963, Nam June Paik's debuted his video sculpture entitled \"Music/Electronic Television\" at the Parnass Gallery in Wupertal, which used 13 altered televisions. In May 1963 Wolf Vostell shows his installation \"6 TV-Dé-coll/age\" at the Smolin Gallery in New York utilized six televisions, each with an anomaly. Shigeko Kubota was also an innovator in the use of video in sculptural form. Her \"Duchampiana: Nude Descending a Staircase\" was the first video sculpture acquired by the Museum of Modern Art. This work is a reference to Marcel Duchamp's \"Nude Descending a Staircase, No. 2\" (1912) Video sculpturist are becoming influential among early 21st century artists. One of Paik's video sculptures in which the six windows of a 1936 Chrysler Airstream were replaced with video monitors sold for $75,000 in 2002.\n\nCharlotte Moorman was a notable subject of video sculptures as a renowned topless cellist.\n\nThere are several developments in current video sculptures. The proliferation of powerful projectors and pixel-bending technology has enabled large-scale works often created for specific events and locations. Other artists like make use of multiple LCD screens or video walls and incorporate computer generated images. A different approach is used by artists like Madeleine Altmann, who creates sculptures with recycled cathode ray tube monitors.\n\n\n"}
{"id": "46838619", "url": "https://en.wikipedia.org/wiki?curid=46838619", "title": "Welding blanket", "text": "Welding blanket\n\nA welding blanket is a piece of safety equipment designed to protect equipment and the welder while welding. A welding blanket typically consists of a layer of flexible protective material containing unexpanded vermiculite and inorganic heat resistant fibrous material. When contacted by spatter of molten metal during a welding process the material protects the surrounding areas by maintaining the structural integrity of the blanket. Many modern welding blankets are made of flame retardant fiberglass and can stand working temperatures ranging from 300 to 2,500 degrees Fahrenheit. Historically welding blankets were made of woven asbestos, however due to safety concerns have been widely discontinued.\n"}
{"id": "424022", "url": "https://en.wikipedia.org/wiki?curid=424022", "title": "Windrow composting", "text": "Windrow composting\n\nIn agriculture, windrow composting is the production of compost by piling organic matter or biodegradable waste, such as animal manure and crop residues, in long rows (\"windrows\"). This method is suited to producing large volumes of compost. These rows are generally turned to improve porosity and oxygen content, mix in or remove moisture, and redistribute cooler and hotter portions of the pile. Windrow composting is a commonly used farm scale composting method. Composting process control parameters include the initial ratios of carbon and nitrogen rich materials, the amount of bulking agent added to assure air porosity, the pile size, moisture content, and turning frequency.\n\nThe temperature of the windrows must be measured and logged constantly to determine the optimum time to turn them for quicker compost production.\n\nCompost windrow turners were developed to produce compost on a large scale by Fletcher Sims Jr. of Canyon, Texas . They are traditionally a large machine that straddles a windrow of 4 feet (1.25 meters) or more high, by as much as 12 feet (3.5 meters) across. Although smaller machines exist for small windrows, most operations use large machines for volume production. Turners drive through the windrow at a slow rate of forward movement. They have a steel drum with paddles that are rapidly turning. As the turner moves through the windrow, fresh air (oxygen) is injected into the compost by the drum/paddle assembly, and waste gases produced by bacterial decomposition are vented. The oxygen feeds the aerobic bacteria and thus speeds the composting process.\n\nTo properly use a compost windrow turner, it is ideal to compost on a hard surfaced pad. Heavy-duty compost windrow turners allow the user to obtain optimum results with the aerobic hot composting process. By using four wheel drive or tracks the windrow turner is capable of turning compost in windrows located in remote locations. With a self-trailering option this allows the compost windrow turner to convert itself into a trailer to be pulled by a semi-truck tractor. These two options combined allow the compost windrow turner to be easily hauled anywhere and to work compost windrows in muddy and wet locations.\n\nMolasses-based distilleries all over the world generate large amount of effluent termed as spent wash or vinasse. For each liter of alcohol produced, around 8 liters of effluent is generated. This effluent has COD of 1,50,000 PPM and BOD of 60,000 PPM and even more. This effluent needs to be treated and the only effective method for conclusive disposal is by composting.\n\nSugar factories generate pressmud / cachaza during the process and the same has about 30% fibers as carbon and has large amounts of water. This pressmud is dumped on prepared land in the form of 100 meters long windrows of 3 meters x 1.5 meters and spent wash is sprayed on the windrow while the windrow is being turned. These machines help consume spent wash of about 2.5 times of the volume of the pressmud, which means that a 100 meters of windrow accommodates about 166 MT of pressmud and uses about 415 m³ of Spent wash in 50 days.\n\nMicrobial Culture (organic solution) TRIO COM-CULT is used about 1 kg per MT of pressmud for fast de-composing of the spent wash. Hundreds of thousands of square meters of spent wash is being composted all over the world in countries like India, Colombia, Brazil, Thailand, Indonesia, South Africa etc.\n\nThe compost yard has to be prepared in such a way that the land is impervious and does not allow the liquid effluent to pass down into the earth. The compost thus generated is of excellent quality and is rich in nutrients.\n\n\n"}
{"id": "34846447", "url": "https://en.wikipedia.org/wiki?curid=34846447", "title": "ZColo", "text": "ZColo\n\nzColo is a wholly owned subsidiary of Zayo Group, operating the colocation and data center services. zColo's principal service offering is colocation space and power with carrier-neutral data center and interconnection services. As of May 2015, they operate 35 colocation facilities in North America and 8 in Europe.\n\n\n\n"}
