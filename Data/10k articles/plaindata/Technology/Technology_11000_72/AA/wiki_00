{"id": "33714634", "url": "https://en.wikipedia.org/wiki?curid=33714634", "title": "Alamanda Polymers", "text": "Alamanda Polymers\n\nAlamanda Polymers, Inc. is a life sciences company that was formed in 2007 that specializes in formation of polyamino acids to specific chain links for research purposes.\n\nAlamanda Polymers was formed in 2007 and produces specialized polyamino acids to specific chain links for research purposes.\n\n\n\nE. Tkachenko, M. Sabouri-Ghomi, O. Pertz, C. Kim, E. Gutierrez, M. Machacek, A. Groisman, G. Danuser, M. H. Ginsberg Nat. Cell Biol. 2011, 13, 660-667.\nA. Romoser, D. Ritter, R. Majitha, K. E. Meissner, M. McShane, C. M. Sayes PLoS One 2011, 6, e22079.\nC. Fante, A. Eldar-Boock, R. Satchi-Fainaro, H. M. I. Osborn, F. Greco J. Med. Chem. 2011, 54, 5255-5259.\n\n"}
{"id": "12974874", "url": "https://en.wikipedia.org/wiki?curid=12974874", "title": "Ammonium nitrate disasters", "text": "Ammonium nitrate disasters\n\nWhen heated, ammonium nitrate decomposes non-explosively into gases including oxygen; however, it can be induced to decompose explosively by detonation. Large stockpiles of the material can be a major fire risk due to their supporting oxidation, and may also detonate, as happened in the Texas City disaster of 1947, which led to major changes in the regulations for storage and handling.\n\nThere are two major classes of incidents resulting in explosions:\n\nAmmonium nitrate decomposes in temperatures above . Pure AN is stable and will stop decomposing once the heat source is removed, but when catalysts are present, the reaction can become self-sustaining (known as self-sustaining decomposition, or SSD). This is a well-known hazard with some types of NPK fertilizers and is responsible for the loss of several cargo ships.\n\n"}
{"id": "27719417", "url": "https://en.wikipedia.org/wiki?curid=27719417", "title": "Analytik Jena", "text": "Analytik Jena\n\nAnalytik Jena AG, based in Jena (Thuringia, Germany), is a provider of analytical, bioanalytical and optical systems for industrial and scientific applications. Analytik Jena was founded in 1990 as a sales and service company for analytical technology. The company has been listed on the Frankfurt Stock Exchange from July 3, 2000 until March 27, 2015. Analytik Jena and its subsidiaries employ about 1,100 people in more than 90 countries. The international Group earns two-thirds of sales abroad and maintains business relationships in more than 120 countries around the globe.\n\nAnalytik Jena was founded jointly by the present-day Executive Board member Klaus Berka along with Jens Adomat in 1990, shortly after East Germany‘s peaceful revolution. At the beginning, the two former Zeiss employees managed the distribution of analytical measuring instruments, primarily in Thuringia, Saxony and Saxony-Anhalt.\n\nIn 1994, first shares in IDC Geräteentwicklungsgesellschaft mbH Langewiesen were acquired, a company whose products Analytik Jena had been selling since 1992. Due to the purchase of the laboratory analysis technology of Carl Zeiss Jena GmbH at the end of 1995, the Company's market share in the instrument business increased further. Analytik Jena had become a manufacturing company in the analytical technology segment, laying the foundation for the “Analytical Instrumentation” business unit. This acquisition marked the beginning of the research and development of analytical systems in the area of atomic absorption spectrometry and molecular spectroscopy.\n\nOn July 3, 2000, the shares of Analytik Jena AG were traded for the first time on the Frankfurt Stock Exchange‘s “Neuer Markt” segment. In this way, important liquidity for the further expansion of sales activity and the advancement of research and development was secured. In the subsequent years, Analytik Jena enforced its growth path due to strategic acquisitions, the expansion of international sales activities and the founding of subsidiaries in Germany and abroad. After expanding to Eastern and Western Europe and to the USA, Analytik Jena has grown significantly in Asia. Especially China is an important foreign market of the Company.\n\nIn order to take advantage of the opportunities offered by the biotechnology market, Analytik Jena has found its third business unit “Life Science”. With the acquisition of Biometra GmbH, CyBio AG, and the US company UVP, LLC the Company expanded its product portfolio.\n\nOne of the most important business acquisitions in the history of Analytik Jena AG has been closed in 2014. With this acquisition, the Company entered the rapidly growing global ICP-MS (Inductively Coupled Plasma Mass Spectrometry) market. Analytik Jena thereby further broadened its atomic spectroscopy portfolio and is among the few suppliers that offer all three core technologies for elemental trace analysis (AAS, ICP-OES, and ICP-MS).\n\nToday, Swiss group Endress+Hauser has the majority holding of Analytik Jena AG. In September 2015, Endress+Hauser submitted the squeeze-out request to the Analytik Jena AG. At this time, Endress+Hauser held a stake of 96.18% in the share capital of Analytik Jena AG. Since March 27, 2015 the shares of the Jena AG are no longer listed on the regulated market of the Frankfurt Stock Exchange.\n\nAnalytik Jena’s business model is structured on the basis of corporate activities in the three operational segments Analytical Instrumentation and Life Science. With these symbiotically interwoven business units for specialized instruments, Analytik Jena offers products for a technology sector that is both modern and in demand.\n\nAnalytik Jena provides high-end analysis systems for the qualitative and quantitative analysis of liquids, solids, and gases in environmental, foodstuffs, pharmaceutical, medical, and agricultural Analysis.\n\nAnalytik Jena’s business unit Life Science is a system provider for bioanalysis. It demonstrates the biotechnological competence of Analytik Jena AG and provides a wide product spectrum for automated total, as well as individual solutions for molecular diagnostics. Its products are focused to offer quality and the reproducibility of laboratory results. Besides the unit offers customized solutions, it is also able to adapt its products to the customer’s needs. The wide product spectrum is made possible through the synergism between Analytik Jena and its product lines Biometra and .\n\n\nThe Analytik Jena Group headquarter is located in Jena, Germany with additional branches. Outside Germany Analytik Jena is represented in Thailand (Far East Office), Russia (Moscow Office), China (Beijing Office), and UAE (Dubai Branch Office).\n\nThe following companies are domestic subsidiaries of Analytik Jena:\nAnalytik Jena has also subsidiaries in foreign countries:\n\n"}
{"id": "14244253", "url": "https://en.wikipedia.org/wiki?curid=14244253", "title": "Anemostat", "text": "Anemostat\n\nAn anemostat is a device used to regulate airflow and pressure in a room or system requiring complex airflow patterns. Anemostat is the trademark name of the Anemostat company, however, the company name has become synonymous with the device. The word is a combination of anemometer and thermostat. Anemostats are used in hospitals to stabilize room conditions, and to prevent the spread of airborne pathogens. In laboratories with fume hoods, anemostats help to ensure adequate conditions for optimum ventilation.\n\nThe original design was developed by Franz J. Kurth.\n"}
{"id": "2967265", "url": "https://en.wikipedia.org/wiki?curid=2967265", "title": "Arturo Rosenblueth", "text": "Arturo Rosenblueth\n\nArturo Rosenblueth Stearns (October 2, 1900 – September 20, 1970) was a Mexican researcher, physician and physiologist, who is known as one of the pioneers of cybernetics.\n\nRosenblueth was born in 1900 in Ciudad Guerrero, Chihuahua. He began his studies in Mexico City, then traveled to Berlin and Paris where he obtained his medical degree. Returning to Mexico city in 1927, he engaged in teaching and research in physiology. In 1930 he obtained a Guggenheim Scholarship and moved to Harvard University, to the department of Physiology, then directed by Walter Cannon. With Cannon he explored the chemical mediation of homeostasis. Rosenblueth cowrote research papers with both Cannon and Norbert Wiener, pioneer of cybernetics. Rosenblueth was an influential member of the core group at the Macy Conferences.\n\nIn 1944, Rosenblueth became professor of physiology at the National Autonomous University of Mexico. Eventually he became head of the Physiology Laboratory of the National Institute of Cardiology, head of the Physiology Department and, in 1961, director of the Center for Scientific Research and Advanced Studies (Cinvestav) at the National Polytechnic Institute.\n\nBetween 1947 and 1949, and again between 1951 and 1952, using grants from the Rockefeller Foundation, he returned to Harvard to further collaborate with Wiener.\n\nArturo Rosenblueth died September 20, 1970, in Mexico City.\n\nSince the 1930s Rosenblueth worked with Cannon on issues related with Chemical transmission among nervous elements. Between 1931 and 1945 he worked with several specialists, among them Cannon, del Pozo, H.G. Schwartz, and Norbert Wiener. With Wiener and Julian Bigelow he wrote \"Behavior, Purpose and Teleology\", which, according to Wiener himself, set the bases for the new science of Cybernetics.\n\nIn his 1943 cybernetic classification \"Behavior, Purpose and Teleology\", purpose is a behavior subclass. Behavior is active or passive and active behavior is purposeful or random. Active purposeful behavior is then either feedback teleological on non-teleological. Negative feedback is important to guide the positive goal route. Purposeful teleological feedback helps guide the predictive behavior orders. Teleology is feedback controlled purpose.\n\nRosenblueth's classification system was criticized and the need for an external observability to the purposeful behavior was established to validate the behavior and goal-attainment. The purpose of observing and observed systems is respectively distinguished by the system's subjective autonomy and objective control.\n\nHe devoted himself to the fields of nervous impulse transmissions, neuromuscular transmission, synaptic transmission, the propagation of impulses in the heart, the control of blood circulation, and the physiology of brain cortex. However he also taught several courses of mathematics and even musicology.\n\n- \"The Role of Models in Science,\" with Norbert Wiener\n- \"The Role of Models in Science,\" with Norbert Wiener\n\n"}
{"id": "24531172", "url": "https://en.wikipedia.org/wiki?curid=24531172", "title": "Chicony Electronics", "text": "Chicony Electronics\n\nChicony Electronics Co., Ltd. () is a Taiwan-based multinational electronics manufacturer. Its product lineup includes input devices, power supplies and digital image products. It offers desktop keyboards, mobile keyboards, digital cameras, personal-computer cameras, integrated webcams and digital video cameras. It has also been a well known manufacturer of motherboards for personal computers and notebooks. The company was founded in 1983 and is based in Taipei, Taiwan. As of 2009 it has operations in Australia, Brazil, Canada, China, the Czech Republic, Germany, Ireland, Japan, Mexico, the Philippines, Singapore, Thailand, Taiwan, the United Kingdom and the United States.\n\nNotable clients have included HP, GoPro, Google, Dropcam, Lenovo, etc. \n"}
{"id": "441179", "url": "https://en.wikipedia.org/wiki?curid=441179", "title": "Combustion chamber", "text": "Combustion chamber\n\nA combustion chamber is that part of an internal combustion engine (ICE) or a reaction engine in which the fuel/air mix is burned.\n\nICEs typically comprise reciprocating piston engines, rotary engines, gas turbines and jet turbines.\n\nThe combustion process increases the internal energy of a gas, which translates into an increase in temperature, pressure, or volume depending on the configuration. In an enclosure, for example the cylinder of a reciprocating engine, the volume is controlled and the combustion creates an increase in pressure. In a continuous flow system, for example a jet engine combustor, the pressure is controlled and the combustion creates an increase in volume.\nThis increase in pressure or volume can be used to do work, for example, to move a piston on a crankshaft or a turbine disc in a gas turbine. If the gas velocity changes, thrust is produced, such as in the nozzle of a rocket engine.\n\nAt top dead centre the pistons of a petrol engine are flush (or nearly flush) with the top of the cylinder block. The combustion chamber may be a recess either in the cylinder head, or in the top of the piston. A design with the combustion chamber in the piston is called a Heron head, where the head is machined flat but the pistons are dished. The Heron head has proved even more thermodynamically efficient than the hemispherical head. Intake valves permit the inflow of a fuel air mix; and exhaust valves allow burnt gases to be scavenged.\nVarious shapes of combustion chamber have been used, such as: L-head (or flathead) for side-valve engines; \"bathtub\", \"hemispherical\", and \"wedge\" for overhead valve engines; and \"pent-roof\" for engines having 3, 4 or 5 valves per cylinder. The shape of the chamber has a marked effect on power output, efficiency and emissions; the designer's objectives are to burn all of the mixture as completely as possible while avoiding excessive temperatures (which create NOx). This is best achieved with a compact rather than elongated chamber. \nThe intake valve/port is usually placed to give the mixture a pronounced \"swirl\" pattern (the term \"swirl\" is preferable to \"turbulence\", which implies movement without a pronounced pattern) above the rising piston, improving mixing and combustion. The shape of the piston top also affects the amount of swirl. Another design feature to promote turbulence for good fuel/air mixing is \"squish\", where the fuel/air mix is \"squished\" at high pressure by the rising piston. Where swirl is particularly important, combustion chambers in the piston may be favoured.\nIgnition typically occurs around 15 degrees before top dead centre. The spark plug must be sited so that the flame front can progress throughout the combustion chamber. Good design should avoid narrow crevices where stagnant \"end gas\" can become trapped, as this gas may detonate violently after the main charge, adding little useful work and potentially damaging the engine. \n\nDiesel engines fall into two broad classes:\n\n\nDirect injection engines usually give better fuel economy but indirect injection engines can use a lower grade of fuel.\n\nHarry Ricardo was prominent in developing combustion chambers for diesel engines, the best known being the Ricardo Comet.\n\nThe combustion chamber in gas turbines and jet engines (including ramjets and scramjets) is called the combustor.\n\nThe combustor is fed with high pressure air by the compression system, adds fuel and burns the mix and feeds the hot, high pressure exhaust into the turbine components of the engine or out the exhaust nozzle.\n\nDifferent types of combustors exist, mainly:\n\nThe term combustion chamber is also used to refer to an additional space between the firebox and boiler in a steam locomotive. This space is used to allow further combustion of the fuel, providing greater heat to the boiler.\n\nLarge steam locomotives usually have a combustion chamber in the boiler to allow the use of shorter firetubes. This is because:\n\n\nMicro combustion chambers are the devices in which combustion happens at a very small volume, due to which surface to volume ratio increases which plays a vital role in stabilizing the flame.\n\n"}
{"id": "13749212", "url": "https://en.wikipedia.org/wiki?curid=13749212", "title": "Department of Biotechnology", "text": "Department of Biotechnology\n\nThe Department of Biotechnology (DBT) is an Indian government department, under the Ministry of Science and Technology responsible for administrating development and commercialisation in the field of modern biology and biotechnology in India. It was set up in 1986.\n\nThrough several research and development projects, demonstrations, grants and creation of infrastructural facilities a clear visible impact of this field has been seen. The department has made significant achievements in the growth and application of biotechnology in the broad areas of agriculture, health care, animal sciences, environment, and industry. The proven technologies at the laboratory level have been scaled up and demonstrated in field. \n\nPatenting of innovations, technology transfer to industries and close interaction with them have given a new direction to biotechnology research in India. Initiatives have been taken to promote transgenic research in plants with emphasis on pest and disease resistance, nutritional quality, silk-worm genome analysis etc.\n\nOn the other hand, molecular biology of human genetic disorders, brain research, plant genome research, development, validation and commercialisation of diagnostic kits and vaccines for communicable diseases, food biotechnology, biodiversity conservation and bioprospecting, setting up of micropropagation parks and biotechnology based development for SC/ST, rural areas, women and for different States. Organisation's mandate and the centres of excellence that it has set up to achieve this goal are listed below.\n\n\nIn December 2015, the Department of Biotechnology launched the National Biotechnology Development Strategy 2015–2020 programme. The stated aim of the programme is to intensify research in the fields of vaccines, humane genome, infectious and chronic diseases, crop science, animal agriculture and aqua culture, food and nutrition, environmental management and technologies for clean energy. The mission, through stakeholders in the biotechnology and technology domains is backed with significant investments to create new products, creating a strong infrastructure for research and development, commercialization, and empowering human resources scientifically and technologically.\n\n\n\n"}
{"id": "48301954", "url": "https://en.wikipedia.org/wiki?curid=48301954", "title": "Digitron (company)", "text": "Digitron (company)\n\nDigitron is a Croatian electronics company located in Buje, Istria. Their name became eponymous for a handheld calculator in the former Yugoslav area. They are responsible for the release of Europe's first pocket calculator in 1971, called DB 800.\n\n"}
{"id": "42279242", "url": "https://en.wikipedia.org/wiki?curid=42279242", "title": "Division on Engineering and Physical Sciences", "text": "Division on Engineering and Physical Sciences\n\nThe Division on Engineering and Physical Sciences is a part of the National Research Council in the United States, which serves as an independent adviser to the President, the Congress and federal agencies on scientific and technical questions of national importance. The National Research Council is jointly administered by the National Academy of Sciences, the National Academy of Engineering, and the National Academy of Medicine. Its mission is to assess the role of science and technology in important public policy issues and decisions in public programs. \n\nThe division's activities are organized around four broad areas:\n\n"}
{"id": "25519718", "url": "https://en.wikipedia.org/wiki?curid=25519718", "title": "DocuWare", "text": "DocuWare\n\nDocuWare is a provider of solutions for document management and workflow automation, also referred to as enterprise content management (ECM) or more recently content services. The company is headquartered in Germany and the United States.\n\nDocuWare is also the name of the flagship product offered by the company. Besides being offered as cloud-based Software as a Service (SaaS), it is also available with feature parity for installation on-premises. The software is available in 16 languages. DocuWare is distributed via a global network of 500 authorized DocuWare partners (ADP) as well as directly to key accounts. As of January 2017, DocuWare is used by 16,000 customers in 70 countries.\n\nOn October 27, 1988, DOCUNET GmbH was founded in Germering, Germany (near Munich) by President Jürgen Biffar. Since 1990, Mr Biffar has been managing the company with his colleague, Thomas Schneck, currently President of Sales. DOCUNET AG was renamed to DocuWare AG in August 2000.\n\nSince 1999, DocuWare has outsourced parts of its development to Sofia, Bulgaria. As of 2016, Nemetschek OOD had 42 employees working on the DocuWare product. DocuWare GmbH holds a 20 percent stake in Nemetschek OOD.\n\n• DocuWare Corporation (New Windsor), founded January 1, 2001\n\n• DocuWare Ltd (Nottinghamshire), founded April 1, 2005\n\n• DocuWare SARL (Paris), founded September 1, 2008\n\n• DocuWare S.L. (Barcelona), founded July 1, 2009\n\nIn April 2012, an investment agreement was signed between the company and Morgan Stanley Expansion Capital LP, a Morgan Stanley Investment Management private equity fund. Its aim was promoting and accelerating the global growth of DocuWare. The legal form, AG (Public Holding Company) changed to GmbH (limited liability corporation).\n\nThe company acquired U.S.-based Westbrook Technologies Inc., developer of Fortis ECM software in August 2013. In 2014, Westbrook Technologies Inc. was merged into DocuWare Corporation.\n\nAt the beginning of 2016 DocuWare appointed Dr Michael Berger as its Chief Technology Officer (CTO). Dr Berger joined the company in 2008 as Vice President Research & Development.\n\n1988 – Development of a DACS-Board (DACS = Document Archiving and Communication System)\n\n1990 – DACS Office, first software solution running on Windows 3.0 to store, search, display and print documents (Windows 3.0: Released May 22, 1990)\n\n1993 – DocuWare 3.0, customizable file cabinets, structure not pre-set; Sending of documents out of DocuWare\n\n1998 – DocuWare 4.0, first 32-Bit Version\n\n2006 – DocuWare 5.0, new development based on .NET technology\n\n2008 – DocuWare Web Client, web-based document management\n\n2009 – DocuWare SaaS, first SaaS via private cloud\n\n2012 – DocuWare Online, first SaaS via public cloud\n\n2014 – DocuWare Online was renamed to DocuWare Cloud and ported to Microsoft Azure Platform\n\nISO 9001\n\nSOC 2\n\nIDW ERS FAIT 3\n\nHIPAA-compliant\n\nBuyers Laboratory Inc. (BLI) - 2014 5-Star Rated for Product and Program Version 6 Online \n\nBuyers Laboratory Inc. (BLI) - 2013 5-Star Rated for Product and Program Version 6 and wins Summer Pick Award Outstanding Document Management Solution \n\n\n"}
{"id": "76086", "url": "https://en.wikipedia.org/wiki?curid=76086", "title": "Electric motor", "text": "Electric motor\n\nAn electric motor is an electrical machine that converts electrical energy into mechanical energy. Most electric motors operate through the interaction between the motor's magnetic field and winding currents to generate force in the form of rotation. Electric motors can be powered by direct current (DC) sources, such as from batteries, motor vehicles or rectifiers, or by alternating current (AC) sources, such as a power grid, inverters or electrical generators. An electric generator is mechanically identical to an electric motor, but operates in the reverse direction, accepting mechanical energy (such as from flowing water) and converting this mechanical energy into electrical energy. \n\nElectric motors may be classified by considerations such as power source type, internal construction, application and type of motion output. In addition to AC versus DC types, motors may be brushed or brushless, may be of various phase (see single-phase, two-phase, or three-phase), and may be either air-cooled or liquid-cooled. General-purpose motors with standard dimensions and characteristics provide convenient mechanical power for industrial use. The largest electric motors are used for ship propulsion, pipeline compression and pumped-storage applications with ratings reaching 100 megawatts. Electric motors are found in industrial fans, blowers and pumps, machine tools, household appliances, power tools and disk drives. Small motors may be found in electric watches. \n\nIn certain applications, such as in regenerative braking with traction motors, electric motors can be used in reverse as generators to recover energy that might otherwise be lost as heat and friction.\n\nElectric motors produce linear or rotary force (torque) and can be distinguished from devices such as magnetic solenoids and loudspeakers that convert electricity into motion but do not generate usable mechanical force, which are respectively referred to as actuators and transducers.\n\nThe first electric motors were simple electrostatic devices described in experiments by Scottish monk Andrew Gordon and American experimenter Benjamin Franklin in the 1740s. The theoretical principle behind them, Ampère's force law, was discovered by André-Marie Ampère in 1820. The law described the production of mechanical force by the interactions of an electric current and a magnetic field. The conversion of electrical energy into mechanical energy by electromagnetic means was demonstrated by English scientist Michael Faraday in 1821. A free-hanging wire was dipped into a pool of mercury, on which a permanent magnet (PM) was placed. When a current was passed through the wire, the wire rotated around the magnet, showing that the current gave rise to a close circular magnetic field around the wire. This motor is often demonstrated in physics experiments, substituting brine for (toxic) mercury. Though Barlow's wheel was an early refinement to this Faraday demonstration, these and similar homopolar motors remained unsuited to practical application until late in the century.\nIn 1827, Hungarian physicist Ányos Jedlik started experimenting with electromagnetic coils. After Jedlik solved the technical problems of continuous rotation with the invention of the commutator, he called his early devices \"electromagnetic self-rotors\". Although they were used only for teaching, in 1828 Jedlik demonstrated the first device to contain the three main components of practical DC motors: the stator, rotor and commutator. The device employed no permanent magnets, as the magnetic fields of both the stationary and revolving components were produced solely by the currents flowing through their windings.\n\nAfter many other more or less successful attempts with relatively weak rotating and reciprocating apparatus Prussian Moritz von Jacobi created the first real rotating electric motor in May 1834. It developed remarkable mechanical output power. His motor set a world record, which Jacobi improved four years later in September 1838. His second motor was powerful enough to drive a boat with 14 people across a wide river. It was also in 1839/40 that other developers managed to build motors with similar and then higher performance.\n\nThe first commutator capable of turning machinery was invented by British scientist William Sturgeon in 1832. Following Sturgeon's work, a commutator-type direct-current electric motor was built by American inventor Thomas Davenport, which he patented in 1837. The motors ran at up to 600 revolutions per minute, and powered machine tools and a printing press. Due to the high cost of primary battery power, the motors were commercially unsuccessful and bankrupted Davenport. Several inventors followed Sturgeon in the development of DC motors, but all encountered the same battery cost issues. No electricity distribution system was available at the time. No practical commercial market emerged for these motors.\n\nIn 1855, Jedlik built a device using similar principles to those used in his electromagnetic self-rotors that was capable of useful work. He built a model electric vehicle that same year.\n\nA major turning point came in 1864, when Antonio Pacinotti first described the ring armature. This featured symmetrically-grouped coils closed upon themselves and connected to the bars of a commutator, the brushes of which delivered practically non-fluctuating current. The first commercially successful DC motors followed the invention by Zénobe Gramme who, in 1871, reinvented Pacinotti's design. In 1873, Gramme showed that his dynamo could be used as a motor, by connecting two such DC motors up to 2 km from each other, using one as a generator. \n\nIn 1886, Frank Julian Sprague invented the first practical DC motor, a non-sparking device that maintained relatively constant speed under variable loads. Other Sprague electric inventions about this time greatly improved grid electric distribution (prior work done while employed by Thomas Edison), allowed power from electric motors to be returned to the electric grid, provided for electric distribution to trolleys via overhead wires and the trolley pole, and provided control systems for electric operations. This allowed Sprague to use electric motors to invent the first electric trolley system in 1887–88 in Richmond, Virginia, the electric elevator and control system in 1892, and the electric subway with independently-powered centrally-controlled cars. The latter were first installed in 1892 in Chicago by the South Side Elevated Railroad, where it became popularly known as the \"L\". Sprague's motor and related inventions led to an explosion of interest and use in electric motors for industry. The development of electric motors of acceptable efficiency was delayed for several decades by failure to recognize the extreme importance of an air gap between the rotor and stator. Efficient designs have a comparatively small air gap. The St. Louis motor, long used in classrooms to illustrate motor principles, is extremely inefficient for the same reason, as well as appearing nothing like a modern motor.\n\nElectric motors revolutionized industry. Industrial processes were no longer limited by power transmission using line shafts, belts, compressed air or hydraulic pressure. Instead, every machine could be equipped with its own power source, providing easy control at the point of use, and improving power transmission efficiency. Electric motors applied in agriculture eliminated human and animal muscle power from such tasks as handling grain or pumping water. Household uses of electric motors reduced heavy labor in the home and made higher standards of convenience, comfort and safety possible. Today, electric motors consume more than half of the electric energy produced in the US.\n\nIn 1824 French physicist François Arago formulated the existence of rotating magnetic fields, termed Arago's rotations, which, by manually turning switches on and off, Walter Baily demonstrated in 1879 as in effect the first primitive induction motor.In the 1880s many inventors were trying to develop workable AC motors because AC's advantages in long-distance high-voltage transmission were offset by the inability to operate motors on AC. The first alternating-current commutatorless induction motors were independently invented by Galileo Ferraris and Nikola Tesla, in 1885 and 1887, respectively. In 1888, the \"Royal Academy of Science of Turin\" published Ferraris's research detailing the foundations of motor operation, while concluding that \"the apparatus based on that principle could not be of any commercial importance as motor.\"\n\nIn 1888, Tesla presented his paper \"A New System for Alternating Current Motors and Transformers\" to the AIEE that described three patented two-phase four-stator-pole motor types: one with a four-pole rotor forming a non-self-starting reluctance motor, another with a wound rotor forming a self-starting induction motor, and the third a true synchronous motor with separately excited DC supply to rotor winding.\n\nOne of the patents Tesla filed in 1887, however, also described a shorted-winding-rotor induction motor. George Westinghouse promptly bought Tesla's patents, employed Tesla to develop them, and assigned C. F. Scott to help Tesla; however, Tesla left for other pursuits in 1889. The constant speed AC induction motor was found not to be suitable for street cars, but Westinghouse engineers successfully adapted it to power a mining operation in Telluride, Colorado in 1891.\n\nSteadfast in his promotion of three-phase development, Mikhail Dolivo-Dobrovolsky invented the three-phase cage-rotor induction motor in 1889 and the three-limb transformer in 1890. This type of motor is now used for the vast majority of commercial applications. However, he claimed that Tesla's motor was not practical because of two-phase pulsations, which prompted him to persist in his three-phase work. Although Westinghouse achieved its first practical induction motor in 1892 and developed a line of polyphase 60 hertz induction motors in 1893, these early Westinghouse motors were two-phase motors with wound rotors. B. G. Lamme later developed a rotating bar winding rotor.\n\nThe General Electric Company began developing three-phase induction motors in 1891. By 1896, General Electric and Westinghouse signed a cross-licensing agreement for the bar-winding-rotor design, later called the squirrel-cage rotor. Induction motor improvements flowing from these inventions and innovations were such that a 100-horsepower (HP) induction motor currently has the same mounting dimensions as a 7.5 HP motor in 1897.\n\nIn an electric motor, the moving part is the rotor, which turns the shaft to deliver the mechanical power. The rotor usually has conductors laid into it that carry currents, which interact with the magnetic field of the stator to generate the forces that turn the shaft. Alternatively, some rotors carry permanent magnets, and the stator holds the conductors.\n\nThe rotor is supported by bearings, which allow the rotor to turn on its axis. The bearings are in turn supported by the motor housing. The motor shaft extends through the bearings to the outside of the motor, where the load is applied. Because the forces of the load are exerted beyond the outermost bearing, the load is said to be \"overhung\".\n\nThe stator is the stationary part of the motor’s electromagnetic circuit and usually consists of either windings or permanent magnets. The stator core is made up of many thin metal sheets, called laminations. Laminations are used to reduce energy losses that would result if a solid core were used.\n\nThe distance between the rotor and stator is called the air gap. The air gap has important effects, and is generally as small as possible, as a large gap has a strong negative effect on performance. It is the main source of the low power factor at which motors operate. The magnetizing current increases with the air gap. For this reason, the air gap should be minimal. Very small gaps may pose mechanical problems in addition to noise and losses.\n\nWindings are wires that are laid in coils, usually wrapped around a laminated soft iron magnetic core so as to form magnetic poles when energized with current.\n\nElectric machines come in two basic magnet field pole configurations: \"salient-\" and \"nonsalient-pole\" configurations. In the salient-pole machine the pole's magnetic field is produced by a winding wound around the pole below the pole face. In the \"nonsalient-pole\", or distributed field, or round-rotor, machine, the winding is distributed in pole face slots. A shaded-pole motor has a winding around part of the pole that delays the phase of the magnetic field for that pole.\n\nSome motors have conductors that consist of thicker metal, such as bars or sheets of metal, usually copper, alternatively aluminum. These are usually powered by electromagnetic induction.\n\nA commutator is a mechanism used to switch the input of most DC machines and certain AC machines. It consists of slip-ring segments insulated from each other and from the shaft. The motor's armature current is supplied through stationary brushes in contact with the revolving commutator, which causes required current reversal, and applies power to the machine in an optimal manner as the rotor rotates from pole to pole. In absence of such current reversal, the motor would brake to a stop. In light of improved technologies in the electronic-controller, sensorless-control, induction-motor, and permanent-magnet-motor fields, externally-commutated induction and permanent-magnet motors are displacing electromechanically-commutated motors.\n\nA DC motor is usually supplied through slip ring commutator as described above. AC motors' commutation can be either slip ring commutator or externally commutated type, can be fixed-speed or variable-speed control type, and can be synchronous or asynchronous type. Universal motors can run on either AC or DC.\n\nFixed-speed controlled AC motors are provided with direct-on-line or soft-start starters.\n\nVariable-speed controlled AC motors are provided with a range of different power inverter, variable-frequency drive or electronic commutator technologies.\n\nThe term electronic commutator is usually associated with self-commutated brushless DC motor and switched reluctance motor applications.\n\nElectric motors operate on three different physical principles: magnetism, electrostatics and piezoelectricity. By far, the most common is magnetism.\n\nIn magnetic motors, magnetic fields are formed in both the rotor and the stator. The product between these two fields gives rise to a force, and thus a torque on the motor shaft. One, or both, of these fields must be made to change with the rotation of the motor. This is done by switching the poles on and off at the right time, or varying the strength of the pole.\n\nThe main types are DC motors and AC motors, the former increasingly being displaced by the latter. \n\nAC electric motors are either asynchronous or synchronous.\n\nOnce started, a synchronous motor requires synchronism with the moving magnetic field's synchronous speed for all normal torque conditions.\n\nIn synchronous machines, the magnetic field must be provided by means other than induction such as from separately excited windings or permanent magnets.\n\nA fractional-horsepower (FHP) motor either has a rating below about 1 horsepower (0.746 kW), or is manufactured with a standard-frame size smaller than a standard 1 HP motor. Many household and industrial motors are in the fractional-horsepower class.\nNotes:\n\nAbbreviations:\n\nBy definition, all self-commutated DC motors run on DC electric power. Most DC motors are small permanent magnet (PM) types. They contain a brushed internal mechanical commutation to reverse motor windings' current in synchronism with rotation.\n\nA commutated DC motor has a set of rotating windings wound on an armature mounted on a rotating shaft. The shaft also carries the commutator, a long-lasting rotary electrical switch that periodically reverses the flow of current in the rotor windings as the shaft rotates. Thus, every brushed DC motor has AC flowing through its rotating windings. Current flows through one or more pairs of brushes that bear on the commutator; the brushes connect an external source of electric power to the rotating armature.\n\nThe rotating armature consists of one or more coils of wire wound around a laminated, magnetically \"soft\" ferromagnetic core. Current from the brushes flows through the commutator and one winding of the armature, making it a temporary magnet (an electromagnet). The magnetic field produced by the armature interacts with a stationary magnetic field produced by either PMs or another winding (a field coil), as part of the motor frame. The force between the two magnetic fields tends to rotate the motor shaft. The commutator switches power to the coils as the rotor turns, keeping the magnetic poles of the rotor from ever fully aligning with the magnetic poles of the stator field, so that the rotor never stops (like a compass needle does), but rather keeps rotating as long as power is applied.\n\nMany of the limitations of the classic commutator DC motor are due to the need for brushes to press against the commutator. This creates friction. Sparks are created by the brushes making and breaking circuits through the rotor coils as the brushes cross the insulating gaps between commutator sections. Depending on the commutator design, this may include the brushes shorting together adjacent sections – and hence coil ends – momentarily while crossing the gaps. Furthermore, the inductance of the rotor coils causes the voltage across each to rise when its circuit is opened, increasing the sparking of the brushes. This sparking limits the maximum speed of the machine, as too-rapid sparking will overheat, erode, or even melt the commutator. The current density per unit area of the brushes, in combination with their resistivity, limits the output of the motor. The making and breaking of electric contact also generates electrical noise; sparking generates RFI. Brushes eventually wear out and require replacement, and the commutator itself is subject to wear and maintenance (on larger motors) or replacement (on small motors). The commutator assembly on a large motor is a costly element, requiring precision assembly of many parts. On small motors, the commutator is usually permanently integrated into the rotor, so replacing it usually requires replacing the whole rotor.\n\nWhile most commutators are cylindrical, some are flat discs consisting of several segments (typically, at least three) mounted on an insulator.\n\nLarge brushes are desired for a larger brush contact area to maximize motor output, but small brushes are desired for low mass to maximize the speed at which the motor can run without the brushes excessively bouncing and sparking. (Small brushes are also desirable for lower cost.) Stiffer brush springs can also be used to make brushes of a given mass work at a higher speed, but at the cost of greater friction losses (lower efficiency) and accelerated brush and commutator wear. Therefore, DC motor brush design entails a trade-off between output power, speed, and efficiency/wear.\n\nDC machines are defined as follows:\nThere are five types of brushed DC motor:-\n\nA PM (permanent magnet) motor does not have a field winding on the stator frame, instead relying on PMs to provide the magnetic field against which the rotor field interacts to produce torque. Compensating windings in series with the armature may be used on large motors to improve commutation under load. Because this field is fixed, it cannot be adjusted for speed control. PM fields (stators) are convenient in miniature motors to eliminate the power consumption of the field winding. Most larger DC motors are of the \"dynamo\" type, which have stator windings. Historically, PMs could not be made to retain high flux if they were disassembled; field windings were more practical to obtain the needed amount of flux. However, large PMs are costly, as well as dangerous and difficult to assemble; this favors wound fields for large machines.\n\nTo minimize overall weight and size, miniature PM motors may use high energy magnets made with neodymium or other strategic elements; most such are neodymium-iron-boron alloy. With their higher flux density, electric machines with high-energy PMs are at least competitive with all optimally designed singly-fed synchronous and induction electric machines. Miniature motors resemble the structure in the illustration, except that they have at least three rotor poles (to ensure starting, regardless of rotor position) and their outer housing is a steel tube that magnetically links the exteriors of the curved field magnets.\n\nSome of the problems of the brushed DC motor are eliminated in the BLDC design. In this motor, the mechanical \"rotating switch\" or commutator is replaced by an external electronic switch synchronised to the rotor's position. BLDC motors are typically 85–90% efficient or more. Efficiency for a BLDC motor of up to 96.5% have been reported, whereas DC motors with brushgear are typically 75–80% efficient.\n\nThe BLDC motor's characteristic trapezoidal counter-electromotive force (CEMF) waveform is derived partly from the stator windings being evenly distributed, and partly from the placement of the rotor's permanent magnets. Also known as electronically commutated DC or inside out DC motors, the stator windings of trapezoidal BLDC motors can be with single-phase, two-phase or three-phase and use Hall effect sensors mounted on their windings for rotor position sensing and low cost closed-loop control of the electronic commutator.\n\nBLDC motors are commonly used where precise speed control is necessary, as in computer disk drives or in video cassette recorders, the spindles within CD, CD-ROM (etc.) drives, and mechanisms within office products, such as fans, laser printers and photocopiers. They have several advantages over conventional motors:\nModern BLDC motors range in power from a fraction of a watt to many kilowatts. Larger BLDC motors up to about 100 kW rating are used in electric vehicles. They also find significant use in high-performance electric model aircraft.\n\nThe SRM has no brushes or permanent magnets, and the rotor has no electric currents.\nInstead, torque comes from a slight misalignment of poles on the rotor with poles on the stator.\nThe rotor aligns itself with the magnetic field of the stator, while the stator field windings are sequentially energized to rotate the stator field.\n\nThe magnetic flux created by the field windings follows the path of least magnetic reluctance, meaning the flux will flow through poles of the rotor that are closest to the energized poles of the stator, thereby magnetizing those poles of the rotor and creating torque. As the rotor turns, different windings will be energized, keeping the rotor turning.\n\nSRMs are used in some appliances and vehicles.\n\nA commutated electrically excited series or parallel wound motor is referred to as a universal motor because it can be designed to operate on AC or DC power. A universal motor can operate well on AC because the current in both the field and the armature coils (and hence the resultant magnetic fields) will alternate (reverse polarity) in synchronism, and hence the resulting mechanical force will occur in a constant direction of rotation.\n\nOperating at normal power line frequencies, universal motors are often found in a range less than . Universal motors also formed the basis of the traditional railway traction motor in electric railways. In this application, the use of AC to power a motor originally designed to run on DC would lead to efficiency losses due to eddy current heating of their magnetic components, particularly the motor field pole-pieces that, for DC, would have used solid (un-laminated) iron and they are now rarely used.\n\nAn advantage of the universal motor is that AC supplies may be used on motors that have some characteristics more common in DC motors, specifically high starting torque and very compact design if high running speeds are used. The negative aspect is the maintenance and short life problems caused by the commutator. Such motors are used in devices, such as food mixers and power tools, that are used only intermittently, and often have high starting-torque demands. Multiple taps on the field coil provide (imprecise) stepped speed control. Household blenders that advertise many speeds frequently combine a field coil with several taps and a diode that can be inserted in series with the motor (causing the motor to run on half-wave rectified AC). Universal motors also lend themselves to electronic speed control and, as such, are an ideal choice for devices like domestic washing machines. The motor can be used to agitate the drum (both forwards and in reverse) by switching the field winding with respect to the armature.\n\nWhereas SCIMs cannot turn a shaft faster than allowed by the power line frequency, universal motors can run at much higher speeds. This makes them useful for appliances such as blenders, vacuum cleaners, and hair dryers where high speed and light weight are desirable. They are also commonly used in portable power tools, such as drills, sanders, circular and jig saws, where the motor's characteristics work well. Many vacuum cleaner and weed trimmer motors exceed , while many similar miniature grinders exceed .\n\nThe design of AC induction and synchronous motors is optimized for operation on single-phase or polyphase sinusoidal or quasi-sinusoidal waveform power such as supplied for fixed-speed application from the AC power grid or for variable-speed application from VFD controllers. An AC motor has two parts: a stationary stator having coils supplied with AC to produce a rotating magnetic field, and a rotor attached to the output shaft that is given a torque by the rotating field.\n\nAn induction motor is an asynchronous AC motor where power is transferred to the rotor by electromagnetic induction, much like transformer action. An induction motor resembles a rotating transformer, because the stator (stationary part) is essentially the primary side of the transformer and the rotor (rotating part) is the secondary side. Polyphase induction motors are widely used in industry.\n\nInduction motors may be further divided into Squirrel Cage Induction Motors and Wound Rotor Induction Motors (WRIMs). SCIMs have a heavy winding made up of solid bars, usually aluminum or copper, joined by rings at the ends of the rotor. When one considers only the bars and rings as a whole, they are much like an animal's rotating exercise cage, hence the name.\n\nCurrents induced into this winding provide the rotor magnetic field. The shape of the rotor bars determines the speed-torque characteristics. At low speeds, the current induced in the squirrel cage is nearly at line frequency and tends to be in the outer parts of the rotor cage. As the motor accelerates, the slip frequency becomes lower, and more current is in the interior of the winding. By shaping the bars to change the resistance of the winding portions in the interior and outer parts of the cage, effectively a variable resistance is inserted in the rotor circuit. However, the majority of such motors have uniform bars.\n\nIn a WRIM, the rotor winding is made of many turns of insulated wire and is connected to slip rings on the motor shaft. An external resistor or other control devices can be connected in the rotor circuit. Resistors allow control of the motor speed, although significant power is dissipated in the external resistance. A converter can be fed from the rotor circuit and return the slip-frequency power that would otherwise be wasted back into the power system through an inverter or separate motor-generator.\n\nThe WRIM is used primarily to start a high inertia load or a load that requires a very high starting torque across the full speed range. By correctly selecting the resistors used in the secondary resistance or slip ring starter, the motor is able to produce maximum torque at a relatively low supply current from zero speed to full speed. This type of motor also offers controllable speed.\n\nMotor speed can be changed because the torque curve of the motor is effectively modified by the amount of resistance connected to the rotor circuit. Increasing the value of resistance will move the speed of maximum torque down. If the resistance connected to the rotor is increased beyond the point where the maximum torque occurs at zero speed, the torque will be further reduced.\n\nWhen used with a load that has a torque curve that increases with speed, the motor will operate at the speed where the torque developed by the motor is equal to the load torque. Reducing the load will cause the motor to speed up, and increasing the load will cause the motor to slow down until the load and motor torque are equal. Operated in this manner, the slip losses are dissipated in the secondary resistors and can be very significant. The speed regulation and net efficiency is also very poor.\n\nA torque motor is a specialized form of electric motor that can operate indefinitely while stalled, that is, with the rotor blocked from turning, without incurring damage. In this mode of operation, the motor will apply a steady torque to the load (hence the name).\n\nA common application of a torque motor would be the supply- and take-up reel motors in a tape drive. In this application, driven from a low voltage, the characteristics of these motors allow a relatively constant light tension to be applied to the tape whether or not the capstan is feeding tape past the tape heads. Driven from a higher voltage, (and so delivering a higher torque), the torque motors can also achieve fast-forward and rewind operation without requiring any additional mechanics such as gears or clutches. In the computer gaming world, torque motors are used in force feedback steering wheels.\n\nAnother common application is the control of the throttle of an internal combustion engine in conjunction with an electronic governor. In this usage, the motor works against a return spring to move the throttle in accordance with the output of the governor. The latter monitors engine speed by counting electrical pulses from the ignition system or from a magnetic pickup and, depending on the speed, makes small adjustments to the amount of current applied to the motor. If the engine starts to slow down relative to the desired speed, the current will be increased, the motor will develop more torque, pulling against the return spring and opening the throttle. Should the engine run too fast, the governor will reduce the current being applied to the motor, causing the return spring to pull back and close the throttle.\n\nA synchronous electric motor is an AC motor distinguished by a rotor spinning with coils passing magnets at the same rate as the AC and resulting in a magnetic field that drives it. Another way of saying this is that it has zero slip under usual operating conditions. Contrast this with an induction motor, which must slip to produce torque. One type of synchronous motor is like an induction motor except the rotor is excited by a DC field. Slip rings and brushes are used to conduct current to the rotor. The rotor poles connect to each other and move at the same speed hence the name synchronous motor. Another type, for low load torque, has flats ground onto a conventional squirrel-cage rotor to create discrete poles. Yet another, such as made by Hammond for its pre-World War II clocks, and in the older Hammond organs, has no rotor windings and discrete poles. It is not self-starting. The clock requires manual starting by a small knob on the back, while the older Hammond organs had an auxiliary starting motor connected by a spring-loaded manually operated switch.\n\nFinally, hysteresis synchronous motors typically are (essentially) two-phase motors with a phase-shifting capacitor for one phase. They start like induction motors, but when slip rate decreases sufficiently, the rotor (a smooth cylinder) becomes temporarily magnetized. Its distributed poles make it act like a permanent magnet synchronous motor (PMSM). The rotor material, like that of a common nail, will stay magnetized, but can also be demagnetized with little difficulty. Once running, the rotor poles stay in place; they do not drift.\n\nLow-power synchronous timing motors (such as those for traditional electric clocks) may have multi-pole permanent magnet external cup rotors, and use shading coils to provide starting torque. \"Telechron\" clock motors have shaded poles for starting torque, and a two-spoke ring rotor that performs like a discrete two-pole rotor.\n\nDoubly fed electric motors have two independent multiphase winding sets, which contribute active (i.e., working) power to the energy conversion process, with at least one of the winding sets electronically controlled for variable speed operation. Two independent multiphase winding sets (i.e., dual armature) are the maximum provided in a single package without topology duplication. Doubly-fed electric motors are machines with an effective constant torque speed range that is twice synchronous speed for a given frequency of excitation. This is twice the constant torque speed range as singly-fed electric machines, which have only one active winding set.\n\nA doubly-fed motor allows for a smaller electronic converter but the cost of the rotor winding and slip rings may offset the saving in the power electronics components. Difficulties with controlling speed near synchronous speed limit applications.\n\nNothing in the principle of any of the motors described above requires that the iron (steel) portions of the rotor actually rotate. If the soft magnetic material of the rotor is made in the form of a cylinder, then (except for the effect of hysteresis) torque is exerted only on the windings of the electromagnets. Taking advantage of this fact is the \"coreless or ironless DC motor\", a specialized form of a permanent magnet DC motor. Optimized for rapid acceleration, these motors have a rotor that is constructed without any iron core. The rotor can take the form of a winding-filled cylinder, or a self-supporting structure comprising only the magnet wire and the bonding material. The rotor can fit inside the stator magnets; a magnetically soft stationary cylinder inside the rotor provides a return path for the stator magnetic flux. A second arrangement has the rotor winding basket surrounding the stator magnets. In that design, the rotor fits inside a magnetically soft cylinder that can serve as the housing for the motor, and likewise provides a return path for the flux.\n\nBecause the rotor is much lighter in weight (mass) than a conventional rotor formed from copper windings on steel laminations, the rotor can accelerate much more rapidly, often achieving a mechanical time constant under one ms. This is especially true if the windings use aluminum rather than the heavier copper. But because there is no metal mass in the rotor to act as a heat sink, even small coreless motors must often be cooled by forced air. Overheating might be an issue for coreless DC motor designs. Modern software, such as Motor-CAD, can help to increase the thermal efficiency of motors while still in the design stage.\n\nAmong these types are the disc-rotor types, described in more detail in the next section.\n\nThe vibrating alert of cellular phones is sometimes generated by tiny cylindrical permanent-magnet field types, but there are also disc-shaped types that have a thin multipolar disc field magnet, and an intentionally unbalanced molded-plastic rotor structure with two bonded coreless coils. Metal brushes and a flat commutator switch power to the rotor coils.\n\nRelated limited-travel actuators have no core and a bonded coil placed between the poles of high-flux thin permanent magnets. These are the fast head positioners for rigid-disk (\"hard disk\") drives. Although the contemporary design differs considerably from that of loudspeakers, it is still loosely (and incorrectly) referred to as a \"voice coil\" structure, because some earlier rigid-disk-drive heads moved in straight lines, and had a drive structure much like that of a loudspeaker.\n\nThe printed armature or pancake motor has the windings shaped as a disc running between arrays of high-flux magnets. The magnets are arranged in a circle facing the rotor with space in between to form an axial air gap. This design is commonly known as the pancake motor because of its flat profile. The technology has had many brand names since its inception, such as ServoDisc.\n\nThe printed armature (originally formed on a printed circuit board) in a printed armature motor is made from punched copper sheets that are laminated together using advanced composites to form a thin rigid disc. The printed armature has a unique construction in the brushed motor world in that it does not have a separate ring commutator. The brushes run directly on the armature surface making the whole design very compact.\n\nAn alternative manufacturing method is to use wound copper wire laid flat with a central conventional commutator, in a flower and petal shape. The windings are typically stabilized with electrical epoxy potting systems. These are filled epoxies that have moderate, mixed viscosity and a long gel time. They are highlighted by low shrinkage and low exotherm, and are typically UL 1446 recognized as a potting compound insulated with 180 °C, Class H rating.\n\nThe unique advantage of ironless DC motors is the absence of cogging (torque variations caused by changing attraction between the iron and the magnets). Parasitic eddy currents cannot form in the rotor as it is totally ironless, although iron rotors are laminated. This can greatly improve efficiency, but variable-speed controllers must use a higher switching rate (>40 kHz) or DC because of decreased electromagnetic induction.\n\nThese motors were originally invented to drive the capstan(s) of magnetic tape drives, where minimal time to reach operating speed and minimal stopping distance were critical. Pancake motors are widely used in high-performance servo-controlled systems, robotic systems, industrial automation and medical devices. Due to the variety of constructions now available, the technology is used in applications from high temperature military to low cost pump and basic servos.\n\nAnother approach (Magnax) is to use a single stator sandwiched between two rotors. One such design has produced peak power of 15 kW/kg, sustained power around 7.5 kW/kg. This yokeless axial flux motor offers a shorter flux path, keeping the magnets further from the axis. The design allows zero winding overhang; 100 percent of the windings are active. This is enhanced with the use of rectangular-section copper wire. The motors can be stacked to work in parallel. Instabilities are minimized by ensuring that the two rotor discs put equal and opposing forces onto the stator disc. The rotors are connected directly to one another via a shaft ring, cancelling out the magnetic forces.\n\nMagnax motors range in size from in diameter.\n\nA servomotor is a motor, very often sold as a complete module, which is used within a position-control or speed-control feedback control system. Servomotors are used in applications such as machine tools, pen plotters, and other process systems. Motors intended for use in a servomechanism must have well-documented characteristics for speed, torque, and power. The speed vs. torque curve is quite important and is high ratio for a servo motor. Dynamic response characteristics such as winding inductance and rotor inertia are also important; these factors limit the overall performance of the servomechanism loop. Large, powerful, but slow-responding servo loops may use conventional AC or DC motors and drive systems with position or speed feedback on the motor. As dynamic response requirements increase, more specialized motor designs such as coreless motors are used. AC motors' superior power density and acceleration characteristics compared to that of DC motors tends to favor permanent magnet synchronous, BLDC, induction, and SRM drive applications.\n\nA servo system differs from some stepper motor applications in that the position feedback is continuous while the motor is running. A stepper system inherently operates open-loop - relying on the motor not to \"miss steps\" for short term accuracy - with any feedback such as a \"home\" switch or position encoder being external to the motor system. For instance, when a typical dot matrix computer printer starts up, its controller makes the print head stepper motor drive to its left-hand limit, where a position sensor defines home position and stops stepping. As long as power is on, a bidirectional counter in the printer's microprocessor keeps track of print-head position.\n\nStepper motors are a type of motor frequently used when precise rotations are required. In a stepper motor an internal rotor containing permanent magnets or a magnetically soft rotor with salient poles is controlled by a set of external magnets that are switched electronically. A stepper motor may also be thought of as a cross between a DC electric motor and a rotary solenoid. As each coil is energized in turn, the rotor aligns itself with the magnetic field produced by the energized field winding. Unlike a synchronous motor, in its application, the stepper motor may not rotate continuously; instead, it \"steps\"—starts and then quickly stops again—from one position to the next as field windings are energized and de-energized in sequence. Depending on the sequence, the rotor may turn forwards or backwards, and it may change direction, stop, speed up or slow down arbitrarily at any time.\n\nSimple stepper motor drivers entirely energize or entirely de-energize the field windings, leading the rotor to \"cog\" to a limited number of positions; more sophisticated drivers can proportionally control the power to the field windings, allowing the rotors to position between the cog points and thereby rotate extremely smoothly. This mode of operation is often called microstepping. Computer controlled stepper motors are one of the most versatile forms of positioning systems, particularly when part of a digital servo-controlled system.\n\nStepper motors can be rotated to a specific angle in discrete steps with ease, and hence stepper motors are used for read/write head positioning in computer floppy diskette drives. They were used for the same purpose in pre-gigabyte era computer disk drives, where the precision and speed they offered was adequate for the correct positioning of the read/write head of a hard disk drive. As drive density increased, the precision and speed limitations of stepper motors made them obsolete for hard drives—the precision limitation made them unusable, and the speed limitation made them uncompetitive—thus newer hard disk drives use voice coil-based head actuator systems. (The term \"voice coil\" in this connection is historic; it refers to the structure in a typical (cone type) loudspeaker. This structure was used for a while to position the heads. Modern drives have a pivoted coil mount; the coil swings back and forth, something like a blade of a rotating fan. Nevertheless, like a voice coil, modern actuator coil conductors (the magnet wire) move perpendicular to the magnetic lines of force.)\n\nStepper motors were and still are often used in computer printers, optical scanners, and digital photocopiers to move the optical scanning element, the print head carriage (of dot matrix and inkjet printers), and the platen or feed rollers. Likewise, many computer plotters (which since the early 1990s have been replaced with large-format inkjet and laser printers) used rotary stepper motors for pen and platen movement; the typical alternatives here were either linear stepper motors or servomotors with closed-loop analog control systems.\n\nSo-called quartz analog wristwatches contain the smallest commonplace stepping motors; they have one coil, draw very little power, and have a permanent magnet rotor. The same kind of motor drives battery-powered quartz clocks. Some of these watches, such as chronographs, contain more than one stepping motor.\n\nClosely related in design to three-phase AC synchronous motors, stepper motors and SRMs are classified as variable reluctance motor type. Stepper motors were and still are often used in computer printers, optical scanners, and computer numerical control (CNC) machines such as routers, plasma cutters and CNC lathes.\n\nA linear motor is essentially any electric motor that has been \"unrolled\" so that, instead of producing a torque (rotation), it produces a straight-line force along its length.\n\nLinear motors are most commonly induction motors or stepper motors. Linear motors are commonly found in many roller-coasters where the rapid motion of the motorless railcar is controlled by the rail. They are also used in maglev trains, where the train \"flies\" over the ground. On a smaller scale, the 1978 era HP 7225A pen plotter used two linear stepper motors to move the pen along the X and Y axes.\n\nThe fundamental purpose of the vast majority of the world's electric motors is to electromagnetically induce relative movement in an air gap between a stator and rotor to produce useful torque or linear force.\n\nAccording to Lorentz force law the force of a winding conductor can be given simply by:\nor more generally, to handle conductors with any geometry:\nThe most general approaches to calculating the forces in motors use tensors.\n\nWhere rpm is shaft speed and T is torque, a motor's mechanical power output P is given by,\n\nin British units with T expressed in foot-pounds,\nin SI units with shaft angular speed expressed in radians per second, and T expressed in newton-meters,\n\nFor a linear motor, with force F expressed in newtons and velocity v expressed in meters per second,\n\nIn an asynchronous or induction motor, the relationship between motor speed and air gap power is, neglecting skin effect, given by the following:\n\nSince the armature windings of a direct-current or universal motor are moving through a magnetic field, they have a voltage induced in them. This voltage tends to oppose the motor supply voltage and so is called \"back electromotive force (emf)\". The voltage is proportional to the running speed of the motor. The back emf of the motor, plus the voltage drop across the winding internal resistance and brushes, must equal the voltage at the brushes. This provides the fundamental mechanism of speed regulation in a DC motor. If the mechanical load increases, the motor slows down; a lower back emf results, and more current is drawn from the supply. This increased current provides the additional torque to balance the new load.\n\nIn AC machines, it is sometimes useful to consider a back emf source within the machine; as an example, this is of particular concern for close speed regulation of induction motors on VFDs.\n\nMotor losses are mainly due to resistive losses in windings, core losses and mechanical losses in bearings, and aerodynamic losses, particularly where cooling fans are present, also occur.\n\nLosses also occur in commutation, mechanical commutators spark, and electronic commutators and also dissipate heat.\n\nTo calculate a motor's efficiency, the mechanical output power is divided by the electrical input power:\nwhere formula_8 is energy conversion efficiency, formula_9 is electrical input power, and formula_10 is mechanical output power:\n\nwhere formula_13 is input voltage, formula_14 is input current, formula_15 is output torque, and formula_16 is output angular velocity. It is possible to derive analytically the point of maximum efficiency. It is typically at less than 1/2 the stall torque.\n\nVarious regulatory authorities in many countries have introduced and implemented legislation to encourage the manufacture and use of higher-efficiency electric motors.\n\nEric Laithwaite proposed a metric to determine the 'goodness' of an electric motor:\nformula_17\n\nWhere:\n\nFrom this, he showed that the most efficient motors are likely to have relatively large magnetic poles. However, the equation only directly relates to non PM motors.\n\nAll the electromagnetic motors, and that includes the types mentioned here derive the torque from the vector product of the interacting fields. For calculating the torque it is necessary to know the fields in the air gap. Once these have been established by mathematical analysis using FEA or other tools the torque may be calculated as the integral of all the vectors of force multiplied by the radius of each vector. The current flowing in the winding is producing the fields and for a motor using a magnetic material the field is not linearly proportional to the current. This makes the calculation difficult but a computer can do the many calculations needed.\n\nOnce this is done a figure relating the current to the torque can be used as a useful parameter for motor selection. The maximum torque for a motor will depend on the maximum current although this will usually be only usable until thermal considerations take precedence.\n\nWhen optimally designed within a given core saturation constraint and for a given active current (i.e., torque current), voltage, pole-pair number, excitation frequency (i.e., synchronous speed), and air-gap flux density, all categories of electric motors or generators will exhibit virtually the same maximum continuous shaft torque (i.e., operating torque) within a given air-gap area with winding slots and back-iron depth, which determines the physical size of electromagnetic core. Some applications require bursts of torque beyond the maximum operating torque, such as short bursts of torque to accelerate an electric vehicle from standstill. Always limited by magnetic core saturation or safe operating temperature rise and voltage, the capacity for torque bursts beyond the maximum operating torque differs significantly between categories of electric motors or generators.\n\nCapacity for bursts of torque should not be confused with field weakening capability. Field weakening allows an electric machine to operate beyond the designed frequency of excitation. Field weakening is done when the maximum speed cannot be reached by increasing the applied voltage. This applies to only motors with current controlled fields and therefore cannot be achieved with permanent magnet motors.\n\nElectric machines without a transformer circuit topology, such as that of WRSMs or PMSMs, cannot realize bursts of torque higher than the maximum designed torque without saturating the magnetic core and rendering any increase in current as useless. Furthermore, the permanent magnet assembly of PMSMs can be irreparably damaged, if bursts of torque exceeding the maximum operating torque rating are attempted.\n\nElectric machines with a transformer circuit topology, such as induction machines, induction doubly-fed electric machines, and induction or synchronous wound-rotor doubly-fed (WRDF) machines, exhibit very high bursts of torque because the emf-induced active current on either side of the transformer oppose each other and thus contribute nothing to the transformer coupled magnetic core flux density, which would otherwise lead to core saturation.\n\nElectric machines that rely on induction or asynchronous principles short-circuit one port of the transformer circuit and as a result, the reactive impedance of the transformer circuit becomes dominant as slip increases, which limits the magnitude of active (i.e., real) current. Still, bursts of torque that are two to three times higher than the maximum design torque are realizable.\n\nThe brushless wound-rotor synchronous doubly-fed (BWRSDF) machine is the only electric machine with a truly dual ported transformer circuit topology (i.e., both ports independently excited with no short-circuited port). The dual ported transformer circuit topology is known to be unstable and requires a multiphase slip-ring-brush assembly to propagate limited power to the rotor winding set. If a precision means were available to instantaneously control torque angle and slip for synchronous operation during motoring or generating while simultaneously providing brushless power to the rotor winding set, the active current of the BWRSDF machine would be independent of the reactive impedance of the transformer circuit and bursts of torque significantly higher than the maximum operating torque and far beyond the practical capability of any other type of electric machine would be realizable. Torque bursts greater than eight times operating torque have been calculated.\n\nThe continuous torque density of conventional electric machines is determined by the size of the air-gap area and the back-iron depth, which are determined by the power rating of the armature winding set, the speed of the machine, and the achievable air-gap flux density before core saturation. Despite the high coercivity of neodymium or samarium-cobalt permanent magnets, continuous torque density is virtually the same amongst electric machines with optimally designed armature winding sets. Continuous torque density relates to method of cooling and permissible period of operation before destruction by overheating of windings or permanent magnet damage.\n\nOther sources state that various e-machine topologies have differing torque density. One source shows the following:\nwhere — specific torque density is normalized to 1.0 for the SPM — brushless ac, 180° current conduction, SPM is Surface Permanent Magnet machine.\n\nTorque density is approximately four times greater for electric motors which are liquid cooled, as compared to those which are air cooled.\n\nA source comparing direct current (DC), induction motors (IM), permanent magnet synchronous motors (PMSM) and switched reluctance motors (SRM) showed:\nAnother source notes that permanent-magnet synchronous machines of up to 1 MW have considerably higher torque density than induction machines.\n\nThe continuous power density is determined by the product of the continuous torque density and the constant torque speed range of the electric machine.\n\nAcoustic noise and vibrations of electric motors are usually classified in three sources:\nThe latter source, which can be responsible for the \"whining noise\" of electric motors, is called electromagnetically-excited acoustic noise.\n\nThe following are major design, manufacturing, and testing standards covering electric motors:\n( Third Revision )\n\nAn electrostatic motor is based on the attraction and repulsion of electric charge. Usually, electrostatic motors are the dual of conventional coil-based motors. They typically require a high-voltage power supply, although very small motors employ lower voltages. Conventional electric motors instead employ magnetic attraction and repulsion, and require high current at low voltages. In the 1750s, the first electrostatic motors were developed by Benjamin Franklin and Andrew Gordon. Today, the electrostatic motor finds frequent use in micro-electro-mechanical systems (MEMS) where their drive voltages are below 100 volts, and where moving, charged plates are far easier to fabricate than coils and iron cores. Also, the molecular machinery that runs living cells is often based on linear and rotary electrostatic motors.\n\nA piezoelectric motor or piezo motor is a type of electric motor based upon the change in shape of a piezoelectric material when an electric field is applied. Piezoelectric motors make use of the converse piezoelectric effect whereby the material produces acoustic or ultrasonic vibrations to produce linear or rotary motion. In one mechanism, the elongation in a single plane is used to make a series stretches and position holds, similar to the way a caterpillar moves.\n\nAn electrically powered spacecraft propulsion system uses electric motor technology to propel spacecraft in outer space, most systems being based on electrically powering propellant to high speed, with some systems being based on electrodynamic tethers principles of propulsion to the magnetosphere.\n\n\n\n\n"}
{"id": "739365", "url": "https://en.wikipedia.org/wiki?curid=739365", "title": "Enlarger", "text": "Enlarger\n\nAn enlarger is a specialized transparency projector used to produce photographic prints from film or glass negatives, or from transparencies.\n\nAll enlargers consist of a light source, normally an incandescent light bulb, a condenser or translucent screen to provide even illumination, a holder for the negative or transparency, and a specialized lens for projection. The light passes through a film holder, which holds the exposed and developed photographic negative or transparency.\n\nPrints made with an enlarger are called \"enlargements\". Typically, enlargers are used in a darkroom, an enclosed space from which extraneous light may be excluded; some commercial enlargers have an integral dark box so that they can be used in a light-filled room.\n\nA condenser enlarger consists of a light source, a condensing lens, a holder for the negative and a projecting lens. The condenser provides even illumination to the negative beneath it.\n\nA diffuser enlarger's light source is diffused by translucent glass or plastic, providing even illumination for the film.\n\nCondenser enlargers produce higher contrast than diffusers because light is scattered from its path by the negative's image silver; this is called the Callier effect. The condenser's increased contrast emphasises any negative defects, such as dirt and scratches, and image grain.\n\nDiffuser enlargers produce an image of the same contrast as a contact print from the negative.\n\nDedicated color enlargers typically contain an adjustable filter mechanism - the color head - between the light source and the negative, enabling the user to adjust the amount of red, green and blue light reaching the negative to control color balance. Other models have a drawer where cut filters can be inserted into the light path, synthesize colour by additive mixing of light from colored lamps with adjustable intensity or duty cycle, or expose the receiving medium sequentially using red, green and blue light. These enlargers can also be used with variable-contrast monochrome papers.\n\nDigital enlargers project an image from an LCD screen at the film plane, to produce a photographic enlargement from a digital file.\n\nMost modern enlargers are vertically mounted with the head pointing downward and adjusted up or down to change the size of the image projected onto the enlarger's base, or a work table if the unit is mounted to the wall.\n\nA horizontal enlarger consists of a trestle, with the head mounted on crossbars between two or more posts for extra stability. A horizontal enlarger structure is used when high quality, large format enlargements are required such as when photographs are taken from aircraft for mapping and taxation purposes.\n\nThe parts of the enlarger include baseboard, enlarger head, elevation knob, filter holder, negative carrier, glass plate, focus knob, girder scale, timer, bellows, and housing lift.\n\nThe image from the negative or transparency is projected through a lens fitted with an adjustable iris aperture, onto a flat surface bearing the sensitized photographic paper. By adjusting the ratio of distance from film to lens to the distance from lens to paper, various degrees of enlargement may be obtained, with the physical enlargement ratio limited only by the structure of the enlarger and the size of the paper. As the image size is changed it is also necessary to change the focus of the lens. Some enlargers, such as Leica's \"Autofocus\" enlargers, perform this automatically.\n\nAn easel is used to hold the paper perfectly flat. Some easels are designed with adjustable overlapping flat steel \"blades\" to crop the image on the paper to the desired size while keeping an unexposed white border about the image. Paper is sometimes placed directly on the table or enlarger base, and held down flat with metal strips.\n\nThe enlargement is made by first focusing the image with the lamp on, the lens at maximum aperture and the easel empty, usually with the aid of a focus finder. The lamp is turned off, or in some cases, shuttered by a light-tight mechanism.\n\nThe image is focused by changing the distance between the lens and the film, achieved by adjusting the length of a light-tight bellows with a geared rack and pinion mechanism.\nThe lens is set to its working aperture. Enlarging lenses have an optimum range of apertures which yield a sharp image from corner to corner, which is 3 f/ stops smaller than the maximum aperture of the lens. For an enlarging lens with a maximum aperture of f/2.8, the optimal aperture would be f/8. The lens is normally set to this aperture and any color filtration dialed in, if making a color print or one on variable-contrast black-and-white paper.\n\nThe enlarger's lamp or shutter mechanism is controlled either by an electronic timer, or by the operator - who marks time with a clock, metronome or simply by counting seconds - shuttering or turning off the lamp when the exposure is complete. The exposed paper can be processed immediately or placed in a light-tight container for later processing.\n\nDigitally controlled commercial enlargers typically adjust exposure in steps known as printer points; twelve printer points makes a factor of two change in exposure.\n\nAfter exposure, photographic paper is developed, fixed, washed and dried using the gelatin silver process.\n\nAutomated photo print machines have the same basic elements and integrate each of the steps outlined above in a single complex machine under operator and computer control.\n\nRather than project directly from the film negative to the print paper, a digital image may first be captured from the negative. This allows the operator or computer to quickly determine adjustments to brightness, contrast, clipping, and other characteristics. The image is then rendered by passing light through the negative and a built-in computer-controlled enlarger optically projects this image to the paper for final exposure.\n\nAs a byproduct of the process a compact disc recording may be made of the digital images, although a subsequent print made from these may be quite inferior to an image made from the negative due to digitization noise and lack of dynamic range which are characteristics of the digitizing process.\n\nFor better images, the negatives may be reprinted using the same automated machine under operator selection of the print to be made.\n\n\nThe practical amount of enlargement (irrespective of the enlarger structure) will depend upon the grain size of the negative, the sharpness (accuracy) of both the camera and projector lenses, blur in the image due to subject motion and camera shake during the exposure, and the intended viewing distance of the final product.\n\nFor example, a 5 by 7 inch print intended for viewing in a scrapbook at 18 inches may be unsuitable for enlargement as an 8 by 10 inch print to be hung on a hallway wall to be viewed at the same distance, but usable at a larger 5 by 7 feet (twelve times larger) on a billboard to be viewed no closer than eighteen feet (twelve times more distant).\n\nAs the photographic market shifts away from film-based towards electronic imaging technology, many manufacturers no longer make enlargers for the professional photographer. Durst, who made high quality enlargers, stopped producing them in 2005, but still supports already sold models.\nManufacturers old and new include:\n\n"}
{"id": "6371524", "url": "https://en.wikipedia.org/wiki?curid=6371524", "title": "Environmental Research Institute of Michigan", "text": "Environmental Research Institute of Michigan\n\nThe Environmental Research Institute of Michigan (ERIM) was a research institute at Ann Arbor, Michigan, founded in 1972. The institute contributed to the development of remote sensing, radar, and holography. \n\nERIM grew out of a military and environmental research arm of the University of Michigan. A for-profit enterprise, ERIM International, split off from it in 1997. This was successively acquired by Veridian Corporation, by General Dynamics - moving to Ypsilanti - and in 2014 by MacDonald, Dettwiler and Associates (MDA). Meanwhile, the remaining non-profit portion became the Altarum Institute in 2001. Part of Altarum was in 2006 merged into the Michigan Tech Research Institute (MTRI), conducting environmental and remote sensing research, while health systems research continues at Altarum Institute.\n\nThe Environmental Research Institute of Michigan (ERIM) began as Willow Run Laboratories in 1946, but was established as a private not for profit research institute when it formally separated from the University of Michigan in 1972. ERIM contributed to the development of remote sensing for environmental and military applications. Much of the laboratory's early history is not publicized because ERIM and its predecessor organizations were developing remote sensing and surveillance systems to collect military intelligence from aircraft and satellites.\n\nPrior to Willow Run Laboratories's establishment in 1946, with the Cold War just beginning, the War Department was concerned about keeping the U.S. in the forefront of applied science. University researchers newly returned from WWII service were anxious to use their expertise to address those concerns.\n\nAt the University of Michigan’s College of Engineering, Professors William Gould Dow and Emerson W. Conlon approached Wright-Patterson Air Force Base scientists with their proposal to conduct a large-scale research project. The project was designed to prove the feasibility of what would later be referred to as an antiballistic missile system. The proposal was accepted, called WIZARD, and it was this contract that gave birth to ERIM’s first predecessor organization, the Michigan Aeronautical Research Laboratory (MARC).\n\nDuring the Vietnam War, protesters at the University of Michigan forced the University to sever formal ties with Willow Run Laboratories. Research from ERIM provided technology for military surveillance, as well as information and models for better prediction and understanding of floods, fires, agricultural crops and remotely sensed information, including studies of the Bering Glacier.\n\nERIM became well known in the scientific community through a series of international conferences on remote sensing and geospatial information technologies, in the spirit of the first International Symposium on Remote Sensing of the Environment held in Ann Arbor, Michigan in 1962.\n\nERIM played key-roles in the development and implementation of synthetic aperture radar (SAR), at the time, an entirely new concept of radar technology. Development of an optical processing system for SAR data led Emmett Leith, Adam Kozma and Juris Upatnieks to use the newly invented laser in conjunction with the holographic theories outlined by Dennis Gabor. Leith and Upatnieks developed a practical technique for wave-front recording and reconstruction using lasers, thereby making possible the field now known as holography. Holograms have since found a wide variety of applications.\n\nIn 1997, claiming an inability to compete with for-profit industry, President Peter Banks and ERIM's board of trustees spun off a for-profit subsidiary under the name ERIM International, Incorporated. Nearly all of ERIM's R&D work and contracts moved to the for-profit entity, while the buildings, property, and a small fraction of personnel and contracts were retained by the not-for-profit ERIM. Banks and many board members allotted themselves high numbers of shares in the new for-profit venture, which were exercised for considerable sums when ERIM International, Inc. was sold to Veridian Corp. of Washington, D.C. in 1999. Veridian was later bought by General Dynamics Advanced Information Systems in 2003, and the heritage-ERIM part of the organization became its Michigan Research and Development Center (MRDC) and moved from Ann Arbor to nearby Ypsilanti. Proceeds from the sale of ERIM International were used by the not-for-profit ERIM to acquire Center for Electronic Commerce (CEC), another Ann Arbor not for profit in 1998. ERIM acquired Vector Research, Inc. (VRI) in 2001 to form the Altarum Institute, headquartered in Ann Arbor, Michigan.\n\nOn October 1, 2006, Michigan Technological University purchased the Altarum Institute's Environmental and Emerging Technologies Division (EETD), which was mostly comprised by the heritage-ERIM portion of the Institute, to form the Michigan Tech Research Institute (MTRI). The Altarum Institute is a nonprofit now solely focused on health systems research, while the environmental and remote sensing research continues at MTRI.\n\nOn October 3, 2014, MacDonald, Dettwiler and Associates (MDA), Canadian corporation, completed the acquisition of the Ypsilanti, Michigan building and employees therein from General Dynamics Advanced Information Systems. With MDA's corporate restructuring following the acquisition of Digital Globe in 2017, this line of business was shifted into the newly-formed subsidiary Radiant Solutions .\n\n\n"}
{"id": "18263853", "url": "https://en.wikipedia.org/wiki?curid=18263853", "title": "Environmental Technology Verification Program", "text": "Environmental Technology Verification Program\n\nEnvironmental Technology Verification (ETV) consists in the verification of the performance of environmental technologies or in other words is \"the establishment or validation of environmental technology performance by qualified third parties based on test data generated through testing using established protocols or specific requirements\". \nThere are several ETV programmes running all over the world, organised through government initiatives, with the pioneer programme being the one developed in the United States of America, followed by the Canadian ETV Programme. Other programmes have run or are running in South Korea, Japan, Bangladesh, Denmark, France, Europe, Philippines and China. Each programme has its own definitions, structure and procedures and programmes are not always inter-compatible. In 2007, an ETV International Working Group was formed to work on the convergence of the different programmes towards mutual recognition - under the \"motto\" \"Verified once, verified everywhere\". The work of this group was at the origin of the request for drafting an ETV ISO standard. This concluded in the establishment of an ISO working group under Technical Committee 207 (Environmental Management), Sub-committee 4, Working Group 5 - Environmental Technology Verification (ISO/TC 207/SC 4/WG 5). When concluded the ISO standard will have the number ISO/NP 14034.\n\nThe Environmental Technology Verification (ETV) Program of the Environmental Protection Agency (EPA) in the United States develops testing protocols and verifies the performance of innovative environmental technologies that can address problems that threaten human health or the natural environment. ETV was created to accelerate the entrance of new environmental technologies into the domestic and international marketplace by providing objective technology information on commercial ready technologies. ETV is a voluntary program. Developers/vendors of environmental technologies are not required to participate in the program, nor are they required to seek verification. ETV does not pass or fail and does not rank technologies. All verification reports and statements are made publicly available on the ETV Web site.\n\nETV has five centers which are called verification organizations. These verification centers are run through a cooperative agreement\n\nThe center verifies the performance of commercial-ready technologies that monitor contaminants and natural species in air, water, and soil. The center tests both field-portable and stationary monitors, as well as innovative technologies that can be used to describe the environment (site characterization).\n\nThis center verifies commercial-ready technologies that control stationary and mobile air pollution sources, and mitigate the effects of indoor air pollutants.\n\nThis center verifies the performance of commercial-ready drinking water treatment systems for use in small communities, or individual homes and businesses.\n\nThis center verifies the performance of commercial-ready technologies that produce, mitigate, monitor, or sequester greenhouse gas emissions.\n\nThis center verifies the performance of commercial-ready technologies that protect groundwater and surface waters from contamination.\n\nA component of ETV which was added in 2005 to address priority environmental technology categories for meeting the USEPA needs for credible performance information. Priority is given to technologies that can address high-risk environmental problems.\n\nOther topical efforts are listed below.\n\nETV has verified over 400 technologies and developed more than 90 protocols. A survey of participating vendors completed in 2001 showed overwhelming support for the ETV program. Responses indicated that 73 percent of the vendors were using ETV information in product marketing, and 92 percent of those surveyed responded that they would recommend ETV to other vendors.\n\nIn 2006, EPA published a two-volume set of case studies which document actual and projected outcomes from verifications of technologies in 15 technology categories (ETV Program Case Studies Vol 1 EPA/600/R-06/001 and ETV Program Case Studies Vol II EPA/600/R-06/082).\n\nAn Association of State Drinking Water Administrators (ASDWA) survey showed that 34 states recognize and use ETV reports. ASDWA and its members rely heavily on these evaluations to support the use of new technologies and products in the drinking water industry.\n\nDesignating a product or technology as ETV “verified” does not mean a given technology reduces every emission, has no drawbacks, or outperforms solutions \"not\" on the “verified” list.\n\nDesignating a product or technology as “verified” means that a given technology produced “X” outcome, when tested according to a specific protocol.\n\nof 5% during \"line haul duty\", but the % of error was + / - 4%, and under the heaviest load, there was no reading given for fuel consumption reduction. The Envirofuels verification report indicates that TPM increased as little as 40%, and as much as 170%. Envirofuels Diesel Fuel Catalyzer, while \"verified', actually increased TPM emissions, and showed what amounts to an inconclusive result for fuel use reduction.\n\nThe composition of TPM (total diesel particulate matter) is the sum of \"dry\" particulates,and \"wet\" particulates.\n\n\"Dry\" Particulate emissions are also known as inorganic soot, black carbon, or elemental carbon.\n\n\"Wet\" particulates are also known as organic carbon, soluble organic fractions (SOFs) and volatile organic compounds (VOCs).\n\nThe exact ratio of \"wet to dry\" diesel particulate matter will vary by engine load, duty cycle, fuel composition and specification,and engine tuning.\n\nAn opacity reading is a measurement of the level of \"visible\" inorganic carbon, also known as soot. Opacity measurements cannot detect organic carbon emissions, VOC / SOF emissions, or NOx emissions.\n\nSpecialized instrumentation is required to determine organic carbon levels,and to detect other unseen particulates. When used in conjunction with an opacity meter, the technician can detect ( for example) an increase in TPM, and detect a decrease in visible smoke ( opacity ) emissions.\n\nThe ETV verification program ( and other verification pathways )publish the verification reports, technology options charts, and technical summaries, once testing has been completed.\n\nThe ETV testing facility will issue press releases on behalf of the technology vendor, upon completion of testing.\n\nThe ETV verification program reports all outcomes, and leaves the ultimate decision regarding the suitability and applicability of a given technology to the discretion of the end user. Additional research may be necessary in order to adequately address specific situations.\n\n\"By “verify,” ETV means to establish the performance of a technology (i.e., confirm, corroborate, substantiate, validate). ETV verification does not imply approval, certification, or designation by EPA, but rather provides a quantitative assessment of the performance of a technology under specific, predetermined criteria or protocols and adequate data quality assurance procedures.\"\n\n\"The (VDRP) Verification Program evaluates technologies to support their use in the market while providing customers with confidence that verified technologies will provide emission reductions as listed. This Verification process evaluates the emission reduction performance of retrofit technologies, including their durability, and identifies engine operating criteria and conditions that must exist for these technologies to achieve those reductions.\"\n\n\"Mention of commercial product names does not imply endorsement or recommendation\"\n\nETV has been developed in different European countries as part of government initiatives and/or as part of funded research projects. Research projects included TESTNET, PROMOTE, AIR ETV, TRITECH ETV and ADVANCE ETV. Formal programs and initiatives took place in Denmark with the Danish Centre for Environmental Technology Verification (DANETV), the Nordic countries, including Denmark, Sweden, Finland and Norway, with the Nordic Environmental Technology Verification (NOWATEC) project, in France with the French ETV program and in a partnership between Denmark, The Netherlands and Germany with the Verification of Environmental Technologies for Agricultural Production (VERA). The European Union launched in 2011 an ETV Pilot Programme with the support from seven EU member states: Belgium, Czech Republic, Denmark, Finland, France, Poland and United Kingdom. This initiative was initially prepared under the Environmental Technologies Action Plan (ETAP) from the European Commission and was then followed under the Eco-Innovation Plan.\n\nEnvironmental Technology Verification (ETV) is a new tool to help innovative environmental technologies reach the market. Claims about the performance of innovative environmental technologies can be verified by qualified third parties called \"Verification Bodies\". The \"Statement of Verification\" delivered at the end of the ETV process can be used as evidence that the claims made about the innovation are both credible and scientifically sound. With proof of performance credibly assured, innovations can expect an easier market access and/or a larger market share and the technological risk is reduced for technology purchasers.\n\nUnder the EU-ETV Pilot Programme, there are four Verification Bodies:\n\n"}
{"id": "34890242", "url": "https://en.wikipedia.org/wiki?curid=34890242", "title": "Experi-Metal v. Comerica", "text": "Experi-Metal v. Comerica\n\nExperi-Metal, Inc., v. Comerica Bank (docket number: 2:2009cv14890) is a decision by the United States District Court for the Eastern District of Michigan in a case of a phishing attack that resulted in unauthorized wire transfers of US$1.9 million through Experi-Metal's online banking accounts. The court held Comerica liable for losses of US$560,000 that could not be recovered from the phishing attack, on the ground that the bank had not acted in good faith when it failed to recognize the transfers as fraudulent.\n\nExperi-Metal, a Macomb, Michigan-based company, held accounts with Comerica, headquartered in Dallas, Texas. Experi-Metal had signed up for a NetVision Wire Transfer service allowing it to send and receive payments and incoming fund transfers through the Internet.\n\nAt approximately 7:35 am on January 22, 2009, an Experi-Metal employee opened a phishing email containing a link to a web page purporting to be a \"Comerica Business Connect Customer Form\". Following the email's link, the employee then proceeded to provide his security token identification, WebID and login information to a phony site. As a result, the fraudulent third parties gained access to Experi-Metal's accounts held with Comerica.\n\nIn a six-and-a-half-hour period between 7:30 am and 2:02 pm, 93 fraudulent transfers were made from Experi-Metal's accounts totaling US$1,901,269.00. The majority of the transfers were directed to bank accounts in Russia, Estonia and China.\n\nBetween 7:40 am and 1:59 pm, transfers totaling US$5.6 million were executed among accounts using the information obtained from the phishing attack. In one account, the transfers resulted in an overdraft of US$5 million.\n\nAt 11:30 am, Comerica was alerted to the potential fraud by a telephone call from a JP Morgan Chase employee who had noticed suspicious wire transfers sent from an Experi-Metal account to a bank in Moscow, Russia. Sometime between 11:47 am and 11:59 am, Comerica alerted Experi-Metal to the transfers and confirmed that the legitimate account holder had not made any transactions during the course of the day. By 12:25 pm, Comerica put a hold on Experi-Metal's online banking transactions and began to \"kill\" its user session in an attempt to forcefully remove the people making the transfers from the Comerica online service.\n\nComerica was successful in recovering a portion of the transfers. In total, US$561,399 was lost in the fraudulent transfers arising out of the phishing scheme.\n\nThe court considered two main issues in its decision. The first issue was whether the Experi-Metal employee whose confidential information was used to initiate the fraudulent transfers was authorized to initiate transfers on behalf of the company, and in turn, whether Comerica complied with its own security procedures in accepting the orders. The second issue was whether Comerica acted in \"good faith\" in accepting the orders on Experi-Metal's account.\n\nThere was some question as to whether the Experi-Metal employee who fell victim to the phishing incident was authorized to make wire transfers on behalf of the company. The issue was raised in the context of whether Comerica was complying with its security procedures when it accepted the wire transfers that were made using his account user information on January 22, 2009.\n\nAfter considering several contextual factors, the court concluded that the employee who had provided his account user information was authorized to initiate transfers with Comerica on behalf of Experi-Metal. As a result, Comerica was found to be in compliance with its own security protocols when it accepted the orders.\n\nA second issue in the case concerned the issue of 'good faith' on Comerica's part in accepting the wire transfers initiated by the fraudulent third parties.\n\nUnder Michigan law, wire transfer orders are effective as orders of the customer even if they are not actually ordered by the customer, provided certain criteria are met. The issue in this case was whether the orders were accepted in good faith and in compliance with the security procedures, written agreements or instructions of the customer. If the orders made to Comerica on Experi-Metal's account were not received in \"good faith\", they would not be effective.\n\nWhile the court found that Comerica's security procedures were commercially reasonable, it found the bank failed to prove it had accepted orders for the fraudulent transfers in good faith. Under Michigan law good faith requires \"honesty in fact and the observance of reasonable commercial standards for fair dealing.\"\n\nBecause there was no suggestion that Comerica's employees acted dishonestly in accepting the fraudulent orders, the court moved to the element of the good faith test dealing with reasonable commercial standards for fair dealing. Here, the court found Comerica failed to meet the burden of proving that its employees met reasonable commercial standards of fair dealing in the context of the fraudulent transfers, and in particular with respect to the unusual overdrafts to the Experi-Metal accounts. On this last point, the court made specific reference to the overdrafts of US$5 million on an Experi-Metal account that usually had a $0 balance.\n\nPrimarily on the basis that Experi-Metal's online wire transfer orders were not received in good faith, the court ordered Comerica to compensate Experi-Metal for its losses. Comerica reportedly reached an out of court settlement with Experi-Metal soon after the court's decision.\n\n\"Experi-Metal v. Comerica\" represents a relatively early decision in an emerging area of case law relating to online banking fraud in the US.\n\nIn Patco Construction v. People's United Bank a US District Court in Maine held that the defendant bank was not liable for US$588,000 in fraudulent transfers that were believed to result from Zeus keylogger malware attacks.\n\nPatco was an online banking customer and account holder at People's Bank at the time of the malware attacks. Between May 7 and May 16, 2009 unknown third parties made multiple online transfers totaling US$588,851 out of Patco's account. Ultimately, the bank was able to block US$243,406 of the fraudulent transfers.\n\nPatco alleged that its losses were related to People's Bank's deficient online security. The court found that People's Bank did suffer from some security weaknesses, but that on the whole, its security procedures were commercially reasonable. Accordingly, it found that the bank was not liable for the losses resulting from the fraudulent transfers. Although the facts of this case differ from those in \"Experi-Metal v. Comerica,\" it may be a challenge to reconcile the contrast between the two decisions. However, in July 2012, this decision was reversed by an appellate court. The parties later settled out of court, with People's United Bank paying the remainder of what was stolen from Patco's account, as well as $45,000 in interest.\n\n\"In a landmark decision, the 1st Circuit Court of Appeals held in \"Patco Construction Company, Inc. v. People's United Bank\", No. 11-2031 (1st Cir. July 3, 2012) that People's United Bank (d/b/a Ocean Bank) was required to reimburse its customer, PATCO Construction Co., for approximately $580,000 that had been stolen from PATCO'S bank account. In so doing, the court reversed the decision of the U.S. District Court for the District of Maine that had granted summary judgment in the bank's favor.\" \n\nIn Village View v. Professional Business Bank a similar claim was filed in the Superior Court of California in June 2011. Village View sued for losses incurred as a result of unauthorized and fraudulent wire transfers made from its account with Professional Business Bank on March 16–17, 2010, totaling US$195,874.\n\nThe attacks began with a banking Trojan disguised as a UPS shipping receipt, which was accepted and opened into the Village View network by unsuspecting employees. The file was later found to contain malware that did several things including disabling of email notifications normally sent by the bank each time a transfer was made from Village View's account. The fraudulent transfers were made to international accounts, including banks in Latvia.\n\nVillage View Escrow alleges in its claim that the unauthorized transfers were a result of Professional Business Bank's inadequate security system. Specifically, Village View alleges a failure on the part of Professional Business Bank to provide 'commercially reasonable security' procedures in accordance with California law and an accompanying failure to accept the orders for wire transfers in 'good faith.'\n\nWire transfer fraud and phishing are the sub-types of bank fraud used against Experi-Metal.\n\nAmong US banking institutions, December 2011 saw US national banks targeted most frequently by phishing at 85%, followed by regional US banks at 9% and US credit unions at 6%. In terms of overall volume of phishing worldwide during the same period, the UK was a target 50% of the time, followed by the US at 28%, Brazil at 5%, South Africa at 4% and Canada at 2%.\n\nMalware such as the Zeus Trojan has been used extensively by criminals to steal personal banking information which can then be used to make fraudulent transfers out of the victims' bank accounts. In some cases, the perpetrators of the attacks have been caught and prosecuted, both within the US, as well as in other countries.\n\nWhile the types of activities in \"Experi-Metal v. Comerica\" might fall under the Computer Fraud and Abuse Act as an offense, the challenges of determining jurisdiction in an online environment, identifying perpetrators and collecting evidence remain as potentially significant obstacles in any attempts to enforce such legislation.\n"}
{"id": "20954465", "url": "https://en.wikipedia.org/wiki?curid=20954465", "title": "Flue-gas condensation", "text": "Flue-gas condensation\n\nFlue gas condensation is a process, where flue gas is cooled below its water dew point and the heat released by the resulting condensation of water is recovered as low temperature heat. \n\nCooling of the flue gas can be performed either directly with a heat exchanger or indirectly via a condensing scrubber.\n\nThe condensation of water releases more than per ton of condensed water, which can be recovered in the cooler for e.g. district heating purposes.\n\nExcess condensed water must continuously be removed from the process.\n\nThe downstream gas is saturated with water, so even though significant amounts of water may have been removed from the cooled gas, it is likely to leave a visible stack plume of water vapor. \n\nThe heat recovery potential of flue gas condensation is highest for fuels with a high moisture content (e.g. biomass and municipal waste), and where heat is useful at the lowest possible temperatures. Thus flue gas condensation is normally implemented at biomass fired boilers and waste incinerators connected district heating grids with relatively low return temperatures (below approximately ).\n\nFlue gas condensation may cause the heat recovered to exceed the Lower Heating Value of the input fuel, and thus an efficiency greater than 100%. Since historically most combustion processes have not condensed the fuel, usual efficiency calculations assume the combustion products are not condensed. This assumption is implicit when basing calculations on the Lower Heating Value. A more rigorous approach would be to base efficiency calculations on the Higher Heating Value, which typically results in efficiencies less than 100%.\n\nShould the flue gases be cooled below , even efficiencies based on the Higher Heating Value may exceed 100%, since typical heating value definitions assume that all heat is released when combustion products are cooled to somewhere between and .\n\n\n"}
{"id": "2500337", "url": "https://en.wikipedia.org/wiki?curid=2500337", "title": "Full Belly Project", "text": "Full Belly Project\n\nThe Full Belly Project Ltd is a non-profit organization based out of Wilmington, North Carolina, which designs labor-saving devices to improve the lives of people in developing communities. Their main devices are the Universal Nut Sheller (UNS) and the Rocker Water Pump.\nThe first device they designed, the UNS, was a peanut sheller but farmers started using it for other crops. The UNS is now used for coffee, jatropha, neem, and shea in 35 countries in Africa, Asia and Latin America. Since 2009, their research has been focused on designing a water pump.\n\nTheir primary objective was to increase the cost effectiveness of peanut agriculture as a means of sustainable development in those countries, through the development of affordable appropriate technology. There are an estimated half billion people across the globe in over 100 countries, primarily in the equatorial regions and particularly in Africa, dependent upon peanuts as their primary source of protein. \n\nThe major limiting factor for growing peanuts has always been the time- and labor-intensive process of hand-shelling peanuts, a job usually relegated to women and children. Overcoming this technical obstacle has been a goal of agricultural research for some years. When Dr. T. Williams, Senior Research Scientist at University of Georgia and an expert on all 15,000 cultivars of peanuts, was first approached by Jock Brandis, the project's engineer, he stated that an affordable peanut sheller is considered the \"holy grail of sustainable development\". \n\nThe final design for the machine was completed in January 2005, and has come to be known as the Universal Nut Sheller. This relatively small, hand-powered device made from two pieces of concrete and a handful of metal pieces is able to shell at a rate of 50 kg of peanuts an hour. On average an individual woman or child can hand shell 1.5 kg of peanuts in a single hour. Furthermore, one set of fiberglass molds can reproduce an indefinite number of machines. Raw materials for the machine include only half a sack of concrete and a few metal parts, which cost less than $50 US per machine. Maintenance is nearly zero, and a machine's lifespan is estimated at twenty years.\n\nOther versions of the UNS have been designed:\n\nIn keeping with their mission of changing the world for the “bottom billion” through production of self-sustainable technologies, the Full Belly Project and partners at Diversey introduced the soap press in 2013. Known as Soap for Hope, this project reuses a portion the millions of pounds of soap that are thrown away annually by local hotels and resorts. The soap press allows communities to produce and distribute recycled soap products to fend off disease, promote better health, and generate income for impoverished communities around the world. The Full Belly Project developed a “soap press in a box” micro-factory to ship abroad for assembly, granting entrepreneurial opportunity to those with less access to proper sanitation and resources.\n\nThe soap press in now in 18 countries around the world providing over 41,000 people with soap.\n\nPresently the organization is collaborating with like-minded development organizations in an effort to target areas most in need of this sustainable technology. Since finishing the final design of the Universal Nut Sheller, The Full Belly Project has distributed machines in The Bahamas, Uganda, Liberia, Côte d'Ivoire, Ghana, Gambia, Zambia, the Philippines, Haiti, Guyana, India, Kenya, Mali, Nigeria, Sudan, Democratic Republic of Congo, Senegal, Guatemala, Tajikistan, Sierra Leone, Zimbabwe, and Malawi.\n\nWith a few manual adjustments the machine is also capable of shelling winged beans, neem seeds (from neem trees, also known as margosa), jatropha curcas, wet and dried coffee, and shea. All of these seeds/nuts are lucrative once they have been processed. Most cultivation of these crops occurs in developing countries. Machines like the Universal Nut Sheller add value to these crops and instantly improve the lives of those who use these labor-saving devices.\n\nThis technology is particularly empowering for women in that it relieves them of the burden of hand-shelling jatropha curcas, shea, coffee and peanuts. Women provide the majority of agricultural labor, and with the time saved by this machine, would have the opportunity to dedicate themselves to other obligations. Collection of firewood, a chore often left to children, can be greatly reduced by the simple processing of shelled peanut hulls into fuel briquette.\n\n\n\n"}
{"id": "41348278", "url": "https://en.wikipedia.org/wiki?curid=41348278", "title": "GAL22V10", "text": "GAL22V10\n\nThe GAL22V10 is a series of programmable-logic devices implemented as CMOS-based generic array logic ICs, created by Lattice Semiconductor, and available as a Dual inline package or a Plastic leaded chip carrier. While it is an example of a standard production GAL device it is often used in educational settings as a basic PLD.\n\nThe GAL22V10 has 12 input pins, and 10 pins that can be configured as either inputs or outputs, and exists in various switching speeds, from 25 to 4 ns. Each output is driven by an output-logic macrocell, with an output-enable product term, and a variable number of product terms, ranging from eight to sixteen. Each OLMC may be set to output as inverting or non-inverting, and be placed into either registered or combinatorial mode. In registered mode, each macrocell actively uses a D-flip-flop to hold a state under control of the data input from the logic portion of the macrocell and the rising edge of the clock signal, while in combinatorial mode the flip-flop is removed from the macrocell and the outputs are driven directly by the logic. In the latter mode, the pin may also dynamically switch between input and output based on the product term. In either mode the pin value is fed back into the array as a product term. Combinations are set using an EPROM. The output registers can be preloaded into a potentially invalid state for testing by a GAL22V10 programmer. Inputs and outputs include active pull-ups and are transistor-transistor logic compatible due to high-impedance buffers.\nA user electronic signature section is included for details such as user ID codes, revision IDs, or asset tagging on official Lattice Semiconductor units, as well as a static ES section for compatibility with non-Lattice Semiconductor GAL22V10 units. In addition, a security cell is included which, when set, disallows the retrieval of the array logic from the chip, until a new set of logic is set.\n\nLatch-up protection is implemented using n-pullups and a charge pump in the official Lattice Semiconductor models.\n\nThe GAL22V10D had been discontinued by Lattice Semiconductor as of June 2010 with the last shipment in June 2011. No pin-compatible replacements have been offered or recommended by Lattice for this PLD However, other pin-compatible alternatives exist from other manufacturers (e.g. Atmel ATF22V10).\n\n"}
{"id": "35845586", "url": "https://en.wikipedia.org/wiki?curid=35845586", "title": "Geostationary Ocean Color Imager", "text": "Geostationary Ocean Color Imager\n\nGeostationary Ocean Color Imager (GOCI, ), is the world’s first geostationary orbit satellite image sensor in order to observe or monitor an ocean-color around the Korean Peninsula [1][2]. The spatial resolution of GOCI is about 500m and the range of target area is about 2,500km×2,500km centered on Korean Peninsula. GOCI was loaded on Communication, Ocean, and Meteorological Satellite (COMS) of South Korea which was launched in June, 2010. It will be operated by Korea Ocean Satellite Center (KOSC) at Korea Institute of Ocean Science & Technology (KIOST), and capture the images of ocean-color around the Korean Peninsula 8 times a day for 7.7 years.\n\nThe ocean data products that can be derived from the measurements are mainly the chlorophyll concentration, the optical diffuse attenuation coefficients, the concentration of dissolved organic material or yellow substance, and the concentration of suspended particles in the near-surface zone of the sea. In operational oceanography, satellite derived data products are used in conjunction with numerical models and in situ measurements to provide forecasting and now casting of the ocean state. Such information is of genuine interest for many categories of users.\n\n\n"}
{"id": "3442409", "url": "https://en.wikipedia.org/wiki?curid=3442409", "title": "Grey literature", "text": "Grey literature\n\nGrey literature (or gray literature) are materials and research produced by organizations outside of the traditional commercial or academic publishing and distribution channels. Common grey literature publication types include reports (annual, research, technical, project, etc.), working papers, government documents, white papers and evaluations. Organizations that produce grey literature include government departments and agencies, civil society or non-governmental organisations, academic centres and departments, and private companies and consultants.\n\nGrey literature may be made available to the public, or distributed privately within organizations or groups, and may lack a systematic means of distribution and collection. The standard of quality, review and production of grey literature can vary considerably. Grey literature may be difficult to discover, access, and evaluate, but this can be addressed through the formulation of sound search strategies.\n\nWhile a hazy definition of \"grey literature\" had existed previously, the term is generally understood to have been coined by the researcher Charles P. Auger, who wrote \"Use of Reports Literature\" in 1975. The literature he referred to consisted of intelligence reports and notes on atomic research produced in vast quantities by the Allied Forces during World War II. In a conference held by the British Lending Library Division in 1978, Auger used the term \"grey literature\" to describe the concept for the first time. His concepts focused upon a \"vast body of documents\", with \"continuing increasing quantity\", that were characterized by the \"difficulty it presents to the librarian\". Auger described the documentation as having great ambiguity between temporary character and durability, and by a growing impact on scientific research. While acknowledging the challenges of reports literature, he recognized that it held a number of advantages \"over other means of dissemination, including greater speed, greater flexibility and the opportunity to go into considerable detail if necessary\". Auger considered reports a \"half-published\" communication medium with a \"complex interrelationship [to] scientific journals\". In 1989 Auger published the second edition of \"The Documentation of the European Communities: A Guide\", which contained the first usage of the term \"grey literature\" in a published work.\n\nThe \"Luxembourg definition\", discussed and approved at the Third International Conference on Grey Literature in 1997, defined grey literature as \"that which is produced on all levels of government, academics, business and industry in print and electronic formats, but which is not controlled by commercial publishers\". In 2004, at the Sixth Conference in New York City, a postscript was added to the definition for purposes of clarification: grey literature is \"...not controlled by commercial publishers, i.e., where publishing is not the primary activity of the producing body\". This definition is now widely accepted by the scholarly community.\n\nThe U.S. Interagency Gray Literature Working Group (IGLWG), in its \"Gray Information Functional Plan\" of 1995, defined grey literature as \"foreign or domestic open source material that usually is available through specialized channels and may not enter normal channels or systems of publication, distribution, bibliographic control, or acquisition by booksellers or subscription agents\". Thus grey literature is usually inaccessible through relevant reference tools such as databases and indexes, which rely upon the reporting of subscription agents.\n\nOther terms used for this material include: report literature, government publications, policy documents, fugitive literature, nonconventional literature, unpublished literature, non-traditional publications, and ephemeral publications. With the introduction of desktop publishing and the Internet, new terms include: electronic publications, online publications, online resources, open-access research, and digital documents.\n\nThough the concept is difficult to define, the term grey literature is an agreed collective term that researchers and information professionals can use to discuss this distinct but disparate group of resources.\n\nIn 2010, D.J. Farace and J. Schöpfel pointed out that existing definitions of grey literature were predominantly economic, and argued that in a changing research environment, with new channels of scientific communication, grey literature needed a new conceptual framework. They proposed the \"Prague definition\" as follows:\n\nDue to the rapid increase web publishing and access to documents, the focus of grey literature has shifted to quality, intellectual property, curation, and accessibility.\n\nThe term \"grey literature\" acts as a collective noun to refer to a large number of publications types produced by organizations for various reasons. These include: research and project reports, annual or activity reports, theses, conference proceedings, preprints, working papers, newsletters, technical reports, recommendations and technical standards, patents, technical notes, data and statistics, presentations, field notes, laboratory research books, academic courseware, lecture notes, evaluations, and many more. The international network GreyNet maintains an online listing of document types.\n\nOrganizations produce grey literature as a means of encapsulating, storing and sharing information for their own use, and for wider distribution. This can take the form of a record of data and information on a site or project (archaeological records, survey data, working papers); sharing information on how and why things occurred (technical reports and specifications, briefings, evaluations, project reports); describing and advocating for changes to public policy, practice or legislation (white papers, discussion papers, submissions); meeting statutory or other requirements for information sharing or management (annual reports, consultation documents); and many other reasons.\n\nOrganizations are often looking to create the required output, sharing it with relevant parties quickly and easily, without the delays and restrictions of academic journal and book publishing. Often there is little incentive or justification for organizations or individuals to publish in academic journals and books, and often no need to charge for access to organizational outputs. Indeed, some information organizations may be required to make certain information and documents public. On the other hand, grey literature is not necessarily always free, with some resources, such as market reports, selling for thousands of dollars. However, this is the exception and on the whole grey literature, while costly to produce, is usually made available for free.\n\nWhile production and research quality may be extremely high (with organizational reputation vested in the end product), the producing body, not being a formal publisher, generally lacks the channels for extensive distribution and bibliographic control.\n\nInformation and research professionals generally draw a distinction between ephemera and grey literature. However, there are certain overlaps between the two media and they undoubtedly share common frustrations such as bibliographic control issues. Unique written documents such as manuscripts and archives, and personal communications, are not usually considered to fall under the heading of grey literature, although they again share some of the same problems of control and access.\n\nThe relative importance of grey literature is largely dependent on research disciplines and subjects, on methodological approaches, and on the sources they use. In some fields, especially in the life sciences and medical sciences, there has been a traditional preference for only using peer-reviewed academic journals while in others, such as agriculture, aeronautics and the engineering sciences in general, grey literature resources tend to predominate.\n\nIn the last few decades, systematic literature reviews in health and medicine have established the importance of discovering and analyzing grey literature as part of the evidence-base and in order to avoid publication bias.\n\nGrey literature is particularly important as a means of distributing scientific and technical and public policy and practice information. Professionals insist on its importance for two main reasons: research results are often more detailed in reports, doctoral theses and conference proceedings than in journals, and they are distributed in these forms up to 12 or even 18 months before being published elsewhere. Some results simply are not published anywhere else.\n\nIn particular, public administrations and public and industrial research laboratories produce a great deal of “grey” material, often for internal and in some cases “restricted” dissemination. The notion of evidence-based policy has also seen some recognition of the importance of grey literature as part of the evidence-base; however, the term is not yet widely used in public policy and the social sciences more broadly.\n\nFor a number of reasons, discovery, access, evaluation and curation of grey literature pose a number of difficulties.\n\nGenerally, grey literature lacks any strict or meaningful bibliographic control. Basic information such as authors, publication dates and publishing or corporate bodies may not be easily identified. Similarly, the nonprofessional layouts and formats, low print runs and non-conventional channels of distribution make the organized collection of grey literature a challenge compared to journals and books.\n\nAlthough grey literature is often discussed with reference to scientific research, it is by no means restricted to any one field: outside the hard sciences, it presents significant challenges in archaeology where site surveys and excavation reports, containing unique data, have frequently been produced and circulated in informal \"grey\" formats.\n\nSome of the problems of accessing grey literature have decreased since the late 1990s as government, professional, business and university bodies have increasingly published their reports and other official or review documents online. The informal nature of grey literature has meant that it has become more numerous as the technology that allows people to create documentation has improved. Less expensive and more sophisticated printers increased the ease of creating grey literature. And the ability to post documents on the internet has resulted in a tremendous boom. The impact of this trend has been greatly boosted since the early 2000s, as the growth of major search engines has made retrieving grey literature simultaneously easier and more cluttered. Grey reports are thus far more easily found online than they were, often at no cost to access. Most users of reports and other grey documents have migrated to using online copies, and efforts by libraries to collect hard-copy versions have generally declined in consequence.\n\nHowever, many problems remain because originators often fail to produce online reports or publications to an adequate bibliographic standard (often omitting a publication date, for instance). Documents are often not assigned permanent URLs or DOI numbers, or stored in electronic depositories, so that link rot can develop within citations, reference lists, databases and websites. Copyright law and the copyrighted status of many reports inhibits their downloading and electronic storage and there is a lack of large scale collecting of digital grey literature. Securing long-term access to and management of grey literature in the digital era thus remains a considerable problem.\n\nThe amount of digital grey literature now available also poses a problem for finding relevant resources and to be able to assess their credibility and quality given the number of resources now available. At the same time a great deal of grey literature remains hidden, either not made public or not made discoverable via search engines.\n\nVarious databases and libraries collect and make available print and digital grey literature; however, the cost and difficulty of finding and cataloguing grey literature mean that it is still difficult to find large collections. The British Library began collecting print grey literature in the post WWII period and now has an extensive collection of print resources. Australian and New Zealand Policy Online has an extensive collection of grey literature on a wide range of public policy issues, Arxiv is a collection of preprints on physics and other sciences, RePEc is a collection of economics working papers.\n\nMany university libraries provide subject guides that give information on grey literature and suggestions for databases. ROAR and OpenDOAR are directories of Open Access (OA) Institutional Repositories (IR) and subject repositories many of which contain some grey literature.\n\nThe annual International grey literature conference series has been organised since 1993 by the Europe-based organisation GreyNet Research in this field of information has been systematically documented and archived via the International Conference Series on Grey Literature (1993, Vol.1)...(2014, Vol.16)\n\nGreynet also produces a journal on grey literature and has been a key advocate for the recognition and study of grey literature, particularly in library and information sciences. \"The Grey Journal\" (2005, Vol.1)...(2014, Vol.10). (print: , online: ). The Grey Journal appears three times a year—in spring, summer, and autumn. Each issue in a volume is thematic and deals with one or more related topics in the field of grey literature. \"The Grey Journal\" appears both in print and electronic formats. The electronic version on article level is available via EBSCO's LISTA-FT Database (EBSCO Publishing). \"The Grey Journal\" is indexed by Scopus and others.\n\nOn 16 May 2014, the Pisa Declaration on Policy Development for Grey Literature Resources was ratified and published.\n\n"}
{"id": "2004299", "url": "https://en.wikipedia.org/wiki?curid=2004299", "title": "Hospital gown", "text": "Hospital gown\n\nA hospital gown, also called a johnny gown or johnny is \"a long loose piece of clothing worn in a hospital by someone doing or having an operation.\" It can be used as clothing for bedridden patients.\n\nHospital gowns worn by patients are designed so that hospital staff can easily access the part of the patient's body being treated.\n\nThe hospital gown is made of fabric that can withstand repeated laundering in hot water, usually cotton, and is fastened at the back with twill tape ties. Disposable hospital gowns may be made of paper or thin plastic, with paper or plastic ties.\n\nSome gowns have snaps along the top of the shoulder and sleeves, so that the gown can be removed without disrupting intravenous lines in the patient's arms.\n\nUsed paper hospital gowns are associated with hospital infections, which could be avoided by proper disposal.\n\nA Canadian study surveying patients at five hospitals determined 57 percent could have worn more clothing below the waist, but only 11 percent wore more than a gown. The physicians conducting the survey said gowns should not be required unless they are necessary. Although they are cheaper and easier to wash, Dr. Todd Lee, of Royal Victoria Hospital in Montreal, said gowns are not necessary unless the patient is incontinent or has an injury in the lower body. Otherwise, Lee said, pajamas or regular clothes may be acceptable.\n\nWhen 9-year-old Luke Lange complained about wearing a hospital gown when being treated for Hodgkin's lymphoma, his mother adapted some t-shirts for him to wear, using snap tape on the sides. Other children saw the t-shirt and wanted one too. Two years later, the organization Luke's FastBreaks had raised $1 million for children's cancer and given out over 5000 of the t-shirts. They were long enough to wear like the gowns, but some preferred to wear them like t-shirts. Briton Lynn, executive director of Luke's FastBreaks, said the t-shirts helped children have a more positive attitude.\n\nIn November 2006, Robert Wood Johnson Foundation gave a $236,000 grant to a team at North Carolina State University to design a new gown based on \"style, cost, durability, comfort, function\" and other qualities. Studies had been done on updating the garment first used when most patients had to stay in bed, but not designed for modesty when patients got out of bed. NCSU professor Traci Lamar said, \"\"Now doctors want patients up and walking quickly.\" Still, traditional gowns could be washed many times and could be handled a lot. Lamar's team worked to come up with a \"more comfortable, less revealing\" design. Surveys found that nurses did not like the ties in the back because knots could form, and some patients wore more than one gown at once, with one tied in front and the other in back. Many patients disliked how lightweight gowns were. In April 2009, the NCSU team showed potential new designs at a reception, and they were preparing to ask for more funding as they developed a prototype. Meanwhile, some hospitals were offering alternatives, including gowns that opened in the front or on the side, and drawstring pants, cotton tops and boxers. These cost more than traditional gowns.\n\nIn 2009, Fatima Ba-Alawi was honored for her DCS (dignity, comfort, safety) gown at a RCN conference on London. Four years after she started using her skills making dresses to redesign hospital gowns, NHS trusts were using the design. The reversible gowns have plastic poppers which make it easier to change without moving the patient and save staff time, and side pockets for drips or catheters, along with a pouch for cardio equipment. One version called the Faith Gown has a detachable head scarf and long sleeves.\n\nAnother redesign in England came from Ben de Lisi, one of six receiving grants. The Design Council was scheduled to show his design, which did not open in the back but did allow access, in March 2010.\n\nThe Cleveland Clinic changed its gowns in 2010 because the CEO had heard many complaints. \n\nMany patients feel that hospital gowns are unfashionable, Diane von Furstenberg was commissioned to design stylish hospital gowns based on her fashionable wrap dress by the Cleveland Clinic. The new design was reversible with a V-neck in both the front and the back, with softer fabric.\n\nJoel Sherman in his blog \"Adolescent Boys and Genital Exams Reducing Embarrassment\" says it is quite common for many teenage boys to be upset when changing into a hospital gown, especially if the wearer associates the look of the gown to women's clothing, women's nightgowns, or lingerie.\n\nLamar's additional funding came from RocketHub. At NCSU Fashion Week in 2013, Lamar's design was mentioned as \"functional and dignified,\" but not shown \"to prevent any patent infringements\". A prototype, made of DermaFabric and made at Precision Fabrics in Greensboro, North Carolina, was to be tested at WakeMed.\n\nBirmingham Children's Hospital in England introduced the polyester/cotton Dignity Giving Suit in March 2013, after 18 months of work. Patients and health care professionals liked the suits with Velcro fasteners on the seams. Other area hospitals were interested. Adults wanted the gowns to be made for them as well as children.\n\nA design patented in 2014 by Janice Fredrickson had a side opening and sleeve openings, and could be put on without the patient sitting up. One version had pockets for telemetry wires and for drainage bags. It was suggested that different colors be used for different patients, such as those at risk of falling.\n\nIn 2015, Henry Ford Health System of Detroit was working on its own design, similar to a bathrobe with cotton blend. In tests, patients liked the new design. But any update was likely to cost more, as well as harder to take care of. The Model G design, to be made by Carhartt of Michigan, used snaps on the front and shoulders.\n\nAccording to the BBC, in England hospital gowns are made much more modestly, taking patient dignity into consideration.\n\n"}
{"id": "41259687", "url": "https://en.wikipedia.org/wiki?curid=41259687", "title": "ICOR Impact", "text": "ICOR Impact\n\niCOR is an education software which collects data for classroom observations.\n\nClassroom observations are a collection of lesson observations that is useful for teacher training and professional development.\n\n\n"}
{"id": "2953298", "url": "https://en.wikipedia.org/wiki?curid=2953298", "title": "K12Planet", "text": "K12Planet\n\nK12Planet.com was a website that allowed teachers to post student grades online. As of September 2012, the product is no longer offered. These grades can be accessed by students and parents to check a student's progress. Teachers can add messages to a message board for the whole school or just for certain classes or individual students. This can be used to display homework online, and allows school administrators to check grades of students and to post other info such as sports schedules and other events.\n\nAdministrators can organize class scheduling through MacSchool, WinSchool or Chancery and can provide usernames and passwords to parents and students. The parents and students can then access the grades through their web browser.\n\nK12Planet and other similar websites (such as Thinkwave and MyGradeBook) allow parents continual access to student performance rather than just at the end of the quarter or through a parent-teacher conference.\n\n"}
{"id": "25373711", "url": "https://en.wikipedia.org/wiki?curid=25373711", "title": "Lecture recording", "text": "Lecture recording\n\nLecture recording refers to the process of recording and archiving the content of a lecture, conference, or seminar. It consists of hardware and software components that work in synergy to record audio and visual components of the lecture. It is widely used in universities and higher education in the UK to provide support for students. 71% of institutions replying to a UCISA survey in 2016 indicated that this technology was available in their institution. Where lecture recording is done at scale the recording system may be integrated with the timetabling system and the collection of metadata may be automated.\n\nHardware is used to capture the lecturer's voice along with the video of the lecturer. Sometimes, the lecturer may use visual aids to support their speech, such as slide shows, which are presented to the audience with some kind of projector. In this case, such slide shows can also be recorded. Once captured, the data is then either stored directly on the capture hardware or sent to a server over a LAN or the Internet. After some processing to adapt the video formats to the desired distribution mechanism, codecs, etc., viewers are then able to remotely access the recording, either in real time or ex post facto.\n\nThe recording of a lecture or presentation may use any combination of the tools: Microphone, Camera, Screen capture, Presentation capture, or Document camera.\n\nSoftware is used both on the capture hardware, the viewer's computer, and the production server. Software ranges from simple web browsers and video players to stand-alone software programs made specifically for viewing lectures. The viewer's as well as the presenter's software must be compatible with the software on the server which receives the content from the capture hardware, produces it, and sends it to the viewer's computer on-demand. OpenCast is an open-source video capture system available in higher education.\n\nModern lecture recording software supports advanced features such as indexing through OCR, instant search, real-time video editing and annotation, along with other advanced features.\n\nLecture recording is often used in the flipped classroom learning model as a means to provide materials outside of traditional lectures or seminars. Students are able to self-study by playing back and interacting with recorded lectures.\n\nSome educational institutions use lecture recording as a means to replace the traditional classroom with an online classroom. Lecture recording can also be used to create reference materials as a supplementary resource. Lecture recording is not always welcomed by university faculty. Faculty attitudes to this kind of technology enhanced learning may vary across disciplines. Student research has found that more students than staff expect lecture recording to be beneficial to learning. In the most part students watch lectures for pragmatic reasons rather than lecture quality. Students do not view recorded lectures as a replacement for attending live lectures, and often continue to attend face to face sessions. Students who use recorded lectures as a supplement sometimes score significantly higher in subsequent assessment.\n\nSome firms may also use lecture recordings as advertising, collaboration, or training materials.\n\nIn addition, lecture capture technology may offer compliance, for example with United States legislation such as Section 508 for students with disabilities. In the United Kingdom, under the Equality Act 2010, universities should provide reasonable adjustments. Providing access to lecture recordings may be considered to be such a reasonable adjustment.\n\nLecture captures are becoming increasingly popular. Many institutions including University of Manchester, University of Glasgow, University of Bristol and University of Loughborough provide guidelines for recording and presentation of lectures which would be useful for learning and there is a growing literature that considers the benefits of lecture capture to students and tutors, as well as the extent of student usage of lecture recordings. Research at Cornell University found increased flexibility for students and faculty.\n\nREC:all (Recording and Augmenting Lectures for Learning) is transnational learning technology project supported by the European Commission under the Life Long Learning Programme, it aims to explore new ways in which lecture capture can become more pedagogically valuable and engaging, and is investigating a variety of learning design, technical and legal issues.\n"}
{"id": "32613119", "url": "https://en.wikipedia.org/wiki?curid=32613119", "title": "Ministry of Science (Serbia)", "text": "Ministry of Science (Serbia)\n\nThe Ministry of Science of the Republic of Serbia () was the ministry in the Government of Serbia which was in charge of science. The ministry was merged into the Ministry of Education, Science and Technological Development on 14 March 2011.\n\nPolitical Party:\n\n"}
{"id": "41147989", "url": "https://en.wikipedia.org/wiki?curid=41147989", "title": "Mobile Rocket Base", "text": "Mobile Rocket Base\n\nThe Mobile Rocket Base (), abbreviated MORABA, is a department of the \"DLR Space Operations and Astronaut Training\" in Oberpfaffenhofen near Munich. Since the 1960s, the MORABA has performed scientific high altitude research missions with unmanned rockets and balloons, and has developed the required mechanical and electrical systems. Their operational areas include upper atmosphere research, microgravity research, astronomy, geophysics, materials science, as well as hypersonic research.\n\n\"EuroLaunch\", a cooperation between MORABA and SSC Esrange, offers international launch services for stratospheric balloons and sounding rockets.\nSince 1971, MORABA also cooperates with the Brazilian Instituto de Aeronáutica e Espaço (IAE) of the Departamento de Ciência e Tecnologia Aeroespacial (DCTA).\n\nThe mobile ground stations and antennas for telemetry (reception of data) and tele-command (transmission of control commands), as well as range instrumentation radar stations for the exact trajectory measurement, are part of the Mobile Infrastructure. This includes also ground support systems for communication, power supplies, etc.\nThe main tasks include tracking the trajectory of sounding rockets or research balloons, RF data reception, data processing, distribution and archiving. Transport, setup and maintenance of the mobile stations, as well as mission preparation, are also part of the activities.\n\nThe Electrical Flight Systems group develops, builds, and qualifies the necessary electrical and electronic systems aboard the rockets and balloons. Among others, this includes onboard computer systems, control and measurement equipment, data management and telemetry components, and RF transmission systems. Attitude, rate, and acceleration sensors are calibrated and flown in these systems. Cold gas rate and attitude control systems are developed and qualified by this group. The construction of Electrical Ground Support Equipment (EGSE) is also in the scope of tasks.\n\nThe Mechanical Flight Systems group performs structural calculations, aerodynamic and thermal analyses. Also, mechanical configuration and design of the payload and the entire vehicle are planned and accomplished, as well as the integration of scientific experiments and the spin balancing of the final payload.\nFurthermore, this group develops and maintains separation and payload recovery systems.\n\nThe tasks of the Launch Services group include procurement, inspection, modification and storage of rocket motors and pyrotechnics, setup of the mobile launch pads, assembling the rocket stages and ignition systems, integrating the payload and rocket motors, and loading the rocket onto the launcher.\nFurthermore, the group is responsible for support of flight safety, trajectory calculations, aerodynamic calculations, and selection of the rocket configuration.\n\n\nThe Mobile Rocket Base emerged from the \"Working Group For Space Exploration\", a common foundation of the Max Planck Institute for Extraterrestrial Physics (MPE) and the former German Laboratory for Aviation (DVL). This working group was founded in 1965 and had its first headquarters in Munich.\nSince April 1969 the Mobile Rocket Base is located in Oberpfaffenhofen, near Munich, in the department \"Space Operations and Astronaut Training\" of the German Aerospace Center (DLR).\n\nThe first mobile campaign to study the solar eclipse in May 1966 on the Greek island of Euboea, on behalf of the European Space Research Organisation (ESRO), demonstrated the feasibility to move and set up extensive technical and scientific equipment in a short time in a remote location.\n\nIn cooperation with the MPE, a campaign for the study of the magnetic field of the Earth was carried out in the spring of 1967, at Esrange (Sweden). With five Nike-Apache rockets, artificial barium clouds (Aurora Borealis) were created in about 100 km altitude, in order to visualize the magnetic field lines.\n\nIn addition to the rocket launch sites in Scandinavia (Kiruna and Andenes), also Sardinia, Wallops Island and Matagorda Island (USA), White Sands (USA), Greenland, Trivandrum (India), Woomera (Australia), Huelva (Spain), Natal (Brazil), and Adelaide Island (Antarctica) were used for the launch of payloads with various scientific purposes.\nIn the following years, sounding rockets were used mainly in the fields of upper atmosphere and research into the causes of global warming.\n\nWith the start of the \"HEATPIPE 1\" payload, manufactured by Dornier, Friedrichshafen, a new field of application for sounding rockets emerged. The launch took place on January 22, 1976 at Esrange, with the aim to investigate the function of heat pipes and latent heat storage in a microgravity environment for their application in future satellite projects.\n\nInitially intended as a supplementary programme for the German Spacelab missions, the first launch of a TEXUS payload took place on December 13, 1977 with a two-stage Skylark rocket at Esrange. In the following years, up to four TEXUS missions (6 minutes of microgravity) were flown per year, with numerous experiments. In order to meet the ever-increasing number of experiments with different requirements and objectives, the Swedish MASER programme, the MAXUS programme (13 minutes microgravity), and the MiniTEXUS programme (3 minutes microgravity) were initiated, under the aegis of the European Space Agency (ESA).\n\nAlready in May 1975, the Mobile Rocket Base was involved in research balloons with the flight of the 2.5-ton \"Spectro-Stratoscope\" instrument in Palestine (Texas), for the reception of the PCM data from the payload.\nTogether with the Max Planck Institute for Aeronomy in Lindau (Harz, Germany), international balloon flights were carried out in Aire-sur-l'Adour (France), Mendoza (Argentina), and Hyderabad (India), to explore the chemical composition (and pollution) of the atmosphere in different altitudes and latitudes.\n\nIn the early years of German sounding rocket research, the payloads were designed and built by engineers and technicians from the companies Dornier, MBB, Kayser-Threde, and ERNO. Parallel to the withdrawal of Dornier and MBB from payload construction for sounding rockets, MORABA developed and tested, in addition to the already existing spin rate and attitude control modules and parachute recovery systems, mechanical and electrical flight systems which were difficult or impossible to obtain on the market. The first and successful application of a rocket-qualified data acquisition and transmission / reception system on a satellite was in the re-entry experiment \"EXPRESS\". For this project, in late 1994 a complete transmitting and receiving station with all necessary functions was set up by MORABA in the South Australian desert and was operated over the turn of the year. After the launch with a Japanese satellite launcher and a 7-day orbital flight, the landing of the EXPRESS capsule was planned to occur in the vicinity of the town of Coober Pedy (Australia). However, due to a malfunction of the rocket, the capsule entered an elliptical orbit with very low Perigee and landed in Africa after only a few revolutions.\n\nFurther applications of developments of MORABA in space flight missions included experiments aboard the Russian space station Mir and the DLR satellite BIRD.\n\nSimilar to the novel use of the rocket platform for microgravity research, the use of sounding rockets as a flying hypersonic wind tunnel has increased steadily. With the help of sounding rockets, large flight models are brought to speeds of above Mach 12 and reentry periods of up to one minute can be achieved.\nFor the DLR Sharp Edge Flight Experiment (SHEFEX) MORABA designed and built the payload subsystems and the rocket system. With its launch in October 2005 from Andenes (Norway) the project has been successfully completed.\nFurther experiments of this kind were the also successfully completed follow-up project SHEFEX II (launched in June 2012), as well as the scramjet engine research programs HIFiRE and Scramspace.\n\n\n\n\n"}
{"id": "15108934", "url": "https://en.wikipedia.org/wiki?curid=15108934", "title": "Needle threader", "text": "Needle threader\n\nA needle threader is a device for helping to put thread through the eye of a needle. Many kinds exist, though a common type combines a short length of fine wire bent into a diamond shape, with one corner held by a piece of tinplate or plastic. The user passes the wire loop through the needle eye, passes the string through the wire loop, and finally pulls both the loop back through the needle by the handle, which pulls the thread through. The typical needle threader of this type has the image of a woman in profile stamped into the plate handle.\n\nAnother type of needle threader is mechanically operated. These may be part of a sewing or embroidery machine, or standalone tools.\n\n"}
{"id": "6781215", "url": "https://en.wikipedia.org/wiki?curid=6781215", "title": "Network search engine", "text": "Network search engine\n\nComputer networks are connected together to form larger networks such as campus networks, corporate networks, or the Internet. Routers are network devices that may be used to connect these networks (e.g., a home network connected to the network of an Internet service provider). When a router interconnects many networks or handles much network traffic, it may become a bottleneck and cause network congestion (i.e., traffic loss).\n\nA number of techniques have been developed to prevent such problems. One of them is the network search engine (NSE), also known as \"network search element\". This special-purpose device helps a router perform one of its core and repeated functions very fast: address lookup. Besides routing, NSE-based address lookup is also used to keep track of network service usage for billing purposes, or to look up patterns of information in the data passing through the network for security reasons .\n\nNetwork search engines are often available as ASIC chips to be interfaced with the network processor of the router. Content-addressable memory and Trie are two techniques commonly used when implementing NSEs.\n\n\"IDT Next Generation Search Engines\".\n"}
{"id": "1867619", "url": "https://en.wikipedia.org/wiki?curid=1867619", "title": "Organotin chemistry", "text": "Organotin chemistry\n\nOrganotin compounds or stannanes are chemical compounds based on tin with hydrocarbon substituents. Organotin chemistry is part of the wider field of organometallic chemistry. The first organotin compound was diethyltin diiodide ((CH)SnI), discovered by Edward Frankland in 1849. The area grew rapidly in the 1900s, especially after the discovery of the Grignard reagents, which are useful for producing Sn-C bonds. The area remains rich with many applications in industry and continuing activity in the research laboratory.\n\nOrganotin compounds are generally classified according to their oxidation states. Tin(IV) compounds are much more common and more useful.\n\nThe tetraorgano derivatives are invariably tetrahedral. Compounds of the type SnRR'R\"R\"' have been resolved into individual enantiomers.\n\nOrganotin chlorides have the formula RSnCl for values of \"n\" up to 3. Bromides, iodides, and fluorides are also known but less important. These compounds are known for many R groups. They are always tetrahedral. The tri- and dihalides form adducts with good Lewis bases such as pyridine. The fluorides tend to associate such that dimethyltin difluoride forms sheet-like polymers. Di- and especially triorganotin halides, e.g. tributyltin chloride, exhibit toxicities approaching that of hydrogen cyanide.\n\nOrganotin hydrides have the formula RSnH for values of \"n\" up to 4. The parent member of this series, stannane (SnH), is an unstable colourless gas. Stability is correlated with the number of organic substituents. Tributyltin hydride is used as a source of hydride radical in some organic reactions.\n\nOrganotin oxides and hydroxides are common products from the hydrolysis of organotin halides. Unlike the corresponding derivatives of silicon and germanium, tin oxides and hydroxides often adopt structures with penta- and even hexacoordinated tin centres, especially for the diorgano- and monoorgano derivatives. The group Sn-O-Sn is called a stannoxane. Structurally simplest of the oxides and hydroxides are the triorganotin derivatives. A commercially important triorganotin hydroxides is the acaricide Cyhexatin (also called Plictran), (CH)SnOH. Such triorganotin hydroxides exist in equilibrium with the distannoxanes:\n\nWith only two organic substituents on each Sn centre, the diorganotin oxides and hydroxides are structurally more complex than the triorgano derivatives. The simple geminal diols (RSn(OH)) and monomeric stannanones (RSn=O) are unknown. Diorganotin oxides (RSnO) are polymers except when the organic substituents are very bulky, in which case cyclic trimers or, in the case of R = CH(SiMe) dimers, with SnO and SnO rings. The distannoxanes exist as dimers of dimers with the formula [RSnX]O wherein the X groups (e.g., chloride, hydroxide, carboxylate) can be terminal or bridging (see Table). The hydrolysis of the monoorganotin trihalides has the potential to generate stannanoic acids, RSnO2H. As for the diorganotin oxides/hydroxides, the monoorganotin species form structurally complex because of the occurrence of dehydration/hydration, aggregation. Illustrative is the hydrolysis of butyltin trichloride to give [(BuSn)O(OH)].\n\nUnlike carbon(IV) analogues but somewhat like silicon compounds, tin(IV) can also be coordinated to five and even six atoms instead of the regular four. These hypercoordinated compounds usually have electronegative substituents. Numerous examples of hypervalency are provided by the organotin oxides and associated carboxylates and related pseudohalide derivatives. The organotin halides for adducts, e.g. MeSnCl(bipyridine).\n\nThe all-organic penta- and hexaorganostannates have even been characterized, while in the subsequent year a six-coordinated tetraorganotin compound was reported. A crystal structure of room-temperature stable (in argon) all-carbon pentaorganostannane was reported as the lithium salt with this structure:\n\nIn this distorted trigonal bipyramidal structure the carbon to tin bond lengths (2.26 Å apical, 2.17 Å equatorial) are larger than regular C-Sn bonds (2.14 Å) reflecting its hypervalent nature.\n\nSome reactions of triorganotin halides implicate a role for R3Sn+ intermediates. Such cations are analogous to carbocations. They have been characterized crystallographically when the organic substituents are large, such as 2,4,6-triisopropylphenyl.\n\nTin radicals, with the formula RSn, are called stannyl radicals. They are invoked as intermediates in certain atom-transfer reactions. For example, tributyltin hydride (tri-n-butylstannane) serves as a useful source of \"hydrogen atoms\" because of the stability of the tributytin radical.\n\nOrganotin(II) compounds are somewhat rare. Compounds with the empirical formula SnR are somewhat fragile and exist as rings or polymers when R is not bulky. The polymers, called polystannanes, have the formula (SnR).\n\nIn principle divalent tin compounds might be expected to form analogues of alkenes with a formal double bond. Indeed, compounds with the formula SnR, called distannenes, are known for certain organic substituents. The Sn centres tend to be highly pyramidal. Monomeric compounds with the formula SnR, analogues of carbenes are also known in a few cases. One example is Sn(SiR), where R is the very bulky CH(SiMe) (Me = methyl). Such species reversibly dimerize to the distannylene upon crystallization:\n\nStannenes, compounds with tin–carbon double bonds, are exemplified by derivatives of stannabenzene. Stannoles, structural analogs of cyclopentadiene, exhibit little C-Sn double bond character.\n\nCompounds of Sn(I) are rare and only observed with very bulky ligands. One prominent family of cages is accessed by pyrolysis of the 2,6-diethylphenyl-substituted tristannylene [Sn(CH-2,6-Et)], which affords the cubane-type cluster and a prismane. These cages contain Sn(I) and have the formula [Sn(CH-2,6-Et)] where n = 8, 10. A stannyne contains a carbon to tin triple bond and a distannyne a triple bond between two tin atoms (RSnSnR). Distannynes only exist for extremely bulky substituents. Unlike alkynes, the C-Sn-Sn-C core of these distannynes are nonlinear, although they are planar. The Sn-Sn distance is 3.066(1) Å, and the Sn-Sn-C angles are 99.25(14)°. Such compounds are prepared by reduction of bulky aryltin(II) halides.\nOrganotin compounds can be synthesised by numerous methods. Classic is the reaction of a Grignard reagent with tin halides for example tin tetrachloride. An example is provided by the synthesis of tetraethyltin:\n\nThe symmetrical tetraorganotin compounds, especially tetraalkyl derivatives, can then be converted to various mixed chlorides by redistribution reactions (also known as the \"Kocheshkov comproportionation\" in the case of organotin compounds):\nA related method involves redistribution of tin halides with organoaluminium compounds.\n\nThe mixed organo-halo tin compounds can be converted to the mixed organic derivatives, as illustrated by the synthesis of dibutyldivinyltin:\n\nThe organotin hydrides are generated by reduction of the mixed alkyl chlorides. For example, treatment of dibutyltin dichloride with lithium aluminium hydride gives the dibutyltin dihydride, a colourless distillable oil:\nThe Wurtz-like coupling of alkyl sodium compounds with tin halides yields tetraorganotin compounds.\n\nImportant reactions, discussed above, usually focus on organotin halides and pseudohalides with nucleophiles. In the area of organic synthesis, the Stille reaction is considered important. It entails coupling reaction with sp2-hybridized organic halides catalyzed by palladium:\n\nand organostannane additions (nucleophilic addition of an allyl-, allenyl-, or propargylstannanes to an aldehydes and imines). Organotin compounds are also used extensively in radical chemistry (e.g. radical cyclizations, Barton–McCombie deoxygenation, Barton decarboxylation, etc.).\n\nAn organotin compound is commercially applied as stabilizers in polyvinyl chloride. In this capacity, they suppress degradation by removing allylic chloride groups and by absorbing hydrogen chloride. This application consumes about 20,000 tons of tin each year. The main class of organotin compounds are diorganotin dithiolates with the formula RSn(SR'). The Sn-S bond is the reactive component. Diorganotin carboxylates, e.g., dibutyltin dilaurate, are used as catalysts for the formation of polyurethanes, for vulcanization of silicones, and transesterification.\n\nn-Butyltin trichloride is used in the production of tin dioxide layers on glass bottles by chemical vapor deposition.\n\n\"Tributyltins\" are used as industrial biocides, e.g. as antifungal agents in textiles and paper, wood pulp and paper mill systems, breweries, and industrial cooling systems. Triphenyltin derivativess are used as active components of antifungal paints and agricultural fungicides. Other triorganotins are used as miticides and acaricides. Tributyltin oxide has been extensively used as a wood preservative.\n\nTributyltin compounds were once widely used as marine anti-biofouling agents to improve the efficiency of ocean-going ships. Concerns over toxicity of these compounds (some reports describe biological effects to marine life at a concentration of 1 nanogram per liter) led to a worldwide ban by the International Maritime Organization.\n\nOrganotin complexes have been studied in anticancer therapy.\n\nTriorganotin compounds can be highly toxic. Tri-\"n\"-alkyltins are phytotoxic and therefore cannot be used in agriculture. Depending on the organic groups, they can be powerful bactericides and fungicides. Reflecting their high bioactivity, \"tributyltins\" were once used in marine anti-fouling paint.\n\nTetraorgano-, diorgano-, and monoorganotin compounds generally exhibit low toxicity and low biological activity. DBT may however be immunotoxic.\n\n"}
{"id": "158644", "url": "https://en.wikipedia.org/wiki?curid=158644", "title": "Paddle", "text": "Paddle\n\nA paddle is a tool used for pushing against liquids, either as a form of propulsion in a boat or as an implement for mixing.\n\nPaddles commonly used in canoes consist of a wooden, fibreglass, carbon fibre or metal rod (the \"shaft\") with a handle on one end and a rigid sheet (the \"blade\") on the other end. Paddles for use in kayaks are longer, with a blade on each end; they are handled from the middle of the shaft.\n\nKayak paddles having blades in the same plane (when viewed down the shaft) are called \"un-feathered.\" Paddles with blades in different planes are called \"feathered\". Feathered paddles are measured by the degree of feather, such as 30, 45, or even 90 degrees. Many modern paddles are made of two pieces which can be snapped together in either feathered or unfeathered settings. The shaft is normally straight but in some cases a 'crank' is added with the aim of making the paddle more comfortable and reducing the strain on the wrist. Because the kayak paddle is not supported by the boat, paddles made of lighter materials are desired; it is not uncommon for a kayak paddle to be two pounds ( ) or less and very expensive paddles can be as light as . In addition, weight savings are more desirable at the ends of the paddle rather than in the middle.\n\nCheaper kayak paddles have an aluminium shaft while more expensive ones use a lighter fibreglass or carbon fibre shaft. Some paddles have a smaller diameter shaft for people with smaller hands. Paddle length varies with a longer paddle being better suited for stronger people, taller people, and people using the paddle in a wider kayak. Some paddle makers have an online paddle size calculator. Blades vary in size and shape. A blade with a larger surface area may be desirable for a strong person with good shoulder joints, but tiring for a weaker person or a person with less than perfect shoulder joints. Some paddle makers offer blades in three sizes.\nBecause normal paddling involves alternately dipping and raising the paddle blades, the colour of the blades may affect the visibility of the kayaker to powerboats operators under limited visibility conditions. For this reason white or yellow blades may offer a safety advantage over black or blue blades. Of course, kayakers should wear a headlamp or have other lighting on their kayak under conditions of limited lighting. However, if a powerboat operator must look straight into a sun low in the sky to see a kayaker, the motion of brightly coloured paddle blades may be of more value than lighting on the kayak. Highly reflective water resistant tape (e.g. SOLAS tape) may be affixed to the paddle blades and boat to enhance visibility.\n\nThe paddle is held with two hands, some distance apart from each other. For normal use, it is drawn through the water from front (bow) to back (stern) to drive the boat forwards. The two blades of a kayak paddle are dipped alternately on either side of the kayak. A paddle is distinguished from an oar in that the paddle is held in the user's hands and completely supported by the paddler, whereas an oar is primarily supported by the boat, through the use of oarlocks. Gloves may be worn to prevent blistering for long periods of paddling.\n\nOn mechanical paddle steamers, the motorized paddling is not done with a mass of paddles or oars but by rotating one or a few paddle wheels (rather the inverse of a water mill).\n\nRacing paddles also have special designs. They are generally less flat and are curved to catch more water which will enable racing paddlers to maximize the efficiency of their stroke. Bent shaft paddles, popular with tripping and marathon canoers, have a blade that is angled from the shaft, usually 12 to 15 degrees.\n\n\n"}
{"id": "2749088", "url": "https://en.wikipedia.org/wiki?curid=2749088", "title": "Paper cutter", "text": "Paper cutter\n\nA paper cutter (also known as a paper trimmer, also sometimes described as a \"paper guillotine\") is a tool often found in offices and classrooms, designed to cut a large set of paper sheets at once with a straight edge.\n\nPaper cutters, similar to those of today, were patented in 1844 and 1852 by Guillaume Massiquot. They have been around since the late 1830s, when, in 1837, Thirault built a model with a fixed blade to a flat surface. Since the middle of the 19th century, considerable improvements have been made by Fomm and Krause of Germany, Furnival in England, and Oswego and Seybold in the United States.\n\nPaper cutters vary in size, usually from about in length on each side for office work to (an edge of A1 paper) in design workshops. The surface will usually have a grid either painted or inscribed on it, often in half-inch increments, and may have a ruler across the top. At the very least, it must have a flat edge against which the user may line up the paper at right-angles before passing it under the blade. It is usually relatively heavy, so that it will remain steady while in use.\n\nOn the right-hand edge is a long, curved steel blade, often referred to as a knife, attached to the base at one corner. Larger versions have a strong compression coil spring as part of the attachment mechanism that pulls the knife against the stationary edge as the knife is drawn down to cut the paper. The other end of the knife unit is a handle. The stationary right edge of the base is also steel, with an exposed, finely-ground edge. When the knife is pulled down to cut paper, the action resembles that of a pair of scissors, only instead of two knives moving against each other, one is stationary. The combination of a blade mounted to a steady base produces clean and straight cuts, the likes of which would have otherwise required a ruler and razor blade to achieve on a single page. Paper cutters are also used for cutting thin sheet metal, cardboard, and plastic. The blade on a paper cutter is made of steel, which provides long-term durability. The steel blade can be resharpened as needed.\n\nA variant design uses a wheel-shaped blade mounted on a sliding shuttle attached to a rail. This type of paper cutter is known as a rotary paper cutter. Advantages of this design include being able to make wavy cuts, perforations or to simply to score the paper without cutting, merely by substituting various types of circular blades. With a rotary cutter, it is also almost impossible for the user to cut him/herself, except while changing the blade. This makes it safer for home use. Higher-end versions of rotary paper cutters are used for precision paper cutting and are popular for trimming photographs. \n\nAn even simpler design uses double-edged blades which do not rotate, but cut like a penknife. While cheaper, this design is not preferable for serious work due to its tendency to tear paper, and poor performance with thick media.\n\nMost paper cutters come equipped with a finger guard to prevent users from accidentally cutting themselves or severing a digit while using the apparatus. However, injuries are still possible if the device is not used with proper care or attention.\n\nIn the modern paper industry, larger machines are used to cut large stacks of paper, cardboard, or similar material. Such machines operate in a manner similar to a guillotine. Commercial versions are motorized and automated, and include clamping mechanisms to prevent shifting of the material during the cutting process. \n\n"}
{"id": "17173574", "url": "https://en.wikipedia.org/wiki?curid=17173574", "title": "Percolation trench", "text": "Percolation trench\n\nA percolation trench, also called an infiltration trench, is a type of best management practice (BMP) that is used to manage stormwater runoff, prevent flooding and downstream erosion, and improve water quality in an adjacent river, stream, lake or bay. It is a shallow excavated trench filled with gravel or crushed stone that is designed to infiltrate stormwater though permeable soils into the groundwater aquifer.\n\nA percolation trench is similar to a dry well, which is typically an excavated hole filled with gravel. Another similar drainage structure is a French drain, which directs water away from a building foundation, but is usually not designed to protect water quality.\n\nPercolation trenches are often used to treat runoff from impervious surfaces, such as sidewalks and parking lots, on sites where there is limited space available for managing stormwater. They are effective at treating stormwater only if the soil has sufficient porosity. To function properly, a trench must be designed with a pretreatment structure such as a grass channel or swale, in order to capture sediment and avoid clogging the trench. It may not be appropriate for sites where there is a possibility of groundwater contamination, or where there is soil with a high clay content that could clog the trench.\n\n\n"}
{"id": "2748878", "url": "https://en.wikipedia.org/wiki?curid=2748878", "title": "Pressure washing", "text": "Pressure washing\n\nPressure washing or power washing is the use of high-pressure water spray to remove loose paint, mold, grime, dust, mud, chewing gum and dirt from surfaces and objects such as buildings, vehicles and concrete surfaces. The volume of a mechanical pressure washer is expressed in gallons or litres per minute, often designed into the pump and not variable. The pressure, expressed in pounds per square inch, pascals, or bar, is designed into the pump but can be varied by adjusting the unloader valve. Machines that produce pressures from 750 to 30,000 psi (5 to 200 MPa) or more are available.\n\nThe terms pressure washing and power washing are used interchangeably in many scenarios, and there is some debate as to whether or not they are actually different processes.\n\nA pressure washing surface cleaner is a tool consisting of two to four high-pressure jets on a rotating bar that swivels when water is flowing. This action creates a uniformed cleaning pattern that can clean flat surfaces at a rapid rate. \n\nHydro-jet cleaning is a more powerful form of power washing, employed to remove buildup and debris in tanks and lines.\n\nPressure washing is employed by businesses and homeowners to reduce allergies, minimize hazards, and improve aesthetics. A pressure washer is used to clean surfaces such as:\n\n\nDepending upon the surface to be cleaned, higher or lower pressure should be used, as well as the appropriate nozzle.\n\nPressure washer nozzles alter the direction of flow and velocity of the water. Nozzles allow users to reach a greater distance or apply more pressure to a difficult to clean surface, however, they can be dangerous. Nozzles are color coded for easy identification, with black nozzles covering the widest degree (65°) and red nozzles covering the least (0°). Great care should be taken when using a 0° nozzle as it can cause injury to the user and damage to surfaces.\n\n\n"}
{"id": "322533", "url": "https://en.wikipedia.org/wiki?curid=322533", "title": "Project Orion (nuclear propulsion)", "text": "Project Orion (nuclear propulsion)\n\nProject Orion was a study of a spacecraft intended to be directly propelled by a series of explosions of atomic bombs behind the craft (nuclear pulse propulsion). Early versions of this vehicle were proposed to take off from the ground with significant associated nuclear fallout; later versions were presented for use only in space. Six tests were launched.\n\nThe idea of rocket propulsion by combustion of explosive substance was first proposed by Russian explosives expert Nikolai Kibalchich in 1881, and in 1891 similar ideas were developed independently by German engineer Hermann Ganswindt. General proposals of nuclear propulsion were first made by Stanislaw Ulam in 1946, and preliminary calculations were made by F. Reines and Ulam in a Los Alamos memorandum dated 1947. The actual project, initiated in 1958, was led by Ted Taylor at General Atomics and physicist Freeman Dyson, who at Taylor's request took a year away from the Institute for Advanced Study in Princeton to work on the project.\n\nThe Orion concept offered high thrust and high specific impulse, or propellant efficiency, at the same time. The unprecedented extreme power requirements for doing so would be met by nuclear explosions, of such power relative to the vehicle's mass as to be survived only by using external detonations without attempting to contain them in internal structures. As a qualitative comparison, traditional chemical rockets—such as the Saturn V that took the Apollo program to the Moon—produce high thrust with low specific impulse, whereas electric ion engines produce a small amount of thrust very efficiently. Orion would have offered performance greater than the most advanced conventional or nuclear rocket engines then under consideration. Supporters of Project Orion felt that it had potential for cheap interplanetary travel, but it lost political approval over concerns with fallout from its propulsion.\n\nThe Partial Test Ban Treaty of 1963 is generally acknowledged to have ended the project. However, from Project Longshot to Project Daedalus, Mini-Mag Orion, and other proposals which reach engineering analysis at the level of considering thermal power dissipation, the principle of external nuclear pulse propulsion to maximize survivable power has remained common among serious concepts for interstellar flight without external power beaming and for very high-performance interplanetary flight. Such later proposals have tended to modify the basic principle by envisioning equipment driving detonation of much smaller fission or fusion pellets, although in contrast Project Orion's larger nuclear pulse units (nuclear bombs) were based on less speculative technology.\n\n\"To Mars by A-Bomb: The Secret History of Project Orion\" was a 2003 BBC documentary film about the project.\n\nThe Orion nuclear pulse drive combines a very high exhaust velocity, from 19 to 31 km/s (12 to 19 mi/s) in typical interplanetary designs, with meganewtons of thrust. Many spacecraft propulsion drives can achieve one of these or the other, but nuclear pulse rockets are the only proposed technology that could potentially meet the extreme power requirements to deliver both at once (see spacecraft propulsion for more speculative systems).\n\nSpecific impulse (\"I\") measures how much thrust can be derived from a given mass of fuel, and is a standard figure of merit for rocketry. For any rocket propulsion, since the kinetic energy of exhaust goes up with velocity squared (kinetic energy = ½ mv), whereas the momentum and thrust go up with velocity linearly (momentum = mv), obtaining a particular level of thrust (as in a number of g acceleration) requires far more power each time that exhaust velocity and \"I\" are much increased in a design goal. (For instance, the most fundamental reason that current and proposed electric propulsion systems of high \"I\" tend to be low thrust is due to their limits on available power. Their thrust is actually inversely proportional to \"I\" if power going into exhaust is constant or at its limit from heat dissipation needs or other engineering constraints.) The Orion concept detonates nuclear explosions externally at a rate of power release which is beyond what nuclear reactors could survive internally with known materials and design.\n\nSince weight is no limitation, an Orion craft can be extremely robust. An unmanned craft could tolerate very large accelerations, perhaps 100 \"g\". A human-crewed Orion, however, must use some sort of damping system behind the pusher plate to smooth the near instantaneous acceleration to a level that humans can comfortably withstand – typically about 2 to 4 \"g\".\n\nThe high performance depends on the high exhaust velocity, in order to maximize the rocket's force for a given mass of propellant. The velocity of the plasma debris is proportional to the square root of the change in the temperature (\"T\") of the nuclear fireball. Since fireballs routinely achieve ten million degrees Celsius or more in less than a millisecond, they create very high velocities. However, a practical design must also limit the destructive radius of the fireball. The diameter of the nuclear fireball is proportional to the square root of the bomb's explosive yield.\n\nThe shape of the bomb's reaction mass is critical to efficiency. The original project designed bombs with a reaction mass made of tungsten. The bomb's geometry and materials focused the X-rays and plasma from the core of nuclear explosive to hit the reaction mass. In effect each bomb would be a nuclear shaped charge.\n\nA bomb with a cylinder of reaction mass expands into a flat, disk-shaped wave of plasma when it explodes. A bomb with a disk-shaped reaction mass expands into a far more efficient cigar-shaped wave of plasma debris. The cigar shape focuses much of the plasma to impinge onto the pusher-plate.\n\nThe maximum effective specific impulse, \"I\", of an Orion nuclear pulse drive generally is equal to:\n\nwhere \"C\" is the collimation factor (what fraction of the explosion plasma debris will actually hit the impulse absorber plate when a pulse unit explodes), \"V\" is the nuclear pulse unit plasma debris velocity, and \"g\" is the standard acceleration of gravity (9.81 m/s; this factor is not necessary if \"I\" is measured in N·s/kg or m/s). A collimation factor of nearly 0.5 can be achieved by matching the diameter of the pusher plate to the diameter of the nuclear fireball created by the explosion of a nuclear pulse unit.\n\nThe smaller the bomb, the smaller each impulse will be, so the higher the rate of impulses and more than will be needed to achieve orbit. Smaller impulses also mean less \"g\" shock on the pusher plate and less need for damping to smooth out the acceleration.\n\nThe optimal Orion drive bomblet yield (for the human crewed 4,000 ton reference design) was calculated to be in the region of 0.15 kt, with approx 800 bombs needed to orbit and a bomb rate of approx 1 per second.\n\nThe following can be found in George Dyson's book. The figures for the comparison with Saturn V are taken from this section and converted from metric (kg) to US short tons (abbreviated \"t\" here).\n\nIn late 1958 to early 1959, it was realized that the smallest practical vehicle would be determined by the smallest achievable bomb yield. The use of 0.03 kt (sea-level yield) bombs would give vehicle mass of 880 tons. However, this was regarded as too small for anything other than an orbital test vehicle and the team soon focused on a 4,000 ton \"base design\".\n\nAt that time, the details of small bomb designs were shrouded in secrecy. Many Orion design reports had all details of bombs removed before release. Contrast the above details with the 1959 report by General Atomics, which explored the parameters of three different sizes of hypothetical Orion spacecraft:\n\nThe biggest design above is the \"super\" Orion design; at 8 million tonnes, it could easily be a city. In interviews, the designers contemplated the large ship as a possible interstellar ark. This extreme design could be built with materials and techniques that could be obtained in 1958 or were anticipated to be available shortly after.\n\nMost of the three thousand tonnes of each of the \"super\" Orion's propulsion units would be inert material such as polyethylene, or boron salts, used to transmit the force of the propulsion units detonation to the Orion's pusher plate, and absorb neutrons to minimize fallout. One design proposed by Freeman Dyson for the \"Super Orion\" called for the pusher plate to be composed primarily of uranium or a transuranic element so that upon reaching a nearby star system the plate could be converted to nuclear fuel.\n\nThe Orion nuclear pulse rocket design has extremely high performance. Orion nuclear pulse rockets using nuclear fission type pulse units were originally intended for use on interplanetary space flights.\n\nMissions that were designed for an Orion vehicle in the original project included single stage (i.e., directly from Earth's surface) to Mars and back, and a trip to one of the moons of Saturn.\n\nFreeman Dyson performed the first analysis of what kinds of Orion missions were possible to reach Alpha Centauri, the nearest star system to the Sun. His 1968 paper \"Interstellar Transport\" (\"Physics Today\", October 1968, pp. 41–45) retained the concept of large nuclear explosions but Dyson moved away from the use of fission bombs and considered the use of one megaton deuterium fusion explosions instead. His conclusions were simple: the debris velocity of fusion explosions was probably in the 3000–30,000 km/s range and the reflecting geometry of Orion's hemispherical pusher plate would reduce that range to 750–15,000 km/s.\n\nTo estimate the upper and lower limits of what could be done using contemporary technology (in 1968), Dyson considered two starship designs. The more conservative \"energy limited\" pusher plate design simply had to absorb all the thermal energy of each impinging explosion (4×10 joules, half of which would be absorbed by the pusher plate) without melting. Dyson estimated that if the exposed surface consisted of copper with a thickness of 1 mm, then the diameter and mass of the hemispherical pusher plate would have to be 20 kilometers and 5 million tonnes, respectively. 100 seconds would be required to allow the copper to radiatively cool before the next explosion. It would then take on the order of 1000 years for the energy-limited heat sink Orion design to reach Alpha Centauri.\n\nIn order to improve on this performance while reducing size and cost, Dyson also considered an alternative \"momentum limited\" pusher plate design where an ablation coating of the exposed surface is substituted to get rid of the excess heat. The limitation is then set by the capacity of shock absorbers to transfer momentum from the impulsively accelerated pusher plate to the smoothly accelerated vehicle. Dyson calculated that the properties of available materials limited the velocity transferred by each explosion to ~30 meters per second independent of the size and nature of the explosion. If the vehicle is to be accelerated at 1 Earth gravity (9.81 m/s) with this velocity transfer, then the pulse rate is one explosion every three seconds. The dimensions and performance of Dyson's vehicles are given in the table below\n\nLater studies indicate that the top cruise velocity that can theoretically be achieved are a few percent of the speed of light (0.08-0.1c). An atomic (fission) Orion can achieve perhaps 9%-11% of the speed of light. A nuclear pulse drive starship powered by Fusion-antimatter catalyzed nuclear pulse propulsion units would be similarly in the 10% range and pure Matter-antimatter annihilation rockets would be theoretically capable of obtaining a velocity between 50% to 80% of the speed of light. In each case saving fuel for slowing down halves the max. speed. The concept of using a magnetic sail to decelerate the spacecraft as it approaches its destination has been discussed as an alternative to using propellant; this would allow the ship to travel near the maximum theoretical velocity.\n\nAt 0.1\"c\", Orion thermonuclear starships would require a flight time of at least 44 years to reach Alpha Centauri, not counting time needed to reach that speed (about 36 days at constant acceleration of 1\"g\" or 9.8 m/s). At 0.1\"c\", an Orion starship would require 100 years to travel 10 light years. The astronomer Carl Sagan suggested that this would be an excellent use for current stockpiles of nuclear weapons.\n\nA concept similar to Orion was designed by the British Interplanetary Society (B.I.S.) in the years 1973–1974. Project Daedalus was to be a robotic interstellar probe to Barnard's Star that would travel at 12% of the speed of light. In 1989, a similar concept was studied by the U.S. Navy and NASA in Project Longshot. Both of these concepts require significant advances in fusion technology, and therefore cannot be built at present, unlike Orion.\n\nFrom 1998 to the present, the nuclear engineering department at Pennsylvania State University has been developing two improved versions of project Orion known as Project ICAN and Project AIMStar using compact antimatter catalyzed nuclear pulse propulsion units, rather than the large inertial confinement fusion ignition systems proposed in Project Daedalus and Longshot.\n\nThe expense of the fissionable materials required was thought to be high, until the physicist Ted Taylor showed that with the right designs for explosives, the amount of fissionables used on launch was close to constant for every size of Orion from 2,000 tons to 8,000,000 tons. The larger bombs used more explosives to super-compress the fissionables, increasing efficiency. The extra debris from the explosives also serves as additional propulsion mass.\n\nThe bulk of costs for historical nuclear defense programs have been for delivery and support systems, rather than for production cost of the bombs directly (with warheads being 7% of the U.S. 1946-1996 expense total according to one study). After initial infrastructure development and investment, the marginal cost of additional nuclear bombs in mass production can be relatively low. In the 1980s, some U.S. thermonuclear warheads had $1.1 million estimated cost each ($630 million for 560). For the perhaps simpler fission pulse units to be used by one Orion design, a 1964 source estimated a cost of $40000 or less each in mass production, which would be up to approximately $0.3 million each in modern-day dollars adjusted for inflation.\n\nProject Daedalus later proposed fusion explosives (deuterium or tritium pellets) detonated by electron beam inertial confinement. This is the same principle behind inertial confinement fusion. Theoretically, it could be scaled down to far smaller explosions, and require small shock absorbers.\n\nFrom 1957 to 1964 this information was used to design a spacecraft propulsion system called Orion, in which nuclear explosives would be thrown behind a pusher-plate mounted on the bottom of a spacecraft and exploded. The shock wave and radiation from the detonation would impact against the underside of the pusher plate, giving it a powerful push. The pusher plate would be mounted on large two-stage shock absorbers that would smoothly transmit acceleration to the rest of the spacecraft.\n\nDuring take-off, there were concerns of danger from fluidic shrapnel being reflected from the ground. One proposed solution was to use a flat plate of conventional explosives spread over the pusher plate, and detonate this to lift the ship from the ground before going nuclear. This would lift the ship far enough into the air that the first focused nuclear blast would not create debris capable of harming the ship.\nA preliminary design for a nuclear pulse unit was produced. It proposed the use of a shaped-charge fusion-boosted fission explosive. The explosive was wrapped in a beryllium oxide channel filler, which was surrounded by a uranium radiation mirror. The mirror and channel filler were open ended, and in this open end a flat plate of tungsten propellant was placed. The whole unit was built into a can with a diameter no larger than and weighed just over so it could be handled by machinery scaled-up from a soft-drink vending machine; Coca-Cola was consulted on the design.\n\nAt 1 microsecond after ignition the gamma bomb plasma and neutrons would heat the channel filler and be somewhat contained by the uranium shell. At 2–3 microseconds the channel filler would transmit some of the energy to the propellant, which vaporized. The flat plate of propellant formed a cigar-shaped explosion aimed at the pusher plate.\n\nThe plasma would cool to 14,000 °C as it traversed the 25 m distance to the pusher plate and then reheat to 67,000 °C as, at about 300 microseconds, it hits the pusher plate and is recompressed. This temperature emits ultraviolet light, which is poorly transmitted through most plasmas. This helps keep the pusher plate cool. The cigar shaped distribution profile and low density of the plasma reduces the instantaneous shock to the pusher plate.\n\nThe pusher plates thickness would decrease by, approximately, a factor of 6 from the center to the edge so that the net velocity of the inner and outer parts of the plate are the same, even though the momentum transferred by the plasma increases from the center outwards.\n\nAt low altitudes where the surrounding air is dense gamma scattering could potentially harm the crew without a radiation shield, a radiation refuge would also be necessary on long missions to survive solar flares. Radiation shielding effectiveness increases exponentially with shield thickness, see gamma ray for a discussion of shielding. On ships with a mass greater than 1,000 tonnes the structural bulk of the ship, its stores along with the mass of the bombs and propellant, would provide more than adequate shielding for the crew. Stability was initially thought to be a problem due to inaccuracies in the placement of the bombs, but it was later shown that the effects would cancel out.\n\nNumerous model flight tests, using conventional explosives, were conducted at Point Loma, San Diego in 1959. On November 14, 1959 the one-meter model, also known as \"Hot Rod\" and \"putt-putt\", first flew using RDX (chemical explosives) in a controlled flight for 23 seconds to a height of 56 meters. Film of the tests has been transcribed to video and were featured on the BBC TV program \"To Mars by A-Bomb\" in 2003 with comments by Freeman Dyson and Arthur C. Clarke. The model landed by parachute undamaged and is in the collection of the Smithsonian National Air and Space Museum.\n\nThe first proposed shock absorber was a ring-shaped airbag. It was soon realized that, should an explosion fail, the 500-1000 tonne pusher plate would tear away the airbag on the rebound. So a two-stage detuned spring and piston shock absorber design was developed. On the reference design the first stage mechanical absorber was tuned to 4.5 times the pulse frequency whilst the second stage gas piston was tuned to 0.5 times the pulse frequency. This permitted timing tolerances of 10 ms in each explosion.\n\nThe final design coped with bomb failure by overshooting and rebounding into a center position. Thus following a failure and on initial ground launch it would be necessary to start or restart the sequence with a lower yield device. In the 1950s methods of adjusting bomb yield were in their infancy and considerable thought was given to providing a means of swapping out a standard yield bomb for a smaller yield one in a 2 or 3 second time frame or to provide an alternative means of firing low yield bombs. Modern variable yield devices would allow a single standardized explosive to be tuned down, configured to a lower yield, automatically.\n\nThe bombs had to be launched behind the pusher plate with enough velocity to explode 20–30 meters beyond it every 1.1 seconds. Numerous proposals were investigated, from multiple guns poking over the edge of the pusher plate to rocket propelled bombs launched from roller coaster tracks, however the final reference design used a simple gas gun to shoot the devices through a hole in the center of the pusher plate.\n\nExposure to repeated nuclear blasts raises the problem of \"ablation\" (erosion) of the pusher plate. Calculations and experiments indicated that a steel pusher plate would ablate less than 1 mm, if unprotected. If sprayed with an oil it would not ablate at all (this was discovered by accident; a test plate had oily fingerprints on it and the fingerprints suffered no ablation). The absorption spectra of carbon and hydrogen minimize heating. The design temperature of the shockwave, 67,000 °C, emits ultraviolet light. Most materials and elements are opaque to ultraviolet especially at the 340 MPa pressures the plate experiences. This prevents the plate from melting or ablating.\n\nOne issue that remained unresolved at the conclusion of the project was whether or not the turbulence created by the combination of the propellant and ablated pusher plate would dramatically increase the total ablation of the pusher plate. According to Freeman Dyson in the 1960s they would have had to actually perform a test with a real nuclear explosive to determine this; with modern simulation technology this could be determined fairly accurately without such empirical investigation.\n\nAnother potential problem with the pusher plate is that of spalling—shards of metal—potentially flying off the top of the plate. The shockwave from the impacting plasma on the bottom of the plate passes through the plate and reaches the top surface. At that point spalling may occur damaging the pusher plate. For that reason alternative substances, plywood and fiberglass, were investigated for the surface layer of the pusher plate and thought to be acceptable.\n\nIf the conventional explosives in the nuclear bomb detonate but a nuclear explosion does not ignite, shrapnel could strike and potentially critically damage the pusher plate.\n\nTrue engineering tests of the vehicle systems were thought to be impossible because several thousand nuclear explosions could not be performed in any one place. Experiments were designed to test pusher plates in nuclear fireballs and long-term tests of pusher plates could occur in space. Several of these tests almost flew. The shock-absorber designs could be tested at full-scale on Earth using chemical explosives.\n\nBut the main unsolved problem for a launch from the surface of the Earth was thought to be nuclear fallout. Freeman Dyson, group leader on the project, estimated back in the 1960s that with conventional nuclear weapons each launch would statistically cause on average between 0.1 and 1 fatal cancers from the fallout. That estimate is based on no threshold model assumptions, a method often used in estimates of statistical deaths from other industrial activities. Each few million dollars of efficiency indirectly gained or lost in the world economy may statistically average lives saved or lost, in terms of opportunity gains versus costs. Indirect effects could matter for whether the overall influence of an Orion-based space program on future human global mortality would be a net increase or a net decrease, including if change in launch costs and capabilities affected space exploration, space colonization, the odds of long-term human species survival, space-based solar power, or other hypotheticals.\n\nDanger to human life was not a reason given for shelving the project. The reasons included lack of a mission requirement, the fact that no one in the U.S. government could think of any reason to put thousands of tons of payload into orbit, the decision to focus on rockets for the Moon mission, and ultimately the signing of the Partial Test Ban Treaty in 1963. The danger to electronic systems on the ground from electromagnetic pulse was not considered to be significant from the sub-kiloton blasts proposed since solid-state integrated circuits were not in general use at the time.\n\nFrom many smaller detonations combined the fallout for the entire launch of a 6,000 short ton (5,500 metric ton) Orion is equal to the detonation of a typical 10 megaton (40 petajoule) nuclear weapon as an air_burst, therefore most of its fallout would be the comparatively dilute delayed fallout. Assuming the use of nuclear explosives with a high portion of total yield from fission, it would produce a combined fallout total similar to the surface burst yield of the \"Mike\" shot of Operation Ivy, a 10.4 Megaton device detonated in 1952. The comparison is not quite perfect as, due to its surface burst location, \"Ivy Mike\" created a large amount of early fallout contamination. Historical above-ground nuclear weapon tests included 189 megatons of fission yield and caused average global radiation exposure per person peaking at 0.11 mSv/a in 1963, with a 0.007 mSv/a residual in modern times, superimposed upon other sources of exposure, primarily natural background radiation, which averages 2.4 mSv/a globally but varies greatly, such as 6 mSv/a in some high-altitude cities. Any comparison would be influenced by how population dosage is affected by detonation locations, with very remote sites preferred.\n\nWith special designs of the nuclear explosive Ted Taylor estimated that fission product fallout could be reduced tenfold, or even to zero, if a pure fusion explosive could be constructed instead. A 100% pure fusion explosive has yet to be successfully developed, according to declassified US government documents, although relatively clean PNEs (Peaceful nuclear explosions) were tested for canal excavation by the Soviet Union in the 1970s with 98% fusion yield in the \"Taiga\" test's 15 kiloton devices, 0.3 kilotons fission, which excavated part of the proposed Pechora–Kama Canal.\n\nThe vehicle's propulsion system and its test program would violate the Partial Test Ban Treaty of 1963, as currently written, which prohibits all nuclear detonations except those conducted underground as an attempt to slow the arms race and to limit the amount of radiation in the atmosphere caused by nuclear detonations. There was an effort by the US government to put an exception into the 1963 treaty to allow for the use of nuclear propulsion for spaceflight but Soviet fears about military applications kept the exception out of the treaty. This limitation would affect only the US, Russia, and the United Kingdom. It would also violate the Comprehensive Nuclear-Test-Ban Treaty which has been signed by the United States and China as well as the de facto moratorium on nuclear testing that the declared nuclear powers have imposed since the 1990s.\n\nThe launch of such an Orion nuclear bomb rocket from the ground or low Earth orbit would generate an electromagnetic pulse that could cause significant damage to computers and satellites as well as flooding the van Allen belts with high-energy radiation. This problem might be solved by launching from very remote areas, the EMP footprint would be a few hundred miles wide. A few relatively small space-based electrodynamic tethers could be deployed to quickly eject the energetic particles from the capture angles of the Van Allen belts.\n\nAn Orion spacecraft could be boosted by non-nuclear means to a safer distance only activating its drive well away from Earth and its satellites. The Lofstrom launch loop or a space elevator hypothetically provide excellent solutions; in the case of the space elevator, existing carbon nanotubes composites, with the exception of possibly Colossal carbon tubes do not yet have sufficient tensile strength. All chemical rocket designs are extremely inefficient and expensive when launching large mass into orbit but could be employed if the result were cost effective.\n\nA test that was similar to the test of a pusher plate occurred as an accidental side effect of a nuclear containment test called \"Pascal-B\" conducted on 27 August 1957. The test's experimental designer Dr. Robert Brownlee performed a highly approximate calculation that suggested that the low-yield nuclear explosive would accelerate the massive (900 kg) steel capping plate to six times escape velocity. The plate was never found but Dr. Brownlee believes that the plate never left the atmosphere, for example it could have been vaporized by compression heating of the atmosphere due to its high speed. The calculated velocity was interesting enough that the crew trained a high-speed camera on the plate which, unfortunately, only appeared in one frame indicating a very high lower bound for the speed of the plate.\n\nAs discussed by Arthur C. Clarke in his recollections of the making of \"\" in \"The Lost Worlds of 2001\", a nuclear pulse version of the \"Discovery\" was considered. However the \"Discovery\" in the movie did not use this idea as Stanley Kubrick thought it might be considered parody after making \"\".\n\n\n"}
{"id": "399116", "url": "https://en.wikipedia.org/wiki?curid=399116", "title": "Quantum solvent", "text": "Quantum solvent\n\nA quantum solvent is essentially a superfluid (aka a quantum liquid) used to dissolve another chemical species. Any superfluid can theoretically act as a quantum solvent, but in practice the only viable superfluid medium that can currently be used is helium-4, and it has been successfully accomplished in controlled conditions. Such solvents are currently under investigation for use in spectroscopic techniques in the field of analytical chemistry, due to their superior kinetic properties.\n\nAny matter dissolved (or otherwise suspended) in the superfluid will tend to aggregate together in clumps, encapsulated by a 'quantum solvation shell'. Due to the totally frictionless nature of the superfluid medium, the entire object then proceeds to act very much like a nanoscopic ball bearing, allowing effectively complete rotational freedom of the solvated chemical species. A quantum solvation shell consists of a region of non-superfluid helium-4 atoms that surround the molecule(s) and exhibit adiabatic following around the centre of gravity of the solute. As such, the kinetics of an effectively gaseous molecule can be studied without the need to use an actual gas (which can be impractical or impossible). It is necessary to make a small alteration to the rotational constant of the chemical species being examined, in order to compensate for the higher mass entailed by the quantum solvation shell. \n\nQuantum solvation has so far been achieved with a number of organic, inorganic and organometallic compounds, and it has been speculated that as well as the obvious use in the field of spectroscopy, quantum solvents could be used as tools in nanoscale chemical engineering, perhaps to manufacture components for use in nanotechnology.\n"}
{"id": "445066", "url": "https://en.wikipedia.org/wiki?curid=445066", "title": "RoboCop", "text": "RoboCop\n\nRoboCop is a 1987 American cyberpunk action film directed by Paul Verhoeven and written by Edward Neumeier and Michael Miner. The film stars Peter Weller, Nancy Allen, Dan O'Herlihy, Kurtwood Smith, Miguel Ferrer, and Ronny Cox. Set in a crime-ridden Detroit, Michigan, in the near future, \"RoboCop\" centers on police officer Alex Murphy (Weller) who is murdered by a gang of criminals and subsequently revived by the megacorporation Omni Consumer Products (OCP) as a superhuman cyborg law enforcer known as RoboCop.\n\nThemes that make up the basis of \"RoboCop\" include media influence, gentrification, corruption, authoritarianism, greed, privatization, capitalism, identity, dystopia and human nature. It received positive reviews and was cited as one of the best films of 1987, spawning a franchise that included merchandise, two sequels, , a remake, two animated TV series, a television , video games, and a number of comic book adaptations/crossovers. The film was produced for a relatively modest $13 million. Honors for the film include five Saturn Awards, two BAFTA Award nominations and the Academy Award for Best Sound Editing, along with nominations for Best Film Editing and Best Sound Mixing.\n\nIn a dystopian future Detroit, Michigan, is a struggling city on the verge of total collapse due to financial ruin and a high crime rate. The mayor signs a deal with the mega-corporation Omni Consumer Products (OCP), giving it complete control of the underfunded Detroit Police Department. In exchange, OCP will be allowed to turn the run-down sections of Detroit into a high-end utopia called Delta City. OCP's Senior Vice President Richard \"Dick\" Jones proposes assisting the police with the ED-209 law enforcement droid. At its first demonstration, however, ED-209 malfunctions and gruesomely kills an innocent young executive named Kinney. Robert \"Bob\" Morton, an ambitious executive, uses the opportunity to introduce his own experimental cyborg design, \"RoboCop\". To Jones's anger, the company chairman - referred to as the Old Man - approves Morton's plan.\n\nOCP reassigns officers into crime-ridden districts in anticipation that someone will be killed in action for use as a test subject for RoboCop. Elsewhere, Officer Alexander James \"Alex\" Murphy, who has been transferred from Metro South, joins the force and becomes partners with Officer Anne Lewis. On their first patrol, they chase a gang of criminals led by the notorious Clarence Boddicker, which leads to one crew member getting thrown at the police car. The distraction allows Boddicker's gang to hide in an abandoned steel mill where Murphy and Lewis separate on foot to find them. While Murphy manages to kill one of them and attempts to arrest another, he is caught by Boddicker and the others; the gang tortures Murphy, brutally gunning him down before Boddicker delivers the finishing blow. He is declared dead, and OCP selects him as the RoboCop candidate; they replace most of his body with cybernetics, leaving only his human brain. RoboCop is programmed with three directives: serve the public trust, protect the innocent, and uphold the law; but he is left unaware of a secret fourth directive.\n\nRoboCop, assigned to Murphy's former district, proceeds to efficiently rid the streets of crime. Despite this, however, the deteriorating situation in Detroit, led by the mismanagement of OCP, causes the human officers to threaten to strike. Lewis, who witnessed Murphy's \"death\", believes RoboCop is Murphy, based on his exhibiting some behaviors from his old life. As RoboCop suffers from latent memories from Murphy, he discovers his true identity and that his wife and son moved away after Murphy's death. Meanwhile, Jones becomes frustrated when Morton gets promoted for his latest success with RoboCop's reputation; fearing that he will be displaced, Jones discreetly hires Boddicker to kill Morton at his home.\n\nDuring a hold-up at a gas station, RoboCop recognizes the robber as Boddicker’s henchman Emil Antonowsky, and after a shootout that results in the gas station exploding and Emil being injured, manages to arrest him. RoboCop reviews police records, identifies Boddicker and the other gang members before confronting them at a drug deal. During his arrest, Boddicker admits to being hired by Jones, and with this evidence recorded on video, RoboCop attempts to arrest Jones at OCP. Jones openly admits his role in Morton's death but reveals the fourth directive: RoboCop will shut down if attempting to arrest any senior officer of OCP. With RoboCop's programming limiting his abilities, Jones sends an ED-209 stationed in his office to attack RoboCop, severely damaging him with its heavier firepower. RoboCop manages to evade ED-209, but is ambushed by a SWAT unit who have been authorized by Jones to eliminate RoboCop. Lewis, who had been following RoboCop, helps him escape to safety at the steel mill and undergo repairs.\n\nThe police follow through with their strike, creating chaos in the city. Boddicker and his gang are released from prison and acquire new high-powered rifles from Jones to finish off RoboCop. Boddicker, using a tracking system provided to him by Jones, heads towards the steel mill. RoboCop and Lewis work together to eliminate the gang; Lewis gets shot by Boddicker before he attacks RoboCop, who uses his data spike to fatally stab Boddicker in the neck.\n\nRoboCop subsequently heads to OCP headquarters alone, barges in on an executive meeting where the Old Man and Jones are present, and shows them the video of Jones' confession. Jones quickly takes the Old Man hostage, knowing that the fourth directive still protects him. The Old Man promptly fires Jones, nullifying the protection and allowing RoboCop to shoot Jones, who falls out of a window to his death. Grateful, the Old Man commends RoboCop's shooting and asks what his name is; RoboCop smiles and answers, \"Murphy\".\n\n\"RoboCop\" was written by Edward Neumeier and Michael Miner. Neumeier stated that he first got the idea of \"RoboCop\" when he walked with a friend past a poster for \"Blade Runner\". He asked his friend what the film was about and his friend replied, \"It's about a cop hunting robots\". For him, this sparked the idea about a robot cop. While the two were attempting to pitch the screenplay to Hollywood executives, they were accidentally stranded at an airplane terminal with a high-ranking movie executive for several hours. Here, they were able to speak to him about the project, and thus began the series of events which eventually gave rise to \"RoboCop\".\n\nIn 1981 Neumeier wrote the first treatment, about a robot police officer who was not a cyborg but in the development of the story his computer mind became more similar to human. The plot takes place in a fairly distant future, the world is ruled by corporations and it was assumed that this world would be visually similar to the world shown in \"Blade Runner\". The treatment was rejected by many studios because of the incompleteness of the storyline and settings. In 1984 Neumeier met music video director Michael Miner, who worked on a similar idea; his rough draft of the script was called \"SuperCop\", which was about a police officer who has been seriously injured and becomes a donor for experiment to create a cybernetic police officer. Neumeier and Miner felt that they could successfully combine their ideas.\n\n\"RoboCop\" marked the first major Hollywood production for Dutch director Paul Verhoeven. Although he had been working in the Netherlands for more than a decade and directed several films to great acclaim, such as \"Soldier of Orange\" in 1977, he moved to Hollywood in 1984 to seek broader opportunities. While \"RoboCop\" is often credited as his English language debut, he had in fact previously made \"Flesh & Blood\", starring Rutger Hauer and Jennifer Jason Leigh, during 1985.\n\nOn the Criterion Edition audio commentary (available on both the LaserDisc and DVD versions), Verhoeven recalls that, when he first glanced through the script, he discarded it in disgust. Afterwards, his wife, after picking the script from the bin and reading it more thoroughly, convinced him that the plot had more substance than he had originally assumed. \"Repo Man\" director Alex Cox was offered the opportunity to direct before Verhoeven came aboard. Kenneth Johnson, creator of television series \"V\", \"The Bionic Woman\", and \"The Incredible Hulk\", said that he was offered the chance to direct, but turned it down when he was not allowed to change aspects of the script that he considered to be \"mean-spirited, ugly and ultra-violent.\"\n\nThe character of RoboCop itself was inspired by British comic book hero Judge Dredd, as well as the Japanese toku series \"Space Sheriff Gavan\" and the Marvel Comics superhero Rom. A \"Rom\" comic book appears onscreen during the film's convenience store robbery. Another \"Rom\" comic appears in a flashback depicting Murphy's son. Although both Neumeier and Verhoeven have declared themselves staunchly on the political left, Neumeier recalls on the audio commentary to \"Starship Troopers\" that many of his liberal friends perceived \"RoboCop\" as a fascist movie. On the 20th Anniversary DVD, producer Jon Davison referred to the film's message as \"fascism for liberals\"a politically liberal film done in the most violent way possible.\n\nBefore Peter Weller was cast, Rutger Hauer and Michael Ironside were favored to play RoboCop by Verhoeven and the producers, respectively. However, each man's large frame would have made it difficult for either of them to move in the cumbersome RoboCop suit, which had been modeled on hockey gear and designed to be large and bulky. Weller won the role both because Verhoeven felt that he could adequately convey pathos with his lower face, and because Weller was especially lithe and could more easily move inside the suit than a bigger actor.\n\nStephanie Zimbalist, who at the time was one of the stars of the television series \"Remington Steele\", was originally cast as Anne Lewis. NBC had canceled \"Remington Steele\" in 1986, leaving the stars free to accept other roles, subject to options for further episodes on their contracts. However, an upsurge of interest in the show saw the network exercise the options, which meant that Zimbalist was then forced to withdraw from \"RoboCop\", to be replaced by Nancy Allen. Similarly, this move forbid Pierce Brosnan from accepting the role of James Bond in The Living Daylights.\n\nIn the DVD director's commentary, Verhoeven explained that he intentionally chose to cast Kurtwood Smith and Ronny Cox against type by making them the central villains. Cox was an actor who, until then, was primarily known for \"nice-guy\" roles, such as fatherly figures. Similarly, Smith had been cast as more intellectual characters. Verhoeven chose to outfit Smith's character Clarence Boddicker in rimless glasses because of their intellectual association, creating a disparity in the character that Verhoeven found akin to the similarly bespectacled Heinrich Himmler.\n\nFilming began on August 6, 1986, and wrapped on October 20, 1986. The scenes depicting Murphy's death were not filmed until the following January (1987), some months after principal shooting had ceased. Although \"RoboCop\" is set in Detroit, Michigan, many of the urban settings in the film were actually filmed in Dallas, Texas. The futuristic appearances of the Dallas buildings, such as Reunion Tower, are visible in the background during the car chase. The front of Dallas City Hall was used as the exterior for the fictional OCP Headquarters, combined with extensive matte paintings to make the building appear taller than it actually is. The steel mill scenes were filmed at Wheeling-Pittsburgh Steel's Monessen Works, in the Pittsburgh suburb of Monessen, Pennsylvania.\n\nPeter Weller had prepared extensively for the role using a padded costume, as development of the actual RoboCop suit was three weeks behind schedule. By the time shooting was underway and the costume had arrived on set, however, Weller discovered he was almost unable to move in it and needed additional training to become accustomed to it. Weller later revealed to Roger Ebert that during filming, he was losing three pounds a day due to sweat loss while wearing the RoboCop suit in 100 °F (38 °C) temperatures. Weller's personal assistant, Todd Trotter, was responsible for keeping the actor cool in between takes with electric fans and, when available, large ducts connected to free-standing air conditioning units. The suit later had a fan built into it.\n\nMonte Hellman acted as second unit director after Verhoeven began to fall behind schedule. He directed several action scenes.\n\nThe 1986 Ford Taurus was used as the police cruiser in the movie, due to its then-futuristic design. As of May 2012, RoboCop's Taurus is on display at the Branson Auto Museum in Branson, Missouri.\n\nBob Morton's home was filmed in a home near Dallas featured in the show \"Beyond 2000\" in 1987.\n\nThe task of creating the RoboCop suit was given to Rob Bottin. The studio decided that Bottin would be the ideal person to create the RoboCop suit, as he had just finished doing the special effects for John Carpenter's \"The Thing\". A budget of up to one million dollars was allotted to the completion of the suit, making it the most expensive item on the set. A total of six suits were made: three intact and three showing damage.\n\nBottin himself had produced early design sketches for the suit's prototype that the studio accepted enthusiastically, albeit with the request of some minor adjustments. Influenced by the Japanese comic \"The 8 Man\" and the first Metal Hero \"Space Sheriff Gavan\" from Toei, Rob, Paul Verhoeven, and Edward Neumeier came up with the concept of the suit being more of an outer shell, with very little of the actor's actual face being visible. Bottin explained the basis of the design:\nIt's meant to look very speedy and aerodynamic. All the lines are measured to go on a slantforward, forward, forward! All the lines were geometric, and complement every shape on the body from all angles. When Verhoeven came on the project, he requested numerous design changes, additions to the suit which looked more like machine than man-like. I've never done so many conceptional drawings for a director in my entire lifechanging it, and changing it, and changing it!\n\nHowever, the design ended up bearing a closer resemblance to Bottin's original design:\nRoboCop looks the way he does because that's the way a man's body works! Although we went through fifty different variations, developing his character, everything came back to man-like. It's definitely a guy in the suit, which doesn't belittle it any.\n\nThe suit itself was attached to the actor in sections. To wear the helmet, Peter Weller wore a bald cap that allowed the helmet to be removed easily. After almost 10 months of preparation, the RoboCop suit was completed based on life casts from Peter Weller and Bottin's six-foot clay models. The suit's color was supposed to be bright blue; however, it was given a more grayish tint to make it look more metallic and produce less glare on the camera when it was being filmed.\n\nPeter Weller had in the meantime hired Moni Yakim, the head of the Movement Department at Juilliard, to help create an appropriate way for him to move his body while wearing the RoboCop suit. He and Moni had envisioned RoboCop moving like a snake, dancing around its targets very elusively. The suit, however, proved to be too heavy and cumbersome. Instead, at the suggestion of Moni, it was decided that they would slow down RoboCop's movements in order to make them more appealing and plausible. Filming stopped for three days, allowing Peter and Paul Verhoeven to discuss new movements for the suit.\n\nThe original gun for RoboCop was a Desert Eagle, but this was deemed too small. A Beretta 93R was heavily modified by Ray Williams of Freshour Machine, Texas City, Texas, who extended the gun barrel to make it look bigger and more proportional to RoboCop's hand. The gun holster itself was a standalone piece that was not integrated into the suit. Off-screen technicians would operate the device on cue by pulling cables that would force the holster to open up and allow the gun to be placed inside.\n\nThe ED-209 stop motion model was designed by Craig Hayes, who also built the full size models, and animated by Phil Tippett, a veteran stop-motion animator. As one of the setpieces of the movie, the ED-209's look and animated sequences were under the close supervision of director Paul Verhoeven, who sometimes acted out the robot's movements himself. ED-209 was voiced by producer Jon Davison. Davies and Tippett would go on to collaborate on many more projects.\n\nIn one scene, Emil attempts to run down RoboCop, but instead accidentally drives into a vat of toxic waste, causing the flesh to melt off his face and hands. These effects were also conceived and designed by Bottin, who was inspired by Rick Baker's work on \"The Incredible Melting Man\", and who dubbed the \"RoboCop\" effects \"the Melting Man\" as an homage to the production.\n\nChiodo Brothers Productions fabricated and animated the dinosaur puppet in the 6000 SUX commercial. The dinosaur itself was animated by Don Waller, who also had a cameo in the same sequence, reacting to the rampaging creature in a tight close-up.\n\nThe soundtrack score for the movie was composed by Basil Poledouris, who used both synthesized and orchestral music as a mirror to the man-versus-machine theme of the movie. The score alternates brass-heavy material, including the RoboCop theme and ED-209's theme, with more introverted pieces for strings, such as during RoboCop's homecoming scene. The music was performed by the Sinfonia of London, conducted by Howard Blake and Tony Britten. The soundtrack initially was released by Varèse Sarabande containing highlights from the score in an order different from that heard in the movie. The final four tracks were included on later CD re-issues.\n\nThe complete score in film sequence order was released in 2015 containing four previously unreleased tracks, the full end credits suite and the four previous CD bonus tracks.\n\nOn the theatrical trailer, the theme of \"The Terminator\" (1984) was used instead of the RoboCop theme. The theme song also made its way into the arcade and NES \"RoboCop\" video games.\n\nThe song \"Show Me Your Spine\" by P.T.P. was played during the nightclub scene. P.T.P was a short-lived side project consisting of members of the band Ministry and Skinny Puppy. However, this song was not available in any official form and could only be heard in the film. It was eventually released in 2004 on a compilation album called \"Side Trax\" by Ministry.\n\nThe movie was originally given an X rating by the Motion Picture Association of America (MPAA) in 1987 due to its graphic violence, in sharp contrast to most other X-rated movies that received the rating due to strong sexual content. To appease the requirements of the ratings board, Verhoeven reduced the blood and gore in the most violent scenes in the movie, including ED-209's shooting of Kinney in the boardroom, Bobby being shot in the leg, the Boddicker gang's execution of Murphy with shotguns, and the final battle with Boddicker (in which RoboCop stabs him in the neck with his neural spike and Boddicker's blood spatters onto RoboCop's chest). Verhoeven also added humorous commercials throughout the news broadcasts to lighten the mood and distract from the violent aspects of the movie (most of the commercials satirize various aspects of the American consumer culture, such as the commercial for the 6000 SUX sedan). After 11 original X ratings, the film was eventually given an R rating. The original uncut version was included on the Criterion Collection LaserDisc and DVD of the film (both out of print), the 2005 trilogy box set, and the 2007 anniversary edition—the latter two were released by MGM and were unrated. The 2014 Blu-ray 4K master edition also features this unrated cut.\n\nRegarding the omitted scenes, Verhoeven stated on the 2007 anniversary edition DVD that he had wanted the violence to be \"over the top\", in an almost comical fashion (such as the scene involving the executive that is killed by ED-209, and Bob Morton immediately asking \"Somebody wanna call a \"goddamn\" paramedic?!\", which was meant as black comedy). Verhoeven also stated that the tone of the violence was changed to a more upsetting tone due to the deletions requested by the MPAA, and that the deletions also removed footage of the extensive animatronic puppet of Murphy just before he is executed by Boddicker.\n\nThe following scenes are now presented in the uncut version of the movie available on Blu-ray and DVD:\n\n1) During the boardroom meeting, ED-209 begins to shoot Kinney who falls back on a table. Additional shots are then shown of him being shot at continuously in rapid succession. There is also an additional, quick shot of ED-209's programmer unsuccessfully trying to shut it down by ripping out its power cord.\n\n2) Murphy's death scene is the most altered scene in the movie. Additional shots include when his hand is shot off, Murphy gets up and a visible stump is seen where his hand used to be. An additional shot shows his right arm being shot off completely and him screaming in pain. In the theatrical version, this scene is missing and Murphy is merely shown with one arm without any explanation. The final seconds of his murder shows an additional shot of the camera spinning around Murphy as he withers in pain and Clarence shoots him in the head and a hole appears. This original shot was achieved using a sophisticated animatronic puppet of Peter Weller controlled off-screen. One final overhead shot also shows the bloody aftermath of what has happened to Murphy as Lewis looks on shocked at his dead body.\n\n3) Bob Morton's death now shows a close up of additional bullet holes puncturing his leg before he falls down.\n\n4) When Clarence Boddiker is stabbed in the neck, there is an additional shot of him holding his neck as he turns around and blood spurts from the wound like a fountain.\n\n\"RoboCop\" was released in American theaters on July 17, 1987. The film opened no. 1 at the US box office and grossed over $8 million in its opening weekend and another $6 million in its second weekend, again regaining the top spot at the box office. It topped rival films released at the same time, including \"Full Metal Jacket\" and \"Superman IV\". In total, it grossed $53.4 million during its North American run, making it the 16th most successful film that year. It also grossed an additional $24,036,000 from video rentals in the United States.\n\nThe R-rated cinema version of \"RoboCop\" was released on VHS and LaserDisc in February 1988. It was sold for $89.98 on VHS, and for $39.98 on S-VHS. It grossed $24,036,000 from rentals.\n\nThe film was released on Philips CD-i VCD (Video CD) in 1994. Criterion released the uncut \"director's edit\" on LaserDisc in 1996. In 1998, Criterion released it on DVD, while Image Entertainment released the R-rated cinema version. The R-rated cinema version was slated to be released on Blu-Ray by Sony in 2006. However, Sony pulled the Blu-Ray just days before it was to be released. Only a handful of copies were released to the general public, mainly reviewers, making it very rare. It was re-released on Region 1 DVD in 2007 and on Blu-ray in 2010 as part of the Blu-ray \"Robocop\" Trilogy. A remastered Blu-ray edition was released in January 2014 to tie into the 2014 remake.\n\nOn Metacritic, the film has an average rating of 67/100, indicating \"generally favorable reviews\". Rotten Tomatoes retrospectively gave it a rating of 89% based on reviews from 63 critics and an average rating of 7.8/10. The site's consensus is: \"While over-the-top and gory, \"RoboCop\" is also a surprisingly smart sci-fi flick that uses ultraviolence to disguise its satire of American culture\".\n\nRoger Ebert praised the film, calling \"RoboCop\" \"a thriller with a difference,\" praising the way it puts the audience off-guard, and calling it a thriller not easily categorized with splashes of other genres added. Ebert praised Weller for his performance and his ability to elicit sympathy despite the layers of makeup and prosthetics. Walter Goodman, writing for \"The New York Times\", believed the film's anti-corporate message \"has more trouble emerging from Mr. Verhoeven's sizzling battles than poor Murphy does from his robosuit.\"\n\nFeminist Susan Faludi called \"RoboCop\" one of \"an endless stream of war and action movies\" in which \"women are reduced to mute and incidental characters or banished altogether.\" Author Rene Denfeld disagreed with Faludi's characterization of the film, calling it her \"favorite blow-'em-up movie,\" citing Officer Lewis as an example of an \"independent and smart police officer.\" In the commentary track of the Special Edition DVD release, Verhoeven explained that he intentionally depicted Officer Lewis as \"gender neutral\", thus her hidden gender during her introductory scene, a physical fight with a male in the police station, and the choice to give the character short hair.\n\nIn 2007, \"Entertainment Weekly\" named the film the 14th greatest action film of all time, and Complex rated it as the 19th best action film of all time in 2016. In 2008, it was selected by \"Empire\" magazine as one of \"The 500 Greatest Movies of All Time\", placing at #404. \"The New York Times\" also included the film in their list of \"The Best 1000 Movies Ever Made\". AMC Filmsite.org and Film.com rated it as one of the best films of 1987.\n\nThe film was on the ballot for two of the American Film Institute's \"100 Series\" lists. These included \"100 Years…100 Thrills\", a list of America's most heart-pounding films, and AFI's \"Ten Top Ten\", a list of the best 10 films in 10 \"classic\" American film genres. \"RoboCop\" was a candidate in the science fiction category. At its release, British director Ken Russell said that this was the best science fiction film since Fritz Lang's \"Metropolis\" (1927).\n\nIn a 2013 interview, Edward Neumeier reflected on how the film's script is starting to play into reality: \"We are now living in the world that I was proposing in \"RoboCop\" … how big corporations will 'take care of us' and … how they won't.\"\n\nTwo of the primary themes explored by \"RoboCop\" are the media and human nature. On the Criterion Edition DVD commentary track, executive producer Jon Davison and writer Edward Neumeier both relate the film to the decay of American industry from the 1970s through the early 1980s, with the abandoned \"Rust Belt style\" factories that RoboCop and Clarence Boddicker's gang use as hideouts reflecting this concern. Massive unemployment is prevalent, being reported frequently on the news, as are poverty and the crime that results from economic hardship.\n\nDirector Paul Verhoeven, known for his heavy use of Christian symbolism, states in the documentary \"Flesh and Steel: The Making of RoboCop\" (featured on the \"RoboCop\" DVD) that his intention was to portray RoboCop as a Christ figure. This is represented in: Murphy's horrific death (read: crucifixion); his return (read: resurrection) as Robocop; the showdown with Clarence Boddicker at the steel mill, which finds RoboCop trudging through ankle-deep water (this creates the illusion of him walking on water).\n\nSome fans have taken said portrayal a step further, by likening RoboCop's showdown with Dick Jones to Christ's final battle against Satan...\n\n(1) Lucifer's rebellion against God = Jones taking the OCP Chairman hostage\n\n(2) Lucifer being stripped of his prestige and cast out of Heaven = \"Dick, YOU'RE FIRED!\"\n\n(3) Satan's fall from grace = RoboCop blasting Jones backward through a window\n\n(4) The roles of God and Christ as Heavenly Father and Son, the latter of whom had a name few others knew = \"Nice shooting, son; what's your name?\" \"...Murphy.\"\n\n...Verhoeven himself, however, disagrees: \"It's a sharp observation, but none of that was on my mind at the time.\"\n\nDarian Leader considers \"RoboCop\" one example of how the cinema has dealt with the concept of masculinity, showing that to be a man requires more than having the body of a man: something symbolic that is not ultimately human must be added. He sees \"RoboCop\" as similar to \"The Terminator\" and \"The Six Million Dollar Man\" in this respect. Leader wrote of \"RoboCop\": The RoboCop is a family man who is destroyed by thugs, then rebuilt as a robot by science. His son always insists, before the transformation, that his human father perform the gun spinning trick he sees on TV. When the robot can finally do this properly, he is no longer just a male biological body: he is a body plus machinery, a body which includes within it the symbolic circuitry of science. Old heroes had bits of metal \"outside\" them (knights), but modern heroes have bits of metal \"inside\" them. To be a man today thus involves this kind of real incorporation of symbolic properties.\n\nPhilosopher and cultural critic Slavoj Žižek wrote that: \n\"RoboCop\", a futuristic story about a policeman shot to death and then revived after all parts of his body have been replaced by artificial substitutes, introduces a more tragic note: the hero who finds himself literally \"between two deaths\"clinically dead and at the same time provided with a new, mechanical body—starts to remember fragments of his previous, \"human\" life and thus undergoes a process of resubjectivication, changing gradually back from pure incarnated drive to a being of desire. (...) [I]f there is a phenomenon that fully deserves to be called the \"fundamental fantasy of contemporary mass culture,\" it is this fantasy of the return of the living dead: the fantasy of a person who does not want to stay dead but returns again and again to pose a threat to the living.\n\nThe depiction of Murphy's struggles in reasserting his humanity also deals with themes of identity. This is even touched upon in the cyborg's construction. On the \"Robocop: 20th Anniversary Collector's Edition DVD\", Paul Sammon states:\nRob Bottin and Paul Verhoeven, and Ed Neumeier had all come up with a concept that there would be such a potential for psychological disruption. Even if you had supposedly wiped someone's memories and emotions they'd still might have some kind of residual humanity where, if they'd looked at themselves as a complete robot with no relation to their past organic form, they'd completely freak out and have a psychotic breakdown. So the idea was that surgeons had literally skinned off Alex Murphy's face and then placed it on the cyborg. So it's not like they transplanted his head, they just took his face off and laid it on the cyborg, and that was to give him his own little sense of identity.\n\nThe film novelization, written by Ed Naha, was released on June 1, 1987. The novel differed in several ways from the film by following one of the earlier drafts of the screenplay. It expanded on Murphy's struggle with being part man and part machine, and his memories. It also included more \"humanized\" dialogue from RoboCop, as opposed to the minimal, cold dialogue heard in the film.\n\nThe success of the movie spawned a large franchise, including merchandise, two sequels, , a remake, two animated TV series, a television , video games, and a number of comic book adaptations/crossovers.\n\nIn January 2018, it was announced that original \"RoboCop\" writer Ed Neumeier was writing a direct sequel to the 1987 film that would ignore the two previous sequels and the 2014 remake. “We’re not supposed to say too much. There’s been a bunch of other RoboCop movies and there was recently a remake and I would say this would be kind of going back to the old RoboCop we all love and starting there and going forward. So it’s a continuation really of the first movie. In my mind. So it’s a little bit more of the old school thing” Neumeier said.\n\nIn February 2011, a humorous ploy asked Detroit Mayor Dave Bing if there was to be a RoboCop statue in his \"New Detroit\" proposal, which was planned to turn Detroit back into a prosperous city again. When Bing said there was no such plan, and word of this reached the Internet, several fundraising events raised enough money for the statue, which would be built at the Imagination Station. There were plans to unveil the RoboCop statue in spring of 2014. On May 2, 2018, Imagination Station announced an agreement with the Michigan Science Center to serve as the home of the permanent installation of the RoboCop statue.\n\nFollowing MGM's outright acquisition of the defunct Orion Pictures film and television library, including Robocop, Metro-Goldwyn-Mayer and Sony produced a remake of \"RoboCop\", directed by José Padilha. Joel Kinnaman plays the role of Alex Murphy and Gary Oldman is \"Norton\", a new character, \"the scientist who creates RoboCop and finds himself torn between the ideals of the machine trying to rediscover its humanity and the callous needs of a corporation.\" Samuel L. Jackson plays a powerful and charismatic media mogul, while Michael Keaton plays the CEO of Omnicorp after Hugh Laurie dropped out of the project in August 2012. Actress Abbie Cornish plays Murphy's wife and \"Watchmen\" star Jackie Earle Haley plays Maddox, the man who gives RoboCop his military training. The film was finally released in the United States on February 12, 2014.\n\n"}
{"id": "30873089", "url": "https://en.wikipedia.org/wiki?curid=30873089", "title": "Rocket propellant", "text": "Rocket propellant\n\nRocket propellant is a material used either directly by a rocket as the reaction mass (propulsive mass) that is ejected, typically with very high speed, from a rocket engine to produce thrust and thus provide spacecraft propulsion, or indirectly to produce the reaction mass in a chemical reaction. \nEach rocket type requires a different kind of propellant: chemical rockets require propellants capable of undergoing exothermic chemical reactions, which provide the energy to accelerate the resulting gases through the nozzle. Thermal rockets instead use inert propellants of low molecular weight that are chemically compatible with the heating mechanism at high temperatures, while cold gas thrusters use pressurized, easily stored inert gases. Electric propulsion requires propellants that are easily ionized or made into plasma and in the extreme case of nuclear pulse propulsion the propellant consists of many small, non-weapon nuclear explosives of which the resulting shock wave propels the spacecraft away from the explosive, thereby creating propulsion. One such spacecraft was designed (but never built), dubbed as \"Project Orion\" (not to be confused with the NASA Orion spacecraft).\n\nRocket propellant is either a high oxygen-containing fuel or a mixture of fuel plus oxidant, whose combustion takes place, in a definite and controlled manner with the evolution of a huge volume of gas. In the rocket engine, the propellant is burnt in the combustion chamber and the hot jet of gases (usually at a temperature of 3,000°C and a pressure of 300 kg/cm^2 ) escapes through the nozzle at very high velocity. Rockets create thrust by expelling mass backward in a high-speed jet (see \"Newton's Third Law\"). Chemical rockets, the subject of this article, create thrust by reacting propellants within a combustion chamber into a very hot gas at high pressure, which is then expanded and accelerated by passage through a nozzle at the rear of the rocket. The amount of the resulting forward force, known as thrust, that is produced is the mass flow rate of the propellants multiplied by their exhaust velocity (relative to the rocket), as specified by Newton's third law of motion. Thrust is, therefore, the equal and opposite reaction that moves the rocket, and not by the interaction of the exhaust stream with air around the rocket. Equivalently, one can think of a rocket being accelerated upwards by the pressure of the combusting gases against the combustion chamber and nozzle. This operational principle stands in contrast to the commonly-held assumption that a rocket \"pushes\" against the air behind or below it. Rockets in fact perform better in outer space (where there is nothing behind or beneath them to push against), because there is a reduction in air pressure on the outside of the engine, and because it is possible to fit a longer nozzle without suffering from flow separation, in addition to the lack of air drag.\n\nThe maximum velocity that a rocket can attain in the absence of any external forces is primarily a function of its mass ratio and its \"exhaust velocity\". The relationship is described by the \"rocket equation\": formula_1, where formula_2 is the final velocity, formula_3 is the exhaust velocity relative to the rocket, formula_4 is the initial total mass, and formula_5 is the mass after the propellant is burned. The mass ratio expresses what proportion of the rocket is propellant (fuel/oxidizer combination) prior to engine ignition. Typically, a single-stage rocket might have a mass fraction of 90% propellant, 10% structure, and hence a mass ratio of 9:1 . The impulse delivered by the motor to the rocket vehicle per weight of propellant consumed is the rocket propellant's \"specific impulse\". A propellant with a higher specific impulse is said to be more efficient as more thrust is produced per unit of propellant.\n\nLower rocket stages usually use high-density (low volume per unit mass) propellants due to their lighter tankage to propellant weight ratios and because higher performance propellants require higher expansion ratios for maximum performance than can be attained when operated in the atmosphere. Thus, the Saturn V first stage used kerosene-liquid oxygen rather than the liquid hydrogen-liquid oxygen used on its upper stages. Similarly, the Space Shuttle used high-thrust, high-density solid rocket boosters for its lift-off with the liquid hydrogen-liquid oxygen Space Shuttle Main Engines used partly for lift-off but primarily for orbital insertion.\n\nSolid rocket propellant was first developed during the 13th century under the Chinese Song dynasty. The Song Chinese first used solid propellant (gunpowder fuel) in 1232 during the military siege of Kaifeng.\n\nThere are four main types of chemical rocket propellants: solid, storable liquid, cryogenic liquid, and a liquid monopropellant. Hybrid solid/liquid bi-propellant rocket engines are starting to see limited use as well.\n\nSolid propellants are either \"composites\" composed mostly of large, distinct macroscopic particles or single-, double-, or triple-bases (depending on the number of primary ingredients), which are homogeneous mixtures of one or more primary ingredients. Composites typically consist of a mixture of granules of solid oxidizer (examples: ammonium nitrate, ammonium dinitramide, ammonium perchlorate, potassium nitrate) in a polymer binder (binding agent) with flakes or powders of energetic compounds (examples: RDX, HMX), metallic additives (examples: aluminium, beryllium), plasticizers, stabilizers, and/or burn rate modifiers (iron oxide, copper oxide). Single-, double-, or triple-bases are mixtures of the fuel, oxidizer, binders, and plasticizers that are macroscopically indistinguishable and often blended as liquids and cured in a single batch. Often, the ingredients of a double-base propellant have multiple roles. For example, RDX is both a fuel and oxidizer while nitrocellulose is a fuel, oxidizer, and plasticizer. Further complicating categorization, there are many propellants that contain elements of double-base and composite propellants, which often contain some amount of energetic additives homogeneously mixed into the binder. In the case of gunpowder (a pressed composite without a polymeric binder) the fuel is charcoal, the oxidizer is potassium nitrate, and sulphur serves as a catalyst. (Note: sulphur is not a true catalyst in gunpowder as it is consumed to a great extent into a variety of reaction products such as KS.) During the 1950s and 60s researchers in the United States developed ammonium perchlorate composite propellant (APCP). This mixture is typically 69-70% finely ground ammonium perchlorate (an oxidizer), combined with 16-20% fine aluminium powder (a fuel), held together in a base of 11-14% polybutadiene acrylonitrile (PBAN) or Hydroxyl-terminated polybutadiene (polybutadiene rubber fuel). The mixture is formed as a thickened liquid and then cast into the correct shape and cured into a firm but flexible load-bearing solid. Historically the tally of APCP solid propellants is relatively small. The military, however, uses a wide variety of different types of solid propellants some of which exceed the performance of APCP. A comparison of the highest specific impulses achieved with the various solid and liquid propellant combinations used in current launch vehicles is given in the article on solid-fuel rockets.\n\nSolid propellant rockets are much easier to store and handle than liquid propellant rockets. High propellant density makes for compact size as well. These features plus simplicity and low cost make solid propellant rockets ideal for military applications. In the 1970s and 1980s, the U.S. switched entirely to solid-fueled ICBMs: the LGM-30 Minuteman and LG-118A Peacekeeper (MX). In the 1980s and 1990s, the USSR/Russia also deployed solid-fueled ICBMs (RT-23, RT-2PM, and RT-2UTTH), but retains two liquid-fueled ICBMs (R-36 and UR-100N). All solid-fueled ICBMs on both sides had three initial solid stages, and those with multiple independently targeted warheads had a precision maneuverable bus used to fine tune the trajectory of the re-entry vehicles. U.S. Minuteman III ICBMs were reduced to a single warhead by 2011 in accordance with the START treaty leaving only the Navy's Trident sub-launched ICBMs with multiple warheads.\n\nTheir simplicity also makes solid rockets a good choice whenever large amounts of thrust are needed and the cost is an issue. The Space Shuttle and many other orbital launch vehicles use solid-fueled rockets in their boost stages (solid rocket boosters) for this reason.\n\nRelative to liquid fuel rockets, solid fuel rockets have lower specific impulse, a measure of propellant efficiency. The propellant mass ratios of solid propellant upper stages are usually in the .91 to .93 range which is as good as or better than that of most liquid propellant upper stages but overall performance is less than for liquid stages because of the solids' lower exhaust velocities. The high mass ratios possible with (unsegmented) solids is a result of high propellant density and very high strength-to-weight ratio filament-wound motor casings. A drawback to solid rockets is that they cannot be throttled in real time, although a programmed thrust schedule can be created by adjusting the interior propellant geometry. Solid rockets can be vented to extinguish combustion or reverse thrust as a means of controlling range or accommodating warhead separation. Casting large amounts of propellant requires consistency and repeatability which is assured by computer control. Casting voids in propellant can adversely affect burn rate so the blending and casting take place under vacuum and the propellant blend is spread thin and scanned to assure no large gas bubbles are introduced into the motor. Solid fuel rockets are intolerant to cracks and voids and often require post-processing such as X-ray scans to identify faults. Since the combustion process is dependent on the surface area of the fuel; voids and cracks represent local increases in burning surface area. This increases the local temperature, system pressure and radiative heat flux to the surface. This positive feedback loop further increases burn rate and can easily lead to catastrophic failure typically due to case failure or nozzle system damage.\n\nThe most common liquid propellants in use today:\n\n\nThese include propellants such as the letter-coded rocket propellants used by Germany in World War II used for the Messerschmitt Me 163 \"Komet's\" Walter HWK 109-509 motor and the V-2 pioneer SRBM missile, and the Soviet/Russian utilized syntin, which is synthetic cyclopropane, CH which was used on Soyuz U2 until 1995. Syntin develops about 10 seconds greater specific impulse than kerosene.\n\nLiquid-fueled rockets have higher specific impulse than solid rockets and are capable of being throttled, shut down, and restarted. Only the combustion chamber of a liquid-fueled rocket needs to withstand high combustion pressures and temperatures and they can be regeneratively cooled by the liquid propellant. On vehicles employing turbopumps, the propellant tanks are at very much lower pressure than the combustion chamber. For these reasons, most orbital launch vehicles use liquid propellants.\n\nThe primary performance advantage of liquid propellants is due to the oxidizer. Several practical liquid oxidizers (liquid oxygen, nitrogen tetroxide, and hydrogen peroxide) are available which have better specific impulse than the ammonium perchlorate used in most solid rockets, when paired with comparable fuels. These facts have led to the use of hybrid propellants: a storable oxidizer used with a solid fuel, which retains most virtues of both liquids (high ISP) and solids (simplicity). (The newest nitramine solid propellants based on CL-20 (HNIW) can match the performance of NTO/UDMH storable liquid propellants, but cannot be controlled as can the storable liquids.)\n\nWhile liquid propellants are cheaper than solid propellants, for orbital launchers, the cost savings do not, and historically have not mattered; the cost of the propellant is a very small portion of the overall cost of the rocket. Some propellants, notably oxygen and nitrogen, may be able to be collected from the upper atmosphere, and transferred up to low-Earth orbit for use in propellant depots at substantially reduced cost.\n\nThe main difficulties with liquid propellants are also with the oxidizers. These are generally at least moderately difficult to store and handle due to their high reactivity with common materials, may have extreme toxicity (nitric acid, nitrogen tetroxide), require moderately cryogenic storage (liquid oxygen), or both (FLOX, a fluorine/LOX mix). Several exotic oxidizers have been proposed: liquid ozone (O), ClF, and ClF, all of which are unstable, energetic, and toxic.\n\nLiquid-fueled rockets also require potentially troublesome valves and seals and thermally stressed combustion chambers, which increase the cost of the rocket. Many employ specially designed turbopumps which raise the cost enormously due to difficult fluid flow patterns that exist within the casings.\n\nA gas propellant usually involves some sort of compressed gas. However, due to the low density of the gas and high weight of the pressure vessel required to contain it, gases see little current use but are sometimes used for vernier engines, particularly with inert propellants like nitrogen.\n\nGOX (gaseous oxygen) was used as the oxidizer for the Buran program's orbital maneuvering system.\n\nA hybrid rocket usually has a solid fuel and a liquid or NEMA oxidizer. The fluid oxidizer can make it possible to throttle and restart the motor just like a liquid-fueled rocket. Hybrid rockets can also be environmentally safer than solid rockets since some high-performance solid-phase oxidizers contain chlorine (specifically composites with ammonium perchlorate), versus the more benign liquid oxygen or nitrous oxide often used in hybrids. This is only true for specific hybrid systems. There have been hybrids which have used chlorine or fluorine compounds as oxidizers and hazardous materials such as beryllium compounds mixed into the solid fuel grain. Because just one constituent is a fluid, hybrids can be simpler than liquid rockets depending motive force used to transport the fluid into the combustion chamber. Fewer fluids typically mean fewer and smaller piping systems, valves and pumps (if utilized).\n\nHybrid motors suffer two major drawbacks. The first, shared with solid rocket motors, is that the casing around the fuel grain must be built to withstand full combustion pressure and often extreme temperatures as well. However, modern composite structures handle this problem well, and when used with nitrous oxide and a solid rubber propellant (HTPB), relatively small percentage of fuel is needed anyway, so the combustion chamber is not especially large.\n\nThe primary remaining difficulty with hybrids is with mixing the propellants during the combustion process. In solid propellants, the oxidizer and fuel are mixed in a factory in carefully controlled conditions. Liquid propellants are generally mixed by the injector at the top of the combustion chamber, which directs many small swift-moving streams of fuel and oxidizer into one another. Liquid-fueled rocket injector design has been studied at great length and still resists reliable performance prediction. In a hybrid motor, the mixing happens at the melting or evaporating surface of the fuel. The mixing is not a well-controlled process and generally, quite a lot of propellant is left unburned, which limits the efficiency of the motor. The combustion rate of the fuel is largely determined by the oxidizer flux and exposed fuel surface area. This combustion rate is not usually sufficient for high power operations such as boost stages unless the surface area or oxidizer flux is high. Too high of oxidizer flux can lead to flooding and loss of flame holding that locally extinguishes the combustion. Surface area can be increased, typically by longer grains or multiple ports, but this can increase combustion chamber size, reduce grain strength and/or reduce volumetric loading. Additionally, as the burn continues, the hole down the center of the grain (the 'port') widens and the mixture ratio tends to become more oxidizer rich.\n\nThere has been much less development of hybrid motors than solid and liquid motors. For military use, ease of handling and maintenance have driven the use of solid rockets. For orbital work, liquid fuels are more efficient than hybrids and most development has concentrated there. There has recently been an increase in hybrid motor development for nonmilitary suborbital work:\n\nSome work has been done on gelling liquid propellants to give a propellant with low vapor pressure to reduce the risk of an accidental fireball. Gelled propellant behaves like a solid propellant in storage and like a liquid propellant in use.\n\nSome rocket designs have their propellants obtain their energy from non-chemical or even external sources. For example, water rockets use the compressed gas, typically air, to force the water out of the rocket.\n\nSolar thermal rockets and nuclear thermal rockets typically propose to use liquid hydrogen for an \"I\" (Specific Impulse) of around 600–900 seconds, or in some cases water that is exhausted as steam for an \"I\" of about 190 seconds.\n\nAdditionally for low performance requirements such as attitude control jets, inert gases such as nitrogen have been employed. \n\nNuclear thermal rockets pass a propellant over a central reactor, heating the propellant and causing it to expand rapidly out a rocket nozzle, pushing the craft forward. The propellant itself is not directly interacting with the interior of the reactor, so the propellant is not irradiated.\n\nSolar thermal rockets use concentrated sunlight to heat a propellant, rather than using a nuclear reactor.\n\nThe theoretical exhaust velocity of a given propellant chemistry is \na function of the energy released per unit of propellant mass (specific\nenergy). Unburned fuel or oxidizer drags down the specific energy.\nHowever, most rockets run fuel-rich mixtures.\n\nThe usual explanation for fuel-rich mixtures is that fuel-rich\nmixtures have lower molecular weight exhaust, which by reducing\nformula_6 increases the ratio formula_7\nwhich is approximately equal to the theoretical exhaust velocity. Fuel-rich\nmixtures actually have lower theoretical exhaust velocities, because\nformula_8 decreases as fast or faster than formula_6.\n\nThe nozzle of the rocket converts the thermal energy of the\npropellants into directed kinetic energy. This conversion happens in\na short time, on the order of one millisecond. During the conversion, energy\nmust transfer very quickly from the rotational and vibrational states\nof the exhaust molecules into translation. Molecules with fewer atoms\n(like CO and H) store less energy in vibration and\nrotation than molecules with more atoms (like CO and\nHO). These smaller molecules transfer more of their rotational and\nvibrational energy to translation energy than larger molecules, and\nthe resulting improvement in nozzle efficiency is large enough\nthat real rocket engines improve their actual exhaust\nvelocity by running rich mixtures with somewhat lower theoretical\nexhaust velocities.\n\nThe effect of exhaust molecular weight on nozzle efficiency is most\nimportant for nozzles operating near sea level. High expansion\nrockets operating in a vacuum see a much smaller effect, and so are\nrun less rich. The Saturn-II stage (a LOX/LH rocket)\nvaried its mixture ratio during flight to optimize performance.\n\nLOX/hydrocarbon rockets are run only somewhat rich (O/F mass ratio of\n3 rather than stoichiometric of 3.4 to 4) because the energy release\nper unit mass drops off quickly as the mixture ratio deviates from\nstoichiometric. LOX/LH rockets are run very rich (O/F mass\nratio of 4 rather than stoichiometric 8) because hydrogen is so light\nthat the energy release per unit mass of propellant drops very slowly\nwith extra hydrogen. In fact, LOX/LH rockets are\ngenerally limited in how rich they run by the performance penalty of\nthe mass of the extra hydrogen tankage, rather than the mass of the\nhydrogen itself.\n\nAnother reason for running rich is that off-stoichiometric mixtures\nburn cooler than stoichiometric mixtures, which makes engine cooling\neasier. Because fuel-rich combustion products are less chemically reactive (corrosive) than oxygenated products, a vast majority of rocket engines are designed to run fuel-rich, with at least one exception for the Russian RD-180 preburner, which burns LOX and RP-1 at a ratio of 2.72.\n\nAdditionally, mixture ratios can be dynamic during launch. This can be exploited with designs that adjust the oxidizer to fuel ratio (along with overall thrust) during the flight to maximize overall system performance. For instance, during lift-off thrust is a premium while specific impulse is less so. As such, the system can be optimized by carefully adjusting the O/F ratio so the engine runs cooler at higher thrust levels. This also allows for the engine to be designed slightly more compactly, improving its overall thrust to weight performance.\n\nAlthough liquid hydrogen gives a high \"I\", its low density is a significant disadvantage: hydrogen occupies about 7x more volume per kilogram than dense fuels such as kerosene. This not only penalizes the tankage, but also the pipes and fuel pumps leading from the tank, which need to be 7x bigger and heavier. (The oxidizer side of the engine and tankage is of course unaffected.) This makes the vehicle's dry mass much higher, so the use of liquid hydrogen is not as advantageous as might be expected. Indeed, some dense hydrocarbon/LOX propellant combinations have higher performance when the dry mass penalties are included.\n\nDue to lower \"I\", dense propellant launch vehicles have a higher takeoff mass, but this does not mean a proportionately high cost; on the contrary, the vehicle may well end up cheaper. Liquid hydrogen is quite an expensive fuel to produce and store, and causes many practical difficulties with design and manufacture of the vehicle.\n\nBecause of the higher overall weight, a dense-fueled launch vehicle necessarily requires higher takeoff thrust, but it carries this thrust capability all the way to orbit. This, in combination with the better thrust/weight ratios, means that dense-fueled vehicles reach orbit earlier, thereby minimizing losses due to gravity drag. Thus, the effective delta-v requirement for these vehicles is reduced.\n\nHowever, liquid hydrogen does give clear advantages when the overall mass needs to be minimized; for example, the Saturn V vehicle used it on the upper stages; this reduced weight meant that the dense-fueled first stage could be made significantly smaller, saving quite a lot of money.\n\nTripropellant rockets designs often try to use an optimum mix of propellants for launch vehicles. These use mainly dense fuel while at low altitude and switch across to hydrogen at higher altitude. Studies by Robert Salkeld in the 1960s proposed SSTO using this technique. The Space Shuttle approximated this by using dense solid rocket boosters for the majority of the thrust for the first 120 seconds, the main engines, burning a fuel-rich hydrogen and oxygen mixture operate continuously throughout the launch but only provide the majority of thrust at higher altitudes after SRB burnout.\n\n\n"}
{"id": "1753270", "url": "https://en.wikipedia.org/wiki?curid=1753270", "title": "Self-assembled monolayer", "text": "Self-assembled monolayer\n\nSelf-assembled monolayers (SAM) of organic molecules are molecular assemblies formed spontaneously on surfaces by adsorption and are organized into more or less large ordered domains. In some cases molecules that form the monolayer do not interact strongly with the substrate. This is the case for instance of the two-dimensional supramolecular networks of e.g. perylenetetracarboxylic dianhydride (PTCDA) on gold or of e.g. porphyrins on highly oriented pyrolitic graphite (HOPG). In other cases the molecules possess a head group that has a strong affinity to the substrate and anchors the molecule to it. Such a SAM consisting of a head group, tail and functional end group is depicted in Figure 1. Common head groups include thiols, silanes, phosphonates, etc.\n\nSAMs are created by the chemisorption of \"head groups\" onto a substrate from either the vapor or liquid phase followed by a slow organization of \"tail groups\". Initially, at small molecular density on the surface, adsorbate molecules form either a disordered mass of molecules or form an ordered two-dimensional \"lying down phase\", and at higher molecular coverage, over a period of minutes to hours, begin to form three-dimensional crystalline or semicrystalline structures on the substrate surface. The \"head groups\" assemble together on the substrate, while the tail groups assemble far from the substrate. Areas of close-packed molecules nucleate and grow until the surface of the substrate is covered in a single monolayer.\n\nAdsorbate molecules adsorb readily because they lower the surface free-energy of the substrate and are stable due to the strong chemisorption of the \"head groups.\" These bonds create monolayers that are more stable than the physisorbed bonds of Langmuir–Blodgett films. A Trichlorosilane based \"head group\", for example in a FDTS molecule, reacts with an hydroxyl group on a substrate, and forms very stable, covalent bond [R-Si-O-substrate] with an energy of 452 kJ/mol. Thiol-metal bonds are on the order of 100 kJ/mol, making them fairly stable in a variety of temperatures, solvents, and potentials. The monolayer packs tightly due to van der Waals interactions, thereby reducing its own free energy. The adsorption can be described by the Langmuir adsorption isotherm if lateral interactions are neglected. If they cannot be neglected, the adsorption is better described by the Frumkin isotherm.\n\nSelecting the type of head group depends on the application of the SAM. Typically, head groups are connected to a molecular chain in which the terminal end can be functionalized (i.e. adding –OH, –NH2, –COOH, or –SH groups) to vary the wetting and interfacial properties. An appropriate substrate is chosen to react with the head group. Substrates can be planar surfaces, such as silicon and metals, or curved surfaces, such as nanoparticles. Alkanethiols are the most commonly used molecules for SAMs. Alkanethiols are molecules with an alkyl chain, (C-C)ⁿ chain, as the back bone, a tail group, and a S-H head group. Other types of interesting molecules include aromatic thiols, of interest in molecular electronics, in which the alkane chain is (partly) replaced by aromatic rings. An example is the dithiol 1,4-Benzenedimethanethiol (SHCHCHCHSH)). Interest in such dithiols stems from the possibility of linking the two sulfur ends to metallic contacts, which was first used in molecular conduction measurements. Thiols are frequently used on noble metal substrates because of the strong affinity of sulfur for these metals. The sulfur gold interaction is semi-covalent and has a strength of approximately 45kcal/mol. In addition, gold is an inert and biocompatible material that is easy to acquire. It is also easy to pattern via lithography, a useful feature for applications in nanoelectromechanical systems (NEMS). Additionally, it can withstand harsh chemical cleaning treatments. Recently other chalcogenide SAMs: selenides and tellurides have attracted attention in a search for different bonding characteristics to substrates affecting the SAM characteristics and which could be of interest in some applications such as molecular electronics. Silanes are generally used on nonmetallic oxide surfaces; however monolayers formed from covalent bonds between silicon and carbon or oxygen cannot be considered self assembled because they do not form reversibly. Self-assembled monolayers of thiolates on noble metals are a special case because the metal-metal bonds become reversible after the formation of the thiolate-metal complex. This reversibility is what gives rise to vacancy islands and it is why SAMs of alkanethiolates can be thermally desorbed and undergo exchange with free thiols.\n\nMetal substrates for use in SAMs can be produced through physical vapor deposition techniques, electrodeposition or electroless deposition. Thiol or selenium SAMs produced by adsorption from solution are typically made by immersing a substrate into a dilute solution of alkane thiol in ethanol, though many different solvents can be used besides use of pure liquids. While SAMs are often allowed to form over 12 to 72 hours at room temperature, SAMs of alkanethiolates form within minutes. Special attention is essential in some cases, such as that of dithiol SAMs to avoid problems due to oxidation or photoinduced processes, which can affect terminal groups and lead to disorder and multilayer formation. In this case appropriate choice of solvents, their degassing by inert gasses and preparation in the absence of light is crucial and allows formation of \"standing up\" SAMs with free –SH groups. Self-assembled monolayers can also be adsorbed from the vapor phase. In some cases when obtaining an ordered assembly is difficult or when different density phases need to be obtained substitutional self-assembly is used. Here one first forms the SAM of a given type of molecules, which give rise to ordered assembly and then a second assembly phase is performed (e.g. by immersion into a different solution). This method has also been used to give information on relative binding strengths of SAMs with different head groups and more generally on self-assembly characteristics.\n\nThe thicknesses of SAMs can be measured using ellipsometry and X-ray photoelectron spectroscopy (XPS), which also give information on interfacial properties. The order in the SAM and orientation of molecules can be probed by Near Edge Xray Absorption Fine Structure (NEXAFS) and Fourier Transform Infrared Spectroscopy in Reflection Absorption Infrared Spectroscopy (RAIRS) studies. Numerous other spectroscopic techniques are used such as Second-harmonic generation (SHG), Sum-frequency generation (SFG), Surface-enhanced Raman scattering (SERS), as well as High-resolution electron energy loss spectroscopy (HREELS). The structures of SAMs are commonly determined using scanning probe microscopy techniques such as atomic force microscopy (AFM) and scanning tunneling microscopy (STM). STM has been able to help understand the mechanisms of SAM formation as well as determine the important structural features that lend SAMs their integrity as surface-stable entities. In particular STM can image the shape, spatial distribution, terminal groups and their packing structure. AFM offers an equally powerful tool without the requirement of the SAM being conducting or semi-conducting. AFM has been used to determine chemical functionality, conductance, magnetic properties, surface charge, and frictional forces of SAMs. More recently, however, diffractive methods have also been used. The structure can be used to characterize the kinetics and defects found on the monolayer surface. These techniques have also shown physical differences between SAMs with planar substrates and nanoparticle substrates.\nAn alternative characterisation instrument for measuring the self-assembly in real time is dual polarisation interferometry where the refractive index, thickness, mass and birefringence of the self assembled layer are quantified at high resolution. Contact angle measurements can be used to determine the surface free-energy which reflects the average composition of the surface of the SAM and can be used to probe the kinetics and thermodynamics of the formation of SAMs. The kinetics of adsorption and temperature induced desorption as well as information on structure can also be obtained in real time by ion scattering techniques such as low energy ion scattering (LEIS) and time of flight direct recoil spectroscopy (TOFDRS).\n\nDefects due to both external and intrinsic factors may appear. External factors include the cleanliness of the substrate, method of preparation, and purity of the adsorbates. SAMs intrinsically form defects due to the thermodynamics of formation, e.g. thiol SAMs on gold typically exhibit etch pits (monatomic vacancy islands) likely due to extraction of adatoms from the substrate and formation of adatom-adsorbate moieties. Recently, a new type of fluorosurfactants have found that can form nearly perfect monolayer on gold substrate due to the increase of mobility of gold surface atoms.\n\nThe structure of SAMs is also dependent on the curvature of the substrate. SAMs on nanoparticles, including colloids and nanocrystals, \"stabilize the reactive surface of the particle and present organic functional groups at the particle-solvent interface\". These organic functional groups are useful for applications, such as immunoassays or sensors, that are dependent on chemical composition of the surface.\n\nThere is evidence that SAM formation occurs in two steps: an initial fast step of adsorption and a second slower step of monolayer organization. Adsorption occurs at the liquid–liquid, liquid–vapor, and liquid-solid interfaces. The transport of molecules to the surface occurs due to a combination of diffusion and convective transport. According to the Langmuir or Avrami kinetic model the rate of deposition onto the surface is proportional to the free space of the surface.\nWhere θ is the proportional amount of area deposited and k is the rate constant. Although this model is robust it is only used for approximations because it fails to take into account intermediate processes. Dual polarisation interferometry being a real time technique with ~10 Hz resolution can measure the kinetics of monolayer self-assembly directly.\n\nOnce the molecules are at the surface the self-organization occurs in three phases:\nThe phase transitions in which a SAM forms depends on the temperature of the environment relative to the triple point temperature, the temperature in which the tip of the low-density phase intersects with the intermediate-phase region. At temperatures below the triple point the growth goes from phase 1 to phase 2 where many islands form with the final SAM structure, but are surrounded by random molecules. Similar to nucleation in metals, as these islands grow larger they intersect forming boundaries until they end up in phase 3, as seen below.\nAt temperatures above the triple point the growth is more complex and can take two paths. In the first path the heads of the SAM organize to their near final locations with the tail groups loosely formed on top. Then as they transit to phase 3, the tail groups become ordered and straighten out. In the second path the molecules start in a lying down position along the surface. These then form into islands of ordered SAMs, where they grow into phase 3, as seen below.\n\nThe nature in which the tail groups organize themselves into a straight ordered monolayer is dependent on the inter-molecular attraction, or van der Waals forces, between the tail groups. To minimize the free energy of the organic layer the molecules adopt conformations that allow high degree of Van der Waals forces with some hydrogen bonding. The small size of the SAM molecules are important here because Van der Waals forces arise from the dipoles of molecules and are thus much weaker than the surrounding surface forces at larger scales. The assembly process begins with a small group of molecules, usually two, getting close enough that the Van der Waals forces overcome the surrounding force. The forces between the molecules orient them so they are in their straight, optimal, configuration. Then as other molecules come close by they interact with these already organized molecules in the same fashion and become a part of the conformed group. When this occurs across a large area the molecules support each other into forming their SAM shape seen in Figure 1. The orientation of the molecules can be described with two parameters: α and β. α is the angle of tilt of the backbone from the surface normal. In typical applications α varies from 0 to 60 degrees depending on the substrate and type of SAM molecule. β is the angle of rotation along the long axis of tee molecule. β is usually between 30 and 40 degrees. In some cases existence of kinetic traps hindering the final ordered orientation has been pointed out. Thus in case of dithiols formation of a \"lying down\" phase was considered an impediment to formation of \"standing up\" phase, however various recent studies indicate this is not the case.\n\nMany of the SAM properties, such as thickness, are determined in the first few minutes. However, it may take hours for defects to be eliminated via annealing and for final SAM properties to be determined. The exact kinetics of SAM formation depends on the adsorbate, solvent and substrate properties. In general, however, the kinetics are dependent on both preparations conditions and material properties of the solvent, adsorbate and substrate. Specifically, kinetics for adsorption from a liquid solution are dependent on:\n\nThe final structure of the SAM is also dependent on the chain length and the structure of both the adsorbate and the substrate. Steric hindrance and metal substrate properties, for example, can affect the packing density of the film, while chain length affects SAM thickness. Longer chain length also increases the thermodynamic stability.\n\nThis first strategy involves locally depositing self-assembled monolayers on the surface only where the nanostructure will later be located. This strategy is advantageous because it involves high throughput methods that generally involve fewer steps than the other two strategies. The major techniques that use this strategy are:\n\nThe locally remove strategy begins with covering the entire surface with a SAM. Then individual SAM molecules are removed from locations where the deposition of nanostructures is not desired. The end result is the same as in the locally attract strategy, the difference being in the way this is achieved. The major techniques that use this strategy are:\n\nThe final strategy focuses not on the deposition or removal of SAMS, but the modification of terminal groups. In the first case the terminal group can be modified to remove functionality so that SAM molecule will be inert. In the same regards the terminal group can be modified to add functionality so it can accept different materials or have different properties than the original SAM terminal group. The major techniques that use this strategy are:\n\nSAMs are an inexpensive and versatile surface coating for applications including control of wetting and adhesion, chemical resistance, bio compatibility, sensitization, and molecular recognition for sensors and nano fabrication. Areas of application for SAMs include biology, electrochemistry and electronics, nanoelectromechanical systems (NEMS) and microelectromechanical systems (MEMS), and everyday household goods. SAMs can serve as models for studying membrane properties of cells and organelles and cell attachment on surfaces. SAMs can also be used to modify the surface properties of electrodes for electrochemistry, general electronics, and various NEMS and MEMS. For example, the properties of SAMs can be used to control electron transfer in electrochemistry. They can serve to protect metals from harsh chemicals and etchants. SAMs can also reduce sticking of NEMS and MEMS components in humid environments. In the same way, SAMs can alter the properties of glass. A common household product, Rain-X, utilizes SAMs to create a hydrophobic monolayer on car windshields to keep them clear of rain. Another application is an anti-adhesion coating on nanoimprint lithography (NIL) tools and stamps. One can also coat injection molding tools for polymer replication with a Perfluordecyltrichlorosilane SAM.\n\nThin film SAMs can also be placed on nanostructures. In this way they functionalize the nanostructure. This is advantageous because the nanostructure can now selectively attach itself to other molecules or SAMs. This technique is useful in biosensors or other MEMS devices that need to separate one type of molecule from its environment. One example is the use of magnetic nanoparticles to remove a fungus from a blood stream. The nanoparticle is coated with a SAM that binds to the fungus. As the contaminated blood is filtered through a MEMS device the magnetic nanoparticles are inserted into the blood where they bind to the fungus and are then magnetically driven out of the blood stream into a nearby laminar waste stream.\n\nSAMs are also useful in depositing nanostructures, because each adsorbate molecule can be tailored to attract two different materials. Current techniques utilize the head to attract to a surface, like a plate of gold. The terminal group is then modified to attract a specific material like a particular nanoparticle, wire, ribbon, or other nanostructure. In this way, wherever the a SAM is patterned to a surface there will be nanostructures attached to the tail groups. One example is the use of two types of SAMs to align single wall carbon nanotubes, SWNTs. Dip pen nanolithography was used to pattern a 16-mercaptohexadecanoic acid (MHA)SAM and the rest of the surface was passivated with 1-octadecanethiol (ODT) SAM. The polar solvent that is carrying the SWNTs is attracted to the hydrophilic MHA; as the solvent evaporates, the SWNTs are close enough to the MHA SAM to attach to it due to Van der Waals forces. The nanotubes thus line up with the MHA-ODT boundary. Using this technique Chad Mirkin, Schatz and their co-workers were able to make complex two-dimensional shapes, a representation of a shape created is shown to the right.\nAnother application of patterned SAMs is the functionalization of biosensors. The tail groups can be modified so they have an affinity for cells, proteins, or molecules. The SAM can then be placed onto a biosensor so that binding of these molecules can be detected. The ability to pattern these SAMs allows them to be placed in configurations that increase sensitivity and do not damage or interfere with other components of the biosensor.\n\nThere has been considerable interest in use of SAMs for new materials e.g. via formation of two- or three-dimensional metal organic superlattices by assembly of SAM capped nanoparticles or layer by layer SAM-nanoparticle arrays using dithiols.\n\n"}
{"id": "44453175", "url": "https://en.wikipedia.org/wiki?curid=44453175", "title": "Selfie stick", "text": "Selfie stick\n\nA selfie stick is used to take photographs or video by positioning a digital camera device, typically a smartphone, beyond the normal range of the arm. This allows for shots to be taken at angles and distances that would not have been possible with the human arm by itself. The sticks are typically extensible, with a handle on one end and an adjustable clamp on the other end to hold the device in place. Some are connected to a smartphone via its jack plug, while others are tethered using Bluetooth controls. The connection between the device and the selfie stick lets the user decide when to take the picture or start recording a video by clicking a button located on the handle. Models designed for compact cameras have a mirror behind the viewscreen so that the shot can be lined up.\n\nIn contrast to a monopod for stabilising a camera on the ground, a selfie stick's arm is thickest and strongest at the opposite end from the camera in order to provide better grip and balance when held aloft. Safety concerns and the inconvenience the product causes to others have resulted in them being banned at many venues, including all Disney Parks, both Universal Studios Orlando, and Hollywood.\n\nHomemade selfie sticks could date back as early as 1925. A photo from that year shows a man taking a photograph of himself and his wife, with the aid of a long pole which is pointed towards the camera and out of frame. Amateur box cameras of the period would not have been able to capture a self-portrait in focus when held at arm's length, requiring photographers to use remote shutter devices such as cables or sticks.\n\nA device which has been likened to the selfie stick appears in the 1969 Czechoslovak sci-fi film \"I Killed Einstein, Gentlemen\". One character holds a silver stick in front of herself and another character, smiles at the end of the stick as it produces a camera flash, and immediately unfurls a printed photograph of the pair from the stick's handle.\n\nThe 1983 \"Minolta Disc-7\" camera had a convex mirror on its front to allow the composition of self-portraits, and its packaging showed the camera mounted on a stick while used for such a purpose. A \"telescopic extender\" for compact handheld cameras was patented by Ueda Hiroshi and Mima Yujiro in 1983, and a Japanese selfie stick was featured in a 1995 book of \"101 Un-Useless Japanese Inventions\". While dismissed as a \"useless invention\" at the time, the selfie stick later gained global popularity in the 21st century.\n\nCanadian inventor Wayne Fromm patented his \"Quik Pod\" in 2005 and becoming commercially available in the United States the following year. In 2012, Yeong-Ming Wang filed a patent for a \"multi-axis omni-directional shooting extender\" capable of holding a smartphone, which won a silver medal at the 2013 Concours Lepine. The term \"selfie stick\" did not become widely used until 2014. Extended forms of selfie sticks can hold laptop computers to take selfies from a webcam. By the fall of 2015 technology news noted that there was a large variety of selfie sticks available on the market; Molly McCugh of \"Wired\" magazine wrote in October 2015, \"Some are very, very long; some aren't so long; some are bedazzled. Some look like hands. Some are spoons. But they are all, at the end of the day, one thing: A stick that takes selfies.\" \n\nThe selfie stick was listed in \"Time\" magazine's 25 best inventions of 2014. While the \"New York Post\" named the selfie stick the most controversial gift of 2014. At the end of December 2014, Bloomberg News noted that selfie sticks had ruled the 2014 holiday season as the “must-have” gift of the year. The selfie stick has been criticized for its association with the perceived narcissism and self-absorption of contemporary society, with commentators in 2015 dubbing the tool the \"Narcisstick\" or \"Wand of Narcissus\". Despite various bans, selfie sticks proved so popular that a selfie stick store was opened in Times Square during the summer of 2015. In 2016 it was reported that Coca-Cola had created a \"selfie bottle\" with an attached camera that takes pictures when it's tipped for drinking.\n\nOne is able to attach their device to the end of the selfie stick and then extend it beyond the normal reach of the arm. Different models of stick are triggered in various ways, such as pressing a button on the stick handle which is connected to the device (usually using the jack plug), pressing a button on a wireless remote (often via Bluetooth), using the camera's built-in timer, or making a sound for the device to sense it for it to then start recording a video or taking a picture.\n\nThe smartphone's physical means of triggering the camera, such as the sound volume controls or the touchscreen camera button of the device, are replicated on headphones with on-cord controls. When selfie sticks are plugged into the jack plug, they are seen by the device as headphones.\n\nThe selfie stick gives more practical use in situations that require assistance for taking photos/videos at difficult angles that need to be taken from an extended, elevated distance beyond the arm’s reach. It allows the user to take photos and videos in otherwise dangerous or impossible situations, such as recording footage inside a very deep hole, over a cliff, or simply at an angle that is too far away from the user.\n\nDespite the selfie stick being one of the most popular items among tourists and families, bans and restrictions on its use have been imposed across a range of public venues generally on the grounds of safety and inconvenience to others.\n\nConcert venues and some music festivals in the United States, Australia and the United Kingdom have banned the use of selfie sticks. Organisers have cited their role in the \"illegal recording\" of bands' sets, and the inconvenience and safety issues to fellow audience members.\n\nMuseums, galleries and historical sites such as the Palace of Versailles have banned the sticks because of concerns about possible damage to priceless artworks and other objects.\n\nTheme parks, including Walt Disney World Resort, Six Flags, Universal Orlando, and Universal Studios Hollywood have banned selfie sticks. The sticks have always been banned on rides at Disney World for safety reasons, but after a number of instances where rides had to be stopped because of a guest pulling out a selfie stick in mid-ride, such as incidents on California Screamin' and Big Thunder Mountain Railroad, Disney issued a park-wide ban on the accessories.\n\nSporting events have banned selfie sticks both for their \"nuisance value\" and for interfering with other spectators' enjoyment or view. The Australia Tour Down Under banned the devices citing \"harm to cyclists, officials and yourself\". Emirates Stadium, home of the Arsenal Football Club, bans \"any object that could be used as a weapon or could compromise public safety\", and regards selfie sticks as such an item.\n\nIn 2014, South Korea's radio management agency issued regulations banning sale of unregistered selfie sticks that use Bluetooth technology to trigger the camera, as any such device sold in South Korea is considered a \"telecommunications device\" and must be tested by and registered with the agency. In 2015, Apple banned them from a WWDC Developers Conference, though didn't explicitly state why.\n\n\n"}
{"id": "9773503", "url": "https://en.wikipedia.org/wiki?curid=9773503", "title": "Space Imaging Middle East", "text": "Space Imaging Middle East\n\nSpace Imaging Middle East LLC (SIME) is a regional affiliate of DigitalGlobe, the largest commercial remote sensing company in the world.\n\nEstablished in 1997, SIME started off by introducing high-resolution satellite imagery to the Middle East region, operating from government-run ground stations. Today, SIME, headquartered in the United Arab Emirates, is a leading Geo-Spatial and GIS solutions provider throughout the Middle East, the Persian Gulf, Eastern Africa and Central Asia.\n\nSIME's products and services include photogrammetry, mapping, GIS, 3D models, imagery analysis and reporting software, vehicle tracking, consulting, and training, among others. SIME engages in technology partnerships with leading technology companies, and owns and operates a wide range of sources for satellite and aerial imagery.\n\nThe company's profile was raised during the 2003 Iraq War, when high-resolution satellite imagery licensed from SIME was used by some news organizations to give the public \"bird's eye views\" of various key Iraqi locations, such as Fallujah, and Bagdhad.\n\nIn 2003 SIME supplied 1-meter resolution imagery of Afghanistan to the United Nations Office on Drugs and Crime (UNODC), as part of a survey to monitor the production of illicit crops.\n\nSIME offers imagery collected from various earth observation satellites and aerial sensors, with resolutions varying from 15 centimeters to 20 meters. SIME's satellite constellation includes the Indian Remote Sensing satellite (IRS), IKONOS and CARTOSAT satellites. SIME controls its satellite constellation via a ground station based in the UAE. The Company also has an aircraft permanently stationed in the region to collect aerial imagery.\n\nSIME is the main investor of European Space Imaging in Munich, Germany, which gives SIME direct access to the latest in European satellite technology. SIME has partnered with the German Aerospace Center (DLR) to operate the ground station and to develop hosting and delivery systems of data. It has also teamed up with GAF AG of Munich to provide specialized remote-sensing studies. SIME has established a joint venture with Sanborn Incorporated to provide aerial imaging products and advanced modeling solutions. In the Far East, the Company has teamed up with Mappoint Asia to provide vehicle and asset tracking systems.\n\nHaving direct access to two receiving stations in Germany and UAE, SIME’s satellite reach extensively covers an area extending from the Arctic Circle in the north to the Persian Gulf region in the south and from the Indian Ocean in the east to the Atlantic Ocean in the west. SIME has a substantial satellite and aerial imagery archive, and owns all the images in its database which contain multiple covers of the region spanning a period of several years.\n\nSIME offers applications to a variety of industries in both the government and commercial sectors. These include:\n\n\n"}
{"id": "15796172", "url": "https://en.wikipedia.org/wiki?curid=15796172", "title": "Sputter cleaning", "text": "Sputter cleaning\n\nSputter cleaning is the cleaning of a solid surface in a vacuum by using physical sputtering of the surface. Sputter cleaning is often used in vacuum deposition and ion plating. In 1955 Farnsworth, Schlier, George, and Burger reported using sputter cleaning in an ultra-high-vacuum system to prepare ultra-clean surfaces for low-energy electron-diffraction (LEED) studies. Sputter cleaning became an integral part of the ion plating process. Sputter cleaning has some potential problems such as overheating, gas incorporation in the surface region, bombardment (radiation) damage in the surface region, and the roughening of the surface, particularly if \"over done.\" It is important to have a \"clean\" plasma in order to not continually recontaminate the surface during sputter cleaning. Redeposition of sputtered material on the substrate can also give problems, especially at high sputtering pressures.\n\nSputtering of the surface of a compound or alloy material can result in the surface composition being changed. Often the species with the least mass or the highest vapor pressure is the one preferentially sputtered from the surface. \n\n"}
{"id": "1754078", "url": "https://en.wikipedia.org/wiki?curid=1754078", "title": "Supercow (dairy)", "text": "Supercow (dairy)\n\nSupercow (or super cow) is a term used in the dairy industry to denote lines or individual animals that have superior milk production: that is, which produce more milk per day, or in some cases produce more fat per gallon of milk. \n\nUntil recently, supercows have been developed through selective breeding - either traditional breeding or, since the 1960s, artificial insemination. Now the term tends to be applied to cows that have been genetically altered or whose genome has been studied in order to improve breeding.\n"}
{"id": "33039125", "url": "https://en.wikipedia.org/wiki?curid=33039125", "title": "Susan Wojcicki", "text": "Susan Wojcicki\n\nSusan Diane Wojcicki ( , ; born July 5, 1968) is an American technology executive. She has been the CEO of YouTube since February 2014.\n\nWojcicki was involved in the founding of Google, and became Google's first marketing manager in 1999. She was in charge of Google's original video service, and after observing the success of YouTube, proposed the aquisition of YouTube by Google in 2006.\n\nWojcicki has an estimated net worth of nearly $500 million.\n\nWojcicki is the daughter of Esther Wojcicki, an educator of Russian-Jewish descent, and Stanley Wojcicki, a Polish American physics professor at Stanford University. She has two sisters: Janet Wojcicki, (PhD, anthropologist and epidemiologist) and Anne Wojcicki, founder of 23andMe. She grew up on the Stanford campus with George Dantzig as a neighbor. She attended Gunn High School in Palo Alto, California, and wrote for the school newspaper.\n\nWojcicki's first business was selling \"spice ropes\" door-to-door at age 11. A humanities major in college, she took her first computer science class as a senior.\n\nWojcicki studied history and literature at Harvard University and graduated with honors in 1990. She originally planned on getting a PhD in economics and pursuing a career in academia but changed her plans when she discovered an interest in technology.\n\nShe also received her Master's of Science in economics from the University of California, Santa Cruz in 1993 and a Master of Business Administration from the UCLA Anderson School of Management in 1998.\n\nIn September 1998, the same month that Google was incorporated, its founders Larry Page and Sergey Brin set up office in Wojcicki's garage in Menlo Park. Before becoming Google's first marketing manager in 1999, Wojcicki worked in marketing at Intel Corporation in Santa Clara, California, and was a management consultant at Bain & Company and R.B. Webber & Company. At Google, she worked on the initial viral marketing programs, as well as the first Google Doodles. Wojcicki also took part in the development of successful contributions to Google such as Google Images and Google Books.\n\nWojcicki grew within Google to become senior vice president of Advertising & Commerce and lead the advertising and analytic products, including AdWords, AdSense, DoubleClick, and Google Analytics.\n\nYouTube, then a small start-up, was successfully competing with Google's Google Video service, overseen by Wojcicki. Her response was to propose the purchase of YouTube.\n\nShe handled two of Google’s largest acquisitions — the $1.65 billion purchase of YouTube in 2006 and the $3.1 billion purchase of DoubleClick in 2007.\n\nIn February 2014 she became the CEO of YouTube.\n\nWojcicki, called \"the most important person in advertising\", was named to Time's 100 most influential people in 2015 and described in a later issue of Time as “the most powerful woman on the Internet”.\n\nIn the time that Wojcicki has been CEO of YouTube, the company announced that it had reached 1.9 billion logged-in users a month and that users were watching one billion hours a day. Since taking on the role of CEO, YouTube’s percentage of female employees has risen from 24 to nearly 30 percent.\n\nWojcicki also oversaw the development and release of new YouTube applications and experiences designed to cater to users interested in family gaming, and music content. She also oversaw the launch of YouTube’s advertisement-free subscription service, YouTube Premium (formerly known as YouTube Red), and its over-the-top (OTT) internet television service YouTube TV.\n\nDuring her tenure, YouTube has tightened its policy on videos it regards as potentially violating its policies on hate speech and violent extremism. The more stringent policies came after \"The Times\" showed that \"ads sponsored by the British government and several private sector companies had appeared ahead of YouTube videos supporting terrorist groups\" and several large advertisers withdrew their ads from YouTube in response. The enforcement policies have been criticized as censorship.\n\nDuring the controversy surrounding Logan Paul's YouTube video about a person that committed suicide, Wojcicki said that Paul did not violate YouTube's three strike policy and did not meet the criteria for being banned from the platform.\n\nOn October 22, 2018, Wojcicki criticized Article 13 of the European Union Copyright Directive that would give YouTube the sole responsibility for removing copyrighted content, saying it would pose a threat to content creators' ability to share their work.\n\nWojcicki married Dennis Troper on August 23, 1998, in Belmont, California. They have five children. On December 16, 2014, ahead of taking her fifth maternity leave, Wojcicki wrote an op-ed in the \"Wall Street Journal\" about the importance of paid maternity leave. She is often quoted talking about the importance of finding balance between family and career.\n\nIn addition to her US citizenship she is a Polish citizen. Her grandfather, , was a People's Party and Polish People's Party politician who had been elected MP during the Polish legislative election, 1947.\n\nWojcicki has been an advocate for several causes, including the expansion of paid family leave, the plight of Syrian refugees, countering gender discrimination at technology companies, getting girls interested in computer science and prioritizing coding in schools.\n\nWojcicki was named #1 on the Adweek \"Top 50 Execs\" list in 2013, which recognizes the top media executives within an organization. She was named #27 on Vanity Fair's \"New Establishment\" list in 2015.\n\n\n"}
{"id": "92138", "url": "https://en.wikipedia.org/wiki?curid=92138", "title": "Syncom", "text": "Syncom\n\nSyncom (for \"synchronous communication satellite\") started as a 1961 NASA program for active geosynchronous communication satellites, all of which were developed and manufactured by Hughes Space and Communications. Syncom 2, launched in 1963, was the world's first geosynchronous communications satellite. Syncom 3, launched in 1964, was the world's first geostationary satellite.\n\nIn the 1980s, the series was continued as Syncom IV with some much larger satellites, also manufactured by Hughes. They were leased to the United States military under the Leasat program.\n\nThe three early Syncom satellites were experimental spacecraft built by Hughes Aircraft Company's facility in Culver City, California, by a team led by Harold Rosen, Don Williams, and Thomas Hudspeth. All three satellites were cylindrical in shape, with a diameter of about and a height of about . Pre-launch fueled masses were , and orbital masses were with a payload. They were capable of emitting signals on two transponders at just 2 W. Thus, Syncom satellites were only capable of carrying a single two-way telephone conversation, or 16 Teletype connections. , all three satellites are still in orbit, although no longer functioning.\n\nSyncom 1 was intended to be the first geosynchronous communications satellite. It was launched on February 14, 1963 with the Delta B #16 launch vehicle from Cape Canaveral, but was lost on the way to geosynchronous orbit due to an electronics failure. Seconds after the apogee kick motor for circularizing the orbit was fired, the spacecraft fell silent. Later telescopic observations verified the satellite was in an orbit with a period of almost 24 hours at a 33° inclination.\n\nSyncom 2 was launched by NASA on July 26, 1963 with the Delta B #20 launch vehicle from Cape Canaveral. The satellite successfully kept station at the altitude calculated by Herman Potočnik Noordung in the 1920s.\n\nDuring the first year of Syncom 2 operations, NASA conducted voice, teletype, and facsimile tests, as well as 110 public demonstrations to show the capabilities of this satellite and invite feedback. In August 1963, President John F. Kennedy in Washington, D.C., telephoned Nigerian Prime Minister Abubakar Tafawa Balewa aboard docked in Lagos Harbor; the first live two-way call between heads of government by satellite. The \"Kingsport\" acted as a control station and uplink station.\n\nSyncom 2 also relayed a number of test television transmissions from Fort Dix, New Jersey to a ground station in Andover, Maine, beginning on September 29, 1963. Although it was low-quality video with no audio, it was the first successful television transmission through a geosynchronous satellite.\n\nSyncom 3 was the first geostationary communication satellite, launched on August 19, 1964 with the Delta D #25 launch vehicle from Cape Canaveral. The satellite, in orbit near the International Date Line, had the addition of a wideband channel for television and was used to telecast the 1964 Summer Olympics in Tokyo to the United States. Although Syncom 3 is sometimes credited with the first television program to cross the Pacific Ocean, the Relay 1 satellite first broadcast television from the United States to Japan on November 22, 1963.\n\nTurned off in 1969, Syncom 3 remains in geosynchronous orbit as of December 2012. In 40 years it has drifted 8 degrees to the west, to longitude 172.\n\nThe five satellites of the 1980s Leasat (Leased Satellite) program (Leasat F1 through Leasat F5) were alternatively named Syncom IV-1 to Syncom IV-5 and called HS 381 by the manufacturer. These satellites were considerably larger than Syncoms 1 to 3, weighing 1.3 tonnes each (over 7 tonnes with launch fuel). At , the satellites were the first to be designed for launch from the Space Shuttle payload bay, and were deployed like a Frisbee. The satellites are 30 rpm spin-stabilized with a despun communications and antenna section. They were made with a solid rocket motor for initial perigee burn and hydrazine propellant for station keeping and spin stabilization. The communications systems offers a wideband UHF channel (500 kHz bandwidth), six relay 25 kHz channels, and five narrowband 5 kHz channels. This is in addition to the fleet broadcast frequency, which is in the military's X-band. The system was used by military customers in the US and later in Australia. Most of the satellites were retired in the 1990s, but one would remain operational until 2015. During the First Gulf War, Leasat would be used for personal communications between Secretary of State James Baker and President George H. W. Bush, but was more typically used by \"mobile air, surface, subsurface, and fixed earth stations of the Navy, Marine Corps, Air Force, and Army.\"\n\nHughes was contracted to provide a worldwide communications system based on four satellites, one over the continental United States (CONUS), and one each over the Atlantic, Pacific, and Indian oceans, spaced about 90 degrees apart. Five satellites were ordered, with one as a replacement. Also part of the contract were the associated control systems and ground stations. The lease contracts were typically for 5-year terms, with the lessee having the opportunity to extend the lease or to purchase the equipment outright. The US Navy was the original lessee.\n\nLeasat F1's launch was cancelled just prior to lift-off, and F2 became the first into orbit on August 30, 1984 aboard \"Discovery\" on shuttle mission STS-41-D. F2 was largely successful, but its wideband receiver was out of commission after only 4 months. F1 was launched successfully on November 8, 1984 aboard STS-51-A. This was followed on April 12, 1985 by Leasat F3 on STS-51-D. F3's launch was declared a failure when the satellite failed to start its maneuver to geostationary orbit once released from \"Discovery\". Attempts by Shuttle astronauts to activate F3 with a makeshift \"flyswatter\" were unsuccessful. The satellite was left in low earth orbit, and the Space Shuttle returned to Earth. This failure made front-page news in the \"New York Times\". Hughes had an insurance policy on the satellite, and so claimed a total loss for the spacecraft of about $200 million, an amount underwritten by numerous parties.\n\nHowever, with another satellite planned to be launched, it was determined that a space walk by a subsequent Shuttle crew might be able to \"wake\" the craft. The best guess was that a switch had failed to turn on the satellite. A \"bypass box\" was hastily constructed, NASA was excited to offer assistance, the customer was supportive, and the insurance underwriters agreed to fund the attempt at space salvage – a first.\n\nOn August 27, 1985 \"Discovery\" was again used to launch Leasat F4, and during the same mission (STS-51-I) captured the 15,000 lb stricken F3. Astronaut James van Hoften grappled and then \"manually\" spun down the F3 satellite. After the bypass box was installed by van Hoften and Bill Fisher, van Hoften manually spun the satellite up. Once released, the F3 successfully powered up, fired its perigee motor and obtained a geostationary orbit. (This scenario would play out again in 1992 with Intelsat 603 and \"Endeavour\".) While F3 was now operational, Leasat F4 soon failed and was itself declared a loss after only 40 hours of RF communications.\n\nThe stricken F4 did not remain a complete failure. Data from F4's failure permitted the saving of F1 from a premature failure. Since all of the Leasats are spin-stabilized, they have a bearing point that connects the non-rotating and rotating parts of the spacecraft. After F4's communication failure, it suffered a spin lock while attempting to jostle the communications payload: the spun and despun sections locked together. Remembering this second failure of F4, and with F1 beginning to wear out at the spin bearing, it was decided to \"flip\" F1 every six months to keep the payload in the sun. Thus F1 went on to operate smoothly for its remaining life and never encountered a locked despun section.\n\nLeasat F4 was subsequently powered down and moved to a graveyard orbit with a large amount of station keeping fuel in reserve. This was fortuitous; when another satellite suffered a loss of its fuel ten years later, Hughes engineers pioneered the use of alternative propellants with Leasat F4. Long after its primary mission had failed, F4 was powered back on to test whether a satellite could be kept on station using nonvolatile propellants. F4 was used to perform numerous tests, including maneuvers with oxidizer for propulsion once the hydrazine ran out.\n\nThe fifth and last Leasat (F5), which was built as a spare, was successfully launched by \"Columbia\" mission STS-32 on January 9, 1990. The last active Leasat, it was officially decommissioned on September 24, 2015, at 18:25:13 UTC. F5 was one of the longest-serving and most successful commercial satellites. Towards the end of its 25-year life, F5 had been leased by the Australian Defence Force for UHF service.\n\n\n\n"}
{"id": "12292824", "url": "https://en.wikipedia.org/wiki?curid=12292824", "title": "TN 61", "text": "TN 61\n\nThe TN 61 was a French nuclear warhead.\n\nIt was a lighter weight version of the TN 60 warhead and quickly replaced it in service on the M20 SLBMs on the Redoutable class SSBNs. The TN 61 was also used on the SSBS S3D IRBM missiles. The 1 to 1.2 megaton yield weapon was in service between 1977 and 1991.\n\n"}
{"id": "23326147", "url": "https://en.wikipedia.org/wiki?curid=23326147", "title": "Terso Solutions", "text": "Terso Solutions\n\nTerso Solutions, Inc., located in Madison, Wisconsin, USA, is the developer and distributor of an automated system for storage and distribution of high value research reagents and medical supplies. Terso first developed a reagent stocking system, which uses RAIN radio frequency identification (RFID) tags along with secure access control, linked to the internet. These combined technologies allow inventory to be securely tracked and managed remotely.\nDeveloped initially as an on-site inventory supplier for Promega products, the privately held Terso Solutions, Inc. was spun off from Promega Corporation in 2005. In February 2010, Terso Solutions formed Terso GmbH in Mannheim, Germany, in response to increasing demand for its RFID units in Europe. \n\nTerso Solutions employs 48 people. The company holds 17 United States patents, 2 European patents, and 3 Japanese patents. Joe Pleshek has been CEO of Terso Solutions since 2008.\n\nTerso Solutions offers RFID-enabled cabinets, refrigerators and freezers as part of an automated inventory management system for healthcare. Terso’s products are typically used in cath labs, emergency rooms, and hospital supply rooms. Benefits hospitals realize from Terso’s units include improved regulatory compliance, expiration date management, reduced manual inventory processes and secure, 24/7 real-time access to high value inventory.\n\nThe company’s RFID-enabled cabinets, refrigerators and freezers are used by biotechnology and biopharmaceutical laboratories. Labs and the manufacturers and distributors who supply them use the company’s products to gain more secure control over inventory, ensure product integrity through 24/7 temperature monitoring, expiration date management and by eliminating surprise product outages.\n\nTerso also offers an RFID-enabled Smart Stockroom, which is marketed as an efficient 24/7 system for managing medical and research inventory. The room-based solution is designed to allow users to access products anytime without manual sign out, and to capture all of this data in real-time.\n\nRadio-frequency identification or RFID tags work by emitting radio waves, either actively or passively, which allow the object or item carrying the tag, to be identified. Active RFID tags involve a power source such as a battery and emit signals, while passive devices require a scanning or reading device for signal identification. Tags can be worn, attached to or implanted in an object. Microchips used in pet animals such as cats and dogs work by RFID. When an animal is found, if it has a microchip bearing an RFID device implanted, the animal can be scanned and the owner’s contact information transmitted.\nTerso Solutions supply cabinets and freezers provide temperature-controlled storage and on-site location of supplies. This allows 24/7 access to research reagents, as well as automated inventory control. The cabinets and freezers provide secured access, so that only authorized users can remove the products. This controlled access, combined with RFID tags, allows real-time tracking of products and is designed to allow easy inventory assessment and timely restocking as the supply of reagents is depleted.\n\nA new and rapidly evolving field of medicine surrounds the collection and distribution of tissues for reuse in human patients, including tissues as diverse as eyes, musculoskeletal tissue, eggs, sperm and blood stem cells. Experts agree that the field, in its infancy, has the ability to improve the quality of life, or in some cases preserve life for many patients.\nAs the list of potential patients grows, so does the complexity of collecting, storing and distributing high quality tissue. The number of regulatory agencies involved in tissue procurement and distribution itself is substantial, including the Food and Drug Administration (FDA), the Centers for Disease Control and Prevention (CDC), the Joint Commission on Accreditation of Healthcare Organizations (JCAHO) and the American Association of Tissue Banks (AATB). These agencies cooperatively define the processes by which biological materials from donors are collected, stored, distributed and used. Storage of such tissues in RFID-enabled cabinets and freezers is an ideal means of ensuring proper storage of, tracking both supplier and user of, and maintaining inventory of such tissues.\n\n"}
{"id": "17888010", "url": "https://en.wikipedia.org/wiki?curid=17888010", "title": "Timatic", "text": "Timatic\n\nTimatic is the database containing cross border passenger documentation requirements. Timatic stands for Travel Information Manual Automatic and is used by airport ground staff to determine whether a passenger can be carried, as well as by airlines and travel agents to provide this information to travellers at the time of booking. This is critical for airlines due to the fines levied by immigration authorities every time a passenger is carried who does not have the correct travel documentation, as well as the airline's costs to return the incorrectly-boarded passenger to the original airport from which the passenger departed. \nThe information contained in Timatic includes:\nTimatic was first established in 1963 and is managed by the International Air Transport Association (IATA). Over 60 million travellers have their documentation requirements checked against the Timatic database every year.\n\nIt is available in a number of forms including:\n\n"}
{"id": "962498", "url": "https://en.wikipedia.org/wiki?curid=962498", "title": "UCL Business", "text": "UCL Business\n\nUCL Business PLC (UCLB) is the technology-transfer company of University College London Hospital NHS Foundation Trsust (UCL) and is headquartered in London, United Kingdom. It is a wholly owned subsidiary of UCL and is responsible for conducting technology development and commercial transactions for the university. It is based on Tottenham Court Road in the Camden area of Central London.\n\nUCL's first technology-transfer company was founded in 1989 as UCL Ventures. UCL Ventures merged with the technology-transfer company of the Royal Free Hospital, Freemedic plc (founded in 1993) to form UCL Biomedica PLC.\n\nUCL Business PLC was created in August 2006 through the merger of UCL BioMedica with UCL’s internal knowledge transfer department, also known as UCL Business. The former UCL Business had been established to provide a link between UCL's academics and industry, and to aid the development of commercially valuable technologies arising from UCL, whilst UCL BioMedica had been established to commercialise opportunities arising from UCL’s biomedical research strengths, as well as to conduct clinical trials. The integration of the two technology transfer activities created a single organisation focused on delivering the complete commercialisation process from patent registration and support for the creation of new businesses, through to licensing and sales of technologies to industry partners.\n\nUCLB provided advice and a grant support to Siavash Haroun Mahdavi in helping him to establish the robotics company Complex Matters.\n\nIn January 2011, BioVex, a cancer vaccines company spun-out of UCL in 1999, was sold to Amgen for $1 billion.\n\nUCLB has equity stake in all of its companies, the majority being spin-outs arising from technologies developed across the full range of faculties within UCL, including biomedical, biotechnology, engineering, mathematics, physical sciences and build environment companies. These include Ark Therapeutics Ltd, Arrow Therapeutics Ltd, Biovex Ltd, Canbex, Domainex Ltd, Evexar Medical Ltd, Genex Biosystems, Intercytex, Medic-to-Medic, Pentraxin Therapeutics, PolyMASC Pharmaceuticals PLC, Proaxon, Spirogen Ltd, Stanmore Implants Worldwide Ltd, Advanced Design Technology, AS Built Solutions, Bloomsbury DSP, Endomagnetics, EuroTempest, Ixico, Quantemol, Senceive, Space Syntax, Zinwave.\n\n"}
{"id": "19256144", "url": "https://en.wikipedia.org/wiki?curid=19256144", "title": "United States gravity control propulsion research", "text": "United States gravity control propulsion research\n\nAmerican interest in \"gravity control propulsion research\" intensified during the early 1950s. Literature from that period used the terms anti-gravity, anti-gravitation, baricentric, counterbary, electrogravitics (eGrav), G-projects, gravitics, gravity control, and gravity propulsion. Their publicized goals were to develop and discover technologies and theories for the manipulation of gravity or gravity-like fields for propulsion. Although general relativity theory appeared to prohibit anti-gravity propulsion, several programs were funded to develop it through gravitation research from 1955 to 1974. The names of many contributors to general relativity and those of the golden age of general relativity have appeared among documents about the institutions that had served as the theoretical research components of those programs. The existence and 1950s emergence of the gravity control propulsion research have not been a subject of controversy for aerospace writers, critics, and conspiracy theory advocates, but their rationale, effectiveness, and longevity have been the objects of contested views.\n\nMainstream newspapers, popular magazines, technical journals, and declassified papers reported the existence of the gravity control propulsion research. For example, the title of the March 1956 \"Aero Digest\" article about the intensified interest was \"Anti-gravity Booming.\" A. V. Cleaver made the following statement about the programs in his article:\n\nThe gravitics programs had not been evinced by any technological artifacts, such as the Project Pluto Tory IIA, the world's first nuclear ramjet. Commemorative monuments by the Gravity Research Foundation have been the artifacts attesting to the early commitments to finding materials and methods to manipulate gravity. The endeavor had the resources and publicity of an initiative, but writers from that period did not describe them with that term. Gladych stated:\n\nThe writings about the gravity control propulsion research effort had disclosed the \"players\" and resources while prudently withholding both the specific features of the research and the identity of its coordinating body. Publicized and telecasted conspiracy theory anecdotes have suggested much higher levels of success to the G-projects than mainstream science.\n\nRecent historical analysis and reports have attracted attention to the agencies and firms that had participated in the gravity control propulsion research. James E. Allen, BAE Systems consultant and engineering professor at Kingston University, referred to those programs in his history of novel propulsion systems for the journal \"Progress in Aerospace Sciences\". Research by Dr. David Kaiser, Associate Professor of the History of Science, Massachusetts Institute of Technology, manifested the contributions made by the Gravity Research Foundation to the pedagogical aspects of the golden age of general relativity. Dr. Joshua Goldberg, Syracuse University, described the Air Force's support of relativity research during that period. Progress reports and anecdotes and Internet resumes of former visiting and staff scientists have been the sources of the history of the Research Institute for Advanced Study (RIAS). Former aviation editor of \"Jane's Defense Weekly\", Nick Cook, drew attention to the antigravity programs through worldwide publications of his book, \"The Hunt for Zero Point\", and subsequent televised documentaries. Mainstream historical accounts of the G-projects have been supplemented with conspiracy theory anecdotes.\n\nLists of the research institutes, industrial sites, and policy makers along with statements from prominent physicists were provided in five comprehensive works that had been published during the early years of the gravity control propulsion research. Aviation Studies (International) Limited, London, published a detailed report about those activities by the Gravity Research Group that was later declassified. The \"Journal of the British Interplanetary Society\" and \"The Aeroplane\" published the propulsion survey and critical assessment of the American gravitics research by the internationally recognized astronautics historian A. V. Cleaver. The \"New York Herald Tribune\" and \"Miami Herald\" published a series of three articles by one of the world's greatest aviation journalists of the twentieth century, Ansel Talbert. Talbert's two series of newspaper articles took place in the midst of the policy-by-press-release era. Neither his, nor the writings that followed the five prominent works from that period, yielded denials and/or retractions.\n\nGravity control propulsion research had been the subject of widely published UFO and conspiracy theory literature. The documented testimonies of whistleblowers edited by Dr. Steven Greer, Director of the Disclosure Project; anecdotes and schematics by Mark McCandlish and Milton William Cooper; and the reports by Philip J. Corso, David Darlington, and Donald Keyhoe, famous UFO researcher, have suggested incorporation of reverse engineering of recovered extraterrestrial vehicles with the anti-gravity propulsion projects had enabled them to continue beyond 1973 to successfully manufacture antigravity vehicles. Branches of the military and defense agencies have denied and refuted such claims.\n\nTalbert indicated the rationale for the intensified interest in gravity control propulsion research had stemmed from the works of three physicists. They were Bryce DeWitt's prize-winning Gravity Research Foundation essay; the book \"Gravity and the Universe\" by Pascual Jordan; and presentations to the International Astronautical Federation by Dr. Burkhard Heim. DeWitt's essay discouraged the pursuit of materials that shield, reflect, and/or insulate gravity and emphasized the need to encourage young physicists to pursue gravitational research. He opened his essay with the following paragraph:\n\nSeveral articles cited his essay during and after the gravity control propulsion research period. Within a few years facilities emerged embodying the theme of DeWitt's call for increased stimuli for research.\n\nPhysical principle surveys by Cleaver and Weyl stated the antigravity research was not based on any recognized theoretical breakthroughs. Cleaver's skepticism suggested an alternative rationale for establishing that research was based on a science fiction novel. Weyl charged publishers with poor journalism; attacked their terminology; and gave the highest rating for prospective physical principles for gravity control propulsion to Burkhard Heim's works. Stambler leveled harsh criticisms against Gluraheff's gravitation hypothesis. Talbert and other authors listed the following three agencies as the principal facilities that had conducted the theoretical research:\n\nSeveral articles contained expressions of gratitude for the support to the gravity control propulsion endeavor by the Gravity Research Foundation. Even though the Foundation was a humble, non-profit organization, its creator, Roger Babson, used his wealth and influence to mobilize industries; raise private and government funding; and motivate engineers and physicists to conduct research in gravity shielding and control. According to his autobiography: \"The purpose of the Foundation is to encourage others to work on gravity problems and aid others in obtaining rewards for their efforts.\"\n\nDuring Babson's lifetime, the Foundation conducted Gravity Day Conferences each summer; established a library on gravity; solicited essays that addressed (1.) various prospects for shielding gravity, (2.) the development and/or discovery of materials that could convert gravitational force into heat, or (3.) methods of manipulating gravity; and installed monuments at various universities that cited its antigravity focus.\n\nIn September, 1956, the General Physics Laboratory of the Aeronautical Research Laboratories (ARL) at Wright-Patterson Air Force Base, Dayton, Ohio, commenced an intense program to coordinate research into gravitational and unified field theories with the hiring of Joshua N. Goldberg. Creation by ARL of Goldberg's program may have been coincidental to Talbert's disclosures of commitments to gravity control propulsion research. The precise rationale for creating the program and justifying its budgets and personnel may never be determined. Neither Goldberg nor the Air Force's Deputy for Scientific and Technical Information, Walter Blados, were able to locate the founding documents. Roy Kerr, a former ARL scientist, stated the antigravity propulsion purpose of ARL was \"rubbish\" and that \"The only real use that the USAF made of us was when some crackpot sent them a proposal for antigravity or for converting rotary motion inside a spaceship to a translational driving system.\" The December 30, 1957 issue of Product Engineering closed its report with the following statement:\n\nDuring the following sixteen years, its name was changed to the Aerospace Research Laboratories. The ARL scientists produced nineteen technical reports and over seventy peer-reviewed journal articles. The Air Force's Foreign Technical Division, and other agencies, investigated stories about Soviet attempts to understand gravity. Such actions were consistent with the paranoia of the Cold War.\n\nThe funding for the military components of the gravity control propulsion research had been terminated by the Mansfield Amendment of 1973. Black project experts, conspiracy theorists, and whistleblowers had suggested the gravity control propulsion efforts had achieved their goals and had been continued decades beyond 1973.\n\nThe Research Institute for Advanced Study (RIAS) was conceived by George S. Trimble, the vice president for aviation and advanced propulsion systems, Glenn L. Martin Company, and was placed under the direct supervision of Welcome Bender. The first person Bender hired was Louis Witten internationally recognized authority on gravitation physics. Talbert's article had announced Trimble's completion of contractual agreements with Pascual Jordan and Burkhard Heim for RIAS. Subsequent hires yielded a half dozen gravity researchers known as the field theory group. Sir Arthur C. Clarke and others stated RIAS' assemble of talent was very qualified for the task of discovering new principles that could be used to develop gravity control propulsion systems.\n\nThe quest for propulsion through gravity control was vaguely implied in various publications. Works by Cook and Cleaver summarized statements in the RIAS brochures. Cook had equated the broad range of RIAS's mission statements with those of Skunk Works. In 1958, Mallan reported \"the control of the force of gravity itself for propulsion\" was one of the unorthodox goals initiated by Trimble for RIAS.\n\nRIAS was renamed the Research Institute for Advanced Studies during the sixties when the American-Marietta Company merged with Martin to become the Martin Marietta Company. The 1995 merger that yielded the Lockheed Martin Company modified its goals and not its name.\n\nTalbert's newspaper series and subsequent articles in technical magazines and journals listed the names of aerospace firms conducting gravity control propulsion research.\n\nThe Gravity Research Group indicated those companies had constructed \"rigs\" to improve the performance of Thomas Townsend Brown's gravitators through attempts to develop materials with high dielectric constants (k). Gravity Rand Limited provided a set of guidelines to help management conduct research and nurture creativity. Articles about the gravity propulsion research by the aerospace firms ceased after 1974. None of the companies featured in those publications had filed retractions. The following aerospace firms have been cited in the works published from 1955 through 1974:\n\n\nNone of the reported experimental breakthroughs published during the 1950s and 1960s have been recognized by the aerospace community.\n\nVarious reports indicated Brown's gravitators were the main experimental focus of the gravity control propulsion research. According to G. Harry Stine and Intel, research on Brown's gravitators became classified immediately after demonstrations of 30% weight reductions. Thomas Townsend Brown had obtained a British patent for high voltage, symmetric, parallel plate capacitors, that he called gravitators, in 1928. Brown claimed they would produce a net thrust in the direction of the anode of the capacitor that varied slightly with the positions of the Moon. The scientific community rejected such claims as products of pseudoscience and/or misinterpretations of ion wind effects.\n\nIndependent research found small amounts of lift from Brown's gravitator based on an inefficient use of ionic propulsion. The devices were named Ion Lifters or Ionocraft and were reported to be able to lift the empty shell of a vehicle under ideal conditions, but not the additional machinery required to generate the electric field. Gravity effects were not found in the independent research.\n\nIn July 1960, \"Missiles and Rockets\" reported Martin N. Kaplan, Senior Research Engineer, Electronics Division, Ryan Aeronautical Company, San Diego, had conducted anti-gravitational experiments yielding the promise of impulses, accelerations, and decelerations one hundred times the pull of gravity. Neither comments nor criticism of the report appeared in subsequent articles during the period of intensified gravity control propulsion research (see Section 1 of tractor beam for similar reports).\n\nRobert L. Forward, Hughes Research Laboratories, Malibu, described the theoretical generation of dipole gravitational fields by accelerating a super-dense fluid through pipes wound around a torus.\n\nMany of the contributors to general relativity have been supported by and/or associated with the ARL, RIAS, and/or the Gravity Research Foundation. The decades preceding the 1955 revelation of the gravity control propulsion research were a low water mark for general relativity. The following summarizes how the components of that research had stimulated the resurgence of general relativity:\n\nEven though some of the physicists who attended the Gravity Day Conferences quietly mocked the anti-gravity mission of the Foundation, it provided significant contributions to mainstream physics. The \"International Journal of Modern Physics D\" has featured selected papers from the Gravity Research Foundation essay competition. Many have been incorporated with the collections of the Niels Bohr Library. A few of the Foundation essay contest winners became Nobel laureates (e.g., Ilya Prigogine, Maurice Allais, George F. Smoot). Foundation essays have been among the resources graduate students check for new ideas. Kaiser summarized the Foundation's influence in the following manner:\n\nFoundation trustee, Agnew Bahnson, contacted Dr. Bryce DeWitt with a proposal to fund the creation of a gravity research institute. DeWitt had won the first prize for the 1953 essay contest. The proposed name was changed to the Institute for Field Physics and it was established in 1956 at the University of North Carolina at Chapel Hill under the direction of Bryce and his wife, Cécile DeWitt-Morette.\n\nThe peer reviewed physics journal, \"Physica C\", published a report by Eugene Podkletnov and Nieminen about gravity-like shielding. Although their work had gained international attention, researchers were not able to replicate Podkletnov's initial conditions. But, analyses by Giovanni Modanese and Ning Wu indicated various applications of quantum gravity theory could allow gravitational shielding phenomena. Those achievements have not been pursued by the scientific community.\n\nThe list of prominent contributors to the golden age of general relativity, contains the names of several scientists who had authored the nineteen ARL Technical Reports and/or seventy papers. The ARL sponsored papers were published in the \"Proceedings of the Royal Society of London, Physical Review, Journal of Mathematical Physics, Physical Review Letters, Physical Review D, Review of Modern Physics, General Relativity and Gravitation, International Journal of Theoretical Physics\", and \"Nuovo Cimento B\". Some of the ARL papers were written in collaboration with RIAS, the U.S. Army Signal Research and Development Laboratory at Fort Monmouth, New Jersey, and the Office of Naval Research. The ARL had provided significant enhancements to general relativity theory. For example, Roy Kerr's description of the behavior of space-time in the vicinity of a rotating mass was among those works. Goldberg concluded: \"However, it should be recognized that, in the United States, the Department of Defense played an essential role in building a strong scientific community without widespread encroachment on academic values.\"\n\nThe growth of nonlinear differential equations during the fifties was stimulated by RIAS. One of the leading groups in dynamical systems and control theory, the Lefschetz Center for Dynamical Systems, was a spinoff from RIAS. After the launch of Sputnik, world-class mathematician Solomon Lefschetz came out of retirement to join RIAS in 1958 and formed the world's largest group of mathematicians devoted to research in nonlinear differential equations. The RIAS mathematics group stimulated the growth of nonlinear differential equations through conferences and publications. It left RIAS in 1964 to form the Lefschetz Center for Dynamical Systems at Brown University, Providence, Rhode Island.\n\nOn May 9, 2001, Mark McCandlish testified on the televised news conference held by the Disclosure Project, at the National Press Club, Washington, D.C. He stated gravity control propulsion research had started in the fifties and had successfully reverse engineered the vehicle retrieved from the Roswell crash site to build three Alien Reproduction Vehicles (ARVs) by 1981. McCandlish described their propulsion systems in terms of Thomas Townsend Brown's gravitators and provided a line drawing of its interior. The diagram closely resembled the drawing provided earlier in Milton William Cooper's book. Another Disclosure Project whistleblower, Philip J. Corso, stated in his book the craft retrieved from the second crash site at Roswell, New Mexico, had a propulsion system resembling Thomas Townsend Brown's gravitators. And, Corso's book featured several gravity control propulsion statements made by Hermann Oberth.\n\nSoon after the end of the Cold War, a small group of scientists and engineers openly expressed their desire to use technologies developed by black projects for civil applications. Steven Greer formed the Disclosure Project in 1995 to help those and other research whistleblowers share their information with and to petition Congress. By 2001, it had provided reports to two Congressional hearings and had acquired over 400 members from branches of the military and aerospace industry.\n\nDuring the early 1960s, Keyhoe published excerpts from a letter by Hermann Oberth that presented explanations for the flight characteristics of UFO's in terms of gravity control propulsion. Prior to Oberth's letter, Keyhoe had supported arguments for magnetic forces as the source of propulsion for UFO's. The letter caused him to search for the existence of gravity control propulsion research programs. The following is a segment of his findings he had released in his 1966 and 1974 publications:\n\nDuring his press conferences on February 2, 1955 in Bogotá, and February 10, 1955 in Grand Rapids, Michigan aviation pioneer William Lear, stated one of his reasons for believing in flying saucers was the existence of American research efforts into antigravity. Talbert's series of newspaper articles about the intensified interest in gravity control propulsion research were published during the Thanksgiving week of that year.\n\n"}
