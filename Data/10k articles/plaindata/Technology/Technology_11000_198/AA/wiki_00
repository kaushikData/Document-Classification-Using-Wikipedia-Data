{"id": "4528440", "url": "https://en.wikipedia.org/wiki?curid=4528440", "title": "American Society for Enology and Viticulture", "text": "American Society for Enology and Viticulture\n\nThe American Society for Enology and Viticulture, founded in 1950, is a non-profit, scientific wine production industry organization headquartered in Davis, California. \n\nIts membership of 2,400 includes professionals from wineries, vineyards, academic institutions and organizations. In addition, it has 120 Industrial Affiliates (companies).\n\nIt is dedicated to promoting the interests of enologists, viticulturists, and others in the fields of wine and grape research and production throughout the world. \n\nThe society publishes the American Journal of Enology and Viticulture. \n\n\n"}
{"id": "44810542", "url": "https://en.wikipedia.org/wiki?curid=44810542", "title": "Asia-Oceania Top University League on Engineering", "text": "Asia-Oceania Top University League on Engineering\n\nThe Asia-Oceania Top University League on Engineering (abbreviated AOTULE, pronounced \"our tool\") is a league consisting of 13 engineering faculties within Asia and Oceania universities. AOTULE's mission is to improve the quality of its member's educational programs and promote research activity among members primarily through exchange of information between deans, faculty members and administration staff at its annual meeting. It also organizes graduate student exchange programs and conferences where graduate students present their latest research results in an interdisciplinary format.\n\nThe seeds for forming AOTULE began in 2006 with discussions between senior engineering faculty at Tokyo Institute of Technology and Monash University. to promote graduate engineering student mobility within Asia and Oceania universities similar to the ERASMUS+ program offered by EU universities that is funded by the European commission. AOTULE was subsequently founded in 2007 at the Tokyo Institute of Technology, by holding its inaugural meeting where participating Engineering Deans signed a memorandum of understanding. Each Fall since 2007, an AOTULE member has organized and hosted the annual AOTULE student conference, administration staff and Dean's meeting as noted below. \n\nThe annual Deans', administrative staff meeting and student conference is held annually in October or November. Attendance at the annual conference varies between 130 - 180 participants. The Deans' meeting brings together Deans for discussions on topical educational programs related to the annual Deans meeting theme as well as joint educational programs, degree programs, accreditation, collaborations with industry and study-abroad programming. At the administration staff meeting, staff discuss issues related to administering student exchanges and share best practices. The student conference provides students an international forum to discuss their research with a multidisciplinary student audience. The 2013 AOTULE meeting and conference was held at Chulalongkorn in October 17–19, 2013, which coincided with the 100th anniversary of Chulalongkorn university. The 2016 AOTULE annual meeting and conference will be held at the Hong Kong University of Science and Technology (HKUST) in Hong Kong on Nov. 23-25, 2016. The 2017 AOTULE Deans meeting and conferences was held on Nov. 23-24, 2017 at the Hanoi University of Science and Technology (HUST) in Hanoi, Vietnam.\n\nTo promote student mobility, AOTULE members organize intra-AOTULE student short stays and research exchanges varying in length from one week at Chulalongkorn University to three months at Tokyo Institute of Technology. These exchanges facilitate global engineering, cross-cultural competencies, foreign language learning, and research experiences by students since the majority of AOTULE members' students live in countries where English is not the native language. AOTULE members such as Tokyo Tech's School of Engineering have used AOTULE as a test bed for creating new research exchange programs that are later broaden to university-wide programs with research university partners in the USA and EU . Recently, there has been growing numbers of double degree graduate programs signed between AOTULE member institutions to allow participating graduate students to obtain two degrees by completing graduation requirements at two institutions. This allows double degree participants an opportunity to learn more about the host country where they are studying, undertake a research project in greater depth and establish a greater network of peers than that provided by a short term exchanges. \n\n\nAOTULE * \n"}
{"id": "1634352", "url": "https://en.wikipedia.org/wiki?curid=1634352", "title": "Atomic battery", "text": "Atomic battery\n\nThe terms atomic battery, nuclear battery, tritium battery and radioisotope generator are used to describe a device which uses energy from the decay of a radioactive isotope to generate electricity. Like nuclear reactors, they generate electricity from atomic energy, but differ in that they do not use a chain reaction. Compared to other batteries they are very costly, but have an extremely long life and high energy density, and so they are mainly used as power sources for equipment that must operate unattended for long periods of time, such as spacecraft, pacemakers, underwater systems and automated scientific stations in remote parts of the world.\n\nNuclear battery technology began in 1913, when Henry Moseley first demonstrated the beta cell. The field received considerable in-depth research attention for applications requiring long-life power sources for space needs during the 1950s and 1960s. In 1954 RCA researched a small atomic battery for small radio receivers and hearing aids. Since RCA's initial research and development in the early 1950s, many types and methods have been designed to extract electrical energy from nuclear sources. The scientific principles are well known, but modern nano-scale technology and new wide bandgap semiconductors have created new devices and interesting material properties not previously available.\n\nBatteries using the energy of radioisotope decay to provide long-lived power (10–20 years) are being developed internationally. Conversion techniques can be grouped into two types: thermal and non-thermal. The thermal converters (whose output power is a function of a temperature differential) include thermoelectric and thermionic generators. The non-thermal converters (whose output power is not a function of a temperature difference) extract a fraction of the incident energy as it is being degraded into heat rather than using thermal energy to run electrons in a cycle. Atomic batteries usually have an efficiency of 0.1–5%. High efficiency betavoltaics have 6–8%.\n\nA thermionic converter consists of a hot electrode which thermionically emits electrons over a space charge barrier to a cooler electrode, producing a useful power output. Caesium vapor is used to optimize the electrode work functions and provide an ion supply (by surface ionization) to neutralize the electron space charge.\n\nA radioisotope thermoelectric generator (RTG) uses thermocouples. Each thermocouple is formed from two wires of different metals (or other materials). A temperature gradient along the length of each wire produces a voltage gradient from one end of the wire to the other; but the different materials produce different voltages per degree of temperature difference. By connecting the wires at one end, heating that end but cooling the other end, a usable, but small (millivolts), voltage is generated between the unconnected wire ends. In practice, many are connected in series to generate a larger voltage from the same heat source, as heat flows from the hot ends to the cold ends. Metal thermocouples have low thermal-to-electrical efficiency. However, the carrier density and charge can be adjusted in semiconductor materials such as bismuth telluride and silicon germanium to achieve much higher conversion efficiencies.\n\nThermophotovoltaic cells work by the same principles as a photovoltaic cell, except that they convert infrared light (rather than visible light) emitted by a hot surface, into electricity. Thermophotovoltaic cells have an efficiency slightly higher than thermoelectric couples and can be overlaid on thermoelectric couples, potentially doubling efficiency. The University of Houston TPV Radioisotope Power Conversion Technology development effort is aiming at combining thermophotovoltaic cells concurrently with thermocouples to provide a 3- to 4-fold improvement in system efficiency over current thermoelectric radioisotope generators. \n\nA Stirling radioisotope generator is a Stirling engine driven by the temperature difference produced by a radioisotope. A more efficient version, the advanced Stirling radioisotope generator, was under development by NASA, but was cancelled in 2013 due to large-scale cost overruns.\n\nNon-thermal converters extract a fraction of the nuclear energy as it is being degraded into heat. Their outputs are not functions of temperature differences as are thermoelectric and thermionic converters. Non-thermal generators can be grouped into three classes.\n\nIn the first type, the primary generator consists of a capacitor which is charged by the current of charged particles from a radioactive layer deposited on one of the electrodes. Spacing can be either vacuum or dielectric. Negatively charged beta particles or positively charged alpha particles, positrons or fission fragments may be utilized. Although this form of nuclear-electric generator dates back to 1913, few applications have been found in the past for the extremely low currents and inconveniently high voltages provided by direct charging generators. Oscillator/transformer systems are employed to reduce the voltages, then rectifiers are used to transform the AC power back to direct current.\n\nEnglish physicist H.G.J. Moseley constructed the first of these. Moseley’s apparatus consisted of a glass globe silvered on the inside with a radium emitter mounted on the tip of a wire at the center. The charged particles from the radium created a flow of electricity as they moved quickly from the radium to the inside surface of the sphere. As late as 1945 the Moseley model guided other efforts to build experimental batteries generating electricity from the emissions of radioactive elements.\n\nBetavoltaics are generators of electric current, in effect a form of battery, which use energy from a radioactive source emitting beta particles (electrons). A common source used is the hydrogen isotope, tritium. Unlike most nuclear power sources, which use nuclear radiation to generate heat, which then generates electricity (thermoelectric and thermionic sources), betavoltaics use a non-thermal conversion process, using a semiconductor p-n junction.\n\nBetavoltaics are particularly well-suited to low-power electrical applications where long life of the energy source is needed, such as implantable medical devices or military and space applications.\n\nAlphavoltaic power sources are devices that use a semiconductor junction to produce electrical particle from energetic alpha particles.\n\nAn optolectric nuclear battery has also been proposed by researchers of the Kurchatov Institute in Moscow. A beta-emitter (such as technetium-99) would stimulate an excimer mixture, and the light would power a photocell. The battery would consist of an excimer mixture of argon/xenon in a pressure vessel with an internal mirrored surface, finely-divided Tc-99, and an intermittent ultrasonic stirrer, illuminating a photocell with a bandgap tuned for the excimer. The advantage of this design is that precision electrode assemblies are not needed, and most beta particles escape the finely-divided bulk material to contribute to the battery's net power.\n\nElectromechanical atomic batteries use the buildup of charge between two plates to pull one bendable plate towards the other, until the two plates touch, discharge, equalizing the electrostatic buildup, and spring back. The mechanical motion produced can be used to produce electricity through flexing of a piezoelectric material or through a linear generator. Milliwatts of power are produced in pulses depending on the charge rate, in some cases multiple times per second (35 Hz).\n\nAtomic batteries use radioisotopes that produce low energy beta particles or sometimes alpha particles of varying energies. Low energy beta particles are needed to prevent the production of high energy penetrating Bremsstrahlung radiation that would require heavy shielding. Radioisotopes such as tritium, nickel-63, promethium-147, and technetium-99 have been tested. Plutonium-238, curium-242, curium-244 and strontium-90 have been used.\n\n\n"}
{"id": "43867008", "url": "https://en.wikipedia.org/wiki?curid=43867008", "title": "Autonomous decentralized system", "text": "Autonomous decentralized system\n\nAn autonomous decentralized system (or ADS) is a decentralized system composed of modules or components that are designed to operate independently but are capable of interacting with each other to meet the overall goal of the system. This design paradigm enables the system to continue to function in the event of component failures. It also enables maintenance and repair to be carried out while the system remains operational. Autonomous decentralized systems have a number of applications including industrial production lines, railway signalling and robotics.\n\nThe ADS has been recently expanded from control applications to service application and embedded systems, thus autonomous decentralized service systems and autonomous decentralized device systems.\n\nAutonomous decentralized systems were first proposed in 1977.\n\nADS received significant attention as such systems have been deployed in Japanese railway systems for many years safely with over 7 billion trips, proving the value of this concept. Japan railway with ADS is considered as a smart train as it also learns.\n\nTo recognizing this outstanding contribution, Dr. Kinji Mori has received numerous awards including 2013 IEEE Life Fellow, 2012 Distinguished Service Award, Tokyo Metropolitan Government, 2012 Distinguished Specialist among 1000 in the world, Chinese Government, 2008 IEICE Fellow, 1995 IEEE Fellow 1994 Research and Development Award of Excellence Achievers, Science and Technology Agency, 1994 Ichimura Industrial Prize, 1992 Technology Achievement Award, Society of Instrument and Control Engineers, 1988 National Patent Award, Science and Technology Agency, and 1988 Mainichi Technology Prize of Excellence. Dr. Mori donated the cash from Ichimura Industrial Price to IEEE to fund the IEEE Kanai Award.\n\nSince 1977, ADS has been a subject of research by many researchers in the world including US, Japan, EU particularly Germany, and China.\n\nAn ADS is a decoupled architecture where each component or subsystem communicates by message passing using shared data fields. A unique feature of the ADS is that there is no central operating system or coordinator. Instead each subsystem manages its own functionality and its coordination with other subsystems. When a subsystem needs to interact with other subsystems it broadcasts the shared data fields containing the request to all other subsystems. This broadcast does not include the identification or address of any other subsystem. Rather the other subsystems will, depending on their purpose and function, receive the broadcast message and make their own determination on what action (if any) to take.\n\nAs ADS moves into the service-oriented architecture (SOA) or ADSS (Autonomous Decentralized Service System), the data transmission can be carried out by ESB (Enterprise Service Bus), and each agent can a service that receives data from the ESB and acts according to the service specification. The results are again transmitted by the ESB to other autonomous agents.\n\nAn ADS is also similar to a blackboard system used in AI where a collection of agents will act upon seeing any data change in the common blackboard.\n\nAn ADS may include human in the loop, with both human and autonomous agents both co-learn at the same time to perform the system functionality.\n\nCloud computing also uses autonomous computing, but its architecture and framework are different from ADS.\n\nOne application of ADS is software testing, particularly combinatorial testing. A framework has been proposed based on ADS for concurrent combinatorial testing using AR and TA.\n\nIEEE International Symposium on Autonomous Decentralized Systems (ISADS) is the major conference on this topic. The Symposium is a biannual event and the first Symposium was held in 1993.\nISADS 1993: March 30 - April 1, 1993 in Kawasaki, Japan. \nISADS 1995： April 25–27, 1995, Phoenix, Arizona, USA.\nISADS 1997：April 9–11, 1997, Berlin, Germany.\nISADS 1999：March 20–23, 1999, Tokyo, Japan.\nISADS 2001：March 26–28, 2001, Dallas, Texas, USA.\nISADS 2003： April 9–11, 2003, Pisa, Italy.\nISADS 2005：April 4–8, 2005, Chengdu, China.\nISADS 2007： March 21–23, 2007, Sedona, Arizona, USA.\nISADS 2009： March 23–25, 2009, Athens, Greece.\nISADS 2011：June 29 - July 1, 2011, Kobe, Japan.\nISADS 2013：March 6–8, 2013, Mexico City, Mexico.\nISADS 2015：March 25–27, 2015, Taichung, Taiwan.\n\n"}
{"id": "1228774", "url": "https://en.wikipedia.org/wiki?curid=1228774", "title": "Axel Wenner-Gren", "text": "Axel Wenner-Gren\n\nAxel Lennart Wenner-Gren (5 June 1881 – 24 November 1961) was a Swedish entrepreneur and one of the wealthiest men in the world during the 1930s.\n\nHe was born on 5 June 1881 in Uddevalla, a town on the west coast of Sweden. He was the fourth of six children (four girls and two boys) born to Leonard and the much younger Alice Wenner-Gren (née Albin), though only three of them grew to adulthood, Axel himself, his oldest sister Anna, and his younger brother Hugo.\n\nHaving spent his school years in Uddevalla he moved to Gothenburg where he was employed for five years in the spice importing company of a maternal uncle. During this time, he learned English, French, and German at the local Berlitz school, and music at the local YMCA .\nIn 1902, at the age of 21, he left Sweden to further his studies in Germany. He first studied in the university town of Greifswald where he took some summer courses before moving on to Berlin where he studied at the Berliner Handelsakademie from which he graduated much sooner than usual.\n\nAfter some difficulty, he found work with the German subsidiary of Alfa Laval Separator where he developed skills as a salesman, before quitting in 1904 to work selling agricultural machinery near Stuttgart which, with financial support from his father, had become his first financial enterprise.\n\nIn 1908, he traveled to America where he learned about engines for agricultural use, returning to Europe the same year. While in Vienna in 1908 he saw the Santo vacuum cleaner in the shop of Gustaf Paalen who had exclusive rights to distribute them throughout Europe . After initially failing to become a European distributor for the Santo vacuum cleaner in his own right, he entered into a partnership with Paalen, purchasing a twenty percent interest in the company.\n\nEarlier in his life he notably collaborated with Fredrik Ljungström.\n\nWenner-Gren amassed a fortune from his early appreciation that the industrial vacuum cleaner could be adapted for domestic use. Soon after the First World War he persuaded the Swedish lighting company Electrolux, for which he then worked (securing the contract to floodlight the opening ceremony of the Panama Canal, among other successes), to buy the patent to a cleaner and to pay him for sales in company stock. By the early 1930s, Wenner-Gren was the owner of Electrolux, and the firm was a leading brand in both vacuum cleaner and refrigerator technology.\n\nWenner-Gren also diversified his interests into the ownership of newspapers, banks and arms manufacturers, and acquired many of the holdings of the disgraced safety-match tycoon Ivar Kreuger. In Mexico in the 1930s, he was in economic alliance with Maximino Ávila Camacho, strongman of the Mexican state of Puebla, whose brother Manuel Ávila Camacho became President of Mexico in 1940.\n\nWenner-Gren was reported to be a friend of Hermann Göring, whose first wife was a Swede, and in the late 1930s convinced himself that he could avert the coming world war by acting as a conduit between Göring and the British and American governments. His efforts proved unsuccessful, with all parties regarding him as a self-promoting nuisance without much influence on the plans of the Nazi regime. However, others are suspicious of his role in the war, citing how (his) original Bank of the Bahamas was used to fund the Nazis and his friendship with Göring as potential proof of his private support for the Nazis.\n\nA disconsolate Wenner-Gren retired to his estate in the Bahamas, in Hog Island (now Paradise Island), where he resumed his friendship with the islands' governor, the Duke of Windsor and former King of the United Kingdom Edward VIII. Early in the war his rumored friendship with Göring and the suspected German sympathies of the Duke led first the Americans and, following their lead, the British, to place him on an economic blacklist, enabling them to freeze his assets in Nassau. There proved to be little or no foundation to their suspicions that Wenner-Gren was a Nazi agent, notwithstanding the appearance of his steam yacht \"Southern Cross\" (the world's largest at the time) along with ships from the Allied Navies at the site of the sinking of the liner \"SS Athenia\" on the first day of the war. Wenner-Gren's yacht the \"Southern Cross\" rescued over three hundred survivors of the sinking and transferred some to nearby Allied ships and others continued to the U.S.\n\nIn the 1950s, Axel Wenner-Gren also got involved in the early computer business. For a railroad project connecting California with Alaska, he got in touch with Glenn Hagen, previously an engineer with Northrop Aircraft, who had founded Logistics Research in Redondo Beach outside Los Angeles, developing computers based on magnetic drum memory. In November 1952, Wenner-Gren helped the company to incorporate. He soon controlled the company and renamed it ALWAC (the Axel L. Wenner-Gren Automatic Computer). ALWAC I was used in 1953, ALWAC II and ALWAC III in 1954, ALWAC III-E in 1955. In 1956 and 1957, the model ALWAC III-E was considered a competitor to the IBM 650, having fewer parts and good economy, but no more than 30 units seem to have been delivered. Soon after this, magnetic drum machines were made obsolete by the introduction of the magnetic core memory. By 1956 the number of employees tripled to over 300 and the company was relocated to an industrial park in Hawthorne, California. The appearance of the transistor in the electronics industry in 1957 was a financial shock for all vacuum tube computer makers and by 1958 ALWAC in Hawthorn closed and its employees, with the help of Wenner-Gren himself, were successful hired by Litton Industries and Autonetics and several smaller electronics companies. The follow-up ALWAC 800 was a failed design that never went beyond prototype, using not only core memory but also magnetic logic (a combination of semiconductor diodes and magnetic cores, cf. Hewitt Crane), and presold contracts nearly ruined the company. Development was transferred to Sweden in 1958. The next model, named Wegematic 1000, a slight upgrade of the III-E, was shipped in 1960. Only a dozen were delivered and half of them were give-aways to universities, including one unit for the Weizmann Institute in Israel. In exchange, Wenner-Gren received several honorary titles.\n\nAmong Wenner-Gren's other interests were monorail train systems. His company, ALWEG, built the original Disneyland Monorail System in 1959 and the Seattle Center Monorail in 1962. Wenner-Gren continued his fascination with speculative railway projects, as he collaborated with Canadian W.A.C. Bennett to build a railway north from Prince George into the untapped Peace River, Rocky Mountain Trench and eventually Alaska. Parts of the railway were built by the Pacific Great Eastern Railway after Wenner-Gren's death, including the needless Fort Nelson branch, yet the meeting produced outcomes lasting to this day. The interest in the north spurred a spate of mega-industrial projects in the region: the Bennett Dam flooding vast valleys, gas pipelines and plants at Taylor, coal mines and pulp mills.\n\nIn late 1909, while returning from a trip to America on board a trans-Atlantic liner, he met Marguerite Gauntier Liggett who had been born on 15 October 1891 in Kansas City, Missouri. She was traveling with her sister, actress Gene Gauntier, to Europe to complete her musical training as an opera singer. After brief romance, when the ship arrived at Southampton, they travelled to London where they married on 14 December 1909 before traveling on to Berlin where she was going to complete her studies.\n\nWenner-Gren also founded and endowed The Viking Fund in 1941, an organization supporting anthropological research. In 1941, the endowment funded the Wenner-Gren Aeronautical Research Laboratory, now called the Wenner-Gren Laboratory at the University of Kentucky. The lab has since changed its focus to Biomedical Engineering. The Viking fund was later renamed the Wenner-Gren Foundation for Anthropological Research.\n\n\n(On pp. 21–22, points to Leifland 1989 as conclusive evidence that Wenner-Gren's blacklisting was a miscarriage of justice).\n"}
{"id": "20881584", "url": "https://en.wikipedia.org/wiki?curid=20881584", "title": "Beverage opener", "text": "Beverage opener\n\nA beverage opener (also known as a multi-opener) is a device used to open beverage cans, plastic bottles or glass bottles, which are the three most common beverage containers. \n\nBeverage openers vary in size but commonly include a glass bottle cap opener, a plastic bottle cap grip for greater twisting torque and a stay tab lifting mechanism for metal beverage cans. Some cans have extra conveniences built in such as magnetic backing to catch and hold metal caps or stick to a refrigerator door.\n\nBeverage openers are useful for opening every day beverage containers for those who have limited hand strength as it eliminates the need for strong twisting or pulling motions. Plastic bottles may become stuck due to a high volume of carbonation released during shipping or overtightening. Some do not have fingernails with which to properly use a stay tab and glass bottles almost always require some sort of bottle opener.\n\n"}
{"id": "20654937", "url": "https://en.wikipedia.org/wiki?curid=20654937", "title": "Bills of credit", "text": "Bills of credit\n\nBills of credit are documents similar to banknotes issued by a government that represent a government's indebtedness to the holder. They are typically designed to circulate as currency or currency substitutes. Bills of credit are mentioned in Article One, Section 10, Clause One (also known as the Contract Clause) of the United States Constitution, where their issuance by state governments is prohibited.\n\nBritish colonies in North America would issue bills of credit in order to deal with fiscal crises, although doing so without receiving them as revenue in like amounts would increase the money supply, resulting in price inflation and a drop in value relative to the pound sterling. The documents would circulate as if they were currency, and colonial governments would accept them as payment for debts like taxes. They were not always considered legal tender for private debts.\n\nColonial decisions on the issuance of bills of credit were also frequently the subject of disputes between differing factions within the colony, and with royally appointed governors. Between 1690 and 1750 the matter was regularly debated in the Province of Massachusetts Bay, where merchants and lenders stood to lose value when new bills were issued, and borrowers stood to gain, because they could repay their debts with depreciated bills. The Massachusetts bills were finally retired in 1749 when the province received a large payment in coin for its financial contributions to the 1745 Siege of Louisbourg. The Province of New Jersey issued bills of credit beginning in the 1710s, but successfully managed to avoid significant inflationary effects.\n\nArticle I, Section 10, Clause 1 prohibits the states from issuing Bills of Credit. The prohibition of states issuing Bills of Credit came in direct response to how states managed their financial policy during the era of the Articles of Confederation. While all states in theory recognized the American Continental as their official currency, in reality, nearly every state issued its own Bills of credit, which further devalued the Continental and led to its eventual collapse as a currency.\n\nThe painful experience of the runaway inflation and collapse of the Continental dollar prompted the delegates to the Constitutional Convention to include the Contract Clause into the United States Constitution, so that the individual states could not issue bills of credit or \"make any Thing but gold and silver Coin a Tender in Payment of Debts.\" This restriction of bills of credit was extended to the Federal government, as the power to \"emit bills\" from the Articles of Confederation was abolished, leaving Congress with the power \"to borrow money on credit.\"\n\nThe United States Government has, at numerous times throughout American History, issued Bills of Credit to utilize in place of paper currency. Most instances of this have occurred during wartime.\n\nDuring the American Revolutionary War the Continental Congress frequently issued bills of credit referred to as Continentals. Because of inflation they rapidly declined in value, leading to the unfavorable comparison that something was \"not worth a Continental\".\n\nIn 1862, the United States Department of the Treasury began to issue United States Notes as obligations of the United States. United States Notes are examples of Bills of Credit as they used to be inserted by the Treasury into circulation free of interest (production of these notes was halted in 1971 during termination of the Bretton Woods system, yet Congress retains the power to put more into circulation at any time, and $300 million remain in circulation still).\n\nInterest-bearing notes are a grouping of Civil War-era bills of credit-related emissions of the US Treasury. The grouping includes the one- and two-year notes authorized by the Act of March 3, 1863, which bore interest at five percent annually, were a legal tender at face value, and were issued in denominations of $10, $20, $50, $100, $500 and $1000.\n\nCompound interest treasury notes were emissions of the United States Treasury Department authorized in 1863 and 1864 with aspects of both paper money and debt. They were issued in denominations of $10, $20, $50, $100, $500 and $1000. While they were legal tender at face value, they were redeemable after three years with six percent annual interest compounded semi-annually.\n\nIn the absence of efficient investment banks, the hybrid nature of these instruments allowed the government to directly distribute debt by paying the notes out to creditors as legal tender, and then relying on interest-seeking parties to eventually remove them from circulation in order to redeem them with interest at maturity.\n\nThe Refunding Certificate was a type of interest-bearing banknote that the United States Treasury issued in 1879. They issued it only in the $10 denomination, depicting Benjamin Franklin. Their issuance reflects the end of a coin-hoarding period that began during the American Civil War, and represented a return to public confidence in paper money.\n\nFederal Reserve Bank Notes, issued between 1915 and 1934, are bills of credit that are legal tender in the United States. They had the same value as other kinds of notes of similar face value. Federal Reserve Bank Notes differ from Federal Reserve Notes in that they are backed by one of the twelve Federal Reserve Banks, rather than by all collectively. They were backed in a similar way to National Bank Notes, using U.S. bonds, but issued by Federal Reserve banks instead of by chartered National banks. Federal Reserve Bank Notes are no longer issued.\n\nLegal writers—as opposed to economic historians—incorrectly assume that the constitutional phrase \"Bills of Credit\" was simply a synonym for paper money, but it only refers to one, though very important, type of paper currency. The Constitution explicitly prohibits the states from issuing bills of credit and coining money. States are only permitted to make gold and silver coin legal tender.\n\n\n"}
{"id": "25889566", "url": "https://en.wikipedia.org/wiki?curid=25889566", "title": "Center of Financial Technologies", "text": "Center of Financial Technologies\n\nCenter of Financial Technologies (CFT) (, \"Centr Finansovyh Tekhnologiy\") is a Russian software company that provides software solutions and services to financial, healthcare and public sectors. It has been described as \"Russia’s national payment system (like PayPal for Russia)\".\n\nThe Company offers banking software and processing services:\n\nIn financial services, the company's software is used in retail banking and corporate banking to automate processes. CFT also provides solution to capital market activities. \n\n\n\nCFT Group is among the top five largest software developers on the Russian market.\nAccording to IBS Intelligence, CFT occupies the 1st place according to the number of new contracts for core banking system on the Russian and CIS markets.\nAmong CFT customers are such banks as Sberbank of Russia, Gazprombank, MDM Bank, and Uralsib.\n\nOn the 17th of November 2016 Russian investigators searched the offices of CFT in connection to the ongoing investigation of the criminal case against Group Board Member and Chairman of CFT Nikolai Smirnov and others under article 127.1 of the Criminal Code ( \"human trafficking\"). See reference 3 below for an English-language account of this case.\n\n"}
{"id": "8765284", "url": "https://en.wikipedia.org/wiki?curid=8765284", "title": "Channel memory", "text": "Channel memory\n\nAn automatic channel memory system (ACMS) is a system in which a digitally controlled radio tuner such as a TV set or VCR could search and memorize TV channels automatically. While more common in television, it can also be used to store presets for radio stations. This is often called a channel scan, though that may also refer to a \"preview\" mode which plays each station it finds for a few seconds and then moves on to the next, without affecting memory.\n\nA typical TV device allows an automatic channel scan to be performed from a menu accessed by a button on the TV set, or sometimes only on the remote control. This applied first to analog TV sets — sometimes those with digital LED displays, or later always those with on-screen displays. These simply searched for the video carrier signal on every channel. (Before the advent of ACMS, many sets would search for the next channel every time it was changed.)\n\nIt now also applies to digital TV, which must not only find the signal itself, but also decode its metadata enough to remap channel numbers to their proper locations. In the case of the American ATSC system, the ATSC tuner uses PSIP metadata to do this. The internal channel map for digital TV stations is different from the presets or \"favorites\" that the user has programmed. Just as with analog TV (which worked \"only\" by turning a preset on or off for each station/channel), users of digital television adapters and other similar tuners can choose to ignore channels that are still in the channel map. \n\nAnalog station presets and digital channel maps are normally deleted when a new scan is started. On some tuners, digital channel maps can be added-to with an \"easy-add\" channel scan, which is useful for finding new stations without losing old ones that may be weak or currently off-air, or not aimed-at with an antenna rotator or other set-top TV antenna adjustment. If a station adds a digital subchannel, most digital TV tuners will find it automatically as soon as the user turns to another channel that is carried by that station, adding it to the channel map. \n\nMany will also automatically add a new digital station's subchannels by tuning manually to the station's physical channel, though if this conflicts with the virtual channel number of another station, a complete re-scan may be the only solution. (Choosing an unused subchannel number [i.e. 30.99] on that major channel number [i.e. 30] may avoid the remap on existing subchannels [i.e. 30.1] and force the tuner to listen on that physical channel.) This has often happened in the U.S., where stations (especially LPTV) find it easiest to place their digital operations on a vacated analog channel. \n\nThe same problem also occurs when the same station moves its digital transmission back to its old analog channel. There is no way to delete a station from the internal channel map, and a re-scan may be needed; or if the new physical channel is found, it may leave the old mapping in place, causing duplicate channels that cannot be accessed through direct entry of the numbers without also pressing the channel-up/down buttons.\n\nMany modern TV sets do an ACMS scan automatically as part of the setup process the user is guided through when the device is initially plugged-in. While early sets often lost their memory in a power outage or by otherwise being disconnected from mains electricity, all of them now have non-volatile RAM or a backup of some sort.\n"}
{"id": "1021510", "url": "https://en.wikipedia.org/wiki?curid=1021510", "title": "Dilution of precision (navigation)", "text": "Dilution of precision (navigation)\n\nDilution of precision (DOP), or geometric dilution of precision (GDOP), is a term used in satellite navigation and geomatics engineering to specify the additional multiplicative effect of navigation satellite geometry on positional measurement precision.\n\nThe concept of dilution of precision (DOP) originated with users of the Loran-C navigation system. The idea of Geometric DOP is to state how errors in the measurement will affect the final state estimation. This can be defined as:\n\nformula_1\n\nConceptually you can geometrically imagine errors on a measurement resulting in the formula_2 term changing. Ideally small changes in the measured data will not result in large changes in output location, as such a result would indicate the solution is very sensitive to errors. The interpretation of this formula is shown in the figure to the right, showing two possible scenarios with acceptable and poor GDOP.\n\nMore recently, the term has come into much wider usage with the development and adoption of GPS. Neglecting ionospheric and tropospheric effects, the signal from navigation satellites has a fixed precision. Therefore, the relative satellite-receiver geometry plays a major role in determining the precision of estimated positions and times. Due to the relative geometry of any given satellite to a receiver, the precision in the pseudorange of the satellite translates to a corresponding component in each of the four dimensions of position measured by the receiver (i.e., formula_3, formula_4, formula_5, and formula_6). The precision of multiple satellites in view of a receiver combine according to the relative position of the satellites to determine the level of precision in each dimension of the receiver measurement. When visible navigation satellites are close together in the sky, the geometry is said to be weak and the DOP value is high; when far apart, the geometry is strong and the DOP value is low. Consider two overlapping rings, or annuli, of different centres. If they overlap at right angles, the greatest extent of the overlap is much smaller than if they overlap in near parallel. Thus a low DOP value represents a better positional precision due to the wider angular separation between the satellites used to calculate a unit's position. Other factors that can increase the effective DOP are obstructions such as nearby mountains or buildings.\n\nDOP can be expressed as a number of separate measurements:\nThese values follow mathematically from the positions of the usable satellites. Signal receivers allow the display of these positions (\"skyplot\") as well as the DOP values.\n\nThe term can also be applied to other location systems that employ several geographical spaced sites. It can occur in electronic-counter-counter-measures (electronic warfare) when computing the location of enemy emitters (radar jammers and radio communications devices). Using such an interferometry technique can provide certain geometric layout where there are degrees of freedom that cannot be accounted for due to inadequate configurations.\n\nThe effect of geometry of the satellites on position error is called geometric dilution of precision (GDOP) and it is roughly interpreted as ratio of position error to the range error. Imagine that a square pyramid is formed by lines joining four satellites with the receiver at the tip of the pyramid. The larger the volume of the pyramid, the better (lower) the value of GDOP; the smaller its volume, the worse (higher) the value of GDOP will be. Similarly, the greater the number of satellites, the better the value of GDOP.\n\nThe DOP factors are functions of the diagonal elements of the covariance matrix of the parameters, expressed either in a global or a local geodetic frame.\n\nAs a first step in computing DOP, consider the unit vectors from the receiver to satellite i: formula_7 where formula_8 and where formula_9 and formula_5 denote the position of the receiver and formula_11 and formula_12 denote the position of satellite i. Formulate the matrix, A, which (for 4 range measurement residual equations) is:\nThe first three elements of each row of \"A\" are the components of a unit vector from the receiver to the indicated satellite. If the elements in the fourth column are \"c\" which denotes the speed of light then the formula_14 factor (time dilution) is always 1. If the elements in the fourth column are \"-1\" then the formula_14 factor is calculated properly. Formulate the matrix, \"Q\", as:\n\nIn general: formula_17 where formula_18 is the Jacobian of the sensor measurement residual equations formula_19, with respect to the unknowns, formula_20; formula_21 is the Jacobian of the sensor measurement residual equations with respect to the measured quantities formula_22, and formula_23 is the correlation matrix for noise in the measured quantities. For the preceding case of 4 range measurement residual equations: formula_24, formula_25, formula_26, formula_27, formula_28, formula_29, formula_30, formula_31 and the measurement noises for the different formula_32 have been assumed to be independent which makes formula_33. This formula for Q arises from applying Best Linear Unbiased Estimation to a linearized version of the sensor measurement residual equations about the current solution formula_34, except in the case of B.L.U.E. formula_23 is a noise covariance matrix rather than the noise correlation matrix used in DOP, and the reason DOP makes this substitution is to obtain a relative error. When formula_23 is a noise covariance matrix, formula_37 is an estimate of the matrix of covariance of noise in the unknowns due to the noise in the measured quantities. It is the estimate obtained by the First Order second Moment (F.OR.M.) uncertainty quantification technique which was state of the art in the 1980s. In order for the F.OR.M. theory to be strictly applicable, either the input noise distributions need to be Gaussian or the measurement noise standard deviations need to be small relative to rate of change in the output near the solution. In this context, the second criteria is typically the one that is satisfied.\n\nThis (i.e. for the 4 range measurement residual equations) computation is in accordance with where the weighting matrix, formula_38, has been set to the identity matrix.\n\nThe elements of \"Q\" are designated as:\nPDOP, TDOP and GDOP are given by:\n\nin agreement with Section 1.4.9 of \"Principles of Satellite Positioning\". More generally, GDOP is the square root of the trace of the formula_37 matrix.\n\nThe horizontal dilution of precision, formula_42, and the vertical dilution of precision, formula_43, are both dependent on the coordinate system used. To correspond to the local horizon plane and the local vertical, \"x\", \"y\", and \"z\" should denote positions in either a north, east, down coordinate system or an east, north, up coordinate system.\n\n"}
{"id": "7921791", "url": "https://en.wikipedia.org/wiki?curid=7921791", "title": "Directional boring", "text": "Directional boring\n\nDirectional Boring, often undifferentiated from Horizontal Directional Drilling (HDD), is a minimal impact trenchless method of installing underground pipe, conduit, or cables in a relatively shallow arc or radius along a prescribed underground bore path by using a surface-launched drilling rig. With respect to the pipeline/utility industry, the term \"Directional Boring\" or \"Bore\" is generally reserved for mini/small sized drilling rigs, small diameter bores, and crossing lengths in terms of hundreds of feet. Generally, the term Horizontal Directional Drilling (HDD) is intended to describe large/maxi sized drilling rigs, large diameter bores, and crossing lengths in terms of thousand of feet. Directional Boring and HDD, while similar in process to each other, are markedly different from directional drilling associated with the oil industry. The methods offer significant advantages over traditional cut and cover pipeline/utility installations and are routinely used when trenching or excavating is not practical. Boring/HDD can be utilized with various pipe materials such as PVC, polyethylene, polypropylene, ductile iron, and steel as long as the pipe is sized appropriately to withstand installation stresses imparted during pullback. \n\nDirectional Boring/HDD is generally accomplished in three principle phases. First, a small diameter pilot hole is drilled along a directional path from one surface point to another. Next, the bore created during pilot hole drilling is enlarged to a diameter that will facilitate installation of a pipeline. Lastly, the pipeline is pulled into the enlarge hole, thus creating a continuous segment of pipe underground and exposed only at the two initial end points.\n\nDirectional Boring/HDD is suitable for a variety of soil conditions and obstacles including road, landscape, wetland/marshes, and river crossings. Problematic soil conditions most often encountered in evaluating the feasibility of an HDD installation is large grain content in the form of gravel, cobbles, and boulders. Other subsurface conditions which can impact the feasibility of an HDD installation include excessive rock strength and abrasivity, poor rock quality, and solution cavities in karst formations.\n\nThe equipment used in horizontal directional drilling depends on the outer diameter of the pipe, length of the run, ground conditions and the surroundings above ground. \n\nFor larger bores, directional drills equipped with as much as 600 tonnes (or more) of thrust/pullback (Vermeer D1320x900) is used in conjunction with a mud reclaimer, excavator, and multiple pumps and hoses to supply the drilling fluid to the drillstem. Directional drilling stems are made from heat-treated high-carbon steel in diameters of Drill stem sections are manufactured in 3.0, 4.6, and 9.1 meter lengths with male and female threading on opposite ends. It is common for a directional drill to carry as much as of drilling stems. Drilling heads are available for different types of rock or soil. The drilling head has multiple water ports to allow removal of material. A talon bit head uses carbide-tipped blades to cut through material while steered by the operator. Another type of head is a mud-motor that is used in rocky landscapes. Supporting equipment is often required, and may include a drilling fluid (mud) recycling system, shale shaker, mud cleaner, centrifugal pump, and mud tanks.\n\nFor smaller bores, portable equipment is under development for drilling from a house basement to a shared water pipe.\n\nDirectional boring is used for installing infrastructure such as telecommunications and power cable conduits, water lines, sewer lines, gas lines, oil lines, product pipelines, and environmental remediation casings. It is used for crossing waterways, roadways, shore approaches, congested areas, environmentally sensitive areas, and areas where other methods are costlier or not possible. It is used instead of other techniques to provide less traffic disruption, lower cost, deeper and/or longer installation, no access pit, shorter completion times, directional capabilities, and environmental safety.\n\nThe technique has extensive use in urban areas for developing subsurface utilities as it helps in avoiding extensive open cut trenches. The use requires that the operator have complete information about existing utilities so that they can plan the alignment to avoid damaging those utilities. Since uncontrolled drilling can lead to damage, different agencies/government authorities owning the urban \"right-of-way\" or the utilities have rules for safe work execution. For standardization of the techniques, different trenchless technology promoting organizations have developed guidelines for this technique.\n\nThe process starts with the receiving hole and entrance pits. These pits will allow the drilling fluid to be collected and reclaimed to reduce costs and prevent waste. The first stage drills a pilot hole on the designed path, and the second stage (reaming) enlarges the hole by passing a larger cutting tool known as the back reamer. The reamer's diameter depends on the size of the pipe to be pulled back through the bore hole. The driller increases the diameter according to the outer diameter or the conduit and to achieve optimal production. The third stage places the product or casing pipe in the enlarged hole by way of the drill stem; it is pulled behind the reamer to allow centering of the pipe in the newly reamed path.\n\nHorizontal directional drilling is done with the help of a viscous fluid known as drilling fluid. It is a mixture of water and, usually, bentonite or polymer continuously pumped to the cutting head or drill bit to facilitate the removal of cuttings, stabilize the bore hole, cool the cutting head, and lubricate the passage of the product pipe. The drilling fluid is sent into a machine called a reclaimer which removes the drill cuttings and maintains the proper viscosity of the fluid. Drilling fluid holds the cuttings in suspension to prevent them from clogging the bore. A clogged bore creates back pressure on the cutting head, slowing production.\n\nLocation and guidance of the drilling head is an important part of the drilling operation, as the drilling head is under the ground while drilling and, in most cases, not visible from the ground surface. Uncontrolled or unguided drilling can lead to substantial destruction, which can be eliminated by properly locating and guiding the drill head.\n\nThere are three types of locating equipment for locating the bore head: the \"walk-over\" locating system, the \"wire-line\" locating system and the gyro guided drilling, where a full inertial navigation system is located close to the drill head.\n\n\nAll three systems have their own merits, and a particular system is chosen depending upon the site requirements.\n\n\n\n"}
{"id": "15637826", "url": "https://en.wikipedia.org/wiki?curid=15637826", "title": "Dolphin Computer Access", "text": "Dolphin Computer Access\n\nDolphin Computer Access is a British company based in Worcester that designs, creates and sells software for people who are blind or have vision and print impairments, dyslexia and other specific learning difficulties. The company was set up in 1986 and now has offices in the United Kingdom, United States, Sweden and Norway. Through the use of Dolphin's screen enlargers, screen readers and braille output, users can operate word processors, spreadsheets, databases and the internet. The company's customers include Microsoft, the Inland Revenue, the BBC, the Royal Air Force, New College Worcester and Vodafone.\n\nDolphin's product SuperNova has won the UK WOW! award for technology in education.\n\nIn 2005 Dolphin worked alongside BT and the National Library for the Blind to develop a prototype synthetic voice application enabling books, magazines and newspapers to be converted into audio format, thus enabling them to be read by visually impaired computer users.\n\nIn 2008 the company announced a merger with Durham based Software Express Distribution Limited, a software development company and producer of Guide, a software package which enables visually impaired users to access Windows.\n"}
{"id": "39336973", "url": "https://en.wikipedia.org/wiki?curid=39336973", "title": "Dualin", "text": "Dualin\n\nDualin is an explosive material based on nitroglycerin and nitrogenized cellulose using sawdust or wood pulp. It is inferior to dynamite and more liable to explosion. Dualin was invented by the Prussian chemist Lieutenant Dittmar in April 1869.\n"}
{"id": "7135316", "url": "https://en.wikipedia.org/wiki?curid=7135316", "title": "Earthquake bomb", "text": "Earthquake bomb\n\nThe earthquake bomb, or seismic bomb, was a concept that was invented by the British aeronautical engineer Barnes Wallis early in World War II and subsequently developed and used during the war against strategic targets in Europe. A seismic bomb differs somewhat in concept from traditional bombs, which usually explode at or near the surface, and destroy their target directly by explosive force. In contrast, a seismic bomb is dropped from high altitude to attain very high speed as it falls and upon impact, penetrates and explodes deep underground, causing massive caverns or craters known as \"camouflets\", as well as intense shockwaves. In this way, the seismic bomb can affect targets that are too massive to be affected by a conventional bomb, as well as damage or destroy difficult targets such as bridges and viaducts.\n\nEarthquake bombs were used towards the end of World War II on massively reinforced installations, such as submarine pens with concrete walls several metres thick, underground caverns, buried tunnels, and bridges.\n\nDuring development Barnes Wallis theorised that a highly aerodynamic, very heavy bomb with a delayed detonation would cause damage to a target through shock waves travelling through the ground, hence the nickname earthquake bombs.\n\nThe airmen who dropped the bombs reported that the target structures stood undamaged by the detonation; \"But then the crater collapsed, the ground shifted and the target collapsed\". Later computer simulations reached the same conclusions; the significant part of the damage was done by generating a cavity in the ground, and that cavity collapsing in on itself causing the ground to shift and the target's foundation to shift or break causing catastrophic structural damage to the target supported by said foundation. The shifting ground caused any larger structure to become severely damaged, even if the bomb missed the target but created a crater near it.\n\nThey were not true seismic weapons, but effective cratering weapons.\n\nAn explosion in air does not transfer much energy into a solid, as their differing acoustic impedances makes an impedance mismatch that reflects most of the energy. Due to the lack of accuracy of bombing in the face of anti-aircraft defences, air forces used area bombardment, dropping large numbers of bombs so that it would be likely that the target would be hit. Although a direct hit from a light bomb would destroy an unprotected target, it was comparatively easy to armour ground targets with many yards of concrete, and thus render critical installations such as bunkers essentially bombproof. If the bomb could be designed to explode in water, soil, or other less compressible materials, the explosive force would be transmitted more efficiently to the target.\n\nBarnes Wallis' idea was to drop a large, heavy bomb with a hard armoured tip at supersonic speed (as fast as an artillery shell) so that it penetrated the ground like a ten-ton bullet being fired straight down. It was then set to explode underground, ideally to the side of, or underneath, a hardened target. The resulting shock wave would produce the equivalent of a 3.6 magnitude earthquake, destroying any nearby structures such as dams, railways, viaducts, etc. Any concrete reinforcement of the target would probably serve to enclose the force better.\n\nWallis also argued that, if the bomb penetrated deep enough, the explosion would not breach the surface of the ground and would thus produce an underground cavern (a \"camouflet\") which would remove the structure's underground support, thus causing it to collapse. The process was graphically described as a \"trapdoor effect\" or \"hangman's drop\".\n\nWallis foresaw that disrupting German industry would remove its ability to fight, and also understood that precision bombing was virtually impossible in the late 1930s. The technology for precision aiming was developed during World War II, and Barnes Wallis' ideas were then shown to be superbly successful (see for example the Bielefeld raid 14 March 1945), considering the standards at the time.\n\nWallis' first concept was for a ten-ton bomb that would explode some underground. To achieve this, the bomb would have had to be dropped from . The RAF had no aircraft at the time capable of carrying a ten-ton bomb load aloft, let alone lifting it to such a height. Wallis designed a six-engine aeroplane for the task, called the \"Victory Bomber\", but he was not taken seriously by the military hierarchy of the day.\n\nWallis then took a different line in developing a means to destroy Germany's industrial structure with attacks on its supply of hydroelectric power. After he had developed the bouncing bomb and shown its possibilities, however, RAF Bomber Command were prepared to listen to his other ideas, even though they often thought them strange. The officer classes of the RAF at that time were often trained not in science or engineering, but in the classics, Roman and Greek history and language. They provided enough support to let him continue his research.\n\nLater in the war, Barnes Wallis made bombs based on the \"earthquake bomb concept\", such as the 6-ton Tallboy and then the 10-ton Grand Slam, although these were never dropped from more than about . Even from this relatively low altitude, the earthquake bomb had the ability to disrupt German industry while causing minimum civilian casualties. It was used to disable the V2 factory, bury the V3 guns, sink the battleship \"Tirpitz\" and damage the U-boats' protective pens at St. Nazaire, as well as to attack many other targets which had been impossible to damage before. One of the most spectacular attacks was shortly after D-Day, when the Tallboy was used to prevent German tank reinforcements from moving by train. Rather than blow up the tracks — which would have been repaired in a day or so — the bombs were targeted on a tunnel near Saumur which carried the line under a mountain. Twenty-five Lancasters dropped the first Tallboys on the mountain, penetrating straight through the rock, and one of them exploded in the tunnel below. As a result, the entire rail line remained unusable until the end of the war.\n\nAfter World War II, the United States developed the T12 demolition bomb, which was designed to create an earthquake effect. Given the availability of nuclear weapons with surface detonating laydown delivery, however, there was little or no development of conventional deep penetrating bombs until the 1991 Gulf War. During the Gulf War, the need for a conventional deep penetrator became clear. In three weeks, a cooperative effort directed by the Armament Systems Division at Eglin Air Force Base in Florida developed the GBU-28 that was used successfully by F-111Fs against a deep underground complex not far from Baghdad just before the end of the war.\n\nThe United States has developed a Massive Ordnance Penetrator, designed to attack very deeply buried targets without the use of nuclear weapons with the inherent huge levels of radioactive pollution and their attendant risk of retaliation in kind.\n\nAnglo-American bomb tests (Project Ruby) on the comparative effectiveness of large bombs against reinforced concrete structures were carried out in 1946.\n\n"}
{"id": "56432443", "url": "https://en.wikipedia.org/wiki?curid=56432443", "title": "EasyGo", "text": "EasyGo\n\nEasyGo is a joint venture between Norway, Sweden, Denmark and Austria, that enables use of a single electronic toll tag on toll roads, ferries and bridges in all the member countries. The purpose of EasyGo is to enable the use of one OBE for payment when driving through any toll facility one might encounter on the way through Northern Europe and Austria.\n\nEasyGo is based on DSRC 5.8 GHz microwave technology and there are major differences between the operators. The toll stations have different design and there is no common EasyGo signage, although there are some common features.\nEasyGo was Europe's first commercial cross-border toll collection service. Initial discussions began in 2004 when the Svinesund Bridge between Norway and Sweden was being built. The Norwegian Public Roads Administration and the Swedish Transport Agency together with Sund & Bælt (operator of the Great Belt Fixed Link) and Øresundsbro Konsortiet (Danish/Swedish joint venture, operator of the Øresund Bridge) established EasyGo in 2007.\n\nAustrian ASFiNAG joined the partnership in 2009.\n\nAll existing systems implemented in the Nordic countries by 2007 (AutoPASS and BroBizz) are included, and no revision of the laws in the countries was required. EasyGo countries have four different currencies and variable VAT levels.\n\nThe EasyGo Basic service is for vehicles only travelling in Scandinavia or has a maximum allowable weight of 3.5 tons.\n\nEasyGo+ is a cross-border toll collection service allowing drivers of vehicles over 3.5 tons to pay tolls in Austria, Denmark, Sweden and Norway, using only one OBE in all four countries.\n\nThere are several Service providers that provide the EasyGo services. However, some of Service Providers only supply OBEs for one of the services.\n\n"}
{"id": "9225", "url": "https://en.wikipedia.org/wiki?curid=9225", "title": "Electronic paper", "text": "Electronic paper\n\nElectronic paper and e-paper, also sometimes electronic ink or e-ink, are display devices that mimic the appearance of ordinary ink on paper. Unlike conventional backlit flat panel displays that emit light, electronic paper displays reflect light like paper. This may make them more comfortable to read, and provide a wider viewing angle than most light-emitting displays. The contrast ratio in electronic displays available as of 2008 approaches newspaper, and newly (2008) developed displays are slightly better. An ideal e-paper display can be read in direct sunlight without the image appearing to fade.\n\nMany electronic paper technologies hold static text and images indefinitely without electricity. Flexible electronic paper uses plastic substrates and plastic electronics for the display backplane. There is ongoing competition among manufacturers to provide full-color ability.\n\nApplications of electronic visual displays include electronic pricing labels in retail shops and digital signage, time tables at bus stations, electronic billboards, smartphone displays, and e-readers able to display digital versions of books and magazines.\n\nElectronic paper was first developed in the 1970s by Nick Sheridon at Xerox's Palo Alto Research Center. The first electronic paper, called Gyricon, consisted of polyethylene spheres between 75 and 106 micrometers across. Each sphere is a janus particle composed of negatively charged black plastic on one side and positively charged white plastic on the other (each bead is thus a dipole). The spheres are embedded in a transparent silicone sheet, with each sphere suspended in a bubble of oil so that they can rotate freely. The polarity of the voltage applied to each pair of electrodes then determines whether the white or black side is face-up, thus giving the pixel a white or black appearance.\nAt the FPD 2008 exhibition, Japanese company Soken demonstrated a wall with electronic wall-paper using this technology. In 2007, the Estonian company Visitret Displays was developing this kind of display using polyvinylidene fluoride (PVDF) as the material for the spheres, dramatically improving the video speed and decreasing the control voltage.\n\nIn the simplest implementation of an electrophoretic display, titanium dioxide (titania) particles approximately one micrometer in diameter are dispersed in a hydrocarbon oil. A dark-colored dye is also added to the oil, along with surfactants and charging agents that cause the particles to take on an electric charge. This mixture is placed between two parallel, conductive plates separated by a gap of 10 to 100 micrometres. When a voltage is applied across the two plates, the particles migrate electrophoretically to the plate that bears the opposite charge from that on the particles. When the particles are located at the front (viewing) side of the display, it appears white, because light is scattered back to the viewer by the high-index titania particles. When the particles are located at the rear side of the display, it appears dark, because the incident light is absorbed by the colored dye. If the rear electrode is divided into a number of small picture elements (pixels), then an image can be formed by applying the appropriate voltage to each region of the display to create a pattern of reflecting and absorbing regions.\n\nElectrophoretic displays are considered prime examples of the electronic paper category, because of their paper-like appearance and low power consumption.\n\nExamples of commercial electrophoretic displays include the high-resolution active matrix displays used in the Amazon Kindle, Barnes & Noble Nook, Sony Librie, Sony Reader, Kobo eReader and iRex iLiad e-readers. These displays are constructed from an electrophoretic imaging film manufactured by E Ink Corporation. A mobile phone that used the technology is the Motorola Fone.\n\nElectrophoretic Display technology has also been developed by Sipix and Bridgestone/Delta.\nSiPix is now part of E Ink. The Sipix design uses a flexible 0.15 mm Microcup architecture, instead of E Ink's 0.04 mm diameter microcapsules.\nBridgestone Corp.'s Advanced Materials Division cooperated with Delta Optoelectronics Inc. in developing the Quick Response Liquid Powder Display (QR-LPD) technology.\n\nElectrophoretic displays can be manufactured using the Electronics on Plastic by Laser Release (EPLaR) process developed by Philips Research to enable existing AM-LCD manufacturing plants to create flexible plastic displays.\n\nAn electrophoretic display forms images by rearranging charged pigment particles with an applied electric field.\nIn the 1990s another type of electronic ink based on a microencapsulated electrophoretic display was conceived and prototyped by a team of undergraduates at MIT as described in their Nature paper. J.D. Albert, Barrett Comiskey, Joseph Jacobson, Jeremy Rubin and Russ Wilcox co-founded E Ink Corporation in 1997 to commercialize the technology. E ink subsequently formed a partnership with Philips Components two years later to develop and market the technology. In 2005, Philips sold the electronic paper business as well as its related patents to Prime View International. \"It has for many years been an ambition of researchers in display media to create a flexible low-cost system that is the electronic analogue of paper. In this context, microparticle-based displays have long intrigued researchers. Switchable contrast in such displays is achieved by the electromigration of highly scattering or absorbing microparticles (in the size range 0.1–5 μm), quite distinct from the molecular-scale properties that govern the behaviour of the more familiar liquid-crystal displays. Micro-particle-based displays possess intrinsic bistability, exhibit extremely low power d.c. field addressing and have demonstrated high contrast and reflectivity. These features, combined with a near-lambertian viewing characteristic, result in an 'ink on paper' look. But such displays have to date suffered from short lifetimes and difficulty in manufacture. Here we report the synthesis of an electrophoretic ink based on the microencapsulation of an electrophoretic dispersion. The use of a microencapsulated electrophoretic medium solves the lifetime issues and permits the fabrication of a bistable electronic display solely by means of printing. This system may satisfy the practical requirements of electronic paper.\"\n\nThis used tiny microcapsules filled with electrically charged white particles suspended in a colored oil. In early versions, the underlying circuitry controlled whether the white particles were at the top of the capsule (so it looked white to the viewer) or at the bottom of the capsule (so the viewer saw the color of the oil). This was essentially a reintroduction of the well-known electrophoretic display technology, but microcapsules meant the display could be made on flexible plastic sheets instead of glass.\nOne early version of electronic paper consists of a sheet of very small transparent capsules, each about 40 micrometers across. Each capsule contains an oily solution containing black dye (the electronic ink), with numerous white titanium dioxide particles suspended within. The particles are slightly negatively charged, and each one is naturally white.\nThe screen holds microcapsules in a layer of liquid polymer, sandwiched between two arrays of electrodes, the upper of which is transparent. The two arrays are aligned to divide the sheet into pixels, and each pixel corresponds to a pair of electrodes situated on either side of the sheet. The sheet is laminated with transparent plastic for protection, resulting in an overall thickness of 80 micrometers, or twice that of ordinary paper.\nThe network of electrodes connects to display circuitry, which turns the electronic ink 'on' and 'off' at specific pixels by applying a voltage to specific electrode pairs. A negative charge to the surface electrode repels the particles to the bottom of local capsules, forcing the black dye to the surface and turning the pixel black. Reversing the voltage has the opposite effect. It forces the particles to the surface, turning the pixel white. A more recent implementation of this concept requires only one layer of electrodes beneath the microcapsules.\n\nThese are commercially referred to as: Active Matrix Electrophoretic Displays (AMEPD).\n\nElectro-wetting display (EWD) is based on controlling the shape of a confined water/oil interface by an applied voltage. With no voltage applied, the (colored) oil forms a flat film between the water and a hydrophobic (water-repellent) insulating coating of an electrode, resulting in a colored pixel. When a voltage is applied between the electrode and the water, the interfacial tension between the water and the coating changes. As a result, the stacked state is no longer stable, causing the water to move the oil aside. This makes a partly transparent pixel, or, if a reflective white surface is under the switchable element, a white pixel. Because of the small pixel size, the user only experiences the average reflection, which provides a high-brightness, high-contrast switchable element.\n\nDisplays based on electro-wetting provide several attractive features. The switching between white and colored reflection is fast enough to display video content. It's a low-power and low-voltage technology, and displays based on the effect can be made flat and thin. The reflectivity and contrast are better than or equal to other reflective display types and approach the visual qualities of paper. In addition, the technology offers a unique path toward high-brightness full-color displays, leading to displays that are four times brighter than reflective LCDs and twice as bright as other emerging technologies. Instead of using red, green and blue (RGB) filters or alternating segments of the three primary colors, which effectively result in only one third of the display reflecting light in the desired color, electro-wetting allows for a system in which one sub-pixel can switch two different colors independently.\n\nThis results in the availability of two thirds of the display area to reflect light in any desired color. This is achieved by building up a pixel with a stack of two independently controllable colored oil films plus a color filter.\n\nThe colors are cyan, magenta and yellow, which is a subtractive system, comparable to the principle used in inkjet printing for example. Compared to LCD, brightness is gained because no polarisers are required.\n\nExamples of commercial electrowetting displays include Liquavista, ITRI, and ADT.\n\nElectrofluidic displays are a variation of an electrowetting display. Electrofluidic displays place an aqueous pigment dispersion inside a tiny reservoir. The reservoir comprises <5-10% of the viewable pixel area and therefore the pigment is substantially hidden from view. Voltage is used to electromechanically pull the pigment out of the reservoir and spread it as a film directly behind the viewing substrate. As a result, the display takes on color and brightness similar to that of conventional pigments printed on paper. When voltage is removed liquid surface tension causes the pigment dispersion to rapidly recoil into the reservoir. As reported in the May 2009 Issue of Nature Photonics, the technology can potentially provide >85 % white state reflectance for electronic paper.\n\nThe core technology was invented at the Novel Devices Laboratory at the University of Cincinnati. The technology is currently being commercialized by Gamma Dynamics.\n\nTechnology used in electronic visual displays that can create various colors via interference of reflected light. The color is selected with an electrically switched light modulator comprising a microscopic cavity that is switched on and off using driver integrated circuits similar to those used to address liquid crystal displays (LCD).\n\nPlasmonic nanostructures with conductive polymers have also been suggested as one kind of electronic paper. The material has two parts. The first part is a highly reflective metasurface made by metal-insulator-metal films tens of nanometers in thickness including nanoscale holes. The metasurfaces can reflect different colors depending on the thickness of the insulator. The three primary colors red, green and blue can be used as pixels for full color displays. The second part is a polymer with optical absorption controllable by an electrochemical potential. After growing the polymer on the plasmonic metasurfaces, the reflection of the metasurfaces can be modulated by the applied voltage. This technology presents broad range colors, high polarization-independent reflection (>50 %), strong contrast (>30 %), fast response time (hundreds of ms), and long-term stability. In addition, it has ultralow power consumption (< 0.5 mW/cm2) and potential for high resolution (>10000 dpi). Since the ultrathin metasurfaces are flexible and the polymer is soft, the whole system can be bent.Desired future improvements for this technology include bistability, cheaper materials and implementation with TFT arrays.\n\n\nOther research efforts into e-paper have involved using organic transistors embedded into flexible substrates, including attempts to build them into conventional paper.\nSimple color e-paper consists of a thin colored optical filter added to the monochrome technology described above. The array of pixels is divided into triads, typically consisting of the standard cyan, magenta and yellow, in the same way as CRT monitors (although using subtractive primary colors as opposed to additive primary colors). The display is then controlled like any other electronic color display.\n\nExamples of electrochromic displays include rdot Displays, Acreo, Ajjer, Aveso and Ntera.\n\nElectronic paper technologies have a very low refresh rate compared to other low-power display technologies, such as LCDs. This prevents producers from implementing sophisticated interactive applications (using fast-moving menus, mouse pointers or scrolling) like those common on standard mobile devices. An example of this limit is that a document cannot be smoothly zoomed without either extreme blurring during the transition or a very slow zoom.\n\nAnother limit is that a shadow of an image may be visible after refreshing parts of the screen. Such shadows are termed \"ghost images\", and the effect is termed \"ghosting\". This effect is reminiscent of screen burn-in but, unlike screen burn-in, is solved after the screen is refreshed several times. Turning every pixel white, then black, then white, helps normalize the contrast of the pixels. This is why several devices with this technology \"flash\" the entire screen white and black when loading a new image.\n\nE Ink Corporation of E Ink Holdings Inc. released the first colored e-ink displays to be used in a marketed product. The Ectaco Jetbook Color was released in 2012 as the first colored e-ink e-reader, which used E Ink's Triton display technology. E Ink in early 2015 also announced another color e-ink technology called Prism. This new technology is a color changing film that can be used for e-reader, but Prism is also marketed as a film that can be integrated into architectural design such as \"wall, ceiling panel, or entire room instantly.\" The disadvantage of these current color displays is that they are considerably more expensive than standard E Ink displays. The JetBook Color costs roughly nine times more than other popular e-readers such as the Amazon Kindle. As of January 2015, Prism had not been announced to be used in the plans for any e-reader devices.\n\nSeveral companies are simultaneously developing electronic paper and ink. While the technologies used by each company provide many of the same features, each has its own distinct technological advantages. All electronic paper technologies face the following general challenges:\n\n\nElectronic ink can be applied to flexible or rigid materials. For flexible displays, the base requires a thin, flexible material tough enough to withstand considerable wear, such as extremely thin plastic. The method of how the inks are encapsulated and then applied to the substrate is what distinguishes each company from others. These processes are complex and are carefully guarded industry secrets. Nevertheless, making electronic paper is less complex and costly than LCDs.\n\nThere are many approaches to electronic paper, with many companies developing technology in this area. Other technologies being applied to electronic paper include modifications of liquid crystal displays, electrochromic displays, and the electronic equivalent of an Etch A Sketch at Kyushu University. Advantages of electronic paper includes low power usage (power is only drawn when the display is updated), flexibility and better readability than most displays. Electronic ink can be printed on any surface, including walls, billboards, product labels and T-shirts. The ink's flexibility would also make it possible to develop rollable displays for electronic devices.\n\nIn December 2005 Seiko released the first electronic ink based watch called the Spectrum SVRD001 wristwatch, which has a flexible electrophoretic display and in March 2010 Seiko released a second generation of this famous e-ink watch with an active matrix display. The Pebble smart watch (2013) uses a low-power memory LCD manufactured by Sharp for its e-paper display.\n\nIn 2004 Sony released the Librié in Japan, the first e-book reader with an electronic paper E Ink display. In September 2006, Sony released the PRS-500 Sony Reader e-book reader in the USA. On October 2, 2007, Sony announced the PRS-505, an updated version of the Reader. In November 2008, Sony released the PRS-700BC, which incorporated a backlight and a touchscreen.\n\nIn late 2007, Amazon began producing and marketing the Amazon Kindle, an e-book reader with an e-paper display. In February 2009, Amazon released the Kindle 2 and in May 2009 the larger Kindle DX was announced. In July 2010 the third generation Kindle was announced, with notable design changes. The fourth generation of Kindle, called Touch, was announced in September 2011 that was the Kindle's first departure from keyboards and page turn buttons in favor of touchscreens. In September 2012, Amazon announced the fifth generation of the Kindle called the Paperwhite, which incorporates a LED frontlight and a higher contrast display.\n\nIn November 2009, Barnes and Noble launched the Barnes & Noble Nook, running an Android operating system. It differs from other e-readers in having a replaceable battery, and a separate touch-screen color LCD below the main electronic paper reading screen.\n\nIn 2017 Sony and reMarkable offered e-books tailored for writing with a smart stylus.\n\nIn February 2006, the Flemish daily \"De Tijd\" distributed an electronic version of the paper to select subscribers in a limited marketing study, using a pre-release version of the iRex iLiad. This was the first recorded application of electronic ink to newspaper publishing.\n\nThe French daily \"Les Échos\" announced the official launch of an electronic version of the paper on a subscription basis, in September 2007. Two offers were available, combining a one-year subscription and a reading device. The offer included either a light (176g) reading device (adapted for Les Echos by Ganaxa) or the iRex iLiad. Two different processing platforms were used to deliver readable information of the daily, one based on the newly developed GPP electronic ink platform from \"Ganaxa\", and the other one developed internally by Les Echos.\n\nFlexible display cards enable financial payment cardholders to generate a one-time password to reduce online banking and transaction fraud. Electronic paper offers a flat and thin alternative to existing key fob tokens for data security. The world's first ISO compliant smart card with an embedded display was developed by Innovative Card Technologies and nCryptone in 2005. The cards were manufactured by Nagra ID.\n\nSome devices, like USB flash drives, have used electronic paper to display status information, such as available storage space. Once the image on the electronic paper has been set, it requires no power to maintain, so the readout can be seen even when the flash drive is not plugged in.\n\nMotorola's low-cost mobile phone, the Motorola F3, uses an alphanumeric black-and-white electrophoretic display.\n\nThe Samsung Alias 2 mobile phone incorporates electronic ink from E Ink into the keypad, which allows the keypad to change character sets and orientation while in different display modes.\n\nOn December 12, 2012, Yota Devices announced the first \"YotaPhone\" prototype and was later released in December 2013, a unique double-display smartphone. It has a 4.3-inch, HD LCD on the front and an e-ink display on the back with smart battery usage.\nYota has updated its range with the Yotaphone 2 and the YOTA 3.\n\nIn December 2015, CRBT Siam launched the Siam 7X, a dual-screen Android phone.\nThe Siam Duo is an updated model.\n\nE-Paper based electronic shelf labels (ESL) are used to digitally display the prices of goods at retail stores. Electronic paper based labels are updated via two-way infrared or radio technology.\n\nBecause of its energy-saving properties, electronic paper has proved a technology suited to digital signage applications.\n\nOther proposed applications include clothes, digital photo frames, information boards and keyboards. Keyboards with dynamically changeable keys are useful for less represented languages, non-standard keyboard layouts such as Dvorak, or for special non-alphabetical applications such as video editing or games.\nThe reMarkable is a writer tablet for reading and note taking.\n\n\n\n\n"}
{"id": "45700602", "url": "https://en.wikipedia.org/wiki?curid=45700602", "title": "Energy and environmental engineering", "text": "Energy and environmental engineering\n\nEnergy and environmental engineering is a branch of engineering which seeks to efficiently use energy and to maintain the environment. Energy engineers require knowledge across many disciplines. Careers include work in the built environment, renewable and traditional energy industries.\n\nIn this area, solar radiation is important and must be understood. Solar radiation affects the Earth's weather and daylight available. This affects not only the Earth's environment but also the smaller internal environments which we create.\n\nEnergy engineering requires at least an understanding of mechanics, thermodynamics, mathematics, materials, stoichiometry, electrical machines, manufacturing processes and energy systems.\n\nEnvironmental engineering can be branched into two main areas: internal environments and outdoor environments.\n\nInternal environments may consist of housing or offices or other commercial properties. In this area, the environmental engineering sometimes stands for the designing of building services to condition the internal environment to a comfortable state or the removal of excess pollutants such as carbon dioxide or other harmful substances.\n\nExternal environments may be water courses, air, land or seas, and may require new strategies for harnessing energy or the creation of treatment facilities for polluting technologies.\n\nThis broad degree area covers many areas but is mainly mechanically and electrically biased. It seeks to explore cleaner, more efficient ways of using fossil fuels, while investigating and developing systems using renewable and sustainable resources, such as solar, wind and wave energy.\n"}
{"id": "56510441", "url": "https://en.wikipedia.org/wiki?curid=56510441", "title": "Entuity Network Management", "text": "Entuity Network Management\n\nEntuity is a data analytics platform for enterprise networks. It was founded in London in 1997, and is currently based in Boston. It was listed as one of the most promising networking solution providers in 2017 by CIOReview.\n"}
{"id": "7340495", "url": "https://en.wikipedia.org/wiki?curid=7340495", "title": "Eye shadow", "text": "Eye shadow\n\nEye shadow is a cosmetic that is applied on the eyelids and under the eyebrows. It is commonly used to make the wearer's eyes stand out or look more attractive. \n\nEye shadow can add depth and dimension to one's eyes, complement the eye color, make one's eyes appear larger, or simply draw attention to the eyes. Eye shadow comes in many different colors and textures. It is usually made from a powder and mica, but can also be found in liquid, pencil, cream or mousse form.\nCivilizations across the world use eye shadow predominantly on females but also occasionally on males. In Western society, it is seen as a feminine cosmetic, even when used by men. In Gothic fashion, black or similarly dark-colored eye shadow and other types of eye makeup are popular among both sexes.\n\nMany people use eye shadow simply to improve their appearance, but it is also commonly used in theatre and other plays, to create a memorable look, with bright, bold colors. Depending on skin tone and experience, the effect of eye shadow usually brings out glamour and gains attention. The use of eye shadow attempts to replicate the natural eye shadow that some women exhibit due to a natural contrasting pigmentation on their eyelids. Natural eye shadow can range anywhere from a glossy shine to one's eyelids, to a pinkish tone, or even a silver look.\n\nEye shadow can be applied in a wide variety of ways depending upon the desired look and formulation. Typically application is done using fingers or brushes. The most important aspect of applying eye shadow, and makeup in general, is blending well. However, you must not forget to include a primer to limit the chances of creases in your eye shadow later.\nTo remove eye shadow, a commercial eye makeup remover can be utilized, though a rich face wash will usually remove all traces of color. Generally it is easy to remove, and simple water and soap can be used. Eye shadow, eyeliner, and mascara may also be removed using baby oil. There are also makeup wipes that can be used.\n\nCosmetics have been used for as long as there have been people to use them. Face painting is mentioned in the Old Testament (Book of Ezekiel 23:40), and eye shadow was used in Egyptian burials dating back to 10,000 BC. The word \"cosmetae\" was first used to describe Roman slaves whose duty was to bathe men and women in perfume.\n\nAs early as 10,000 BC, men and women used scented oils and ointments to clean and soften their skin and mask body odor. Dyes and paints were used to color the skin, body and hair. They rouged their lips and cheeks, stained their nails with henna, and lined their eyes and eyebrows heavily with kohl. Kohl was a dark-colored powder made of crushed antimony, burnt almonds, lead, oxidized copper, ochre, ash, malachite, and chrysocolla (a blue-green copper ore) or any combination thereof. \nIt was applied with a small stick. The upper and lower eyelids were painted in a line that extended to the sides of the face for an almond effect. In addition to reducing sun glare, it was believed that kohl eyeliner could restore good eyesight and reduce eye infection. Kohl was kept in a small, flat-bottomed pot with a wide, tiny rim and a flat, disk-shaped lid.\n\nAccording to images of the time, the use of makeup was not limited to women. Highly polished silver and copper mirrors aided the application of makeup.\n\nIn Greece, precious oils, perfumes, cosmetic powders, eye shadows, skin glosses, paints, beauty unguents, and hair dyes were in universal use. Export and sale of these items formed an important part of trade around the Mediterranean. During the 7th and 8th centuries BC, Corinthian, Rhodian and East Greek traders dominated markets in perfume flasks and cosmetic containers. The containers included aryballoi, alabastra, pyxides and other small specialized shapes.\n\nMen and women in the Near East painted their faces with kohl just like the Egyptians did. This was to protect them from the ‘evil eye.’ After the defeat of the Greeks by the Romans, the Romans adapted the Egyptian custom, albeit with different ends. To the Romans, applying eye shadow became a matter of fashion and esthetics. Other cosmetics took on a medicinal application in Rome. Plagues were so rampant throughout Rome that aromatic gums and resins were burned to repel demons and bad spirits.\n\nCommon ingredients in eye shadows consist of talc, mica, sericite, magnesium stearate, colorants, and preservatives. Fillers in eye shadows are primarily talc. The liquid binders are typically a silicone and the dry binders are typically magnesium stearate. In order to make an eye shadow, there has to be a balance between the fillers, dry binders as well as liquid binders.\n\n\n"}
{"id": "29071218", "url": "https://en.wikipedia.org/wiki?curid=29071218", "title": "FaceVsion", "text": "FaceVsion\n\nFacevsion, sometimes written faceVsion, was a HD WebCam manufacturer in Taiwan. It reportedly closed some or all of its offices in February 2011.\n\nA related company, faceVsion Technology USA, is or was located in Fremont, California in the United States.\n\n\n\n"}
{"id": "560807", "url": "https://en.wikipedia.org/wiki?curid=560807", "title": "Feedlot", "text": "Feedlot\n\nA feedlot or feed yard is a type of animal feeding operation (AFO) which is used in intensive animal farming for finishing livestock, notably beef cattle, but also swine, horses, sheep, turkeys, chickens or ducks, prior to slaughter. Large beef feedlots are called concentrated animal feeding operations (CAFO) in the United States and intensive livestock operations (ILOs) or confined feeding operations (CFO) in Canada. They may contain thousands of animals in an array of pens.\n\nThe basic principle of the feedlot is to increase the amount of meat each animal produces as quickly as possible; if animals are kept in confined quarters rather than being allowed to range freely over grassland, they will put on weight more quickly. \nMost feedlots require some type of governmental permit and must have plans in place to deal with the large amount of waste that is generated. The Environmental Protection Agency has authority under the Clean Water Act to regulate all animal feeding operations in the United States. This authority is delegated to individual states in some cases. In Canada, regulation of feedlots is shared between all levels of government, while in Australia this role is handled by the National Feedlot Accreditation Scheme (NFAS).\n\nPrior to entering a feedlot, cattle spend most of their life grazing on rangeland or on immature fields of grain such as green wheat pasture. Once cattle obtain an entry-level weight, about , typically at about a year old, they are transferred to a feedlot for the next six to eight months to be fed to gain weight for eventual slaughter. They eat a specialized animal feed which consists of corn, corn byproducts (some of which is derived from ethanol and high fructose corn syrup production), milo, barley, and other grains as well as roughage which may consist of alfalfa, corn stalks, sorghum, or other hay, cottonseed meal, and premixes composed of microingredients such as vitamins, minerals, chemical preservatives, antibiotics, fermentation products, and other essential ingredients that are purchased from premix companies, usually in sacked form, for blending into commercial rations. Because of the availability of these products, a farmer who uses his own grain can formulate his own rations and be assured his animals are getting the recommended levels of minerals and vitamins. In the American northwest and Canada, barley, low grade durum wheat, chick peas (garbanzo beans), oats and occasionally potatoes are used as feed.\n\nIn a typical feedlot, a cow's diet is roughly 62% roughage, 31% grain, 5% supplements (minerals and vitamins), and 2% premix. High-grain diets lower the pH in the animals' rumen. Due to the stressors of these conditions, and due to some illnesses, it may be necessary to give the animals antibiotics on occasion.\n\nFeedlot diets are high in protein, to encourage growth of muscle mass and the deposition of some fat (known as marbling in butchered meat). The marbling is desirable to consumers, as it contributes to flavor and tenderness. The animal may gain an additional 400 pounds (180 kg) during its approximate 200 days in the feedlot. Once cattle are fattened up to their finished weight, the fed cattle are transported to a slaughterhouse.\n\nIncreasing numbers of cattle feedlots are utilizing out-wintering pads made of timber residue bedding in their operations. Nutrients are retained in the waste timber and livestock effluent and can be recycled within the farm system after use.\n\nThe beef industry today is highly dependent upon technology, but this has not always been true. In the early 20th century, feeder operations were separate from all other related operations and feedlots were non-existent. They appeared in the 1950s and 1960s as a result of hybrid grains and irrigation techniques; the ensuing larger grain crops led to abundant grain harvests. However, the first known feedlot was designed and built by Gustavus Swift in 1876 on the south side of Chicago. It was suddenly possible to feed large numbers of cattle in one location and so, to cut transportation costs, grain farm and feedlot locations merged. Cattle were no longer sent from all across the southern states to places like California, where large slaughter houses were located. In the 1980s, meat packers followed the path of feedlots and are now located close by them as well.\n\nThere are many methods used to sell cattle to meat packers. Spot, or cash, marketing is the traditional and most commonly used method. Prices are influenced by current demand and are determined by live weight or per head. Similar to this is forward contracting, in which prices are determined the same way but are not directly influenced by market demand fluctuations. Forward contracts determine the selling price between the two parties negotiating for a set amount of time. However, this method is the least used because it requires some knowledge of production costs and the willingness of both sides to take a risk in the futures market. Another method, formula pricing, is becoming the most popular process, as it more accurately represents the value of meat received by the packer. This requires trust between the packers and feedlots though, and is under criticism from the feedlots because the amount paid to the feedlots is determined by the packers’ assessment of the meat received. Finally, live- or carcass-weight based formula pricing is most common. Other types include grid pricing and boxed beef pricing. The most controversial marketing method stems from the vertical integration of packer-owned feedlots, which still represents less than 10% of all methods, but has been growing over the years.\n\nThe practice of feeding cattle largely in feedlots has been widely criticized by animal welfare organizations. One concern is that as ruminants, cattle are suited to eating grass, not grain. Consequently, cattle may have issues such as bloating, diarrhea and digestive discomfort. There are also concerns with water contamination from feedlot runoff. It is not only the animals who are concentrated; it is also their waste. Whereas manure is dispersed over a wide area when cattle graze freely and may have some benefits for the soil depending on climate and topography, the density of manure mixed with the soil in a concentrated operation poses potential health risks.\n\nThe alternative to feedlots is to allow cattle to graze on grass throughout their lives. Though controlled grazing methods of this sort necessitate higher beef prices, controlled grazing has benefits for the environment and for the cattle themselves. Controlled grazing provides a fresher and more natural diet, lower stress and their manure can be used to fertilize the land.\n\n\n"}
{"id": "48697908", "url": "https://en.wikipedia.org/wiki?curid=48697908", "title": "GS1 EDI", "text": "GS1 EDI\n\nGS1 EDI is a set of global electronic messaging standards for business documents used in Electronic Data Interchange (EDI). The standards are developed and maintained by GS1. GS1 EDI is part of the overall GS1 system, fully integrated with other GS1 standards, increasing the speed and accuracy of the supply chain.\nExamples of GS1 EDI standards include messages such as: Order, Despatch Advice (Shipping Notice), Invoice, Transport Instruction, etc. \nThe development and maintenance of all GS1 standards is based on a rigorous process called the Global Standard Management Process (GSMP). GS1 develops its global supply chain standards in partnership with the industries using them. Any organization can submit a request to modify the standard. Maintenance releases of GS1 EDI standards are typically published every two years, while code lists can be updated up to 4 times a year.\n\nGS1 developed the following sets of complementary EDI standards:\n\n\nThese groups of standards are being implemented in parallel by various users, GS1 supports and maintains all of them.\nGS1 EDI standards are designed to work together with other GS1 standards for the identification and labeling of goods, locations, parties and packages. This means that the information and product flows can be combined to provide business with tool enabling traceability, visibility and safety.\nIn EDI, it is essential to unambiguously identify products, services and parties involved in the transaction. In GS1 EDI standard messages, each product, party and location is identified by a unique GS1 identification key, e.g.:\n\n\nUsing the GS1 ID Keys enables master data alignment between trading partners before any trading transaction takes place. This ensures data quality, eliminates errors and removes the need to send redundant information in electronic messages (such as product specifications, party addresses, etc.).\n\nGS1 EDI standards are developed based on other global standards, such as:\n\n\nUser companies are involved in the development of GS1 standards, either directly or via industry associations, such as The Consumer Goods Forum.\n\nGS1 EDI standards are globally used by companies and organizations from different sectors and applied in various processes like Retail Up- and Downstream, Transport and Warehouse Management, Healthcare, Defense, Finance, Packaging (collaborative artwork development), Cash Handling, public administration and much more.\n\n\n"}
{"id": "4942600", "url": "https://en.wikipedia.org/wiki?curid=4942600", "title": "Greater Noida", "text": "Greater Noida\n\nGreater Noida City is a north Indian city with a population in excess of 100,000, located in the Gautam Budh Nagar district of the northern state of Uttar Pradesh. The city was created under the \"UP Industrial Area Development Act, 1976\". It is a part of the National Capital Region (NCR) of India. Situated 30 km south-east of capital city of New Delhi, it takes around 30 minutes to travel between the cities via the Noida-Greater Noida Expressway.\n\nIn the early 1980s, the government of India realised that the rapid rate at which Delhi was expanding would result in chaos, so they planned to develop residential and industrial areas around the capital to reduce the demographic burden. Before Greater Noida City, there were two areas that had been developed—Gurgaon, across the border from Haryana, and Noida, across the border with Uttar Pradesh. \n\nGreater Noida Notified Area - 38000 Ha Comprising 124 villages (308 km)\nNoida's infrastructure was carefully laid out, but the 1990s saw huge growth in the Indian economy. Migration to cities like Delhi, Mumbai, Kolkata, Chennai, Hyderabad and Bangalore exceeded planning estimates. Noida was developed to accommodate population growth for 20–25 years. The massive population influx from Delhi, however, caused it to overload in a mere 15 years, although intake is not complete and illegal mining remains a problem.\nThe government of Uttar Pradesh decided to develop another city as an extension to Noida with better planning. The idea was to create a world-class city approximately 25 km from Noida. A railway station near Boraki and an international airport were included later in the plan intending to develop Greater Noida as an independent city.\n\nDuring the 1990s, the Noida extension (now a part of Gautam Buddh Nagar) became what is today known as Greater Noida. The development of the city is managed by the Greater NOIDA Authority. Greater Noida is connected to Agra by the six-lane Yamuna Expressway. The annual Indian Grand Prix is held at the Buddh International Circuit. Roads are wide with service lanes for every major road. The sectors are named by letters of the Greek alphabet. All cabling and utilities have been built underground. Alpha, Beta, and Gamma are the oldest sectors. The other emerging sectors include Xu, Delta, Mu, Omicron and Tau. The present GNIDA office is in Gamma II sector just opposite the historical village Rampur Jagir/Jahangir where the revolutionary Pandit Ram Prasad Bismil lived in 1919 when he was hidden underground after the Mainpuri conspiracy. A park has been named \"Amar Shaheed Pt. Ram Prasad Bismil Udyan\" by the Uttar Pradesh Government.\n\nThe 12th, 14th and 16th Auto Expos (The Motor Show) were held at India Expo Mart, Greater Noida, in February 2014, 2016 and 2018 respectively.\nIn 2018, Gamma was officially declared as the capital of Greater noida.\n\nGreater Noida West, previously known as Noida extension, is a part of Greater Noida and consists of 16 villages: Khairpur Gurjar, Shahberi, Devla, Patwari, Ghanghola, Bisrakh, Roza-Yakubpur, Haibatpur, Itaida, Patwari, Aminabad, Asadallapur, Maincha and Chipyana Buzurg. All sectors under Noida Extension (Sector 1 to 4) are a part of the Greater NOIDA Authority.\n\nAs of late 2012, plans were being formed to rename it to Greater Noida (West). Planners intended the area to provide NCR region housing.\n\nAs per provisional data of the 2011 census, Greater Noida had a population of 107,676, with 58,662 males and 49,014 females. The literacy rate was 86.54%, 91.48% of males and 80.65% of females. The demographics of Greater Noida mainly consist of students, corporate employees, and labourers. Students are often temporary residents from other parts of India and abroad. Greater Noida and Noida combined have approximately 300 villages.\n\nThe city's infrastructure is looked after by the Greater NOIDA Authority, a statutory authority set-up under \"Uttar Pradesh Industrial Area Development Act, 1976\". Authority's head is its Chairman, who is an IAS officer, the authority's daily matters however, are looked after by its CEO, who is also an IAS officer. Greater NOIDA Authority comes under the Infrastructure and Industrial Development Department of Uttar Pradesh Government. The current Chairman is Rahul Bhatnagar , whereas the current CEO is Debashish Panda.\n\nThe Guatam Budh Nagar district is a part of Meerut Division, headed by the Divisional Commissioner, who is an IAS officer of high seniority, the Commissioner is the head of local government institutions (including Municipal Corporations) in the division, is in-charge of infrastructure development in his division, and is also responsible for maintaining law and order in the division. The District Magistrate, hence, reports to the Divisional Commissioner of Meerut. The current Commissioner is Prabhat Kumar.\n\nGautam Budh Nagar district administration is headed by the District Magistrate of Gautam Budh Nagar, who is an IAS officer. The DM is in charge of property records and revenue collection for the central government and oversee the national elections held in the city. The DM is also responsible for maintaining law and order in the city, hence the SSP of Gautam Budh Nagar also reports to the DM of Gautam Budh Nagar. The District Magistrate is assisted by one Chief Development Officer, three Additional District Magistrates (Executive, Finance/Revenue and Land Acquisition) and one City Magistrate. The district has divided into three Tehsils named Sadar, Dadri and Jewar each headed by a Sub-Divisional Magistrate who reports to the District Magistrate. The current DM is Brajesh Narain Singh.\n\nGautam Budh Nagar district comes under Meerut police zone and Meerut police range of Uttar Pradesh Police. Meerut zone is headed by an IPS officer in the rank of Additional Director General of Police (ADG), whereas Meerut range is headed by an IPS officer in the rank of Inspector General of Police (IG). The Current ADG, Meerut Zone is Prashant Kumar, whereas the current IG, Meerut Range is Ram Kumar.\n\nPolice Administration of Gautam Budh Nagar is headed by the Senior Superintendent of Police (SSP) who is an IPS officer and is accountable to the District Magistrate for Law and Order enforcement. He is assisted by four Superintendents of Police (SP)/Additional Superintendents of Police (Addl. SP) (City, Rural Area, Traffic and Crime). The district is divided into eight police circles, each responsibility of a Circle Officer (CO) in the rank of Deputy Superintendent of Police. SP (Traffic) and SP (Crime) are assisted by one Circle Officer in the rank of Deputy Superintendent of Police each. Greater Noida city is divided into three police circles viz. City-I,City-II and City-III, each looked after by a CO in rank of Deputy Superintendent of Police. Superintendent of Police (Rural Area) is the SP in-charge of Greater Noida and its circles. The current SSP is Love Kumar, whereas current SP (RA) is Suniti.\n\nGreater Noida has a similar climate to Delhi: very hot and dry during summer, hot and humid during monsoons, pleasant and dry during spring and autumn, and cool to cold during winters.\n\nAccording to the Bureau of Indian Standards, the town falls under seismic zone-III, on a scale of I to V (in order of increasing proneness to earthquakes) while the wind and cyclone zoning is a \"very high damage risk\", according to the UNDP report. Greater Noida has a Tropical Savanna Climate with three main seasons: summer, monsoon and winter. Aside from monsoon weather, it mainly remains dry.\n\nIn summer, i.e. from March to June, the temperature ranges from a maximum of 45 °C (i.e. 113 °F) to a minimum of 23 °C (73 °F). Monsoon season prevails during mid-June to mid-September with an average rainfall of 93.2 cm (36.7 inches). The cold waves from the Himalayan region make the winters in Greater Noida very chilly. Temperatures fall down to as low as 3 to 4 °C at the peak of winter. In January, a dense fog envelopes the city, reducing visibility on the streets.\n\n\nSchools\n\n\n\n\nLocated on Yamuna Expressway, Jaypee Sports City is a planned city aimed for sports, complete with various sports venues like an international standard cricket stadium, a hockey stadium, and an international Formula 1 racing circuit.\n\nOn 30 October 2011, Greater Noida hosted the inaugural Formula One Indian Grand Prix at the Buddh International Circuit constructed by Jaypee Group. It was the seventeenth round of the 2011 Formula One season, and the first Formula One Grand Prix to take place on the Indian subcontinent and even the circuit is the first of its kind in South Asia. The second and third Formula One Airtel Indian Grands Prix, held in October 2012 and 2013, were won by Red Bull Racing Driver Sebastian Vettel, his second and third consecutive wins in India.\n\nGreater Noida Cricket Stadium, also known as \"Shaheed Vijay Singh Pathik Stadium\", is located near Jaypee Green Golf Course. The stadium hosted its first Ranji Trophy match between Uttar Pradesh and Baroda from December 1–4, 2015. The ground would now be used by the national cricket team of Afghanistan as its home ground.\n\nNational badminton coach Pullela Gopichand has opened a badminton academy in Greater Noida Stadium.\n\nJaypee Greens Golf Course, an 18-hole, par-72 course designed by Greg Norman, is situated in Greater Noida. The course opened in June 2000 and received a \"Tourism Friendly Golf Course\" award from India's Ministry of Tourism in 2011. It is the longest course in India.\n\nThere will also be a hockey stadium which is under construction and has a sports training academy and infrastructure for other sports.\n\nThe Time Trial cycling event for the 2010 Commonwealth Games was held at Noida–Greater Noida Expressway.Greater Noida Sports Complex Ground is also the Home Ground of Afghanistan Cricket Team\n\nOf late, Greater Noida has attracted a lot of interest from major corporate houses for setting up their businesses in the city. In November 2016, Patanjali Ayurved announced that it would be investing Rs. 2,000 Crore in a greenfield investment in Greater Noida. The project has been approved by Uttar Pradesh Cabinet. A clutch of mobile manufacturers have also shown interest in investing in Greater Noida. Taiwan Electrical and Electronics Manufacturers' Association will develop a 210-acre greenfield electronic manufacturing cluster in Greater Noida with an investment of USD 200 million.\n\nThe construction of metro is underway in the twin cities of Noida and Greater Noida. This project was announced in 2013. The metro would primarily run across the Noida-Greater Noida Expressway. Delhi Metro already connects some parts of Noida. By November 2018, a 30 km metro link is expected to become operational between Noida and Greater Noida. This link will be operated and managed by an independent entity called Noida Metro Rail Corporation. Keeping in mind the commuter convenience and to provide the last mile connectivity to commuters in Noida and Greater Noida, NMRC launched bus services in December 2016. Another metro link to Greater Noida has also been approved which will connect Greater Noida (West) with Noida.\n"}
{"id": "414980", "url": "https://en.wikipedia.org/wiki?curid=414980", "title": "Harry Ricardo", "text": "Harry Ricardo\n\nSir Harry Ralph Ricardo (26 January 1885 – 18 May 1974) was one of the foremost engine designers and researchers in the early years of the development of the internal combustion engine.\n\nAmong his many other works, he improved the engines that were used in the first tanks, oversaw the research into the physics of internal combustion that led to the use of octane ratings, was instrumental in development of the sleeve valve engine design, and invented the Diesel \"Comet\" Swirl chamber that made high-speed diesel engines economically feasible.\n\nHarry Ricardo was born at 13 Bedford Square, London in 1885, the eldest of three children, and only son of Halsey Ricardo, the architect, and his wife Catherine Jane, daughter of Sir Alexander Meadows Rendel, a civil engineer. Ricardo was descended from a brother of the famous political economist David Ricardo, a Sephardi Jew of Portuguese origin. He was one of the first people in England to see an automobile when his grandfather purchased one in 1898. He was from a relatively wealthy family and educated at Rugby School. In October 1903 he joined Trinity College, Cambridge as a civil engineering student. Ricardo had been using tools and building engines since the age of ten.\n\nIn 1911 Ricardo married Beatrice Bertha Hale, an art student at the Slade School of Art, in London. Her father, Charles Bowdich Hale, was the Ricardos' family doctor. They had three daughters, and lived most of their married life at Lancing and Edburton in West Sussex.\n\nIn 1904, at the end of his first year at Cambridge, he decided to enter the University Automobile Club's event, which was a competition to design a machine that could travel the furthest on of petrol. His engine had a single cylinder, and was the heaviest entered, but his motorcycle design won the competition, having covered a distance of . He was then persuaded to join the Professor of Mechanism and Applied Mechanics, Bertram Hopkinson, working on research into engine performance. He graduated with a degree in 1906 and spent another year researching at Cambridge.\n\nRicardo is said by Percy Kidner, then Co-managing director of Vauxhall, to have had a hand in the design of the Vauxhall engine designed by Laurence Pomeroy for the RAC trial of 1908.\n\nBefore graduation, Ricardo had designed a two-stroke motorcycle engine to study the effect of mixture strength upon the combustion process. When he graduated, the small firm of Messrs Lloyd and Plaister expressed interest in making the engine. Ricardo produced designs for two sizes, and the smaller one sold about 50 engines until 1914, when the war halted production.\n\nIn 1909 he designed a two-stroke 3.3-litre engine, for his cousin Ralph Ricardo, who had established a small car manufacturing company, \"Two Stroke Engine Company\", at Shoreham-by-Sea. The engine was to be used in the \"Dolphin\" car. The cars were well made, but it became apparent that they cost more to make than the selling price. The company fared better making two-stroke engines for fishing boats. In 1911 the firm collapsed and Ralph departed for India. Ricardo continued to design engines for small electric lighting sets; these were produced by two companies until 1914.\n\nIn 1915 Ricardo set up a new company, \"Engine Patents Ltd.\", which developed the engine that would eventually be used in the first successful tank design, the British Mark V. The Daimler sleeve-valve engine used in the Mark I created copious amounts of smoke, which easily gave away its position. Ricardo was asked to look at the problem of reducing smoky exhaust gases and decided that a new engine was needed. Existing companies were able to undertake construction of such an engine but not the design, so Ricardo designed it himself. As well as having reduced smoke emissions, the new engine was much more powerful than the existing ones. The new six-cylinder engine produced , compared with , and later modifications produced and By April 1917 one hundred engines were being produced a week. A total of over 8,000 of his tank engines were put into military service, making it the first British-designed engine to be produced in large numbers. The Mark IX tank, as well as the British version of the Mark VIII, also used a Ricardo engine. In addition to being fitted to tanks, several hundred of the engines were used in France for providing power and light to base workshops, hospitals, camps, etc.\n\nIn 1917 his old mentor, Bertram Hopkinson, who was now Technical Director at the Air Ministry, invited him to join the new engine research facility at the Department of Military Aeronautics, later to become the RAE. In 1918 Hopkinson was killed while flying a Bristol Fighter, and Ricardo took over his position. From that point on the department produced a string of experimental engines and research reports that constantly drove the British, and world, engine industry.\n\nOne of his first major research projects was on the problems of irregular combustion, known as \"knocking\" or \"pinging\". To study the problem he built a unique variable-compression test engine. This led to the development of an octane rating system for fuels, and considerable investment into octane improving additives and refining systems. The dramatic reduction in fuel use as a result of higher-octane fuel was directly responsible for allowing Alcock and Brown to fly the Atlantic in their Vickers Vimy bombers adapted with his modifications.\n\nIn 1919 Ricardo was studying the phenomena affecting the combustion within the petrol engine and the diesel engine. He realised that turbulence within the combustion chamber increased flame speed, and that he could achieve this by offsetting the cylinder head. He also realised that making the chamber as compact as possible would reduce the distance that the flame had to travel and would reduce the likelihood of detonation. He later developed the induction swirl chamber, which was an attempt to achieve orderly air motion in a diesel engine, the swirl being initiated by inclined ports and accentuated by forcing the air into a small cylindrical volume. Finally he developed the compression swirl chamber for diesel engines. This design embodied intense swirl with a reasonable rate of pressure rise and good fuel consumption.\n\nThe compression swirl chamber design was called a \"Comet\" design (patented in 1931) and was subsequently licensed to a large number of companies for use in trucks, buses, tractors and cranes, as well as private cars and taxis. A Comet combustion chamber was used in the first Associated Equipment Company (AEC) diesel buses operated in 1931 by London General Omnibus Co, later part of the London Passenger Transport Board/London Transport. A later development of it featured in the world's first volume production diesel passenger car, the 1934 Citroën Rosalie. This meant that Britain led the world in the field of high-speed diesels for road transport at that time. This advantage was lost to the United Kingdom as a result of the heavy tax imposed on diesel fuel in the budget of 1938.\n\nRicardo designed the 1921 T.T. Vauxhall engine which was described by Cecil Clutton in \"Motor Sport\" as a \"tour de force\" in the 1922 RAC T. T. O Payne in the Ricardo Vauxhall came 3rd, Jean Chassagne on a 1921 Grand Prix Sunbeam winning outright. The engine was later developed by Mays and Villiers, who fitted a supercharger, and was still a winner fifteen years later.\n\nIn 1922 and 1923 Ricardo published a two-volume work \"The Internal Combustion Engine\".\n\nIn 1927 he formed Ricardo Consulting Engineers (now known as Ricardo plc) in Shoreham-by-Sea, which has become one of the foremost automotive consulting firms worldwide and is publicly listed on the London Stock Exchange.\n\nAlthough Ricardo did not invent the sleeve valve, in 1927, he produced a seminal research paper that outlined the advantages of the sleeve valve, and suggested that poppet valve engines would not be able to offer power outputs much beyond 1500 hp (1,100 kW). A number of sleeve valve aircraft engines were developed following this paper, notably by Napier, Bristol and Rolls-Royce. Bristol produced the Perseus, Hercules, Taurus and the Centaurus, Napier produced the Napier Sabre, and Rolls-Royce produced the Eagle and Crecy, all using sleeve valves.\n\nIn 1929 Ricardo was elected Fellow of the Royal Society.\n\nRicardo's work on the sleeve valve affected the development of British aircraft engines in the thirties and during the war. He even enhanced the famous Rolls-Royce Merlin engine in the Mosquito by giving it an oxygen enrichment system to improve its performance.\n\nRicardo's work exerted influence all around the world. While his work guaranteed England a supply of fuels of ever-increasing power during the 1930s, it also helped Germany to develop synthetic high-octane aviation fuel, for example for the Focke-Wulf Fw 190 which inflicted heavy losses among the RAF's Supermarine Spitfires in 1942. Likewise, Ricardo's research on the detonation-inhibiting qualities of water injection was exploited by German engineers (MW 50) to provide their aero-engines with a particularly powerful special emergency power rating.\n\nDuring 1941–1945 Ricardo was a member of the War Cabinet engineering advisory committee.\n\nRicardo also assisted in the design of the combustion chambers and fuel control system of Sir Frank Whittle's jet engine.\n\nIn 1944 Ricardo was elected president of the Institution of Mechanical Engineers. In 1945 he and his wife moved from Shoreham-by-Sea to Graffham, also in West Sussex. In 1948 Ricardo was knighted in recognition for his work in the field of internal combustion engineering.\n\nIn 1964 Ricardo retired from active work in Ricardo Consulting Engineers but kept in touch with various engineers within the company.\n\nIn 1974, at the age of 89, Ricardo suffered a pelvic injury in a fall. He died six weeks later, on 18 May.\nOn 16 June 2005 a blue plaque was placed outside the house where he was born in Bedford Square, London. On 1 July 2010, the Institution of Mechanical Engineers bestowed an Engineering Heritage Award on Sir Harry Ricardo in recognition of his life and work as one of the foremost engineers of the twentieth century. The first internal combustion engine which Harry Ricardo designed and built as a schoolboy currently displays this Engineering Heritage plaque in the Ricardo plc company exhibition area.\nDuring the 1960s a second round of development of the Comet system was started, the company now armed with considerably more powerful test apparatus. The refined design was immediately used in many diverse applications, including cars, taxis, buses, trucks, industrial engines, earth-moving equipment, cranes and marine before eventually being superseded by more efficient direct injection systems which are now used in the majority of diesel engines.\n\nIn 1978 the US Department of Energy hired Ricardo Consulting to research the Stirling engine as a car engine. A series of engines, eventually forty-five in total, were built to test this system and showed very low emissions, but the efficiency was compromised by the need to operate under transient conditions—the design was best running at a single speed, making it less feasible as a car engine.\n\nIn 1986, the Voyager was the first aircraft to fly around the world non-stop and without refuelling. Ricardo Consulting redesigned the otherwise \"stock\" Teledyne Continental engine to incorporate a highly efficient combustion system and water cooling, thereby dramatically reducing drag and improving fuel economy.\n\nRicardo worked on spark-ignition engines throughout his career, including pioneering work on direct injection gasoline engines for aircraft engines in the 1930s, but his best-known work was in the development of high-speed diesel engines, in particular for cars. He was responsible for designing the combustion-chamber for the first diesel car engines produced in quantity for the Citroën Rosalie in the mid-thirties. Later, various versions of Ricardo's Comet pre-combustion chamber were deployed in almost all car diesel engines over six decades, up to the mid 1990s, and the Comet remains one of Ricardo's best-known personal achievements in automobile engineering.\n\nToday several stratified charge engines are in use in the automobile market. Stratified charge is a technology that has come of age relatively recently, as a result of advances in manufacturing and electronics, but it was a feature of the first engine that Harry Ricardo built, while still in his teens, in the early years of the 20th century.\n\n\n\n"}
{"id": "17104229", "url": "https://en.wikipedia.org/wiki?curid=17104229", "title": "Hellbox", "text": "Hellbox\n\nA hellbox is a receptacle where cast metal \"sorts\" are thrown after printing. The job of sorting the type from the hellbox and putting it back into the job case was given to the apprentice, known as a printer's devil. Later, when continuous casting or hot metal typesetting machines such as the Linotype and Monotype became popular, the hellbox was used for storing discarded or broken type that were melted down and recast.\n\n"}
{"id": "23534602", "url": "https://en.wikipedia.org/wiki?curid=23534602", "title": "Human–computer interaction", "text": "Human–computer interaction\n\nHuman–computer interaction (HCI) researches the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. \nAs a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their seminal 1983 book, \"The Psychology of Human–Computer Interaction\", although the authors first used the term in 1980 and the first known use was in 1975. The term connotes that, unlike other tools with only limited uses (such as a hammer, useful for driving nails but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human–computer interaction to human-to-human interaction, an analogy which is crucial to theoretical considerations in the field.\n\nHumans interact with computers in many ways; the interface between humans and computers is crucial to facilitating this interaction. Desktop applications, internet browsers, handheld computers, and computer kiosks make use of the prevalent graphical user interfaces (GUI) of today. Voice user interfaces (VUI) are used for speech recognition and synthesizing systems, and the emerging multi-modal and Graphical user interfaces (GUI) allow humans to engage with embodied character agents in a way that cannot be achieved with other interface paradigms. The growth in human–computer interaction field has been in quality of interaction, and in different branching in its history. Instead of designing regular interfaces, the different research branches have had a different focus on the concepts of multimodality rather than unimodality, intelligent adaptive interfaces rather than command/action based ones, and finally active rather than passive interfaces.\n\nThe Association for Computing Machinery (ACM) defines human–computer interaction as \"a discipline concerned with the design, evaluation and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them\". An important facet of HCI is user satisfaction (or simply End User Computing Satisfaction).\n\"Because human–computer interaction studies a human and a machine in communication, it draws from supporting knowledge on both the machine and the human side. On the machine side, techniques in computer graphics, operating systems, programming languages, and development environments are relevant. On the human side, communication theory, graphic and industrial design disciplines, linguistics, social sciences, cognitive psychology, social psychology, and human factors such as computer user satisfaction are relevant. And, of course, engineering and design methods are relevant.\" Due to the multidisciplinary nature of HCI, people with different backgrounds contribute to its success. HCI is also sometimes termed \"human–machine interaction\" (HMI), \"man-machine interaction\" (MMI) or \"computer-human interaction\" (CHI).\n\nPoorly designed human-machine interfaces can lead to many unexpected problems. A classic example is the Three Mile Island accident, a nuclear meltdown accident, where investigations concluded that the design of the human-machine interface was at least partly responsible for the disaster. Similarly, accidents in aviation have resulted from manufacturers' decisions to use non-standard flight instrument or throttle quadrant layouts: even though the new designs were proposed to be superior in basic human-machine interaction, pilots had already ingrained the \"standard\" layout and thus the conceptually good idea actually had undesirable results.\n\nHuman–computer interaction studies the ways in which humans make, or do not make, use of computational artifacts, systems and infrastructures. In doing so, much of the research in the field seeks to \"improve\" human–computer interaction by improving the \"usability\" of computer interfaces. How usability is to be precisely understood, how it relates to other social and cultural values and when it is, and when it may not be a desirable property of computer interfaces is increasingly debated.\n\nMuch of the research in the field of human–computer interaction takes an interest in:\n\n\nVisions of what researchers in the field seek to achieve vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities. When pursuing a post-cognitivist perspective, researchers of HCI may seek to align computer interfaces with existing social practices or existing sociocultural values.\n\nResearchers in HCI are interested in developing design methodologies, experimenting with devices, prototyping software and hardware systems, exploring interaction paradigms, and developing models and theories of interaction.\n\nHCI differs from human factors and ergonomics as HCI focuses more on users working specifically with computers, rather than other kinds of machines or designed artifacts. There is also a focus in HCI on how to implement the computer software and hardware mechanisms to support human–computer interaction. Thus, \"human factors\" is a broader term; HCI could be described as the human factors of computers – although some experts try to differentiate these areas. \n\nHCI also differs from human factors in that there is less of a focus on repetitive work-oriented tasks and procedures, and much less emphasis on physical stress and the physical form or industrial design of the user interface, such as keyboards and mouse devices.\n\nThree areas of study have substantial overlap with HCI even as the focus of inquiry shifts. Personal information management (PIM) studies how people acquire and use personal information (computer based and other) to complete tasks. In computer-supported cooperative work (CSCW), emphasis is placed on the use of computing systems in support of the collaborative work. The principles of human interaction management (HIM) extend the scope of CSCW to an organizational level and can be implemented without use of computers.\n\nWhen evaluating a current user interface, or designing a new user interface, it is important to keep in mind the following experimental design principles:\nRepeat the iterative design process until a sensible, user-friendly interface is created.\n\nA number of diverse methodologies outlining techniques for human–computer interaction design have emerged since the rise of the field in the 1980s. Most design methodologies stem from a model for how users, designers, and technical systems interact. Early methodologies, for example, treated users' cognitive processes as predictable and quantifiable and encouraged design practitioners to look to cognitive science results in areas such as memory and attention when designing user interfaces. Modern models tend to focus on a constant feedback and conversation between users, designers, and engineers and push for technical systems to be wrapped around the types of experiences users want to have, rather than wrapping user experience around a completed system.\n\n\nDisplays are human-made artifacts designed to support the perception of relevant system variables and to facilitate further processing of that information. Before a display is designed, the task that the display is intended to support must be defined (e.g. navigating, controlling, decision making, learning, entertaining, etc.). A user or operator must be able to process whatever information that a system generates and displays; therefore, the information must be displayed according to principles in a manner that will support perception, situation awareness, and understanding.\n\nChristopher Wickens et al. defined 13 principles of display design in their book \"An Introduction to Human Factors Engineering\".\n\nThese principles of human perception and information processing can be utilized to create an effective display design. A reduction in errors, a reduction in required training time, an increase in efficiency, and an increase in user satisfaction are a few of the many potential benefits that can be achieved through utilization of these principles.\n\nCertain principles may not be applicable to different displays or situations. Some principles may seem to be conflicting, and there is no simple solution to say that one principle is more important than another. The principles may be tailored to a specific design or situation. Striking a functional balance among the principles is critical for an effective design.\n\n\"1. Make displays legible (or audible)\". A display's legibility is critical and necessary for designing a usable display. If the characters or objects being displayed cannot be discernible, then the operator cannot effectively make use of them.\n\n\"2. Avoid absolute judgment limits\". Do not ask the user to determine the level of a variable on the basis of a single sensory variable (e.g. colour, size, loudness). These sensory variables can contain many possible levels.\n\n\"3. Top-down processing\". Signals are likely perceived and interpreted in accordance with what is expected based on a user's experience. If a signal is presented contrary to the user's expectation, more physical evidence of that signal may need to be presented to assure that it is understood correctly.\n\n\"4. Redundancy gain\". If a signal is presented more than once, it is more likely that it will be understood correctly. This can be done by presenting the signal in alternative physical forms (e.g. colour and shape, voice and print, etc.), as redundancy does not imply repetition. A traffic light is a good example of redundancy, as colour and position are redundant.\n\n\"5. Similarity causes confusion: Use distinguishable elements\". Signals that appear to be similar will likely be confused. The ratio of similar features to different features causes signals to be similar. For example, A423B9 is more similar to A423B8 than 92 is to 93. Unnecessarily similar features should be removed and dissimilar features should be highlighted.\n\n\"6. Principle of pictorial realism\". A display should look like the variable that it represents (e.g. high temperature on a thermometer shown as a higher vertical level). If there are multiple elements, they can be configured in a manner that looks like it would in the represented environment.\n\n\"7. Principle of the moving part\". Moving elements should move in a pattern and direction compatible with the user's mental model of how it actually moves in the system. For example, the moving element on an altimeter should move upward with increasing altitude.\n\n\"8. Minimizing information access cost\" or interaction cost. When the user's attention is diverted from one location to another to access necessary information, there is an associated cost in time or effort. A display design should minimize this cost by allowing for frequently accessed sources to be located at the nearest possible position. However, adequate legibility should not be sacrificed to reduce this cost.\n\n\"9. Proximity compatibility principle\". Divided attention between two information sources may be necessary for the completion of one task. These sources must be mentally integrated and are defined to have close mental proximity. Information access costs should be low, which can be achieved in many ways (e.g. proximity, linkage by common colours, patterns, shapes, etc.). However, close display proximity can be harmful by causing too much clutter.\n\n\"10. Principle of multiple resources\". A user can more easily process information across different resources. For example, visual and auditory information can be presented simultaneously rather than presenting all visual or all auditory information.\n\n\"11. Replace memory with visual information: knowledge in the world\". A user should not need to retain important information solely in working memory or retrieve it from long-term memory. A menu, checklist, or another display can aid the user by easing the use of their memory. However, the use of memory may sometimes benefit the user by eliminating the need to reference some type of knowledge in the world (e.g., an expert computer operator would rather use direct commands from memory than refer to a manual). The use of knowledge in a user's head and knowledge in the world must be balanced for an effective design.\n\n\"12. Principle of predictive aiding\". Proactive actions are usually more effective than reactive actions. A display should attempt to eliminate resource-demanding cognitive tasks and replace them with simpler perceptual tasks to reduce the use of the user's mental resources. This will allow the user to focus on current conditions, and to consider possible future conditions. An example of a predictive aid is a road sign displaying the distance to a certain destination.\n\n\"13. Principle of consistency\". Old habits from other displays will easily transfer to support processing of new displays if they are designed consistently. A user's long-term memory will trigger actions that are expected to be appropriate. A design must accept this fact and utilize consistency among different displays.\n\nThe human–computer interface can be described as the point of communication between the human user and the computer. The flow of information between the human and computer is defined as the \"loop of interaction\". The loop of interaction has several aspects to it, including:\n\n\nTopics in human-computer interaction include:\n\nEnd-user development studies how ordinary users could routinely tailor applications to their own needs and to invent new applications based on their understanding of their own domains. With their deeper knowledge, users could increasingly be important sources of new applications at the expense of generic programmers with systems expertise but low domain expertise.\n\nComputation is passing beyond computers into every object for which uses can be found. Embedded systems make the environment alive with little computations and automated processes, from computerized cooking appliances to lighting and plumbing fixtures to window blinds to automobile braking systems to greeting cards. The expected difference in the future is the addition of networked communications that will allow many of these embedded computations to coordinate with each other and with the user. Human interfaces to these embedded devices will in many cases be disparate from those appropriate to workstations.\n\nAugmented reality refers to the notion of layering relevant information into our vision of the world. Existing projects show real-time statistics to users performing difficult tasks, such as manufacturing. Future work might include augmenting our social interactions by providing additional information about those we converse with.\n\nIn recent years, there has been an explosion of social science research focusing on interactions as the unit of analysis. Much of this research draws from psychology, social psychology, and sociology. For example, one study found out that people expected a computer with a man's name to cost more than a machine with a woman's name. Other research finds that individuals perceive their interactions with computers more positively than humans, despite behaving the same way towards these machines.\n\nIn human and computer interactions, a semantic gap usually exists between human and computer's understandings towards mutual behaviors. Ontology (information science), as a formal representation of domain-specific knowledge, can be used to address this problem, through solving the semantic ambiguities between the two parties.\n\nIn the interaction of humans and computers, research has studied how computers can detect, process and react to human emotions to develop emotionally intelligent information systems. Researchers have suggested several 'affect-detection channels'. The potential of telling human emotions in an automated and digital fashion lies in improvements to the effectiveness of human-computer interaction. The influence of emotions in human-computer interaction has been studied in fields such as financial decision making using ECG and organisational knowledge sharing using eye tracking and face readers as affect-detection channels. In these fields it has been shown that affect-detection channels have the potential to detect human emotions and that information systems can incorporate the data obtained from affect-detection channels to improve decision models.\n\nTraditionally, computer use was modeled as a human–computer dyad in which the two were connected by a narrow explicit communication channel, such as text-based terminals. Much work has been done to make the interaction between a computing system and a human more reflective of the multidimensional nature of everyday communication. Because of potential issues, human–computer interaction shifted focus beyond the interface to respond to observations as articulated by D. Engelbart: \"If ease of use was the only valid criterion, people would stick to tricycles and never try bicycles.\"\n\nThe means by which humans interact with computers continues to evolve rapidly. Human–computer interaction is affected by developments in computing. These forces include:\n\n\n\nOne of the main conferences for new research in human–computer interaction is the annually held Association for Computing Machinery's (ACM) \"Conference on Human Factors in Computing Systems\", usually referred to by its short name CHI (pronounced \"kai\", or \"khai\"). CHI is organized by ACM Special Interest Group on Computer–Human Interaction (SIGCHI). CHI is a large conference, with thousands of attendants, and is quite broad in scope. It is attended by academics, practitioners and industry people, with company sponsors such as Google, Microsoft, and PayPal.\n\nThere are also dozens of other smaller, regional or specialized HCI-related conferences held around the world each year, including:\n\n\n\n\n\n\n\n\n\n"}
{"id": "24071063", "url": "https://en.wikipedia.org/wiki?curid=24071063", "title": "Hybricon Corporation", "text": "Hybricon Corporation\n\nHybricon Corporation is a provider of systems packaging solutions serving the Military, Aerospace, Homeland Security, Medical and high-end Industrial markets and develops embedded computing systems and solutions for war fighter critical missions using OpenVPX, VPX, VXS, VMEbus, VME64X, CompactPCI, Rugged MicroTCA, and custom bus structures.\n\nHybricon is a member of the PCI Industrial Computer Manufacturers Group (PICMG), VME International Trade Association, and member of the OpenVPX Industry Working Standards Group. Hybricon along with Curtiss-Wright were the first to demonstrate a live OpenVPX System at MILCOM in Boston, MA in October 2009. The system included an OpenVPX backplane in a Hybricon SFF-4 Small Form Factor conduction-cooled chassis with Curtiss-Wright Control small form factor 3U cards.\n\nIn 2010, Curtiss-Wright Controls Electronic Systems acquired Hybricon for $19 million in cash.\n\nIn 2015, Atrenne Integrated Solutions acquired Hybricon.\n\n\n"}
{"id": "27393742", "url": "https://en.wikipedia.org/wiki?curid=27393742", "title": "IBall (company)", "text": "IBall (company)\n\nBest IT World (India) Pvt. Ltd. operating as iBall is a Indian privately held consumer electronics company headquartered in Mumbai, Maharashtra, India, which imports computer peripherals, smartphones and tablets from original equipment manufacturers (OEMs). iBall started operations in September 2001, initially selling computer mice. , the company sold consumer electronics products in 27 different product categories.\n\nIn 2014, iBall launched the Andi Uddaan smartphone for women. An SOS button located at the back of the phone sounds a loud siren and automatically sends text messages (SMS) to five pre-selected contacts when pressed.\n\nIn May 2015, iBall launched the iBall Slide i701 in collaboration with Intel and Microsoft.\n\nIn May 2016, iBall in a strategic partnership with Intel and Microsoft claimed to have launched India's most affordable Windows 10 Laptop - iBall CompBook at Rs.9,999.\n"}
{"id": "25593450", "url": "https://en.wikipedia.org/wiki?curid=25593450", "title": "Infant massage", "text": "Infant massage\n\nInfant massage is a type of complementary and alternative treatment that uses massage therapy for babies. Evidence is insufficient to support its use in either full term or preterm babies.\n\nAyurvedic medicine in ancient India taught the use of infant massage. It was also has been encouraged in China during the Qing dynasty. At present it is part of traditional childcare in South Asia and elsewhere where daily massage by mothers is seen as \"instilling fearlessness, hardening bone structure, enhancing movement and limb coordination, and increasing weight\". Other areas where infant massage is regularly used are African countries and areas in the former Soviet Union. In Western culture, infant massage has been increasingly used in neonatal intensive care units for pre-term infants who are in stressful environments and have limited tactile stimulation.\n\nA 2013 Cochrane review of massage therapy for babies less than 6 months of age who were born at term found that the evidence was insufficient to support its use. A 2004 Cochrane review looking at massage therapy for pre-term and low birth weight was insufficient to justify its use.\n\nVarious mechanisms have been proposed as to suggest how massage therapy might benefit infants. For pre-term infants, it has been suggested that any weight gain may be due to improved metabolic efficiency or by reducing the adverse reaction of stress through decreasing stress behavior or stress hormones. Other possible mechanisms include increased vagal activity and secretion of insulin and gastrin as well as improved parent-infant relationships.\n\nReviews of the literature have found no significant risks for adverse events with massage theory with either full term or pre-term infants. One study found that the use of certain oils in traditional societies such as mustard oil or olive oil might adversely affect pre-term newborn skin barrier function, while integrity and permeability while other oils that are linoleate-enriched such as sunflower seed oil may improve them.\n\n"}
{"id": "464877", "url": "https://en.wikipedia.org/wiki?curid=464877", "title": "Information management", "text": "Information management\n\nInformation management (IM) concerns a cycle of organizational activity: the acquisition of information from one or more sources, the custodianship and the distribution of that information to those who need it, and its ultimate disposition through archiving or deletion.\n\nThis cycle of organisational involvement with information involves a variety of stakeholders, including those who are responsible for assuring the quality, accessibility and utility of acquired information; those who are responsible for its safe storage and disposal; and those who need it for decision making. Stakeholders might have rights to originate, change, distribute or delete information according to organisational information management policies.\n\nInformation management embraces all the generic concepts of management, including the planning, organizing, structuring, processing, controlling, evaluation and reporting of information activities, all of which is needed in order to meet the needs of those with organisational roles or functions that depend on information. These generic concepts allow the information to be presented to the audience or the correct group of people. After individuals are able to put that information to use, it then gains more value.\n\nInformation management is closely related to, and overlaps with, the management of \"data\", \"systems\", \"technology\", \"processes\" and – where the availability of information is critical to organisational success – \"strategy\". This broad view of the realm of information management contrasts with the earlier, more traditional view, that the life cycle of managing information is an operational matter that requires specific procedures, organisational capabilities and standards that deal with information as a product or a service.\n\nIn the 1970s, the management of information largely concerned matters closer to what would now be called data management: punched cards, magnetic tapes and other record-keeping media, involving a life cycle of such formats requiring origination, distribution, backup, maintenance and disposal. At this time the huge potential of information technology began to be recognised: for example a single chip storing a whole book, or electronic mail moving messages instantly around the world, remarkable ideas at the time. With the proliferation of information technology and the extending reach of information systems in the 1980s and 1990s, information management took on a new form. Progressive businesses such as British Petroleum transformed the vocabulary of what was then \"IT management\", so that “systems analysts” became “business analysts”, “monopoly supply” became a mixture of “insourcing” and “outsourcing”, and the large IT function was transformed into “lean teams” that began to allow some agility in the processes that harness information for business benefit. The scope of senior management interest in information at British Petroleum extended from the creation of value through improved business processes, based upon the effective management of information, permitting the implementation of appropriate information systems (or “applications”) that were operated on IT infrastructure that was outsourced. In this way, information management was no longer a simple job that could be performed by anyone who had nothing else to do, it became highly strategic and a matter for senior management attention. An understanding of the technologies involved, an ability to manage information systems projects and business change well, and a willingness to align technology and business strategies all became necessary.\n\nIn the transitional period leading up to the strategic view of information management, Venkatraman (a strong advocate of this transition and transformation, proffered a simple arrangement of ideas that succinctly brought together the managements of data, information, and knowledge (see the figure)) argued that:\n\nThis is often referred to as the DIKAR model: Data, Information, Knowledge, Action and Result, it gives a strong clue as to the layers involved in aligning technology and organisational strategies, and it can be seen as a pivotal moment in changing attitudes to information management. The recognition that information management is an investment that must deliver meaningful results is important to all modern organisations that depend on information and good decision-making for their success.\n\nIt is commonly believed that good information management is crucial to the smooth working of organisations, and although there is no commonly accepted theory of information management \"per se\", behavioural and organisational theories help. Following the behavioural science theory of management, mainly developed at Carnegie Mellon University and prominently supported by March and Simon, most of what goes on in modern organizations is actually information handling and decision making. One crucial factor in information handling and decision making is an individual's ability to process information and to make decisions under limitations that might derive from the context: a person's age, the situational complexity, or a lack of requisite quality in the information that is at hand – all of which is exacerbated by the rapid advance of technology and the new kinds of system that it enables, especially as the social web emerges as a phenomenon that business cannot ignore. And yet, well before there was any general recognition of the importance of information management in organisations, March and Simon argued that organizations have to be considered as cooperative systems, with a high level of information processing and a vast need for decision making at various levels. Instead of using the model of the \"economic man\", as advocated in classical theory they proposed \"administrative man\" as an alternative, based on their argumentation about the cognitive limits of rationality. Additionally they proposed the notion of satisficing, which entails searching through the available alternatives until an acceptability threshold is met - another idea that still has currency.\n\nIn addition to the organisational factors mentioned by March and Simon, there are other issues that stem from economic and environmental dynamics. There is the cost of collecting and evaluating the information needed to take a decision, including the time and effort required. The transaction cost associated with information processes can be high. In particular, established organizational rules and procedures can prevent the taking of the most appropriate decision, leading to sub-optimum outcomes \n. This is an issue that has been presented as a major problem with bureaucratic organizations that lose the economies of strategic change because of entrenched attitudes.\n\nAccording to the Carnegie Mellon School an organization's ability to process information is at the core of organizational and managerial competency, and an organization's strategies must be designed to improve information processing capability and as information systems that provide that capability became formalised and automated, competencies were severely tested at many levels. It was recognised that organisations needed to be able to learn and adapt in ways that were never so evident before and academics began to organise and publish definitive works concerning the strategic management of information, and information systems. Concurrently, the ideas of business process management and knowledge management although much of the optimistic early thinking about business process redesign has since been discredited in the information management literature. In the strategic studies field, it is considered of the highest priority the understanding of the information environment, conceived as the aggregate of individuals, organizations, and systems that collect, process, disseminate, or act on information. This environment consists of three interrelated dimensions which continuously interact with individuals, organizations, and systems. These dimensions are the physical, informational, and cognitive.\n\nVenkatraman has provided a simple view of the requisite capabilities of an organisation that wants to manage information well – the DIKAR model (see above). He also worked with others to understand how technology and business strategies could be appropriately aligned in order to identify specific capabilities that are needed. This work was paralleled by other writers in the world of consulting, practice and academia.\n\nBytheway has collected and organised basic tools and techniques for information management in a single volume. At the heart of his view of information management is a portfolio model that takes account of the surging interest in external sources of information and the need to organise un-structured information external so as to make it useful (see the figure).\n\nSuch an information portfolio as this shows how information can be gathered and usefully organised, in four stages:\n\nStage 1: Taking advantage of public information: recognise and adopt well-structured external schemes of reference data, such as post codes, weather data, GPS positioning data and travel timetables, exemplified in the personal computing press.\n\nStage 2: Tagging the noise on the world wide web: use existing schemes such as post codes and GPS data or more typically by adding “tags”, or construct a formal ontology that provides structure. Shirky provides an overview of these two approaches.\n\nStage 3: Sifting and analysing: in the wider world the generalised ontologies that are under development extend to hundreds of entities and hundreds of relations between them and provide the means to elicit meaning from large volumes of data. Structured data in databases works best when that structure reflects a higher-level information model – an ontology, or an entity-relationship model.\n\nStage 4: Structuring and archiving: with the large volume of data available from sources such as the social web and from the miniature telemetry systems used in personal health management, new ways to archive and then trawl data for meaningful information. Map-reduce methods, originating from functional programming, are a more recent way of eliciting information from large archival datasets that is becoming interesting to regular businesses that have very large data resources to work with, but it requires advanced multi-processor resources.\n\nThe Information Management Body of Knowledge was made available on the world wide web in 2004 \nand sets out to show that the required management competencies to derive real benefits from an investment in information are complex and multi-layered. The framework model that is the basis for understanding competencies comprises six “knowledge” areas and four “process” areas:\nThe IMBOK is based on the argument that there are six areas of required management competency, two of which (“business process management” and “business information management”) are very closely related.\n\n\nEven with full capability and competency within the six knowledge areas, it is argued that things can still go wrong. The problem lies in the migration of ideas and information management value from one area of competency to another. Summarising what Bytheway explains in some detail (and supported by selected secondary references):\n\n\nThere are always many ways to see a business, and the information management viewpoint is only one way. It is important to remember that other areas of business activity will also contribute to strategy – it is not only good information management that moves a business forwards. Corporate governance, human resource management, product development and marketing will all have an important role to play in strategic ways, and we must not see one domain of activity alone as the sole source of strategic success. On the other hand, corporate governance, human resource management, product development and marketing are all dependent on effective information management, and so in the final analysis our competency to manage information well, on the broad basis that is offered here, can be said to be predominant.\n\nOrganizations are often confronted with many information management challenges and issues at the operational level, especially when organisational change is engendered. The novelty of new systems architectures and a lack of experience with new styles of information management requires a level of organisational change management that is notoriously difficult to deliver. As a result of a general organisational reluctance to change, to enable new forms of information management, there might be (for example): a shortfall in the requisite resources, a failure to acknowledge new classes of information and the new procedures that use them, a lack of support from senior management leading to a loss of strategic vision, and even political manoeuvring that undermines the operation of the whole organisation. However, the implementation of new forms of information management should normally lead to operational benefits.\n\nIn early work, taking an information processing view of organisation design, Jay Galbraith has identified five tactical areas to increase information processing capacity and reduce the need for information processing.\n\n\nThe lateral relations concept leads to an organizational form that is different from the simple hierarchy, the “matrix organization”. This brings together the vertical (hierarchical) view of an organisation and the horizontal (product or project) view of the work that it does visible to the outside world. The creation of a matrix organization is one management response to a persistent fluidity of external demand, avoiding multifarious and spurious responses to episodic demands that tend to be dealt with individually.\n\n\n"}
{"id": "30931040", "url": "https://en.wikipedia.org/wiki?curid=30931040", "title": "Institute for Food Safety and Health", "text": "Institute for Food Safety and Health\n\nThe Institute for Food Safety and Health (IFSH) is a research consortium consisting of the United States Food and Drug Administration’s Center for Food Safety and Applied Nutrition (FDA CFSAN), Illinois Institute of Technology (IIT) and the food industry. Under the cooperative agreement, the Institute was established by IIT to bring together the food safety and technology expertise of academia, industry and government as a consortium in the common goal of enhancing and improving the safety of food for U.S. consumers.\n\nThe Institute was established as the National Center for Food Safety and Technology (NCFST) in 1988 with a $6.9 million donation of seven buildings and grounds by Corn Products International to IIT in 1988, located in the IIT Moffett Campus. The Moffett Technical Center consists of office, laboratory and pilot plant space located in Bedford Park, IL in the near west suburbs of Chicago. It was built in 1948-49 and named in honor of George M. Moffett, then-president of Corn Products Refining Co.\n\nThe main building features bas relief murals created by architectural sculptor Lee Lawrie, who is best known for the \"Atlas\" statue in front of New York City’s Rockefeller Center. The panels depict various scenes of the history of corn and corn production in the Americas, including a buffalo hunt showing primitive corn grass, Native Americans cultivating and harvesting corn, and American corn milling and processing. One panel, labeled \"Argo,\" appears to be symbolic of the Moffett Technical Center itself, showing an engineer working in the pilot plant, a chemist with retorts and test tubes, and a scientist at a microscope.\n\nIn 2011, the NCFST was rebranded as the Institute for Food Safety and Health. The NCFST now operates as a center within the institute, along with new platforms such as the Center for Nutrition Research, Center for Processing Innovation, and the Center for Specialty Programs. IFSH has FDA personnel, IIT faculty and students, and more than 50 food industry-related partner companies.\" The institute is currently located in Bedford Park, IL, although it formerly identified as being located in Summit, IL.\n\nThe Moffett campus contains food microbiology, food chemistry and packaging, food process engineering, methods validation, and proficiency testing laboratories. The institute also contains a BSL-3 laboratory and biocontainment pilot plant, which is the only BSL-3 biocontainment plant in the United States. \n\nThe Department of Food Science and Nutrition offers certification in several topics, a doctorate degree, and masters degrees.\n"}
{"id": "9396764", "url": "https://en.wikipedia.org/wiki?curid=9396764", "title": "KC Space Pirates", "text": "KC Space Pirates\n\nThe KC Space Pirates is a team that competed in the 2006, 2007, and 2009 Space Elevator Games beamed energy climber competition and is planning to enter in the climber competition. The team is affiliated with, but is not sponsored by, the KC Robotics Society.\n\nThe competition is put on by the Spaceward Foundation. The goal of the competition is to encourage universities and groups to research and create designs for beaming power to distant objects, but for the competition Spaceward has used the Space Elevator concept to make it more challenging and to show how beamed power could work. NASA has put up the top prize of up to 2,000,000 ($900,000 for the 2 meters/second category and $1,100,000 for the 5 meters/second category) for the 2009 competition. The 2 meters/second prize was won during the 2009 competition so only the 5 meters/second category remains for 2010.\n\nThe competition is in the form of a race, 1 km (3,281 ft) straight up. The climbers are unmanned, have a maximum allowed weight of 25 kg (55 lbs), and may use no fuel or batteries to climb—they must only be powered by beamed energy. So far, the top designs have been reflected sunlight and laser. The KC Space Pirates used sunlight reflected off of a large array of mirrors concentrated onto a highly efficient array of solar cells in 2006 and 2007. They switched to using an infrared laser for the 2009 competition and will continue to do so in 2010.\n\nThe KC Space Pirates was the only 2009 team to have a fully automated laser tracking system. They did well in each competition but fell short of the money.\n\n"}
{"id": "51315186", "url": "https://en.wikipedia.org/wiki?curid=51315186", "title": "Lahore Knowledge Park", "text": "Lahore Knowledge Park\n\nLahore Knowledge Park is an under-construction science park located on a 852 acres located on Bedian Road in Lahore District, Pakistan.\n\nOwned and managed by the Lahore Knowledge Park Company with an initial investment of $1 billion, of which $200 million is invested by Government of Punjab. The project is designed by Frost & Sullivan. The park include's universities, science and innovation hubs, a retail and central business district, a residential district, an entertainment zone and green areas.\n\nPakistan Kidney and Liver Institute and Research Center is under construction in the park.\n\nThe park's masterplan divides it into five categories:\nAs of 2016, the COMSATS Institute of Information Technology and Lancaster University will jointly set up a graduate school in the park. Information Technology University will also move its campus to the park.\n"}
{"id": "38660691", "url": "https://en.wikipedia.org/wiki?curid=38660691", "title": "Laser-powered phosphor display", "text": "Laser-powered phosphor display\n\nLaser-powered phosphor display (LPD) is a large-format display technology similar to the cathode ray tube (CRT). Prysm, Inc., a video wall designer and manufacturer in Silicon Valley, California, invented and patented the LPD technology. The key components of the LPD technology are its TD2 tiles, its image processor and its backing frame that supports LPD tile arrays. The company unveiled the LPD in January 2010. \n\nThe concept behind the LPD technology is quite simple. LPD uses a set of movable mirrors to direct several beams of light from several ultra-violet lasers onto a screen made of a plastic-glass hybrid material coated with color phosphor stripes. The laser draws an image onto the screen by scanning line by line from top to bottom. The energy from the lasers' light activates the phosphors, which emit photons, producing an image.\n\nThe building blocks of every Prysm video wall are the Laser Phosphor Display (LPD) tiles called the TD2. Video walls are implemented using this new generation LPD TD2 tile, a virtually seamless, bezel-free building block. TD2, launched at InfoComm 2013, features increased resolution, brightness and enhanced uniformity. A variable number of TD2 tiles can be arranged in arbitrary configurations to form videowalls in various sizes and shapes.\n\nThe main difference between the LPD and CRT technologies is that the first relies on laser whereas the second uses an electron gun to activate the phosphorescent substance that creates images. \n\nAnother competitor, plasma display technology, consists of small cells of ionized gases that emit light–a process that requires a relatively large amount of power. And a conventional laser television, such as the LaserVue, made by Mitsubishi, uses red, blue, and green lasers and a micromirror device that combines and directs the light. This is essentially a rear-projection display that wasn't popular due to cost.\n\nLPD requires less electricity than competing technologies including LCD and light-emitting diode (LED).IAC reported a 70% reduction in power by switching to LPD, and Prysm says LPD uses up to 75 percent less power than most other display technologies on the market. An LPD device differs significantly from LCD in that more than 90 percent of the original light is lost in the latter process. \n\nThe TD2, building block of a video wall, does not suffer the problem of low brightness, contains no toxic component, has no consumables, and generates little heat. Its displays are highly configurable and can be stacked seamlessly to create supersized high-resolution video walls of almost any size or shape. \n\nAccording to Prysm, the LPD technology has other advantages including great black levels, a wide 180-degree viewing angle, a 65,000-hour panel life with no burn-in issues, completely recyclable components, and their production process is mercury free.[*]\n\nLPD competes with liquid crystal display (LCD), plasma display panel (PDP), surface electron display (SED) and other large-format display technologies.\n\nOne disadvantage of LPD is that the displays are deeper than some competing technologies, \"each TD1 Tile including all peripherals measures almost 17 inches deep\". Depending on the frame type, the total installed depth varies between 24 and 30 inches.\n\nThe earliest embodiment of this technology, the TD1 tile was launched in June 2010. Prysm began shipping the TD1 tiles in February 2011. \n\nLPD powered by the Prysm \"digital workplace platform\" software, is used as a giant touchscreen display, a digital signage and in customer experience centers. The first LPD retail installation went on display at American Eagle Outfitters in New York in late 2010. Other LPD deployments include a 120 feet long by 10 feet tall videowall at media company InterActiveCorp (IAC)'s headquarters building in New York in New York City, a 40-foot, 180-degree, interactive videowall at General Electric’s (GE) Customer Experience Center in Toronto and television studios, and several videowalls for venues including Dubai TV and Sprint. Prysm digital workplace platform is a shared cloud workspace where multiple users can upload and view videos, documents, presentations and other media.\n\n\n\n"}
{"id": "9454493", "url": "https://en.wikipedia.org/wiki?curid=9454493", "title": "Licensing Executives Society International", "text": "Licensing Executives Society International\n\nThe Licensing Executives Society International, or LES International (LESI, or formally \"LES International, Inc.\"), is a not for profit, non-political, umbrella organisation having 32 national and regional member societies, interested in technology transfers or licensing of intellectual property rights. It was founded in 1965 in the United States. As of 2011, LESI had \"more than 10,000 individual members, including representatives of companies, scientists, engineers, academicians, governmental officials, lawyers, patent and trademark attorneys, and consultants\".\n\nLESI publishes a quarterly journal called \"les Nouvelles\". In 2011, LESI launched an initiative to establish international standards for the valuation of intellectual property.\n"}
{"id": "5935171", "url": "https://en.wikipedia.org/wiki?curid=5935171", "title": "Linjesender", "text": "Linjesender\n\nA linjesender (English: \"line transmitter\") was a low-power longwave transmitter system used for broadcasting in Norway. It consisted of a power line communication system, which fed the radio programme on a frequency in the longwave broadcasting range into domestic powerlines.\n\nThe last linjesender in Norway was closed in 1987 although the Swiss counterpart survived another ten years.\n\nThe typical powers used by linjesenders were between 250 watts and 2 kW. Most systems used frequencies in the longwave band or in between the LW and MW band although some used medium wave or frequencies below the standard LW band which required special receivers.\n\nWired broadcasting had several advantages over conventional broadcasting: \n\nOn the other hand there were practical and economic difficulties in extending such services to remote or thinly populated regions. Wired broadcasting could also be used by governments as a tool of censorship through promoting ownership of wire-only receivers which could not receive foreign stations.\n\nSimilar systems were used in Germany, where it was called \"Drahtfunk\" (\"wire radio\") and in Switzerland, where it was called \"Telefonrundspruch\" (\"telephone broadcast\"), both of these systems used domestic telephone lines. In some countries occupied by Germany during the Second World War these systems entirely replaced conventional broadcasting. In the Netherlands all standard receivers were confiscated and replaced with wire-only sets as these could receive local and German broadcasts but not enemy stations such as the BBC.\n\nIn the 1930s some towns in Great Britain used wire broadcasting experimentally either over dedicated cables (sometimes as baseband audio) or through power lines. However as the coverage of conventional broadcast stations improved the popularity of these \"radio relay\" or \"rediffusion\" systems waned and local councils were often hostile to their installation.\n\n\n"}
{"id": "20983659", "url": "https://en.wikipedia.org/wiki?curid=20983659", "title": "List of Japanese inventions and discoveries", "text": "List of Japanese inventions and discoveries\n\nThis is a list of Japanese inventions and discoveries. The Japanese have made contributions across a number of scientific and technological domains. In particular, the country has played a crucial role in the digital revolution since the 20th century, with many modern revolutionary and widespread technologies in fields such as electronics and robotics introduced by Japanese inventors and entrepreneurs. Japanese popular culture, strongly shaped by its electronic technologies, commands considerable influence around the world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChindōgu is the Japanese art of inventing ingenious everyday gadgets that, on the face of it, seem like an ideal solution to a particular problem. However, Chindōgu has a distinctive feature: anyone actually attempting to use one of these inventions would find that it causes so many new problems, or such significant social embarrassment, that effectively it has no utility whatsoever. Thus, Chindōgu are sometimes described as \"unuseless\" – that is, they cannot be regarded as 'useless' in an absolute sense, since they do actually solve a problem; however, in practical terms, they cannot positively be called \"useful.\" The term \"Chindōgu\" was coined by Kenji Kawakami.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "9840203", "url": "https://en.wikipedia.org/wiki?curid=9840203", "title": "List of longest runways", "text": "List of longest runways\n\n\n\n"}
{"id": "635166", "url": "https://en.wikipedia.org/wiki?curid=635166", "title": "Mill town", "text": "Mill town\n\nA mill town, also known as factory town or mill village, is typically a settlement that developed around one or more mills or factories, usually cotton mills or factories producing textiles.\n\nIn the United Kingdom, the term \"mill town\" usually refers to the 19th century textile manufacturing towns of northern England and the Scottish Lowlands, particularly those in Lancashire (cotton) and Yorkshire (wool).\n\nSome former mill towns have a symbol of the textile industry in their town badge. Some towns may have statues dedicated to textile workers (e.g. Colne) or have a symbol in the badge of local schools (e.g. Ossett School).\n\nThe list below includes some towns where textiles was not the predominant industry. For example, mining was a key industry in Wigan and Leigh in Greater Manchester, and in Ossett in Yorkshire.\n\nOn his tour of northern England in 1849, irish\n\nScottish publisher Angus Reach said:\n\nThe town grew out of a textile factory founded in 1833 by the sons of Feliks Lubienski, who owned the land where it was built. They brought in a specialist from France and his newly designed machines. He was French inventor, Philippe de Girard from Lourmarin. He became a director of the firm. The factory town developed during the 19th century into a significant textile mill town in Poland.\nIn honour of Girard, 'Ruda Guzowska' as the original estate was called, was renamed Żyrardów, a toponym derived of the polonised spelling of Girard's name.\n\nMost of Żyrardów's monuments are located in the manufacturing area which dates from the 19th and early 20th centuries. It is widely believed that Żyrardów's textile settlement is the only entire urban industrial complex from the 19th-century to be preserved in Europe.\n\nBeginning with Samuel Slater and technological information smuggled out of England by Francis Cabot Lowell, large mills were established in New England in the early to mid 19th century. Mill towns, sometimes planned, built and owned as a company town, grew in the shadow of the industries. The region became a manufacturing powerhouse along rivers like the Housatonic, Quinebaug, Shetucket, Blackstone, Merrimack, Nashua, Cocheco, Saco, Androscoggin, Kennebec or Winooski.\n\nIn the 20th century, alternatives to water power were developed, and it became more profitable for companies to manufacture textiles in southern states where cotton was grown and winters did not require significant heating costs. Finally, the Great Depression acted as a catalyst that sent several struggling New England firms into bankruptcy.\n\n\n\n"}
{"id": "31425173", "url": "https://en.wikipedia.org/wiki?curid=31425173", "title": "Monome", "text": "Monome\n\nMonome is a family of interface devices for computers made by an Upstate New York company of the same name. Brian Crabtree and his partner Kelli Cain are credited for Monome's creation.\n\nThe Monome has a minimalist design, and has been complimented for its interface design. It is a box with a grid of back-lit buttons, with no labels or icons. No screws are visible on any surface of the Monome. The box that holds the Monome is made entirely of wood, usually walnut, with a clear aluminum top plate. There are three different size options for the Monome: sixteen by sixteen, sixteen by eight, and eight by eight.\n\nMonome devices do not produce any sound on their own; they must be connected to a computer via USB, in which an app affords use to the device. The creators of Monome said: \"The wonderful thing about this device is that is doesn't do anything really... It wasn't intended for any specific application. We'll make several applications, and others will make more. We hope to share as many of these as possible.\" A core design principle of the Monome is that it is not intended for any one specific application — the function of each button and the decision as to which lights are lit are completely up to the software communicating with the device over the Open Sound Control protocol. The Monome is not strictly a musical device. Depending the software used, the Monome can function as anything from a sample cutter to a math machine.\n\nSince 2006, several models have been produced, with typical sizes ranging from 64 to 256 buttons — plus a very limited run of 512-button devices.\n\nIn 2011, the first non-grid controller in the Monome family was introduced, the Monome Arc. The Monome Arc consists of two aluminum knobs mounted on a rectangular walnut box. Each knob is surrounded by a ring of 64 LED lights, and similarly to the Monome, affords function only by USB connection to an application. Some critics of the Arc have said it should be positioned as a complement to the Monome rather than a \"complete control paradigm\" that can stand alone, and that its overly-simplistic design can be seen as a limiting factor for independent use.\n\nIn September 2013, Monome introduced another open-sourced controller, Aleph. Aleph was designed and engineered by original Monome creator Brian Crabtree and Ezra Buchla, musician and son of Don Buchla. The Aleph differs greatly from the minimalist Monome and Arc machines. Aleph contains four digital inputs and outputs, four control voltage inputs and outputs, multiple optical encoders, a display screen and USB - all of which can be completely reprogrammed at the software level. Full release and distribution was planned for late Fall 2013.\n\nSeveral applications provide sample sequencing capabilities. One such application is MLR, an application that allows for live sequencing and re-cutting of samples. There are also many applications that allow for synthesis either via their own internal synthesizers or by sending MIDI/OSC messages to external synthesizers.\n\nDespite being produced irregularly in small quantities since its introduction in 2006, the Monome button-grid controller has had a significant impact on electronic music. Together with the physically similar Yamaha Tenori-On, which was released a year later in 2007, the Monome inspired interest in minimalist, grid-based music controllers throughout the industry. That interest spawned hobbyist projects like the Arduinome and commercial products like the Akai APC40, the Novation Launchpad, and the Livid Instruments Block and Ohm64. GridFest, a festival for Monome enthusiasts, took place in New Mexico in May 2011, following the release of the Monome Arc earlier that year.\n\n\n\n"}
{"id": "56283993", "url": "https://en.wikipedia.org/wiki?curid=56283993", "title": "Octagon Systems", "text": "Octagon Systems\n\nOctagon Systems Corporation is an industrial computer design and manufacturing company based in Westminster, Colorado. Octagon Systems designs, manufactures, sells, repairs and supports its line of industrial, mobile and rugged computer systems for industries including mining, military, transportation and others. The company has international representatives in Africa, Asia, Europe, North America and South America.\nOctagon Systems was founded in 1981 and introduced an embedded computer with a high level language and software development system and operating systems on a solid state disk. Octagon’s services and systems grew with new industrial computer system solutions including use in the STD Bus market and development of single-board computers.\n\nOctagon Systems co-authored the EPIC (form factor). EPIC™ embedded computing specification that became a world standard. Growing applicational use of Octagon’s products led to use in areas such as public transportation systems, rugged computing systems for mining operations as well as others.\nOctagon Systems products expanded into new markets as the use of industrial, transportation and rugged computer systems became increasingly common for a wide array of applications. The U.S. Navy chose Octagon’ products for a major long term contract to support amphibious warfare computing needs and Octagon products have been deployed in more than 100 mines worldwide.\n\nOctagon Systems’ XMB Mobile Servers won the Flagship Product award from COTS Journal - The Journal of Military Electronics & Computing in 2006. \nOctagon Systems has been ISO certified since 1993.\nOctagon Systems was a founding member of the Small Form Factor Special Interest Group in 2007.\nOctagon Systems co-authored the EPIC (form factor) or EPIC™ embedded computing specification.\n"}
{"id": "25960463", "url": "https://en.wikipedia.org/wiki?curid=25960463", "title": "Pipeline bridge", "text": "Pipeline bridge\n\nA pipeline bridge is a bridge for running a pipeline over a river or another obstacle. Pipeline bridges for liquids and gases are, as a rule, only built when it is not possible to run the pipeline on a conventional bridge or under the river. However, as it is more common to run pipelines for centralized heating systems overhead, for this application even small pipeline bridges are common.\n\nAs there is normally a steady flow in pipelines, they can be designed as suspension bridges.\n\nA pipeline bridge may be equipped with a walkway for maintenance purposes but, in most cases, this is not open for public access for safety and security reasons.\n"}
{"id": "49401671", "url": "https://en.wikipedia.org/wiki?curid=49401671", "title": "QCT (Quanta Cloud Technology)", "text": "QCT (Quanta Cloud Technology)\n\nQCT is a provider of data center hardware and cloud solutions that are used by hyperscale data center operators, such as Facebook and Rackspace.\n\nIn conjunction with its parent company, Quanta Computer, QCT sells approximately one out of every seven server manufactured in the world.\n\nQCT is an early contributor to the Open Compute Project, a non-profit organization launched by Facebook engineers to open source the designs of data center products. QCT manufactured the Facebook servers that were among the first designs under the project.\n\nIn addition to its headquarters in Taoyuan, Taiwan, QCT has offices in San Jose, Seattle, Beijing, Hangzhou and Tokyo.\n\nQCT is a subsidiary of Quanta Computer Inc., a Fortune Global 500 technology engineering and manufacturing company. Quanta Computer, an original design manufacturer founded in 1988. In May 2012, Quanta Computer launched QCT to offer data center products and services under its own brands.\n\n\n\nMike Yang serves as general manager of QCT.\n\nQCT received the \"Editors’ Choice Award: Top 5 Vendors to Watch in 2014\" in HPCwire Readers’ and Editors’ Choice Awards\n\n\n\n\n\n\n"}
{"id": "16326506", "url": "https://en.wikipedia.org/wiki?curid=16326506", "title": "Raising of Chicago", "text": "Raising of Chicago\n\nDuring the 1850s and 1860s, engineers carried out a piecemeal raising of the level of central Chicago. Streets, sidewalks, and buildings were physically raised on hydraulic jacks or jackscrews. The work was funded by private property owners and public funds.\n\nDuring the 19th century, the elevation of the Chicago area was not much higher than the shorelines of Lake Michigan, so for many years there was little or no naturally occurring drainage from the city surface. The lack of drainage caused unpleasant living conditions, and standing water harbored pathogens that caused numerous epidemics. Epidemics including typhoid fever and dysentery blighted Chicago six years in a row culminating in the 1854 outbreak of cholera that killed six percent of the city’s population.\n\nThe crisis forced the city's engineers and aldermen to take the drainage problem seriously and after many heated discussions—and following at least one false start—a solution eventually materialized. In 1856, engineer Ellis S. Chesbrough drafted a plan for the installation of a citywide sewerage system and submitted it to the Common Council, which adopted the plan. Workers then laid drains, covered and refinished roads and sidewalks with several feet of soil, and raised most buildings to the new grade with hydraulic jacks.\n\nIn January 1858, the first masonry building in Chicago to be thus raised—a four-story, long, 750-ton brick structure situated at the north-east corner of Randolph Street and Dearborn Street—was lifted on two hundred jackscrews to its new grade, which was higher than the old one, “without the slightest injury to the building.” It was the first of more than fifty comparably large masonry buildings to be raised that year. The contractor was Bostonian engineer, James Brown, who went on to partner with longtime Chicago engineer James Hollingsworth; Brown and Hollingsworth became the first and, it seems, the busiest building raising partnership in the city. Before the year was out, they were lifting brick buildings more than long, and the following spring they took the contract to raise a brick block more than twice that length.\n\nBy 1860, confidence was sufficiently high that a consortium of no fewer than six engineers—including Brown, Hollingsworth and George Pullman—took on one of the most impressive locations in the city and hoisted it up complete and in one go. They lifted half a city block on Lake Street, between Clark Street and LaSalle Street; a solid masonry row of shops, offices, printeries, etc., long, comprising brick and stone buildings, some four stories high, some five, having a footprint taking up almost of space, and an estimated all in weight including hanging sidewalks of thirty five thousand tons. Businesses operating out of these premises were not closed down for the lifting; as the buildings were being raised, people came, went, shopped and worked in them as if nothing out of the ordinary were happening. In five days the entire assembly was elevated in the air by a team consisting of six hundred men using six thousand jackscrews, ready for new foundation walls to be built underneath. The spectacle drew crowds of thousands, who were on the final day permitted to walk at the old ground level, among the jacks.\n\nThe following year a team led by Ely, Smith, and Pullman raised the Tremont House hotel on the south-east corner of Lake Street and Dearborn Street. This building was luxuriously appointed, was of brick construction, was six stories high, and had a footprint taking up over of space. Once again business as usual was maintained as this vast hotel parted from the ground it was standing on, and indeed some of the guests staying there at the time—among whose number were several VIPs and a US Senator—were completely oblivious to the feat as the five hundred men operating their five thousand jackscrews worked under covered trenches. One patron was puzzled to note that the front steps leading from the street into the hotel were becoming steeper every day and that when he checked out, the windows were several feet above his head, whereas before they had been at eye level. This huge hotel, which until just the previous year had been the tallest building in Chicago, was in fact raised fully without a hitch.\n\nAnother notable feat was the raising of the Robbins Building, an iron building long, wide and five stories high, located at the corner of South Water Street and Wells Street. This was a very heavy building; its ornate iron frame, its twelve-inch (305 mm) thick masonry wall filling, and its “floors filled with heavy goods” made for a weight estimated at 27,000 tons, a large load to raise over a relatively small area. Hollingsworth and Coughlin took the contract and in November 1865 lifted not only the building but also the of stone sidewalk outside it. The complete mass of iron and masonry was raised , “without the slightest crack or damage.”\n\nThere is evidence in primary document sources that at least one building in Chicago, the Franklin House on Franklin Street, was raised hydraulically by the engineer John C. Lane, of the Lane and Stratton partnership. The team had apparently been using this method of lifting buildings in San Francisco since 1853.\n\nMany of central Chicago’s hurriedly-erected wooden frame buildings were now considered wholly inappropriate to the burgeoning and increasingly wealthy city. Rather than raise them several feet, proprietors often preferred to relocate these old frame buildings, replacing them with new masonry blocks built to the latest grade. Consequently, the practice of putting the old multi-story, intact and furnished wooden buildings—sometimes entire rows of them \"en bloc\"—on rollers and moving them to the outskirts of town or to the suburbs was so common as to be considered nothing more than routine traffic. Traveller David Macrae wrote incredulously, “Never a day passed during my stay in the city that I did not meet one or more houses shifting their quarters. One day I met nine. Going out Great Madison Street in the horse cars we had to stop twice to let houses get across.” As discussed above, business did not suffer; shop owners would keep their shops open, even as people had to climb in through a moving front door. Brick buildings also were moved from one location to another, and in 1866, the first of these—a building of two and a half stories—made the short move from Madison Street out to Monroe Street. Later, many other brick buildings were rolled much greater distances across Chicago.\n\n\n"}
{"id": "57917666", "url": "https://en.wikipedia.org/wiki?curid=57917666", "title": "SIRE Radar", "text": "SIRE Radar\n\nThe Synchronous Impulse Reconstruction (SIRE) radar is a multiple-input, multiple-output (MIMO) radar system designed by the Army Research Laboratory (ARL) to detect landmines and improvised explosive devices (IEDs). It consists of a low frequency, impulse-based ultra-wideband (UWB) radar that uses 16 receivers with 2 transmitters at the ends of the 2 meter-wide receive array that send alternating, orthogonal waveforms into the ground and return signals reflected from targets in a given area. The SIRE radar system comes mounted on top of a vehicle and receives signals that form images that uncover up to 33 meters in the direction that the transmitters are facing. It is able to collect and process data as part of an affordable and lightweight package due to slow (40 MHz) yet inexpensive analog-to-digital (A/D) converters that sample the wide bandwidth of radar signals. It uses a GPS and Augmented Reality (AR) technology in conjunction with camera to create a live video stream with a more comprehensive visual display of the targets. \n\nThe SIRE radar functions primarily as a method of assessing the surrounding environment and determining whether the path being traversed is safe for vehicular navigation. In general, radar systems have an advantage over optical or laser sensor system because they are not hindered by the presence of fog or dust blocking their line of sight. However, most radar systems use high-frequency microwave radiation, which have difficulty penetrating grass and other foliage. In contrast, the SIRE radar can penetrate foliage, various media, and even the ground to detect hidden or buried IEDs due to its use of low-frequency microwave radiation.\n\nThe data acquisition cycle for the SIRE radar consist of the following steps:\n\n\nThe transmitters used in the SIRE radar are transversal electromagnetic (TEM) horns that generate short, 1 nanosecond-long radar pulses with a pulse repetition frequency (PRF) of 1 MHz and a frequency band from 300 to 2500 MHz. The peak power output for the transmitter is 6 watts, while the average power is 5 watts to reduce interference potential. The TEM horns can handle a 200 ohm characteristic impedance and were chosen since they provide good pulse fidelity and low reflected power. The two transmitters alternate in activity with each cycle of the data acquisition process.\n\nThe receivers used in the SIRE radar are Vivaldi notch antennas which are arranged in a uniform linear array that span the width of the vehicle. Each receiver is connected to a separate receiver channel. The imaging method relies on the back-projection algorithm, where the data from all 16 receiver channels are integrated at successive ranges as the vehicle moves forwards.\n\nIn order to prevent radio-frequency interference (RFI) from outside sources, such as radio, TV and wireless communication signals, in the radio frequency band, the SIRE radar employs several techniques to suppress or extract these signals from the UWB radar data. Instead of conventional methods of screening like the notch filtering approach, the SIRE radar narrowband and wideband RFI screening process involve averaging repeated measurements from the same range-profile.\n\nThe mounted SIRE radar system comes in two modes depending on its orientation on top of the vehicle. The most commonly used mode is the forward-looking mode, where the radar faces towards the front of the vehicle in the direction it is traveling. An alternative is the side-looking mode, where the antenna frame that supports the SIRE radar system is rotated 90 degrees and the direction of the radar is perpendicular to the path of the vehicle. The side-looking mode is designed to survey the area behind walls and map the interior of enclosed buildings.\n"}
{"id": "13690575", "url": "https://en.wikipedia.org/wiki?curid=13690575", "title": "Solar power", "text": "Solar power\n\nSolar power is the conversion of energy from sunlight into electricity, either directly using photovoltaics (PV), indirectly using concentrated solar power, or a combination. Concentrated solar power systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Photovoltaic cells convert light into an electric current using the photovoltaic effect.\n\nPhotovoltaics were initially solely used as a source of electricity for small and medium-sized applications, from the calculator powered by a single solar cell to remote homes powered by an off-grid rooftop PV system. Commercial concentrated solar power plants were first developed in the 1980s. The 392 MW Ivanpah installation is the largest concentrating solar power plant in the world, located in the Mojave Desert of California.\n\nAs the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and utility-scale solar power stations with hundreds of megawatts are being built. Solar PV is rapidly becoming an inexpensive, low-carbon technology to harness renewable energy from the Sun. The current largest photovoltaic power station in the world is the 850 MW Longyangxia Dam Solar Park, in Qinghai, China.\n\nThe International Energy Agency projected in 2014 that under its \"high renewables\" scenario, by 2050, solar photovoltaics and concentrated solar power would contribute about 16 and 11 percent, respectively, of the worldwide electricity consumption, and solar would be the world's largest source of electricity. Most solar installations would be in China and India. In 2017, solar power provided 1.7% of total worldwide electricity production, growing at 35% per annum.\n\nMany industrialized nations have installed significant solar power capacity into their grids to supplement or provide an alternative to conventional energy sources while an increasing number of less developed nations have turned to solar to reduce dependence on expensive imported fuels \"(see solar power by country)\". Long distance transmission allows remote renewable energy resources to displace fossil fuel consumption. Solar power plants use one of two technologies:\n\nA solar cell, or photovoltaic cell (PV), is a device that converts light into electric current using the photovoltaic effect. The first solar cell was constructed by Charles Fritts in the 1880s. The German industrialist Ernst Werner von Siemens was among those who recognized the importance of this discovery. In 1931, the German engineer Bruno Lange developed a photo cell using silver selenide in place of copper oxide, although the prototype selenium cells converted less than 1% of incident light into electricity. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the silicon solar cell in 1954. These early solar cells cost 286 USD/watt and reached efficiencies of 4.5–6%.\n\nThe array of a photovoltaic power system, or PV system, produces direct current (DC) power which fluctuates with the sunlight's intensity. \nFor practical use this usually requires conversion to certain desired voltages or alternating current (AC), through the use of inverters. \nMultiple solar cells are connected inside modules. \nModules are wired together to form arrays, then tied to an inverter, which produces power at the desired voltage, and for AC, the desired frequency/phase.\n\nMany residential PV systems are connected to the grid wherever available, especially in developed countries with large markets. \nIn these grid-connected PV systems, use of energy storage is optional. \nIn certain applications such as satellites, lighthouses, or in developing countries, batteries or additional power generators are often added as back-ups. Such stand-alone power systems permit operations at night and at other times of limited sunlight.\n\nConcentrated solar power (CSP), also called \"concentrated solar thermal\", uses lenses or mirrors and tracking systems to concentrate sunlight, then use the resulting heat to generate electricity from conventional steam-driven turbines.\n\nA wide range of concentrating technologies exists: among the best known are the parabolic trough, the compact linear Fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage. Thermal storage efficiently allows up to 24-hour electricity generation.\n\nA \"parabolic trough\" consists of a linear parabolic reflector that concentrates light onto a receiver positioned along the reflector's focal line. The receiver is a tube positioned along the focal points of the linear parabolic mirror and is filled with a working fluid. The reflector is made to follow the sun during daylight hours by tracking along a single axis. Parabolic trough systems provide the best land-use factor of any solar technology. The SEGS plants in California and Acciona's Nevada Solar One near Boulder City, Nevada are representatives of this technology.\n\n\"Compact Linear Fresnel Reflectors\" are CSP-plants which use many thin mirror strips instead of parabolic mirrors to concentrate sunlight onto two tubes with working fluid. This has the advantage that flat mirrors can be used which are much cheaper than parabolic mirrors, and that more reflectors can be placed in the same amount of space, allowing more of the available sunlight to be used. Concentrating linear fresnel reflectors can be used in either large or more compact plants.\n\nThe \"Stirling solar dish\" combines a parabolic concentrating dish with a Stirling engine which normally drives an electric generator. The advantages of Stirling solar over photovoltaic cells are higher efficiency of converting sunlight into electricity and longer lifetime.\nParabolic dish systems give the highest efficiency among CSP technologies. \nThe 50 kW Big Dish in Canberra, Australia is an example of this technology.\n\nA \"solar power tower\" uses an array of tracking reflectors (heliostats) to concentrate light on a central receiver atop a tower. Power towers can achieve higher (thermal-to-electricity conversion) efficiency than linear tracking CSP schemes and better energy storage capability than dish stirling technologies. The PS10 Solar Power Plant and PS20 solar power plant are examples of this technology.\n\nA hybrid system combines (C)PV and CSP with one another or with other forms of generation such as diesel, wind and biogas. The combined form of generation may enable the system to modulate power output as a function of demand or at least reduce the fluctuating nature of solar power and the consumption of non renewable fuel. Hybrid systems are most often found on islands.\n\nThe idea is to increase the efficiency of the combined solar/thermoelectric system to convert the solar radiation into useful electricity.\n\n</div>\n\nThe early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. Charles Fritts installed the world's first rooftop photovoltaic solar array, using 1%-efficient selenium cells, on a New York City roof in 1884. However, development of solar technologies stagnated in the early 20th century in the face of the increasing availability, economy, and utility of coal and petroleum. In 1974 it was estimated that only six private homes in all of North America were entirely heated or cooled by functional solar power systems. The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world and brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the United States (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer–ISE). \nBetween 1970 and 1983 installations of photovoltaic systems grew rapidly, but falling oil prices in the early 1980s moderated the growth of photovoltaics from 1984 to 1996.\n\nIn the mid-1990s, development of both, residential and commercial rooftop solar as well as utility-scale photovoltaic power stations, began to accelerate again due to supply issues with oil and natural gas, global warming concerns, and the improving economic position of PV relative to other energy technologies. In the early 2000s, the adoption of feed-in tariffs—a policy mechanism, that gives renewables priority on the grid and defines a fixed price for the generated electricity—led to a high level of investment security and to a soaring number of PV deployments in Europe.\n\nFor several years, worldwide growth of solar PV was driven by European deployment, but has since shifted to Asia, especially China and Japan, and to a growing number of countries and regions all over the world, including, but not limited to, Australia, Canada, Chile, India, Israel, Mexico, South Africa, South Korea, Thailand, and the United States.\n\nWorldwide growth of photovoltaics has averaged 40% per year from 2000 to 2013 and total installed capacity reached 303 GW at the end of 2016 with China having the most cumulative installations (78 GW) and Honduras having the highest theoretical percentage of annual electricity usage which could be generated by solar PV (12.5%). The largest manufacturers are located in China.\n\nConcentrated solar power (CSP) also started to grow rapidly, increasing its capacity nearly tenfold from 2004 to 2013, albeit from a lower level and involving fewer countries than solar PV. \nAs of the end of 2013, worldwide cumulative CSP-capacity reached 3,425 MW.\n\nIn 2010, the International Energy Agency predicted that global solar PV capacity could reach 3,000 GW or 11% of projected global electricity generation by 2050—enough to generate 4,500 TWh of electricity. \nFour years later, in 2014, the agency projected that, under its \"high renewables\" scenario, solar power could supply 27% of global electricity generation by 2050 (16% from PV and 11% from CSP).\n\nThe Desert Sunlight Solar Farm is a 550 MW power plant in Riverside County, California, that uses thin-film CdTe-modules made by First Solar. As of November 2014, the 550 megawatt Topaz Solar Farm was the largest photovoltaic power plant in the world. This was surpassed by the 579 MW Solar Star complex. The current largest photovoltaic power station in the world is Longyangxia Dam Solar Park, in Gonghe County, Qinghai, China.\n\nCommercial concentrating solar power (CSP) plants, also called \"solar thermal power stations\", were first developed in the 1980s. The 377 MW Ivanpah Solar Power Facility, located in California's Mojave Desert, is the world’s largest solar thermal power plant project. Other large CSP plants include the Solnova Solar Power Station (150 MW), the Andasol solar power station (150 MW), and Extresol Solar Power Station (150 MW), all in Spain. The principal advantage of CSP is the ability to efficiently add thermal storage, allowing the dispatching of electricity over up to a 24-hour period. Since peak electricity demand typically occurs at about 5 pm, many CSP power plants use 3 to 5 hours of thermal storage.\n\nThe typical cost factors for solar power include the costs of the modules, the frame to hold them, wiring, inverters, labour cost, any land that might be required, the grid connection, maintenance and the solar insolation that location will receive. Adjusting for inflation, it cost $96 per watt for a solar module in the mid-1970s. Process improvements and a very large boost in production have brought that figure down to 68 cents per watt in February 2016, according to data from Bloomberg New Energy Finance. Palo Alto California signed a wholesale purchase agreement in 2016 that secured solar power for 3.7 cents per kilowatt-hour. And in sunny Dubai large-scale solar generated electricity sold in 2016 for just 2.99 cents per kilowatt-hour – \"competitive with any form of fossil-based electricity — and cheaper than most.\"\n\nPhotovoltaic systems use no fuel, and modules typically last 25 to 40 years. Thus, capital costs make up most of the cost of solar power. Operations and maintenance costs for new utility-scale solar plants in the US are estimated to be 9 percent of the cost of photovoltaic electricity, and 17 percent of the cost of solar thermal electricity. Governments have created various financial incentives to encourage the use of solar power, such as feed-in tariff programs. Also, Renewable portfolio standards impose a government mandate that utilities generate or acquire a certain percentage of renewable power regardless of increased energy procurement costs. In most states, RPS goals can be achieved by any combination of solar, wind, biomass, landfill gas, ocean, geothermal, municipal solid waste, hydroelectric, hydrogen, or fuel cell technologies.\n\nThe PV industry is beginning to adopt levelized cost of electricity (LCOE) as the unit of cost. The electrical energy generated is sold in units of kilowatt-hours (kWh). As a rule of thumb, and depending on the local insolation, 1 watt-peak of installed solar PV capacity generates about 1 to 2 kWh of electricity per year. This corresponds to a capacity factor of around 10–20%. The product of the local cost of electricity and the insolation determines the break even point for solar power. The International Conference on Solar Photovoltaic Investments, organized by EPIA, has estimated that PV systems will pay back their investors in 8 to 12 years. As a result, since 2006 it has been economical for investors to install photovoltaics for free in return for a long term power purchase agreement. Fifty percent of commercial systems in the United States were installed in this manner in 2007 and over 90% by 2009.\n\nShi Zhengrong has said that, as of 2012, unsubsidised solar power is already competitive with fossil fuels in India, Hawaii, Italy and Spain. He said \"We are at a tipping point. No longer are renewable power sources like solar and wind a luxury of the rich. They are now starting to compete in the real world without subsidies\". \"Solar power will be able to compete without subsidies against conventional power sources in half the world by 2015\".\n\nIn its 2014 edition of the \"Technology Roadmap: Solar Photovoltaic Energy\" report, the International Energy Agency (IEA) published prices for residential, commercial and utility-scale PV systems for eight major markets as of 2013 \"(see table below)\". However, DOE's SunShot Initiative has reported much lower U.S. installation prices. In 2014, prices continued to decline. The SunShot Initiative modeled U.S. system prices to be in the range of $1.80 to $3.29 per watt. Other sources identify similar price ranges of $1.70 to $3.50 for the different market segments in the U.S., and in the highly penetrated German market, prices for residential and small commercial rooftop systems of up to 100 kW declined to $1.36 per watt (€1.24/W) by the end of 2014. In 2015, Deutsche Bank estimated costs for small residential rooftop systems in the U.S. around $2.90 per watt. Costs for utility-scale systems in China and India were estimated as low as $1.00 per watt.\n\nGrid parity, the point at which the cost of photovoltaic electricity is equal to or cheaper than the price of grid power, is more easily achieved in areas with abundant sun and high costs for electricity such as in California and Japan. In 2008, The levelized cost of electricity for solar PV was $0.25/kWh or less in most of the OECD countries. By late 2011, the fully loaded cost was predicted to fall below $0.15/kWh for most of the OECD and to reach $0.10/kWh in sunnier regions. These cost levels are driving three emerging trends: vertical integration of the supply chain, origination of power purchase agreements (PPAs) by solar power companies, and unexpected risk for traditional power generation companies, grid operators and wind turbine manufacturers.\n\nGrid parity was first reached in Spain in 2013, Hawaii and other islands that otherwise use fossil fuel (diesel fuel) to produce electricity, and most of the US is expected to reach grid parity by 2015.\n\nIn 2007, General Electric's Chief Engineer predicted grid parity without subsidies in sunny parts of the United States by around 2015; other companies predicted an earlier date: the cost of solar power will be below grid parity for more than half of residential customers and 10% of commercial customers in the OECD, as long as grid electricity prices do not decrease through 2010.\n\nThe productivity of solar power in a region depends on solar irradiance, which varies through the day and is influenced by latitude and climate.\n\nThe locations with highest annual solar irradiance lie in the arid tropics and subtropics. Deserts lying in low latitudes usually have few clouds, and can receive sunshine for more than ten hours a day. These hot deserts form the \"Global Sun Belt\" circling the world. This belt consists of extensive swathes of land in Northern Africa, Southern Africa, Southwest Asia, Middle East, and Australia, as well as the much smaller deserts of North and South America. Africa's eastern Sahara Desert, also known as the Libyan Desert, has been observed to be the sunniest place on Earth according to NASA.\n\nDifferent measurements of solar irradiance (direct normal irradiance, global horizontal irradiance) are mapped below :\n\nIn cases of self consumption of the solar energy, the payback time is calculated based on how much electricity is not purchased from the grid. For example, in Germany, with electricity prices of 0.25 €/kWh and insolation of 900 kWh/kW, one kWp will save €225 per year, and with an installation cost of 1700 €/KWp the system cost will be returned in less than seven years. However, in many cases, the patterns of generation and consumption do not coincide, and some or all of the energy is fed back into the grid. The electricity is sold, and at other times when energy is taken from the grid, electricity is bought. The relative costs and prices obtained affect the economics. In many markets, the price paid for sold PV electricity is significantly lower than the price of bought electricity, which incentivizes self consumption. Moreover, separate self consumption incentives have been used in e.g. Germany and Italy. Grid interaction regulation has also included limitations of grid feed-in in some regions in Germany with high amounts of installed PV capacity. By increasing self consumption, the grid feed-in can be limited without curtailment, which wastes electricity.\n\nA good match between generation and consumption is key for high self consumption, and should be considered when deciding where to install solar power and how to dimension the installation. The match can be improved with batteries or controllable electricity consumption. However, batteries are expensive and profitability may require provision of other services from them besides self consumption increase. Hot water storage tanks with electric heating with heat pumps or resistance heaters can provide low-cost storage for self consumption of solar power. Shiftable loads, such as dishwashers, tumble dryers and washing machines, can provide controllable consumption with only a limited effect on the users, but their effect on self consumption of solar power may be limited.\n\nThe political purpose of incentive policies for PV is to facilitate an initial small-scale deployment to begin to grow the industry, even where the cost of PV is significantly above grid parity, to allow the industry to achieve the economies of scale necessary to reach grid parity. The policies are implemented to promote national energy independence, high tech job creation and reduction of CO emissions. Three incentive mechanisms are often used in combination as investment subsidies: the authorities refund part of the cost of installation of the system, the electricity utility buys PV electricity from the producer under a multiyear contract at a guaranteed rate, and Solar Renewable Energy Certificates (SRECs)\n\nWith investment subsidies, the financial burden falls upon the taxpayer, while with feed-in tariffs the extra cost is distributed across the utilities' customer bases. While the investment subsidy may be simpler to administer, the main argument in favour of feed-in tariffs is the encouragement of quality. Investment subsidies are paid out as a function of the nameplate capacity of the installed system and are independent of its actual power yield over time, thus rewarding the overstatement of power and tolerating poor durability and maintenance. Some electric companies offer rebates to their customers, such as Austin Energy in Texas, which offers $2.50/watt installed up to $15,000.\n\nIn net metering the price of the electricity produced is the same as the price supplied to the consumer, and the consumer is billed on the difference between production and consumption. Net metering can usually be done with no changes to standard electricity meters, which accurately measure power in both directions and automatically report the difference, and because it allows homeowners and businesses to generate electricity at a different time from consumption, effectively using the grid as a giant storage battery. With net metering, deficits are billed each month while surpluses are rolled over to the following month. Best practices call for perpetual roll over of kWh credits. Excess credits upon termination of service are either lost, or paid for at a rate ranging from wholesale to retail rate or above, as can be excess annual credits. In New Jersey, annual excess credits are paid at the wholesale rate, as are left over credits when a customer terminates service.\n\nWith feed-in tariffs, the financial burden falls upon the consumer. They reward the number of kilowatt-hours produced over a long period of time, but because the rate is set by the authorities, it may result in perceived overpayment. The price paid per kilowatt-hour under a feed-in tariff exceeds the price of grid electricity. Net metering refers to the case where the price paid by the utility is the same as the price charged.\n\nThe complexity of approvals in California, Spain and Italy has prevented comparable growth to Germany even though the return on investment is better. In some countries, additional incentives are offered for BIPV compared to stand alone PV.\n\nAlternatively, SRECs allow for a market mechanism to set the price of the solar generated electricity subsity. In this mechanism, a renewable energy production or consumption target is set, and the utility (more technically the Load Serving Entity) is obliged to purchase renewable energy or face a fine (Alternative Compliance Payment or ACP). The producer is credited for an SREC for every 1,000 kWh of electricity produced. If the utility buys this SREC and retires it, they avoid paying the ACP. In principle this system delivers the cheapest renewable energy, since the all solar facilities are eligible and can be installed in the most economic locations. Uncertainties about the future value of SRECs have led to long-term SREC contract markets to give clarity to their prices and allow solar developers to pre-sell and hedge their credits.\n\nFinancial incentives for photovoltaics differ across countries, including Australia, China, Germany, Israel, Japan, and the United States and even across states within the US.\n\nThe Japanese government through its Ministry of International Trade and Industry ran a successful programme of subsidies from 1994 to 2003. By the end of 2004, Japan led the world in installed PV capacity with over 1.1 GW.\n\nIn 2004, the German government introduced the first large-scale feed-in tariff system, under the German Renewable Energy Act, which resulted in explosive growth of PV installations in Germany. At the outset the FIT was over 3x the retail price or 8x the industrial price. The principle behind the German system is a 20-year flat rate contract. The value of new contracts is programmed to decrease each year, in order to encourage the industry to pass on lower costs to the end users. The programme has been more successful than expected with over 1GW installed in 2006, and political pressure is mounting to decrease the tariff to lessen the future burden on consumers.\n\nSubsequently, Spain, Italy, Greece—that enjoyed an early success with domestic solar-thermal installations for hot water needs—and France introduced feed-in tariffs. None have replicated the programmed decrease of FIT in new contracts though, making the German incentive relatively less and less attractive compared to other countries. The French and Greek FIT offer a high premium (EUR 0.55/kWh) for building integrated systems. California, Greece, France and Italy have 30–50% more insolation than Germany making them financially more attractive. The Greek domestic \"solar roof\" programme (adopted in June 2009 for installations up to 10 kW) has internal rates of return of 10–15% at current commercial installation costs, which, furthermore, is tax free.\n\nIn 2006 California approved the 'California Solar Initiative', offering a choice of investment subsidies or FIT for small and medium systems and a FIT for large systems. The small-system FIT of $0.39 per kWh (far less than EU countries) expires in just 5 years, and the alternate \"EPBB\" residential investment incentive is modest, averaging perhaps 20% of cost. All California incentives are scheduled to decrease in the future depending as a function of the amount of PV capacity installed.\n\nAt the end of 2006, the Ontario Power Authority (OPA, Canada) began its Standard Offer Program, a precursor to the Green Energy Act, and the first in North America for distributed renewable projects of less than 10 MW. The feed-in tariff guaranteed a fixed price of $0.42 CDN per kWh over a period of twenty years. Unlike net metering, all the electricity produced was sold to the OPA at the given rate.\n\nThe overwhelming majority of electricity produced worldwide is used immediately, since storage is usually more expensive and because traditional generators can adapt to demand. However both solar power and wind power are variable renewable energy, meaning that all available output must be taken whenever it is available by moving through transmission lines to \"where it can be used now\". Since solar energy is not available at night, storing its energy is potentially an important issue particularly in off-grid and for future 100% renewable energy scenarios to have continuous electricity availability.\n\nSolar electricity is inherently variable and predictable by time of day, location, and seasons. In addition solar is intermittent due to day/night cycles and unpredictable weather. How much of a special challenge solar power is in any given electric utility varies significantly. In a summer peak utility, solar is well matched to daytime cooling demands. In winter peak utilities, solar displaces other forms of generation, reducing their capacity factors.\n\nIn an electricity system without grid energy storage, generation from stored fuels (coal, biomass, natural gas, nuclear) must be go up and down in reaction to the rise and fall of solar electricity (see load following power plant). While hydroelectric and natural gas plants can quickly follow solar being intermittent due to the weather, coal, biomass and nuclear plants usually take considerable time to respond to load and can only be scheduled to follow the predictable variation. Depending on local circumstances, beyond about 20–40% of total generation, grid-connected intermittent sources like solar tend to require investment in some combination of grid interconnections, energy storage or demand side management. Integrating large amounts of solar power with existing generation equipment has caused issues in some cases. For example, in Germany, California and Hawaii, electricity prices have been known to go negative when solar is generating a lot of power, displacing existing baseload generation contracts.\n\nConventional hydroelectricity works very well in conjunction with solar power, water can be held back or released from a reservoir behind a dam as required. Where a suitable river is not available, pumped-storage hydroelectricity uses solar power to pump water to a high reservoir on sunny days then the energy is recovered at night and in bad weather by releasing water via a hydroelectric plant to a low reservoir where the cycle can begin again. \nHowever, this cycle can lose 20% of the energy to round trip inefficiencies, this plus the construction costs add to the expense of implementing high levels of solar power.\n\nConcentrated solar power plants may use thermal storage to store solar energy, such as in high-temperature molten salts. These salts are an effective storage medium because they are low-cost, have a high specific heat capacity, and can deliver heat at temperatures compatible with conventional power systems. This method of energy storage is used, for example, by the Solar Two power station, allowing it to store 1.44 TJ in its 68 m³ storage tank, enough to provide full output for close to 39 hours, with an efficiency of about 99%.\n\nIn stand alone PV systems batteries are traditionally used to store excess electricity. With grid-connected photovoltaic power system, excess electricity can be sent to the electrical grid. Net metering and feed-in tariff programs give these systems a credit for the electricity they produce. This credit offsets electricity provided from the grid when the system cannot meet demand, effectively trading with the grid instead of storing excess electricity. Credits are normally rolled over from month to month and any remaining surplus settled annually. \nWhen wind and solar are a small fraction of the grid power, other generation techniques can adjust their output appropriately, but as these forms of variable power grow, additional balance on the grid is needed. As prices are rapidly declining, PV systems increasingly use rechargeable batteries to store a surplus to be later used at night. Batteries used for grid-storage stabilize the electrical grid by leveling out peak loads usually for several minutes, and in rare cases for hours. In the future, less expensive batteries could play an important role on the electrical grid, as they can charge during periods when generation exceeds demand and feed their stored energy into the grid when demand is higher than generation.\n\nAlthough not permitted under the US National Electric Code, it is technically possible to have a “plug and play” PV microinverter. A recent review article found that careful system design would enable such systems to meet all technical, though not all safety requirements. There are several companies selling plug and play solar systems available on the web, but there is a concern that if people install their own it will reduce the enormous employment advantage solar has over fossil fuels.\n\nCommon battery technologies used in today's home PV systems include, the valve regulated lead-acid battery– a modified version of the conventional lead–acid battery, nickel–cadmium and lithium-ion batteries. Lead-acid batteries are currently the predominant technology used in small-scale, residential PV systems, due to their high reliability, low self discharge and investment and maintenance costs, despite shorter lifetime and lower energy density. However, lithium-ion batteries have the potential to replace lead-acid batteries in the near future, as they are being intensively developed and lower prices are expected due to economies of scale provided by large production facilities such as the Gigafactory 1. In addition, the Li-ion batteries of plug-in electric cars may serve as a future storage devices in a vehicle-to-grid system. Since most vehicles are parked an average of 95 percent of the time, their batteries could be used to let electricity flow from the car to the power lines and back. Other rechargeable batteries used for distributed PV systems include, sodium–sulfur and vanadium redox batteries, two prominent types of a molten salt and a flow battery, respectively.\n\nThe combination of wind and solar PV has the advantage that the two sources complement each other because the peak operating times for each system occur at different times of the day and year. The power generation of such solar hybrid power systems is therefore more constant and fluctuates less than each of the two component subsystems. Solar power is seasonal, particularly in northern/southern climates, away from the equator, suggesting a need for long term seasonal storage in a medium such as hydrogen or pumped hydroelectric. The Institute for Solar Energy Supply Technology of the University of Kassel pilot-tested a combined power plant linking solar, wind, biogas and hydrostorage to provide load-following power from renewable sources.\n\nResearch is also undertaken in this field of artificial photosynthesis. It involves the use of nanotechnology to store solar electromagnetic energy in chemical bonds, by splitting water to produce hydrogen fuel or then combining with carbon dioxide to make biopolymers such as methanol. Many large national and regional research projects on artificial photosynthesis are now trying to develop techniques integrating improved light capture, quantum coherence methods of electron transfer and cheap catalytic materials that operate under a variety of atmospheric conditions. Senior researchers in the field have made the public policy case for a Global Project on Artificial Photosynthesis to address critical energy security and environmental sustainability issues.\n\nUnlike fossil fuel based technologies, solar power does not lead to any harmful emissions during operation, but the production of the panels leads to some amount of pollution.\n\nThe life-cycle greenhouse-gas emissions of solar power are in the range of 22 to 46 gram (g) per kilowatt-hour (kWh) depending on if solar thermal or solar PV is being analyzed, respectively. With this potentially being decreased to 15 g/kWh in the future. For comparison (of weighted averages), a combined cycle gas-fired power plant emits some 400–599 g/kWh, an oil-fired power plant 893 g/kWh, a coal-fired power plant 915–994 g/kWh or with carbon capture and storage some 200 g/kWh, and a geothermal high-temp. power plant 91–122 g/kWh. \nThe life cycle emission intensity of hydro, wind and nuclear power are lower than solar's as of 2011 as published by the IPCC, and discussed in the article Life-cycle greenhouse-gas emissions of energy sources. Similar to all energy sources were their total life cycle emissions primarily lay in the construction and transportation phase, the switch to low carbon power in the manufacturing and transportation of solar devices would further reduce carbon emissions. BP Solar owns two factories built by Solarex (one in Maryland, the other in Virginia) in which all of the energy used to manufacture solar panels is produced by solar panels. A 1-kilowatt system eliminates the burning of approximately 170 pounds of coal, 300 pounds of carbon dioxide from being released into the atmosphere, and saves up to 105 gallons of water consumption monthly.\n\nThe US National Renewable Energy Laboratory (NREL), in harmonizing the disparate estimates of life-cycle GHG emissions for solar PV, found that the most critical parameter was the solar insolation of the site: GHG emissions factors for PV solar are inversely proportional to insolation. For a site with insolation of 1700 kWh/m2/year, typical of southern Europe, NREL researchers estimated GHG emissions of 45 ge/kWh. Using the same assumptions, at Phoenix, USA, with insolation of 2400 kWh/m2/year, the GHG emissions factor would be reduced to 32 g of COe/kWh.\n\nThe New Zealand Parliamentary Commissioner for the Environment found that the solar PV would have little impact on the country's greenhouse gas emissions. The country already generates 80 percent of its electricity from renewable resources (primarily hydroelectricity and geothermal) and national electricity usage peaks on winter evenings whereas solar generation peaks on summer afternoons, meaning a large uptake of solar PV would end up displacing other renewable generators before fossil-fueled power plants.\n\nThe energy payback time (EPBT) of a power generating system is the time required to generate as much energy as is consumed during production and lifetime operation of the system. Due to improving production technologies the payback time has been decreasing constantly since the introduction of PV systems in the energy market. \nIn 2000 the energy payback time of PV systems was estimated as 8 to 11 years and in 2006 this was estimated to be 1.5 to 3.5 years for crystalline silicon PV systems and 1–1.5 years for thin film technologies (S. Europe). These figures fell to 0.75–3.5 years in 2013, with an average of about 2 years for crystalline silicon PV and CIS systems.\n\nAnother economic measure, closely related to the energy payback time, is the energy returned on energy invested (EROEI) or energy return on investment (EROI), which is the ratio of electricity generated divided by the energy required to build \"and maintain\" the equipment. (This is not the same as the economic return on investment (ROI), which varies according to local energy prices, subsidies available and metering techniques.) With expected lifetimes of 30 years, the EROEI of PV systems are in the range of 10 to 30, thus generating enough energy over their lifetimes to reproduce themselves many times (6–31 reproductions) depending on what type of material, balance of system (BOS), and the geographic location of the system.\n\nSolar power includes plants with among the lowest water consumption per unit of electricity (photovoltaic), and also power plants with among the highest water consumption (concentrating solar power with wet-cooling systems).\n\nPhotovoltaic power plants use very little water for operations. Life-cycle water consumption for utility-scale operations is estimated to be 12 gallons per megawatt-hour for flat-panel PV solar. Only wind power, which consumes essentially no water during operations, has a lower water consumption intensity.\n\nConcentrating solar power plants with wet-cooling systems, on the other hand, have the highest water-consumption intensities of any conventional type of electric power plant; only fossil-fuel plants with carbon-capture and storage may have higher water intensities. A 2013 study comparing various sources of electricity found that the median water consumption during operations of concentrating solar power plants with wet cooling was 810 ga/MWhr for power tower plants and 890 gal/MWhr for trough plants. This was higher than the operational water consumption (with cooling towers) for nuclear (720 gal/MWhr), coal (530 gal/MWhr), or natural gas (210). A 2011 study by the National Renewable Energy Laboratory came to similar conclusions: for power plants with cooling towers, water consumption during operations was 865 gal/MWhr for CSP trough, 786 gal/MWhr for CSP tower, 687 gal/MWhr for coal, 672 gal/MWhr for nuclear, and 198 gal/MWhr for natural gas. The Solar Energy Industries Association noted that the Nevada Solar One trough CSP plant consumes 850 gal/MWhr. The issue of water consumption is heightened because CSP plants are often located in arid environments where water is scarce.\n\nIn 2007, the US Congress directed the Department of Energy to report on ways to reduce water consumption by CSP. The subsequent report noted that dry cooling technology was available that, although more expensive to build and operate, could reduce water consumption by CSP by 91 to 95 percent. A hybrid wet/dry cooling system could reduce water consumption by 32 to 58 percent. A 2015 report by NREL noted that of the 24 operating CSP power plants in the US, 4 used dry cooling systems. The four dry-cooled systems were the three power plants at the Ivanpah Solar Power Facility near Barstow, California, and the Genesis Solar Energy Project in Riverside County, California. Of 15 CSP projects under construction or development in the US as of March 2015, 6 were wet systems, 7 were dry systems, 1 hybrid, and 1 unspecified.\n\nAlthough many older thermoelectric power plants with once-through cooling or cooling ponds \"use\" more water than CSP, meaning that more water passes through their systems, most of the cooling water returns to the water body available for other uses, and they \"consume\" less water by evaporation. For instance, the median coal power plant in the US with once-through cooling uses 36,350 gal/MWhr, but only 250 gal/MWhr (less than one percent) is lost through evaporation. Since the 1970s, the majority of US power plants have used recirculating systems such as cooling towers rather than once-through systems.\n\nOne issue that has often raised concerns is the use of cadmium (Cd), a toxic heavy metal that has the tendency to accumulate in ecological food chains. It is used as semiconductor component in CdTe solar cells and as buffer layer for certain CIGS cells in the form of CdS. \nThe amount of cadmium used in thin-film PV modules is relatively small (5–10 g/m²) and with proper recycling and emission control techniques in place the cadmium emissions from module production can be almost zero. Current PV technologies lead to cadmium emissions of 0.3–0.9 microgram/kWh over the whole life-cycle. Most of these emissions arise through the use of coal power for the manufacturing of the modules, and coal and lignite combustion leads to much higher emissions of cadmium. Life-cycle cadmium emissions from coal is 3.1 microgram/kWh, lignite 6.2, and natural gas 0.2 microgram/kWh.\n\nIn a life-cycle analysis it has been noted, that if electricity produced by photovoltaic panels were used to manufacture the modules instead of electricity from burning coal, cadmium emissions from coal power usage in the manufacturing process could be entirely eliminated.\n\nIn the case of crystalline silicon modules, the solder material, that joins together the copper strings of the cells, contains about 36 percent of lead (Pb). Moreover, the paste used for screen printing front and back contacts contains traces of Pb and sometimes Cd as well. It is estimated that about 1,000 metric tonnes of Pb have been used for 100 gigawatts of c-Si solar modules. However, there is no fundamental need for lead in the solder alloy.\n\nSome media sources have reported that concentrated solar power plants have injured or killed large numbers of birds due to intense heat from the concentrated sunrays. This adverse effect does not apply to PV solar power plants, and some of the claims may have been overstated or exaggerated.\n\nA 2014-published life-cycle analysis of land use for various sources of electricity concluded that the large-scale implementation of solar and wind potentially reduces pollution-related\nenvironmental impacts. The study found that the land-use footprint, given in square meter-years per megawatt-hour (ma/MWh), was lowest for wind, natural gas and rooftop PV, with 0.26, 0.49 and 0.59, respectively, and followed by utility-scale solar PV with 7.9. For CSP, the footprint was 9 and 14, using parabolic troughs and solar towers, respectively. The largest footprint had coal-fired power plants with 18 ma/MWh.\n\nConcentrator photovoltaics (CPV) systems employ sunlight concentrated onto photovoltaic surfaces for the purpose of electrical power production. Contrary to conventional photovoltaic systems, it uses lenses and curved mirrors to focus sunlight onto small, but highly efficient, multi-junction solar cells. Solar concentrators of all varieties may be used, and these are often mounted on a solar tracker in order to keep the focal point upon the cell as the sun moves across the sky. Luminescent solar concentrators (when combined with a PV-solar cell) can also be regarded as a CPV system. Concentrated photovoltaics are useful as they can improve efficiency of PV-solar panels drastically.\n\nIn addition, most solar panels on spacecraft are also made of high efficient multi-junction photovoltaic cells to derive electricity from sunlight when operating in the inner Solar System.\n\nFloatovoltaics are an emerging form of PV systems that float on the surface of irrigation canals, water reservoirs, quarry lakes, and tailing ponds. Several systems exist in France, India, Japan, Korea, the United Kingdom and the United States. These systems reduce the need of valuable land area, save drinking water that would otherwise be lost through evaporation, and show a higher efficiency of solar energy conversion, as the panels are kept at a cooler temperature than they would be on land. Although not floating, other dual-use facilities with solar power include fisheries.\n\n\n"}
{"id": "3721610", "url": "https://en.wikipedia.org/wiki?curid=3721610", "title": "Synergistic gardening", "text": "Synergistic gardening\n\nSynergistic gardening is a system of organic gardening, developed by Emilia Hazelip. The system is strongly influenced by permaculture, as well as the work of Masanobu Fukuoka and Marc Bonfils. After establishing the garden, there is no further digging, ploughing or tilling, and no use of external inputs such as manures and other fertilizers, or pesticides. Soil health is maintained by the selection of plants, mulching, and recycling of plant residues.\n\n"}
{"id": "25825205", "url": "https://en.wikipedia.org/wiki?curid=25825205", "title": "TECO Electric and Machinery", "text": "TECO Electric and Machinery\n\nTECO Electric & Machinery Co., Ltd. (TECO; ) is a Taiwanese company which was established on 12 June 1956, starting out as an industrial motor manufacturer. Over the years, it has successfully diversified into a conglomerate with worldwide business operations. Now it is the third biggest medium-voltage motor producer in the world, with around 8% market share and also ranked number 5 in global low-voltage A/C motor market, representing 4% of the world. The Company also engages in the manufacture, installation, wholesale, retail of various types of electrical and mechanical equipment, telecommunication equipment and home appliances.\n\n1956 Established\n\n1970 Produced air conditioners and entered the home appliances market\n\n1986 Joint venture with Westinghouse Electric to form TECO Westinghouse Motor\n\n1989 Founded TECO Industry Malaysia Sdn. Bhd.\n\n1990 Founded Toshiba Compressor (Taiwan) Corp. with Toshiba\n\n1992 Established Yatec Engineering Corporation with Yaskawa Electric Manufacture Co., Ltd.\n\n1995 Acquired Westinghouse Motor Co., Ltd (USA)\n\n1998 Founded TECO Electro Devices Co., Ltd. for manufacturing of stepping motors\n\n1999 Founded TECO (Dong Guan) Air Conditioning Equipment Co., Ltd.\n2000 Founded Suzhou TECO Electric & Machinery Co., Ltd.\n2001 Established Smart Card Division for National Health Insurance IC-card project\n\n2002 Founded Wuxi TECO with China Steel, Nippon Steel and Marubeni-Itochu Steel\n2003 Merged Tai-An Electric Co., Ltd.\n\n2004 Established Jiangxi TECO Electric & Machinery Co., Ltd.\n\n2005 Founded Yaskawa TECO Motor Engineering Corp.\n\n2006 Strategic alliance with CTC to set up the first wind power project in Texas, USA\n\n2007 Launch into wind power generation; introduce a 2MW wind power generator\n\n2010 Rollout of TECO's first 2MW wind power turbine\n\n2011 TECO Middle East KSA\n\n\n\n"}
{"id": "4142888", "url": "https://en.wikipedia.org/wiki?curid=4142888", "title": "Telligent Systems", "text": "Telligent Systems\n\nTelligent, A Verint Company is an enterprise collaboration and community software business founded in 2004 by Rob Howard. The company changed its name to Zimbra, Inc in September 2013 after completing the acquisition of Zimbra from VMWare. In August 2015 Zimbra's Telligent business was acquired by Verint Systems, Inc. Verint continues to operate Telligent as an independent business unit. Also in August 2015 the remaining assets of Zimbra, Inc were acquired by Synacor.\n\nTelligent was founded by Rob Howard in 2004. Howard was previously a founding member of Microsoft's ASP.NET team and helped build and run the Microsoft ASP.NET community. \n\nTelligent introduced its first product, Community Server, in the fall of 2004. Community Server was an integrated community platform that brought together blogs, wikis, forums, user profiles, etc. Community Server was based on the work done by Rob Howard on the ASP.NET Forums, Jason Alexander on nGallery, and Scott Watermasysk on .Text.\n\nDell and MySpace both became Telligent customers in 2006. Dell started a blog on Telligent's platform in response to Jeff Jarvis' post about his dissatisfaction with a Dell laptop.\n\nAt the end of 2007 Telligent introduced a social analytics tool called Harvest Reporting Server.\n\nIn 2008, Intel Capital became Telligent's first capital partner.\n\nIn 2009, Patrick Brandt took over as CEO and Rob Howard became CTO. Patrick Brandt was previously CEO of Skywire Software, which was acquired by Oracle for an undisclosed amount. Telligent also re-branded its product offering as follows: Community Server became Telligent Community, Harvest Reporting Server became Telligent Analytics, and Community Server Evolution became Telligent Enterprise. Telligent also formally introduced the Telligent Evolution platform upon which Telligent Community and Telligent Enterprise were based.\n\nAs part of the company re-positioning, Telligent discontinued development and support for Graffiti CMS and made it available as an open source project. Telligent also discontinued BlogMailr, a free email-to-blog service.\n\nIn 2010 David Mitchell joined Telligent's board. David is currently CEO of Global 360 and was previously CEO of WebMethods. Telligent additionally added Wendy Gibson as Chief Marketing Officer.\n\nOn December 19, 2011, Telligent acquired Leverage Software.\n\nOn July 15, 2013, Telligent acquired Zimbra from Vmware. \n\nOn August 1, 2015, Telligent was acquired by Verint Systems, Inc.\n\nTelligent's products are built on the Microsoft .NET and Microsoft SQL Server platform. They are primarily used as on-premises, white label software solutions.\n\nTelligent Community (formerly Community Server), built on the Telligent Evolution platform, is Telligent's flagship product. It was first introduced in 2004 and the most recent version is 5.6 as of October 2010.\n\nTelligent Community is designed to support external facing communities and the primary use cases are: digital marketing, support communities, and networking.\n\nTelligent Enterprise, built on the Telligent Evolution platform, was first introduced in 2008 in response to users of Telligent Community asking Telligent to provide an employee-focused solution. Telligent Enterprise version 2.6 was released October 2010.\n\nTelligent Enterprise is designed to support enterprise 2.0 / internal communities, private business-to-business communities, and private networking communities. An emphasis on integration with enterprise email systems, such as Microsoft Exchange Server, and enterprise identity management systems, such as Microsoft Active Directory, are examples of how Telligent Enterprise differs from Telligent Community.\n\nTelligent introduced Telligent Analytics in 2007 as Harvest Reporting Server. Telligent Analytics is designed to analyze people and information created within the Telligent Evolution platform. This includes both Telligent Community and Telligent Enterprise. It additionally includes any data created on applications that run on the Telligent Evolution platform.\n\n"}
