{"id": "46425181", "url": "https://en.wikipedia.org/wiki?curid=46425181", "title": "A.L. Monsohn Lithography", "text": "A.L. Monsohn Lithography\n\nThe A.L. Monsohn Lithographic Press (דפוס אבן א\"ל מאנזאהן) was established in Jerusalem in 1892 by Abraham-Leib (or Avrom-Leyb) Monsohn II (Jerusalem, c.1871-1930) and his brother Moshe-Mordechai (Meyshe-Mordkhe). Sponsored by members of the Hamburger family, the brothers had been sent to Frankfurt in 1890 to study lithography. Upon returning to Jerusalem with a hand press, they established the A.L. Monsohn Lithographic Press in the Old City of Jerusalem. At first it was situated in Bab al-Huta; it was later moved to the courtyard opposite what is today the Old Yishuv Court Museum (Hebrew: ) at 6 Or Ha-Hayim Street in the Jewish Quarter, where Abraham-Leib Monsohn lived with his family. Leaving the Old City, the press was relocated to the Mamilla section of Jerusalem, and later to Yosef Ziv Street in the Tel Arza neighborhood. The Monsohn Press produced about 300 color prints per day, the only color printing done at the time in Jerusalem. In 1894 they imported a new machine which could print 1,000 copies a day—a great advance in local printing. The founders of the Monsohn press produced Jewish-themed color postcards, greeting cards, Jewish National Fund stamps, and maps documenting the evolution of the Jewish settlement in Eretz Israel in the nineteenth-twentieth centuries; religious material such as decorative plaques for synagogues, portraits of Old Yishuv rabbis such as Shmuel Salant, Mizrah posters indicating the direction of prayer for synagogues, memorial posters, and posters for Sukkot booths;, color frontispieces for books such as Pentateuch volumes and the early song collections of Abraham Zvi Idelsohn (e.g., \"Shire Zion\", Jerusalem 1908); artistic wedding invitations; and later, government posters; and labels, packaging and advertisements for the pioneering entrepreneurs of Eretz Israel. Many of the postcards and maps can be seen online, as can the artistic invitations to his children's weddings which Monsohn published in the Jerusalem Hebrew press (e.g., that for his son Menachem Mendel Monsohn and his wife Zipporah in the 24 June 1914 issue of \"Moria\"). The Monsohn Press received special permission from the city's rabbis to print for Christians and Moslems, so long as the material could not be used to missionize. While Eretz Israel was under Ottoman control, Abraham-Leib Monsohn also printed the maps for the Ottoman military leader Djemal Pasha, in his headquarters in Mount Scopus.\n\nFor years, the Monsohn (later, Monson/Monzon) Press was considered the best and most innovative in the country—pioneering in such techniques as gold-embossing and offset printing, among others. Early items for tourists included collections of \"Flowers of the Holy Land\" (c. 1910-1918)—pressed local flowers accompanied by scenes from the Eretz Israel countryside and relevant verses from the Bible, edited by Jsac Chagise (or Itzhak Haggis), an immigrant from Vitebsk, and bound in carved olive wood boards. Shortly after World War I Monsohn (now spelled מונזון) used zincography to produce the prints included in the Hebrew \"Gannenu\" educational booklets for young children illustrated by Ze'ev Raban of the Bezalel Academy of Art and Design and printed in Jerusalem by Hayim Refael Hakohen (vol. 1, 1919; vols. 2-3, 1920). In 1934 Monsohn moved into the new, western part of Jerusalem, in a shop with four presses and 30 workers, including Abraham-Leib's sons, David, Yosef, Moshe and Shimon, and his daughter Raytse's husband, Abraham Barmacz. The concern did business with all sectors of the city's population, including Arabs, for whom they printed in Arabic. Among their clients were members of the Ginio, Havilio, and Elite families, and Shemen, Dubek, and other renowned national brands, manufacturing products such as wine, candies, oil, and cigarettes. They also printed movie and travel posters, and government posters, postcards and documents, hotel luggage labels, receipts for Bikur Cholim Hospital and other local institutions, metal charity boxes, Melnik, Rosin & Co. (Jerusalem) embroidery designs (c.1900), and cloth covers for \"hallot\". During the Tzena austerity period Monsohn was the exclusive printer of government coupon booklets.\n\nShimon Monson (or Monzon, b. 1907; son of Abraham-Leib II) and Shimon Barmacz (b. 1922; son of Raytsa Monsohn Barmacz [b. 1901]), recipient of the Yakir Yerushalayim award), were responsible for the press in its final stage, during which it also produced Jewish National Fund calendars, color maps, illustrations of animals in the Bible, tourist brochures, and printed books, especially photo-offset editions of sacred works, of which they printed over 80 (e.g., \"Mishnah Berurah\", 6 vols., 1950; \"Miqra'ot Gedolot\", 5 vols., 1955; \"Ḥoq Le-Yisrael\", 5 vols., 1956; \"Shulhan Arukh\", 2 vols., 1956-1957; \"Zohar\", 5 vols., 1958-1960; \"Moreh Nevukhim\", 3 vols., 1960). In 1955 Shimon Monson also printed the 1955 first Jerusalem edition of Vladimir Nabokov's \"Lolita\" for the Olympia Press (2 vols.). The revolutionary Koren Publishers Jerusalem Bible was printed at the press of Shimon Monzon (Pentateuch, 1959; complete Bible, 1965 and later printings). Unable to compete with larger, more modernized concerns, the Monsohn/Monzon Press closed in 1992. A grandson of the founders helped establish Keter Press, printer of the first edition of the \"Encyclopedia Judaica\" and still one of Israel's leading printing establishments. Shimon Barmacz's son, Mordechai (b. 1948), established the Hebron Press in Kiryat Arba. Elyakim Monzon (b. 1927), son of Abraham-Leib's son Yosef (b. 1903), also engaged in printing. Moshe Monzon (b. 1958), grandson of Abraham-Leib's son Moshe, produces artistic tapestries. The prints produced by the A.L. Monsohn Lithography are today sought by collectors the world over.\n"}
{"id": "9044575", "url": "https://en.wikipedia.org/wiki?curid=9044575", "title": "A. Hunter Dupree", "text": "A. Hunter Dupree\n\nAnderson Hunter Dupree (born 29 January 1921, in Hillsboro, Texas) is an American historian and one of the pioneer historians of the history of science and technology in the United States.\n\nThe son of a lawyer, George W. Dupree, and his wife, Sarah Hunter, he attended Oberlin College, where he earned his bachelor of arts degree (summa cum laude) in 1942. Upon completion of his undergraduate work, he joined the United States Navy in 1942-1946, and became a Lieutenant in the U.S. Naval Reserve.\n\nAt the end of World War II, Dupree married Marguerite Louise Arnold (c. 1918-May 27, 2014) in 1946, having two children including the historian Marguerite Dupree and the harpsichord maker Anderson H. Dupree.\n\nHunter Dupree entered Harvard University, where he completed his master's degree in 1947, and his Ph.D. in 1952, having written his doctoral dissertation on Asa Gray, titled \"Asa Gray: The Development of a Statesman of Science, 1810-1848\". Marguerite earned a Ph.D. in history from Harvard and also taught at universities.\n\nIn 1950, Dupree took up his first academic position as assistant professor of history at Texas Technological College (now Texas Tech University) in Lubbock, Texas, where he remained until 1952, when he was appointed a research fellow at the Gray Herbarium at Harvard University. He served two appointments there in 1952-54 and 1955-56. In addition, he served as project director on grants at the National Science Foundation, 1953-55.\n\nIn 1956, the University of California, Berkeley appointed Dupree as visiting assistant professor of history, then promoted to associate professor in 1958, and professor of history in 1961. He served additionally as assistant to the chancellor in 1960-62, and director of the Bancroft Library in 1965-66. In addition, Dupree was a consultant to the committee on science and public policy at the National Academy of Science in 1963-64. He remained at Berkeley until 1968.\n\nDespite being a competent academic, Dupree's tenure in the Bancroft Library directorship was brief and stormy. He was appointed to the directorship in June 1965 but clashed repeatedly with the staff over library internal policy, which sparked a near-revolution among its employees. Part of the disagreement involved Dupree's emphasis on modernizing traditional library practices and the way collections, particularly manuscripts, were handled in the collections. He also required an accounting for long-term projects on which the library devoted resources but could show few actual results. In January 1966 The UC-Berkeley president returned him to the history faculty that same June.\n\nIn 1968, Brown University appointed Dupree George L. Littlefield Professor of History, a position he held until his retirement in 1981. While in this post, he served as a consultant to the Panel on Science and Technology and Astronautics, U.S. House of Representatives, 1969–73; trustee of the American Textile History Museum, a member of the NASA Historical Advisory Committee, and the Atomic Energy Commission's Historical Advisory Committee.\n\n\n• 'Some Letters from Charles Darwin to Jeffries Wyman', \"Isis\" Vol.42,Part 2., No.128. (June,1951), pp. 104–110.\n\n• 'Thomas Nuttall’s Controversy With Asa Gray', \"Rhodora\", Vol. 54, (1952), pp. 293 –303.\n\n• 'Science vs. the Military: Dr. James Morrow and the Perry Expedition', \"The Pacific Historical Review\", vol. 22, no. 1, (1953), pp. 29–37.\n\n• 'Jeffries Wyman’s views on evolution', \"Isis\", vol. 44 (1953), pp. 243-246.\n\n• \"Science in the Federal Government, a history of policies and activities to 1940.\" (1957, 1986)\n\n• \"Asa Gray, 1810-1888\" (1959, 1968, 1988)\n\n• \"What manuscripts the historian wants saved\", \"Isis\", vol. 53 (1962), pp. 63–66.\n\n• \"Darwiniana; essays and reviews pertaining to Darwinism\" by Asa Gray; edited A. Hunter Dupree. (1963)\n\n• \"Science and the emergence of modern America, 1865-1916\", edited by A. Hunter Dupree. (1963)\n\n• \"Some general implications of the research of the Harvard University Program on Technology and Society\" edited by Emmanuel G. Mesthene. Comment: the anticipation of change by Simon Ramo. Comment: Is technology predictable? by Peter F. Drucker. Comment: the role of technology in society and the need for historical perspective by A. Hunter Dupree. Comment on the comments by Emmanuel G. Mesthene. (1969)\n\n• \"The crisis in authority\", \"Brown Alumni Monthly\", vol. 70, no. 1, (1969)\n\n• \"Science and society: past, present, and future\" edited by Nicholas H. Steneck with a contribution by A. Hunter Dupree (1975)\n\n• \"Sir Joseph Banks and the origins of science policy\". James Ford Bell Lecture; no. 22. (1984).\n\n"}
{"id": "410476", "url": "https://en.wikipedia.org/wiki?curid=410476", "title": "Angelic Layer", "text": "Angelic Layer\n\nThe manga was adapted into a 26-episode anime series produced by Bones titled which aired on TV Tokyo from April 1, 2001 – September 23, 2001. Seven volumes of videos were released by ADV Films on VHS and DVD in 2003. It was re-released in 2005 as a five volume box set. North American publisher Dark Horse Comics re-releases \"Angelic Layer\" in omnibus format in 2011. Sentai Filmworks will re-release the series under their Sentai Selects label on November 24, 2015. Anime Limited announced they would release the series in the UK in 2018.\n\n\"Angelic Layer\" takes place in the same universe as Clamp's later work \"Chobits\", which similarly deals with the relationship between humans, human-created devices, toys, and godlike power. Several characters also appear in Clamp's \"\" including most of the main characters, as well as the angel Blanche.\n\nThe primary protagonist is Misaki Suzuhara. Despite her short appearance she is a seventh grader who just moved to Tokyo to live with her aunt, Shouko Asami. After arriving in the city outside of Tokyo Station, Misaki watches a battle between two dolls on a big live-screen called Angelic Layer, a highly popular game in which players (called Deus) buy and custom-design dolls known as Angels that are moved by mental control when on a field called the \"layer.\"\n\nInterested in learning about Angelic Layer, an eccentric man wearing a white lab coat and glasses, calling himself \"Icchan\" (いっちゃん), encourages Misaki to purchase and create her own angel. She wants the angel to be \"a short girl, but strong and happy\", and names it Hikaru, based on Hikaru Shidō from Clamp's \"Magic Knight Rayearth\" (a manga in \"Angelic Layer\"'s world). Even though she's clueless about the game, Misaki soon competes in tournaments and is assisted and watched carefully by Icchan. Later, Icchan's identity is revealed as Ichiro Mihara, the co-creator of Angelic Layer.\n\nMisaki begins studying at the Eriol Academy, an educational institution which include grades from kindergarten through high school. There she becomes friends with Hatoko Kobayashi, a very intelligent and mature kindergarten girl who is a famous Deus and an Angelic Layer expert. Her incredibly fast angel Suzuka is a favourite contender in tournaments. Misaki also befriends Hatoko's older brother Kōtarō and his friend Tamayo Kizaki, a girl fascinated by martial arts. Both turn out to be Misaki's classmates.\n\nWhile adjusting to her new surroundings, Misaki is also gripped by her past. Her thoughts often dwell on her mother, whom she has not seen since pre-school. Eventually, Misaki learns that her mother was key in the development of Angelic Layer, which she worked on in an attempt to develop a perfect prosthesis for her multiple sclerosis, which has confined her to a wheelchair. Her mother is also the Deus of Athena and the champion of Angelic Layer.\n\nThe differences in the anime series: Misaki names her angel of her favorite doll from childhood. The ending to the manga also has different couplings. In the manga, Misaki's mother does not have multiple sclerosis. Icchan plays an important role in the \"Chobits\" storyline, but this connection was reduced to a single scene in the anime; the \"Chobits\" anime was also made by a different company. Kaede's younger brother Minoru is also a \"Chobits\" character.\n\nOpening Theme:\n\nEnding Theme:\n\nAll of the background musical scores was composed, arranged and conducted by Kōhei Tanaka, the composer of \"One Piece\" and \"Sakura Wars\".\n\n\n\"Angelic Layer\" has received mixed reviews. THEM Anime Reviews on the other hand gave \"Angelic Layer\" a 5-star rating, noting that the character designs were well presented and the animation was colourful, also that \"Angelic Layer\" portrayed the concepts of friendship and how \"through common interests, even very different people can be friends\".\n\"Angelic Layer\" won the Animation Kobe Award for TV Feature in 2001. Anime News Network however, was less kind, comparing it to Pokémon and Digimon, and calling it a glorified tie-in to \"a toy you can't purchase.\" \n\n"}
{"id": "59026869", "url": "https://en.wikipedia.org/wiki?curid=59026869", "title": "Ann Kelleher", "text": "Ann Kelleher\n\nAnn B. Kelleher, from Macroom, County Cork, is an engineer, and a senior officer of Intel Corporation. She has headed up Intel's high-volume integrated circuit production, and was the first Irish woman to be named as a vice-president of the corporation.\n\nKelleher is originally from Macroom in County Cork, Ireland, where her father worked as a farmer. With parental encouragement, Kelleher pursued maths and science in secondary school, and chose to study engineering in University College Cork (UCC), being one of just five women in a class of 55. She specialised in electrical engineering and graduated in 1987. Kelleher continued electrical engineering studies at Masters level instead of accepting a job offer at Digital in Clonmel, Co. Tipperary, graduating in 1989. She then became the first ever female to receive a PhD from Ireland's National Microelectronics Research Centre (NMRC), now part of the Tyndall National Institute at UCC, in 1993. She ran a process integration group in Tyndall from 93-96.\n\nIn 1996 Kelleher joined Intel Ireland in Leixlip, Co. Kildare, as a process engineer, where she quickly achieved promotion. Her first major role of responsibility was as factory manager of \"Fab 24\", a plant for the manufacturing of semiconductors, in Leixlip, when it opened in 2006. At the time, it was Intel's first chip factory in Europe using advanced 65nm process technology. In 2008, Kelleher decided to go to the United States to become the plant manager at Intel’s Fab 12 plant in Chandler, Arizona. Kelleher also site managed Intel’s Rio Rancho, New Mexico Fab 11X fabrication facility for a period before finally moving to Portland, Oregon in 2015 where she lives as of 2018.\n\nKelleher was general manager of the Fab/Sort Manufacturing organisation. In that role, she was responsible for all aspects of Intel’s high-volume silicon manufacturing.\n\nIn 2011, Kelleher became the first Irish woman in the history of the company to be named as a vice president. In January 2018, she was promoted to senior vice-president of the Technology and Manufacturing Group, a unit of around 30,000 staff. Her work includes corporate quality assurance, customer fulfillment and supply-chain management, and strategic planning for the company’s worldwide manufacturing operations.\n\nIn 2018 Kelleher was one of 25 women recognised in the \"Ireland's Most Powerful Women Awards.\" Later that year, \"Forbes\" suggested that Kelleher would be well suited to replacing Elon Musk as the Chairman of Tesla, Inc. due to her experience in \"in high-tech and high-volume manufacturing\".\n\nKelleher is an advocate for women working in engineering roles and holding senior management positions in the tech industry. She has said that more girls should study science, technology, engineering, and mathematics (STEM) subjects in school and university. Kelleher has also argued that more women should be applying for senior management roles.\n\nKelleher travels to Asia and Europe a few times a year, as well as visiting the family home.\n"}
{"id": "2341503", "url": "https://en.wikipedia.org/wiki?curid=2341503", "title": "Barometric light", "text": "Barometric light\n\nBarometric light is a name for the light that is emitted by a mercury-filled barometer tube when the tube is shaken. The discovery of this phenomenon in 1675 revealed the possibility of electric lighting.\n\nThe earliest barometers were simply glass tubes that were closed at one end and filled with mercury. The tube was then inverted and its open end was submerged in a cup of mercury. The mercury then drained out of the tube until the pressure of the mercury in the tube — as measured at the surface of the mercury in the cup — equaled the atmosphere's pressure on the same surface.\n\nIn order to produce barometric light, the glass tube must be very clean and the mercury must be pure. If the barometer is then shaken, a band of light will appear on the glass at the meniscus of the mercury whenever the mercury moves downward.\n\nWhen mercury contacts glass, the mercury transfers electrons to the glass. Whenever the mercury pulls free of the glass, these electrons are released from the glass into the surroundings, where they collide with gas molecules, causing the gas to glow — just as the collision of electrons and neon atoms causes a neon lamp to glow.\n\nBarometric light was first observed in 1675 by the French astronomer Jean Picard: \"Towards the year 1676, Monsieur Picard was transporting his barometer from the Observatory to Port Saint Michel during the night, [when] he noticed a light in a part of the tube where the mercury was moving; this phenomenon having surprised him, he immediately reported it to the \"sçavans\", … \" The Swiss mathematician Johann Bernoulli studied the phenomenon while teaching at Groningen, the Netherlands, and in 1700 he demonstrated the phenomenon to the French Academy. After learning of the phenomenon from Bernoulli, the Englishman Francis Hauksbee investigated the subject extensively. Hauksbee showed that a complete vacuum was not essential to the phenomenon, for the same glow was apparent when mercury was shaken with air only partially rarefied, and that even without using the barometric tube, bulbs containing low-pressure gases could be made to glow via externally applied static electricity. The phenomenon was also studied by contemporaries of Hauksbee, including the Frenchman Pierre Polinière and a French mathematician, Gabriel-Philippe de la Hire, and subsequently by many others.\n\n"}
{"id": "23669028", "url": "https://en.wikipedia.org/wiki?curid=23669028", "title": "Beth Simone Noveck", "text": "Beth Simone Noveck\n\nBeth Simone Noveck (born 1971) is the Jerry Hultin Global Network Professor at New York University’s Tandon School of Engineering and director of the Governance Lab. She was the United States deputy chief technology officer for open government and led President Obama's Open Government Initiative. Based at the White House Office of Science and Technology Policy until January 2011, she is an expert on technology and institutional innovation. On May 16, 2011 George Osborne announced that Noveck had been recruited to a position in the United Kingdom government. She is a Commissioner for the Global Commission on Internet Governance. She is the author of Smart Citizens, Smarter State: The Technologies of Expertise and the Future of Government (Harvard 2015), Wiki Government: How Technology Can Make Government Better, Democracy Stronger, and Citizens More Powerful (Brookings 2009), and co-editor of the State of Play: Law and Virtual Worlds (NYU 2006).\n\nShe graduated from Harvard University with an AM \"magna cum laude\", and University of Innsbruck with a PhD. She graduated from Yale Law School with a JD.\n\nShe directs The Governance Lab and its MacArthur Research Network on Opening Governance, which is designed to improve governance in governments and elsewhere.\n\nShe is currently the Jerry Hultin Global Network Professor at New York University’s Tandon School of Engineering. She was formerly the Jacob K. Javits Visiting Professor at the Robert F. Wagner Graduate School of Public Service and a visiting professor at the MIT Media Lab. She is a professor of law at New York Law School and a Senior Fellow at the Yale Law School Information Society Project. She served in the White House as the first United States Deputy Chief Technology Officer and director of the White House Open Government Initiative (2009-2011). UK Prime Minister David Cameron appointed her senior advisor for Open Government, and she served on the Obama-Biden transition team. She’s also designed or collaborated on Unchat, The Do Tank, Peer To Patent, Data.gov, Challenge.gov and the Gov Lab’s Living Labs and training platform, The Academy.\n\nShe serves on the Global Commission on Internet Governance and chaired the ICANN Strategy Panel on Multi-Stakeholder Innovation. She is a member of the Advisory Board of the Open Contracting Partnership. She was named one of the Top 100 Global Thinkers by \"Foreign Policy\", one of the “100 Most Creative People in Business” by Fast Company and one of the “Top Women in Technology” by Huffington Post. She has also been honored by both the National Democratic Institute and Public Knowledge for her work in civic technology.\n\nShe is the author of Wiki Government: How Technology Can Make Government Better, Democracy Stronger and Citizens More Powerful, which has also appeared in Arabic, Russian, Chinese and in an audio edition, and co-editor of The State of Play: Law, Games and Virtual Worlds. Her latest book Smart Citizens, Smarter State: The Technologies of Expertise and the Future of Governing appeared with Harvard University Press in 2015.\n\nPreviously, Noveck directed the Institute for Information Law & Policy and the Democracy Design Workshop at New York Law School where she is on-leave as a professor. She is the founder of the \"Do Tank,\" and the State of Play Conferences, and launched Peer-to-Patent, the first community patent review project, in collaboration with the United States Patent and Trade Office. She has taught in the areas of intellectual property, innovation, and constitutional law, as well as courses on electronic democracy and electronic government.\n\nIn August 2018 Noveck was nominated as one of ten members for the newly created \"Digitalrat\", a council to advise the Federal government of Germany on issues concerning the digital transformation of society.\n\n"}
{"id": "1713537", "url": "https://en.wikipedia.org/wiki?curid=1713537", "title": "Bioenergy", "text": "Bioenergy\n\nBioenergy is renewable energy made available from materials derived from biological sources. Biomass is any organic material which has stored sunlight in the form of chemical energy. As a fuel it may include wood, wood waste, straw, and other crop residues, manure, sugarcane, and many other by-products from a variety of agricultural processes. By 2010, there was of globally installed bioenergy capacity for electricity generation, of which was in the United States.\n\nIn its most narrow sense it is a synonym to biofuel, which is fuel derived from biological sources. In its broader sense it includes biomass, the biological material used as a biofuel, as well as the social, economic, scientific and technical fields associated with using biological sources for energy. This is a common misconception, as bioenergy is the energy extracted from the biomass, as the biomass is the fuel and the bioenergy is the energy contained in the fuel\n\nThere is a slight tendency for the word \"bioenergy\" to be favoured in Europe compared with \"biofuel\" in America.\n\nOne of the advantages of biomass fuel is that it is often a by-product, residue or waste-product of other processes, such as farming, animal husbandry and forestry. In theory this means there is no competition between fuel and food production, although this is not always the case. Land use, existing biomass industries and relevant conversion technologies must be considered when evaluating suitability of developing biomass as feedstock for energy.\n\nBiomass is the material derived from recently living organisms, which includes plants, animals and their byproducts. Manure, garden waste and crop residues are all sources of biomass. It is a renewable energy source based on the carbon cycle, unlike other natural resources such as petroleum, coal, and nuclear fuels. Another source includes Animal waste, which is a persistent and unavoidable pollutant produced primarily by the animals housed in industrial-sized farms.\n\nThere are also agricultural products specifically being grown for biofuel production. These include corn, and soybeans and to some extent willow and switchgrass on a pre-commercial research level, primarily in the United States; rapeseed, wheat, sugar beet, and willow ( in Sweden) primarily in Europe; sugarcane in Brazil; palm oil and miscanthus in Southeast Asia; sorghum and cassava in China; and jatropha in India. Hemp has also been proven to work as a biofuel. Biodegradable outputs from industry, agriculture, forestry and households can be used for biofuel production, using e.g. anaerobic digestion to produce biogas, gasification to produce syngas or by direct combustion. Examples of biodegradable wastes include straw, timber, manure, rice husks, sewage, and food waste. The use of biomass fuels can therefore contribute to waste management as well as fuel security and help to prevent or slow down climate change, although alone they are not a comprehensive solution to these problems.\n\nBiomass can be converted to other usable forms of energy like methane gas or transportation fuels like ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gas—also called \"landfill gas\" or \"biogas.\" Crops, such as corn and sugar cane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products like vegetable oils and animal fats. Also, Biomass to liquids (BTLs) and cellulosic ethanol are still under research.\n\nThe use of municipal and household waste is on the forefront of new sources for biomass, and is a largely discarded resource on which new research is being conducted for use of energy production. A new bioenergy sewage treatment process aimed at developing countries is now on the horizon; the Omni Processor is a self-sustaining process which uses the sewerage solids as fuel to convert sewage waste water into drinking water and electrical energy.\nSewage sludge is a point of focus in current research for developing bioenergy from biomass. The large quantity being produced by households at a continuous rate presents an opportunity to extract valuable compounds contained within it which can be then used to produce bioenergy. The main form of bioenergy being produced from sewage is methane, but producing other forms is still being researched. The use of sewage to produce methane reduces the amount of waste put into landfills, its costs of transportation and disposal, and also keeps a larger amount of gas out of the atmosphere, as more is able to be captured. .\n\nThe biomass used for electricity production ranges by region. Forest byproducts, such as wood residues, are popular in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks). Animal husbandry residues, such as poultry litter, is popular in the UK.\n\nSucrose accounts for little more than 30% of the chemical energy stored in the mature plant; 35% is in the leaves and stem tips, which are left in the fields during harvest, and 35% are in the fibrous material (bagasse) left over from pressing.\n\nThe production process of sugar and ethanol in Brazil takes full advantage of the energy stored in sugarcane. Part of the bagasse is currently burned at the mill to provide heat for distillation and electricity to run the machinery. This allows ethanol plants to be energetically self-sufficient and even sell surplus electricity to utilities; current production is for self-use and for sale. This secondary activity is expected to boom now that utilities have been induced to pay \"fair price \"(about US$10/GJ or US$0.036/kWh) for 10 year contracts. This is approximately half of what the World Bank considers the reference price for investing in similar projects (see below). The energy is especially valuable to utilities because it is produced mainly in the dry season when hydroelectric dams are running low. Estimates of potential power generation from bagasse range from , depending on technology. Higher estimates assume gasification of biomass, replacement of current low-pressure steam boilers and turbines by high-pressure ones, and use of harvest trash currently left behind in the fields. For comparison, Brazil's Angra I nuclear plant generates .\n\nPresently, it is economically viable to extract about 288 MJ of electricity from the residues of one tonne of sugarcane, of which about 180 MJ are used in the plant itself. Thus a medium-size distillery processing of sugarcane per year could sell about of surplus electricity. At current prices, it would earn US$18 million from sugar and ethanol sales, and about US$1 million from surplus electricity sales. With advanced boiler and turbine technology, the electricity yield could be increased to 648 MJ per tonne of sugarcane, but current electricity prices do not justify the necessary investment. (According to one report, the World Bank would only finance investments in bagasse power generation if the price were at least US$19/GJ or US$0.068/kWh.)\n\nBagasse burning is environmentally friendly compared to other fuels like oil and coal. Its ash content is only 2.5% (against 30–50% of coal), and it contains very little sulfur. Since it burns at relatively low temperatures, it produces little nitrous oxides. Moreover, bagasse is being sold for use as a fuel (replacing heavy fuel oil) in various industries, including citrus juice concentrate, vegetable oil, ceramics, and tyre recycling. The state of São Paulo alone used , saving about US$35 million in fuel oil imports.\n\nResearchers working with cellulosic ethanol are trying to make the extraction of ethanol from sugarcane bagasse and other plants viable on an industrial scale.\n\nAnother form of bioenergy can be attained from microbial fuel cells, in which chemical energy stored in wastewater or soil is converted directly into electrical energy via the metabolic processes of electrogenic micro-organisms. The power generation capability of this technology has not been found to be economically viable till date, however, this technology has found been found to be more useful for chemical treatment processes and student education.\n\nSome forms of forest bioenergy have recently come under fire from a number of environmental organizations, including Greenpeace and the Natural Resources Defense Council, for the harmful impacts they can have on forests and the climate. Greenpeace recently released a report entitled Fuelling a BioMess which outlines their concerns around forest bioenergy. Because any part of the tree can be burned, the harvesting of trees for energy production encourages Whole-Tree Harvesting, which removes more nutrients and soil cover than regular harvesting, and can be harmful to the long-term health of the forest. In some jurisdictions, forest biomass is increasingly consisting of elements essential to functioning forest ecosystems, including standing trees, naturally disturbed forests and remains of traditional logging operations that were previously left in the forest. Environmental groups also cite recent scientific research which has found that it can take many decades for the carbon released by burning biomass to be recaptured by regrowing trees, and even longer in low productivity areas; furthermore, logging operations may disturb forest soils and cause them to release stored carbon. In light of the pressing need to reduce greenhouse gas emissions in the short term in order to mitigate the effects of climate change, a number of environmental groups are opposing the large-scale use of forest biomass in energy production.\n\nThe New Scientist described a scenario in a September 2016 article which illustrated why the journal believed bioenergy can be bad: Suppose you cut down a 50-year oak tree in your garden and use the logs to heat your house instead of coal. Wood emits more carbon dioxide than coal per unit of heat gained and the roots left in the soil emit more carbon dioxide as they rot. If you plant another tree it will soak up that carbon dioxide in about 50 years. But if you had left the original tree in place it would have soaked up the carbon dioxide from the coal and more. It could take centuries before cutting down the tree would give any benefit. But the world needed to cut carbon dioxide over the next few decades if the global warming was to be kept below 3 degrees C. The journal also concluded that official claimed carbon reductions from renewables had been overstated. The European Union, for example, got more 64% of its renewable energy from biomass (mostly wood) but United Nations and EU rules did not count the carbon emissions from burning biomass.\n\nRecently, a new company called Mango materials used bacterial fermentation to produce an intracellular biopolymer, polyhydroxyalkanoate from methane. The great advantage of biopolymers is that it is biodegradable which makes it environment friendly. Because methane is being used that decreases the price of polymers that it would compete with traditional plastics. Also, because methane would be converted into biopolymer that would reduce methane emissions. Chief Executive Officer Molly Morse said that the unused methane would be enough to produce more than three billion pounds of biopolymer. Morse announced in 2017 that using this polymer will reduce the waste in the textile industry because it will be reproduced as biopolymer again in every effective industrial loop.\n\n\n"}
{"id": "46289875", "url": "https://en.wikipedia.org/wiki?curid=46289875", "title": "Blue Force Gear", "text": "Blue Force Gear\n\nBlue Force Gear, Inc. is a United States manufacturer of body armor, Modular Lightweight Load-carrying Equipment (MOLLE) gear, firearm slings, and other tactical equipment that was established in Pooler, Georgia during January 2004. They design and manufacture equipment for law enforcement, U.S. Armed Forces (and NATO Forces), and Sport Shooters. Select equipment manufactured contains a National Stock Number (NSN) and a National Item Identification Number (NIIN) for supplying armed forces. Its name, Blue Force Gear, is derived from military symbology dating back to World War I: blue being for Allied forces and red for enemy forces.\n\nThey are most known for their firearm sling, the Vickers Sling.\n\nOn May 1, 2012 the Vickers Combat Application Sling was assigned NSN: 1005-01-604-0627. Combat trials led by U.S. Marines validated the issue of the NSN in combat trials in Afghanistan (2011) for their new M27 Infantry Rifle. The sling was fully authorized for use on the M4, M4A1 and M16 series of rifles by the United States Marine Corps. The sling then joins over 100,000 other Blue Force Gear slings already issued across the United States Armed Forces.\n\nThe Vickers Sling, a firearm sling, is named after Larry Vickers. Larry Vickers, a retired US Army 1st SFOD-Delta (Delta Force) combat veteran, took part in Operation Just Cause and participated in Operation Acid Gambit, a mission to extract a CIA operative, Kurt Muse, from Modelo Prison in Panama City, Panama. Blue Force Gear and Larry Vickers both originally designed the Vickers Sling product line, the Vickers Combat Application Sling (VCAS).\n\n\n\n\n\n\n\n\n\n"}
{"id": "4210325", "url": "https://en.wikipedia.org/wiki?curid=4210325", "title": "Brass fastener", "text": "Brass fastener\n\nA brass fastener, brad, paper fastener or split pin is a stationery item used for securing multiple sheets of paper together.\nA patent of the fastener was issued in 1866 to George W McGill.\nThe fastener is inserted into punched holes in the stack of paper, and the leaves, or tines, of the legs are separated and bent over to secure the paper. This holds the pin in place and the sheets of paper together. For few sheets of paper, holes can be made using the sharp end of the fastener.\n\nA split pin may be used in place of staples, but they are more commonly used in situations where rotation around the joint is desirable. This lends split pins to use in mobile paper and cardboard models, and they are often used as modern scrapbooking embellishments. In the film industry, brass fasteners are an industry standard in binding screenplays.\n\nIt is shaped somewhat like a nail with a round head and flat, split length. Brass fasteners are made of a soft metal such as brass and the tines are typically of two slightly different lengths to allow easy separation. A brass fastener is similar in design and function to the mechanical counterpart split pins.\n\n"}
{"id": "3563231", "url": "https://en.wikipedia.org/wiki?curid=3563231", "title": "Bubbleator", "text": "Bubbleator\n\nThe Bubbleator was a large, bubble-shaped hydraulic elevator with transparent acrylic glass walls operated from an elevated chair built for the 1962 World's Fair in Seattle. These transparent walls gave the illusion of looking through an actual 'soap bubble' by refracting light to obtain a rainbow-like effect for the riders inside. It was originally part of the Washington State Coliseum (now a sports venue known as KeyArena), where it lifted 100 passengers at a time up one floor through a structure of interlocking aluminum cubes to the \"World of Tomorrow\" exhibit. T. C. Howard of Synergetics, Inc. designed the Bubbleator and the exhibit. After the fair, the Bubbleator was relocated to the Center House at Seattle Center. By 1984, it had been removed and put in storage to make way for the Seattle Children's Museum. It was sold to a private owner in Des Moines, Washington, who recycled the upper part of the dome into a greenhouse. The control chair, which had also been in private hands, was donated to the Museum of History and Industry in 2005.\n\nWhile boarding the Bubbleator, passengers were commanded by an ethereal female voice to \"Please move to the rear of the sphere\", or the \"Martian type\" male elevator operator would say, \"Step to the rear of the Sphere\" in a creepy sci-fi type voice.\n\nThe soundtrack for the Bubbleator was conducted by Attilio Mineo and released as \"Man in Space with Sounds.\"\n"}
{"id": "46421293", "url": "https://en.wikipedia.org/wiki?curid=46421293", "title": "Cellular shades", "text": "Cellular shades\n\nCellular shades are a window covering used to block or filter light and insulate windows to save energy. Cell size can vary. Cell shapes hold trapped air and create a barrier between the window surface and the a room. Since there's no test for shades they can't be ranked, but they qualified for a 2011 US energy tax credit.\n\nWindows and doors make up for approximately one-third of a home’s total thermal loss, according to the Natural Resources Defense Council. This applies to heat loss in winter, and entry of undesired heat in summer. When air inside the room comes in contact with windows, it is cooled or warmed. By convection this air then circulates around the room. Cell shapes in the blinds hold trapped air and create a barrier between the window surface and the room. Shades, however, provide only slight control of air infiltration.\n\nCellular shades can be constructed as single cell, double cell, or triple cell shades. Single cell fabric has an R-Value between 0.28 and 0.44 (1.6 and 2.5 imperial), and double cell fabric has a metric R-value between 0.49 and 0.70 (2.8 and 4.0 imperial). A 6 mm (¼”) thick single pane window has a metric R-value of 0.16 (0.91 imperial).\n\nUnlike window blinds, which are made of hard materials, they are made of a soft paper- or cloth-like material. Typically spun lace and bonded polyester are used, but other fabrics can be used during the manufacturing process.\n\nIn common with all blinds cellular shades can reduce of solar gain in summer, and provide black out for sleeping. Also as with other window treatments, they are raised and lowered horizontally with a string. Cordless cellular shades are available to reduce the risk of strangulation for small children.\n"}
{"id": "9415645", "url": "https://en.wikipedia.org/wiki?curid=9415645", "title": "Clinker (waste)", "text": "Clinker (waste)\n\nClinker is a general name given to waste from industrial processes, particularly those that involve smelting metals, welding, burning fossil fuels and use of a blacksmith's forge, which commonly causes a large buildup of clinker around the tuyere. Clinker often forms a loose, dark deposit consisting of waste materials such as coke, coal, slag, charcoal, and grit. Clinker often has a glassy look to it, usually because of the formation of molten silica compounds during processing. Clinker generally is much denser than coke, and, unlike coke, generally contains too little carbon to be of any value as fuel.\n\n\"Clinker\" is from Dutch, and was originally used in English for bricks; see clinker brick. The term was later applied to hard residue, due to similar appearance.\n\nClinker often is reused as a cheap material for paving footpaths. It is laid and rolled, and forms a hard path with a rough surface that presents less risk of slipping than most loose materials. In sufficient thickness such a layer drains well and is valuable for controlling muddiness. However, if laid without sufficient adhesive, it needs frequent rolling and addition of more clinker to maintain the path in good condition if it is subject to heavy foot traffic. \n\nIn sewage treatment works, the foul water first is screened to remove floating debris. Then it is sedimented to remove insoluble particles. After this, it is sprayed over a filter bed of clinker. Aerobic microbes soon grow in hollows in the clinker, where they kill harmful anaerobic bacteria in the water and remove much of the offensive organic waste.\n\nHistorically, clinker from coal-burning steamships simply was discarded overboard, leaving detectable trails on the seabed of some prominent steamship routes. As such, the deposits have proven to be of both biological and archaeological interest.\n\n"}
{"id": "20878968", "url": "https://en.wikipedia.org/wiki?curid=20878968", "title": "Correct sampling", "text": "Correct sampling\n\nDuring sampling of granular materials (whether airborne, suspended in liquid, aerosol, or aggragated), correct sampling is defined in Gy's sampling theory as a sampling scenario in which all particles in a population have the same probability of ending up in the sample.\n\nThe concentration of the property of interest in a sample can be a biased estimate for the concentration of the property of interest in the population from which the sample is drawn. Although generally non-zero, for correct sampling this bias is thought to be negligible.\n\n"}
{"id": "3103029", "url": "https://en.wikipedia.org/wiki?curid=3103029", "title": "Doming (television)", "text": "Doming (television)\n\nDoming is a phenomenon found on some CRT televisions in which parts of the shadow mask become heated. In televisions that exhibit this behavior, it tends to occur in high-contrast scenes in which there is a largely dark scene with one or more localized bright spots. As the electron beam hits the shadow mask in these areas it heats unevenly. The shadow mask warps due to the heat differences, which causes the electron gun to hit the wrong colored phosphors and incorrect colors to be displayed in the affected area.\n"}
{"id": "17960514", "url": "https://en.wikipedia.org/wiki?curid=17960514", "title": "Drum stick", "text": "Drum stick\n\nA drumstick is a type of percussion mallet used particularly for playing snare drum, drum kit and some other percussion instruments, and particularly for playing unpitched percussion.\n\nSpecialized beaters used on some other percussion instruments, such as the metal beater or \"wand\" used with a triangle, and particularly beaters or \"mallets\" used with tuned percussion such as xylophone and timpani, are not normally referred to as \"drumsticks\". Drumsticks generally have all of the following characteristics:\n\n\nThe archetypical drumstick is turned from a single piece of wood, most commonly of hickory, less commonly of maple, and least commonly but still in significant numbers, of oak. Drumsticks of the traditional form are also made from metal, carbon fibre and other modern materials.\nThe tip or bead is the part most often used to strike the instrument. Originally and still commonly of the same piece of wood as the rest of the stick, sticks with nylon tips have also been available since 1958, originally conceived by Jonathan Humphrey and Joe Calato in Niagara Falls, NY. In the 1970s, an acetal tip was introduced, designed by Ken Drinan and Paul Kiersted.\n\nTips of whatever material are of various shapes, including acorn, barrel, oval, teardrop, pointed and round.\n\nThe shoulder of the stick is the part that tapers towards the tip, and is normally slightly convex. It is often used for playing the bell of a cymbal. It can also be used to produce a cymbal crash when applied with a glancing motion to the bow or edge of a cymbal, and for playing ride patterns on china, swish and pang cymbals.\n\nThe shaft is the body of the stick, and is cylindrical for most applications including drum kit and orchestral work. It is used for playing cross stick and applied in a glancing motion to the rim of a cymbal for the loudest cymbal crashes.\n\nThe butt is the opposite end of the stick to the tip. Some rock and metal musicians use it rather than the tip.\n\nPlain wooden drumsticks are most commonly described using a number to describe the weight and diameter of the stick followed by one or more letters to describe the tip. For example, a 7A is a common jazz stick with a wooden tip, while a 7AN is the same weight of stick with a nylon tip, and a 7B is a wooden tip but with a different tip profile, shorter and rounder than a 7A. A 5A is a common wood tipped rock stick, heavier than a 7A but with a similar profile. The numbers are most commonly odd but even numbers are used occasionally, in the range 2 (heaviest) to 9 (lightest).\n\nThe exact meanings of both numbers and letters differ from manufacturer to manufacturer, and some sticks are not described using this system at all, just being known as \"Jazz\" (typically a 7A, 8A or 8D) or \"Heavy Rock\" (typically a 5B) for example. The most general purpose stick is a 5A. However, there is no one stick for any particular style of music.\n\nThere are two main ways of holding drumsticks:\n\nTraditional grip was developed to conveniently play a snare drum while riding a horse, and was documented and popularised by Sanford A. Moeller in \"The Art of Snare Drumming\" (1925). It was the standard grip for kit drummers in the first half of the twentieth century and remains popular, and the standard grip for most snare drummers.\n\nMatched grips are standardized for most other instruments, and became popular for kit drumming towards the middle of the twentieth century, threatening to displace the traditional grip for kit drumming. However the traditional grip has since made a comeback, and both types of grip are still used and promoted by leading kit drummers and teachers.\n\n"}
{"id": "31032136", "url": "https://en.wikipedia.org/wiki?curid=31032136", "title": "Engineering for Change", "text": "Engineering for Change\n\nEngineering for Change (E4C) is an online platform and international community of engineers, scientists, non-governmental organizations, local community advocates and other innovators working to solve global development problems. The organization's founders are the American Society of Mechanical Engineers, the Institute of Electrical and Electronics Engineers, and Engineers Without Borders USA. E4C facilitates the development of affordable, locally appropriate and sustainable solutions to the most pressing humanitarian challenges and shares them freely online as a form of open source appropriate technology.\n\nMembers of the E4C community use the platform's online tools to share knowledge and collaborate. They work together to design and apply technical solutions wherever they see the need. Solutions fall into seven categories on the organization's Web site, and they can include big infrastructural projects such as community water purification and bridge building, or smaller, personal technologies such as bicycle-powered electricity generators and cellphone applications for healthcare.\n\nIn 2009, the American Society of Mechanical Engineers created a website to pull together the disparate sources of information on appropriate technology and solutions in global development. The site aggregated information, hosted a library of often little-known technologies, and offered tools to enable collaboration among development teams worldwide. Throughout 2010, the site operated in alpha and then beta with a mostly closed group of users. A public site, at engineeringforchange.info, mirrored some of the content on the test site, but without all of its functionality. IEEE and EWB-USA signed on as partners in time for the public launch on January 4, 2011.\n\nAt present, the organization has more than 25,000 members. Many of them are also members of the founding organizations, which have a combined membership of more than 500,000.\n\nE4C users can post projects they are working on and challenges they are having to gain insight from the wider community. They can use an open-source archive of solutions to development issues that include models for development projects, tested devices and other information gleaned from global organizations. The members can learn how to use their skills in developing countries and resource-poor areas from experts in their fields. They can also track the projects that interest and contribute their own advice and information.\n\nEducation is an important part of Engineering for Change. The Web site provides educational materials on how to design and implement solutions, and an archive of relevant academic programs.\n\n\n"}
{"id": "18915587", "url": "https://en.wikipedia.org/wiki?curid=18915587", "title": "Fischer assay", "text": "Fischer assay\n\nThe Fischer assay is a standardized laboratory test for determining the oil yield from oil shale to be expected from a conventional shale oil extraction. A 100 gram oil shale sample crushed to <2.38 mm is heated in a small aluminum retort to at a rate of 12°C/min (22°F/min), and held at that temperature for 40 minutes. The distilled vapors of oil, gas, and water are passed through a condenser and cooled with ice water into a graduated centrifuge tube. The oil yields achieved by other technologies are often reported as a percentage of the Fischer Assay oil yield.\n\nThe original Fischer Assay test was developed in the early low temperature coal retorting research by Franz Joseph Emil Fischer and Hans Schrader. It was adapted for evaluating oil shale yields in 1949 by K. E. Stanfield and I. C. Frost.\n\n"}
{"id": "29637649", "url": "https://en.wikipedia.org/wiki?curid=29637649", "title": "Gizmo For You", "text": "Gizmo For You\n\nrdGizmo For You LTD is in Larnaca, Cyprus. Initially manufactured and sold electronic devices based on member ideas and suggestions in a form of \"Wishes\". Currently the organization is focused on a single project under the name OSRC (Open Source Remote Control) which it designed and created.\n\nrdGizmo For You LTD was founded in 2007 by Demetris Rouslan Zavorotnitsienko, the CEO and Lead developer of the organization. Its first products were GPS trackers and hand-held PDA's as well as many other devices which were all Open Source and available to the public for download or modifications. The name Gizmo For You came about because the company creates \"Gizmos\" and naturally \"For You\" relates to everyone since anyone can participate or initiate a project.\n\nIn late 2011 the CEO and founder of the organization, created a device under the name OSRC (Open Source Remote Control). Based on these designs the company redirected all its resources towards the project, recreating the web space to accommodate the newly developed series of products and their community. All past designs, made by Gizmo For You were transferred to the OSRC website and are hosted there for historical reasons.\n\nIn 2013 a need for a 3D printer became clear for the OSRC project and the company created the Ilios 3D printer. Unfortunately due to lack of financing, the OSRC project was suspended and the Ilios 3D printer website was created and separated from the OSRC project. Currently the company is mainly concentrated on developing and supporting the Ilios 3D printer.\n\n"}
{"id": "8699846", "url": "https://en.wikipedia.org/wiki?curid=8699846", "title": "Gold-aluminium intermetallic", "text": "Gold-aluminium intermetallic\n\nA gold-aluminium intermetallic is an intermetallic compound of gold and aluminium that occurs at contacts between the two metals. \n\nThese intermetallics have different properties than the individual metals, which can cause problems in wire bonding in microelectronics. The main compounds formed are AuAl (white plague) and AuAl (purple plague), which both form at high temperatures. \n\nWhite plague is the name of the compound AuAl as well as the problem it causes. It has low electrical conductivity, so its formation at the joint leads to an increase of electrical resistance which can lead to total failure. Purple plague (sometimes known as 'purple death' or Roberts-Austen's 'purple gold') is a brittle, bright-purple compound, AuAl, or about 78.5%Au-21.5%Al by mass. AuAl is the most stable thermally of the Au-Al intermetallic compounds, with a melting point of 1060°C (see phase diagram), similar to that of pure gold. The process of the growth of the intermetallic layers causes reduction in volume, and hence creates cavities in the metal near the interface between gold and aluminium.\n\nOther gold-aluminium intermetallics can cause problems as well. Below 624°C, purple plague is replaced by AuAl, a tan-colored substance. It is a poor conductor and can cause electrical failure of the joint that can lead to mechanical failure. At lower temperatures, about 400–450°C, an interdiffusion process takes place at the junction. This leads to formation of layers of several intermetallic compounds with different compositions, from gold-rich to aluminium-rich, with different growth rates. Cavities form as the denser, faster-growing layers consume the slower-growing ones. This process, known as Kirkendall voiding, leads to both increased electrical resistance and mechanical weakening of the wire bond. When the voids are collected along the diffusion front, a process aided by contaminants present in the lattice, it is known as Horsting voiding, a process similar to and often confused with Kirkendall voiding.\n\nAll problems caused by gold-aluminium intermetallics can be prevented either by using bonding processes that avoid high temperatures (e.g. ultrasonic welding), or by designing circuitry in such a way as to avoid aluminium-to-gold contact using aluminium-to-aluminium or gold-to-gold junctions.\n\n\n"}
{"id": "38749092", "url": "https://en.wikipedia.org/wiki?curid=38749092", "title": "Graphene antenna", "text": "Graphene antenna\n\nA graphene antenna is a proposed high-frequency antenna based on graphene, a one atom thick two dimensional carbon crystal, that would enhance radio communications. The unique structure of graphene would enable these enhancements. Ultimately, the choice of graphene for the basis of this nano antenna was due to the behavior of electrons. This is currently being researched and graphene appears to be a feasible basis for antennas.\n\nIt would be unfeasible to simply reduce traditional metallic antennas to nano sizes, because they would require tremendously high frequencies to operate. Consequently, it would require a lot of power to operate them. Furthermore, electrons in these traditional metals are not very mobile at nano sizes and the necessary electromagnetic waves would not form. However, these limitations would not be an issue with graphene's unique capabilities. A flake of graphene has the potential to hold a series of metal electrodes. Consequently, it would be possible to develop an antenna from this material.\n\nGraphene has a unique structure, wherein, electrons are able to move with minimal resistance. This enables electricity to move at a much faster speed than in metal, which is used for current antennas. Furthermore, as the electrons oscillate, they create an electromagnetic wave atop the graphene layer, referred to as the surface plasmon polariton wave. This would enable the antenna to operate at the lower end of the terahertz frequency, which would be more efficient than the current copper based antennas. Ultimately, researchers envision that graphene will be able to break through the limitations of current antennas.\n\nIt has been estimated that speeds of up to terabits per second can be achieved using such a device. Traditional antennas would require very high frequencies to operate at nano scales, making it an unfeasible option. However, the unique slower movement of electrons in graphene would enable it to operate at lower frequencies making it a feasible option for a nano sized antenna.\n\nResearchers from the Department of Energy’s Oak Ridge National Laboratory (ORNL) have discovered a unique way to create an atomic antenna. Two sheets of graphene can be connected by a silicon wire that is approximately 0.1 nanometer in diameter. This is approximately 100 times smaller than current metal based wires, which can only be reduced to 50 nanometers. This silicon wire however, is a plasmotic device, which would enable the formation of surface plasmon polariton waves required to operate this nano antenna.\n\nSamsung has funded $120,000 for research into the graphene antenna to a team of researchers from the Georgia Institute of Technology and the Polytechnic University of Catalonia. Their research has shown that graphene is a feasible material to make nano antennas with. They have simulated how the electrons would behave, and have confirmed that surface plasmon polariton waves should form. This wave is essential for the graphene antenna to operate at the low end of the terahertz range, making it more efficient than traditional antenna designs. Researchers are currently working on implementing their research, and finding a way to propagate the electromagnetic waves necessary to operate the antenna. Their findings were published in the IEEE Journal on Selected Areas in Communications.\n\nA collaboration between the University of Manchester and an industrial partner developed a new way to manufacture graphene antennas for radio-frequency identification. The antennas are paper-based, flexible and environmentally friendly. Their findings were published in Applied Physics Letters and are being commercialised by Graphene Security.\n\n\n"}
{"id": "595294", "url": "https://en.wikipedia.org/wiki?curid=595294", "title": "Hansen Writing Ball", "text": "Hansen Writing Ball\n\nThe Hansen Writing Ball is an early typewriter. It was invented in 1865 and patented and put into production in 1870, and was the first commercially produced typewriter.\n\nThe writing ball (Danish: \"skrivekugle\") was invented in 1865 by the reverend Rasmus Malling-Hansen (1835–1890) principal of the Royal Institute for the Deaf-Mutes in Copenhagen.\n\nThe Hansen ball was a combination of unusual design and ergonomic innovations: its distinctive feature was an arrangement of 52 keys on a large brass hemisphere, causing the machine to resemble an oversized pincushion. From the book \"Hvem er Skrivekuglens Opfinder\", written by Malling-Hansen's daughter Johanne Agerskov, we know how Malling-Hansen made experiments with a model of his writing ball made out of porcelain. He tried out different placements of the letters on the keys, to work out the placement that led to the quickest writing speed. He ended up placing the most frequently used letters to be touched by the fastest writing fingers, and also placed most of the vowels to the left and the consonants to the right. This, together with the short pistons which went directly through the ball, made the writing speed of the writing ball very fast.\n\nLike most of the early 19th-century typewriters, it did not allow the paper to be seen as it passed through the device.\n\nThe first models typed on a paper attached to a cylinder, which could be made to move both rotationally with the cylinder and longitudinally along the cylinder, enabling the user to format and space the letters manually. The user would attach a piece of white paper and a sheet of coloring paper onto the cylinder by way of several clips. These models also included an electro-magnet for the Ball which controlled both the typewriter's movement and manipulation, thus making Malling-Hansen's machine the first electric typewriter. This electro-magnet was powered by a 10 or 12-cell battery, and controlled a mechanical escapement in the typewriter's clockwork, moving the carriage a fixed amount each time one of the pistons was depressed.\n\nMalling-Hansen made several improvements on his invention throughout the 1870s and 1880s, and in 1874 he patented the next model, and now the cylinder was replaced by a flat mechanical paper-frame. The electromagnetic battery was still used to move the paper along as the Ball typed upon it, and the design led to a lower possibility for error. Malling-Hansen improved further on his design, and created a semi-cylindrical frame to hold one sheet of paper. This best known model was first patented in 1875, and now the battery was replaced by a mechanical escapement. All these improvements made for a simpler and more compact writing apparatus.\n\nIt was exhibited at a great industrial exhibition in Copenhagen in 1873, at the world exhibition in Vienna in 1873, and at the Paris exhibition or Exposition Universelle. All through the 1870s it won several awards.\n\nThe writing ball was sold in many countries in Europe, and it is known that it was still in use in offices in London as late as 1909. But due to its hand-crafted production, it was overtaken in the market by the mass-produced Sholes and Glidden typewriter which E. Remington and Sons started to make in 1873.\n\nMalling-Hansen also invented a very high speed writing machine for stenography, called the Takygraf, and a copying technique called the Xerografi—both invented in 1872.\n\nMore-or-less intact Hansen balls have fetched hundreds of thousands of Euros in auctions. Few remain in existence today.\n\nIn 1881, when he had serious problems with his sight, Friedrich Nietzsche wanted to buy a typewriter to enable him to continue his writing, and from letters to his sister it is known that he personally was in contact with \"the inventor of the typewriter, Mr Malling-Hansen from Copenhagen\". He mentioned to his sister that he had received letters and also a typewritten postcard as an example. Nietzsche received his writing ball in 1882 directly from the inventor, Rasmus Malling-Hansen, in Copenhagen, Denmark. It was the newest model, the portable tall one with a color ribbon, serial number 125, and several typescripts are known to have been written by him on this writing ball (approximately 60). It is known that Nietzsche was also familiar with the newest model from E. Remington and Sons (model 2), but he wanted to buy a portable typewriter, so he chose to buy the Malling-Hansen writing ball, as this model was lightweight and easy to carry. Unfortunately, Nietzsche wasn't totally satisfied with his purchase and never really mastered the use of the instrument. A number of theories have been advanced to explain why Nietzsche did not make more use of it. For example, Rüdiger Safranski indicates it was \"defective\". New research indicates Nietzsche was not aware that his trouble in using the machine had been caused by damage to it during transportation to Genoa in Italy, where he lived at the time, and when he turned to a mechanic who had no typewriter repair skills, the man managed to damage the writing ball even more. Nietzsche claimed that his thoughts were influenced by his use of a typewriter (\"Our writing instruments contribute to our thoughts\", 1882). As one researcher has noted, \"Nietzsche's interest in rhetoric and his experience of the typewriter framed his understanding of language in a highly symbolic way: the traditions of the philosophy of language versus the scientific and technological conditions of knowledge.\" On February 16, 1882 he even made a poem about his writing ball.\n\n"}
{"id": "2435889", "url": "https://en.wikipedia.org/wiki?curid=2435889", "title": "History of photography", "text": "History of photography\n\nThe history of photography began in remote antiquity with the discovery of two critical principles; the camera obscura image projection and the observation that some substances are visibly altered by exposure to light. Apart from a possibly photographic but unrecognised process used on the Turin Shroud there are no artefacts or descriptions that indicate any attempt to capture images with light sensitive materials prior to the 18th century.\nAround 1717 Johann Heinrich Schulze captured cut-out letters on a bottle of a light-sensitive slurry, but he apparently never thought of making the results durable. Around 1800 Thomas Wedgwood made the first reliably documented, although unsuccessful attempt at capturing camera images in permanent form. His experiments did produce detailed photograms, but Wedgwood and his associate Humphry Davy found no way to fix these images.\n\nIn the mid-1820s, Nicéphore Niépce first managed to fix an image that was captured with a camera, but at least eight hours or even several days of exposure in the camera were required and the earliest results were very crude. Niépce's associate Louis Daguerre went on to develop the daguerreotype process, the first publicly announced and commercially viable photographic process. The daguerreotype required only minutes of exposure in the camera, and produced clear, finely detailed results. The details were introduced as a gift to the world in 1839, a date generally accepted as the birth year of practical photography.\nThe metal-based daguerreotype process soon had some competition from the paper-based calotype negative and salt print processes invented by William Henry Fox Talbot. Subsequent innovations made photography easier and more versatile. New materials reduced the required camera exposure time from minutes to seconds, and eventually to a small fraction of a second; new photographic media were more economical, sensitive or convenient, including roll films for casual use by amateurs. In the mid-20th century, developments made it possible for amateurs to take pictures in natural color as well as in black-and-white.\n\nThe commercial introduction of computer-based electronic digital cameras in the 1990s soon revolutionized photography. During the first decade of the 21st century, traditional film-based photochemical methods were increasingly marginalized as the practical advantages of the new technology became widely appreciated and the image quality of moderately priced digital cameras was continually improved. Especially since cameras became a standard feature on smartphones, taking pictures (and instantly publishing them online) has become an ubiquitous everyday practice around the world.\n\nThe coining of the word \"photography\" is usually attributed to Sir John Herschel in 1839. It is based on the Greek \"φῶς (phōs)\", (genitive: \"phōtós\") meaning \"light\", and \"γραφή (graphê)\", meaning \"drawing, writing\", together meaning \"drawing with light\".\n\nA natural phenomenon, known as camera obscura or pinhole image, can project a (reversed) image through a small opening onto an opposite surface. This principle may have been known and used in prehistoric times. The earliest known written record of the camera obscura is to be found in Chinese writings called Mozi, dated to the 4th century BCE. Until the 16th century the camera obscura was mainly used to study optics and astronomy, especially to safely watch solar eclipses without damaging the eyes. \nIn the later half of the 16th century some technical improvements were developed: a (biconvex) lens in the opening (first described by Gerolamo Cardano in 1550) and a diaphragm restricting the aperture (Daniel Barbaro in 1568) gave a brighter and sharper image. In 1558 Giambattista della Porta advised using the camera obscura as a drawing aid in his popular and influential books. Della Porta's advice was widely adopted by artists and since the 17th century portable versions of the camera obscura were commonly used - first as a tent, later as boxes. The box type camera obscura was the basis for the earliest photographic cameras when photography was developed in the early 19th century.\nThe notion that light can affect various substances - for instance the suntanning of skin or fading of textile - must have been around since very early times. Ideas of fixing the image seen in mirrors or other ways of creating images automatically may also have been in people's mind long before anything like photography was developed. However, there seem to be no historical records of any ideas even remotely resembling photography before 1725, despite early knowledge of light-sensitive materials and the camera obscura.\nIt has been suggested that some lost type of photographic technology must have been applied before 1357: the Shroud of Turin contains an image that resembles a sepia photographic negative and is much clearer when it is converted to a positive image. The actual method that resulted in this image has not yet been conclusively identified. It first appeared in historical records in 1357 and radiocarbon dating tests indicate it was probably made between 1260 and 1390. No other examples of detailed negative images from before the 19th century are known.\n\nAlbertus Magnus (1193/1206–80) discovered silver nitrate and noted that it could blacken skin. Silver nitrate would later be used as a light sensitive material in the photographic emulsion on photographic glass plates and film.\n\nGeorg Fabricius (1516–71) discovered silver chloride, later used to make photographic paper.\n\nIn 1614 Angelo Sala wrote in his paper \"Septem Planetarum terrestrium Spagirica recensio\": \"When you expose powdered silver nitrate to sunlight, it turns black as ink\". He also noted that paper wrapped around silver nitrate for a year had turned black.\n\nWilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694.\n\nAround 1717 German polymath Johann Heinrich Schulze accidentally discovered that a slurry of chalk and nitric acid into which some silver particles had been dissolved was darkened by sunlight. After experiments with threads that had created lines on the bottled substance after he placed it in direct sunlight for a while, he applied stencils of words to the bottle. The stencils produced copies of the text in dark red, almost violet characters on the surface of the otherwise whitish contents. The impressions persisted until they were erased by shaking the bottle or until overall exposure to light obliterated them. Schulze named the substance \"Scotophorus\", when he published his findings in 1719. He thought the discovery could be applied to detect whether metals or minerals contained any silver and hoped that further experimentation by others would lead to some other useful results. Schulze's process resembled later photogram techniques and is sometimes regarded as the very first form of photography.\n\nThe early science fiction novel \"Giphantie\" (1760) by the French Tiphaigne de la Roche described something quite similar to (colour) photography, a process that fixes fleeting images formed by rays of light: \"They coat a piece of canvas with this material, and place it in front of the object to capture. The first effect of this cloth is similar to that of a mirror, but by means of its viscous nature the prepared canvas, as is not the case with the mirror, retains a facsimile of the image. The mirror represents images faithfully, but retains none; our canvas reflects them no less faithfully, but retains them all. This impression of the image is instantaneous. The canvas is then removed and deposited in a dark place. An hour later the impression is dry, and you have a picture the more precious in that no art can imitate its truthfulness.\" De la Roche thus imagined a process that made use of a special substance in combination with the qualities of a mirror, rather than the camera obscura. The hour of drying in a dark place suggests he possibly thought about the light sensitivity of the material, but he attributes the effect to its viscous nature.\n\nIn 1777, the chemist Carl Wilhelm Scheele was studying the more intrinsically light-sensitive silver chloride and determined that light darkened it by disintegrating it into microscopic dark particles of metallic silver. Of greater potential usefulness, Scheele found that ammonia dissolved the silver chloride but not the dark particles. This discovery could have been used to stabilize or \"fix\" a camera image captured with silver chloride, but was not picked up by the earliest photography experimenters.\n\nScheele also noted that red light did not have much effect on silver chloride (a feature that would later be applied to be able to see while printing black and white photographs in darkrooms).\n\nAlthough Thomas Wedgwood felt inspired by Scheele's writings in general, he must have missed or forgotten these experiments: he found no method to fix the photogram and shadow images he managed to capture around 1800 (see below).\n\nThomas Wedgwood (1771-1805) is believed to have been the first person to have thought of creating permanent pictures by capturing camera images on material coated with a light-sensitive chemical. He originally wanted to capture the images of a camera obscura, but found they were too faint to have an effect upon the silver nitrate solution that was advised to him as a light-sensitive substance. Wedgwood did manage to copy painted glass plates and captured shadows on white leather as well as on paper moistened with a silver nitrate solution. Attempts to preserve the results with their \"distinct tints of brown or black, sensibly differing in intensity\" failed. \nIt is unclear when Wedgwood's experiments took place. He may have started before 1790; James Watt wrote a letter to Thomas Wedgwood's father Josiah Wedgwood to thank him \"for your instructions as to the Silver Pictures, about which, when at home, I will make some experiments\". This letter (now lost) is believed to have been written in 1790, 1791 or 1799. In 1802 an account by Humphry Davy detailing Wedgwood's experiments was published in an early journal of the Royal Institution with the title \"An Account of a Method of Copying Paintings upon Glass, and of Making Profiles, by the Agency of Light upon Nitrate of Silver\". \nDavy added that the method could be used for objects that are partly opaque and partly transparent to create accurate representations of for instance \"the woody fibres of leaves and the wings of insects\". He also found that solar microscope images of small objects were easily captured on prepared paper. Davy, apparently unaware or forgetful about Scheele's discovery, concluded that substances should be found to get rid of (or deactivate) the unexposed particles in silver nitrate or silver chloride \"to render the process as useful as it is elegant\". Wedgwood may have prematurely abandoned his experiments due to his frail and failing health. He died aged 34 in 1805.\n\nDavy seems not to have continued the experiments. Although the journal of the small, infant Royal Institution probably reached its very small group of members, the article eventually must have been read by many more people. It was reviewed by David Brewster in the \"Edinburgh Magazine\" in December 1802, appeared in chemistry textbooks as early as 1803, was translated into French, and published in German in 1811. Readers of the article may have been discouraged to find a fixer, because the highly acclaimed scientist Davy had already tried and failed. Apparently the article was not noted by Niépce or Daguerre, and by Talbot only after he had developed his own processes.\n\nFrench balloonist/professor/inventor Jacques Charles is believed to have captured fleeting negative photograms of silhouettes on light sensitive paper at the start of the 19th century, prior to Wedgwood. Charles died in 1823 without documenting the process, but purportedly demonstrated it in his lectures at the Louvre. It was not publicized until François Arago mentioned it at his introduction of the details of the Daguerreotype to the world in 1839. He later wrote that the first idea of fixing the images of the camera obscura or the solar microscope with chemical substances belonged to Charles. Later historians probably only built on Arago's information and much later the unsupported year 1780 was attached to it. Since Arago indicated the first years of the 19th century and a date prior to Wedgwood's process published in 1802, this would mean that Charles' demonstrations took place in 1800 or 1801 - assuming Arago was this accurate almost 40 years later.\n\nIn 1816 Nicéphore Niépce, using paper coated with silver chloride, succeeded in photographing the images formed in a small camera, but the photographs were negatives, darkest where the camera image was lightest and vice versa, and they were not permanent in the sense of being reasonably light-fast; like earlier experimenters, Niépce could find no way to prevent the coating from darkening all over when it was exposed to light for viewing. Disenchanted with silver salts, he turned his attention to light-sensitive organic substances.\n\nThe oldest surviving photograph of the image formed in a camera was created by Niépce in 1826 or 1827. It was made on a polished sheet of pewter and the light-sensitive substance was a thin coating of bitumen, a naturally occurring petroleum tar, which was dissolved in lavender oil, applied to the surface of the pewter and allowed to dry before use. After a very long exposure in the camera (traditionally said to be eight hours, but now believed to be several days), the bitumen was sufficiently hardened in proportion to its exposure to light that the unhardened part could be removed with a solvent, leaving a positive image with the light areas represented by hardened bitumen and the dark areas by bare pewter. To see the image plainly, the plate had to be lit and viewed in such a way that the bare metal appeared dark and the bitumen relatively light.\n\nIn partnership, Niépce in Chalon-sur-Saône and Louis Daguerre in Paris refined the bitumen process, substituting a more sensitive resin and a very different post-exposure treatment that yielded higher-quality and more easily viewed images. Exposure times in the camera, although substantially reduced, were still measured in hours.\n\nNiépce died suddenly in 1833, leaving his notes to Daguerre. More interested in silver-based processes than Niépce had been, Daguerre experimented with photographing camera images directly onto a mirror-like silver-surfaced plate that had been fumed with iodine vapor, which reacted with the silver to form a coating of silver iodide. As with the bitumen process, the result appeared as a positive when it was suitably lit and viewed. Exposure times were still impractically long until Daguerre made the pivotal discovery that an invisibly slight or \"latent\" image produced on such a plate by a much shorter exposure could be \"developed\" to full visibility by mercury fumes. This brought the required exposure time down to a few minutes under optimum conditions. A strong hot solution of common salt served to stabilize or fix the image by removing the remaining silver iodide. On 7 January 1839, this first complete practical photographic process was announced at a meeting of the French Academy of Sciences, and the news quickly spread. At first, all details of the process were withheld and specimens were shown only at Daguerre's studio, under his close supervision, to Academy members and other distinguished guests. Arrangements were made for the French government to buy the rights in exchange for pensions for Niépce's son and Daguerre and present the invention to the world (with the exception of Great Britain, where an agent for Daguerre patented it) as a free gift. Complete instructions were made public on 19 August 1839. Known as the Daguerreotype process, it was the most common commercial process until the late 1850s. It was superseded by the collodion process.\n\nAfter reading early reports of Daguerre's invention, Henry Fox Talbot, who had succeeded in creating stabilized photographic negatives on paper in 1835, worked on perfecting his own process. In early 1839, he acquired a key improvement, an effective fixer, from his friend John Herschel, a polymath scientist who had previously shown that hyposulfite of soda (commonly called \"hypo\" and now known formally as sodium thiosulfate) would dissolve silver salts. News of this solvent also benefited Daguerre, who soon adopted it as a more efficient alternative to his original hot salt water method.\nTalbot's early silver chloride \"sensitive paper\" experiments required camera exposures of an hour or more. In 1841, Talbot invented the calotype process, which, like Daguerre's process, used the principle of chemical development of a faint or invisible \"latent\" image to reduce the exposure time to a few minutes. Paper with a coating of silver iodide was exposed in the camera and developed into a translucent negative image. Unlike a daguerreotype, which could only be copied by rephotographing it with a camera, a calotype negative could be used to make a large number of positive prints by simple contact printing. The calotype had yet another distinction compared to other early photographic processes, in that the finished product lacked fine clarity due to its translucent paper negative. This was seen as a positive attribute for portraits because it softened the appearance of the human face. Talbot patented this process, which greatly limited its adoption, and spent many years pressing lawsuits against alleged infringers. He attempted to enforce a very broad interpretation of his patent, earning himself the ill will of photographers who were using the related glass-based processes later introduced by other inventors, but he was eventually defeated. Nonetheless, Talbot's developed-out silver halide negative process is the basic technology used by chemical film cameras today. Hippolyte Bayard had also developed a method of photography but delayed announcing it, and so was not recognized as its inventor.\n\nIn 1839, John Herschel made the first glass negative, but his process was difficult to reproduce. Slovene Janez Puhar invented a process for making photographs on glass in 1841; it was recognized on June 17, 1852 in Paris by the Académie Nationale Agricole, Manufacturière et Commerciale. In 1847, Nicephore Niépce's cousin, the chemist Niépce St. Victor, published his invention of a process for making glass plates with an albumen emulsion; the Langenheim brothers of Philadelphia and John Whipple and William Breed Jones of Boston also invented workable negative-on-glass processes in the mid-1840s.\n\nIn 1851 Frederick Scott Archer invented the collodion process. Photographer and children's author Lewis Carroll used this process. (Carroll refers to the process as \"Tablotype\" in the story \"A Photographer's Day Out\")\n\nHerbert Bowyer Berkeley experimented with his own version of collodion emulsions after Samman introduced the idea of adding dithionite to the pyrogallol developer. Berkeley discovered that with his own addition of sulfite, to absorb the sulfur dioxide given off by the chemical dithionite in the developer, that dithionite was not required in the developing process. In 1881 he published his discovery. Berkeley's formula contained pyrogallol, sulfite and citric acid. Ammonia was added just before use to make the formula alkaline. The new formula was sold by the Platinotype Company in London as Sulpho-Pyrogallol Developer.\n\nNineteenth-century experimentation with photographic processes frequently became proprietary.The German-born, New Orleans photographer Theodore Lilienthal successfully sought legal redress in an 1881 infringement case involving his \"Lambert Process\" in the Eastern District of Louisiana.\n\nThe daguerreotype proved popular in response to the demand for portraiture that emerged from the middle classes during the Industrial Revolution. This demand, which could not be met in volume and in cost by oil painting, added to the push for the development of photography.\n\nRoger Fenton and Philip Henry Delamotte helped popularize the new way of recording events, the first by his Crimean War pictures, the second by his record of the disassembly and reconstruction of The Crystal Palace in London. Other mid-nineteenth-century photographers established the medium as a more precise means than engraving or lithography of making a record of landscapes and architecture: for example, Robert Macpherson's broad range of photographs of Rome, the interior of the Vatican, and the surrounding countryside became a sophisticated tourist's visual record of his own travels.\n\nIn America, by 1851 a broadside by daguerreotypist Augustus Washington was advertising prices ranging from 50 cents to $10. However, daguerreotypes were fragile and difficult to copy. Photographers encouraged chemists to refine the process of making many copies cheaply, which eventually led them back to Talbot's process.\n\nUltimately, the photographic process came about from a series of refinements and improvements in the first 20 years. In 1884 George Eastman, of Rochester, New York, developed dry gel on paper, or film, to replace the photographic plate so that a photographer no longer needed to carry boxes of plates and toxic chemicals around. In July 1888 Eastman's Kodak camera went on the market with the slogan \"You press the button, we do the rest\". Now anyone could take a photograph and leave the complex parts of the process to others, and photography became available for the mass-market in 1901 with the introduction of the Kodak Brownie.\n\nDaguerreotype cameras were advertised in Calcutta a year after their invention in France — but photographic societies in Bombay, Calcutta and Madras were beginning to pop up from the 1850s onward. As the practice of photography evolved, a contrasting style developed alongside the predominantly European influence on the artform. This turn is most notable in the work of Raja Deen Dayal, India's most celebrated 19th-century photographer, whose appointment as court photographer to the sixth Nizam of Hyderabad allowed him unique access to the inner circles of aristocratic life.\n\nIn March 1858, the Venetian photographer Felice Beato traveled to Lucknow on assignment from the British War Department to document the massacres there. His most famous photograph is of corpses inside the walled garden of the Secundra Bagh.\n\nA European woman working in Calcutta in the early 1860s, E. Mayer, was likely the first woman to practice photography professionally in India. She operated a portrait studio for women.\n\nA practical means of color photography was sought from the very beginning. Results were demonstrated by Edmond Becquerel as early as 1848, but exposures lasting for hours or days were required and the captured colors were so light-sensitive they would only bear very brief inspection in dim light.\n\nThe first durable color photograph was a set of three black-and-white photographs taken through red, green, and blue color filters and shown superimposed by using three projectors with similar filters. It was taken by Thomas Sutton in 1861 for use in a lecture by the Scottish physicist James Clerk Maxwell, who had proposed the method in 1855. The photographic emulsions then in use were insensitive to most of the spectrum, so the result was very imperfect and the demonstration was soon forgotten. Maxwell's method is now most widely known through the early 20th century work of Sergei Prokudin-Gorskii. It was made practical by Hermann Wilhelm Vogel's 1873 discovery of a way to make emulsions sensitive to the rest of the spectrum, gradually introduced into commercial use beginning in the mid-1880s.\n\nTwo French inventors, Louis Ducos du Hauron and Charles Cros, working unknown to each other during the 1860s, famously unveiled their nearly identical ideas on the same day in 1869. Included were methods for viewing a set of three color-filtered black-and-white photographs in color without having to project them, and for using them to make full-color prints on paper.\n\nThe first widely used method of color photography was the Autochrome plate, a process inventors and brothers Auguste and Louis Lumière began working on in the 1890s and commercially introduced in 1907. It was based on one of Louis Ducos du Hauron's ideas: instead of taking three separate photographs through color filters, take one through a mosaic of tiny color filters overlaid on the emulsion and view the results through an identical mosaic. If the individual filter elements were small enough, the three primary colors of red, blue, and green would blend together in the eye and produce the same additive color synthesis as the filtered projection of three separate photographs. \n\nAutochrome plates had an integral mosaic filter layer with roughly five million previously dyed potato grains per square inch added to the surface. Then through the use of a rolling press, five tons of pressure were used to flatten the grains, enabling every one of them to capture and absorb color and their microscopic size allowing the illusion that the colors are merged. The final step was adding a coat of the light capturing substance silver bromide after which a color image could be imprinted and developed. In order to see it, reversal processing was used to develop each plate into a transparent positive that could be viewed directly or projected with an ordinary projector. One of the drawbacks of the technology is an exposure time of at least a second was required during the day in bright light and the worse the light is, the time required quickly goes up. An indoor portrait required a few minutes with the subject not being able to move or else the picture would come out blurry. This was because the grains absorbed the color fairly slowly and that a filter of a yellowish-orange color was added to the plate to keep the photograph from coming out excessively blue. Although necessary, the filter had the effect of reducing the amount of light that was absorbed. Another drawback was that the film could only be enlarged so much until the many dots that make up the image become apparent.\n\nCompeting screen plate products soon appeared and film-based versions were eventually made. All were expensive and until the 1930s none was \"fast\" enough for hand-held snapshot-taking, so they mostly served a niche market of affluent advanced amateurs.\n\nA new era in color photography began with the introduction of Kodachrome film, available for 16 mm home movies in 1935 and 35 mm slides in 1936. It captured the red, green, and blue color components in three layers of emulsion. A complex processing operation produced complementary cyan, magenta, and yellow dye images in those layers, resulting in a subtractive color image. Maxwell's method of taking three separate filtered black-and-white photographs continued to serve special purposes into the 1950s and beyond, and Polachrome, an \"instant\" slide film that used the Autochrome's additive principle, was available until 2003, but the few color print and slide films still being made in 2015 all use the multilayer emulsion approach pioneered by Kodachrome.\n\nIn 1957, a team led by Russell A. Kirsch at the National Institute of Standards and Technology developed a binary digital version of an existing technology, the wirephoto drum scanner, so that alphanumeric characters, diagrams, photographs and other graphics could be transferred into digital computer memory. One of the first photographs scanned was a picture of Kirsch's infant son Walden. The resolution was 176x176 pixels with only one bit per pixel, i.e., stark black and white with no intermediate gray tones, but by combining multiple scans of the photograph done with different black-white threshold settings, grayscale information could also be acquired.\n\nThe charge-coupled device (CCD) is the image-capturing optoelectronic component in first-generation digital cameras. It was invented in 1969 by Willard Boyle and George E. Smith at AT&T Bell Labs as a memory device. The lab was working on the Picturephone and on the development of semiconductor bubble memory. Merging these two initiatives, Boyle and Smith conceived of the design of what they termed \"Charge 'Bubble' Devices\". The essence of the design was the ability to transfer charge along the surface of a semiconductor. It was Dr. Michael Tompsett from Bell Labs however, who discovered that the CCD could be used as an imaging sensor. The CCD has increasingly been replaced by the active pixel sensor (APS), commonly used in cell phone cameras. These mobile phone cameras are used by billions of people worldwide, dramatically increasing photographic activity and material and also fueling citizen journalism.\n\n\nThe web has been a popular medium for storing and sharing photos ever since the first photograph was published on the web by Tim Berners-Lee in 1992 (an image of the CERN house band Les Horribles Cernettes). Since then sites and apps such as Facebook, Flickr, Instagram, Picasa (discontinued in 2016), Imgur and Photobucket have been used by many millions of people to share their pictures.\n\n\n\n"}
{"id": "22025789", "url": "https://en.wikipedia.org/wiki?curid=22025789", "title": "IThentic", "text": "IThentic\n\niThentic is an online and mobile video content production and distribution company based in Toronto, Ontario Canada. iThentic concentrates on production of original content as well as aggregation of content created solely by independent video producers for mobile and online presentation.\n\niThentic was founded in June 2006 by film and television industry veterans Catherine Tait, partner in Duopoly, former President and COO of Salter Street Films, and former Executive Director of the Independent Feature Project; Al Cattabiani, Founder and former CEO of Wellspring Media; and Liz Manne, partner in Duopoly, former EVP, Programming and Marketing, Sundance Channel, and former EVP of Fine Line Features.\n\nIn July 2007, iThentic launched \"Global Mobile: Food\", a collaboration with ITVS, the San Francisco-based Independent Television Service, that invited eight international filmmakers to create short films on the subject of food. The project was showcased at the 2008 Berlin Film Festival.\n\nIn 2007-8, iThentic co-produced with the National Film Board of Canada \"Mobile Stories: Obsession\", an interactive series of videos and website exploring the narrative format for mobile platforms. The project was nominated for a Canadian New Media Award as well as the International Animation Festival awards. www.mobilestories.ca\n\nIn 2011, iThentic produced an interactive web series entitled \"Guidestones\". The series was a co-production with writer/director Jay Ferguson's 3 O'Clock.TV and was released in 2012. Produced with the assistance of the Independent Production Fund and the Ontario Media Development Council, the series was filmed in Canada, The United States and India.\n\niThentic produces and manages the Shortnonstop Mobile Movie Festival on behalf of The Canadian Film Centre’s Worldwide Short Film Festival. Shortnonstop awards cash prizes of $1,500 to the best entry every quarter. Shortnonstop accepts short film submissions under three minutes running time in English or no dialogue, and from any country of origin. Past SNS winners include Peter Lacalamita's \"The Red Kite\" and \"Moonstruck\", Devin Lim's \"Grimoire\", Keith Claxton's \"Walker Stalker\", Clemens Kogler's \"Herr Bar\", Karen Weiss's \"Bad Head Day\" and most recently Tor Kristoffersen's \"Enough\".\n\nWinners of the Shortnonstop Festival for 2010:\n\niThentic has launched its brand new Shortsnonstop Audience Choice Award, which allows friends and fans to vote for their favorite online film. Audience Award deadlines run one month after entry deadline.\n\nIn April 2007, Barna Alper Productions made an investment in iThentic, shifting its base of operations from New York City to Toronto, making the joint venture the first Canadian company dedicated to the production and aggregation of independently produced video content destined for online and mobile distribution. In November 2008, iThentic entered into a joint venture with Smiley Guy Studios, the Toronto-based production company. iThentic's operations remain based in Toronto but are now managed by Smiley Guy and CEO Jonas Diamond. Catherine Tait serves as Chairman of the company.\n\niThentic's founding partners include the Canadian Film Centre, Center for Asian American Media, Creative Capital, Film Catcher, Filmmaker Magazine, IFP, Indo American Arts Council, ITVS, Link TV and SXSW.\n\n"}
{"id": "29599200", "url": "https://en.wikipedia.org/wiki?curid=29599200", "title": "Ian Hughes (epredator)", "text": "Ian Hughes (epredator)\n\nIan Hughes, also known as epredator, (born 30 or 31 August 1967 in Norfolk, England) is a British metaverse evangelist and Television Personality. In 2006, he set leading set of like-minded individuals and subsequently many thousands of colleagues at IBM into virtual worlds like Second Life, and beyond. (Hughes' effort in Second Life where he is known as Epredator Potato was documented extensively in a 2007 report.) This sparked the massive growth in interest from enterprises and press alike. Hughes was a public figure in Web 2.0 and, formerly, a blogger on Eightbar, a site maintained by former and current IBM employees on the fringes of innovation within their labs. He has also voiced Evil Eater, the antagonist in Weetakid.\n\nA self-described programmer since he was 14, Hughes attended Oriel Grammar School in Gorleston, Great Yarmouth, Norfolk, and De Montfort University (formerly Leicester Polytechnic) in Leicester, Leicestershire, where he received a BSc in Information Technology.\n\nHughes is a former Consulting IT Specialist who worked on emerging technologies at IBM for 20 years. He is now an independent consultant under the business called Feeding Edge Ltd. As a gamer, he describes seeing a massive increase in the capability and design ethics within games and the rise of online gaming.\n\nIn 1997, Hughes started working on the World Wide Web, changing his perspective on the science and technology and business due to the much richer mix of people involved in the web revolution. He told part of this story in a 2008 interview with Rita J. King on \"The Imagination Age\", in which he described how in 1992 he was one of the earliest advocates of using PCs at IBM.\n\nAs a public speaker and Internet personality (Tim Guest's book \"\" features Hughes in a chapter) he shares his experiences both of the growing virtual worlds industry, the changes in culture, personal experiences of challenging the status quo and leading through doing. \"Every time I see something as stupid, I realise it's the thing I should pay the most attention to,\" he said in a 2008 interview.\n\nCultural and technology changes and new ways to operate across companies is a core theme. In 2008 he received the first industry award for Innovation in Virtual Worlds in the Enterprise. \"When I'm in the metaverse, I'm physically at my computer, mentally I am at that virtual location,\" Hughes told the BBC in 2007. As a digital native his epredator persona spans many Web 2.0 places, blogs, PlayStation Network, World of Warcraft, Xbox Live, Twitter, Flickr, Linkedin, etc. Understanding how to use that presence, still representing himself but with elements of theatre brought about by more creative expression online, he says, has led him beyond being the programmer he grew up as.\n\nAs a metaverse evangelist, a term it is believed he first brought into popular use, he is an advocate for the widespread application and use of virtual worlds. Virtual worlds, Hughes says, extend to more than just entertainment or corporate communication. \"They represent past cultures, existing places, instrumenting the planet, showing business process models in actions the list goes on. Metaverses can also be used to deliver virtual goods, but now making those virtual goods real again is possible. Rapid fabrication and the rise of the fabricaneur heralds another phase in our existence online.\"\n\nHe presents a thread on emerging technology on the television series The Cool Stuff Collective by Archie Productions, which was launched in 2010, covering topics such as 3D printers, haptics, scanning and virtual worlds.\n\nHughes is chairman of the British Computer Society (BCS) Animation and Games Specialist Group.\n\n"}
{"id": "58697773", "url": "https://en.wikipedia.org/wiki?curid=58697773", "title": "Lin Xiangdi", "text": "Lin Xiangdi\n\nLin Xiangdi (; 8 February 1934 – 29 July 2018) was a Chinese optoelectronic engineer. He was an academician of the Chinese Academy of Engineering and served as President of the Southwest University of Science and Technology in Sichuan.\n\nLin was born 8 February 1934 in Nantong, Jiangsu, China. After graduating from Zhejiang University in 1956, he worked for the Changchun Institute of Optics, Fine Mechanics and Physics of the Chinese Academy of Sciences (CAS).\n\nIn 1972, Lin moved to Sichuan Province to help establish the Institute of Optics and Electronics of the CAS, and became vice president of the institute. He was elected an academician of the Chinese Academy of Engineering in 1997, and served as president of the Chengdu branch of the CAS. He served as president of the Southwest University of Science and Technology in Mianyang from December 2000 to October 2004, and continued to be president emeritus until his death.\n\nLin died on 29 July 2018 in Chengdu, aged 84.\n"}
{"id": "2348505", "url": "https://en.wikipedia.org/wiki?curid=2348505", "title": "List of World War II weapons of the United Kingdom", "text": "List of World War II weapons of the United Kingdom\n\n\n\n\n\n\n\n\n\n\n\n\n\"see also List of World War II military vehicles by country, United Kingdom\"\n\n\n\n\n\n\n\n\n\nGas bombs \nSmoke bombs\n\n"}
{"id": "27325855", "url": "https://en.wikipedia.org/wiki?curid=27325855", "title": "List of active missiles of the United States military", "text": "List of active missiles of the United States military\n\nThe following is a list of active missiles of the United States military.\n"}
{"id": "29922159", "url": "https://en.wikipedia.org/wiki?curid=29922159", "title": "London Pneumatic Despatch Company", "text": "London Pneumatic Despatch Company\n\nThe London Pneumatic Despatch Company (also known as the London Pneumatic Dispatch Company) was formed on 30 June 1859, to design, build and operate an underground railway system for the carrying of mail, parcels and light freight between locations in London. The system was used between 1863 and 1874.\n\nSir Rowland Hill of the General Post Office commissioned two engineers to investigate the feasibility of a pneumatic tube-based system between the General Post Office and the West District Central Post Office. In 1855 and 1856 they reported favourably but there would be significant cost. The scheme was not progressed.\n\nIn 1859 Thomas Webster Rammell and Josiah Latimer Clark proposed an underground tube network in central London \"for the more speedy and convenient circulation of despatches and parcels\".\n\nThe company was founded in 1859 with offices at 6 Victoria Street, Westminster. Capital of £150,000 was sought through 15,000 shares at £10 each. The company's directors were its chairman Richard Temple-Grenville, 3rd Duke of Buckingham and Chandos, deputy chairman Mark Huish, Thomas Brassey, Edwin Clark, the Hon. William Napier, John Horatio Lloyd, William Henry Smith and Sir Charles Henry John Rich.\n\nWith initial funding of £25,000 (£ in ), the company tested the technology and constructed a pilot route at the Soho Foundry of Boulton and Watt in Birmingham. The first full-scale trial was at Battersea during the summer of 1861. A single tube was installed, 452 yards long, with curves of up to radius and gradients of up to 1 in 22. narrow gauge track was cast inside the tube. Wheeled capsules were fitted with vulcanised rubber flaps to make an air seal. Power was provided by a 30 horse-power steam engine with a diameter fan. Single capsules weighed up to 3 tonnes, and achieved speeds up to 40 mph (60 km/h).\n\nA permanent line of narrow gauge was constructed between Euston railway station and the North West District Post Office in Eversholt Street, a distance of approximately a third of a mile. The line was tested from 15 January 1863, and operation started on 20 February 1863. A capsule conveying up to 35 bags of mail could make the short journey between terminals in one minute. Thirteen journeys were operated each day, with a daily operating cost of £1 4s 5d. The Post Office was charged a nominal fee for use of the service, presumably to encourage them to accept the technology.\n\nA journalist made a report on the first scheme at Euston:\n\nThe company sought to develop further lines within London, and attempted to raise an additional £125,000 (£ in ), of capital. The prospectus proposed a network of lines between \"points so important that it is unnecessary to dwell upon the magnitude of the traffic that must naturally arise between them\". The first line was to have been a route linking the Camden Town and Euston (Square) stations of the London and North Western Railway.\nWork started on a narrow gauge line from Euston to Holborn in September 1863. The tubes were constructed by the Staveley Coal and Iron Company. The first 'trains' ran on 10 October 1865 after a demonstration in which the chairman, Richard Temple-Grenville, 3rd Duke of Buckingham and Chandos, travelled from Holborn to Euston in one of the capsules.\n\nAnother line from Holborn to Gresham Street via the General Post Office on St Martin's le Grand was under construction in 1865. By the time of the 1866 financial crisis caused by the Overend, Gurney and Company collapse, a ⅜ mile tube from Holborn to Hatton Garden had been constructed. Total expenditure so far was £150,000 (£ in ).\n\nConstruction restarted in 1868, and it was completed to St. Martin's le Grand (for the General Post Office) in 1869. Capsules from the General Post Office reached Newgate Street within 17 minutes, at speeds of up to 60 mph.\n\nThe Post Office made several trials of the system, but there were not substantial time savings to be made, and by 1874, the Post Office abandoned its use, and the company went into liquidation in 1875. The Edinburgh Evening News reported in 1876 that the trucks containing the parcels continually stuck in the tunnels, and this was the reason for the failure of the company.\n\nIn late 1921, an agreement was reached between the Pneumatic Despatch Company and the Postmaster General for the sale (for £7,500) of the remaining infrastructure of \"the tube\" to the Postmaster General. The agreement recognised that \"the Company has not for many years past worked the tube and the same is not now in working order\" and that various persons had made unauthorised breaches in the tube as originally constructed. The agreement was confirmed by the Post Office (Pneumatic Tubes) Act 1922, which also repealed enactments from 1859, 1864 and 1872 authorising the company.\n\nTwo of the original vehicles survive, having been recovered in 1930, one in the Museum of London and the other in the National Railway Museum at York.\n\n\n"}
{"id": "21593205", "url": "https://en.wikipedia.org/wiki?curid=21593205", "title": "M7 grenade launcher", "text": "M7 grenade launcher\n\nThe M7 grenade launcher, formally rifle grenade launcher, M7, was a 22 mm rifle grenade launcher attachment for the M1 Garand rifle that saw widespread use throughout World War II and the Korean War. The M7 was a tube-shaped device, with one end slotting over the barrel of the rifle, the other end holding the grenade in place. Blank cartridges were loaded into the rifle prior to firing. When fired, the expanding gases generated by the cartridges propelled the grenade forward with considerable force. The M7 could fire grenades up to , compared with the maximum of achieved by hand grenades.\n\nFragmentation, anti-armor and smoke grenades were available for the M7.\n\nWhen the United States entered World War II in 1941, all infantry were issued with the Mk 2 fragmentation hand grenade. Owing to its hand-thrown nature, it had a range of only and could not be used against armored targets. To keep its weight down, it had to have a small charge, with a fatality radius of just . For longer ranges, rifle grenade attachments were available for the M1903 Springfield (M1 grenade launcher) and M1917 Enfield (M2 grenade launcher). These rifles were limited standard, however, and had been all but replaced by the new service rifle, the M1 Garand, by 1943. To rectify this, U.S. Army Ordnance designed a new launcher attachment for the M1 Garand, designated the M7, which could fire much heavier grenades up to 250 yards. M7 compatible fragmentation grenades had a fatality radius of . It entered production and service in 1943.\n\nThe M7 grenade launcher was a tube-shaped device with an overall length (including the mounting bracket) of 7.5 inches. One end fitted onto the barrel of the M1 Garand rifle enclosing the muzzle, and was held in place with the rifle's bayonet lug. The other end was cylindrical with a small clip that held the grenade in place by friction. To launch a grenade, a special high-powered cartridge made specifically for the purpose (the .30-06 Springfield M3 grenade cartridge) was chambered in the rifle. Using markings engraved onto the device to determine the desired range, the rifle grenade was slipped over the launcher. When fired, the resulting expanding gases propelled the grenade a considerable distance depending on the grenade type, the vertical angle that the rifle was held, and how far the grenade was positioned onto the launcher. Since the device disabled the semi-automatic function of the rifle to prevent damage to the gun from firing grenades, the rifle could not be fired normally when the M7 launcher was in place; the gun could be fired in an emergency by cycling the action by hand. Fragmentation, anti-tank, and smoke grenades and pyrotechnic signals were available along with an adapter to enable the use of hand grenades. One to three M7 grenade launchers were issued to each rifle squad depending on period. It was also issued to support and headquarters elements.\n\nThe M7 series grenade launcher came with accessories. The M15 auxiliary sight was mounted on the left-hand side of the stock accompanied by a template to allow it to be properly positioned on the stock. A spirit level allowed the user to figure out the arc of the weapon to aid in aiming the grenade. A rubber recoil boot (part No. B200968) could be slipped on the butt of the Garand's stock to reduce recoil when it was fired.\n\nThe M7 booster cartridge or \"Vitamin Pill\" could be inserted into the M7 grenade launcher's muzzle to increase its range by an additional 100 to 150 yards. These booster charges resembled a short .45 Colt blank cartridge, with a small hole in the base. When the blank cartridge was fired, its flame would travel up the rifle's barrel, ignite the booster charge, and propel the booster cartridge into the base of the rifle grenade. The booster cartridge would fall out of the rifle grenades tail fin assembly in flight.\n\nThe M8 grenade launcher was similar except it was designed to be mounted on the M1 carbine and used the .30 Carbine M6 grenade cartridge. Users of the M8 needed to be careful, as the recoil could crack or break the carbine's stock.\n\n\n"}
{"id": "11179042", "url": "https://en.wikipedia.org/wiki?curid=11179042", "title": "Norator", "text": "Norator\n\nIn electronics, a norator is a theoretical linear, time-invariant one-port which can have an arbitrary current and voltage between its terminals. A norator represents a controlled voltage or current source with infinite gain.\n\nInserting a norator in a circuit schematic provides whatever current and voltage the outside circuit demands, in particular, the demands of Kirchhoff's circuit laws. For example, the output of an ideal opamp behaves as a norator, producing nonzero output voltage and current that meet circuit requirements despite a zero input.\n\nA norator is often paired with a nullator to form a nullor.\n\nTwo trivial cases are worth noting: A nullator in parallel with a norator is equivalent to a short (zero voltage any current) and a nullator in series with a norator is an open circuit (zero current, any voltage).\n\n"}
{"id": "5734754", "url": "https://en.wikipedia.org/wiki?curid=5734754", "title": "OMA Device Management", "text": "OMA Device Management\n\nOMA Device Management is a device management protocol specified by the Open Mobile Alliance (OMA) Device Management (DM) Working Group and the Data Synchronization (DS) Working Group. The current approved specification of OMA DM is version 1.2.1, the latest modifications to this version released in June 2008. The candidate release 2.0 was scheduled to be finalized in September 2013.\n\nOMA DM specification is designed for management of mobile devices such as mobile phones, PDAs, and tablet computers. Device management is intended to support the following uses:\n\nAll of the above functions are supported by the OMA DM specification, and a device may optionally implement all or a subset of these features. Since OMA DM specification is aimed at mobile devices, it is designed with sensitivity to the following:\n\nOMA DM was originally developed by The SyncML Initiative Ltd, an industry consortium formed by many mobile device manufacturers. The SyncML Initiative got consolidated into the OMA umbrella as the scope and use of the specification was expanded to include many more devices and support global operation.\n\nTechnically, the OMA DM protocol uses XML for data exchange, more specifically the sub-set defined by SyncML. The device management takes place by communication between a server (which is managing the device) and the client (the device being managed). OMA DM is designed to support and utilize any number of data transports such as:\n\nThe communication protocol is a request-response protocol. Authentication and challenge of authentication are built-in to ensure the server and client are communicating only after proper validation. The server and client are both stateful, meaning a specific sequence of messages are to be exchanged only after authentication is completed to perform any task.\n\nThe communication is initiated by the OMA DM server, asynchronously, using any of the methods available such as a WAP Push or SMS. The initial message from server to client is said to be in the form of a notification, or alert message.\n\nOnce the communication is established between the server and client, a sequence of messages might be exchanged to complete a given device management task. OMA DM does provide for alerts, which are messages that can occur out of sequence, and can be initiated by either server or client. Such alerts are used to handle errors, abnormal terminations etc.\n\nSeveral parameters relating to the communication such as the maximum message size can be negotiated between the server and client during the initiation of a session. In order to transfer large objects, the protocol does allow for sending them in smaller chunks.\n\nError recovery based on timeouts are not specified completely, hence, different implementations could possibly differ (protocol is not fully specified relating to these, and seem to leave them open intentionally).\n\nThe protocol specifies exchange of Packages during a session, each package consisting of several messages, and each message in turn consisting of one or more commands. The server initiates the commands and the client is expected to execute the commands and return the result via a reply message.\n\n"}
{"id": "51796334", "url": "https://en.wikipedia.org/wiki?curid=51796334", "title": "Ocean data acquisition system", "text": "Ocean data acquisition system\n\nAn ocean data acquisition system (ODAS) is a set of instruments deployed at sea to collect as much meteorological and oceanographic data as possible. With their sensors, these systems deliver data both on the state of the ocean itself and the surrounding lower atmosphere. The use of microelectronics and technologies with efficient energy consumption allows to increase the types and numbers of sensor deployed on a single device.\n\nAccording to Intergovernmental Oceanographic Commission and World Meteorological Organization (WMO), \"ODAS means a structure, platform, installation, buoy, or other device, not being a ship, together with its appurtenant equipment, deployed at sea essentially for the purpose of collecting, storing or transmitting samples or data relating to the marine environment or the atmosphere or the uses thereof.\"\n\nEach hour, the data gathered by the system is transferred to the WMO's Global Telecommunications System by a geostationary satellite after having gone through a number of quality checks. Real-time data with information on the maritime environment can then be used for forecasts of physical states like weather, ocean currents or wave conditions which, in turn, may serve to warn seafarers of unfavourable conditions in the area.\n\nODAS can be mounted on the following structures:\n\nODAS buoys are not navigational aids but have been included into the IALA Maritime Buoyage System. The structures have a fixed geographical position.\n\nData gathered by an ODAS may include the following parameters:\n\n"}
{"id": "55049050", "url": "https://en.wikipedia.org/wiki?curid=55049050", "title": "Outcome Health", "text": "Outcome Health\n\nOutcome Health is a healthcare technology company founded by Rishi Shah and valued at $5.6 billion in May 2017. It is registered in Delaware as ContextMedia Health LLC.\n\nIn May 2017, a funding round with Goldman Sachs, CapitalG, Pritzker Group, and others invested $600 million in Outcome Health, giving it a $5.6 billion valuation. This is the largest single funding round in Chicago since Groupon in 2011, when it raised $950 million in its fifth funding round.\n\nAccording to a report in The Wall Street Journal unnamed former employees and advertisers accused the company of overcharging their customers for advertisements and misquoting third-party analyses and falsifying documents on the ads' performance. According to the accusations, Outcome Health reported that the ads appeared on more video screens than they had installed. Lanny Davis, a company spokesperson, responded by saying a law firm had been hired to \"review allegations about certain employees’ conduct that have been raised internally.\"\n\nAs of January 2018, Outcome Health decided to settle outstanding investor lawsuits in exchange for having Shah and Agarwal step down.\n\nIn June 2018, Matt McNally, former chief media officer at Publicis Health, was announced as the company's new CEO. \n\n\n\n\n\n\nhttps://www.wsj.com/articles/outcome-healths-investors-receive-subpoenas-from-justice-department-1510276020\n"}
{"id": "33161902", "url": "https://en.wikipedia.org/wiki?curid=33161902", "title": "Payment order", "text": "Payment order\n\nPayment order, in international banking, is a directive to a bank from a bank account holder instructing the bank to make a payment or series of payments to a third party.\n\n\"Payment orders\" are post-contract instruments often used to pay fee agreements to agents and usually contain conditions for the payment to be met such as successful completion of contract requirements.\n\n\"Payment orders\" with \"conditions\" should not be confused with \"conditional payment orders\". Conditional payment orders are pre-contract instruments consisting of a documented fee agreement between the beneficiary and the payer, proof of ability for the payer to pay which is often issued by Swift MT799 to the recipient's bank, and occasionally may include bank instructions for the establishment of a payment order following contract execution. Either payment orders or conditional payment orders are assumed to be irrevocable unless otherwise stated.\n\nPayment orders with conditions may be established after signing of a contract and posting of a letter of credit or other financial instrument with the paying bank but are never put in place prior to contract execution because of the risk that the contract will not materialize.\n\nMahesh\n"}
{"id": "19483098", "url": "https://en.wikipedia.org/wiki?curid=19483098", "title": "Petroleum and Explosives Safety Organisation", "text": "Petroleum and Explosives Safety Organisation\n\nPetroleum And Explosives Safety Organisation (PESO) is the department formed by Government of India to control and administer the usage of explosives, petrol stations in India. The agency issues licenses for Operation of Petrol Stations under Form XIV, Licenses to operate Petroleum Product Transportation vehicles, Licenses for Refineries, Petrochemical Complexes, etc. The Department is headed by Chief Controller of Explosives and is headquartered at Nagpur in the State of Maharashtra in India. The Northern Regional Office is based at Faridabad in the state of\nHaryana. The agency has framed various rules like \"Petroleum Rules 2002\" for the safe operation of petrol stations, \"Explosive Rules 1983\", \"Gas Cylinder Rules 2002( applicable to all states)\", and \"Stative & Mobile Pressure Vessels (Unfired) 1981\" for LPG Vessels. It functions under Department Of Industrial Policy and Promotion (DIPP).\n\nThe Petroleum and Explosives Safety Organisation (PESO) with its headquarter at Nagpur and a network of 5 Circle offices, 18 Sub-circle offices and Central Testing Station has been continuing to serve the public and industry for over 100 years in all matters relating to safety in manufacture, storage, transport and handling of explosives, petroleum, compressed gases and other hazardous substances through comprehensive administration of Explosives Act, 1884, Petroleum Act, 1934, Inflammable Substances Act, 1952 and rules framed there under viz. Explosives Rules, 2008, Gas Cylinders Rules, 2004, Static & Mobile Pressure Vessels (Unfired) Rules, 1981, Petroleum Rules, 2002, Calcium Carbide Rules, 1987 & Cinematographic Films Rules, 1948.\n\n"}
{"id": "46402706", "url": "https://en.wikipedia.org/wiki?curid=46402706", "title": "Pickled fruit", "text": "Pickled fruit\n\nPickled fruit refers to fruit that has been pickled. Pickling is the process of food preservation by either anaerobic fermentation in brine or immersion in vinegar. Many types of fruit are pickled. Some examples include peaches, apples, crab apple, pears, plums, grapes, currant, tomato and olives. Vinegar may also be prepared from fruit, such as apple cider vinegar.\n\nPickled peaches may be prepared from medium-sized, non-melting clingstone peaches that are small-seeded. In the United States prior to around 1960, some were prepared from small, unripe freestone peaches. They may be prepared with sugar, cinnamon, cloves and allspice to add flavor. Pickled peaches may be used to accompany meats and in salads, and also have other uses.\n\nPickled pears may be prepared with sugar, cinnamon, cloves and allspice to add flavor, and may be referred to as spiced pears. They may be prepared from underripe pears. Pickled pears may be used to accompany dishes such as roasts and salads, among others.\n\n\nIn Malaysia, some fruits are pickled when they are unripe, such as \"belimbing\", \"kedondong\", \"chermai\", lime, pineapple, papaya, mango and nutmeg.\n"}
{"id": "1659679", "url": "https://en.wikipedia.org/wiki?curid=1659679", "title": "Protein microarray", "text": "Protein microarray\n\nA protein microarray (or protein chip) is a high-throughput method used to track the interactions and activities of proteins, and to determine their function, and determining function on a large scale. Its main advantage lies in the fact that large numbers of proteins can be tracked in parallel. The chip consists of a support surface such as a glass slide, nitrocellulose membrane, bead, or microtitre plate, to which an array of capture proteins is bound. Probe molecules, typically labeled with a fluorescent dye, are added to the array. Any reaction between the probe and the immobilised protein emits a fluorescent signal that is read by a laser scanner. Protein microarrays are rapid, automated, economical, and highly sensitive, consuming small quantities of samples and reagents. The concept and methodology of protein microarrays was first introduced and illustrated in antibody microarrays (also referred to as antibody matrix) in 1983 in a scientific publication and a series of patents. The high-throughput technology behind the protein microarray was relatively easy to develop since it is based on the technology developed for DNA microarrays, which have become the most widely used microarrays.\n\nProtein microarrays were developed due to the limitations of using DNA microarrays for determining gene expression levels in proteomics. The quantity of mRNA in the cell often doesn't reflect the expression levels of the proteins they correspond to. Since it is usually the protein, rather than the mRNA, that has the functional role in cell response, a novel approach was needed. Additionally post-translational modifications, which are often critical for determining protein function, are not visible on DNA microarrays. Protein microarrays replace traditional proteomics techniques such as 2D gel electrophoresis or chromatography, which were time consuming, labor-intensive and ill-suited for the analysis of low abundant proteins.\n\nThe proteins are arrayed onto a solid surface such as microscope slides, membranes, beads or microtitre plates. The function of this surface is to provide a support onto which proteins can be immobilized. It should demonstrate maximal binding properties, whilst maintaining the protein in its native conformation so that its binding ability is retained. Microscope slides made of glass or silicon are a popular choice since they are compatible with the easily obtained robotic arrayers and laser scanners that have been developed for DNA microarray technology. Nitrocellulose film slides are broadly accepted as the highest protein binding substrate for protein microarray applications.\n\nThe chosen solid surface is then covered with a coating that must serve the simultaneous functions of immobilising the protein, preventing its denaturation, orienting it in the appropriate direction so that its binding sites are accessible, and providing a hydrophilic environment in which the binding reaction can occur. In addition, it also needs to display minimal non-specific binding in order to minimize background noise in the detection systems. Furthermore, it needs to be compatible with different detection systems. Immobilising agents include layers of aluminium or gold, hydrophilic polymers, and polyacrylamide gels, or treatment with amines, aldehyde or epoxy. Thin-film technologies like physical vapour deposition (PVD) and chemical vapour deposition (CVD) are employed to apply the coating to the support surface.\n\nAn aqueous environment is essential at all stages of array manufacture and operation to prevent protein denaturation. Therefore, sample buffers contain a high percent of glycerol (to lower the freezing point), and the humidity of the manufacturing environment is carefully regulated. Microwells have the dual advantage of providing an aqueous environment while preventing cross-contamination between samples.\n\nIn the most common type of protein array, robots place large numbers of proteins or their ligands onto a coated solid support in a pre-defined pattern. This is known as robotic contact printing or robotic spotting. Another fabrication method is ink-jetting, a drop-on-demand, non-contact method of dispersing the protein polymers onto the solid surface in the desired pattern. Piezoelectric spotting is a similar method to ink-jet printing. The printhead moves across the array, and at each spot uses electric stimulation to deliver the protein molecules onto the surface via tiny jets. This is also a non-contact process. Photolithography is a fourth method of arraying the proteins onto the surface. Light is used in association with photomasks, opaque plates with holes or transparencies that allow light to shine through in a defined pattern. A series of chemical treatments then enables deposition of the protein in the desired pattern upon the material underneath the photomask.\n\nThe capture molecules arrayed on the solid surface may be antibodies, antigens, aptamers (nucleic acid-based ligands), affibodies (small molecules engineered to mimic monoclonal antibodies), or full length proteins. Sources of such proteins include cell-based expression systems for recombinant proteins, purification from natural sources, production in vitro by cell-free translation systems, and synthetic methods for peptides. Many of these methods can be automated for high throughput production but care must be taken to avoid conditions of synthesis or extraction that result in a denatured protein which, since it no longer recognizes its binding partner, renders the array useless.\n\nProteins are highly sensitive to changes in their microenvironment. This presents a challenge in maintaining protein arrays in a stable condition over extended periods of time. In situ methods — invented and published by Mingyue He and Michael Taussig in 2001 — involve on-chip synthesis of proteins as and when required, directly from the DNA using cell-free protein expression systems. Since DNA is a highly stable molecule it does not deteriorate over time and is therefore suited to long-term storage. This approach is also advantageous in that it circumvents the laborious and often costly processes of separate protein purification and DNA cloning, since proteins are made and immobilised simultaneously in a single step on the chip surface. Examples of in situ techniques are PISA (protein in situ array), NAPPA (nucleic acid programmable protein array) and DAPA (DNA array to protein array).\n\nThere are three types of protein microarrays that are currently used to study the biochemical activities of proteins.\n\nAnalytical microarrays are also known as capture arrays. In this technique, a library of antibodies, aptamers or affibodies is arrayed on the support surface. These are used as capture molecules since each binds specifically to a particular protein. The array is probed with a complex protein solution such as a cell lysate. Analysis of the resulting binding reactions using various detection systems can provide information about expression levels of particular proteins in the sample as well as measurements of binding affinities and specificities. This type of microarray is especially useful in comparing protein expression in different solutions. For instance the response of the cells to a particular factor can be identified by comparing the lysates of cells treated with specific substances or grown under certain conditions with the lysates of control cells. Another application is in the identification and profiling of diseased tissues.\n\nReverse phase protein microarray (RPPA) involve complex samples, such as tissue lysates. Cells are isolated from various tissues of interest and are lysed. The lysate is arrayed onto the microarray and probed with antibodies against the target protein of interest. These antibodies are typically detected with chemiluminescent, fluorescent or colorimetric assays. Reference peptides are printed on the slides to allow for protein quantification of the sample lysates. RPAs allow for the determination of the presence of altered proteins or other agents that may be the result of disease. Specifically, post-translational modifications, which are typically altered as a result of disease can be detected using RPAs.\n\nFunctional protein microarrays (also known as target protein arrays) are constructed by immobilising large numbers of purified proteins and are used to identify protein–protein, protein–DNA, protein–RNA, protein–phospholipid, and protein–small-molecule interactions, to assay enzymatic activity and to detect antibodies and demonstrate their specificity. They differ from analytical arrays in that functional protein arrays are composed of arrays containing full-length functional proteins or protein domains. These protein chips are used to study the biochemical activities of the entire proteome in a single experiment.\n\nThe key element in any functional protein microarray-based assay is the arrayed proteins must retain their native structure, such that meaningful functional interactions can take place on the array surface.The advantages of controlling the precise mode of surface attachment through use of an appropriate affinity tag are that the immobilised proteins will have a homogenous orientation resulting in a higher specific activity and higher signal-to-noise ratio in assays, with less interference from non-specific interactions.\n\nProtein array detection methods must give a high signal and a low background. The most common and widely used method for detection is fluorescence labeling which is highly sensitive, safe and compatible with readily available microarray laser scanners. Other labels can be used, such as affinity, photochemical or radioisotope tags. These labels are attached to the probe itself and can interfere with the probe-target protein reaction. Therefore, a number of label free detection methods are available, such as surface plasmon resonance (SPR), carbon nanotubes, carbon nanowire sensors (where detection occurs via changes in conductance) and microelectromechanical system (MEMS) cantilevers. All these label free detection methods are relatively new and are not yet suitable for high-throughput protein interaction detection; however, they do offer much promise for the future.\n\nProtein quantitation on nitrocellulose coated glass slides can use near-IR fluorescent detection. This limits interferences due to auto-fluorescence of the nitrocellulose at the UV wavelengths used for standard fluorescent detection probes.\n\nThere are five major areas where protein arrays are being applied: diagnostics, proteomics, protein functional analysis, antibody characterization, and treatment development.\n\nDiagnostics involves the detection of antigens and antibodies in blood samples; the profiling of sera to discover new disease biomarkers; the monitoring of disease states and responses to therapy in personalized medicine; the monitoring of environment and food. Digital bioassay is an example of using protein microarray for diagnostic purposes. In this technology, an array of microwells on a glass/polymer chip are seeded with magnetic beads (coated with fluorescent tagged antibodies), subjected to targeted antigens and then characterised by a microscope through counting fluorescing wells. A cost-effective fabrication platform (using OSTE polymers) for such microwell arrays has been recently demonstrated and the bio-assay model system has been successfully characterised.\n\nProteomics pertains to protein expression profiling i.e. which proteins are expressed in the lysate of a particular cell.\n\nProtein functional analysis is the identification of protein–protein interactions (e.g. identification of members of a protein complex), protein–phospholipid interactions, small molecule targets, enzymatic substrates (particularly the substrates of kinases) and receptor ligands.\n\nAntibody characterization is characterizing cross-reactivity, specificity and mapping epitopes.\n\nTreatment development involves the development of antigen-specific therapies for autoimmunity, cancer and allergies; the identification of small molecule targets that could potentially be used as new drugs.\n\nDespite the considerable investments made by several companies, proteins chips have yet to flood the market. Manufacturers have found that proteins are actually quite difficult to handle. Production of reliable, consistent, high-throughput proteins that are correctly folded and functional is fraught with difficulties as they often result in low-yield of proteins due to decreased solubility and formation of inclusion bodies. A protein chip requires a lot more steps in its creation than does a DNA chip.\n\nThere are a number of approaches to this problem which differ fundamentally according to whether the proteins are immobilised through non-specific, poorly defined interactions, or through a specific set of known interactions. The former approach is attractive in its simplicity and is compatible with purified proteins derived from native or recombinant sources but suffers from a number of risks. Most notable amongst these relate to the uncontrolled nature of the interactions between each protein and the surface; at best, this might give rise to a heterogeneous population of proteins in which active sites are sometimes occluded by the surface; at worst, it might destroy activity altogether due to partial or complete surface-mediated unfolding of the immobilised protein.\n\nChallenges include: 1) finding a surface and a method of attachment that allows the proteins to maintain their secondary or tertiary structure and thus their biological activity and their interactions with other molecules, 2) producing an array with a long shelf life so that the proteins on the chip do not denature over a short time, 3) identifying and isolating antibodies or other capture molecules against every protein in the human genome, 4) quantifying the levels of bound protein while assuring sensitivity and avoiding background noise, 5) extracting the detected protein from the chip in order to further analyze it, 6) reducing non-specific binding by the capture agents, 7) the capacity of the chip must be sufficient to allow as complete a representation of the proteome to be visualized as possible; abundant proteins overwhelm the detection of less abundant proteins such as signaling molecules and receptors, which are generally of more therapeutic interest.\n\n"}
{"id": "38370599", "url": "https://en.wikipedia.org/wiki?curid=38370599", "title": "Quantum vacuum thruster", "text": "Quantum vacuum thruster\n\nA quantum vacuum thruster (QVT or Q-thruster) is a theoretical system that uses the same principles and equations of motion that a conventional plasma thruster would use, namely magnetohydrodynamics (MHD), to make predictions about the behavior of the propellant. However, rather than using a conventional plasma as a propellant, a QVT uses the quantum vacuum fluctuations of the zero-point field. If QVT systems were to truly work they would eliminate the need to carry any propellant, as the system uses the quantum vacuum to assist with thrust. It would also allow for much higher specific impulses for QVT systems compared to other spacecraft as they would be limited only by their power supply’s energy storage densities. Harold White's Advanced Propulsion Physics Laboratory (NASA Eagleworks) suggests that their RF cavity may be an example of a quantum vacuum thruster (QVT or Q-thruster).\n\nThe name and concept is controversial. In 2008, Yu Zhu and others at China's Northwestern Polytechnical University claimed to measure thrust from such a thruster, but called it a \"microwave thruster without propellant\" working on quantum principles. In 2011 it was mentioned as something to be studied by Harold G. White and his team at NASA's Eagleworks Laboratories, who were working with a prototype of such a thruster. Other physicists, such as Sean M. Carroll and John Baez, dismiss it because the quantum vacuum as currently understood is not a plasma and does not possess plasma-like characteristics.\n\nA vacuum can be viewed not as empty space but as the combination of all zero-point fields. According to quantum field theory the universe is made up of matter fields whose quanta are fermions (e.g. electrons and quarks) and force fields, whose quanta are bosons (i.e. photons and gluons). All these fields have some intrinsic zero-point energy. Describing the quantum vacuum, a \"Physics Today\" article cited by the NASA team describes this ensemble of fields as \"a turbulent sea, roiling with waves associated with a panoply of force-mediating fields such as the photon and Higgs fields\". Given the equivalence of mass and energy expressed by Einstein's E = mc, any point in space that contains energy can be thought of as having mass to create particles. Virtual particles spontaneously flash into existence and annihilate each other at every point in space due to the energy of quantum fluctuations. Many real physical effects attributed to these vacuum fluctuations have been experimentally verified, such as spontaneous emission, Casimir force, Lamb shift, magnetic moment of the electron and Delbrück scattering; these effects are usually called \"radiative corrections\".\nThe Casimir effect is a weak force between two uncharged conductive plates caused by the zero-point energy of the vacuum. It was first observed experimentally by Lamoreaux (1997) and results showing the force have been repeatedly replicated. Several scientists including White have highlighted that a net thrust can indeed be induced on a spacecraft via the related \"dynamical Casimir effect\". The dynamic Casimir effect was observed experimentally for the first time in 2011 by Wilson et al. In the dynamical Casimir effect electromagnetic radiation is emitted when a mirror is accelerated through space at relativistic speeds. When the speed of the mirror begins to match the speed of the photons, some photons become separated from their virtual pair and so do not get annihilated. Virtual photons become real and the mirror begins to produce light. This is an example of Unruh radiation. A publication by Feigel (2004) raised the possibility of a Casimir-like effect that transfers momentum from zero-point quantum fluctuations to matter, controlled by applied electric and magnetic fields. These results were debated in a number of follow up papers in particular van Tiggelen et al. (2006) found no momentum transfer for homogeneous fields, but predict a very small transfer for a Casimir-like field geometry. This cumulated with Birkeland & Brevik (2007) who showed that electromagnetic vacuum fields can cause broken symmetries (anisotropy) in the transfer of momentum or, put another way, that the extraction of momentum from electromagnetic zero-point fluctuations is possible in an analogous way that the extraction of energy is possible from the Casimir effect. Birkeland & Brevik highlight that momentum asymmetries exist throughout nature and that the artificial stimulation of these by electric and magnetic fields have already been experimentally observed in complex liquids. This relates to the Abraham–Minkowski controversy, a long theoretical and experimental debate that continues to the current time. It is widely recognized that this controversy is an argument about definition of the interaction between matter and fields. \nIt has been argued that momentum transfer between matter and electromagnetic fields relating to the Abraham-Minikowski issue would allow for propellant-less drives.\n\nA QVT system seeks to make use of this predicted Casimir-like momentum transfer. It is argued that when the vacuum is exposed to crossed electric and magnetic fields (i.e. E and B-fields), it will induce a drift of the entire vacuum plasma which is orthogonal to that of the applied E x B fields. In a 2015 paper White highlighted that the presence of ordinary matter is predicted to cause an energy perturbation in the surrounding quantum vacuum such that the local vacuum state has a different energy density when compared with the \"empty\" cosmological vacuum energy state. This suggests the possibility of modelling the vacuum as a dynamic entity as opposed to it being an immutable and non-degradable state. White models of the perturbed quantum vacuum around a hydrogen atom as a Dirac vacuum consisting of virtual electron-positron pairs. Given the nontrivial variability in local energy densities resulting from virtual pair production, he suggests the tools of magnetohydrodynamics (MHD) can be used to model the quasiclassical behavior of the quantum vacuum as a plasma.\n\nWhite compares changes in vacuum energy density induced by matter to the hypothetical chameleon field or quintessence currently being discussed in the scientific literature. It is claimed the existence of a “chameleon” field whose mass is dependent on the local matter density may be an explanation for dark energy. A number of notable physicists, such as Sean Carroll, see the idea of a dynamical vacuum energy as the simplest and best explanation for dark energy. Evidence for quintessence would come from violations of Einstein's equivalence principle and variation of the fundamental constants ideas which are due to be tested by the Euclid telescope which is set to launch in 2020.\n\nSystems utilizing Casimir effects have thus far been shown to only create very small forces and are generally considered one-shot devices that would require a subsequent energy to recharge them (i.e. Forward's \"vacuum fluctuation battery\"). The ability of systems to use the zero-point field continuously as a source of energy or propellant is much more contentious (though peer-reviewed models have been proposed). There is debate over which formalisms of quantum mechanics apply to propulsion physics under such circumstances, the more refined Quantum Electrodynamics (QED), or the relatively undeveloped and controversial Stochastical Quantum Electrodynamics (SED). SED describes electromagnetic energy at absolute zero as a stochastic, fluctuating zero-point field. In SED the motion of a particle immersed in the stochastic zero-point radiation field generally results in highly nonlinear behaviour. Quantum effects emerge as a result of permanent matter-field interactions not possible to describe in QED The typical mathematical models used in classical electromagnetism, quantum electrodynamics (QED) and the standard model view electromagnetism as a U(1) gauge theory, which topologically restricts any complex nonlinear interaction. The electromagnetic vacuum in these theories is generally viewed as a linear system with no overall observable consequence. For many practical calculations zero-point energy is dismissed by fiat in the mathematical model as a constant that may be canceled or as a term that has no physical effect.\n\nThe 2016 NASA paper highlights that stochastic electrodynamics (SED) allows for a pilot-wave interpretation of quantum mechanics. Pilot-wave interpretations of quantum mechanics are a family of deterministic nonlocal theories distinct from other more mainstream interpretations such as the Copenhagen interpretation and Everett's many-worlds interpretation. Pioneering experiments by Couder and Fort beginning in 2006 have shown that macroscopic classical pilot-waves can exhibit characteristics previously thought to be restricted to the quantum realm. Hydrodynamic pilot-wave analogs have been able to duplicate the double slit experiment, tunneling, quantized orbits, and numerous other quantum phenomena and as such pilot-wave theories are experiencing a resurgence in interest. Coulder and Fort note in their 2006 paper that pilot-waves are nonlinear dissipative systems sustained by external forces. A dissipative system is characterized by the spontaneous appearance of symmetry breaking (anisotropy) and the formation of complex, sometimes chaotic or emergent, dynamics where interacting fields can exhibit long range correlations. In SED the zero point field (ZPF) plays the role of the pilot wave that guides real particles on their way. Modern approaches to SED consider wave and particle-like quantum effects as well-coordinated emergent systems that are the result of speculated sub-quantum interactions with the zero-point field\n\nSome notable physicists have found the Q-thruster concept to be implausible. For example, mathematical physicist John Baez has criticized the reference to \"quantum vacuum virtual plasma\" noting that: \"There's no such thing as 'virtual plasma' \". Noted Caltech theoretical physicist Sean M. Carroll has also affirmed this statement, writing \"[t]here is no such thing as a ‘quantum vacuum virtual plasma,’...\". In addition, Lafleur found that quantum field theory predicts no net force, implying that the measured thrusts are unlikely to be due to quantum effects. However, Lafleur noted that this conclusion was based on the assumption that the electric and magnetic fields were homogeneous, whereas certain theories posit a small net force in inhomogeneous vacuums.\n\nNotably, the violation of energy and momentum conservation laws have been heavily criticized. In a presentation at Nasa Ames Research Centre in November 2014, Harold White addressed the issue of conservation of momentum by stating that the Q-thruster conserves momentum by creating a wake or anisotropic state in the quantum vacuum. White indicated that once false positives were ruled out, Eagleworks would explore the momentum distribution and divergence angle of the quantum vacuum wake using a second Q-thruster to measure the quantum vacuum wake. In a paper published in January 2014, White proposed to address the conservation of momentum issue by stating that the Q-thruster pushes quantum particles (electrons/positrons) in one direction, whereas the Q-thruster recoils to conserve momentum in the other direction. White stated that this principle was similar to how a submarine uses its propeller to push water in one direction, while the submarine recoils to conserve momentum. Hence, the violations of fundamental laws of physics can be avoided.\n\nA number of physicists have suggested that a spacecraft or object may generate thrust through its interaction with the quantum vacuum. For example, Fabrizio Pinto in a 2006 paper published in the \"Journal of the British Interplanetary Society\" noted it may be possible to bring a cluster of polarisable vacuum particles to a hover in the laboratory and then to transfer thrust to a macroscopic accelerating vehicle. Similarly, Jordan Maclay in a 2004 paper titled \"A Gedanken Spacecraft that Operates Using the Quantum Vacuum (Dynamic Casimir Effect)\" published in the scientific journal \"Foundations of Physics\" noted that it is possible to accelerate a spacecraft based on the dynamic Casimir effect, in which electromagnetic radiation is emitted when an uncharged mirror is properly accelerated in vacuum. Similarly, Puthoff noted in a 2010 paper titled \"Engineering the Zero-Point Field and Polarizable Vacuum For Interstellar Flight\" published in the \"Journal of the British Interplanetary Society\" noted that it may be possible that the quantum vacuum might be manipulated so as to provide energy/thrust for future space vehicles. Likewise, researcher Yoshinari Minami in a 2008 paper titled \"Preliminary Theoretical Considerations for Getting Thrust via Squeezed Vacuum\" published in the \"Journal of the British Interplanetary Society\" noted the theoretical possibility of extracting thrust from the excited vacuum induced by controlling squeezed light. In addition, Alexander Feigel in a 2009 paper noted that propulsion in quantum vacuum may be achieved by rotating or aggregating magneto-electric nano-particles in strong perpendicular electrical and magnetic fields.\n\nHowever, according to Puthoff, although this method can produce angular momentum causing a static disk (known as a Feynman disk) to begin to rotate, it cannot induce linear momentum due to a phenomenon known as \"hidden momentum\" that cancels the ability of the proposed E×B propulsion method to generate linear momentum. However, some recent experimental and theoretical work by van Tiggelen and colleagues suggests that linear momentum may be transferred from the quantum vacuum in the presence of an external magnetic field.\n\nIn 2013, the Eagleworks team tested a device called the Serrano Field Effect Thruster, built by Gravitec Inc. at the request of Boeing and DARPA. The Eagleworks team has theorized that this device is a Q-thruster. The thruster consists of a set of circular dielectrics sandwiched between electrodes; its inventor describes it device as producing thrust through a preselected shaping of an electric field. Gravitec Inc. alleges that in 2011 they tested the \"asymmetrical capacitor\" device in a high vacuum several times and have ruled out ion wind or electrostatic forces as an explanation for the thrust produced. In February through June 2013, the Eagleworks team evaluated the SFE test article in and out of a Faraday Shield and at various vacuum conditions. Thrust was observed in the ~1–20 N/kW range. The magnitude of the thrust scaled approximately with the cube of the input voltage (20–110 μN). As of 2015, the researchers have not published a peer-reviewed paper detailing the results of this experiment.\n\nUsing a torsion pendulum, White's team claimed to have measured 30–50 \"μ\"N of thrust from a microwave cavity resonator designed by Guido Fetta in an attempt at propellant-less propulsion. Using the same measurement equipment, a non-zero force was also measured on a \"null\" resonator that was not designed to experience any such force, which they suggest hints at \"interaction with the quantum vacuum virtual plasma\". All measurements were performed at atmospheric pressure, presumably in contact with air, and with no analysis of systematic errors, except for the use of an RF load without the resonant cavity interior as a control device. In early 2015, Paul March from that team made new results public, claiming positive experimental force measurements with a torsional pendulum in a hard vacuum: about 50 µN with 50 W of input power at 5.0×10 torr, and new null-thrust tests. The claims of the team have not yet been published in a peer-reviewed journal, only as a conference paper in 2013.\n\nYu Zhu previously claimed to have measured anomalous thrust arising from a similar device, using power levels roughly 100 times greater, and measuring thrust roughly 1000 times greater.\n\nAs of 2015, Eagleworks is attempting to gather performance data to support the development of a Q-thruster engineering prototype for reaction-control-system applications in the force range of 0.1–1 N with a corresponding input electrical power range of 0.3–3 kW. The group plans to begin by testing a refurbished test article to improve the historical performance of a 2006 experiment that attempted to demonstrate the Woodward effect. The photograph shows the test article and the plot diagram shows the thrust trace from a 500g load cell in experiments performed in 2006.\n\nThe group hopes that testing the device on a high-fidelity torsion pendulum (1–4 μN at 10–40 W) will unambiguously demonstrate the feasibility of this concept. The team is maintaining a dialogue with the ISS national labs office for an on-orbit detailed test objective (DTO) to test the Q-thrusters operation in the vacuum and weightlessness of outer space.\n\n\n"}
{"id": "4169615", "url": "https://en.wikipedia.org/wiki?curid=4169615", "title": "Real-time data", "text": "Real-time data\n\nReal-time data (RTD) is information that is delivered immediately after collection. There is no delay in the timeliness of the information provided. Real-time data is often used for navigation or tracking. Such data is usually processed using real-time computing although it can also be stored for later or off-line data analysis.\n\nReal-time data is not the same as dynamic data. Real-time data can be dynamic (e.g. a variable indicating current location) or static (e.g. a fresh log entry indicating location at a specific time).\n\nReal-time economic data, and other official statistics, are often based on preliminary estimates, and therefore are frequently adjusted as better estimates become available. These later adjusted data are called \"revised data\". \nThe terms real-time economic data and real-time economic analysis were coined by Francis\nX. Diebold and Glenn D. Rudebusch. Macroeconomist Glenn D. Rudebusch defined real-time analysis as 'the use of sequential information sets that were actually available as history unfolded.' Macroeconomist Athanasios Orphanides has argued that economic policy rules may have very different effects when based on error-prone real-time data (as they inevitably are in reality) than they would if policy makers followed the same rules but had more accurate data available.\n\nIn order to better understand the accuracy of economic data and its effects on economic decisions, some economic organizations, such as the Federal Reserve Bank of St. Louis, Federal Reserve Bank of Philadelphia and the Euro-Area Business Cycle Network (EABCN), have made databases available that contain both real-time data and subsequent revised estimates of the same data.\n\nReal-time bidding is programmatic real-time auctions that sell digital-ad impressions. Entities on both the buying and selling sides require almost instantaneous access to data in order to make decisions, forcing real-time data to the forefront of their needs. To support these needs, new strategies and technologies, such Druid have arisen and are quickly evolving.\n\n\n"}
{"id": "17202238", "url": "https://en.wikipedia.org/wiki?curid=17202238", "title": "Rosetta Genomics", "text": "Rosetta Genomics\n\nRosetta Genomics Ltd. is a molecular diagnostics company with offices in Israel and the United States that uses micro-ribonucleic acid (microRNA) biomarkers to develop diagnostic tests designed to differentiate between various types of cancer. The company expects the first three tests based on its technology to be submitted for regulatory approval in 2008. The diagnostic tests will differentiate between squamous and non-squamous non-small cell lung cancer (NSCLC); differentiate between adenocarcinoma and peritoneal mesothelioma; and seek to identify the origin of tumors in patients representing cancer of unknown primary (CUP). Using a single microRNA, the highly sensitive, highly specific test for squamous and non-squamous lung cancer has passed the prevalidation phase and has been submitted for approval to the New York State Department of Health Clinical Laboratory Evaluation Program in April 2008.\n\nIn April 2008, \"Nature Biotechnology\" published a study by Rosetta Genomics’ scientists whose findings demonstrate microRNAs' significant potential to act as effective biomarkers that may be applied in a diagnostic test designed to identify the primary tumor site in patients CUP. In addition to its diagnostic programs, Rosetta Genomics is collaborating with Isis Pharmaceuticals to develop a microRNA-based therapy for Hepatocellular carcinoma (HCC), a form of liver cancer.\n\nRosetta Genomics has developed a microRNA discovery process.\n\nRosetta Genomics has developed several proprietary technologies that enable the Company to work with microRNAs. At the basis of these technologies are proprietary microRNA extraction protocols that include sensitive extraction of microRNAs from most body fluids, including serum, urine, saliva, with virtually no microRNA lost in the extraction process. The company has also developed a microRNA extraction protocol from Formalin Fixed Paraffin\nEmbedded, or FFPE, samples. This allows extraction of microRNAs from samples preserved at room temperature.\n\nOnce microRNAs are extracted, Rosetta Genomics’ technology is capable of detecting and quantifying the microRNAs using two custom designed platform technologies which utilize Quantitative Real Time PCR (or qRT-PCR) and microarrays.\n\nThe Company’s proprietary microarray platform covers approximately 850 human microRNAs, including approximately 180 microRNAs which are Rosetta Genomics’ proprietary microRNAs. The array’s high specificity allows discriminating homologous family members.\n\nIn January 2008, Rosetta Genomics announced a collaboration agreement with the Henry Ford Health System in Detroit, Michigan to develop microRNA-based diagnostics and prognostics for brain cancer. Also in January 2008, Rosetta Genomics announced that its subsidiary, Rosetta Genomics Inc. has received a license to use Roche Molecular Systems' PCR technology in microRNA-based diagnostic tests. In September 2007, Rosetta Genomics said it will work with New York University Medical\nCenter to develop a microRNA-based diagnostic test for melanoma.\n\nIn May 2007, Rosetta Genomics announced Columbia University Medical Center would utilize its Clinical Laboratory Improvement Amendments (CLIA)-certified laboratory to perform the clinical validation of Rosetta Genomics’ diagnostics program for cancer of unknown primary.\n\nIn February 2006, Isis Pharmaceuticals Inc. and Rosetta Genomics said they will collaborate to develop antisense drugs that inhibit microRNA in the liver to treat cancers there. Antisense drugs are a class of compounds that interfere with genetic material that gets translated into harmful proteins.\n\nRosetta Genomics was founded by Isaac Bentwich in 2000 to pursue commercial applications of microRNA research. The company had its IPO on March 6, 2007 and is traded on the NASDAQ. Rosetta Genomics expects the funds raised to advance its microRNA-based diagnostic and therapeutic cancer products through initial clinical validation, defined as success in identifying the specific biomarker panels via blinded tests of samples supplied\nby medical institutions.\n\nIn 2016, the company was ranked #27 on the Deloitte Fast 500 North America list.\n\nOn 31 May, 2018, it was announced that Rosetta Genomics filed for Chapter 7 bankruptcy.\n\n"}
{"id": "23047237", "url": "https://en.wikipedia.org/wiki?curid=23047237", "title": "SCR-203", "text": "SCR-203\n\nThe SCR-203 was a U.S. Army radio transceiver used during World War II and designed to be mounted on an animal pack saddle. \nThe SCR-203 was a low power, short range, portable command set designed to clamp onto a Phillips pack saddle for animal-pack transportation and operation. It was used primarily by the cavalry, and field artillery. It consisted of the BC-228 transmitter, the BC-227 receiver, and BC-235 control box and could transmit 7.5 watts AM voice, MCW, or CW for a range of approximately 30 miles. Its radio frequency coverage was approximately 2.0 to 3.0 Mhz. The unit was powered by various battery packs and a GN-35 hand cranked generator and used a 25 ft whip antenna.\n\nThe mule-packed radio concept dates back to before World War I, but due to the decline of the Horse Cavalry and the miniaturization of radio components, the SCR-203 was the last of the U.S. Army's radio sets designed to be animal-mounted. The SCR-203 was replaced by the SCR-245.\n\n\n\n"}
{"id": "58741468", "url": "https://en.wikipedia.org/wiki?curid=58741468", "title": "Saskia Van Uffelen", "text": "Saskia Van Uffelen\n\nSaskia Van Uffelen is a Belgian business woman, active in the ICT field, and involved in activities related to the skills impact of digital transformation.\n\nShe started her career in the IT sector in 1984, held several roles in IT companies (Xerox, Compaq, HP and Arinso), became CEO of Bull & CSB Consulting in 2008, and is CEO of Ericsson Belux since 2014.\n\nShe holds positions on the board of Belgian postal company bpost SA, High voltage company Elia System Operator SA, Elia Asset SA, and insurance company AXA.\n\nHer public interventions, (e.g. TEDx events) focus on embracing the digital and organisational change whilst putting the human factor in the centre.\n\nIn 2012, as part of the European DigitalChampions initiative, Belgium nominated her to be part of the Digital Champion expert Group. Her focus is on improving ICT skills.\n\nIn 2014 she started the coordination of the Belgian coalition on digital skills and jobs, part of the European coalition on digital skills and jobs and launched the official Federal Belgian initiative digitalchampion.be. \n\nIn 2017 she became part of the European Digital Skills and Jobs Coalition Governing Board, overseeing the national coalitions on digital skills.\n\nShe became member of the Digital minds for Belgium, an informal advisory body of the Belgian Governement related to digital initiatives. \n\nIn 2017 she became co-founder of Becentral, an initiative aimed at digital transformation of the Belgian society.\n\n\nShe published the management book \"Iedereen baas\" (in Dutch language) or \"Tous patron\" about the fact that 4 generations (generation X, generation Y, generation Z babyboomers) of employees have to work together on the workfloor.\n\nShe is also co-author of the book \"social technologies in business: connect share, lead\" on how social technologies are transforming organisations.\n"}
{"id": "43296931", "url": "https://en.wikipedia.org/wiki?curid=43296931", "title": "Sidney L. Pressey", "text": "Sidney L. Pressey\n\nSidney Leavitt Pressey (Brooklyn, New York, December 28, 1888 – July 1, 1979) was Professor of Psychology at Ohio State University for many years. He is famous for having invented a teaching machine many years before the idea became popular. \n\nPressey joined Ohio State in 1921, and stayed there until he retired in 1959. He continued publishing after retirement, with 18 papers between 1959 and 1967. He was a cognitive psychologist who \"rejected a view of learning as an accumulation of responses governed by environmental stimuli in favor of one governed by meaning, intention, and purpose\". In fact, he had been a cognitive psychologist his entire life, well before the \"mythical birthday of the cognitive revolution in psychology\".\n\nPressey's idea started as a machine for administering multiple-choice questions (MCQs) to students. MCQs were (and are still) a basic method for testing students in the United States. Pressey's machine had a window with a question and four answers. The student pressed the key to the chosen answer. The machine recorded the answer on a counter to the back of the machine, and showed the next question. \n\nThe great idea was to fix the machine so that it would not move on until the student chose the right answer. Then it was easy to show that this second arrangement taught the students which were the right answers. This was the first demonstration that a machine could teach, and also a demonstration that knowledge of results was the cause of the learning. This kind of feedback to the learner is basic: it just tells the learner whether they are right or not. Later work on other kinds of learning material showed that even better results were got when the feedback contained more explanatory material.\n\nPressey continued to improve his devices after World War II, and the papers of Pressey and his colleagues are reprinted in a leading sourcebook.\n\nA number of reviews credit Pressey with being the originator of teaching machines, and of important aspects of programmed learning. This was long before the better known efforts of B.F. Skinner. The review by Klaus gave a special appreciation of Pressey and his work. Skinner, who was responsible for bringing the whole subject into popular view, acknowledged Pressey's work in his 1958 paper on teaching machines.\n\nPressey's own term was \"adjunct autoinstruction\". He thought it important to follow learning by questions \"to enhance the clarity and stability of cognitive structure by correcting misapprehensions, and deferring the instruction of new matter until there had been such clarification and elucidation\". The topic itself might be programmed, or it might not.\n\nPressey's major textbook \"Psychology and the new education\", 1937 and 1944,\nis a prototypical cognitive text for student teachers. He writes (p369) of a diagnostic attack on teaching problems:\nPressey goes on to quote more published examples, and gives the data from some of these studies. The whole of chapter 10, The nature and control of the learning process, is directly relevant to the ideas of programmed learning which developed after World War II in the United States.\n\nPressey's whole approach to educational psychology ran in opposition to the influence of B.F. Skinner and the behaviorists, as this quotation illustrates:\n\n\n"}
{"id": "10369551", "url": "https://en.wikipedia.org/wiki?curid=10369551", "title": "Slickline", "text": "Slickline\n\nSlickline refers to a single strand wire which is used to run tools into wellbore for several purposes. It is used in the oil and gas industry, but also describes that niche of the industry that involves using a slickline truck or doing a slickline job.\n\nSlickline looks like a long, smooth, unbraided wire, often shiny, silver/chrome in appearance. It comes in varying lengths, according to the depth of wells in the area it is used (it can be ordered to specification) up to 35,000 feet in length. It is used to lower and raise downhole tools used in oil and gas well maintenance to the appropriate depth of the drilled well. In use and appearance it is connected by the drum it is spooled off of in the back of the slickline truck to the wireline sheave a round wheel grooved and sized to accept a specified line and positioned to redirect the line to another sheave that will allow the slickline to enter the wellbore. Slickline is used to lower downhole tools into an oil or gas well to perform a specified maintenance job downhole. Downhole refers to the area in the pipe below surface, the pipe being either the casing cemented in the hole by the drilling rig (which keeps the drilled hole from caving in and pressure from the various oil or gas zones downhole from feeding into one another) or the tubing, a smaller diameter pipe hung inside the casing.\n\nSlickline is more commonly used in production tubing. The wireline operator monitors at surface the slickline tension via a weight indicator gauge and the depth via a depth counter 'zeroed' from surface, lowers the downhole tool to the proper depth, completes the job by manipulating the downhole tool mechanically, checks to make sure it worked if possible, and pulls the tool back out by winding the slickline back onto the drum it was spooled from. The slickline drum is controlled by a hydraulic pump, which in turn is controlled by the 'slickline operator'.\n\nSlickline comes in different sizes and grades. The larger the size, and higher the grade, generally means the higher line tension can be pulled before the line snaps at the weakest spot and causes a costly 'fishing' job. Due to downhole tools getting stuck because of malfunctions or 'downhole conditions' including sand, scale, salt, asphaltenes, and other well byproducts settling or loosening off the pipe walls because of agitation either by the downhole tools or a change in downhole inflow, sometimes it is necessary to pull hard on the tools to bring them back uphole to surface. If the tools are stuck, and the operator pulls too hard, the line will snap or pull apart at the weakest spot, which is generally closer to surface as the further uphole the weak point in the line is, the more weight it has to support (the weight of the line).\n\nWeak spots in the line can be caused by making the circle around the counter wheel, making a bend around a sheave, a kink in a line from normal use (when rigging up the equipment extra line must be pulled out from the truck to give enough slack when the pressure control lubricator is picked up - this leaves line coiled on the often rutted ground, and sometimes it snags and kinks the line).\n\nWhen the slickline parts, this can create an expensive 'fishing' job. It is called fishing because you often have to try different 'fishing' tools until you get a 'bite', then you have to work the original tools downhole free, or cut off the slickline where they join the tools downhole so that you can pull the broken slickline back to surface and out of the way, in order to fish the stuck toolstring. Because of the downtime involved in 'fishing', meaning not being able to flow the oil/gas well, the client is losing money by lack of production and also the cost of the slickline unit to fish, and the cost of what is left in the hole if it is not fished out (in the oil/gas industry, if the cause of the fishing job was not the fault of the slickline company, the oil/gas company is usually responsible to pay for it, and it can be very expensive).\n\nSlickline was originally called measuring line, because the line was flat like a tape measure, and marked with depth increments so the operators would know how deep in the hole they were. This probably changed because the flat measuring line wasn't as strong as the modern slickline, and separate depth counters were developed.\nIt is advantageous to keep the diameter of the wire as small as possible for the following reasons:\n\n\nThe disadvantage of a smaller diameter slickline is the lower strength. Depth and the nature of the job (a tool that must be pulled hard or might be stuck) will affect what slickline truck (different trucks specialize in different sizes of line) used.\n\nThe sizes of solid wireline in most common uses are: 0.092\", 0.108\", 0.125\", 0.140\", 0.150\", and 0.160\" in diameter, and are obtainable from the wire-drawing mills in one-piece standard lengths of 18,000, 20,000, 25,000 and 30,000 foot lengths. Other diameters and lengths are usually available on request from the suppliers, with the largest size currently available at 0.188\".\n\nSlickline tools operate with a mechanical action, controlled from surface in the wireline trucks operators compartment. Typically, this mechanical action is accomplished by the operation of jars. There are generally two types of jars; mechanical and hydraulic.\n\nMechanical jars look like a long, tubular piece of machined metal that slides longer or shorter approximately 75% to 90% of its total length. They give the effect of hammering on the downhole tools. The weight or hit of the 'hammer' depends on how much sinker bar is added above the jars. Generally, a slickline operator controls the downhole tools with taps and hits from the sinker bar via the mechanical jars, controlled at surface by lowering or raising the toolstring and monitoring weight, depth, and pressure. Mechanical jars for slickline can hit up or down the hole, making them a versatile form of jarring.\n\nHydraulic jars for slickline are generally meant to jar up only, because not enough sinker bar is able to feasibly be lubricated in to jar down on the downhole tools. Hydraulic jars work by the operator pulling up on the line, which puts an upward force on the top of the hydraulic jars. The bottom of the hydraulic jars is usually attached by threaded connection to the mechanical jars, which are attached to the downhole tools. Depending on how hard the operator pulls on the hydraulic jars will affect how fast they hit, and how hard they hit. When the top is pulled on, the inner mandrel begins to slide upwards. It has a restriction in it that hydraulic fluid has to bypass as it is pulled upwards, until it reaches an area of no restriction, allowing it to slide rapidly. The reason for the initial tighter restriction is to allow the operator to pull his line to the desired hitting range.\n\nGenerally once he hits that range on his weight indicator, he waits while the jars open to the less restricted point, whereupon the sinker bar travels upwards rapidly, providing an upwards hit on the downhole tools. The jars can then be 'reset' by lowering the line until the weight of the sinker bar closes, or pushes the inner mandrel of the hydraulic jars back to the starting position. Because the hydraulic jars are designed to provide a wait time to allow the operator to get up to the desired line tension, they can provide a very effective upwards hit.\n\nMechanical jar and hydraulic jar hitting power is affected by the length of the jars (the longer the length, they faster they can travel before they stop), the mass of the weight above them (the more the mass, the harder they will hit), and the tension of the line pulling on them.\n\nSome completion components may be deployed and retrieved on slickline such as wireline retrievable safety valves, battery powered downhole gauges, perforating, placing explosively set bridge plugs, and placing or retrieving gas lift valves. Slickline can also be used for fishing, the process of trying to retrieve other equipment and wire, which has been dropped down the hole.\n\nThe most common applications for slickline are:\n\n\nBraided line is generally used when the strength of slickline is insufficient for the task. Most commonly, this is for heavy fishing such as retrieving broken drill pipe.\n\nThis type of tool can be extended and closed rapidly to induce a mechanical shock to the tool string. This shock can induce certain components such as plugs to lock into place and then unlock for retrieving. Jars are commonly used to shear small brass or steel pins that are put in place to function certain down-hole tools at a certain moment. The operator can use the jars to shear the pins at a predetermined depth. Spang jars are manually operated by the wireline operator who either lifts or lowers wire rapidly, requiring a great deal of expertise.\n\nStem essentially just serves to add weight to the toolstring. The weight may be necessary to overcome the pressure of the well. Some variations of stem, called roller stem, may have wheels built into the tool to allow the tool string to glide more easily down moderately deviated wells. Stem give the hammering action to the tool string which in turn allows the jars to transmit the force given by the movement of the stems bars. Depending on well conditions, either extra small OD stems are use or extra large. The range can be from .75\" to 3.50\" OD and the stems normally come in 2 ft, 3 ft or 5 ft lengths. The connection to the rope socket or other tools can be a threaded connection or a QLS system (quick connect).\n\nThese are tools designed for fishing other wireline components which have been dropped or placed in the well down hole. All wireline tools are designed with 'fishing necks' on their top side, intended to be easily grabbed by pulling tools with a matching 'id' to that of the 'od' of the fishing neck. Pulling tools are also used for retrieving seated components such as plug prongs. Almost all pulling tools are equipped with a safety feature (shear pin) so they may release a stuck tool and allow the tool string to be brought to the surface for changes in components (hydraulic jars for example).\n\nA gauge cutter is a tool with a round, open-ended bottom which is milled to an accurate size. Large openings above the bottom of the tool allow for fluid bypass while running in the hole.\nMost often a gauge ring will be the first tool ran on a slickline operation. A gauge ring that is just undersized will allow the operator to ensure clear tubing down to the deepest projected working depth; for example 2 7/8\" tubing containing 2.313\" profiles would call for a gauge ring between 2.25\" - 2.30\". \nA gauge ring can also be used to remove light paraffin that may have built up in the tubing. Often a variety of different sized gauges and/or scratchers will be run to remove parafin little by little.Gauge cutter can be used for drift runs also.\n\nIf an obstruction is found downhole, a lead impression block can be run to help determine its nature. The LIB has a malleable lead base in which the obstruction can leave an impression when they meet. \nThe LIB is called Wireline Camera because of its function to mark any object downhole.\nThey are also sometimes called \"confusion\" blocks because they only give a two-dimensional view of the down-hole object, making it hard for an inexperienced person to determine what three-dimensional object is in the hole\n\nBailers are downhole tools that are generally long and tubular shaped, and are used for both getting samples of downhole solids (sand, scale, asphaltines, rust, rubber and debris from well servicing operations) and for 'bailing' the unwanted downhole solids from the well. Bailers are attached either via threaded connection or releasable downhole tool to the wireline toolstring, and are manipulated from surface by the wireline operator. Bailers usually have an interchangeable bottom (the shoe) which also houses a check to keep the solids from falling or washing out of the bottom.\n\nA sample bailer is generally around a meter long, and has a hollow tube (the barrel) usually around 40 mm in diameter, with a 'ball check' on the bottom and an opening at the top. This tool is beat downwards into the as yet unknown obstruction using the mechanical jars and weight above of the wireline toolstring. Generally, after a predetermined amount of 'hits', hopefully allowing a usable sample of solids to fill the barrel. When the tool is pulled upwards, the solids usually (hopefully) settle the ball check onto its 'seat', which will keep the solids in the barrel during the return trip to surface, where the solids can be inspected to determine what the downhole obstruction was. This procedure can be 'hit and miss', the success depending on how readily the solid was accepted into the barrel, and if the ball check was properly seated on the return trip to surface. If the ball check is not seated (sometimes a large, hard piece of solid will sit in between the ball and seat) downhole fluids tend to 'wash' the sample out of the bottom of the sample bailer, leaving the inspectors at surface wondering if the tool actually collected a sample. Persistence is generally a good rule of thumb with this tool.\n\nA stroke bailer functions like a 'Chinese water pump', and is used to collect unwanted solids from the wellbore. A stroke bailer is long and tubular looking, with a smaller rod that extends from the top, a hole in the bottom, and is generally around 7 meters long, but the length depends on how much barrel section is added to the bailer. The barrel 'free floats' on the stroke rod, which is attached to the wireline toolstring. The tool is usually 'spudded' into the downhole solid, then the wireline toolstring is pulled upwards, which in turn pulls the stroke up through the barrel. Ideally, this draws the downhole solid in through the bottom 'shoe' of the tool, past the check and into the barrel for collection. The tool is usually stroked either a predetermined number of times, or until it appears the tool is not stroking, which can mean either it is full, or stuck.\n\nA hydrostatic bailer functions like a 'vacuum', and is used to suck up unwanted solids from the wellbore. A hydrostatic bailer is generally around 2.5 meters long and is tubular looking, with two 10 mm holes on opposing sides at the top of the tool, and a hole in the bottom. A hydrostatic bailer uses a pinned plug with o-ring seals at the bottom, and a plug at the top to maintain the surface pressure that it was assembled at (nominally around 100 kPa) all the way to the bottom of the well, whereupon it is spudded into the downhole solids, which ideally pushes the shoe into the bottom plug, which shears the pin on the bottom plug. An oil or gas well's pressure downhole is always more than atmospheric pressure at surface, due to the formation pressure, and a combination of depth and hydrostatic weight of wellbore fluids. Sometimes fluid will be added to the wellbore to assist in bailing by bringing up the pressure, and also lubricating the downhole solid. Because the pressure inside the bailer is much less than the downhole wellbore pressure, any solids that are loose enough are 'sucked up' by the vacuum formed when the bottom plug is sheared and travels upwards through the barrel, followed by the solids. At the same time, due to the changed from negative pressure to positive pressure, the top plug pops out (and is caught by the top part of the tool), and excess flow is directed out through the 10 mm ports on the sides of the top of the tool. These ports allow the barrel to fill more readily. Then the bailer is returned to surface where it is taken apart, the solids are emptied, and it is cleaned and serviced with new o-ring seals. Care must be taken when disassembling at surface as the tool is potentially charged with the downhole pressure (possibly many tens of thousands of kpa) and may 'blow apart' when being unthreaded if not bled off first.\n\nThese tools are primarily used to 'set' plugs into locking profiles (nipples) located in the tubing; however, the term 'running tool' refers to a downhole tool attached to the wireline toolstring that is used to 'run' another tool that is meant to be left downhole when the toolstring returns to surface. In general, a running tool is attached to a downhole 'locking tool' that locates and locks into the selected downhole profile (nipple). The 'locking tool', or 'lock' for short, can be attached via threaded connection to the top of a variety of different tools, including but not limited to, downhole chokes (flow rate restrictors sized according to a pre-determined calculation), one-way check valves (TKX style plugs), instrument hangers, and most commonly, tubing plugs. The lock is fitted onto the running tool and attached using shear pins made of brass or steel. When the target profile is reached the lock can be set by seating the lock into the profile using mechanical jars (spangs) until the locking keys have locked the lock into the profile, whereupon the operator usually 'pull tests' the lock to give an indication it is properly 'set', then shears off the shear pins with his mechanical or hydraulic jars to allow the 'toolstring' to return to surface. There are many different types of running tools, some are mechanically complex and able to be made 'selective' in order to pass through profiles in order to reach one of the same size but a different depth; some are relatively simple, such as an 'F' collarstop running tool, which is essentially a metal rod which fits inside the collarstop downhole tool which is pinned in place.\n"}
{"id": "40226694", "url": "https://en.wikipedia.org/wiki?curid=40226694", "title": "Suzhou Oriental Semiconductor", "text": "Suzhou Oriental Semiconductor\n\nSuzhou Oriental Semiconductor Co., Ltd (), abbreviated as Oriental Semiconductor (), was founded in 2008, located in SIP, Suzhou, China. It invented the world's first semi-floating gate transistor (SFGT) with Fudan University in Shanghai. The related research paper was published on \"Science\" on August 9, 2013.\n\n"}
{"id": "41443291", "url": "https://en.wikipedia.org/wiki?curid=41443291", "title": "Swill milk scandal", "text": "Swill milk scandal\n\nThe Swill milk scandal was a major adulterated food scandal in New York in the 1850s. \"The New York Times\" reported an estimate that in one year 8,000 infants died from swill milk.\n\nIt is named swill milk because cows were fed swill which was residual mash from nearby distilleries. The milk was whitened with plaster of Paris, thickened with starch and eggs, and hued with molasses.\n\nAfter the extraction of alcohol from the macerated grain, the residual mash still contains nutrients, and therefore it was an economical advantage to keep cows stabled near distilleries and feed them with swill.\n\nThe New York Academy of Medicine carried out an examination and established the connection of swill milk with the increased infant mortality in the city. The topic of swill milk was also well exposed in pamphlets and caricatures of the time.\n\nIn May 1858, \"Frank Leslie's Illustrated Newspaper\" did a landmark exposé of the distillery-dairies of Manhattan and Brooklyn that marketed so-called swill milk that came from cows fed on distillery waste and then adulterated with water, eggs, flour, and other ingredients that increased the volume and masked the adulteration. Swill milk dairies were noted for their filthy conditions and overpowering stench both caused by the close confinement of hundreds (sometimes thousands) of cows in narrow stalls where, once they were tied, they would stay for the rest of their lives, often standing in their own manure, covered with flies and sores, and suffering from a range of virulent diseases. These cows were fed on boiling distillery waste, often leaving the cows with rotting teeth and other maladies. The milk drawn from the cows was routinely adulterated with water, rotten eggs, flour, burnt sugar and other adulterants with the finished product then marketed falsely as \"pure country milk\" or \"Orange County Milk\".\n\nIn an editorial published at the height of the scandal, the \"New York Times\" described swill milk as a \"bluish, white compound of true milk, pus and dirty water, which, on standing, deposits a yellowish, brown sediment that is manufactured in the stables attached to large distilleries by running the refuse distillery slops through the udders of dying cows and over the unwashed hands of milkers...\"\n\nFrank Leslie's exposé caused widespread public outrage and local politicians were strongly pressured to punish and regulate the distillery-dairies, which were formally complained to be \"swill milk nuisance\". The Tammany Hall politician Alderman Michael Tuomey, known as \"Butcher Mike\" defended the distillers vigorously throughout the scandal—in fact, he was put in charge of the Board of Health investigation. \"Frank Leslie's Illustrated Newspaper\" staked out distillery owner Bradish Johnson's mansion at 21st and Broadway, and reported that in the midst of the investigation, Tuomey was observed making late night visits. Tuomey assumed a central role in the ensuing investigations, and, with fellow Aldermen E. Harrison Reed and William Tucker, shielded the dairies and turned the hearings into one-sided exercises designed to make dairy critics and established health authorities look ridiculous, even going to the extent of arguing that swill milk was actually as good or better for children than regular milk. With Reed and others, Tuomey successfully blocked any serious inquiry into the dairies and stymied calls for reform. The Board of Health exonerated the distillers, but public outcry led to the passage of the first food safety laws in the form of milk regulations in 1862. Tuomey became known for his attempts to block the new regulations, and earned the new moniker \"Swill Milk\" Tuomey. In addition to Tuomey's assistance in clearing up the unclean image milk developed, Robert Hartley a social reformist, aided in the restoration of milk being a nutritional and safe-to-drink beverage. During the mid to late nineteenth century, Hartley utilized Biblical references in his essays to appeal to the urban community. He asserted that universal milk consumption could help alleviate society's \"sins\", poverty, and alcohol consumption.\n\nRobert Hartley was America's first consumer advocate and milk agitator. After resigning from his job as a factory manager in Mohawk Valley, New York. Hartley decided to move to NYC, to dedicate his life to joining and establishing many of the cities major reform organizations: such as, the New York City Temperance Society, The City Mission tract Society, and The New York Association for Improving the Condition of the Poor (DuPuis). Hartley had a desire to better the lives of humans, which is why he was interested in New York City's poor milk supply.\n\nHartley's desire to perfect human society provided the basis for his interest in the New York City milk supply. As a temperance reformer, he aimed to eliminate the extra profits provided to the city's brewers through the linked milk and alcohol production systems. As a social reformer interested in the welfare of the poor, he improved the New York milk supply. Beginning in the 1830s in newspaper articles and lectures, and eventually summarized in a publication titled \"A Historical, Scientific, and Practical Essay on Milk as an Article of Human Sustenance\", Hartley exposed the unsanitary practices of the swill milk system. He was the first American to make a sustained argument that milk was the perfect food.\n"}
{"id": "5912825", "url": "https://en.wikipedia.org/wiki?curid=5912825", "title": "Typewriting Behavior", "text": "Typewriting Behavior\n\nTypewriting Behavior is a book by August Dvorak, Nellie Merrick, William Dealey and Gertrude Ford. It was published in 1936 by the American Book Company. It is currently out of print but can be found in most major libraries.\n\nThe book is a study on the psychology of typing. It gives a scientific approach to teaching and learning typewriting, from personalities to patterns and machine effects. It gives an in depth overview on the subject of typing.\n\nThis book also introduced the Dvorak Simplified Keyboard.\n\n"}
{"id": "5166771", "url": "https://en.wikipedia.org/wiki?curid=5166771", "title": "Universal library", "text": "Universal library\n\nA universal library is a library with universal collections. This may be expressed in terms of it containing all existing information, useful information, all books, all works (regardless of format) or even all possible works. This ideal, although unrealizable, has influenced and continues to influence librarians and others and be a goal which is aspired to. Universal libraries are often assumed to have a complete set of useful features (such as finding aids, translation tools, alternative formats, etc.).\n\nThe Library of Alexandria is generally regarded as the first library approaching universality, although this idea may be more mythical than real. It is estimated that at one time, this library contained between 30 and 70 percent of all works in existence. The re-founded modern library has a non-universal collections policy.\n\nAs a phrase, the \"universal library\" can be traced back to the naturalist Conrad Gessner's \"Bibliotheca universalis\" of 1545.\n\nIn the 17th century, the ideal of universality continued to be attractive. The French librarian Gabriel Naudé wrote:\n\nAnd therefore I shall ever think it extremely necessary, to collect for this purpose all sorts of books, (under such precautions, yet, as I shall establish) seeing a Library which is erected for the public benefit, ought to be universal; but which it can never be, unlesse it comprehend all the principal authors, that have written upon the great diversity of particular subjects, and chiefly upon all the arts and sciences; [...] For certainly there is nothing which renders a Library more recommendable, then when every man findes in it that which he is in search of ...\nScience fiction has used the device of a library which is universal in the sense that it not only contains all existing written works, but all possible written works. This idea appeared in Kurd Lasswitz's 1901 story \"The Universal Library\" and Borges's essay \"The Total Library\" before its more famous expression in Borges's story \"The Library of Babel\". Such a library, however, would be as useless as it would be complete. A similar idea was a planet called Memory Alpha, (from the Star Trek episode \"The Lights of Zetar\") which was the Federation's \"storehouse of computer databases containing all cultural history and scientific data it has acquired.\". It has been commented that the Internet already approaches this state.\n\nIn Discworld, Terry Pratchett's fantasy world, all libraries in the multiverse are connected in \"L-space\", effectively creating a single semi-universal library.\n\nWith the advent of cheap widely available digital storage, the ideal of universality, although still impossible to attain, has become closer to being feasible. Many projects are now attempting to collect a section of human knowledge into one database. These projects vary in breadth and scope, and none are complete. Examples include digitization projects such as Project Gutenberg and Carnegie-Mellon's Universal library, digital libraries which are using book scanning to collect public domain works; The European Library, an integrated catalog for Europe's national libraries; and the Wikimedia Foundation, which, using the Wiki system, is attempting to collect the breadth of important human knowledge under various open content projects such as Wikipedia and Wiktionary. However, many technical and legal problems remain for the dissemination of all possible knowledge on the Internet.\n\nCurrent barriers to the construction of a universal digital library include:\n\n"}
{"id": "588001", "url": "https://en.wikipedia.org/wiki?curid=588001", "title": "Waste treatment", "text": "Waste treatment\n\nWaste treatment refers to the activities required to ensure that waste has the least practicable impact on the environment. In many countries various forms of waste treatment are required by law.\n\nThe treatment of solid wastes is a key component of waste management. Different forms of solid waste treatment are graded in the waste hierarchy.\n\nAgricultural wastewater treatment is treatment and disposal of liquid animal waste, pesticide residues etc. from agriculture.\n\nIndustrial wastewater treatment is the treatment of wet wastes from manufacturing industry and commerce including mining, quarrying and heavy industries\n\nSewage treatment is the treatment and disposal of human waste. Sewage is produced by all human communities and is often left to compost naturally or is treated using processes that separate solid materials by settlement and then convert soluble contaminants into biological sludge and into gases such as carbon dioxide or methane.\n\nRadioactive waste treatment is the treatment and containment of radioactive waste.\n\n"}
{"id": "3862103", "url": "https://en.wikipedia.org/wiki?curid=3862103", "title": "Winbond", "text": "Winbond\n\nWinbond Electronics Corporation () is a Taiwan-based corporation founded in 1987 that produces semiconductors and several types of integrated circuits, most notably Dynamic RAM, Static RAM, microcontrollers, and personal computer ICs, namely Super I/O chips. Winbond is currently the largest brand name integrated circuit supplier in Taiwan, and one of the biggest suppliers of semiconductor solutions worldwide.\n\nComputer IC, Consumer Electronics IC, and Logic Product Foundry of Winbond product lines have been spun off as Nuvoton Technology Corporation on 1 July 2008.\n\n\n"}
