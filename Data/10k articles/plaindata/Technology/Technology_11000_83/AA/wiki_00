{"id": "4075738", "url": "https://en.wikipedia.org/wiki?curid=4075738", "title": "Actor model later history", "text": "Actor model later history\n\nIn computer science, the Actor model, first published in 1973 , is a mathematical model of concurrent computation. This article reports on the later history of the Actor model in which major themes were investigation of the basic power of the model, study of issues of compositionality, development of architectures, and application to Open systems. It is the follow on article to Actor model middle history which reports on the initial implementations, initial applications, and development of the first proof theory and denotational model.\n\nInvestigations began into the basic power of the Actor model. Carl Hewitt [1985] argued that because of the use of Arbiters that the Actor model was more powerful than logic programming (see indeterminacy in concurrent computation).\n\nA family of Prolog-like concurrent message passing systems using unification of shared variables and data structure streams for messages were developed by Keith Clark, Hervé Gallaire, Steve Gregory, Vijay Saraswat, Udi Shapiro, Kazunori Ueda, \"etc.\" Some of these authors made claims that these systems were based on mathematical logic. However, like the Actor model, the Prolog-like concurrent systems were based on message passing and consequently were subject to indeterminacy in the ordering of messages in streams that was similar to the indeterminacy in arrival ordering of messages sent to Actors. Consequently Carl Hewitt and Gul Agha [1991] concluded that the Prolog-like concurrent systems were neither deductive nor logical. They were not deductive because computational steps did not follow deductively from their predecessors and they were not logical because no system of mathematical logic was capable of deriving the facts of subsequent computational situations from their predecessors\n\nCompositionality concerns composing systems from subsystems. Issues of compositionality had proven to be serious limitations for previous theories of computation including the lambda calculus and Petri nets. \"E.g.,\" two lambda expressions are not a lambda expression and two Petri nets are not a Petri net and cannot influence each other.\n\nIn his doctoral dissertation Gul Agha addressed issues of compositionality in the Actor model. Actor \"configurations\" have \"receptionists\" that can receive messages from outside and may have the addresses of the receptionists of other Actor configurations. In this way two Actor configurations can be composed into another configuration whose subconfigurations can communicate with each other. Actor configurations have the advantage that they can have multiple Actors (\"i.e.\" the receptionists) which receive messages from outside without the disadvantage of having to poll to get messages from multiple sources (see issues with getting messages from multiple channels).\n\nCarl Hewitt [1985] pointed out that openness was becoming a fundamental challenge in software system development. Open distributed systems are required to meet the following challenges:\n\n\nCarl Hewitt and Jeff Inman [1991] worked to develop semantics for Open Systems to address issues that had arisen in Distributed Artificial Intelligence. Carl Hewitt and Carl Manning [1994] reported on the development of Participatory Semantics for Open Systems.\n\nResearchers at Caltech under the leadership of Chuck Seitz developed the Cosmic Cube which was one of the first message-passing Actor architectures. Subsequently at MIT researchers under the leadership of Bill Dally developed the J Machine.\n\nKohei Honda and Mario Tokoro 1991, José Meseguer 1992, Ugo Montanari and Carolyn Talcott 1998, M. Gaspari and G. Zavattaro 1999 have attempted to relate Actor semantics to algebra. Also John Darlington and Y. K. Guo 1994 have attempted to relate linear logic to Actor semantics.\n\nHowever, none of the above formalisms addresses the crucial property of guarantee of service (see unbounded nondeterminism).\n\nRecent developments in the Actor model have come from several sources.\n\nHardware development is furthering both local and nonlocal massive concurrency. Local concurrency is being enabled by new hardware for 64-bit many-core microprocessors, multi-chip modules, and high performance interconnect. Nonlocal concurrency is being enabled by new hardware for wired and wireless broadband packet switched communications. Both local and nonlocal storage capacities are growing exponentially. These hardware developments pose enormous modelling challenges. Hewitt [Hewitt 2006a, 2006b] is attempting to use the Actor model to address these challenges.\n\n"}
{"id": "9319179", "url": "https://en.wikipedia.org/wiki?curid=9319179", "title": "Alinco", "text": "Alinco\n\nAlinco ( \"Alinco Inc.\") () is a Japanese manufacturer of radio and amplification equipment, and in the Japanese market, metal products, construction equipment, and exercise equipment.\n\nEstablished in 1938 in Osaka, Japan, it also has offices in Tokyo, Takatsuki, manufacturing facilities in Toyama and Hyōgo in Japan, and one in Suzhou, China.\n\n"}
{"id": "8572024", "url": "https://en.wikipedia.org/wiki?curid=8572024", "title": "American Competitiveness Institute", "text": "American Competitiveness Institute\n\nThe American Competitiveness Institute now known as ACI Technologies Inc. (ACI) is an American scientific research corporation founded by Alan J. Criswell in 1992. Located in Philadelphia, Pennsylvania, ACI operates a facility dedicated to the advancement of the electronics industry and research and development of various electronic manufacturing methods and materials. Early in its existence, ACI acquired a defense contract from the Office of Naval Research known as the Electronics Manufacturing Productivity Facility (EMPF).\n\n\n"}
{"id": "23642794", "url": "https://en.wikipedia.org/wiki?curid=23642794", "title": "Apache Cordova", "text": "Apache Cordova\n\nApache Cordova (formerly PhoneGap) is a mobile application development framework originally created by Nitobi. Adobe Systems purchased Nitobi in 2011, rebranded it as PhoneGap, and later released an open source version of the software called Apache Cordova. Apache Cordova enables software programmers to build applications for mobile devices using CSS3, HTML5, and JavaScript instead of relying on platform-specific APIs like those in Android, iOS, or Windows Phone. It enables wrapping up of CSS, HTML, and JavaScript code depending upon the platform of the device. It extends the features of HTML and JavaScript to work with the device. The resulting applications are hybrid, meaning that they are neither truly native mobile application (because all layout rendering is done via Web views instead of the platform's native UI framework) nor purely Web-based (because they are not just Web apps, but are packaged as apps for distribution and have access to native device APIs). Mixing native and hybrid code snippets has been possible since version 1.9.\n\nThe software was previously called just \"PhoneGap\", then \"Apache Callback\". As open-source software, Apache Cordova allows wrappers around it, such as Appery.io or Intel XDK.\n\nPhoneGap is Adobe's commercial version of Cordova along with its associated ecosystem. Many other tools and frameworks are also built on top of Cordova, including Ionic, Monaca, TACO, Onsen UI, Visual Studio, GapDebug, App Builder, Cocoon, Framework7, Quasar Framework, Evothings Studio, NSB/AppStudio, Mobiscroll, the Intel XDK, and the Telerik Platform. These tools use Cordova, and not PhoneGap for their core tools.\n\nContributors to the Apache Cordova project include Adobe, BlackBerry, Google, IBM, Intel, Microsoft, Mozilla, and others.\n\nFirst developed at an iPhoneDevCamp event in San Francisco, PhoneGap went on to win the People's Choice Award at O'Reilly Media's 2009 Web 2.0 Conference, and the framework has been used to develop many apps. Apple Inc. has confirmed that the framework has its approval, even with the new 4.0 developer license agreement changes. The PhoneGap framework is used by several mobile application platforms such as Monaca, appMobi, Convertigo, ViziApps, and Worklight as the backbone of their mobile client development engine.\n\nAdobe officially announced the acquisition of Nitobi Software (the original developer) on October 4, 2011. Coinciding with that, the PhoneGap code was contributed to the Apache Software Foundation to start a new project called Apache Cordova. The project's original name, Apache Callback, was viewed as too generic. Then, it also appears in Adobe Systems as \"Adobe PhoneGap\" and also as \"Adobe PhoneGap Build\".\n\nEarly versions of PhoneGap required an Apple computer to create iOS apps and a Windows computer to create Windows Mobile apps. After September 2012, Adobe's PhoneGap Build service allows programmers to upload CSS, HTML, and JavaScript source code to a \"cloud compiler\" that generates apps for every supported platform.\n\nThe core of Apache Cordova applications use CSS3 and HTML5 for their rendering and JavaScript for their logic. HTML5 provides access to underlying hardware such as the accelerometer, camera, and GPS. However, browsers' support for HTML5-based device access is not consistent across mobile browsers, particularly older versions of Android. To overcome these limitations, Apache Cordova embeds the HTML5 code inside a native WebView on the device, using a foreign function interface to access the native resources of it.\n\nApache Cordova can be extended with native plug-ins, allowing developers to add more functionalities that can be called from JavaScript, making it communicate directly between the native layer and the HTML5 page. These plugins allow access to the device's accelerometer, camera, compass, file system, microphone, and more.\n\nHowever, the use of Web-based technologies leads some Apache Cordova applications to run slower than native applications with similar functionality. Adobe Systems warns that applications built with Apache Cordova may be rejected by Apple for being too slow or not feeling \"native\" enough (having appearance and functionality consistent with what users have come to expect on the platform).\n\nApache Cordova currently supports development for the operating systems Apple iOS, Bada, BlackBerry, Firefox OS, Google Android, LG webOS, Microsoft Windows Phone (7 and 8), Nokia Symbian OS, Tizen (SDK 2.x), and Ubuntu Touch. The table below is a list of supported features for each operating system.\n\n\n"}
{"id": "17923291", "url": "https://en.wikipedia.org/wiki?curid=17923291", "title": "Banred", "text": "Banred\n\nBanred is a network of automated teller machines (ATMs) in Uruguay with over 330 locations. It used to be two separate networks, RedBanc and Bancomat, which merged in 2005.\n\nMost private banks are members of the Banred network, including BBVA, Itaú, Santander and Scotiabank. its main competitir is RedBROU, owned by state bank BROU.\n\n"}
{"id": "14999140", "url": "https://en.wikipedia.org/wiki?curid=14999140", "title": "Bedourie oven", "text": "Bedourie oven\n\nThe Bedourie oven is an Australian adaptation of the camp oven (Dutch oven). Drovers working on Bedourie Station, in western Queensland, found that the heavy cast iron camp ovens they used for cooking would often break as a result of falling from their pack horses. The Bedourie oven was developed in response to this problem. Made from mild steel, it was lighter and less brittle than cast iron, and would not break if dropped.\n\n"}
{"id": "38212757", "url": "https://en.wikipedia.org/wiki?curid=38212757", "title": "Bricks and clicks", "text": "Bricks and clicks\n\nBricks and clicks (aka clicks and bricks; click and mortar; bricks, clicks and flips; Womble Store Method (WSM); or WAMBAM ('Web Application Meets Bricks and Mortar') is a jargon term for a business model by which a company integrates both offline (\"bricks\") and online (\"clicks\") presences, sometimes with the third extra \"flips\" (physical catalogs). Additionally, many will also offer telephone ordering and mobile phone apps, or at least provide telephone sales support. The advent of mobile web has made businesses operating bricks and clicks businesses especially popular, because it means customers can do tasks like shopping when they have spare time and do not have to be at a computer. Many of these users prefer to use mobile shopping sites.\n\nA popular example of the \"bricks and clicks\" model is when a chain of stores allows the customer to order products either online or physically in one of their stores, also allowing them to either pick-up their order directly at a local branch of the store or get it delivered to their home. There are many alternative combinations of this model. The success of the model in many sectors has lessened the credibility of some analysts who argued that the Internet would render traditional retailers obsolete through disintermediation.\n\nOne of the first known purchases from a company arguably operating a bricks and clicks business model was a Pizza Hut pizza ordered over the internet in 1994. The great surge in adoption of the bricks and clicks model came around 2000, with large retailers such as Wal Mart starting websites that allow users to browse the same goods they would find in store from the comfort of their homes.\n\nThe bricks and clicks model has typically been used by traditional retailers who have extensive logistics and supply chains, but are well known and often respected for their traditional physical presence. Part of the reason for its success is that it is far easier for a traditional retailer to establish an online presence than it is for a start-up company to employ a successful purely online one, or for an online only retailer to establish a traditional presence, including a strong and well recognised brand, without having a large marketing budget. It can also be said that adoption of a bricks and clicks model where a customer can return items to a brick and mortar store can reduce wasted costs to a business such as shipping for undelivered and returned items that would traditionally be incurred.\n\nA bricks and clicks business model can benefit various members of a customer base. For example, supermarkets often have different customer types requiring alternative shopping options; one group may wish to see the goods directly before purchase and like the convenience of quickly shopping on-the-fly, while another group may require a different convenience of shopping online and getting the order delivered when it suits them, having a bricks and clicks model means both customer groups are satisfied. Other previously online-only retailers have stated that they have found benefit in adding a brick-and-mortar presence to their online-only business, as customers can physically see and test products before purchase as well as get advice and support on any purchases they have made. Additionally, consumers are likely to feel safer and have more confidence using a bricks-and-clicks business if they already know the brand from a brick-and-mortar store.\n\nA major factor in the success or failure of this business model is in the control of costs, as usually maintaining a physical presence —paying for many physical store premises and their staffing— requires larger capital expenditure which online only businesses do not usually have. Conversely, a business selling more luxurious, often expensive, or only occasionally purchased products —like cars— may find sales are more common with a physical presence, due to the more considered nature of the purchasing decision, though they may still offer online product information. However, some car manufacturers such as Dacia have introduced online configurators that allow a customer to configure and order complete cars online, only going to a dealership to collect the completed car, which has proven popular with customers.\n\n\"On the other hand, an online-only service can remain a best-in-class operation because its executives focus on just the online business.\" It has been argued that a bricks and clicks business model is more difficult to implement than an online only model. In the future, the bricks and clicks model may be more successful, but in 2010 some online only businesses grew at a staggering 30%, while some bricks and clicks businesses grew at a paltry 3%. The key factor for a bricks and clicks business model to be successful \"will, to a large extent, be determined by a company’s ability to manage the trade-offs between separation and integration\" of their retail and online businesses.\n\n\nAn advantage to the consumer and a potential disadvantage to businesses is that by adopting a bricks and clicks business model and allowing customers to purchase goods or services remotely, it is legislated in many jurisdictions that consumers are granted more rights to protect them. In the UK, for example, any goods purchased from a bricks and clicks business over a 'click and collect' service would allow the buyer protection under the Consumer Protection (Distance Selling) Regulations 2000, namely the right to return a product or cancel a service within 14 days of purchase for a full refund. Similar rights are afforded to EU Residents, who gain protection under European Directive 97/7/EC. In the USA, the Federal Trade Commission legislate specifically over how a distance sale should be conducted and the rights that a consumer has, namely a '3 day' rule allowing items ordered over the web to be returned within three days.\n\nAn example of a retailer falling foul of this legislation is British clothing retailer Next, who were found to be breaking the laws by only allowing a customer to return goods that they had ordered if they paid return postage costs.\n\nIn the UK, the method is known as \"Click and Collect\". This term was invented by British retailer Argos who already offered \"Ring and Reserve\" and \"Text and Take Home\" offerings for telephone and SMS ordering respectively, where goods would be held so the customer would pay in store. As these existing services used alliterations for their name, they needed a name for their online ordering proposition and came up with Click and Collect.\n\nBritish retailer John Lewis has found success in adopting a bricks and clicks business model, with the online ordering service outperforming brick and mortar sales for several years running. Online auction website eBay have also launched a scheme in cooperation with catalogue shop Argos that allows goods sold by third parties to be collected in a brick-and-mortar location, which allows the customer to collect goods at their convenience rather than wait at home for a delivery company.\n\nIn 2013, music retailer HMV went into administration despite having operated both brick-and-mortar stores and an online presence. This was put down by many to the high overheads of operating the brick-and-mortar side of the business making HMV unable to compete with pure-clicks retailers such as Amazon.com.\n\n\n"}
{"id": "38218021", "url": "https://en.wikipedia.org/wiki?curid=38218021", "title": "Cadence Biomedical", "text": "Cadence Biomedical\n\nCadence Biomedical is a medical device company that provides orthotic products to help individuals with severe mobility impairments to walk again. The company is located in Seattle, Washington and was founded in 2007 under the name Empowering Engineering Technologies.\n\nCadence Biomedical released its first product, the Kickstart Walking System, in August 2012. Kickstart is a wearable device, or orthosis, that gives users stability and the ability to walk independently. The device is intended to improve mobility for those in stroke recovery, or for individuals with neurological injuries such as spinal cord injury, traumatic brain injury, multiple sclerosis or muscular dystrophy. It is designed to provide walking assistance and stability for those who have difficulty walking, especially if they experience difficulty lifting their knee, catching toes when taking a step, lack of endurance, or problems with coordination, balance, or stability.\n\nThe device uses no external power or batteries to provide assistance, but functions similarly to robotic exoskeletons in that it helps to move the legs forward and augments existing strength. The product was released in 2012 and is available through orthotists in the United States.\n"}
{"id": "8663903", "url": "https://en.wikipedia.org/wiki?curid=8663903", "title": "Cardiovascular technologist", "text": "Cardiovascular technologist\n\nCardiovascular technologists (also known as a cardiovascular or vascular technicians) are health professionals that deal with the circulatory system.\n\nTechnologists who use ultrasound to examine the heart chambers, valves, and vessels are referred to as cardiac sonographers. They use ultrasound instrumentation to create images called echocardiograms. An echocardiogram may be performed while the patient is either resting or physically active. Technologists may administer medication to physically active patients to assess their heart function. Cardiac sonographers also may assist physicians who perform transesophageal echocardiography, which involves placing a tube in the patient’s esophagus to obtain ultrasound images.\n\nThose who assist physicians in the diagnosis of disorders affecting the circulation are known as vascular technologists, vascular specialists or vascular sonographers. They obtain a medical history, evaluate pulses and assess blood flow in arteries and veins by listening to the vascular flow sounds for abnormalities. Then they perform a noninvasive procedure using ultrasound instrumentation to record vascular information such as vascular blood flow, blood pressure, changes in limb volume, oxygen saturation, cerebral circulation, peripheral circulation, and abdominal circulation. Many of these tests are performed during or immediately after surgery.\n\nCardiovascular technicians who obtain EKGs are known as electrocardiograph (or EKG) technicians. To take a basic EKG, which traces electrical impulses transmitted by the heart, technicians attach electrodes to the patient’s chest, arms, and legs, and then manipulate switches on an EKG machine to obtain a reading. An EKG is printed out for interpretation by the physician. This test is done before most kinds of surgery or as part of a routine physical examination, especially on persons who have reached middle age or who have a history of cardiovascular problems.\n\nEKG technicians with advanced training setup Holter monitor and stress testing. For Holter monitoring, technicians place electrodes on the patient’s chest and attach a portable EKG monitor to the patient’s belt. Following 24 or more hours of normal activity by the patient, the technician removes a tape from the monitor and places it in a scanner. After checking the quality of the recorded impulses on an electronic screen, the technician usually prints the information from the tape for analysis by a physician. Physicians use the output from the scanner to diagnose heart ailments, such as heart rhythm abnormalities or problems with pacemakers.\n\nFor a treadmill stress test, EKG technicians document the patient’s medical history, explain the procedure, connect the patient to an EKG monitor, and obtain a baseline reading and resting blood pressure. Next, they monitor the heart’s performance while the patient is walking on a treadmill, gradually increasing the treadmill’s speed to observe the effect of increased exertion.\n\nThe position is generally unlicensed and skills are learned on the job; however, two- and four-year training programs to learn advanced ECG technical skills are available at junior colleges and community colleges.\n\nAmerican Registry of Diagnostic Medical Sonographers (A.R.D.M.S.)\n"}
{"id": "5584556", "url": "https://en.wikipedia.org/wiki?curid=5584556", "title": "Configurator", "text": "Configurator\n\nConfigurators, also known as choice boards, design systems, toolkits, or co-design platforms, are responsible for guiding the user through the configuration process. Different variations are represented, visualized, assessed and priced which starts a learning-by-doing process for the user. While the term “configurator” or “configuration system” is quoted rather often in literature, it is used for the most part in a technical sense addressing a software tool. The success of such an interaction system is, however, not only defined by its technological capabilities, but also by its integration in the whole sale environment, its ability to allow for learning by doing, to provide experience and process satisfaction, and its integration into the brand concept. ()\n\nConfigurators can be found in various forms and different industries (). They are employed in B2B (business to business) as well as B2C (business to consumer) markets and are operated either by trained staff or customers themselves. Whereas B2B configurators are primarily used to support sales and lift production efficiency, B2C configurators are often employed as design tools that allow customers to \"co-design\" their own products. This is reflected in different advantages according to usage:\n\nFor B2B:\nFor B2C:\n\nConfigurators enable mass customization, which depends on a deep and efficient integration of customers into value creation. Salvador et al. identified three fundamental capabilities determining the ability of a company to mass-customize its offering, i.e. solution space development, robust process design and choice navigation (). Configurators serve as an important tool for choice navigation. Configurators have been widely used in e-Commerce. Examples can be found in different industries like accessories, apparel, automobile, food, industrial goods etc. The main challenge of choice navigation lies in the ability to support customers in identifying their own solutions while minimizing complexity and the burden of choice, i.e. improving the experience of customer needs elicitation and interaction in a configuration process. Many efforts have been put along this direction to enhance the efficiency of configurator design, such as adaptive configurators(;). The prediction is integrated into the configurator to improve the quality and speed of configuration process. Configurators may also be used to limit or eliminate mass customization if intended to do so. This is accomplished through limiting of allowable options in data models.\n\nAccording to (), configurators can be classified as rule based, model based and case based, depending on the reasoning techniques used.\n\n\n"}
{"id": "895368", "url": "https://en.wikipedia.org/wiki?curid=895368", "title": "Considered harmful", "text": "Considered harmful\n\nConsidered harmful is a part of a phrasal template used in the titles of at least 65 critical essays in computer science and related disciplines.\nIts use in this context originated in 1968 with Edsger Dijkstra's letter \"Go To Statement Considered Harmful\".\n\n\"Considered harmful\" was popularized by Edsger Dijkstra's letter \"Go To Statement Considered Harmful\",\npublished in the March 1968 \"Communications of the ACM\" (CACM), in which he criticized the excessive use of the GOTO statement in programming languages of the day and advocated structured programming instead. The original title of the letter, as submitted to CACM, was \"A Case Against the Goto Statement\", but CACM editor Niklaus Wirth changed the title to \"Go To Statement Considered Harmful\". Regarding this new title, Donald Knuth quipped that \"Dr. Goto cheerfully complained that he was always being eliminated.\"\n\nFrank Rubin published a criticism of Dijkstra's letter in the March 1987 CACM where it appeared under the title \"'GOTO Considered Harmful' Considered Harmful\". The May 1987 CACM printed further replies, both for and against, under the title \"'\"GOTO Considered Harmful\" Considered Harmful' Considered Harmful?\". Dijkstra's own response to this controversy was titled \"On a Somewhat Disappointing Correspondence\".\n\n\"Considered harmful\" was already a journalistic cliché used in headlines, well before the Dijkstra article, as in, for example, the headline over a letter published in 1949 in \"The New York Times\": \"Rent Control Controversy / Enacting Now of Hasty Legislation Considered Harmful\".\n\n\n"}
{"id": "7876158", "url": "https://en.wikipedia.org/wiki?curid=7876158", "title": "Degradation (telecommunications)", "text": "Degradation (telecommunications)\n\nIn telecommunication, degradation is the loss of quality of an electronic signal, which may be categorized as either \"\"graceful\" or \"catastrophic\"\", and has the following meanings: \n\n\nThere are several forms and causes of degradation in electric signals, both in the time domain and in the physical domain, including runt pulse, voltage spike, jitter, wander, swim, drift, glitch, ringing, crosstalk, antenna effect (not the same antenna effect as in IC manufacturing), and phase noise.\n\nDegradation usually refers to reduction in quality of an analog or digital signal. When a signal is being transmitted or received, it undergoes changes which are undesirable. These changes are called degradation. Degradation is usually caused by:\n\n\nA signal has two important factors: frequency and wavelength. If weather is fine and temperature is normal, the signal can be transmitted within given frequency and wavelength limits. The signal travels with velocity \"c\" ≤ 3*10 m/s, which is equal to the speed of light. For frequency \"f\" Hz and wavelength \"λ\" m, we have\nAs such, when weather conditions deteriorate, frequency \"f\" has to be increased. This causes the wavelength \"λ\" to decrease, which means that the signal then travels lesser distance.\n"}
{"id": "28131812", "url": "https://en.wikipedia.org/wiki?curid=28131812", "title": "E. Lilian Todd", "text": "E. Lilian Todd\n\nEmma Lilian Todd (1865–1937), originally from Washington, D.C. and later New York City, was a self-taught inventor who grew up with a love for mechanical devices. \"The New York Times\" issue of November 28, 1909, identifies her as the first woman in the world to design airplanes, which she started in 1906 or earlier. In 1910, her latest design flew, test-piloted by Didier Masson.\n\nTodd was born Washington, D.C. in 1865. The 1870 U.S. census lists her as \"Lily,\" living with her mother Mary Todd and her sister Cora in the U.S. capital. Her father is not mentioned in the census, however. In the November 1909 issue of \"Woman's Home Companion\", an autobiographical article mentions her grandfather (probably on her mother's side), from whom she inherited her mechanical and inventive talent.\n\nTodd received her education in Washington, D.C. and taught herself typewriting to earn a living. Her first job was at the Patent Office, but left two years later to work in the office of the governor of Pennsylvania (she claims in her article that she became the first woman to receive an appointment in the executive department of that state). Then she went back to New York to continue her work with patents, began to study law, and became a member of the first Woman's Law Class of New York University (circa 1890). In 1896, she was issued a patent for a typewriter copy-holder (number 553292) which she shared with George W. Parker. Todd later worked as a secretary to the director-general of the Women's National War Relief Association during the Spanish–American War.\n\nAfter about 1903, Todd turned her attention to \"mechanical and aeronautic toys.\" She was further inspired after seeing airships in London and at the 1904 Louisiana Purchase Exposition in St. Louis, as well as a sketch of an airplane in a 1906 Parisian newspaper. Later that year, Todd attracted national attention when she exhibited her first design at Madison Square Garden in an aero show. Philanthropist Olivia Sage, the widow of financier and politician Russell Sage, was among those interested in Todd's work. Mrs. Sage became Todd's patron and gave her $7,000 to design and build her aircraft. Todd's first full-sized biplane began construction as early as the fall of 1908 by the Wittemann Brothers of Staten Island.\n\nThe framework was constructed out of straight-grained spruce, the upper coverings of the wings were muslin, the lower covering was seven-ounce army duck. Piano wire held the wings together. The airplane had two seats, and was in length, powered by modified Rinek motor.\n\nRealizing the importance of aviation, Todd started the first Junior Aero Club in 1908 to foster the education of future aviators. The club met at Todd's residence in New York, where her living room had become her workshop and was decorated by aircraft models of her own design and other mechanical toys. Todd was also credited with inventing and patenting a cabinet with a folding table, a cannon that could be triggered by solar power, a sundial, and an aeolian harp device that could be attached to a tree.\n\nAs she mentions in the 1909 article, Todd wanted to pilot her own airplane and applied for permission at the Richmond Borough Commissioner of Public Works. She also considered applying for a permit to fly it anywhere in the United States. Her permit was denied, however. Nevertheless, on November 7, 1910, the aircraft made a powered hop of over the Garden City aviation field with Didier Masson at the controls, but proved unable to sustain flight.\n\nTodd's career in airplane design ended abruptly after she was hired by Mrs. Sage in January 1911, despite Mrs. Sage's interest in aviation and the financing of Todd's biplane.\n\nAfter the death of Mrs. Sage, Todd moved to Pasadena, California, during the first half of the 1920s, as noted in the Voter Registrations of 1924 and onward. She moved to Corona Del Mar, California, in 1936. Todd died on September 26, 1937, at Huntington Memorial Hospital in Pasadena. Her body was cremated and her remains were sent to New York, but her burial site remains unknown.\n\nIn 2013, director and animator Kristina Yee created a short film entitled \"Miss Todd\" with a student team at the National Film and Television School. \"'Miss Todd' is a short, stop-motion, musical animation about the first woman in the world to build and design an airplane.\" The film is inspired by E. Lillian Todd. The film won the Foreign Film Award Gold Medal at the 2013 Student Academy Awards. Additionally, the team put out a book entitled \"Miss Todd and Her Wonderful Flying Machine,\" published by Compendium, Inc.\n\nIn 2015, the music artist Elizaveta released the video \"Icarus,\" which entirely features animation from \"Miss Todd.\"\n"}
{"id": "27248059", "url": "https://en.wikipedia.org/wiki?curid=27248059", "title": "ERacks", "text": "ERacks\n\neRacks Open Source Systems was founded in 1999. The company provides computer systems based on open source software, including various distributions of Linux, *BSD and OpenSolaris, and manufactures rack-mounted servers (including NAS systems, firewalls, mail and web servers), desktops, laptops and netbooks.\n\neRacks also manufactures quiet systems for recording studios.\n"}
{"id": "44391001", "url": "https://en.wikipedia.org/wiki?curid=44391001", "title": "Exploding wire method", "text": "Exploding wire method\n\nExploding Wire Method (also known as EWM) is a high energy density process by which a rising current is applied to a thin electrically conductive wire. The heat vaporizes the wire, and an electric arc over that vapor creates a shockwave and explosion. Exploding Wire Method is best known to be used as a detonator in nuclear munitions, high intensity light source, and production method for metal nanoparticles.\n\nExploding Wire Method has a surprisingly long history for a process only recently appropriated. Progress on the comprehension of the mechanism was intermittent, and even at present day there are many aspects that remain not fully understood.\n\nOne of the first documented cases of using electricity to melt a metal occurred in the late 1700s and is credited to Martin van Marum who melted 70 feet of metal wire with 64 Leyden Jars as a capacitor. Van Marum's generator was built in 1784, and is now located in the Teylers Museum in the Netherlands. Years later, Benjamin Franklin vaporized thin gold leaf to burn images onto paper. While neither Marum nor Franklin actually incited the exploding wire phenomenon, they were both important steps towards its discovery.\n\nEdward Nairne was the first to note the existence of the exploding wire method in 1774 with silver and copper wire. Subsequently, Michael Faraday used EWM to deposit thin gold films through the solidification of vaporized metal on adjacent surfaces. Then, vapor deposits of metal gas as a result of EWM were studied by August Toepler during the 1800s. Spectrography investigation of the process, led by J.A. Anderson, became widespread in the 1900s. The spectrography experiments enabled a better understanding and subsequently the first glimpses of practical application. The mid 20th century saw experiments with EWM as a light source and for the production of nanoparticles in aluminum, uranium and plutonium wires. Congruently, Luis Álvarez and Lawrence H. Johnston of the Manhattan Project found use for EWM in the development of nuclear detonators.\n\nCurrent day research focuses on utilizing EWM to produce nanoparticles as well as better understanding specifics of the mechanism such as the effects of the system environment on the process.\n\nThe basic components needed for the exploding wire method are a thin conductive wire and a capacitor. The wire is typically gold, aluminum, iron or platinum, and is usually less than 0.5mm in diameter. The capacitor has an energy consumption of about 25 kWh/kg and discharges a pulse of charge density 10 - 10 A/mm, leading to temperatures up to 100,000K. The phenomenon occurs over a time period of only 10-10 seconds.\n\nThe process is as follows:\n\nEWM research has suggested possible applications in the excitation of optical masers, high intensity light sources for communications, spacecraft propulsion, joining difficult materials such as quartz, and generation of high power radio-frequency pulses. The most promising applications of EWM are as a detonator, light source, and for the production of nanoparticles.\n\nEWM has found its most common use as a detonator, named the exploding-bridgewire detonator, for nuclear bombs. Bridgewire detonators are advantageous over chemical fuses as the explosion is consistent and occurs only a few microseconds after the current is applied, with variation of only a few tens of nanoseconds from detonator to detonator.\n\nEWM is an effective mechanism by which to get a short duration high intensity light source. The peak intensity for copper wire, for example, is 9.6*10 candle power/cm. J.A. Anderson wrote in his initial spectrography studies that the light was comparable to a black body at 20,000K. The advantage of a flash produced in this way is that it is easily reproducible with little variation in intensity. The linear nature of the wire allows for specifically shaped and angled light flashes and different types of wires can be used to produce different colors of light. The light source can be used in interferometry, flash photolysis, quantitative spectroscopy, and high-speed photography.\n\nNanoparticles are created by EWM when the ambient gas of the system cools the recently produced vaporous metal. EWM can be used to cheaply and efficiently produce nanoparticles at a rate of 50-300 grams per hour and at a purity of above 99%. The process requires a relatively low energy consumption as little energy is lost in an electric to thermal energy conversion. Environmental effects are minimal due to the process taking place in a closed system. The Particles can be as small as 10 nm but are most commonly below 100 nm in diameter. Physical attributes of the nanopowder can be altered depending on the parameters of the explosion. For example, as the voltage of the capacitor is raised, the particle diameter decreases. Also, the pressure of the gas environment can change the dispersiveness of the nanoparticles. Through such manipulations the functionality of the nanopowder may be altered.\n\nWhen EWM is performed in a standard atmosphere containing oxygen, metal oxides are formed. Pure metal nanoparticles can also be produced with EWM in an inert environment, usually argon gas or distilled water. Pure metal nanopowders must be kept in their inert environment because they ignite when exposed to oxygen in air. Often, the metal vapor is contained by operating the mechanism within a steel box or similar container.\n\nNanoparticles are a relatively new material used in medicine, manufacturing, environmental cleanup and circuitry. Metal oxide and pure metal nanoparticles are used in Catalysis, sensors, oxygen antioxident, self repairing metal, ceramics, UV ray protection, odor proofing, improved batteries, printable circuits, optoelectronic materials, and Environmental remediation. The demand for metal nanoparticles, and therefore production methods, has increased as interest in nanotechnology continues to rise. Despite its overwhelming simplicity and efficiency, it is difficult to modify the experimental apparatus to be used on an industrial scale. As such, EWM has not seen widespread utilization in material production industry due to issues in manufacturing quantity.\n\n"}
{"id": "2490859", "url": "https://en.wikipedia.org/wiki?curid=2490859", "title": "Flight management system", "text": "Flight management system\n\nA flight management system (FMS) is a fundamental component of a modern airliner's avionics. An FMS is a specialized computer system that automates a wide variety of in-flight tasks, reducing the workload on the flight crew to the point that modern civilian aircraft no longer carry flight engineers or navigators. A primary function is in-flight management of the flight plan. Using various sensors (such as GPS and INS often backed up by radio navigation) to determine the aircraft's position, the FMS can guide the aircraft along the flight plan. From the cockpit, the FMS is normally controlled through a Control Display Unit (CDU) which incorporates a small screen and keyboard or touchscreen. The FMS sends the flight plan for display to the Electronic Flight Instrument System (EFIS), Navigation Display (ND), or Multifunction Display (MFD). The FMS can be summarised as being a dual system consisting of the Flight Management Computer (FMC), CDU and a cross talk bus. \n\nThe modern FMS was introduced on the Boeing 767, though earlier navigation computers did exist. Now, systems similar to FMS exist on aircraft as small as the Cessna 182. In its evolution an FMS has had many different sizes, capabilities and controls. However certain characteristics are common to all FMS.\n\nAll FMS contain a navigation database. The navigation database contains the elements from which the flight plan is constructed. These are defined via the ARINC 424 standard. The navigation database (NDB) is normally updated every 28 days, in order to ensure that its contents are current. Each FMS contains only a subset of the ARINC / AIRAC data, relevant to the capabilities of the FMS.\n\nThe NDB contains all of the information required for building a flight plan, consisting of:\n\nWaypoints can also be defined by the pilot(s) along the route or by reference to other waypoints with entry of a place in the form of a waypoint (e.g. a VOR, NDB, ILS, airport or waypoint/intersection)\n\nThe flight plan is generally determined on the ground, before departure either by the pilot for smaller aircraft or a professional dispatcher for airliners. It is entered into the FMS either by typing it in, selecting it from a saved library of common routes (Company Routes) or via an ACARS datalink with the airline dispatch center.\n\nDuring preflight, other information relevant to managing the flight plan is entered. This can include performance information such as gross weight, fuel weight and center of gravity. It will include altitudes including the initial cruise altitude. For aircraft that do not have a GPS, the initial position is also required.\n\nThe pilot uses the FMS to modify the flight plan in flight for a variety of reasons. Significant engineering design minimizes the keystrokes in order to minimize pilot workload in flight and eliminate any confusing information (Hazardously Misleading Information).\nThe FMS also sends the flight plan information for display on the Navigation Display (ND) of the flight deck instruments Electronic Flight Instrument System (EFIS). The flight plan generally appears as a magenta line, with other airports, radio aids and waypoints displayed.\n\nSpecial flight plans, often for tactical requirements including search patterns, rendezvous, in-flight refueling tanker orbits, calculated air release points (CARP) for accurate parachute jumps are just a few of the special flight plans some FMS can calculate.\n\nOnce in flight, a principal task of the FMS is to determine the aircraft's position and the accuracy of that position. Simple FMS use a single sensor, generally GPS in order to determine position. But modern FMS use as many sensors as they can, such as VORs, in order to determine and validate their exact position. Some FMS use a Kalman filter to integrate the positions from the various sensors into a single position. Common sensors include:\nThe FMS constantly crosschecks the various sensors and determines a single aircraft position and accuracy. The accuracy is described as the Actual Navigation Performance (ANP) a circle that the aircraft can be anywhere within measured as the diameter in nautical miles.\nModern airspace has a set required navigation performance (RNP). The aircraft must have its ANP less than its RNP in order to operate in certain high-level airspace.\n\nGiven the flight plan and the aircraft's position, the FMS calculates the course to follow. The pilot can follow this course manually (much like following a VOR radial), or the autopilot can be set to follow the course.\n\nThe FMS mode is normally called LNAV or Lateral Navigation for the lateral flight plan and VNAV or vertical navigation for the vertical flight plan. VNAV provides speed and pitch or altitude targets and LNAV provides roll steering command to the autopilot.\n\nSophisticated aircraft, generally airliners such as the Airbus A320 or Boeing 737 and other turbofan powered aircraft, have full performance Vertical Navigation (VNAV). The purpose of VNAV is to predict and optimize the vertical path. Guidance includes control of the pitch axis and control of the throttle.\n\nIn order to have the information necessary to accomplish this, the FMS must have a detailed flight and engine model. With this information, the function can build a predicted vertical path along the lateral flight plan. This detailed flight model is generally only available from the aircraft manufacturer.\n\nDuring pre-flight, the FMS builds the vertical profile. It uses the initial aircraft empty weight, fuel weight, centre of gravity and initial cruise altitude, plus the lateral flight plan.\nA vertical path starts with a climb to cruise altitude. Some SID waypoints have vertical constraints such as \"At or ABOVE 8,000\". The climb may use a reduced thrust or \"FLEX\" climb to save stress on the engines. Each must be considered in the predictions of the vertical profile.\n\nImplementation of an accurate VNAV is difficult and expensive, but it pays off in fuel savings primarily in cruise and descent. In cruise, where most of the fuel is burned, there are multiple methods for fuel savings.\n\nAs an aircraft burns fuel it gets lighter and can cruise higher where it is generally more efficient. Step climbs or cruise climbs facilitate this. VNAV can determine where the step or cruise climbs (where the aircraft drifts up) should occur to minimize fuel consumption.\n\nPerformance optimization allows the FMS to determine the best or most economical speed to fly in level flight. This is often called the ECON speed. This is based on the cost index, which is entered to give a weighting between speed and fuel efficiency. Generally a cost index of 999 gives ECON speeds as fast as possible without consideration of fuel and a cost index of Zero gives maximum efficiency. ECON mode is the VNAV speed used by most airliners in cruise.\n\nRTA or required time of arrival allows the VNAV system to target arrival at a particular waypoint at a defined time. This is often useful for airport arrival slot scheduling. In this case, VNAV regulates the cruise speed or cost index to ensure the RTA is met.\n\nThe first thing the VNAV calculates for the descent is the top of descent point (TOD). This is the point where an efficient and comfortable descent begins. Normally this will involve an idle descent, but for some aircraft an idle descent is too steep and uncomfortable. The FMS calculates the TOD by “flying” the descent backwards from touchdown through the approach and up to cruise. It does this using the flight plan, the aircraft flight model and descent winds. For airline FMS, this is a very sophisticated and accurate prediction, for simple FMS (on smaller aircraft) it can be determined by a “rule of thumb” such as a 3 degree descent path.\n\nFrom the TOD, the VNAV determines a four-dimensional predicted path. As the VNAV commands the throttles to idle, the aircraft begins its descent along the VNAV path. If either the predicted path is incorrect or the downpath winds different from the predictions, then the aircraft will not perfectly follow the path. The aircraft varies the pitch in order to maintain the path. Since the throttles are at idle this will modulate the speed. Normally the FMS allows the speed to vary within a small band. After this, either the throttles advance (if the aircraft is below path) or the FMS requests speed brakes with a message such as “ADD DRAG” (if the aircraft is above path).\n\nAn ideal idle descent, also known as a “green descent” uses the minimum fuel, minimizes pollution (both at high altitude and local to the airport) and minimizes local noise. While most modern FMS of large airliners are capable of idle descents, most air traffic control systems cannot handle multiple aircraft each using its own optimum descent path to the airport, at this time. Thus the use of idle descents is minimized by Air Traffic Control.\n\n\n"}
{"id": "41691180", "url": "https://en.wikipedia.org/wiki?curid=41691180", "title": "Floating reedbeds", "text": "Floating reedbeds\n\nFloating reedbeds are artificial or natural systems consisting of buoyancy and reeds. Plants including rice and wheat can be cultivated on floating reedbeds. The primary purpose of artificial floating reedbeds is to improve water quality through biofiltration, preventing algal blooms through denitrification and plant nutrient uptake, with a secondary benefit of habitat provision.\n\nModern floating reedbeds are increasingly being used by local government and land managers to improve water quality at source, reducing pollutants in surface water bodies and providing biodiversity habitat. Examples include Gold Coast City Council in Australia. Artificial floating reedbeds are commonly anchored to the shoreline or bottom of a water body, to ensure the system does not float away in a storm event or create a hazard.\n\nBuoyancy in artificial floating reedbeds is commonly provided by polyethylene or polyurethane flotation foam, or polyethylene or PVC plastic containing air voids. Growth media includes coconut fibre, mats made of polyester or recycled PET bottles, synthetic geotechnical mat, open cell polyurethane foam, jute, soil and sand. Additional elements may be added such as activated carbon, zeolites, and materials that accumulate pollutants. \n"}
{"id": "5060227", "url": "https://en.wikipedia.org/wiki?curid=5060227", "title": "Fonio husking machine", "text": "Fonio husking machine\n\nA Fonio husking machine was invented by Sanoussi Diakité, a Senegalese mechanical engineer. Diakité was awarded the Rolex Award in 1996 for the invention. Fonio is a staple crop in western Africa. Because the fonio grains are so small, it is difficult to remove the brittle outer shell. \"For hundreds of years, African women have carried out the painstaking task of preparing fonio by pounding and threshing a grain and sand mixture with a pestle and mortar. After one hour of this tedious work, only two kilograms of fonio are available for consumption and fifteen liters of water are needed to remove the sand.\" The whole process has been reduced from a 1-hour job to a 6-minute job.\n\nDiakité's solution was a 50 kg device that gently abrades the surface of the seed before passing through a rotating mechanism, which removes the husks.\n\nThis device is an example of appropriate technology, because of its ability to solve a significant problem in developing countries and the ease with which it can be manufactured. In 2008, his work was one of the laureates of The Tech Awards in the Health Award category.\n\nDiakité received the Innovation Prize for Africa in 2013.\n\n"}
{"id": "420067", "url": "https://en.wikipedia.org/wiki?curid=420067", "title": "Foot-pound (energy)", "text": "Foot-pound (energy)\n\nThe foot pound-force (symbol: ft⋅lbf or ft⋅lb) is a unit of work or energy in the Engineering and Gravitational Systems in United States customary and imperial units of measure. It is the energy transferred upon applying a force of one pound-force (lbf) through a linear displacement of one foot. The corresponding SI unit is the joule.\n\nThe foot-pound is often used to specify the muzzle energy of a bullet in small arms ballistics, particularly in the United States.\n\"Foot-pound\" is also used as a unit of torque (see \"pound-foot (torque)\"). In the United States this unit is often used to specify, for example, the tightness of a bolt or the output of an engine. Although they are dimensionally equivalent, energy (a scalar) and torque (a vector) are distinct physical quantities. Both energy and torque can be expressed as a product of a force vector with a displacement vector (hence pounds and feet); energy is the scalar product of the two, and torque is the vector product.\n\n1 foot pound-force is equivalent to:\n\n1 foot pound-force per second is equivalent to:\n\nRelated conversions:\n\n"}
{"id": "30965266", "url": "https://en.wikipedia.org/wiki?curid=30965266", "title": "Fresh'n", "text": "Fresh'n\n\nFresh'n was a United States toilet paper substitute, a moistened flushable biodegradable toilet towel. It was marketed with the slogan, \"I don't use toilet paper.\"\n"}
{"id": "44907259", "url": "https://en.wikipedia.org/wiki?curid=44907259", "title": "GPS2SMS", "text": "GPS2SMS\n\nThe GPS2SMS term describes the transfer of co-ordinates of an object or person by means of SMS text messaging. For this, the following applications are suitable:\n\nSMS messages can be sent from any device that has a GSM modem or phone and a GPS receiver. There are commercial handheld devices in the market, built-in systems for vehicles, software for computers or mobile devices, as well as some homebuilding projects, such as a small device that connects a mobile phone with a GPS.\n\nFor the detection of theft of a vehicle, a device may be used which sends an SMS alert to the monitor coordinates of the object, as soon as it leaves a particular area.\n\n"}
{"id": "41135509", "url": "https://en.wikipedia.org/wiki?curid=41135509", "title": "Gamma loop", "text": "Gamma loop\n\nA Gamma loop is a loop about alloys of steel. When adding additives to iron, a stable zone of austenite may increase or decrease. Some additives decrease the Austenite stable zone. In this case, the phase boundary becomes a loop. Adding Gamma loop additives prevents the steel from suffering phase transition to other solid states.\n"}
{"id": "6202324", "url": "https://en.wikipedia.org/wiki?curid=6202324", "title": "Haskins Laboratories", "text": "Haskins Laboratories\n\nHaskins Laboratories is an independent 501(c) non-profit corporation, founded in 1935 and located in New Haven, Connecticut, since 1970. It is a multidisciplinary and international community of researchers which conducts basic research on spoken and written language. A guiding perspective of their research is to view speech and language as biological processes. The Laboratories has a long history of technological and theoretical innovation, from creating the rules for speech synthesis and the first working prototype of a reading machine for the blind to developing the landmark concept of phonemic awareness as the critical preparation for learning to read.\n\nHaskins Laboratories is equipped, in-house, with a comprehensive suite of tools and capabilities to advance its mission of research into language and literacy. These include (as of 2014):\n\nScores of researchers have contributed to scientific breakthroughs at Haskins Laboratories since its founding. All of them are indebted to the pioneering work and leadership of Caryl Parker Haskins, Franklin S. Cooper, Alvin Liberman, Seymour Hutner and Luigi Provasoli. This history focuses on the research program of the main division of Haskins Laboratories that, since the 1940s, has been most well known for its work in the areas of speech, language and reading.\n\nCaryl Haskins and Franklin S. Cooper established Haskins Laboratories in 1935. It was originally affiliated with Harvard University, MIT, and Union College in Schenectady, NY. Caryl Haskins conducted research in microbiology, radiation physics, and other fields in Cambridge, MA and Schenectady. In 1939 the Laboratories moved its center to New York City. Seymour Hutner joined the staff to set up a research program in microbiology, genetics, and nutrition. The descendant of this program is now part of Pace University in New York.\n\nThe U. S. Office of Scientific Research and Development, under Vannevar Bush asked Haskins Laboratories to evaluate and develop technologies for assisting blinded World War II veterans. Experimental psychologist Alvin Liberman joined the Laboratories to assist in developing a \"sound alphabet\" to represent the letters in a text for use in a reading machine for the blind. Luigi Provasoli joined the Laboratories to set up a research program in marine biology. The program in marine biology moved to Yale University in 1970 and disbanded with Provasoli's retirement in 1978.\n\nFranklin S. Cooper invented the pattern playback, a machine that converts pictures of the acoustic patterns of speech back into sound. With this device, Alvin Liberman, Cooper, and Pierre Delattre (and later joined by Katherine Safford Harris, Leigh Lisker, Arthur Abramson, and others), discovered the acoustic cues for the perception of phonetic segments (consonants and vowels). Liberman and colleagues proposed a motor theory of speech perception to resolve the acoustic complexity: they hypothesized that we perceive speech by tapping into a biological specialization, a speech module, that contains knowledge of the acoustic consequences of articulation. Liberman, aided by Frances Ingemann and others, organized the results of the work on speech cues into a groundbreaking set of rules for speech synthesis by the Pattern Playback .\n\nFranklin S. Cooper and Katherine Safford Harris, working with Peter MacNeilage, were the first researchers in the U.S. to use electromyographic techniques, pioneered at the University of Tokyo, to study the neuromuscular organization of speech. Leigh Lisker and Arthur Abramson looked for simplification at the level of articulatory action in the voicing of certain contrasting consonants. They showed that many acoustic properties of voicing contrasts arise from variations in voice onset time, the relative phasing of the onset of vocal cord vibration and the end of a consonant. Their work has been widely replicated and elaborated, here and abroad, over the following decades. Donald Shankweiler and Michael Studdert-Kennedy used a dichotic listening technique (presenting different nonsense syllables simultaneously to opposite ears) to demonstrate the dissociation of phonetic (speech) and auditory (nonspeech) perception by finding that phonetic structure devoid of meaning is an integral part of language, typically processed in the left cerebral hemisphere. Liberman, Cooper, Shankweiler, and Studdert-Kennedy summarized and interpreted fifteen years of research in \"Perception of the Speech Code,\" still among the most cited papers in the speech literature. It set the agenda for many years of research at Haskins and elsewhere by describing speech as a code in which speakers overlap (or coarticulate) segments to form syllables. Researchers at Haskins connected their first computer to a speech synthesizer designed by the Laboratories' engineers. Ignatius Mattingly, with British collaborators, John N. Holmes and J.N. Shearme , adapted the Pattern playback rules to write the first computer program for synthesizing continuous speech from a phonetically spelled input. A further step toward a reading machine for the blind combined Mattingly's program with an automatic look-up procedure for converting alphabetic text into strings of phonetic symbols.\n\nIn 1970 Haskins Laboratories moved to New Haven, Connecticut, and entered into affiliation agreements with Yale University and the University of Connecticut. Isabelle Liberman, Donald Shankweiler, and Alvin Liberman teamed up with Ignatius Mattingly to study the relationship between speech perception and reading, a topic implicit in the Laboratories' research program since its inception. They developed the concept of phonemic awareness, the knowledge that would-be readers must be aware of the phonemic structure of their language in order to be able to read. Leonard Katz related the work to contemporary cognitive theory and provided expertise in experimental design and data analysis. Under the broad rubric of the \"alphabetic principle,\" this is the core of the Laboratories' present program of reading pedagogy. Patrick Nye joined the Laboratories to lead a team working on the reading machine for the blind. The project culminated when the addition of an optical character recognizer allowed investigators to assemble the first automatic text-to-speech reading machine. By the end of the decade this technology had advanced to the point where commercial concerns assumed the task of designing and manufacturing reading machines for the blind .\n\nIn 1973 Franklin S. Cooper was selected to form a panel of six experts charged with investigating the famous 18-minute gap in the White House office tapes of President Richard Nixon related to the Watergate scandal \n\nBuilding on earlier work, Philip Rubin developed the sinewave synthesis program, which was then used by Robert Remez, Rubin, and colleagues to show that listeners can perceive continuous speech without traditional speech cues from a pattern of sinewaves that track the changing resonances of the vocal tract. This paved the way for a view of speech as a dynamic pattern of trajectories through articulatory-acoustic space. Philip Rubin and colleagues developed Paul Mermelstein's anatomically simplified vocal tract model , originally worked on at Bell Laboratories, into the first articulatory synthesizer that can be controlled in a physically meaningful way and used for interactive experiments.\n\nStudies of different writing systems supported the controversial hypothesis that all reading necessarily activates the phonological form of a word before, or at the same time, as its meaning. Work included experiments by Georgije Lukatela , Michael Turvey, Leonard Katz, Ram Frost , Laurie Feldman , and Shlomo Bentin, in a variety of languages. Cross-language work on reading, including investigations of the brain process involved, remains a large part of the Laboratories' program today.\n\nVarious researchers developed compatible theoretical accounts of speech production, speech perception and phonological knowledge. Carol Fowler proposed a direct realism theory of speech perception: listeners perceive gestures not by means of a specialized decoder, as in the motor theory, but because information in the acoustic signal specifies the gestures that form it. J. A. Scott Kelso and colleagues demonstrated functional synergies in speech gestures experimentally. Elliot Saltzman developed a dynamical systems theory of synergetic action and implemented the theory as a working model of speech production. Linguists Catherine Browman and Louis Goldstein developed the theory of articulatory phonology , in which gestures are the basic units of both phonetic action and phonological knowledge. Articulatory phonology, the task dynamic model, and the articulatory synthesis model are combined into a gestural computational model of speech production .\n\nKatherine Safford Harris, Frederica Bell-Berti and colleagues studied the phasing and cohesion of articulatory speech gestures. Kenneth Pugh was among the first scientists to use functional magnetic resonance imaging (fMRI) to reveal brain activity associated with reading and reading disabilities. Pugh, Donald Shankweiler, Weija Ni , Einar Mencl , and colleagues developed novel applications of neuroimaging to measure brain activity associated with understanding sentences. Philip Rubin, Louis Goldstein and Mark Tiede designed a radical revision of the articulatory synthesis model, known as CASY , the configurable articulatory synthesizer. This 3-dimensional model of the vocal tract permits researchers to replicate MRI images of actual speakers. Douglas Whalen, Goldstein, Rubin and colleagues extended this work to study the relation between speech production and perception. Donald Shankweiler, Susan Brady, Anne Fowler , and others explored whether weak memory and perception in poor readers are tied specifically to phonological deficits. Evidence rejected broader cognitive deficits underlying reading difficulties and raised questions about impaired phonological representations in disabled readers.\n\nAnne Fowler and Susan Brady launched the Early Reading Success (ERS) program , part of the Haskins Literacy Initiative which promotes the science of teaching reading. The ERS program was a demonstration project examining the efficacy of professional development in reading instruction for teachers of children in kindergarten through second grade. The Mastering Reading Instruction program , which combines professional development with Haskins-trained mentors, was a continuation of ERS. David Ostry and colleagues explored the neurological underpinning of motor control using a robot arm to influence jaw movement. Douglas Whalen and Khalil Iskarous pioneered the pairing of ultrasound, used here to monitor articulators that cannot be seen, and Optotrak, an opto-electronic position-tracking device, used here to monitor visible articulators. Christine Shadle joined Haskins in 2004 to head up a project investigating the speech production goals for fricatives. Donald Shankweiler and David Braze developed an eye movement laboratory that combines eye tracking data with brain activity measures for investigating reading processes in normal and disabled readers. Laura Koenig and Jorge C. Lucero studied the development of laryngeal and aerodynamic control in children's speech. In March 2005 Haskins Laboratories moved to a new state-of-the-art facility on George Street in New Haven. In 2008 Ken Pugh of Yale University was named President and Director of Research, succeeding Carol Fowler who remains at Haskins as a Senior Advisor. In 2009 Haskins released its new Strategic Plan , which features new \"Birth-to-Five\" and Bilingualism initiatives.\n\nThe Haskins Training Institute was established in 2011 to provide direct educational opportunities in Haskins Laboratories' core areas of research (language, speech perception, speech production, literacy). The Training Institute serves to communicate this knowledge to the public through accessible seminars, small conferences, and intern and training positions.\n\nCapabilities in the eye movement labs is expanded to include 3 eye trackers, including one with the ability to capture synchronous gaze and EEG data, and another able to capture synchronous gaze and speech signals.\n\nIn December 2015, Haskins Laboratories convened a Global Literacy Summit. This was a three-day meeting of scientists and representatives from governmental and non-governmental organizations around the globe, who are working with programs in the developing world to support literacy and education in disadvantaged populations.\n\n\n"}
{"id": "3189819", "url": "https://en.wikipedia.org/wiki?curid=3189819", "title": "History of candle making", "text": "History of candle making\n\nCandle making was developed independently in many places throughout history. \nCandles were used by the early Greeks to honour the goddess Artemis' birth on the sixth day of every lunar month.\n\nCandles were made by the Romans beginning about 500 BC. These were true dipped candles and made from tallow. Evidence for candles made from whale fat in China dates back to the Qin Dynasty (221–206 BC). In India, wax from boiling cinnamon was used for temple candles.\n\nIn parts of Europe, the Middle-East and Africa, where lamp oil made from olives was readily available, candle making remained unknown until the early middle-ages. Candles were primarily made from tallow and beeswax in ancient times, but have been made from spermaceti, purified animal fats (stearin) and paraffin wax in recent centuries.\n\nThe early Greeks used candles to honour the goddess Artemis' birth on the sixth day of every lunar month.\n\nRomans began making true dipped candles from tallow, beginning around 500 BC. While oil lamps were the most widely used source of illumination in Roman Italy, candles were common and regularly given as gifts during Saturnalia.\n\nThe mausoleum of Qin Shi Huang (259–210 BC), contained candles made from whale fat. The word zhú was used as candle during the Warring States period (403–221 BC); some excavated bronzewares from that era feature a pricket thought to hold a candle.\n\nThe Han Dynasty (202 BC – 220 AD) \"Jizhupian\" dictionary of about 40 BC hints at candles being made of beeswax, while the \"Book of Jin\" (compiled in 648) covering the Jin Dynasty (265–420) makes a solid reference to the beeswax candle in regards to its use by the statesman Zhou Yi (d. 322). An excavated earthenware bowl from the 4th century AD, located at the Luoyang Museum, has a hollowed socket where traces of wax were found. Generally these Chinese candles were molded in paper tubes, using rolled rice paper for the wick, and wax from an indigenous insect that was combined with seeds.\n\nWax from boiling cinnamon was used for temple candles in India. Yak butter was used for candles in Tibet\n\nThere is a fish called the eulachon or \"candlefish\", a type of smelt which is found from Oregon to Alaska. During the 1st century AD, indigenous people from this region used oil from this fish for illumination. A simple candle could be made by putting the dried fish on a forked stick and then lighting it.\n\nAfter the collapse of the Roman empire, trading disruptions made olive oil, the most common fuel for oil lamps, unavailable throughout much of Europe. As a consequence, candles became more widely used. By contrast, in North Africa and the Middle East, candle-making remained relatively unknown due to the availability of olive oil.\n\nCandles were commonplace throughout Europe in the Middle Ages. Candle makers (known as chandlers) made candles from fats saved from the kitchen or sold their own candles from within their shops. The trade of the chandler is also recorded by the more picturesque name of \"smeremongere\", since they oversaw the manufacture of sauces, vinegar, soap and cheese. The popularity of candles is shown by their use in Candlemas and in Saint Lucy festivities.\n\nTallow, fat from cows or sheep, became the standard material used in candles in Europe. The unpleasant smell of tallow candles is due to the glycerine they contain. The smell of the manufacturing process was so unpleasant that it was banned by ordinance in several European cities. Beeswax was discovered to be an excellent substance for candle production without the unpleasant odour, but remained restricted in usage for the rich and for churches and royal events, due to their great expense.\n\nIn England and France, candle making had become a guild craft by the 13th century. The Tallow Chandlers Company of London was formed in about 1300 in London, and in 1456 was granted a coat of arms. The Wax Chandlers Company dating from about 1330, acquired its charter in 1484. By 1415, tallow candles were used in street lighting. The first candle mould comes from the 15th century in Paris.\n\nWith the growth of the whaling industry in the 18th century, spermaceti, an oil that comes from a cavity in the head of the sperm whale, became a widely used substance for candle making. The spermaceti was obtained by crystallizing the oil from the sperm whale and was the first candle substance to become available in mass quantities. Like beeswax, spermaceti wax did not create a repugnant odor when burned, and produced a significantly brighter light. It was also harder than either tallow or beeswax, so it would not soften or bend in the summer heat. The first \"standard candles\" were made from spermaceti wax.\n\nBy 1800, an even cheaper alternative was discovered. Colza oil, derived from Brassica campestris, and a similar oil derived from rapeseed, yielded candles that produce clear, smokeless flames. The French chemists Michel Eugène Chevreul (1786–1889) and Joseph-Louis Gay-Lussac (1778–1850) patented stearin in 1825. Like tallow, this was derived from animals, but had no glycerine content.\n\nThe manufacture of candles became an industrialised mass market in the mid 19th century. In 1834, Joseph Morgan, a pewterer from Manchester, England, patented a machine that revolutionised candle making. It allowed for continuous production of molded candles by using a cylinder with a moveable piston to eject candles as they solidified. This more efficient mechanized production produced about 1,500 candles per hour, (according to his patent \". . with three men and five boys [the machine] will manufacture two tons of candle in twelve hours\"). This allowed candles to become an easily affordable commodity for the masses.\nAt this time, candlemakers also began to fashion wicks out of tightly braided (rather than simply twisted) strands of cotton. This technique makes wicks curl over as they burn, maintaining the height of the wick and therefore the flame. Because much of the excess wick is incinerated, these are referred to as \"self-trimming\" or \"self-consuming\" wicks.\n\nIn 1848 James Young established the world’s first oil refinery at the Alfreton Ironworks in Riddings, Derbyshire. Two paraffin wax candles were made from the naturally occurring paraffin wax present in the oil and these candles illuminated a lecture at the Royal Institution by Lyon Playfair. In the mid-1850s, James Young succeeded in distilling paraffin wax from coal and oil shales at Bathgate in West Lothian and developed a commercially viable method of production. The Paraffin wax was processed by distilling residue left after crude petroleum was refined. \n\nParaffin could be used to make inexpensive candles of high quality. It was a bluish-white wax, burned cleanly, and left no unpleasant odor, unlike tallow candles. A drawback to the substance was that early coal- and petroleum-derived paraffin waxes had a very low melting point. The introduction of stearin, discovered by Michel Eugène Chevreul, solved this problem. Stearin is hard and durable, with a convenient melting range of 54–72.5 °C (129.2–162.5 °F). By the end of the 19th century, most candles being manufactured consisted of paraffin and stearic acid.\n\nBy the late 19th century, Price's Candles, based in London was the largest candle manufacturer in the world. The company traced its origins back to 1829, when William Wilson invested in 1,000 acres (4 km²) of coconut plantation in Sri Lanka. His aim was to make candles from coconut oil. Later he tried palm oil from palm trees. An accidental discovery swept all his ambitions aside when his son George Wilson, a talented chemist, distilled the first petroleum oil in 1854. George also pioneered the implementation of the technique of steam distillation, and was thus able to manufacture candles from a wide range of raw materials, including skin fat, bone fat, fish oil and industrial greases.\n\nIn America, Syracuse, New York developed into a global center for candle manufacturing from the mid-nineteenth century. Manufacturers included Will & Baumer, Mack Miller, Muench Kruezer, and Cathedral Candle Company.\n\nDespite advances in candle making, the candle industry declined rapidly upon the introduction of superior methods of lighting, including kerosene and lamps and the 1879 invention of the incandescent light bulb.\n\nFrom this point on, candles came to be marketed as more of a decorative item. Candles became available in a broad array of sizes, shapes and colors, and consumer interest in scented candles began to grow. During the 1990s, new types of candle waxes were being developed due to an unusually high demand for candles. Paraffin, a by-product of oil, was quickly replaced by new waxes and wax blends due to rising costs.\n\nCandle manufacturers looked at waxes such as soy, palm and flax-seed oil, often blending them with paraffin in hopes of getting the performance of paraffin with the price benefits of the other waxes. The creation of unique wax blends, now requiring different fragrance chemistries and loads, placed pressure for innovation on the candle wick manufacturing industry to meet performance needs with the often tougher to burn formulations. \n"}
{"id": "7719516", "url": "https://en.wikipedia.org/wiki?curid=7719516", "title": "Huntley Project", "text": "Huntley Project\n\nThe Huntley Project is an irrigation project in southern Montana that was established by the United States Bureau of Reclamation in 1907. The district includes the towns of Huntley, Worden, Ballantine, and Pompeys Pillar.\n\nSince the Huntley Project was established, the district's main cash crops have been sugar beets and alfalfa. Silage for the local cattle industry is also important.\n\nBison-hunting Plains Indians, especially the Cheyenne, Crow, and Sioux, frequented this region south of the Yellowstone River from the 17th century. William Clark passed through in July 1806 with members of the Corps of Exploration and inscribed his name on Pompey's Rock. The Yellowstone River provided a route into this sagebrush-covered country for white fur trappers, hunters, and settlers. The U.S. Army made war on the Indian tribes over several decades, and the famous Battle of Little Bighorn took place nearby in June 1876. \n\nThe district that now includes the Huntley Project was designated as part of the Crow Indian Reservation under a treaty ratified on May 7, 1868. This preserved the area from occupation by white homesteads and cattle ranches, but by 1880 the virtual extinction of the bison made the traditional Crow economy impossible to sustain. By 1895 Crow farmers successfully irrigated and farmed part of the reservation, which had been considered an arid wasteland. \n\nIn 1882 the city of Billings, linked to the Northern Pacific Railroad, was founded within a few miles of the Crow reservation. In 1904 the United States government obtained the northern part of the reservation by cession from the Crow Indians. Congress authorized the Reclamation Service of the U.S. Department of the Interior to survey the land for a possible irrigation project. The Huntley Project, the fifth federal project to convert arid western land to farmland, was authorized in April 1905, and construction began in October. Despite cost overruns and unforeseen problems, the Pryor Division of the project, including Worden and Ballantine, was completed and receiving water by 1907.\n\nThe project took its name from the town of Huntley, a station on the Northern Pacific Railroad. In 1907 the new Huntley Project townsites of Worden, Ballantine, and Pompeys Pillar were laid out at intervals of about six miles along the railroad. On May 21, 1907 President Theodore Roosevelt declared the Pryor Division to be open for settlement. Farm units were distributed by lottery, but the pace of settlement may have been slowed by \"lottery fanatics\" from nearby Billings who drew numbers out of idle curiosity, with no intention of settling on the land. Of the first 1,000 names drawn, only 76 applied for a farm unit. The population of Huntley Project farms rose to 2,107 in 1917, but fell again to a probable low of 1,015 in 1923.\n\nMuch of the irrigation infrastructure of the Huntley Project had been poorly designed and cheaply built, and replacements were soon needed. Growing demand for water required more irrigation canals and an auxiliary pumping station, and by 1913 problems with inadequate drainage had caused hundreds of acres of arid land to become waterlogged. Settlers found that alkali in the soil made it difficult to profitably farm the area, and many farms failed in the recession following World War I. Poor crop yields made it difficult for farmers to pay their portion of the district's costs, so Reclamation refused to fund needed improvements. Tensions between Reclamation and the district farmers peaked during the 1920s, until on January 2, 1927 the two sides settled their dispute with a new contract. \n\nLow water and dry weather in the 1930s led to the construction of the first retention dam, then a concrete weir in 1934. The Anita Dam and Reservoir project, about six miles southeast of Ballantine, was completed in 1937 by Civilian Conservation Corps workers.\n\n"}
{"id": "1322528", "url": "https://en.wikipedia.org/wiki?curid=1322528", "title": "Joint Electronics Type Designation System", "text": "Joint Electronics Type Designation System\n\nThe Joint Electronics Type Designation System (JETDS), which was previously known as the Joint Army-Navy Nomenclature System (AN System. JAN) and the Joint Communications-Electronics Nomenclature System, is a method developed by the U.S. War Department during World War II for assigning an unclassified designator to electronic equipment. In 1957, the JETDS was formalized in MIL-STD-196.\n\nComputer software and commercial unmodified electronics for which the manufacturer maintains design control are not covered.\n\nElectronic materiel, from a military point of view, generally includes those electronic devices employed in data processing, detection and tracking (underwater, sea, land-based, air and space), recognition and identification, communications, aids to navigation, weapons control and evaluation, flight control, and electronics countermeasures. Nomenclature is assigned to:\n\nIn the JETDS system, complete equipment sets or systems are designated with a sequence of letters and digits prefixed by AN/, then three letters, a hyphen, a number, and (occasionally) some optional letters (AN/AAA-nnn suffixed by (Vn){hardware/software version} or (T){training equipment} . The three letters tell where the equipment is used, what it does and its purpose. For example, the AN/PRC-77 is a Portable Radio used for two way Communications. The model numbers for any given type of equipment are assigned sequentially, thus higher numbers indicate more modern systems.\n\nThe three letter codes have the following meanings:\n\n\n\n\nFollowing the three-letter designation, after a dash, is a number, uniquely identifying the equipment. Different variants of the same equipment may be given an additional letter and other suffixes (for example, AN/SPY-1A, AN/SPY-1B, etc.), while entirely new equipment within the same category is given a new number (for example, AN/SPY-3).\n\nA suffix \"(V)\", parenthetical V, indicates variable components. A number may follow the parenthetical V to identify a specific configuration. Or the number will identify the precise quantity of equipment required for a specific configuration.\n\nA suffix of \"(P)\", parenthetical P, indicates a plug in module or component of a system which changes the function, frequency, or characteristics.\n\nA suffix of \"(C)\", parenthetical C, indicates NSA-controlled cryptographic/classified item.\n\nA suffix of \"-T\", dash T, indicates equipment designed to provide training in the operation of a specific set.\n\nFor example:\n\nAN/ABC-1(V)4 would be the 4th variable of the AN/ABC-1 equipment.\nOT-1957(V)1/ABC-1(V) would be one OT-1957 required for the particular configuration. \nAN/ABC-1(C) would be an NSA-controlled cryptographic/classified item.\nR00(P)/ABC-1 would be a plug in module for the AN/ABC-1 equipment. \nAN/ABC-1-T1 would be the first training set for the AN/ABC-1 equipment. \n\nSubsystems (\"groups\") are designated by a two letter code (without the AN/ prefix), followed by a number, followed by slash and one, two or three letters from the three letter codes for systems. For example, BA-1234/PRC would be a battery for portable radio sets. Some subsystems will have the designation for the system they belong to. For example, RT-859/APX-72 and C-6820/APX-72, the /APX-72 indicates both are part of the AN/APX-72 system.\n\nJETDS was adopted 16 February 1943 by the Joint Communications Board for all new Army and Navy airborne, radio, and radar equipment. Over time it was extended to cover the Marine Corps and the Navy's ship, submarine, amphibious, and ground electronic equipment. When the Air Force was established as a separate department, it continued the use of the system for electronic equipment. JETDS was adopted by the United States Coast Guard in 1950, Canada in 1951 and the NSA in 1959 (though the NSA continued to use its own TSEC telecommunications security nomenclature). In 1957 the U.S. Department of Defense approved a military standard for the nomenclature, MIL-STD-196. The system has been modified over time, with some types (e.g. carrier pigeon -B-) dropped and others (e.g. computers and cryptographic equipment) added. The latest version, MIL-STD-196F, was issued in 2013.\n\n\n\n"}
{"id": "25617311", "url": "https://en.wikipedia.org/wiki?curid=25617311", "title": "Kansas Building Science Institute", "text": "Kansas Building Science Institute\n\nThe Kansas Building Science Institute is a vocational school located in Manhattan, Kansas. The Institute conducts week-long Home Energy Rater Trainings (HERS) as well as Building Performance Index (BPI) and Weatherization (WX) Trainings, among others.\n\nThe Institute conducts trainings in a multi-purpose classroom and training center in Manhattan. The campus also includes a furnace lab, a mobile home for Weatherization Trainings, and an attached house to perform test ratings and inspections on. Other houses around Manhattan are also used for this purpose.\n\n"}
{"id": "2076640", "url": "https://en.wikipedia.org/wiki?curid=2076640", "title": "List of battery sizes", "text": "List of battery sizes\n\nThis article lists the sizes, shapes, and general characteristics of some common primary and secondary battery types in household and light industrial use.\n\nHistorically the term \"battery\" referred to a collection of electrochemical cells connected in series, however in modern times the term has come to refer to any collection of cells (or single cell) packaged in a container with external connections provided to power electrical devices, leading to the variety of standardized form factors available today.\n\nThe long history of disposable dry cells means that many different manufacturer-specific and national standards were used to designate sizes, long before international standards were agreed upon. Technical standards for battery sizes and types are published by standards organizations such as International Electrotechnical Commission (IEC) and American National Standards Institute (ANSI). Many popular sizes are still referred to by old standard or manufacturer designations, and some non-systematic designations have been included in current international standards due to wide use.\n\nThe complete nomenclature for a battery specifies size, chemistry, terminal arrangement, and special characteristics. The same physically interchangeable cell size or battery size may have widely different characteristics; physical interchangeability is not the sole factor in substituting a battery.\n\nThe current IEC standards for portable primary (non-rechargeable) batteries bear the 60086 number. The relevant US standards are the ANSI C18 series, which are developed by a committee of the US National Electrical Manufacturers Association (NEMA).\n\nBoth standards have several parts that cover general principles, physical specifications, and safety. Designations by IEC and ANSI standards do not entirely agree, though harmonization is in progress. Also, manufacturers have their systems for identifying cell types, so cross-reference tables are useful to identify equivalent types from different manufacturers.\nLead-acid automotive starting, lighting and ignition batteries have been standardized according to IEC standard 60095 and in North America by standards published by BCI.\n\nManufacturers may assign proprietary names and numbers to their batteries, disregarding common, colloquial, IEC, and ANSI naming conventions (see LR44 battery as an example). Often this is done to steer customers towards a specific brand, and away from competing or generic brands, by obfuscating the common name. For example, if a remote control needs a new battery and the battery compartment has the label, \"Replace with CX472 type battery,\" many customers will buy that specific brand, not realizing that this is simply a brand name for a common type of battery. For example, British standard \"U\" series batteries were often sold under manufacturer prefixes such as \"C\", \"SP\", \"HP\", etc.; Ever Ready sold \"U2\" (D) batteries as \"SP2\" (standard-duty zinc carbon) and \"HP2\" (heavy duty zinc chloride).\n\nOn the other hand, with obscure battery types the designation assigned by a specific brand will sometimes become the most common name for that battery type, as other manufacturers copy or modify the name so that customers recognize it.\n\nThe terminal voltage of a battery cell depends on the chemicals and materials used in its construction, and not on its physical size. For example, primary (non-rechargeable) alkaline batteries have a nominal voltage of 1.5 volts. Rechargeable NiCd (nickel cadmium) and NiMH (nickel metal hydride) typically output 1.25 volts per cell. Devices intended for use with primary batteries may not operate properly with these cells, given the reduction in voltage.\n\nDry Leclanche (carbon-zinc), alkaline and lithium batteries are the most common modern types. Mercury batteries had stable cell terminal voltages around 1.35 volts. From the late 1940s until the mid-1990s, mercury batteries were made in many consumer and industrial sizes. They are no longer available since careless disposal can release toxic mercury into the environment. They have been replaced in some applications by zinc-air batteries, which also produce 1.35 volts.\n\nThe full battery designation identifies not only the size, shape and terminal layout of the battery but also the chemistry (and therefore the voltage per cell) and the number of cells in the battery. For example, a CR123 battery is always Li-MnO ('lithium') chemistry, in addition to its unique size.\n\nThe following tables give the common battery chemistries for the current common sizes of batteries. See Battery Chemistries for a list of other electrochemical systems.\n\nCylindrical cells typically have a positive terminal nub at one end, and a flat negative terminal at the other. A cell with a nub on the positive terminal is called a button-top, and a cell without a positive nub is called a flat-top. Two different cells of the same nominal size, e.g. two 18650 cells, may have different diameter buttons if made by different manufacturers, and this can lead to incompatibility with devices. Flat-top cells cannot be used in series without modification or soldering into position, because the flat positive terminal of one cell cannot contact with the next cell's negative terminal. Rarely however, a manufacturer may include tiny bumps on the \"negative\" terminal, so flat-tops can be used in series.\n\nIt is important to check the battery contacts in a device before attempting to install cells, because some will not work with flat-tops or with button-tops whose buttons are the wrong diameter. Some devices have a small bump or spring where the positive terminal of the cell connects, and this allows the use of either button- or flat-top cells. Other devices have a flat area that can only be contacted by a button-top. To prevent damage if a cell is inserted backwards, some devices have a raised plastic ring around the positive contact. This stops the flat negative end of a cell from connecting accidentally, but also stops the positive end of a flat-top or of a button-top with too large button from connecting.\n\nThese are round batteries with height longer than their diameter. In zinc-carbon or alkaline types they produce around 1.5V per cell when fresh. Other types produce other voltages per package, as low as 1.2V for rechargeable nickel-cadmium, up 12V for the A23 alkaline battery, a stack of 8 cells in the same overall format. This package has a positive nub terminal at the cap of the cell, and the negative terminal at the bottom of the can; the side of the can is not used as a terminal. The polarity of the can side may even change, according to their chemistry and whether the can is sealed from the positive or negative end. \n\nAs well as other types, digital and film cameras often use specialized primary batteries to produce a compact product. Flashlights and portable electronic devices may also use these types.\n\nCoin-shaped cells are thin compared to their diameter. Polarity is usually stamped on the metal casing.\n\nThe IEC prefix \"CR\" denotes lithium manganese dioxide chemistry. Since Li-MnO cells produce 3 volts there are no widely available alternative chemistries for a lithium coin battery. The \"BR\" prefix indicates a round lithium/carbon monofluoride cell. See lithium battery for discussion of the different performance characteristics. One Li-MnO cell can replace two alkaline or silver-oxide cells.\n\nIEC designation numbers indicate the physical dimensions of the cylindrical cell. Cells less than one centimeter in height are assigned four-digit numbers, where the first two digits are the diameter in millimeters, while the last two digits are the height in tenths of millimeters. Taller cells are assigned five-digit numbers, where the first two digits are the diameter in millimeters, followed by the last three digits indicating the height in tenths of millimeters.\n\nAll these lithium cells are rated nominally 3 volts (on-load), with open circuit voltage about 3.6 volts. Manufacturers may have their own part numbers for IEC standard size cells. The capacity listed is for a constant resistance discharge down to 2.0 volts per cell.\n\nRound button cells have heights less than their diameter. The metal can is the positive terminal, and the cap is the negative terminal.\n\nButton cells are commonly used in electric watches, clocks, and timers. IEC batteries that meet the international IEC 60086-3 standard for watch batteries carry a \"W\" suffix. Other uses include calculators, laser pointers, toys, LED \"blinkies\", and novelties.\n\nIEC designation numbers indicate the physical dimensions of the cylindrical cell. Cells less than one centimeter in height are assigned 4-digit numbers, where the first 2 digits are the diameter in millimeters, while the last 2 digits are the height in tenths of millimeters. Taller cells are assigned 5-digit numbers, where the first 2 digits are the diameter in millimeters, followed by the last 3 digits indicating the height in tenths of millimeters.\n\nIn the IEC designations, cell types with an \"SR\" prefix use silver oxide chemistry and provide 1.55 volts, while the \"LR\" prefix batteries use alkaline chemistry and provide 1.5 volts. Common alternative manufacturer's prefixes for these two types are \"SG\" for silver oxide and \"AG\" for alkaline. Since there are no \"common\" names beyond the AG designation, many vendors use these four designations interchangeably for the same physical sized cell.\n\nThe functional differences are that silver oxide batteries typically have 50% greater capacity than alkaline chemistry, relatively slowly declining voltage during discharge compared to alkaline types of the same size, and superior leakage resistance. The ultimate energy capacity of a silver battery may be as much as twice that of an alkaline. Also, a silver cell with a flat discharge characteristic is preferable for devices that need a steady voltage, such as photographic light meters, and devices that will not operate below a certain voltage; for example, some digital calipers, which do not work below 1.38V.\n\nAlkaline batteries are usually cheaper than silver oxide equivalents. Inexpensive devices are sometimes supplied fitted with alkaline batteries, though they would benefit from the use of silver oxide batteries. Exhausted silver oxide cells are often recycled to recover their precious metal content, whereas depleted alkaline cells are discarded with household trash or recycled, depending on the local practices.\n\nMercury batteries were formerly commonly made in button sizes for watches, but due to careless disposal and the resulting mercury pollution hazard, they are no longer available. This is also a concern for users of vintage camera equipment, which typically used a mercury button battery in the exposure meter for its very steady voltage characteristic. Substitute non-mercury batteries have been produced to replace certain discontinued mercury batteries, typically by incorporating a miniature voltage regulator to simulate the flat voltage discharge characteristics of the original batteries.\n\nIn the following table, sizes are shown for the silver-oxide IEC number; types and capacity are identified as \"(L)\" for alkaline, \"(M)\" for mercury (no longer manufactured), and \"(S)\" for silver-oxide. Some sizes may be interchangeably used in battery holders. For example, the 189/389 cell is 3.1 mm high and was designated 1131, while the 190/390 size is 3.0 mm high and was designated 1130, but a battery holder will accept either size. \n\nMiniature zinc-air batteries are button cells that use oxygen in air as a reactant and have very high capacity for their size. Each cell needs around 1 cc of air per minute at a 10 mA discharge rate. These cells are commonly used in hearing aids. A sealing tab keeps air out of the cell in storage; a few weeks after breaking the seal the electrolyte will dry out and the battery becomes unusable, regardless of use. Nominal voltage on discharge is 1.2 V.\n\nLithium-ion rechargeable batteries are generally not interchangeable with primary types using different chemistry, though certain lithium primary cell sizes do have lithium-ion rechargeable equivalents. Most rechargeable cylindrical cells use a chemistry with a nominal voltage around 3.7 volts, but cells produce only 3.2 volts.\n\nLithium-ion cells are made in various sizes, often assembled into packs for portable equipment. Many types are also available with an internal protection circuit to prevent over-discharge and short-circuit damage. This can increase their physical length; for example, an 18650 is around long, but may be around long with an internal protection circuit. Safe and economic recharging requires a charger specified for these cells. Popular applications include laptop battery packs, electronic cigarettes, flashlights, electric vehicles, and cordless power tools.\n\nCommonly-used designation numbers indicate the physical dimensions of the cylindrical cell, in a way similar to the system used for lithium button primary cells. The larger rechargeable cells are typically assigned five-digit numbers, where the first two digits are the (approximate) diameter in millimeters, followed by the last three digits indicating the (approximate) height in tenths of millimeters.\n\nThese types are no longer manufactured or only used in legacy applications.\n\nThe PP (\"Power Pack\") series was manufactured by Ever Ready in the UK (Eveready in the US). The series comprised multi-cell carbon-zinc batteries used for portable electronic devices. Most sizes are uncommon today, however the PP3 size (and to a lesser extent PP8 and PP9, which are used in electric fencing and marine applications respectively) is readily available. The PP4 was cylindrical; all the other types were rectangular. Most had snap terminals as seen on the common PP3 type. These came in two incompatible sizes, as is evident in some of the pictures below, those on larger, mostly older, battery types such as the PP9 being somewhat larger than those on the smaller batteries such as the PP3.\n\n\n"}
{"id": "5151456", "url": "https://en.wikipedia.org/wiki?curid=5151456", "title": "Louis Quinze", "text": "Louis Quinze\n\nThe Louis XV style or Louis Quinze is a style of architecture and decorative arts which appeared during the reign of Louis XV of France. From 1710 until about 1730, the period known as the Regency, it was largely an extension of the \"Style Louis XIV\" of his great-grandfather and predecessor, Louis XIV of France. From about 1730 until about 1750, it became more original, decorative and exuberant, in what was known as the \"rocaille\" style, under the influence of the King's mistress, Madame de Pompadour. It marked the beginning of the European Rococo movement. From 1750 until the King's death in 1774, it became more sober, ordered, and began to show the influences of neoclassicism.\n\nThe chief architect of the King was Jacques Gabriel from 1734 until 1742, and then his more famous son, Ange-Jacques Gabriel, until the end of the reign. His major works included the Ecole Militaire, the ensemble of buildings overlooking the Place Louis XV (now Place de la Concorde; 1761-1770), and the Petit Trianon at Versailles (1764). Over the course of the reign of Louis XV, while interiors were lavishly decorated, the facades gradually became simpler, less ornamented and more classical. The facades designed by Gabriel were carefully rhymed and balanced by rows of windows and columns, and, on large buildings like the Place de la Concorde, often featured grand arcades on the street level, and classical pediments or balustrades on the roofline. Ornamental features sometimes included curving wrought-iron balconies with undulating rocaille designs, similar to the rocaille decoration of the interiors.\n\nThe religious architecture of the period was also sober and monumental, and it tended, at the end of the reign, toward the neoclassical. Major examples include the Church of Saint-Genevieve (now the Panthéon), built from 1758 to 1790 to a design by Jacques-Germain Soufflot, and the Church of Saint-Philippe-du-Roule (1765-1777) by Jean Chalgrin, which featured an enomrous barrel-vaulted nave.\n\nInterior decoration during the reign of Louis XV fell into two periods; the first especially featured rocaille ornament, sculpted sinuous curves and counter-curves, often in floral and vegetative patterns, applied to the panels of the walls, often with medallions in the center. The panels large mirrors were framed in often framed with sculpted palm leaves or other floral decoration. Unlike the rococo style, the ornament was usually restrained, symmetrical and balanced. In the early period of the style, the designs were often inspired by French versions of Chinese art, animals, especially monkeys (\"Singerie\") and arabesques, or themes taken from works of the artists of the period, including Jean Bérain the Younger, Watteau and Jean Audran.\n\nAfter 1750, in reaction to the excesses of the earlier style, the designs and moldings on the interior walls were white or pale colored, more geometric, decorated with sculpted garlands, roses, and crowns, and ornamented with designs inspired by ancient Greece and Rome. This style was found in the \"Salon de Compagnie\" at the Petit Trianon, and it was the predecessor of the Louis XVI style.\n\nThe chairs of the Louis XV style, compared with those of Louis XIV, were characterized by lightness, comfort and harmony of lines. The traverse support of the legs disappeared, and the chairs were designed so one could sit back comfortably. The legs had a curving 'S shape. The carved decor featured sculpted fleurettes, palmettes, seashells, and foliage. The \"dossier\", or back of the chair, was \"violones\", slightly curved like a violin. Several new variants of chairs appeared including the \"bergere\", with stuffed upholstered arms, \"A confessional\", with upholstered and padded arms; the \"Marquise\", a \"bergere\" seating two persons, with a low back, and short arms.\n\nThe Console table was a table designed to be placed against a wall, usually used for displaying art objects; it was almost always in the rocaille style, with undulating curves, modeled after seashells and foliage. very sinuous, twisting rocaille modeled after seashells and foliage.\n\nThe Commode was a new type of furniture which had first appeared late in the reign of Louis XIV. It was a chest drawers resting on four S-shaped legs. It usually featured gilded bronze ornament, but during the reign of Louis XV, it was also covered with plaques of exotic woods of different colors in geometric patterns or floral shapes. A particular variation, called the \"façon de Chine\" or \"Chinese fashion\" emerged, which contrasted the gilded bronze against black lacquered wood. A large number of skilled \"ébénistes\" from around Europe were employed to fine wood Commodes and other furniture for the King. They included Jean-François Oeben, Roger Vandercruse Lacroix, Gilles Joubert, Antoine Gaudreau, and Martin Carlin.\n\nA variety of other new types of furniture appeared, including the \"chiffonier\", a cabinet with five drawers, and the \"table de toilette\", a kind of desk-table with three shutters, the central one having a mirror.\n\nLater in the reign of Louis XV, between 1755 and 1760, tastes in furniture began to change. The \"rocaille\" designs began more discreet and restrained, and the influence of antiquity and neo-classicism began to appear in new designs of furniture. The Commodes became to have more geometric forms; the decoration turned from rocaille to geometric forms, garlands of oak leaves, flowers and classical motifs. A new type of tall cabinet, the \"Cartonnier\", made its appearance between 1760 and 1765. It took its inspiration from Greek mythology and architecture, with friezes, vaulting, sculpted trophies, bronze lion heads, and other classic, elements.\n\nThe dominant subjects of painting in the early reign of Louis XV were mythology and history, the same as those of Louis XV. Later in the reign, when Louis began to construct new apartments within the palaces of Versailles and Fontanebleau, his tastes turned more to pastoral scenes and genre painting. Madame de Pompadour, the king's mistress, was also one of the major patrons of the artists of the period.\n\nThe most favored artist of the King was François Boucher, He produced for the King art of every description; religious paintings, genre scenes, landscapes, pastorals, and exotic scenes, frequently featuring gatherings of cheerful and seductive nudes. As the king's other great passion was hunting, he painted \"Leopard hunt\" (1765) and \"Crocodile hunt\" (1767) for the King's new apartments at Versailles. In 1767, near the end of the career, he was named First Painter of the King.\n\nOther notable painters included Jean Baptiste Oudry, whose hunting scenes decorated royal apartments in Versailles, and were made into tapestries and popular engravings; the portrait artists Maurice Quentin de la Tour and Jean-Marc Nattier, who made portraits for the royal family and aristocracy; and the genre painter Jean-Baptiste-Siméon Chardin.\nThe sculptural styles of the \"Grand Siécle\" of Louis XIV continued to dominate during most of the reign of Louis XV. Madame de Pompadour was a particularly enthusiastic patroness of sculpture, and many busts and statues were made of her or commissioned by her. The most prominent sculptors of the early period were the Guillaume Coustou the Younger and his brother, Guillaume Coustou the Elder, Robert Le Lorrain, and Edmé Bouchardon. Bouchardon created the equestrian statue of Louis XV for the center of the new Place Louis XV (now Place de la Concorde) which was modeled after that of Louis XIV in the Place Louis le Grand (now Place Vendôme) by François Girardon. After the death of Bouchardon, the statue was finished by another major monumentalist of the period, Jean-Baptiste Pigalle. In the later part of the reign of Louis XV, sculptors began to give greater attention to the faces; the leaders of this new style were Jean-Antoine Houdon noted for his busts of celebrated authors and statesmen, and Augustin Pajou, who made notable portrait busts of the natural scientist Georges-Louis Leclerc, Comte de Buffon and Madame du Barry. Sculpture began to reach a larger popular audience during this period, thanks to reproductions made from terra cotta and unglazed porcelain.\n\nIn the later years of his reign, Louis constructed a major new square in the center of the city, Place Louis XV (now Place de la Concorde, with a harmonious row of new buildings, designed by Ange-Jacques Gabriel. He built other monumental squares in the centers of Rennes and Bordeaux. He also constructed one monumental fountain in Paris, the Fontaine des Quatre-Saisons, with statuary by Edmé Bouchardon; but it was poorly sited on a narrow street, and while it had an abundance of sculpture, because of the antiquated water supply of Paris, it produced very little water. The fountain was criticized by Voltaire in a letter to the Count de Caylus in 1739, while it was still under construction:\n\n\n"}
{"id": "48066081", "url": "https://en.wikipedia.org/wiki?curid=48066081", "title": "Ministry of Energy, New and Renewable Energy Maharashtra", "text": "Ministry of Energy, New and Renewable Energy Maharashtra\n\nMinistry of Energy, New and Renewable Energy Maharashtra or MAHAURJA is a ministry of Government of Maharashtra. The ministry is currently headed by Chandrashekhar Bawankule, a Cabinet Minister.\n\nThe Ministry is mainly responsible for research and development, intellectual property protection, and international cooperation, promotion, and coordination in renewable energy sources such as wind power, small hydro, biogas, and solar power. The broad aim of the Ministry is to develop and deploy new and renewable energy for supplementing the energy requirements of India.\n\nThe ministry is headquartered in Mantralaya, Mumbai, Mumbai. According to the Central New and Renewable Energy Ministry's 2012–2013 annual report, India has made significant advances in several renewable energy sectors which include, Solar energy, Wind power, and Hydroelectricity.\n\nThe mission of the Ministry is to bring in Energy Security; Increase the share of clean power; increase Energy Availability and Access; improve Energy Affordability; and maximise Energy Equity.\n\nThe major functional area or Allocation of Business of MNRE are:\n\n\n\n"}
{"id": "64343", "url": "https://en.wikipedia.org/wiki?curid=64343", "title": "Moiré pattern", "text": "Moiré pattern\n\nIn mathematics, physics, and art, a moiré pattern (English: /mwɑːɹˈeɪ/; ) or moiré fringes are large-scale interference patterns that can be produced when an opaque ruled pattern with transparent gaps is overlaid on another similar pattern. For the moiré interference pattern to appear, the two patterns must not be completely identical, but rather e.g. displaced, rotated or have slightly different pitch.\n\nMoiré patterns appear in many different situations. In printing, the printed pattern of dots can interfere with the image. In television and digital photography, a pattern on an object being photographed can interfere with the shape of the light sensors to generate unwanted artifacts. They are also sometimes created deliberately – in micrometers they are used to amplify the effects of very small movements.\n\nIn physics, its manifestation is the beat phenomenon that occurs in many wave interference conditions.\n\nThe term originates from \"moire\" (\"moiré\" in its French adjectival form), a type of textile, traditionally of silk but now also of cotton or synthetic fiber, with a rippled or \"watered\" appearance. Moire, or \"watered textile\", is made by pressing two layers of the textile when wet. The similar but imperfect spacing of the threads creates a characteristic pattern which remains after the fabric dries.\n\nIn French, the adjective \"moiré\" (in use since at least 1823) derives from the earlier verb \"moirer\", \"to produce a watered textile by weaving or pressing\". \"Moirer\", in turn, is a variation of the word \"mouaire\" which is an adoption of the English \"mohair\" (in use since at least 1570). \"Mohair\" comes from the Arabic \"mukhayyar\" (, \"chosen\"), a cloth made from the wool of the Angora goat. \"Mukhayyar\" () descends from \"khayyara\" (, \"he chose\"). \"Chosen\" is meant in the sense of \"a choice, or excellent, cloth\". It has also been suggested that the Arabic word was formed from the Latin \"marmoreus\", meaning \"like marble\".\n\nBy 1660 (in the writings of Samuel Pepys), \"moire\" (or \"moyre\") had been adopted in English. \"Moire\" (pronounced \"mwar\") and \"moiré\" (pronounced \"mwar-ay\") are now used somewhat interchangeably in English, though \"moire\" is more often used for the cloth and \"moiré\" for the pattern.\n\nIn the liquid crystal display industry, moiré is often referred to by the Japanese word \"mura\", which roughly translates to \"unevenness; irregularity; lack of uniformity; nonuniformity; inequality.\"\n\nMoiré patterns are often an artifact of images produced by various digital imaging and computer graphics techniques, for example when scanning a halftone picture or ray tracing a checkered plane (the latter being a special case of aliasing, due to undersampling a fine regular pattern). This can be overcome in texture mapping through the use of mipmapping and anisotropic filtering.\n\nThe drawing on the upper right shows a moiré pattern. The lines could represent fibers in moiré silk, or lines drawn on paper or on a computer screen. The nonlinear interaction of the optical patterns of lines creates a real and visible pattern of roughly parallel dark and light bands, the moiré pattern, superimposed on the lines.\n\nThe moiré effect also occurs between overlapping transparent objects. For example, an invisible phase mask is made of a transparent polymer with a wavy thickness profile. As light shines through two overlaid masks of similar phase patterns, a broad moiré pattern occurs on a screen some distance away. This phase moiré effect and the classical moiré effect from opaque lines are two ends of a continuous spectrum in optics, which is called the universal moiré effect. The phase moiré effect is the basis for a type of broadband interferometer in x-ray and particle wave applications. It also provides a way to reveal hidden patterns in invisible layers.\n\nLine moiré is one type of moiré pattern; a pattern that appears when superposing two transparent layers containing correlated opaque patterns. Line moiré is the case when the superposed patterns comprise straight or curved lines. When moving the layer patterns, the moiré patterns transform or move at a faster speed. This effect is called optical moiré speedup.\n\nMore complex line moiré patterns are created if the lines are curved or not exactly parallel.\n\nShape moiré is one type of moiré pattern demonstrating the phenomenon of moiré magnification. 1D shape moiré is the particular simplified case of 2D shape moiré. One-dimensional patterns may appear when superimposing an opaque layer containing tiny horizontal transparent lines on top of a layer containing a complex shape which is periodically repeating along the vertical axis.\n\nMoiré patterns revealing complex shapes, or sequences of symbols embedded in one of the layers (in form of periodically repeated compressed shapes) are created with shape moiré, otherwise called \"band moiré\" patterns. One of the most important properties of shape moiré is its ability to magnify tiny shapes along either one or both axes, that is, stretching. A common 2D example of moiré magnification occurs when viewing a chain-link fence through a second chain-link fence of identical design. The fine structure of the design is visible even at great distances.\n\nLet us consider two patterns made of parallel and equidistant lines, e.g., vertical lines. The step of the first pattern is , the step of the second is , with .\n\nIf the lines of the patterns are superimposed at the left of the figure, the shift between the lines increase when going to the right. After a given number of lines, the patterns are opposed: the lines of the second pattern are between the lines of the first pattern. If we look from a far distance, we have the feeling of pale zones when the lines are superimposed (there is white between the lines), and of dark zones when the lines are \"opposed\".\n\nThe middle of the first dark zone is when the shift is equal to . The th line of the second pattern is shifted by compared to the th line of the first network. The middle of the first dark zone thus corresponds to\nthat is\nThe distance between the middle of a pale zone and a dark zone is\nthe distance between the middle of two dark zones, which is also the distance between two pale zones, is\nFrom this formula, we can see that:\n\nThe principle of the moiré is similar to the Vernier scale.\n\nThe essence of the moiré effect is the (mainly visual) perception of a distinctly different third pattern which is caused by inexact superimposition of two similar patterns. The mathematical representation of these patterns is not trivially obtained and can seem somewhat arbitrary. In this section we shall give a mathematical example of two parallel patterns whose superimposition forms a moiré pattern, and show one way (of many possible ways) these patterns and the moiré effect can be rendered mathematically.\n\nThe visibility of these patterns is dependent on the medium or substrate in which they appear, and these may be opaque (as for example on paper) or transparent (as for example in plastic film). For purposes of discussion we shall assume the two primary patterns are each printed in greyscale ink on a white sheet, where the opacity (e.g., shade of grey) of the \"printed\" part is given by a value between 0 (white) and 1 (black) inclusive, with representing neutral grey. Any value less than 0 or greater than 1 using this grey scale is essentially \"unprintable\".\n\nWe shall also choose to represent the opacity of the pattern resulting from printing one pattern atop the other at a given point on the paper as the average (i.e. the arithmetic mean) of each pattern's opacity at that position, which is half their sum, and, as calculated, does not exceed 1. (This choice is not unique. Any other method to combine the functions that satisfies keeping the resultant function value within the bounds [0,1] will also serve; arithmetic averaging has the virtue of simplicity—with hopefully minimal damage to one's concepts of the printmaking process.)\n\nWe now consider the \"printing\" superimposition of two almost similar, sinusoidally varying, grey-scale patterns to show how they produce a moiré effect in first printing one pattern on the paper, and then printing the other pattern over the first, keeping their coordinate axes in register. We represent the grey intensity in each pattern by a positive opacity function of distance along a fixed direction (say, the x-coordinate) in the paper plane, in the form\n\nwhere the presence of 1 keeps the function positive definite, and the division by 2 prevents function values greater than 1.\n\nThe quantity represents the periodic variation (i.e., spatial frequency) of the pattern's grey intensity, measured as the number of intensity cycles per unit distance. Since the sine function is cyclic over argument changes of , the distance increment per intensity cycle (the wavelength) obtains when , or .\n\nConsider now two such patterns, where one has a slightly different periodic variation from the other:\n\nsuch that .\n\nThe average of these two functions, representing the superimposed printed image, evaluates as follows:\n\nwhere it is easily shown that\n\nand\n\nThis function average, , clearly lies in the range [0,1]. Since the periodic variation is the average of and therefore close to and , the moiré effect is distinctively demonstrated by the sinusoidal envelope \"beat\" function , whose periodic variation is half the difference of the periodic variations and (and evidently much lower in frequency).\n\nOther one-dimensional moiré effects include the classic beat frequency tone which is heard when two pure notes of almost identical pitch are sounded simultaneously. This is an acoustic version of the moiré effect in the one dimension of time: the original two notes are still present—but the listener's \"perception\" is of two pitches that are the average of and half the difference of the frequencies of the two notes. Aliasing in sampling of time-varying signals also belongs to this moiré paradigm.\n\nLet us consider two patterns with the same step , but the second pattern is rotated by an angle . Seen from afar, we can also see darker and paler lines: the pale lines correspond to the lines of nodes, that is, lines passing through the intersections of the two patterns.\n\nIf we consider a cell of the lattice formed, we can see that it is a rhombus with the four sides equal to ; (we have a right triangle whose hypotenuse is and the side opposite to the angle is ).\n\nThe pale lines correspond to the small diagonal of the rhombus. As the diagonals are the bisectors of the neighbouring sides, we can see that the pale line makes an angle equal to with the perpendicular of each pattern's line.\n\nAdditionally, the spacing between two pale lines is , half of the long diagonal. The long diagonal is the hypotenuse of a right triangle and the sides of the right angle are and . The Pythagorean theorem gives:\nthat is:\nthus\n\nWhen is very small () the following small-angle approximations can be made:\nthus\n\nWe can see that the smaller is, the farther apart the pale lines; when both patterns are parallel (), the spacing between the pale lines is infinite (there is no pale line).\n\nThere are thus two ways to determine : by the orientation of the pale lines and by their spacing\nIf we choose to measure the angle, the final error is proportional to the measurement error. If we choose to measure the spacing, the final error is proportional to the inverse of the spacing. Thus, for the small angles, it is best to measure the spacing.\n\nIn graphic arts and prepress, the usual technology for printing full-color images involves the superimposition of halftone screens. These are regular rectangular dot patterns—often four of them, printed in cyan, yellow, magenta, and black. Some kind of moiré pattern is inevitable, but in favorable circumstances the pattern is \"tight\"; that is, the spatial frequency of the moiré is so high that it is not noticeable. In the graphic arts, the term \"moiré\" means an \"excessively visible\" moiré pattern. Part of the prepress art consists of selecting screen angles and halftone frequencies which minimize moiré. The visibility of moiré is not entirely predictable. The same set of screens may produce good results with some images, but visible moiré with others.\n\nMoiré patterns are commonly seen on television screens when a person is wearing a shirt or jacket of a particular weave or pattern, such as a houndstooth jacket. This is due to interlaced scanning in televisions and non-film cameras, referred to as interline twitter. As the person moves about, the moiré pattern is quite noticeable. Because of this, newscasters and other professionals who appear on TV regularly are instructed to avoid clothing which could cause the effect.\n\nPhotographs of a TV screen taken with a digital camera often exhibit moiré patterns. Since both the TV screen and the digital camera use a scanning technique to produce or to capture pictures with horizontal scan lines, the conflicting sets of lines cause the moiré patterns. To avoid the effect, the digital camera can be aimed at an angle of 30 degrees to the TV screen.\n\nThe Moiré effect is used in shoreside beacons called \"Inogon leading marks\" or \"Inogon lights\" to mark underwater hazards (usually pipelines or cables). The moiré effect creates arrows that point towards an imaginary line marking the hazard; as navigators pass over the hazard, the arrows on the beacon appear to become vertical bands before changing back to arrows pointing in the reverse direction. An example can be found in the UK on the East shore of Southampton water, opposite Fawley oil refinery (). Similar moiré effect beacons can be used to guide mariners to the centre point of an oncoming bridge; when the vessel is aligned with the centreline, vertical lines are visible.\n\nIn manufacturing industries, these patterns are used for studying microscopic strain in materials: by deforming a grid with respect to a reference grid and measuring the moiré pattern, the stress levels and patterns can be deduced. This technique is attractive because the scale of the moiré pattern is much larger than the deflection that causes it, making measurement easier.\n\nThe moiré effect can be used in strain measurement: the operator just has to draw a pattern on the object, and superimpose the reference pattern to the deformed pattern on the deformed object.\n\nA similar effect can be obtained by the superposition of an holographic image of the object to the object itself: the hologram is the reference step, and the difference with the object are the deformations, which appear as pale and dark lines.\n\n\"See also: theory of elasticity, strain tensor and holographic interferometry.\"\n\nSome image scanner driver programs provide an optional filter, called a \"descreen\" filter, to remove Moiré-pattern artifacts which would otherwise be produced when scanning printed halftone images to produce digital images.\n\nMany banknotes exploit the tendency of digital scanners to produce moiré patterns by including fine circular or wavy designs that are likely to exhibit a moiré pattern when scanned and printed.\n\nIn super-resolution microscopy, the moiré pattern can be used to obtain images with a resolution higher than the diffraction limit, using a technique known as structured illumination microscopy.\n\nIn scanning tunneling microscopy, moiré fringes appears if surface atomic layers have a different crystal structure than the bulk crystal. This can for example be due to surface reconstruction of the crystal, or when a thin layer of a second crystal is on the surface, e.g. single-layer, double-layer graphene, or Van der Waals heterostructure of graphene and hBN .\n\nIn transmission electron microscopy (TEM), translational Moiré fringes can be seen as parallel contrast lines formed in phase-contrast TEM imaging by the interference of diffracting crystal lattice planes that are overlapping, and which might have different spacing and/or orientation, Most of the Moiré contrast observations reported in the literature are obtained using high-resolution phase contrast imaging in TEM. However, if probe aberration-corrected high-angle annular dark field scanning transmission electron microscopy (HAADF-STEM) imaging is used, more direct interpretation of the crystal structure in terms of atom types and positions is obtained.\n\nIn materials science, known examples exhibiting Moiré contrast are e.g. thin films or nanoparticles of MX-type (M = Ti, Nb; X = C, N) overlapping with austenitic matrix. Both phases, MX and the matrix, have face-centered cubic crystal structure and cube-on-cube orientation relationship. However, they have significant lattice misfit of about 20 to 24% (based on the chemical composition of alloy) what then leads to Moiré effect.\n\n\n"}
{"id": "23413251", "url": "https://en.wikipedia.org/wiki?curid=23413251", "title": "Neuralyzer", "text": "Neuralyzer\n\nA neuralyzer, sometimes spelled as neuralizer, is a device seen in the \"Men in Black\" franchise. It is one of the signature tools and considered standard issue employed by the Men in Black. It is a device about the size of an average pen that gives a bright flash which erases the memories of the past hours, days, weeks, months or years, depending on the chosen settings. It first appeared in the first issue of the comic book series, and has appeared in all three films, as well as the . It also has changed from red to blue.\n\nThe device in the comic books was referred to as a 'neurolyser'. This is also where the concept of using Ray-Ban sunglasses was introduced. It is seen more in the shape of a flashlight than a pen-like shape made popular by the films. It does not simply erase memory like the movie counterpart does, but is a device used for manipulation and control. When neurolysed, a witness is simply hypnotized and does what ever Agent K or J wants them to do, such as provide information, believe a lie and/or pass that lie along as fact.\n\nA neuralyzer wipes the memory of a target or witness, putting them under a hypnotic state and making them susceptible to suggestion and implantation of false memories. It is used by MIB as a moral substitute to witness elimination by murder, while keeping both the agency's existence and the presence of aliens on Earth unknown to the public. The length of memory erased can be changed using buttons and dials. There are different neuralyzer models with different button and dial configurations, which allow the setting of time spans ranging from minutes to years. The device produces a bright, noisy flash similar to that of a camera.\n\nTo avoid the device's effects, agents wear Ray-Ban sunglasses with deeply tinted black lenses that filter out the rays. After using the neuralyzer, the agents have a short period of time to supply new memories to replace the ones they have just erased. If there are more witnesses to neuralyze, or if they are pressed for time, a special team is sent in to supply memories. The neuralyzer cannot delete a specific memory; it can only erase all memories formed during the chosen time span up to the moment of its use.\n\nAfter being neuralyzed, the witnesses forget the events that are meant to be removed from their memory. However, they can sometimes experience déjà vu if they encounter something closely associated with the erased memories. Agent J (Will Smith) and Dr. Laurel Weaver (Linda Fiorentino) both experience this feeling when they meet in the city morgue; the two had previously met at a police station prior to J's joining MIB, but Agent K had wiped both their memories of the encounter at different times. Agents who retire, resign, or are fired from MIB are neuralyzed, as are rejected agent candidates and trainees.\n\nChanges to the neuralyzer have been featured in the films. In the first movie, the color of its eye was red. This was later changed to blue in the second film. It remains blue in the third film, although the shade is more of a turquoise rather than the previous film's aqua, and the design of the neuralyzer itself is considerably different compared to the previous films. In the first movie, Agent J calls the neuralyzer the \"flashy thing\"; he disapproves of K's liberal use of it and believes that they should supply the victims with pleasant memories rather than mundane or depressing ones. At the end of the film, J neuralyzes K so he can retire from MIB, wiping out 35 years of his memory.\n\nIn the second movie, J acquires a reputation for neuralyzing multiple partners and throwing them out of MIB. He attempts to justify his use of the device on Agent T, citing T's emotional breakdown in a public place, and on Agent L (formerly Laurel Weaver), saying that she wanted to resume her work as a medical examiner. The MIB have also installed neuralyzers in many places, such as Agent J's car and The Statue of Liberty, which has one installed in its torch that is powerful enough to wipe the memories of everyone in the greater New York and New Jersey area.\n\nIn the third film the original neuralyzer is seen. It is a large contraption that requires strapping the subject into a platform that is then inserted inside the neuralyzer. Later, the young Agent K (Josh Brolin) is seen with an early portable neuralyzer attached to a battery pack that is clipped to his belt. He used it on young James Darrell Edwards III, saying his father was a hero.\n\nA deneuralyzer is a special chamber room that serves to reverse the effects of a neuralyzer. While the neuralyzer is a small object and works in a single flash, deleting memory within seconds, it takes a huge room for a deneuralyzer and the memory scan lasts for around 5–10 minutes, depending on the memory needed to be recovered. The deneuralyzer is a chamber with a single seat, a huge computer and metal fittings put on a person's head to recover the memory.\n\nThe deneuralyzer was only seen and mentioned in the second film, and it was stated that the only \"official\" deneuralyzer is owned by the MiB. It was designed as a huge toilet bowl, in which J had to reverse the neuralyzer effects on K. However, when the MiB HQ was breached, an emergency protocol filled the room with water and flushed it like a toilet, sending the two agents out to Times Square.\n\nJack Jeebs, an alien pawn shop owner, owned a second, illegal deneuralyzer in his basement. Unlike the highly sophisticated MiB deneuralyzer, his was built in a Rube Goldberg machine-like style out of primitive tools (bowling ball, old industrial fan, barber chair and a whipped cream mixer), and was controlled by a small computer placed next to it using software Jeebs developed. It was also highly unreliable: Jeebs powered the machine using a boat engine, and the barber chair was loose, shaking K and sending him flying across the room after the process. The machine also malfunctioned and caused a blackout in the entire New York area for a few seconds, but it was up and running again soon after. However, the deneuralyzer was a success, although it took K some time to regain some memories. Jeebs never used it to return memories to other beings, only using the exhaust once to \"make some hot-air popcorn\". The deneuralyzer took a minute to complete its job. According to Jeebs, his deneuralyzer cannot be used if one is wearing any jewelry or/and is allergic to shellfish. He took the plans for the machine off the internet (where he also advertised the deneuralyzer) and said that he would bring K's memory back in an instant if he had the more advanced MiB software instead of his own.\n\nThe neuralyzer has gained popularity outside the franchise. Sundance Channel ranked it No. 4 on its list of \"Top 10 Film Inventions We Wish Were Real,\" saying that the silver pen-looking thing with the red light needs to exist in real life. Top10Kid.com ranked it at No. 7, saying \"The uses of eliminating someone’s memories are endless, especially if you are prone to screwing up a lot.\" MSN puts it \"The greatest fictional inventions of all time.\"\n\nThe neuralyzer also appeared in a special segment of \"How Stuff Works: Sci-Fi Saved My Life\" dedicated to the MiB franchise. Multiple remakes of it are available in the Android Market complete with sounds and a flash.\n\nIn \"The Simpsons\" episode \"The Fool Monty,\" when Smithers leaves Vice President Dick Cheney's employment to return to working for Mr. Burns, Cheney zaps him with a neuralyzer. It appeared again in \"The Simpsons\" comic No. 178 entitled \"The Thingama-Bob from Outer Space,\" in which Sideshow Bob is taken away by a man wearing a black suit and sunglasses who promises him that he won't remember anything of what has just happened. It also appeared in the \"Family Guy\" episode entitled \"From Method to Madness,\" in which Lois Griffin uses it on Chris when he cannot stop saying \"boobies\" after meeting a family of nudists. It appeared again in \"Leggo My Meg-O\", where Stewie uses it to erase Meg's memories after killing her apparent kidnapper and boyfriend. In the video game \"Marvel Heroes\", the Neuralyzer appears as an item that resets story progress, referencing the device's ability to make one forget. The item description says \"a gift from some friends out of town\"\n"}
{"id": "48970140", "url": "https://en.wikipedia.org/wiki?curid=48970140", "title": "Piccolo (firecracker)", "text": "Piccolo (firecracker)\n\nPiccolo (also sold under the trade names Great Bawang, Piccolo Corsair and numerous others) is a name given to a type of firecracker in the form of a thin small cylindrical stick filled with gunpowder and lit in the same way as a match.\n\nPiccolo sticks ignite by rubbing the phosphorus-tipped head on a specially designed striking surface similar to a regular match, and can explode even on wet surfaces or underwater due to its thick cardboard shell. They are usually sold in boxes with pirate-themed artwork though variants exist with either generic or differently-themed packaging.\n\nAs they are readily available and inexpensive, it has since earned notoriety as a leading cause of firecracker-related injuries in the Philippines especially among small children, either due to premature ignition or accidental ingestion. The Department of Health, along with the Bureau of Fire Protection and local government agencies, has taken steps to outlaw the sale and importation of piccolo sticks. Counterfeit and repackaged versions are being sold in flea markets and sidewalk stalls such as in Divisoria, with most bearing patently false markings such as \"Made in Bulacan\" and instructions in Tagalog in an attempt to disguise its origin and thus evade detection by authorities.\n"}
{"id": "5425926", "url": "https://en.wikipedia.org/wiki?curid=5425926", "title": "Q-Connector", "text": "Q-Connector\n\nQ-Connector or ASUS Q-Connector, is an adapter, sometimes included with ASUS motherboards, which sits in between the motherboard front panel connectors and the front panel cables. The Q-Connector is marked with bigger text than the front panel connectors on the motherboard, as well as protruding from the motherboard, limiting obstruction from heatsinks and other connectors.\n\n\n"}
{"id": "23568658", "url": "https://en.wikipedia.org/wiki?curid=23568658", "title": "Radioanalytical chemistry", "text": "Radioanalytical chemistry\n\nRadioanalytical chemistry focuses on the analysis of sample for their radionuclide content. Various methods are employed to purify and identify the radioelement of interest through chemical methods and sample measurement techniques.\n\nThe field of radioanalytical chemistry was originally developed by Marie Curie with contributions by Ernest Rutherford and Frederick Soddy. They developed chemical separation and radiation measurement techniques on terrestrial radioactive substances. During the twenty years that followed 1897 the concepts of radionuclides was born. Since Curie's time, applications of radioanalytical chemistry have proliferated. Modern advances in nuclear and radiochemistry research have allowed practitioners to apply chemistry and nuclear procedures to elucidate nuclear properties and reactions, used radioactive substances as tracers, and measure radionuclides in many different types of samples.\n\nThe importance of radioanalytical chemistry spans many fields including chemistry, physics, medicine, pharmacology, biology, ecology, hydrology, geology, forensics, atmospheric sciences, health protection, archeology, and engineering. Applications include: forming and characterizing new elements, determining the age of materials, and creating radioactive reagents for specific tracer use in tissues and organs. The ongoing goal of radioanalytical researchers is to develop more radionuclides and lower concentrations in people and the environment.\n\nAlpha decay is characterized by the emission of an alpha particle, a He nucleus. The mode of this decay causes the parent nucleus to decrease by two protons and two neutrons. This type of decay follows the relation:\n\nformula_1 \n\nBeta decay is characterized by the emission of a neutrino and a negatron which is equivalent to an electron. This process occurs when a nucleus has an excess of neutrons with respect to protons, as compared to the stable isobar. This type of transition converts a neutron into a proton; similarly, a positron is released when a proton is converted into a neutron. These decays follows the relation:\n\nformula_2\n\nformula_3 \n\nGamma ray emission follows the previously discussed modes of decay when the decay leaves a daughter nucleus in an excited state. This nucleus is capable of further de-excitation to a lower energy state by the release of a photon. This decay follows the relation:\n\nformula_4 \n\nGaseous ionization detectors collect and record the electrons freed from gaseous atoms and molecules by the interaction of radiation released by the source. A voltage potential is applied between two electrodes within a sealed system. Since the gaseous atoms are ionized after they interact with radiation they are attracted to the anode which produces a signal. It is important to vary the applied voltage such that the response falls within a critical proportional range.\n\nThe operating principle of Semiconductor detectors is similar to gas ionization detectors: expect instead of ionization gas atoms, free electrons and holes are produced which create a signal at the electrodes. The advantage of solid state detectors is the greater resolution of the resultant energy spectrum. Usually NaI(Tl) detectors are used; for more precise applications Ge(Li) and Si(Li) detectors have been developed. For extra sensitive measurements high-pure germanium detectors are used under a liquid nitrogen environment.\n\nScintillation detectors uses a photo luminescent source (such as ZnS) which interacts with radiation. When a radioactive particle decays and strikes the photo luminescent material a photon is released. This photon is multiplied in a photomultiplier tube which converts light into an electrical signal. This signal is then processed and converted into a channel. By comparing the number of counts to the energy level (typically in keV or MeV) the type of decay can be determined.\n\nDue to radioactive nucleotides have similar properties to their stable, inactive, counterparts similar analytical chemistry separation techniques can be used. These separation methods include precipitation, Ion Exchange, Liquid Liquid extraction, Solid Phase extraction, Distillation, and Electrodeposition.\n\nSamples with very low concentrations are difficult to measure accurately due to the radioactive atoms unexpectedly depositing on surfaces. Sample loss at trace levels may be due to adhesion to container walls and filter surface sites by ionic or electrostatic adsorption, as well as metal foils and glass slides. Sample loss is an ever present concern, especially at the beginning of the analysis path where sequential steps may compound these losses. \n\nVarious solutions are known to circumvent these losses which include adding an inactive carrier or adding a tracer. Research has also shown that pretreatment of glassware and plastic surfaces can reduce radionuclide sorption by saturating the sites.\n\nDue to the inherent nature of radionuclides yielding low concentrations a common technique to improve yields is the addition of carrier ions or tracers. Isotope dilution involves the addition of a known amount of radionuclide tracer to the sample that contains a known stable element. This is done at the start of the analysis procedure so once the final measurements are taken, sample loss is considered. This procedure avoids the need for any quantitative recovery which greatly simplifies the analytical process. \n\nCarrier addition is the reverse technique of tracer addition. Instead of isotope dilution, a known mass of stable carrier ion is added to radionuclide sample solution. The carrier reagent must be calibrated prior to addition to the sample. To verify the resultant measurements, the expected 100% yield is compared to the actual yield. Any loss in yield is analogous to any losses in the radioactive sample. Typically the amount of carrier added is conventionally selected for the ease of weighing such that the accuracy of the resultant weight is within 1%. For alpha particles, special techniques must be applied to obtain the required thin sample sources.\n\nAs this is an analytical chemistry technique quality control is an important factor to maintain. A laboratory must produce trustworthy results. This can be accomplished by a laboratories continual effort to maintain instrument calibration, measurement reproducibility, and applicability of analytical methods. In all laboratories there must be a quality assurance plan. This plan describes the quality system and procedures in place to obtain consistent results. Such results must be authentic, appropriately documented, and technically defensible.\" Such elements of quality assurance include organization, personnel training, laboratory operating procedures, procurement documents, chain of custody records, standard certificates, analytical records, standard procedures, QC sample analysis program and results, instrument testing and maintenance records, results of performance demonstration projects, results of data assessment, audit reports, and record retention policies. \n\nThe cost of quality assurance is continually on the rise but the benefits far outweigh this cost. The average quality assurance workload was risen from 10% to a modern load of 20-30%. This heightened focus on quality assurance ensures that quality measurements that are reliable are achieved. The cost of failure far outweighs the cost of prevention and appraisal. Finally, results must be scientifically defensible by adhering to stringent regulations in the event of a lawsuit.\n\n"}
{"id": "33538564", "url": "https://en.wikipedia.org/wiki?curid=33538564", "title": "Reach-In", "text": "Reach-In\n\nReach-In formerly Apriori Control is a technology company that \"provides real-time physical interactions over the internet.\" It manufactures devices and webcams that can be controlled remotely. A popular product is iPet Companion, which allows users to view and interact with pets.\n\nReach-In (Apriori, LLC) developed iPet Companion in cooperation with the Idaho Humane Society and the Oregon Humane Society. Using cat toys tied to robotic arms, website visitors could play with kittens housed at the societies' animal shelters. Since the installation, pet adoptions have increased at both shelters.\n\nThe idea was born serendipidously when an engineer's cat distracted him from working on a robotic arm. Reach-In (Apriori, LLC) donated a prototype system to the Idaho Humane Society for the initial launch in June 2010. All the toy mechanisms were broken within the first week, and had to be strengthened to survive the \"destructive capacity of a roomful of kittens.\"\n\nAn iPet Companion system was installed at Bideawee in New York City in October 2011, making it the only shelter on the East Coast of the United States with the system. The Michigan Humane Society Berman Center for Animal Care in Westland, Michigan has a system.\n\nIn 2012, Kong Company partnered with Reach-In to install iPet Companion technology in humane societies around the United States, “Pets Need to Play” campaign. Shelters house the interactive toys for six weeks at their facilities to help adoptable pets find forever homes, and increase sponsorships for the shelters. Shelters include Foothills Animal Shelter in Kong's home Golden, Colorado, Arizona Animal Welfare League & Society for Prevention of Cruelty to Animals in Valley, Arizona, Wisconsin Humane Society in Milwaukee, Wisconsin. and Los Angeles Best Friends Pet Adoption & Spay Neuter Center in Mission Hills, California.\n\nThe web interface requires Microsoft Silverlight.\n\nAnother application for this technology includes a submarine and gaming. Dive Commander project created an online physically interactive submarine game controlled over the web. Users take control of the sub to find clues to an underwater treasure hunt, at the same time the technology lets a second user take control of the \"Nuisance Buttons\" trying to disrupt the passage of the Captain in Charge.\n\nReach-In has worked with Universities in medical research. Thermal Mannequin Laboratory provides an innovative new technology for measuring the heat exchange properties of protective clothing worn by persons engaged in hot outdoor activities. Convective and radiative heat exchange properties of garments can be documented while being exposed to wind and Infrared radiation. Tests performed in this laboratory to-date have contributed to the design and development of new clothing systems for agricultural workers. The principles used in the new garment designs can now also be incorporated into garments worn by athletes and other persons exposed to hot outdoor environments. Allowing students around the world to participate in such experiments, greater individual learning and improved teaching effiences in the field of ergonomics is envisioned.\n\nReach-In also conducted a Be@Hospital experiment where physicians around the globe could physically interact during a presentation of a tracheotomy.\n\nLive Band Interactive was tailored to the music industry, allowing fans to interact with their favorite bands. During May 2012, Radio Boise leased the Reach-In media equipment for a month, allowing radio listeners to go on-line and control a camera and objects in the live studio as bands were playing during the fund-raising campaign.\n\n"}
{"id": "15682002", "url": "https://en.wikipedia.org/wiki?curid=15682002", "title": "Seven stages of action", "text": "Seven stages of action\n\nSeven stages of action is a term coined by the usability consultant Donald Norman. \nHe explains this phrase in chapter two of his book \"The Design of Everyday Things\", in the context of explaining the psychology of a person behind the task performed by him or her.\n\nThe history behind the action cycle starts from a conference in Italy attended by Donald Norman.\nThis excerpt has been taken from the book \"The Design of Everyday Things\":\n\nI am in Italy at a conference. I watch the next speaker attempt to thread a film onto a projector that he never used before. He puts the reel into place, then takes it off and reverses it. Another person comes to help. Jointly they thread the film through the projector and hold the free end, discussing how to put it on the takeup reel. Two more people come over to help and then another. The voices grow louder, in three languages: Italian, German and English. One person investigates the controls, manipulating each and announcing the result. Confusion mounts. I can no longer observe all that is happening. The conference organizer comes over. After a few moments he turns and faces the audience, who had been waiting patiently in the auditorium. \"Ahem,\" he says, \"is anybody expert in projectors?\" Finally, fourteen minutes after the speaker had started to thread the film (and eight minutes after the scheduled start of the session) a blue-coated technician appears. He scowls, then promptly takes the entire film off the projector, rethreads it, and gets it working.\nNorman pondered on the reasons that made something like threading of a projector difficult to do. To examine this, he wanted to know what happened when something implied nothing. In order to do that, he examined the structure of an action. So to get something done, a notion of what is wanted – the goal that is to be achieved, needs to be started. Then, something is done to the world i.e. take action to move oneself or manipulate someone or something. Finally, the checking is required if the goal was made. This led to formulation of Stages of Execution and Evaluation.\n\n\"Execution\" formally means to perform or do something. Norman explains that a person sitting on an armchair while reading a book at dusk, might need more light when it becomes dimmer and dimmer. To do that, he needs to switch on the button of a lamp i.e. get more light (the goal). To do this, one must need to specify on how to move one's body, how to stretch to reach the light switch and how to extend one's finger to push the button. The goal has to be translated into an intention, which in turn has to be made into an action sequence.\n\nThus, formulation of stages of execution:\n\n\"Evaluation\" formally means to examine and calculate. Norman explains that after turning on the light, we evaluate if it is actually turned on. A careful judgement is then passed on how the light has affected our world i.e. the room in which the person is sitting on the armchair while reading a book.\n\nThe formulation of the stages of evaluation can be described as:\n\nSeven Stages of Action constitute four stages of execution, three stages of evaluation and our goals.\n\n1. Forming the goal\n\n2. Forming the intention\n\n3. Specifying an action\n\n4. Executing the action\n\n5. Perceiving the state of the world\n\n6. Interpreting the state of the world\n\n7. Evaluating the outcome \n\nThe difference between the intentions and the allowable actions is the \"Gulf of execution\".\n\n\"Consider the movie projector example: one problem resulted from the Gulf of Execution. The person wanted to set up the projector. Ideally, this would be a simple thing to do. But no, a long, complex sequence was required. It wasn't all clear what actions had to be done to accomplish the intentions of setting up the projector and showing the film.\"\nThe \"Gulf of evaluation\" reflects the amount of effort that the person must exert to interpret the physical state of the system and to determine how well the expectations and intentions have been met.\n\n\"In the movie projector example there was also a problem with the Gulf of Evaluation. Even when the film was in the projector, it was difficult to tell if it had been threaded correctly.\"\nThe seven-stage structure is referenced as design aid to act as a basic checklist for designers' questions to ensure that the Gulfs of Execution and Evaluation are bridged.\n\nThe Seven Stages of Action can be broken down into 4 main principles of good design:\n\n\n"}
{"id": "29192", "url": "https://en.wikipedia.org/wiki?curid=29192", "title": "Space elevator", "text": "Space elevator\n\nA space elevator is a proposed type of planet-to-space transportation system. The main component would be a cable (also called a tether) anchored to the surface and extending into space. The design would permit vehicles to travel along the cable from a planetary surface, such as the Earth's, directly into space or orbit, without the use of large rockets. An Earth-based space elevator would consist of a cable with one end attached to the surface near the equator and the other end in space beyond geostationary orbit (35,786 km altitude). The competing forces of gravity, which is stronger at the lower end, and the outward/upward centrifugal force, which is stronger at the upper end, would result in the cable being held up, under tension, and stationary over a single position on Earth. With the tether deployed, climbers could repeatedly climb the tether to space by mechanical means, releasing their cargo to orbit. Climbers could also descend the tether to return cargo to the surface from orbit.\n\nThe concept of a tower reaching geosynchronous orbit was first published in 1895 by Konstantin Tsiolkovsky. His proposal was for a free-standing tower reaching from the surface of Earth to the height of geostationary orbit. Like all buildings, Tsiolkovsky's structure would be under compression, supporting its weight from below. Since 1959, most ideas for space elevators have focused on purely tensile structures, with the weight of the system held up from above by centrifugal forces. In the tensile concepts, a space tether reaches from a large mass (the counterweight) beyond geostationary orbit to the ground. This structure is held in tension between Earth and the counterweight like an upside-down plumb bob.\n\nTo construct a space elevator on Earth, the cable material would need to be both stronger and lighter (have greater specific strength) than any known material. Development of new materials that meet the demanding specific strength requirement must happen before designs can progress beyond discussion stage. Carbon nanotubes (CNTs) have been identified as possibly being able to meet the specific strength requirements for an Earth space elevator. Other materials considered have been boron nitride nanotubes, and diamond nanothreads, which were first constructed in 2014.\n\nA prototype was launched in 2018 to tether to future stations as well as the International Space Station. It is a miniature version to be further examined before making the decision to build up a large structure in the coming years. \n\nThe concept is applicable to other planets and celestial bodies. For locations in the solar system with weaker gravity than Earth's (such as the Moon or Mars), the strength-to-density requirements for tether materials are not as problematic. Currently available materials (such as Kevlar) are strong and light enough that they could be used as the tether material for elevators there.\n\nThe key concept of the space elevator appeared in 1895 when Russian scientist Konstantin Tsiolkovsky was inspired by the Eiffel Tower in Paris. He considered a similar tower that reached all the way into space and was built from the ground up to the altitude of 35,786 kilometers, the height of geostationary orbit. He noted that the top of such a tower would be circling Earth as in a geostationary orbit. Objects would attain horizontal velocity as they rode up the tower, and an object released at the tower's top would have enough horizontal velocity to remain there in geostationary orbit. Tsiolkovsky's conceptual tower was a compression structure, while modern concepts call for a tensile structure (or \"tether\").\n\nBuilding a compression structure from the ground up proved an unrealistic task as there was no material in existence with enough compressive strength to support its own weight under such conditions. In 1959 another Russian scientist, Yuri N. Artsutanov, suggested a more feasible proposal. Artsutanov suggested using a geostationary satellite as the base from which to deploy the structure downward. By using a counterweight, a cable would be lowered from geostationary orbit to the surface of Earth, while the counterweight was extended from the satellite away from Earth, keeping the cable constantly over the same spot on the surface of the Earth. Artsutanov's idea was introduced to the Russian-speaking public in an interview published in the Sunday supplement of \"Komsomolskaya Pravda\" in 1960, but was not available in English until much later. He also proposed tapering the cable thickness so that the stress in the cable was constant. This gave a thinner cable at ground level that became thickest at the level of geostationary orbit.\n\nBoth the tower and cable ideas were proposed in the quasi-humorous \"Ariadne\" column in \"New Scientist\", December 24, 1964.\n\nIn 1966, Isaacs, Vine, Bradner and Bachus, four American engineers, reinvented the concept, naming it a \"Sky-Hook\", and published their analysis in the journal \"Science\". They decided to determine what type of material would be required to build a space elevator, assuming it would be a straight cable with no variations in its cross section, and found that the strength required would be twice that of any then-existing material including graphite, quartz, and diamond.\n\nIn 1975 an American scientist, Jerome Pearson, reinvented the concept yet again, publishing his analysis in the journal \"Acta Astronautica\". He designed a tapered cross section that would be better suited to building the elevator. The completed cable would be thickest at the geostationary orbit, where the tension was greatest, and would be narrowest at the tips to reduce the amount of weight per unit area of cross section that any point on the cable would have to bear. He suggested using a counterweight that would be slowly extended out to , almost half the distance to the Moon as the lower section of the elevator was built. Without a large counterweight, the upper portion of the cable would have to be longer than the lower due to the way gravitational and centrifugal forces change with distance from Earth. His analysis included disturbances such as the gravitation of the Moon, wind and moving payloads up and down the cable. The weight of the material needed to build the elevator would have required thousands of Space Shuttle trips, although part of the material could be transported up the elevator when a minimum strength strand reached the ground or be manufactured in space from asteroidal or lunar ore.\n\nAfter the development of carbon nanotubes in the 1990s, engineer David Smitherman of NASA/Marshall's Advanced Projects Office realized that the high strength of these materials might make the concept of a space elevator feasible, and put together a workshop at the Marshall Space Flight Center, inviting many scientists and engineers to discuss concepts and compile plans for an elevator to turn the concept into a reality.\n\nIn 2000, another American scientist, Bradley C. Edwards, suggested creating a long paper-thin ribbon using a carbon nanotube composite material. He chose the wide-thin ribbon-like cross-section shape rather than earlier circular cross-section concepts because that shape would stand a greater chance of surviving impacts by meteoroids. The ribbon cross-section shape also provided large surface area for climbers to climb with simple rollers. Supported by the NASA Institute for Advanced Concepts, Edwards' work was expanded to cover the deployment scenario, climber design, power delivery system, orbital debris avoidance, anchor system, surviving atomic oxygen, avoiding lightning and hurricanes by locating the anchor in the western equatorial Pacific, construction costs, construction schedule, and environmental hazards.\n\nTo speed space elevator development, proponents have organized several competitions, similar to the Ansari X Prize, for relevant technologies. Among them are , which organized annual competitions for climbers, ribbons and power-beaming systems from 2005 to 2009, the Robogames Space Elevator Ribbon Climbing competition, as well as NASA's Centennial Challenges program, which, in March 2005, announced a partnership with the Spaceward Foundation (the operator of Elevator:2010), raising the total value of prizes to US$400,000.\nThe first European Space Elevator Challenge (EuSEC) to establish a climber structure took place in August 2011.\n\nIn 2005, \"the LiftPort Group of space elevator companies announced that it will be building a carbon nanotube manufacturing plant in Millville, New Jersey, to supply various glass, plastic and metal companies with these strong materials. Although LiftPort hopes to eventually use carbon nanotubes in the construction of a space elevator, this move will allow it to make money in the short term and conduct research and development into new production methods.\" Their announced goal was a space elevator launch in 2010. On February 13, 2006 the LiftPort Group announced that, earlier the same month, they had tested a mile of \"space-elevator tether\" made of carbon-fiber composite strings and fiberglass tape measuring wide and 1 mm (approx. 13 sheets of paper) thick, lifted with balloons.\n\nIn 2007, held the 2007 Space Elevator games, which featured US$500,000 awards for each of the two competitions, ($1,000,000 total) as well as an additional $4,000,000 to be awarded over the next five years for space elevator related technologies. No teams won the competition, but a team from MIT entered the first 2-gram (0.07 oz), 100-percent carbon nanotube entry into the competition. Japan held an international conference in November 2008 to draw up a timetable for building the elevator.\n\nIn 2008 the book \"Leaving the Planet by Space Elevator\" by Dr. Brad Edwards and Philip Ragan was published in Japanese and entered the Japanese best-seller list. This led to Shuichi Ono, chairman of the Japan Space Elevator Association, unveiling a space-elevator plan, putting forth what observers considered an extremely low cost estimate of a trillion yen (£5 billion / $8 billion) to build one.\n\nIn 2012, the Obayashi Corporation announced that in 38 years it could build a space elevator using carbon nanotube technology. At 200 kilometers per hour, the design's 30-passenger climber would be able to reach the GEO level after a 7.5 day trip. No cost estimates, finance plans, or other specifics were made. This, along with timing and other factors, hinted that the announcement was made largely to provide publicity for the opening of one of the company's other projects in Tokyo.\n\nIn 2013, the International Academy of Astronautics published a technological feasibility assessment which concluded that the critical capability improvement needed was the tether material, which was projected to achieve the necessary strength-to-weight ratio within 20 years. The four-year long study looked into many facets of space elevator development including missions, development schedules, financial investments, revenue flow, and benefits. It was reported that it would be possible to operationally survive smaller impacts and avoid larger impacts, with meteors and space debris, and that the estimated cost of lifting a kilogram of payload to GEO and beyond would be $500.\n\nIn 2014, Google X's Rapid Evaluation R&D team began the design of a Space Elevator, eventually finding that no one had yet manufactured a perfectly formed carbon nanotube strand longer than a meter. They thus decided to put the project in \"deep freeze\" and also keep tabs on any advances in the carbon nanotube field.\n\nIn 2018, researchers at Japan's Shizuoka University launched a mini-elevator consisting of two cube stats and a tether. The prototype was launched as a test bed for a larger structure.\n\nIn 1979, space elevators were introduced to a broader audience with the simultaneous publication of Arthur C. Clarke's novel, \"The Fountains of Paradise\", in which engineers construct a space elevator on top of a mountain peak in the fictional island country of \"Taprobane\" (loosely based on Sri Lanka, albeit moved south to the Equator), and Charles Sheffield's first novel, \"The Web Between the Worlds\", also featuring the building of a space elevator. Three years later, in Robert A. Heinlein's 1982 novel \"Friday\" the principal character makes use of the \"Nairobi Beanstalk\" in the course of her travels. In Kim Stanley Robinson's 1993 novel \"Red Mars\", colonists build a space elevator on Mars that allows both for more colonists to arrive and also for natural resources mined there to be able to leave for Earth. In David Gerrold's 2000 novel, \"Jumping Off The Planet\", a family excursion up the Ecuador \"beanstalk\" is actually a child-custody kidnapping. Gerrold's book also examines some of the industrial applications of a mature elevator technology. In a biological version, Joan Slonczewski's 2011 novel \"The Highest Frontier\" depicts a college student ascending a space elevator constructed of self-healing cables of anthrax bacilli. The engineered bacteria can regrow the cables when severed by space debris.\n\nA space elevator cable rotates along with the rotation of the Earth. Therefore, objects attached to the cable would experience upward centrifugal force in the direction opposing the downward gravitational force. The higher up the cable the object is located, the less the gravitational pull of the Earth, and the stronger the upward centrifugal force due to the rotation, so that more centrifugal force opposes less gravity. The centrifugal force and the gravity are balanced at geosynchronous equatorial orbit (GEO). Above GEO, the centrifugal force is stronger than gravity, causing objects attached to the cable there to pull \"upward\" on it.\n\nThe net force for objects attached to the cable is called the \"apparent gravitational field\". The apparent gravitational field for attached objects is the (downward) gravity minus the (upward) centrifugal force. The apparent gravity experienced by an object on the cable is zero at GEO, downward below GEO, and upward above GEO.\n\nThe apparent gravitational field can be represented this way:\n\nwhere\n\nAt some point up the cable, the two terms (downward gravity and upward centrifugal force) are equal and opposite. Objects fixed to the cable at that point put no weight on the cable. This altitude (r) depends on the mass of the planet and its rotation rate. Setting actual gravity equal to centrifugal acceleration gives:\nOn Earth, this distance is above the surface, the altitude of geostationary orbit.\n\nOn the cable \"below\" geostationary orbit, downward gravity would be greater than the upward centrifugal force, so the apparent gravity would pull objects attached to the cable downward. Any object released from the cable below that level would initially accelerate downward along the cable. Then gradually it would deflect eastward from the cable. On the cable \"above\" the level of stationary orbit, upward centrifugal force would be greater than downward gravity, so the apparent gravity would pull objects attached to the cable \"upward\". Any object released from the cable \"above\" the geosynchronous level would initially accelerate \"upward\" along the cable. Then gradually it would deflect westward from the cable.\n\nHistorically, the main technical problem has been considered the ability of the cable to hold up, with tension, the weight of itself below any given point. The greatest tension on a space elevator cable is at the point of geostationary orbit, above the Earth's equator. This means that the cable material, combined with its design, must be strong enough to hold up its own weight from the surface up to . A cable which is thicker in cross section at that height than at the surface could better hold up its own weight over a longer length. How the cross section area tapers from the maximum at to the minimum at the surface is therefore an important design factor for a space elevator cable.\n\nTo maximize the usable excess strength for a given amount of cable material, the cable's cross section area would need to be designed for the most part in such a way that the stress (i.e., the tension per unit of cross sectional area) is constant along the length of the cable. The constant-stress criterion is a starting point in the design of the cable cross section as it changes with altitude. Other factors considered in more detailed designs include thickening at altitudes where more space junk is present, consideration of the point stresses imposed by climbers, and the use of varied materials. To account for these and other factors, modern detailed cross section designs seek to achieve the largest \"safety margin\" possible, with as little variation over altitude and time as possible. In simple starting-point designs, that equates to constant-stress.\n\nIn the constant-stress case, the cross-section follows this differential equation:\nor\nor\nwhere\n\nThe value of \"g\" is given by the first equation, which yields:\n\nthe variation being taken between \"r\" (ground) and \"r\" (geostationary).\n\nBetween these two points, this quantity can be expressed as:\nor\nwhere formula_1 is the ratio between the centrifugal force on the equator and the gravitational force.\n\nTo compare materials, the \"specific strength\" of the material for the space elevator can be expressed in terms of the \"characteristic length\", or \"free breaking length\": the length of an un-tapered cylindrical cable at which it will break under its own weight under constant gravity. For a given material, that length is formula_2, where formula_3, formula_4 and formula_5 are as defined above.\n\nThe free breaking length needed is given by the equation\nFor a space elevator with a material with formula_6, the section at the synchronous orbit needs to be \"formula_7\" times as much as at ground level. For a space elevator with a material with formula_8, the section at the synchronous orbit needs to be \"formula_9\" times as much as at ground level.\n\nThere are a variety of space elevator designs. Almost every design includes a base station, a cable, climbers, and a counterweight. Earth's rotation creates upward centrifugal force on the counterweight. The counterweight is held down by the cable while the cable is held up and taut by the counterweight. The base station anchors the whole system to the surface of the Earth. Climbers climb up and down the cable with cargo.\n\nModern concepts for the base station/anchor are typically mobile stations, large oceangoing vessels or other mobile platforms. Mobile base stations would have the advantage over the earlier stationary concepts (with land-based anchors) by being able to maneuver to avoid high winds, storms, and space debris. Oceanic anchor points are also typically in international waters, simplifying and reducing cost of negotiating territory use for the base station.\n\nStationary land based platforms would have simpler and less costly logistical access to the base. They also would have an advantage of being able to be at high altitude, such as on top of mountains. In an alternate concept, the base station could be a tower, forming a space elevator which comprises both a compression tower close to the surface, and a tether structure at higher altitudes. Combining a compression structure with a tension structure would reduce loads from the atmosphere at the Earth end of the tether, and reduce the distance into the Earth's gravity field the cable needs to extend, and thus reduce the critical strength-to-density requirements for the cable material, all other design factors being equal.\n\nA space elevator cable would need to carry its own weight as well as the additional weight of climbers. The required strength of the cable would vary along its length. This is because at various points it would have to carry the weight of the cable below, or provide a downward force to retain the cable and counterweight above. Maximum tension on a space elevator cable would be at geosynchronous altitude so the cable would have to be thickest there and taper carefully as it approaches Earth. Any potential cable design may be characterized by the taper factor – the ratio between the cable's radius at geosynchronous altitude and at the Earth's surface.\n\nThe cable would need to be made of a material with a large tensile strength/density ratio. For example, the Edwards space elevator design assumes a cable material with a specific strength of at least 100,000 kN/(kg/m). This value takes into consideration the entire weight of the space elevator. An untapered space elevator cable would need a material capable of sustaining a length of of its own weight \"at sea level\" to reach a geostationary altitude of without yielding. Therefore, a material with very high strength and lightness is needed.\n\nFor comparison, metals like titanium, steel or aluminium alloys have breaking lengths of only 20–30 km. Modern fibre materials such as kevlar, fibreglass and carbon/graphite fibre have breaking lengths of 100–400 km. Nanoengineered materials such as carbon nanotubes and, more recently discovered, graphene ribbons (perfect two-dimensional sheets of carbon) are expected to have breaking lengths of 5000–6000 km at sea level, and also are able to conduct electrical power.\n\nFor a space elevator on Earth, with its comparatively high gravity, the cable material would need to be stronger and lighter than currently available materials. For this reason, there has been a focus on the development of new materials that meet the demanding specific strength requirement. For high specific strength, carbon has advantages because it is only the 6th element in the periodic table. Carbon has comparatively few of the protons and neutrons which contribute most of the dead weight of any material. Most of the interatomic bonding forces of any element are contributed by only the outer few electrons. For carbon, the strength and stability of those bonds is high compared to the mass of the atom. The challenge in using carbon nanotubes remains to extend to macroscopic sizes the production of such material that are still perfect on the microscopic scale (as microscopic defects are most responsible for material weakness).\n\nIn 2014, diamond nanothreads were first synthesized. Since they have strength properties similar to carbon nanotubes, diamond nanothreads were quickly seen as candidate cable material as well.\n\nA space elevator cannot be an elevator in the typical sense (with moving cables) due to the need for the cable to be significantly wider at the center than at the tips. While various designs employing moving cables have been proposed, most cable designs call for the \"elevator\" to climb up a stationary cable.\n\nClimbers cover a wide range of designs. On elevator designs whose cables are planar ribbons, most propose to use pairs of rollers to hold the cable with friction.\n\nClimbers would need to be paced at optimal timings so as to minimize cable stress and oscillations and to maximize throughput. Lighter climbers could be sent up more often, with several going up at the same time. This would increase throughput somewhat, but would lower the mass of each individual payload.\nThe horizontal speed, i.e. due to orbital rotation, of each part of the cable increases with altitude, proportional to distance from the center of the Earth, reaching low orbital speed at a point approximately 66 percent of the height between the surface and geostationary orbit, or a height of about 23,400 km. A payload released at this point would go into a highly eccentric elliptical orbit, staying just barely clear from atmospheric reentry, with the periapsis at the same altitude as LEO and the apoapsis at the release height. With increasing release height the orbit would become less eccentric as both periapsis and apoapsis increase, becoming circular at geostationary level.\nWhen the payload has reached GEO, the horizontal speed is exactly the speed of a circular orbit at that level, so that if released, it would remain adjacent to that point on the cable. The payload can also continue climbing further up the cable beyond GEO, allowing it to obtain higher speed at jettison. If released from 100,000 km, the payload would have enough speed to reach the asteroid belt.\n\nAs a payload is lifted up a space elevator, it would gain not only altitude, but horizontal speed (angular momentum) as well. The angular momentum is taken from the Earth's rotation. As the climber ascends, it is initially moving slower than each successive part of cable it is moving on to. This is the Coriolis force: the climber \"drags\" (westward) on the cable, as it climbs, and slightly decreases the Earth's rotation speed. The opposite process would occur for descending payloads: the cable is tilted eastward, thus slightly increasing Earth's rotation speed.\n\nThe overall effect of the centrifugal force acting on the cable would cause it to constantly try to return to the energetically favorable vertical orientation, so after an object has been lifted on the cable, the counterweight would swing back toward the vertical like an inverted pendulum. Space elevators and their loads would be designed so that the center of mass is always well-enough above the level of geostationary orbit to hold up the whole system. Lift and descent operations would need to be carefully planned so as to keep the pendulum-like motion of the counterweight around the tether point under control.\n\nClimber speed would be limited by the Coriolis force, available power, and by the need to ensure the climber's accelerating force does not break the cable. Climbers would also need to maintain a minimum average speed in order to move material up and down economically and expeditiously. At the speed of a very fast car or train of it will take about 5 days to climb to geosynchronous orbit.\n\nBoth power and energy are significant issues for climbers—the climbers would need to gain a large amount of potential energy as quickly as possible to clear the cable for the next payload.\n\nVarious methods have been proposed to get that energy to the climber:\n\nWireless energy transfer such as laser power beaming is currently considered the most likely method, using megawatt powered free electron or solid state lasers in combination with adaptive mirrors approximately wide and a photovoltaic array on the climber tuned to the laser frequency for efficiency. For climber designs powered by power beaming, this efficiency is an important design goal. Unused energy would need to be re-radiated away with heat-dissipation systems, which add to weight.\n\nYoshio Aoki, a professor of precision machinery engineering at Nihon University and director of the Japan Space Elevator Association, suggested including a second cable and using the conductivity of carbon nanotubes to provide power.\n\nSeveral solutions have been proposed to act as a counterweight:\nExtending the cable has the advantage of some simplicity of the task and the fact that a payload that went to the end of the counterweight-cable would acquire considerable velocity relative to the Earth, allowing it to be launched into interplanetary space. Its disadvantage is the need to produce greater amounts of cable material as opposed to using just anything available that has mass.\n\nAn object attached to a space elevator at a radius of approximately 53,100 km would be at escape velocity when released. Transfer orbits to the L1 and L2 Lagrangian points could be attained by release at 50,630 and 51,240 km, respectively, and transfer to lunar orbit from 50,960 km.\n\nAt the end of Pearson's cable, the tangential velocity is 10.93 kilometers per second (6.79 mi/s). That is more than enough to escape Earth's gravitational field and send probes at least as far out as Jupiter. Once at Jupiter, a gravitational assist maneuver could permit solar escape velocity to be reached.\n\nA space elevator could also be constructed on other planets, asteroids and moons.\n\nA Martian tether could be much shorter than one on Earth. Mars' surface gravity is 38 percent of Earth's, while it rotates around its axis in about the same time as Earth. Because of this, Martian stationary orbit is much closer to the surface, and hence the elevator could be much shorter. Current materials are already sufficiently strong to construct such an elevator. Building a Martian elevator would be complicated by the Martian moon Phobos, which is in a low orbit and intersects the Equator regularly (twice every orbital period of 11 h 6 min).\n\nOn the near side of the Moon, the strength-to-density required of the tether of a lunar space elevator exists in currently available materials. A lunar space elevator would be about long. Since the Moon does not rotate fast enough, there is no effective lunar-stationary orbit, but the Lagrangian points could be used. The near side would extend through the Earth-Moon L1 point from an anchor point near the center of the visible part of Earth's Moon.\n\nOn the far side of the Moon, a lunar space elevator would need to be very long—more than twice the length of an Earth elevator—but due to the low gravity of the Moon, could also be made of existing engineering materials.\n\nRapidly spinning asteroids or moons could use cables to eject materials to convenient points, such as Earth orbits; or conversely, to eject materials to send a portion of the mass of the asteroid or moon to Earth orbit or a Lagrangian point. Freeman Dyson, a physicist and mathematician, has suggested using such smaller systems as power generators at points distant from the Sun where solar power is uneconomical.\n\nA space elevator using presently available engineering materials could be constructed between mutually tidally locked worlds, such as Pluto and Charon or the components of binary asteroid 90 Antiope, with no terminus disconnect, according to Francis Graham of Kent State University. However, spooled variable lengths of cable must be used due to ellipticity of the orbits.\n\nThe construction of a space elevator would need reduction of some technical risk. Some advances in engineering, manufacturing and physical technology are required. Once a first space elevator is built, the second one and all others would have the use of the previous ones to assist in construction, making their costs considerably lower. Such follow-on space elevators would also benefit from the great reduction in technical risk achieved by the construction of the first space elevator.\n\nPrior to the work of Edwards in 2000 most concepts for constructing a space elevator had the cable manufactured in space. That was thought to be necessary for such a large and long object and for such a large counterweight. Manufacturing the cable in space would be done in principle by using an asteroid or Near-Earth object for source material. These earlier concepts for construction require a large preexisting space-faring infrastructure to maneuver an asteroid into its needed orbit around Earth. They also required the development of technologies for manufacture in space of large quantities of exacting materials.\n\nSince 2001, most work has focused on simpler methods of construction requiring much smaller space infrastructures. They conceive the launch of a long cable on a large spool, followed by deployment of it in space. The spool would be initially parked in a geostationary orbit above the planned anchor point. A long cable would be dropped \"downward\" (toward Earth) and would be balanced by a mass being dropped \"upward\" (away from Earth) for the whole system to remain on the geosynchronous orbit. Earlier designs imagined the balancing mass to be another cable (with counterweight) extending upward, with the main spool remaining at the original geosynchronous orbit level. Most current designs elevate the spool itself as the main cable is paid out, a simpler process. When the lower end of the cable is long enough to reach the surface of the Earth (at the equator), it would be anchored. Once anchored, the center of mass would be elevated more (by adding mass at the upper end or by paying out more cable). This would add more tension to the whole cable, which could then be used as an elevator cable.\n\nOne plan for construction uses conventional rockets to place a \"minimum size\" initial seed cable of only 19,800 kg. This first very small ribbon would be adequate to support the first 619 kg climber. The first 207 climbers would carry up and attach more cable to the original, increasing its cross section area and widening the initial ribbon to about 160 mm wide at its widest point. The result would be a 750-ton cable with a lift capacity of 20 tons per climber.\n\nFor early systems, transit times from the surface to the level of geosynchronous orbit would be about five days. On these early systems, the time spent moving through the Van Allen radiation belts would be enough that passengers would need to be protected from radiation by shielding, which would add mass to the climber and decrease payload.\n\nA space elevator would present a navigational hazard, both to aircraft and spacecraft. Aircraft could be diverted by air-traffic control restrictions. All objects in stable orbits that have perigee below the maximum altitude of the cable that are not synchronous with the cable would impact the cable eventually, unless avoiding action is taken. One potential solution proposed by Edwards is to use a movable anchor (a sea anchor) to allow the tether to \"dodge\" any space debris large enough to track.\n\nImpacts by space objects such as meteoroids, micrometeorites and orbiting man-made debris pose another design constraint on the cable. A cable would need to be designed to maneuver out of the way of debris, or absorb impacts of small debris without breaking.\n\nWith a space elevator, materials might be sent into orbit at a fraction of the current cost. As of 2000, conventional rocket designs cost about US$25,000 per kilogram (US$11,000 per pound) for transfer to geostationary orbit. Current space elevator proposals envision payload prices starting as low as $220 per kilogram ($100 per pound), similar to the $5–$300/kg estimates of the Launch loop, but higher than the $310/ton to 500 km orbit quoted to Dr. Jerry Pournelle for an orbital airship system.\n\nPhilip Ragan, co-author of the book \"Leaving the Planet by Space Elevator\", states that \"The first country to deploy a space elevator will have a 95 percent cost advantage and could potentially control all space activities.\"\n\nThe International Space Elevator Consortium (ISEC) was formed to promote the development, construction, and operation of a space elevator as \"a revolutionary and efficient way to space for all humanity\". It was formed after the Space Elevator Conference in Redmond, Washington in July 2008 and became an affiliate organization with the National Space Society in August 2013.\n\nISEC coordinates with the two other major societies focusing on space elevators: the Japanese Space Elevator Association and EuroSpaceward. ISEC supports symposia and presentations at the International Academy of Astronautics and the International Astronautical Federation Congress each year. The organization published two issues of a peer-reviewed journal on space elevators called \"CLIMB\".\n\nISEC also conducts one-year studies focusing on individual topics. The process involves experts for one year of discussions on the topic of choice and culminates in a draft report that is presented and reviewed at the ISEC Space Elevator conference workshop. This review of the major conclusions allows input from space elevator enthusiasts as well as other experts. Topics that have concluded are: 2010 - Space Elevator Survivability, Space Debris Mitigation, 2012 - Space Elevator Concept of Operations, 2013 - Design Consideration for Tether Climbers, 2014 - Space Elevator Architectures and Roadmaps. 2015 - Design Characteristics of a Space Elevator Earth Port, 2017 - Design Considerations for the Space Elevator Apex Anchor and GEO Node.\nThe conventional current concept of a \"Space Elevator\" has evolved from a static compressive structure reaching to the level of GEO, to the modern baseline idea of a static tensile structure anchored to the ground and extending to well above the level of GEO. In the current usage by practitioners (and in this article), a \"Space Elevator\" means the Tsiolkovsky-Artsutanov-Pearson type as considered by the International Space Elevator Consortium. This conventional type is a static structure fixed to the ground and extending into space high enough that cargo can climb the structure up from the ground to a level where simple release will put the cargo into an orbit.\n\nSome concepts related to this modern baseline are not usually termed a \"Space Elevator\", but are similar in some way and are sometimes termed \"Space Elevator\" by their proponents. For example, Hans Moravec published an article in 1977 called \"A Non-Synchronous Orbital Skyhook\" describing a concept using a rotating cable. The rotation speed would exactly match the orbital speed in such a way that the tip velocity at the lowest point was zero compared to the object to be \"elevated\". It would dynamically grapple and then \"elevate\" high flying objects to orbit or low orbiting objects to higher orbit.\n\nThe original concept envisioned by Tsiolkovsky was a compression structure, a concept similar to an aerial mast. While such structures might reach space (100 km, 62 mi), they are unlikely to reach geostationary orbit. The concept of a Tsiolkovsky tower combined with a classic space elevator cable (reaching above the level of GEO) has been suggested. Other ideas use very tall compressive towers to reduce the demands on launch vehicles. The vehicle is \"elevated\" up the tower, which may extend as high as above the atmosphere, and is launched from the top. Such a tall tower to access near-space altitudes of has been proposed by various researchers.\n\nOther concepts for non-rocket spacelaunch related to a space elevator (or parts of a space elevator) include an orbital ring, a pneumatic space tower, a space fountain, a launch loop, a skyhook, a space tether, and a buoyant \"SpaceShaft\".\n\n"}
{"id": "24496299", "url": "https://en.wikipedia.org/wiki?curid=24496299", "title": "Tech Valley", "text": "Tech Valley\n\nTech Valley began as a marketing name for the eastern part of the U.S. state of New York, encompassing the Capital District and the Hudson Valley. Originated in 1998 to promote the greater Albany area as a high-tech competitor to regions such as Silicon Valley and Boston, the moniker subsequently grew to represent the counties in New York between IBM's Westchester County plants in the south and the Canada–United States border to the north, and has since evolved to constitute both the technologically-oriented metonym and the geographic territory comprising most of New York State north of New York City. The area's high technology ecosystem is supported by technologically-focused academic institutions including Rensselaer Polytechnic Institute and the State University of New York Polytechnic Institute.\n\nTech Valley grew from encompassing 19 counties straddling both sides of the Adirondack Northway and the New York Thruway, and with heavy state taxpayer subsidy, has experienced significant growth in the computer hardware side of the high-technology industry, with great strides in the nanotechnology sector, digital electronics design, and water- and electricity-dependent integrated microchip circuit manufacturing, involving companies including IBM in Armonk and its Thomas J. Watson Research Center in Yorktown Heights, GlobalFoundries in Malta, and others. As of 2015, venture capital investment in Tech Valley had grown to US$163 million. Westchester County has developed a burgeoning biotechnology sector in the 21st century, with over US$1 billion in planned private investment as of 2016, earning the county the nickname \"Biochester\".\n\nThe name \"Tech Valley\", or \"Techneurial Valley\" as was originally used, is usually credited to Wallace \"Wally\" Altes, a former president of the Albany-Colonie Regional Chamber of Commerce (\"the Chamber\"), while the shortened name from \"Techneurial\" to \"Tech\" was the idea of Jay Burgess. In 1998, the Albany-Colonie Chamber began using Tech Valley as a marketing name for an initial ten-county area centered on New York's Capital District to show in name the merging of entrepreneurial activity and high-tech companies in the region. Tech Valley then evolved into a 19-county region in eastern New York stretching from the Canadian-US border to the northern suburbs of the city of New York. The 19 counties were Albany, Clinton, Columbia, Dutchess, Essex, Franklin, Fulton, Greene, Hamilton, Herkimer, Montgomery, Orange, Rensselaer, Saratoga, Schenectady, Schoharie, Ulster, Warren, and Washington. That region was 15,637 square miles, about 270 miles north-south at its longest and about 80 miles east-west at its widest. In 2010, those 19 counties had a population estimate in 2010 of 2,312,952, a 9.2 percent increase over the 2000 census; population density was 148 people/sq. mile. 51 percent of the population was female, with 48.2 percent male. 88.5 percent of the population was White, 6.2 percent Black, 4.9 percent Latino, 1.5 percent Asian, with a median age of 37.5 years.\n\nFrom the inception of the name, the Chamber stated that it would not limit the label of Tech Valley to just the Capital District; rather, Tech Valley was envisioned as running from IBM's Westchester County plants and headquarters north to Saratoga Springs and west up the Mohawk Valley. Early businesses that used the Tech Valley name helped spread the word, businesses such as Albany Molecular Research Inc. (AMRI) who used the phrase in its job recruitment material, MapInfo Corporation, Tech Valley Communications, Tech Valley Office Interiors, and Tech Valley Homes Real Estate. The first use of the phrase by a business may have been the accounting firm Urbach, Kahn, & Werlin in 1998, which put the Tech Valley name and logo on its postage meter, shortly before that the Chamber had begun instituting a new telephone greeting, \"Albany-Colonie Chamber. Tech Valley. May I help you\".\nAlso in 1998, Rupprecht & Patashnick put \"Made in New York's Tech Valley\" stickers on all its air quality sensors for the Environmental Protection Agency's (EPA) national monitoring network. In 2000, Tech Valley license plates became available, with three numbers and the letters TEC, for a $34.50 fee, they were the first plates in New York that had a website on them- techvalley.org.\n\nInitially, the name Tech Valley was derided as over-enthusiastic self-boosterism, but SEMATECH's decision in 2002 to relocate its headquarters to the University at Albany, SUNY began Tech Valley's rise in the public's perception. In 2004, however, when Bill Gates was asked by an \"Albany Times Union\" reporter what he thought about Tech Valley, he responded that he had no idea where that was; two years later though, $400,000 from the Bill and Melinda Gates Foundation was used to fund the Tech Valley High School.\n\nThe goal of luring a computer chip fabrication plant (chip fab) was one of the earliest goals of, and reasons underlying, the Tech Valley name. The plan to get a chip fab to the Capital District predates the Tech Valley slogan. In 1997, New York set out submissions for possible chip fab sites that it could whittle to 10 sites around the state that would be pre-approved and pre-permitted for a chip plant. Years before that the Rensselaer Polytechnic Institute's RPI Tech Park had been visited by semiconductor companies, but they had chosen not to build. The renewed interest by the region in luring them was spurred by the research centers and training of specialists for the industry by area colleges such as the University at Albany, Rensselaer Polytechnic Institute, and Hudson Valley Community College. Responding to the state's request for potential sites, Rensselaer County proposed the same RPI Tech Park site; Schenectady County proposed two sites, one of which was in Hillside Industrial Park in Niskayuna; Saratoga County proposed two sites; and Albany County proposed three sites, two in Bethlehem and one in Guilderland. The state ultimately decided on 13 sites it would aggressively promote, most of which were in Tech Valley. As one of the thirteen sites chosen, the RPI Tech Park site originally met little opposition from the town of North Greenbush in which it sat. As time progressed opposition grew in response to concerns about potential impacts on traffic and the environment. The RPI Tech Park site, which by October 1999 had become one of only nine sites still being marketed by the state, ended when the North Greenbush town council voted to terminate the review process. A site in Wallkill, Orange County was the first site in Tech Valley and in the entire state to receive pre-approval for a chip fab.\n\nIn 2002, the Saratoga Economic Development Corporation (SEDC) began to tout its proposed tech park, to be named the Luther Forest Technology Campus, as a site for a chip plant. It would be there that GlobalFoundries, a spin-off of Advanced Micro Devices (AMD), decided to build a $4.2 billion chip fab, ground breaking was in July 2009. The State of New York gave nearly $1.4 billion in cash and tax incentives, the largest such package in state history. New York's incentive package was the same as that offered by Russia, China, and Brazil; though it was not the deciding factor it meant that any region not offering the package was out of contention for the fab. The deciding factor on picking Tech Valley was the $5 billion College of Nanoscale Science and Engineering at the SUNY Polytechnic Institute and the resulting \"high-tech ecosystem\" put in place during Governor George Pataki's administration.\n\nThe Tech Valley Chamber Coalition is an organization that is made up of 24 local chambers of commerce from 19 counties of Tech Valley. Those 24 chambers represent over 21,000 businesses, schools, and organizations that employ more than 531,000 workers. It was formed in June 2002 and manages the Tech Valley Portal, and publishes an annual publication called \"Images of Tech Valley\".\n\nThe 24 local chambers are-\n\n\n"}
{"id": "55905084", "url": "https://en.wikipedia.org/wiki?curid=55905084", "title": "The Turing Guide", "text": "The Turing Guide\n\nThe Turing Guide (2017), written by Jack Copeland, Jonathan Bowen, Mark Sprevak, Robin Wilson, and others, is a book about the work and life of the British mathematician, philosopher, and early computer scientist, Alan Turing (1912–1954).\n\nThe book includes 42 contributed chapters by a variety of authors, including some contemporaries of Alan Turing. The book was published in January 2017 by Oxford University Press, in hardcover, paperback, and e-book formats. \n\n\"The Turing Guide\" is divided into eight main parts, covering various aspects of Alan Turing’s life and work:\n\n\nThe book includes a foreword by Andrew Hodges, preface, notes on the contributors, endnotes, and an index.\n\nThe following 33 authors contributed to chapters in the book:\nThe book has been reviewed in a number of publications and online, including:\n\n\n\n"}
{"id": "839260", "url": "https://en.wikipedia.org/wiki?curid=839260", "title": "Venus Express", "text": "Venus Express\n\nVenus Express (VEX) was the first Venus exploration mission of the European Space Agency (ESA). Launched in November 2005, it arrived at Venus in April 2006 and began continuously sending back science data from its polar orbit around Venus. Equipped with seven scientific instruments, the main objective of the mission was the long term observation of the Venusian atmosphere. The observation over such long periods of time had never been done in previous missions to Venus, and was key to a better understanding of the atmospheric dynamics. It was hoped that such studies can contribute to an understanding of atmospheric dynamics in general, while also contributing to an understanding of climate change on Earth. ESA concluded the mission in December 2014.\n\nThe mission was proposed in 2001 to reuse the design of the \"Mars Express\" mission. However, some mission characteristics led to design changes: primarily in the areas of thermal control, communications and electrical power. For example, since Mars is approximately twice as far from the Sun as Venus, the radiant heating of the spacecraft is four times greater for \"Venus Express\" than \"Mars Express\". Also, the ionizing radiation environment is harsher. On the other hand, the more intense illumination of the solar panels results in more generated photovoltaic power. The \"Venus Express\" mission also uses some spare instruments developed for the \"Rosetta\" spacecraft. The mission was proposed by a consortium led by D. Titov (Germany), E. Lellouch (France) and F. Taylor (United Kingdom).\n\nThe launch window for \"Venus Express\" was open from 26 October to 23 November 2005, with the launch initially set for 26 October 4:43 UTC. However, problems with the insulation from the Fregat upper stage led to a two-week launch delay to inspect and clear out the small insulation debris that migrated on the spacecraft. It was eventually launched by a Soyuz-FG/Fregat rocket from the Baikonur Cosmodrome in Kazakhstan on 9 November 2005 at 03:33:34 UTC into a parking Earth orbit and 1 h 36 min after launch put into its transfer orbit to Venus. A first trajectory correction maneuver was successfully performed on 11 November 2005. It arrived at Venus on 11 April 2006, after 153 days of journey, and fired its main engine between 07:10:29 and 08:00:42 UTC SCET to reduce its velocity so that it could be captured by Venusian gravity into a nine-day orbit of . The burn was monitored from ESA's Control Centre, ESOC, in Darmstadt, Germany.\n\nSeven further orbit control maneuvers, two with the main engine and five with the thrusters, were required for \"Venus Express\" to reach its final operational 24-hour orbit around Venus.\n\n\"Venus Express\" entered its target orbit at apoapsis on 7 May 2006 at 13:31 UTC, when the spacecraft was from Earth. At this point the spacecraft was running on an ellipse substantially closer to the planet than during the initial orbit. The polar orbit ranged between over Venus. The periapsis was located almost above the North pole (80° North latitude), and it took 24 hours for the spacecraft to travel around the planet.\n\n\"Venus Express\" studied the Venusian atmosphere and clouds in detail, the plasma environment and the surface characteristics of Venus from orbit. It is also made global maps of the Venusian surface temperatures. Its nominal mission was originally planned to last for 500 Earth days (approximately two Venusian sidereal days), but the mission was extended five times: first on 28 February 2007 until early May 2009; then on 4 February 2009 until 31 December 2009; and then on 7 October 2009 until 31 December 2012. On 22 November 2010, the mission was extended to 2014. On 20 June 2013, the mission was extended a final time until 2015.\n\nOn 28 November 2014, mission control lost contact with \"Venus Express\". Intermittent contact was reestablished on 3 December 2014, though there was no control over the spacecraft, likely due to exhaustion of propellant. On 16 December 2014, ESA announced that the \"Venus Express\" mission had ended. A carrier signal was still being received from the vehicle, but no data was being transmitted. Mission manager Patrick Martin expected the spacecraft would fall below in early January 2015, with destruction occurring in late January or early February. The spacecraft's carrier signal was last detected by ESA on 18 January 2015.\n\nASPERA-4: An acronym for \"Analyzer of Space Plasmas and Energetic Atoms,\" ASPERA-4 investigated the interaction between the solar wind and the Venusian atmosphere, determine the impact of plasma processes on the atmosphere, determine global distribution of plasma and neutral gas, study energetic neutral atoms, ions and electrons, and analyze other aspects of the near Venus environment. ASPERA-4 is a re-use of the ASPERA-3 design used on \"Mars Express\", but adapted for the harsher near-Venus environment.\n\nVMC: The Venus Monitoring Camera is a wide-angle, multi-channel CCD. The VMC is designed for global imaging of the planet. It operates in the visible, ultraviolet, and near infrared spectral ranges, and maps surface brightness distribution searching for volcanic activity, monitoring airglow, studying the distribution of unknown ultraviolet absorbing phenomenon at the cloud-tops, and making other science observations. It is derived in part by the \"Mars Express\" High Resolution Stereo Camera (HRSC) and the \"Rosetta\" Optical, Spectroscopic and Infrared Remote Imaging System (OSIRIS). The camera includes an FPGA to pre-process image data, reducing the amount transmitted to Earth. The consortium of institutions responsible for the VMC includes the Max Planck Institute for Solar System Research, the Institute of Planetary Research at the German Aerospace Center and the Institute of Computer and Communication Network Engineering at Technische Universität Braunschweig. It is not to be confused with Visual Monitoring Camera mounted on \"Mars Express\", of which it is an evolution.\n\nMAG: The magnetometer is designed to measure the strength of Venus's magnetic field and the direction of it as affected by the solar wind and Venus itself. It mapped the magnetosheath, magnetotail, ionosphere, and magnetic barrier in high resolution in three-dimensions, aid ASPERA-4 in the study of the interaction of the solar wind with the atmosphere of Venus, identify the boundaries between plasma regions, and carry planetary observations as well (such as the search for and characterization of Venus lightning). MAG is derived from the \"Rosetta\" lander's ROMAP instrument.\n\nOne measuring device is placed at the surface of the sonde, the identical second of the pair is placed a necessary distance off the body of the sonde by unfolding a 1 m long boom (carbon composite tube). Two redundant pyrotechnical cutters cut one loop of thin rope to free the power of metal springs. The driven knee lever rotates the boom perpendicularly outwards and latches it in place. Only the use of a pair of sensors together with the rotation of the sonde allows to resolve the small natural magnetic field beneath the fields of the disturbing fields of the probe itself. The measurements to identify the fields produced by the craft took place on the route from Earth to Venus. The lack of magnetic cleanness was due to the reuse of the \"Mars Express\" spacecraft bus which did not carry a magnetometer. By combining the data from two-point simultaneous measurements and using software to identify and remove interference generated by \"Venus Express\" itself, it was possible to obtain results of a quality comparable to those produced by a magnetically clean craft.\n\nPFS: The \"Planetary Fourier Spectrometer\" (PFS) operates in the infrared between the 0.9 µm and 45 µm wavelength range and is designed to perform vertical optical sounding of the Venus atmosphere. It performed global, long-term monitoring of the three-dimensional temperature field in the lower atmosphere (cloud level up to 100 kilometers). Furthermore, it searched for minor atmospheric constituents that may be present, but had not yet been detected, analyzed atmospheric aerosols, and investigated surface to atmosphere exchange processes. The design is based on a spectrometer on \"Mars Express\", but modified for optimal performance for the \"Venus Express\" mission.\n\nSPICAV: The \"SPectroscopy for Investigation of Characteristics of the Atmosphere of Venus\" (SPICAV) is an imaging spectrometer that was used for analyzing radiation in the infrared and ultraviolet wavelengths. It is derived from the \"SPICAM\" instrument flown on \"Mars Express\". However, SPICAV has an additional channel known as SOIR (Solar Occultation at Infrared) that was used to observe the Sun through Venus's atmosphere in the infrared.\n\nVIRTIS: The \"Visible and Infrared Thermal Imaging Spectrometer\" (VIRTIS) is an imaging spectrometer that observes in the near-ultraviolet, visible, and infrared parts of the electromagnetic spectrum. It analyzed all layers of the atmosphere, surface temperature and surface/atmosphere interaction phenomena.\n\nVeRa: Venus Radio Science is a radio sounding experiment that transmitted radio waves from the spacecraft and passed them through the atmosphere or reflected them off the surface. These radio waves were received by a ground station on Earth for analysis of the ionosphere, atmosphere and surface of Venus. It is derived from the Radio Science Investigation instrument flown on \"Rosetta\".\n\nStarting out in the early planetary system with similar sizes and chemical compositions, the histories of Venus and Earth have diverged in spectacular fashion. It is hoped that the \"Venus Express\" mission data that was obtained can contribute not only to an in-depth understanding of how the Venusian atmosphere is structured, but also to an understanding of the changes that led to the current greenhouse atmospheric conditions. Such an understanding may contribute to the study of climate change on Earth.\n\n\"Venus Express\" was also used to observe signs of life on Earth from Venus orbit. In images acquired by the probe, Earth was less than one pixel in size, which mimics observations of Earth-sized planets in other planetary systems. These observations were then used to develop methods for habitability studies of exoplanets.\n\nImportant events for \"Venus Express\" include:\n\n\n"}
{"id": "2795558", "url": "https://en.wikipedia.org/wiki?curid=2795558", "title": "Virtual tour", "text": "Virtual tour\n\nA virtual tour is a simulation of an existing location, usually composed of a sequence of videos or still images. It may also use other multimedia elements such as sound effects, music, narration, and text. It is distinguished from the use of live television to affect tele-tourism.\n\nThe phrase \"virtual tour\" is often used to describe a variety of videos and photographic-based media. Panorama indicates an unbroken view, since a panorama can be either a series of photographs or panning video footage. However, the phrases \"panoramic tour\" and \"virtual tour\" have mostly been associated with virtual tours created using still cameras. Such virtual tours are made up of a number of shots taken from a single vantage point. The camera and lens are rotated around what is referred to as a no parallax point (the exact point at the back of the lens where the light converges).\n\nA video tour is a full motion video of a location. Unlike the virtual tour's static wrap-around feel, a video tour is a linear walk-through of a location. Using a video camera, the location is filmed at a walking pace while moving continuously from one point to another throughout the subject location.\n\nThe origin of the term 'virtual tour' dates to 1994. The first example of a virtual tour was a museum visitor interpretive tour, consisting of 'walk-through' of a 3D reconstruction of Dudley Castle in England as it was in 1550. This consisted of a computer controlled laserdisc based system designed by British-based engineer Colin Johnson.\n\nOne of the first users of a virtual tour was Queen Elizabeth II, when she officially opened the visitor centre in June 1994. Because the Queen's officials had requested titles, descriptions and instructions of all activities, the system was named and described as: \"Virtual Tour, being a cross between Virtual Reality and Royal Tour.\" Details of the original project can be viewed online. The system was featured in a conference held by the British Museum in November 1994 and in a subsequent technical paper.\n\nThere are several ways of stitching virtual tours together.\n\nThis involves the rotation of a digital camera, typically in the portrait (up and down) position and centered directly over the tripod. As the operator manually rotates the camera clockwise, the camera stops or clicks into a detent at regular intervals, such as every 30° of rotation. The rotator can be adjusted by changing the position of \"detent ring or bolt,\" into another slot, to alter the interval of rotation: 40°, 60°, 90° etc.\n\nIf a given camera lens supports a wider view, one could select a larger detent value (for example, 60° instead of 30°). With a larger detent interval, fewer images are needed to capture a complete panoramic scene. The photographer may only need to take 6 shots as opposed to 10 shots to capture the same panorama. The combination of a precision rotator and a digital camera allows the photographer to take rectangular \"slices\" of any scene (indoors or outdoors). With a typical point and shoot digital camera, the photographer will snap 8, 10, 12 or 14 slices of a scene. Using specialized \"photo stitching\" software, the operator then assembles the \"slices\" into a single rectangular image, typically 4,500 pixels to 6,000 pixels wide. This technique, while extremely time consuming, has remained popular even through today as the required equipment, rotator heads and software are relatively inexpensive and easy to learn. A stitched panoramic view is also called \"cylindrical\"—as the resulting stitched panorama allows panning in a complete 360°, but offers a limited vertical field of about 50° degrees above or below the horizon line.\n\nThis method requires the use of a \"Fisheye lens\" lens equipped digital SLR camera. The 2-shot fish eye camera system was made popular by IPiX in the mid-1990s and a two-shot rotator head that rotated and locked into 0° and 180° positions only. The camera was an Olympus or Nikon CoolPix camera and the lenses used were the Nikon FC-E8 or FC-E9 fish-eye lens. The IPiX 360 camera system enabled photographers to capture a full 360 X 360 floor to ceiling view of any scene with just 4 shots as opposed to the more time consuming 8, 10, or 12-shot rectilinear produced panoramas described above. This type of virtual tour required more expensive virtual tour camera equipment including (for example) a Sigma 8mm f/3.5 lens which allowed photographers to set their rotator heads to 90° and capture a complete virtual tour of any scene in just 4 shots (0°, 90°, 180°, 270°).\n\nThis technique was one of the first forms of immersive, floor to ceiling virtual tours. Apple Computer pioneered this with the release of Apple's QuickTime VR in the early 1990s. Free utility software such as Cubic Converter and others allowed photographers to stitch and convert their panoramas into a \"cube\" like box to achieve a complete 360 X 360 view. Today, this technique is considered rather \"old school,\" and spherical stitching has become more mainstream for producing these types of tours.\n\nUsing one-shot panoramic optics one can create quick and easy panoramic videos and images such as the type used on the iPhone.\n\nWhile programs such as Adobe Photoshop have new features that allow users to stitch images together, they only support \"rectilinear\" types of stitching. Photoshop cannot produce them as quickly or accurately as stitching software programs can such as Autodesk Stitcher. This is because there is sophisticated math and camera-lens profiles that are needed to create the desired panorama image which is based on your camera's depth of field (FOV) and the type of lens used. Cameras such as the Nikon D3 or D700 have a full frame digital SLR cameras, whereas the Nikon D90 or Canon T2i (Rebel line of Digital EOS cameras) have a smaller sensor. When full frame digital SLR cameras are used with a fish eye lens such as a Sigma 8mm F/3.5, a full circular image is captured. This allows you to shoot 2 or 3 shots per view to create a 360 X 360 stitched panoramic image. When used with a non full frame digital SLR camera like the Nikon D90 or Canon digital Rebel and similar cameras, 4-shots are required with the camera in the portrait position. The resulting image will have the left and right sides cropped off each of the 4 images and each of the four corners, creating a rounded image.\n\nWith the expansion of video on the internet, video-based virtual tours are growing in popularity. Video cameras are used to pan and walk-through real subject properties. The benefit of this method is that the point of view is constantly changing throughout a pan. However, capturing high-quality video requires significantly more technical skill and equipment than taking digital still pictures. Video also eliminates viewer control of the tour. Therefore, the tour is the same for all viewers and subject matter is chosen by the videographer. Editing digital video requires proficiency with video editing software and has higher computer hardware requirements. Also, displaying video over the internet requires more bandwidth. Due to these difficulties, the task of creating video-based tours is often left to professionals.\n\nRecently different groups have been using Google's system to provide access to private areas, which were previously unavailable to the general public.\n\nVarious software products can be used to create virtual tours.\n\nVirtual tours are used extensively for universities and in the real estate and hospitality industries. Virtual tours can allow a user to view an environment while on-line. Currently a variety of industries use such technology to help market their services and product. Over the last few years the quality and accessibility of virtual tours has improved considerably, with some websites allowing the user to navigate the tours by clicking on maps or integrated floor plans.\n\nFor most business purposes, a virtual tour must be accessible from everywhere. The major solution is a web-based virtual tour. In addition, a rich and useful virtual tour is not just a series of panoramic pictures. A better experience can be obtained by viewing a variety of materials such as that obtained from videos, texts, and still pictures in an interactive web content. There are many ways to gather data in a mixed web content, such as using rich content builders (Java applet or Adobe Flash being two examples) or a Web content management system.\n\nFlash-based tours are becoming very popular today. A study done by the PEW Research Group showed that more than 5 million Americans watched virtual tours every day in 2004. PEW's research data which showed that Americans watching virtual tours rose from 54 million people in 2004 to 72 million people by August 2006, a two-year increase of 18 million.\n\nThanks in part to the recent explosion of many Internet devices, such as Apple's iPad, iPhone and other tablet computing platforms powered entirely by Google's Android 3 operating systems such as Motorola's Xoom, it can be predicted that consumption of virtual tour content, through the use of Adobe Flash and HTML5/CSS3 driven virtual tours will only increase over time.\n\nVirtual tours are very popular in the real estate industry. Several types of such tours exist, including simple options such as interactive floor plans, and more sophisticated options such as full-service virtual tours. An interactive floor plan shows photographs of a property with the aid of a floor plan and arrows to indicate where each photograph was taken. Clicking on arrows shows the user where the camera was and which way the camera was pointing. Full service virtual tours are usually created by a professional photographer who will visit the property being sold, take several photos, and run them through stitching software. Matterport offers 3D camera services to create virtual tours. Full service virtual tours are usually more expensive than interactive floor plans because of the expense of the photographer, higher-end equipment used, such as a digital SLR camera, and specialized software. Real estate virtual tours are typically linked to the listing in the Multiple Listing Service.\n\n3D virtual tour technology has been increasingly used in the documentation and preservation of historic properties that are at risk of being razed or undergoing restricted public access. 3D virtual models using standard file formats, such as the Object file (.obj) format, can be stored in digital archives for future academic research and exploration.\n\nVirtual tours are also popular in the hospitality industry. Hotels are increasingly offering online tours on their websites, ranging from \"360\" stitched photos to professionally produced video tours. These tours are typically offered by hotels in an effort to increase booking revenue by providing online viewers with an immersive view of the property and its amenities.\n\nVirtual walk videos are documentary motion pictures shot as the camera continuously moves forward through an urban or natural area. The effect is to allow viewers to experience the sights they would see and the sounds they would hear were they actually traveling along a particular route at the same pace as the camera. Virtual walks based on real-world photography typically do not require the use of virtual reality goggles or headsets of the kind used by gamers.\n\nIn realistically simulating the experience of moving through space, virtual walks—or virtual runs or bicycle rides—differ from conventional travel videos, which typically consist of a sequence of mostly static camera setups along a particular route or within a given area. The advantage of the conventional travel video is that one or more narrators or on-screen guides can provide insights into the geographical, historical, political, military, cultural, geological, or architectural aspects of the area.\n\nVirtual walks appeal to those who want to experience the sights and sounds of particular places in the country or the world, but who may not have the time or the financial or physical resources to actually travel there. They also appeal to treadmill or elliptical trainer users, for whom walking or running while watching these videos enhances the reality of the experience (and, at a minimum, reduce the boredom of the exercise).\n\nSome feature-length narrative motion pictures have made use of the virtual walk technique for dramatic purposes. These include the opening sequences of Orson Welles’ \"Touch of Evil\" and Robert Altman’s \"The Player\", the famous tracking shot through the Copacabana in Martin Scorcese’s \"Goodfellas\", Alexander Sokurov’s \"Russian Ark\" (which consists of a single 96-minute Steadicam take), and, more recently Alfonso Cuarón’s long tracking shots in \"Gravity\", and almost the entire narrative structure of Alejandro Gonzáles Iñárrito’s \"Birdman\".\n\n"}
{"id": "2932442", "url": "https://en.wikipedia.org/wiki?curid=2932442", "title": "Visual design elements and principles", "text": "Visual design elements and principles\n\nVisual design elements and principles describe fundamental ideas about the practice of visual design.\n\nDesign elements are the basic units of any visual design which form its structure and convey visual messages. Painter and design theorist Maitland E. Graves (1902-1978), who attempted to gestate the fundamental principles of aesthetic order in visual design, in his book, \"The Art of Color and Design\" (1941), defined the elements of design as line, direction, shape, size, texture, value, and color, concluding that \"these elements are the materials from which all designs are built.\"\n\nColor is the result of light reflecting back from an object to our eyes. The color that our eyes perceive is determined by the pigment of the object itself. Color theory and the color wheel are often referred to when studying colour combinations in visual design. Color is often deemed to be an important element of design as it is a universal language which presents the countless possibilities of visual communication. \n\nHue, saturation, and brightness are the three characteristics that describe color.\n\n\nColor theory studies colour mixing and colour combinations. It is one of the first things that marked a progressive design approach. In visual design, designers refer to color theory as a body of practical guidance to achieving certain visual impacts with specific colour combinations. Theoretical color knowledge is implemented in designs in order to achieve a successful color design.\n\n\nColor harmony, often referred to as a \"measure of aesthetics\", studies which colour combinations are harmonious and pleasing to the eye, and which colour combinations are not. Color harmony is a main concern for designers given that colors always exist in the presence of other colors in form or space.\n\nWhen a designer harmonizes colours, the relationships among a set of colours are enhanced to increase the way they compliment one another. Colors are harmonized to achieve a balanced, unified, and aesthetically pleasing effect for the viewer. \n\nColor harmony is achieved in a variety of ways, some of which consist of combining a set of colors that share the same hue, or a set of colors that share the same values for two of the three color characteristics (hue, saturation, brightness). Colour harmony can also be achieved by simply combining colors that are considered compatible to one another as represented in the color wheel.\n\n\nColor contrasts are studied with a pair of colours, as opposed to color harmony, which studies a set of colours. In color contrasting, two colors with perceivable differences in aspects such as luminance, or saturation, are placed side by side to create contrast. \n\nJohannes Itten presented seven kinds of color contrasts: contrast of light and dark, contrast of hue, contrast of temperature, contrast of saturation, simultaneous contrast, contrast of sizes, and contrast of complementary. These seven kinds of color contrasts have inspired past works involving color schemes in design.\n\n\nColor schemes are defined as the set of colors chosen for a design. They are often made up of two or more colors that look appealing beside one another, and that create an aesthetic feeling when used together. Color schemes depend on color harmony as they point to which colors look pleasing beside beside one another. \n\nA satisfactory design product is often accompanied by a successful color scheme. Over time, color design tools with the function of generating color schemes were developed to facilitate color harmonizing for designers.\n\nLine is an element of art defined by a point moving in space. Lines can be vertical, horizontal, diagonal or curved. They can be any width or texture. And can be continuous, implied, or broken. On top of that, there are different types of line, aside from the ones previously mentioned. For example, you could have a line that is \nhorizontal and zigzagged or a line that is vertical and zigzagged. Different lines create different moods, it all depends on what mood you are using a line to create.\n\nA point is basically the beginning of “something” in “nothing”. It forces the mind to think upon its position and gives something to build upon in both imagination and space. Some abstract points in a group can provoke human imagination to link it with familiar shapes or forms\n\nA shape is defined as a two or more dimensional area that stands out from the space next to or around it due to a defined or implied boundary, or because of differences of value, color, or texture. Shapes are recognizable objects and forms and are usually composed of other elements of design. \n\nFor example, a square that is drawn on a piece of paper is considered a shape. It is created with a series of lines which serve as a boundary that shapes the square and separates it from the space around it that is not part of the square. \n\nGeometric shapes or mechanical shapes are shapes that can be drawn using a ruler or compass, such as squares, circles, triangles, ellipses, parallelograms, stars, and so on. Mechanical shapes, whether simple or complex, produce a feeling of control and order.\n\nOrganic shapes are irregular shapes that are often complex and resemble shapes that are found in nature. Organic shapes can be drawn by hand, which is why they are sometimes subjective and only exist in the imagination of the artist.\n\nCurvilinear shapes are composed of curved lines and smooth edges. They give off a more natural feeling to the shape. In contrast, rectilinear shapes are composed of sharp edges and right angles, and give off a sense of order in the composition. They look more manmade, structured, and artificial. Artists can choose to create a composition that revolves mainly around one of these styles of shape, or they can choose to combine both.\n\nTexture refers to the physical and visual qualities of a surface. \n\n\n\nTactile texture, also known as \"actual texture\", refers to the physical three-dimensional texture of an object. Tactile texture can be perceived by the sense of touch. A person can feel the tactile texture of a sculpture by running their hand over its surface and feelings its ridges and dents. \n\n\nVisual texture, also referred to as \"implied texture\", is not detectable by our sense of touch, but by our sense of sight. Visual texture is the illusion of a real texture on a two-dimensional surface. Any texture perceived in an image or photograph is a visual texture. A photograph of rough tree bark is considered a visual texture. It creates the impression of a real texture on a two-dimensional surface which would remain smooth to the touch no matter how rough the represented texture is..\n\nIn painting, different paints are used to achieve different types of textures. Paints such as oil, acrylic, and encaustic are thicker and more opaque and are used to create three-dimensional impressions on the surface. Other paints, such as watercolor, tend to be used for visual textures, because they are thinner and have transparency, and do not leave much tactile texture on the surface.\n\nMany textures appear to repeat the same motif. When a motif is repeated over and over again in a surface, it results in a pattern. Patterns are frequently used in fashion design or textile design, where motifs are repeated to create decorative patterns on fabric or other textile materials. Patterns are also used in architectural design, where decorative structural elements such as windows, columns, or pediments, are incorporated into building design.\n\nIn design, space is concerned with the area deep within the moment of designated design, the design will take place on. For a two-dimensional design, space concerns creating the illusion of a third dimension on a flat surface:\n\n\nIn visual design, form is described as the way an artist arranges elements in the entirety of a composition. It may also be described as any three-dimensional object. Form can be measured, from top to bottom (height), side to side (width), and from back to front (depth). Form is also defined by light and dark. It can be defined by the presence of shadows on surfaces or faces of an object. There are two types of form, geometric (man-made) and natural (organic form). Form may be created by the combining of two or more shapes. It may be enhanced by tone, texture or color. It can be illustrated or constructed.\n\nPrinciples applied to the elements of design that bring them together into one design. How one applies these principles determines how successful a design may be.\n\nAccording to Alex White, author of \"The Elements of Graphic Design\", to achieve visual unity is a main goal of graphic design. When all elements are in agreement, a design is considered unified. No individual part is viewed as more important than the whole design. A good balance between unity and variety must be established to avoid a chaotic or a lifeless design.\n\n\n\nIt is a state of equalized tension and equilibrium, which may not always be calm.\n\n\nA good design contains elements that lead the reader through each element in order of its significance. The type and images should be expressed starting from most important to the least important.\n\nUsing the relative size of elements against each other can attract attention to a focal point. When elements are designed larger than life, scale is being used to show drama.\n\nDominance is created by contrasting size, positioning, color, style, or shape.The focal point should dominate the design with scale and contrast without sacrificing the unity of the whole.\n\nPlanning a consistent and similar design is an important aspect of a designer's work to make their focal point visible. Too much similarity is boring but without similarity important elements will not exist and an image without contrast is uneventful so the key is to find the balance between similarity and contrast.\n\nThere are several ways to develop a similar environment:\n\n\nMovement is the path the viewer’s eye takes through the artwork, often to focal areas. Such movement can be directed along lines edges, shape and color within the artwork, and more.\n\n\n\n"}
{"id": "21362362", "url": "https://en.wikipedia.org/wiki?curid=21362362", "title": "Vogart Crafts Corporation", "text": "Vogart Crafts Corporation\n\nVogart Crafts Corporation, based in New York City, was best known for designing and manufacturing iron-on embroidery transfer designs.\n\nIt was a United States corporation in business from about 1930 until around 1990, when its parent corporation, Pioneer Systems, Inc., caused it to file for bankruptcy under Chapter 11. \n\nEli J. Segal served as Vogart Crafts Corporation's CEO for a period of time, until his resignation in 1981. \n\n"}
{"id": "15865266", "url": "https://en.wikipedia.org/wiki?curid=15865266", "title": "Waru Waru", "text": "Waru Waru\n\nWaru Waru is an agricultural technique developed by pre Hispanic people in the Andes region of South America, from Colombia to Bolivia. It is dated around 300 B.C.\n\nThe technique has been revived in 1984, in Tiwanaku, Bolivia as well as Puno, Peru.\n\nThe technique combines raised beds with irrigation channels to prevent damage by soil erosion during floods. The technique ensures both collecting of water (either fluvial water, rainwater or phreatic water) and subsequent drainage. The drainage aspect makes it particularly interesting for many areas subjected to risks of brutal floods, such as tropical parts of Bolivia and Peru where it emerged. Waru Waru has been used in many countries, like China.\n\n"}
{"id": "1048596", "url": "https://en.wikipedia.org/wiki?curid=1048596", "title": "Wine press", "text": "Wine press\n\nA wine press is a device used to extract juice from crushed grapes during wine making. There are a number of different styles of presses that are used by wine makers but their overall functionality is the same. Each style of press exerts controlled pressure in order to free the juice from the fruit (most often grapes). The pressure must be controlled, especially with grapes, in order to avoid crushing the seeds and releasing a great deal of undesirable tannins into the wine. Wine was being made at least as long ago as 4000 BC; in 2011, a wine press was unearthed in Armenia with red wine dated 6,000 years old.\n\nA basket press consists of a large basket filled with the crushed grapes. Pressure is applied through a plate that is forced down onto the fruit. The mechanism to lower the plate is often either a screw or a hydraulic device. The juice flows through openings in the basket. The basket style press was the first type of mechanized press to be developed, and its basic design has not changed in nearly 1000 years.\n\nA horizontal screw press works using the same principle as the basket press. Instead of a plate being brought down to put pressure on the grapes, plates from either side of a closed cylinder are brought together to squeeze the grapes. Generally the volume of grapes handled is significantly greater than that of a basket press.\n\nA bladder press consists of a large cylinder, closed at each end, into which the fruit is loaded. To press the grapes, a large bladder expands and pushes the grapes against the sides. The juice then flows out through small openings in the cylinder. The cylinder rotates during the process to help homogenize the pressure that is placed on the grapes.\n\nA continuous screw press differs from the above presses in that it does not process a single batch of grapes at a time. Instead it uses an Archimedes' screw to continuously force grapes up against the wall of the device. Juice is extracted, and the pomace continues through to the end where it is extracted. This style of press is rarely used to produce table wines, and some countries forbid its use for higher quality wines.\n\nFlash release is a technique used in wine pressing. The technique allows for a better extraction of phenolic compounds.\n\n"}
{"id": "1868929", "url": "https://en.wikipedia.org/wiki?curid=1868929", "title": "Wobbe index", "text": "Wobbe index\n\nThe Wobbe Index (WI) or Wobbe number is an indicator of the interchangeability of fuel gases such as natural gas, liquefied petroleum gas (LPG), and town gas and is frequently defined in the specifications of gas supply and transport utilities.\n\nIf formula_1 is the higher heating value, or higher calorific value, and formula_2 is the specific gravity, the Wobbe Index, formula_3, is defined as:\n\nThe Wobbe Index is used to compare the combustion energy output of different composition fuel gases in an appliance (fire, cooker etc.). If two fuels have identical Wobbe Indices then for given pressure and valve settings the energy output will also be identical. Typically variations of up to 5% are allowed as these would not be noticeable to the consumer.\n\nThe Wobbe Index is a critical factor to minimise the impact of the changeover when analyzing the use of \"substitute natural gas\" (SNG) fuels such as propane-air mixtures. The Wobbe Index also requires the addition of propane to some upgraded biomethane products, particularly in regions where natural gas has a high calorific value such as Sweden \n\nThe Wobbe index is expressed in MJ/Nm³ (or sometimes in BTU/scf). In the case of natural gas (molar mass 17 g/mol), the typical heating value is around 39 MJ/Nm³ (1,050 BTU/scf) and the specific gravity is approximately 0.59, giving a typical Wobbe index of 51 MJ/Nm³ (1,367 BTU/scf).\n\nThere are three ranges or \"families\" of fuel gases that have been internationally agreed based on Wobbe index. Family 1 covers manufactured gases, family 2 covers natural gases (with high and low ranges) and family 3 covers liquefied petroleum gas (LPG). Combustion equipment is typically designed to burn a fuel gas within a particular family: hydrogen-rich town gas, natural gas or LPG.\nOther flame characteristics and composition limits may determine the acceptability of the replacement gas, e.g. flame speed, \"yellow tipping\" due to incomplete combustion, sulfur content, oxygen content, etc.\n\nIn spite of its usefulness, Wobbe index alone is not a good indicator of the interchangeability of two or more gases, or mixtures of them. It is necessary to bear in mind other criteria while determining the plenty substitution of a fuel by other, different of the one used to adjust the burning system.\n"}
{"id": "15686634", "url": "https://en.wikipedia.org/wiki?curid=15686634", "title": "World's largest weather vane", "text": "World's largest weather vane\n\nAccording to the Guinness World Records, the world's largest weather vane is located in Jerez, Spain.\n\nAccording to the explore north website a challenger for the title of world's largest weather vane is located in Whitehorse, Yukon. The weather vane is a retired Douglas DC-3 atop a swiveling support. Located beside the Whitehorse airport the weather vane is used mainly by pilots to determine wind direction. This weather vane only requires a 5 km/hour wind to rotate.\n\nOther claimants to the title are located in:\n\nWestlock, Alberta, Canada: The Canadian Tractor Museum is home to the tallest functioning weather vane at 50 feet. This weather vane was erected in 2006 with a 1942 Case Model D Tractor atop it after being a volunteer project for 2 years. \n\n"}
