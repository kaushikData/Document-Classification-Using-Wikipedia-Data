{"id": "599306", "url": "https://en.wikipedia.org/wiki?curid=599306", "title": "ABA routing transit number", "text": "ABA routing transit number\n\nAn ABA routing transit number (ABA RTN) is a nine-digit code, used in the United States, which appears on the bottom of negotiable instruments such as checks to identify the financial institution on which it was drawn. The ABA RTN was originally designed to facilitate the sorting, bundling, and shipment of paper checks back to the drawer's (check writer's) account. As new payment methods were developed (ACH and Wire), the system was expanded to accommodate these payment methods.\n\nThe ABA RTN is necessary for the Federal Reserve Banks to process Fedwire funds transfers, and by the ACH Network to process direct deposits, bill payments, and other such automated transfers.\n\nThe ABA RTN system was developed in 1910 by the American Bankers Association.\n\nSince 1911, the American Bankers Association has partnered with a series of registrars, currently Accuity, to manage the ABA routing number system. Accuity is the Official Routing Number Registrar and is responsible for assigning ABA RTNs and managing the ABA RTN system. Accuity publishes the \"American Bankers Association Key to Routing Numbers\" semi-annually. The \"Key Book\" contains the listing of all ABA RTNs that have been assigned.\n\nThere are approximately 26,895 active ABA RTNs currently in use. Every financial institution in the United States has at least one. The Routing Number Policy allows for up to five ABA RTNs to be assigned to a financial institution. Many institutions have more than five ABA RTNs as a result of mergers.\n\nABA RTNs are only for use in payment transactions within the United States. They are used on paper check, wire transfers, and ACH transactions. On a paper check, the ABA RTN is usually the middle set of nine numbers printed at the bottom of the check. Domestic transfers that use the ABA RTN will usually be returned to the paying bank.\n\nIncoming international wire transfers also use a BIC code, also known as a SWIFT code, as they are administered by the Society for Worldwide Interbank Financial Telecommunication (SWIFT) and defined by ISO 9362. In addition, many international financial institutions use an IBAN code.\n\nThe IBAN was originally developed to facilitate payments within the European Union but the format is flexible enough to be applied globally. It consists of an ISO 3166-1 alpha-2 country code, followed by two check digits that are calculated using a mod-97 technique, and Basic Bank Account Number (BBAN) with up to thirty alphanumeric characters. The BBAN includes the domestic bank account number and potentially routing information. The national banking communities decide individually on a fixed length for all BBAN in their country.\n\nThe bank numbers in the United States were originated by the American Bankers Association (ABA) in 1911. Banks had been disagreeing on identification. The ABA arranged a meeting of clearing house managers in Chicago in December 1910. The gathering chose a committee to assign each bank in the country convenient numbers to use. In May 1911, the American Bankers Association released the codes. The numerical committee was W. G. Schroeder, C. R. McKay, and J. A. Walker. The publisher of the new directory was Rand-McNally and Company. The ABA clearing house codes are like the sub-headings in a decimal outline. The prefixes mean locations and the suffixes banking firms within those locations. Half of the prefixes represent major cities the other half represent regions of the United States. Lower prefixes are used for higher populations, first based on the 1910 U. S. Census. Likewise, within each prefix area banks are numbered in order of city population and bank seniority, although single-bank towns are numbered in alphabetical order. When a new bank is being organized, the current publisher of the directory of banks assigns it a transit code. The American Bankers Association asked banks to use the directory exclusively so banks would agree on how to sort checks. The book was abbreviated \"Key to Numerical System of The American Bankers Association,\" and as the \"Key\". It was published by Rand McNally & Co. In 1952 by Rand McNally moved its corporate headquarters to Skokie, Illinois, and became more interested in publishing maps. Also in Skokie is a company called Accuity, which from its history has been the official registrar of ABA bank numbers since 1911. By 2014 it was the publisher of the semi-annual \"ABA Key to Routing Numbers\" and was owned by Reed Business Information, British publisher of reference works for professionals, which in turn is owned by Reed-Elsevier, English-Dutch publisher of online format reference works for professionals. Over the years the ABA's identification numbers for banks accommodated the Federal Reserve Act, the Expedited Funds Act and the Check 21 Act. By 2014 the \"Key\" included the U. S. Federal Reserve's nine-digit magnetic-ink routing numbers.\n\nThe ABA RTN appears in two forms on a standard check – the fraction form and the MICR (magnetic ink character recognition) form. Both forms give essentially the same information, though there are slight differences.\n\nThe MICR forms are the main form – it is printed in magnetic ink, and is machine-readable; it appears at the bottom left of a check, and consists of nine digits.\n\nThe fraction form was used for manual processing before the invention of the MICR line, and still serves as a backup in check processing should the MICR line become illegible or torn; it generally appears in the upper right part of a check near the date.\n\nThe MICR number is of the form\nwhere XXXX is Federal Reserve Routing Symbol, YYYY is ABA Institution Identifier,\nand C is the Check Digit, while the fraction is of the form:\nwhere PP is a 1 or 2 digit Prefix, no longer used in processing, but still printed, representing the bank's check processing center location, with 1 through 49 for processing centers located in a major city, and 50 through 99 representing processing is done at a non-major city in a particular state. Sometimes a branch number or the account number are printed below the fraction form; branch number is not used in processing, while the account number is listed in MICR form at the bottom. Further, the Federal Reserve Routing Symbol and ABA Institution Identifier may have fewer than 4 digits in the fraction form. The essential data, shared by both forms, is the Federal Reserve Routing Symbol (XXXX), and the ABA Institution Identifier (YYYY), and these are usually the same in both the fraction form and the MICR, with only the order and format switched (and left-padded with 0s to ensure that they are 4 digits long).\n\nThe prefix and the Federal Reserve Routing Symbol (XXXX) are determined by the bank's geographical location and treatment by the Federal Reserve type, while the remaining data (YYYY, and Branch number, if present) depends on the specific bank, and are unique within a Federal Reserve district.\n\nIn the check depicted above right, the fraction form is \"11-3167/1210\" (with \"01\" below it) and MICR form is \"129131673\" which are analyzed as follows:\n\nIn the case of a MICR line that is illegible or torn, the check can still be processed without the check digit. Typically, a repair strip or sleeve is attached to the check, then a new MICR line is imprinted. Either 021200025 or 0212-0002 (with a hyphen, but no check digit) may be printed, and both are 9 digits. The former (with check digit) is preferred to ensure better accuracy, but requires computing the check digit, while the latter is easily determined by inspection of the fraction, with minimal clerical handling.\n\nThe MICR routing number consists of 9 digits:\nwhere XXXX is Federal Reserve Routing Symbol, YYYY is ABA Institution Identifier,\nand C is the Check Digit.\n\nThe Federal Reserve uses the ABA RTN system for processing its customers' payments. The ABA RTNs were originally assigned in the systematic way outlined below, reflecting a financial institution's geographical location and internal handling by the Federal Reserve. Following consolidation of the Federal Reserve's check processing facilities, and the consolidation in the banking industry, the RTN a financial institution uses may not reflect the \"Fed District\" where the financial institution's place of business is located. Check processing is now centralized at the Federal Reserve Bank of Atlanta.\n\nThe first two digits of the nine digit RTN must be in the ranges 00 through 12, 21 through 32, 61 through 72, or 80.\n\nThe digits are assigned as follows:\n\nThe first two digits correspond to the 12 Federal Reserve Banks as follows:\n\nThe third digit corresponds to the Federal Reserve check processing center originally assigned to the bank.\n\nThe fourth digit is \"0\" if the bank is located in the Federal Reserve city proper, and otherwise is 1–9, according to which state in the Federal Reserve district it is.\n\nThe fifth through eighth digits constitute the bank's unique ABA identity within the given Federal Reserve district.\n\nThe ninth, check digit provides a checksum test using a position-weighted sum of each of the digits. High-speed check-sorting equipment will typically verify the checksum and if it fails, route the item to a reject pocket for manual examination, repair, and re-sorting. Mis-routings to an incorrect bank are thus greatly reduced.\n\nThe following condition must hold:\nIn terms of weights, this is 371 371 371. This allows one to catch any single-digit error (incorrectly inputting one digit), together with most transposition errors. 1, 3, and 7 are used because they (together with 9) are coprime to 10; using a coefficient that is divisible by 2 or 5 would lose information (because formula_2), and thus would not catch some substitution errors. These do not catch transpositions of two digits that differ by 5 (0 and 5, 1 and 6, 2 and 7, 3 and 8, 4 and 9), but captures other transposition errors.\n\nAs an example, consider 111000025 (which is a valid routing number of Bank of America in Virginia). Applying the formula, we get:\n\nThe symbol that delimits a routing transit number is the MICR E-13B transit character (Unicode value U+2446): ⑆\n\nIf your computer cannot display this character, it may be seen here.\n\nThe fraction form looks like a fraction, with a numerator and a denominator.\n\nThe numerator consists of two parts separated by a dash. The prefix (no longer used in check processing, yet still printed on most checks) is a 1 or 2 digit code (P or PP) indicating the region where the bank is located. The numbers 1 to 49 are cities, assigned by size of the cities in 1910. The numbers 50 to 99 are states, assigned in a rough spatial geographic order, and are used for banks located outside one of the 49 numbered cities.\n\nThe second part of the numerator (after the dash) is the bank's ABA Institution Identifier, which also forms digits 5 to 8 of the nine digit routing number (YYYY).\n\nThe denominator is also part of the routing number; by adding leading zeroes to make up four digits where necessary (e.g. 212 is written as 0212, 31 is written as 0031, etc.), it forms the first four digits of the routing number (XXXX).\n\nThere might also be a fourth element printed to the right of the fraction: this is the bank's branch number. It is not included in the MICR line. It would only be used internally by the bank, e.g. to show where the signature card is located, where to contact the responsible officer in case of an overdraft, etc.\n\nFor example, a check from Wachovia Bank in Yardley, PA, has a fraction of 55-2/212 and a routing number of 021200025. The prefix (55) no longer has any relevance, but from the remainder of the fraction, the first 8 digits of the routing number (02120002) can be determined, and the check digit (the last digit, 5 in this example) can be calculated by using the check digit formula (thus giving 021200025).\n\nThis table is up to date as of 2009. One weakness of the current routing table arrangement is that Alaska, American Samoa, Guam, Hawaii, Puerto Rico and the US Virgin Islands share the same routing code.\n\nGeneral Category\n\nCanada has similar but different transaction routing structures\n\n\n"}
{"id": "15858905", "url": "https://en.wikipedia.org/wiki?curid=15858905", "title": "Ancient Chinese glass", "text": "Ancient Chinese glass\n\nAncient Chinese glass refers to all types of glass manufactured in China prior to the Qing Dynasty (1644–1911). In Chinese history, glass played a peripheral role in the arts and crafts, when compared to ceramics and metal work. The limited archaeological distribution and use of glass objects are evidence of the rarity of the material. Literary sources date the first manufacture of glass to the 5th century AD. However, the earliest archaeological evidence for glass manufacture in China comes from the Warring States period (475 BC to 221 BC).\n\nChinese learned to manufacture glass comparatively later than the Mesopotamians, Egyptians and Indians. Imported glass objects first reached China during the late Spring and Autumn period – early Warring States period (early 5th century BC), in the form of polychrome ‘eye beads’. These imports created the impetus for the production of indigenous glass beads.\n\nDuring the Han period (206 BC to 220 AD) the use of glass diversified. The introduction of glass casting in this period encouraged the production of moulded objects, such as bi disks and other ritual objects. The Chinese glass objects from the Warring States period and Han Dynasty vary greatly in chemical composition from the imported glass objects. The glasses from this period contain high levels of barium oxide (BaO) and lead, distinguishing them from the soda-lime-silica glasses of Western Asia and Mesopotamia. At the end of the Han Dynasty (AD 220), the lead-barium glass tradition declined, with glass production only resuming during the 4th and 5th centuries AD.\n\nAt present, it is accepted that in China, glassmaking began around the 5th century BCE during the late Spring and Autumn to early Warring States periods. Chemical analyses of glass samples dating to this time have identified no less than three glass systems: potash-lime, lead-barium, and potash; of these, lead-barium was the most significant in early China.\n\nDuring the Warring States period and the Han Dynasty (5th century BC to early 3rd century AD) glass was imported from regions outside of East Asia, such as Mesopotamia. Imported Western faience and glass probably inspired the production of the first Chinese glasses. The main group of objects with Western influences are eye beads or dragonfly-eyed beads. The key difference between Near Eastern eye beads and Chinese eye beads is their chemical composition. The coloured glasses used to produce the Chinese eye beads have a high lead and barium content. This type of composition was exclusive to China; and it was used to produce a range of glass objects until the end of the Han Dynasty.\n\nTable 1 shows examples of chemical compositions of some Chinese glass objects.\n\"Table 1: Elemental compositions of Chinese glass from the Warring States Period to Han Dynasty. Data from the first 4 objects was taken from Brill et al. 1991; data from glass garment plaque was taken from Cheng Zhuhai and Zhou Changyuan 1991; and data from the ritual disk was taken from Shi Meiguang 1991.\"\n\nThe Chinese lead-barium glasses typically present 5 to 15% BaO. The source of barium in the glass in not clear; however it is possible that ancient Chinese glassmakers used witherite (a mineral form of barium carbonate) as an ingredient. The use of a separate ingredient implies that barium had a specific function. This function could have been to flux the glass, by lowering the melting point of the melt; or stabilize the glass, by making it less soluble to water. It could also have been added to opacify the glass. Electron microprobe analysis of glass fragments have shown that the turbidity of certain lead-barium glasses is produced by barium disilicate crystals. This turbidity gives the glasses a jade-like appearance.\n\nThe period between the Warring States period and the Han Dynasty shaped the early Chinese glass industry. Most of the glass objects from this period come from archaeological excavations of tombs. Because of this, most glass objects have fairly secure dating and context. During this period Chinese mainly used glass to produce two distinct types of objects, polychrome eye beads and monochrome funerary objects\n\nThe earliest types of glass objects found in China are polychrome eye beads or dragonfly-eyed beads. The beads are found in burials from the late Spring and Autumn and early Warring States periods (early 5th century BC) up to late Warring State – early Western Han period. Most beads have a monochrome glass body covered by several layers of coloured glass. The layers of different colour glass are applied in alternating fashion to produce concentric circles. The patterns of circles resemble eyes, giving the beads their name. This style of bead originated in the Near East during the mid 2nd millennium BC. The stylistic influence later spread to the Mediterranean, Central Asia and China.\n\nDuring the early 5th century BC, the imported eye beads were considered exotic objects. They were mainly deposited in high status burials, such as the tomb of a male aristocrat of the Qi state at Langjiazhuang, Shadong. This situation changes during the middle and late Warring States Period. Eye beads from this period are manufactured from Chinese lead-barium glass and are mainly found in the regions of the middle Yangzi River Valley, indicating a connection to the Chu kingdom. In this context, the beads became more common and available to a larger part of the Chinese society. Evidence of this is the presence of eye beads in medium and small burials, with modest funerary furnishings, as well as large, high-status burials.\n\nThe use of eye beads in burials rapidly declined at the beginning of the Western Han period. This is believed to be a result of the invasion of Chu kingdom territories by Qin and Han armies at the end of the 3rd century. The collapse of the Chu kingdom would have brought production of eye-beads to an end.\n\nDuring the Warring States and Han periods, Chinese glassworkers took advantage of the similarities between glass and jade. In this period many glass objects, found in burial contexts, were made of opaque green, light green or milky white glass. These objects have similar shapes to their jade counterpart, and were no doubt imitating that precious stone. Among these objects there were bi disks, ‘glass garments’ (or glass suits), sword accessories and vessels, among others.\n\nA \"bi\" disk is a ritual object that resembles a flattened torus. The earliest archeological specimens were carved from stone (usually nephrite) and date back to the late Neolithic period; they became important burial elements during the 3rd millennium BC. They were placed on or near the head of the deceased person.\n\nGlass \"bi\" disks are the most numerous kind of monochrome glass objects. They first became abundant in the Chu kingdom during the Warring States period. \"Bi\" disks from this period tend to be between 7.9 and 9.4 cm in diameter. The glass \"bi\" disks look very similar to their stone contemporaries, usually decorated on one side with a simple grain or cloud pattern typical of jade objects. They are mainly found in medium to small-sized tombs, indicating the middle strata of society rather than the elite. This suggests that glass \"bi\" disks were regarded as cheaper alternatives to jade \"bi\" disks.\n\nPlaques from glass burial suits or glass garments are directly linked to jade objects. Several pieces of burial suit plaques have been found in a few wooden-chambered burials from the late West Han Dynasty. The plaques come in different shapes, with rectangular being the most common. Some of these rectangular pieces have perforations in 4 corners, indicating that they were strung together, or sewn into a fabric, to form a protective burial suit. Other shapes, such as circular, triangular and rhomboidal, are often decorated with moulded patterns. All the glass plaque shapes have jade counterparts, suggesting that glass burial suits were a cheaper alternative to jade burial suits.\n\nThe earliest known Chinese glass vessels come from Western Han Dynasty tombs. To this date only two tombs are known to have had glass vessels among their funerary objects: the tomb of the Liu Dao, Prince of Chu in Xuzhou (128 BC), Jiangsu Province; and the tomb of Liu Sheng, Prince Jing of Zhongshan (113BC) at Mancheng. The tomb at Xuzhou contained 16 light green cylindrical cups; while Prince Liu Sheng’s tomb contained 2 shallow double-handled cups and a plate. All of the vessels were traditional Chinese shapes and made of lead-barium glass. Vessels in these forms were normally made out of lacquer or ceramic, although some jade vessels in those shapes are also known. All vessels were produced by mould casting.\n\n\n"}
{"id": "44240196", "url": "https://en.wikipedia.org/wiki?curid=44240196", "title": "Angie Tribeca", "text": "Angie Tribeca\n\nAngie Tribeca is an American comedy television series, created by Steve and Nancy Carell and aired on TBS. The series, a satire of the police procedural genre, stars Rashida Jones as the titular police detective Angie Tribeca. It also stars Hayes MacArthur, Jere Burns, Deon Cole and Andrée Vermeulen in supporting roles.\n\nThe first season of \"Angie Tribeca\" premiered on January 17–18, 2016, the second season premiered on June 6 of that year, and the third season premiered on April 10, 2017.\n\nAngie Tribeca is a 10-year veteran of the Los Angeles Police Department's elite RHCU (Really Heinous Crimes Unit), who is assigned a new partner. The format of each episode involves a different criminal case for the LAPD to solve. The show has almost nonstop witty one liners, jokes, visual humor, or irony.\n\nThe series was announced by TBS in mid-2014 with a ten-episode order. It was promoted as \"...a hilarious spoof of police procedurals in the spirit of \"Police Squad!\". A few gags presented in the preview reel were cited as similar to the classic TV cop show satire \"Sledge Hammer!\".\n\nOriginally intended to premiere in late 2015, in November 2015 it was announced that the first season of ten episodes would run uninterrupted on the network and be released through video on demand starting on January 17, 2016. All 10 episodes of season 1 premiered during a 25-hour TV marathon on January 17–18, 2016.\n\nA second season of an additional ten episodes premiered on June 6, 2016. On July 6, 2016, TBS renewed the series for a third season, which premiered on April 10, 2017. A fourth season, scheduled for 2018, adds Bobby Cannavale to the cast.\n\n\n\n\"Angie Tribeca\" has been met with positive reviews from critics. On Rotten Tomatoes, the series has an approval rating of 88% based on 31 reviews, with an average rating of 7.8/10. The site's critical consensus reads, \"\"Angie Tribeca\"s unique blend of sharp wit and broad humor – and the obvious fun being had by a talented cast – make for a consistent, charmingly absurd spoof of police procedurals.\" Metacritic gives it a weighted average score of 78 out of 100 based on 13 critics, indicating \"generally favorable reviews\".\n\n\n"}
{"id": "40728", "url": "https://en.wikipedia.org/wiki?curid=40728", "title": "Artificial transmission line", "text": "Artificial transmission line\n\nIn telecommunication, an artificial transmission line is a two-port electrical network that has the characteristic impedance, transmission time delay, phase shift, or other parameter(s) of a real transmission line. It can be used to simulate a real transmission line in one or more of these respects.\n\nEarly artificial lines were used in telephony research and took the form of a cascade of lattice phase equalisers to provide the necessary delay. The lattice phase circuit was invented by Otto Zobel in the 1920s.\n"}
{"id": "637137", "url": "https://en.wikipedia.org/wiki?curid=637137", "title": "Auspex, Inc.", "text": "Auspex, Inc.\n\nAuspex, Inc. was founded 1985 by Michael Henney, Burt Brockman and Terry Kisner to develop SCADA software systems. In 1986, Rich Newell joined Auspex and the company started providing software support for SGM, Inc.'s RCS-7 line of SCADA products. In 1987 Auspex acquired the RCS-7 product line from SGM, Inc. In 2005, Auspex started marketing LeakTrack 2000, a liquid pipeline leak detections system.\n"}
{"id": "45328964", "url": "https://en.wikipedia.org/wiki?curid=45328964", "title": "Bifilar sundial", "text": "Bifilar sundial\n\nA bifilar dial is a type of sundial invented by the German mathematician Hugo Michnik in 1922. It has two non-touching threads parallel to the dial. Usually the second thread is orthogonal-(perpendicular) to the first.\n\nThe intersection of the two threads' shadows gives the local apparent time.\n\nWhen the threads have the correctly calculated separation, the hour-lines on the horizontal surface are uniformly drawn. The angle between successive hour-lines is constant. The hour-lines for successive hours are 15 degrees apart.\n\nThe bifilar dial was invented in April 1922 by the mathematician and maths teacher, Hugo Michnik, from Beuthen, Upper Silesia. He studied the horizontal dial- starting on a conventional XYZ cartesian framework and building up a general projection which he states was an exceptional case of a Steiner transformation. He related the trace of the sun to conic sections and the angle on the dial-plate to the hour angle and the calculation of local apparent time, using conventional hours and the historic Italian and Babylonian hours. He refers in the paper, to a previous publication on the theory of sundials in 1914.\n\nHis method has been applied to vertical near-declinant dials, and a more general declining-reclining dial.\n\nWork has been subsequently done by Dominique Collin.\n\nThis was the dial that Hugo Michnik invented and studied. By simplifying the general example so:\nthe shadow is thrown on a dial-plate marked out like a simple equatorial sundial.\n\nThe first wire formula_2 is orientated north-south at a constant distance formula_3 from the dial plate formula_4<br>\nThe second wire formula_5 is orientated east-west at a constant distance formula_6 from the dial plate formula_4 (thus formula_5 is orthogonal to formula_2 which lies on the plane of the meridian ).\n\nIn this proof formula_10 (pronounced phi) is the latitude of the dial plate.\n\nRespectively, formula_11 and formula_12 are the vertical projections of wires formula_2 and formula_5 on the dial plate formula_4.\n\nPoint formula_16 is the point on the dial plate directly under the two wires' intersection.<br> That point is the origin of the X,Y co-ordinate system referred to below.\n\nThe X-axis is the east-west line passing through the origin. The Y-axis is the north-south line passing through the origin. The positive Y direction is northward.\n\nOne can show that if the position of the sun is known and determined by the spherical coordinates formula_17 et formula_18 (pronounced t-dot and delta, respectively the known as the hour angle et declination), the co-ordinates formula_19 and formula_20 of point formula_21, the intersection on the two shadows on the dial-plate formula_4 have values of :\n\nEliminating the variable formula_18 in the two preceding equations, one obtains a new equation defined for formula_19 and formula_20 which gives, as a function of the latitude formula_10 and the solar hour angle solaire formula_17, the equation of the trace of the sun associated with the local apparent time. In its simplest form this equation is written:\nThis relation shows that the hour traces are indeed line segments and the meeting-point of these line segments is the point formula_30:\n\nIn other words, point C is south of point O (where the wires intersect), by a distance of codice_3, where formula_10 is the latitude.\n\nIf one arranges the two wire heights formula_6 and formula_3 such :\n\nthen the equation for the hour lines can be simply written as:\nat all times, the intersection formula_21 of the shadows on the dial plate formula_4 is such that the angle formula_41 is equal to the hour angle formula_17 of the sun so thus represents solar time.\n\nSo provided the sundial respects the la condition formula_1 the trace of the sun corresponds to the hour-angle shown by lines (rays) centred on the point formula_30 and the 13 rays that correspond to the hours 6:00, 7:00, 8:00, 9:00... 15:00, 16:00, 17:00, 18:00 are regularly spaced at a constant angle of 15°, about point C. \n\nA London dial is the name given to dials set for 51° 30' N. A simple London bifilar dial has a dial plate with 13 line segments drawn outward from a centre-point C, with each hour's line drawn 15° clockwise from the previous hour's line. The midday line is aligned towards the north.\n\nThe north-south wire is 10 cm (formula_45) above the midday line. That east-west wire is placed at a height of 7.826 (formula_46) centimeters- equivalent to 10cm x sin(51° 30'). This passes through C. The east-west wire crosses the north-south wire 6.275 cm north of the centre-point C- that being the equivalent of - 7.826 (formula_46) divided by tan (51° 30').\n\nWhether a sundial is a bifilar, or whether it's the familiar flat-dial with a straight style (like the usual horizontal and vertical-declining sundials), making it reclining, vertical-declining, or reclining-declining is exactly the same. The declining or reclining-declining mounting is achieved in exactly the same manner, whether the dial is bifilar, or the usual straight-style flat dial.\n\nFor any flat-dial, whether bifilar, or ordinary straight-style, the north celestial pole has a certain altitude, measured from the plane of the dial.\n\n\n\n\n"}
{"id": "3842275", "url": "https://en.wikipedia.org/wiki?curid=3842275", "title": "Butcher block", "text": "Butcher block\n\nButcher block, butcher's block is a style of assembled wood (often hard maple, teak, or walnut) used as heavy duty chopping blocks, table tops, and cutting boards. It was commonly used in butcher shops and meat processing plants but has now become popular in home use.\n\nThere are two basic styles of butcher block: end grain and edge grain.\n\nButcher blocks have been used in butcher shops for centuries, and still are in many European countries. Increasingly, though, butcher block is being used in domestic kitchens as an alternative to stone and laminate countertops. This has created a new industry in the kitchen design arena and many furniture manufacturers and hardwood flooring companies are getting into the production of butcher blocks and butcher block countertops, in part because the countertops can be constructed from left-over wood that would otherwise be discarded.\nProper care of a butcher block is important for longevity of the material and, to a degree, food safety.\n\nIf the material is regularly exposed to water and not well cleaned, then mold can form. The seams where the wood is joined can buckle as the wood expands and contracts. Keeping the material well oiled allows it to maintain a rich color and its water repelling properties. \n\nTo properly care for the butcher block any foreign material should first be removed, it can be disinfected with vinegar (or in extreme cases, bleach), allowed to dry well, and refreshed with mineral oil or other food safe sealant\n\n"}
{"id": "16566497", "url": "https://en.wikipedia.org/wiki?curid=16566497", "title": "CV-2000", "text": "CV-2000\n\nCV-2000 was one of the world's first home video tape recorders (VTR), introduced by Sony in August, 1965. The 'CV' in the model name stood for 'Consumer Video'. This was Sony's domestic format throughout the 1960s.\n\nThe CV-2000 was developed by Sony engineer Nobutoshi Kihara. On its release, each machine cost US$695. It used video tape in a reel-to-reel format, meaning the tape had to be manually threaded around the helical scan video head drum. The CV-2000 was one-tenth the weight and price of other analog video recording products of its era. It recorded television programs in black and white using the skip field process, which produced a maximum 200-lines resolution. The tape moved at a speed of 7.5 inches per second. Each reel of video tape cost US$40, and could hold one hour of video. Although CV-2000 was aimed at the home market, it was mainly used in business and educational institutions.\n\nTen models were developed in the CV series: CV-2000, TCV-2010, TCV-2020, CV-2100, TCV-2110, TCV-2120, CV-2200, DV-2400, CV-2600 and CV-5100. Sony also sold an optional 'Video Camera Ensemble', known as the VCK-2000. This add-on kit contained a separate video camera, a microphone, and a tripod.\n\nOne of its shortcomings as a format was the omission of the ability to adjust tracking, which made interchangeability of tapes between different machines almost impossible. Sony's later AV series machines included this feature. The CV video recorders fell into disuse with the arrival of the EIAJ type 1 standard that was used by many companies, including Sony with their AV series machines.\n"}
{"id": "23305579", "url": "https://en.wikipedia.org/wiki?curid=23305579", "title": "Cantera (software)", "text": "Cantera (software)\n\nCantera is an open-source chemical kinetics software used for solving chemically reacting laminar flows. It has been used as a third-party library in external reacting flow simulation codes, such as FUEGO and CADS, using Fortran, C++, etc. to evaluate properties and chemical source terms that appear in the application's governing equations. Cantera was originally written and developed by Prof. Dave Goodwin of California Institute of Technology. It is written in C++ and can be used from C++, Python, Matlab and Fortran.\n\n\n"}
{"id": "2646252", "url": "https://en.wikipedia.org/wiki?curid=2646252", "title": "China Oilfield Services", "text": "China Oilfield Services\n\nChina Oilfield Services (COSL) is an oilfield services company. It is a majority owned subsidiary of Chinese state owned company CNOOC Group. It also has a listed sister company in Hong Kong, CNOOC Limited.\n\nChina Oilfield Services usually purchases off shore vessels (OSVs) and operates them in Southeast Asia, the Middle East and Central Asia in off shore projects of CNOOC. It also operates in Indonesia, Malaysia and the Caspian Sea.\n\nCOSL's overseas revenue surged 133% year on year in the first half of 2015 to 209.8 million yuan, driven by demand for its services in Indonesia, West Africa and the Middle East.\n\nCOSL had an office in Kuala Lumpur, Malaysia briefly in the early 2000s when it had its first western Vice President, Alan Good, a British born Oilman who helped launch the company internationally and pushed for further growth in the international sector.\n\nCOSL also maintains an office in Houston, TX as COSL America\n\nGlobally, the number of listed players in the oilfield services sector is just over 100, with a combined market capitalisation of about US$250 billion. The top five account for 40% of the industry's market capitalisation. The two largest companies in this sector are Schlumberger with a 2007 year-end market cap of US$117.6 billion and Halliburton with a 2007-year end market cap of US$33.4 billion.\n\nAccording to Mr Chen, US firms tend to be the most expensive with enterprise values running two to three times book values, compared with one or less in Europe and Asia.\n\nCOSL claims a 95% share of China's market for offshore drilling services, 70% of the marine support and transportation market, 60% of the well survey services market and more than 50% of the seismic data collection market.\n\nGlobally, about 15% of oil companies' capital expenditure goes to exploration, 35% to field development and 50% to production.\n\nCOSL posted a 22.7% year-on-year rise in interim net profit to 555.9 million Yuan, ($115 million).\nOn 23 September 2008, COSL acquired Norway's Awilco Offshore ASA (AWO) for 17.1 billion yuan (2.51 billion U.S. dollars), which is to be merged into COSL Drilling Europe AS, a wholly owned subsidiary of COSL. The deal received backing from the relevant government departments of Norway and Statoil, the state oil company of Norway. The deal increased the number of COSL's operating rigs to 22 from 15 and created the world's eighth largest rig fleet.\n"}
{"id": "5875710", "url": "https://en.wikipedia.org/wiki?curid=5875710", "title": "Chris Messina (open-source advocate)", "text": "Chris Messina (open-source advocate)\n\nChristopher Reaves Messina (born January 7, 1981) is the inventor of the hashtag as it is currently used on social media platforms. In a 2007 tweet Messina proposed vertical/associational grouping of messages, trends, and events on Twitter by the means of hashtags. Simply put the hashtag was to be a type of metadata tag that allowed users to apply dynamic, user-generated tagging which made it possible for others to easily find messages with a specific theme or content; it allowed easy, informal markup of folk taxonomy without need of any formal taxonomy or markup language. Hashtags have since been referred to as the \"eavesdroppers\", \"wormholes\", \"time-machines\", and \"veins\" of the internet. \n\nAlthough Twitter's initial response to Messina's proposed use of hashtags was negative \"these things are for nerds\" a series of events, including the devastating fire in San Diego County later that year, saw the first widespread use of #sandiegofire to allow users to easily track updates about the fire. The use of hashtags itself then eventually spread like wild-fire on Twitter, and by the end of the decade could be seen in most emerging as well as established social media platforms including Instagram, Facebook, Reddit, and YouTube. So much so that Instagram had to officially place a \"30 hashtags\" limit on its posts to prevent people from abusing the use of hashtags. A limit which Instagrammers eventually circumvented by posting hashtags in the comments section of their posts. As of 2018 more than 85% of the top 50 websites by traffic on the Internet use hashtags and their use is highly common with millennials, Gen Z, politicians, influencers, and celebrities worldwide. \n\nMessina subsequently went on to become the Developer Experience Lead at Uber from 2016 to 2017. And as of 2018 ranks as the No. 1 hunter on ProductHunt.com. He is a technology evangelist who is an advocate for open source, open standards, microformats, and OAuth. Messina is also known for his involvement in helping to create the BarCamp, Spread Firefox, and coworking movements. \n\nMessina was employed as an Open Source Advocate at identity company Vidoop and before that was the co-founder of marketing agency Citizen Agency. He worked at Google as an Open Web Advocate, leaving to join startup NeonMob. He graduated from Carnegie Mellon University in 2003 with a BA in Communication Design. From 2016 to January 2017, Messina lead the Developer Experience team at Uber where he enforced the terms and conditions of Uber's proprietary APIs.\n\nMessina co-founded Citizen Agency, a company which describes itself as \"Internet consultancy that specializes in developing community-centric strategies around product research, design, development and marketing\" with Tara Hunt and Ben Metcalfe, who has since left the company.\n\nMessina was an advocate of open-source software, most notably Firefox and Flock. As a volunteer for the Spread Firefox campaign, he designed the 2004 Firefox advert which appeared in \"The New York Times\" on December 16, 2004. In 2008, he won a Google-O'Reilly Open Source Award for Best Community Amplifier for BarCamp, Microformats and Spread Firefox.\n\nIn February 2018, Messina launched Molly, an AMA-style website where the questions are answered using the person's social media posts.\n\nThe International Telecommunication Union approved in November 1988 a recommendation that put the hash sign on the right side of the 0 in the button arrangement for push buttons on telephones. This same arrangement is still used today in most software phones (see Android dialer for example). The ITU recommendation had 2 design options for the hash: a European version where the hash sign was built with a 90 degree angle and a North American version with an 80 degree angle. The North American version seems to have prevailed as most hash signs in Europe now follow the 80 degree inclination.\nThe pound sign was adopted for use within IRC networks circa 1988 to label groups and topics. Channels or topics that are available across an entire IRC network are prefixed with a hash symbol (as opposed to those local to a server, which use an ampersand).\n\nThe use of the pound sign in IRC inspired Chris Messina to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network. He posted the first hashtag on Twitter. Messina’s suggestion to use the hashtag was not adopted by Twitter, but the practice took off after hashtags were widely used in tweets relating to the 2007 San Diego forest fires in Southern California. According to Messina, he suggested use of the hashtag to make it easy for \"lay\" users to search for content and find specific relevant updates; they were for people who do not have the technological knowledge to navigate the site. Therefore, the hashtag \"was created organically by Twitter users as a way to categorize messages.\"  Today they are for anyone, either with or without technical knowledge, to easily impose enough annotation to be useful without needing a more formal system or adhering to many technical details.\n\nInternationally, the hashtag became a practice of writing style for Twitter posts during the 2009–2010 Iranian election protests; Twitter users inside and outside Iran used both English- and Persian-language hashtags in communications during the events. The first published use of the term \"hash tag\" was in a blog post by Stowe Boyd, \"Hash Tags = Twitter Groupings,\" on August 26, 2007, according to lexicographer Ben Zimmer, chair of the American Dialect Society's New Words Committee.\n\nBeginning July 2, 2009, Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced \"Trending Topics\" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to spam the trending list and ensure that hashtags trend naturally.\n\nAlthough the hashtag started out most popularly on Twitter as the main social media platform for this use, the use has extended to other social media sites including Instagram, Facebook, Flickr, Tumblr, and Google+.\n\nMessina was featured with Hunt, also his ex-girlfriend, in \"So Open it Hurts\", in \"San Francisco Magazine\" (August 2008). The article detailed their very public and open relationship shared on the internet, and the lessons they derived from that experience.\n\n\n"}
{"id": "4931997", "url": "https://en.wikipedia.org/wiki?curid=4931997", "title": "Conversion coating", "text": "Conversion coating\n\nConversion coatings are coatings for metals where the part‘s surface is subjected to a chemical or electro-chemical process by the coating material which converts it into a decorative or protective substance. Examples include chromate conversion coatings, phosphate conversion coatings, bluing, black oxide coatings on steel, and anodizing. They are used for corrosion protection, to add decorative color and as paint primers. \n\n"}
{"id": "27838362", "url": "https://en.wikipedia.org/wiki?curid=27838362", "title": "EPANET", "text": "EPANET\n\nEPANET is a public domain, water distribution system modeling software package developed by the United States Environmental Protection Agency's (EPA) Water Supply and Water Resources Division. It performs extended-period simulation of hydraulic and water-quality behavior within pressurized pipe networks and is designed to be \"a research tool that improves our understanding of the movement and fate of drinking-water constituents within distribution systems\". EPANET first appeared in 1993.\n\nEPANET 2 is available both as a standalone program and as an open-source toolkit (Application Programming Interface in C). Its computational engine is used by many software companies that developed more powerful, proprietary packages, often GIS-centric. The EPANET \".inp\" input file format, which represents network topology, water consumption, and control rules, is supported by many free and commercial modeling packages. Therefore, it is arguably considered as the industry standard.\n\nEPANET provides an integrated environment for editing network input data, running hydraulic and water quality simulations, and viewing the results in a variety of formats. EPANET provides a fully equipped and extended period of hydraulic analysis that can handle systems of any size. The package also supports the simulation of spatially and temporally varying water demand, constant or variable speed pumps, and the minor head losses for bends and fittings. The modeling provides information such as flows in pipes, pressures at junctions, propagation of a contaminant, chlorine concentration, water age, and even alternative scenario analysis. This helps to compute pumping energy and cost and then model various types of valves, including shutoffs, check pressure regulating and flow control.\n\nEPANET’s water quality modeling functionality allows users to analyze the movement of a reactive or non-reactive tracer material which spreads through the network over time. It rates the reactive material as is grows, tracks the percentage of flow from the given nodes. The package employs the global reaction rate coefficient which can be modified on a pipe-by-pipe basis. The storage tanks can be modeled as complete mix, plug flow or two-compartment reactors.\n\nThe visual network editor of EPANET simplifies the process of building piping network models and editing their properties. These various types of data reporting visualization tools are used to assist to analyze the networks, which include the graphics views, tabular views, and special reports.\n\nEPANET hydraulics engine computes headlosses along the pipes by using one of the three formulas:\nSince the pipe segment headloss equation is used within the network solver, the formula above is selected for the entire model.\n\nWithin EPANET, pumps are modeled using a head-flow curve, which defines the relationship between hydraulic head imparted to the system by the pump and flow conveyed by the pump. The model calculates the flow conveyed by the pump element for a given system head condition based on this curve. EPANET can also model a pump as a constant power input, effectively adding a given amount of energy to the system downstream of the pump element.\n\nThe network hydraulics solver employed by EPANET uses the \"Gradient Method\" first proposed by Todini and Pilati, which is a variant of Newton-Raphson method.\n\nEPANET includes the capability to model water age and predict flow of non-reactive and, under simplified conditions, reactive materials. This capability is frequently used to predict chlorine residuals within water distribution systems. While the internal water quality simulation capabilities only evaluates decay or growth of a single constituent, an extension is available (EPANET-MSX), which allows modeling of interactions between constituents.\n\nEPANET's computational engine is available for download as a separate dynamic link library for incorporation into other applications. The source code for EPANET 2 is available on the EPA's EPANET website.\n\nIn 2012 the EPANET toolkit, written in C, was rewritten in Java in a more object-oriented style. The code in Java is available on Github: https://github.com/Baseform/Baseform-Epanet-Java-Library.\n\nEPANET uses a binary file format, but also includes the capability for importing and exporting data in dxf, metafile, and ASCII file formats. EPANET's ASCII file format is called an input file within EPANET, and uses a file extension \".inp\". The input file can include data describing network topology, water consumption, and control rules, and is supported by many free and commercial modeling packages.\n\nWhile EPANET is used as the computational engine for most water distribution system models, most models are developed and maintained in hydraulic modeling packages based on EPANET's computational engine. Some of the major hydraulic modeling packages are:\n\nMost of these applications allow for multiple demand conditions, planning scenarios, and various methods of integrating with other data sources an agency may already have in place not supported in EPANET, such as GIS, and support additional types of analyses not found in EPANET.\n\n\n"}
{"id": "1602748", "url": "https://en.wikipedia.org/wiki?curid=1602748", "title": "Egg slicer", "text": "Egg slicer\n\nAn egg slicer is a food preparation utensil used to slice peeled, hard-boiled eggs quickly and evenly. An egg slicer consists of a slotted dish for holding the egg and a hinged plate of wires or blades that can be closed to slice.\nIt was invented at the beginning of the 20th century by the German (1875-1951) who also invented the bread cutter. The first egg slicers were produced in Berlin-Lichtenberg.\n\nDutch comedic duo Van Kooten en de Bie created a mock documentary about a Dutch egg slicer factory in 1983.\n\nSome egg slicers that have thin wires can be played as string instruments. A recorded example is English experimental music group Coil's 'The Gimp (Sometimes)', where primary group member John Balance played an egg slicer solo, dubbing it a 'mini-harp', on both the studio version and later live performances.\n\n"}
{"id": "19428835", "url": "https://en.wikipedia.org/wiki?curid=19428835", "title": "Epergne", "text": "Epergne\n\nEpergne ( or ) is a type of table centerpiece, usually made of silver, but may be made of any metal or glass or porcelain.\n\nAn epergne generally has a large central \"bowl\" or basket sitting on three to five feet. From this center \"bowl\" radiate branches supporting small baskets, dishes, or candleholders. There may be between two and seven branches. Epergnes were traditionally made from silver, however from around the start of the 20th century, glass was also employed.\n\nAn epergne may be used to hold any type of food or dessert. It may also be used as a designer object to hold candles, flowers or ornaments for a holiday etc. \n\nIn traditional use, an epergne is a fancy way to display side dishes, fruit, or sweetmeats, or can be used for chips, dips, or other finger foods etc.\n\nThe derivation is probably from the French \"épargne\" meaning \"saving,\" the idea being that dinner guests were saved the trouble of passing dishes (although an epergne in French is called a \"surtout\"). In addition the word \"épergne\" in French can also mean \"spare\", another way of saying \"to save\", or a spare, meaning \"reserve or extra\".\n"}
{"id": "1826418", "url": "https://en.wikipedia.org/wiki?curid=1826418", "title": "Feed line", "text": "Feed line\n\nIn a radio antenna, the feed line (feedline), or feeder, is the cable or other transmission line that connects the antenna with the radio transmitter or receiver. In a transmitting antenna, it feeds the radio frequency (RF) current from the transmitter to the antenna, where it is radiated as radio waves. In a receiving antenna it transfers the tiny RF voltage induced in the antenna by the radio wave to the receiver. In order to carry RF current efficiently, feed lines are made of specialized types of cable called transmission line. The most widely used types of feed line are coaxial cable, twin-lead, ladder line, and at microwave frequencies, waveguide.\n\nParticularly with a transmitting antenna, the feed line is a critical component that must be adjusted to work correctly with the antenna and transmitter. Each type of transmission line has a specific characteristic impedance. This must be matched to the impedance of the antenna and the transmitter, to transfer power efficiently to the antenna. If these impedances are not matched it can cause a condition called standing waves on the feed line, in which the RF energy is reflected back toward the transmitter, wasting energy and possibly overheating the transmitter. This adjustment is done with a device called an antenna tuner in the transmitter, and sometimes a matching network at the antenna. The degree of mismatch between the feedline and the antenna is measured by an instrument called an SWR meter (standing wave ratio meter), which measures the \"standing wave ratio\" (SWR) on the line. \n\nTwin lead is used to connect FM radios and television receivers with their antennas, although it has been largely replaced in the latter application by coaxial cable, and as a feedline for low power transmitters such as amateur radio transmitters. It consists of two wire conductors running parallel to each other with a precisely constant spacing, molded in polyethylene insulating material in a flat ribbon-like cable. The distance between the two wires is small relative to the wavelength of the RF signal carried on the wire. Furthermore, the RF current in one wire is equal in magnitude and opposite in direction to the RF current on the other wire (it is inverted). Thus, if both wires radiate energy equally, the radiated energies will cancel each other out and there will be near zero radiation at any distance from the wire. Twin lead is also immune to external noise or RF energies. Any unwanted external noise or unwanted RF energy induced on the wire from external energy sources will be induced in both wires at the same time and equally in magnitude and direction. At the end of the transmission line the inverted signal wire is restored to normal (non-inverted now) and added back to the original non-inverted signal wire by the receiving circuitry. Any noise will now be equal in magnitude and opposite in direction and cancel itself out. \n\nTwin lead is considered a Balanced line.\n\nCoaxial cable is probably the most widely used type of feedline, used for frequencies below the microwave (SHF) range. It consists of a wire center conductor and a braided or solid metallic \"shield\" conductor, usually copper or aluminum surrounding it. The center conductor is separated from the outer shield by a dielectric, usually plastic foam, to keep the separation between the two conductors precisely constant. The shield is covered with an outer plastic insulation jacket. In \"hard coax\" cable, used for high power transmitting applications like television transmitters, the shield is a rigid or flexible metal pipe containing a compressed gas such as nitrogen, and the internal conductor is held centered with periodic plastic spacers. It is a type of unbalanced line, the shield conductor is usually connected to electrical ground. Coaxial cable's advantage is that the enclosing shield conductor isolates the cable from external electromagnetic fields, so it is very immune to interference.\n\nWaveguide is used at microwave (SHF) frequencies, at which other types of feedline have excessive power losses. A waveguide is a hollow metallic conductor or pipe. It can have a circular or square cross-section. Waveguide runs are often pressurized with nitrogen gas to keep moisure out. The RF signal travels through the pipe similarly to the way sound travels in a tube. The metal walls keep it from radiating energy outwards and also prevent interference from entering the waveguide. Because of the cost and maintenance waveguide entails, microwave antennas often have the output stage of the transmitter or the RF front end of the receiver located at the antenna, and the signal is fed to or from the rest of the transmitter or receiver at a lower frequency, using coaxial cable. \n\nA waveguide is considered an unbalanced transmission line.\n\nThis is a comparison of a few common feed line characteristics.\nLarger lists are available in other articles, references, and directly from manufacturers.\n"}
{"id": "22360836", "url": "https://en.wikipedia.org/wiki?curid=22360836", "title": "Google Modular Data Center", "text": "Google Modular Data Center\n\nThe Google Modular Data Center was a modular data center built from a set of shipping containers, and used by Google to house some of its servers. \n\nThe data centers were rumored to cost $600 million USD each, and use from 50 to 103 megawatts of electricity. They housed the computing resources that comprise the Google platform.\n\nGoogle was reported in November 2005 to be working on their own shipping container datacenter. Google's patent on the concept was still pushed through the patent system and was successfully issued in October 2007. In 2009 Google announced that their first container based data center has been in production since 2005.\n\n\n"}
{"id": "1519778", "url": "https://en.wikipedia.org/wiki?curid=1519778", "title": "Hiboy", "text": "Hiboy\n\nA Hiboy is a specialized, high-clearance type of farm tractor designed to operate in high crops without damaging them. The largest producer of hiboys is Hagie Manufacturing Company of Clarion, Iowa, United States. The most common uses of hiboys are for detasseling, spraying herbicides, and applying glyphosate directly to weeds growing above crop height with a wick or wiper.\n"}
{"id": "2211921", "url": "https://en.wikipedia.org/wiki?curid=2211921", "title": "Ida Rhodes", "text": "Ida Rhodes\n\nIda Rhodes (born Hadassah Itzkowitz; May 15, 1900 – February 1, 1986) was an American mathematician who became a member of the clique of influential women at the heart of early computer development in the United States.\n\nHadassah Itzkowitz was born in a Jewish village between Nemyriv and Tulchyn in the Ukraine. She was 13 years old in 1913 when her parents, David and Bessie Sinkler Itzkowitz, brought her to the United States (her name was changed upon entering the U.S.)\n\nRhodes was awarded the New York State Cash Scholarship and a Cornell University Scholarship and began studying mathematics at Cornell University only six years after coming to the United States, from 1919-1923. During her time at Cornell University she worked as a nurse's aid at Ithaca City Hospital. She was elected to the honorary organizations Phi Beta Kappa (1922) and Phi Kappa Phi (1923). \nShe received her BA in mathematics in February, 1923 and her MA in September of the same year, graduating Phi Beta Kappa.\n\nRhodes had her first encounter with Albert Einstein in 1922 and in 1936 encountered him again in 1936 at Princeton, where a group of mathematicians traveled to spend the weekend in informal seminars. \nShe later studied at Columbia University in 1930-31. She held numerous positions involving mathematical computations before she joined the Mathematical Tables Project in 1940, where she worked under Gertrude Blanch, whom she would later credit as her mentor.\n\nIda Rhodes was a pioneer in the analysis of systems of programming, and with Betty Holberton designed the C-10 programming language in the early 1950s for the UNIVAC I. She also designed the original computer used for the Social Security Administration. In 1949, the Department of Commerce awarded her a Gold Medal for \"significant pioneering leadership and outstanding contributions to the scientific progress of the Nation in the functional design and the application of electronic digital computing equipment\".\n\nThough she retired in 1964, Rhodes continued to consult for the Applied Mathematics Division of the National Bureau of Standards until 1971. Her work became much more widely known after her retirement, as she took the occasion to travel around the globe, lecturing and maintaining international correspondence. In 1976, the Department of Commerce presented her with a further Certificate of Appreciation on the 25th Anniversary of UNIVAC I, and then at the 1981 Computer Conference cited her a third time as a \"UNIVAC I pioneer.\" She died in 1986.\n\nIn an unusual case of an old specialized algorithm still in use, and still credited to the original developer, Rhodes was responsible for the \"Jewish Holiday\" algorithm used in calendar programs to this day. While at the National Bureau of Standards (now NIST), she also did original work in machine translation of natural languages.\n\n"}
{"id": "21992478", "url": "https://en.wikipedia.org/wiki?curid=21992478", "title": "Integrated Sensor is Structure", "text": "Integrated Sensor is Structure\n\nThe Integrated Sensor is Structure (ISIS) was a program managed by the United States Air Force (USAF) Research Laboratory to research the feasibility of using an unmanned airship as a high-altitude aerial reconnaissance and surveillance platform. It is sometimes called Integrated Sensor is the Structure, as a fundamental innovation was the use of the airship structure as the sensing component of a state-of-the-art radar system.\n\nIn 2006, contracts were awarded to Raytheon for development of a large-area, light, Active electronically scanned array antenna which could be bonded to the structure of a blimp, Northrop Grumman for antenna development, and Lockheed Martin for development of the airship. As proposed, the -long surveillance airship could be launched from the US and stationed for up to 10 years at an altitude of , observing the movement of vehicles, aircraft, and people below. At that altitude, the airship would be beyond the range of most surface-to-air and air-to-air missiles. The airship would be filled with helium and powered, at least in part, by solar-powered hydrogen fuel cells.\n\nOn March 12, 2009, the USAF announced that it had budgeted $400 million for work on ISIS . In April 2009, DARPA awarded a $399.9 million contract to Lockheed Martin as the systems integrator and Raytheon as the radar developer for phase three of the project: the construction of a one-third scale model, which would remain in the air for up to a year. The ultimate goal was to provide radar capable of delivering persistent, wide-area surveillance tracking and engagement of air targets within a 600-kilometer area and ground targets within a area, according to DARPA. The model blimp was to have radar coverage of about 7,176 square yards (6,000 square meters) and be tested at an altitude of above the ground. The contract initially awarded $100 million to the two companies, with the rest to follow in phases, with a completion date of March 2013.\n\nAs of 2012, the development of the airframe had been delayed to focus on \"radar risk reduction\". The United States Department of Defense ended the program in 2015. $471 million had been spent from 2007 through 2012.\n\n"}
{"id": "35626979", "url": "https://en.wikipedia.org/wiki?curid=35626979", "title": "Introduction to Outer Space", "text": "Introduction to Outer Space\n\nIntroduction to Outer Space is a pamphlet about space exploration edited by the White House the 26 March 1958. At first, a report of the President's Science Advisory Committee presided by Dr James R. Killian in the aftermath of the Sputnik 1 launch, Dwight D. Eisenhower found it so informative and interesting that he decided to make it available to everybody for 15 cents. It presents in simple terms to the layman the future of space exploration.\n\nIt has been suggested that the \"Introduction to Outer Space\", produced in an effort to garner support for a national space program in the wake of the Sputnik flight, was the origin of the phrase \"Where no man has gone before\", as the booklet read on its first page:\n\nThe first of these factors is the compelling urge of man to explore and to discover, the thrust of curiosity that leads men to try to go where no one has gone before. Most of the surface of the earth has now been explored and men now turn on the exploration of outer space as their next objective.\n\n"}
{"id": "10694483", "url": "https://en.wikipedia.org/wiki?curid=10694483", "title": "Joint (building)", "text": "Joint (building)\n\nA building joint is a junction where building elements meet without applying a static load from one element to another. When one or more of these vertical or horizontal elements that meet are required by the local building code to have a fire-resistance rating, the resulting opening that makes up the joint must be firestopped in order to restore the required compartmentalisation.\n\nSuch joints are often subject to movement, as a function of the building's design basis. For a sample certification listing indicating fire tested motion, click here. Firestops must be able to demonstrate the ability to withstand operational movement prior to fire testing. Firestops for such building joints can be qualified to UL 2079 -- Tests for Fire Resistance of Building Joint Systems.\n\nWhether or not the building elements forming the joint have a fire-resistance rating, the joint design must still consider the anticipated operational movement of each joint. Timing is also important, as freshly poured concrete shrinks particularly during the first few months of a new building, potentially causing joint size changes.\n\nFirestopping at the \"perimeter slab edge\", which is gaps between the floors and the backpan of the curtain wall or precast concrete panels, slow the passage of fire and combustion gases between floors. The firestop at the perimeter slab edge is considered a continuation of the fire-resistance rating of the floor slab. The curtain wall itself, however, is not ordinarily required to have a rating. Fire compartmentalisation is based upon compartments enclosed on all sides to avoid fire and smoke migration beyond each engaged compartment. A non-fire-resistance rated curtain wall prevents the complete compartment (or envelope) from being fire-resistance rated. The provision of firestopping at the joint of the floor slab and curtain wall can still add to the overall fire safety of the building by slowing the advance of the fire in the building interior from floor to floor. The use of fire sprinklers has been shown to mitigate the spread of fire by keeping the fire from growing to a size large enough to cause structural damage and break the exterior glass. If the building is not equipped with sprinklers, the fire can travel up the outside of the curtain wall if the glass on the floor of fire origin is shattered due to radiant heat, causing flames to lick up the outside of the building, resulting in the glass in floors above to break. Falling glass can endanger pedestrians, firefighters and firehoses below. An example of mode of fire spread is the First Interstate Bank Fire in Los Angeles, California. This building is now the Aon Center.\n\nThe fire leapfrogged up the tower by shattering the glass. The exterior curtain wall aluminium skeleton holding the glass failed. Exceptionally sound cementitious spray fireproofing also helped to delay and ultimately to avoid the possible collapse of the building, by preventing the steel building structure from reaching the critical temperature at which steel weakens and can no longer hold its intended load. This fire proved the positive, collective effect of both active fire protection (sprinklers) to control fire size in its incipient stages, and passive fire protection (fireproofing) if the fire sprinklers are not effective.\n\nWhere fire-resistance rated wall assemblies, be they masonry or drywall, meet the underside of the floor slab above, a movement joint results, which can be subject to compression, as the freshly placed concrete cures and shrinks all over a new building. This joint must be firestopped in a flexible manner. For concrete block walls that are not load-bearing, the contractor is often instructed to stop the wall about 25mm short of the slab above. The drywaller has to fasten a track to the underside of the slab above, which holds the metal studs. The drywall is then either stopped short allowing for a flexible buffer to be inserted or caulked in place, or one may use purpose-made tracks that hold the studs but permit vertical movement without damage to the system.\n\n\n"}
{"id": "1291409", "url": "https://en.wikipedia.org/wiki?curid=1291409", "title": "Laydown delivery", "text": "Laydown delivery\n\nLaydown delivery is a mode of attack using a freefall nuclear weapon: the bomb's descent to the target is slowed by ribbon parachute so that it actually lands on the ground before detonating. Laydown delivery requires the weapon's case to be reinforced so that it can survive the force of impact and generally involves a time-delay fuze to trigger detonation e.g. 45 seconds after hitting the ground. Laydown mode can be used to increase the effect of the weapon's blast on built-up targets such as submarine pens or to transmit a shock wave through the ground to attack deeply-buried targets. An attack of this type produces large amounts of radioactive fallout.\n\nIt has the additional advantage of allowing the carrier aircraft to fly very low and still escape from ground zero without being damaged or destroyed by effects of the nuclear explosion. That is particularly important for high-yield nuclear weapons such as the B83 and B53 nuclear bombs. Low-altitude delivery also helps hide the aircraft from surface-to-air missiles. It was for that reason that laydown was selected for the Vickers Valiant bomber of the Royal Air Force, as the design became increasingly vulnerable to Soviet weapons, especially the SA-2 missile. The low-level laydown delivery was referred to as \"Equipment 2 Foxtrot\" in RAF parlance; alternatives included \"2 Echo\" toss bombing and \"2 Hotel\", a particular climbing delivery method used by the Avro Vulcan.\n\n"}
{"id": "40171", "url": "https://en.wikipedia.org/wiki?curid=40171", "title": "Lead(II) azide", "text": "Lead(II) azide\n\nLead azide (Pb(N)) is an inorganic compound. More so than other azides, is explosive. It is used in detonators to initiate secondary explosives. In a commercially usable form, it is a white to buff powder.\n\nLead azide is prepared by metathesis between sodium azide and lead nitrate. Dextrin can be added to the solution to stabilize the precipitated product. The solid is not very hygroscopic, and water does not reduce its impact sensitivity. It is normally shipped in a dextrinated solution that lowers its sensitivity. When protected from humidity, it is completely stable in storage. An alternative method involves dissolving lead acetate in a sodium azide solution.\n\nLead azide in its pure form was first prepared by Theodor Curtius in 1891. Due to sensitivity and stability concerns, the dextrinated form of lead azide (MIL-L-3055) was developed in the 1920s and 1930s with large scale production by DuPont Co beginning in 1932. Detonator development during World War II resulted in the need for a form of lead azide with a more brisant output. RD-1333 lead azide (MIL-DTL-46225), a version of lead azide with sodium carboxymethylcellulose as a precipitating agent, was developed to meet that need. The Vietnam War saw an accelerated need for lead azide and it was during this time that Special Purpose Lead Azide (MIL-L-14758) was developed; the US government also began stockpiling lead azide in large quantities. After the Vietnam War, the use of lead azide dramatically decreased. Due to the size of the US stockpile, the manufacture of lead azide in the US ceased completely by the early 1990s. In the 2000s, concerns about the age and stability of stockpiled lead azide led the US government to investigate methods to dispose of its stockpiled lead azide and obtain new manufacturers.\n\nLead azide is highly sensitive and usually handled and stored under water in insulated rubber containers. It will explode after a fall of around 150 mm (6 in) or in the presence of a static discharge of 7 millijoules. Its detonation velocity is around .\n\nAmmonium acetate and sodium dichromate are used to destroy small quantities of lead azide.\n\nLead azide has immediate deflagration to detonation transition (DDT) this means that even small amounts undergo full detonation(after being hit by flame or static electricity).\n\nLead azide reacts with copper, zinc, cadmium, or alloys containing these metals to form other azides. For example, copper azide is even more explosive and too sensitive to be used commercially.\n\nLead azide was a component of the six .22 caliber Devastator rounds fired from a Röhm RG-14 revolver by John Hinckley, Jr. in his assassination attempt on U.S. President Ronald Reagan on March 30, 1981. The rounds consisted of lead azide centers with lacquer-sealed aluminum tips designed to explode upon impact.\n\n\n"}
{"id": "2397236", "url": "https://en.wikipedia.org/wiki?curid=2397236", "title": "Linux adoption", "text": "Linux adoption\n\nLinux adoption is the adoption of Linux computer operating systems (OS) by households, nonprofit organizations, businesses, and governments.\n\nMany factors have resulted in the expanded use of Linux systems by traditional desktop users as well as operators of server systems, including the desire to minimize software costs, increase network security and support for open-source philosophical principles. In recent years several governments, at various levels, have enacted policies shifting state-owned computers to Linux from proprietary software regimes.\n\nIn August 2010, Jeffrey Hammond, principal analyst at Forrester Research, declared, \"Linux has crossed the chasm to mainstream adoption,\" a statement attested by the large number of enterprises that had transitioned to Linux during the late-2000s recession. In a company survey completed in the third quarter of 2009, 48% of surveyed companies reported using an open-source operating system.\n\nThe Linux Foundation regularly releases publications regarding the Linux kernel, Linux OS distributions, and related themes. One such publication, \"Linux Adoption Trends: A Survey of Enterprise End Users,\" is freely available upon registration.\n\nTraditionally, the term Linux adoption, refers to adoption of a Linux OS made for \"desktop\" computers, the original intended use (or adoption on servers, that is essentially the same form of OS). Adoption of that form on personal computers is still low relatively, while adoption of the Android operating system is very high. The term Linux adoption, often overlooks that operating system or other uses such as in Chrome OS that also use the Linux kernel (but have almost nothing else in common, not even the name – Linux – usually applied; while Android is the most popular variant – in fact the most popular operating system in the world).\n\nOutside of traditional web services, Linux powers many of the biggest Internet properties (e.g., Google, Amazon, Facebook, eBay, Twitter or Yahoo!).\n\nLinux is used on desktop computers, servers and supercomputers, as well as a wide range of devices.\n\nBecause Linux desktop distributions are not usually distributed by retail sale, there are no sales numbers that indicate the number of users. One downloaded file may be used to create many CDs and each CD may be used to install the operating system on multiple computers. On the other hand, the file might be used only for a test and the installation erased soon after. Due to these factors estimates of current Linux desktop often rely on webpage hits by computers identifying themselves as running Linux. The use of these statistics has been criticized as unreliable and as underestimating Linux use.\n\nUsing webpage hits as a measure, until 2008, Linux accounted for only about 1% of desktop market share, while Microsoft Windows operating systems held more than 90%. This might have been because Linux was not seen at that time as a direct replacement for Windows.\n\n, \"W3Counter\" estimated \"Linux\" web browser market share to be 4.63%, while \"Android\" versions 6, 5 and 4 combined (which is based on the Linux kernel) were estimated to be 33.77%.\n\nThe \"Linux Counter\" uses an alternate method of estimating adoption, asking users to register and then using a mathematical model to estimate the total number of desktop users. In November 2014 this method estimated 73 million Linux users.\n\nIn September 2014 Pornhub released usage statistics of their website and reported 1.7% Linux users.\n\nThe Unity game engine gathers user statistics and showed in March 2016 0.4% Linux users. Similarly, the Steam client tracks usage and reported in May 2015 around 1% Linux users.\n\nIn April 2009, Aaron Seigo of KDE indicated that most web-page counter methods produce Linux adoption numbers that are far too low given the system's extensive penetration into non-North American markets, especially China. He stated that the North American-based web-measurement methods produce high Windows numbers and ignore the widespread use of Linux in other parts of the world. In estimating true worldwide desktop adoption and accounting for the Windows-distorted environment in the USA and Canada he indicated that at least 8% of the world desktops run Linux distributions and possibly as high as 10–12% and that the numbers are rising quickly. Other commentators have echoed this same belief, noting that competitors are expending a lot of effort to discredit Linux, which is incongruent with a tiny market share:\n\nIn May 2009, Preston Gralla, contributing editor to Computerworld.com, in reacting to the Net Applications web hit numbers showing that Linux use was over 1%, said that \"Linux will never become an important desktop or notebook operating system\". He reasoned that the upsurge in Linux desktop use recently seen was due to Linux netbooks, a trend he saw as already diminishing and which would be further eroded when Windows 7 became available (and indeed, Linux netbooks did fall by the wayside, though whether they were solely responsible for the upsurge in Linux usage is open to question). He concluded: \"As a desktop operating system, Linux isn't important enough to think about. For servers, it's top-notch, but you likely won't use it on your desktop – even though it did finally manage to crack the 1% barrier after 18 years\".\n\nIn 2009, Microsoft then-CEO Steve Ballmer indicated that Linux had a greater desktop market share than Mac, stating that in recent years Linux had \"certainly increased its share somewhat\". Just under a third of all Dell netbook sales in 2009 had Linux installed.\n\nCaitlyn Martin, researching retail market numbers in the summer of 2010 also concluded that the traditional numbers mentioned for Linux desktop adoption were far too low:\n\nReasons to change from other operating systems to Linux include better system stability, better malware protection, low or no cost, that most distributions come complete with application software and hardware drivers, simplified updates for all installed software, free software licensing, availability of application repositories and access to the source code. Linux desktop distributions also offer multiple desktop workspaces, greater customization, free and unlimited support through forums, and an operating system that doesn't slow down over time. Environmental reasons are also cited, as Linux operating systems usually do not come in boxes and other retail packaging, but are downloaded via the Internet. The lower system specifications also mean that older hardware can be kept in use instead of being recycled or discarded. Linux distributions also get security vulnerabilities patched much more quickly than non-free operating systems and improvements in Linux have been occurring at a faster rate than those in Windows.\n\nA report in The Economist in December 2007 said:\n\nFurther investments have been made to improve desktop Linux usability since that 2007 report.\n\nIndian bulk computer purchaser the Electronics Corporation of Tamil Nadu (ELCOT) started recommending only Linux in June 2008. Following testing they stated: \"ELCOT has been using SUSE Linux and Ubuntu Linux operating systems on desktop and laptop computers numbering over 2,000 during the past two years and found them far superior as compared to other operating systems, notably the Microsoft Operating System.\"\n\nIn many developing nations, such as China, where, due to widespread software piracy, Microsoft Windows can be easily obtained for free, Linux distributions are gaining a high level of adoption. Hence in these countries where there is essentially no cost barrier to obtaining proprietary operating systems, users are adopting Linux based on its merit, rather than on price.\n\nIn January 2001, Microsoft then-CEO Bill Gates explained the attraction of adopting Linux in an internal memo that was released in the \"Comes vs Microsoft\" case. He said:\n\nThe greatest barrier to Linux desktop adoption is probably that few desktop PCs come with it from the factory. A.Y. Siu asserted in 2006 that most people use Windows simply because most PCs come with Windows pre-installed; they didn't choose it. Linux has much lower market penetration because in most cases users have to install it themselves, a task that is beyond the capabilities of many PC users: \"Most users won’t even use Windows restore CDs, let alone install Windows from scratch. Why would they install an unfamiliar operating system on their computers?\"\n\nTechRepublic writer Jack Wallen expands on this barrier, saying in August 2008:\n\nLinus Torvalds stated, in his June 2012 interaction with students at Aalto University, that although Linux was originally conceived as a desktop system, that has been the only market where it has not flourished. He suggested that the key reason that keeps Linux from getting a substantial presence in the desktop market is that the average desktop user does not want to install an operating system, so getting manufacturers to sell computers with Linux pre-installed would be the missing piece to fulfill the vision of Linux in the desktop market. He added that Chromebooks, by shipping with the Linux-based Chrome OS, could provide the key turning point in such a transition, much like Android allowed Linux to spread in the mobile space.\n\nIn September 2012, GNOME developer Michael Meeks also indicated that the main reason for the lack of adoption of Linux desktops is the lack of manufacturers shipping computers with it pre-installed, supporting Siu's arguments from six years earlier. Meeks also indicated that users wouldn't embrace desktop Linux until there is a wider range of applications and developers won't create that wider range of applications until there are more users, a classic Catch-22 situation.\n\nIn an openSUSE survey conducted in 2007, 69.5% of respondents said they dual booted a Microsoft Windows operating system in addition to a Linux operating system. In early 2007 Bill Whyman, an analyst at Precursor Advisors, noted that \"there still isn't a compelling alternative to the Microsoft infrastructure on the desktop.\"\n\nApplication support, the quality of peripheral support, and end user support were at one time seen as the biggest obstacles to desktop Linux adoption. According to a 2006 survey by The Linux Foundation, these factors were seen as a \"major obstacle\" for 56%, 49%, and 33% of respondents respectively at that time.\n\nThe November 2006 \"Desktop Linux Client Survey\" identified the foremost barrier for deploying Linux desktops was that users were accustomed to Windows applications which had not been ported to Linux and which they \"just can't live without\". These included Microsoft Office, Adobe Photoshop, Autodesk AutoCAD, Microsoft Project, Visio and Intuit QuickBooks. This creates a chicken or the egg situation where developers make programs for Windows due to its market share, and consumers use Windows due to availability of said programs.\nIn a DesktopLinux.com survey conducted in 2007, 72% of respondents said they used ways to run Windows applications on Linux.\n\n51% of respondents to the 2006 Linux Foundation survey, believed that cross-distribution Linux desktop standards should be the top priority for the Linux desktop community, highlighting the fact that the fragmented Linux market is preventing application vendors from developing, distributing and supporting the operating system. In May 2008, Gartner predicted that \"version control and incompatibilities will continue to plague open-source OSs and associated middleware\" in the 2013 timeframe.\n\nBy 2008, the design of Linux applications and the porting of Windows and Apple applications had progressed to the point where it was difficult to find an application that did not have an equivalent for Linux, providing adequate or better capabilities.\n\nAn example of application progress can be seen comparing the main productivity suite for Linux, OpenOffice.org, to Microsoft Office. With the release of OpenOffice.org 3.0 in October 2008 Ars Technica assessed the two:\n\nIn the past the availability and quality of open source device drivers were issues for Linux desktops. Particular areas which were lacking drivers included printers as well as wireless and audio cards. For example, in early 2007, Dell did not sell specific hardware and software with Ubuntu 7.04 computers, including printers, projectors, Bluetooth keyboards and mice, TV tuners and remote controls, desktop modems and Blu-ray disc drives, due to incompatibilities at that time, as well as legal issues.\n\nBy 2008, most Linux hardware support and driver issues had been adequately addressed. In September 2008, Jack Wallen's assessment was:\n\nSome critics have stated that compared to Windows, Linux is lacking in end-user support. Linux has traditionally been seen as requiring much more technical expertise. Dell's website described open source software as requiring intermediate or advanced knowledge to use. In September 2007, the founder of the Ubuntu project, Mark Shuttleworth, commented that \"it would be reasonable to say that this is not ready for the mass market.\"\n\nIn October 2004, Chief Technical Officer of Adeptiva Linux, Stephan February, noted at that time that Linux was a very technical software product, and few people outside the technical community were able to support consumers. Windows users are able to rely on friends and family for help, but Linux users generally use discussion boards, which can be uncomfortable for consumers.\n\nIn 2005, Dominic Humphries summarized the difference in user tech support:\n\nMore recently critics have found that the Linux user support model, using community-based forum support, has greatly improved. In 2008 Jack Wallen stated:\n\nIn addressing the question of user support, Manu Cornet said:\n\nLinux's credibility has also been under attack at times, but as Ron Miller of LinuxPlanet points out:\n\nThere is continuing debate about the total cost of ownership of Linux, with Gartner warning in 2005 that the costs of migration may exceed the cost benefits of Linux. Gartner reiterated the warning in 2008, predicting that \"by 2013, a majority of Linux deployments will have no real software total cost of ownership (TCO) advantage over other operating systems.\" However, in the Comes v. Microsoft lawsuit, Plaintiff's exhibit 2817 revealed that Microsoft successfully lobbied Gartner for changing their TCO model in favour of Microsoft in 1998. Organizations that have moved to Linux have disagreed with these warnings. Sterling Ball, CEO of Ernie Ball, the world's leading maker of premium guitar strings and a 2003 Linux adopter, said of total cost of ownership arguments: \"I think that's propaganda...What about the cost of dealing with a virus? We don't have 'em...There's no doubt that what I'm doing is cheaper to operate. The analyst guys can say whatever they want.\"\n\nIn the SCO-Linux controversies, the SCO Group had alleged that UNIX source code donated by IBM was illegally incorporated into Linux. The threat that SCO might be able to legally assert ownership of Linux initially caused some potential Linux adopters to delay that move. The court cases bankrupted SCO in 2007 after it lost its four-year court battle over the ownership of the UNIX copyrights. SCO's case had hinged on showing that Linux included intellectual property that had been misappropriated from UNIX, but the case failed when the court discovered that Novell and not SCO was the rightful owner of the copyrights. During the legal process, it was revealed that SCO's claims about Linux were fraudulent and that SCO's internal source code audits had showed no evidence of infringement.\n\nA rival operating system vendor, Green Hills Software, has called the open source paradigm of Linux \"fundamentally insecure\".\n\nThe US Army does not agree that Linux is a security problem. Brigadier General Nick Justice, the Deputy Program Officer for the Army's Program Executive Office, Command, Control and Communications Tactical (PEO C3T), said in April 2007:\n\nIn 2008, Gartner analysts predicted that mobile devices like Netbooks with Linux could potentially break the dominance of Microsoft's Windows as operating system provider, as the netbook concept focuses on OS-agnostic applications built as Web applications and browsing. Until 2008 the netbook market was dominated by Linux-powered devices; this changed in 2009 after Windows XP became available as option. One of the reasons given was that many customers returned Linux-based netbooks as they were still expecting a Windows-like environment, despite the netbook vision: a web-surfing and web-application device.\n\nIn 2011, Google introduced the Chromebook, a web thin client running the Linux-based Chrome OS, with the ability to use web applications and remote desktop in to other computers running Windows, Mac OS X, a traditional Linux distribution or Chrome OS, using Chrome Remote Desktop. In 2012 Google and Samsung introduced the first version of the Chromebox, a small-form-factor desktop equivalent to the Chromebook.\n\nBy 2013, Chromebooks had captured 20–25% of the sub-$300 US laptop market.\n\n\"Note: The term \"mobile devices\" in the computing context refers to cellphones and tablets; \"\", the term does not include regular laptops, despite the fact that they have always been designed to be mobile.\"\n\nAndroid, which is based on Linux and is open source, is the most popular mobile platform. During the second quarter of 2013, 79.3% of smartphones sold worldwide were running Android. Android tablets are also available.\n\nFirefox OS was another open source Linux-based mobile operating system, which has now been discontinued.\n\nNokia previously produced some phones running a variant of Linux (e.g. the Nokia N900), but in 2013, Nokia's handset division was bought by Microsoft.\n\nSmartphones are gradually replacing these kinds of embedded devices, but they still exist. An example are the Portable media players. Some of the OEM firmware is Linux based. A community-driven fully free and open-source project is Rockbox.\n\nIn-vehicle infotainment hardware usually involves some kind of display, either built into the Dashboard or additional displays. The GENIVI Alliance works on a Linux-based open platform to run the IVI. It may have an interface to some values delivered by the Engine control unit but is albeit completely separate system. There will be a special variant of Tizen for IVI, different for the Tizen for smartphones in several regards.\n\nLinux is often used in various single- or multi-purpose computer appliances and embedded systems.\n\nCustomer-premises equipment are a group of devices that are embedded and have no graphical user interface in the common sense. Some are remotely operated via Secure Shell or via some Web-based user interface running on some lightweight web server software. Most of the OEM firmware is based on the Linux kernel and other free and open-source software, e.g. Das U-Boot and Busybox. There are also a couple of community driven projects, e.g. OpenWrt.\n\nSmaller scale embedded network-attached storage-devices are also mostly Linux-driven.\n\nLinux became popular in the Internet server market particularly due to the LAMP software bundle. In September 2008 Steve Ballmer (Microsoft CEO) claimed 60% of servers run Linux and 40% run Windows Server. According to IDC's report covering Q2 2013, Linux was up to 23.2% of worldwide server revenue although this does compensate for the potential price disparity between Linux and non-Linux servers. In May 2014, W3Techs estimated that 67.5% of the top 10 million (according to Alexa) websites run some form of Unix, and Linux is used by at least 57.2% of all those websites which use Unix.\n\nLinux-based solution stacks come with all the general advantages and benefits of free and open-source software. Some more commonly known examples are:\n\nAccording to the Netcraft, , the Apache HTTP Server had the highest market share.\n\nThere are various freely available implementations of LDAP servers. Additionally, Univention Corporate Server, as an integrated management system based on Debian, supports the functions provided by Microsoft Active Directory for the administration of computers running Microsoft Windows.\n\nFree routing software available for Linux includes BIRD, B.A.T.M.A.N., Quagga and XORP. Whether on Customer-premises equipment, on Personal computer hardware or on server-hardware, the mainline Linux kernel or an adapted highly optimized Linux kernel is capable of doing routing at rates that are limited by the hardware bus throughput.\n\nLinux is the most popular operating system among supercomputers due to the general advantages and benefits of free and open-source software, like superior performance, flexibility, speed and lower costs. In November 2008 Linux held an 87.8 percent share of the world's top 500 supercomputers.\n\nAs of June 2016, the operating systems used on the world's top 500 supercomputers were:\nIn January 2010, Weiwu Hu, chief architect of the Loongson family of CPUs at the Institute of Computing Technology, which is part of the Chinese Academy of Sciences, confirmed that the new Dawning 6000 supercomputer will use Chinese-made Loongson processors and will run Linux as its operating system. The most recent supercomputer the organization built, the Dawning 5000a, which was first run in 2008, used AMD chips and ran Windows HPC Server 2008.\n\nMany organizations advocate for Linux adoption. The foremost of these is the Linux Foundation which hosts and sponsors the key kernel developers, manages the Linux trademark, manages the Open Source Developer Travel Fund, provides legal aid to open source developers and companies through the Linux Legal Defense Fund, sponsors kernel.org and also hosts the Patent Commons Project.\n\nThe International Free and Open Source Software Foundation (iFOSSF) is a nonprofit organization based in Michigan, USA dedicated to accelerating and promoting the adoption of FOSS worldwide through research and civil society partnership networks.\n\nThe Open Invention Network was formed to protect vendors and customers from patent royalty fees while using OSS.\n\nOther advocates for Linux include:\n\nGartner claimed that Linux-powered personal computers accounted for 4% of unit sales in 2008. However, it is common for users to install Linux in addition to (as a dual boot arrangement) or in place of a factory-installed Microsoft Windows operating system.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49008928", "url": "https://en.wikipedia.org/wiki?curid=49008928", "title": "List of Windows 10 Mobile devices", "text": "List of Windows 10 Mobile devices\n\nThis is a list of all devices coming natively with Microsoft's Windows 10 Mobile operating system. The list also includes devices running two additional flavours of Windows 10 for mobile devices, Windows 10 Mobile Enterprise and Windows 10 IoT Mobile Enterprise. All devices below come with SD card support.\n\nProcessors supported are Qualcomm's Snapdragon 210, 212, 410, 617, 800, 801, 808, 810 and 820 as well as Rockchip's RK3288.\n\n"}
{"id": "666314", "url": "https://en.wikipedia.org/wiki?curid=666314", "title": "M4 (missile)", "text": "M4 (missile)\n\nThe M4 was a French submarine-launched ballistic missile (SLBM) deployed on the nuclear s (except the \"Redoutable\" herself, which was not refitted).\n\nThey entered service on May 1, 1985. They were the first French MIRV-capable nuclear missiles, with six 150 kilotonne warheads.\n\nThey were replaced by the improved M45 SLBM missile which is currently in active service aboard the\n\n"}
{"id": "85332", "url": "https://en.wikipedia.org/wiki?curid=85332", "title": "Man page", "text": "Man page\n\nA man page (short for manual page) is a form of software documentation usually found on a Unix or Unix-like operating system. Topics covered include computer programs (including library and system calls), formal standards and conventions, and even abstract concepts. A user may invoke a man page by issuing the codice_1 command.\n\nBy default, codice_1 typically uses a terminal pager program such as codice_3 or codice_4 to display its output.\n\nIn the first two years of the history of Unix, no documentation existed. The \"Unix Programmer's Manual\" was first published on November 3, 1971. The first actual man pages were written by Dennis Ritchie and Ken Thompson at the insistence of their manager Doug McIlroy in 1971. Aside from the man pages, the \"Programmer's Manual\" also accumulated a set of short papers, some of them tutorials (e.g. for general Unix usage, the C programming language, and tools such as Yacc), and others more detailed descriptions of operating system features. The printed version of the manual initially fit into a single binder, but as of PWB/UNIX and the 7th Edition of Research Unix, it was split into two volumes with the printed man pages forming Volume 1.\n\nLater versions of the documentation imitated the first man pages' terseness. Ritchie added a \"How to get started\" section to the Third Edition introduction, and Lorinda Cherry provided the \"Purple Card\" pocket reference for the Sixth and Seventh Editions. Versions of the software were named after the revision of the manual; the seventh edition of the \"Unix Programmer's Manual\", for example, came with the 7th Edition or Version 7 of Unix.\n\nFor the Fourth Edition the man pages were formatted using the troff typesetting package and its set of codice_5 macros (which were completely revised between the Sixth and Seventh Editions of the \"Manual\", but have since not drastically changed). At the time, the availability of online documentation through the manual page system was regarded as a great advance. To this day, virtually every Unix command line application comes with a man page, and many Unix users perceive a program's lack of man pages as a sign of low quality; indeed, some projects, such as Debian, go out of their way to write man pages for programs lacking one. The modern descendants of 4.4BSD also distribute man pages as one of the primary forms of system documentation (having replaced the old codice_5 macros with the newer codice_7).\n\nFew alternatives to codice_1 have enjoyed much popularity, with the possible exception of GNU Project's \"codice_9\" system, an early and simple hypertext system.\nIn addition, some Unix GUI applications (particularly those built using the GNOME and KDE development environments) now provide end-user documentation in HTML and include embedded HTML viewers such as codice_10 for reading the help within the application.\n\nMan pages are usually written in English, but translations into other languages may be available on the system.\nThe default format of the man pages is troff, with either the macro package man (appearance oriented) or mdoc (semantic oriented). This makes it possible to typeset a man page into PostScript, PDF, and various other formats for viewing or printing.\n\nMost Unix systems have a package for the man2html command, which enables users to browse their man pages using an html browser (textproc/man2html on FreeBSD or \"man\" on some Linux distribution).\n\nIn 2010, OpenBSD deprecated troff for formatting manpages in favour of mandoc, a specialised compiler/formatter for manpages with native support for output in PostScript, HTML, XHTML, and the terminal.\n\nIn February 2013, the BSD community saw a new open source mdoc.su service launched, which unified and shortened access to the man.cgi scripts of the major modern BSD projects through a unique nginx-based deterministic URL shortening service for the *BSD man pages.\n\nThere was a hidden easter egg in the man-db version of the man command that would cause the command to return \"gimme gimme gimme\" when run at 00:30 (a reference to the ABBA song Gimme! Gimme! Gimme! (A Man After Midnight). It was introduced in 2011 but first restricted and then removed in 2017 after finally being found.\n\nTo read a manual page for a Unix command, a user can type:\n\nPages are traditionally referred to using the notation \"name(section)\": for example, . The section refers to different ways the topic might be referenced - for example, as a system call, or a shell (command line) command or package, or a package's configuration file, or as a coding construct / header.\n\nThe same page name may appear in more than one section of the manual, such as when the names of system calls, user commands, or macro packages coincide. Examples are and , or and . The syntax for accessing the non-default manual section varies between different man implementations. \n\nOn Solaris and illumos, for example, the syntax for reading is:\n\nOn Linux and BSD derivatives the same invocation would be:\nwhich searches for \"printf\" in section 3 of the man pages.\n\nThe manual is generally split into eight numbered sections, organized as follows (on Research Unix, BSD, macOS and Linux):\n\nUnix System V uses a similar numbering scheme, except in a different order:\n\nOn some systems some of the following sections are available:\n\nSome sections are further subdivided by means of a suffix; for example, in some systems, section 3C is for C library calls, 3M is for the math library, and so on. A consequence of this is that section 8 (system administration commands) is sometimes relegated to the 1M subsection of the main commands section. Some subsection suffixes have a general meaning across sections:\nSome versions of man cache the formatted versions of the last several pages viewed.\n\nAll man pages follow a common layout that is optimized for presentation on a simple ASCII text display, possibly without any form of highlighting or font control. Sections present may include: \n\n\nOther sections may be present, but these are not well standardized across man pages. Common examples include: OPTIONS, EXIT STATUS, RETURN VALUE, ENVIRONMENT, BUGS, FILES, AUTHOR, REPORTING BUGS, HISTORY and COPYRIGHT.\n\n\n"}
{"id": "44242891", "url": "https://en.wikipedia.org/wiki?curid=44242891", "title": "Nahum Sharfman", "text": "Nahum Sharfman\n\nNahum Sharfman was an Israeli tech entrepreneur who, along with Amir Ashkenazi, founded Shopping.com. Shopping.com was later acquired by eBay for $650 million. He was also the Chairman of social content sharing site eSnips Ltd. \n\nPrior to shopping.com, Sharfman co-founded the IT security company Commtouch for which he served as the Chairman of the Board from its inception in February 1991 to November 1997. Commtouch IPO'ed in 2000.\n\nPrior to starting his own ventures, Sharfman spent 11 years working for National Semiconductor. He received a Ph.D. in High Energy Nuclear Physics from Carnegie Mellon University and M.S. and B.S. degrees in Physics from the Technion, the Israel Institute of Technology.\n\nSharfman and his wife, Nava, died in a plane crash on the Mount Ainos in the Ioanian island of Cephallonia, Greece while en route to Corfu on April 29, 2009.\n"}
{"id": "44765252", "url": "https://en.wikipedia.org/wiki?curid=44765252", "title": "Nano electrokinetic thruster", "text": "Nano electrokinetic thruster\n\nThe Nano electrokinetic thruster is a theoretical space propulsion system based on the principle of electro-osmosis (also electroosmotic flow). It allows for a high specific impulse and high thrust-to-power ratio as well as a high final velocity which makes it suitable for a wide variety of applications. Due to difficulties in the production of the needed carbon nanotubes experimental testing has not yet started.\n\nThe principle of electro-osmosis or electroosmotic flow creates a flow of an electrolyte through a very small tube in the nano-meter range. To achieve this flow there is a cathode and an anode at the ends of the tube over which a voltage is applied. Due to this voltage the ions in the electrolyte stored in a reservoir directly connected to the tube can be accelerated and ejected. This way electrical energy is transformed into kinetic energy. The amount of thrust created by one nano thruster is in the micro newton range, however due to its size it makes sense to arrange a big amount in an array to achieve sufficient thrust. The thrust, exit velocity of the ions and the mass flow rate of the electrolyte are influenced by the applied voltage which makes it easy to regulate those parameters. The applied voltage and the pH-value of the electrolyte (amount of ions it contains) also vary the balance between thrust, efficiency and maximal exhaust velocity (determines the maximal achievable flight velocity). It is also theoretically possible to achieve a very high efficiency of nearly 100% as well as a high specific impulse and high thrust-to-power ratio. This system has not yet been built and experimentally tested because of difficulties with the production of the nano-tubes needed for it.\n\nNano electrokinetic thrusters have a very high efficiency, specific impulse, exhaust velocity and thrust-to-power ratio which make them suitable for a wide variety of applications. Due to the fact that a thruster is made up out of an array of multiple nano thrusters it is easily possible to design thursters in all sizes and thrust ranges. These properties give the nano electrokinetic thrusters very good thrust control which makes it applicable for a wide range of spacecraft ranging from maneuvering thrusters for small spacecraft, such as satellites, to the primary propulsion system of interplanetary or interstellar spacecraft. This system also doesn't require additional heat or radiation shielding to protect the rest of the space craft which make the system (not including fuel compartment) light in comparison to other designs.\n\nThe production of the required carbon nano tubes is very expensive and with current production methods the amount of surface defects in the produced carbon nano tubes is high which reduces the efficiency significantly and makes it unreliable. This design also requires a high potential difference in the range of 300 to 500 volts as well as a sufficient storage tank for the liquid electrolyte needed which increase the weight of the overall system.\n"}
{"id": "22151", "url": "https://en.wikipedia.org/wiki?curid=22151", "title": "Nuclear reactor", "text": "Nuclear reactor\n\nA nuclear reactor, formerly known as an atomic pile, is a device used to initiate and control a self-sustained nuclear chain reaction. Nuclear reactors are used at nuclear power plants for electricity generation and in propulsion of ships. Heat from nuclear fission is passed to a working fluid (water or gas), which in turn runs through steam turbines. These either drive a ship's propellers or turn electrical generators' shafts. Nuclear generated steam in principle can be used for industrial process heat or for district heating. Some reactors are used to produce isotopes for medical and industrial use, or for production of weapons-grade plutonium. Some are run only for research. As of April 2014, the IAEA reports there are 435 nuclear power reactors in operation, in 31 countries around the world.\nBy 2017, this increased to 447 operable reactors according to the World Nuclear Association.\n\nJust as conventional power-stations generate electricity by harnessing the thermal energy released from burning fossil fuels, nuclear reactors convert the energy released by controlled nuclear fission into thermal energy for further conversion to mechanical or electrical forms.\n\nWhen a large fissile atomic nucleus such as uranium-235 or plutonium-239 absorbs a neutron, it may undergo nuclear fission. The heavy nucleus splits into two or more lighter nuclei, (the fission products), releasing kinetic energy, gamma radiation, and free neutrons. A portion of these neutrons may later be absorbed by other fissile atoms and trigger further fission events, which release more neutrons, and so on. This is known as a nuclear chain reaction.\n\nTo control such a nuclear chain reaction, neutron poisons and neutron moderators can change the portion of neutrons that will go on to cause more fission. Nuclear reactors generally have automatic and manual systems to shut the fission reaction down if monitoring detects unsafe conditions.\n\nCommonly used moderators include regular (light) water (in 74.8% of the world's reactors), solid graphite (20% of reactors) and heavy water (5% of reactors). Some experimental types of reactor have used beryllium, and hydrocarbons have been suggested as another possibility.\n\nThe reactor core generates heat in a number of ways:\n\nA kilogram of uranium-235 (U-235) converted via nuclear processes releases approximately three million times more energy than a kilogram of coal burned conventionally (7.2 × 10 joules per kilogram of uranium-235 versus 2.4 × 10 joules per kilogram of coal).\n\nA nuclear reactor coolant — usually water but sometimes a gas or a liquid metal (like liquid sodium) or molten salt — is circulated past the reactor core to absorb the heat that it generates. The heat is carried away from the reactor and is then used to generate steam. Most reactor systems employ a cooling system that is physically separated from the water that will be boiled to produce pressurized steam for the turbines, like the pressurized water reactor. However, in some reactors the water for the steam turbines is boiled directly by the reactor core; for example the boiling water reactor.\n\nThe rate of fission reactions within a reactor core can be adjusted by controlling the quantity of neutrons that are able to induce further fission events. Nuclear reactors typically employ several methods of neutron control to adjust the reactor's power output. Some of these methods arising naturally from the physics of radioactive decay and are simply accounted for during the reactor's operation, while others are mechanisms engineered into the reactor design for a distinct purpose.\n\nThe fastest method for adjusting levels of fission-inducing neutrons in a reactor is via movement of the control rods. Control rods are made of neutron poisons and therefore tend to absorb neutrons. When a control rod is inserted deeper into the reactor, it absorbs more neutrons than the material it displaces—often the moderator. This action results in fewer neutrons available to cause fission and reduces the reactor's power output. Conversely, extracting the control rod will result in an increase in the rate of fission events and an increase in power.\n\nThe physics of radioactive decay also affects neutron populations in a reactor. One such process is delayed neutron emission by a number of neutron-rich fission isotopes. These delayed neutrons account for about 0.65% of the total neutrons produced in fission, with the remainder (termed \"prompt neutrons\") released immediately upon fission. The fission products which produce delayed neutrons have half lives for their decay by neutron emission that range from milliseconds to as long as several minutes, and so considerable time is required to determine exactly when a reactor reaches the critical point. Keeping the reactor in the zone of chain-reactivity where delayed neutrons are \"necessary\" to achieve a critical mass state allows mechanical devices or human operators to control a chain reaction in \"real time\"; otherwise the time between achievement of criticality and nuclear meltdown as a result of an exponential power surge from the normal nuclear chain reaction, would be too short to allow for intervention. This last stage, where delayed neutrons are no longer required to maintain criticality, is known as the prompt critical point. There is a scale for describing criticality in numerical form, in which bare criticality is known as \"zero dollars\" and the prompt critical point is \"one dollar\", and other points in the process interpolated in cents.\n\nIn some reactors, the coolant also acts as a neutron moderator. A moderator increases the power of the reactor by causing the fast neutrons that are released from fission to lose energy and become thermal neutrons. Thermal neutrons are more likely than fast neutrons to cause fission. If the coolant is a moderator, then temperature changes can affect the density of the coolant/moderator and therefore change power output. A higher temperature coolant would be less dense, and therefore a less effective moderator.\n\nIn other reactors the coolant acts as a poison by absorbing neutrons in the same way that the control rods do. In these reactors power output can be increased by heating the coolant, which makes it a less dense poison. Nuclear reactors generally have automatic and manual systems to scram the reactor in an emergency shut down. These systems insert large amounts of poison (often boron in the form of boric acid) into the reactor to shut the fission reaction down if unsafe conditions are detected or anticipated.\n\nMost types of reactors are sensitive to a process variously known as xenon poisoning, or the iodine pit. The common fission product Xenon-135 produced in the fission process acts as a neutron poison that absorbs neutrons and therefore tends to shut the reactor down. Xenon-135 accumulation can be controlled by keeping power levels high enough to destroy it by neutron absorption as fast as it is produced. Fission also produces iodine-135, which in turn decays (with a half-life of 6.57 hours) to new xenon-135. When the reactor is shut down, iodine-135 continues to decay to xenon-135, making restarting the reactor more difficult for a day or two, as the xenon-135 decays into cesium-135, which is not nearly as poisonous as xenon-135, with a half-life of 9.2 hours. This temporary state is the \"iodine pit.\" If the reactor has sufficient extra reactivity capacity, it can be restarted. As the extra xenon-135 is transmuted to xenon-136, which is much less a neutron poison, within a few hours the reactor experiences a \"xenon burnoff (power) transient\". Control rods must be further inserted to replace the neutron absorption of the lost xenon-135. Failure to properly follow such a procedure was a key step in the Chernobyl disaster.\n\nReactors used in nuclear marine propulsion (especially nuclear submarines) often cannot be run at continuous power around the clock in the same way that land-based power reactors are normally run, and in addition often need to have a very long core life without refueling. For this reason many designs use highly enriched uranium but incorporate burnable neutron poison in the fuel rods. This allows the reactor to be constructed with an excess of fissionable material, which is nevertheless made relatively safe early in the reactor's fuel burn-cycle by the presence of the neutron-absorbing material which is later replaced by normally produced long-lived neutron poisons (far longer-lived than xenon-135) which gradually accumulate over the fuel load's operating life.\n\nThe energy released in the fission process generates heat, some of which can be converted into usable energy. A common method of harnessing this thermal energy is to use it to boil water to produce pressurized steam which will then drive a steam turbine that turns an alternator and generates electricity.\n\nThe neutron was discovered in 1932 by British physicist James Chadwick. The concept of a nuclear chain reaction brought about by nuclear reactions mediated by neutrons was first realized shortly thereafter, by Hungarian scientist Leó Szilárd, in 1933. He filed a patent for his idea of a simple reactor the following year while working at the Admiralty in London. However, Szilárd's idea did not incorporate the idea of nuclear fission as a neutron source, since that process was not yet discovered. Szilárd's ideas for nuclear reactors using neutron-mediated nuclear chain reactions in light elements proved unworkable.\n\nInspiration for a new type of reactor using uranium came from the discovery by Lise Meitner, Fritz Strassmann and Otto Hahn in 1938 that bombardment of uranium with neutrons (provided by an alpha-on-beryllium fusion reaction, a \"neutron howitzer\") produced a barium residue, which they reasoned was created by the fissioning of the uranium nuclei. Subsequent studies in early 1939 (one of them by Szilárd and Fermi) revealed that several neutrons were also released during the fissioning, making available the opportunity for the nuclear chain reaction that Szilárd had envisioned six years previously.\n\nOn 2 August 1939 Albert Einstein signed a letter to President Franklin D. Roosevelt (written by Szilárd) suggesting that the discovery of uranium's fission could lead to the development of \"extremely powerful bombs of a new type\", giving impetus to the study of reactors and fission. Szilárd and Einstein knew each other well and had worked together years previously, but Einstein had never thought about this possibility for nuclear energy until Szilard reported it to him, at the beginning of his quest to produce the Einstein-Szilárd letter to alert the U.S. government.\n\nShortly after, Hitler's Germany invaded Poland in 1939, starting World War II in Europe. The U.S. was not yet officially at war, but in October, when the Einstein-Szilárd letter was delivered to him, Roosevelt commented that the purpose of doing the research was to make sure \"the Nazis don't blow us up.\" The U.S. nuclear project followed, although with some delay as there remained skepticism (some of it from Fermi) and also little action from the small number of officials in the government who were initially charged with moving the project forward.\n\nThe following year the U.S. Government received the Frisch–Peierls memorandum from the UK, which stated that the amount of uranium needed for a chain reaction was far lower than had previously been thought. The memorandum was a product of the MAUD Committee, which was working on the UK atomic bomb project, known as Tube Alloys, later to be subsumed within the Manhattan Project.\nEventually, the first artificial nuclear reactor, Chicago Pile-1, was constructed at the University of Chicago, by a team led by Italian physicist Enrico Fermi, in late 1942. By this time, the program had been pressured for a year by U.S. entry into the war. The Chicago Pile achieved criticality on 2 December 1942 at 3:25 PM. The reactor support structure was made of wood, which supported a pile (hence the name) of graphite blocks, embedded in which was natural uranium-oxide 'pseudospheres' or 'briquettes'.\n\nSoon after the Chicago Pile, the U.S. military developed a number of nuclear reactors for the Manhattan Project starting in 1943. The primary purpose for the largest reactors (located at the Hanford Site in Washington), was the mass production of plutonium for nuclear weapons. Fermi and Szilard applied for a patent on reactors on 19 December 1944. Its issuance was delayed for 10 years because of wartime secrecy.\n\n\"World's first nuclear power plant\" is the claim made by signs at the site of the EBR-I, which is now a museum near Arco, Idaho. Originally called \"Chicago Pile-4\", it was carried out under the direction of Walter Zinn for Argonne National Laboratory. This experimental LMFBR operated by the U.S. Atomic Energy Commission produced 0.8 kW in a test on 20 December 1951 and 100 kW (electrical) the following day, having a design output of 200 kW (electrical).\n\nBesides the military uses of nuclear reactors, there were political reasons to pursue civilian use of atomic energy. U.S. President Dwight Eisenhower made his famous Atoms for Peace speech to the UN General Assembly on 8 December 1953. This diplomacy led to the dissemination of reactor technology to U.S. institutions and worldwide.\n\nThe first nuclear power plant built for civil purposes was the AM-1 Obninsk Nuclear Power Plant, launched on 27 June 1954 in the Soviet Union. It produced around 5 MW (electrical).\n\nAfter World War II, the U.S. military sought other uses for nuclear reactor technology. Research by the Army and the Air Force never came to fruition; however, the U.S. Navy succeeded when they steamed the USS \"Nautilus\" (SSN-571) on nuclear power 17 January 1955.\n\nThe first commercial nuclear power station, Calder Hall in Sellafield, England was opened in 1956 with an initial capacity of 50 MW (later 200 MW).\n\nThe first portable nuclear reactor \"Alco PM-2A\" used to generate electrical power (2 MW) for Camp Century from 1960.\n\nNuclear Reactors are classified by several methods; a brief outline of these classification methods is provided.\n\nAll commercial power reactors are based on nuclear fission. They generally use uranium and its product plutonium as nuclear fuel, though a thorium fuel cycle is also possible. Fission reactors can be divided roughly into two classes, depending on the energy of the neutrons that sustain the fission chain reaction:\n\nUsed by thermal reactors:\n\n\n\nIn 2003, the French Commissariat à l'Énergie Atomique (CEA) was the first to refer to \"Gen II\" types in Nucleonics Week.\n\nThe first mentioning of \"Gen III\" was in 2000, in conjunction with the launch of the Generation IV International Forum (GIF) plans.\n\n\"Gen IV\" was named in 2000, by the United States Department of Energy (DOE) for developing new plant types.\n\n\n\n\n\n\nMore than a dozen advanced reactor designs are in various stages of development. Some are evolutionary from the PWR, BWR and PHWR designs above, some are more radical departures. The former include the advanced boiling water reactor (ABWR), two of which are now operating with others under construction, and the planned passively safe Economic Simplified Boiling Water Reactor (ESBWR) and AP1000 units (see Nuclear Power 2010 Program).\n\nGeneration IV reactors are a set of theoretical nuclear reactor designs currently being researched. These designs are generally not expected to be available for commercial construction before 2030. Current reactors in operation around the world are generally considered second- or third-generation systems, with the first-generation systems having been retired some time ago. Research into these reactor types was officially started by the Generation IV International Forum (GIF) based on eight technology goals. The primary goals being to improve nuclear safety, improve proliferation resistance, minimize waste and natural resource utilization, and to decrease the cost to build and run such plants.\n\nGeneration V reactors are designs which are theoretically possible, but which are not being actively considered or researched at present. Though such reactors could be built with current or near term technology, they trigger little interest for reasons of economics, practicality, or safety.\n\nControlled nuclear fusion could in principle be used in fusion power plants to produce power without the complexities of handling actinides, but significant scientific and technical obstacles remain. Several fusion reactors have been built, but only recently reactors have been able to release more energy than the amount of energy used in the process. Despite research having started in the 1950s, no commercial fusion reactor is expected before 2050. The ITER project is currently leading the effort to harness fusion power.\n\nThermal reactors generally depend on refined and enriched uranium. Some nuclear reactors can operate with a mixture of plutonium and uranium (see MOX). The process by which uranium ore is mined, processed, enriched, used, possibly reprocessed and disposed of is known as the nuclear fuel cycle.\n\nUnder 1% of the uranium found in nature is the easily fissionable U-235 isotope and as a result most reactor designs require enriched fuel.\nEnrichment involves increasing the percentage of U-235 and is usually done by means of gaseous diffusion or gas centrifuge. The enriched result is then converted into uranium dioxide powder, which is pressed and fired into pellet form. These pellets are stacked into tubes which are then sealed and called fuel rods. Many of these fuel rods are used in each nuclear reactor.\n\nMost BWR and PWR commercial reactors use uranium enriched to about 4% U-235, and some commercial reactors with a high neutron economy do not require the fuel to be enriched at all (that is, they can use natural uranium). According to the International Atomic Energy Agency there are at least 100 research reactors in the world fueled by highly enriched (weapons-grade/90% enrichment uranium). Theft risk of this fuel (potentially used in the production of a nuclear weapon) has led to campaigns advocating conversion of this type of reactor to low-enrichment uranium (which poses less threat of proliferation).\n\nFissile U-235 and non-fissile but fissionable and fertile U-238 are both used in the fission process. U-235 is fissionable by thermal (i.e. slow-moving) neutrons. A thermal neutron is one which is moving about the same speed as the atoms around it. Since all atoms vibrate proportionally to their absolute temperature, a thermal neutron has the best opportunity to fission U-235 when it is moving at this same vibrational speed. On the other hand, U-238 is more likely to capture a neutron when the neutron is moving very fast. This U-239 atom will soon decay into plutonium-239, which is another fuel. Pu-239 is a viable fuel and must be accounted for even when a highly enriched uranium fuel is used. Plutonium fissions will dominate the U-235 fissions in some reactors, especially after the initial loading of U-235 is spent. Plutonium is fissionable with both fast and thermal neutrons, which make it ideal for either nuclear reactors or nuclear bombs.\n\nMost reactor designs in existence are thermal reactors and typically use water as a neutron moderator (moderator means that it slows down the neutron to a thermal speed) and as a coolant. But in a fast breeder reactor, some other kind of coolant is used which will not moderate or slow the neutrons down much. This enables fast neutrons to dominate, which can effectively be used to constantly replenish the fuel supply. By merely placing cheap unenriched uranium into such a core, the non-fissionable U-238 will be turned into Pu-239, \"breeding\" fuel.\n\nIn thorium fuel cycle thorium-232 absorbs a neutron in either a fast or thermal reactor. The thorium-233 beta decays to protactinium-233 and then to uranium-233, which in turn is used as fuel. Hence, like uranium-238, thorium-232 is a fertile material.\n\nThe amount of energy in the reservoir of nuclear fuel is frequently expressed in terms of \"full-power days,\" which is the number of 24-hour periods (days) a reactor is scheduled for operation at full power output for the generation of heat energy. The number of full-power days in a reactor's operating cycle (between refueling outage times) is related to the amount of fissile uranium-235 (U-235) contained in the fuel assemblies at the beginning of the cycle. A higher percentage of U-235 in the core at the beginning of a cycle will permit the reactor to be run for a greater number of full-power days.\n\nAt the end of the operating cycle, the fuel in some of the assemblies is \"spent\" and is discharged and replaced with new (fresh) fuel assemblies, although in practice it is the buildup of reaction poisons in nuclear fuel that determines the lifetime of nuclear fuel in a reactor. Long before all possible fission has taken place, the buildup of long-lived neutron absorbing fission byproducts impedes the chain reaction. The fraction of the reactor's fuel core replaced during refueling is typically one-fourth for a boiling-water reactor and one-third for a pressurized-water reactor. The disposition and storage of this spent fuel is one of the most challenging aspects of the operation of a commercial nuclear power plant. This nuclear waste is highly radioactive and its toxicity presents a danger for thousands of years.\n\nNot all reactors need to be shut down for refueling; for example, pebble bed reactors, RBMK reactors, molten salt reactors, Magnox, AGR and CANDU reactors allow fuel to be shifted through the reactor while it is running. In a CANDU reactor, this also allows individual fuel elements to be situated within the reactor core that are best suited to the amount of U-235 in the fuel element.\n\nThe amount of energy extracted from nuclear fuel is called its burnup, which is expressed in terms of the heat energy produced per initial unit of fuel weight. Burn up is commonly expressed as megawatt days thermal per metric ton of initial heavy metal.\n\nNuclear safety covers the actions taken to prevent nuclear and radiation accidents and incidents or to limit their consequences. The nuclear power industry has improved the safety and performance of reactors, and has proposed new safer (but generally untested) reactor designs but there is no guarantee that the reactors will be designed, built and operated correctly. Mistakes do occur and the designers of reactors at Fukushima in Japan did not anticipate that a tsunami generated by an earthquake would disable the backup systems that were supposed to stabilize the reactor after the earthquake, despite multiple warnings by the NRG and the Japanese nuclear safety administration. According to UBS AG, the Fukushima I nuclear accidents have cast doubt on whether even an advanced economy like Japan can master nuclear safety. Catastrophic scenarios involving terrorist attacks are also conceivable. An interdisciplinary team from MIT has estimated that given the expected growth of nuclear power from 2005–2055, at least four serious nuclear accidents would be expected in that period.\n\nSome serious nuclear and radiation accidents have occurred. Nuclear power plant accidents include the SL-1 accident (1961), the Three Mile Island accident (1979), Chernobyl disaster (1986), and the Fukushima Daiichi nuclear disaster (2011). Nuclear-powered submarine mishaps include the K-19 reactor accident (1961), the K-27 reactor accident (1968), and the K-431 reactor accident (1985).\n\nNuclear reactors have been launched into Earth orbit at least 34 times. A number of incidents connected with the unmanned nuclear-reactor-powered Soviet RORSAT radar satellite program resulted in spent nuclear fuel re-entering the Earth's atmosphere from orbit.\n\nAlthough nuclear fission reactors are often thought of as being solely a product of modern technology, the first nuclear fission reactors were in fact naturally occurring. A natural nuclear fission reactor can occur under certain circumstances that mimic the conditions in a constructed reactor. Fifteen natural fission reactors have so far been found in three separate ore deposits at the Oklo uranium mine in Gabon, West Africa. First discovered in 1972 by French physicist Francis Perrin, they are collectively known as the Oklo Fossil Reactors. Self-sustaining nuclear fission reactions took place in these reactors approximately 1.5 billion years ago, and ran for a few hundred thousand years, averaging 100 kW of power output during that time. The concept of a natural nuclear reactor was theorized as early as 1956 by Paul Kuroda at the University of Arkansas.\n\nSuch reactors can no longer form on Earth in its present geologic period. Radioactive decay of formerly abundant uranium-235 over the time span of hundreds of millions of years has reduced the proportion of this naturally occurring fissile isotope to below the amount required to sustain a chain reaction.\n\nThe natural nuclear reactors formed when a uranium-rich mineral deposit became inundated with groundwater that acted as a neutron moderator, and a strong chain reaction took place. The water moderator would boil away as the reaction increased, slowing it back down again and preventing a meltdown. The fission reaction was sustained for hundreds of thousands of years.\n\nThese natural reactors are extensively studied by scientists interested in geologic radioactive waste disposal. They offer a case study of how radioactive isotopes migrate through the Earth's crust. This is a significant area of controversy as opponents of geologic waste disposal fear that isotopes from stored waste could end up in water supplies or be carried into the environment.\n\nNuclear reactors produce tritium as part of normal operations, which is eventually released into the environment in trace quantities.\n\nAs an isotope of hydrogen, tritium (T) frequently binds to oxygen and forms TO. This molecule is chemically identical to HO and so is both colorless and odorless, however the additional neutrons in the hydrogen nuclei cause the tritium to undergo beta decay with a half-life of 12.3 years. Despite being measurable, the tritium released by nuclear power plants is minimal. The United States NRC estimates that a person drinking water for one year out of a well contaminated by what they would consider to be a significant tritiated water spill would receive a radiation dose of 0.3 millirem. For comparison, this is an order of magnitude less than the 4 millirem a person receives on a round trip flight from Washington, D.C. to Los Angeles, a consequence of less atmospheric protection against highly energetic cosmic rays at high altitudes.\n\nThe amounts of strontium-90 released from nuclear power plants under normal operations is so low as to be undetectable above natural background radiation. Detectable strontium-90 in ground water and the general environment can be traced to weapons testing and the Chernobyl accident that occurred during the mid-20th century.\n\n\n"}
{"id": "19934350", "url": "https://en.wikipedia.org/wiki?curid=19934350", "title": "Otto Brune", "text": "Otto Brune\n\nOtto Walter Heinrich Oscar Brune (10 January 1901 – 1982) undertook some key investigations into network synthesis at the Massachusetts Institute of Technology (MIT) where he graduated in 1929. His doctoral thesis was supervised by Wilhelm Cauer (and Ernst Guillemin) who suggested that he provide a proof of the necessary and sufficient conditions for the realisability of multi-port impedances. Cauer himself had found a necessary condition but had failed to prove it to be sufficient. The goal for researchers then was \"to remove the restrictions implicit in the Foster-Cauer realisations and find conditions on Z equivalent to realisability by a network composed of arbitrary interconnections of positive-valued R, C and L.\" There was a race to solve the problem and Brune won it.\n\nBrune coined the term \"positive-real\" (PR) for that class of analytic functions that are realisable as an electrical network using passive components. Brune also showed that if the case is limited to scalar PR functions then it is not necessary to allow ideal transformers (a limit to the usefulness of the theory) to be assured of a realisable network solution. The eponymous \"Brune cycle\" continued fractions were invented by Brune to facilitate this proof.\n\nThe so-called Brune theorem is:a. The impedance \"Z\"(\"s\") of any electric network composed of passive components is positive real.b. If \"Z\"(\"s\") is positive real it is realisable by a network having as components passive (positive) \"R, C, L as well as ideal transformers T\".Brune is also responsible for the Brune test for determining the permissibility of interconnecting two-port networks. For his work, Brune is recognized as one of those who laid the foundation of network analysis by means of mathematics. For instance, American computer scientist Ernst Guillemin dedicated his book \"Synthesis of Passive Network\" to Brune, describing him with these words: \"In my opinion the one primarily responsible for establishing a very broad and mathematically rigorous basis for realization theory generally was Otto Brune.\"\n\nBrune was born in Kimberley, South Africa in 1901 and returned there in 1935. He became Principal Research Officer at the National Research Laboratories, Pretoria.\n"}
{"id": "6504692", "url": "https://en.wikipedia.org/wiki?curid=6504692", "title": "Outline of technology", "text": "Outline of technology\n\nThe following outline is provided as an overview of and topical guide to technology:\n\nTechnology – collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technologies. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.\n\n\n\n\nHistory of technology\n\n\n\n\n\n\nPotential technology of the future includes:\n\nHypothetical technology – \n\nPhilosophy of technology – \n\n\n\n\n\n\n\n\n\n\n\nFictional technology – \n\n\n\n\n\n\n\n"}
{"id": "8557925", "url": "https://en.wikipedia.org/wiki?curid=8557925", "title": "Paracone", "text": "Paracone\n\nA paracone is a 1960s atmospheric reentry or spaceflight mission abort concept using an inflatable ballistic cone.\n\nA notable feature of the paracone concept is that it facilitates an abort throughout the entire flight profile.\n\n"}
{"id": "55224515", "url": "https://en.wikipedia.org/wiki?curid=55224515", "title": "Phase-out of lightweight plastic bags in Australia", "text": "Phase-out of lightweight plastic bags in Australia\n\nThe phase-out of lightweight plastic bags in Australia is being pursued at local and state/territory level rather than nationally, with plastic bag bans implemented or pending in all states and territories except New South Wales. Environmental groups have expressed their concern that Australia was lagging other countries in the phase-out of lightweight plastic bags, including Botswana, Somalia and Tanzania. In 2016 it was estimated that of the 5 billion plastic bags used annually by Australians, 150 million ended up as litter.\n\nIn 2003 the Tasmanian town of Coles Bay was the first location in Australia to ban plastic bags. Although in 2008 the then-Environment Minister Peter Garrett announced the Rudd Government's goal of a nationwide plastic bag ban by year's end, he later abandoned the initiative due to cost of living concerns and disagreement about the policy among state and territory governments. This led to states and territories pursuing their own approaches.\n\nThe introduction of the \"Zero Waste\" program in South Australia led to the first statewide lightweight bag ban being introduced in October 2008. It is estimated that this move has saved 400 million bags annually.\nThe most recent jurisdiction to announce a ban on plastic bags is Victoria, to commence on a date to be announced in early 2018.\n\nSince 2018, the country's two largest supermarket chains, Coles and Woolworths, voluntarily removed free lightweight plastic bags from their stores and began charging for reusable bags instead. The reusable bags are sold at 15 cents in both chains.\n\nIn Australia, 6 billion HDPE bags were used in 2002. Usage reduced to 5.6 billion in 2004 and 3.9 billion in 2007. According to Keep Australia Beautiful's 2015-16 National Litter Index, plastic bags made up 1% of the country's litter.\n\nIn April 2017, \"The Project\" and Clean Up Australia launched a Change.org petition for the premiers of the three states without plastic bag bans - Victoria, New South Wales and Western Australia - to \"ban the bag\". After a segment on \"The Project\" about the petition, #BanTheBag became the top trending topic on Twitter. In February 2018, a motion was put in the Australian Federal Parliament calling for a nationwide ban of lightweight non-biodegradable plastic shopping bags. Senate inquiries into recycling and marine plastic pollution also led to tripartisan recommendations by the Liberal, Labor and Greens parties to ban all single-use plastics in the country by 2023 and for all states and territories to ban plastic bags.\n\nThe \"Plastic Shopping Bags (Waste Avoidance) Bill\" passed the Parliament of South Australia on 13 November 2008, with a transitional period of 1 January to 4 May 2009. With an official start date of 4 May 2009, South Australia was the first state or territory in Australia to ban plastic bags at the checkout, with retailers facing fines of up to $5,000 for distributing banned bags and retailer suppliers fined up to $20,000. The ban does not extend to heavier plastic bags or fruit and vegetable bags.\n\nThe Northern Territory banned plastic bags with effect from 1 September 2011. Biodegradable and heavier bags remain legal.\n\nThe ACT was the second jurisdiction after South Australia to pass a law banning plastic bags, with the ban entering into effect on 1 November 2011. Plastic barrier bags for fruit and vegetables are exempt.\n\nIn 2003 Coles Bay was the first Australian town to completely ban non-biodegradable plastic bags. In November 2010 a Greens motion introduced to Tasmania's parliament to ban plastic bags received tripartisan support from Labor, the Liberals and the Greens. A statewide ban began on 1 November 2013. A 2015 review found widespread support for the ban but a mixed environmental impact.\n\nIn November 2016 the Queensland Government announced it would ban plastic bags from 2018, after the Opposition promised that it would implement a ban if it won the next state election. The implementing legislation passed the Queensland Parliament on 6 September 2017, accompanied by a container refund recycling scheme. Both initiatives are due to take effect on 1 July 2018.\n\nTwo attempts by the City of Fremantle in 2013 and 2015 to introduce a citywide plastic bag ban were blocked through disallowance motions moved in the Western Australian Legislative Council. In September 2017, the Western Australian Government announced it would commence a statewide plastic bag ban on 1 July 2018.\n\nIn mid-2017, Victoria's state government expressed its preference for a national ban, while confirming it would be open to a state-level ban in future. In October 2017, Daniel Andrews stated that Victoria would introduce a ban on plastic bags, with the start date to be announced in early 2018 after consultation with industry and the community. The Government subsequently announced the plastic bag ban would commence by late 2019.\n\nThe New South Wales Government currently has no plans to introduce a ban. In 2017 Premier Gladys Berejiklian stated that no regulation was needed because retailers responsible for 80% of plastic bags in the state (Coles, Woolworths and Harris Farm) would voluntarily stop providing free plastic bags themselves.\n\nAldi has never offered free plastic bags to customers since opening its first Australian store in 2001. Customers are charged 15 cents for a thick plastic bag and more for fabric or cooler bags. In 2009 Target Australia announced it would stop giving out free bags in its stores, instead charging 10 cents per bag. In 2013 Target again providing free bags, stating that it had received about 500 formal customer complaints per year about the lack of free bags. Target and its sister chain Kmart Australia plan to withdraw single use plastic bags nationwide in 2019. Bunnings Warehouse has charged 10 cents for plastic bags since 2003, while IKEA stopped offering free bags in 2013.\n\nIn 2017 the country's largest supermarket chains, Woolworths and Coles, announced that they would stop providing free plastic bags to customers nationwide in 2018, and would sell thicker plastic bags for 15 cents each. Withdrawing free bags is expected to save the retailers $170 million per year in reduced packaging costs while creating a revenue stream from selling thicker plastic bags. \n\nThe move was criticised by conservative commentators including Andrew Bolt and Steve Price. The loss of free bags also sparked backlash from some consumers, with several stealing shopping baskets or trolleys in protest and reports of abuse or assault towards staff. Criticisms include the continued overuse of plastic by retailers, poor quality of the replacement bags and allegations that the true motive was profiteering. Marketing academics described the backlash as caused by the retailers breaking the perceived \"psychological contract\" with consumers to either provide free plastic bags or otherwise remove plastic throughout the store for environmental benefit, rather than merely charging for something that had previously been provided without additional charge.\n\nIn response to customer backlash Coles reversed its position, offering to provide reusable plastic bags for free indefinitely in most states. This led to criticism from Greenpeace, Craig Reucassel and other environmentalists, causing Coles to reverse its reversal and announce it would again charge for bags from 29 August 2018 onwards. Woolworths also extended the timeframe for providing free plastic bags, until 8 July 2018. Harris Farms ceased providing free plastic bags in January 2018, instead offering free reusable paper bags or recycled boxes to consumers. Craveable Brands also moved to phase out single use plastic bags in its Oporto, Red Rooster and Chicken Treat chains by July 2018.\n\nA side-effect of the plastic bag ban observed in South Australia was the increased use of bin liners, which have a greater environmental impact than plastic bags as they do not break down well in modern landfills. Environmentally-friendly alternatives suggested instead of bin liners include composting food scraps and using free community newspapers as liners instead.\n\nA 2007 report into shopping bag alternatives noted that paper bags were less environmentally friendly than plastic bags due to a higher carbon footprint. Similarly, cotton bags were unsuitable due to the pesticides used and high volume of water needed to create them. The \"greenest\" option was using recycled plastic bags.\n\nConcern has been expressed about potential unintended adverse health outcomes associated with the plastic bag ban rollout due to the lack of care by consumers in maintaining alternative shopping bags in a clean and healthy condition. Overseas experiences in locations such as San Francisco, where increased illness and even deaths were reported in the aftermath of similar bans to those in Australian states, suggest that this is a real concern.\n\n"}
{"id": "23437839", "url": "https://en.wikipedia.org/wiki?curid=23437839", "title": "Positive-real function", "text": "Positive-real function\n\nPositive-real functions, often abbreviated to PR function, are a kind of mathematical function that first arose in electrical network analysis. They are complex functions, \"Z\"(\"s\"), of a complex variable, \"s\". A rational function is defined to have the PR property if it has a positive real part and is analytic in the right halfplane of the complex plane and takes on real values on the real axis.\n\nIn symbols the definition is,\n\nIn electrical network analysis, \"Z\"(\"s\") represents an impedance expression and \"s\" is the complex frequency variable, often expressed as its real and imaginary parts;\n\nin which terms the PR condition can be stated;\n\nThe importance to network analysis of the PR condition lies in the realisability condition. \"Z\"(\"s\") is realisable as a one-port rational impedance if and only if it meets the PR condition. Realisable in this sense means that the impedance can be constructed from a finite (hence rational) number of discrete ideal passive linear elements (resistors, inductors and capacitors in electrical terminology).\n\nThe term \"positive-real function\" was originally defined by Otto Brune to describe any function \"Z\"(\"s\") which\nMany authors strictly adhere to this definition by explicitly requiring rationality, or by restricting attention to rational functions, at least in the first instance. However, a similar more general condition, not restricted to rational functions had earlier been considered by Cauer, and some authors ascribe the term \"positive-real\" to this type of condition, while other consider it to be a generalization of the basic definition.\n\nThe condition was first proposed by Wilhelm Cauer (1926) who determined that it was a necessary condition. Otto Brune (1931) coined the term positive-real for the condition and proved that it was both necessary and sufficient for realisability.\n\n\nA couple of generalizations are sometimes made, with intention of characterizing the immittance functions of a wider class of passive linear electrical networks.\n\nThe impedance \"Z\"(\"s\") of a network consisting of an infinite number of components (such as a semi-infinite ladder), need not be a rational function of \"s\", and in particular may have branch points on the negative real \"s\"-axis. To accommodate such functions in the definition of PR, it is therefore necessary to relax the condition that the function be real for all real \"s\", and only require this when \"s\" is positive. Thus, a possibly irrational function \"Z\"(\"s\") is PR if and only if\nSome authors start from this more general definition, and then particularize it to the rational case.\n\nLinear electrical networks with more than one port may be described by impedance or admittance matrices. So by extending the definition of PR to matrix-valued functions, linear multi-port networks which are passive may be distinguished from those that are not. A possibly irrational matrix-valued function \"Z\"(\"s\") is PR if and only if\n\n"}
{"id": "1508243", "url": "https://en.wikipedia.org/wiki?curid=1508243", "title": "Push-button", "text": "Push-button\n\nA push-button (also spelled pushbutton) or simply button is a simple switch mechanism for controlling some aspect of a machine or a process. Buttons are typically made out of hard material, usually plastic or metal. The surface is usually flat or shaped to accommodate the human finger or hand, so as to be easily depressed or pushed. Buttons are most often biased switches, although many un-biased buttons (due to their physical nature) still require a spring to return to their un-pushed state.\nTerms for the \"pushing\" of a button include pressing, depressing, mashing, slapping, hitting, and punching.\n\nThe \"push-button\" has been utilized in calculators, push-button telephones, kitchen appliances, and various other mechanical and electronic devices, home and commercial.\n\nIn industrial and commercial applications, push buttons can be connected together by a mechanical linkage so that the act of pushing one button causes the other button to be released. In this way, a stop button can \"force\" a start button to be released. This method of linkage is used in simple manual operations in which the machine or process has no electrical circuits for control.\n\nRed pushbuttons can also have large heads (called mushroom heads) for easy operation and to facilitate the stopping of a machine. These pushbuttons are called emergency stop buttons and for increased safety are mandated by the electrical code in many jurisdictions. This large mushroom shape can also be found in buttons for use with operators who need to wear gloves for their work and could not actuate a regular flush-mounted push button.\n\nAs an aid for operators and users in industrial or commercial applications, a pilot light is commonly added to draw the attention of the user and to provide feedback if the button is pushed. Typically this light is included into the center of the pushbutton and a lens replaces the pushbutton hard center disk. The source of the energy to illuminate the light is not directly tied to the contacts on the back of the pushbutton but to the action the pushbutton controls. In this way a start button when pushed will cause the process or machine operation to be started and a secondary contact designed into the operation or process will close to turn on the pilot light and signify the action of pushing the button caused the resultant process or action to start.\n\nTo avoid an operator from pushing the wrong button in error, pushbuttons are often color-coded to associate them with their function. Commonly used colors are red for stopping the machine or process and green for starting the machine or process.\n\nIn popular culture, the phrase \"the button\" (sometimes capitalized) refers to a (usually fictional) button that a military or government leader could press to launch nuclear weapons.\n\nAkin to fire alarm switches, some big red buttons, when deployed with suitable visual and audible warnings such as flashing lights and sirens for extreme exigent emergencies, are known as \"scram switches\" (from the slang term scram, \"get out of here\"). Generally, such buttons are connected to large scale functions, beyond a regular fire alarm, such as automated shutdown procedures, complete facility power cut, fire suppression like halogen release, etc.\n\nA variant of this is the scramble switch which triggers an alarm to activate emergent personnel to proactively attend to and go to such disasters. An air raid siren at an air base initiates such action, where the fighter pilots are alerted and \"scrambled\" to their planes to defend the base.\n\n"}
{"id": "28579329", "url": "https://en.wikipedia.org/wiki?curid=28579329", "title": "Qi hardware", "text": "Qi hardware\n\nQi hardware is a project which produces copyleft hardware, in an attempt to apply the Free Software Foundation's GNU GPL concept of copylefting software to the hardware layer. The project is both a community of popular open hardware websites and a company, co-founded by Wolfgang Spraul and Yi Zhang, that makes hardware products. Formed from the now defunct Openmoko project, key members went on to form Qi Hardware Inc. and Sharism At Work Ltd. Thus far, the project has released the Ben Nanonote, the Milkymist One, and the Ben WPAN wireless project to create a copyleft wireless platform.\n\nCopyleft hardware is essentially requiring that all plans for hardware design (i.e. schematics, bill of materials and PCB layout data) are released under the Creative Commons license Attribution-ShareAlike (CC BY-SA) and that the software needed to both manufacture the device and at least some software, including device drivers, necessary to use the hardware is released under the GNU General Public License. Technology for copyleft hardware are to be patent-free, and hence, all hardware which is Qi hardware is to be released early, often and publicly on the Internet.\n\nThe primary examples of Qi hardware projects are the Ben NanoNote pocket computer, Elphel 353 video camera and Milkymist One video synthesizer.\n\n\n"}
{"id": "31979181", "url": "https://en.wikipedia.org/wiki?curid=31979181", "title": "Renewable Fuels Regulators Club", "text": "Renewable Fuels Regulators Club\n\nThe Renewable Fuels Regulators Club (or REFUREC) is a network of governmental institutions that offers a pan-European platform for discussion, information exchange and tackling cross-border issues relating to the biofuels market in the European Union and beyond.\n\nREFUREC was initiated in 2009 by the Renewable Fuels Agency, a UK Government non-departmental public body, created by the Department for Transport to implement the Renewable Transport Fuel Obligation or RTFO.\n\nThe Renewable Fuels Regulators Club was established to help address consistent implementation and regulation of the biofuels market. By facilitating and fostering stronger working relations between counterparts working in the field throughout Europe, REFUREC aims to minimise the regulatory burden of the new rules, and to maximise the overall effectiveness of the Renewables Directive.\n\nREFUREC meets on a quarterly basis to share knowledge, ideas and strategies on how best to implement workable interpretations of the Renewables Directive across respective borders. The member states of the EU and the European Free Trade Association (EFTA) have evolved different methods of regulating biofuel consumption. The different starting points, combined with the intrinsic subtleties and complexities of the legislation are what lead REFUREC to believe that this kind of close co-operation is key to successful implementation of the Renewables Directive.\n\nThe administrative part is provided by an informal, rotating secretariat. April 2011 - June 2012, the Spanish Comisión Nacional de Energía (CNE) led on the secretariat work and successfully handed over in July 2012 to the Dutch Emissions Authority (Nederlandse Emissieautoriteit, Netherlands). October 2013 - September 2015, the Swedish Energy Agency coordinated the secretariat work. In October 2015, the Finnish Energy Authority took the lead and provided secretariat support for two years. Since September 2017, REFUREC is maintained by Orkustofnun, Iceland’s National Energy Authority.\n\nThe inaugural meeting was held on 4 February 2010 in London, and was attended by representatives from the UK, Denmark, France, Germany, Hungary, the Netherlands, Portugal, the Republic of Ireland, the Slovak Republic, Spain, Sweden, and the European Commission. To date, further workshops have been held in Brussels, Bonn, Madrid, The Hague, Copenhagen, Lisbon, Stockholm, Valletta, Dublin, Oslo, Luxembourg, Paris, Bratislava, Helsinki, Vienna, Tallinn and Reykjavik.\n\nSince its kick-off meeting in 2009, membership has grown rapidly and currently organisations from 31 countries are participating. REFUREC has matured into a successful network of biofuel sustainability regulators, and into a positive model of European co-operation.\n\n"}
{"id": "33011862", "url": "https://en.wikipedia.org/wiki?curid=33011862", "title": "Smoke canopy", "text": "Smoke canopy\n\nA smoke canopy is a device hung over a fire to gather the smoke and vent it through a wall or roof. \n\nFireplaces were not used during much of the Middle Ages, because there were no chimneys, which were not invented and popularized until the 12th century. Most fires for heating were placed on hearths in the middles of rooms, and the smoke was allowed to rise to vents in the roof or high in walls. Smoke canopies provided an alternative, gathering the smoke above a fire and venting, usually through a wall clearing the living area from harmful chemicals.\n\nMost of the pictures we have of medieval smoke canopies show them being used in kitchens. They usually appeared over hearths that were placed against stone walls. Some were over free standing hearths, but this required special venting of the smoke.\n\nFire canopies are used to this day, especially in summer houses and mountain lodges as an alternative to expensive fireplace.\n\n\n"}
{"id": "15422616", "url": "https://en.wikipedia.org/wiki?curid=15422616", "title": "Society for Industrial Archeology", "text": "Society for Industrial Archeology\n\nThe Society for Industrial Archeology (SIA) is a North American nonprofit organization dedicated to studying and preserving historic industrial sites, structures and equipment. It was founded in 1971 in Washington, D.C., and its members are primarily from the United States and Canada, although there is some crossover with similar industrial archaeology organizations in the United Kingdom. SIA's headquarters is currently located in the Department of Social Sciences at Michigan Technological University in Houghton, Michigan. In addition to the national organization, there are thirteen regional chapters throughout the United States.\n\nSince its founding in October 1971, SIA members have gathered at an annual conference, which also serves as the annual business meeting required by its bylaws. The annual conference is typically held each spring. The Fall Tour, a second annual gathering usually held in September or October, began in 1972. Both annual events feature visits to local industrial sites, both active and historical. The conference additionally includes a day of paper sessions. Industrial heritage study tours to other countries began in 1990 and occur on an irregular schedule.\n\nSIA produces two official publications, the formal, peer-reviewed \"IA, The Journal of the Society for Industrial Archeology\" twice a year and the less formal quarterly \"Society for Industrial Archeology Newsletter (SIAN).\" The annual conference is typically accompanied by a custom guidebook profiling local industrial and cultural sites. In addition, SIA produces occasional publications on special topics.\n\nThe \"Society for Industrial Archeology Newsletter (SIAN)\" (ISSN ) is the Society's official newsletter, published quarterly. \"SIAN\" publishes articles about recent events, summaries of the Society's annual conferences and fall tours, news about local chapters, brief abstracts of articles on industrial archeology-related subjects, and other notes and queries.\n\nThe first issue of \"SIAN\" was published in January 1972 and reported on the Society's founding. Volumes 1 through 9 consisted of six issues per calendar year, not including two special supplemental issues published in 1972. The current quarterly publication schedule with Winter, Spring, Summer, and Fall issues began with Volume 10 in 1981.\n\n"}
{"id": "43312966", "url": "https://en.wikipedia.org/wiki?curid=43312966", "title": "Soundhawk", "text": "Soundhawk\n\nSoundhawk is an American corporation headquartered in Cupertino, California, that introduced a Smart Listening System in June 2014. The Smart Listening System is a set of products designed to help people hear, communicate and connect with greater clarity in noisy situations. It comes with three pieces of hardware: the Scoop (for the ear), wireless microphone and charging case. The pieces are unified by a mobile application that includes over one million auditory profiles users can personalize to their environment.\n\nSoundhawk was founded by Rodney Perkins, Stanford University School of Medicine, Otologist. He is also the founder of ReSound and the California Ear Institute at Stanford University Executive leadership at Soundhawk includes Dr. Rodney Perkins as CEO; Drew Dundas, formerly director of Audiology and Assistant Professor of Otolaryngology at UCSF as President and Chief Technology Officer; Steve Manser former engineering leader at Apple, Inc., Palm, Inc., and Hewlett-Packard, as Chief Operations Officer and Vice President of Engineering. The Soundhawk product was built by a group of former hardware and software engineers from Palm, Inc., Apple, Inc., Hewlett-Packard, and Amazon.com.\n\nSoundhawk is backed by True Ventures, Foxconn Technology Group and other Silicon Valley entrepreneurs.\n\nSoundhawk has suspended operation as per their official website, which as of October 1, 2016 reads \"down for maintenance, check back soon\". As of March 2018, support pages are no longer available as the domain name connections have expired; nor can it be found through the Apple Store. Amazon.com also lists the device, but says it's currently not available, and they don't know when it will be.\n\nThe Smart Listening System blends consumer technology and hearing science to help people hear, communicate and connect. Soundhawk includes three pieces of hardware: the Scoop, the Wireless Mic and the Charging Case. Unifying the System is a mobile app with patented technology that personalizes Soundhawk to the user and their environments.\n\nThe Soundhawk app is used to set up and personalize the system and is available for download to iOS and Android. The app's patented technology enables many unique hearing profiles to be personalized to the acoustics of different environments.\n\nThe Scoop is a wearable device for the ear that connects with all components of the Smart Listening System™. It uses adaptive audio processing to enhance specific sound frequencies and elevate what the user intends to hear while reducing background noise. Included are four different colored ear tips for fit personalization. The Scoop connects wirelessly to users' phones or tablets via Bluetooth.\n\nThe Wireless Mic is a wireless microphone used to hear a specific sound source in a variety of environments. It picks up the intended sound and delivers it to the Scoop. The Wireless Mic comes with a clip for attaching it to clothing or placing it on a table.\n\nThe Charging Case is a base for the Scoop and Wireless Mic that protects and charges the system. Depending on use, the Charging Case can recharge the Scoop and Wireless Mic up to two additional times.\n\nSoundhawk can be used to help people participate, connect, and actively listen in any environment. Soundhawk can be used in everyday situations like talking on the phone, dining in noisy restaurants, watching TV or attending business meetings and social gatherings. There are three primary ways that Soundhawk can be used:\n"}
{"id": "16827990", "url": "https://en.wikipedia.org/wiki?curid=16827990", "title": "Space logistics", "text": "Space logistics\n\nAccording to the AIAA Space Logistics Technical Committee, space logistics is\n\nHowever, this definition in its larger sense includes terrestrial logistics in support of space travel, including any additional \"design and development, acquisition, storage, movement, distribution, maintenance, evacuation, and disposition of space materiel\", movement of people in space (both routine and for medical and other emergencies), and contracting and supplying any required support services for maintaining space travel.\n\nWernher von Braun spoke of the necessity (and the underdevelopment) of space logistics as early as 1960:\n\nJames D. Baker and Frank Eichstadt of SPACEHAB wrote, in 2005:\n\nAccording to Manufacturing Business Technology,\n\nAmong the supply classes identified by the MIT Space Logistics Center:\n\nIn the category of space transportation for ISS Support, one might list:\n\nA snapshot of the logistics of a single space facility, the International Space Station, was provided in 2005 via a comprehensive study done by James Baker and Frank Eichstadt. This article section makes extensive reference to that study.\n\n, the United States Space Shuttle, the Russian Progress, and to a very limited extent, the Russian Soyuz vehicles were the only space transport systems capable of transporting ISS cargo.\n\nHowever, in 2004, it was already anticipated that the European Automated Transfer Vehicle (ATV) and Japanese H-IIA Transfer Vehicle (HTV) would be introduced into service before the end of ISS Assembly. As of 2004, the US Shuttle transported the majority of the pressurized and unpressurized cargo and provides virtually all of the recoverable down mass capability (the capability of non-destructive reentry of cargo).\n\nBaker and Eichstadt also wrote, in 2005:\n\nBaker and Eichstadt also wrote, in 2005:\n\nBaker and Eichstadt also wrote, in 2005:\n\nBaker and Eichstadt also wrote, in 2005:\n\nBaker and Eichstadt also wrote, in 2005:\n\nBaker and Eichstadt also wrote, in 2005:\n\nWhile significant focus of space logistics is on \"upmass\", or payload mass carried up to orbit from Earth, space station operations also have significant downmass requirements. \nReturning cargo from low-Earth orbit to Earth is known as transporting \"downmass\", the total logistics payload mass that is returned from space to the surface of the Earth for subsequent use or analysis.\nDownmass logistics are important aspects of research and manufacturing work that occurs in orbital space facilities.\n\nFor the International Space Station, there have been periods of time when downmass capability was severely restricted. For example, for approximately ten months from the time of the retirement of the Space Shuttle following the STS-135 mission in July 2011—and the resultant loss of the Space Shuttle's ability to return payload mass—an increasing concern became returning downmass cargo from low-Earth orbit to Earth for subsequent use or analysis.\nDuring this period of time, of the four space vehicles capable of reaching and delivering cargo to the International Space Station, only the Russian Soyuz vehicle could return even a very small cargo payload to Earth. The Soyuz cargo downmass capability was limited as the entire space capsule was filled to capacity with the three ISS crew members who return on each Soyuz return. None of the remaining cargo resupply vehicles — the Russian Space Agency Progress, the European Space Agency (ESA) ATV, the Japan Aerospace Exploration Agency (JAXA) HTV — can return any downmass cargo for terrestrial use or examination.\n\nAfter 2012, with the successful berthing of the commercially contracted SpaceX Dragon during the Dragon C2+ mission in May 2012, and the initiation of operational cargo flights in October 2012, downmass capability from the ISS is now per Dragon flight, a service that is uniquely provided by the Dragon cargo capsule.\n\nNine additional Dragon cargo resupply flights are scheduled to depart the ISS with downmass in the next several years.\n\n\n"}
{"id": "3986309", "url": "https://en.wikipedia.org/wiki?curid=3986309", "title": "Strand jack", "text": "Strand jack\n\nA strand jack (also known as strandjack) is a jack used to lift very heavy (e.g. thousands tons or more with multiple jacks) loads for construction and engineering purposes. Strandjacking was invented by VSL Australia's Patrick Kilkeary & Bruce Ramsay in 1969 for concrete post tensioning systems, and are now used all over the world for heavy lifting, to erect bridges, offshore structures, refineries, power stations, major buildings and other structures where the use of conventional cranes is either impractical or too expensive.\n\nStrand jacks can be used horizontally for pulling objects and structures, and are widely used in the oil and gas industry for skidded loadouts. Oil rigs of 38,000 t have been moved in this way from the place of construction on to a barge.\nSince multiple jacks can be operated simultaneously by hydraulic controllers, they can be used in tandem to lift very large loads of thousands of tons. Even the tandem use of two cranes is very difficult operation. \n\nA strand jack is a hollow hydraulic cylinder with a set of steel cables (the \"strands\") passing through the open centre, each one passing through two clamps - one mounted to either end of the cylinder.\n\nThe jack operates in the manner of a caterpillar's walk: climbing (or descending) along the strands by releasing the clamp at one end, expanding the cylinder, clamping there, releasing the trailing end, contracting, and clamping the trailing end before starting over again. The real significance of this device lies in the facility for precision control.\n\nThe expansion and contraction can be done at any speed, and paused at any location. Although a jack may lift only 1700 tons or so, there exist computer control systems that can operate 120 jacks simultaneously, offering fingertip feel movement control over extremely massive objects.\n\nStrand jacking is a construction process whereby large pre-fabricated building sections are carefully lifted and precisely placed. The alternative would be to do all assembly \"in situ\", even if expensive, technically risky, or dangerous.\n\nStrand jacks for heavy lifting and skidding operations are owned and operated by a large number of construction and heavy lifting companies around the world. They are currently manufactured by a small number of companies based in Europe.\n\nUses outside of construction \n"}
{"id": "58438", "url": "https://en.wikipedia.org/wiki?curid=58438", "title": "Timeline of diving technology", "text": "Timeline of diving technology\n\nThe timeline of underwater diving technology is a chronological list of notable events in the history of underwater diving.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are other diving history chronologies at:\n"}
{"id": "43139852", "url": "https://en.wikipedia.org/wiki?curid=43139852", "title": "Total conversion (energy source)", "text": "Total conversion (energy source)\n\nIn some science fiction stories, total conversion may mean higher or complete conversion of matter into energy, or vice versa in some proportion of \"E = mc\".\n\nPositron and electron production: For photons at high-energy (MeV scale and higher), photon-photon collisions can efficiently convert the photon energy into matter in the form of a positron and an electron:\n\nProton and antiproton production: Conventional matter consists of protons and electrons, with electrons having insignificant mass compared to protons. One conventional model for producing protons from energy is extremely high-energy cosmic ray protons collide with nuclei in the interstellar medium, via the reaction: + A → + + + A. (A represents an atom, p a proton, and an antiproton.) A portion of the kinetic energy of the initial proton is used to create two additional nuclei: another proton plus an antiproton.\n\nConventional nuclear reactions such as nuclear fission and nuclear fusion convert relatively small amounts of matter only indirectly into useful energy, such as electricity or rocket thrust. For electricity production released nuclear energy in the form of heat is typically used to boil water to turn a turbine-generator.\n\nPossibly matter is almost completely converted into energy in the cores of neutron stars and \"black holes\" by a process of nuclei collapse resulting in: proton → positron + 938 MeV, resulting in a >450 MeV positron-electron jet. Trace nuclei swept up in such a beam would achieve an approximate energy of (nucleus mass/electron mass) × 450 MeV, for example an iron atom could achieve about 45 TeV. An up to 45 TeV atom impacting a proton in the interstellar medium should result in the p + A process described above.\n\nIon-electron or positron-electron plasma with magnetic confinement theoretically allows direct conversion of particle energy to electricity by the separation of the positive particles from the negative particles with magnetic deflection. Direct conversion of particle energy to thrust is theoretically simpler, merely requiring magnetically directing a neutral plasma beam. Present lab production of relativistic 5 MeV positron-electron beams mimic on a small scale the relativistic jets from compact stars, and allow small scale studies how different elements interact with 5 MeV positron-electron beams, how energy is transferred to particles, the shock effect of GRBs, and possible direct thrust and electricity generation from neutral plasmas. Lab positron-electron plasmas could be useful for studying compact star jets and other phenomena. However thrust generation or magnetically separating neutral beams for electrical generation will probably only be useful if there is a practical continuous process for generating neutral plasma by nuclear reactions.\n"}
{"id": "58735847", "url": "https://en.wikipedia.org/wiki?curid=58735847", "title": "U-Report", "text": "U-Report\n\nU-Report is a free SMS social monitoring tool and real-time information system for community participation, designed to strengthen community-led development, citizen engagement, and positive change. SMS polls and alerts are sent out to U-reporters and real-time response information is collected. Results and ideas are shared back with the community. Issues polled include among others health, education, water, sanitation and hygiene, youth unemployment, HIV/ AIDS, disease outbreaks; social welfare sectors. The initiative is currently operational in 41 countries and covers more than 3 million people.\n\nUganda became the first country in which UNICEF launched the U-Report mobile initiative in May 2011. The country's population is one of the youngest in the world with over 55% of the population younger than 18 years and 78% - up to 30 years. Also there is a high level of using phones - 48%.\n\nThe success of U-Report in Uganda has prompted the launch of similar initiatives in Zambia in December 2012 and in Nigeria in June 2014.\n\nIn Zambia, U-report was used to accelerate HIV prevention among adolescents and young people. As a result of the U-Report, a significant increase in voluntary HIV testing was recorded - 40% compared to an average of 24% before.\n\nIn Nigeria, U-Report concentrates surveys on social and medical issues. The number of reporters in Nigeria has increased dramatically in subsequent years and Nigeria has become the first country out of 1 million reporters.\n\nIn July 2015, 15 countries initiated a project with a total number of more than a million reporters.\n\nUkraine became the first in Europe and the world's 18th largest U-Report country. The presentation took place on April 23, 2016 in Kyiv with the participation of 1,500 people and the well-known Ukrainian music band Pianoboy. During the year of activity in December 2016, there were over 25,000 u-reporters in Ukraine with 53% younger than 19. As for September 30 2018, there are 68,273 u-reporters in Ukraine.\n\nThe last country that joined the project was Malawi, March 28, 2018.\n\nUNICEF Innovation has been working with SMS systems since 2007, when an open source platform called RapidSMS was developed. RapidPro is an open-source platform which allows the implementation of an SMS application without the need of a programmer. It provides real-time information and data analytics. It was developed by the UNICEF global Innovation Centre in collaboration with Nyuruka, a Rwandan software development firm. An additional advantage is the ability to create fields required for a specific context, in each country, different fields for the presentation of identical data.\n\nTwo categories of information were identified based on data available from a sample of 6 countries (Mali, Burundi, Cameroon, Zimbabwe, Sierra Leone and Central African Republic): identification (age, gender, occupation) and location ( different administrative levels) and a set of best practices for naming variables.\n\nTelegram was chosen first in early 2016 because it was one of the first messaging applications to provide an open API that allowed organizations to develop and integrate chatbots with other platforms. Telegram, however, had low penetration in the countries where U-Report operates, so it was never used at scale. Later in 2016, U-Report tested the Facebook Messenger API for delivering U-Report surveys. By late 2017, the integration was still at an early stage, but the U-Report team viewed its partnership with Facebook as a critical investment that would result in cost savings to U-Report and U-Reporters because, unlike SMS, there is no per-message cost to use Facebook Messenger. \n\nIn December 2015, U-Report Indonesia was launched on Twitter. Data received can be disaggregated by age, gender, States, LGAs, Wards and settlements in real time.\n\nUNICEF examines the results of surveys and collaborates with organizations and government agencies to apply the results in practice: in programs, laws, decisions.\n\nOne example of U-Report in actionis in Liberia on sexual abuse in schools. To assess the magnitude of the situation, U-Report asked 13,000 users in Liberia whether teachers at their schools were exchanging grades for sex. An astonishing 86% of those polled said yes. Following U-Report’s discovery of a “Sex 4 Grades” epidemic, hotlines across the country were inundated by victims who now felt safe enough to reach out for support. Liberia’s Minister of Education and UNICEF are now collaborating on a plan to address the issue. \n\nUNICEF Pakistan has since launched an MHM innovation challenge to encourage U-Reporters to come up with their own solutions to break the taboo. U-Reporters responded. 60 submitted valid proposals of which 7 were selected as grant winners of up to PKR 250,000 to develop their ideas.  Winning ideas include a sanitary napkin vending machine, an artificial intelligence tool and an app that will be developed by young U-Reporters in partnership with Islamabad Engineering School.\n\nThe project has two categories of users involved in the activities: u-reporters and u-ambassadors. U-reporters - people who are interviewed and the U-ambassadors are the young people who distribute the idea of the project, organize the work with U-Reporters.\n\nDavid Beckham, UNICEF Goodwill Ambassador, supports the initiative. In September 2015, Beckham spoke with UN Secretary-General Ban Ki-moon and Anthony Lake, executive director of UNICEF, in the building of the General Assembly of the United Nations before the recently launched Youth Assembly, installed by Google, which displays messages from children from around the world.\n\nTaras Topola, the front of the group ANTITALY, is the official representative of U-Report in Ukraine. He constantly speaks at events, gives interviews. In 2017, Taras Topola made a tour with Ukraine in order to attract young people to U-report. \n\nGoodwill Ambassador to UNICEF, Hollywood actor Orlando Bloom supports the project at the global level. In 2017, he called on young people to join the project through video talk. And in 2018, in the framework of UNICEF, he came to Ukraine by visiting several kindergartens and schools in Slavyansk.\n\n\n"}
