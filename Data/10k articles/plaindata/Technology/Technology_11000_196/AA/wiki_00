{"id": "12628378", "url": "https://en.wikipedia.org/wiki?curid=12628378", "title": "ADSL loop extender", "text": "ADSL loop extender\n\nA DSL loop extender is a device that a telephone company can place between subscriber premises equipment and central office interfaces to extend the distance and increase the channel capacity of digital subscriber line (DSL) connections. ADSL repeaters are deployed by rural telephone companies trying to provide rural Internet service to farms and small towns where it is impractical to place the DSLAM closer to the subscriber. Typical distance improvements with a loop extender are shown in the diagram below, with rate in megabits per second and distance in thousands of feet.\n\nMultiple loop extenders can be placed on a line, effectively making the reach of the ADSL signal infinite. That is, it is possible to reach any subscriber with any ADSL speed if one uses multiple loop extenders.\n\nA repeater can either be an amplifier or a re-generator. Amplifiers increase the signal level of the analog transmission signal; re-generators demodulate the signal to binary, then re-modulate it into the original transmission frequency. Because regeneration restores the signal to binary, an indefinite number of re-generators can be placed on a line and is the preferred choice for services like T1 (Digital Signal 1) that have no distance limits. Because of the simplicity of the amplifier circuits, amplifiers are of lower cost than re-generators.\n\nBefore the development of ADSL loop extenders and remote DSLAMs, ADSL was limited to 3–6 miles (5–10 km) from the Central Office depending on the wire gauge used. An ADSL Loop Extender works as an amplifier, boosting the signal level so it can travel longer distances. In some cases, service can now be established as far as 10 miles from the Central Office.\n\nIn 2006, US telco promoted Fiber to the Home. This was driven by a rapidly growing housing sector that was creating the \"greenfield\" customers that are needed to make fiber to the home profitable. Later, with the housing sector in a serious recession, that \"greenfield\" seems to be drying up fast. With most of the \"brownfield\" market already tapped for ADSL, Telcos finally are interested in extending ADSL to those semi-rural areas that have never been important before.\n\nIn 2010, the US Federal Government updated the subsidies paid to rural telephone companies so that broadband is subsidized rather than phone service in a program called Connect America Fund. In order to qualify for subsidy, the telephone company must provide 4 mbits downstream and 1 mbit upstream. This has increased the demand for ADSL loop extenders because loop extenders will allow the telephone companies to reach the most distant subscribers in a way that is more cost effective than deploying remote DSLAMs.\n\nSome ADSL loop extenders aren't repeaters, but instead convert to a different signal (like G.shdsl) that fares better over extreme distances. This is because G.shdsl can use the lower frequencies that ADSL reserved for voice use.\n\nCrosstalk has been analyzed using T1.417 Method B and found to be compliant. Since the objective is to provide DSL to locations where no other data service can reach, or is needed (e.g. onto a farm), and ordinary telephone service is very immune to cross talk from ADSL due to ADSL not using voice frequency range, the cross talk issue is further reduced. Converting to G.shdsl or other technologies has problems too. These technologies have limited downstream speed, thus are less useful except to extend services to the most distant customers. Their many components (special C.O., re-generators, CPE) make them more expensive than ADSL amplifiers.\n\n"}
{"id": "44428974", "url": "https://en.wikipedia.org/wiki?curid=44428974", "title": "ATUM", "text": "ATUM\n\nDNA2.0 is now ATUM provides products and services for life science and Synthetic biology research. DNA2.0/ATUM also provides free access to research tools such as Gene Designer, DNA Atlas and a gRNA designer.\n\nDNA2.0 (now ATUM) was founded in 2003, in Menlo Park, California. The company is privately held and continues to have all research, development and production in California, currently in their 50,000 sq ft Newark facility. It began and continues as a gene synthesis and protein engineering provider to academia, government and the pharmaceutical, chemical, agricultural and biotechnology industries. Gene Synthesis rapidly replaced molecular cloning for many academic and corporate labs, as \"foundries for the biotechnology age\" allowing made-to-order genes for biological research. \nDNA2.0 was featured on the PBS show Nova ScienceNow to show how genes are created synthetically in a lab. In 2008, the company supplied some of the DNA stretches used to create a synthetic bacterial genome. \nDan Rather Reports included DNA2.0 in their episode on Synthetic Biology and how it is solving \"some of the most important problems facing the world.\"\nIn 2009, The Scientist named the codon design algorithms (now trademarked as GeneGPS) developed by DNA2.0 as one of the Top 10 Innovations of the year for Life Sciences. DNA2.0 developed the Electra Vector System, a universal cloning system that utilizes the type IIS restriction enzyme SapI and T4 DNA ligase in a single-tube reaction. DNA2.0 has made some molecular components, such as synthetic fluorescent proteins, available in open-access collections of DNA parts (BioBricks Foundation). DNA2.0 is a founding member of the International Gene Synthesis Consortium (IGSC) to promote biosecurity in the gene-synthesis industry. There are over 1,200 published scientific articles using DNA2.0 products and/or services, of which 44 include company employees as an author(s).\n\n\n\n\n"}
{"id": "24889368", "url": "https://en.wikipedia.org/wiki?curid=24889368", "title": "Appcelerator Titanium", "text": "Appcelerator Titanium\n\nTitanium SDK is an open-source framework that allows the creation of native mobile apps on platforms including iOS, Android and Windows UWP from a single JavaScript codebase, developed by Appcelerator.\n\nIn February 2013, \"Business Insider\" estimated that 10% of all smartphones worldwide ran Titanium-built apps. , Titanium had amassed over 950,000 developer registrations.\n\nThe core component of Titanium is the Apache-licensed software development kit, Titanium SDK. Appcelerator also makes Alloy, an Apache-licensed, Titanium-based model–view–controller framework, and Appcelerator Studio a proprietary integrated development environment starting for free.\n\nThe core features of Titanium SDK include:\n\nAll application source code gets deployed to the mobile device where it is interpreted using a JavaScript engine; Mozilla's Rhino is used on Android and BlackBerry, and Apple's JavascriptCore is used on iOS. In 2011 it was announced that a port to Google's V8 JavaScript engine is in development which, when complete, will significantly improve performance. Program loading takes longer than it does for programs developed with the native SDKs, as the interpreter and all required libraries must be loaded before interpreting the source code on the device can begin.\n\nTitanium does much more than just build apps that run on multiple platforms. Its true power comes from how it helps in developing \"best of breed\", \"native\" apps that take full advantage of the platforms on which they run. It enables:\n\nWhen it was introduced in December 2008, Titanium was intended for developing cross-platform desktop applications and was sometimes compared to Adobe Air. However, it added support for developing iPhone and Android mobile applications in June 2009, and in 2012, Titanium Desktop was spun off into a separate, community-driven project named TideSDK. Support for developing iPad-based tablet apps was added in April 2010. BlackBerry support was announced on June 2010, and has been in beta since April 2013. Tizen support was also added in April 2013 with the 3.1.0 Titanium Studio and SDK releases. The latest addition to the platform in 2016 has been Hyperloop, a technology to access native API's on iOS, Android and Windows with JavaScript.\n\nIn April 2010, Appcelerator expanded the Titanium product line with the Titanium Tablet SDK. The Titanium Tablet SDK draws heavily from the existing support for iPhone, but it also includes native support for iPad-only user interface controls such as split views and popovers. Initially the mobile SDK only supported development for iPad, but support now includes Android-based tablets as well.\n\nIn June 2011, Appcelerator released Studio and Titanium Mobile 1.7. Studio is a full open standards IDE that is derived from Aptana Studio which Appcelerator acquired in January 2011.\n\nIn June 2013, Jeff Haynie, Appcelerator's CEO, announced that the company had begun Ti.Next, a project to rewrite the Titanium SDK in Javascript for improved performance and to bring Titanium's end users, who write in Javascript, closer to the internal code. In a blog post, he wrote:\nWe believe JavaScript should be the right language to build Titanium, not just apps on top of the Titanium SDK. With Ti.Next, we've created a small microkernel design that will allow us to have minimal bootstrap code in the native language (C, Java, C#, etc) that talks to a common set of compilers, tools and a single JavaScript Virtual Machine. We have found a way to make the WebKit KJS work on multiple platforms instead of using different VMs per platform. This means we can heavily optimize the microkernel (herein after called the \"TiRuntime\") and maintenance, optimizations and profiling can be greatly simplified. We're talking about vs. 100K LOC per platform.\n\nIn January 2016, Appcelerator was acquired by Axway, a global software company with more than 11,000 public- and private-sector customers in 100 countries. Since then, the Indie plans have been made free again, including native API access with Hyperloop.\n\n"}
{"id": "19415128", "url": "https://en.wikipedia.org/wiki?curid=19415128", "title": "Armed Forces Financial Network", "text": "Armed Forces Financial Network\n\nThe Armed Forces Financial Network (AFFN) is an interbank network with a mission to increase the versatility of participating financial institutions to better serve the U.S. military. It provides electronic financial transactions services to the military banking and defence credit union sector. It connects approximately 346 banks and credit unions with over 500,000 ATMs and 2.3 million point-of-sale locations that are often in or near military bases both in the United States and overseas. John M. Broda is the President and Chief Executive Officer of the Network.\n\nThe Armed Forces Financial Network was founded in 1985 at the request of the United States Army to support the \"Surepay\" direct deposit initiative. The AFFN's role in the Surepay initiative was to be one way U.S. military personnel could gain access to their money.\n\nIn 2017 the Armed Forces Financial Network launched a chip and pin travel card.\n"}
{"id": "49812757", "url": "https://en.wikipedia.org/wiki?curid=49812757", "title": "Baking (make-up)", "text": "Baking (make-up)\n\nBaking (verb), also known as 'cooking' is a make-up technique originally used in Drag, now popularised by celebrities and make-up artists. This technique includes applying a heavy amount of translucent powder under the eyes and on the high points of the face, including the 'T' zone, to set the base make-up. It is also used to extract oils from your face.\n\nThis technique can be used on a variety of different skin types and shades. To highlight the under eye area and high points of the face such as the cheek bones, the bridge of the nose, the chin and the forehead. The effects of the powder are to soak up facial oils and to help the foundation melt into the skin. This prevents creasing in the baked areas while maximising coverage and longevity of the make-up to leave the face looking matte.\n\nThe cosmetic technique of baking is a \"very old make-up technique\" that may be traced back to times such as Ancient Egypt and the early Victorian era.\n\nThe use of make-up can be traced back to times as early as Ancient Egypt. High class members of Egyptian society would use make-up to display their wealth, beliefs and positions within the community. Cosmetics were regarded as a symbol of high status and a sign of holiness and were often a major part of the burial ceremony along with jewellery.\n\nAlthough, Egyptians are renowned for heavy eye make-up, facial products were also used but were mainly focused on cheeks.\n\nDespite the fact that make-up developed over time it remained an ideological concept of wealth and a display of class. In the lead up and throughout the early Victorian era cosmetics took aspects from Ancient Egypt and began to be used by actresses and prostitutes to define the face and eyes, however this was condemned by Queen Elizabeth I who described the type of dark make-up as 'vulgar'.\n\nFrom the Renaissance up until the 20th century the lower classes predominately worked outside in jobs such as agricultural work, therefore, the stereotypical light-coloured British skin was darkened by exposure to the sun, this created the trend of 'pale faces' in mainland Europe. This type of cosmetic technique was used especially by royalty and the higher classes to segregate them from the poor. The whiter the complexion, the higher the class and nobility of the person. Consequently why Queen Elizabeth I is famous for her pale skin.\n\nBaking was brought into the spotlight by celebrities such as Kim Kardashian whose make-up artist, Mario Dedivanovic uses the technique to set the famous Kardashian's make-up for events. He argues that this practice shouldn't be used every day and he only uses it for celebrities that like \"a very dramatic, long lasting, matte finish to the face\" or if he is working on stage performers. This technique was also brought into light with the increased visibility of the drag community following the popular television show RuPaul's Drag Race. The technique of baking can be seen throughout the episodes when the queens are in the 'work room' preparing for the runway. Baking isn't just used for drag performers, it has been adopted by many make-up artists and is now used throughout special effects make-up to set bruising and cuts so that the products applied to the face melt in seamlessly and look as realistic as possible.\n\nThis tecnhnique was then popularised further by many bloggers and vloggers who have created videos and pages, teaching make-up enthusiasts and the general public how to bake or cook their face. This type of tutorial has taken over social media sites such as Youtube and Instagram, bringing baking into the mainstream as a new internet 'buzzword'. These such videos have gathered thousands of views, one of the most popular tutorials, by Heidi Hamoud, managed to gain over 2 millions views since its upload in April 2015.\n\nSomeone who has taken this technique directly from the drag community, into the social media and make-up tutorial scene, is Miss Fame. A season 7 contestant on RuPaul's drag race, who is now planning a make-up tour where she plans to teach people the art of 'painting', a term used in the drag community to refer to 'making up'. Miss Fame features heavily on Youtube, not only with her own channel of tutorials and how to guides but also throughout other 'beauty gurus' channels where she 'paints' them and provides viewers with tips and tricks from the drag community.\n\n\nAs seen above there are different ways in which one can bake the face. Jamie Greenberg, a make-up artist,said that baking falls under the umbrella of contouring, which includes highlighting and shading. Therefore baking can be used alongside contouring as a non-shimmery form of highlighting to emphasize the higher points of the face. Paired with contouring, it can also work to give the illusion of definition. Baking directly underneath the contour on the cheekbones can create a more crisp and clean contour. Paired with a highlighter or a strobing cream or powder, baking can set a base for a highlight, as well as intensifying it.\n\nBaking is used by many performers as it increases the longevity of the make-up, meaning it will not easily melt or rub off during performances. This technique is utilised by many make-up artists for celebrities and models as \"a way to set your make-up when standing under hot lights\", as it reduces the need for touch-ups. It also increases the coverage of the make-up giving a more poreless and flawless, matte finish to the face, something which works well for celebrities and performers especially when flash photography is being used.\n\nBaking can also be used as a form of colour correction. People tend to get dark circles under their eyes, baking this area is said to reduce the appearance of dark circles as the light shade and brightness from the powder or concealer can counteract the darkness around the eyes. People often bake their under eyes to reduce dark circles and also to prevent creasing in their fine lines and wrinkles, especially in the under eye area. Although, many make-up artists argue this isn't a good technique for ageing skin.\n\nFinally, make-up artists also use baking for practical reasons. For example, when applying eye shadow, they often bake under the eyes to catch any eye shadow that may fall down onto the face from its application, allowing the product to be easily brushed away with the excess powder.\n\n"}
{"id": "36702929", "url": "https://en.wikipedia.org/wiki?curid=36702929", "title": "Barricade tape", "text": "Barricade tape\n\nBarricade tape is brightly colored tape (often incorporating a two-tone pattern of alternating yellow-black or red-white stripes or the words \"Caution\" or \"Danger\" in prominent lettering) that is used to warn or catch the attention of passersby of an area or situation containing a possible hazard. It acts as a minor impediment to prevent accidental entrance to that area or situation and as a result enhances general safety. \nBarricade tape is also known as construction tape or barrier tape or in reference to the safety hazard involved as caution tape, warning tape, danger tape or hazard tape. When used by a police force, the tape is named police tape.\n\nThe tape is often wrapped and affixed as a visual warning sign and demarcation, for instance against entering a dangerous area, such as an industrial or commercial building site, a roadworks construction site or the scene of an accident or a crime (for crime scene preservation), or against handling inoperative machinery or appliances.\n\nBarricade tape is made with durable, resilient, tear-proof plastic materials such as polyethylene, polypropylene, or nylon. It is at least 2 mm thick and 3 inches wide. However, different manufacturers offer different sizes and thicknesses of barricade tape. Other common sizes of barricade tape are 2, 4 or 6 inches wide and 4 or 10 mm thick.\n\nBarricade tape often has a bright background and pre-printed bold warning text. It is also possible to purchase plain barricade tape and write a custom message on it. However, care should be taken when using custom tape, as all barricade tape designs are required to comply with Occupational Safety and Health Administration (OSHA) and American National Standards Institute (ANSI) regulations (when used for purposes subject to regulation by these organizations).\n\nBarricade tape is used according to the color specifications set by OSHA and ANSI. Barricade tape may be use primarily as a safety precaution for various industries and procedures.\n\nConstruction Tape – is used in construction zones to notify people about ongoing construction and that there are possible hazards within the demarcated area. Construction tape usually employs a yellow-black color combination and incorporates printed text, such as \"Under Construction\", \"Caution\", \"Work Zone\", and \"Keep Out\" (among others). This type of barrier tape is commonly found at the site of renovations, demolition, and minor repairs.\n\nHazard Tape – is used in locations containing a substantial danger. Examples include electrocution hazards or areas within which there is a risk of exposure to toxic chemicals. Hazard tape is available in different color combinations, each of which indicates certain classes of threat. For example, yellow-black tape is used to signal the presence of a physical hazard (e.g., a hole), while magenta-yellow denotes a radiation hazard. This type of barrier tape is commonly used in laboratories, production areas, and industrial zones.\n\nTraffic Control Device Tape – this type of barrier tape, as its name implies is used to control traffic, whether foot traffic or vehicle traffic. Traffic control device tapes are used as temporary traffic signal to redirect traffic during parade or whenever a road is closed. These are usually brightly colored, either in solid orange or orange-white combination.\n\nPolice Tape or Law Enforcement Tape – this type of barrier tape is used to isolate, protect and preserve a crime scene. Police tape is used to notify the public that an investigation is on going and that a particular area is restricted. This is usually seen with a yellow-white, yellow-black or blue-white color combination.\n\nFirefighter Tape – serves the same purpose as police tape and hazard tape. Firefighter tape is used to isolate a particular area during or after a fire to keep the public away from fire-related risks (e.g., smoke inhalation, airborne particulate matter, and damaged structures).\n\nThe choice of colours of barricade tape depends on the contrast with its background, which in the case of the sky, can vary from black to white.\n\nTo have a reasonable chance of being visible against most backgrounds, the tape needs a light colour (white or yellow) and a darker colour.\n\nOSHA and ANSI provide precise specification for barricade tape colors. These are found in OSHA regulations 1910.22 and 1910.144 and ANSI Z535.5-2007, \"Safety Tags and Barricade Tapes (for Temporary Hazards)\". However, the dimension, thickness, and materials of the barricade tape are left to the discretion of the manufacturer.\n\n\n"}
{"id": "5498814", "url": "https://en.wikipedia.org/wiki?curid=5498814", "title": "Bridge management system", "text": "Bridge management system\n\n\" \"Bridge management\" redirects here.\n\nA bridge management system or BMS is a means for managing bridges throughout design, construction, operation and maintenance of the bridges. As funds available become tighter, road authorities around the world are facing challenges related to bridge management and the escalating maintenance requirements of large infrastructure assets. Bridge management systems help agencies to meet their objectives, such as building inventories and inspection databases, planning for maintenance, repair and rehabilitation (MR&R) interventions in a systematic way, optimizing the allocation of financial resources, and increasing the safety of bridge users.\n\nThe major tasks in bridge management are: collection of inventory data; inspection; assessment of condition and strength; repair, strengthening or replacement of components; and prioritizing the allocation of funds. A BMS is a means of managing bridge information to formulate maintenance programs within cost limitations. A BMS includes four basic components: data storage, cost and deterioration models, optimization and analysis models, and updating functions. \n"}
{"id": "35695335", "url": "https://en.wikipedia.org/wiki?curid=35695335", "title": "Carmelite Water", "text": "Carmelite Water\n\nCarmelite water is an alcoholic extract of lemon balm and other herbs. It was initially crafted in the 14th Century by Carmelite nuns from the Abbey of St Just, and was commercialized under the name \"Eau de Carmes\". It is used as an herbal tonic and toilet water.\n"}
{"id": "7744414", "url": "https://en.wikipedia.org/wiki?curid=7744414", "title": "Customer attrition", "text": "Customer attrition\n\nCustomer attrition, also known as customer churn, customer turnover, or customer defection, is the loss of clients or customers.\n\nBanks, telephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer attrition analysis and customer attrition rates as one of their key business metrics (along with cash flow, EBITDA, etc.) because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.\n\nCompanies usually make a distinction between voluntary churn and involuntary churn. Voluntary churn occurs due to a decision by the customer to switch to another company or service provider, involuntary churn occurs due to circumstances such as a customer's relocation to a long-term care facility, death, or the relocation to a distant location. In most applications, involuntary reasons for churn are excluded from the analytical models. Analysts tend to concentrate on voluntary churn, because it typically occurs due to factors of the company-customer relationship which companies control, such as how billing interactions are handled or how after-sales help is provided.\n\nWhen companies are measuring their customer turnover, they typically make the distinction between gross attrition and net attrition. Gross attrition is the loss of existing customers and their associated recurring revenue for contracted goods or services during a particular period. Net attrition is gross attrition plus the addition or recruitment of similar customers at the original location. Financial institutions often track and measure attrition using a weighted calculation called Recurring Monthly Revenue (or RMR). In the 2000s, there are also a number of business intelligence software programs which can mine databases of customer information and analyze the factors that are associated with customer attrition, such as dissatisfaction with service or technical support, billing disputes, or a disagreement over company policies. More sophisticated predictive analytics software use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn.\n\nFinancial services such as banking and insurance use applications of predictive analytics for churn modeling, because customer retention is an essential part of most financial services' business models. Other sectors have also discovered the power of predictive analytics, including retailing, telecommunications and pay-TV operators. One of the main objectives of modeling customer churn is to determine the causal factors, so that the company can try to prevent the attrition from happening in the future. Some companies want to prevent their \"good\" customers from deteriorating (e.g., by falling behind in their payments) and becoming less profitable customers, so they introduced the notion of partial customer churn.\n\nCustomer attrition merits special attention by mobile telecom service providers worldwide. This is due to the low barriers to switching to a competing service provider especially with the advent of Mobile Number Portability (MNP) in several countries. This allows customers to switch to another provider while preserving their phone numbers. While mature markets with high teledensity (phone market penetration) have churn rates ranging from 1% to 2% per month, high growth developing markets such as India and China are experiencing churn rates between 3% to 4% per month. By deploying new technologies such churn prediction models coupled with effective retention programs, customer attrition could be better managed to stem the significant revenue loss from defecting customers.\n\nCustomer attrition is a major concern for US and Canadian banks, because they have much higher churn rates than banks in Western Europe. US and Canadian banks with the lowest churn rates have achieved customer turnover rates as low as 12% per year, by using tactics such as free checking accounts, online banking and bill payment, and improved customer service. However, once banks can improve their churn rates by improving customer service, they can reach a point beyond which further customer service will not improve retention; other tactics or approaches need to be explored.\n\nChurn or Customer attrition is often used as an indicator of customer satisfaction. However the churn rate can be kept artificially low by making it difficult for the customers to resiliate their services. This can include ignoring resiliations requests, implementing lengthy and complicated resiliation procedures to follow through by an average consumer and various other barriers to resiliation. Thus, churn can improve while customer satisfaction deteriorates. This practice is short sighted and will backfire. However, it was shown to be common in telephone companies and among internet providers.\n\nScholars have studied customer attrition at European financial services companies, and investigated the predictors of churn and how the use of customer relationship management (CRM) approaches can impact churn rates. Several studies combine several different types of predictors to develop a churn model. This model can take demographic characteristics, environmental changes, and other factors into account.\n\nResearch on customer attrition data modeling may provide businesses with several tools for enhancing customer retention. Using data mining and software, one may apply statistical methods to develop nonlinear attrition causation models. One researcher notes that \"...retaining existing customers is more profitable than acquiring new customers due primarily to savings on acquisition costs, the higher volume of service consumption, and customer referrals.\" The argument is that to build an \"...effective customer retention program,\" managers have to come to an understanding of \"...why customers leave\" and \"...identify the customers with high risk of leaving\" by accurately predicting customer attrition.\n\nIn the business context, \"churn\" refers both to customers' migration and to their loss of value. So, \"churn rate\" refers, on the one hand, to the percentage of customers who end their relation with the organization, or, on the other hand, to the customers who still receive their services, but not as much or not as often as they used to.\nCurrent organizations face therefore a huge challenge: to be able to anticipate to customers’ abandon in order to retain them on time, reducing this way costs and risks and gaining efficiency and competitivity.\nThere are in the market advanced analytics tools and applications, especially designed to analyze in depth the enormous amount of data inside the organizations, and to make predictions based on the information obtained from analyzing and exploring those data. They aim to put at the service of marketing departments and agencies –and of all business users- the necessary weapons to:\n\nThere are organizations that have developed international standards regarding recognition and sharing of global best practice in customer service in order to reduce customer attrition. The International Customer Service Institute has developed The International Customer Service Standard to strategically align organizations so they focus on delivering excellence in customer service, whilst at the same time providing recognition of success through a 3rd Party registration scheme.\n\nNot all customer attrition is bad. For many firms, it is useful and desirable that unprofitable customers should churn away. This is known as customer divestment of unprofitable customers. However, simply because a customer is unprofitable does not mean that the customer should be divested, because there are strategic reasons for retaining unprofitable customers.\n\n\n"}
{"id": "12347279", "url": "https://en.wikipedia.org/wiki?curid=12347279", "title": "Density logging", "text": "Density logging\n\nDensity logging is a well logging tool that can provide a continuous record of a formation's bulk density along the length of a borehole. In geology, bulk density is a function of the density of the minerals forming a rock (i.e. matrix) and the fluid enclosed in the pore spaces. This is one of three well logging tools that are commonly used to calculate porosity, the other two being sonic logging and neutron porosity logging\n\nThe tool was initially developed in the 1950s and was in use throughout the hydrocarbon industry by the 1960s. A type of active nuclear tool, a radioactive source and detector are lowered down the borehole and the source emits medium-energy gamma rays into the formation. Radioactive sources are typically a directional Cs-137 source. These gamma rays interact with electrons in the formation and are scattered in an interaction known as Compton scattering. The number of scattered gamma rays that reach the detector, placed at a set distance from the emitter, is related to the formation's electron density, which itself is related to the formation's bulk density (formula_1) via\nwhere formula_3 is the atomic number, and formula_4 is the molecular weight of the compound. For most elements formula_5 is about 1/2 (except for hydrogen where this ratio is 1). The electron density (formula_6) in g/cm³ determines the response of the density tool.\n\nThe tool itself initially consisted of a radioactive source and a single detector, but this configuration is susceptible to the effects of the drilling fluid. In a similar way to how the sonic logging tool was improved to compensate for borehole effects, density logging now conventionally uses 2 or more detectors. In a 2 detector configuration, the short-spaced detector has a much shallower depth of investigation than the long-spaced detector so it is used to measure the effect that the drilling fluid has on the gamma ray detection. This result is then used to correct the long-spaced detector.\n\nAssuming that the measured bulk density (formula_1) only depends on matrix density (formula_8) and fluid density (formula_9), and that these values are known along the wellbore, porosity (formula_10) can be inferred by the formula\n\nCommon values of matrix density formula_8 (in g/cm³) are:\n\n\nThis method is the most reliable porosity indicator for sandstones and limestones because their density is well known. On the other hand, the density of clay minerals such as mudstone is highly variable, depending on depositional environment, overburden pressure, type of clay mineral and many other factors. It can vary from 2.1 (montmorillonite) to 2.76 (chlorite) so this tool is not as useful for determining their porosity. A fluid bulk density formula_9 of 1 g/cm³ is appropriate where the water is fresh but highly saline water has a slightly higher density and lower values should be used for hydrocarbon reservoirs, depending on the hydrocarbon density and residual saturation.\n\nIn some applications hydrocarbons are indicated by the presence of abnormally high log porosities.\n\nSonic logging\n"}
{"id": "684703", "url": "https://en.wikipedia.org/wiki?curid=684703", "title": "Distance measuring equipment", "text": "Distance measuring equipment\n\nDistance measuring equipment (DME) is a transponder-based radio navigation technology that measures slant range distance by timing the propagation delay of VHF or UHF radio signals.\n\nDeveloped in Australia, it was invented by James \"Gerry\" Gerrand under the supervision of Edward George \"Taffy\" Bowen while employed as Chief of the Division of Radiophysics of the Commonwealth Scientific and Industrial Research Organisation (CSIRO). Another engineered version of the system was deployed by Amalgamated Wireless Australasia Limited in the early 1950s operating in the 200 MHz VHF band. This Australian domestic version was referred to by the Federal Department of Civil Aviation as DME(D) (or DME Domestic), and the later international version adopted by ICAO as DME(I). \nDME is similar to secondary radar, except in reverse. The system was a post-war development of the IFF (identification friend or foe) systems of World War II. To maintain compatibility, DME is functionally identical to the distance measuring component of TACAN.\n\nAircraft use DME to determine their distance from a land-based transponder by sending and receiving pulse pairs – two pulses of fixed duration and separation. The ground stations are typically collocated with VORs or ILS systems. A low-power DME can be collocated with an ILS glide slope antenna installation where it provides an accurate distance to touchdown function, similar to that otherwise provided by ILS marker beacons.\n\nA typical Distance measuring equipment ground transponder system for en-route or terminal navigation will have a 1 kW peak pulse output on the assigned UHF channel.\n\nThe DME system comprises a UHF transmitter/receiver (interrogator) in the aircraft and a UHF receiver/transmitter (transponder) on the ground.\n\nSEARCH MODE: 150 interrogation pulse-pairs per second.\n\nThe aircraft interrogates the ground transponder with a series of pulse-pairs (interrogations) and, after a precise time delay (typically 50 microseconds), the ground station replies with an identical sequence of pulse-pairs. The DME receiver in the aircraft searches for reply pulse-pairs (X-mode= 12 microsecond spacing) with the correct interval and reply pattern to its original interrogation pattern. (Pulse-pairs that are not coincident with the individual aircraft's interrogation pattern e.g. not synchronous, are referred to as filler pulse-pairs, or squitter. Also, replies to other aircraft that are therefore non-synchronous also appear as squitter).\n\nTRACK MODE: less than 30 interrogation Pulse-pairs per second, as the average number of pulses in SEARCH and TRACK is limited to max 30 pulse pairs per second.\n\nThe aircraft interrogator locks on to the DME ground station once it recognizes a particular reply pulse sequence has the same spacing as the original interrogation sequence. Once the receiver is locked on, it has a narrower window in which to look for the echoes and can retain lock.\n\nA radio signal takes approximately 12.36 microseconds to travel to the target and back—also referred to as a radar-mile. The time difference between interrogation and reply, minus the 50 microsecond ground transponder delay, is measured by the interrogator's timing circuitry and converted to a distance measurement (slant range), in nautical miles, then displayed on the cockpit DME display.\n\nThe distance formula, \"distance = rate * time\", is used by the DME receiver to calculate its distance from the DME ground station. The rate in the calculation is the velocity of the radio pulse, which is the speed of light (roughly ). The time in the calculation is \"(total time – 50µs)/2\".\n\nThe accuracy of DME ground stations is 185 m (±0.1 nmi). It's important to understand that DME provides the physical distance from the aircraft to the DME transponder. This distance is often referred to as 'slant range' and depends trigonometrically upon both the altitude above the transponder and the ground distance from it.\n\nFor example, an aircraft directly above the DME station at 6,076 ft (1 nmi) altitude would still show on the DME readout. The aircraft is technically a mile away, just a mile straight up. Slant range error is most pronounced at high altitudes when close to the DME station.\n\nRadio-navigation aids must keep a certain degree of accuracy, given by international standards, FAA, EASA, ICAO, etc. To assure this is the case, flight inspection organizations check periodically critical parameters with properly equipped aircraft to calibrate and certify DME precision.\n\nICAO recommends accuracy of less than the sum of 0.25 nmi plus 1.25% of the distance measured.\n\nA typical DME ground based responder beacon has a limit of 2700 interrrogations per second (pulse pairs per second – pps). This means that it can provide distance information for up to 100 aircraft at a time—95% of transmissions for aircraft in tracking mode (typically 25 pps) and 5% in search mode (typically 150 pps). Above this limit the transponder avoids overload by limiting the sensitivity (gain) of the receiver. Replies to weaker (normally the more distant) interrogations are ignored to lower the transponder load.\n\nDME frequencies are paired to VHF omnidirectional range (VOR) frequencies and a DME interrogator is designed to automatically tune to the corresponding DME frequency when the associated VOR frequency is selected. An airplane’s DME interrogator uses frequencies from 1025 to 1150 MHz. DME transponders transmit on a channel in the 962 to 1213 MHz range and receive on a corresponding channel between 1025 and 1150 MHz. \nThe band is divided into 126 channels for interrogation and 126 channels for reply. The interrogation and reply frequencies always differ by 63 MHz. The spacing of all channels is 1 MHz with a signal spectrum width of 100 kHz.\n\nTechnical references to X and Y channels relate only to the spacing of the individual pulses in the DME pulse pair, 12 microsecond spacing for X channels and 30 microsecond spacing for Y channels.\n\nDME facilities identify themselves with a 1,350 Hz Morse code three letter identity. If collocated with a VOR or ILS, it will have the same identity code as the parent facility. Additionally, the DME will identify itself between those of the parent facility. The DME identity is 1,350 Hz to differentiate itself from the 1,020 Hz tone of the VOR or the ILS localizer.\n\nA terminal DME, referred to as a TDME in navigational charts, is a DME that is designed to provide a 0 reading at the threshold point of the runway, regardless of the physical location of the equipment. It is typically associated with ILS or other instrument approach.\n\nDME operation will continue and possibly expand as an alternate navigation source to space-based navigational systems such as GPS and Galileo.\n\n\n"}
{"id": "5748939", "url": "https://en.wikipedia.org/wiki?curid=5748939", "title": "Escape crew capsule", "text": "Escape crew capsule\n\nAn escape crew capsule is an escape capsule that allows one or more occupants of an aircraft or spacecraft to escape from the craft while it is subjected to extreme conditions, such as high speed or altitude. The occupant remains encapsulated and protected until such time as the external environment is suitable for direct exposure or the capsule reaches the ground.\n\nThere are two ways to do this:\n\nSome examples of U.S. military aircraft that have escape crew capsules are:\n\nPioneering developments in jettisonable-cockpit style escape capsule systems occurred in Nazi Germany, by both \"Heinkel Flugzeugwerke\" and by the \"Deutsche Forschungsanstalt für Segelflug\" (German Institute for Glider Research). \"Heinkel Flugzeugwerke\" built the first ejection-seat-equipped combat aircraft, the Heinkel He 219. \"Deutsche Forschungsanstalt für Segelflug\" flew the Heinkel He 176 rocket plane (in 1939), and the DFS 228 research aircraft, both of which had a jettisonable nose.\n\nThe British design for a supersonic test aircraft Miles M.52 of necessity had a jettisonable pilot capsule at the front of the aircraft. The aircraft progressed no further than a nearly complete airframe before cancellation in 1946, although a one-third scale remote control model successfully broke the sound barrier in 1948.\n\nThe first American attempt to design such an escape capsule was for the U.S. Navy F4D Skyray. It was tested in 1951-52 but was never installed in the aircraft. The Bell X-2, designed for flight in excess of Mach 3, could jettison the cockpit, though the pilot would still have to jump out and descend under his own parachute. The first production aircraft with an escape crew capsule was the Mach 2 B-58 Hustler. It was developed by the Stanley Aviation Company for Convair. The capsule was pressurized, sheltered the pilot from the airstream, and contained food and survival supplies. During testing of the \"Stanley Capsule\" in 1962, a bear became the first living creature to survive a supersonic ejection.\n\nThe Mach 3 XB-70's two crew escape capsules did not work well the only time they were needed. On June 8, 1966, XB-70 airframe AV/2 was involved in a mid-air crash with an F-104 Starfighter. Maj. Carl Cross's seat was unable to retract backwards into the escape capsule due to high-\"g\"-forces as the plane spiraled downwards. He died in the crash. Maj. Al White's seat did retract but his elbow protruded from the capsule and blocked the closing clamshell doors. He struggled to free his trapped elbow. As soon as he freed the doors, he was ejected from the plane and descended by parachute as planned. Due to pain and confusion, White failed to trigger the manually activated airbag that would normally cushion the capsule upon landing. When the capsule hit the ground, White was subjected to an estimated 33 to 44 \"g\" (320 to 430 m/s²). He received serious injuries, but nevertheless survived.\nIn the 1960s and 1970s, the F-111 and B-1A introduced the method of jettisoning the entire front fuselage as a means of crew escape. The crew remains strapped in the cabin, unencumbered by a parachute harness, while 27,000 lbf (120 kN) of thrust from rockets accelerates the module away from the rest of the aircraft. A single, large parachute retards the descent of the capsule. On landing, an airbag system cushions the landing. In the event of a water landing the airbag acts as a flotation device. Additional airbags could be activated to right the capsule in the event of a water landing (similar to the Apollo capsule), or an additional airbag could be selected for auxiliary flotation. With a movement of a pin at the base of the pilot's control stick, a bilge pump could be activated and extra air pumped into the airbags.\n\nThree of the four B-1A prototypes featured a single crew escape capsule for the crew members. For the fourth prototype and for the B-1B, this was changed to use conventional ejection seats. One source gives the reason \"due to concerns about servicing the pyrotechnical components of the system,\" while another says this was done \"to save cost and weight.\" On August 29, 1984, B-1A prototype #2 crashed and the capsule was ejected at low altitude. The parachute deployed improperly and one of the three crew died.\n\nKelly Johnson, founder of Lockheed Skunk Works, and developer of the U-2 and SR-71 Blackbird family of spy planes, commented on escape crew capsules when discussing development of the YF-12A (Blackbird) ejection seat: \"We set ourselves a very high goal in providing crew escape systems. We were determined to develop a system good for zero escape velocity on the ground and through the complete flight spectrum, having speeds above Mach 3 at 100,000 feet. We did achieve our design goals... I have never been convinced that a capsule ejection is required for anything other than high velocity re-entry from outer space. Our escape system in a very important sense really provides a capsule, which is the pressure suit, which is surely capable of meeting the speeds and temperatures likely to be encountered in the near future of manned aircraft.\" Rather than using escape capsules, SR-71 and U-2 pilots wore full pressure suits for high-altitude ejections. The suits were also heat-resistant so that SR-71 pilots could survive the high temperatures generated from a Mach 3 ejection.\n\n\n"}
{"id": "53247133", "url": "https://en.wikipedia.org/wiki?curid=53247133", "title": "European Lead Factory", "text": "European Lead Factory\n\nThe European Lead Factory is a public-private partnership that aims to accelerate early drug discovery in Europe. The European Lead Factory is funded by the Innovative Medicines Initiative and consists of a pan-European consortium that includes 7 pharmaceutical companies as well as partners from academia and small and medium-sized enterprises (SMEs).\n\nThe European Lead Factory is operational since 2013 and consists of two main components: the Joint European Compound Library and the European Screening Centre. Together these elements provide a platform for pharmaceutical researchers in Europe to identify drug discovery starting points, by connecting innovative drug targets to high-quality small molecules. The result is defined in a ‘hit list’: a number of compounds that show affinity for the target. The compounds on those lists can either be used as probes to better understand biological pathways or as starting points for lead discovery efforts for novel drugs. These hits can be further optimised outside of the European Lead Factory, for affinity but also for drug-like properties as selectivity, solubility and metabolism in the human body. The ultimate goal is that these candidate drugs will solve unmet medical needs when fully approved as drug by the authorities.\n\nThe Joint European Compound Library has a collection of around 500,000 chemical compounds selected from private company collections and complemented by the novel molecules synthesised by the European Lead Factory chemistry partners. European researchers from academia as well as SMEs and patient organisations submit their biological target to be screened against the compound collection by the European Lead Factory researchers by means of industrial standard high-throughput screening.\n"}
{"id": "17683377", "url": "https://en.wikipedia.org/wiki?curid=17683377", "title": "Floating wind turbine", "text": "Floating wind turbine\n\nA floating wind turbine is an offshore wind turbine mounted on a floating structure that allows the turbine to generate electricity in water depths where fixed-foundation turbines are not feasible.\nFloating wind farms have the potential to significantly increase the sea area available for offshore wind farms, especially in countries with limited shallow waters, such as Japan.\nLocating wind farms farther offshore can also reduce visual pollution, provide better accommodation for fishing and shipping lanes, and reach stronger and more consistent winds.\n\nCommercial floating wind turbines are mostly at the early phase of development, with several single turbine prototypes having been installed since 2007. \nAs of 2018, the only operational floating wind farm is Hywind Scotland, developed by Statoil and commissioned in October 2017. \nThe farm has 5 floating turbines with a total capacity of 30 MW.\n\nThe concept for large-scale offshore floating wind turbines was introduced by Professor William E. Heronemus at the University of Massachusetts Amherst in 1972. \nIt was not until the mid 1990s, after the commercial wind industry was well established, that the topic was taken up again by the mainstream research community. \nBlue H Technologies of the Netherlands deployed the first 80-kW floating wind turbine off the coast of Apulia, Italy in December 2007. \nThe prototype was installed in waters 113 m deep in order to gather test data on wind and sea conditions, and was decommissioned at the end of 2008.\nThe turbine utilized a tension-leg platform design and a two-bladed turbine.\nThe first large-capacity, 2.3-megawatt floating wind turbine was Hywind, which became operational in the North Sea near Norway in September 2009. \nThe turbine was constructed by Siemens Wind Power and mounted on a floating tower with a 100 m deep draft, with a float tower constructed by Technip.\n\nAfter assembly in the calmer waters of Åmøy Fjord near Stavanger, Norway, the 120 m tall tower was towed 10 km offshore into 220 m deep water, 10 km southwest of Karmøy, on 6 June 2009 for a two-year test deployment. \nAlexandra Beck Gjorv of Statoil said, \"[The experiment] should help move offshore wind farms out of sight … The global market for such turbines is potentially enormous, depending on how low we can press costs.\" \nHywind, owned by Statoil, cost 400 million kroner (around US$62 million) to build and deploy. The long submarine power transmission cable was installed in July 2009 and system test including rotor blades and initial power transmission was conducted shortly thereafter.\nThe installation was expected to generate about 9 GW·h of electricity annually.\nIn 2010 it survived 11 meter waves with seemingly no wear.\nBy 2016, the turbine had produced 50 GWh; an overall capacity factor of 41%.\n\nIn October 2011, WindFloat prototype was installed 4 km offshore of Aguçadoura, Portugal by Principle Power in approximately 45 m of water (previously the Aguçadoura Wave Farm site). \nThe WindFloat was fitted with a Vestas V80 2.0-MW offshore wind turbine and grid connected. \nThe installation was the first offshore wind turbine to be deployed without the use of any offshore heavy lift vessels as the turbine was fully commissioned onshore prior to the unit's being towed offshore. \nThis is the first offshore wind turbine installed in open Atlantic waters, and the first to employ a semi-submersible type floating foundation.\n\nSeaTwirl deployed its first floating grid connected wind turbine off the coast of Sweden in August 2011. \nIt was tested and decommissioned. \nThe design intended to store energy in a flywheel, thus, energy could be produced even after the wind stopped blowing.\n\nIn June 2013, the University of Maine deployed the VolturnUS 1:8, a tall floating turbine prototype that is 1:8th the scale of a 6-megawatt (MW), rotor diameter design. \nVolturnUS 1:8 was the first grid-connected offshore wind turbine deployed in the Americas. \nThe VolturnUS design utilizes a concrete semi-submersible floating hull and a composite materials tower designed to reduce both capital and Operation & Maintenance costs, and to allow local manufacturing. \nThe technology was the result of collaborative research and development conducted by the University of Maine-led DeepCwind Consortium.\n\nIn 2013, Statoil pulled out of the $120 million project of four 3-MW turbines floating in 140 m depth of water near Boothbay Harbor, Maine citing change in legislation, and focused on their five 6-MW turbines in Scotland instead, where the average wind speed is 10 m/s and the water depth is 100 m.\n\nIn 2015, Statoil received permission to install \"Hywind Scotland\", a 30 MW floating wind farm off Peterhead using 5 Hywind turbines.\nStatoil also plans to test a 1 MWh lithium-ion battery system (called Batwind) with the farm. \nHywind Scotland was commissioned in October 2017, becoming the first commercial floating wind farm in operation.\nConstruction of the NOK 2 billion (£152m) project started in 2016 in Spain, Norway and Scotland. \nThe turbines were assembled at Stord in Norway in summer 2017 using the Saipem 7000 floating crane, and the finished turbines were moved to near Peterhead. \nThree suction cup anchors hold each turbine.\n\nThe first floating turbine in Japan was floated near Fukue Island in 2016, after a 5-year demonstration period near shore.\nThe 2-MW turbine was developed by Hitachi.\n\nIn June 2016, Maine’s New England Aqua Ventus I floating offshore wind demonstration project, designed by the DeepCwind Consortium, was selected by the U.S. Department of Energy to participate in the Offshore Wind Advanced Technology Demonstration program.\n\nTwo common types of engineered design for anchoring floating structures include tension-leg and catenary loose mooring systems.\n\"Tension leg mooring systems\" have vertical tethers under tension providing large restoring moments in pitch and roll. \n\"Catenary mooring systems\" provide station–keeping for an offshore structure yet provide little stiffness at low tensions.\" \nA third form of mooring system is the \"ballasted catenary\" configuration, created by adding multiple-tonne weights hanging from the midsection of each anchor cable in order to provide additional cable tension and therefore increase stiffness of the above-water floating structure.\n\nThe IEC 61400–3 design standard requires that a loads analysis be based on site-specific external conditions such as wind, wave and currents.\nThe IEC 61400–3-2 standard applies specifically to floating wind turbines.\n\nThe technical feasibility of deepwater floating wind turbines is not questioned, as the long-term survivability of floating structures has been successfully demonstrated by the marine and offshore oil industries over many decades. However, the economics that allowed the deployment of thousands of offshore oil rigs have yet to be demonstrated for floating wind turbine platforms. For deepwater wind turbines, a floating structure will replace pile-driven monopoles or conventional concrete bases that are commonly used as foundations for shallow water and land-based turbines. The floating structure must provide enough buoyancy to support the weight of the turbine and to restrain pitch, roll and heave motions within acceptable limits. The capital costs for the wind turbine itself will not be significantly higher than current marine-proofed turbine costs in shallow water. Therefore, the economics of deepwater wind turbines will be determined primarily by the additional costs of the floating structure and power distribution system, which are offset by higher offshore winds and close proximity to large load centres (e.g. shorter transmission runs).\n\nWith empirical data obtained from fixed-bottom installations off many countries since the late 1990s, representative costs are well understood. \nShallow-water turbines cost between 2.4 and 3 million United States dollars per megawatt to install, according to the World Energy Council, while the practical feasibility and per-unit economics of deep-water, floating-turbine offshore wind is yet to be established. \nInitial deployment of single full-capacity turbines in deep-water locations began only in 2009.\n\n, new feasibility studies are supporting that floating turbines are becoming both technically and economically viable in the UK and global energy markets. \n\"The higher up-front costs associated with developing floating wind turbines would be offset by the fact that they would be able to access areas of deep water off the coastline of the UK where winds are stronger and reliable.\"\nThe recent Offshore Valuation study conducted in the UK has confirmed that using just one third of the UK's wind, wave and tidal resource could generate energy equivalent to 1 billion barrels of oil per year; the same as North Sea oil and gas production. \nA significant challenge when using this approach is the coordination needed to develop transmission lines.\n\nA 2015 report by Carbon Trust recommends 11 ways to reduce cost. Also in 2015, researchers at University of Stuttgart estimated cost at €230/MWh.\n\nWhen oil fields become depleted, the operator injects water to keep pressure high for further extraction. This requires power, but installing gas turbines means shutting down the extraction process, losing valuable income. The classification society DNV GL has calculated that in some cases a floating wind turbine can economically provide power for injection, as the oil platform can keep on producing, avoiding a costly pause.\n\nIn 2016 DNV GL, ExxonMobil and others approved calculations of saving $3/barrel of oil using a 6MW Hywind instead of traditional engines, driving two 2MW pumps injecting water into an offshore oil well. At least 44,000 barrels of processed water per day can be injected, even on calm June days. The project began laboratory testing in 2017.\n\nThe only operational floating wind farm is Hywind Scotland. \nThe farm has 5 floating turbines with a total capacity of 30 MW.\n\n, Japan had planned to build a pilot floating wind farm, with six 2-megawatt turbines, off the Fukushima coast of northeast Japan where the recent disaster has created a scarcity of electric power. \nAfter the evaluation phase is complete in 2016, \"Japan plans to build as many as 80 floating wind turbines off Fukushima by 2020.\"\nThe cost is expected to be in the range of 10–20 billion Yen over five years to build the first six floating wind turbines.\nIn 2011, some foreign companies had also planned to bid on the 1-GW large floating wind farm that Japan hoped to build by 2020.\nIn March 2012, Japan’s Ministry of Economy, Trade and Industry approved a 12.5bn yen ($154m) project to float a 2-MW Fuji in March 2013 and two 7-MW Mitsubishi hydraulic \"SeaAngel\" later about 20–40 km offshore in 100–150 metres of water depth. The Japanese Wind Power Association claims a potential of 519 GW of floating offshore wind capacity in Japan.\nThe first turbine became operational in November 2013.\nIn 2018, NEDO has announced two tenders to be launched in early February, aiming to support the development of both floating and fixed-bottom offshore wind projects in the country\nThe US State of Maine solicited proposals in September 2010 to build a floating wind farm. \nThe RFP is seeking proposals for 25 MW of deep-water offshore wind capacity to supply power for 20-year long-term contract period in the Gulf of Maine. \nProposals were due by May 2011.\nIn April 2012 Statoil received state regulatory approval to build a large four-unit demonstration wind farm off the coast of Maine.\n, the \"Hywind 2\" 4-tower, 12–15 MW wind farm was being developed by Statoil North America for placement off the east coast of Maine in -deep water of the Atlantic Ocean. \nLike the first Hywind installation off Norway, the turbine foundation will be a \"spar floater\".\nThe State of Maine Public Utility Commission voted to approve the construction and fund the US$120 million project by adding approximately 75 cents/month to the average retail electricity consumer. Power could be flowing into the grid no earlier than 2016.\nAs a result of legislation in 2013 (LD 1472) by the State of Maine, Statoil placed the planned Hywind Maine floating wind turbine development project on hold in July 2013. \nThe legislation required the Maine Public Utilities Commission to undertake a second round of bidding for the offshore wind sites with a different set of ground rules, which subsequently led Statoil to suspend due to increased uncertainty and risk in the project. Statoil is considering other locations for its initial US demonstration project.\nSome vendors who could bid on the proposed project in Maine expressed concerns in 2010 about dealing with the United States regulatory environment. Since the proposed site is in Federal waters, developers would need a permit from the US Minerals Management Service, \"which took more than seven years to approve a yet-to-be-built, shallow-water wind project off Cape Cod\". Also, the agency is under fire in June 2010 for lax oversight of deepwater oil drilling in Federal waters.\n\"Uncertainty over regulatory hurdles in the United States … is 'the Achilles heel' for Maine's ambitions for deepwater wind.\"\n\nScale modeling and computer modeling attempt to predict the behavior of large–scale wind turbines in order to avoid costly failures and to expand the use of offshore wind power from fixed to floating foundations. Topics for research in this field include:\n\n\n\nAs they are suitable for towing, floating wind turbine units can be relocated to any location on the sea without much additional cost. So they can be used as prototype test units to practically assess the design adequacy and wind power potential of prospective sites.\n\nWhen the transmission of generated wind power to nearby land is not economical, the power can be used in power to gas applications to produce hydrogen gas, ammonia / urea, reverse osmosis water desalination, natural gas, LPG, alkylate / gasoline, etc. on floating platforms which can be easily transported to nearby consuming centres.\n\nFloating wind turbines can be used to provide motive power for achieving artificial upwelling of nutrient-rich deep ocean water to the surface for enhancing fisheries growth in areas with tropical and temperate weather. Though deep seawater (below 50 metres depth) is rich in nutrients such as nitrogen and phosphorus, the phytoplankton growth is poor due to the absence of sunlight. The most productive ocean fishing grounds are located in cold water seas at high latitudes where natural upwelling of deep sea water occurs due to inverse thermocline temperatures. The electricity generated by the floating wind turbine would be used to drive high–flow and low–head water pumps to draw cold water from below 50 meters water depth and mixed with warm surface water by eductors before releasing into the sea. Mediterranean Sea, Black Sea, Caspian Sea, Red Sea, Persian Gulf, deep water lakes/reservoirs are suitable for artificial upwelling for enhancing fish catch economically. These units can also be mobile type to utilise the seasonal favourable winds all around the year.\n\nRisø and 11 international partners started a 4-year program called DeepWind in October 2010 to create and test economical floating Vertical Axis Wind Turbines up to 20 MW. The program is supported with €3m through EUs Seventh Framework Programme.\nPartners include TUDelft, Aalborg University, SINTEF, Equinor and United States National Renewable Energy Laboratory.\n\nThe GICON-TLP is a floating substructure system based on a tension leg platform (TLP) developed by GICON GmbH.\nThe system is deployable from 45 meters to 350 meters water depth. \nIt consists of six major components: four buoyancy bodies, horizontal pipes for structural base, vertical pipes which pass through the water line, angled piles for connection with the transition piece. \nCasted nodes are used to connect all components. \nThe TLP can be equipped with an offshore wind turbine in range of 6–10 MW.\n\nThe GICON-TLP is anchored to the seabed via four pre-tensioned mooring ropes with a buoyant gravity-base anchor consisting of concrete. No pile driving or drilling for anchoring is necessary. All ropes are connected at the corners of the square based system.\nThe TLP for a 6MW wind turbine is currently being developed by the GICON Group and their key partner, the Endowed Chair for Wind Energy Technology (LWET) at the University of Rostock, utilizing prefabricated steel-concrete composite components in combination with components of steel. \nA main focus of the TLP design is on the modularity and the possibility of assembly in any dry dock near to the installation site and without the use of construction vessels. \nAfter offshore location is reached, joints of TLP and anchor will be decoupled and the gravity anchor will be lowered down by using ballast water. Once the anchor has reached the bottom, it is filled with sand. One unique feature of the system is the sufficient floating stability during transport as well as during operations.\n\nIn October 2017, model tests took place in the model test facility of French École Centrale de Nantes (ECN) with 1:50 model of the GICON®-TLP incl. wind turbine. Based on this test a TRL of 5 was reached.\n\nIdeol’s engineers have developed and patented a ring-shaped floating foundation based on a central opening system (Damping Pool) used for optimizing foundation + wind turbine stability. As such, the sloshing water contained in this central opening counteracts the swell-induced floater oscillations. Foundation-fastened mooring lines are simply attached to the seabed to hold the assembly in position. This floating foundation is compatible with all wind turbines without any modification and has reduced dimensions (from 36 to 55 metres per side for a wind turbine between 2 and 8 MW). Manufacturable in concrete or steel, this floating foundation allows for local construction near project sites. Ideol leads the FLOATGEN project, a floating wind turbine demonstration project based on Ideol’s technology, built by Bouygues Travaux Publics and to be installed off the coast of Le Croisic on the offshore experimentation site of Ecole Centrale de Nantes (SEM-REV). The construction of this project, France's first offshore wind turbine (precisely 2 MW), is completed and the floating wind turbine was installed in April 2018.\n\nIn June 2015, the company has sealed its first commercial contract with the Japanese conglomerate Hitachi Zosen, for the design of the two latest Japanese floating offshore wind demonstrators. In July 2016, Ideol and Hitachi Zosen have signed a contract launching the construction phase of their 2 floating offshore wind turbines. Construction of the first one, manufactured in steel, is completed in dry dock and due to be commissioned before September 2018.\n\nIn August 2017, the French government has selected Eolmed, a consortium led by Quadran in association with Ideol, Bouygues Travaux Publics and Senvion, a French renewable energy developer, for the development and construction of a 25MW Mediterranean floating offshore wind farm 15 km off the coastal town of Gruissan (Languedoc-Roussillon), planned to be commissioned 2020.\n\nNautica Windpower has proposed a technique for potentially reducing system weight, complexity and costs for deepwater sites. Scale model tests in open water have been conducted (September 2007) in Lake Erie and structural dynamics modeling was done in 2010 for larger designs. Nautica Windpower's Advanced Floating Turbine (AFT) uses a single mooring line and a downwind two-bladed rotor configuration that is deflection tolerant and aligns itself with the wind without an active yaw system. Two-bladed, downwind turbine designs that can accommodate flexibility in the blades will potentially prolong blade lifetime, diminish structural system loads and reduce offshore maintenance needs, yielding lower lifecycle costs.\n\nSeaTwirl develops a floating vertical axis wind turbine (VAWT). The floater is based on a SPAR solution and is rotating along with the turbine. The concept limits the need for moving parts as well as bearings in the hub region. SeaTwirl is based in Gothenburg Sweden and is registered on the European growth market First North. 2015 SeaTwirl launched a 30kW prototype in the archipelago of Sweden which is connected to the grid at Lysekil. The aim is to scale the concept and to prove the scalability with a turbine of 1MW size in 2020. The concept is scalable for sizes well over 10MW.\n\nVolturnUS is North America’s first floating grid-connected wind turbine. \nIt was lowered into the Penobscot River in Maine on 31 May 2013 by the University of Maine Advanced Structures and Composites Center and its partners.\nDuring its deployment, it experienced numerous storm events representative of design environmental conditions prescribed by the American Bureau of Shipping (ABS) Guide for Building and Classing Floating Offshore Wind Turbines, 2013.\n\nThe VolturnUS floating concrete hull technology can support wind turbines in water depths of 45 m or more. \nWith 12 independent cost estimates from around the U.S. and the world, it has been found to significantly reduce costs compared to existing floating systems.\nThe design has also received a complete third-party engineering review.\n\nIn June 2016, the UMaine-led New England Aqua Ventus I project won top tier status from the US Department of Energy (DOE) Advanced Technology Demonstration Program for Offshore Wind. This means that the Aqua Ventus project is now automatically eligible for an additional $39.9 Million in construction funding from the DOE, as long as the project continues to meet its milestones.\n\nWindFloat is a floating foundation for offshore wind turbines designed and patented by Principle Power.\nA full-scale prototype was constructed in 2011 by Windplus, a joint-venture between EDP, Repsol, Principle Power, A. Silva Matos, Inovcapital, and FAI. \nThe complete system was assembled and commissioned onshore including the turbine. \nThe entire structure was then wet-towed (from southern to northern Portugal) to its final installed location offshore of Aguçadoura, Portugal, previously the Aguçadoura Wave Farm. \nThe WindFloat was equipped with a Vestas v80 2.0-megawatt turbine and installation was completed on 22 October 2011. \nA year later, the turbine had produced 3 GWh.\nThe cost of this project is around €20 million (about US $26 million). \nThis single wind turbine can produce energy to power 1300 homes.\n\nThe subsea metal structure is reported to improve dynamic stability, whilst still maintaining shallow draft, by dampening wave– and turbine–induced motion utilizing a tri-column triangular platform with the wind turbine positioned on one of the three columns. \nThe triangular platform is then \"moored\" using a conventional catenary mooring consisting of four lines, two of which are connected to the column supporting the turbine, thus creating an \"asymmetric mooring.\"\nAs the wind shifts direction and changes the loads on the turbine and foundation, a secondary hull-trim system shifts ballast water between each of the three columns. \nThis permits the platform to maintain even keel while producing the maximum amount of energy. \nThis is in contrast to other floating concepts which have implemented control strategies that de-power the turbine to compensate for changes in turbine thrust-induced overturning moment.\nThis technology could allow wind turbines to be sited in offshore areas that were previously considered inaccessible, areas having water depth exceeding 40 m and more powerful wind resources than shallow-water offshore wind farms typically encounter.\n\nA 25 MW WindFloat project received government permission in December 2016, with EU funding the €48 million transmission cable. \nThe €100 million project is expected to be funded by 2017 and operational by 2019.\nPrinciple Power was planning a 30-MW WindFloat project using 6-MW Siemens turbines in 366 m of water near Coos Bay, Oregon to be operational in 2017, but the project has since been cancelled.\n\nA combined floating wave and wind power plant was installed at Vindeby Offshore Wind Farm in 2010.\n\nAn open source project was proposed by former Siemens director Henrik Stiesdal in 2015 to be assessed by DNV GL. It suggests using tension leg platforms with replaceable pressurized tanks anchored to sheet walls.\n\nThe International Energy Agency (IEA), under the auspices of their \"Offshore Code Comparison Collaboration\" (OC3) initiative, has completed high-level design and simulation modeling of the \"OC-3 Hywind\" system, a 5-MW wind turbine installed on a floating spar buoy, moored with catenary mooring lines, in water depth of 320 metres. \nThe spar buoy platform would extend 120 metres below the surface and the mass of such a system, including ballast would exceed 7.4 million kg.\n\nVertiWind is a floating vertical axis wind turbine design created by Nenuphar whose mooring system and floater are designed by Technip. \n\n\n"}
{"id": "30310233", "url": "https://en.wikipedia.org/wiki?curid=30310233", "title": "GSP Jupiter", "text": "GSP Jupiter\n\nGSP Jupiter is a semi-submersible, jackup independent leg cantilever drilling rig operated by GSP Drilling, a Grup Servicii Petroliere subsidiary, and currently laid up in the port of Limassol. The drilling unit is registered in Malta.\n\n\"GSP Jupiter\" drilling rig was designed by Sonnat Offshore and was built by Petrom at the Galaţi Shipyard in 1987. The rig was completely reconstructed and refurbished in 2007 at a cost of US$55 million. The rig was owned and operated by Petrom from 1987 to 2005 when the company sold its six offshore platforms (including Atlas, Jupiter, Orizont, Prometeu and Saturn) to Grup Servicii Petroliere for US$100 million.\n\n\"GSP Jupiter\" has a length of , breadth of , draft of , height of and depth of . She has a maximum drilling depth of and she could operate at a water depth of . As a drilling rig, \"GSP Jupiter\" is equipped with advanced drilling equipment and has to meet strict levels of certification under international law. \"GSP Jupiter\" is able to maneuver with its own engines (to counter drift and ocean currents), but for long-distance relocation it must be moved by specialist tugboats. The rig is capable of withstanding severe sea conditions including waves and winds.\n\nCurrently the GSP Jupiter is operated by the British company Melrose Resources which uses the drilling rig at its Black Sea oil and natural gas prospects. On 19 May 2010 the GSP Jupiter started a drilling program at the Kaliakra East gas field which is thought to have 59 bcft of natural gas. After finishing this program the rig will be moved to the Kavarna East gas field.\n\n"}
{"id": "4955396", "url": "https://en.wikipedia.org/wiki?curid=4955396", "title": "Galvanic isolation", "text": "Galvanic isolation\n\nGalvanic isolation is a principle of isolating functional sections of electrical systems to prevent current flow; no direct conduction path is permitted. Energy or information can still be exchanged between the sections by other means, such as capacitance, induction or electromagnetic waves, or by optical, acoustic or mechanical means.\n\nGalvanic isolation is used where two or more electric circuits must communicate, but their grounds may be at different potentials. It is an effective method of breaking ground loops by preventing unwanted current from flowing between two units sharing a ground conductor. Galvanic isolation is also used for safety, preventing accidental current from reaching ground through a person's body.\n\nTransformers couple by magnetic flux. The primary and secondary windings of a transformer are not connected to each other (an autotransformer has a conductive connection between its windings and so does not provide isolation). The voltage difference that may safely be applied between windings without risk of breakdown (the isolation voltage) is specified in kilovolts by an industry standard. The same applies to transductors. While transformers are usually used to change voltages, isolation transformers with a 1:1 ratio are used in safety applications.\n\nIf two electronic systems have a common ground, they are not galvanically isolated. The common ground might not normally and intentionally have connection to functional poles, but might become connected. For this reason isolation transformers do not supply a \"GND/earth pole\".\n\nOpto-isolators transmit information by light waves. The sender (light source) and receiver (photosensitive device) are not electrically connected; typically they are held in place within a matrix of transparent, insulating plastic.\n\nCapacitors allow alternating current (ac) to flow, but block direct current; they couple ac signals between circuits at different direct voltages. Depending on conditions, a capacitor may fail and become \"short circuited\", ending its electric isolation function, which creates risk to the \"now connected circuit\" and, possibly, human danger. To address this question, there are special ratings for capacitors used for safety isolation, such as \"Class Y\".\n\nHall effect sensors allow an inductor to transfer information across a small gap magnetically. Unlike opto-isolators they do not contain a light source with a finite life, and in contrast to a transformer based approach they don't require DC balancing.\n\nMagnetocouplers use giant magnetoresistance (GMR) to couple from AC down to DC.\n\nOptocouplers are used within a system to decouple a function block from another connected to the power grid or other high voltage, for safety and equipment protection. For example, power semiconductors connected to the line voltage may be switched by optocouplers driven from low-voltage circuits, which need not be insulated for the higher line voltage. \n\nTransformers allow the output of a device to \"float\" relative to ground to avoid \npotential ground loops. Power isolation transformers increase the safety of a device, so that a person touching a live portion of the circuit will not have current flow through them to earth. Power sockets intended for electric razor supply may use an isolation transformer to prevent an electric shock if the razor should be dropped into water.\n\n"}
{"id": "20624225", "url": "https://en.wikipedia.org/wiki?curid=20624225", "title": "Geotek", "text": "Geotek\n\nGeotek is a UK company that designs and manufactures specialised geological and oceanographic equipment.\n\nFounded by Peter Schultheiss in 1989, Geotek was incorporated in 1994 when Quentin Huggett joined as a company director and the business moved from Haslemere, Surrey to its current location in Daventry, Northamptonshire where John Roberts joined as Technical Director.\n\nThe company supplies of equipment for the automated analysis of sediment and rock cores. The Multi-Sensor Core Logger has been sold and contracted to universities, institutions and survey companies who use the instrument for applications including studies of climate change\n, offshore site investigation and oil, gas and minerals exploration.\n\nThe company also specifies and manages marine environmental surveys of the seafloor and managed the Strategic Environmental Assessment for Oil and Gas and Renewable Energy for the Department of Trade and Industry. This was the UK's first fully online Government consultation process.\n\nIn recent years Geotek developed pressure core analysis, particularly for the study of gas hydrates, and has worked in a number of gas hydrate investigations funded by national governments.\nThe Geotek Multi-Sensor Core Logger (MSCL) is non-destructive core logging equipment that can work with both whole round and split cores of both hard rocks and soft sediments. The MSCL systems enable a number of geophysical measurements to be obtained automatically on sediment or rock cores under a variety of conditions.\n\n\n"}
{"id": "12336792", "url": "https://en.wikipedia.org/wiki?curid=12336792", "title": "Glasswort", "text": "Glasswort\n\nThe glassworts are various succulent, annual halophytes plants, that is, plants that thrive in saline environments, such as seacoasts and salt marshes. The original English glasswort plants belong to the genus \"Salicornia\", but today the glassworts include halophyte plants from several genera, some of which are native to continents unknown to the medieval English, and growing in ecosystems, such as mangrove swamps, never envisioned when the term glasswort was coined.\n\nThe common name \"glasswort\" came into use in the 16th century to describe plants growing in England whose ashes could be used for making soda-based (as opposed to potash-based) glass. \n\nThe ashes of glasswort plants, and also of their Mediterranean counterpart saltwort plants, yield soda ash, which is an important ingredient for glassmaking and soapmaking. Soda ash is an alkali whose active ingredient is now known to be sodium carbonate. \nGlasswort and saltwort plants sequester the sodium they absorb from salt water into their tissues (see \"Salsola soda\"). Ashing of the plants converts some of this sodium into sodium carbonate (or \"soda,\" in one of the old uses of the term).\n\nIn the medieval and early post-medieval centuries, various glasswort plants were collected at tidal marshes and other saline places in the Mediterranean region. The collected plants were burned. The resulting ashes were mixed with water. Sodium carbonate is soluble in water. Non-soluble components of the ashes sank to the bottom of the water container. The water with the sodium carbonate dissolved in it was then transferred to another container, and then the water was evaporated off, leaving behind the sodium carbonate. Another major component of the ashes that is soluble in water is potassium carbonate, a.k.a. potash. The resulting product consisted mainly of a mixture of sodium carbonate and potassium carbonate. This product was called \"soda ash\" (was also called \"alkali\"). It contained 20% to 30% sodium carbonate. For glassmaking, it was superior to a potash product obtained by the same procedure from the ashes of non-salty plants. If plant ashes were not washed as just described, they were still usable in glassmaking but the results were not as good.\n\nThe appearance of the word \"glasswort\" in English is reasonably contemporaneous with a 16th-century resurgence in English glassmaking, which had suffered a long decline after Roman times. This resurgence was led by glassmakers who emigrated to England from Lorraine and from Venice. The Lorraine glassmakers brought with them the technology of forest glass, the greenish glass that used potash from wood ashes as a flux. The Venetian glassmakers brought with them the technology of cristallo, the immaculately clear glass that used soda ash as a flux. These glassmakers would have recognized \"Salicornia europaea\" growing in England as a source for soda ash. Prior to their arrival, it was said that the plant \"hath no name in English.\"\n\nBy the 18th century, Spain had an enormous industry producing soda ash from saltworts; the soda ash from this source was known as \"barrilla\". Scotland had a large 18th-century industry producing soda ash from seaweed. The source of this ash was kelp. This industry was so lucrative that it led to overpopulation in the Western Isles of Scotland, and one estimate is that 100,000 people were occupied with \"kelping\" during the summer months. In the same period, soda ash (\"la soude de Narbonne\") was produced in quantity from glasswort proper around Narbonne, France. The commercialization of the Leblanc process for synthesizing sodium carbonate (from salt, limestone, and sulfuric acid) brought an end to the era of farming for soda ash in the first half of the 19th century.\n\nYoung shoots of \"Salicornia europaea\" are tender and can be eaten raw as a salad, Glasswort salad. The plant can further be prepared in several ways – cooked, steamed, or stir fried – and eaten as a vegetable dish.\n\nPlants that have been called glassworts include:\n"}
{"id": "36030800", "url": "https://en.wikipedia.org/wiki?curid=36030800", "title": "GraphExeter", "text": "GraphExeter\n\nGraphExeter is a material consisting of a few graphene sheets with a layer of ferric chloride molecules in between each graphene sheet. It was created by The Centre for Graphene Science at the University of Exeter in collaboration with the University of Bath.\n\n\n"}
{"id": "20361587", "url": "https://en.wikipedia.org/wiki?curid=20361587", "title": "Imagineering", "text": "Imagineering\n\nImagineering is a portmanteau combining the words \"imagination\" and \"engineering\". Imagineering is the implementation of creative ideas in practical form.\n\nThe word is a registered trademark of The Walt Disney Company, and is well known for its use within the name of Walt Disney Imagineering; however, contrary to popular belief, the term was neither coined by Disney, nor did it originate there. The word was \"invented\" by Alcoa around 1940, and appeared widely in numerous publications of several disciplines such as urban design, geography and politics, evolutionary economics, corporate culture and futures studies.\n\nDuring World War II, Alcoa created an internal \"Imagineering\" program to encourage innovative usage of aluminum in order to keep up with demand. A \"Time\" magazine ad from February 16, 1942, titled \"The Place They Do Imagineering\" relates the origin:\nOther notable pre-Disney usages include an October 24, 1942, mention in \"The New York Times\" in an article titled \"Christian Imagineering,\" a 1944 Oxford English Dictionary entry which cites an advertisement from the \"Wall Street Journal\", and the use by artist Arthur Radebaugh to describe his work, which was mentioned in the article \"Black Light Magic,\" in the \"Portsmouth Times\", Portsmouth, Ohio, 1947.\n\nOther early usage includes Richard F. Sailer's 1957 article \"BRAINSTORMING IS IMAGINation enginEERING\" written for the \"National Carbon Company Management Magazine\", and reprinted by the Union Carbide Company.\n\nWED Enterprises applied for a trademark for the term in 1967, claiming first use in 1962.\n\nIn 1981, a book with the title \"Imagineering for Health: Self-Healing Through the Use of the Mind\", authored by Serge King, was published by Quest Books (Wheaton, Illinois).\n\n\n\"Imagineering\" has also been used by:\n"}
{"id": "43087368", "url": "https://en.wikipedia.org/wiki?curid=43087368", "title": "Jinisys Software", "text": "Jinisys Software\n\nJinisys Software Inc. is a Filipino software engineering company that delivers various property management system for hospitality and real estates. Headquartered in Cebu Holdings Center, Cebu Business Park, Cebu City, Philippines, with other offices in Northgate Cyberzone, Filinvest City, Alabang.\nThe company's well-known products are Folio+, Event+, Resto+, Call+, Queue+, and SAP Business One as re-seller. The company \nalso provides custom software development services.\n\nIt started by two people who have seen the lack automation towards hotel management in the Philippines. As software engineers, they started to develop a software that could handle most hotel management tasks from basic to advance users. They began to distribute their hotel software to a few clients and their customer grew in time. The company was legally established on year 2008 through Philippine SEC registration.\n\nOn September 5, 2012, Jinisys Software filed and registered a patent right to Intellectual Property Office of the Philippines for its product Folio+. Folio+ is a hotel reservation system with additional features that mainly automates hotel management tasks.\n\nJinisys Software's Resto+ has been included on BIR's list of accredited POS software in the Philippines. Resto+ is a restaurant management software that internally interacts with Point of sale software and hardware.\n\nTwenty local universities in the Philippines integrated Folio+ with their hospitality curriculum.\n\nIn the year 2014, the company started expanding towards Asian countries specifically in Papua New Guinea.\n\nJinisys Software is a partner of SAP Philippines and a Cisco certified partner (Cisco, n.d.). The company is also an Avaya Partner.\n\n"}
{"id": "32761799", "url": "https://en.wikipedia.org/wiki?curid=32761799", "title": "LKFS", "text": "LKFS\n\nLoudness, K-weighted, relative to full scale (LKFS) is a loudness standard designed to enable normalization of audio levels for delivery of broadcast TV and other video. Loudness units relative to full scale (LUFS) is a synonym for LKFS that was introduced in EBU R128. Loudness units (LU) is an additional unit used in EBU R128. It describes L without direct absolute reference and therefore describes loudness level differences.\n\nLKFS is standardized in ITU-R BS.1770. In March 2011, the ITU introduced a loudness gate in the second revision of the recommendation, ITU-R BS.1770-2. In August 2012, the ITU released the third revision of this recommendation ITU-R BS.1770-3. In October 2015, the ITU released the fourth revision of this recommendation ITU-R BS.1770-4.\n\nThe EBU has suggested that the ITU should change the unit to LUFS, as LKFS does not comply with scientific naming conventions and is not in line with the standard set out in ISO 80000-8. Furthermore, they suggest the symbol for 'Loudness level, k-weighted' should be L, which would make L and LUFS equivalent when LUFS indicates the value of L with reference to digital full scale.\n\nLKFS and LUFS are identical in that they are both measured in absolute scale and both equal to one decibel (dB).\n"}
{"id": "56357045", "url": "https://en.wikipedia.org/wiki?curid=56357045", "title": "Lars Löfgren", "text": "Lars Löfgren\n\nLars Löfgren is a Swedish cybernetician. He was awarded the Wiener Gold Medal by the American Society for Cybernetics in 2008.\n\nLars Löfgren was involved in extending the logical and linguistic approaches to various problems raised by early cybernetics. His work helped\ndevelop a more consistent conceptual base for cybernetics through a holistic approach to second order cybernetics.\n\nHe was one of the internationally renown cyberneticians invited by Heinz von Förster to the Biological Computer Laboratory, but he did most of his work while professor at Lund University.\n\n"}
{"id": "2994661", "url": "https://en.wikipedia.org/wiki?curid=2994661", "title": "Lewis number", "text": "Lewis number\n\nThe Lewis number (Le) is a dimensionless number defined as the ratio of thermal diffusivity to mass diffusivity. It is used to characterize fluid flows where there is simultaneous heat and mass transfer.\n\nIt is defined as\n\nwhere formula_2 is the thermal diffusivity and formula_3 the mass diffusivity, formula_4 the thermal conductivity, formula_5 the density, formula_6 the mixture-averaged diffusion coefficient, and formula_7 the specific heat capacity at constant pressure. \n\nThe Lewis number can also be expressed in terms of the Prandtl number and the Schmidt number :\n\nIt is named after Warren K. Lewis (1882–1975), who was the first head of the Chemical Engineering Department at MIT. Some workers in the field of combustion assume (incorrectly) that the Lewis number was named for Bernard Lewis (1899–1993), who for many years was a major figure in the field of combustion research.\n\n"}
{"id": "47225586", "url": "https://en.wikipedia.org/wiki?curid=47225586", "title": "Light of Hope", "text": "Light of Hope\n\nLight of Hope abbreviated as LoH, is a project created by a team of some enthusiastic people who are working to make a change in the education system of Bangladesh. Aim of this organization is to provide e-learning facility to rural schools where there is no electricity. They provide Laptops, Projectors, Audio Visual e-learning materials, Solar system etc. to those schools which are situated in remote area where there is no electricity or enough facilities for proper education. They also provide books and essential education materials to the students of those remote areas. \nThe founder of this project is Md. Waliullah Bhuiyan,(Manager, BRAC). Mr. Nasimul Islam Maruf and Md. Asaduzzaman Shoeb co-founded this project. They both are the assistant professor of Electric and Electronics at AIUB. Now this project has almost 100 active members working on it. There are also some other sub-projects running under this project.\n\nIn 2014, LoH opened its first solar powered school in Bangladesh\n\nBack in 2009, Md. Waliullah Bhuiyan, founder of Light of Hope, was working on a rural project with BRAC in a small village of Patuakhali. In that village, they were talking with a group of women about how they make money for living. As there was no electricity on that 'Chor' area, some people from local market developed solar energy for electricity. The village people started going to the local market and charge their cell phones with exchange of an amount of money. Electricity providers were making profit out of this as well as using it for their own works. So, the women of that area used the idea as it could help them to earn some extra money. Ambiya Khatun was the pioneer of this idea. She first applied that idea in practical. She convinced her husband who was a rickshaw puller, to start this business. Somehow that poor couple managed the money and bought a solar system from Grameen Shakti on monthly installment and set their business up. The business went successful and they made a good profit out of this. Watching them doing business successfully, many people got involved into this business and the area was being developed gradually. Md. Waliullah Bhuiyan was inspired by this improvement. \nMd. Waliullah Bhuiyan made the plan of this project and shared it with his two friends Nasimul Islam Maruf and Md. Asaduzzaman Shoeb. They liked the plan and designed the project. But there was financial problem. They did not have enough money to start the project and no one was agreed to sponsor it. So, they were waiting for an opportunity to start their project.\n\nDell issued its second Dell Education Challenge in 2013. Dell sought their inspiration, imagination and innovation to re-engineer today's learning environments, both in and outside of the classroom. The competition sought to continue to inspire innovative ideas from around the world that would help solve today's biggest issues in education, including those identified in a poll commissioned by Dell about new education models. The poll showed that respondents view a personalized approach to learning as most effective.\n\nMore than 400 projects were submitted in the last Dell Education Challenge and the winning teams were bringing their ideas to fruition. Grand prize winner, Forward Tutoring, was helping to create more support for individual learning needs and support for students by students via their online platform where credits are earned for volunteering in the community. Those students then redeemed their credits for tutoring from other qualified students. Tutors in turn could earn scholarships and internships from supporting organizations. Dell's competition was mainly focused on social entrepreneurship.\n\nThe founder, Md. Waliullah Bhuiyan and his co-founders, Mr. Nasimul Islam Maruf and Md. Asaduzzaman Shoeb shared their plan with their friends and family. As all of them engineers, they started thinking about how they can use our idea in practical field. Then Dell Education Challenge 2013 came up. They were studying in different countries at that time. They arranged a meeting on the skype and discussed the matter. After discussing, they submitted their project to Dell Education Challenge, 2013. Their idea got selected to the top 50. After the final selection, their idea got the third position and they got an invitation letter to Texas. Md. Waliullah went to Texas for the final competition. The other participant teams from U.S. and Texas, had already taken their project on pilot phase. But Md. Waliullah's idea was still not on working condition. If they had enough funding, they would start working. So, they won the third position along with the 2500 dollar price.They went for crowdfunding over the internet and gathered 4500 dollar. With a total sum of 6000 dollars they started their pilot project targeting two of schools as our first mission. Today those two of schools are running their multimedia education successfully. One school is in Chittagong and another is in Kishoregonj in Bangladesh. Now they have their project going successfully. They have targeted 4000 schools all over Bangladesh to be multimedia schools. \nLight of Hope started their journey with a school project. And now they have another project running called 'PORUA- The Reader'. This is a campaign for collecting books. The goal of this project is to create reader among the poor and neglected children who has thirst for knowledge but cannot afford books. In this Porua campaign, the 'Porua' team members collect books in different ways like they go to different schools and universities to collect books. They mainly ask for primary level story books. They also ask different social organization for donation. For an example, there is an organization in America named Room to Read. They donate books as a social service. Team 'Porua' asked them for donation. Also people who want to donate money can contact with this 'Porua' team. They ran their first campaign on AIUB, as they started the journey of our organization from there. 'Porua' team collected almost 200 books in one day and they got a wonderful response. Their campaign went successful and they wanted to spread the campaign countrywide. They opened a Facebook event for collecting books and set our representative in different universities for collecting books. Many organizations like BRAC, Room to Read, Save the Children etc. donated books to this project. rokomari.com donated 200 books. They have 3 years project plan for 'Porua- the Reader' and their target is to set up library in 500 schools.\n\n"}
{"id": "36949438", "url": "https://en.wikipedia.org/wiki?curid=36949438", "title": "Lip stain", "text": "Lip stain\n\nLip stain is a cosmetic product used to color the lips, usually in form of a liquid or gel. It generally stays on longer than lipstick by leaving a stain of color on the lips.\n\nHowever, it can dry the lips and is not recommended for winter.\n"}
{"id": "3490877", "url": "https://en.wikipedia.org/wiki?curid=3490877", "title": "Meat thermometer", "text": "Meat thermometer\n\nA meat thermometer or cooking thermometer is a thermometer used to measure the internal temperature of meat, especially roasts and steaks, and other cooked foods. The degree of \"doneness\" of meat or bread correlates closely with the internal temperature, so that a thermometer reading indicates when it is cooked as desired. When cooking, food should always be cooked so that the interior reaches a temperature sufficient, that in the case of meat is enough to kill pathogens that may cause foodborne illness or, in the case of bread, that is done baking; the thermometer helps to ensure this.\n\nA meat thermometer is a unit which will measure core temperature of meats while cooking. It will have a metal probe with a sharp point which is pushed into the meat, and a dial or digital display. Some show the temperature only; others also have markings to indicate when different kinds of meat are done to a specified degree (e.g., \"beef medium rare\").\n\nMeat thermometers are usually designed to have the probe in the meat during cooking. Some use a bimetallic strip which rotates a needle which shows the temperature on a dial; the whole thermometer can be left inside the oven during cooking. Another variety commonly used on turkey is the pop-up timer, which uses a spring held in by a soft material that \"pops up\" when the meat reaches a set temperature.\n\nOther types use an electronic sensor in the probe, connected by a flexible heat-resistant cable to a display. The probe is inserted in the meat, and the cable comes out of the oven (oven seals are flexible enough to allow this without damage) and is connected to the display. These types can be set to sound an alarm when the specified temperature is reached. Wireless types, where the display does not have to be close to the oven, are also available.\n\nMeat thermometers have many different models, such as single probe and multiprobe.\n\nSingle probe models are usually the cheapest, but they can only monitor one section of the meat. So you need to insert the probe into different places to monitor the whole piece of meat.\nMultiprobe models are more expensive, and usually can connect 2-8 probes. More probes means you can more accurately monitor the temperature of the whole piece meat.\n\nThe probe can be inserted into the meat before starting cooking, and cooking continued until the desired internal temperature is reached. Alternatively the meat can be cooked for a certain time and taken out of the oven, and the temperature checked before serving. The tip of the probe should be in the thickest part of the meat, but not touching bone, which conducts heat and gives an overestimate of the meat temperature.\n\nFor poultry insert the meat thermometer into the thigh, but do not touch the bone. The suggested temperature for poultry to reach before it is safe to consume is 74 °C (165 °F), unless the poultry is stuffed, in which case the temperature in the center of the stuffing should be about 74 °C (165 °F).\n\nFor beef, lamb, or veal insert the meat thermometer away from bone, fat, or cartilage. The meat should reach a temperature of between 63 °C (145 °F) for medium-rare, and 77 °C (170 °F) for well done.\n\nPork needs to reach the same temperature 71 °C (160 °F) as beef, lamb, or veal and the same rules for use of the thermometer apply.\n\nFor ground meat, you should insert the digital food thermometer into the thickest part of the piece. For hamburgers you should insert the thermometer probe through the side of the patty, all the way to the middle. Make sure to check each piece of meat or patty because heat can be uneven. Temperature should be 71 °C (160 °F) for beef, lamb, veal, or pork and 74 °C (165 °F) for poultry.\n\nFor casseroles, and eggs insert the thermometer into the thickest area. The temperature for casseroles should be 71 °C (160 °F) and for eggs 74 °C (165 °F).\n\nFor fish the temperature should be 70 °C (158 °F). For shellfish (for example, shrimp, lobster, crab, scallops, clams, mussels and oysters) the temperature should be at 74 °C (165 °F). For shellfish that are difficult to open, discard any that did not open during cooking.\nWhen baking the dough\n\n\n\n"}
{"id": "33735616", "url": "https://en.wikipedia.org/wiki?curid=33735616", "title": "Medical photography", "text": "Medical photography\n\nMedical photography is a specialized area of photography that concerns itself with the documentation of the clinical presentation of patients, medical and surgical procedures, medical devices and specimens from autopsy. The practice requires a high level of technical skill to present the photograph free from misleading information that may cause misinterpretation. The photographs are used in clinical documentation, research, publication in scientific journals and teaching.\n\nMedical photographers document patients at various stages of an illness, injuries and before and after surgical procedures. They record the work of healthcare professionals to assist in the planning of treatment and education of the public and other healthcare professionals. The nature of the work requires a respect for and sensitivity to people, an awareness of sterile procedures and an adherence to privacy legislation and policies.\n\nThe BioCommunications Association, in a survey commissioned in 2008 of individuals working in medical photography, found that most medical photographers are employed by university affiliated hospitals and research centers. Ten percent were freelancers working in specialty clinics such as dermatology, ophthalmology and plastic surgery. A few of these provided services to the medical-legal profession. Medical photographers photograph patients in clinics, wards and in operating rooms. They may also be called to photograph an autopsy and gross specimens in the pathology department. Specialized photography techniques using photomacrography and ultra-violet and fluorescence photography may also be used. The role of the medical photographer has changed over the years from being exclusively medical to incorporating more general photography of a commercial or editorial nature to support public relations and education. Video production is playing an increased role; medical photographers are often responsible for video conferencing from operating rooms and are involved in tele-medicine. Departments employing medical photographers tend to number five people or less. Some medical photographers specialize in areas such as ophthalmology and dermatology.\n\nMost medical photographers have a degree in photography from a college or university and frequently have a degree in the sciences. They need to have a good understanding of photographic and optical principles, and also understand the technical requirements of a particular job in order select or modify equipment. Knowledge of digital imaging software is necessary to edit and output images while maintaining scale and color balance.\n\nAn interest in science and medicine are important. A basic knowledge of anatomy and physiology coupled with a working knowledge of medical terminology is required in order to discuss the photographic needs with medical staff and other healthcare providers. Because they are working with patients, medical photographers must have the manners and sensitivity to make patients comfortable while being photographed. They must also be aware of the laws governing privacy and copyright.\n\nThe sciences were quick to realize the merits of photography because of its perceived ability to present an objective image of what was seen. This solved a problem of representation by artists who were asked to produce illustrations only from description or highly influenced by the interpretation of physicians and surgeons. The first application of photography in medicine appears in 1840 when Alfred François Donné of the Charité Hospital in Paris photographed sections of bones and teeth. He began making daguerreotypes through a microscope. Donné published engravings made from photographs by his student Léon Foucault. Hugh Welch Diamond, a physician and founding member of the Royal Photographic Society, used photography as a tool in medicine, particularly in the field of mental illness. He was working in the women’s section of the Surrey County Asylum in Twickenham in 1852, where he attempted to create a catalog of visual signs of insanity by photographing the patients and organizing the photographs by symptom. Guillaume-Benjamin Duchenne de Boulogne began photographing inmates in the Salpêtrière mental hospital in Paris in 1856. He devised a method for activating individual muscles of the face through electronic stimulation. With the assistance of Adrien Tournachon, brother of Felix Nadar, he photographed facial expressions and at one point listed 53 emotions that could be identified based on the muscular action. His work was published in 1862 in \"Mécanisme de la physionomie humaine\" in what was the most remarkable of all photographically illustrated books in medical science prior to 1900.\nDr. Jean-Martin Charcot, a student of Duchenne de Boulogne, believed like Diamond that photographs would play a significant role in the diagnosis and management of patients. A medical photography unit was established at Salpêtrière hospital in Paris in 1878 by Charcot. He hired Albert Londe who worked at Salpêtrière under Charcot's supervision. Londe was to not only make photographs but to create new apparatus to record signs and symptoms. Charcot began publishing \"Nouvelle iconographie de la Salpêtriere\" in 1888 that used photographs to show clinical presentations of cases at Salpêtrière. Londe published a major reference on the practice of medical photography \"La Photographie médicale\". in 1893. Londe developed a systematic method for photographing patients in fixed views that took into account depth of field and distortion caused by lens design and lens to subject distance.\n\nThere was growing interest in cultures and peoples in distant regions of the globe and photography was a way to place them under study especially when combined with influences from the study of phrenology and Darwin’s work on natural selection. In 1850, Joseph T. Zealy (1812–93) was commissioned by Louis Agassiz to make daguerreotypes of plantation workers of African origin in the southern United States of America. The pictures were intended as scientific documentation to support theories of ethnology. Carl Damman published a collection of photographs of different ethnic groups in \"Anthropologisch-ethnographisches Album in Photographien\". and in the same year William Marshall published \"A phrenologist amongst the Todas, or the Study of a Primitive Tribe in South India. History, Character, Customs, Religion, Infanticide, Polyandry, Language\". Thomas Huxley established a system of photographing the human body with fixed views which included a rod of known dimension to make measurements. Francis Galton believed it was possible to systematically organize traits of inheritable attributes, intellectual, moral and physical with respect to families, groups, classes and racial types. He believed that mental attributes could be measured by studying physical attributes. In an effort to identify and group characteristics, he made composites of up to two hundred photographs to create a universal physiognomy example of a group or type.\nDr. Reed B. Bontecou, a physician and soldier from New York, took the camera to the American Civil War (1861–1865) and photographed wounded soldiers as well as documenting treatments, surgeries and working conditions of the physician. The albums of wounded American Civil War soldiers treated and photographed by Bontecou have appeared in numerous exhibitions, many of the images were displayed at the Metropolitan Museum of Art as part of the Photography and the American Civil War exhibition. The Burns Archive Press book \"Shooting Soldiers: Civil War Medical Photography By Reed B. Bonteco\", contains a large selection of these photographs and a history of Bontecou.\n\nAttempts to publish medical photographs in anatomy text books was met with limited success in the early years of photography. The lack of textural and tonal variation made photographs difficult to interpret. This may have been due to the spectral sensitivity of early materials to blue, violet and ultra-violet light. This grouped the other tones together and rendered them as similar shades of black. Orthochromatic plates did not become commercially available until 1883 and even then the process allowed separation only of the blues, greens and yellows. In 1861, Nicolaus Rüdinger published \"Atlas des peripherischen Nervensystems des menchlichen Körpers, Cotta’schen\", using photographs by Joseph Albert of frozen sections. The photographs had to be retouched to make the structures obvious. Sterophotography became of interest as a way to add a three-dimensional quality to show the spatial relationships of gross anatomy and clinical case studies. Between 1894-1900, Albert Neisser of Leipzig produced a stereo atlas of anatomy and pathology. David Waterston published a set of stereo cards in 1905 to be used in a stereo-viewer. The cards showed labelled dissections, descriptive labels and came packaged with the stereoscopic viewer.\n\nThere were attempts to photograph inside the body as early as 1883. Emil Behnke used a carbon arc lamp, lenses and reflectors to photograph human vocal cords at exposures of ¼ second. Walter Woodbury had published a “photogastroscope” in 1890 that showed pictures of the interior of the stomach and in 1894, Max Nitze published photographs of the bladder using a cystoscope.\n\nBy 1870, Maury and Duhring had established a journal based on using medical photography, \"The Photographic Review of Medicine and Surgery\", published by Lippincott in Philadelphia, USA provided case studies and before and after photographs. Most major centres of medical education had adopted photography as a method of documentation and study by the 1900s. Many photographers were working in multi-faceted disciplines from radiology, pathology and ophthalmology. Medical photography became a special field of photography and in 1931 a group of photographers working in medicine came together at Yale University in the United States of America to form the Biological Photographic Association, which later became the BioCommunications Association Inc. The group published a journal; the Journal of Biological Photography which was later incorporated into the Journal of BioCommunication. Other organizations formed in England, Scandinavia and Australia. Photography continues today to play a role in medicine through documentation, research and education.\n\nWith the ubiquitous use of mobile phones for medical photography, mobile phone use for medical photography has been a rising issue in Canada. From 2000, the federal and provincial governments of Canada passed legislation to regulate the use, collection and disclosure of medical photography by healthcare professionals. As a result, Canadian companies have developed to create specialized mobile apps, such as ShareSmart, and businesses have sought to provide solutions to comply with the new regulatory scheme.\n\n"}
{"id": "20254389", "url": "https://en.wikipedia.org/wiki?curid=20254389", "title": "Mining and metallurgy in medieval Europe", "text": "Mining and metallurgy in medieval Europe\n\nDuring the Middle Ages from the 5th century AD to the 16th century, Western Europe saw a blooming period for the mining industry. The first important mines here were those at Goslar in the Harz mountains, taken into commission in the tenth century. Another famous mining town is Falun in Sweden where copper has been mined since the thirteenth century.\nThe rise of Western European mining industry depended, of course, closely on the increasing weight of Western Europe on the stage of world history. Although the subject has sometimes been overlooked by historians, advances in Medieval mining and metallurgy made the flourishing of Western European civilization to a large extent possible.\n\nMetallurgical activities were also encouraged by central political power, regional authorities, monastic orders and ecclesiastical overlords, who always tried to have control and claimed Regalian rights over the mines and a share in the output, both in private lands and regions belonging to the Crown. They were particularly interested in the extraction of the precious metal ores, but not only, and for this reason the mines in their territories were open to all miners (Nef 1987, 706-715).\n\nThe social, political, economic stagnation and decline that followed the Roman World affected Europe, throughout the early medieval period, and had critical impact upon the technological progress, trade and social organization. Technological developments that affected the course of metal production were only feasible within a stable political environment, and this was not the case until the 9th century (Martinon-Torres & Rehren in press, a).\n\nDuring the first medieval centuries, the output of metal was in a steady decline and constraint in small scale activities. Miners adopted methods much less efficient than those of the Roman times. Ores were extracted only from shallow depths or from remnants of former abandoned mines, assuming that the old shafts weren't already sunk. The vicinity of the mine to villages or towns was also a determinant factor when deciding about working on site, because of the high cost of material transportation (Martinon-Torres & Rehren in press, b). It seems like only the output of iron diminished less in relation to the rest of the base and precious metals until the 8th century. This fact, correlated with the dramatic decrease in copper production, in particular, may indicate a possible displacement of copper and bronze artifacts from iron ones (Forbes 1957, 64; Bayley et al. 2008, 50).\n\nBy the end of the 9th century, economic and social conditions, which dictated the increased need of metal for agriculture, arms, stirrups, and decoration, started to favor metallurgy and a slow but steady general progress was noted. Smelting sites were multiplied and new mines were discovered and exploited, like the well-known Mines of Rammelsberg, close to the town of Goslar by the Harz Mountains. Open-cast mining and metallurgical activities were mostly concentrated in the Eastern Alps, Saxony, Bohemia, Tuscany, the Rhineland, Gaul and Spain (Nef 1987). French, Flemish, but mainly German miners and metallurgists were the generators of metal production.\n\nThe period right after the 10th century, marks the widespread application of several innovations in the field of mining, and ore treatment. It marks a shift to large scale and better quality production. Medieval miners, and metallurgists, had to find solutions for the practical problems that limited former metal production, in order to meet the market demands for metals. The increased demand for metal was due to the remarkable population growth from the 11th to the 13th centuries. This growth had impact on agriculture, trade, and building construction, including the great Gothic churches.\n\nThe main concern had to do with inefficient means for draining water out of shafts and tunnels in underground mining. This resulted in the flooding of mines which limited the extraction of ore to shallow depths close to the surface.\n\nThe secondary concerns were the separation of the metal bearing minerals from the worthless material that surrounds, or is closely mixed with, it. There was also the difficulty of the transportation of the ore, which resulted in additional high costs.\n\nThe economic value of mining resulted in investment in the development of solutions to these problems, which had a distinct positive impact on medieval metal output. This included innovations such as water power using waterwheels for powering draining engines, bellows, hammers; or the introduction of advanced types of furnaces. These innovations were not adopted at once, or applied to all mines and smelting sites. Throughout the medieval period these technical innovations, and the traditional techniques, coexisted. Their application depended on the time period, and geographical region. Water power in medieval mining and metallurgy was introduced well before the 11th century, but it was only in the 11th century that it was widely applied. The introduction of the blast furnace, mostly for iron smelting, in all the established centres of metallurgy contributed to quantitative and qualitative improvement of the metal output, making metallic iron available at a lower price.\n\nIn addition, cupellation, developed in the 8th century, was more often used. It is used for the refinement of lead-silver ores, to separate the silver from the lead (Bayley 2008). Parallel production with more than one technical method, and different treatment of ores would occur wherever multiple ores were present at one site. (Rehren et al. 1999).\n\nUnderground work in shafts, although limited in depth, was accomplished either by fire-setting for massive ore bodies or with iron tools for smaller scale extraction of limited veins. The sorting of base and precious metal ores was completed underground and they were transferred separately (Martinon-Torres & Rehren in press, b).\n\nPermanent mining in Sweden proper begun in the High Middle Ages and did not spread to Finland until 1530 when the first iron mine was begun operations there.\n\nBy the 14th century, the majority of the more easily accessible ore deposits were exhausted. Thus, more advanced technological achievements were introduced in order to keep up with the demand in metal. The alchemical laboratory, separating precious metals from the baser ones they are typically found with, was an essential feature of the metallurgical enterprise. However, a significant hiatus in underground mining was noted during the 14th and the early 15th century because of a series of historical events with severe social and economic impacts. The Great Famine (1315–1317), the Black Death (1347–1353), which diminished European population by 1/3 to 1/2, and the Hundred Years War (1337–1453), which amongst others caused severe deforestation, had also dramatic influences in metallurgical industry and trade. Lead mining, for example, ground to a halt due to the Black Death pandemic, when atmospheric lead pollution from smelting dropped to natural levels (zero) for the first and only time in the last 2000 years. The great demand of metals, e.g. for armour, could not be met due to the lack of manpower and capital investment.\n\nIt was only by the end of the 13th century that great capital expenditures were invested and more sophisticated machinery was installed in underground mining, which resulted in reaching great depths. The wider application of water- and horse-power was necessary for draining water out of these deep shafts. Also, acid parting in separating gold from silver was introduced in the 14th century (Bayley 2008). However, notable signs of recovery were present only after the mid-15th century, when the improved methods were widely adopted (Nef 1987, 723).\n\nDeterminant for the European metal production and trade was the discovery of the New World, which has affected world economy ever since. Even though new rich ore deposits were found in Central Europe during the 15th century, this was not enough to meet the large amounts of precious metal imports from America.\n\nMetallurgists throughout medieval Europe were free to move within different regions. German metallurgists in search of rich precious metal ores, for instance, took the leading part in mining and affected the course of metal production, not only in East and South Germany but in almost all Central Europe and the Eastern Alps. As mining gradually became a task for specialized craftsmen, miners moved in large groups and they formed settlements with their own customs close to mines. They were always welcome by the regional authorities since the latter were interested in increasing the revenue and the exploitation of the mineral-rich subsurface was quite profitable. The authorities, lay and ecclesiastical, claimed a part of the output and smiths and miners were provided with land for cottages, mills, forges, farming, and pasture and they were allowed to use streams and lumber (Nef 1987, 706-715).\n\nProgressing to the high and late Middle Ages, as smelting sites became geographically independent from mines, metalworking was separated from ore smelting. The urban expansion from the 10th century onwards and the dominant role of towns provided metallurgists with the right environment to develop and improve their technology. Metallurgists got organized in guilds and, usually, their workshops were concentrated in town peripheries (McLees 1996).\n\nIn medieval societies, liberal and mechanical arts were considered as totally different from each other. Metallurgists, as all craftsmen and artisans, lacked the methodical intellectual background but they were the pioneers of causal thinking, based on empirical observation and experimentation (Zilsel 2000).\n\n\n"}
{"id": "817238", "url": "https://en.wikipedia.org/wiki?curid=817238", "title": "National innovation system", "text": "National innovation system\n\nThe National Innovation System (also NIS, National System of Innovation) is the flow of technology and information among people, enterprises and institutions which is key to the innovative process on the national level. According to innovation system theory, innovation and technology development are results of a complex set of relationships among actors in the system, which includes enterprises, universities and government research institutes.\n\nThe term National System of Innovation originated when Christopher Freeman and Bengt-Åke Lundvall worked together in the late 1980s. Freeman's research drew heavily on political economy of Friedrich List and his historical account of the rise of Japan as an economic superpower. Lundvall's work explored the important social interactions between suppliers and customers and their role in encouraging innovation in Denmark. Apart from a general definition, as above, there is no canonical definition of national innovation systems. A few dominant definitions are listed below (quoted by the OECD publication National Innovation Systems, 1997) which overlap quite a bit:\n\nA national system of innovation has been defined as follows:\n\n\nA country’s innovative performance largely depends on how these actors relate to\neach other as elements of a collective system of knowledge creation and use as well as\nthe technologies they use. For example, public research institutes, academia and\nindustry serve as research producers carrying out R&D activities. On the other hand,\ngovernments either central or regional play the role of coordinator among research\nproducers in terms of their policy instruments, visions and perspectives for the future.\nFurthermore, in order to promote innovation the different innovative actors must\nhave strong linkages with each other based on a strong level of trust and governments\nshould promote and activate trust among the different innovation actors. The linkages can take the form of joint research, personnel exchanges, crosspatenting,\nand purchase of equipment (OECD, 1997).\nFinally, NSI are shaped by distinct socio-cultural qualities of national communities.\nTherefore, there are national trajectories of innovativeness, technology orientation and\nlearning, which results in each nation, either highly developed or not, having some\nkind of NSI, no matter if working well or not. Further more, the\nSuccess factors of NSI have been seen by many scholars in the creation of supportive\ninstitutions and organizations (with a key role of education) and collaboration\nlinkages Bridging Scales in Innovation Policies throughout the various elements that\nconstitute a NSI. Examples include public R&D and companies, as well as common\nobjectives and innovative cultures of agents, altogether entailing self reinforcing\nprogress and synergies. Differences in the structures and strategies of NSI among\nvarious economically successful countries indicate however, that there is no universal\nbest practise recipe.\n\n\n\n"}
{"id": "11151565", "url": "https://en.wikipedia.org/wiki?curid=11151565", "title": "Normally unmanned installation", "text": "Normally unmanned installation\n\nA Normally Unmanned Installation (NUI) is a type of automated offshore Oil/Gas platform designed to be primarily operated remotely, without the constant presence of personnel. \n\nThese generally were characterized by their small size, often consisting of just a well bay with a helipad on top. \nThey are often a compromise of providing the convenience of surface wellheads, which are easier to build and maintain, while avoiding the high operating costs of a full production platform.\n\nThey are generally only used in shallower water, where constructing many small NUIs is a relatively easy and cheap option as compared to the cost of using subsea wells. \n\nThis can be seen in the Southern North Sea where large numbers of wells are on smaller NUIs, compared with the more northern areas of the continental shelf where fewer larger platforms and subsea sites are the norm. NUIs are also common offshore Western Australia where distances to populated areas are huge.\n\nNUIs are commonly serviced from a nearby larger platform, e.g., Mungo serviced from Marnock. These installations will include an emergency shelter with essential food and water in order to provide a safe refuge in the event that weather or other considerations prevent a visiting crew from returning to base. Regular visits may be made for routine maintenance and for smaller well work such as wireline operations. Anything larger requires a drilling rig to be brought in, but this is still an advantage over subsea wells, which require a drilling rig or light intervention vessel for any well intervention.\n\nIn recent times NUI has become a phase within the decommissioning of previously manned offshore installations. Platforms would generally be positively isolated from hydrocarbons and flushed clean to suit environmental issues. Platforms would then only be visited infrequently for integrity checks and maintenance of temporary equipment left for power requirements and safety functionality. The platform would then remain in this phase until further decommissioning activities are carried out to remove the topsides in modules or piece small, then remove the jacket.\n\n"}
{"id": "37651645", "url": "https://en.wikipedia.org/wiki?curid=37651645", "title": "Outline of control engineering", "text": "Outline of control engineering\n\nThe following outline is provided as an overview of and topical guide to control engineering:\n\nControl engineering – engineering discipline that applies control theory to design systems with desired behaviors. The practice uses sensors to measure the output performance of the device being controlled and those measurements can be used to give feedback to the input actuators that can make corrections toward desired performance. When a device is designed to perform without the need of human inputs for correction it is called automatic control (such as cruise control for regulating a car's speed).\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2824536", "url": "https://en.wikipedia.org/wiki?curid=2824536", "title": "Peaceful nuclear explosion", "text": "Peaceful nuclear explosion\n\nPeaceful nuclear explosions (PNEs) are nuclear explosions conducted for non-military purposes. Proposed uses include excavation for the building of canals and harbours, electrical generation, the use of nuclear explosions to drive spacecraft, and as a form of wide-area fracking. PNEs were an area of some research from the late 1950s into the 1980s, primarily in the United States and Soviet Union.\n\nIn the US, a series of tests were carried out under Project Plowshare. Some of the ideas considered included blasting a new Panama Canal, the use of underground explosions to create electricity, and a variety of geological studies. The largest of the excavation tests was carried out in the Sedan nuclear test in 1962, which released large amounts of radioactive gas into the air. By the late 1960s, public opposition to Plowshare was increasing, and a 1970s study of the economics of the concepts suggested they had no practical use. Plowshares saw decreasing interest from the 1960s, and was officially cancelled in 1977.\n\nThe Soviet program started a few years after the US efforts and explored many of the same concepts under their Nuclear Explosions for the National Economy program. The program was more extensive, eventually conducting 239 nuclear explosions. Some of these tests also released radioactivity, including a significant release of plutonium into the groundwater and the polluting of an area near the Volga River. A major part of the program in the 1970s and 80s was the use of very small bombs to produce shock waves as a seismic measuring tool, and as part of these experiments, two bombs were successfully used to seal blown-out oil wells. The program officially ended in 1988.\n\nAs part of ongoing arms control efforts, both programs came to be controlled by a variety of agreements. Most notable among these is the 1976 Peaceful Nuclear Explosions Treaty. The Comprehensive Nuclear-Test-Ban Treaty of 1996 prohibits all nuclear explosions, regardless of whether they are for peaceful purposes or not. Since that time the topic has been raised several times, often as a method of asteroid impact avoidance.\n\nIn the PNE Treaty, the signatories agreed: not to carry out any individual nuclear explosions having a yield exceeding 150 kilotons; not to carry out any group explosion (consisting of a number of individual explosions) having an aggregate yield exceeding 1,500 kilotons; and not to carry out any group explosion having an aggregate yield exceeding 150 kilotons unless the individual explosions in the group could be identified and measured by agreed verification procedures. The parties also reaffirmed their obligations to comply fully with the Limited Test Ban Treaty of 1963.\n\nThe parties reserve the right to carry out nuclear explosions for peaceful purposes in the territory of another country if requested to do so, but only in full compliance with the yield limitations and other provisions of the PNE Treaty and in accord with the Non-Proliferation Treaty.\n\nArticles IV and V of the PNE Treaty set forth the agreed verification arrangements. In addition to the use of national technical means, the treaty states that information and access to sites of explosions will be provided by each side, and includes a commitment not to interfere with verification means and procedures.\n\nThe protocol to the PNE Treaty sets forth the specific agreed arrangements for ensuring that no weapon-related benefits precluded by the Threshold Test Ban Treaty are derived by carrying out a nuclear explosion used for peaceful purposes, including provisions for use of the hydrodynamic yield measurement method, seismic monitoring and on-site inspection.\n\nThe agreed statement that accompanies the treaty specifies that a \"peaceful application\" of an underground nuclear explosion would not include the developmental testing of any nuclear explosive.\n\nOperation Plowshare was the name of the U.S. program for the development of techniques to use nuclear explosives for peaceful purposes. The name was coined in 1961, taken from Micah 4:3 (\"And he shall judge among the nations, and shall rebuke many people: and they shall beat their swords into plowshares, and their spears into pruning hooks: nation shall not lift up sword against nation, neither shall they learn war any more\"). Twenty-eight nuclear blasts were detonated between 1961 and 1973.\n\nOne of the first U.S. proposals for peaceful nuclear explosions that came close to being carried out was Project Chariot, which would have used several hydrogen bombs to create an artificial harbor at Cape Thompson, Alaska. It was never carried out due to concerns for the native populations and the fact that there was little potential use for the harbor to justify its risk and expense. There was also talk of using nuclear explosions to excavate a second Panama Canal.\n\nThe largest excavation experiment took place in 1962 at the Department of Energy's Nevada Test Site. The Sedan nuclear test carried out as part of Operation Storax displaced 12 million tons of earth, creating the largest human-made crater in the world, generating a large nuclear fallout over Nevada and Utah. Three tests were conducted in order to stimulate natural gas production, but the effort was abandoned as impractical because of cost and radioactive contamination of the gas.\n\nThere were many negative impacts from Project Plowshare's 27 nuclear explosions. For example, the Gasbuggy site, located 55 miles east of Farmington, New Mexico, still contains nuclear contamination from a single subsurface blast in 1967. Other consequences included blighted land, relocated communities, tritium-contaminated water, radioactivity, and fallout from debris being hurled high into the atmosphere. These were ignored and downplayed until the program was terminated in 1977, due in large part to public opposition, after $770 million had been spent on the project.\n\nThe Soviet Union conducted a much more vigorous program of 239 nuclear tests, some with multiple devices, between 1965 and 1988 under the auspices of Program No. 6 and Program No. 7 – Nuclear Explosions for the National Economy.\n\nThe initial program was patterned on the US version, with the same basic concepts being studied. One test, Chagan test in January 1965, has been described as a \"near clone\" of the US Sedan shot. Like Sedan, Chagan also resulted in a massive plume of radioactive material being blown high into the atmosphere, with an estimated 20% of the fission products with it. Detection of the plume over Japan led to accusations by the US that the Soviets had carried out an above-ground test in violation of the Partial Test Ban Treaty, but these charges were later dropped.\n\nThe later, and more extensive, \"Deep Seismic Sounding\" Program focused on the use of much smaller explosions for various geological uses. Some of these tests are considered to be operational, not purely experimental. These included the use of peaceful nuclear explosions to create deep seismic profiles. Compared to the usage of conventional explosives or mechanical methods, nuclear explosions allow the collection of longer seismic profiles (up to several thousand kilometers).\n\nThere are proponents for continuing the PNE programs in modern Russia. They state that the program already paid for itself and saved the USSR billions of rubles and can save even more if continued. They also say that the PNE is the only feasible way to put out large gushers and fires on natural gas deposits and the safest and most economically viable way to destroy chemical weapons.\n\nTheir opponents, including the discredited, Alexey Yablokov, state that all PNE technologies have non-nuclear alternatives and that many PNEs actually caused nuclear disasters.\n\nReports on the successful Soviet use of nuclear explosions in extinguishing out-of-control gas well fires were widely cited in United States policy discussions of options for stopping the 2010 Gulf of Mexico Deepwater Horizon oil spill.\n\nGermany at one time considered manufacturing nuclear explosives for civil engineering purposes. In the early 1970s a feasibility study was conducted for a project to build a canal from the Mediterranean Sea to the Qattara Depression in the Western Desert of Egypt using nuclear demolition. This project proposed to use 213 devices, with yields of 1 to 1.5 megatons detonated at depths of 100 to 500 meters, to build this canal for the purpose of producing hydroelectric power.\n\nThe Smiling Buddha, India's first explosive nuclear device was described by the Indian Government as a peaceful nuclear explosion.\n\nIn Australia, nuclear blasting was proposed as a way of mining iron ore in the Pilbara.\n\nApart from their use as weapons, nuclear explosives have been tested and used, in a similar manner to chemical high explosives, for various non-military uses. These have included large-scale earth moving, isotope production and the stimulation and the closing-off of the flow of natural gas.\n\nAt the peak of the Atomic Age, the United States initiated Operation Plowshare, involving \"peaceful nuclear explosions\". The United States Atomic Energy Commission chairman announced that the Plowshares project was intended to \"highlight the peaceful applications of nuclear explosive devices and thereby create a climate of world opinion that is more favorable to weapons development and tests\". The Operation Plowshare program included 27 nuclear tests designed towards investigating these non-weapons uses from 1961 through 1973. Due to the inability of the U.S. physicists to reduce the fission fraction of small, approximately 1 kiloton, yield nuclear devices that would have been required for many civil engineering projects, when long term health and clean-up costs from fission products were included in the cost, there was virtually no economic advantage over conventional explosives, except for potentially the very largest of projects.\nThe Qattara Depression Project, as developed by Professor Friedrich Bassler, who during his appointment to the West German ministry of economics in 1968, put forth a plan to create a Saharan lake and hydroelectric power station by blasting a tunnel between the Mediterranean sea and the Qattara Depression in Egypt, an area that lies below sea level. The core problem of the entire project was the water supply to the depression. Calculations by Bassler showed that digging a canal or tunnel would be too expensive, therefore Bassler determined that the use of nuclear explosive devices, to excavate the canal or tunnel, would be the most economical. The Egyptian government declined to pursue the idea.\n\nThe Soviet Union conducted a much more exhaustive program than Plowshare, with 239 nuclear tests, between 1965 and 1988. Furthermore, many of the \"tests\" were considered economic applications, not tests, in the Nuclear Explosions for the National Economy program.\n\nThese included one 30 kiloton explosion being used to close the Uzbekistani \"Urtabulak\" gas well in 1966 that had been blowing since 1963, and a few months later a 47 kiloton explosive was used to seal a higher pressure blowout at the nearby \"Pamuk\" gas field.\n\nThe public records for devices that produced the highest proportion of their yield via fusion-only reactions are possibly the Taiga Soviet peaceful nuclear explosions of the 1970s, with 98% of their 15 kiloton explosive yield being derived from fusion reactions, a total fission fraction of 0.3 kilotons in a 15 kt device.\n\nThe repeated detonation of nuclear devices underground in salt domes, in a somewhat analogous manner to the explosions that power a car internal combustion engine (in that it would be a heat engine) has also been proposed as a means of fusion power, in what is termed PACER. Other investigated uses for low-yield peaceful nuclear explosions were underground detonations to stimulate, by a process analogous to fracking, the flow of petroleum and natural gas in tight formations, this was most developed in the Soviet Union, with an increase in the production of many well heads being reported.\n\nIn 2015, billionaire entrepreneur Elon Musk popularized an approach in which the cold planet Mars, could be terraformed by utilizing the detonation of high-fusion-yielding thermonuclear devices over the mostly dry-ice containing carbon dioxide-icecaps on the planet. Musk's specific suggested plan would not be very feasible within the limitations of the energy of historically manufactured ~megaton range nuclear devices, therefore requiring major advancements in nuclear devices for it to be considered feasible. In part due to these fundamental problems with the concept, the physicist Michio Kaku who is considered to have initially put forward the concept, instead suggests to use nuclear reactors in the typical land based district heating manner to make isolated tropical biomes on the Martian surface. \nAlternatively, as nuclear detonations are presently somewhat energy limited in terms of \"demonstrated\" achievable yield, the use of an off-the-shelf nuclear explosive device could be employed to \"nudge\" a Martian-grazing comet towards the poles of the planet and upon impact would be a much more efficient scheme to deliver the required energy, water vapor, greenhouse gases and other biologically significant volatiles that could begin to quickly terraform Mars, one such missed opportunity for this occurred in October 2014, when a \"1 in a million years\" comet designated as C/2013 A1, also known as comet \"Siding Spring\", came within 87,000 miles of the martian atmosphere.\n\nThe discovery and synthesis of new chemical elements by nuclear transmutation, and their production in the necessary quantities to allow the studying of their properties, was carried out in nuclear explosive device testing. For example, the discovery of the short lived einsteinium and fermium, both created under the intense neutron flux environment within thermonuclear explosions, followed the first Teller-Ulam thermonuclear device test – Ivy Mike. The rapid capture of so many neutrons required in the synthesis of einsteinium would provide the needed direct experimental confirmation of the so-called r-process, the multiple neutron absorptions needed to explain the cosmic nucleosynthesis (production) of all heavy chemical elements heavier than nickel on the periodic table, in supernova explosions, before beta decay, with the r-process explaining the existence of many stable elements in the universe.\n\nThe worldwide presence of new isotopes from atmospheric testing beginning in the 1950s led to the 2008 development of a reliable way to detect art forgeries. Paintings created after that period may contain traces of caesium-137 and strontium-90, isotopes that did not exist in nature before 1945. (Fission products were produced in the natural nuclear fission reactor at Oklo about 1.7 billion years ago, but these decayed away before the earliest known human painting.)\n\nBoth climatology and particularly aerosol science, a subfield of atmospheric science, were largely created to answer the question of how far and wide fallout would travel. Similar to radioactive tracers used in hydrology and materials testing, fallout and the neutron activation of nitrogen gas served as a radioactive tracer that was used to measure and then help model global circulations in the atmosphere by following the movements of fallout aerosols.\n\nAfter the Van Allen Belts surrounding Earth were published about in 1958, James Van Allen suggested that a nuclear detonation would be one way of probing the magnetic phenomenon, data obtained from the August 1958 Project Argus test shots, a high altitude nuclear explosion investigation, were vital to the early understanding of Earth's magnetosphere.\n\nSoviet nuclear physicist and Nobel peace prize recipient Andrei Sakharov also proposed the idea that earthquakes could be mitigated and particle accelerators could be made by utilizing nuclear explosions, with the latter created by connecting a nuclear explosive device with another of his inventions, the explosively pumped flux compression generator, to accelerate protons to collide with each other to probe their inner workings, an endeavor that is now done at much lower energy levels with non-explosive superconducting magnets in CERN. Sakharov suggested to replace the copper coil in his MK generators by a big superconductor solenoid to magnetically compress and focus underground nuclear explosions into a shaped charge effect. He theorized this could focus 10 positively charged protons per second on a 1 mm surface, then envisaged making two such beams collide in the form of a supercollider.\n\nUnderground nuclear explosive data from peaceful nuclear explosion test shots have been used to investigate the composition of Earth's mantle, analogous to the exploration geophysics practice of mineral prospecting with chemical explosives in \"deep seismic sounding\" reflection seismology.\n\nProject A119, proposed in the 1960s, which as Apollo scientist Gary Latham explained, would have been the detonating of a \"smallish\" nuclear device on the Moon in order to facilitate research into its geologic make-up. Analogous in concept to the comparatively low yield explosion created by the water prospecting (LCROSS) Lunar Crater Observation and Sensing Satellite mission, which launched in 2009 and released the \"Centaur\" kinetic energy impactor, an impactor with a mass of 2,305 kg (5,081 lb), and an impact velocity of about , releasing the kinetic energy equivalent of detonating approximately 2 tons of TNT (8.86 GJ).\nThe first preliminary examination of the effects of nuclear detonations upon various metal and non-metal materials, occurred in 1955 with Operation Teapot, were a chain of approximately basketball sized spheres of material, were arrayed at fixed aerial distances, descending from the shot tower. In what was then a surprising experimental observation, all but the spheres directly within the shot tower survived, with the greatest ablation noted on the aluminum sphere located 60 feet from the detonation point, were slightly over 1 inch of surface material, was absent upon recovery. These spheres are often referred to as \"Lew Allen's balls\", after the project manager during the experiments.\n\nThe ablation data collected for various materials and the distances the spheres were propelled, serve as the bedrock for the nuclear pulse propulsion study, Project Orion. The direct use of nuclear explosives, by using the impact of ablated propellant plasma from a nuclear shaped charge acting on the rear pusher plate of a ship, was and continues to be seriously studied as a potential propulsion mechanism.\n\nAlthough likely never achieving orbit due to aerodynamic drag, the first macroscopic object to obtain Earth orbital velocity was a \"manhole cover\" propelled by the somewhat focused detonation of test shot Pascal-B in August 1957. The use of a subterranean shaft and nuclear device to propel an object to escape velocity has since been termed a \"thunder well\".\nIn the 1970s Edward Teller, in the United States, popularized the concept of using a nuclear detonation to power an explosively pumped \"soft\" X-ray laser as a component of a ballistic missile defense shield known as Project Excalibur. This created dozens of highly focused X-ray beams that would cause the missile to break up due to laser ablation.\n\nLaser ablation is one of the damage mechanisms of a laser weapon, but it is also one of the researched methods behind pulsed laser propulsion intended for spacecraft, though usually powered by means of conventionally pumped, laser arrays. For example, ground flight testing by Professor Leik Myrabo, using a non-nuclear, conventionally powered pulsed laser test-bed, successfully lifted a lightcraft 72 meters in altitude by a method similar to ablative laser propulsion in 2000.\n\nA powerful solar system based \"soft\" X-ray, to ultraviolet, laser system has been calculated to be capable of propelling an interstellar spacecraft, by the light sail principle, to 11% of the speed of light. In 1972 it was also calculated that a 1 Terawatt, 1-km diameter x-ray laser with 1 angstrom wavelength impinging on a 1-km diameter sail, could propel a spacecraft to Alpha Centauri in 10 years.\n\nA proposed means of averting an asteroid impacting with Earth, assuming low lead times between detection and Earth impact, is to detonate one, or a series, of nuclear explosive devices, on, in, or in a stand-off proximity orientation with the asteroid, with the latter method occurring far enough away from the incoming threat to prevent the potential fracturing of the near-Earth object, but still close enough to generate a high thrust laser ablation effect.\n\nA 2007 NASA analysis of impact avoidance strategies using various technologies stated:\n\nNuclear stand-off explosions are assessed to be 10–100 times more effective than the non-nuclear alternatives analyzed in this study. Other techniques involving the surface or subsurface use of nuclear explosives may be more efficient, but they run an increased risk of fracturing the target near-Earth object. They also carry higher development and operations risks.\n\n\n"}
{"id": "695553", "url": "https://en.wikipedia.org/wiki?curid=695553", "title": "Perkins Brailler", "text": "Perkins Brailler\n\nThe Perkins Brailler is a \"braille typewriter\" with a key corresponding to each of the six dots of the braille code, a space key, a backspace key, and a line space key. Like a manual typewriter, it has two side knobs to advance paper through the machine and a carriage return lever above the keys. The rollers that hold and advance the paper have grooves designed to avoid crushing the raised dots the brailler creates.\n\nAlthough braille notation was designed for people who are blind or visually impaired to read, prior to the introduction of the Perkins Brailler, writing braille was a cumbersome process. Braille writers created braille characters with a stylus and slate (as developed by Louis Braille) or by using one of the complex, expensive, and fragile braille writing machines available at the time.\n\nThe original Perkins Brailler was produced in 1951 by David Abraham (1896–1978), a woodworking teacher at the Perkins School for the Blind. The director of the Perkins School for the Blind, Gabriel Farrell, asked Abraham to create an inexpensive and reliable machine to allow students to more easily write braille. Farrell and Abraham worked with Edward Waterhouse, who was a math teacher at Perkins, to create the design for the Brailler.\n\nIn 2008, a lighter and quieter version was developed and launched. It also includes an erase key and an integrated carrying handle. The new model won the Silver Award in the 2009 International Design Excellence Awards.\n\nThe paper placement is achieved by rolling the paper onto an internal drum, unrolling it when the user presses a line-feed key, and using a clock-like escapement to move an embossing carriage over the paper. A system of six cams consisting of rods with a square cross-section transfers keystrokes to the wire-like styli contained in the carriage. Tolerances are close, and the buildup of oily dirt with normal use necessitates periodic cleaning and adjustment.\n\nA new version of the Perkins Brailler, the SMART Brailler, was invented by David S. Morgan and released in 2011. The SMART Brailler is based on the mechanical action of the classic Perkins Brailler, and, when unpowered, is operable as a standard Brailler. The SMART Brailler includes sensors capturing the mechanical motion of the embosser, and, when powered, adds text-to-speech audio feedback and a digital display for use by both sighted and blind individuals. Software for the SMART Brailler includes multi-lingual speech and Braille support, including English, UK English, Arabic, French, German, Italian, Portuguese, Polish, Russian, and Turkish.\n\nWith the advent of computers, many users create braille output using a computer and a braille embosser connected to the computer. Visually impaired users can read the computer screen by using screen reader computer software and/or braille displays. Users of such a system can use a computer keyboard in the standard way for typing or can use a special keyboard driver that allows the six keys \"sdf-jkl\" to be used as a braille entry device similar to the Perkins Brailler.\n\nMany visually impaired users use electronic portable note-taking devices that allow keyboard entry in braille using the 6-key layout of the Perkins Brailler and output in synthesized speech and/or a one or two-line refreshable braille display consisting of tiny pins made of metal and plastic.\n\nNotetakers include PDA features such as an address book and calculator. Because of the many moving parts and the accessibility of the refreshable braille displays to the environment, notetakers are typically quite expensive. They are easily damaged and must be returned to their country of origin for periodic cleaning.\n\n\n"}
{"id": "19336290", "url": "https://en.wikipedia.org/wiki?curid=19336290", "title": "Planter (farm implement)", "text": "Planter (farm implement)\n\nA planter is a farm implement, usually towed behind a tractor, that sows (plants) seeds in rows throughout a field. It is connected to the tractor with a drawbar or a three-point hitch. Planters lay the seeds down in precise manner along rows. Planters vary greatly in size, from 1 row to 54, with the biggest in the world being the 48-row John Deere DB120. Such larger and newer planters comprise multiple modules called row units. The row units are spaced evenly along the planter at intervals that vary widely by crop and locale. The most common row spacing in the United States today is 30 inches.\n\nVarious machines meter out seeds for sowing in rows. The ones that handle larger seeds tend to be called planters, whereas the ones that handle smaller seeds tend to be called seed drills, grain drills, and seeders (including precision seeders). They all share a set of similar concepts in the ways that they work, but there is established usage in which the machines for sowing some crops including maize (corn), beans, and peas are mostly called planters, whereas those that sow cereals are drills.\n\nOn smaller and older planters, a marker extends out to the side half the width of the planter and creates a line in the field where the tractor should be centered for the next pass. The marker is usually a single disc harrow disc on a rod on each side of the planter. On larger and more modern planters, GPS navigation and auto-steer systems for the tractor are often used, eliminating the need for the marker. Some precision farming equipment such as Case IH AFS uses GPS/RKS and computer-controlled planter to sow seeds to precise position accurate within 2 cm. In an irregularly shaped field, the precision farming equipment will automatically hold the seed release over area already sewn when the tractor has to run overlapping pattern to avoid obstacles such as trees.\n\nOlder planters commonly have a seed bin for each row and a fertilizer bin for two or more rows. In each seed bin plates are installed with a certain number of teeth and tooth spacing according to the type of seed to be sown and the rate at which the seeds are to be sown. The tooth size (actually the size of the space between the teeth) is just big enough to allow one seed in at a time but not big enough for two. Modern planters often have a large bin for seeds that are distributed to each row known as central commodity systems.\nA class of planters that dig down farther than others are called listers. They are not used much anymore, as their use belonged to a set of high-till methods that low-till and no-till methods have largely replaced. Corn listers were common on the Great Plains in the 1920s through 1950s.\n\nThere are different types of planters available with the main difference being mechanically driven vs. hydraulic/electrical driven. In a mechanical drive system the unit works by a small suspended tire being driven by another which is in contact with the ground (driven) tire. As the operator lowers the planter the two tires make contact and the planter is engaged. When the driven wheel begins to turn it then turns a series of gears that determine the population of the seed produced. The gears can be changed by the operator in order to change the planting population. A hydraulic driven system came about to correct the shortfalls of the ground driven system. Hydraulic driven systems allow the operator to change population on the go, as well as allowing the computer controller to follow a prepared prescription for an individual field. The system also allowed for plant populations to be infinite in that mechanical gears systems are limited to set number of population settings and gears available from manufactures. In 2014 John Deere introduced the ExactEmerge row unit which introduced high-speed planting. Precision Planting followed suit and released the vDrive system. These system were unique, not that they were electrical, but that they allowed an operator to double their speed when planting. Other manufacturers had already developed an electrical planter, but lacked these additional improvements. Traditionally, an operator would plant at about 4.5-5.5 mph for optimal performance. However, with the advent of these systems electrical motors match the speed of the tractor and \"dead-drop\" the seed in the trench using either a belt or brush-belt which cause the forward momentum of the planter to be offset by the rearward momentum of the seed. Older systems would instead drop the seed through a tube after the meter rather than place it in the seed trench directly.\n"}
{"id": "28413819", "url": "https://en.wikipedia.org/wiki?curid=28413819", "title": "Poly(p-phenylene oxide)", "text": "Poly(p-phenylene oxide)\n\nPoly(\"p\"-phenylene oxide) or poly(\"p\"-phenylene ether) (PPE) is a high-temperature thermoplastic. It is rarely used in its pure form due to difficulties in processing. It is mainly used as blend with polystyrene, high impact styrene-butadiene copolymer or polyamide. PPO is a registered trademark of SABIC Innovative Plastics IP B.V. under which various polyphenylene ether resins are sold.\n\nPolyphenylene ether was discovered in 1956 by Allan Hay, and was commercialized by General Electric in 1960.\n\nWhile it was one of the cheapest high-temperature resistant plastics, processing was difficult, while the impact and heat resistance gradually decreased with time. Mixing it with polystyrene in any ratio could compensate for the disadvantages. In the 1960s, modified PPE came into the market under the trademark Noryl.\n\nPPE is an amorphous high-performance plastic. The glass transition temperature is 215 °C, but it can be varied by mixing with polystyrene. Through modification and the incorporation of fillers such as glass fibers, the properties can be extensively modified.\n\nPPE blends are used for structural parts, electronics, household and automotive items that depend on high heat resistance, dimensional stability and accuracy. They are also used in medicine for sterilizable instruments made of plastic.\n\nThis plastic is processed by injection molding or extrusion; depending on the type, the processing temperature is 260-300 °C. The surface can be printed, hot-stamped, painted or metallized. Welds are possible by means of heating element, friction or ultrasonic welding. It can be glued with halogenated solvents or various adhesives.\n\nThis plastic is also used to produce air separation membranes for generating nitrogen. The PPO is spun into a hollow fiber membrane with a porous support layer and a very thin outer skin. The permeation of oxygen occurs from inside to out across the thin outer skin with an extremely high flux. Due to the manufacturing process, the fiber has excellent dimensional stability and strength. Unlike hollow fiber membranes made from polysulfone, the aging process of the fiber is relatively quick so that air separation performance remains stable throughout the life of the membrane. PPO makes the air separation performance suitable for low temperature (35-70F)(2-21C) applications where polysulfone membranes require heated air to increase permeation.\n\nNatural phenols can be enzymatically polymerised. Laccase and peroxidase induce the polymerization of syringic acid to give a poly(1,4-phenylene oxide) bearing a carboxylic acid at one end and a phenolic hydroxyl group at the other.\n\n\"Translated from the article \"\" on the German Wikipedia.\"\n"}
{"id": "38970682", "url": "https://en.wikipedia.org/wiki?curid=38970682", "title": "Positronic (company)", "text": "Positronic (company)\n\nPositronic is a manufacturing company based in Springfield, Missouri. The company manufactures and supplies electronic connectors that are utilized in a variety of industries worldwide including military, aerospace, telecommunications, medical, industrial and test equipment among others.\n\nAlthough the company is headquartered in Springfield, Missouri, and its main manufacturing plant is also located there, Positronic also has plants in Puerto Rico, France, China, Singapore, and India. Sales offices are located worldwide in major metropolitan areas.\n\nPositronic was founded by Jack Gentry, a former marines officer and a metallurgical engineer. Upon being discharged from the military, Gentry worked for Honeywell as a sales engineer.\n\nOn a flight from Los Angeles to New York City, Gentry met business manager and philanthropist Harry Gray. Four months later, Gray called Gentry and offered him a job with Litton Industries. During his stay at Litton Industries, Gentry established a plant of the company in Springfield, Missouri. However, feeling that he needed to explore other opportunities, he decided to create his own small company. By October 1966, he had founded Positronic Industries in Springfield, Missouri and initially, the company manufactured electronic components and connectors for the aerospace industry.\n\nEight years later, Positronic moved to Rogersville, Missouri, and in the following years, it progressed and managed three expansions. Tragically, the company’s growth was shaken in February 1983, when a fire destroyed its new headquarters and manufacturing plant. Fearing that the company's competitors would take advantage of the situation, Gentry and his employees worked tirelessly to rebuild Positronic. They moved the company back to Springfield, Missouri, where Positronic’s headquarters is still located today. And as the company gradually recovered from its losses, Positronic decided to venture overseas. Just months after the fire, the company expanded its operations into Europe. This expansion continued into Puerto Rico in 1991, Singapore in 1995, and India in 2004.\n\nBeginning in 1993, Positronic began offering on-the-job training as a part of its apprenticeship program. The program is geared toward high school students seeking jobs in high-tech manufacturing, and it is offered to 17 and 18-year-olds as a school-to-work career opportunity. After graduation, successful apprentices are offered both employment with Positronic and funding for their college education for two years. Positronic also used the Ozarks Technical Community College training program for its own employee co-operative program.\n\nIn March 1995, employees of the main plant in Springfield were sent home after a chemical accident released a chlorine odor throughout the facility. A treatment plant operator caused the accident by improperly mixing chemicals in a waste treatment holding tank, creating pockets of chlorine gas. No one was harmed in the incident.\n\nDespite the Asian financial crisis in the late 1990s, Positronic opened its 43,000 ft assembly plant in Mount Vernon, Missouri in 1999. Although the company had established a presence in Mount Vernon years before on July 5, 1988, the opening of the new facility was a part of Positronic’s expansion into the Mount Vernon area. The facility houses the company's plastic molding department.\n\nIn 2002, less than a year after the September 11 attacks, then-US Senatorial candidate Jim Talent of Missouri visited Springfield. Talent put forward the message that the US government had to increase its military spending, especially in regard to building a strong missile defense system. On his tour of Positronic's facility on North Eldon Avenue, Talent stated that increasing the government’s defense spending would produce more jobs for the American people. Positronic, a creator of domestic manufacturing jobs, produces electronic products that are being used in satellites, missiles, and other military equipment.\n\nIn 2011, Positronic moved eighty-one jobs from the two Springfield manufacturing plants to its Mount Vernon manufacturing plant. The shift was made in order to double the workforce of the Mount Vernon plant and to consolidate the company's two electrical connector assembly lines. With the increase in workforce, the company became one of the largest private employers in Mount Vernon.\n\nIn the same year, the company provided two $15,000 gift funds to Ozarks Technical Community College. The funds supported the expansion of the Information Commons West on the Springfield campus and financed the Middle College program. A new classroom in the Information Commons West was named in honor of Positronic.\n\nIn March 2013, Positronic launched its new online commerce website, PosiShop. PosiShop stocks and sells common Positronic connectors online.\n\nPositronic offers the following products:\n\nOn August 2011, Positronic developed a new generation of backshell for D-subminiature connectors, which combines features that meet the needs the requirements for EMI/RFI protection for cable connectors.\n\nOn January 2013, the company has expanded the Scorpion series of power or signal connectors. The connectors come in an 8.20mm low-profile version and features a one-piece insulator and a modular tool design. Earlier in 2011, the product series was announced as being used for Advanced Telecommunications Computing Architecture's PICMG 3.8 standard.\n\nIn 1993, Positronic received the ISO 9000 standard, becoming the first company in the Springfield area to receive the recognition. The company also received an ISO 9001 in 2007, and an AS9100 certification in 2009 for its North American operations.\n\nPositronic was a recipient of the Gold Industrial Wastewater Pretreatment Compliance Award in 2000 and 2001. The award was given by The Missouri Water Environment Association Positronic was also a recipient of the 2008 Manufacturer of the Year Award from the Springfield Area Chamber of Commerce.\n\nIn February 10, 2004, Jack Gentry and Positronic were recognized in an honorary speech given by Roy Blunt of the United States House of Representatives. Blunt praised Gentry for his invaluable commitment to improving the US manufacturing sector and expanding the US manufacturing market abroad.\n\nOn January 11, 2011, the Positronic’s Mount Vernon plant became a member of the Missouri Safety and Health Achievement Recognition Program. This designation is earned by companies who have achieved an excellent workplace safety record, virtually eliminating the occurrence of employee injuries while on the job.\n\n"}
{"id": "52125016", "url": "https://en.wikipedia.org/wiki?curid=52125016", "title": "Processing aid", "text": "Processing aid\n\nA processing aid is a substance used in the production of processed food, and which may end up in the finished product, but which is not, by law, required to be disclosed to the consumer as an ingredient.\n\nNGOs, journalists, and food writers have raised concerns that the current laws on processing aids amount to a loophole that enables food producers to avoid transparency, and thereby to deceive consumers as to the contents of their food.\n\nUnder the United Kingdom food labelling regulations, a \"processing aid\" is defined as follows:\n\nUnder the law of the United States of America, a substance is legally a \"processing aid\" and can be excluded from ingredients labels if it meets any of the following criteria:\n"}
{"id": "17969440", "url": "https://en.wikipedia.org/wiki?curid=17969440", "title": "Quadrafile", "text": "Quadrafile\n\nQuadrafile was an LP recording released in 1976 intended as a demonstration of four different systems of quadraphonic sound reproduction on phonograph records.\n\nThe record was a double album, with four sides containing identical material presented in each of the four LP based quadraphonic sound formats: SQ, QS, CD-4 and UD-4. These had evolved as the result of four competing companies (CBS, Sansui, JVC and Nippon Columbia respectively) pursuing their own quadraphonic systems independently. This, and the incompatibility of the systems were factors in the slow uptake and eventual downfall of quadraphonic recordings.\n\nThe project was put together by Mike Thorne, then editor of \"Studio Sound\" magazine. Able to persuade JVC and CBS via his own contacts, he soon found agreement from Sansui and Nippon Columbia and all four companies agreed to encode a side of the album each. Assembling material was more difficult since compilations spanning record labels were not as commonplace as they eventually became.\n\nThe eventual album included a set of single and paired reference sounds, a surround-sound demonstration named \"Electronic Footsy\" (created in collaboration with engineer Tony Faulkner), a Stéphane Grappelli violin duet and two classical pieces by Mahler and Bartok, the latter being conducted by Pierre Boulez.\n\nThe album also included Pink Floyd's \"Money\" and 2 excerpts from Mike Oldfield's \"Tubular Bells\". To ensure consistency in the mastering process, Thorne took possession of the actual master tapes of the Pink Floyd album \"Dark Side of the Moon\", which caused him considerable paranoia to the extent that he hid it inside his piano, reasoning that a burglar was unlikely to steal such a hefty object.\n\nThe album was released just as quadraphonic sound was on the wane. Only 5,000 numbered copies were pressed and are extremely rare collector's items. Some collectors have sought out this release as the only example of Mike Oldfield and Pink Floyd recordings being presented in the a particular quad format.\n\nIn 1976, \"Music Educators Journal\" described \"Quadrafile\" in an article on useful materials for music professionals.\n"}
{"id": "2850373", "url": "https://en.wikipedia.org/wiki?curid=2850373", "title": "Quantum flux parametron", "text": "Quantum flux parametron\n\nInvented by Eiichi Goto at the University of Tokyo, the Quantum Flux Parametron (QFP) is an improvement over his earlier parametron based digital logic technology. Unlike its predecessor, QFP uses superconducting Josephson junctions on integrated circuits to improve speed and energy efficiency enormously. In some applications, the complexity of the cryogenic cooling system required is negligible compared to the potential speed gains. While his design makes use of quantum principles, it is not a quantum computer technology, gaining speed only through higher clock speeds. Apart from the speed advantage over traditional CMOS integrated circuit design is that parametrons can be operated with zero energy loss (no local increase in entropy), making reversible computing possible. Low energy use and heat generation is critical in supercomputer design, where thermal load per unit volume has become one of the main limiting factors.\n\nA related technology is the Rapid Single Flux Quantum digital logic.\n\n"}
{"id": "32264875", "url": "https://en.wikipedia.org/wiki?curid=32264875", "title": "Raspberry Pi Foundation", "text": "Raspberry Pi Foundation\n\nThe Raspberry Pi Foundation is a charity founded in 2009 to promote the study of basic computer science in schools, and is responsible for developing a single-board computer called the Raspberry Pi, the UK's best-selling PC of all time.\n\nThe Raspberry Pi Foundation is a charitable organization registered with the Charity Commission for England and Wales. The board of trustees was assembled by 2008 and the \"Raspberry Pi Foundation\" was founded as a registered charity in May 2009 in Caldecote, Cambridgeshire, UK. In 2016, The Foundation moved its headquarters to Station Road, Cambridge, Cambridge. The Foundation is supported by the University of Cambridge Computer Laboratory and Broadcom.\nIts aim is to \"promote the study of computer science and related topics, especially at school level, and to put the fun back into learning computing.\" Project co-founder Eben Upton is a former academic, currently employed by Broadcom as a system-on-chip architect and associate technical director. Components, albeit in small numbers, were able to be sourced from suppliers, due to the charitable status of the organization.\n\nWhen the decline in numbers and skills of students applying for Computer Science became a concern for a team that included Eben Upton, Rob Mullins, Jack Lang and Alan Mycroft at the University of Cambridge’s Computer Laboratory in 2006, a need for a tiny and affordable computer came to their minds. Several versions of the early Raspberry Pi prototypes were designed but were very limited by the high cost and low power processors for mobile devices at that time.\n\nIn 2008, the team started a collaboration with Pete Lomas, MD of Norcott Technologies and David Braben, the co-author of the seminal BBC micro game Elite, and formed the Raspberry Pi Foundation. Three years later, the Raspberry Pi Model B was born and it had sold over two million units within two years of mass production.\n\nThe original founders of the organization includes\n\nIn early 2013 the organization split into two parts: Raspberry Pi Foundation which is responsible for the charitable and educational activities; and Raspberry Pi (Trading) Ltd responsible for the engineering and trading activities. Raspberry Pi (Trading) Ltd is a wholly owned subsidiary of Raspberry Pi Foundation, with the money earned from sales of Raspberry Pi products being used to fund the charitable work of the Foundation. Eben Upton was initially CEO of both divisions, but in September 2013 Lance Howarth became CEO of the Raspberry Pi Foundation, with Eben Upton remaining as CEO of Raspberry Pi (Trading) Ltd. Philip Colligan took over from Lance Howarth as CEO of Raspberry Pi Foundation in July 2015.\n\nAs of 31 December 2015, the foundation has 7 Trustees:\n\nThe Board of Trustees is elected by and supported by the Members of the Foundation, with Members serving in a voluntary role and coming from a range of backgrounds.\n\nPrince Andrew, Duke of York serves as Patron of the Raspberry Pi Foundation.\n\nThe Foundation expected that children would program using Scratch and that the input/output functionality would be used to control external devices. Additionally, the low power requirement facilitates battery-powered usage in robots, while the video capabilities have led to interest in use as a home media centre.\n\nIn April 2014, the foundation announced a £1 million education fund to support projects that enhance the understanding of computing and to promote the use of technology in other subjects, particularly STEM and creative arts for children.\nThey offer to provide up to 50% of the total projected costs to successful applicants. Carrie Anne Philbin is the Director of Education.\n\nIn October 2011, the logo was selected from a number submitted from open competition. A shortlist of six was drawn up, with the final judging taking several days. The chosen design was created by Paul Beech and based on a buckyball.\n\nThe Raspberry Pi Foundation publishes three magazines. \"The MagPi\" is the official magazine of the Raspberry Pi. \"Hello World\" is a \"computing and digital making\" magazine and was first published in January 2017. Wireframe, launched in November 2018, is a magazine about videogames and videogame development.\n\nIn 2011, the Raspberry Pi Foundation developed a single-board computer named the Raspberry Pi. The Foundation's goal was to offer two versions, priced at US$25 and $35 (plus local taxes). The Foundation started accepting orders for the higher priced model on 29 February 2012. The Raspberry Pi is intended to stimulate the teaching of computer science in schools.\n\nIn 2015 the foundation unveiled the Raspberry Pi Zero. This version of the microcomputer had a significantly reduced form factor and a lower price, launching at £4/$5. The new model features a 1Ghz, Single-core CPU; 512MB RAM, Mini HDMI and USB ports, Micro USB power, HAT-compatible 40-pin header as well as Composite video and reset headers . As a fully functioning Linux system the Raspberry Pi Zero's 1 GHz processor is comparable to the middle of the road for the Intel Pentium 3 architecture (450 MHz to 1.4 GHz), a standard in 2000. The reduced price and smaller form factor encourages use in smaller and embedded projects.\n"}
{"id": "46707936", "url": "https://en.wikipedia.org/wiki?curid=46707936", "title": "S11 House", "text": "S11 House\n\nS11 House, located in Petaling Jaya, is Malaysia’s first Green Building Index Platinum rated house that won the Tropical Building Category of the Asean Energy Awards in 2013, designed by Ar. Dr. Tan Loke Mun. This single residential structure was conceptualized along the lines of a large tree canopy and is equipped with 5 KW peak photo-voltaic installation on the roof; rainwater is also collected on the canopy roof drains and is stored in harvesting tanks. Water features and multiple pools are located at the two extreme north-south ends to provide evaporative cooling for the house. The house proves that even in the hot and humid tropical climate of South East Asia, it is possible to lessen a building's reliance on air-conditioning and also minimize its use of other natural resources, most notably water and electricity.\n\nA composting yard treats all the household organic and garden wastes and provides high grade compost fertilizer for the vegetable and fruit gardens. It has an overall roof U value of 0.14. The glazing roof comprises 9.38mm thick low-E safety laminated glass which can be opened up to 90%. The overall building envelope OTTV is 29.63.\n\nS11 House is a testament to the opportunities available to build green for the architect and owner of this house, Dr. Tan. The successful structure has since been replicated in the S14 House and other commissions of private homes. The hope is that the constant experiments done on these private homes eventually will find their way into the larger consumer and mass market housing industry. This is Tan’s process of taking a theory and testing it on a lab project (usually a house) and then finding a larger mass market application for the idea. The house is physical proof that upper-end residential houses can work sustainably with the environment regarding their construction and on-going maintenance.\n\n"}
{"id": "47218207", "url": "https://en.wikipedia.org/wiki?curid=47218207", "title": "Smartisan", "text": "Smartisan\n\nSmartisan Technology Co., Ltd. (), commonly known as Smartisan, is a Chinese multinational technology company headquartered in Beijing and Chengdu. It designs and markets consumer electronic devices and online services. Its hardware product line includes the Smartisan smartphone and the Smartisan earphone. Its consumer software include the Smartisan OS operating system. Its online services include the Smartisan Store, the Smartisan OS App Store and Smiling Cloud.\n\nSmartisan was founded by Luo Yonghao on , to develop and sell smartphones.\n\nOn , Smartisan announced Smartisan OS, an smartphone platform based on the Android operating system. On , Smartisan announced their first smartphone, the Smartisan T1. On 29 December 2015, Smartisan announced their second smartphone, the Smartisan T2.\n\nComing out with latest design and modest specifications, company launched Jianguo ahead of Smartisan T2 debut. Jianguo introduces Smartisan OS 2.0 version.\n\nDuring 2016 and 2017, Smartisan was the lead financial contributor to the OpenBSD project.\nОга\n\nAccording to the company website, the English name \"Smartisan\" is a portmanteau of \"smart\" and \"artisan\", signifying \"artisanship in the smartphone era.\"\n\n"}
{"id": "36976336", "url": "https://en.wikipedia.org/wiki?curid=36976336", "title": "Snow knife", "text": "Snow knife\n\nA snow knife or snow saw (Inuktitut: \"pana\") is a tool used in the construction of \"igluit\" (snow houses) by the Inuit people of the Arctic or as a weapon. The snow knife was originally made from available materials such as bone or horn but the Inuit adapted to using metal after the arrival of Europeans.\n\nThe American Association for the Advancement of Science noted in 1883:\n"}
{"id": "40533764", "url": "https://en.wikipedia.org/wiki?curid=40533764", "title": "Space elevator competitions", "text": "Space elevator competitions\n\nA space elevator is a theoretical system using a super-strong ribbon going from the surface of the Earth to a point beyond Geosynchronous orbit. The center of gravity of the ribbon would be exactly in geosynchronous orbit, so that the ribbon would always stay above the anchor point. Vehicles would climb the ribbon powered by a beam of energy projected from the surface of the Earth. Building a space elevator requires materials and techniques that do not currently exist. A variety of Space Elevator competitions have been held in order to stimulate the development of such materials and techniques.\n\nSpace elevators were first conceived in 1895, but until the discovery of carbon nanotubes, no technology was envisioned that could make them possible. Building an actual elevator is still out of reach, but the directions for research are clear. This makes the area ripe for incentive prizes like the X Prize, and prizes and competitions have been set up since 2005 to encourage the development of relevant technologies. There are two main areas of research remaining, and these are where the competitions focus: building cables (\"a Tether challenge\"), and climbing and descending cables (\"a Power Beam challenge\").\n\nIn a Power Beam Challenge, each team designs and builds a climber (a machine capable of traveling up and down a tether ribbon). In a Tether challenge, each team attempts to build the longest and strongest cable. In the Power Beam challenge climber carry a payload. Power is beamed from a transmitter to a receiver on the climber. With each competition, the tethers reach higher altitudes, and the climbers are expected to climb further. Each competition can have minimum lengths and maximum weight per meter for cables, and minimum speed and distance goals for climbers.\n\nLike many competitions modeled after the X prize, competitors have to meet a minimum baseline, and then prizes are awarded to the best entry that exceed that target. In 2005, there was only a climbing challenge, and none of the entrants met the minimum speed requirement of 1 m/s. Starting in 2006, , sponsored by spaceward.org and NASA conducted a series of competitions. For 2006, the prize was increased, and the speed requirement dropped slightly to 50 meters in under a minute. 13 teams entered, and one was able to climb the 50 meters in 58 seconds. In 2009 at Edwards Air Force Base, the challenge was climbing a 900 m tether, and one entry managed the feat several times, with a top speed of 3.5 m/s. NASA didn't renew their sponsorship after 2009, pending \"further advancements in material science\".\n\nThe International Space Elevator Consortium was formed in 2008, and has held annual conferences. They announced a $10,000 Strong Tether Challenge competition for 2013. The Challenge was canceled for lack of competitors. The 2011, 2012, and 2013 ISEC conferences also featured FIRST-style High School robotics competitions for climbers. and occasional competitions.\n\nThe Japan Space Elevator Association held a climbing competition in August 2013. Hot air balloons were used to hoist a tether, and Team Okusawa's entry succeeded in climbing to 1100 meters, and a team from Nihon University reached 1200 meters. (The sources are in Japanese.)\n\nThe Japan Space Elevator Association held a climbing competition in August 2014. Hot air balloons were used to hoist both rope (11 mm) and ribbon (35 mm x 2 mm) to 200 m and 1200 m. Team Okusawa climbed to 1200 m and descended twice. Kanagawa University carried a 100 kg payload to 123 m on the 200 m ribbon. Kanagawa University's three teams climbed respectively to 1200 m (rope), 1150 m (rope) and 1100 m (ribbon). Munich University of Technology reached 1000 m (rope).\n\n"}
{"id": "17887714", "url": "https://en.wikipedia.org/wiki?curid=17887714", "title": "Supersonic gas separation", "text": "Supersonic gas separation\n\nSupersonic gas separation is a technology to remove one or several gaseous components out of a mixed gas (typically raw natural gas). The process condensates the target components by cooling the gas through expansion in a Laval nozzle and then separates the condensates from the dried gas through an integrated cyclonic gas/liquid separator. The separator is only using a part of the field pressure as energy and has technical and commercial advantages when compared to commonly used conventional technologies.\n\nRaw natural gas out of a well is usually not a salable product but a mix of various hydro-carbonic gases with other gases, liquids and solid contaminants. This raw gas needs gas conditioning to get it ready for pipeline transport and processing in a gas processing plant to separate it into its components.Some of the common processing steps are CO removal, dehydration, LPG extraction, dew-pointing. Technologies used to achieve these steps are adsorption, absorption, membranes and low temperature systems achieved by refrigeration or expansion through a Joule Thomson Valve or a Turboexpander.\nIf such expansion is done through the Supersonic Gas Separator instead, frequently mechanical, economical and operational advantages can be gained as detailed below.\n\nA supersonic gas separator consists of several consecutive sections in tubular form, usually designed as flanged pieces of pipe.\n\nThe feed gas (consisting of at least two components) first enters a section with an arrangement of static blades or wings, which induce a fast swirl in the gas.\nThereafter the gas stream flows through a Laval nozzle, where it accelerates to supersonic speeds and undergoes a deep pressure drop to about 30% of feed pressure. This is a near isentropic process and the corresponding temperature reduction leads to condensation of target components of the mixed feed gas, which form a fine mist. The droplets agglomerate to larger drops, and the swirl of the gas causes cyclonic separation.\nThe dry gas continues forward, while the liquid phase together with some slip gas (about 30% of the total stream) is separated by a concentric divider and exits the device as a separate stream. The final section are diffusers for both streams, where the gas is slowed down and about 80% of the feed pressure (depending on application) is recovered. This section might also include another set of static devices to undo the swirling motion.\n\nThe supersonic separator requires a certain process scheme, which includes further auxiliary equipment and often forms a skid or processing block.\nThe typical basic scheme for supersonic separation is an arrangement where the feed gas is pre-cooled in a heat exchanger by the dry stream of the separator unit.\n\nThe liquid phase from the supersonic separator goes into a 2-phase or 3-phase separator, where the slip gas is separated from water and/or from liquid hydrocarbons. The gaseous phase of this secondary separator joins the dry gas of the supersonic separator, the liquids go for transport, storage or further processing and the water for treatment and disposal.\n\nDepending on the task at hand other schemes are possible and for certain cases have advantages. Those variations are very much part of the supersonic gas separation process to achieve thermodynamic efficiency and several of them are protected by patents.\n\nThe supersonic gas separator recovers part of the pressure drop needed for cooling and as such has a higher efficiency than a JT valve in all conditions of operation.\n\nThe supersonic gas separator can in many cases have a 10–20% higher efficiency than a turboexpander.\n\nThe supersonic separator has a smaller footprint and a lower weight than a turboexpander or contactor columns. This is of particular advantage for platforms, FPSOs and crowded installations.\nIt needs a lower capital investment and lower operating expenditure as it is completely static. Very little maintenance is required and no (or greatly reduced) amounts of chemicals.\n\nThe fact that no operational or maintenance personnel is required might enable unmanning of usually manned platforms with the associated large savings in capital and operational expenditure.\n\nThe fields of application commercially developed until today on an industrial scale are:\n\nApplications in the development stage for near term commercialization are:\n\nThere are several patents on supersonic gas separation, relating to features of the device as well as methods.\nThe technology has been researched and proven in laboratory installations since about 1998, special HYSYS modules have been developed as well as 3D gas computer modeling. The supersonic gas separation technology has meanwhile moved successfully into industrial applications (e.g. in Nigeria, Malaysia and Russia) for dehydration as well as for LPG extraction.\nConsultancy, engineering and equipment for supersonic gas separation are being offered by ENGO Engineering Ltd. under the brand \"3S\". They are also provided by Twister BV, a Dutch firm affiliated with Royal Dutch Shell, under the brand \"Twister Supersonic Separator\".\n"}
{"id": "57128025", "url": "https://en.wikipedia.org/wiki?curid=57128025", "title": "Typewriter mystery game", "text": "Typewriter mystery game\n\nA typewriter mystery game was a specific type of typewriter art popular in the mid-20th century. \n\nA typewriter owner would be presented with a set of instructions: press a key this many times, press another key, move on to the next line. Upon finishing the typing, a picture would emerge on the page. First lines of a simple typewriter mystery could look like this:\n\nTypewriter mystery games were published in magazines (such as Woman's Realm and The Journal of Business Education), and collected in separate books. The “mystery” in the name refers to the fact that a visual result of the instructions would sometimes be presented on a different page, in the following issue of the magazine, or withheld altogether, making typing the only immediate way to discover the picture.\n\nThe end result of a typewriter mystery game would be a picture similar to the later ASCII art, except it would often use overtyping – making several passes over the same line, unavailable or difficult on computer screens. The photo would often be a portrait of a person or an animal.\n\n"}
{"id": "21571745", "url": "https://en.wikipedia.org/wiki?curid=21571745", "title": "Ultraperformance Nanophotonic Intrachip Communications", "text": "Ultraperformance Nanophotonic Intrachip Communications\n\nThe Ultraperformance Nanophotonic Intrachip Communications (UNIC) program is a project of DARPA. It is dedicated to funding projects by chip makers to create nanophotonic microprocessors.\n\nOn March 3, 2008, Sun Microsystems announced that it had received a US$44.29M grant from UNIC for a \"five and a half-year research project focused on microchip interconnectivity via on-chip optical networks enabled by Silicon photonics and proximity communication\"; this project also involves collaboration with Luxtera, Kotura, Stanford University and the University of California, San Diego.\n\n"}
{"id": "2581277", "url": "https://en.wikipedia.org/wiki?curid=2581277", "title": "Wall plate", "text": "Wall plate\n\nA plate or wall plate is a horizontal, structural, load-bearing member in wooden building framing.\n\nA plate in timber framing is \"A piece of Timber upon which some considerable weight is framed...Hence Ground-Plate...Window-plate [obsolete]...\" etc. Also called a wall plate, raising plate, or top plate, An exception to the use of the term plate for a large, load-bearing timber in a wall is the bressummer, a timber supporting a wall over a wall opening (see also: lintel). These are common in Australia.\n\nThe terms sole plate or sill plate are used for the members at the bottom of a wall at the foundation but are most often just called a \"sole\" or \"sill\" without the word \"plate\". Other load bearing timbers use the term plate but are not in the wall such as \"crown plate\", a purlin-like beam carried by crown posts in roof framing, and a \"purlin plate\" which supports common rafters.\n\nIn platform framing there are three types of wall plates and are located at the top and bottom of a wall section, and the two hold the wall studs parallel and spaced at the correct interval. Each type continues in a piecewise fashion around the whole perimeter of the structure.\n\n\n"}
