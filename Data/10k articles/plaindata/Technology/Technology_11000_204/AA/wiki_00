{"id": "34007442", "url": "https://en.wikipedia.org/wiki?curid=34007442", "title": "ARM Cortex-R", "text": "ARM Cortex-R\n\nThe ARM Cortex-R is a family of 32-bit RISC ARM processor cores licensed by Arm Holdings. The cores are optimized for hard real-time and safety-critical applications. Cores in this family implement the ARM Real-time (R) profile, which is one of three architecture profiles, the other two being the Application (A) profile implemented by the Cortex-A family and the Microcontroller (M) profile implemented by the Cortex-M family. The ARM Cortex-R family of microprocessors currently consists of ARM Cortex-R4(F), ARM Cortex-R5(F), ARM Cortex-R7(F), ARM Cortex-R8(F), and ARM Cortex-R52(F).\n\nThe ARM Cortex-R is a family of ARM cores implementing the R profile of the ARM architecture; that profile is designed for high performance hard real-time and safety critical applications. It is similar to the A profile for applications processing but adds features which make it more fault tolerant and suitable for use in hard real-time and safety critical applications.\n\nReal time and safety critical features added include:\n\n\nARM Holdings neither manufactures nor sells CPU devices based on its own designs, but rather licenses the core designs to interested parties. ARM offers a variety of licensing terms, varying in cost and deliverables. To all licensees, ARM provides an integratable hardware description of the ARM core, as well as complete software development toolset and the right to sell manufactured silicon containing the ARM CPU.\n\nIntegrated device manufacturers (IDM) receive the ARM Processor IP as synthesizable RTL (written in Verilog). In this form, they have the ability to perform architectural level optimizations and extensions. This allows the manufacturer to achieve custom design goals, such as higher clock speed, very low power consumption, instruction set extensions, optimizations for size, debug support, etc. To determine which components have been included in a particular ARM CPU chip, consult the manufacturer datasheet and related documentation.\n\nThe Cortex-R is suitable for use in computer-controlled systems where very low latency and/or a high level of safety is required. An example of a hard real-time, safety critical application would be a modern electronic braking system in an automobile. The system not only needs to be fast and responsive to a plethora of sensor data input, but is also responsible for human safety. A failure of such a system could lead to severe injury or loss of life.\n\nOther examples of hard real-time and/or safety critical applications include:\n\n\n\n\n\n"}
{"id": "42176541", "url": "https://en.wikipedia.org/wiki?curid=42176541", "title": "Airline service trolley", "text": "Airline service trolley\n\nAn airline service trolley, also known as an airline catering trolley, airline meal trolley, or trolley cart, is a small serving cart supplied by an air carrier for use by flight attendants inside the aircraft for transport of beverages, airline meals, and other items during a flight.\n\nThe airline service trolley system was introduced in the late 1960s at the same time as a new generation of large \"widebody\" aircraft were entering into service with the airlines. The significantly larger number of passengers on these aircraft meant that meals could no longer be efficiently delivered by hand, as they had been up until that time.\n\nThe growth of at-seat service on long distance rail has led to the adoption of similar service trollies for this purpose.\n\nThe trolley is a rigid box form with castering wheels at each corner that can be braked to hold the trolley in position. Full and half size trollies are made. The front (both full and half size) and rear (full size only) have doors, and handles are provided at the top. There are currently several design families of trolley in use:\n\nIndividual carriers often customize an existing trolley family for their uses. Full size trolleys are generally about wide, tall, long, and weigh about unladen. Lighter weight designs are also available.\n\n"}
{"id": "13131925", "url": "https://en.wikipedia.org/wiki?curid=13131925", "title": "Amobee", "text": "Amobee\n\nAmobee is a marketing technology company whose Brand Intelligence technology measures digital engagement. Their unified platform enables marketers to plan media campaigns.\n\nAmobee is a wholly owned subsidiary of Singtel. Amobee operates across North America, Europe, Middle East, Asia and Australia.\n\nAmobee was founded in May 2005 by Gil Shulman, Saul Rurka and Zohar Levkovitz. It went live in June 2005. One year later, the company recruited Roger Cameron Wood, a mobile industry veteran, who established the company's U.S. presence in San Francisco.\nIn 2009, Amobee purchased media company RingRingMedia in order to expand their advertising customer base, enabling mobile media owners to sell advertising space to companies on a collective basis. At the time of acquisition, RingRingMedia bought more than $2 million (£1.2 million) of mobile media a month, serving more than 4 billion ad impressions a month.\n\nIn July 2011, Trevor Healy, formerly of Jajah was appointed Chief executive officer. Zohar Levkovitz will stay with Amobee as Vice Chairman.\n\nAmobee was acquired by Singtel in March 2012 for $321 million.\n\nIn June 2014, Amobee acquired two of its competitors: Adconion Direct for $235M and Kontera for $150M, in an attempt to consolidate its market position.\n\nIn December 2016, Kim Perell, President of Amobee and former CEO of Adconion Direct was appointed Chief Executive Officer.\n\nOn February 22, 2017, Amobee announced an agreement to acquire Turn, a global technology platform for marketers and agencies.\n\nIn March 2018, Amobee partnered with TruSignal, Inc to launch custom bid algorithms for marketers.\n\nAmobee's original backing was from financial backing from Sequoia Capital, Accel Partners and Globespan Capital, as well as strategic investments from Motorola, Cisco, Vodafone and Telefónica. Since 2012, Amobee has been a wholly owned subsidiary of Singtel.\n\n"}
{"id": "2742801", "url": "https://en.wikipedia.org/wiki?curid=2742801", "title": "Anthony DeMaria Labs", "text": "Anthony DeMaria Labs\n\nAnthony DeMaria Labs, also referred to as ADL, is a small manufacturer of audio equipment specializing in vacuum tube based pieces, although not exclusively. Their products are used by a wide variety of artists in both live performance and studio applications, and are used by such artists as Jimmy Buffett and Bootsy Collins.\n\n"}
{"id": "40018619", "url": "https://en.wikipedia.org/wiki?curid=40018619", "title": "Arteris", "text": "Arteris\n\nArteris, Inc. is a multinational technology firm that develops the on-chip interconnect fabric technology used in System-on-Chip (SoC) semiconductor designs for a variety of devices, particularly in mobile and consumer markets. The company specializes in the development and distribution of Network-on-Chip (NoC) interconnect Intellectual Property (IP) solutions. It is best known for its flagship product, Arteris FlexNoC, which is used in more than 60 percent of mobile and wireless SoC designs.\n\nArteris, Inc. is headquartered in Campbell, California. K. Charles Janac is the company’s President and CEO.\n\nIn 2012, the Silicon Valley \"San Jose Business Journal\" ranked Arteris as the 4th fastest-growing private company in Silicon Valley. Arteris has also been in the Inc. 500 list of America’s fastest growing companies for two years running.\n\nArteris was founded in 2003 by Philippe Boucard and two other engineering executives who had worked together at T.Sqware, a startup that was acquired by Globespan. Company executives wished to address problems with existing monolithic bus and crossbar interconnect technologies, such as wire and routing congestion, increased heat and power consumption, failed timing closure, and increased die area. The firm’s leadership sought and received venture capital totaling $44.1 million for the creation of its new technology from investors, including ARM Holdings, Crescendo Ventures, DoCoMo Capital, Qualcomm, Synopsys, TVM Capital, and Ventech.\n\nBy 2006, Arteris developed the first commercially available NoC IP product, called NoC Solution, followed in 2009 by a more advanced product, FlexNoC. The products used “packetization and a distributed network of small interconnect elements to address congestion, timing, power and performance issues.” Arteris marketed FlexNoC as an improvement on traditional SoCs interconnect fabrics, citing its reduction in gate count by 30 percent, reduction of wires by 50 percent, and a more compact chip floor as compared to a functionally equivalent hybrid bus or crossbar.\n\nDesigners of SoCs began to take advantage of the technology’s increased design efficiency, flexibility, and a significant reduction in production costs. By 2012, the company had over 40 semiconductor customers, including Qualcomm, Samsung, Texas Instruments, Toshiba, and LG Electronics, with 200 million SoCs being produced with Arteris IP. The company’s volume is projected to grow to over 1 billion units per year by 2015.\n\nIn October 2013, Qualcomm Technologies, Inc. acquired the FlexNoC network-on-chip product portfolio, but Arteris retained existing customer contracts and to continue licensing FlexNoC and modifying the source code for customer support. Qualcomm will provide engineering deliverables for the FlexNoC product line and updates to Arteris. Qualcomm does not maintain any ownership interest in Arteris.\nIn February 2014, Arteris named Craig Forrest as Chief Technology Officer, Dave Parry as Vice President of Engineering, and Benoit de Lescure as Director of Application Engineering.\n\nArteris claims to have had 61 licensees for its interconnect IP product since its inception in 2003. There are approximately 40 active customers who are publicly disclosed.\n\nThese licensees include top-20 semiconductor makers Samsung Electronics, Qualcomm, Toshiba Semiconductor, Texas Instruments, STMicroelectronics, Renesas Electronics, Freescale Semiconductor and Marvell Technology Group.\nArteris has also signed licenses with Chinese semiconductor companies including Actions Semiconductor, Allwinner, HiSilicon (Huawei), InfoTM, Ingenic, Leadcore, Nufront, RDA Semiconductor, Rockchip, Socle and Spreadtrum.\nOther publicly announced licensees of Arteris products include Altera, Core Logic, CSR, GCT, GUC, iC-Logic, LG, MegaChips, Mobileye, MtekVision, NTT Electronics, Open-Silicon, Pixelworks, Renesas Electronics, Sckipio, Schneider, ST-Ericsson / Ericsson, and VIA Telecom.\n\nArteris offers a number of Network-on-Chip products, including FlexNoC for high performance SoCs, the FlexLLI MIPI interchip link IP for connecting multiple chips and dies, and the FlexWay interconnect fabric for smaller SoCs. The firm’s technology is used in a variety of consumer electronics, including mobile phones and tablets, modems, gaming consoles, digital televisions, automotive systems, and other applications.\n\n"}
{"id": "24925850", "url": "https://en.wikipedia.org/wiki?curid=24925850", "title": "BB cream", "text": "BB cream\n\nBB cream stands for blemish balm, blemish base, beblesh balm, and in Western markets, beauty balm. \n\nCompared to a tinted cream, which would be just a cream with a very light tint, BB Cream is a foundation, moisturizer, and sunblock skincare product. \n\nThe CC cream came later on and describes a Color Correction cream. A CC cream has all the benefits of a BB cream but with the specific feature of homogenizing the coloration of the skin, by correcting the complexion concerns such as redness or dullness.\n\nWhat became BB cream was originally formulated in the 1960s in Germany by dermatologist Dr. Christine Schrammek to protect her patients' skin after facial peels and surgery.\n\nBB creams come in a variety of different formulations. Because Korean companies focused initially on the Korean and East Asian markets, they are offered in a limited number of hues. Instead of offering multiple shades for different skin colors, most formulae are designed to oxidize to match the user's skin tone. The skin-whitening properties of the cream as sold in the Asian market are an important element in its popularity.\n\nThe cream is promoted as a multi-tasker and all-in-one treatment, but Korean women mostly use it as an alternative to foundation, particularly those with Western formulations that tend to be too heavy for their tastes. The coverage is often mineral-based, and is intended to both cover and treat blemishes such as acne, sun spots, and age spots. It also has anti-wrinkle, anti-inflammatory, and soothing effects. Several contain hyaluronic acid and Vitamin C. \n\nBB creams make up 13 percent of the cosmetics market in South Korea. Some Korean brands also offer BB creams for men. Notable Korean brands include Etude House, Missha, Nature Republic, Skin Food, Sulhwasoo,The Face Shop, and SKIN79.\n\nWestern cosmetics companies began to launch BB creams in the Western market in 2012, though some of these creams have been criticized for lacking the skin-caring functions that BB creams normally have, and for being no more than tinted moisturizer. Early arrivals included Boscia, Clinique, Dior, Estée Lauder, Garnier, Marcelle, Maybelline, Revlon and Smashbox. Lab Series makes a BB cream for men. Certain BB creams have been tailored for Western markets: Estée Lauder, for example, has not included the whitening properties in their formulation for North America.\n\nBB creams advertised as cruelty-free include Smashbox (owned by Estée Lauder) and The Body Shop (owned by L'Oreal). The definition of \"cruelty-free\" varies. The Body Shop BB cream is certified by the Leaping Bunny Program, which means, according to the certification process, that no new animal testing has been used in any phase of product development by the company, its laboratories, or the suppliers of its ingredients. As of May 2013, Amore Pacific, which has as its subsidiaries Etude House and Laneige, has ended animal testing on all ingredients and cosmetics.\n\nProducts certified as cruelty-free may still contain animal products and may not be suitable for vegans. Vegan BB creams include the Superdrug own brand BB cream, BB cream souffles from Haut Cosmetics, 100% Pure Cosmetics, Multi-Mineral BB Cream from Pacifica, and the Evenly Radiant BB Crème from Dermae.\n\n"}
{"id": "56464264", "url": "https://en.wikipedia.org/wiki?curid=56464264", "title": "Bobble Keyboard (App)", "text": "Bobble Keyboard (App)\n\nBobble Keyboard is an input method app for Android and iOS devices founded in 2015 by Ankit Prasad and Mohd. Wassem. It was developed and produced by Talent Unlimited Online Services Private Limited.\n\nIn its history, Bobble has received funding from SAIF Partners, Deep Kalra (founder of MakeMyTrip), Amit Ranjan (co-founder of SlideShare), and Sachin & Binny Bansal (founders of Flipkart). It raised 3 rounds of funding between 2014 and 2016.\n\nBobble makes use of a self-learning algorithm to create personalized content for its users. Bobble started off as a selfie-based sticker app as more features were added. It has a facial recognition feature, used to convert selfies into GIFs and stickers. Its other features included multilingual language support, glide typing, custom fonts, themes, voice typing and autocorrection. The application works in compatibility with platforms such as Facebook, Snapchat and Whatsapp Messenger. Bobble's features work in real-time with its ability to match facial expressions, head and facial tone with the sticker's emotion, theme and body.\n\nBobble is available in over 150 countries, including India, Ireland, New Zealand, UK, United States, Canada, Australia & Singapore.\n\nIn 2016, Bobble collaborated with Indus OS to became its official distributor in India.\n\nIn 2017, Bobble partnered with Gionee India and was promoted on the Gionee app store. It also became the default keyboard for Zen Mobile handsets.\n\nIt has collaborated with brands such as Tinder, Zomato, Apple Inc., Foxconn, Eros Now and Baidu.\n\nDuring its initial release in India, Bobble has been downloaded more than 6 million times in a span of 15 months. In July 2017, Bobble topped the indic keyboards category on the Google Play Store, being ranked first in India and second globally.\n\nSince 2015, it has been downloaded over 10 million times and it crossed a tally of 1.5 million active users by December of that year. As of 2017, the app is listed on Play Store's \"top 150 most engaging apps globally\".\n"}
{"id": "33090164", "url": "https://en.wikipedia.org/wiki?curid=33090164", "title": "British Journal of Educational Technology", "text": "British Journal of Educational Technology\n\nThe British Journal of Educational Technology is a peer-reviewed academic journal published by Wiley on behalf of the British Educational Research Association. The journal covers developments in educational technology and articles cover the whole range of education and training, concentrating on the theory, applications, and development of educational technology and communications.\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 2.41, ranking it 23rd out of 235 journals in the category \"Education & Educational Research\".\n\n \n"}
{"id": "8692316", "url": "https://en.wikipedia.org/wiki?curid=8692316", "title": "CAS Corporation", "text": "CAS Corporation\n\nCAS Corporation is a Korean manufacturer of electronic weighing equipment, founded on April 19, 1983 by its current Chief Executive Manager, Dong-Jin Kim.\n\nCAS has been continuously developing over the years and by 2006, its equipment was exported to over 120 countries. By 2011, the company had representatives in Germany, USA, Canada, Poland, Vietnam, Bangladesh, India, China, Japan, Russia, Turkey, Great Britain and other countries. Its equipment has been certified and approved for commercial use, among others, in USA, Canada, Russia, European Union (OIML), Australia, Japan.\n\nCAS was the first Korean company to export electronic scales in 1987 and to develop a label printing scale in 1992 (model LP, which was an abbreviation for \"label printing\").\n"}
{"id": "53686202", "url": "https://en.wikipedia.org/wiki?curid=53686202", "title": "Charles Chapman (engineer)", "text": "Charles Chapman (engineer)\n\nCharles Wallace Chapman (4 August 1897 - December 1979) was a British mechanical engineer, who designed the first diesel engine suitable for an automobile, the \"high speed diesel engine\" (compression ignition engine).\n\nHe was born in Lancaster, Lancashire.\n\nHe attended Lancaster Royal Grammar School. He served in the First World War as a lieutenant in the RNVR. He later gained a master's degree in engineering from the University of Liverpool.\n\nIn the 1920s he worked as personal assistant to Sir Ernest Petter, who owned Petters (Ipswich) Ltd. At this company he worked with Frank Perkins.\n\nOn 7 June 1932 he jointly founded Perkins Engines in Peterborough (then in Northamptonshire) with Frank Perkins (engineer), who he first met in 1929. Perkins Engines was created to build high speed diesel engines. Francis Arthur Perkins was the businessman, and Charles Chapman provided technical skill.\n\nDuring the Second World War, he designed the Perkins S6 marine diesel engine, which powered the Royal Navy's air-sea rescue craft. He also designed the T1 engine for boats, which was not made. He resigned from Perkins in November 1942. Frank Perkins died in 1967.\n\nDuring the Second World War he carried out work for the Air Ministry.\n\nHe died in Winchelsea in East Sussex, aged 82 in 1979.\n\n\n\n"}
{"id": "8491882", "url": "https://en.wikipedia.org/wiki?curid=8491882", "title": "Chip select", "text": "Chip select\n\nChip select (CS) or slave select (SS) is the name of a control line in digital electronics used to select one (or a set) of integrated circuits (commonly called \"chips\") out of several connected to the same computer bus, usually utilizing the three-state logic. \n\nOne bus that uses the chip/slave select is the Serial Peripheral Interface Bus (SPI bus).\n\nWhen an engineer needs to connect several devices to the same set of input wires (e.g., a computer bus), but retain the ability to send and receive data or commands to each device independently of the others on the bus, they can use a chip select. The chip select is a command pin on many integrated circuits which connects the I/O pins on the device to the internal circuitry of that device.\n\nWhen the chip select pin is held in the inactive state, the chip or device is \"deaf\", and pays no heed to changes in the state of its other input pins; it holds its outputs in the high impedance state, so other chips can drive those signals. When the chip select pin is held in the active state, the chip or device assumes that any input changes it \"hears\" are meant for it, and responds as if it is the only chip on the bus. Because the other chips have their chip select pins in the inactive state, their outputs are high impedance, allowing the single selected chip to drive its outputs.\n"}
{"id": "12284805", "url": "https://en.wikipedia.org/wiki?curid=12284805", "title": "Coalbrookdale by Night", "text": "Coalbrookdale by Night\n\nCoalbrookdale by Night is an 1801 oil painting by Philip James de Loutherbourg.\n\nThe painting depicts the Madeley Wood (or Bedlam) Furnaces, which belonged to the Coalbrookdale Company from 1776 to 1796. The picture has come to symbolize the birth of the Industrial Revolution in the Ironbridge Gorge, Shropshire, England. It is held in the collections of the Science Museum in London.\n\nLoutherbourg undertook tours of England and Wales during 1786 and 1800, observing industrial activity at the time. \"Coalbrookdale by Night\" provides a view of the Bedlam Furnaces in Madeley Dale, downstream along the River Severn from the town of Ironbridge itself.\n"}
{"id": "35536988", "url": "https://en.wikipedia.org/wiki?curid=35536988", "title": "Conversion marketing", "text": "Conversion marketing\n\nIn electronic commerce, conversion marketing is marketing with the intention of increasing \"conversions--\"that is, site visitors who are paying customers. The process of improving the conversion rate is called conversion rate optimization. However, different sites may consider a \"conversion\" to be a result other than a sale. Say a customer were to abandon an online shopping cart. The company could market a special offer, like free shipping, to convert the visitor into a paying customer. A company may also try to recover the customer through an online engagement method, such as proactive chat, to attempt to assist the customer through the purchase process.\n\nThe efficacy of conversion marketing is measured by the conversion rate: the number of customers who have completed a transaction divided by the total number of website visitors. Conversion rates for electronic storefronts are usually low. Conversion marketing can boost this number as well as online revenue and website traffic.\n\nConversion marketing attempts to solve low online conversions through optimized customer service, which requires a complex combination of personalized customer experience management, web analytics, and the use of customer feedback to contribute to process flow improvement and site design.\n\nBy focusing on improving site flow, online customer service channels, and online experience conversion marketing is commonly viewed as a long-term investment rather than a quick fix . Increased site traffic over the past 10 years has done little to increase overall conversion rates, so conversion marketing focuses not on driving additional traffic but converting existing traffic. It requires proactive engagement with consumers using real time analytics to determine if visitors are confused and show signs of abandoning the site; then developing the tools and messages to inform consumers about available products, and ultimately persuading them to convert online. Ideally, the customer would maintain a relationship post-sale through support or re-engagement campaigns. Conversion marketing affects all phases of the customer life-cycle, and several conversion marketing solutions are utilized to help ease the transition from one phase to the next.\n\nThe conversion rate is the proportion of visitors to a website who take action to go beyond a casual content view or website visit, as a result of subtle or direct requests from marketers, advertisers, and content creators.\n\nSuccessful conversions are defined differently by individual marketers, advertisers, and content creators. To online retailers, for example, a successful conversion may be defined as the sale of a product to a consumer whose interest in the item was initially sparked by clicking a banner advertisement. To content creators, a successful conversion may refer to a membership registration, newsletter subscription, software download, or other activity.\n\nFor websites that seek to generate offline responses, for example telephone calls or foot traffic to a store, measuring conversion rates can be difficult because a phone call or personal visit is not automatically traced to its source, such as the Yellow Pages, website, or referral. Possible solutions include asking each caller or shopper how they heard about the business and using a toll-free number on the website that forwards to the existing line.\n\nFor websites where the response occurs on the site itself, a conversion funnel can be set up in a site's analytics package to track user behavior.\n\n\nAmong many possible actions to increase the conversion rate, the most relevant may be:\n\n\n"}
{"id": "1403024", "url": "https://en.wikipedia.org/wiki?curid=1403024", "title": "Corwin Hansch", "text": "Corwin Hansch\n\nCorwin Herman Hansch (October 6, 1918 – May 8, 2011) was a Professor of Chemistry at Pomona College in California. He became known as the 'father of computer-assisted molecule design.'\n\nHe was born on October 6, 1918 in Kenmare, North Dakota.\n\nHe earned a B.S. from the University of Illinois in 1940 and a Ph.D. from New York University in 1944.\n\nHansch worked on the Manhattan Project at the University of Chicago and as a group leader at DuPont Nemours in Richland, Washington. In February 1946 he received an academic position at Pomona College.\n\nHansch taught Organic Chemistry for many years at Pomona College, and was known for giving complex lectures without using notes. His course in Physical Bio-Organic Medicinal Chemistry was ground-breaking at an undergraduate level.\n\nHansch may be best known as the father of the concept of quantitative structure-activity relationship (QSAR), the quantitative correlation of the physicochemical properties of molecules with their biological activities.\n\nHe is also noted for the Hansch equation, which is used in \n\nResearch Interests:\nOrganic Chemistry; Interaction of organic chemicals with living organisms, Quantitative Structure Activity Relationships (QSAR).\n\n\nHe died of pneumonia on May 8, 2011 in Claremont, California at 92.\n\nHis research group at Pomona College worked on QSAR studies and in building and expanding the database of chemical and physical data as C-QSAR and Bioloom. His postgraduate associates were Rajni Garg, Cynthia D Selassie, Suresh Babu Mekapati, and Alka Kurup.\n\nThe Journal of Computer-Aided Molecular Design carried four obituaries (as found in a Pubmed personal subject [ps] search).\n\nA preliminary search in WorldCat and in PubMed, two among many relevant bibliographic and citation indexes, shows the following:\n\nThe Pomona College Archives holds reprints of Hansch’s articles published between 1962 and 2009 in addition to other materials.\n\n\n"}
{"id": "41947946", "url": "https://en.wikipedia.org/wiki?curid=41947946", "title": "Coulombmeter", "text": "Coulombmeter\n\nA Coulombmeter is a tool for measuring the electrostatic charge of a material. \nA Coulombmeter is used in combination with a Faraday cup or a metal probe \nfor taking charge measures of a material.\n\nA Nanocoulombmeter is a Coulombmeter that is capable of measuring electrostatic charge down to \nthe accuracy of a fraction of a nanocoulomb (nC).\n\nElectrostatic charge on an object can be measured by placing it into the Faraday \nCup. The charge is transferred to the cup and displayed on the meter's display. \nThe Faraday Cup of the Coulombmeter has an outer, grounded metal shield that \nsurrounds an inner electrode. The inner electrode, which is electrically isolated \nfrom the shield, is connected to a meter to measure the charge.\n\nIn the field of semiconductor design, a coulombmeter consists of a meter used in combination \nwith a metal probe tip to pinpoint locations of excess charge on for instance a semiconductor \ndevice. This application of a coulombmeter is useful because electrostatic discharge is a \nleading cause of failure in semiconductor chip designs, and may occur during the fabrication, \ninspection, assembly, and other processes. A coulombmeter allow this electrostatic buildup to \nbe easily measured simply by applying the instrument's probe to \nits lead\n\nNote: coulombmeter is typically written as a single word in order to avoid confusion with the \nmeasurement unit 'coulombs-meter' and also to make it easily searchable in scientific literature.\n\nA nanocoulombmeter, in combination with a Faraday cup, can be used to detect and measure the beams emitted from electron guns and ion guns.\n\nThe Faraday cup consists of a shielded cup with an aperture, which collects the kinetically active electrons or ions, and an output wire, which is connected to an nanocoulombmeter. The Faraday cup can be completely separate from the gun, or it can be part of an assembly mounted to the end of the electron/ion gun and manipulated remotely. Some Faraday cup assemblies include a phosphor screen as well. An array of small Faraday cups or a Faraday cup on mounted on a linear manipulator can be used to measure the distribution of the beam current across the spot; this shows the beam uniformity.\n"}
{"id": "33895468", "url": "https://en.wikipedia.org/wiki?curid=33895468", "title": "Crab trap", "text": "Crab trap\n\nCrab traps are used to bait, lure, and catch crabs for commercial or recreational use. Crabbing or crab fishing is the recreational hobby and commercial occupation of fishing for crabs. Different types of traps are used depending on the type of crab being fished for, geographic location, and personal preference.\n\nCrab has been a viable food source since Native Americans lived and fished on the Delmarva Peninsula. The Chesapeake Bay, which is known for their Chesapeake Bay blue crabs (\"Callinectes sapidus\") derives its name from \"Chesepiook\", a Susquehannock tribe word that means \"Great Water\". These Susquehannock natives led European settlers to some of the best places to catch crabs. Even early treaties between European settlers and Native Americans included provisions for the rights of \"Hunting, Crabbing, Fowling, and Fishing.\" Since then, generations of watermen made their living harvesting crabs and other resources along the Chesapeake Bay developing the most efficient method to catch crabs resulting in modern crab traps.\nSince early European settlers in America, crabbing was an important food source to watermen of the Chesapeake and continues to be the source of income for many families. The Alaskan king crab fishing industry took off in the mid-1800s, and was one of the reasons Alaskans pushed so hard for statehood in 1959. Alaskans wanted to gain control of the area’s natural resources, such as king crabs.\n\nBenjamin F. Lewis invented the crab pot in the 1920s, patented it in 1928, and perfected it ten years later. The crab pot changed the way crabs are harvested on the Chesapeake Bay. The crab pot is the most common method used to catch and harvest crabs worldwide.\n\nCommercial crabbing is a very tough and dangerous job, so it is very important that commercial crab traps catch as many crabs as possible to be able to turn a profit. Commercial crabbing is heavily regulated by local state laws to ensure that the crabs are not over fished and that they are given enough time to breed and repopulate.\n\nUnlike normal traps, commercial crab traps are large in size; some can easily be over 60\" in diameter, allowing the trap to hold a larger amount of crabs than recreational crab traps. Commercial crab traps also contain a small stainless steel plate like a dog tag, which identifies who the trap belongs to in case it is missed or swept by the current from its original location.\n\nAfter World War II, Japanese crab vessels were competition for Alaskan king crab fishermen in the Bering Sea. Japanese crab vessels would crowd around cod boats, where king crabs devoured the fish waste. Ed Shields, a king crab fisherman was aboard a schooner at this time and recalls the Japanese encroaching on the Bristol Bay fishing area. Ed Shields says that his father sent a telegram to Seattle, ordering one dozen high-powered rifles for each vessel and one case of ammunition each.\n\nEd Shields states, \"The coast guard didn’t care for this at all, the State Department didn’t care for it, but the news media did. It made good news. There’s no television at this time, but they did get in the national magazines like Time and Life. The adverse publicity to Japanese manufactured goods was so severe at that time from this campaign, the Japanese decided to pull out of Bristol Bay area and he sent a telegram saying, 'Bristol Bay is all clear now, Japanese gone home.'\"\n\nThe Derelict Crab Trap Removal Program was created by the Louisiana Wildlife and Fisheries Commission in 2004. This was created to remove derelict crab traps from state-owned lakes and river-beds and to reduce the potential impact from these traps. There are also similar programs in other states. They are similar to the program in Louisiana where the traps are removed during a 30-day period. There are programs all over the Gulf Coast, including areas like Texas and Florida. These programs have also been successful with the help of volunteers working together, and over 30,000 derelict traps have been removed in Texas alone. \n\nThe Maryland crab pot is an enclosed framework of wire with four openings. These openings are constructed so that when the crabs enter to eat the bait, they cannot escape, and instead become immediately trapped. Once the crab becomes trapped and cannot leave the same way they entered, they float upward and go through the openings of the inner wire portion, which permanently traps the crab. \n\nThe Maryland crab pot is a cube, generally two cubic feet and when baited and weighted, might weigh fifteen pounds or more. Sometimes it is left on the bottom for twelve to twenty-four hours or more. The end of the nylon rope is attached to a marked floating buoy so the location can be found and the pot retrieved. The Maryland crab pot is baited from the bottom with several oily fish. This is done by turning the pot on its side, stuffing the bait into the wire container, and closing the opening by securing the flap under the rubber tubing. The pot is then dropped into the water and when the crab fisher returns, pulls the pot up and into their boat.\n\nWest Coast crab pots, which are primarily used for catching Dungeness crabs, vary slightly from the Maryland style crab pot. When the crabs enter either of the two funnel-type openings in search of bait, they are unable to exit through these funnel openings and become entrapped in the pot.\n\nRing crab traps are very popular along the Oregon and Washington Coast. They are primarily used in river mouths and protected bays, but it is possible to use crab rings off the open shoreline. A crab ring is a simple piece of equipment that contains two wire rings that form the top and bottom of a collapsible basket. The lower ring is smaller than the upper ring and connected with a strong netting that forms the sides. Heavy chicken wire, cotton webbing or other suitable materials are used for the bottom.\n\nAfter the bait is tied securely to the bottom of the basket, the lower basket sinks to the bay bottom where the sides collapse and the top and bottom rings lie together, leaving only a flat platform of tempting bait that the crab can easily reach. After the ring has been left on the bottom, the crabber raises the ring rapidly by pulling up with a rope, which prevents the crabs from escaping while the basket is pulled to the boat. Since the ring crab trap lays flat on the seafloor, there is nothing that prevents crabs from escaping before pulling it to the surface.\nPyramid crab traps are flat when lying on the bottom of the seafloor, but when raised to the surface, they form the shape of a pyramid. This trap is similar to the ring crab trap because there are no walls or cage that prevents the crabs from escaping before pulling it to the surface. The benefits of the pyramid crab trap over the ring crab trap is that the pyramid crab trap is slightly sturdier and can be used in waters with stronger currents.\n\nBox crab traps are made from a strong non-collapsible wire. The main advantages of this crab trap are that once the crab enters searching for the bait it cannot escape, guaranteeing a catch when the crab enters. Along with this comes the added bonus of not having to regularly check the trap. The down side of this trap is storing and transporting it since it does not collapse.\n\nTrot line crab fishing was used exclusively by commercial crabbers from 1870 to 1929, but this method has since been almost entirely replaced by the use of crab pots and crab traps. A trotline is a baited, hook-less, long line that is usually anchored on the bottom and attached to anchored buoys. This trotline is baited and after some time, the fisherman pulls the trotline up with crabs hopefully biting on the bait.\n\nA crab trap which becomes lost or abandoned (usually by accidental detachment of the float) becomes an ongoing environmental hazard. Crabs will continue to enter this ghost trap to eat the bait, become trapped, and starve to death, attracting more crabs and other bottom-dwelling sea life; a single trap may kill dozens of crabs in this manner. For this reason, crab traps in many jurisdictions are required to have a \"rot-out panel\", a wooden panel the size of the largest entrance into the trap. This panel will disintegrate with a few weeks' exposure to seawater, opening the trap and allowing any crabs inside to escape.\n\nWhales become entangled in crabbing gear. They get entangled in the vertical lines between crab traps on the ocean floor and the surface buoys. For example, as of 2014 there was an increasing number of entanglements off the coasts of the United States. Management measures have been implemented by NOAA National Marine Fisheries Service.\n\n\nEnvironment:\n\n"}
{"id": "1686285", "url": "https://en.wikipedia.org/wiki?curid=1686285", "title": "Cycling probe technology", "text": "Cycling probe technology\n\nCycling probe technology (CPT) is a molecular biological technique for detecting specific DNA sequences. CPT operates under isothermal conditions. In some applications, CPT offers an alternative to PCR. However, unlike PCR, CPT does not generate multiple copies of the target DNA itself, and the amplification of the signal is linear, in contrast to the exponential amplification of the target DNA in PCR. CPT uses a sequence specific chimeric probe which hybridizes to a complementary target DNA sequence and becomes a substrate for RNase H. Cleavage occurs at the RNA internucleotide linkages and results in dissociation of the probe from the target, thereby making it available for the next probe molecule. Integrated electrokinetic systems have been developed for use in CPT.\n\nCycling probe technology makes use of a chimeric nucleic acid probe to detect the presence of a particular DNA sequence. The chimeric probe consists of an RNA segment sandwiched between two DNA segments. The RNA segment contains 4 contiguous purine nucleotides. The probes should be less than 30 nucleotides in length and designed to minimize intra-probe and inter-probe interactions.\n\nCycling probe technology utilizes a cyclic, isothermal process that begins with the hybridization of the chimeric probe with the target DNA. Once hybridized, the probe becomes a suitable substrate for RNase H. RNase H, an endonuclease, cleaves the RNA portion of the probe, resulting in two chimeric fragments. The melting temperature (T) of the newly cleaved fragments is lower than the melting temperature of original probe. Because the CPT reaction is isothermally kept just above the melting point of the original probe, the cleaved fragments dissociate from the target DNA. Once dissociated, the target DNA is free to hybridize with a new probe, beginning the cycle again.\n\nAfter the fragments have been cleaved and dissociated, they become detectable. A common strategy for detecting the fragments involves fluorescence. With this method, a fluorescent marker is attached to the 5’ end of the probe and a quencher is attached to the 3’ end of the probe. When RNase H cleaves the probe, the quencher and fluorescent marker separate, increasing the intensity of the fluorescent marker. Cleaved fragments can alternatively be detected via amplification (e.g., PCR) or further modification to allow for other chemical means of detection.\n\nWhen working with small concentrations of target DNA, the CPT protocol can be modified to increase specificity and efficiency. Increasing allotted time has been shown to improve probe cleavage efficiency. Both increasing RNase H concentrations and use of a probe that isn’t prone to inter-probe and intra-probe interactions has been show to increase specificity.\n\nBecause cycling probe technology does not involve the amplification of target DNA, CPT has a lower risk of cross contamination than PCR. In addition, CPT is faster than PCR and doesn’t require a specialized thermocycler. CPT also does not require running CPT products on a gel.\n\nCPT requires specialized chimeric probes, making CPT assays more expensive than PCR. Because CPT probes are so specific, a new probe must be designed for each unique assay, further increasing cost. Clinical implementation is hampered financially, but it is also limited by the possibility of samples containing nonspecific RNases other than RNase H.\n\nCPT can be used to detect specific DNA sequences and by extension specific genotypes. For example CPT can be used to distinguish GMO produce from non-GMO produce. Clinically, CPT can be used as an alternative to cell culturing in order to detect antibacterial resistance of a pathogen.\n\nCPT, at its core, detects whether a specific sequence is present in a sample. But because cleaved probes accumulate following linear rate kinetics, the amount of target DNA can be quantified. Consequently, CPT has been used to quantify the number of non-coding repeats in organisms.\n\nCPT can be used in conjunction with other technologies, like molecular beacons and qPCR.\n"}
{"id": "7884801", "url": "https://en.wikipedia.org/wiki?curid=7884801", "title": "Dashboard of Sustainability", "text": "Dashboard of Sustainability\n\nThe Dashboard of Sustainability is a free-of-charge, non-commercial software package configured to convey the complex relationships among economic, social, and environmental issues.\nThe software is designed to help developing countries achieve the Millennium Development Goals and work towards sustainable development. The software package was developed by members of the Consultative Group on Sustainable Development Indicators (CGSDI), and has been applied to quite a number of indicator sets, inter alia to the Millennium Development Goals indicators and the United Nations Commission on Sustainable Development indicators.\n\nIn 2002, Dashboard of Sustainability researchers Jochen Jesinghaus and Peter Hardi presented the Dashboard of Sustainability at the Johannesburg Summit and the 2002 World Social Forum in Porto Alegre. It was also included in the resources for the OECD World Forum on Key Indicators.\n\nIn January 2006, the Millennium Project utilized the Dashboard of Sustainability to conclude in their State of the Future report that global prospects for improving the overall health, wealth, and sustainability of humanity are improving, but slowly. In February 2006, it was proposed that the Dashboard of Sustainability be utilized to combine and represent two or more of the following five frameworks presently used for developing sustainability indicators: domain-based, goal-based, issue-based, sectoral, and causal frameworks.\n\n\"Translating a spreadsheet into a dashboard is relatively straightforward, see \"The Manual\", and numerous indicator sets have been translated into the dashboard format. While many of them are not publicly available, the following applications have been put online by their authors.\"\n\nMillennium Development Goals Indicators Dashboard - see screenshot to the right\n\nSustainable Development Indicators Dashboard (UN CSD set)\n\nUNESCO/SCOPE Policy brief on Sustainable Development \nMaternal and Neonatal Program Effort index (MNPI) \n\n\n\n"}
{"id": "21121427", "url": "https://en.wikipedia.org/wiki?curid=21121427", "title": "Digital Realty", "text": "Digital Realty\n\nDigital Realty Trust, Inc. is a real estate investment trust that invests in carrier-neutral data centers and provides colocation and peering services. As of December 31, 2017, the company owned 205 operating data center facilities totaling 32.1 million rentable square feet, of which 152 were in the United States and 38 were in Europe. The properties are concentrated in New York State, Northern Virginia, London, United Kingdom, Dallas, Silicon Valley, and Chicago.\n\nThe company is a member of The Green Grid and has helped pioneer concepts of energy efficient and energy conserving data center design.\n\nThe company was formed in 2004 by GI Partners, which contributed 21 data centers that it acquired through bankruptcy auctions and from distressed companies at a 20–40% discount to replacement cost.\n\nOn November 4, 2004, the company became a public company via an initial public offering. At that time, the company owned 23 properties comprising 5.6 million square feet.\n\nIn August 2006, the company sold a building in Denver, Colorado to TIAA-CREF for $60.42 million. The company also acquired a property in Phoenix, Arizona for $175 million.\n\nBy March 2007, GI Partners had sold all of its shares in the company.\n\nIn January 2010, the company acquired 3 data centers in Massachusetts and Connecticut for $375 million.\n\nIn January 2012, the company acquired a 334,000 square foot data center near Hartsfield-Jackson Atlanta International Airport for $63 million in a leaseback transaction. The company also acquired a data center in San Francisco for $85 million.\n\nIn April 2013, the company acquired the data center of Delta Air Lines in Eagan, Minnesota for $37 million in a leaseback transaction.\n\nIn July 2013, the company doubled capacity at its data center in Chandler, Arizona.\n\nIn May 2015, the company sold a building in Philadelphia for $161 million that it acquired in 2005 for $59 million.\n\nIn October 2015, the company acquired Telx for $1.886 billion.\n\nIn November 2015, the company acquired 125.9 acres of undeveloped land in Loudoun County, Virginia for $43 million and announced plans to build a 2 million square foot data center on the property.\n\nIn July 2016, the company acquired 8 data centers in Europe from Equinix for $874 million.\n\nIn March 2017, the company announced a $22 million expansion of its data center in Atlanta.\n\nIn 2017, the Supreme Court will hear a whistleblower case in which the company fired an employee who had complained internally about the elimination of supervisory controls and the hiding of cost overruns. After he was fired, the employee sued the company, saying he was protected by whistleblower provisions in Dodd-Frank.\n\nIn September 2017, the company completed the acquisition of DuPont Fabros Technology.\n"}
{"id": "8039960", "url": "https://en.wikipedia.org/wiki?curid=8039960", "title": "Emission-aware programming", "text": "Emission-aware programming\n\nEmission-aware programming is a design philosophy aiming to reduce the amount of electromagnetic radiation emitted by electronic devices through proper design of the software executed by the device, rather than changing the hardware.\n\nEmission considerations require the evaluation of many details such as the clock frequencies and switching rates which are related to the oscillator. Rise/fall times and signal harmonics are related to the output driver. The data transfer rates can be dependent on hardware or can be controlled by software and often have to meet a certain protocol. Impedances, trace loading and the various circuit components are hardware related and must be considered from the beginning of the design.\n\nThree basic actions for emission reduction can be defined:\n\n\nIn the following all components that can be influenced by the software design will be described.\n\nMicrocontrollers require a clock source for the switching of the internal transistors. Almost all controllers require an external crystal or ceramic resonator. Since the ceramic resonator is potentially sensitive to spikes which can shorten clock periods typically a Pierce oscillator configuration is used. Harmonic frequencies of the clock cause unwanted emissions.\nThe circuit internal to the MC in simplified form is a NAND gate followed by an inverter.\nThe external oscillator is not the only source of emissions. The system clock circuits consists of an internal RF divider followed by large amplifiers. These amplifiers drive long lines inside the components and might cause interferences.\nThe use of internal oscillators instead of external ones should be preferred. (An additional hardware measure is the use of spread spectrum oscillators.)\n\nThe field strength is proportional to the current as a consequence the power supply, providing the current for the entire system, is a strong source of emissions. Physically, a reduction of the power consumption of the system and the minimising of circuit loops (by the usage of decoupling capacities) emitting the noise, improves the EME performance.\nA software based solution is to temporary disable peripheral devices when not needed and thereby reduce unnecessary power consumption.\n\nIf an MC uses external memory space or peripheral devices continuous transitions on several data/address bus lines are implied. The emission depends on the frequency of the transitions, their quantity, rise/fall time and duration. The quantity of transitions, of port traffic can be influenced by the use of interrupts instead of continuous polling of ports. The use of interrupts is software based as well.\nFor the IRQ or reset pins (input pins) the termination is more important than for general I/O ports. If noise causes these two pins to mis-trigger it will have a negative effect on the circuit behavior. A high current consumption is often observed, particularly in CMOS devices, when the input pins are unconnected due to leakage current internal to the IC. Terminating high\nimpedance input pins can therefore lead to a reduction in supply current and hence reduces emission.\nWith most MC the internal pull-up resistors can be used to set unused pins to a defined voltage level.\n\nOne way of reducing the emissions of an MC System is to extend the rise and fall time (Slew Rate Control). Some controllers like the Motorola HCS08 offer the feature of software controlled slew rate output buffers enabling the user to extend the rise time from 3 ns to 30 ns for each pin separately.\n\nSome microcontrollers offer current limiting, a way of limiting the maximum current driven by the output port (e.g. Motorola MCS12).\n\nThe most effective way of reducing emissions is to temporary shut down unused modules in the MC, thereby saving power. Most controller support several \"sleep modes\".\n\nIn order to reduce electromagnetic emissions on software basis the following measures should be considered:\n\nMost software implemented improvements on emission can only be detected using an Average-Detector!\n"}
{"id": "51036166", "url": "https://en.wikipedia.org/wiki?curid=51036166", "title": "Erich Hüttenhain", "text": "Erich Hüttenhain\n\nErich Hüttenhain (* 26. January 1905 in Siegen ; † 1. December 1990 in Brühl) was a German academic mathematician and cryptographer (Cryptography) and considered a leading cryptanalyst in the Third Reich. He was Head of the cryptanalysis unit at OKW/Chi, the Cipher Department of the High Command of the Wehrmacht.\n\nDr Hüttenhain was the son of a Conrector and studied after the high school diploma () 1924 in Siegen at the University of Marburg, the Johann Wolfgang Goethe-University Frankfurt and the University of Münster. He studied mathematics with Heinrich Behnke and astronomy at Münster. There he was assistant to Martin Lindow\n(1880–1967), who was director of the observatory at Münster. In 1933, at the University of Münster, he took his examination for promotion of Dr. phil. in astronomy under Lindow with the thesis titled: \"Spatial infinitesimal orbits around the libration points in the straight-line case of the (3 + 1) bodies\". In 1936, he was sent to the cipher bureau of the OKW OKW/Chi under Director Min.Rat. Wilhelm Fenner. Erich Hüttenhain had an interest in Mayan chronology which led him to cryptology and thus to OKW/CHi. As a recruitment test, Fenner had sent him a message which had been enciphered with a private cipher. Hüttenhain duly deciphered it and was accepted as a possible cryptanalyst. At OKW/Chi he was employed as a specialist to build a cryptanalytic research unit, and later he was most recently Executive Council Head of group IV Analytical cryptanalysis.\n\nDuring his time in OKW/Chi he succeeded, among other things, in the deciphering of the Japanese Purple cipher machine (William Frederick Friedman) He and his staff also temporarily succeeded in deciphering American rotary machines, such as the M 138 A and the M-209 in North Africa. Later in the war, when the allies invaded Italy, the allies learned in turn by deciphering Italian ciphers that their later systems, e.g. among others the SIGABA designed by Friedman, had not been broken and around that time, Hüttenhain had no more major successes.\n\nAfter World War II, being a high value target, he was taken by TICOM to the USA to be interrogated. For the Americans, he built a machine (which was already used by the Germans during World War II) that deciphered the Russian rotor machine encryption. He also created reports on the successes of the Germans on cryptographic territory during World War II (as deciphering the French naval codes, the Polish diplomat cipher or the security of the Enigma ). After his return he founded in 1947 the \"Society of scientific work\" within the Gehlen Organization, which laid the foundation for the subsequent formation of the German Central Office for Encryption (, a unit of the German Federal Intelligence Service (). His pseudonym in the Gehlen organization was Erich Hammerschmidt. In the first official cryptographic service of the Federal Government, the Unit 114 in the Foreign Ministry, headed by Adolf Paschke founded in 1950, he was the chair of the Scientific Advisory Board, along with Kurt Selchow, Rudolf Schauffler, and Heinz Kuntze, some of the best cryptologists in Germany. During 1956–1970 he served as Deputy Director of the Central Office for Encryption where initially Wilhelm Göing and 1972 Otto Leiberich was his successor. One of the objectives of Hüttenhain was that in contrast to his experiences in the Third Reich where numerous independent cipher bureaux were spread throughout the Reich, all threads for evaluating cryptographic procedures were now to be integrated into a single office.\n\nIn 1926 he was a founding fellow of the Frankfurt Burschenschaft Arminia .\n\nHüttenhain left a posthumous manuscript he wrote in about 1970 and in which he reports on his experience as a cryptologist.\n\n\n"}
{"id": "10147895", "url": "https://en.wikipedia.org/wiki?curid=10147895", "title": "Explosion protection", "text": "Explosion protection\n\nExplosion protection is used to protect all sorts of buildings and civil engineering infrastructure against internal and external explosions or deflagrations. It was widely believed until recently that a building subject to an explosive attack had a chance to remain standing only if it possessed some extraordinary resistive capacity. This belief rested on the assumption that the specific impulse or the time integral of pressure, which is a dominant characteristic of the blast load, is fully beyond our control.\n\n\"Avoidance\" will make it impossible for an explosion or deflagration to occur, for instance by means of suppressing the heat and the pressure needed for an explosion using aluminum mesh structure such as eXess, by means of consistent displacement of the O necessary for an explosion or deflagration to take place, by means of padding gas (f. i. CO or N), or, by means of keeping the concentration of flammable content of an atmosphere consistently below or above the explosive limit, or, by means of consistent elimination of ignition sources.\n\nConstructional explosion protection aims at pre-defined, limited or zero damage that results from applied protective techniques in combination with reinforcement of the equipment or structures that must be expected to become subject to internal explosion pressure and flying debris or external violent impact. \n\nThe technology of protection can range in price dramatically but where the type of device is rational to use, would typically be from least to most expensive solutions: explosion doors and vents (dependent on quantities and common denominators, either may end up the wise price choice); inerting: explosion suppression; isolation – or combinations of same. To focus on the most cost effective, doors typically have lower release pressure capabilities; are not susceptible to fatigue failures or subject to changing release pressures with changes in temperature, as “rupture membrane” type are; capable of leak tight service; service temperatures of up to 2,000°F; and can be more cost effective in small quantities. Rupture membrane type vents can provide a leak tight seal more readily in most cases; have a relatively broad tolerance on their release pressure and are more readily incorporated into systems with discharge ducts.\n\nThere are several fundamental considerations in the review of a system handling potentially explosive dusts, gases or a mixture of the two. Dependent upon the design basis being used, often National Fire Protection Association Guideline 68, the definition of these may vary somewhat. To facilitate providing the reader with an appreciation of the issues rather than a design primer, the following have been limited to the major ones only.\n\n"}
{"id": "14608785", "url": "https://en.wikipedia.org/wiki?curid=14608785", "title": "Firewalls and Internet Security", "text": "Firewalls and Internet Security\n\nFirewalls and Internet Security: Repelling the Wily Hacker, a 1994 book by William R. Cheswick and Steve Bellovin, helped define the concept of a network firewall. \n\nDescribing in detail one of the first major firewall deployments at AT&T, the book influenced the formation of the perimeter security model, which became the dominant network security architecture in the mid-1990s.\n\nIn 2003, a second edition came out, adding Aviel D. Rubin to the authors.\n\n"}
{"id": "6905266", "url": "https://en.wikipedia.org/wiki?curid=6905266", "title": "Flight director (aeronautics)", "text": "Flight director (aeronautics)\n\nIn aviation, a flight director (FD) is a flight instrument that is overlaid on the attitude indicator that shows the pilot of an aircraft the attitude required to follow a certain trajectory to which the flight is to be conducted.\n\nThe flight director computes and displays the proper pitch and bank angles required for the aircraft to follow a selected flight path.\n\nA simple example: The aircraft flies level on 045° heading at flight level FL150 at 260 kt indicated airspeed, the FD bars are thus centered. Then the flight director is set to heading 090° and a new flight level FL200. The aircraft must thus turn to the right and climb.\nThis is done by banking to the right while climbing. The roll bar will deflect to the right and the pitch bar will deflect upwards. The pilot will then pull back on the control column while banking to the right. Once the aircraft reaches the proper bank angle, the FD vertical bar will center and remain centered until it is time to roll back to wings level (when the heading approaches 090°).\nWhen the aircraft approaches FL200 the FD horizontal bar will deflect downwards thus commanding the pilot to lower the nose in order to level off at FL200.\n\nThe FD is generally used in direct connection with the Autopilot (AP), where the FD commands the AP to put the aircraft in the attitude necessary to follow a trajectory. The FD/AP combination is typically used in autopilot coupled low instrument approaches (below 200 feet AGL), or CAT II and CAT III ILS instrument approaches.\n\nThe exact form of the flight director's display varies with the instrument type, either crosshair or command bars (so-called \"cue\").\n\n"}
{"id": "14482575", "url": "https://en.wikipedia.org/wiki?curid=14482575", "title": "Geode (trade association)", "text": "Geode (trade association)\n\nGEODE is a trade association that represents independent energy distribution and distribution-related companies, as well as other organisations in natural gas and electricity, privately or publicly owned, in Europe. It was founded in 1991 and represents 600 companies, both privately and publicly owned, in 10 countries.\n\nGEODE took part in the Gas Regulatory Forum in Madrid in March 2012, and the Electricity Regulatory Forum in Florence in May 2012, both organized by the European Commission. GEODE defends the interests of local distributors before energy authorities on the national and international levels and allows the exchange of expertise, the share of data, and competence.\n\n"}
{"id": "26727713", "url": "https://en.wikipedia.org/wiki?curid=26727713", "title": "Guardian Challenge", "text": "Guardian Challenge\n\nThe United States Air Force's Space and Missile Combat Competition, currently known as Guardian Challenge is a military competition that recognizes the best space and missile combat crews in Air Force Space Command.\n\n"}
{"id": "8288124", "url": "https://en.wikipedia.org/wiki?curid=8288124", "title": "History of clothing and textiles", "text": "History of clothing and textiles\n\nThe study of the history of clothing and textiles traces the development, use, and availability of clothing and textiles over human history. Clothing and textiles reflect the materials and technologies available in different civilizations at different times. The variety and distribution of clothing and textiles within a society reveal social customs and culture.\n\nThe wearing of clothing is exclusively a human characteristic and is a feature of most human societies, though it is not known exactly when various peoples began wearing clothes. anthropologists believe that animal skins and vegetation were adapted into coverings as protection from cold, heat and rain, especially as humans migrated to new climates.\n\nTextiles can be felt or spun fibers made into yarn and subsequently netted, looped, knit or woven to make fabrics, which appeared in the Middle East during the late stone age. From the ancient times to the present day, methods of textile production have continually evolved, and the choices of textiles available have influenced how people carried their possessions, clothed themselves, and decorated their surroundings.\n\nSources available for the study of clothing and textiles include material remains discovered via archaeology; representation of textiles and their manufacture in art; and documents concerning the manufacture, acquisition, use, and trade of fabrics, tools, and finished garments. Scholarship of textile history, especially its earlier stages, is part of material culture studies.\n\nThe development of textile and clothing manufacture in prehistory has been the subject of a number of scholarly studies since the late 20th century. These sources have helped to provide a coherent history of these prehistoric developments. Evidence suggests that humans may have begun wearing clothing as far back as 100,000 to 500,000 years ago.\n\nGenetic analysis suggests that the human body louse, which lives in clothing, may only have diverged from the head louse some 170,000 years ago, which supports evidence that humans began wearing clothing at around this time. These estimates predate the first known human exodus from Africa, although other hominid species who may have worn clothes – and shared these louse infestations – appear to have migrated earlier.\n\nSewing needles have been dated to at least 50,000 years ago (Denisova Cave, Siberia) – and uniquely associated with a human species other than modern humans, i.e. H. Denisova/H. Altai. The oldest possible example is 60,000 years ago, a needle point (missing stem and eye) found in Sibudu Cave, South Africa. Other early examples of needles dating from 41,000-15,000 years ago are found in multiple locations, e.g. Slovenia, Russia, China, Spain and France.\n\nThe earliest dyed flax fibres have been found in a prehistoric cave in the Georgia and date back to 36,000.\n\nThe 25,000 year old Venus Figurine \"Venus of Lespugue\", found in southern France in the Pyrenees, depicts a cloth or twisted fibre skirt. Other figurines from western Europe were adorned with basket hats or caps, belts worn at the waist, and a strap of cloth that wrapped around the body right above the breast. Eastern European figurines wore belts, hung low on the hips and sometimes string skirts.\n\nArchaeologists have discovered artifacts from the same period that appear to have been used in the textile arts: (5000 BC) net gauges, spindle needles and weaving sticks.\n\nThe first actual textile, as opposed to skins sewn together, was probably felt. Surviving examples of Nålebinding, another early textile method, date from 6500 BC. Our knowledge of ancient textiles and clothing has expanded in the recent past thanks to modern technological developments. Our knowledge of cultures varies greatly with the climatic conditions to which archeological deposits are exposed; the Middle East and the arid fringes of China have provided many very early samples in good condition, but the early development of textiles in the Indian subcontinent, sub-Saharan Africa and other moist parts of the world remains unclear. In northern Eurasia, peat bogs can also preserve textiles very well.\nThe first known textile of South America was discovered in Guitarrero Cave in Peru, it was woven out of vegetable fibers and dates back to 8,000 B.C.E. \n\nFrom pre-history through the early Middle Ages, for most of Europe, the Near East and North Africa, two main types of loom dominate textile production. These are the warp-weighted loom and the two-beam loom. The length of the cloth beam determined the width of the cloth woven upon it, and could be as wide as 2–3 meters. The second loom type is the two-beam loom. Early woven clothing was often made of full loom widths draped, tied, or pinned in place.\n\nThroughout the Neolithic and Bronze ages, the fertile grounds of the Eurasian Steppe provided a venue for a network of nomadic communities to develop and interact. The Steppe Route has always connected regions of the Asian continent with trade and transmission of culture, including clothing.\n\nAround 114 BC, the Han Dynasty, initiated the Silk Road Trade Route. Geographically, the Silk Road or Silk Route is an interconnected series of ancient trade routes between Chang'an (today's Xi'an) in China, with Asia Minor and the Mediterranean extending over on land and sea. Trade on the Silk Road was a significant factor in the development of the great civilizations of China, Egypt, Mesopotamia, Persia, the Indian subcontinent, and Rome, and helped to lay the foundations for the modern world. The exchange of luxury textiles was predominant on the Silk Road, which linked traders, merchants, pilgrims, monks, soldiers, nomads and urban dwellers from China to the Mediterranean Sea during various periods of time.\n\nThe earliest known woven textiles of the Near East may be fabrics used to wrap the dead, excavated at a Neolithic site at Çatalhöyük in Anatolia, carbonized in a fire and radiocarbon dated to c. 6000 BC. Evidence exists of flax cultivation from c. 8000 BC in the Near East, but the breeding of sheep with a wooly fleece rather than hair occurs much later, c. 3000 BC.\n\nWe do not know what the people who constituted the Indus Valley Civilization, one of the earliest civilizations of the world, actually wore. Any cloth that might have been worn has long since disintegrated and we have not yet been able to decipher the Indus script. However, historians and archaeologists have managed to piece together some bits of information from clues found in sculptures and figurines.\n\nTerracotta figurines uncovered at Mehrgarh show a male figure wearing what is commonly interpreted to be a turban; female figurines depict women with elaborate headdress and intricate hairstyles. In certain cases, these headdresses have led historians to attach a religious connotation to the figurines and to the interpret the headdresses as symbols of a mother goddess.\n\nOne of the most important recovered figurines is that of the “Priest King” from the site of Mohenjo-daro. It is not only important because scholars have called it a representation of an assumed authority or head of state but also because of what it is wearing, however, it was recently discovered to be an interpretation of a wealthy trader. The calmly seated Priest-King is depicted wearing a shawl with floral patterns. So far, this is the only sculpture from the Indus Valley to show clothing in such explicit detail. However, it does not provide any concrete proof to legitimize the history of clothing in the Harappan times. Harappans may even have used natural colours to dye their fabric. Research shows that the cultivation of indigo plants (genus: Indigofera) was prevalent.\n\nAnother important sculpture is of a dancing girl, also excavated from Mohenjo-daro. She is depicted with no clothing other than a number of bangles upon her arm. B. B. Lal has managed draw parallels between the dancing girl and women today in parts of Rajasthan and Gujarat. He notices how contemporary women continue wearing those bangles even today. Harappans may not have left any evidence of what clothing or textiles they had at that time but they did leave remains of jewellery and beads in large amounts. For instance, the graves of Harappans have yielded various forms of jewellery such as neckpieces, bracelets, rings, and head ornaments. Multiple beads of varying shapes and sizes have also been recovered. This jewellery incorporates various materials such as gold, bronze, terracotta, faience, and shells; imported materials including turquoise and lapis lazuli were used too. This suggests that the Harappans might have engaged in long-distance trade. Long, slender carnelian beads were highly prized by the Harappans. Harappans were also experts in manufacturing microbeads, which have been found in various locations from hearths and graves. These beads were extremely hard to work with and needed extra precision to produce. A special drill has been found both at Lothal and Chanhudaro. Chanhudaro was a centre exclusively devoted to craft production.\n\nEvidence exists for production of linen cloth in Ancient Egypt in the Neolithic period, c. 5500 BC. Cultivation of domesticated wild flax, probably an import from the Levant, is documented as early as c. 6000 BC. Other bast fibers including rush, reed, palm, and papyrus were used alone or with linen to make rope and other textiles. Evidence for wool production in Egypt is scanty at this period.\n\nSpinning techniques included the drop spindle, hand-to-hand spinning, and rolling on the thigh; yarn was also spliced. A horizontal ground loom was used prior to the New Kingdom, when a vertical two-beam loom was introduced, probably from Asia.\n\nLinen bandages were used in the burial custom of mummification, and art depicts Egyptian men wearing linen kilts and women in narrow dresses with various forms of shirts and jackets, often of sheer pleated fabric.\n\nThe earliest evidence of silk production in China was found at the sites of Yangshao culture in Xia, Shanxi, where a cocoon of bombyx mori, the domesticated silkworm, cut in half by a sharp knife is dated to between 5000 and 3000 BC. Fragments of primitive looms are also seen from the sites of Hemudu culture in Yuyao, Zhejiang, dated to about 4000 BC. Scraps of silk were found in a Liangzhu culture site at Qianshanyang in Huzhou, Zhejiang, dating back to 2700 BC. Other fragments have been recovered from royal tombs in the [Shang Dynasty] (c. 1600 – c. 1046 BC).\n\nUnder the Shang Dynasty, Han Chinese clothing or Hanfu consisted of a \"yi\", a narrow-cuffed, knee-length tunic tied with a sash, and a narrow, ankle-length skirt, called \"shang\", worn with a \"bixi\", a length of fabric that reached the knees. Clothing of the elite was made of silk in vivid primary colours.\n\nThe earliest evidence of spinning in Thailand can be found at the archaeological site of Tha Kae located in Central Thailand. Tha Kae was inhabited during the end of the first millennium BC to the late first millennium AD. Here, archaeologists discovered 90 fragments of spindle whorl dated from 3rd century BC to 3rd century AD. And the shape of these finds indicate the connections with south China and India. A spindle whorl is a disc or spherical object that fits onto the spindle to increase as well as maintain the speed of spinning.\n\nThe earliest evidence of weaving in Japan is associated with the Jōmon period. This culture is defined by pottery decorated with cord patterns. In a shell mound in the Miyagi Prefecture, dating back about 5,500, some cloth fragments were discovered made from bark fibers. Hemp fibers were also discovered in the Torihama shell mound, Fukui Prefecture, dating back to the Jōmon period, suggesting that these plants could also have been used for clothing. Some pottery pattern imprints depict also fine mat designs, proving their weaving techniques. The patterns on the Jōmon pottery show people wearing short upper garments, close-fitting trousers, funnel-sleeves, and rope-like belts. The depictions also show clothing with patterns that are embroidered or painted arched designs, though it is not apparent whether this indicates what the clothes look like or whether that simply happens to be the style of representation used. The pottery also shows no distinction between male and female garments. This may have been true because during that time period clothing was more for decoration than social distinction, but it might also just be because of the representation on the pottery rather than how people actually dressed at the time. Since bone needles were also found, it is assumed that they wore dresses that were sewn together.\n\nNext was the Yayoi period, during which rice cultivation was developed. This led to a shift from hunter-gatherer communities to agrarian societies which had a large impact on clothing. According to Chinese literature from that time period, clothing more appropriate to agriculture began to be worn. For example, unsewn fabric wrapper around the body and poncho-type garments with head-holes cut into them. This same literature also indicates that pink or scarlet makeup was worn but also that mannerisms between people of all ages and genders were not very different. However, this is debatable as there were probably cultural prejudices in the Chinese document. There is a common Japanese belief that the Yayoi time period was quite utopian before Chinese influence began to promote the use of clothing to indicate age and gender.\n\nFrom 300 to 550 AD was the Yamato period, and here much of the clothing style can be derived from the artifacts of the time. The tomb statues (haniwa) especially tell us that the clothing style changed from the ones according to the Chinese accounts from the previous age. The statues are usually wearing a two piece outfit that has an upper piece with a front opening and close-cut sleeves with loose trousers for men and a pleated skirt for women. Silk farming had been introduced by the Chinese by this time period but due to silk’s cost it would only be used by people of certain classes or ranks.\n\nThe following periods were the Asuka (550 to 646 AD) and Nara (646 to 794 AD) when Japan developed a more unified government and began to use Chinese laws and social rankings. These new laws required people to wear different styles and colors to indicate social status. Clothing became longer and wider in general and sewing methods were more advanced.\n\nThe classical Filipino clothing varied according to cost and current fashions and so indicated social standing. The basic garments were the Bahag and the tube skirt—what the Maranao call malong—or a light blanket wrapped around instead. But more prestigious clothes, lihin-lihin, were added for public appearances and especially on formal occasions—blouses and tunics, loose smocks with sleeves, capes, or ankle-length robes. The textiles of which they were made were similarly varied. In ascending order of value, they were abaca, abaca decorated with colored cotton thread, cotton, cotton decorated with silk thread, silk, imported printstuff, and an elegant abaca woven of selected fibers almost as thin as silk. In addition, Pigafetta mentioned both G-strings and skirts of bark cloth.\n\nUntailored clothes, however had no particular names. \"Pandong\", a lady's cloak, simply meant any natural covering, like the growth on banana trunk’s or a natal caul. In Panay, the word \"kurong\", meaning curly hair, was applied to any short skirt or blouse; and some better ones made of imported chintz or calico were simply called by the name of the cloth itself, tabas. So, too, the wraparound skirt the Tagalogs called tapis was hardly considered a skirt at all: Visayans just called it habul (woven stuff) or halong (abaca) or even hulun (sash).\n\nThe usual male headdress was the pudong, a turban, though in Panay both men and women also wore a head cloth or bandana called \"saplung\". Commoners wore pudong of rough abaca cloth wrapped around only a few turns so that it was more of a headband than a turban and was therefore called \"pudong-pudong—as\" the crowns and diadems on Christian images were later called. A red \"pudong\" was called magalong, and was the insignia of braves who had killed an enemy. The most prestigious kind of \"pudong\", limited to the most valiant, was, like their G-strings, made of pinayusan, a gauze-thin abaca of fibers selected for their whiteness, tie-dyed a deep scarlet in patterns as fine as embroidery, and burnished to a silky sheen. Such pudong were lengthened with each additional feat of valor: real heroes therefore let one end hang loose with affected carelessness.\nWomen generally wore a kerchief, called tubatub if it was pulled tight over the whole head; but they also had a broad-brimmed hat called sayap or tarindak, woven of sago-palm leaves. Some were evidently signs of rank: when Humabon’s queen went to hear mass during Magellan’s visit, she was preceded by three girls carrying one of her hats. A headdress from Cebu with a deep crown, used by both sexes for travel on foot or by boat, was called \"sarok\", which actually meant to go for water.\n\nFabric in Ancient Greece was woven on a warp-weighted loom. The first extant image of weaving in western art is from a terracotta lekythos in the Metropolitan Museum of Art, NY. The vase, c. 550-530 B.C.E., depicts two women weaving at an upright loom. The warp threads, which run vertically to a bar at the top, are tied together with weights at the bottom, which hold them taut. The woman on the right runs the shuttle containing the weaving thread across the middle of the warp. The woman on the left uses a beater to consolidate the already-woven threads.\n\nDress in classical antiquity favored wide, unsewn lengths of fabric, pinned and draped to the body in various ways.\n\nAncient Greek clothing consisted of lengths of wool or linen, generally rectangular and secured at the shoulders with ornamented pins called fibulae and belted with a sash. Typical garments were the peplos, a loose robe worn by women; the chlamys, a cloak worn by men; and the chiton, a tunic worn by both men and women. Men’s chitons hung to the knees, whereas women’s chitons fell to their ankles. A long cloak called a himation was worn over the peplos or chlamys.\n\nThe toga of ancient Rome was also an unsewn length of wool cloth, worn by male citizens draped around the body in various fashions, over a simple tunic. Early tunics were two simple rectangles joined at the shoulders and sides; later tunics had sewn sleeves. Women wore the draped stola or an ankle-length tunic, with a shawl-like palla as an outer garment. Wool was the preferred fabric, although linen, hemp, and small amounts of expensive imported silk and cotton were also worn.\n\nThe Iron Age is broadly identified as stretching from the end of the Bronze Age around 1200 BC to 500 AD and the beginning of the Medieval period. Bodies and clothing have been found from this period, preserved by the anaerobic and acidic conditions of peat bogs in northwestern Europe. A Danish recreation of clothing found with such bodies indicates woven wool dresses, tunics and skirts. These were largely unshaped and held in place with leather belts and metal brooches or pins. Garments were not always plain, but incorporated decoration with contrasting colours, particularly at the ends and edges of the garment. Men wore breeches, possibly with lower legs wrapped for protection, although Boucher states that long trousers have also been found. Warmth came from woollen shawls and capes of animal skin, probably worn with the fur facing inwards for added comfort. Caps were worn, also made from skins, and there was an emphasis on hair arrangements, from braids to elaborate Suebian knots. Soft laced shoes made from leather protected the foot.\n\nThe history of Medieval European clothing and textiles has inspired a good deal of scholarly interest in the 21st century. Elisabeth Crowfoot, Frances Pritchard, and Kay Staniland authored \"Textiles and Clothing: Medieval Finds from Excavations in London, c.1150-c.1450\" (Boydell Press, 2001). The topic is also the subject of an annual series, \"Medieval Clothing and Textiles\" (Boydell Press), edited by Robin Netherton and Gale R. Owen-Crocker, Emeritus Professor of Anglo-Saxon Culture at the University of Manchester.\n\nThe Byzantines made and exported very richly patterned cloth, woven and embroidered for the upper classes, and resist-dyed and printed for the lower. By Justinian's time the Roman toga had been replaced by the tunica, or long \"chiton\", for both sexes, over which the upper classes wore various other garments, like a \"dalmatica\" (dalmatic), a heavier and shorter type of tunica; short and long cloaks were fastened on the right shoulder.\n\nLeggings and hose were often worn, but are not prominent in depictions of the wealthy; they were associated with barbarians, whether European or Persian.\n\nEuropean dress changed gradually in the years 400 to 1100. People in many countries dressed differently depending on whether they identified with the old Romanised population, or the new invading populations such as Franks, Anglo-Saxons, and Visigoths. Men of the invading peoples generally wore short tunics, with belts, and visible trousers, hose or leggings. The Romanised populations, and the Church, remained faithful to the longer tunics of Roman formal costume.\n\nThe elite imported silk cloth from the Byzantine, and later Muslim, worlds, and also probably cotton. They also could afford bleached linen and dyed and simply patterned wool woven in Europe itself. But embroidered decoration was probably very widespread, though not usually detectable in art. Lower classes wore local or homespun wool, often undyed, trimmed with bands of decoration, variously embroidery, tablet-woven bands, or colorful borders woven into the fabric in the loom.\n\nClothing in 12th and 13th century Europe remained very simple for both men and women, and quite uniform across the subcontinent. The traditional combination of short tunic with hose for working-class men and long tunic with overgown for women and upper class men remained the norm. Most clothing, especially outside the wealthier classes, remained little changed from three or four centuries earlier.\n\nThe 13th century saw great progress in the dyeing and working of wool, which was by far the most important material for outerwear. Linen was increasingly used for clothing that was directly in contact with the skin. Unlike wool, linen could be laundered and bleached in the sun. Cotton, imported raw from Egypt and elsewhere, was used for padding and quilting, and cloths such as buckram and fustian.\n\nCrusaders returning from the Levant brought knowledge of its fine textiles, including light silks, to Western Europe. In Northern Europe, silk was an imported and very expensive luxury. The well-off could afford woven brocades from Italy or even further afield. Fashionable Italian silks of this period featured repeating patterns of roundels and animals, deriving from Ottoman silk-weaving centres in Bursa, and ultimately from Yuan Dynasty China via the Silk Road.\n\nCultural and costume historians agree that the mid-14th century marks the emergence of recognizable \"fashion\" in Europe. From this century onwards, Western fashion changed at a pace quite unknown to other civilizations, whether ancient or contemporary. In most other cultures, only major political changes, such as the Muslim conquest of India, produced radical changes in clothing, and in China, Japan, and the Ottoman Empire fashion changed only slightly over periods of several centuries.\n\nIn this period, the draped garments and straight seams of previous centuries were replaced by curved seams and the beginnings of tailoring, which allowed clothing to more closely fit the human form, as did the use of lacing and buttons. A fashion for \"mi-parti\" or \"parti-coloured\" garments made of two contrasting fabrics, one on each side, arose for men in mid-century, and was especially popular at the English court. Sometimes just the hose would have different colours on each leg.\n\nWool remained the most popular fabric for all classes, followed by linen and hemp. Wool fabrics were available in a wide range of qualities, from rough undyed cloth to fine, dense broadcloth with a velvety nap; high-value broadcloth was a backbone of the English economy and was exported throughout Europe. Wool fabrics were dyed in rich colours, notably reds, greens, golds, and blues.\n\nSilk-weaving was well established around the Mediterranean by the beginning of the 15th century, and figured silks, often silk velvets with silver-gilt wefts, are increasingly seen in Italian dress and in the dress of the wealthy throughout Europe. Stately floral designs featuring a pomegranate or artichoke motif had reached Europe from China in the previous century and became a dominant design in the Ottoman silk-producing cities of Istanbul and Bursa, and spread to silk weavers in Florence, Genoa, Venice, Valencia and Seville in this period.\n\nAs prosperity grew in the 15th century, the urban middle classes, including skilled workers, began to wear more complex clothes that followed, at a distance, the fashions set by the elites. National variations in clothing increased over the century.\n\nBy the first half of the 16th century, the clothing of the Low Countries, German states, and Scandinavia had developed in a different direction than that of England, France, and Italy, although all absorbed the sobering and formal influence of Spanish dress after the mid-1520s.\n\nElaborate slashing was popular, especially in Germany. Black was increasingly worn for the most formal occasions. Bobbin lace arose from passementerie in the mid-16th century, probably in Flanders. This century also saw the rise of the ruff, which grew from a mere ruffle at the neckline of the shirt or chemise to immense cartwheel shapes. At their most extravagant, ruffs required wire supports and were made of fine Italian reticella, a cutwork linen lace.\n\nBy the turn of the 17th century, a sharp distinction could be seen between the sober fashions favored by Protestants in England and the Netherlands, which still showed heavy Spanish influence, and the light, revealing fashions of the French and Italian courts.\n\nThe great flowering of needlelace occurred in this period. Geometric reticella deriving from cutwork was elaborated into true needlelace or \"punto in aria\" (called in England \"point lace\"), which reflected the scrolling floral designs popular for embroidery. Lacemaking centers were established in France to reduce the outflow of cash to Italy.\n\nAccording to Dr. Wolf D. Fuhrig, \"By the second half of the 17th century, Silesia had become an important economic pillar of the Habsburg monarchy, largely on the strength of its textile industry.\"\n\nMughal India (16th to 18th centuries) was the most important center of manufacturing in international trade up until the 18th century. Up until 1750, India produced about 25% of the world's industrial output. The largest manufacturing industry in Mughal India was textile manufacturing, particularly cotton textile manufacturing, which included the production of piece goods, calicos, and muslins, available unbleached and in a variety of colours. The cotton textile industry was responsible for a large part of India's international trade. India had a 25% share of the global textile trade in the early 18th century. Indian cotton textiles were the most important manufactured goods in world trade in the 18th century, consumed across the world from the Americas to Japan. The most important center of cotton production was the Bengal Subah province, particularly around its capital city of Dhaka.\n\nBengal accounted for more than 50% of textiles and around 80% of silks imported by the Dutch from Asia, Bengali silk and cotton textiles were exported in large quantities to Europe, Indonesia, and Japan, and Bengali muslin textiles from Dhaka were sold in Central Asia, where they were known as \"daka\" textiles. Indian textiles dominated the Indian Ocean trade for centuries, were sold in the Atlantic Ocean trade, and had a 38% share of the West African trade in the early 18th century, while Indian calicos were major force in Europe, and Indian textiles accounted for 20% of total English trade with Southern Europe in the early 18th century.\n\nIn early modern Europe, there was significant demand for textiles from Mughal India, including cotton textiles and silk products. European fashion, for example, became increasingly dependent on Mughal Indian textiles and silks. In the late 17th and early 18th centuries, Mughal India accounted for 95% of British imports from Asia.\n\nEmphasis was placed on the adornment of women. Even though the purdah was made compulsory for the Mughal women, we see that this did not stop themselves from experimenting in style and attire. Abul Fazal mentions that there were sixteen components that adorned a woman. These not only included clothing but also other aspects like that of oiling the body and iqtar. Mughal women wore long loose jamas with full sleeves and in winters it was accompanied by a Qaba or a Kashmir shawl used as a coat. Women were very fond of their perfumes and scents. Jewellery in the Mughal tradition signified not only religious values but also style statements.\n\nAcross North America, native people constructed clothing using natural fibers such as cotton and agave as well as leather skins from animals such as deer or beavers. When traders and colonists came from Europe, they brought with them sheep and travelers highly valued the beaver pelts in particular for their warmth. Beaver pelt trade was one of the first commercial endeavors of colonial North America and a cause of the Beaver Wars.\n\nDuring the 18th century, distinction was made between \"full dress\" worn at Court and for formal occasions, and \"undress\" or everyday, daytime clothes. As the decades progressed, fewer and fewer occasions called for full dress which had all but disappeared by the end of the century. Full dress followed the styles of the French court, where rich silks and elaborate embroidery reigned. Men continued to wear the coat, waistcoat and breeches for both full dress and undress; these were now sometimes made of the same fabric and trim, signalling the birth of the three-piece suit.\n\nWomen's silhouettes featured small, domed hoops in the 1730s and early 1740s, which were displaced for formal court wear by side hoops or panniers which later widened to as much as three feet to either side at the court of Marie Antoinette. Fashion reached heights of fantasy and abundant ornamentation, before new enthusiasms for outdoor sports and country pursuits and a long-simmering movement toward simplicity and democratization of dress under the influence of Jean-Jacques Rousseau and the American Revolution led to an entirely new mode and the triumph of British woollen tailoring following the French Revolution.\n\nFor women's dresses, Indian cottons, especially printed chintzes, were imported to Europe in large numbers, and towards the end of the period simple white muslin gowns were in fashion.\n\nDuring the industrial revolution, fabric production was mechanised with machines powered by waterwheels and steam-engines. Production shifted from small cottage based production to mass production based on assembly line organisation. Clothing production, on the other hand, continued to be made by hand.\n\nSewing machines emerged in the 19th century streamlining clothing production.\n\nAlongside these developments were changes in the types and style of clothing produced. During the 1960s, had a major influence on subsequent developments in the industry.\n\nTextiles were not only made in factories. Before this, they were made in local and national markets. Dramatic change in transportation throughout the nation is one source that encouraged the use of factories. New advances such as steamboats, canals, and railroads lowered shipping costs which caused people to buy cheap goods that were produced in other places instead of more expensive goods that were produced locally. Between 1810 and 1840, the development of a national market prompted manufacturing which tripled the output’s worth. This increase in production created a change in industrial methods, such as the use of factories instead of hand made woven materials that families usually made.\n\nThe vast majority of the people who worked in the factories were women. Women went to work in textile factories for a number of reasons. Some women left home to live on their own because of crowding at home; or to save for future marriage portions. The work enabled them to see more of the world, to earn something in anticipation of marriage, and to ease the crowding within the home. They also did it to make money for family back home. The money they sent home was to help out with the trouble some of the farmers were having. They also worked in the millhouses because they could gain a sense of independence and growth as a personal goal.\n\nThe 20th century is marked by new applications for textiles as well as inventions in synthetic fibers and computerized manufacturing control systems.\n\nIn the early 20th century, workers in the clothing and textile industries became unionized in the United States. Later in the 20th century, the industry had expanded to such a degree that such educational institutions as UC Davis established a Division of Textiles and Clothing, The University of Nebraska-Lincoln also created a Department of Textiles, Clothing and Design that offers a Masters of Arts in Textile History, and Iowa State University established a Department of Textiles and Clothing that features a History of costume collection, 1865–1948. Even high school libraries have collections on the history of clothing and textiles.\n\nThe changing lifestyles, activities, and demands of the 20th century favored clothing producers who could more effectively make their products have desired properties, such as increased strength, elasticity, or durability. These properties may be implemented through mechanical solutions, such as different weaving and knitting patterns, by modifications to the fibers, or by finishing (textiles) of the textiles. Since the 1960s, it has been possible to finish textiles to resist stains, flames, wrinkles, and microbial life. Advancement in dye technology allowed for coloring of previously difficult-to-dye natural fibers and synthetic fibers.\n\nFollowing the invention of plastics by petroleum and chemical corporations, fibers could now be made synthetically. Advancements in fiber spinning actuators and control systems allow control over fiber diameter and shape, so Synthetic fibers, may be engineered with more precision than natural fibers. Fibers invented between between 1930 and 1970 include nylon, PTFE, polyester, Spandex, and Kevlar. Clothing producers soon adopted synthetic fibers, often using blends of different fibers for optimized properties. Synthetic fibers can be knit and woven similarly to natural fibers.\n\nThe early 20th century continued the advances of the Industrial Revolution. In The procedural loops required for mechanized textile knitting and weaving already used logic were encoded in punch-cards and tapes. Since the machines were already computers, the invention of small-scale electronics and microcontrollers did not immediately change the possible functions of these machines. In the 1960s, existing machines became outfitted with computerized numeric control (CNC) systems, enabling more accurate and efficient actuation. In 1983, Bonas Machine Company Ltd. presented the first computer-controlled, electronic, Jacquard loom.. In 1988, the first US patent was awarded for a \"pick and place\" robot. Advancements such as these changed the nature of work for machine operators, introducing computer literacy as a skill alongside machine literacy. Advances in sensing technology and data processing of the 20th century include the spectrophotometer for color matching and automatic inspection machines.\n\nIn the 2010s, the global textile industry has come under fire for unsustainable practices. The textile industry is shows to have negative environmental impact at most stages in the production process.\n\nGlobal trade of secondhand clothing have promise for reducing landfill use, however international relations and challenges to textile recycling keep the market small compared to total clothing use.\n\nAdvancements in textile treatment, coating, and dyes have unclear affects in human health, and textile contact dermatitis is increasing in prevalence among textile workers and clothing consumers. \n\nScholars have identified an increase in the rate at which western consumers purchase new clothing, as well as a decrease in the lifespan of clothing. Fast fashion has been suggested to contribute to increased levels of textile waste. \n\nThe worldwide market for textiles and apparel exports in 2013 according to United Nations Commodity Trade Statistics Database stood at $772 billion.\n\nIn 2016, the largest apparel exporting nations were China ($161 billion), Bangladesh ($28 billion), Vietnam ($25 billion), India ($18 billion), Hong Kong ($16 billion), Turkey ($15 billion) and Indonesia ($7 billion).\n\n\n\n"}
{"id": "40016715", "url": "https://en.wikipedia.org/wiki?curid=40016715", "title": "In-space propulsion technologies", "text": "In-space propulsion technologies\n\nProposed in-space propulsion technologies describe the propulsion technologies that could meet future space science and exploration needs. These propulsion technologies are intended to provide effective exploration of our Solar System and will permit mission designers to plan missions to \"fly anytime, anywhere, and complete a host of science objectives at the destinations\" and with greater reliability and safety. With a wide range of possible missions and candidate propulsion technologies, the question of which technologies are \"best\" for future missions is a difficult one. A portfolio of propulsion technologies should be developed to provide optimum solutions for a diverse set of missions and destinations.\n\nIn-space propulsion begins where the upper stage of the launch vehicle leaves off; performing the functions of primary propulsion, reaction control, station keeping, precision pointing, and orbital maneuvering. The main engines used in space provide the primary propulsive force for orbit transfer, planetary trajectories and extra planetary landing and ascent. The reaction control and orbital maneuvering systems provide the propulsive force for orbit maintenance, position control, station keeping, and spacecraft attitude control.\n\nA large fraction of the rocket engines in use today are chemical rockets; that is, they obtain the energy needed to generate thrust by chemical reactions to create a hot gas that is expanded to produce thrust. A significant limitation of chemical propulsion is that it has a relatively low specific impulse (Isp), which is the ratio of the thrust produced to the mass of propellant needed at a certain rate of flow.\n\nA significant improvement (above 30%) in specific impulse can be obtained by using cryogenic propellants, such as liquid oxygen and liquid hydrogen, for example. Historically, these propellants have not been applied beyond upper stages. Furthermore, numerous concepts for advanced propulsion technologies, such as electric propulsion, are commonly used for station keeping on commercial communications satellites and for prime propulsion on some scientific space missions because they have significantly higher values of specific impulse. However, they generally have very small values of thrust and therefore must be operated for long durations to provide the total impulse required by a mission.\n\nSeveral of these technologies offer performance that is significantly better than that achievable with chemical propulsion.\n\nIn-space propulsion represents technologies that can significantly improve a number of critical aspects of the mission. Space exploration is about getting somewhere safely (mission enabling), getting there quickly (reduced transit times), getting a lot of mass there (increased payload mass), and getting there cheaply (lower cost). The simple act of \"getting\" there requires the employment of an in-space propulsion system, and the other metrics are modifiers to this fundamental action.\n\nDevelopment of technologies will result in technical solutions that improve thrust levels, Isp, power, specific mass, (or specific power), volume, system mass, system complexity, operational complexity, commonality with other spacecraft systems, manufacturability, durability, and cost. These types of improvements will yield decreased transit times, increased payload mass, safer spacecraft, and decreased costs. In some instances, development of technologies within this TA will result in mission- enabling breakthroughs that will revolutionize space exploration. There is no single propulsion technology that will benefit all missions or mission types. The requirements for in-space propulsion vary widely due according to their intended application. The described technologies should support everything from small satellites and robotic deep space exploration to space stations and human missions to Mars applications.\n\nThe technology areas are divided into four basic groups: (1) Chemical propulsion, (2) Nonchemical propulsion, (3) Advanced propulsion technologies, and (4) Supporting technologies; based on the physics of the propulsion system and how it derives thrust as well as its technical maturity. Additionally, there may be credible meritorious in-space propulsion concepts not foreseen or reviewed at the time of publication, and which may be shown to be beneficial to future mission applications.\n\nFurthermore, the term \"mission pull\" defines a technology or a performance characteristic necessary to meet a planned NASA mission requirement. Any other relationship between a technology and a mission (an alternate propulsion system, for example) is categorized as \"technology push.\" Also, a space demonstration refers to the spaceflight of a scaled version of a particular technology or of a critical technology subsystem. On the other hand, a space validation would serve as a qualification flight for future mission implementation. A successful validation flight would not require any additional space testing of a particular technology before it can be adopted for a science or exploration mission.\n\nFor both human and robotic exploration, traversing the solar system is a struggle against time and distance. The most distant planets are 4.5–6 billion kilometers from the Sun and to reach them in any reasonable time requires much more capable propulsion systems than conventional chemical rockets. Rapid inner solar system missions with flexible launch dates are difficult, requiring propulsion systems that are beyond today's current state of the art. The logistics, and therefore the total system mass required to support sustained human exploration beyond Earth to destinations such as the Moon, Mars or Near Earth Objects,\nare daunting unless more efficient in-space propulsion technologies are developed and fielded.\n\nWork being performed at the Glenn Research Center develops primary propulsion technologies that can benefit near and mid-term science missions by reducing cost, mass and/or travel times. The In-Space Program is working to develop next generation electric propulsion technologies, including Ion and Hall thrusters. Solar Sails, which are a form of propellantless propulsion, are also being developed. Solar sails rely on the naturally occurring sunlight for the propulsion energy. Other propulsion technologies being developed include advanced chemical propulsion and aerocapture.\n\n"}
{"id": "2283598", "url": "https://en.wikipedia.org/wiki?curid=2283598", "title": "Infrared multiphoton dissociation", "text": "Infrared multiphoton dissociation\n\nInfrared multiple photon dissociation (IRMPD) is a technique used in mass spectrometry to fragment molecules in the gas phase usually for structural analysis of the original (parent) molecule.\n\nAn infrared laser is directed through a window into the vacuum of the mass spectrometer where the ions are. The mechanism of fragmentation involves the absorption by a given ion of multiple infrared photons. The parent ion becomes excited into more energetic vibrational states until a bond(s) is broken resulting in gas phase fragments of the parent ion. In the case of powerful laser pulses, the dissociation proceeds via inner-valence ionization of electrons.\n\nIRMPD is most often used in Fourier transform ion cyclotron resonance mass spectrometry.\n\nBy applying intense tunable IR lasers, like IR-OPOs or IR free electron lasers, the wavelength dependence of the IRMPD yield can be studied. This infrared photodissociation spectroscopy allows for the measurement of vibrational spectra of (unstable) species that can only be prepared in the gas phase. Such species include molecular ions but also neutral species like metal clusters that can be gently ionized after interaction with the IR light for their mass spectrometric detection.\n\nDue to the relatively large differences in IR absorption frequencies that are due to different resonance frequencies for molecules containing different isotopes, this technique has been suggested as a way to perform Isotope separation with difficult-to-separate isotopes, in a single pass. For example, molecules of UF containing U-235 might be ionized completely as a result of such a laser resonance, leaving UF containing the heavier U-238 intact.\n\n"}
{"id": "6593042", "url": "https://en.wikipedia.org/wiki?curid=6593042", "title": "Laurence F. Johnson", "text": "Laurence F. Johnson\n\nLarry Johnson (born December 17, 1950, in Corpus Christi, Texas) is an American futurist, author, and educator. Currently, Johnson serves as the Founder and CEO of EdFutures.org, an international think tank, and as a Senior Fellow of the Center for Digital Education. From 2001-2016, he served as Chief Executive Officer of the New Media Consortium an international consortium of hundreds of universities, colleges, museums, research centers, and technology companies.\n\nThe annual Horizon Report is the most visible component of the Horizon Project, which Johnson founded and led from its inception in 2002 until 2016. The report has since become one of the leading tools used by senior executives in universities and museums to set priorities for technology planning in more than 160 countries.\n\nHe served for nearly 15 years as Chief Executive Officer of the NMC and provided leadership in the areas of strategic planning; program development; fundraising; partnerships with business and industry; the development and management of fiscal and human resources; and policy. Under Johnson's tenure, the New Media Consortium (NMC) grew to be an international not-for-profit consortium of learning-focused organizations dedicated to the exploration and use of new media and new technologies. Its hundreds of member institutions still constitute an elite list of the most highly regarded colleges and universities in the world, as well as leading museums, key research centers, and some of the world's most forward-thinking companies. Under his leadership, the consortium and its members dedicated themselves to exploring and developing potential applications of emerging technologies for learning, research, and creative inquiry. The consortium's Horizon Reports, which Johnson founded and directed for more than a decade, are still regarded worldwide as the most timely and authoritative sources of information on new and emerging technologies available to education anywhere.\n\nJohnson has organized summits and large-scale projects around topics such as Visual literacy, learning objects, educational gaming, the future of scholarship, and the 3D web.\n\nIn April 2008, Johnson presented testimony to the House Subcommittee on Telecommunications and the Internet on the nature and state of virtual worlds.\n\nIn 2009, Johnson helped the NMC and the Edward and Betty Marcus Foundation formalize their longstanding collaboration in support of the visual arts in Texas with the launch of the Edward and Betty Marcus Institute for Digital Education in the Arts (MIDEA), and served as its founding director. An outgrowth of the earlier Digital Education Project for Texas Art Museums, MIDEA built on that four-year systemic effort to increase the capacity of museums across Texas and beyond to use new media to tell compelling stories about art and their collections. The project provides a hub for Texas museum professionals to learn about and discuss all forms of digital media, as well as ongoing training and support for digital arts education. (The project continues its unique work with Texas art museums that has become known as the Texas Testbed.) The learnings from this project inform the national and international efforts of many museums and museum-based organizations all over the world.\n\nAn author of several books, numerous chapters, dozens of articles, and principal investigator for several important national and international studies, he has been recognized for his research by the American Association of Community Colleges and the American Association of University Administrators.\n\nIn 2016, Johnson marked 35 years of service in higher education, and currently serves as the Founder and CEO of EdFutures.org, an international think tank, and as a Senior Fellow of the Center for Digital Education. From 2001-2016, he served as CEO of the New Media Consortium. He served previously as president and CEO of Fox Valley Technical College, a community college serving more than 20,000 FTE in Appleton, Wisconsin. His experiential base includes service at both very large and very small institutions and positions at every level and across all the major areas of college and university work.\n\nBetween 1993 and 1996, he served as vice president for the League for Innovation in the Community College, working at the national level to take the story of community colleges to governmental, foundation, and corporate leaders across the country. As director of the League's Information Technology Initiative, he coordinated what was at the time the world’s largest higher education technology conference, the international Conference of Information Technology.\n\nHe has served on a number of boards, including his current role on the governing board of the Institute for Learning Innovation and the Leadership Advisory Board of the Center for Learning Innovation and Custimized Knowledge Solutions in Dubai. Previously he served on the NMC Board of Directors, the Adobe Systems Higher Education Advisory Board, the Advanced Defense Learning Initiative National Advisory Board, the virtual International Spaceflight Museum, and the Academic Commons Board of Directors.\n\n\n\n\n"}
{"id": "19358228", "url": "https://en.wikipedia.org/wiki?curid=19358228", "title": "Lawn sweeper", "text": "Lawn sweeper\n\nA lawn sweeper, also known as a leaf sweeper, is a garden tool for the mechanical removal of debris, such as fallen leaves, pine needles, twigs, grass clippings or litter, from a lawn or paved area. Lawn sweepers operate via a rotating brush mechanism that sweeps up the debris and deposits it in a collection hopper for disposal.\n\nPush lawn sweepers, which resemble a simple wheeled manual lawn mower, are maneuvered and powered by hand. A forward pushing action transfers power from the wheels to the brush mechanism via a gearing system, causing it to rotate, and the debris is picked up and transferred to a bag or hopper mounted on the machine. Powered lawn sweepers resemble push lawn sweepers, but the brush mechanism is powered by a gasoline or electric motor. Tow lawn sweepers are towed behind a vehicle, such as a garden tractor or ATV and are designed for use in larger areas. They are generally wider and have greater hopper capacities than push and powered lawn sweepers.\n\nLawn sweepers are an alternative to raking or the use of leaf blowers and garden vacuums. They are also an alternative to lawn mower baggers or grass baggers which are generally more expensive and have a smaller capacity.\n\n"}
{"id": "57337824", "url": "https://en.wikipedia.org/wiki?curid=57337824", "title": "Lifeline (safety)", "text": "Lifeline (safety)\n\nA lifeline is a fall protection safety device in the form of an open fence composed of wire and stanchions secured around the perimeter of an area to prevent accidental falls. It is commonly found on sailboats and construction sites, as well as other situations where dangerous falls can occur, such as at scenic overlooks and in caves.\n\nLifeline wire has traditionally been available in 1/8\" and 3/16\" wire diameters, with a soft but durable vinyl coating increasing the effective diameters to 7/32\" and 5/16\" respectively. It\nshould be inspected on an annual basis for signs of corrosion or breakage of wire strands.\n\nIn simplest form, a land-based lifeline consists of a horizontal wire rope cable attached to two or more anchor points on a roof-top, crane runway, bridge or outdoor construction site, or any other elevated work area that poses a fall risk. OSHA defines an anchorage in a fall protection system \"a secure point of attachment for lifelines, lanyards, or deceleration devices\".\n\nConstruction site lifeline systems include dedicated attachment brackets, safety lanyards and harnesses. Construction lifeline systems may be subdivided into those used to arrest workers in the event of a fall (active systems), or restrain workers from reaching a fall hazard (restraint systems). \n"}
{"id": "2938583", "url": "https://en.wikipedia.org/wiki?curid=2938583", "title": "List of auto parts", "text": "List of auto parts\n\nThis is a list of automotive parts mostly for vehicles using internal combustion engines which are manufactured components of automobiles:\n\n [speed of meter sensor]\n\nBack seat\n\nSee also Air conditioning/Automobile air conditioners systems.\n\n\n\n\n"}
{"id": "2366889", "url": "https://en.wikipedia.org/wiki?curid=2366889", "title": "M4 corridor", "text": "M4 corridor\n\nThe M4 corridor is an area in the United Kingdom adjacent to the M4 motorway, which runs from London to South Wales. It is a major high-technology hub. Important cities and towns linked by the M4 include (from east to west) London, Slough, Bracknell, Maidenhead, Reading, Newbury, Swindon, Bath, Bristol, Newport, Cardiff, and Swansea. The area is also served by the Great Western Main Line, including the South Wales Main Line, and London Heathrow Airport. Technology companies with major operations located in the area include Amazon, Citrix Systems, Dell, Alcatel-Lucent, Huawei, Lexmark, LG, Microsoft, Novell, Nvidia, O2, Panasonic, SAP, and Symantec.\n\nThe eastern end of the English M4 corridor is home to a large number of technology companies, particularly in Berkshire, Swindon and the Thames Valley. For this reason this part of the M4 Corridor is sometimes described as England's \"Silicon Valley\". Slough, Windsor, Maidenhead, Reading, Bracknell and Newbury are the main towns in the Berkshire stretch of the M4.\n\nReading is home to many information technology and financial services businesses, including Cisco, Microsoft, ING Direct, Oracle, Prudential, Yell Group and Ericsson. Vodafone has a major corporate campus in Newbury, O2 plc is in Slough. Maidenhead is the home of Hutchison 3G UK's Headquarters and Tesla Motors UK Head Office.\n\nInvestment has gradually spread westwards since the 1980s. In the west the interchange of the M4 motorway and M5 motorway at the Almondsbury Interchange near Bristol had seen considerable growth of industries by the mid 1990s, in adjoining areas.\n\nThe major Welsh towns and cities along the M4 corridor are Bridgend, Cardiff, Llanelli, Neath, Newport, Port Talbot and Swansea. South Wales is an industrial heartland of the UK.\nThe 1980s and 1990s saw the development of the Swansea Enterprise Park. The Celtic Manor Resort located adjacent to the M4 in Newport has undergone significant investment and successfully hosted the 2010 Ryder Cup. Newport has seen significant growth in the electronics industry since the late 1980s and is home to factories for electronics firms such as Alcatel. The 1990s also saw significant investment in Cardiff such as in Cardiff Gate and the Cardiff Bay area. One site of note on the M4 Corridor is Port Talbot Steelworks - the largest steel producer in the UK and one of the biggest steel producers in Europe.\n\nThe opening of the Second Severn Crossing in 1996 resulted in the previous M4 and bridge, serving Chepstow, being renumbered the M48, although the area is still generally considered as falling within the M4 corridor.\n\nSince the start of the 21st Century there has been evidence of more investment west of Cardiff, such as:\n\n\n"}
{"id": "8892368", "url": "https://en.wikipedia.org/wiki?curid=8892368", "title": "Ni1000", "text": "Ni1000\n\nThe Ni1000 is an artificial neural network chip developed by Nestor Corporation. The chip is aimed at image analysis applications, contains more than 3 million transistors and can analyze patterns at the rate of 40,000 per second. Prototypes running with Nestor's OCR software in 1994 were capable of recognizing around 100 handwritten characters per second.\n\n"}
{"id": "5721504", "url": "https://en.wikipedia.org/wiki?curid=5721504", "title": "Nicolas Appert Award", "text": "Nicolas Appert Award\n\nThe Nicolas Appert Award is awarded by the Chicago Section of the Institute of Food Technologists for preeminence in and contributions to the field of food technology. The award has been given annually since 1942 and is named after Nicolas Appert, the French inventor of airtight food preservation. Award winners receive a bronze medal with a front view of Appert and a $5000 honorarium. This is considered one of the highest honors in food technology.\n\nSource: IFT\n"}
{"id": "31102976", "url": "https://en.wikipedia.org/wiki?curid=31102976", "title": "Noguchi table", "text": "Noguchi table\n\nThe Noguchi table is a piece of modernist furniture first produced in the mid-20th century. Introduced by Herman Miller in 1947, it was designed in the United States by Japanese American artist and industrial designer Isamu Noguchi. The Noguchi table comprises a wooden base composed of two identical curved wood pieces, and a heavy plate glass top.\n\nThe Noguchi table was an evolution of a rosewood and glass table Noguchi designed in 1939 for A. Conger Goodyear, president of the Museum of Modern Art. The design team at Herman Miller was so impressed by the table's use of biomorphism that they recruited Noguchi to design a similar table with a freeform sculptural base and biomorphic glass top for use in both residential and office environments. \n\nThe 1947 Herman Miller catalog described the Noguchi coffee table as \"sculpture-for-use\" and \"design for production\". The base was carved from solid walnut, and consisted of two identical parts; when one part \"is reversed and connected to the other by a pivot rod, a base appears which has a smoothly flowing form and an interest rarely found in furniture of any period\". The shape of the two wooden supports produces a self-supporting and stable base, allowing the heavy plate glass top to be placed without the use of connectors.\n\nThe base was originally produced in walnut, birch, and cherry. It was later offered in ebonized walnut. Cherry bases were made only during the first year the table was on the market, and have been highly sought since. Birch bases were discontinued after 1954. As of 2016, the table is available in an ebonized finish, walnut, white ash and natural cherry.\n\nThe top was originally issued in heavy plate glass. In 1965, the thickness of the top was reduced to , and its base height was raised, increasing the table's total height from to .\n\nSince the late 1980s, indexing pins have been installed on the pivot rod with matching slots milled into the legs to ensure that the two leg elements are set up at a 52-degree angle for maximum aesthetic appeal and optimal stability.\n\nThe Noguchi table became one of Herman Miller's most iconic and successful designs. Production ceased in 1973, and the piece became an instant collectible. Herman Miller reissued it in 1980 in a limited edition of about 480 tables. The table was reintroduced again in 1984 for the \"Herman Miller Classics\" line, and has been in production ever since.\n\nDespite their status as modern classics, Noguchi tables are widely available and relatively affordable. This is at least partly because they were in constant production from 1947 until 1973, returned to production in 1984, and have been produced ever since. In addition, the table is very durable, and few have been lost over the years. The base can be dinged and scratched but almost never cracks or breaks. The glass tops are prone to chipping along the edges and scratching on the upper surface, but are so large and heavy they rarely break. The table can support a great weight without damage. Earlier tables are easily distinguished by their ⅞-inch thick tops, but do not command much premium over the current lighter and easier-to-handle ¾″ models. Buyers can expect to pay $500 and up for an undamaged example, and $1,500 and up for an early version in birch. Only the 1947 cherry tables are truly rare collectibles, which rarely show up for sale except at high-end auctions.\n\n"}
{"id": "897118", "url": "https://en.wikipedia.org/wiki?curid=897118", "title": "Panicum virgatum", "text": "Panicum virgatum\n\nPanicum virgatum, commonly known as switchgrass, is a perennial warm season bunchgrass native to North America, where it occurs naturally from 55°N latitude in Canada southwards into the United States and Mexico. Switchgrass is one of the dominant species of the central North American tallgrass prairie and can be found in remnant prairies, in native grass pastures, and naturalized along roadsides. It is used primarily for soil conservation, forage production, game cover, as an ornamental grass, in phytoremediation projects, fiber, electricity, heat production, for biosequestration of atmospheric carbon dioxide, and more recently as a biomass crop for ethanol and butanol.\n\nOther common names for switchgrass include tall panic grass, Wobsqua grass, blackbent, tall prairiegrass, wild redtop, thatchgrass, and Virginia switchgrass.\n\nSwitchgrass is a hardy, deep-rooted, perennial rhizomatous grass that begins growth in late spring. It can grow up to high, but is typically shorter than big bluestem grass or indiangrass. The leaves are long, with a prominent midrib. Switchgrass uses C carbon fixation, giving it an advantage in conditions of drought and high temperature. Its flowers have a well-developed panicle, often up to 60 cm long, and it bears a good crop of seeds. The seeds are 3–6 mm long and up to 1.5 mm wide, and are developed from a single-flowered spikelet. Both glumes are present and well developed. When ripe, the seeds sometimes take on a pink or dull-purple tinge, and turn golden brown with the foliage of the plant in the fall. Switchgrass is both a perennial and self-seeding crop, which means farmers do not have to plant and reseed after annual harvesting. Once established, a switchgrass stand can survive for ten years or longer. Unlike corn, switchgrass can grow on marginal lands and requires relatively modest levels of chemical fertilizers. Overall, it is considered a resource-efficient, low-input crop for producing bioenergy from farmland.\n\nMuch of North America, especially the prairies of the Midwestern United States, was once prime habitat to vast swaths of native grasses, including switchgrass, indiangrass (\"Sorghastrum nutans\"), eastern gamagrass (\"Tripsacum dactyloides\"), big bluestem (\"Andropogon gerardi\"), little bluestem (\"Schizachyrium scoparium\") and others. As European settlers began spreading west across the continent, the native grasses were plowed under and the land converted to crops such as corn, wheat, and oats. Introduced grasses such as fescue, bluegrass, and orchardgrass also replaced the native grasses for use as hay and pasture for cattle.\n\nSwitchgrass is a versatile and adaptable plant. It can grow and even thrive in many weather conditions, lengths of growing seasons, soil types, and land conditions. Its distribution spans south of latitude 55°N from Saskatchewan to Nova Scotia, south over most of the United States east of the Rocky Mountains, and further south into Mexico. As a warm-season perennial grass, most of its growth occurs from late spring through early fall; it becomes dormant and unproductive during colder months. Thus, the productive season in its northern habitat can be as short as three months, but in the southern reaches of its habitat the growing season may be as long as eight months, around the Gulf Coast area.\n\nSwitchgrass is a diverse species, with striking differences between plants. This diversity, which presumably reflects evolution and adaptation to new environments as the species spread across the continent, provides a range of valuable traits for breeding programs. Switchgrass has two distinct forms, or \"cytotypes\": the lowland cultivars, which tend to produce more biomass, and the upland cultivars, which are generally of more northern origin, more cold-tolerant, and therefore usually preferred in northern areas. Upland switchgrass types are generally shorter (≤ 2.4 m, tall) and less coarse than lowland types. Lowland cultivars may grow to ≥ 2.7 m, in favorable environments. Both upland and lowland cultivars are deeply rooted (> 1.8 m, in favorable soils) and have short rhizomes. The upland types tend to have more vigorous rhizomes, so the lowland cultivars may appear to have a bunchgrass habit, while the upland types tend to be more sod-forming. Lowland cultivars appear more plastic in their morphology, produce larger plants if stands become thin or when planted in wide rows, and they seem to be more sensitive to moisture stress than upland cultivars.\n\nIn native prairies, switchgrass is historically found in association with several other important native tallgrass prairie plants, such as big bluestem, indiangrass, little bluestem, sideoats grama, eastern gamagrass, and various forbs (sunflowers, gayfeather, prairie clover, and prairie coneflower). These widely adapted tallgrass species once occupied millions of hectares.\n\nSwitchgrass’ suitability for cultivation in the Gran Chaco is being studied by Argentina’s Instituto Nacional de Tecnología Agropecuaria (INTA).\n\nSwitchgrass can be grown on land considered unsuitable for row crop production, including land that is too erodible for corn production, as well as sandy and gravelly soils in humid regions that typically produce low yields of other farm crops. No single method of establishing switchgrass can be suggested for all situations. The crop can be established both by no-till and conventional tillage. When seeded as part of a diverse mixture, planting guidelines for warm-season grass mixtures for conservation plantings should be followed. Regional guidelines for growing and managing switchgrass for bioenergy or conservation plantings are available. Several key factors can increase the likelihood of success for establishing switchgrass. These include:\nMowing and properly labeled herbicides are recommended for weed control. Chemical weed control can be used in the fall prior to establishment, or before or after planting. Weeds should be mowed just above the height of the growing switchgrass. Hormone herbicides, such as 2,4-D, should be avoided as they are known to reduce development of switchgrass when applied early in the establishing year. Plantings that appear to have failed due to weed infestations are often wrongly assessed, as the failure is often more apparent than real. Switchgrass stands that are initially weedy commonly become well established with appropriate management in subsequent years.\nOnce established, switchgrass can take up to three years to reach its full production potential. Depending on the region, it can typically produce 1/4 to 1/3 of its yield potential in its first year and 2/3 of its potential in the year after seeding.\n\nAfter establishment, switchgrass management will depend on the goal of the seeding. Historically, most switchgrass seedings have been managed for the Conservation Reserve Program in the US. Disturbance such as periodic mowing, burning, or disking is required to optimize the stand’s utility for encouraging biodiversity. Increased attention is being placed on switchgrass management as an energy crop. Generally, the crop requires modest application of nitrogen fertilizer, as it is not a heavy feeder. Typical nitrogen (N) content of senescent material in the fall is 0.5% N. Fertilizer nitrogen applications of about 5 kg N/hectare (ha) applied for each tonne of biomass removed is a general guideline. More specific recommendations for fertilization are available regionally in North America. Herbicides are not often used on switchgrass after the seeding year, as the crop is generally quite competitive with weeds. Most bioenergy conversion processes for switchgrass, including those for cellulosic ethanol and pellet fuel production, can generally accept some alternative species in the harvested biomass. Stands of switchgrass should be harvested no more than twice per year, and one cutting often provides as much biomass as two. Switchgrass can be harvested with the same field equipment used for hay production, and it is well-suited to baling or bulk field harvesting. If its biology is properly taken into consideration, switchgrass can offer great potential as an energy crop.\n\nSwitchgrass can be used as a feedstock for biomass energy production, as ground cover for soil conservation, and to control erosion, for forages and grazing, as game cover, and as feedstock for biodegradable plastics. It can be used by cattle farmers for hay and pasture and as a substitute for wheat straw in many applications, including livestock bedding, straw bale housing, and as a substrate for growing mushrooms. Additionally, switchgrass is grown as a drought-resistant ornamental grass in average to wet soils and in full sun to part shade.\n\nIt is the preferred larval host plant of Dargida rubripennis.\n\nSwitchgrass has been researched as a renewable bioenergy crop since the mid-1980s, because it is a native perennial warm season grass with the ability to produce moderate to high yields on marginal farmlands. It is now being considered for use in several bioenergy conversion processes, including cellulosic ethanol production, biogas, and direct combustion for thermal energy applications. The main agronomic advantages of switchgrass as a bioenergy crop are its stand longevity, drought and flooding tolerance, relatively low herbicide and fertilizer input requirements, ease of management, hardiness in poor soil and climate conditions, and widespread adaptability in temperate climates. In some warm humid southern zones, such as Alabama, it has the ability to produce up to 25 oven-dry tonnes per hectare (ODT/ha). A summary of switchgrass yields across 13 research trial sites in the United States found the top two cultivars in each trial to yield 9.4 to 22.9 t/ha, with an average yield of 14.6 ODT/ha. However, these yields were recorded on small plot trials, and commercial field sites could be expected to be at least 20% lower than these results. In the United States, switchgrass yields appear to be highest in warm humid regions with long growing seasons such as the US Southeast and lowest in the dry short season areas of the Northern Great Plains. The energy inputs required to grow switchgrass are favorable when compared with annual seed bearing crops such as corn, soybean, or canola, which can require relatively high energy inputs for field operations, crop drying, and fertilization. Whole plant herbaceous perennial C4 grass feedstocks are desirable biomass energy feedstocks, as they require fewer fossil energy inputs to grow and effectively capture solar energy because of their C4 photosynthetic system and perennial nature. One study cites it takes from 0.97 to 1.34 GJ to produce 1 tonne of switchgrass, compared with 1.99 to 2.66 GJ to produce 1 tonne of corn. Another study found that switchgrass uses 0.8 GJ/ODT of fossil energy compared to grain corn's 2.9 GJ/ODT. Given that switchgrass contains approximately 18.8 GJ/ODT of biomass, the energy output-to-input ratio for the crop can be up to 20:1. This highly favorable ratio is attributable to its relatively high energy output per hectare and low energy inputs for production.\n\nConsiderable effort is being expended in developing switchgrass as a cellulosic ethanol crop in the USA. In George W. Bush's 2006 State of the Union Address, he proposed using switchgrass for ethanol; since then, over US$100 million has been invested into researching switchgrass as a potential biofuel source. Switchgrass has the potential to produce up to 380 liters of ethanol per tonne harvested. However, current technology for herbaceous biomass conversion to ethanol is about 340 liters per tonne. In contrast, corn ethanol yields about 400 liters per tonne.\n\nThe main advantage of using switchgrass over corn as an ethanol feedstock is its cost of production is generally about 1/2 that of grain corn, and more biomass energy per hectare can be captured in the field. Thus, switchgrass cellulosic ethanol should give a higher yield of ethanol per hectare at lower cost. However, this will depend on whether the cost of constructing and operating cellulosic ethanol plants can be reduced considerably. The switchgrass ethanol industry energy balance is also considered to be substantially better than that of corn ethanol. During the bioconversion process, the lignin fraction of switchgrass can be burned to provide sufficient steam and electricity to operate the biorefinery. Studies have found that for every unit of energy input needed to create a biofuel from switchgrass, four units of energy are yielded. In contrast, corn ethanol yields about 1.28 units of energy per unit of energy input. A recent study from the Great Plains indicated that for ethanol production from switchgrass, this figure is 6.4, or alternatively, that 540% more energy was contained in the ethanol produced than was used in growing the switchgrass and converting it to liquid fuel. However, there remain commercialization barriers to the development of cellulosic ethanol technology. Projections in the early 1990s for commercialization of cellulosic ethanol by the year 2000 have not been met. The commercialization of cellulosic ethanol is thus proving to be a significant challenge, despite noteworthy research efforts.\n\nThermal energy applications for switchgrass appear to be closer to near-term scale-up than cellulosic ethanol for industrial or small-scale applications. For example, switchgrass can be pressed into fuel pellets that are subsequently burned in pellet stoves used to heat homes (which typically burn corn or wood pellets). Switchgrass has been widely tested as a substitute for coal in power generation. The most widely studied project to date has been the Chariton Valley Project in Iowa. The Show-Me-Energy Cooperative (SMEC) in Missouri is using switchgrass and other warm-season grasses, along with wood residues, as feedstocks for pellets used for the firing of a coal-fired power plant. In Eastern Canada, switchgrass is being used on a pilot scale as a feedstock for commercial heating applications. Combustion studies have been undertaken and it appears to be well-suited as a commercial boiler fuel. Research is also being undertaken to develop switchgrass as a pellet fuel because of lack of surplus wood residues in eastern Canada, as a slowdown in the forest products industry in 2009 is now resulting in wood pellet shortages throughout Eastern North America. Generally speaking, the direct firing of switchgrass for thermal applications can provide the highest net energy gain and energy output-to-input ratio of all switchgrass bioconversion processes. Research has found switchgrass, when pelletized and used as a solid biofuel, is a good candidate for displacing fossil fuels. Switchgrass pellets were identified to have a 14.6:1 energy output-to-input ratio, which is substantially better than that for liquid biofuel options from farmland. As a greenhouse gas mitigation strategy, switchgrass pellets were found to be an effective means to use farmland to mitigate greenhouse gases on the order of 7.6-13 tonnes per hectare of CO. In contrast, switchgrass cellulosic ethanol and corn ethanol were found to mitigate 5.2 and 1.5 tonnes of CO per hectare, respectively.\n\nHistorically, the major constraint to the development of grasses for thermal energy applications has been the difficulty associated with burning grasses in conventional boilers, as biomass quality problems can be of particular concern in combustion applications. These technical problems now appear to have been largely resolved through crop management practices such as fall mowing and spring harvesting that allow for leaching to occur, which leads to fewer aerosol-forming compounds (such as K and Cl) and N in the grass. This reduces clinker formation and corrosion, and enables switchgrass to be a clean combustion fuel source for use in smaller combustion appliances. Fall harvested grasses likely have more application for larger commercial and industrial boilers. Switchgrass is also being used to heat small industrial and farm buildings in Germany and China through a process used to make a low quality natural gas substitute.\n\nBai et al. (2010) conducted a study to analyze the environmental sustainability of using switchgrass plant material as a feedstock for ethanol production. Life cycle analysis was used to make this assessment. They compared efficiency of E10, E85, and ethanol with gasoline. They took into account air and water emissions associated with growing, managing, processing and storing the switchgrass crop. They also factored in the transportation of the stored switchgrass to the ethanol plant where they assumed the distance was 20 km. The reductions in global warming potential by using E10 and E85 were 5 and 65%, respectively. Their models also suggested that the “human toxicity potential” and “eco-toxicity potential” were substantially greater for the high ethanol fuels (i.e., E85 and ethanol) than for gasoline and E10.\n\nIn 2014, a genetically altered form of the bacterium Caldicellulosiruptor bescii was created which can cheaply and efficiently turn switchgrass into ethanol.\n\nIn a novel application, US scientists have genetically modified switchgrass to enable it to produce polyhydroxybutyrate, which accumulates in beadlike granules within the plant's cells. In preliminary tests, the dry weight of a plants leaves were shown to comprise up to 3.7% of the polymer. Such low accumulation rates do not, as of 2009, allow for commercial use of switchgrass as a biosource.\n\nSwitchgrass is useful for soil conservation and amendment, particularly in the United States and Canada, where switchgrass is endemic. Switchgrass has a deep fibrous root system – nearly as deep as the plant is tall. Since it, along with other native grasses and forbs, once covered the plains of the United States that are now the Corn Belt, the effects of the past switchgrass habitat have been beneficial, lending to the fertile farmland that exists today. The deep fibrous root systems of switchgrass left a deep rich layer of organic matter in the soils of the Midwest, making those mollisol soils some of the most productive in the world. By returning switchgrass and other perennial prairie grasses as an agricultural crop, many marginal soils may benefit from increased levels of organic material, permeability, and fertility, due to the grass's deep root system.\n\nSoil erosion, both from wind and water, is of great concern in regions where switchgrass grows. Due to its height, switchgrass can form an effective wind erosion barrier. Its root system, also, is excellent for holding soil in place, which helps prevent erosion from flooding and runoff.\nSome highway departments (for example, KDOT) have used switchgrass in their seed mixes when re-establishing growth along roadways. It can also be used on strip mine sites, dikes, and pond dams. Conservation districts in many parts of the United States use it to control erosion in grass waterways because of its ability to anchor soils while providing habitat for wildlife.\n\nSwitchgrass is an excellent forage for cattle; however, it has shown toxicity in horses, sheep, and goats through chemical compounds known as saponins, which cause photosensitivity and liver damage in these animals. Researchers are continuing to learn more about the specific conditions under which switchgrass causes harm to these species, but until more is discovered, it is recommended switchgrass not be fed to them. For cattle, however, it can be fed as hay, or grazed.\n\nGrazing switchgrass calls for watchful management practices to ensure survival of the stand. It is recommended that grazing begin when the plants are about 50 cm tall, and that grazing be discontinued when the plants have been eaten down to about 25 cm, and to rest the pasture 30 – 45 days between grazing periods. Switchgrass becomes stemmy and unpalatable as it matures, but during the target grazing period, it is a favorable forage with a relative feed value (RFV) of 90-104. The grass's upright growth pattern places its growing point off the soil surface onto its stem, so leaving 25 cm of stubble is important for regrowth. When harvesting switchgrass for hay, the first cutting occurs at the late boot stage – around mid-June. This should allow for a second cutting in mid-August, leaving enough regrowth to survive the winter.\n\nSwitchgrass is well-known among wildlife conservationists as good forage and habitat for upland game bird species, such as pheasant, quail, grouse, and wild turkey, and song birds, with its plentiful small seeds and tall cover. A study published in 2015 has shown that switchgrass, when grown in a traditional monoculture, has an adverse impact on some wildlife. Depending on how thickly switchgrass is planted, and what it is partnered with, it also offers excellent forage and cover for other wildlife across the country. For those producers who have switchgrass stands on their farm, it is considered an environmental and aesthetic benefit due to the abundance of wildlife attracted by the switchgrass stands. Some members of Prairie Lands Bio-Products, Inc. in Iowa have even turned this benefit into a profitable business by leasing their switchgrass land for hunting during the proper seasons. The benefits to wildlife can be extended even in large-scale agriculture through the process of strip harvesting, as recommended by The Wildlife Society, which suggests that rather than harvesting an entire field at once, strip harvesting could be practiced so that the entire habitat is not removed, thereby protecting the wildlife inhabiting the switchgrass.\n\n\n"}
{"id": "4997322", "url": "https://en.wikipedia.org/wiki?curid=4997322", "title": "Pencil detonator", "text": "Pencil detonator\n\nA pencil detonator or time pencil is a time fuze designed to be connected to a detonator or short length of safety fuse. They are about the same size and shape as a pencil, hence the name. They were introduced during World War II.\n\nOne type, the British Number Ten Delay Switch (official name, \"Switch, No. 10, Delay\" and often referred to as a \"timing pencil\"), was made of a brass (or in later versions aluminium) tube, with a copper section at one end which contained a glass vial of cupric chloride (the liquid was widely and erroneously reported to be sulfuric acid), while beneath the vial was a spring-loaded striker under tension and held in place by a thin metal wire. The timer was started by crushing the copper section of the tube to break the vial of cupric chloride, which then began to slowly erode the wire holding back the striker. When the wire eventually parted, the striker was propelled down the hollow centre of the detonator, hitting the percussion cap at the other end of the detonator.\n\nNumber ten delay switches had delays ranging from 10 minutes to 24 hours and were accurate to within plus or minus two or three minutes in an hour's delay, and plus or minus an hour in a 12-hour delay, though environmental conditions could affect this. The switches were typically issued in packs of five, all the switches in a pack having the same delay. In use, two switches with the same delay (from different packs if possible) would be placed in the explosive charge in case one switch failed.\n\nAnother, superior type, was developed by Millis Jefferis of MD1 known as the \"Lead Delay switch\" or officially \"Switch, No. 9, L Delay\". Instead of relying on the action of acid on metal (which was subject to temperature variation), it used a piece of metal under stress - the metal in question being a lead alloy that was extremely affected by mechanical creep. A piece of this lead was notched to a set diameter, the diameter setting the time delay. When the starting pin was removed, this wire was placed under tension by the spring-loaded striker, and began to gradually stretch. After certain time it would snap at the notch and allow the striker to hit the percussion cap.\n\nThe delay could be set from a matter of minutes to hours. Manufacture was entirely by MD1. \nGenerally speaking L-delays were slightly less reliable and had shorter delays, but were more reliable underwater (if a No. 10 fuze developed a leak, water would dilute the corrosive liquid and increase the delay or stop the fuze from working).\n\nAnother type of time pencil had a percussion cap but no detonator attached. Instead there was a crimping attachment at one end to allow pyrotechnic fuse to be crimped on. When a time pencil of this type fired, it would light the fuse which would burn towards a detonator crimped onto the other end. Because standard safety fuse burns at around half a metre per minute, it is not practicable to provide delays of more than a few minutes in this way. It was also possible to connect a pencil detonator to so-called \"instantaneous fuse\" (not to be confused with detonating cord) which had an unusually fast burn rate of over 7 metres per second.\n\nPencil detonators are colour-coded to indicate the \"nominal\" time delay, which can range from 10 minutes through to 24 hours. No. 10 delays were normally issued in a tin of 5, all of the same delay, while L-delays were issued in a larger tin which included a mixture of different delays to suit a variety of operations. The time delay of a No. 10 varies according to the concentration of the corrosive liquid in the vial. It is widely reported that the wire thickness varied also, but in fact all used the same diameter of wire. The time delay of a No. 9 is determined solely by the thickness of the notch in the wire, the spring tension, and the temperature. Pencil detonators could be used with any explosive provided a suitable booster was attached. However, plastic explosives (which did not require a booster) were particularly useful during the sabotage missions in which they were often employed. There were also a number of special charges issued with a time pencil already incorporated e.g. some types of limpet mines.\n\nAfter being activated a pencil detonator is silent in operation. It does not fizz or make any other noise. However, unlike clockwork timers, pencil detonators only give approximate time delays. For example, a 2-hour pencil detonator might be accurate to plus or minus 5 minutes, whereas the version offering a 6-hour delay could have a precision of plus or minus 15 minutes. Both No. 9 and No. 10 delays were also significantly affected by the ambient temperature, and were issued along with a chart of temperature corrections—but no thermometer. For example, a pencil detonator designed to fire 24 hours after being activated could in reality give a 30-hour delay - if the weather was very cold. Similarly, during hot weather pencil detonators designed to fire after a 12-hour delay could in reality trigger detonation within 10 hours. The main virtue of pencil detonators is their small size and light weight, plus the fact that they are very quick and easy to use. These are important points during covert operations.\n\nFor very high-value targets it is recommended that two pencil detonators from different batches be used together. That way if one detonator fails the other will almost surely blow the charge. Note that if both detonators were going to work, the explosion will occur at the minimum of the two times; thus this method will also slightly reduce the average delay.\n\nPencil detonators saw heavy use during the Second World War by the Special Air Service, Special Operations Executive and groups such as the French Resistance. A number of pencil detonators were used to detonate the massive amatol charge hidden inside HMS \"Campbeltown\" during the St Nazaire Raid of 1942. The ship exploded over an hour later than anticipated.\n\nApproximately 12 million pencil detonators were produced in Britain during the war. However, in recent years they have been superseded by electronic timers which are more accurate and provide much longer delay times. Since pencil detonators are, unlike electronic timers, completely immune to detection or jamming via electronic countermeasures or EMPs, they may still have applicability in special situations.\n\nThe briefcase bomb used in the July 20 plot used a captured British pencil detonator inserted into a block of British plastic explosives weighing approximately two pounds. The bomb was set to 30 minutes and detonated as planned, but Hitler survived with minor injuries. Stauffenberg could not prepare the second block, though. He got rid of it while driving through the forest to the airfield. His driver, Leutnant Erich Kretz, reported seeing Werner von Haeften throw something into the woods in his mirror.\n\n\n"}
{"id": "17855836", "url": "https://en.wikipedia.org/wiki?curid=17855836", "title": "Prague pneumatic post", "text": "Prague pneumatic post\n\nThe Prague pneumatic post (Czech: Pražská potrubní pošta) is the world's last preserved municipal pneumatic post system. It is an underground system of metal tubes under the wider centre of Prague, totaling about in length. The system started service in 1889 and remained in use by the government, banks and the media until it was rendered inoperative by the August 2002 European floods.\n\nSold on by former owner Telefónica O2 Czech Republic after some limited attempts to make repairs, the system now belongs to businessman Zdeněk Dražil, who has announced plans to repair and reopen it as a working tourist attraction. As of 2012, however, it remains closed.\n\nThe Prague pneumatic post entered public service on March 4, 1889. The first lane had been constructed as early as 1887, but at first it only served internal purposes. It ran from the main post office in Jindřišská st. (next to the Wenceslas Square) to the post bureau at Malé náměstí square (next to the Old Town Square) in the Old Town. (This bureau was situated in a corner house with Linhartská st., belonging to the V. J. Rott company, next to a house that is called 'U Rotta' today.) The first lane was later extended as far as the Prague Castle, making it over 5 km long. Prague was the fifth city in the world to receive a pneumatic post system after London, Vienna, Berlin, and Paris, which was considered a major achievement for Prague.\n\nThe system initially was employed mainly for sending telegrams. Only three stations had been connected between the Prague post and the telegraph office as of 1901.\n\nThe system was established for those desiring to send a document quickly. The document would be taken to the post office and rolled up into a metal capsule. The clerk would then drop the metal capsule down a hatch leading to a predestined location. After the clerk pressed a button, the capsule would be moved by compressed air along a network of tubes beneath the pavement.\n\nThe main growth of the network dates to the economically prosperous era of 1927–1932. In those years, new lanes were constructed and tens of thousands of capsules transported per month. During the Prague Uprising the pneumatic post played a role in supplying the besieged building of the Czech radio.\n\nIn the late 1990s, the system was used by over 20 subscribers and operating at a loss, so kept rather for prestigious reasons. The traffic weakened gradually and the 2002 floods seriously damaged it, flooding 5 of the 11 underground engine rooms.\n\nThe lanes consist of steel pipes of 65 mm bore and wall thickness of 2.5–3 mm. The pipes are connected with tight couplers 14 cm long to ensure perfect coaxial alignment and then welded together, ensuring air-tightness. To prevent stray voltage from causing excess corrosion, ceramic insulators are inserted between the pipe segments at some places. Pipes buried underground are protected from the outside by a layer of fiberglass, wound around at increased temperature and coated with hot asphalt. The pipeline is typically buried under the Prague sidewalks 80–120 cm deep. Inside buildings and in the Prague trunk conduit network the pipes simply are coated with anti-corrosive paint.\n\nThe minimum bend radius is 250 cm for underground pipes, but 300 cm is the most commonly used radius. Inside buildings a bend radius as low as 200 cm is allowed. The bends are made of special annealed pipes at normal temperature, using a custom-made bender.\n\nA signaling cable is laid along with the pipe, enabling communication with the track components.\n\nThe lane segments are equipped with dumb wells, where the pipeline can be opened and inspected, or a stuck capsule removed. For this purpose a heavier capsule can be sent at a pressure of up to 30 atm, knocking the stuck capsule out.\n\nThe system uses aluminium capsules measuring 48 mm in outer-diameter and 200 mm in length. On the rear end they are fitted with a plastic circlet, preventing friction against walls of the pipe and a soft plastic skirt, sealing air behind the capsule. The diameter of the rear circlet is 57 mm. The remaining 8 millimeters of the bore are sealed just by the skirt, allowing for excellent airtightness and low friction at the same time.\n\nEach lane is equipped with a dedicated propulsion unit, consisting of an electrically-powered air pump. One pump can service at most 3 kilometers of pipeline, so it's necessary to use several pumps on longer lanes.\n\nThe pumps must be reversible, creating either pressure or vacuum. The pumps are connected to the pipes with tee-fittings. On both sides of the tee the pipe is equipped with switches activated by a passing capsule.\n\nAt first the pump is set to intake mode, pulling the capsule towards the tee. Before reaching it, the capsule hits the first switch, causing the pump to start reversing. Meanwhile, the capsule reaches the tee-fitting. As it passes the tee, the pump is already fully reversed and starts to push the capsule away.\n\nThe older pumps were bladed, having a single blade, mounted eccentrically inside a 300 mm high cylinder. More recent pumps employ a rotating piston instead.\n\nThe capsules can be loaded with packages up to 5 cm in diameter and 30 cm in length. Their weight can be up to 3 kg. Generally these were rolled-up telegrams, but any package within the set limits could be transported.\n\nFor obvious reasons, dangerous and corrosive substances, which could damage the pipeline, were banned. On the other hand, the travel speed could be adjusted, allowing for transporting fragile packages.\n\nThe Prague pneumatic post network consists of five main lanes arranged in a star topology, fitted with switches and concentrators, and of \"subscriber's lines\". The total length of the lanes is about 55 kilometers. Some of the most frequented segments have two pipes (one for each direction), but the majority of the lanes use a single pipe and the direction is determined by setting up the pumps to run in the desired direction. The main lanes connect the following post offices and bureaus:\n\n\nOriginally there were 16 subscriber's lines, but only 7 have been preserved up till today. A total of 24 pneumatic post stations remain today.\n\nThe network crosses the river Vltava in three spots making use of bridges (Hlávkův most, Mánesův most, most Legií).\n\nAll the lanes converge to the \"main post office\" in Jindřišská St. Here all the packages were carefully recorded and from here the network was controlled and monitored. This is also the place where the packages were forwarded from one lane to another. The capsule was picked up from a receiving pocket by a member of the staff, recorded and inserted into the inlet of another lane.\n\nThe current lane state was indicated by indicator lights on the lane's controller. Up to 10 packages in 30-second intervals could be sent on the same lane at once, although this was rarely used in practice.\n\nWhen sending capsules to switched lanes, the capsules had to be sent out in a predefined order, as the switches could only be activated before commencing the transfer. The first capsule would be diverted, after which the switch would automatically slide back to its neutral position, sending the remaining capsules in the straight direction. Therefore, the capsule that was meant to branch off had to be sent first.\n\n"}
{"id": "4811810", "url": "https://en.wikipedia.org/wiki?curid=4811810", "title": "Recreational Software Advisory Council", "text": "Recreational Software Advisory Council\n\nThe Recreational Software Advisory Council (RSAC) was an independent, non-profit organization founded in the U.S. in 1994 by the Software Publishers Association as well as six other industry leaders in response to video game controversy and threats of government regulation.\n\nThe goal of the council was to provide objective content ratings for computer games, similar to the earlier formed Videogame Rating Council (VRC) and later Entertainment Software Rating Board (ESRB). The RSAC ratings were based on the research of Dr. Donald F. Roberts of Stanford University who studied media and its effect on children.\n\nIn 1993, senators Herb Kohl and Joseph Lieberman raised concerns over the levels of violence and other adult material appearing in video games which were available to children. Under threat of government regulation, industry groups like the Software Publishers Association (SPA), the Association of Shareware Professionals (ASP), and others had concerns about the intrusion of the government, and the costs, delays and subjective judgements of a review-committee-based system.\n\nAt the time, the largest trade group, the SPA had few members in the gaming field, but the ASP had many, and the two organizations decided to work together. Mark Traphagen (an attorney with the SPA) and Rosemary West (ASP board member) appeared before Congress in the summer of 1994 in support of the SPA representation.\n\nThe SPA and ASP (and other industry groups) were opposed to an age-based rating system operated by a review committee as developed by the ESRB, which was proposed by several multi-national console game manufacturers and distributors. The groups preferred a content labeling system that would allow parents to know what was in the games and then make their own judgements about what their children would see.\n\nAn ASP-sponsored committee, led by Jim Green of Software Testing Labs, and staffed by Karen Crowther of Redwood Games, and Randy MacLean of FormGen, developed the initial version of what would become the RSAC ratings. The committee identified the elements most likely to be of concern to parents and developed specific descriptions of the levels of such content that would define the levels reported. The system would be self-administered by game publishers who could use the system to label their games.\n\nThe entire system was turned over to the SPA for its newly formed Recreational Software Advisory Council in 1994. The council formed RSACi in 1995, which was a branch which rated websites.\n\nThe organization was closed in 1999 and reformed into the Internet Content Rating Association (ICRA). The background, formation and rating process of the RSAC and RSACi may be viewed here.\n\nThese RSACi ratings are included and used in the \"Content Advisor\" feature of Microsoft Internet Explorer.\n\n\n"}
{"id": "49181275", "url": "https://en.wikipedia.org/wiki?curid=49181275", "title": "SKS process", "text": "SKS process\n\nThe SKS process is a framework of Stop/Keep-doing/Start that is used to collect or categorize feedback.\n\nYou can ask customers or colleagues:\n\nThis approach is also used in agile development, where it is known as Start/Stop/Continue.\n"}
{"id": "4136591", "url": "https://en.wikipedia.org/wiki?curid=4136591", "title": "Service à la russe", "text": "Service à la russe\n\nService à la russe (French, \"service in the Russian style\") is a manner of dining that involves courses being brought to the table sequentially. It contrasts with \"service à la française\" (\"service in the French style\") in which all the food is brought out at once, in an impressive display.\n\nRussian Ambassador Alexander Kurakin is credited with bringing \"service à la russe\" to France in the early 19th century. It later caught on in England and is now the style in which most modern Western restaurants serve food (with some significant modifications).\n\nFor the most correct \"service à la russe\", the following must be observed:\n\nThe place setting (called a cover) for each guest includes a service plate, all the necessary cutlery except those required for dessert, and stemmed glasses for water, wines and champagne. Atop the service plate is a rolled napkin, and atop that is the place card. Above the plate is a saltcellar, nut dish, and a menu.\n\nThe cutlery to the right of the service plate is, from the outside in, the oyster fork resting in the bowl of the soup spoon, the fish knife, the meat knife and the salad knife (or fruit knife). On the left, from the outside in, are the fish fork, the meat fork and a salad fork (or fruit fork). If both a salad and a fruit course are served, the necessary extra flatware is brought out on a platter, as it is bad form to have more than three knives or forks on the table at once, the oyster fork excepted.\n\nGuests are seated according to their place cards and immediately remove their napkins and place them in their laps. Another view maintains that the napkin is only removed after the host has removed his or hers. (In the same manner, the host is first to begin eating, and guests follow.) The oyster plate is placed atop the service plate. Once that is cleared, the soup plate replaces it. After the soup course is finished, both the soup plate and service plate are removed from the table, and a heated plate is put in their place. The rule is as such: a filled plate is always replaced with an empty one, and no place goes without a plate until just before the dessert course.\n\nThe fish and meat courses are always served from platters because in correct service a filled plate is never placed before a guest, as this would indirectly dictate how much food the guest is to eat.\n\nDirectly before dessert, everything is removed from the place settings except the wine and water glasses. Crumbs are cleared. The dessert plate is then brought out with a doily on top of it, a finger bowl on top of that, and a fork and spoon, the former balanced on the left side of the plate and the latter on the right. Guests remove the doily and finger bowls, move them to the left of the plate and place the fork to the left side of the plate and the spoon to its right. Guests do not actually need to use the finger bowl, since they may have not used their fingers to eat with, unless they also had bread with the meal.\n\nThe number of dishes (or courses) served at a meal \"à la russe\" has changed over time; but an underlying pattern of service—beginning with soup, then moving through various entrées, then to the roast or game, and then to vegetables (including salads), sweets and coffee—persisted from the mid-19th century (when this type of service was introduced into France) up to the Second World War, and continued in a much-reduced form into the 21st century. The order of dishes descends directly from the much older \"service à la française\". In that style of service, all sorts of dishes were arranged on the table and guests served themselves and each other; but as Jean-Louis Flandrin has shown, the order of \"consumption\"—known to the guests of the time but rarely evident from contemporary menus or descriptions of meals—was essentially the same as the order of \"presentation\" in \"service à la russe\".\n\nThe most elaborate version of \"service à la russe\", which reached its pinnacle in the last decades of the Victorian era, was described by Sarah Tyson Rorer in 1886. Rorer was critical of this elaborate service and offered a much simpler alternative, which in fact represents the core principals of this style of service.\n\nThe elaborate and conventional dinner, complete at all points, which the dinner-giving of a century and a half has evolved, is beyond any but the very wealthy. Very few of them succeed in giving it, and still fewer of their guests enjoy it. Its triple triplets of oysters, soup, and fish, the relevé, entrées, and roast, a pause of rum punch to stimulate languishing digestion, game with salad, sweets and ice, coffee to close, and a bewildering series of wines, with an alcoholic appetizer to begin and end, have, however, had their effect in making many feel that a formal dinner must only follow this model from afar. So, with only the resources of a simple household, they compass, with infinite labor, oysters, soup, and fish, add some made dish to the meat, and put salad before and ice cream after the pudding or sweets.But success here, with a moderate income, is as rare as success with the long dinner at the complete table. Try to grasp the theory of the elaborate edifice which custom and convention has piled up, and see if your own resources cannot reproduce its purpose with better success. After having carefully analyzed it, you will see at once that the most complex dinner simply aims to begin with something of easy digestion, slide by some transition to the roast, and make sure that through salad, sweets and coffee, the last half of your dinner shall interest the appetite as well as satisfy hunger. You, have, therefore, soup, roast, dessert, which make up the usual dinner of thoroughly civilized people, and below you will see how, with but moderate resources, you may so vary this as to make a “little dinner” complete and satisfying in itself; more, the most elaborate meal at Delmonico’s cannot do.\n\nIn Britain and the United States, fish is a distinct course; relevés are large, solid joints of meat or whole fowl, generally baked, braised, or boiled but not roasted; entrées are elaborate \"made dishes\" of, typically, fillets of beef or other butcher's meat (and sometimes fowl, but—apart from days of religious observance—not fish), served in fine sauces. Roasts are solid joints of meat (and sometimes fowl) other than feathered game, usually spit-roasted but often baked. Game is feathered not furred, spit-roasted whole and served rather simply. (Rorer's \"roast\" here refers to a roasted entrée, but this terminology is not typical of the period. In her time, the \"roast\" followed the punch, and it was always game, if available.) \n\nAt the time Rorer was writing, Alessandro Filippini, a chef at Delmonico's restaurant on Pine Street in New York, wrote a book of menus for \"every family of means in the habit of giving a few dinners to its friends during the year\", with a brief discussion of table service and a guide to wines. He recommended the types of menus criticized by Rorer but common among the wealthy.\nFrench dinners are generally served in three main courses, viz., Relevés, Entrées, and Rotis; all the rest are considered side courses. It depends entirely on the taste of the host as to how many main courses he desires served. The author would suggest two relevés, three entrées, and one or two rotis; this could be made an elaborate dinner.\n\nAbout a third of Filippini's book contains menus for breakfast, luncheon, and dinner for every day of the year. The dinner menus begin with the \"side courses\", as he calls them: oysters or clams, soup, and hors d'œuvre; followed by the three \"main courses\": several relevés and entrées, and one roti (roast); and finally a few other \"side courses\": sweet entremets, ices, and coffee.\n\nHors-d'œuvre are usually little cold items (such as olives, celery, radishes, charcuterie, caviar), but they might also include hot made dishes (such as timbales, croustades, croquettes). In the French style of \"service à la russe\", used by Filippini for many of his menus, there is no distinct \"fish course\", as both relevés and entrées may be of meat, fowl, or fish indiscriminately. Punch often precedes the roast. The roast can be of meat, fowl, or fish (though fish is generally limited to days of religious observance); but when game is served, it always makes up the roast course. Entremets are the vegetables, including salads, served with the relevés and entrées; they not as a separate course, though they are often listed as such. Sweet entremets are cakes, puddings, and such. Ices are frozen sweets, served as a separate course. Fruit, petits fours, coffee, and cordials are offered at the end of the meal. \n\nA few years after Filippini wrote his book, Charles Ranhofer, another chef at Delmonico's restaurant (variously at the 14th Street, 26th Street, and 44th Street locations), in his cookbook \"The Epicurean\", outlined in great detail the dishes necessary for dinners ranging from six to fourteen courses. The six-course dinner is very much like Rorer's \"little dinner\": oysters, soup, fish, entrée, roast, salad, and dessert. Longer dinners are arranged by adding side dishes, removes, and various cold dishes, and by serving a greater number of entrées and desserts. The longest of these menus is as follows:\nFigure 1—36 covers:\n\n\"S.D.\" are \"side dishes\", i.e. hors d'œuvre. There is a separate fish course, then relevés and entrées. Cold dishes, such as mayonnaise salads and aspics, had become very popular at this time, as is evident in the menu. Roasts could be of butchers' meat, fowl, or game (rarely, if ever, of fish). When more than one dish was appointed for a course (e.g. 2 Soups, 2 Fish, 2 roasts, 2 colds), the guest was expected to choose one or the other, not both; a guest might also decline one or more of the courses.\n\nRanhofer also gives elaborate instructions for the service of wine.\nFIRST SERVICE. \n\"With Oysters.\"—Sauterne, Barsac, Graves, Mont Rachet, Chablis.\n\"After the Soup.\"—Madeira, Sherry or Xeres.\n\"With Fish.\"—(Rhine wines) Johannisberger, Marcobrunner, Hochheimer, Laubenheimer, Liebfraumilch, Steinberger. (Moselle) Brauneberger, Zeltinger, Berncasteler.\n\"With Removes.\"—Côte St. Jacques, Moulin-à-vent, Macon, Clos de Vougeot, Beaune.\n\"With Entrées.\"—St. Émilion, Médoc du Bordelais, St. Julien. Dry champagnes for certain countries.\nIced Punches and Sherbets, Rum, Madeira.\n\nSECOND SERVICE. \"With Roasts.\"—(Burgundies) Pommard, Nuits, Corton, Chambertin, Romanée Conti.\n\"Cold Roasts.\"—Vin de Paille, Steinberger.\n\"With Hot Desserts.\"—(Bordeaux) Château Margaux, Léoville, Laffitte, Château Larose, Pontet-Canet, St. Pierre, Côtes de Rhone, Hermitage and Côte-Rôtie. (Red Champagne) Bouzy, Verzenay, Porto Première.\n\nTHIRD SERVICE. \"With Dessert.\"—(Burgundy) Volnay, Mousseux. (Champagnes) Delmonico, Roederer, Rosé Mousseux, Pommery, Cliquot, Perrier-Jouët, Moët, Mumm.\n\"Wine Liquors.\"—Muscatel, Malaga, Alicante, Malvoisie of Madeira, Lacryma Christi, red and white Cape, Tokay, Constance, Schiraz.\n\"Cordials.\"—Curaçoa [sic], Kirsch, Cognac, Chartreuse, Maraschino, Prunelle, Anisette, Bénédictine.\n\"Beers.\"—Bass’ Ales, Porter, Tivoli, Milwaukee.\n\nSeveral decades later, shorter meals had become the norm and the extravagant dinners of the Victorian period were considered vulgar, as noted by Emily Post in 1922: \nUnder no circumstances would a private dinner, no matter how formal, consist of more than:\nThe menu for an informal dinner would leave out the entrée, and possibly either the hors-d’oeuvre or the soup.\n\nAs a matter of fact, the marked shortening of the menu is in informal dinners and at the home table of the well-to-do. Formal dinners have been as short as the above schedule for twenty-five years. [c.1900.] A dinner interlarded with a row of extra entrées, Roman punch, and hot dessert is unknown except at a public dinner, or in the dining-room of a parvenu. About thirty-five years ago [c.1890] such dinners are said to have been in fashion!\n\nAt the time Post was writing, hors-d’œuvre meant rather narrowly light cold dishes like oysters, clams, melon, or citrus. Entrées meant elaborate \"made dishes\" of fillets of beef or other butcher's meat served in a fine sauces, or some sort of pastry dish. Roasts could be of any meat, which was not necessarily roasted; but the preferred dish of a truly fine dinner was wild feathered game, spit-roasted and served rather simply. Dessert was molded ice-cream only, to the exclusion of all other sweets. Despite Post's complaints about extra entrées, many dinners continued to feature two meat courses between the fish and the roast.\n\nPost's first book was published during Prohibition, and she noted, \"A water glass standing alone at each place makes such a meager and untrimmed looking table that most people put on at least two wine glasses, sherry and champagne, or claret and sherry, and pour something pinkish or yellowish into them. [...] Those few who still have cellars, serve wines exactly as they used to, white wine, claret, sherry and Burgundy warm, champagne ice cold; and after dinner, green mint poured over crushed ice in little glasses, and other liqueurs of room temperature.\"\n\nAfter World War II, dinners were curtailed even more. As Post writes in the 1950 edition of her book, the shorter \"informal\" meal of her earlier book had become the norm for formal dinners:\nUnder no circumstances does a modern dinner, no matter how formal, consist of more than:\nAfter-dinner coffee\n\nIn addition to the set courses, little relish dishes of radishes, celery, olives, or almonds could be set on the table as \"hors-d'œuvre\". Wines, too, were often greatly reduced in number. Amy Vanderbilt noted in her book, \"The Complete Book of Etiquette\", \"At a formal dinner champagne may be the only wine served after the service of sherry with the soup.\"\n\nThis five-course service might be further reduced by serving either soup or fish (or shellfish) as a first course, but not both. Dinners in the French style usually include a cheese course after the roast, generally resulting in a 6-course meal (see, for example, the formal menus in Richard Olney's \"The French Menu Cookbook\"); alternatively, one or more of the other courses can be omitted (see, for example, the formal menus in Simone Beck's \"Simca's Cuisine\"). Dinners in the American style often place the salad as a first course instead of soup, an innovation that appeared in the 1950s in California and was noted by Vanderbilt ; in this arrangement, dessert is served immediately after the roast. Wine service may include a separate wine for each course, or simply be champagne throughout; or, most commonly, service may be limited to three wines: a white for the soup and fish, a red for the roast, and a sweet wine or champagne for dessert. \n\nThese and similar arrangements of four- and five-course formal dinners were the norm throughout the second half of the 20th century.\n\n\n"}
{"id": "36970467", "url": "https://en.wikipedia.org/wiki?curid=36970467", "title": "Simple Energy", "text": "Simple Energy\n\nSimple Energy is a privately held software-as-a-service (\"SaaS\") company.\n\nSimple Energy is headquartered in Boulder, Colorado.\n\nSimple Energy was founded in 2010 by longtime friends Yoav Lurie and Justin Segall, classmates at Duke University. The two had met on a backpacking trip in Pisgah National Forest in 2000.\n\nIn September 2011, then-US CTO Aneesh Chopra challenged the energy industry to model a Green Button, off the successful Blue Button, where energy providers would give energy users their consumption data in an easy to read and use format at the click of the button. In January 2012, Simple Energy became the first third-party application developer to implement the Green Button standard to deploy publicly available applications.\n\n\n"}
{"id": "32997323", "url": "https://en.wikipedia.org/wiki?curid=32997323", "title": "Single-molecule electric motor", "text": "Single-molecule electric motor\n\nThe single-molecule electric motor is an electrically operated motor made from a single butyl methyl sulphide molecule. The molecule is adsorbed onto a copper (111) single-crystal piece by chemisorption. The motor, the world's smallest electric motor, is just a nanometer (billionth of a meter) across (60 000 times smaller than the thickness of a human hair). It was developed by chemists at the Tufts University School of Arts and Sciences and published online September 4, 2011.\n\nSingle-molecule motors have been demonstrated before. These motors were either powered by chemical reactions or by light. This is the first experimental demonstration of electrical energy successfully coupling to directed molecular rotation.\n\nButyl methyl sulfide is an asymmetrical thioether which is achiral in the gas phase. The molecule can adsorbed to the surface through either of the sulfur's two lone pair. This gives rise to the surface bound chirality of the molecule. The asymmetry of the molecular surface interface gives rise to an asymmetrical barrier to rotation. The molecule rotates around this sulfur-copper bond. Electrons quantum tunneling from the STM tip electrically excite molecular vibrations, which couple to rotational modes. The rotation of the motor can be controlled by adjusting the electron flux from the scanning tunneling microscope and the background temperature. The tip of the scanning electron microscope acts as an electrode. The chiralities of the tip of the STM and the molecule determine the rate and direction of rotation. Images taken of the molecule at 5 K and under non-perturbative scanning conditions show a crescent-shaped protrusion of the molecule. When the temperature is raised to 8 K, the molecule starts rotating along six orientations determined by the hexagonal structure of the copper it is adsorbed on. In this case, a STM image taken of the molecule appears as a hexagon as the timescale of the imaging is much slower than the rotation rate of the molecule.\n\nThe six states of rotation of the molecule can be determined by aligning the tip of the scanning electron microscope asymmetrically on the side of one of the lobes of the molecule during spectroscopy measurements. When the butyl tail is nearest to the tip of the microscope, the tunneling current would be maximum and vice versa. The position of the molecule on the surface can be determined by the tunneling current. By plotting the position versus time the rate and direction of rotation can be determined. At higher temperatures, the single-molecule motor rotates too fast (up to one million rotations per second at 100 K) to monitor.\n\nThe single-molecule electric motor can be efficiently used in engineering, nanotechnological applications and medicinal applications, where drugs could be delivered to specified locations more accurately. By altering the chemical structure of the molecule, it could become a component of a nanoelectromechanical system (NEMS). It also has potential to be utilized to generate microwave radiation.\n\n"}
{"id": "866836", "url": "https://en.wikipedia.org/wiki?curid=866836", "title": "Sprengel explosive", "text": "Sprengel explosive\n\nSprengel explosives are a generic class of materials invented by Hermann Sprengel in the 1870s. They consist of stoichiometric mixtures of strong oxidisers and reactive fuels, mixed just prior to use in order to enhance safety. Either the oxidiser or the fuel, or both, should be a liquid to facilitate mixing, and intimate contact between the materials for a fast reaction rate.\n\nSprengel suggested nitric acid, nitrates and chlorates as oxidisers, and nitroaromatics (e.g. nitrobenzene) as fuels. Other Sprengel explosives used at various times include charcoal with liquid oxygen (an oxyliquit), \"Rackarock\", and ANFO ammonium nitrate (oxidiser) mixed with a fuel oil (fuel), normally diesel kerosene or nitromethane. Eventually ANFO supplanted all others because its oxidiser was the safest, and - due to its widespread use as a fertiliser in agriculture - also the cheapest.\n\n\"Rackarock\" consisted of potassium chlorate and nitrobenzene. It was provided in the form of permeable cartridges of the chlorate, which were placed in wire baskets and dipped in the nitrobenzene for a few seconds before use. For underwater use, it could be provided in cans instead. It was famously used in the massive submarine demolition of Flood Rock, a navigational hazard in Long Island Sound in 1885. The charge of over a hundred tonnes of explosive (laid in tunnels 20 metres below sea level) destroyed approximately 600,000 tonnes of rock, and created a wave 76 m high.\n\n"}
{"id": "8678691", "url": "https://en.wikipedia.org/wiki?curid=8678691", "title": "Spring house", "text": "Spring house\n\nA spring house, or springhouse, is a small building, usually of a single room, constructed over a spring. While the original purpose of a springhouse was to keep the spring water clean by excluding fallen leaves, animals, etc., the enclosing structure was also used for refrigeration before the advent of ice delivery and, later, electric refrigeration. The water of the spring maintains a constant cool temperature inside the spring house throughout the year. Food that would otherwise spoil, such as meat, fruit, or dairy products, could be kept there, safe from animal depredations as well. Springhouses thus often also served as pumphouses, milkhouses, and root cellars.\n\nIn settings where no natural spring is available, another source of natural running water, such as a small creek or diverted portion of a larger creek, might be used. In addition, some people put jars of milk in a bucket suspended by a rope in an \"open-mouth\" well during hot weather.\n\nThe Tomahawk Spring spring house at Tomahawk, West Virginia, was listed on the National Register of Historic Places in 1994.\n\n"}
{"id": "5249007", "url": "https://en.wikipedia.org/wiki?curid=5249007", "title": "Stump grinder", "text": "Stump grinder\n\nA stump grinder or stump cutter is a power tool or equipment attachment that removes tree stumps by means of a rotating cutting disc that chips away the wood.\n\nStump grinders can be the size of a lawn mower or as large as truck. Most accomplish their task by means of a high-speed disk with teeth that grinds the stump and roots into small chips.\n\nA typical stump grinder incorporates a cutter wheel with fixed carbide teeth. The cutter wheel movements are controlled by hydraulic cylinders to push the cutter head laterally through the stump and to raise and lower it.\n\nStump grinding is generally performed by a qualified arborist or landscaper, however it is possible to rent them from tool hire companies for DIY projects.\n\nThere are also other types of stump grinders that are applied to tractors, excavators and other construction equipment. These machines can completely remove the roots of the trees in a few seconds or recover the central part of the roots. Vertical stump grinders are usually used in forests, for plant biomass, for the forestry sector, and for the maintenance of green areas, etc. They can destroy an entire root of in diameter to more than one meter deep in 30 seconds. Studies have shown that the normal stump grinder (with disc and teeth) can eliminate most of the first of stumps, but for stumps with a bigger diameter the problem is not solved as the plant can start growing back. Instead, a vertical stump grinder can completely destroy tree stumps of up to in diameter permanently. They can be affixed to tractors, excavators and others earth moving machinery.\n"}
{"id": "1852072", "url": "https://en.wikipedia.org/wiki?curid=1852072", "title": "Terminal server", "text": "Terminal server\n\nA terminal server enables organizations to connect devices with an RS-232, RS-422 or RS-485 serial interface to a local area network (LAN). Products marketed as terminal servers can be very simple devices that do not offer any security functionality, such as data encryption and user authentication. The primary application scenario is to enable serial devices to access network server applications, or vice versa, where security of the data on the LAN is not generally an issue. There are also many terminal servers on the market that have highly advanced security functionality to ensure that only qualified personnel can access various servers and that any data that is transmitted across the LAN, or over the Internet, is encrypted. Usually companies which need a terminal server with these advanced functions want to remotely control, monitor, diagnose and troubleshoot equipment over a telecommunications network.\n\nA console server (also referred to as console access server, console management server, serial concentrator, or serial console server) is a device or service that provides access to the system console of a computing device via networking technologies.\n\nHistorically, a terminal server was a device that attached to serial RS-232 devices, such as \"green screen\" text terminals or serial printers, and transported traffic via TCP/IP, Telnet, SSH or other vendor-specific network protocols (e.g., LAT) via an Ethernet connection.\n\nDigital Equipment Corporation's DECserver 100 (1985), 200 (1986) and 300 (1991) are early examples of this technology. (An earlier version of this product, known as the DECSA Terminal Server was actually a test-bed or proof-of-concept for using the proprietary LAT protocol in commercial production networks.) With the introduction of inexpensive flash memory components, Digital's later DECserver 700 (1991) and 900 (1995) no longer shared with their earlier units the need to download their software from a \"load host\" (usually a Digital VAX or Alpha) using Digital's proprietary Maintenance Operations Protocol (MOP). In fact, these later terminal server products also included much larger flash memory and full support for the Telnet part of the TCP/IP protocol suite. Many other companies entered the terminal-server market with devices pre-loaded with software fully compatible with LAT and Telnet.\n\nA \"terminal server\" is used many ways but from a basic sense if a user has a serial device and they need to move data over the LAN, this is the product they need.\n\n\nA console server (console access server, console management server, serial concentrator, or serial console server) is a device or service that provides access to the system console of a computing device via networking technologies.\n\nMost commonly, a console server provides a number of serial ports, which are then connected to the serial ports of other equipment, such as servers, routers or switches. The consoles of the connected devices can then be accessed by connecting to the console server over a serial link such as a modem, or over a network with terminal emulator software such as telnet or ssh, maintaining survivable connectivity that allows remote users to log in the various consoles without being physically nearby.\n\nDedicated console server appliances are available from a number of manufacturers in many configurations, with the number of serial ports ranging from one to 96. These Console Servers are primarily used for secure remote access to Unix Servers, Linux Servers, switches, routers, firewalls, and any other device on the network with a console port. The purpose is to allow network operations center (NOC) personnel to perform secure remote data center management and out-of-band management of IT assets from anywhere in the world. Products marketed as Console Servers usually have highly advanced security functionality to ensure that only qualified personnel can access various servers and that any data that is transmitted across the LAN, or over the Internet, is encrypted. Marketing a product as a console server is very application specific because it really refers to what the user wants to do—remotely control, monitor, diagnose and troubleshoot equipment over a network or the Internet.\n\nSome users have created their own console servers using off-the-shelf commodity computer hardware, usually with multiport serial cards typically running a slimmed-down Unix-like operating system such as Linux. Such \"home-grown\" console servers can be less expensive, especially if built from components that have been retired in upgrades, and allow greater flexibility by putting full control of the software driving the device in the hands of the administrator. This includes full access to and configurability of a wide array of security protocols and encryption standards, making it possible to create a console server that is more secure. However, this solution may have a higher TCO, less reliability and higher rack-space requirements, since most industrial console servers have the physical dimension of one rack unit (1U), whereas a desktop computer with full-size PCI cards requires at least 3U, making the home-grown solution more costly in the case of a co-located infrastructure.\n\nAn alternative approach to a console server used in some cluster setups is to null-modem wire and daisy-chain consoles to otherwise unused serial ports on nodes with some other primary function.\n\n\n"}
{"id": "21791567", "url": "https://en.wikipedia.org/wiki?curid=21791567", "title": "The Standard School Broadcast", "text": "The Standard School Broadcast\n\nThe Standard School Broadcast was a weekly educational radio program that went on the air in 1928 and promoted music appreciation for students in the western United States. It was the oldest educational radio program in America. Based in San Francisco, California, the series was carried on NBC radio stations and via syndication. \"The Standard School Broadcast\" was devoted to music and American history. Carmen Dragon was music director of the programs for many years. The program's sponsor, Standard Oil of California, was honored with a Peabody Award for the series in 1958. In 1975, the program received the Peabody Institutional Award \"for 47 years of continuous educational radio service\".\n\n\"The Standard School Broadcast\" began in October 1928 and was first heard in 72 schools via the NBC Pacific Network. Predating the comparable CBS Radio series \"The American School of the Air\", it was the oldest educational radio program in the United States.\n\nThe series presented hundreds of topics including the science of music, music as drama, and non-classical forms including jazz and folk. Guests ranged from Dorothy Warenskjold to Louis Armstrong. It evolved from a simple lecture program accompanied by a string trio into a program that combined concert hall, stage and documentary, with a symphony orchestra conducted by Carmen Dragon and a cast of professional performers.\n\n\"The Standard School Broadcast\" was a companion to the classical music radio program, \"The Standard Hour\". Both were sponsored by Standard Oil of California. \"The Standard School Broadcast\" ran on NBC radio and, later, in syndication on the West Coast.\n\nA 1943 brochure shows that the programs were carried on KPO in San Francisco, KFI in Los Angeles, KMJ in Fresno, KGW in Portland, Oregon, KOMO in Seattle, Washington, KHQ in Spokane, Washington, KMED in Medford, Oregon, KDYL in Salt Lake City, Utah, KTAR in Phoenix, Arizona, KGLU in Safford, Arizona, KVOA in Tucson, Arizona, and KYUM in Yuma, Arizona, on Thursday at 10 a.m. Pacific time and 11 a.m. Mountain time. At that time, Carl Kalesch was the music director for the programs and John Grover was the announcer.\n\nThe theme music for \"The Standard School Broadcast,\" as well as \"The Standard Hour,\" was \"This Hour Is Yours\". The theme was composed by Julius Haug, a violinist in the San Francisco Symphony Orchestra.\n\n\"It was simple theme music, 45 seconds long,\" wrote radio historian John Dunning, \"and listeners who loved it and requested 'the entire piece' were surprised when told there was no more.\"\n\nFor many years, the broadcasts originated from NBC's largest radio studio in San Francisco. in the building which housed the network's KPO (later renamed KNBC and then KNBR).\n\nBesides John Grover, announcers for the broadcasts included Hale Sparks and Fred Jorgenson. Many of the programs were preserved on transcription discs or magnetic tape.\n\nIn 1958, \"The Standard School Broadcast\" received a Peabody Award for radio education, \"in recognition of continuous expansion and development over a 30-year period. This outstanding music appreciation series for schools combines educational value with highest musicianship, expert production, and utilization of appropriate musical groups of all types, instrumental and vocal.\"\n\nIn 1975, \"The Standard School Broadcast\" received the Peabody Institutional Award:\n\n\"The Standard School Broadcast\"'s 47 years on the air throughout the Western states is an achievement to which any broadcaster could point with pride. It is doubly impressive to note that its programming has always been imaginative and entertaining as well as commercial-free. Its performers have included many of the world’s most distinguished musicians and almost single-handedly it has introduced the joys of good music to several generations of listeners. \n\nA few episodes of \"The Standard School Broadcast\" are available from old-time radio program collectors.\n\nIn the 1970s, the Chevron Research Company released a series of recordings edited from \"The Standard School Broadcast\" as a public service. The LP records, with teachers guides, were available free of charge to elementary and junior high schools throughout the West, Rocky Mountain area and the Southwest. These out-of-print recordings have been sold on various websites.\n\n\n"}
