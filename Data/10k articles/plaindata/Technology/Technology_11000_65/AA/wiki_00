{"id": "1730358", "url": "https://en.wikipedia.org/wiki?curid=1730358", "title": "ATM Forum", "text": "ATM Forum\n\nThe ATM Forum was founded in 1991 to be the industry consortium to promote Asynchronous Transfer Mode technology used in telecommunication networks; the founding president and chairman was Fred Sammartino of Sun Microsystems. It was a non-profit international organization. The ATM Forum created over 200 implementation agreements.\n\nIn 1996 ATM technology stabilized with the \"Anchorage Accord\", which established the baseline of ATM implementations. While ATM did not live up to every expectation, it remained an important core network technology. \nThe Frame Relay Forum (promoting frame relay) also started in 1991. The MPLS Forum (which supported Multiprotocol Label Switching had begun in 2000. Those two merged in 2003 to become the MPLS and Frame Relay Alliance (MFA).\nIn 2005, the ATM Forum joined forces with the MFA to form the MFA Forum, which was renamed to be the IP/MPLS Forum.\nIn May 2009 the IP/MPLS Forum merged with the Broadband Forum.\n\n\nA widely adopted specification to emerge from the ATM Forum was the Circuit Emulation Service specification (CES). This specification defined a method of creating a service out of mapping TDM DS0 and DS1/E1 Plesiochronous Digital Hierarchy (PDH) signals into Asynchronous Transfer Mode (ATM) cells. It also supported J2 and DS-3 signals. The service was built around the ATM Adaption Layer 1 specification from the ITU. The 1.0 version was approved about 1995 and the 2.0 version was approved in January 1997.\n\n"}
{"id": "13153707", "url": "https://en.wikipedia.org/wiki?curid=13153707", "title": "Accounting machine", "text": "Accounting machine\n\nAn accounting machine, or bookkeeping machine or recording-adder, was generally a calculator and printer combination tailored for a specific commercial activity such as billing, payroll, or ledger. Accounting machines were widespread from the early 1900s to 1980s, but were rendered obsolete by the availability of low-cost computers such as the IBM PC.\n\nThis type of machine is generally distinct from unit record equipment (some unit record machines were also called accounting machines).\n\n\nUnit record equipment\n"}
{"id": "2439847", "url": "https://en.wikipedia.org/wiki?curid=2439847", "title": "Aerial image", "text": "Aerial image\n\nAn aerial image is a projected image which is \"floating in air\", and cannot be viewed normally. It can only be seen from one position in space, often focused by another lens. \n\nAerial image technology was used in optical printers and movie special effects photography before the advent of computer graphics in movie production, and also for combining animation and live action footage onto one piece of film.\n"}
{"id": "3214", "url": "https://en.wikipedia.org/wiki?curid=3214", "title": "Amplifier figures of merit", "text": "Amplifier figures of merit\n\nIn electronics, the figures of merit of an amplifier are numerical measures that characterize its properties and performance. Figures of merit can be given as a list of specifications that include properties such as gain, bandwidth, noise and linearity, among others listed in this article. Figures of merit are important for determining the suitability of a particular amplifier for an intended use.\n\nThe gain of an amplifier is the ratio of output to input power or amplitude, and is usually measured in decibels. When measured in decibels it is logarithmically related to the power ratio: \"G\"(dB)=10 log(\"P\" /\"P\"). RF amplifiers are often specified in terms of the maximum \"power gain\" obtainable, while the voltage gain of audio amplifiers and instrumentation amplifiers will be more often specified. For example, an audio amplifier with a gain given as 20 dB will have a \"voltage gain\" of ten.\n\nThe use of voltage gain figure is appropriate when the amplifier's input impedance is much higher than the source impedance, and the load impedance higher than the amplifier's output impedance.\n\nIf two equivalent amplifiers are being compared, the amplifier with higher gain settings would be more sensitive as it would take less input signal to produce a given amount of power.\n\nThe bandwidth of an amplifier is the range of frequencies for which the amplifier gives \"satisfactory performance\". The definition of \"satisfactory performance\" may be different for different applications. However, a common and well-accepted metric is the half-power points (i.e. frequency where the power goes down by half its peak value) on the output vs. frequency curve. Therefore, bandwidth can be defined as the difference between the lower and upper half power points. This is therefore also known as the bandwidth. Bandwidths (otherwise called \"frequency responses\") for other response tolerances are sometimes quoted (, etc.) or \"plus or minus 1dB\" (roughly the sound level difference people usually can detect).\n\nThe gain of a good quality full-range audio amplifier will be essentially flat between 20 Hz to about 20 kHz (the range of normal human hearing). In ultra-high-fidelity amplifier design, the amplifier's frequency response should extend considerably beyond this (one or more octaves either side) and might have points < 10 Hz and > . Professional touring amplifiers often have input and/or output filtering to sharply limit frequency response beyond ; too much of the amplifier's potential output power would otherwise be wasted on infrasonic and ultrasonic frequencies, and the danger of AM radio interference would increase. Modern switching amplifiers need steep low pass filtering at the output to get rid of high-frequency switching noise and harmonics.\n\nThe range of frequency over which the gain is equal to or greater than 70.7% of its maximum gain is termed as bandwidth.\n\nEfficiency is a measure of how much of the power source is usefully applied to the amplifier's output. Class A amplifiers are very inefficient, in the range of 10–20% with a max efficiency of 25% for direct coupling of the output. Inductive coupling of the output can raise their efficiency to a maximum of 50%.\n\nDrain efficiency is the ratio of output RF power to input DC power when primary input DC power has been fed to the drain of a field-effect transistor. Based on this definition, the drain efficiency cannot exceed 25% for a class A amplifier that is supplied drain bias current through resistors (because RF signal has its zero level at about 50% of the input DC). Manufacturers specify much higher drain efficiencies, and designers are able to obtain higher efficiencies by providing current to the drain of the transistor through an inductor or a transformer winding. In this case the RF zero level is near the DC rail and will swing both above and below the rail during operation. While the voltage level is above the DC rail current is supplied by the inductor.\n\nClass B amplifiers have a very high efficiency but are impractical for audio work because of high levels of distortion (See: Crossover distortion). In practical design, the result of a tradeoff is the class AB design. Modern Class AB amplifiers commonly have peak efficiencies between 30–55% in audio systems and 50-70% in radio frequency systems with a theoretical maximum of 78.5%.\n\nCommercially available Class D switching amplifiers have reported efficiencies as high as 90%. Amplifiers of Class C-F are usually known to be very high-efficiency amplifiers. RCA manufactured an AM broadcast transmitter employing a single class-C low-mu triode with an RF efficiency in the 90% range.\n\nMore efficient amplifiers run cooler, and often do not need any cooling fans even in multi-kilowatt designs. The reason for this is that the loss of efficiency produces heat as a by-product of the energy lost during the conversion of power. In more efficient amplifiers there is less loss of energy so in turn less heat.\n\nIn RF linear Power Amplifiers, such as cellular base stations and broadcast transmitters, special design techniques can be used to improve efficiency. Doherty designs, which use a second output stage as a \"peak\" amplifier, can lift efficiency from the typical 15% up to 30-35% in a narrow bandwidth. Envelope Tracking designs are able to achieve efficiencies of up to 60%, by modulating the supply voltage to the amplifier in line with the envelope of the signal.\n\nAn ideal amplifier would be a totally linear device, but real amplifiers are only linear within limits.\n\nWhen the signal drive to the amplifier is increased, the output also increases until a point is reached where some part of the amplifier becomes saturated and cannot produce any more output; this is called clipping, and results in distortion.\n\nIn most amplifiers a reduction in gain takes place before hard clipping occurs; the result is a \"compression\" effect, which (if the amplifier is an audio amplifier) sounds much less unpleasant to the ear. For these amplifiers, the 1 dB compression point is defined as the input power (or output power) where the gain is 1 dB less than the small signal gain. Sometimes this non linearity is deliberately designed in to reduce the audible unpleasantness of hard clipping under overload.\n\nIll effects of non-linearity can be reduced with negative feedback.\n\nLinearization is an emergent field, and there are many techniques, such as feed forward, predistortion, postdistortion, in order to avoid the undesired effects of the non-linearities.\n\nThis is a measure of how much noise is introduced in the amplification process. Noise is an undesirable but inevitable product of the electronic devices and components; also, much noise results from intentional economies of manufacture and design time. The metric for noise performance of a circuit is noise figure or noise factor. Noise figure is a comparison between the output signal to noise ratio and the thermal noise of the input signal.\n\nOutput dynamic range is the range, usually given in dB, between the smallest and largest useful output levels. The lowest useful level is limited by output noise, while the largest is limited most often by distortion. The ratio of these two is quoted as the amplifier dynamic range. More precisely, if \"S\" = maximal allowed signal power and \"N\" = noise power, the dynamic range \"DR\" is \"DR = (S + N ) /N\".\n\nIn many switched mode amplifiers, dynamic range is limited by the minimum output step size.\n\nSlew rate is the maximum rate of change of the output, usually quoted in volts per second (or microsecond). Many amplifiers are ultimately slew rate limited (typically by the impedance of a drive current having to overcome capacitive effects at some point in the circuit), which sometimes limits the full power bandwidth to frequencies well below the amplifier's small-signal frequency response.\n\nThe rise time, t, of an amplifier is the time taken for the output to change from 10% to 90% of its final level when driven by a step input.\nFor a Gaussian response system (or a simple RC roll off), the rise time is approximated by:\n\nt * BW = 0.35, where t is rise time in seconds and BW is bandwidth in Hz.\n\nThe time taken for the output to settle to within a certain percentage of the final value (for instance 0.1%) is called the settling time, and is usually specified for oscilloscope vertical amplifiers and high-accuracy measurement systems. Ringing refers to an output variation that cycles above and below an amplifier's final value and leads to a delay in reaching a stable output. Ringing is the result of overshoot caused by an underdamped circuit.\n\nIn response to a step input, the overshoot is the amount the output exceeds its final, steady-state value.\n\nStability is an issue in all amplifiers with feedback, whether that feedback is added intentionally or results unintentionally. It is especially an issue when applied over multiple amplifying stages.\n\nStability is a major concern in RF and microwave amplifiers. The degree of an amplifier's stability can be quantified by a so-called stability factor. There are several different stability factors, such as the Stern stability factor and the Linvil stability factor, which specify a condition that must be met for the absolute stability of an amplifier in terms of its two-port parameters.\n\n\n"}
{"id": "1028926", "url": "https://en.wikipedia.org/wiki?curid=1028926", "title": "Architectural acoustics", "text": "Architectural acoustics\n\nArchitectural acoustics (also known as room acoustics and building acoustics) is the science and engineering of achieving a good sound within a building and is a branch of acoustical engineering. The first application of modern scientific methods to architectural acoustics was carried out by Wallace Sabine in the Fogg Museum lecture room who then applied his new found knowledge to the design of Symphony Hall, Boston.\n\nArchitectural acoustics can be about achieving good speech intelligibility in a theatre, restaurant or railway station, enhancing the quality of music in a concert hall or recording studio, or suppressing noise to make offices and homes more productive and pleasant places to work and live in. Architectural acoustic design is usually done by acoustic consultants.\n\nThis science analyzes noise transmission from building exterior envelope to interior and vice versa. The main noise paths are roofs, eaves, walls, windows, door and penetrations. Sufficient control ensures space functionality and is often required based on building use and local municipal codes. An example would be providing a suitable design for a home which is to be constructed close to a high volume roadway, or under the flight path of a major airport, or of the airport itself.\n\nThe science of limiting and/or controlling noise transmission from one building space to another to ensure space functionality and speech privacy. The typical sound paths are ceilings, room partitions, acoustic ceiling panels (such as wood dropped ceiling panels), doors, windows, flanking, ducting and other penetrations. Technical solutions depend on the source of the noise and the path of acoustic transmission, for example noise by steps or noise by (air, water) flow vibrations. An example would be providing suitable party wall design in an apartment complex to minimize the mutual disturbance due to noise by residents in adjacent apartments.\n\n This is the science of controlling a room's surfaces based on sound absorbing and reflecting properties. Excessive reverberation time, which can be calculated, can lead to poor speech intelligibility.\n\nSound reflections create standing waves that produce natural resonances that can be heard as a pleasant sensation or an annoying one. Reflective surfaces can be angled and coordinated to provide good coverage of sound for a listener in a concert hall or music recital space. To illustrate this concept consider the difference between a modern large office meeting room or lecture theater and a traditional classroom with all hard surfaces.\nInterior building surfaces can be constructed of many different materials and finishes. Ideal acoustical panels are those without a face or finish material that interferes with the acoustical infill or substrate. Fabric covered panels are one way to heighten acoustical absorption. Perforated metal also shows sound absorbing qualities. Finish material is used to cover over the acoustical substrate. Mineral fiber board, or Micore, is a commonly used acoustical substrate. Finish materials often consist of fabric, wood or acoustical tile. Fabric can be wrapped around substrates to create what is referred to as a \"pre-fabricated panel\" and often provides good noise absorption if laid onto a wall.\n\nPrefabricated panels are limited to the size of the substrate ranging from to . Fabric retained in a wall-mounted perimeter track system, is referred to as \"on-site acoustical wall panels\". This is constructed by framing the perimeter track into shape, infilling the acoustical substrate and then stretching and tucking the fabric into the perimeter frame system. On-site wall panels can be constructed to accommodate door frames, baseboard, or any other intrusion. Large panels (generally, greater than ) can be created on walls and ceilings with this method. Wood finishes can consist of punched or routed slots and provide a natural look to the interior space, although acoustical absorption may not be great.\n\nThere are four ways to improve workplace acoustics and solve workplace sound problems – the ABCDs.\n\nBuilding services noise control is the science of controlling noise produced by:\n\nInadequate control may lead to elevated sound levels within the space which can be annoying and reduce speech intelligibility. Typical improvements are vibration isolation of mechanical equipment, and sound traps in ductwork. Sound masking can also be created by adjusting HVAC noise to a predetermined level.\n\n\n"}
{"id": "7587599", "url": "https://en.wikipedia.org/wiki?curid=7587599", "title": "Baggage handling system", "text": "Baggage handling system\n\nA baggage handling system (BHS) is a type of conveyor system installed in airports that transports checked luggage from ticket counters to areas where the bags can be loaded onto airplanes. A BHS also transports checked baggage coming from airplanes to baggage claims or to an area where the bag can be loaded onto another airplane.\n\nAlthough the primary function of a BHS is the transportation of bags, a typical BHS will serve other functions involved in making sure that a bag gets to the correct location in the airport. \"Sortation\" is the process of identifying a bag and the information associated with it, to decide where the bag should be directed within the system.\n\nIn addition to sortation, a BHS may also perform the following functions:\n\nThere is an entire process that the BHS controls. From the moment the bag is set on the inbound conveyor, to the gathering conveyor, through sorting until it arrives at the designated aircraft and onto the baggage carousel after the flight, the BHS has control over the bag\n\nMany baggage handling systems offer software to better manage the system. There has also been a breakthrough with \"mobile\" BHS software where managers of the system can check and correct problems from their mobile phone.\n\nPost September 11, 2001, the majority of airports around the world began to implement baggage screening directly into BHS systems. These systems are referred to as \"Checked Baggage Inspection System\" by the Transportation Security Administration (TSA) in the USA, where bags are fed directly into Explosive Detection System (EDS) machines. A CBIS can sort baggage based on each bag's security status assigned by an EDS machine or by a security screening operator.\n\nThe first automated baggage handling system was invented by BNP Associates in 1971, and this technology is in use in almost every major airport worldwide today.\n\n"}
{"id": "41711059", "url": "https://en.wikipedia.org/wiki?curid=41711059", "title": "Belvedere Apartments (Columbia, Missouri)", "text": "Belvedere Apartments (Columbia, Missouri)\n\nBelvedere Apartments is a well-preserved Spanish Colonial Revival-style apartment building located at 206 Hitt Street in downtown Columbia, Missouri. Built in 1927, the building was designed by notable architect, Nelle Peters. The building was added to Columbia's Notable Buildings List in 2008 and is a possible candidate for the National Register of Historic Places.\nAs of 2014, the apartments are being rented by private owners.\n"}
{"id": "32600833", "url": "https://en.wikipedia.org/wiki?curid=32600833", "title": "Bristle blasting", "text": "Bristle blasting\n\nBristle blasting is a mechanical abrasion cleaning process that is performed on metallic surfaces by a brush-like rotary power tool. The tool consists of sharpened, high-carbon steel wire bristle tips that are designed with a forward-angle bend, i.e., the shank of the wire is bent in the direction of tool rotation. During operation, the rotating bristle tips are brought into direct contact with the metallic surface, whereby the bristle tips strike the surface with kinetic energy that is equivalent to processes that use grit blast media. This repeated contact of sharp bristle tips with the target surface results in localized impact, rebound, and the formation of craters, thereby simultaneously cleaning and coarsening the surface, while exposing a contamination-free base metal.\n\nThe difference from conventional wire-brushing with an angle grinder lies first with the mounting of the bristles in the rotating brush. With a conventional rotary brush, the wire bristles are mounted rigidly in the hub, bristles sometimes being twisted or knotted in groups for extra stiffness. With bristle blasting, the wires are mounted resiliently in a flexible belt. As the brush rotates, immediately before contacting the work, rows of bristles are trapped behind a fixed \" 'accelerator bar' \", which causes them to bend backwards in their flexible mount. As the bristles pass the accelerator bar, they are released to spring forwards. The extra energy stored for each bristle when being flexed backwards is added to their kinetic energy when released, and when they impact the surface of the workpiece. This gives a more violent impact with the surface, thus greater cleaning power, compared to a simple rotary brush at the same speed.\n\nAs the geometry of the brush is also distorted from circular by the accelerator bar, the contact angle of the bristle tips with the surface is also nearer to perpendicular than the tangential contact of the circular rotary brush. This encourages a cleaning action and reduces the burnishing action that can be a problem with rotary brushes, where a surface is increasingly polished, but not truly cleaned of surface adherrents. The steeper action of the tips encourages a pitting effect on the surface, which may be considered useful for further painting.\n\nThe cleaning performance of the bristle blasting process is a consequence of synchronized impact of the bristle tips onto the target surface. The tool head with the bristle belt rotates at approximately 2,500 rpm. A so-called 'accelerator bar' detains the bristle tips and by releasing them increases their kinetic energy with which they strike the surface. Immediately after the bristles strike the corroded steel surface, they retract (rebound) from the surface ('single-impact'), which results in corrosion removal and a micro-identation that exposes the sheer metallic surface. The multitude of such primary impact craters generates a texture and surface which in visual cleanliness and roughness (anchor profile) mimics those obtained by grit blasting processes. The cleaned and coarsened surface is deemed favorable for the subsequent application and adhesion of protective films and industrial coatings.\n\nBristle blasting tools are fabricated from high-carbon steel wires that protrude through a flexible circular belt. The belt, in turn, is attached to a rotating hub, which is powered by an electric or pneumatically driven spindle. The tool is lightweight, portable, and easily implemented by workers without the need for elaborate set-up or sophisticated safety apparatus.\n\nBristle blasting is most frequently used for removal of unwanted films and layers of corrosion that can form on metallic surfaces. Common applications include cleaning, preparation, and refurbishment of iron and steel components that are used for fabricating bridges, ships, and pipeline systems.\n\n"}
{"id": "1430422", "url": "https://en.wikipedia.org/wiki?curid=1430422", "title": "Built-in self-test", "text": "Built-in self-test\n\nA built-in self-test (BIST) or built-in test (BIT) is a mechanism that permits a machine to test itself. Engineers design BISTs to meet requirements such as:\n\n\nor constraints such as:\n\n\nThe main purpose of BIST is to reduce the complexity, and thereby decrease the cost and reduce reliance upon external (pattern-programmed) test equipment. BIST reduces cost in two ways:\nBoth lead to a reduction in hourly charges for automated test equipment (ATE) service.\n\nThe BIST name and concept originated with the idea of including a pseudorandom number generator (PRNG) and cyclic redundancy check (CRC) on the IC. If all the registers that hold state in an IC are on one or more internal scan chains, then the function of the registers and the combinational logic between them will generate a unique CRC signature over a large enough sample of random inputs. So all an IC need do is store the expected CRC signature and test for it after a large enough sample set from the PRNG. The CRC comparison with expected signature or the actual resultant CRC signature is typically accessed via the JTAG IEEE 1149.1 standard.\n\nBIST is commonplace in weapons, avionics, medical devices, automotive electronics, complex machinery of all types, unattended machinery of all types, and integrated circuits.\n\nAutomotive tests itself to enhance safety and reliability. For example, most vehicles with antilock brakes test them once per safety interval. If the antilock brake system has a broken wire or other fault, the brake system reverts to operating as a normal brake system. Most automotive engine controllers incorporate a \"limp mode\" for each sensor, so that the engine will continue to operate if the sensor or its wiring fails. Another, more trivial example of a limp mode is that some cars test door switches, and automatically turn lights on using seat-belt occupancy sensors if the door switches fail.\n\nAlmost all avionics now incorporate BIST. In avionics, the purpose is to isolate failing line-replaceable units, which are then removed and repaired elsewhere, usually in depots or at the manufacturer. Commercial aircraft only make money when they fly, so they use BIST to minimize the time on the ground needed for repair and to increase the level of safety of the system which contains BIST. Similar arguments apply to military aircraft. When BIST is used in flight, a fault causes the system to switch to an alternative mode or equipment that still operates. Critical flight equipment is normally duplicated, or redundant. Less critical flight equipment, such as entertainment systems, might have a \"limp mode\" that provides some functions.\n\nBuilt-In-Self-Test is used to make faster, less-expensive integrated circuit manufacturing tests. The IC has a function that verifies all or a portion of the internal functionality of the IC. In some cases, this is valuable to customers, as well. For example, a BIST mechanism is provided in advanced fieldbus systems to verify functionality. At a high level this can be viewed similar to the PC BIOS's power-on self-test (POST) that performs a self-test of the RAM and buses on power-up.\n\nThe typical personal computer tests itself at start-up (called POST) because it's a very complex piece of machinery. Since it includes a computer, a computerized self-test was an obvious, inexpensive feature. Most modern computers, including embedded systems, have self-tests of their computer, memory and software.\n\nUnattended machinery performs self-tests to discover whether it needs maintenance or repair. Typical tests are for temperature, humidity, bad communications, burglars, or a bad power supply. For example, power systems or batteries are often under stress, and can easily overheat or fail. So, they are often tested.\n\nOften the communication test is a critical item in a remote system. One of the most common, and unsung unattended system is the humble telephone concentrator box. This contains complex electronics to accumulate telephone lines or data and route it to a central switch. Telephone concentrators test for communications continuously, by verifying the presence of periodic data patterns called frames (See SONET). Frames repeat about 8,000 times per second. \n\nRemote systems often have tests to loop-back the communications locally, to test transmitter and receiver, and remotely, to test the communication link without using the computer or software at the remote unit. Where electronic loop-backs are absent, the software usually provides the facility. For example, IP defines a local address which is a software loopback (IP-Address 127.0.0.1, usually locally mapped to name \"localhost\"). \n\nMany remote systems have automatic reset features to restart their remote computers. These can be triggered by lack of communications, improper software operation or other critical events. Satellites have automatic reset, and add automatic restart systems for power and attitude control, as well.\n\nMedical devices test themselves to assure their continued safety. Normally there are two tests. A power-on self-test (POST) will perform a comprehensive test. Then, a periodic test will assure that the device has not become unsafe since the power-on self test. Safety-critical devices normally define a \"safety interval\", a period of time too short for injury to occur. The self test of the most critical functions normally is completed at least once per safety interval. The periodic test is normally a subset of the power-on self test.\n\nOne of the first computer-controlled BIST systems was in the U.S.'s Minuteman Missile. Using an internal computer to control the testing reduced the weight of cables and connectors for testing. The Minuteman was one of the first major weapons systems to field a permanently installed computer-controlled self-test.\n\nThere are several specialized versions of BIST which are differentiated according to what they do or how they are implemented:\n\n\n\n"}
{"id": "13128582", "url": "https://en.wikipedia.org/wiki?curid=13128582", "title": "Capacitance probe", "text": "Capacitance probe\n\nCapacitance sensors (or Dielectric sensors) use capacitance to measure the dielectric permittivity of a surrounding medium. \nThe configuration is like the neutron probe where an access tube made of PVC is installed in the soil; probes can also be modular (comb-like) and connected to a logger. The sensing head consists of an oscillator circuit, the frequency is determined by an annular electrode, fringe-effect capacitor, and the dielectric constant of the soil. \nEach capacitor sensor consists of two metal rings mounted on the circuit board at some distance from the top of the access tube. These rings are a pair of electrodes, which form the plates of the capacitor with the soil acting as the dielectric in between. The plates are connected to an oscillator, consisting of an inductor and a capacitor. The oscillating electrical field is generated between the two rings and extends into the soil medium through the wall of the access tube. The capacitor and the oscillator form a circuit, and changes in dielectric constant of surrounding media are detected by changes in the operating frequency. The capacitance sensors are designed to oscillate in excess of 100 MHz inside the access tube in free air. The output of the sensor is the frequency response of the soil’s capacitance due to its soil moisture level.\n\n\nFrequency domain sensor\n\n"}
{"id": "57619800", "url": "https://en.wikipedia.org/wiki?curid=57619800", "title": "Carolyn Griffiths", "text": "Carolyn Griffiths\n\nCarolyn Griffiths is a railway engineer. She was the first Chief Inspector of the UK's Rail Accident Investigation Branch and a President of the Institution of Mechanical Engineers.\n\nGriffiths was elected Fellow of the Royal Academy of Engineering (FREng) in 2013.\n"}
{"id": "46872872", "url": "https://en.wikipedia.org/wiki?curid=46872872", "title": "Class district", "text": "Class district\n\nClass districts () were a classification system for railway goods wagons used by the Deutsche Reichsbahn (1920–1945) in Germany between the wars.\n\nAfter the Deutsche Reichsbahn had been founded in 1920, in 1921 all goods wagons types with the same or similar roles were grouped into so-called class districts. These were named after cities that were the headquarters of a Reichsbahn division or, later, other cities too. Work on re-lettering and renaming the wagons began in 1922 and was largely completed by 1924.\nLegende:\n\n"}
{"id": "30257957", "url": "https://en.wikipedia.org/wiki?curid=30257957", "title": "Constantin Budeanu", "text": "Constantin Budeanu\n\nConstantin Budeanu (28 February 1886 - 1959) was a Romanian electrical engineer who contributed to the analysis of electric networks states and the SI system of units.\n\nHe studied electricity in Paris with a V. Adamachi scholarship gained after the completion of studies in Bucharest. \nHe proposed the unit electric reactive power and he introduced the concept of \"deformed power\" in electric networks.\n\n\nHe was awarded the Order of Labour by the communist authorities of Romania.\n\n"}
{"id": "2475896", "url": "https://en.wikipedia.org/wiki?curid=2475896", "title": "Control knob", "text": "Control knob\n\nA control knob is a rotary control used to provide input to a device when grasped by an operator and turned, so that the degree of rotation corresponds to the desired input. Such knobs are one of the most common components in control systems and are found on all sorts of devices.\n\nSuch knobs vary greatly in form, but as a rule they are expected to be grasped by the fingertips. By convention a clockwise rotation ordinarily produces an \"increased\" input, whatever that is understood to be. Knobs may turn continuously or may have detents to produce discrete selections; they commonly have a scale with a pointer to aid achievement of the desired setting, though it is common for the gradations in the scale to have no concrete meaning (e.g. the markings of a volume control). There is frequently a fixed \"off\" position at the origin of movement in which the knob actuates a switch shutting down whatever behavior is controlled, rather than having a separate on/off switch.\n\nA control knob works by turning a shaft which connects to the component which produces the actual input. Common control components used include potentiometers, variable capacitors, and rotary switches. An example where the knob does not produce a variation in an electrical signal may be found in many toasters, where the darkness knob moves the thermostat in such a way as to change the temperature at which it opens and releases the cooked toast. Some similar controls produce similar inputs using different geometry; for example, the knob may be replaced by a lever which is moved through an angle. Another example is the sliding controls which frequently replace knobs as level controls in audio equipment.\n\nThe use of knobs is an important aspect of the design of user interfaces in these devices. Particular attention needs to be paid to the feedback to the operator from the adjustments being made. The use of a pointer on the knob in conjunction with a scale assists in producing repeatable settings; in other cases there may be a dial or other indicator which is either mechanically linked the knob's rotation (as in many older radio tuners) or which reports the behavior being controlled.\n"}
{"id": "7988291", "url": "https://en.wikipedia.org/wiki?curid=7988291", "title": "Cookie jar", "text": "Cookie jar\n\nCookie jars are utilitarian or decorative ceramic or glass jars often found in American and Canadian kitchens. In the United Kingdom, they are known as biscuit barrels or biscuit jars. If they are cans made out of tinplate, they are called biscuit tins. While used to store actual cookies or biscuits, they are sometimes employed to store other edible items like candy or dog treats, or non-edible items like currency (in the manner of a piggy bank).\n\nCookie jars, also known as biscuit barrels or jars, have been used in England since the latter part of the 18th century. They were often made of glass with metal lids. Cookie jars became popular in America around the time of the Great Depression in 1929. Early American cookie jars were made of glass with metal screw-on lids. In the 1930s, stoneware became predominant as the material for American cookie jars. Early cookie jars typically have simple cylindrical shapes and were often painted with floral or leaf decorations or emblazoned with colorful decals.\n\nThe Brush Pottery Company of Zanesville, Ohio is generally recognized as producing the first ceramic cookie jar. The jar was green with the word \"Cookies\" embossed on the front. Most cookie jar manufacturers followed Brush's move to ceramics in the late 1930s, and designs became more innovative with figures, fruits, vegetables, animals, and other whimsical interpretations such as the Hull \"Little Red Riding Hood\" predominating. The golden period for American cookie jar production covers the years from 1940 until 1970, with several manufacturers rising to prominence.\n\n\nArtist Andy Warhol amassed a collection of 175 ceramic cookie jars. These were in a multitude of shapes and figures. Most were purchased at flea markets. Warhol's collection was featured in a prominent news magazine and sparked an interest in collecting cookie jars. When asked in the 1970s why he pursued the 1930s and 1940s jars, Warhol said simply, \"They are time pieces.\" At an auction of his apartment's contents in 1987, Warhol's collection of cookie jars realized $250,000.\n\n\n\n"}
{"id": "660026", "url": "https://en.wikipedia.org/wiki?curid=660026", "title": "Decibel watt", "text": "Decibel watt\n\nThe decibel watt or dBW is a unit for the measurement of the strength of a signal expressed in decibels relative to one watt. It is used because of its capability to express both very large and very small values of power in a short range of number; e.g., 1 milliwatt = −30 dBW, 1 watt = 0 dBW, 10 watts = 10 dBW, 100 watts = 20 dBW, and 1,000,000 W = 60 dBW.\n\nand also\n\nCompare dBW to dBm, which is referenced to one milliwatt (0.001 W).\n\nA given dBW value expressed in dBm is always 30 more because 1 watt is 1,000 milliwatts, and a ratio of 1,000 (in power) is 30 dB; e.g., 10 dBm (10 mW) is equal to −20 dBW (0.01 W).\n\nAlthough the decibel (dB) is permitted for use alongside SI units, the dBW is not.\n\n"}
{"id": "3733092", "url": "https://en.wikipedia.org/wiki?curid=3733092", "title": "Design leadership", "text": "Design leadership\n\nDesign leadership is a concept complementary to design management. In practice, design managers within companies often operate in the field of design leadership and design leaders in the field of design management. However, the two terms are not interchangeable, they are interdependent. In essence, and at the highest level, design leadership helps to define the future and design management provides key tools for getting there. Both are critically important to business, government and society, and both are necessary in order to maximise value from design activity and investment.\n\nDesign leadership can be described as leadership that generates innovative design solutions. Turner defines design leadership by adding three additional aspects for design leadership,\n\nTurner separates the core responsibilities of design leadership in following six activities:\n\n\n"}
{"id": "42966614", "url": "https://en.wikipedia.org/wiki?curid=42966614", "title": "Diesel (film)", "text": "Diesel (film)\n\nDiesel is a 1942 German biographical film directed by Gerhard Lamprecht and starring Willy Birgel, Hilde Weissner and Paul Wegener. It portrays the life of Rudolf Diesel, the German inventor of the diesel engine. It was one of a series of prestigious biopics made in Nazi Germany portraying genius inventors or artists struggling against the societies in which they live. The film was based on a biography by Eugen Diesel, one of Diesel's children. \n\nIt was shot at the Babelsberg Studios in Berlin. The film's sets were designed by art director Erich Kettelhut. The film was made on a large budget of 2,349,000 reichsmarks, but was a popular box office success and was able to recoup its production costs.\n\n\n"}
{"id": "37822922", "url": "https://en.wikipedia.org/wiki?curid=37822922", "title": "Dip reader", "text": "Dip reader\n\nA DIP reader (Document Insertion Processor) is an electronic device for reading an electronically encoded card that is inserted and then removed from the device.\n\nA typical dip reader is used for reading credit cards where the data are either encoded on a magnetic stripe or an internal computer chip. The magnetic stripe on a card is typically read as the card is extracted. If the card is a smart card, then the data transfer typically takes place when the card is fully inserted. In this case, the card is held while data transfer is taking place.\n"}
{"id": "29567109", "url": "https://en.wikipedia.org/wiki?curid=29567109", "title": "E-Stewards", "text": "E-Stewards\n\nThe e-Stewards Initiative is an electronics waste recycling standard created by the Basel Action Network.\n\nThe program and the organization that created it grew out of the concern that electronic waste generated in wealthy countries was being dismantled in poor countries, often by underage workers. The young workers were being exposed to toxic metals and working in unsafe conditions.\n\nIn 2009, BAN published the e-Stewards Standard for Responsible Recycling and Reuse of Electronic Equipment which set forth requirements for becoming a Certified e-Stewards Recycler—a program that \"recognizes electronics recyclers that adhere to the most stringent environmentally and socially responsible practices when recovering hazardous electronic materials.\" Recyclers that were qualified under the older Pledge program had until 1 September 2011 to achieve certification to the Standard by an e-Stewards Accredited Certification Body accredited by ANAB (ANSI-ASQ National Accreditation Board).\n\nThe e-Stewards Standard for Responsible Recycling and Reuse of Electronic Equipment was developed by the Basel Action Network. It is an industry-specific environmental management system standard that is the basis for the e-Stewards Initiative. On 6 March 2012, BAN released and updated revised Version 2.0 to open public comment prior to its final adoption later in the spring of 2012.\n\nThe certification is available to all electronics recyclers and refurbishers. To achieve an e-Stewards certification organizations are subject to an initial Stage I and Stage II audit. After passing such audits and being accepted by BAN, yearly surveillance audits take place to ensure organizations with the standard and have a registered ISO 14001 environmental management system in place, as well as achieving numerous performance requirements including assuring no export of hazardous electronic wastes to developing countries, no use of prison labor and no dumping of toxic materials in municipal landfills.\n\n\n"}
{"id": "2163724", "url": "https://en.wikipedia.org/wiki?curid=2163724", "title": "Ground glass", "text": "Ground glass\n\nGround glass is glass whose surface has been ground to produce a flat but rough (matte) finish, in which the glass is in small sharp fragments \n\nGround glass surfaces have many applications, ranging from ornamentation on windows and table glassware to scientific uses in optics and laboratory glassware.\n\nIn photography, a sheet of ground glass is used for the manual focusing in some still and motion picture cameras, the ground-glass viewer is inserted in the back of the camera, and the lens opened to its widest aperture. This projects the scene on the ground glass upside down. The photographer focuses and composes using this projected image, sometimes with the aid of a magnifying glass (or loupe). In order to see the image better, a dark cloth is used to block out light, whence came the image of the old-time photographer with his head stuck under a large black cloth.\n\nA ground glass is also used in the reflex finder of an SLR or TLR camera.\n\nIn motion picture cameras, the ground glass is a small, usually removable piece of transparent glass that sits between the rotary disc shutter and the viewfinder. The ground glass usually contains precise markings to show the camera operator the boundaries of the frame or the center reticle, or any other important information. Because the ground glass is positioned between the mirror shutter and the viewfinder, it does not interfere with the image reaching the film and is therefore not recorded over the final image, but rather serves as a reference for the camera operator.\n\nGround glasses commonly serve as a framing reference for a desired aspect ratio. Because most films shot with spherical lenses are shot full-frame and later masked during projection to a more widescreen aspect ratio, it is important not only for the operator to be able to see the boundaries of that aspect ratio, but also for the ground glass to be properly aligned in the camera so that the markings are an exact representation of the boundaries of the image recorded on film.\n\nGround or frosted glass is widely used as a weather- and heat-proof light diffuser in ambient lighting, namely on glass covers or enclosures for lamp fixtures, and sometimes on incandescent bulbs. Its functions include reducing glare and preventing retinal damage by direct sight of the lamp filament. This hides unsightly details of the lamp and fixture without blocking its light, yielding a softer illumination without giving hard shadows.\n\nGround glass surfaces are often found on the glass equipment of chemical laboratories.\n\nGlass flasks, stoppers, valves, funnels, and tubing are often connected together by ground glass joints, matching pairs of conical or spherical surfaces that have been ground to a precise shape.\n\nFlasks and test tubes often have a small ground-glass label area on the side. (Pencil writing on ground glass is largely inert, rub-proof and waterproof, but can be easily erased.)\n\nAn optical microscope may include a ground- or frosted-glass diffuser to evenly illuminate the field behind the specimen. Microscope slides are often ground on the sides and beveled on the corners to soften the edges for safer handling.\n\nPopular belief for many centuries is that ground-up glass (i.e., glass broken into tiny fragments) can kill if swallowed. In fact, this is a myth, as it is largely ineffective.\n\nThe Guy de Maupassant short story \"La Confession\" concerns a jealous girl who poisons her older sister's suitor by inserting ground-up glass into cake.\n\n\n"}
{"id": "13946920", "url": "https://en.wikipedia.org/wiki?curid=13946920", "title": "History of robots", "text": "History of robots\n\nThe history of robots has its origins in the ancient world. The modern concept began to be developed with the onset of the Industrial Revolution, which allowed the use of complex mechanics, and the subsequent introduction of electricity. This made it possible to power machines with small compact motors. In the early 20th century, the notion of a humanoid machine was developed. Today, one can envisage human-sized robots with the capacity for near-human thoughts and movement. \n\nThe first uses of modern robots were in factories as industrial robots – simple fixed machines capable of manufacturing tasks which allowed production with less need for human assistance. Digitally controlled industrial robots and robots using artificial intelligence have been built since the 2000s.\n\nConcepts of artificial servants and companions date at least as far back as the ancient legends of Cadmus, who is said to have sown dragon teeth that turned into soldiers and Pygmalion whose statue of Galatea came to life. Many ancient mythologies included artificial people, such as the talking mechanical handmaidens built by the Greek god Hephaestus (Vulcan to the Romans) out of gold, the clay golems of Jewish legend and clay giants of Norse legend. \n\nIn Greek mythology, Hephaestus created utilitarian three-legged tables that could move about under their own power, and a bronze man, Talos, that defended Crete. Talos was eventually destroyed by Medea who cast a lightning bolt at his single vein of lead. To take the golden fleece Jason was also required to tame two fire-breathing bulls with bronze hooves; and like Cadmus he sowed the teeth of a dragon into soldiers.\n\nThe Indian \"Lokapannatti\" (11th/12th centuries) tells the story of King Ajatashatru of Magadha, who gathered the Buddha's relics and hid them in an underground stupa. The relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya, until they were disarmed by King Ashoka. In the Egyptian legend of Rocail, the younger brother of Seth created a palace and a sepulcher containing autonomous statues that lived out the lives of men so realistically that they were mistaken for having souls.\n\nInspired by European Christian legend medieval Europeans devised brazen heads that could answer questions posed to them. Albertus Magnus was supposed to have constructed an entire android which could perform some domestic tasks, but it was destroyed by Albert's student Thomas Aquinas for disturbing his thought. The most famous legend concerned a bronze head devised by Roger Bacon which was destroyed or scrapped after he missed its moment of operation. Automata resembling humans or animals were popular in the imaginary worlds of medieval literature.\n\nMechanical automata were constructed in the 10th century BC in the Western Zhou Dynasty. The artisan Yan Shi made humanoid automata that could sing and dance. The machine is said to have possessed lifelike organs, like bones, muscles and joints. The Cosmic Engine, a clock tower built by Su Song in Kaifeng, China, in 1088, featured mechanical mannequins that chimed the hours, ringing gongs or bells among other devices. Fetes of automation continued into the Tang Dynasty. Daifeng Ma built a automated dresser servant for the queen. Ying Wenliang built a automata man that proposed toasts at banquets and a wooden woman automata that played the sheng. Among the best documented automata of ancient China are that of Han Zhile, a Japanese who moved to China in the early 9th century BC.\n\nIn the 4th century BC the mathematician Archytas of Tarentum postulated a mechanical bird he called \"The Pigeon\", which was propelled by steam. Yet another early automaton was the clepsydra, made in 250 BC by Ctesibius of Alexandria, a physicist and inventor from Ptolemaic Egypt. Hero of Alexandria made numerous innovations in the field of automata, including one that allegedly could speak. Taking up the earlier reference in Homer's Iliad, Aristotle speculated in his \"Politics\" (ca. 322 BC, book 1, part 4) that automata could some day bring about human equality by making possible the abolition of slavery:\n\nThere is only one condition in which we can imagine managers not needing subordinates, and masters not needing slaves. This condition would be that each instrument could do its own work, at the word of command or by intelligent anticipation, like the statues of Daedalus or the tripods made by Hephaestus, of which Homer relates that \"Of their own motion they entered the conclave of Gods on Olympus\", as if a shuttle should weave of itself, and a plectrum should do its own harp playing.\n\nAl-Jazari (1136–1206), a Muslim inventor during the Artuqid dynasty, designed and constructed a number of automatic machines, including kitchen appliances and musical automata powered by water. One particularly complex automaton included four automatic musicians that floated on a lake.\n\nHero's works on automata were translated into Latin amid the 12th century Renaissance. The early 13th-century artist-engineer Villard de Honnecourt sketched plans for several automata. At the end of the 13th century, Robert II, Count of Artois, built a pleasure garden at his castle at Hesdin that incorporated a number of robots, humanoid and animal.\n\nAmong the first verifiable automation is a humanoid drawn by Leonardo da Vinci (1452–1519) in around 1495. Leonardo's notebooks, rediscovered in the 1950s, contain detailed drawings of a mechanical knight in armour which was able to sit up, wave its arms and move its head and jaw. In 1533, Johannes Müller von Königsberg created an automaton eagle and fly made of iron; both could fly. John Dee is also known for creating a wooden beetle, capable of flying. \nThe 17th century thinker Rene Descartes believed that animals and humans were biological machines. On his last trip to Norway, he took with him a mechanical doll that looked like his dead daughter Francine. In the 18th century the master toy maker Jaques de Vaucanson built for Louis XV an automated duck with hundreds of moving parts, which could eat and drink. Vaucanson subsequently built humanoid automatons, a drummer and fife player were noted for their anatomical similarity to real human beings. Vaucanson's creation inspired European watchmakers to manufacture mechanical automata and it became fashionable among the European aristocracy to collect sophisticated mechanical devices for entertainment. In the 1770s the Swiss Pierre Jaquet-Droz created moving automata that looked like children, which delighted Mary Shelly, who went on to write \"\". The ultimate attempt at automation was The Turk by Wolfgang von Kempelen, a sophisticated machine that could play chess against a human opponent and toured Europe. When the machine was brought to the new world, it prompted Edgar Allan Poe to pen an essay, in which he concluded that it was impossible for mechanical devices to reason or think.\nThe Japanese craftsman Hisashige Tanaka, known as \"Japan's Edison\", created an array of extremely complex mechanical toys, some of which could serve tea, fire arrows drawn from a quiver, or even paint a Japanese \"kanji\" character. The landmark text \"Karakuri Zui\" (\"Illustrated Machinery\") was published in 1796.\n\nIn the book \"The Wonderful Wizard of Oz\", robots were called \"mechanical men\". A notable character was the Tin Woodman, a man made of tin who chopped trees in the forests of Oz.\n\nIn World War I remote control weapons were used, based on the work of Nikola Tesla, who had constructed an electrical boat that could be remotely controlled by radio.\n\nThe term \"robot\" was first used in a play published by the Czech Karel Čapek in 1920. \"R.U.R.\" (Rossum's Universal Robots) was a satire, robots were manufactured biological beings that performed all unpleasant manual labor. According to Čapek, the word was created by his brother Josef from the Czech \"robota\", meaning servitude. The play \"R.U.R,\" replaced the popular use of the word \"automaton\". However, until the 1950s \"robot\" was pronounced \"robit\" in films, radio and television programs: examples are \"The Lonely\" episode of the TV series \"The Twilight Zone\", first aired on 15 November 1959, and the sci-fi radio program \"X Minus One\".\n\nWestinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1927, Fritz Lang's Metropolis was released; the Maschinenmensch (\"machine-human\"), a gynoid humanoid robot, also called \"Parody\", \"Futura\", \"Robotrix\", or the \"Maria impersonator\" (played by German actress Brigitte Helm), was the first robot ever to be depicted on film.\n\nThe most famous Japanese robotic automata was presented to the public in 1927. The Gakutensoku was suppose to have a diplomatic role. Actuated by compressed air, it could write fluidly and raise its eyelids.. Many robots were constructed before the dawn of computer-controlled servomechanisms, for the public relations purposes of major firms. These were essentially machines that could perform a few stunts, like the automata of the 18th century. In 1928, one of the first humanoid robots was exhibited at the annual exhibition of the Model Engineers Society in London. Invented by W. H. Richards, the robot - named Eric - consisted of an aluminium suit of armour with eleven electromagnets and one motor powered by a 12-volt power source. The robot could move its hands and head and could be controlled by remote control or voice control.\n\nIn 1939, the humanoid robot known as Elektro appeared at the World's Fair. Seven feet tall (2.1 m) and weighing 265 pounds (120 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear cam and motor skeleton covered by an aluminium skin.\n\nIn 1939 Konrad Zuse constructed the first programmable electromechanical computer, laying the foundation for the construction of a humanoid machine that is now deemed a robot.\n\nIn 1941 and 1942, Isaac Asimov formulated the Three Laws of Robotics, and in the process coined the word \"robotics\". In 1945 Vannevar Bush published As We May Think, an essay that investigated the potential of electronic data processing. He predicted the rise of computers, digital word processors, voice recognition and machine translation. He was later credited by Ted Nelson, the inventor of hypertext. In 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.\n\nThe first electronic autonomous robots with complex behavior were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors - essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named Elmer and Elsie, were constructed between 1948 and 1949 and were often described as \"tortoises\" due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.\n\nWalter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. Walter's work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's \"turtles\" may be found in the form of BEAM robotics.\n\nIn 1951 Walter published the paper \"A Machine that learns\", documenting how his more advanced mechanical robots acted as intelligent agent by demonstrating conditioned reflex learning. The first digitally operated and programmable robot was invented by George Devol in 1954 and was called the Unimate. This later laid the foundations of the modern robotics industry. \n\nIn Japan robots became popular comic book characters. Robots became cultural icons and the Japanese government was spurred into funding research into robotics. Among the most iconic characters was the Astro Boy, who is taught human feelings such as love, courage and self-doubt. Culturally, robots in Japan became regarded as helpmates to their human counterparts..\n\nDevol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Ewing Township, New Jersey to lift hot pieces of metal from a die casting machine and place them in cooling liquid. \"Without any fanfare, the world's first working robot joined the assembly line at the General Motors plant in Ewing Township in the spring of 1961... It was an automated die-casting mold that dropped red-hot door handles and other such car parts into pools of cooling liquid on a line that moved them along to workers for trimming and buffing.\" Devol's patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.\n\nThe Rancho Arm was developed as a robotic arm to help handicapped patients at the Rancho Los Amigos Hospital in Downey, California; this computer-controlled arm was bought by Stanford University in 1963. In 1967 the first industrial robot was put to productive use in Japan. The Versatran robot had been developed by American Machine and Foundry. A year later a hydraulic robot design by Unimation was put into production by Kawasaki Heavy Industries. Marvin Minsky created the Tentacle Arm in 1968; the arm was computer-controlled and its 12 joints were powered by hydraulics. In 1969 Mechanical Engineering student Victor Scheinman created the Stanford Arm, recognized as the first electronic computer-controlled robotic arm (Unimate's instructions were stored on a magnetic drum).\n\nIn the late-1960s the Vietnam War became the testing ground for automated command technology and sensor networks. In 1966 the McNamara Line was proposed with the aim of requiring virtually no ground forces. This sensor network of seismic and acoustic sensors, photoreconnaissance and sensor-triggered land mines was only partially implemented due to high cost. The first mobile robot capable of reasoning about its surroundings, Shakey, was built in 1970 by the Stanford Research Institute (now SRI International). Shakey combined multiple sensor inputs, including TV cameras, laser rangefinders, and \"bump sensors\" to navigate.\n\nIn the early 1970s precision munitions and smart weapons were developed. Weapons became robotic by implementing terminal guidance. At the end of the Vietnam War the first laser-guided bombs were deployed, which could find their target by following a laser beam that was pointed at the target. During the 1972 Operation Linebacker laser-guided bombs proofed effective, but still depended heavily on human operators. Fire-and-forget weapons were also first deployed in the closing Vietnam War, once launched no further attention or action was required from the operator.\n\nThe development of humanoid robots was advanced considerably by Japanese robotics scientists in the 1970s. Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the world's first full-scale humanoid intelligent robot. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth. This made it the first android.\n\nFreddy and Freddy II were robots built at the University of Edinburgh School of Informatics by Pat Ambler, Robin Popplestone, Austin Tate, and Donald Mitchie, and were capable of assembling wooden blocks in a period of several hours. German based company KUKA built the world's first industrial robot with six electromechanically driven axes, known as FAMULUS. \n\nIn 1974, David Silver designed The Silver Arm, which was capable of fine movements replicating human hands. Feedback was provided by touch and pressure sensors and analyzed by a computer. The SCARA, Selective Compliance Assembly Robot Arm, was created in 1978 as an efficient, 4-axis robotic arm. Best used for picking up parts and placing them in another location, the SCARA was introduced to assembly lines in 1981.\n\nThe Stanford Cart successfully crossed a room full of chairs in 1979. It relied primarily on stereo vision to navigate and determine distances. The Robotics Institute at Carnegie Mellon University was founded in 1979 by Raj Reddy.\n\nTakeo Kanade created the first \"direct drive arm\" in 1981. The first of its kind, the arm's motors were contained within the robot itself, eliminating long transmissions.\n\nIn 1984 Wabot-2 was revealed; capable of playing the organ, Wabot-2 had 10 fingers and two feet. Wabot-2 was able to read a score of music and accompany a person.\n\nIn 1986, Honda began its humanoid research and development program to create robots capable of interacting successfully with humans. A hexapodal robot named Genghis was revealed by MIT in 1989. Genghis was famous for being made quickly and cheaply due to construction methods; Genghis used 4 microprocessors, 22 sensors, and 12 servo motors. Rodney Brooks and Anita M. Flynn published \"Fast, Cheap, and Out of Control: A Robot Invasion of The Solar System\". The paper advocated creating smaller cheaper robots in greater numbers to increase production time and decrease the difficulty of launching robots into space.\n\nIn 1994 one of the most successful robot-assisted surgery appliances was cleared by the FDA. The Cyberknife had been invented by John R. Adler and the first system was installed at Stanford University in 1991. This radiosurgery system integrated image-guided surgery with robotic positioning. The Cyberknife is now deployed to treat patients with brain or spine tumors. A x-ray camera tracks displacement and compensates for motion caused by breathing.\n\nThe biomimetic robot RoboTuna was built by doctoral student David Barrett at the Massachusetts Institute of Technology in 1996 to study how fish swim in water. RoboTuna is designed to swim and to resemble a bluefin tuna. \nHonda's P2 humanoid robot was first shown in 1996. Standing for \"Prototype Model 2\", P2 was an integral part of Honda's humanoid development project; over tall, P2 was smaller than its predecessors and appeared to be more human-like in its motions.\n\nExpected to operate for only seven days, the Sojourner rover finally shuts down after 83 days of operation in 1997. This small robot (only 23 lbs or 10.5 kg) performed semi-autonomous operations on the surface of Mars as part of the Mars Pathfinder mission; equipped with an obstacle avoidance program, Sojourner was capable of planning and navigating routes to study the surface of the planet. Sojourner's ability to navigate with little data about its environment and nearby surroundings allowed it to react to unplanned events and objects.\n\nThe P3 humanoid robot was revealed by Honda in 1998 as a part of the company's continuing humanoid project. In 1999, Sony introduced the AIBO, a robotic dog capable of interacting with humans; the first models released in Japan sold out in 20 minutes. Honda revealed the most advanced result of their humanoid project in 2000, named ASIMO. ASIMO can run, walk, communicate with humans, recognise faces, environment, voices and posture, and interact with its environment. Sony also revealed its Sony Dream Robots, small humanoid robots in development for entertainment. In October 2000, the United Nations estimated that there were 742,500 industrial robots in the world, with more than half of them being used in Japan.\n\nIn April 2001, the Canadarm2 was launched into orbit and attached to the International Space Station. The Canadarm2 is a larger, more capable version of the arm used by the Space Shuttle, and is hailed as \"smarter\". Also in April, the Unmanned Aerial Vehicle Global Hawk made the first autonomous non-stop flight over the Pacific Ocean from Edwards Air Force Base in California to RAAF Base Edinburgh in Southern Australia. The flight was made in 22 hours. \n\nThe popular Roomba, a robotic vacuum cleaner, was first released in 2002 by the company iRobot.\n\nIn 2005, Cornell University revealed a robot capable of self-replication; a set of cubes capable of attaching and detaching, the first robot capable of building copies of itself. Launched in 2003, on January 3 and 24, the Mars rovers Spirit and Opportunity landed on the surface of Mars. Both robots drove many times the distance originally expected, and Opportunity was still operating as of mid-2018 although communications were subsequently lost due to a major dust storm.\n\nSelf-driving cars had made their appearance by around 2005, but there was room for improvement. None of the 15 devices competing in the DARPA Grand Challenge (2004) successfully completed the course; in fact no robot successfully navigated more than 5% of the off-road course, leaving the $1 million prize unclaimed. In 2005, Honda revealed a new version of its ASIMO robot, updated with new behaviors and capabilities. In 2006, Cornell University revealed its \"Starfish\" robot, a four-legged robot capable of self modeling and learning to walk after having been damaged. In 2007, TOMY launched the entertainment robot, i-sobot, a humanoid bipedal robot that can walk like a human and performs kicks and punches and also some entertaining tricks and special actions under \"Special Action Mode\".\nRobonaut 2, the latest generation of the astronaut helpers, was launched to the space station aboard Space Shuttle Discovery on the STS-133 mission in 2011. It is the first humanoid robot in space, and although its primary job for now is teaching engineers how dextrous robots behave in space; the hope is that through upgrades and advancements, it could one day venture outside the station to help spacewalkers make repairs or additions to the station or perform scientific work.\n\nOn 25 October 2017 at the Future Investment Summit in Riyadh, a robot called Sophia and referred to with female pronouns was granted Saudi Arabian citizenship, becoming the first robot ever to have a nationality. This has attracted controversy, as it is not obvious whether this implies that Sophia can vote or marry, or whether a deliberate system shutdown can be considered murder; as well, it is controversial considering how few rights are given to Saudi human women.\n\nCommercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for tasks which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, Earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.\n\nWith recent advances in computer hardware and data management software, artificial representations of humans are also becoming widespread. Examples include OpenMRS and EMRBots.\n\n\n\n"}
{"id": "9488407", "url": "https://en.wikipedia.org/wiki?curid=9488407", "title": "Home server", "text": "Home server\n\nA home server is a computing server located in a private residence providing services to other devices inside or outside the household through a home network or the Internet. Such services may include file and printer serving, media center serving, web serving (on the network or Internet), web caching, file sharing and synchronization, calendar and contact sharing and synchronization, account authentication and backup services.\n\nBecause of the relatively low number of computers on a typical home network, a home server commonly does not require significant computing power. Home servers can be implemented do-it-yourself style with a re-purposed, older computer, or a plug computer; pre-configured commercial home server appliances are also available. An uninterruptible power supply is sometimes used in case of power outages that can possibly corrupt data.\n\nHome servers often run headless, and can be administered remotely through a command shell, or graphically through a remote desktop system such as RDP, VNC, Webmin, Apple Remote Desktop, or many others.\n\nSome home server operating systems (such as Windows Home Server) include a consumer-focused graphical user interface (GUI) for setup and configuration that is available on home computers on the home network (and remotely over the Internet via remote access). Others simply enable users to use native operating system tools for configuration.\n\nHome servers often act as network-attached storage (NAS) providing the major benefit that all users' files can be centrally and securely stored, with flexible permissions applied to them. Such files can be easily accessed from any other system on the network, provided the correct credentials are supplied. This also applies to shared printers.\n\nSuch files can also be shared over the Internet to be accessible from anywhere in the world using remote access.\n\nServers running Unix or Linux with the free Samba suite (or certain Windows Server products - Windows Home Server excluded) can provide domain control, custom logon scripts, and roaming profiles to users of certain versions of Windows. This allows a user to log on from any machine in the domain and have access to her or his \"My Documents\" and personalized Windows and application preferences - multiple accounts on each computer in the home are not needed.\n\nHome servers are often used to serve multi-media content, including photos, music, and video to other devices in the household (and even to the Internet; see Space shifting, Tonido and Orb). Using standard protocols such as DLNA or proprietary systems such as iTunes, users can access their media stored on the home server from any room in the house. Windows XP Media Center Edition, Windows Vista, and Windows 7 can act as a home server, supporting a particular type of media serving that streams the interactive user experience to Media Center Extenders including the Xbox 360.\nWindows Home Server supports media streaming to Xbox 360 and other DLNA-based media receivers via the built-in Windows Media Connect technology. Some Windows Home Server device manufacturers, such as HP, extend this functionality with a full DLNA implementation such as PacketVideo TwonkyMedia server.\n\nThere are many open-source and fully functional programs for media serving available for Linux. LinuxMCE is one example, which allows other devices to boot off a hard drive image on the server, allowing them to become appliances such as set-top boxes. Asterisk, Xine, MythTV (another media serving solution), VideoLAN, SlimServer, DLNA, and many other open-source projects are fully integrated for a seamless home theater/automation/telephony experience.\n\nOn an Apple Macintosh server, options include iTunes, PS3 Media Server, and Elgato. Additionally, for Macs directly connected to TVs, Boxee can act as a full-featured media center interface.\n\nServers are typically always on so the addition of a TV or radio tuner allows recording to be scheduled at any time.\n\nSome home servers provide remote access to media and entertainment content.\n\nA home server can be used to provide remote access into the home from devices on the Internet, using remote desktop software and other remote administration software. For example, Windows Home Server provides remote access to files stored on the home server via a web interface as well as remote access to Remote Desktop sessions on PCs in the house. Similarly, Tonido provides direct access via a web browser from the Internet without requiring any port forwarding or other setup. Some enthusiasts often use VPN technologies as well.\n\nOn a Linux server, two popular tools are (among many) VNC and Webmin. VNC allows clients to remotely view a server GUI desktop as if the user was physically sitting in front of the server. A GUI need not be running on the server console for this to occur; there can be multiple 'virtual' desktop environments open at the same time. Webmin allows users to control many aspects of server configuration and maintenance all from a simple web interface. Both can be configured to be accessed from anywhere on the Internet.\n\nServers can also be accessed remotely using the command line-based Telnet and SSH protocols.\n\nSome users choose to run a web server in order to share files easily and publicly (or privately, on the home network). Others set up web pages and serve them straight from their home, although this may be in violation of some ISPs terms of service. Sometimes these web servers are run on a nonstandard port in order to avoid the ISP's port blocking. Example web servers used on home servers include Apache and IIS.\n\nMany other web servers are available; see Comparison of lightweight web servers, Comparison of web servers.\n\nSome networks have an HTTP proxy which can be used to speed up web access when multiple users visit the same websites, and to get past blocking software while the owner is using the network of some institution that might block certain sites. Public proxies are often slow and unreliable and so it is worth the trouble of setting up one's own private proxy.\n\nSome proxies can be configured to block websites on the local network if it is set up as a transparent proxy.\n\nMany home servers also run e-mail servers that handle e-mail for the owner's domain name. The advantages are having much bigger mailboxes and maximum message size than most commercial e-mail services. Access to the server, since it is on the local network is much faster than using an external service. This also increases security as e-mails do not reside on an off-site server.\n\nHome servers are ideal for utilizing the BitTorrent protocol for downloading and seeding files as some torrents can take days, or even weeks to complete and perform better on an uninterrupted connection. There are many text based clients such as rTorrent and web-based ones such as TorrentFlux and Tonido available for this purpose. BitTorrent also makes it easier for those with limited bandwidth to distribute large files over the Internet.\n\nAn unusual service is the Gopher protocol, a hypertext document retrieval protocol which pre-dated the World Wide Web and was popular in the early 1990s. Many of the remaining gopher servers are run off home servers utilizing PyGopherd and the Bucktooth gopher server.\n\nHome automation requires a device in the home that is available 24/7. Often such home automation controllers are run on a home server.\n\nRelatively low cost CCTV DVR solutions are available that allow recording of video cameras to a home server for security purposes. The video can then be viewed on PCs or other devices in the house.\n\nA series of cheap USB-based webcams can be connected to a home server as a makeshift CCTV system. Optionally these images and video streams can be made available over the Internet using standard protocols.\n\nHome servers can act as a host to family-oriented applications such as a family calendar, to-do lists, and message boards.\n\nBecause a server is always on, an IRC client or IM client running on it will be highly available to the Internet. This way, the chat client will be able to record activity that occurs even while the user is not at the computer, e.g. asleep or at work or school. Textual clients such as Irssi and tmsnc can be detached using GNU Screen for example, and graphical clients such as Pidgin can be detached using xmove. Quassel provides a specific version for this kind of use. Home servers can also be used to run personal XMPP servers and IRC servers as these protocols can support a large number of users on very little bandwidth.\n\nSome multiplayer games such as Continuum, Tremulous, Minecraft, and Doom have server software available which users may download and use to run their own private game server. Some of these servers are password protected, so only a selected group of people such as clan members or whitelisted players can gain access to the server. Others are open for public use and may move to colocation or other forms of paid hosting if they gain a large number of players.\n\nHome servers can be used to host distributed federated social networks like diaspora* and GNU Social. Federation protocols like ActivityPub allow lots of small home servers to interact in a meaningful way and give the perception of being on a large traditional social network. Federation is not just limited to social networks. Many innovative new free software web services are being developed that can allow people to host their own videos, photos, blogs etc. and still participate in the larger federated networks.\n\nHome servers often are platforms that enable third-party products to be built and added over time. For example, Windows Home Server provides a Software Development Kit. Similarly, Tonido provides an application platform that can be extended by writing new applications using their SDK.\n\nHome servers run many different operating systems. Enthusiasts who build their own home servers can use whatever OS is conveniently available or familiar to them, such as Microsoft Windows, Mac OS X, Linux, Solaris, BSD or Plan 9 from Bell Labs.\n\nSingle-board computers are increasingly being used to power home servers, with many of them being ARM devices. Old desktop and laptop computers can also be re-purposed to be used as home servers.\n\nMobile phones are typically just as powerful as ARM-based single board computers. Once mobile phones can run the GNU/Linux operating system, self-hosting might move to mobile devices with each person's data and services being served from their own mobile phone.\n\n\n\n\n\n\n\n"}
{"id": "33966019", "url": "https://en.wikipedia.org/wiki?curid=33966019", "title": "IFA Tulln", "text": "IFA Tulln\n\nThe Department für Agrobiotechnology (IFA-Tulln) is one of the 15 departments of the University of Natural Resources and Life Sciences, Vienna (BOKU) operated in cooperation with the Vienna University of Technology and the University of Veterinary Medicine Vienna at the Campus Tulln Technopol.\n\nThe IFA-Tulln was founded in 1994 as a joint research institution of three major universities in Vienna, the University of Veterinary Medicine Vienna (VetMed), the Vienna University of Technology (TUW) and the University of Natural Resources and Life Sciences Vienna (BOKU). The idea has been to enable the collaboration of scientists with complementary background in the interdisciplinary area of agrobiotechnology under one roof. Their expertise covers modern biotechnology in plant and animal production, environmental biotechnology, animal nutrition, food- and feed science and (bio) analytics and biopolymers. Today about 150 BOKU employees, guest scientists and students are working in 6 institutes at the IFA-Tulln which has become a department of the BOKU in 2004.\n\nIFA-Tulln, which is a department of the University of Natural Resources and Life Sciences Vienna (BOKU), was founded in 1994 as a joint research institution of the BOKU, the University of Veterinary Medicine Vienna and the Vienna University of Technology to enable the collaboration of scientists with complementary background in the interdisciplinary area of agrobiotechnology.\n\nThe department is organised into 6 institutes:\n\nThe first 5 Institutes of the department IFA-Tulln are located in the IFA-Tulln building in Tulln an der Donau on the Campus Tulln Technopol. The sixth Institute (Institute of Animal Nutrition, Products and Nutrition Physiology) is located in Muthgasse, Vienna. The IFA-Tulln building, together with the other working groups of BOKU in the UFT-building (University Research Center Tulln), is forming the “BOKU location Tulln”. Campus Tulln Technopol is part of Technopol Tulln which was founded in 2006 by Ecoplus. Further partners of Technopol Tulln besides BOKU, are: AIT Austrian Institute of Technology, Technopark Tulln GmbH, Technologiezentrum Tulln GmbH, \"Agrana Research & Innovation Center\", University for Applied Sciences Wiener Neustadt Campus Tulln and the city of Tulln.\n\nIn its research, the Institute for Biotechnology in Plant Production focuses on basic and applied research in the areas plant breeding, plant genetics and phytopathology. The logical overlaps between these topics are plant-pathogen interaction, genetics of disease resistance and breeding research for disease resistance. The institute almost exclusively performs its own research work on agricultural crop plants.\n\nThe research focuses on the utilization of natural resources as well as of idle waste material for reinforcements of polymers or for use as thermoplastic matrices. The main processing technologies used in composites- production are extrusion and injection moulding. To a lesser content the Institute also applies compression moulding.\n\nThe Center for Analytical Chemistry (CAC) is aiming to perform cutting edge scientific research and to develop advanced methods in the field of (bio)analytical chemistry. The CAC with its two Christian Doppler Laboratories is pursuing a highly interdisciplinary approach for the determination of chemical contaminants including mycotoxins and allergenic proteins in food. By employing metabolomics based approaches the CAC studies entire biological systems with a special emphasis on plant-fungi interactions.\n\nThere are five research groups within the Institute: \n\nFocus of research activities at the Institute for Environmental Biotechnology is given to the application of microbial metabolism to safeguard the quality of life and preserve natural resources. On the one hand, emphasis is put on degradation or detoxification of pollutants (in soil, water and waste) or the development of monitoring methods to evaluate the success of restoration technologies. On the other hand, the best possible utilization of existing resources by establishment of sustainable material cycles is the central aim of research conducted. \nPractical application and process development for technical implementation are a primary concern independent from fundamental investigation of the underlying microbiological activities. Examples are the scale up of fermentation processes, the development of technical remediation methods and the testing of innovative combined biological-physical processes (e.g. use of membrane bioprocesses) in environmental technology.\n\nThere are 6 research groups established at the Institute for Environmental Biotechnology:\n\nWith respect to methodological and research competence at the department IFA Tulln the institute for Biotechnology in Animal Production concentrates on advanced tissue culture technologies for the in vitro production of embryos.There are three research groups at the Institute for Biotechnology in Animal Production\n\nThe Institute of Animal Nutrition, Products, and Nutrition Physiology (APN) represents the start of the supply chain of food of animal origin. It focuses on adequate feeding of agricultural livestock and the significant impact of animal nutrition on quality and safety of primary products, such as milk, meat and eggs. Aside from nutrients, special emphasis is paid on secondary effects of feed and feed/food components on digestion, metabolism and health.\n\nThe Institute has condensed its mission and structure towards three intrinsic topics: \n\nhttp://www.boku.ac.at/fileadmin/_/H13/Publikationen/Wissensbilanzen/Wissensbilanz_2010/BOKU_Wissensbilanz_2010.pdf\n"}
{"id": "22851371", "url": "https://en.wikipedia.org/wiki?curid=22851371", "title": "Import replacement", "text": "Import replacement\n\nImport replacement refers to an urban free market economic process of entrepreneurs replacing the imports of the city with production from within the city. \n\nThe idea was invented by Jane Jacobs who spun off from the idea of import substitution developed by Andre Gunder Frank and widely discussed during the first and second Latin American debt crisis. Import substitution is a national economic theory implying that if a nation substituted its imports with national production the nation would become wealthier, whereas Jacob's idea is entirely about cities and could be called \"urban import substitution\". However, even this would lead to confusion since, in practice, import substitution in India and Latin America were government subsidized and mandated, whereas Jacobs' concept of import replacement is a free market process of discovery and division of labor within a city.\n"}
{"id": "8296693", "url": "https://en.wikipedia.org/wiki?curid=8296693", "title": "In-car entertainment", "text": "In-car entertainment\n\nIn-car entertainment (ICE), or in-vehicle infotainment (IVI), is a collection of hardware and software in automobiles that provides audio or video entertainment. In car entertainment originated with car audio systems that consisted of radios and cassette or CD players, and now includes automotive navigation systems, video players, USB and Bluetooth connectivity, Carputers, in-car internet, and WiFi. Once controlled by simple dashboards knobs and dials, ICE systems can include steering wheel audio controls and handsfree voice control.\n\nDriven by the demand for more connected vehicles, in-car entertainment is getting more and more sophisticated. Car makers, electronics and software suppliers, as well as newcomers from the Silicon Valley (such as Google and Apple), work together and also compete to come up with infotainment systems that are user-friendly and safe to use. ICE systems are increasingly commonplace with newer vehicle models and several auto makers have developed their own systems: Ford with SYNC and MyFord Touch, Toyota with Entune, Cadillac with CUE (Cadillac User Experience), FCA with Uconnect, etc.\n\nWith the mass adoption of smartphones worldwide, a new issue has emerged: the use of connected devices in the car. According to a 2015 survey conducted by AT&T with a sample of over 2,000 US respondents, \"7-in-10 people engage in smartphone activities while driving\" including social media (40%), web browsing (30%) and even video chatting (10%). This raises safety concerns related to distracted driving and also pushes the automotive industry to integrate those devices in a safe manner. \"Traditional\" car makers increasingly rely on the software and electronics expertise of auto suppliers and technology companies to help them design such systems. Google and Apple's mobile OSs' making the bulk of the market, the two tech companies have developed projection modes (Android Auto, Apple CarPlay) to enable mobile devices to be operated in vehicles through the dashboard head unit so that the vehicle occupants don't manipulate their devices directly, use an interface they are familiar with, and spend more time with their eyes on the road.\n\nPolicies regarding in-car entertainment systems are less developed than cell phone usage laws regarding similar distractions in cars. In the United States, 10 states, D.C., Guam, and the Virgin Islands prohibit all drivers from using handheld cell phones while driving. Additionally, 39 states, D.C., Guam, and the Virgin Islands ban text messaging for all drivers. However, few states have developed laws to limit the content that drivers can view on in-car entertainment systems.\n\nStill, researchers are beginning to analyze the potential impact of distracted drivers on the roads. Charlie Klauer, a researcher at the Virginia Tech Transportation Institute, says that drivers who look at screens have a much higher risk of crashing. Furthermore, the risk of crashing rises exponentially the longer a driver has taken their eyes off the road.\n\nAutomotive companies like Ford and Audi contend that they have tested and revised their latest systems in order to reduce the amount of time that drivers spend looking away from the road.\n\nAs car infotainment systems can access more and more functions of the vehicle (e.g. through the CAN bus), concerns have also been voiced about potential remote car hacking (see drive by wire).\n\n"}
{"id": "44801860", "url": "https://en.wikipedia.org/wiki?curid=44801860", "title": "International Civil Aviation Organization Public Key Directory", "text": "International Civil Aviation Organization Public Key Directory\n\nThe International Civil Aviation Organization Public Key Directory (ICAO PKD) is a database maintained by the International Civil Aviation Organization holding national cryptographic keys related to the authentication of e-passport information.\n\nA September 2011 United States Central Intelligence Agency document released by WikiLeaks in December 2014 explains the purpose and scope of the system:\n\nThe United Nations became the first non-state participant in October 2012, enabling issuing of e-UNLP, the electronic form of the United Nations laissez-passer.\n\nIn December 2014, ICAO reported the PKD as having 45 participants.\n\nIn 2015 the German Bundesdruckerei (German Federal Printing Office) won the request for tender of the ICAO to provide the ICAO PKD.\n\nIn July 2017, ICAO reported the PKD as having 58 participants.\n\nAs of November 2017, 60 participants were part of the ICAO PKD, with the European Union being the 60th member and at the same time the second non-state participant.\n\n\n"}
{"id": "15035566", "url": "https://en.wikipedia.org/wiki?curid=15035566", "title": "Masimo", "text": "Masimo\n\nMasimo is an American manufacturer of noninvasive patient monitoring technologies based in Irvine, California. The company sells more pulse oximetry to hospitals than any other company. Masimo was founded in 1989 by electrical engineer Joe Kiani, who was later joined by fellow engineer Mohamed Diab. Masimo invented measure-through motion and low perfusion pulse oximetry, known as Masimo SET (Signal Extraction Technology). Masimo has been recognized for its intellectual property and for being one of the most innovative companies in the medical device industry. The company went public in 2007 and is currently traded on the NASDAQ stock exchange under the symbol MASI. In 2011, Forbes named Masimo to its list of top 20 public companies under a billion dollars in revenue, based on earnings growth, sales growth, and return on equity. In 2012, Joe Kiani, founder, CEO and Chairman of the Board was named the Ernst & Young National Entrepreneur of the Year - 2012 Life Sciences Award Winner. Kiani was recognized for \"revolutionizing the health care industry by taking risks to create and commercialize noninvasive patient monitoring devices, which include an array of sensors that lead to improved accuracy, a reduction in the overall number of false readings, and ultimately, reduced cost of care.\"\n\nPulse oximetry uses two light emitting diodes (LEDs), one red and one infrared, to measure the absorption of light and translates that into the percentage of hemoglobin molecules that are bound with oxygen, which is called arterial oxygen saturation (SpO). Conventional pulse oximetry assumes that arterial blood is the only blood moving (pulsating) in the measurement site. However, during patient motion, the venous blood also moves, which can cause conventional pulse oximetry to under-read SpO levels because it cannot distinguish between the arterial and venous blood. SET identifies the venous blood signal (which has a lower oxygen saturation level than arterial blood), isolates it, and uses adaptive filters to cancel the noise and extract the arterial signal in order to report accurate SpO and pulse rate. In addition, SET pulse oximetry provides perfusion index (PI) and pleth variability index (PVI). Multiple studies have shown that compared to non-SET pulse oximeters, SET increases the ability to detect life-threatening events and reduces false alarms during challenging conditions. Additional studies have also shown the impact of SET on patient outcomes, such as helping clinicians:\nIn 2011, the American Academy of Pediatrics and the U.S. Department of Health and Human Services recommended mandatory screening for all newborns, using \"motion-tolerant pulse oximeters that report functional oxygen saturation have been validated in low perfusion conditions\". To make this recommendation, the CCHD workgroup relied on two independent studies that exclusively used SET pulse oximetry to assess newborns. In 2012, Masimo received FDA 510(k) clearance for devices and sensors with labeling for screening newborns for CCHD. It marked the first time the FDA cleared specific labeling indicating the use of pulse oximeters, in conjunction with a physical examination, to screen newborns for CCHD.\nIn 2012, the National Health Service (NHS) Technology Adoption Centre in the United Kingdom advised hospitals to use Intraoperative Fluid Management Technologies as a way to improve patient outcomes, and included Masimo's PVI among technologies available for helping clinicians manage fluid during surgeries. In 2013, the French Society for Anaesthesia and Intensive Care (SFAR) added PVI to its guidelines for optimal hemodynamic management of surgical patients.\n\nrainbow Pulse CO-Oximetry uses more than seven wavelengths of light to continuously and noninvasively measure hemoglobin (SpHb), carboxyhemoglobin (SpCO), and methemoglobin (SpMet), in addition to oxygen saturation (SpO), pulse rate, perfusion index (Pi), and pleth variability index (PVi). A study at Massachusetts General Hospital showed that SpHb monitoring helped clinicians decrease the frequency of patients receiving blood transfusions during surgery from 4.5% to 0.6%. Another study from Cairo University showed that SpHb monitoring helped clinicians reduce blood transfusions in high blood loss surgery by an average of 0.9 units per patient. Emergency department studies have shown that SpCO helps clinicians increase the detection of carbon monoxide (CO) poisoning and decreases the time to treatment compared to invasive methods. The Pronto-7 device for noninvasive spot checking of hemoglobin, along with SpO and pulse rate, has been recognized with a gold Medical Design Excellence Award. The World Health Organization called noninvasive hemoglobin an \"innovative medical technology for cost-effectively addressing global health concerns and needs\".\n\nIn October 2014, Masimo announced CE Mark of Oxygen Reserve Index or ORi, the company’s 11th noninvasive parameter, which provides real-time visibility to oxygenation status. ORi is intended to supplement, not replace, oxygen saturation (SpO) monitoring and partial pressure of oxygen (PaO) measurements. ORi can be trended and has optional alarms to notify clinicians of changes in a patient’s oxygen reserve, and may enable proactive interventions to avoid hypoxia and unintended hyperoxia.\n\nPatient SafetyNet is a remote monitoring and notification system designed for patients on medical/surgical care floors. A large study by Dartmouth-Hitchcock Medical Center showed Patient SafetyNet helped clinicians achieve a 65% reduction in distress codes and rescue activations and a 48% decrease in patient transfers to intensive care units (ICU), yielding a savings of 135 Intensive Care Unit (ICU) days annually for an annual opportunity-cost savings of $1.48 million. ECRI Institute gave Dartmouth its Health Devices Achievement Award for its use of Patient SafetyNet to prevent \"severe patient harm\". Masimo has introduced Halo Index in the Patient SafetyNet system, combining multiple physiologic parameters into one number to help clinicians assess overall patient status.\n\nRainbow acoustic monitoring provides noninvasive and continuous measurement of respiration rate using an adhesive sensor with an integrated acoustic transducer that is applied to the patient's neck. Researchers have evaluated acoustic respiration rate (RRa) and found the acceptable accuracy and significantly fewer false alarms than traditional respiration rate monitoring methods, end-tidal carbon dioxide (EtCO) and impedance pneumography.\n\nIn 2010, Masimo began offering brain function monitoring to measure the effects of anesthesia and sedation by monitoring both sides of the brain's electrical activity (EEG). Studies have shown this results in more individualized titration and improved care.\n\nIn 2012, Masimo began offering ultra-compact mainstream and sidestream capnography as well as multigas analyzers for end-tidal carbon dioxide (CO), nitrous oxide (NO), oxygen (O), and anesthetic agents, for use in the operating room, procedural sedation, and in intensive care units (ICU). A multi-center study at Cincinnati Children's Hospital Medical Center, University Medical Center (Tucson, Arizona), and Children's Medical Center (Dallas), found that respiratory rate measured from noninvasive, acoustic monitoring had similar accuracy and precision as nasal capnography, the current standard of care when used in pediatric patients.\n\nOn December 1, 2014, Masimo announced FDA 510(k) clearance of Radius-7 for the Root patient monitoring and connectivity platform, the first and only wearable, wireless monitor with Masimo’s rainbow SET technology, enabling early identification of clinical deterioration while offering patients continuous monitoring with freedom of movement. Radius-7 attaches to the patient’s arm or can be placed alongside the patient in their bed, allowing untethered monitoring. Studies have shown that patient mobility is a key factor in more rapid patient recovery.\niSpO pulse oximeter was awarded the Hot Product Award at the 2013 EMS Today Conference & Exposition.\n\nIn 2012, Masimo embarked on its first Commitment to Action with the Clinton Global Initiative (CGI) to solve the global problem of maternal mortality and anemia. The $1 million, two-year project initially focuses on five villages in Liberia and Uganda – two countries where the epidemics of maternal mortality and anemia are among the worst.\n\nIn 2013, Masimo founder, Chairman and CEO Joe Kiani, created the nonprofit Patient Safety Movement Foundation with a mission to eliminate the more than 200,000 preventable patient deaths that occur in U.S. hospitals each year. The foundation holds annual Patient Safety, Science & Technology Summits, featuring leaders from healthcare, industry, and government; former President Bill Clinton has served as the keynote speaker.\n\n"}
{"id": "25154546", "url": "https://en.wikipedia.org/wiki?curid=25154546", "title": "Mechanical filter", "text": "Mechanical filter\n\nA mechanical filter is a signal processing filter usually used in place of an electronic filter at radio frequencies. Its purpose is the same as that of a normal electronic filter: to pass a range of signal frequencies, but to block others. The filter acts on mechanical vibrations which are the analogue of the electrical signal. At the input and output of the filter, transducers convert the electrical signal into, and then back from, these mechanical vibrations.\n\nThe components of a mechanical filter are all directly analogous to the various elements found in electrical circuits. The mechanical elements obey mathematical functions which are identical to their corresponding electrical elements. This makes it possible to apply electrical network analysis and filter design methods to mechanical filters. Electrical theory has developed a large library of mathematical forms that produce useful filter frequency responses and the mechanical filter designer is able to make direct use of these. It is only necessary to set the mechanical components to appropriate values to produce a filter with an identical response to the electrical counterpart.\n\nSteel alloys and iron–nickel alloys are common materials for mechanical filter components; nickel is sometimes used for the input and output couplings. Resonators in the filter made from these materials need to be machined to precisely adjust their resonance frequency before final assembly.\n\nWhile the meaning of \"mechanical filter\" in this article is one that is used in an electromechanical role, it is possible to use a mechanical design to filter mechanical vibrations or sound waves (which are also essentially mechanical) directly. For example, filtering of audio frequency response in the design of loudspeaker cabinets can be achieved with mechanical components. In the electrical application, in addition to mechanical components which correspond to their electrical counterparts, transducers are needed to convert between the mechanical and electrical domains. A representative selection of the wide variety of component forms and topologies for mechanical filters are presented in this article.\n\nThe theory of mechanical filters was first applied to improving the mechanical parts of phonographs in the 1920s. By the 1950s mechanical filters were being manufactured as self-contained components for applications in radio transmitters and high-end receivers. The high \"quality factor\", \"Q\", that mechanical resonators can attain, far higher than that of an all-electrical LC circuit, made possible the construction of mechanical filters with excellent selectivity. Good selectivity, being important in radio receivers, made such filters highly attractive. Contemporary researchers are working on microelectromechanical filters, the mechanical devices corresponding to electronic integrated circuits.\n\nThe elements of a passive linear electrical network consist of inductors, capacitors and resistors which have the properties of inductance, elastance (inverse capacitance) and resistance, respectively. The mechanical counterparts of these properties are, respectively, mass, stiffness and damping. In most electronic filter designs, only inductor and capacitor elements are used in the body of the filter (although the filter may be terminated with resistors at the input and output). Resistances are not present in a theoretical filter composed of ideal components and only arise in practical designs as unwanted parasitic elements. Likewise, a mechanical filter would ideally consist only of components with the properties of mass and stiffness, but in reality some damping is present as well.\n\nThe mechanical counterparts of voltage and electric current in this type of analysis are, respectively, force (\"F\") and velocity (\"v\") and represent the signal waveforms. From this, a mechanical impedance can be defined in terms of the imaginary angular frequency, \"jω\", which entirely follows the electrical analogy.\n\nThe scheme presented in the table is known as the impedance analogy. Circuit diagrams produced using this analogy match the electrical impedance of the mechanical system seen by the electrical circuit, making it intuitive from an electrical engineering standpoint. There is also the mobility analogy, in which force corresponds to current and velocity corresponds to voltage. This has equally valid results but requires using the reciprocals of the electrical counterparts listed above. Hence, \"M\" → \"C\", \"S\" → 1/\"L\", \"D\" → \"G\" where \"G\" is electrical conductance, the inverse of resistance. Equivalent circuits produced by this scheme are similar, but are the dual impedance forms whereby series elements become parallel, capacitors become inductors, and so on. Circuit diagrams using the mobility analogy more closely match the mechanical arrangement of the circuit, making it more intuitive from a mechanical engineering standpoint. In addition to their application to electromechanical systems, these analogies are widely used to aid analysis in acoustics.\n\nAny mechanical component will unavoidably possess both mass and stiffness. This translates in electrical terms to an LC circuit, that is, a circuit consisting of an inductor and a capacitor, hence mechanical components are resonators and are often used as such. It is still possible to represent inductors and capacitors as individual lumped elements in a mechanical implementation by minimising (but never quite eliminating) the unwanted property. Capacitors may be made of thin, long rods, that is, the mass is minimised and the compliance is maximised. Inductors, on the other hand, may be made of short, wide pieces which maximise the mass in comparison to the compliance of the piece.\n\nMechanical parts act as a transmission line for mechanical vibrations. If the wavelength is short in comparison to the part then a lumped element model as described above is no longer adequate and a distributed element model must be used instead. The mechanical distributed elements are entirely analogous to electrical distributed elements and the mechanical filter designer can use the methods of electrical distributed element filter design.\n\nMechanical filter design was developed by applying the discoveries made in electrical filter theory to mechanics. However, a very early example (1870s) of acoustic filtering was the \"harmonic telegraph\", which arose precisely because electrical resonance was poorly understood but mechanical resonance (in particular, acoustic resonance) was very familiar to engineers. This situation was not to last for long; electrical resonance had been known to science for some time before this, and it was not long before engineers started to produce all-electric designs for filters. In its time, though, the harmonic telegraph was of some importance. The idea was to combine several telegraph signals on one telegraph line by what would now be called frequency division multiplexing thus saving enormously on line installation costs. The key of each operator activated a vibrating electromechanical reed which converted this vibration into an electrical signal. Filtering at the receiving operator was achieved by a similar reed tuned to precisely the same frequency, which would only vibrate and produce a sound from transmissions by the operator with the identical tuning.\n\nVersions of the harmonic telegraph were developed by Elisha Gray, Alexander Graham Bell, Ernest Mercadier and others. Its ability to act as a sound transducer to and from the electrical domain was to inspire the invention of the telephone.\n\nOnce the basics of electrical network analysis began to be established, it was not long before the ideas of complex impedance and filter design theories were carried over into mechanics by analogy. Kennelly, who was also responsible for introducing complex impedance, and Webster were the first to extend the concept of impedance into mechanical systems in 1920. Mechanical admittance and the associated mobility analogy came much later and are due to Firestone in 1932.\n\nIt was not enough to just develop a mechanical analogy. This could be applied to problems that were entirely in the mechanical domain, but for mechanical filters with an electrical application it is necessary to include the transducer in the analogy as well. Poincaré in 1907 was the first to describe a transducer as a pair of linear algebraic equations relating electrical variables (voltage and current) to mechanical variables (force and velocity). These equations can be expressed as a matrix relationship in much the same way as the z-parameters of a two-port network in electrical theory, to which this is entirely analogous:\n\nwhere \"V\" and \"I\" represent the voltage and current respectively on the electrical side of the transducer.\n\nWegel, in 1921, was the first to express these equations in terms of mechanical impedance as well as electrical impedance. The element formula_2 is the open circuit mechanical impedance, that is, the impedance presented by the mechanical side of the transducer when no current is entering the electrical side. The element formula_3, conversely, is the clamped electrical impedance, that is, the impedance presented to the electrical side when the mechanical side is clamped and prevented from moving (velocity is zero). The remaining two elements, formula_4 and formula_5, describe the transducer forward and reverse transfer functions respectively. Once these ideas were in place, engineers were able to extend electrical theory into the mechanical domain and analyse an electromechanical system as a unified whole.\n\nAn early application of these new theoretical tools was in phonographic sound reproduction. A recurring problem with early phonograph designs was that mechanical resonances in the pickup and sound transmission mechanism caused excessively large peaks and troughs in the frequency response, resulting in poor sound quality. In 1923, Harrison of the Western Electric Company filed a patent for a phonograph in which the mechanical design was entirely represented as an electrical circuit. The horn of the phonograph is represented as a transmission line, and is a resistive load for the rest of the circuit, while all the mechanical and acoustic parts—from the pickup needle through to the horn—are translated into lumped components according to the impedance analogy. The circuit arrived at is a ladder topology of series resonant circuits coupled by shunt capacitors. This can be viewed as a bandpass filter circuit. Harrison designed the component values of this filter to have a specific passband corresponding to the desired audio passband (in this case 100 Hz to 6 kHz) and a flat response. Translating these electrical element values back into mechanical quantities provided specifications for the mechanical components in terms of mass and stiffness, which in turn could be translated into physical dimensions for their manufacture. The resulting phonograph has a flat frequency response in its passband and is free of the resonances previously experienced. Shortly after this, Harrison filed another patent using the same methodology on telephone transmit and receive transducers.\nHarrison used Campbell's image filter theory, which was the most advanced filter theory available at the time. In this theory, filter design is viewed essentially as an impedance matching problem. More advanced filter theory was brought to bear on this problem by Norton in 1929 at Bell Labs. Norton followed the same general approach though he later described to Darlington the filter he designed as being \"maximally flat\". Norton's mechanical design predates the paper by Butterworth who is usually credited as the first to describe the electronic maximally flat filter. The equations Norton gives for his filter correspond to a singly terminated Butterworth filter, that is, one driven by an ideal voltage source with no impedance, whereas the form more usually given in texts is for the doubly terminated filter with resistors at both ends, making it hard to recognise the design for what it is. Another unusual feature of Norton's filter design arises from the series capacitor, which represents the stiffness of the diaphragm. This is the only series capacitor in Norton's representation, and without it, the filter could be analysed as a low-pass prototype. Norton moves the capacitor out of the body of the filter to the input at the expense of introducing a transformer into the equivalent circuit (Norton's figure 4). Norton has used here the \"turning round the L\" impedance transform to achieve this.\n\nThe definitive description of the subject from this period is Maxfield and Harrison's 1926 paper. There, they describe not only how mechanical bandpass filters can be applied to sound reproduction systems, but also apply the same principles to recording systems and describe a much improved disc cutting head.\n\nThe first volume production of mechanical filters was undertaken by Collins Radio Company starting in the 1950s. These were originally designed for telephone frequency-division multiplex applications where there is commercial advantage in using high quality filters. Precision and steepness of the transition band leads to a reduced width of guard band, which in turn leads to the ability to squeeze more telephone channels into the same cable. This same feature is useful in radio transmitters for much the same reason. Mechanical filters quickly also found popularity in VHF/UHF radio intermediate frequency (IF) stages of the high end radio sets (military, marine, amateur radio and the like) manufactured by Collins. They were favoured in the radio application because they could achieve much higher Q-factors than the equivalent \"LC\" filter. High \"Q\" allows filters to be designed which have high selectivity, important for distinguishing adjacent radio channels in receivers. They also had an advantage in stability over both \"LC\" filters and monolithic crystal filters. The most popular design for radio applications was torsional resonators because radio IF typically lies in the 100 to 500 kHz band.\n\nBoth magnetostrictive and piezoelectric transducers are used in mechanical filters. Piezoelectric transducers are favoured in recent designs since the piezoelectric material can also be used as one of the resonators of the filter, thus reducing the number of components and thereby saving space. They also avoid the susceptibility to extraneous magnetic fields of the magnetostrictive type of transducer.\n\nA magnetostrictive material is one which changes shape when a magnetic field is applied. In reverse, it produces a magnetic field when distorted. The magnetostrictive transducer requires a coil of conducting wire around the magnetostrictive material. The coil either induces a magnetic field in the transducer and sets it in motion or else picks up an induced current from the motion of the transducer at the filter output. It is also usually necessary to have a small magnet to bias the magnetostrictive material into its operating range. It is possible to dispense with the magnets if the biasing is taken care of on the electronic side by providing a d.c. current superimposed on the signal, but this approach would detract from the generality of the filter design.\n\nThe usual magnetostrictive materials used for the transducer are either ferrite or compressed powdered iron. Mechanical filter designs often have the resonators coupled with steel or nickel-iron wires, but on some designs, especially older ones, nickel wire may be used for the input and output rods. This is because it is possible to wind the transducer coil directly on to a nickel coupling wire since nickel is slightly magnetostrictive. However, it is not strongly so and coupling to the electrical circuit is weak. This scheme also has the disadvantage of eddy currents, a problem that is avoided if ferrites are used instead of nickel.\n\nThe coil of the transducer adds some inductance on the electrical side of the filter. It is common practice to add a capacitor in parallel with the coil so that an additional resonator is formed which can be incorporated into the filter design. While this will not improve performance to the extent that an additional mechanical resonator would, there is some benefit and the coil has to be there in any case.\n\nA piezoelectric material is one which changes shape when an electric field is applied. In reverse, it produces an electric field when it is distorted. A piezoelectric transducer, in essence, is made simply by plating electrodes on to the piezoelectric material. Early piezoelectric materials used in transducers such as barium titanate had poor temperature stability. This precluded the transducer from functioning as one of the resonators; it had to be a separate component. This problem was solved with the introduction of lead zirconate titanate (abbreviated PZT) which is stable enough to be used as a resonator. Another common piezoelectric material is quartz, which has also been used in mechanical filters. However, ceramic materials such as PZT are preferred for their greater electromechanical coupling coefficient.\n\nOne type of piezoelectric transducer is the Langevin type, named after a transducer used by Paul Langevin in early sonar research. This is good for longitudinal modes of vibration. It can also be used on resonators with other modes of vibration if the motion can be mechanically converted into a longitudinal motion. The transducer consists of a layer of piezoelectric material sandwiched transversally into a coupling rod or resonator.\n\nAnother kind of piezoelectric transducer has the piezoelectric material sandwiched in longitudinally, usually into the resonator itself. This kind is good for torsional vibration modes and is called a torsional transducer.\n\nIt is possible to achieve an extremely high \"Q\" with mechanical resonators. Mechanical resonators typically have a \"Q\" of 10,000 or so, and 25,000 can be achieved in torsional resonators using a particular nickel-iron alloy. This is an unreasonably high figure to achieve with LC circuits, whose \"Q\" is limited by the resistance of the inductor coils.\n\nEarly designs in the 1940s and 1950s started by using steel as a resonator material. This has given way to nickel-iron alloys, primarily to maximise the \"Q\" since this is often the primary appeal of mechanical filters rather than price. Some of the metals that have been used for mechanical filter resonators and their \"Q\" are shown in the table.\n\nPiezoelectric crystals are also sometimes used in mechanical filter designs. This is especially true for resonators that are also acting as transducers for inputs and outputs.\n\nOne advantage that mechanical filters have over LC electrical filters is that they can be made very stable. The resonance frequency can be made so stable that it varies only 1.5 parts per billion (ppb) from the specified value over the operating temperature range (), and its average drift with time can be as low as 4 ppb per day. This stability with temperature is another reason for using nickel-iron as the resonator material. Variations with temperature in the resonance frequency (and other features of the frequency function) are directly related to variations in the Young's modulus, which is a measure of stiffness of the material. Materials are therefore sought that have a small temperature coefficient of Young's modulus. In general, Young's modulus has a negative temperature coefficient (materials become less stiff with increasing temperature) but additions of small amounts of certain other elements in the alloy can produce a material with a temperature coefficient that changes sign from negative through zero to positive with temperature. Such a material will have a zero coefficient of temperature with resonance frequency around a particular temperature. It is possible to adjust the point of zero temperature coefficient to a desired position by heat treatment of the alloy.\n\nIt is usually possible for a mechanical part to vibrate in a number of different modes, however the design will be based on a particular vibrational mode and the designer will take steps to try to restrict the resonance to this mode. As well as the straightforward longitudinal mode some others which are used include flexural mode, torsional mode, radial mode and drumhead mode.\n\nModes are numbered according to the number of half-wavelengths in the vibration. Some modes exhibit vibrations in more than one direction (such as drumhead mode which has two) and consequently the mode number consists of more than one number. When the vibration is in one of the higher modes, there will be multiple nodes on the resonator where there is no motion. For some types of resonator, this can provide a convenient place to make a mechanical attachment for structural support. Wires attached at nodes will have no effect on the vibration of the resonator or the overall filter response. In figure 5, some possible anchor points are shown as wires attached at the nodes. The modes shown are (5a) the second longitudinal mode fixed at one end, (5b) the first torsional mode, (5c) the second torsional mode, (5d) the second flexural mode, (5e) first radial expansion mode and (5f) first radially symmetric drumhead mode.\n\nThere are a great many combinations of resonators and transducers that can be used to construct a mechanical filter. A selection of some of these is shown in the diagrams. Figure 6 shows a filter using disc flexural resonators and magnetostrictive transducers. The transducer drives the centre of the first resonator, causing it to vibrate. The edges of the disc move in antiphase to the centre when the driving signal is at, or close to, resonance, and the signal is transmitted through the connecting rods to the next resonator. When the driving signal is not close to resonance, there is little movement at the edges, and the filter rejects (does not pass) the signal. Figure 7 shows a similar idea involving longitudinal resonators connected together in a chain by connecting rods. In this diagram, the filter is driven by piezoelectric transducers. It could equally well have used magnetostrictive transducers. Figure 8 shows a filter using torsional resonators. In this diagram, the input has a torsional piezoelectric transducer and the output has a magnetostrictive transducer. This would be quite unusual in a real design, as both input and output usually have the same type of transducer. The magnetostrictive transducer is only shown here to demonstrate how longitudinal vibrations may be converted to torsional vibrations and vice versa. Figure 9 shows a filter using drumhead mode resonators. The edges of the discs are fixed to the casing of the filter (not shown in the diagram) so the vibration of the disc is in the same modes as the membrane of a drum. Collins calls this type of filter a disc wire filter.\n\nThe various types of resonator are all particularly suited to different frequency bands. Overall, mechanical filters with lumped elements of all kinds can cover frequencies from about 5 to 700 kHz although mechanical filters down as low as a few kilohertz (kHz) are rare. The lower part of this range, below 100 kHz, is best covered with bar flexural resonators. The upper part is better done with torsional resonators. Drumhead disc resonators are in the middle, covering the range from around 100 to 300 kHz.\n\nThe frequency response behaviour of all mechanical filters can be expressed as an equivalent electrical circuit using the impedance analogy described above. An example of this is shown in figure 8b which is the equivalent circuit of the mechanical filter of figure 8a. Elements on the electrical side, such as the inductance of the magnetostrictive transducer, are omitted but would be taken into account in a complete design. The series resonant circuits on the circuit diagram represent the torsional resonators, and the shunt capacitors represent the coupling wires. The component values of the electrical equivalent circuit can be adjusted, more or less at will, by modifying the dimensions of the mechanical components. In this way, all the theoretical tools of electrical analysis and filter design can be brought to bear on the mechanical design. Any filter realisable in electrical theory can, in principle, also be realised as a mechanical filter. In particular, the popular finite element approximations to an ideal filter response of the Butterworth and Chebyshev filters can both readily be realised. As with the electrical counterpart, the more elements that are used, the closer the approximation approaches the ideal, however, for practical reasons the number of resonators does not normally exceed eight.\n\nFrequencies of the order of megahertz (MHz) are above the usual range for mechanical filters. The components start to become very small, or alternatively the components are large compared to the signal wavelength. The lumped element model described above starts to break down and the components must be considered as distributed elements. The frequency at which the transition from lumped to distributed models takes place is much lower for mechanical filters than it is for their electrical counterparts. This is because mechanical vibrations travel at the speed of sound for the material the component is composed of. For solid components, this is many times (x15 for nickel-iron) the speed of sound in air () but still considerably less than the speed of electromagnetic waves (approx. in vacuum). Consequently, mechanical wavelengths are much shorter than electrical wavelengths for the same frequency. Advantage can be taken of these effects by deliberately designing components to be distributed elements, and the components and methods used in electrical distributed element filters can be brought to bear. The equivalents of stubs and impedance transformers are both achievable. Designs which use a mixture of lumped and distributed elements are referred to as semi-lumped.\n\nAn example of such a design is shown in figure 10a. The resonators are disc flexural resonators similar to those shown in figure 6, except that these are energised from an edge, leading to vibration in the fundamental flexural mode with a node in the centre, whereas the figure 6 design is energised in the centre leading to vibration in the second flexural mode at resonance. The resonators are mechanically attached to the housing by pivots at right angles to the coupling wires. The pivots are to ensure free turning of the resonator and minimise losses. The resonators are treated as lumped elements; however, the coupling wires are made exactly one half-wavelength (λ/2) long and are equivalent to a λ/2 open circuit stub in the electrical equivalent circuit. For a narrow-band filter, a stub of this sort has the approximate equivalent circuit of a parallel shunt tuned circuit as shown in figure 10b. Consequently, the connecting wires are being used in this design to add additional resonators into the circuit and will have a better response than one with just the lumped resonators and short couplings. For even higher frequencies, microelectromechanical methods can be used as described below.\n\nBridging wires are rods that couple together resonators that are not adjacent. They can be used to produce poles of attenuation in the stopband. This has the benefit of increasing the stopband rejection. When the pole is placed near the passband edge, it also has the benefit of increasing roll-off and narrowing the transition band. The typical effects of some of these on filter frequency response are shown in figure 11. Bridging across a single resonator (figure 11b) can produce a pole of attenuation in the high stopband. Bridging across two resonators (figure 11c) can produce a pole of attenuation in both the high and the low stopband. Using multiple bridges (figure 11d) will result in multiple poles of attenuation. In this way, the attenuation of the stopbands can be deepened over a broad frequency range.\n\nThe method of coupling between non-adjacent resonators is not limited to mechanical filters. It can be applied to other filter formats and the general term for this class is cross-coupled filter. For instance, channels can be cut between cavity resonators, mutual inductance can be used with discrete component filters, and feedback paths can be used with active analogue or digital filters. Nor was the method first discovered in the field of mechanical filters; the earliest description is in a 1948 patent for filters using microwave cavity resonators. However, mechanical filter designers were the first (1960s) to develop practical filters of this kind and the method became a particular feature of mechanical filters.\n\nA new technology emerging in mechanical filtering is microelectromechanical systems (MEMS). MEMS are very small micromachines with component sizes measured in micrometres (μm), but not as small as nanomachines. These filters can be designed to operate at much higher frequencies than can be achieved with traditional mechanical filters. These systems are mostly fabricated from silicon (Si), silicon nitride (SiN), or polymers. A common component used for radio frequency filtering (and MEMS applications generally), is the cantilever resonator. Cantilevers are simple mechanical components to manufacture by much the same methods used by the semiconductor industry; masking, photolithography and etching, with a final undercutting etch to separate the cantilever from the substrate. The technology has great promise since cantilevers can be produced in large numbers on a single substrate—much as large numbers of transistors are currently contained on a single silicon chip.\n\nThe resonator shown in figure 12 is around 120 μm in length. Experimental complete filters with an operating frequency of 30 GHz have been produced using cantilever varactors as the resonator elements. The size of this filter is around 4×3.5 mm. Cantilever resonators are typically applied at frequencies below 200 MHz, but other structures, such as micro-machined cavities, can be used in the microwave bands. Extremely high \"Q\" resonators can be made with this technology; flexural mode resonators with a \"Q\" in excess of 80,000 at 8 MHz are reported.\n\nThe precision applications in which mechanical filters are used require that the resonators are accurately adjusted to the specified resonance frequency. This is known as \"trimming\" and usually involves a mechanical machining process. In most filter designs, this can be difficult to do once the resonators have been assembled into the complete filter so the resonators are trimmed before assembly. Trimming is done in at least two stages; coarse and fine, with each stage bringing the resonance frequency closer to the specified value. Most trimming methods involve removing material from the resonator which will increase the resonance frequency. The target frequency for a coarse trimming stage consequently needs to be set below the final frequency since the tolerances of the process could otherwise result in a frequency higher than the following fine trimming stage could adjust for.\n\nThe coarsest method of trimming is grinding of the main resonating surface of the resonator; this process has an accuracy of around . Better control can be achieved by grinding the edge of the resonator instead of the main surface. This has a less dramatic effect and consequently better accuracy. Processes that can be used for fine trimming, in order of increasing accuracy, are sandblasting, drilling, and laser ablation. Laser trimming is capable of achieving an accuracy of .\n\nTrimming by hand, rather than machine, was used on some early production components but would now normally only be encountered during product development. Methods available include sanding and filing. It is also possible to add material to the resonator by hand, thus reducing the resonance frequency. One such method is to add solder, but this is not suitable for production use since the solder will tend to reduce the high \"Q\" of the resonator.\n\nIn the case of MEMS filters, it is not possible to trim the resonators outside of the filter because of the integrated nature of the device construction. However, trimming is still a requirement in many MEMS applications. Laser ablation can be used for this but material deposition methods are available as well as material removal. These methods include laser or ion-beam induced deposition.\n\n\n\n"}
{"id": "28317883", "url": "https://en.wikipedia.org/wiki?curid=28317883", "title": "Minister of Food Processing Industries (India)", "text": "Minister of Food Processing Industries (India)\n\nThe Minister of Food Processing Industries is the head of the Ministry of Food Processing Industries and one of the cabinet ministers of the Government of India. The current Minister of Food Processing Industries is Harsimrat Kaur Badal of the Shiromani Akali Dal since 26 May 2014.\n"}
{"id": "1712357", "url": "https://en.wikipedia.org/wiki?curid=1712357", "title": "Multiple-effect evaporator", "text": "Multiple-effect evaporator\n\nA multiple-effect evaporator, as defined in chemical engineering, is an apparatus for efficiently using the heat from steam to evaporate water. In a multiple-effect evaporator, water is boiled in a sequence of vessels, each held at a lower pressure than the last. Because the boiling temperature of water decreases as pressure decreases, the vapor boiled off in one vessel can be used to heat the next, and only the first vessel (at the highest pressure) requires an external source of heat. While in theory, evaporators may be built with an arbitrarily large number of stages, evaporators with more than four stages are rarely practical except in systems where the liquor is the desired product such as in chemical recovery systems where up to seven effects are used.\n\nThe multiple-effect evaporator was invented by an African-American inventor and engineer Norbert Rillieux. Although he may have designed the apparatus during the 1820s and constructed a prototype in 1834, he did not build the first industrially practical evaporator until 1845. Originally designed for concentrating sugar in sugar cane juice, it has since become widely used in all industrial applications where large volumes of water must be evaporated, such as salt production and water desalination.\n\nMultiple effect evaporation commonly uses sensible heat in the condensate to preheat liquor to be flashed. In practice the design liquid flow paths can be somewhat complicated in order to extract the most recoverable heat and to obtain the highest evaporation rates from the equipment. \n\nMultiple-effect evaporation plants in sugar beet factories have up to eight effects. Six effect evaporators are common in the recovery of black liquor in the kraft process for making wood pulp. \n\n"}
{"id": "33374391", "url": "https://en.wikipedia.org/wiki?curid=33374391", "title": "Online model", "text": "Online model\n\nAn online model is a mathematical model which tracks and mirrors a plant or process in real-time, and which is implemented with some form of automatic adaptivity to compensate for model degradation over time.\n\nAn online model is also sometimes referred to as an \"online simulator\" or \"online system\".\n\nAn online model is related to the concept of real-time simulation, as an online model runs in real-time by definition. \nConversely a real-time simulation is not necessarily an online model as it by definition does not require adaptivity in either states or parameters.\n\nWhile many models are adjusted to better fit historical data, traditionally this is done in campaigns or during initial design, and often this is done either manually or with a combination of mathematical and manual methods. By contrast, online models include some automatic procedure to adapt to new process data.\n\nOnline models are an aspect of process simulation that deals with the use of estimation techniques to ensure that the state and parameter of the process model are as close a match as possible to the real plant.\n\nReasons that models may need to be taken online include that it is not possible to find offline data for the entire range of operating conditions or that the process is time-varying\n\nParameter estimation can also be used as a technique to capture the influence of effects that are not explicitly modeled, in which case parameters may need to take on values that differ from text-book or table values.\n\nAn online model as defined here has the useful property that it closely resembles the real plant, and for this reason the online model can at any time by used to assess planned changes in operations, either for control, for optimization and for different operational tasks to be performed. It can then be used amongst other things for real-time monitoring, de-bottlenecking or plant redesign, or for \"what-if\" analysis.\n\nAnalyzing trends of how estimated parameters and states in the updated model have changed over time may itself be useful for detecting errors or events in that have occurred during operation and influenced the plant.\n\nThe concepts of online models have origins in control engineering.\n\nOnline models have three commonalities:\n\nOne important aspect of bringing a model online is parameter estimation. By some means the parameters of the online model should match the real plant for the model to be a useful analog. System identification and estimation theory describe techniques to estimate values of unknown parameters.\n\nUsing observers such as the Kalman filter or the moving-horizon estimator, it is possible to do state estimation, updating the state of the model to ensure that measured and modeled outputs remain as close as possible over time.\n\nIt is possible to combine state and parameter estimation, for instance by using an augmented Kalman filter.\n\nMathematical process models can be used for engineering in the design phase prior to building whole or part of a process plant, but models cannot be brought online at this stage as no process data exists to feed to the model.\nHowever once the plant is built and in use, it is tempting to reuse the model used in the design phase for operation, control and optimization tasks. Re-using models in this manner is often term \"life-cycle simulators\".\n\nAn alternative way to obtain an online model is to build one from scratch for this purpose, the advantage of this approach is that issues such as complexity and simulation speed can be tailored for the needs of online use.\n\nOnline models are used in \n"}
{"id": "3073096", "url": "https://en.wikipedia.org/wiki?curid=3073096", "title": "Open innovation", "text": "Open innovation\n\nOpen innovation is a term used to promote an information age mindset toward innovation that runs counter to the secrecy and silo mentality of traditional corporate research labs. The benefits and driving forces behind increased openness have been noted and discussed as far back as the 1960s, especially as it pertains to interfirm cooperation in R&D. Use of the term 'open innovation' in reference to the increasing embrace of external cooperation in a complex world has been promoted in particular by Henry Chesbrough, adjunct professor and faculty director of the Center for Open Innovation of the Haas School of Business at the University of California, who articulated a modern perspective in his book \"Open Innovation: The new imperative for creating and profiting from technology\" (2003). \n\nThe term was originally referred to as \"a paradigm that assumes that firms can and should use external ideas as well as internal ideas, and internal and external paths to market, as the firms look to advance their technology\". More recently, it is defined as \"a distributed innovation process based on purposively managed knowledge flows across organizational boundaries, using pecuniary and non-pecuniary mechanisms in line with the organization's business model\". This more recent definition acknowledges that open innovation is not solely firm-centric: it also includes creative consumers and communities of user innovators. The boundaries between a firm and its environment have become more permeable; innovations can easily transfer inward and outward between firms and other firms and between firms and creative consumers, resulting in impacts at the level of the consumer, the firm, an industry, and society. \n\nBecause innovations tend to be produced by outsiders and founders in startups, rather than existing organizations, the central idea behind open innovation is that, in a world of widely distributed knowledge, companies cannot afford to rely entirely on their own research, but should instead buy or license processes or inventions (i.e. patents) from other companies. In addition, internal inventions not being used in a firm's business should be taken outside the company (e.g. through licensing, joint ventures or spin-offs).\n\nThe open innovation paradigm can be interpreted to go beyond just using external sources of innovation such as customers, rival companies, and academic institutions, and can be as much a change in the use, management, and employment of intellectual property as it is in the technical and research driven generation of intellectual property. In this sense, it is understood as the systematic encouragement and exploration of a wide range of internal and external sources for innovative opportunities, the integration of this exploration with firm capabilities and resources, and the exploitation of these opportunities through multiple channels.\n\nOpen innovation offers several benefits to companies operating on a program of global collaboration:\n\nImplementing a model of open innovation is naturally associated with a number of risks and challenges, including:\n\nIn the UK the Knowledge Transfer Partnerships (KTP) is a funding mechanism encouraging the partnership between a firm and a knowledge-based partner. A KTP is a collaboration program between a knowledge-based partner (i.e. a research institution), a company partner and one or more associates (i.e. recently qualified persons such as graduates). KTP initiatives aim to deliver significant improvement in business partners’ profitability as a direct result of the partnership through enhanced quality and operations, increased sales and access to new markets. At the end of their KTP project, the three actors involved have to prepare a final report that describes KTP initiative supported the achievement of the project’s innovation goals.\n\nThis approach involves developing and introducing a partially completed product, for the purpose of providing a framework or tool-kit for contributors to access, customize, and exploit. The goal is for the contributors to extend the platform product's functionality while increasing the overall value of the product for everyone involved.\n\nReadily available software frameworks such as a software development kit (SDK), or an application programming interface (API) are common examples of product platforms. This approach is common in markets with strong network effects where demand for the product implementing the framework (such as a mobile phone, or an online application) increases with the number of developers that are attracted to use the platform tool-kit. The high scalability of platforming often results in an increased complexity of administration and quality assurance.\n\nThis model entails implementing a system that encourages competitiveness among contributors by rewarding successful submissions. Developer competitions such as hackathon events fall under this category of open innovation. This method provides organizations with inexpensive access to a large quantity of innovative ideas, while also providing a deeper insight into the needs of their customers and contributors.\n\nWhile mostly oriented toward the end of the product development cycle, this technique involves extensive customer interaction through employees of the host organization. Companies are thus able to accurately incorporate customer input, while also allowing them to be more closely involved in the design process and product management cycle.\n\nSimilarly to product platforming, an organization incorporates their contributors into the development of the product. This differs from platforming in the sense that, in addition to the provision of the framework on which contributors develop, the hosting organization still controls and maintains the eventual products developed in collaboration with their contributors. This method gives organizations more control by ensuring that the correct product is developed as fast as possible, while reducing the overall cost of development. Dr. Henry Chesbrough recently supported this model for open innovation in the optics and photonics industry.\n\nSimilarly to idea competitions, an organization leverages a network of contributors in the design process by offering a reward in the form of an incentive. The difference relates to the fact that the network of contributors are used to develop solutions to identified problems within the development process, as opposed to new products. Emphasis needs to be placed on assessing organisational capabilities to ensure value creation in open innovation.\n\nIn Austria the Ludwig Boltzmann Gesellschaft started a project named \"Tell us!\" about mental health issues and used the concept of open innovation to crowdsource research questions. The institute also launched the first \"Lab for Open Innovation in Science\" to teach 20 selected scientists the concept of open innovation over the course of one year. On Facebook the Ludwig Boltzmann Gesellschaft informs about the lab, the participants and teachers and on news on open innovation in science.\n\nA European startup has proved that engineering crowdsourcing delivers good results for open innovation in technology. This startup, ennomotive, organizes competitions to solve real-life engineering challenges coming from companies. Its global community of engineers submits solutions through an online platform and, after a multi-round filtering process, the company selects and awards the best solutions. This way, complex issues like asphalting in the rain or monitoring wildfires in the forest through IoT have been solved.\n\nThe paradigm of closed innovation holds that successful innovation requires control. Particularly, a company should control the generation of their own ideas, as well as production, marketing, distribution, servicing, financing, and supporting. What drove this idea is that, in the early twentieth century, academic and government institutions were not involved in the commercial application of science. As a result, it was left up to other corporations to take the new product development cycle into their own hands. There just was not the time to wait for the scientific community to become more involved in the practical application of science. There also was not enough time to wait for other companies to start producing some of the components that were required in their final product. These companies became relatively self-sufficient, with little communication directed outwards to other companies or universities.\n\nThroughout the years several factors emerged that paved the way for open innovation paradigms:\n\nThese four factors have resulted in a new market of knowledge. Knowledge is not anymore proprietary to the company. It resides in employees, suppliers, customers, competitors and universities. If companies do not use the knowledge they have inside, someone else will. Innovation can be generated either by means of closed innovation or by open innovation paradigms. There is an ongoing debate on which paradigm will dominate in the future.\n\nModern research of open innovation is divided into two groups, which have several names, but are similar in their essence (discovery and exploitation; outside-in and inside-out; inbound and \noutbound). The common factor for different names is the direction of innovation, whether from outside the company in, or from inside the company out:\n\nThis type of open innovation is when a company freely shares its resources with other partners, without an instant financial reward. The source of profit has an indirect nature and is manifested as a new type of business model.\n\nIn this type of open innovation a company commercialises its inventions and technology through selling or licensing technology to a third party.\n\nThis type of open innovation is when companies use freely available external knowledge, as a source of internal innovation. Before starting any internal R&D project a company should monitor the external environment in search for existing solutions, thus, in this case, internal R&D become tools to absorb external ideas for internal needs.\n\nIn this type of open innovation a company is buying innovation from its partners through licensing, or other procedures, involving monetary reward for external knowledge\n\nOpen source and open innovation might conflict on patent issues. This conflict is particularly apparent when considering technologies that may save lives, or other open-source-appropriate technologies that may assist in poverty reduction or sustainable development. However, open source and open innovation are not mutually exclusive, because participating companies can donate their patents to an independent organization, put them in a common pool, or grant unlimited license use to anybody. Hence some open-source initiatives can merge these two concepts: this is the case for instance for IBM with its \"Eclipse\" platform, which the company presents as a case of open innovation, where competing companies are invited to cooperate inside an open-innovation network.\n\nIn 1997, Eric Raymond, writing about the open-source software movement, coined the term \"the cathedral and the bazaar\". The cathedral represented the conventional method of employing a group of experts to design and develop software (though it could apply to any large-scale creative or innovative work). The bazaar represented the open-source approach. This idea has been amplified by a lot of people, notably Don Tapscott and Anthony D. Williams in their book Wikinomics. Eric Raymond himself is also quoted as saying that 'one cannot code from the ground up in bazaar style. One can test, debug, and improve in bazaar style, but it would be very hard to originate a project in bazaar mode'. In the same vein, Raymond is also quoted as saying 'The individual wizard is where successful bazaar projects generally start'.\n\n\n\n"}
{"id": "8889260", "url": "https://en.wikipedia.org/wiki?curid=8889260", "title": "Payment card industry", "text": "Payment card industry\n\nThe payment card industry (PCI) denotes the debit, credit, prepaid, e-purse, ATM, and POS cards and associated businesses.\n\nThe payment card industry consists of all the organizations which store, process and transmit cardholder data, most notably for debit cards and credit cards. The security standards are developed by the Payment Card Industry Security Standards Council which develops the Payment Card Industry Data Security Standards used throughout the industry. Individual card brands establish compliance requirements that are used by service providers and have their own compliance programs. Major card brands include American Express, Discover Financial Services, China UnionPay, Japan Credit Bureau, MasterCard Worldwide and Visa International. Most companies use member banks that connect and accept transactions from the card brands. Not all card brands use member banks, like American Express, these instead act as their own bank.\n\n, the United States uses a magnetic stripe on a card to process transactions and its security relies on the holder's signature and visual inspection of the card to check for features such as hologram. This system will be outmoded and replaced by EMV in 2015. EMV is a global standard for inter-operation of integrated circuit cards (IC cards or \"chip cards\") and IC card capable point of sale (POS) terminals and automated teller machines (ATMs), for authenticating credit and debit card transactions. It has enhanced security features, but is still susceptible to fraud.\n\nOn 7 September 2006, American Express, Discover Financial Services, Japan Credit Bureau, MasterCard Worldwide and Visa International formed the Payment Card Industry Security Standards Council (PCI SSC) security council with the goal of managing the ongoing evolution of the Payment Card Industry Data Security Standard. The council itself claims to be independent of the various card vendors that make up the council. As of 1 August 2014, the PCI SSC website lists 688 \"Participating Organizations\". Internationally, 61 different financial institutions were noted, including Bank of America, Capital One, JP Morgan Chase, Royal Bank of Scotland, TD Bank and Wells Fargo. A total of 275 merchants were listed, including Amazon.com, Burger King, Citgo, Dell, Equifax, Exxon Mobil, Global Cash Access, Motorola, Microsoft, Southwest Airlines and Walmart.\n\nMasterCard's Nicole Krieg has noted that the Russian credit card market started in early 2000, when issuers first began launching products. However, credit products became especially popular in Russia in 2005, after new legislation took effect. Immense growth was noted in just eight years, by comparing second quarter growth on Visa card purchases, which went from $306 million in 2002 to $61.5 billion in 2010. Merchants who accepted Visa cards also increased from 21,000 to 331,000 during the same period. Visa also noted that they had issued 70 million cards and the Central Bank of the Russian Federation reported that 8.6 million credit cards were on issue.\n\nThe Interac Association is Canada's national organization linking Financial Institutions and enterprises that have proprietary networks, to enable communication with each other for the purpose of exchanging electronic financial transactions. The Association was founded in 1984 by the big five banks. Today, there are over 80 members. The Interac Association is the organization responsible for the development of Canada's national network of two shared electronic financial services: Shared Cash Dispensing (SCD) for cash withdrawals from any ABM not belonging to a cardholder's financial institution; and Interac Direct Payment (IDP) for Debit Card payments at the Point-of-Sale\n\n\n\n"}
{"id": "22050856", "url": "https://en.wikipedia.org/wiki?curid=22050856", "title": "Personal jurisdiction over international defendants in the United States", "text": "Personal jurisdiction over international defendants in the United States\n\nQuestions over personal jurisdiction over international defendants in the United States arise when foreign nationals commit crimes against Americans, or when a person from or in a different country is sued in U.S. courts, or when events took place in another country. Such cases arise when crimes are committed on the high seas or on international flights, when crimes are alleged to be committed by or against Americans in foreign countries (such as under the Foreign Corrupt Practices Act), or when crimes are committed by foreigners against Americans. The Internet also allows computer crime to cross international boundaries.\n\nThere are several mechanisms in public international law whereby the courts of one country (the domestic court) can exercise jurisdiction over a citizen, corporation, or organization of another country (the foreign defendant) to try crimes or civil matters that have affected citizens or businesses within the domestic jurisdiction. Many of these jurisdictional \"hooks\" can even reach conduct that affected the domestic citizen when the citizen was beyond his or her domestic borders. There are five such doctrines:\n\n\nIn the United States, the federal courts have recognized an important mechanism for acquiring jurisdiction over foreign defendants known as the effects doctrine. The effects doctrine is an offshoot of the territorial principle. Briefly, the effects doctrine says that if the effects of extraterritorial behavior or crimes adversely affect commerce or harm citizens within the United States, then jurisdiction in a U.S. court is permissible. The first case to establish the effects doctrine was \"United States v. Alcoa\", 148 F.2d 416 (2d Cir. 1945) (Learned Hand, J.).\n\nThe ALCOA case brought charges against a foreign consortium of aluminum traders and producers who had affected the price of raw aluminum and goods manufactured from aluminum in the United States through unfair trade practices of price fixing in violation of section 1 of the Sherman Antitrust Act (\"every contract, combination ... or conspiracy, in restraint of trade or commerce among the several States, or with foreign nations, is declared to be illegal\").\nThe effects doctrine has also been incorporated into § 402 of the \"Restatement of Foreign Relations Law of the United States\", Third: \"a state has jurisdiction to prescribe law with respect to ... (c) conduct outside its territory that has or is intended to have substantial effect within its territory.\"\n\nIn one case, the decision to allow jurisdiction in a U.S. court over claims of copyright infringement and cybersquatting was premised on an effects doctrine theory of jurisdiction. \"Graduate Management Admission Council v. Raju\", 241 F.Supp.2d 589 (E.D. Va., 2003). The defendant in \"Raju\" was a citizen of India who sold \"official\" past Graduate Record Examinations (GREs) to U.S. customers that were of dubious origin and in violation of the plaintiff, copyright-holder Graduate Management Admission Council (GMAC). These exams were advertised and sold over the Internet.\n\nThe defendant never made an appearance on U.S. territory depriving the plaintiffs of one easy avenue of obtaining \"in personam\" jurisdiction over the defendant – the simple act of being able to serve process on the defendant while the defendant is visiting and within the territory of the United States (this would be the traditional territorial principle of jurisdiction at work, to use terms of international law). The defendant was not a citizen of a particular state. The court described the jurisdiction it exercised over Raju's conduct of selling illegal copies of the exams to potential purchasers in several states within the territory of the U.S. as \"targeting\" the U.S. market for U.S. purchasers. Under these circumstances, the court found that personal jurisdiction was proper under a theory of national jurisdiction: the defendant had targeted the U.S. at large from outside of the territory and intended to avail himself of the opportunity of selling test answers to a U.S. graduate school entrance test to his most likely customers: Americans.\n\nA judgment was issued against the defendant Raju who defaulted by never making an appearance to the district court where he was being sued.\n\nIn a procedurally complicated case, \"Yahoo! Inc. v. La Ligue Contre Le Racisme et l'Antisemitisme (LICRA)\", the 9th Circuit Court of Appeals held that it had personal jurisdiction over two French organizations who sued Yahoo! in a French court. The court found that all of the following acts, in combination, were sufficient contacts to create personal jurisdiction over the French organizations: sending letters to Yahoo!, suing Yahoo! and serving Yahoo! in California, and the suit resulting in orders that Yahoo!'s officers in California comply with French law.\n"}
{"id": "44522280", "url": "https://en.wikipedia.org/wiki?curid=44522280", "title": "Plasmonic Circuitry", "text": "Plasmonic Circuitry\n\nPlasmonics is the study of plasmons, quasiparticles of plasma oscillation in solids such as metals, semi-metals, metal oxides, nitrides, doped semiconductors, etc. An effort is currently being made to implement plasmons in electric circuits, or in an electric circuit analog, to combine the size efficiency of electronics with the data capacity of photonic integrated circuits. Plasmonics can be understood as \"light-on-metal-dielectric-interfaces,\" where electrons oscillate at the surface of a metal due to strong resonant interactions with the electric field of incident light. Due to the high scattering rate of electrons, ohmic losses in plasmonic signals are generally large, which limits the signal transfer distances to the sub-centimeter range, unless hybrid optoplasmonic light guiding networks, or plasmon gain amplification are used. Both surface plasmon polaritons propagating along the metal-dielectric interfaces and localized surface plasmon modes supported by metal nanoparticles are characterized by large momentum values, which enable strong resonant enhancement of the local density of photon states, and can be utilized to enhance weak optical effects of opto-electronic devices.\n\nOne of the biggest issues in making plasmonic circuits a feasible reality is the impractically short propagation length of surface plasmons. Typically, surface plasmons travel distances only on the scale of millimeters before damping diminishes the signal. This is largely due to the unique dispersion relation of surface plasmons, which shows that as confinement increases, resistive damping increases; thus, propagation length decreases. Researchers are attempting to reduce losses in surface plasmon propagation by examining a variety of materials and their respective properties. New promising low-loss plasmonic materials include metal oxides and nitrides as well as graphene. Another foreseeable barrier plasmonic circuits will have to overcome is heat; heat in a plasmonic circuit may or may not exceed the heat generated by complex electronic circuits. It has recently been proposed to reduce heating in plasmonic networks by designing them to support trapped optical vortices, which circulate light powerflow through the inter-particle gaps thus reducing absorption and Ohmic heating, In addition to heat, it is also difficult to change the direction of a plasmonic signal in a circuit without significantly reducing its amplitude and propagation length. One clever solution to the issue of bending the direction of propagation is the use of Bragg mirrors to angle the signal in a particular direction, or even to function as splitters of the signal. Finally, emerging applications of plasmonics for thermal emission manipulation and heat-assisted magnetic recording leverage Ohmic losses in metals to obtain devices with new enhanced functionalities.\n\nOptimal plasmonic waveguide designs strive to maximize both the confinement and propagation length of surface plasmons within a plasmonic circuit. Surface plasmon polaritons are characterized by a complex wave vector, with components parallel and perpendicular to the metal-dielectric interface. The imaginary part of the wave vector component is inversely proportional to the SPP propagation length, while its real part defines the SPP confinement. The SPP dispersion characteristics depend on the dielectric constants of the materials comprising the waveguide. The propagation length and confinement of the surface plasmon polariton wave are inversely related. Therefore, stronger confinement of the mode typically results in shorter propagation lengths. The construction of a practical and usable surface plasmon circuit is heavily dependent on a compromise between propagation and confinement. Maximizing both confinement and propagation length helps mitigate the drawbacks of choosing propagation length over confinement and vice versa. Multiple types of waveguides have been created in pursuit of a plasmonic circuit with strong confinement and sufficient propagation length. Some of the most common types include insulator-metal-insulator (IMI), metal-insulator-metal (MIM), dielectric loaded surface plasmon polariton (DLSPP), gap plasmon polariton (GPP), channel plasmon polariton (CPP), wedge surface plasmon polariton (wedge), and hybrid opto-plasmonic waveguides and networks. Dissipation losses accompanying SPP propagation in metals can be mitigated by gain amplification or by combining them into hybrid networks with photonic elements such as fibers and coupled-resonator waveguides. This design can result in the previously mentioned hybrid plasmonic waveguide, which exhibits subwavelength mode on a scale of one-tenth of the diffraction limit of light, along with an acceptable propagation length.\n\nThe input and output ports of a plasmonic circuit will receive and send optical signals, respectively. To do this, coupling and decoupling of the optical signal to the surface plasmon is necessary. The dispersion relation for the surface plasmon lies entirely below the dispersion relation for light, which means that for coupling to occur additional momentum should be provided by the input coupler to achieve the momentum conservation between incoming light and surface plasmon polariton waves launched in the plasmonic circuit. There are several solutions to this, including using dielectric prisms, gratings, or localized scattering elements on the surface of the metal to help induce coupling by matching the momenta of the incident light and the surface plasmons. After a surface plasmon has been created and sent to a destination, it can then be converted into an electrical signal. This can be achieved by using a photodetector in the metal plane, or decoupling the surface plasmon into freely propagating light that can then be converted into an electrical signal. Alternatively, the signal can be out-coupled into a propagating mode of an optical fiber or waveguide.\n\nThe progress made in surface plasmons over the last 50 years has led to the development in various types of devices, both active and passive. A few of the most prominent areas of active devices are optical, thermo-optical, and electro-optical. All-optical devices have shown the capacity to become a viable source for information processing, communication, and data storage when used as a modulator. In one instance, the interaction of two light beams of different wavelengths was demonstrated by converting them into co-propagating surface plasmons via cadmium selenide quantum dots. Electro-optical devices have combined aspects of both optical and electrical devices in the form of a modulator as well. Specifically, electro-optic modulators have been designed using evanescently coupled resonant metal gratings and nanowires that rely on long-range surface plasmons (LRSP). Likewise, thermo-optic devices, which contain a dielectric material whose refractive index changes with variation in temperature, have also been used as interferometric modulators of SPP signals in addition to directional-coupler switches. Some thermo-optic devices have been shown to utilize LRSP waveguiding along gold stripes that are embedded in a polymer and heated by electrical signals as a means for modulation and directional-coupler switches. Another potential field lies in the use of spasers in areas such as nanoscale lithography, probing, and microscopy.\n\nAlthough active components play an important role in the use of plasmonic circuitry, passive circuits are just as integral and, surprisingly, not trivial to make. Many passive elements such as prisms, lenses, and beam splitters can be implemented in a plasmonic circuit, however fabrication at the nano scale has proven difficult and has adverse effects. Significant losses can occur due to decoupling in situations where a refractive element with a different refractive index is used. However, some steps have been taken to minimize losses and maximize compactness of the photonic components. One such step relies on the use of Bragg reflectors, or mirrors composed of a succession of planes to steer a surface plasmon beam. When optimized, Bragg reflectors can reflect nearly 100% of the incoming power. Another method used to create compact photonic components relies on CPP waveguides as they have displayed strong confinement with acceptable losses less than 3 dB within telecommunication wavelengths. Maximizing loss and compactness with regards to the use of passive devices, as well as active devices, creates more potential for the use of plasmonic circuits.\n"}
{"id": "39528525", "url": "https://en.wikipedia.org/wiki?curid=39528525", "title": "Polycarbonyl", "text": "Polycarbonyl\n\nPolycarbonyl, (also known as polymeric-CO, p-CO or poly-CO) is a solid metastable and explosive polymer of carbon monoxide. The polymer is produced by exposing carbon monoxide to high pressures. The structure of the solid appears amorphous, but may include a zig zag of equally spaced CO groups.\n\nPoly-CO can be produced at pressures of 5.2 GPa. Polymerisation is catalysed by blue light at slightly lower pressures in the δ-phase of solid CO. Another white phase can be made at higher temperatures at 6 or 7 GPa. Poly-CO appears to be a yellow to dark red amorphous phase. Whereas the white phase appears to be crystalline.\n\nR. J. Mills discovered this solid, which was first produced in a tungsten carbide anvil in 1947. Originally this was thought to be polymeric carbon suboxide, but the formation does not yield any gas byproduct such as carbon dioxide. The yield of the solid can be up to 95%.\n\nThe polymer is stable above about 80K. Below this temperature the ε form of solid molecular CO is formed instead. When the pressure is released the polymer remains stable at atmospheric pressure. The solid dissolves in water, alcohol and acetone. When exposed to the atmosphere it is hygroscopic, becomes gluey, and changes colour, becoming darker. The reaction with water produces carboxylic groups.\n\nThe solid stores a high energy. It can decompose explosively forming glassy carbon and carbon dioxide. The energy density stored can be up to 8 kJ/g. During the decomposition the temperature can be 2500K. The density is 1.65 gcm, however most of the solid produced is porous, so the true density is likely to be higher.\n\nInfrared spectroscopy shows bands at 650, 1210, 1440, 1650 and 1760 cm. The 1760 band is likely to be due to the -C-(C=O)-C- structure. The 1600 is due to vibration of a C=C double bond.\n\nThe solid is electrically insulating with an electronic gap energy of 1.9 eV.\n\nNuclear magnetic resonance for the material made from CO shows sharp resonance at 223 ppm due to ester or lactone attached carbon, and 151 ppm due to C=C double bonds. There is also broad resonance at 109 and 189 ppm. Over time of a few days, the 223 ppm peak reduces and all the other features increase in strength.\n\nIdeas of the structure include a zig zag chain of CO pointing in opposite directions, or five atom rings connected by CO and C-C bonds. The rings are lactones of tetronic acid: -C:-(C=O)-(C-O-)-(C=O)-O-. Interconnections between the rings are zig zags of CO.\n\nOther ideas of the structure of the solid, include graphitic carbon with carbon dioxide under pressure, and a polymer with this CO monomer: -(C=O)-O-(C-)=C<. Yet other ideas are that the solid is the same as the polymer of carbon suboxide with oxalic anhydride.\n\n"}
{"id": "3055969", "url": "https://en.wikipedia.org/wiki?curid=3055969", "title": "Racegun", "text": "Racegun\n\n\"Not to be confused with a starter's pistol\"\n\nA racegun is a type of handgun, shotgun, or rifle that has been modified for accuracy, speed, and reliability. Used primarily in NRA Action Pistol (The Bianchi Cup), United States Practical Shooting Association (USPSA)/International Practical Shooting Confederation (IPSC) and similar styles of competition, raceguns are typically based on common guns and modified to function the best within a certain set of rules, such as weight, size, and capacity requirements.\n\nTypical modifications include a match-grade barrel fitted with a recoil compensator, electronic optical sights, match-grade hammer and sear, a tuned trigger, and \"skeletonizing\" (cutouts to reduce mass). In addition to the modifications aforementioned, a typical open class semi-automatic racegun specifically tailored for The Bianchi Cup open division has a \"barricade shroud\" that completely encircles the slide with \"wings\" attached to opposing sides, a \"moving target scope mount\" with a pivoting base adjustable for predetermined lead depending on the bullet's velocity and speed and direction of the moving target, and a form of grip extension that elevates the gun for better line of sight while shooting in the prone position. Depending upon the competition requirement, some raceguns are modified with reduced-weight recoil springs to allow the use of lightly loaded ammunition that is just barely powerful enough to cycle the gun's mechanism, in order to reduce recoil and permit a faster rate of fire. The Steel Challenge speed shooting championship is such an event, where in its early days a 120 power factor hit was required to activate the stop plate to stop the clock is now no longer necessary so that competitors can go faster than ever before. To prevent this, certain sanctioning bodies such as IPSC and USPSA require ammunition to meet specific power factors.\n\nSome organizing bodies in practical shooting, like the International Defensive Pistol Association and the production division of The Bianchi Cup, ban most or all of the modifications that distinguish raceguns from stock firearms, as they feel that such extensive modifications have turned the sport into a technology race rather than a true contest of skill, and thus are no longer \"practical,\" i.e., applicable to real-life self-defense shooting situations. In the case of The Bianchi Cup/NRA Action Pistol, this was also done to increase participation at the grassroots level in order to reduce the equipment cost associated with competing in the open division, which can run upwards of $3,500 USD or more on a Bianchi racegun as compared to a $400 to $1,000 stock gun. Many believe this new division, in addition to the newly implemented $100,000 cash prize award to the first production class shooter to break 1900 at the national championship in May, will hopefully increase participation that has seen a decline for the last two decades or so. It is the opinion of many experienced individuals involved in the sport that perfect scores of 1920 accompanied with nearly perfect X-counts achieved by the top one or two percent (which were mostly sponsored professional competition shooters and very dedicated Bianchi Cup \"specialists\" shooting their tricked out open guns) have discouraged many top shooters from other disciplines from giving this discipline a try, citing the difficulties of obtaining such results even with these specialized raceguns as being too big of a hurdle.\n"}
{"id": "24722997", "url": "https://en.wikipedia.org/wiki?curid=24722997", "title": "Ricoh GR film cameras", "text": "Ricoh GR film cameras\n\nThe Ricoh GR was a series of point-and-shoot, or compact, 35 mm film cameras made by Ricoh. The chronological list of these cameras are the GR1, GR10, GR1s, GR1v and GR21. The GR name was also used for Ricoh's GR series of digital cameras.\n\nThe cameras had a very high quality 1:2.8 28 mm lens. Exposure control could be program automatic or aperture priority semi-automatic. They had a built-in flash and date imprinting versions were also available.\n\nIt received the 1997 TIPA award for best 35 mm Compact Camera.\n\n\nJapanese street photographer Daido Moriyama is known to have used the GR1v.\n\nBritish conflict photographer Philip Jones Griffiths owned and used a GR1.\n\n\n"}
{"id": "26301", "url": "https://en.wikipedia.org/wiki?curid=26301", "title": "Rocket", "text": "Rocket\n\nA rocket (from Italian \"rocchetto\" \"bobbin\") is a missile, spacecraft, aircraft or other vehicle that obtains thrust from a rocket engine. Rocket engine exhaust is formed entirely from propellant carried within the rocket before use. Rocket engines work by action and reaction and push rockets forward simply by expelling their exhaust in the opposite direction at high speed, and can therefore work in the vacuum of space.\n\nIn fact, rockets work more efficiently in space than in an atmosphere. Multistage rockets are capable of attaining escape velocity from Earth and therefore can achieve unlimited maximum altitude. Compared with airbreathing engines, rockets are lightweight and powerful and capable of generating large accelerations. To control their flight, rockets rely on momentum, airfoils, auxiliary reaction engines, gimballed thrust, momentum wheels, deflection of the exhaust stream, propellant flow, spin, and/or gravity.\n\nRockets for military and recreational uses date back to at least 13th century China. Significant scientific, interplanetary and industrial use did not occur until the 20th century, when rocketry was the enabling technology for the Space Age, including setting foot on the Earth's moon. Rockets are now used for fireworks, weaponry, ejection seats, launch vehicles for artificial satellites, human spaceflight, and space exploration.\n\nChemical rockets are the most common type of high power rocket, typically creating a high speed exhaust by the combustion of fuel with an oxidizer. The stored propellant can be a simple pressurized gas or a single liquid fuel that disassociates in the presence of a catalyst (monopropellants), two liquids that spontaneously react on contact (hypergolic propellants), two liquids that must be ignited to react, a solid combination of fuel with oxidizer (solid fuel), or solid fuel with liquid oxidizer (hybrid propellant system). Chemical rockets store a large amount of energy in an easily released form, and can be very dangerous. However, careful design, testing, construction and use minimizes risks.\n\nThe first gunpowder-powered rockets evolved in medieval China under the Song dynasty by the 13th century. The Mongols adopted Chinese rocket technology and the invention spread via the Mongol invasions to the Middle East and to Europe in the mid-13th century. Rockets are recorded in use by the Song navy in a military exercise dated to 1245. Internal-combustion rocket propulsion is mentioned in a reference to 1264, recording that the \"ground-rat\", a type of firework, had frightened the Empress-Mother Gongsheng at a feast held in her honor by her son the Emperor Lizong. Subsequently, rockets are included in the military treatise \"Huolongjing\", also known as the Fire Drake Manual, written by the Chinese artillery officer Jiao Yu in the mid-14th century. This text mentions the first known multistage rocket, the 'fire-dragon issuing from the water' (huo long chu shui), thought to have been used by the Chinese navy.\n\nMedieval and early modern rockets were used militarily as incendiary weapons in sieges. Between 1270 and 1280, Hasan al-Rammah wrote \"al-furusiyyah wa al-manasib al-harbiyya\" (\"The Book of Military Horsemanship and Ingenious War Devices\"), which included 107 gunpowder recipes, 22 of them for rockets.\nIn Europe, Konrad Kyeser described rockets in his military treatise \"Bellifortis\" around 1405.\nThe name \"rocket\" comes from the Italian \"rocchetta\", meaning \"bobbin\" or \"little spindle\", given due to the similarity in shape to the bobbin or spool used to hold the thread to be fed to a spinning wheel.\nLeonhard Fronsperger and Conrad Haas adopted the Italian term into German in the mid-16th century; \"rocket\" appears in English by the early 17th century.\n\"Artis Magnae Artilleriae pars prima\", an important early modern work on rocket artillery, by Kazimierz Siemienowicz, was first printed in Amsterdam in 1650.\n\nThe Mysorean rockets were the first successful iron-cased rockets, developed in the late 18th century in the Kingdom of Mysore (part of present-day India) by Tipu Sultan. The Congreve rocket was a British weapon designed and developed by Sir William Congreve in 1804. This rocket was based directly on the Mysorean rockets, used compressed powder and was fielded in the Napoleonic Wars. It was Congreve rockets that Francis Scott Key was referring to when he wrote of the \"rockets' red glare\" while held captive on a British ship that was laying siege to Fort McHenry in 1814. Together, the Mysorean and British innovations increased the effective range of military rockets from 100 to 2,000 yards.\n\nThe first mathematical treatment of the dynamics of rocket propulsion is due to William Moore (1813). In 1815 Alexander Dmitrievich Zasyadko constructed rocket-launching platforms, which allowed rockets to be fired in salvos (6 rockets at a time), and gun-laying devices. William Hale in 1844 greatly increased the accuracy of rocket artillery. Edward Mounier Boxer further improved the Congreve rocket in 1865.\n\nWilliam Leitch first proposed the concept of using rockets to enable human spaceflight in 1861 . Konstantin Tsiolkovsky later (in 1903) also conceived this idea, and extensively developed a body of theory that has provided the foundation for subsequent spaceflight development. Robert Goddard in 1920 published proposed improvements to rocket technology in \"A Method of Reaching Extreme Altitudes\". In 1923, Hermann Oberth (1894–1989) published \"Die Rakete zu den Planetenräumen\" (\"The Rocket into Planetary Space\")\n\nModern rockets originated in 1926 when Goddard attached a supersonic (de Laval) nozzle to the combustion chamber of a liquid-propellant rocket. These nozzles turn the hot gas from the combustion chamber into a cooler, hypersonic, highly directed jet of gas, more than doubling the thrust and raising the engine efficiency from 2% to 64%.\nUse of liquid propellants instead of gunpowder greatly improved the effectiveness of rocket artillery in World War II, and opened up the possibility of manned spaceflight after 1945.\n\nIn 1943 production of the V-2 rocket began in Germany.\nIn parallel with the German guided-missile programme, rockets were also used on aircraft, either for assisting horizontal take-off (RATO), vertical take-off (Bachem Ba 349 \"Natter\") or for powering them (Me 163, see list of World War II guided missiles of Germany). The Allies' rocket programs were less technological, relying mostly on unguided missiles like the Soviet Katyusha rocket.\nThe Americans captured a large number of German rocket scientists, including Wernher von Braun, in 1945, and brought them to the United States as part of Operation Paperclip. After World War II scientists used rockets to study high-altitude conditions, by radio telemetry of temperature and pressure of the atmosphere, detection of cosmic rays, and further techniques; note too the Bell X-1, the first manned vehicle to break the sound barrier (1947). Independently, in the Soviet Union's space program research continued under the leadership of the chief designer Sergei Korolev (1907–1966).\n\nDuring the Cold War rockets became extremely important militarily with the development of modern intercontinental ballistic missiles (ICBMs).\nThe 1960s saw rapid development of rocket technology, particularly in the Soviet Union (Vostok, Soyuz, Proton) and in the United States (e.g. the X-15). Rockets came into use for space exploration. American manned programs (Project Mercury, Project Gemini and later the Apollo programme) culminated in 1969 with the first manned landing on the moon – using equipment launched by the Saturn V rocket.\n\n\nRocket vehicles are often constructed in the archetypal tall thin \"rocket\" shape that takes off vertically, but there are actually many different types of rockets including:\n\nA rocket design can be as simple as a cardboard tube filled with black powder, but to make an efficient, accurate rocket or missile involves overcoming a number of difficult problems. The main difficulties include cooling the combustion chamber, pumping the fuel (in the case of a liquid fuel), and controlling and correcting the direction of motion.\n\nRockets consist of a propellant, a place to put propellant (such as a propellant tank), and a nozzle. They may also have one or more rocket engines, directional stabilization device(s) (such as fins, vernier engines or engine gimbals for thrust vectoring, gyroscopes) and a structure (typically monocoque) to hold these components together. Rockets intended for high speed atmospheric use also have an aerodynamic fairing such as a nose cone, which usually holds the payload.\n\nAs well as these components, rockets can have any number of other components, such as wings (rocketplanes), parachutes, wheels (rocket cars), even, in a sense, a person (rocket belt). Vehicles frequently possess navigation systems and guidance systems that typically use satellite navigation and inertial navigation systems.\n\nRocket engines employ the principle of jet propulsion. The rocket engines powering rockets come in a great variety of different types; a comprehensive list can be found in rocket engine. Most current rockets are chemically powered rockets (usually internal combustion engines, but some employ a decomposing monopropellant) that emit a hot exhaust gas. A rocket engine can use gas propellants, solid propellant, liquid propellant, or a hybrid mixture of both solid and liquid. Some rockets use heat or pressure that is supplied from a source other than the chemical reaction of propellant(s), such as steam rockets, solar thermal rockets, nuclear thermal rocket engines or simple pressurized rockets such as water rocket or cold gas thrusters. With combustive propellants a chemical reaction is initiated between the fuel and the oxidizer in the combustion chamber, and the resultant hot gases accelerate out of a rocket engine nozzle (or nozzles) at the rearward-facing end of the rocket. The acceleration of these gases through the engine exerts force (\"thrust\") on the combustion chamber and nozzle, propelling the vehicle (according to Newton's Third Law). This actually happens because the force (pressure times area) on the combustion chamber wall is unbalanced by the nozzle opening; this is not the case in any other direction. The shape of the nozzle also generates force by directing the exhaust gas along the axis of the rocket.\n\nRocket propellant is mass that is stored, usually in some form of propellant tank or casing, prior to being used as the propulsive mass that is ejected from a rocket engine in the form of a fluid jet to produce thrust. For chemical rockets often the propellants are a fuel such as liquid hydrogen or kerosene burned with an oxidizer such as liquid oxygen or nitric acid to produce large volumes of very hot gas. The oxidiser is either kept separate and mixed in the combustion chamber, or comes premixed, as with solid rockets.\n\nSometimes the propellant is not burned but still undergoes a chemical reaction, and can be a 'monopropellant' such as hydrazine, nitrous oxide or hydrogen peroxide that can be catalytically decomposed to hot gas.\n\nAlternatively, an inert propellant can be used that can be externally heated, such as in steam rocket, solar thermal rocket or nuclear thermal rockets.\n\nFor smaller, low performance rockets such as attitude control thrusters where high performance is less necessary, a pressurised fluid is used as propellant that simply escapes the spacecraft through a propelling nozzle.\n\nRockets or other similar reaction devices carrying their own propellant must be used when there is no other substance (land, water, or air) or force (gravity, magnetism, light) that a vehicle may usefully employ for propulsion, such as in space. In these circumstances, it is necessary to carry all the propellant to be used.\n\nHowever, they are also useful in other situations:\n\nSome military weapons use rockets to propel warheads to their targets. A rocket and its payload together are generally referred to as a \"missile\" when the weapon has a guidance system (not all missiles use rocket engines, some use other engines such as jets) or as a \"rocket\" if it is unguided. Anti-tank and anti-aircraft missiles use rocket engines to engage targets at high speed at a range of several miles, while intercontinental ballistic missiles can be used to deliver multiple nuclear warheads from thousands of miles, and anti-ballistic missiles try to stop them. Rockets have also been tested for reconnaissance, such as the Ping-Pong rocket, which was launched to surveil enemy targets, however, recon rockets have never come into wide use in the military.\n\nSounding rockets are commonly used to carry instruments that take readings from to above the surface of the Earth.\n\nRocket engines are also used to propel rocket sleds along a rail at extremely high speed. The world record for this is Mach 8.5.\n\nLarger rockets are normally launched from a launch pad that provides stable support until a few seconds after ignition. Due to their high exhaust velocity——rockets are particularly useful when very high speeds are required, such as orbital speed at approximately . Spacecraft delivered into orbital trajectories become artificial satellites, which are used for many commercial purposes. Indeed, rockets remain the only way to launch spacecraft into orbit and beyond. They are also used to rapidly accelerate spacecraft when they change orbits or de-orbit for landing. Also, a rocket may be used to soften a hard parachute landing immediately before touchdown (see retrorocket).\n\nRockets were used to propel a line to a stricken ship so that a Breeches buoy can be used to rescue those on board. Rockets are also used to launch emergency flares.\n\nSome crewed rockets, notably the Saturn V and Soyuz have launch escape systems. This is a small, usually solid rocket that is capable of pulling the crewed capsule away from the main vehicle towards safety at a moments notice. These types of systems have been operated several times, both in testing and in flight, and operated correctly each time.\n\nThis was the case when the Safety Assurance System (Soviet nomenclature) successfully pulled away the L3 capsule during three of the four failed launches of the Soviet moon rocket, N1 vehicles 3L, 5L and 7L. In all three cases the capsule, albeit unmanned, was saved from destruction. It should be noted that only the three aforementioned N1 rockets had functional Safety Assurance Systems. The outstanding vehicle, 6L, had dummy upper stages and therefore no escape system giving the N1 booster a 100% success rate for egress from a failed launch.\n\nA successful escape of a manned capsule occurred when Soyuz T-10, on a mission to the Salyut 7 space station, exploded on the pad.\n\nSolid rocket propelled ejection seats are used in many military aircraft to propel crew away to safety from a vehicle when flight control is lost.\n\nHobbyists build and fly a wide variety of model rockets. Many companies produce model rocket kits and parts but due to their inherent simplicity some hobbyists have been known to make rockets out of almost anything. Rockets are also used in some types of consumer and professional fireworks. A Water Powered Rocket is a type of model rocket using water as its reaction mass. The pressure vessel (the engine of the rocket) is usually a used plastic soft drink bottle. The water is forced out by a pressurized gas, typically compressed air. It is an example of Newton's third law of motion.\n\nThe scale of amateur rocketry can range from a small rocket launched in one's own backyard to a rocket that reached space. Amateur rocketry is split into three categories: low power, mid power, and high power.\n\nAustralia, Austria, Canada, Germany, New Zealand, Switzerland, the United Kingdom, and the United States have high power rocket associations which provide certifications to its members to fly different rocket motor sizes. While joining these organizations is not a requirement, they often provide insurance and flight waivers for their members.\n\nHydrogen peroxide rockets are used to power jet packs, and have been used to power cars and a rocket car holds the all time (albeit unofficial) drag racing record.\n\nCorpulent Stump is the most powerful non commercial rocket ever launched on an Aerotech engine in the United Kingdom.\n\nRocket exhaust generates a significant amount of acoustic energy. As the supersonic exhaust collides with the ambient air, shock waves are formed. The sound intensity from these shock waves depends on the size of the rocket as well as the exhaust velocity. The sound intensity of large, high performance rockets could potentially kill at close range.\n\nThe Space Shuttle generates 180 dB of noise around its base. To combat this, NASA developed a sound suppression system which can flow water at rates up to 900,000 gallons per minute (57 m/s) onto the launch pad. The water reduces the noise level from 180 dB down to 142 dB (the design requirement is 145 dB). Without the sound suppression system, acoustic waves reflect off of the launch pad towards the rocket, vibrating the sensitive payload and crew. These acoustic waves can be so severe that they can destroy the rocket.\n\nNoise is generally most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the jet, as well as reflecting off the ground. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the jet and by deflecting the jet at an angle.\n\nFor crewed rockets various methods are used to reduce the sound intensity for the passengers, and typically the placement of the astronauts far away from the rocket engines helps significantly. For the passengers and crew, when a vehicle goes supersonic the sound cuts off as the sound waves are no longer able to keep up with the vehicle.\n\nThe effect of the combustion of propellant in the rocket engine is to increase the velocity of the resulting gases to very high speeds, hence producing a thrust. Initially, the gases of combustion are sent in every direction, but only those that produce a net thrust have any effect. The ideal direction of motion of the exhaust is in the direction so as to cause thrust. At the top end of the combustion chamber the hot, energetic gas fluid cannot move forward, and so, it pushes upward against the top of the rocket engine's combustion chamber. As the combustion gases approach the exit of the combustion chamber, they increase in speed. The effect of the convergent part of the rocket engine nozzle on the high pressure fluid of combustion gases, is to cause the gases to accelerate to high speed. The higher the speed of the gases, the lower the pressure of the gas (Bernoulli's principle or conservation of energy) acting on that part of the combustion chamber. In a properly designed engine, the flow will reach Mach 1 at the throat of the nozzle. At which point the speed of the flow increases. Beyond the throat of the nozzle, a bell shaped expansion part of the engine allows the gases that are expanding to push against that part of the rocket engine. Thus, the bell part of the nozzle gives additional thrust. Simply expressed, for every action there is an equal and opposite reaction, according to Newton's third law with the result that the exiting gases produce the reaction of a force on the rocket causing it to accelerate the rocket.\nIn a closed chamber, the pressures are equal in each direction and no acceleration occurs. If an opening is provided in the bottom of the chamber then the pressure is no longer acting on the missing section. This opening permits the exhaust to escape. The remaining pressures give a resultant thrust on the side opposite the opening, and these pressures are what push the rocket along.\n\nThe shape of the nozzle is important. Consider a balloon propelled by air coming out of a tapering nozzle. In such a case the combination of air pressure and viscous friction is such that the nozzle does not push the balloon but is \"pulled\" by it. Using a convergent/divergent nozzle gives more force since the exhaust also presses on it as it expands outwards, roughly doubling the total force. If propellant gas is continuously added to the chamber then these pressures can be maintained for as long as propellant remains. Note that in the case of liquid propellant engines, the pumps moving the propellant into the combustion chamber must maintain a pressure larger than the combustion chamber – typically on the order of 100 atmospheres.\n\nAs a side effect, these pressures on the rocket also act on the exhaust in the opposite direction and accelerate this exhaust to very high speeds (according to Newton's Third Law). From the principle of conservation of momentum the speed of the exhaust of a rocket determines how much momentum increase is created for a given amount of propellant. This is called the rocket's \"specific impulse\". Because a rocket, propellant and exhaust in flight, without any external perturbations, may be considered as a closed system, the total momentum is always constant. Therefore, the faster the net speed of the exhaust in one direction, the greater the speed of the rocket can achieve in the opposite direction. This is especially true since the rocket body's mass is typically far lower than the final total exhaust mass.\n\nThe general study of the forces on a rocket is part of the field of ballistics. Spacecraft are further studied in the subfield of astrodynamics.\n\nFlying rockets are primarily affected by the following:\n\nRockets that must travel through the air are usually tall and thin as this shape gives a high ballistic coefficient and minimizes drag losses.\n\nIn addition, the inertia and centrifugal pseudo-force can be significant due to the path of the rocket around the center of a celestial body; when high enough speeds in the right direction and altitude are achieved a stable orbit or escape velocity is obtained.\n\nThese forces, with a stabilizing tail (the \"empennage\") present will, unless deliberate control efforts are made, naturally cause the vehicle to follow a roughly parabolic trajectory termed a gravity turn, and this trajectory is often used at least during the initial part of a launch. (This is true even if the rocket engine is mounted at the nose.) Vehicles can thus maintain low or even zero angle of attack, which minimizes transverse stress on the launch vehicle, permitting a weaker, and hence lighter, launch vehicle.\n\nDrag is a force opposite to the direction of the rocket's motion. This decreases acceleration of the vehicle and produces structural loads. Deceleration force for fast-moving rockets are calculated using the drag equation.\n\nDrag can be minimised by an aerodynamic nose cone and by using a shape with a high ballistic coefficient (the \"classic\" rocket shape—long and thin), and by keeping the rocket's angle of attack as low as possible.\n\nDuring a rocket launch, as the vehicle speed increases, and the atmosphere thins, there is a point of maximum aerodynamic drag called Max Q. This determines the minimum aerodynamic strength of the vehicle, as the rocket must avoid buckling under these forces.\n\nA typical rocket engine can handle a significant fraction of its own mass in propellant each second, with the propellant leaving the nozzle at several kilometres per second. This means that the thrust-to-weight ratio of a rocket engine, and often the entire vehicle can be very high, in extreme cases over 100. This compares with other jet propulsion engines that can exceed 5 for some of the better engines.\n\nIt can be shown that the net thrust of a rocket is:\n\nwhere:\n\nThe effective exhaust velocity formula_4 is more or less the speed the exhaust leaves the vehicle, and in the vacuum of space, the effective exhaust velocity is often equal to the actual average exhaust speed along the thrust axis. However, the effective exhaust velocity allows for various losses, and notably, is reduced when operated within an atmosphere.\n\nThe rate of propellant flow through a rocket engine is often deliberately varied over a flight, to provide a way to control the thrust and thus the airspeed of the vehicle. This, for example, allows minimization of aerodynamic losses and can limit the increase of \"g\"-forces due to the reduction in propellant load.\n\nImpulse is defined as a force acting on an object over time, which in the absence of opposing forces (gravity and aerodynamic drag), changes the momentum (integral of mass and velocity) of the object. As such, it is the best performance class (payload mass and terminal velocity capability) indicator of a rocket, rather than takeoff thrust, mass, or \"power\". The total impulse of a rocket (stage) burning its propellant is:\n\nWhen there is fixed thrust, this is simply:\n\nThe total impulse of a multi-stage rocket is the sum of the impulses of the individual stages.\nAs can be seen from the thrust equation, the effective speed of the exhaust controls the amount of thrust produced from a particular quantity of fuel burnt per second.\n\nAn equivalent measure, the net impulse per weight unit of propellant expelled, is called specific Impulse, formula_7, and this is one of the most important figures that describes a rocket's performance. It is defined such that it is related to the effective exhaust velocity by:\n\nwhere:\n\nThus, the greater the specific impulse, the greater the net thrust and performance of the engine. formula_7 is determined by measurement while testing the engine. In practice the effective exhaust velocities of rockets varies but can be extremely high, ~4500 m/s, about 15 times the sea level speed of sound in air.\n\nThe delta-v capacity of a rocket is the theoretical total change in velocity that a rocket can achieve without any external interference (without air drag or gravity or other forces).\n\nWhen formula_12 is constant, the delta-v that a rocket vehicle can provide can be calculated from the Tsiolkovsky rocket equation:\n\nwhere:\n\nWhen launched from the Earth practical delta-vs for a single rockets carrying payloads can be a few km/s. Some theoretical designs have rockets with delta-vs over 9 km/s.\n\nThe required delta-v can also be calculated for a particular manoeuvre; for example the delta-v to launch from the surface of the Earth to Low earth orbit is about 9.7 km/s, which leaves the vehicle with a sideways speed of about 7.8 km/s at an altitude of around 200 km. In this manoeuvre about 1.9 km/s is lost in air drag, gravity drag and gaining altitude.\n\nThe ratio formula_18 is sometimes called the \"mass ratio\".\n\nAlmost all of a launch vehicle's mass consists of propellant. Mass ratio is, for any 'burn', the ratio between the rocket's initial mass and its final mass. Everything else being equal, a high mass ratio is desirable for good performance, since it indicates that the rocket is lightweight and hence performs better, for essentially the same reasons that low weight is desirable in sports cars.\n\nRockets as a group have the highest thrust-to-weight ratio of any type of engine; and this helps vehicles achieve high mass ratios, which improves the performance of flights. The higher the ratio, the less engine mass is needed to be carried. This permits the carrying of even more propellant, enormously improving the delta-v. Alternatively, some rockets such as for rescue scenarios or racing carry relatively little propellant and payload and thus need only a lightweight structure and instead achieve high accelerations. For example, the Soyuz escape system can produce 20g.\n\nAchievable mass ratios are highly dependent on many factors such as propellant type, the design of engine the vehicle uses, structural safety margins and construction techniques.\n\nThe highest mass ratios are generally achieved with liquid rockets, and these types are usually used for orbital launch vehicles, a situation which calls for a high delta-v. Liquid propellants generally have densities similar to water (with the notable exceptions of liquid hydrogen and liquid methane), and these types are able to use lightweight, low pressure tanks and typically run high-performance turbopumps to force the propellant into the combustion chamber.\n\nSome notable mass fractions are found in the following table (some aircraft are included for comparison purposes):\n\nThus far, the required velocity (delta-v) to achieve orbit has been unattained by any single rocket because the propellant, tankage, structure, guidance, valves and engines and so on, take a particular minimum percentage of take-off mass that is too great for the propellant it carries to achieve that delta-v carrying reasonable payloads. Since Single-stage-to-orbit has so far not been achievable, orbital rockets always have more than one stage.\n\nFor example, the first stage of the Saturn V, carrying the weight of the upper stages, was able to achieve a mass ratio of about 10, and achieved a specific impulse of 263 seconds. This gives a delta-v of around 5.9 km/s whereas around 9.4 km/s delta-v is needed to achieve orbit with all losses allowed for.\n\nThis problem is frequently solved by staging—the rocket sheds excess weight (usually empty tankage and associated engines) during launch. Staging is either \"serial\" where the rockets light after the previous stage has fallen away, or \"parallel\", where rockets are burning together and then detach when they burn out.\n\nThe maximum speeds that can be achieved with staging is theoretically limited only by the speed of light. However the payload that can be carried goes down geometrically with each extra stage needed, while the additional delta-v for each stage is simply additive.\n\nFrom Newton's second law, the acceleration, formula_19, of a vehicle is simply:\n\nWhere m is the instantaneous mass of the vehicle and formula_21 is the net force acting on the rocket (mostly thrust but air drag and other forces can play a part.)\n\nAs the remaining propellant decreases, rocket vehicles become lighter and their acceleration tends to increase until the propellant is exhausted. This means that much of the speed change occurs towards the end of the burn when the vehicle is much lighter. However, the thrust can be throttled to offset or vary this if needed. Discontinuities in acceleration also occur when stages burn out, often starting at a lower acceleration with each new stage firing.\n\nPeak accelerations can be increased by designing the vehicle with a reduced mass, usually achieved by a reduction in the fuel load and tankage and associated structures, but obviously this reduces range, delta-v and burn time. Still, for some applications that rockets are used for, a high peak acceleration applied for just a short time is highly desirable.\n\nThe minimal mass of vehicle consists of a rocket engine with minimal fuel and structure to carry it. In that case the thrust-to-weight ratio of the rocket engine limits the maximum acceleration that can be designed. It turns out that rocket engines generally have truly excellent thrust to weight ratios (137 for the NK-33 engine, some solid rockets are over 1000), and nearly all really high-g vehicles employ or have employed rockets.\n\nThe high accelerations that rockets naturally possess means that rocket vehicles are often capable of vertical takeoff, and in some cases, with suitable guidance and control of the engines, also vertical landing. For these operations to be done it is necessary for a vehicle's engines to provide more than the local gravitational acceleration.\n\nRocket launch vehicles take-off with a great deal of flames, noise and drama, and it might seem obvious that they are grievously inefficient. However, while they are far from perfect, their energy efficiency is not as bad as might be supposed.\n\nThe energy density of a typical rocket propellant is often around one-third that of conventional hydrocarbon fuels; the bulk of the mass is (often relatively inexpensive) oxidizer. Nevertheless, at take-off the rocket has a great deal of energy in the fuel and oxidizer stored within the vehicle. It is of course desirable that as much of the energy of the propellant end up as kinetic or potential energy of the body of the rocket as possible.\n\nEnergy from the fuel is lost in air drag and gravity drag and is used for the rocket to gain altitude and speed. However, much of the lost energy ends up in the exhaust.\n\nIn a chemical propulsion device, the engine efficiency is simply the ratio of the kinetic power of the exhaust gases and the power available from the chemical reaction:\n\n100% efficiency within the engine (engine efficiency formula_23) would mean that all the heat energy of the combustion products is converted into kinetic energy of the jet. This is not possible, but the near-adiabatic high expansion ratio nozzles that can be used with rockets come surprisingly close: when the nozzle expands the gas, the gas is cooled and accelerated, and an energy efficiency of up to 70% can be achieved. Most of the rest is heat energy in the exhaust that is not recovered. The high efficiency is a consequence of the fact that rocket combustion can be performed at very high temperatures and the gas is finally released at much lower temperatures, and so giving good Carnot efficiency.\n\nHowever, engine efficiency is not the whole story. In common with the other jet-based engines, but particularly in rockets due to their high and typically fixed exhaust speeds, rocket vehicles are extremely inefficient at low speeds irrespective of the engine efficiency. The problem is that at low speeds, the exhaust carries away a huge amount of kinetic energy rearward. This phenomenon is termed propulsive efficiency (formula_24).\n\nHowever, as speeds rise, the resultant exhaust speed goes down, and the overall vehicle energetic efficiency rises, reaching a peak of around 100% of the engine efficiency when the vehicle is travelling exactly at the same speed that the exhaust is emitted. In this case the exhaust would ideally stop dead in space behind the moving vehicle, taking away zero energy, and from conservation of energy, all the energy would end up in the vehicle. The efficiency then drops off again at even higher speeds as the exhaust ends up traveling forwards – trailing behind the vehicle.\n\nFrom these principles it can be shown that the propulsive efficiency formula_24 for a rocket moving at speed formula_26 with an exhaust velocity formula_27 is:\n\nAnd the overall (instantaneous) energy efficiency formula_29 is:\n\nFor example, from the equation, with an formula_31 of 0.7, a rocket flying at Mach 0.85 (which most aircraft cruise at) with an exhaust velocity of Mach 10, would have a predicted overall energy efficiency of 5.9%, whereas a conventional, modern, air-breathing jet engine achieves closer to 35% efficiency. Thus a rocket would need about 6x more energy; and allowing for the specific energy of rocket propellant being around one third that of conventional air fuel, roughly 18x more mass of propellant would need to be carried for the same journey. This is why rockets are rarely if ever used for general aviation.\n\nSince the energy ultimately comes from fuel, these considerations mean that rockets are mainly useful when a very high speed is required, such as ICBMs or orbital launch. For example, NASA's space shuttle fires its engines for around 8.5 minutes, consuming 1,000 tonnes of solid propellant (containing 16% aluminium) and an additional 2,000,000 litres of liquid propellant (106,261 kg of liquid hydrogen fuel) to lift the 100,000 kg vehicle (including the 25,000 kg payload) to an altitude of 111 km and an orbital velocity of 30,000 km/h. At this altitude and velocity, the vehicle has a kinetic energy of about 3 TJ and a potential energy of roughly 200 GJ. Given the initial energy of 20 TJ, the Space Shuttle is about 16% energy efficient at launching the orbiter.\n\nThus jet engines, with a better match between speed and jet exhaust speed (such as turbofans—in spite of their worse formula_31)—dominate for subsonic and supersonic atmospheric use, while rockets work best at hypersonic speeds. On the other hand, rockets serve in many short-range \"relatively\" low speed military applications where their low-speed inefficiency is outweighed by their extremely high thrust and hence high accelerations.\n\nOne subtle feature of rockets relates to energy. A rocket stage, while carrying a given load, is capable of giving a particular delta-v. This delta-v means that the speed increases (or decreases) by a particular amount, independent of the initial speed. However, because kinetic energy is a square law on speed, this means that the faster the rocket is travelling before the burn the more orbital energy it gains or loses.\n\nThis fact is used in interplanetary travel. It means that the amount of delta-v to reach other planets, over and above that to reach escape velocity can be much less if the delta-v is applied when the rocket is travelling at high speeds, close to the Earth or other planetary surface; whereas waiting until the rocket has slowed at altitude multiplies up the effort required to achieve the desired trajectory.\n\nThe reliability of rockets, as for all physical systems, is dependent on the quality of engineering design and construction.\n\nBecause of the enormous chemical energy in rocket propellants (greater energy by weight than explosives, but lower than gasoline), consequences of accidents can be severe. Most space missions have some problems. In 1986, following the Space Shuttle Challenger disaster, American physicist Richard Feynman, having served on the Rogers Commission estimated that the chance of an unsafe condition for a launch of the Shuttle was very roughly 1%; more recently the historical per person-flight risk in orbital spaceflight has been calculated to be around 2% or 4%.\n\nThe costs of rockets can be roughly divided into propellant costs, the costs of obtaining and/or producing the 'dry mass' of the rocket, and the costs of any required support equipment and facilities.\n\nMost of the takeoff mass of a rocket is normally propellant. However propellant is seldom more than a few times more expensive than gasoline per kilogram (as of 2009 gasoline was about or less), and although substantial amounts are needed, for all but the very cheapest rockets, it turns out that the propellant costs are usually comparatively small, although not completely negligible. With liquid oxygen costing and liquid hydrogen , the Space Shuttle in 2009 had a liquid propellant expense of approximately $1.4 million for each launch that cost $450 million from other expenses (with 40% of the mass of propellants used by it being liquids in the external fuel tank, 60% solids in the SRBs).\n\nEven though a rocket's non-propellant, dry mass is often only between 5–20% of total mass, nevertheless this cost dominates. For hardware with the performance used in orbital launch vehicles, expenses of $2000–$10,000+ per kilogram of dry weight are common, primarily from engineering, fabrication, and testing; raw materials amount to typically around 2% of total expense. For most rockets except reusable ones (shuttle engines) the engines need not function more than a few minutes, which simplifies design.\n\nExtreme performance requirements for rockets reaching orbit correlate with high cost, including intensive quality control to ensure reliability despite the limited safety factors allowable for weight reasons. Components produced in small numbers if not individually machined can prevent amortization of R&D and facility costs over mass production to the degree seen in more pedestrian manufacturing. Amongst liquid-fueled rockets, complexity can be influenced by how much hardware must be lightweight, like pressure-fed engines can have two orders of magnitude lesser part count than pump-fed engines but lead to more weight by needing greater tank pressure, most often used in just small maneuvering thrusters as a consequence.\n\nTo change the preceding factors for orbital launch vehicles, proposed methods have included mass-producing simple rockets in large quantities or on large scale, or developing reusable rockets meant to fly very frequently to amortize their up-front expense over many payloads, or reducing rocket performance requirements by constructing a non-rocket spacelaunch system for part of the velocity to orbit (or all of it but with most methods involving some rocket use).\n\nThe costs of support equipment, range costs and launch pads generally scale up with the size of the rocket, but vary less with launch rate, and so may be considered to be approximately a fixed cost.\n\nRockets in applications other than launch to orbit (such as military rockets and rocket-assisted take off), commonly not needing comparable performance and sometimes mass-produced, are often relatively inexpensive.\n\nSince about 2010 there has been more competition in the commercial space launch market.\n\nLists\n\nGeneral Rocketry\n\nPropulsion and Propellant\n\nRecreational Rockets\n\nRecreational Pyrotechnic Rocketry\n\nWeaponry\n\nRockets for Research\n\nMisc\n\n<br>\n\nGoverning agencies\n\nInformation sites\n"}
{"id": "19856155", "url": "https://en.wikipedia.org/wiki?curid=19856155", "title": "SPARKLE Computer", "text": "SPARKLE Computer\n\nSparkle Computer Co., Ltd. (stylised as SPARKLE),is a Taiwanese electronics firm established in 1982, based in Taipei. The company currently specialises in manufacturing video cards using Nvidia graphics processing units, along with peripherals (fans and heatsinks) for graphics controllers.\n\nSparkle is one of the few manufacturers of modern discrete video cards that still use the PCI bus, producing PCI versions of GeForce 8 Series and GeForce 9 Series based discrete graphics controllers, and more recently, GeForce 200 series and GeForce 600 series-based GPUs using the aforementioned interface.\n\nSP-PX79GDH (GeForce 7900 GT Series with active cooling - 256MB RAM)\n\nSPARKLE introduced a unique PCI version of the GeForce 8400 series cards in PCI versions.\n\nSPARKLE introduced a unique PCI version of the GeForce 8500 series card in a PCI version.\n\nSPARKLE introduced a unique PCI version of the GeForce 9400 series card in a PCI version.\n\nSPARKLE introduced a unique PCI version of the GeForce 9500 series card in a PCI version.\n\nSPARKLE has also introduced 2 low profile PCI Express cards from the GeForce 9 series:\n\n"}
{"id": "3602041", "url": "https://en.wikipedia.org/wiki?curid=3602041", "title": "Short code", "text": "Short code\n\nShort codes, or short numbers, are short digit sequences, significantly shorter than telephone numbers, that are used to address messages in the Multimedia Messaging System (MMS) and short message service (SMS) systems of mobile network operators. In addition to messaging, they may be used in abbreviated dialing.\n\nShort codes are designed to be easier to read and remember than telephone numbers. Short codes are unique to each operator at the technological level. Even so, providers generally have agreements to avoid overlaps. In some countries, such as the United States, some classes of numbers are inter-operator (U.S. inter-operator numbers are called common short codes).\n\nShort codes are widely used for value-added services such as charity donations, mobile services, ordering ringtones, and television program voting. Messages sent to a short code can be billed at a higher rate than a standard SMS and may even subscribe a customer to a recurring monthly service that will be added to the customer's mobile phone bill until the user texts, for example, the word \"STOP\" to terminate the service.\n\nShort codes are often associated with automated services. An automated program can handle the response and typically requires the sender to start the message with a command word or prefix. The service then responds to the command appropriately.\n\nIn ads or in other printed material where a provider has to provide both a prefix and the short code number, the advertisement will typically follow this format:\n\n\n\n\n\n\n\n\n\n\n\nEthiopia\n\nCodes are four digits in length and start with 8, like 8xxx. Although the telecom sector in Ethiopia is controlled by the government, short code services are outsourced to the private sector. The short codes are used mostly for fundraising, lottery and polling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11051720", "url": "https://en.wikipedia.org/wiki?curid=11051720", "title": "Solar-powered desalination unit", "text": "Solar-powered desalination unit\n\nA solar-powered desalination unit produces potable water from saline water through direct or indirect methods of desalination powered by sunlight. Countries such as Australia, Italy and Egypt have adopted this system as an alternative source of water for the population.\n\nDirect solar desalination produces distillate directly in the solar collector. An example would be a solar still which traps the Sun's energy to obtain freshwater through the process of evaporation and condensation. Indirect solar desalination incorporates solar energy collection systems with conventional desalination systems such as multi-stage flash distillation, multiple effect evaporation, freeze separation or reverse osmosis to produce freshwater.\n\nThe intermittent nature of sunlight and its variable intensity throughout the day makes predicting its efficiency difficult. However, incorporating a thermal energy storage system solves this problem and ensures constant performance even during non-sunlight hours and cloudy days, improving overall efficiency.\n\nOne type of solar desalination unit is a solar still, it is also similar to a condensation trap. A solar still is a simple way of distilling water, using the heat of the Sun to drive evaporation from humid soil, and ambient air to cool a condenser film. Two basic types of solar stills are box and pit stills. In a solar still, impure water is contained outside the collector, where it is evaporated by sunlight shining through clear plastic. The pure water vapor condenses on the cool inside plastic surface and drips down from the weighted low point, where it is collected and removed. The box type is more sophisticated. The basic principles of solar water distillation are simple, yet effective, as distillation replicates the way nature makes rain. The sun's energy heats water to the point of evaporation. As the water evaporates, water vapor rises, condensing on the glass surface for collection. This process removes impurities, such as salts and heavy metals, and eliminates microbiological organisms. The end result is water cleaner than the purest rainwater.\n\nA solar powered desalination unit designed for remote communities has been tested in the Northern Territory of Australia. The \"reverse-osmosis solar installation\" (ROSI) uses membrane filtration to provide a reliable and clean drinking water stream from sources such as brackish groundwater. Solar energy overcomes the usually high-energy operating costs as well as greenhouse emissions of conventional reverse osmosis systems. ROSI can also remove trace contaminants such as arsenic and uranium that may cause certain health problems, and minerals such as calcium carbonate which causes water hardness.\n\nProject leader Dr Andrea Schaefer from the University of Wollongong's Faculty of Engineering said ROSI has the potential to bring clean water to remote communities throughout Australia that do not have access to a town water supply and/or the electricity grid.\n\nGroundwater (which may contain dissolved salts or other contaminants) or surface water (which may have high turbidity or contain microorganisms) is pumped into a tank with an ultrafiltration membrane, which removes viruses and bacteria. This water is fit for cleaning and bathing. Ten percent of that water undergoes nanofiltration and reverse osmosis in the second stage of purification, which removes salts and trace contaminants, producing drinking water. A photovoltaic solar array tracks the Sun and powers the pumps needed to process the water, using the plentiful sunlight available in remote regions of Australia not served by the power grid.\n\n"}
{"id": "4689328", "url": "https://en.wikipedia.org/wiki?curid=4689328", "title": "TNT equivalent", "text": "TNT equivalent\n\nTNT equivalent is a convention for expressing energy, typically used to describe the energy released in an explosion. The \"ton of TNT\" is a unit of energy defined by that convention to be 4.184 gigajoules, which is the approximate energy released in the detonation of a metric ton (1,000 kilograms or one megagram) of TNT. In other words, for each gram of TNT exploded, 4,184 joules of energy are released. \n\nThis convention intends to compare the destructiveness of an event with that of traditional explosive materials, of which TNT is a typical example, although other conventional explosives such as dynamite contain more energy.\n\nThe \"kiloton (of TNT)\" is a unit of energy equal to 4.184 terajoules.\n\nThe \"megaton (of TNT)\" is a unit of energy equal to 4.184 petajoules.\n\nThe kiloton and megaton of TNT have traditionally been used to describe the energy output, and hence the destructive power, of a nuclear weapon. The TNT equivalent appears in various nuclear weapon control treaties, and has been used to characterize the energy released in such other highly destructive events as an asteroid impact.\n\nA gram of TNT releases 2673–6702 J (joules) upon explosion. The energy liberated by one gram of TNT was arbitrarily defined as a matter of convention to be 4184 J, which is exactly one kilocalorie.\n\nAn explosive's energy is normally expressed as the thermodynamic work produced by its detonation, which for TNT has been accurately measured as 4686 J/g from a large sample of air blast experiments, and theoretically calculated to be 4853 J/g.\n\nThe measured, pure heat output of a gram of TNT is only 2724 J, but this is not the important value for explosive blast effect calculations.\n\nAlternative TNT equivalency can be calculated as a function of when in the detonation the value is measured and which property is being compared.\n\nA kiloton of TNT can be visualized as a cube of TNT on a side.\n\n1 ton TNT equivalent is approximately:\n\nThe relative effectiveness factor (RE factor) relates an explosive's demolition power to that of TNT, in units of the TNT equivalent/kg (TNTe/kg). The RE factor is the relative mass of TNT to which an explosive is equivalent: The greater the RE, the more powerful the explosive.\n\nThis enables engineers to determine the proper masses of different explosives when applying blasting formulas developed specifically for TNT. For example, if a timber-cutting formula calls for a charge of 1 kg of TNT, then based on octanitrocubane's RE factor of 2.38, it would take only 1.0/2.38 (or 0.42) kg of it to do the same job. Using PETN, engineers would need 1.0/1.66 (or 0.60) kg to obtain the same effects as 1 kg of TNT. With ANFO or ammonium nitrate, they would require 1.0/0.74 (or 1.35) kg or 1.0/0.42 (or 2.38) kg, respectively.\n\n<nowiki>*</nowiki>: TBX (thermobaric explosives) or EBX (enhanced blast explosives), in a small, confined space, may have over twice the power of destruction. The total power of aluminized mixtures strictly depends on the condition of explosions.\n\n\n"}
{"id": "326225", "url": "https://en.wikipedia.org/wiki?curid=326225", "title": "Technology during World War II", "text": "Technology during World War II\n\nTechnology played a significant role in World War II. Some of the technologies used during the war were developed during the interwar years of the 1920s and 1930s, much was developed in response to needs and lessons learned during the war, while others were beginning to be developed as the war ended. Many wars had major effects on the technologies that we use in our daily lives. However, compared to previous wars, World War II had the greatest effect on the technology and devices that are used today. Technology also played a greater role in the conduct of World War II than in any other war in history, and had a critical role in its final outcome.\n\nMany types of technology were customized for military use, and major developments occurred across several fields including:\n\nWorld War II was the first war where military operations widely targeted the research efforts of the enemy. This included the exfiltration of Niels Bohr from German-occupied Denmark to Britain in 1943; the sabotage of Norwegian heavy water production; and the bombing of Peenemunde.\n\nMilitary operations were also conducted to obtain intelligence on the enemy's technology; for example, the Bruneval Raid for German radar and Operation Most III for the German V-2.\n\nIn August, 1919 the British Ten Year Rule declared the government should not expect another war within ten years. Consequently, they conducted very little military R & D. In contrast, Germany and the Soviet Union were dissatisfied powers who, for different reasons, cooperated with each other on military R & D. The Soviets offered Weimar Germany facilities deep inside the USSR for building and testing arms and for military training, well away from Treaty inspectors' eyes. In return, they asked for access to German technical developments, and for assistance in creating a Red Army General Staff.\n\nThe great artillery manufacturer Krupp was soon active in the south of the USSR, near Rostov-on-Don. In 1925, a flying school was established at Vivupal, near Lipetsk, to train the first pilots for the future Luftwaffe. Since 1926, the Reichswehr had been able to use a tank school at Kazan (codenamed Kama) and a chemical weapons facility in Samara Oblast (codenamed Tomka). In turn, the Red Army gained access to these training facilities, as well as military technology and theory from Weimar Germany.\n\nIn the late 1920s, Germany helped Soviet industry begin to modernize, and to assist in the establishment of tank production facilities at the Leningrad Bolshevik Factory and the Kharkov Locomotive Factory. This cooperation would break down when Hitler rose to power in 1933. The failure of the World Disarmament Conference marked the beginnings of the arms race leading to war.\n\nIn France the lesson of World War I was translated into the Maginot Line which was supposed to hold a line at the border with Germany. The Maginot Line did achieve its political objective of ensuring that any German invasion had to go through Belgium ensuring that France would have Britain as a military ally. France and Russia had more, and much better, tanks than Germany as of the outbreak of their hostilities in 1940. As in World War I, the French generals expected that armour would mostly serve to help infantry break the static trench lines and storm machine gun nests. They thus spread the armour among their infantry divisions, ignoring the new German doctrine of blitzkrieg based on the fast movement using concentrated armour attacks, against which there was no effective defense but mobile anti-tank guns - infantry Antitank rifles not being effective against medium and heavy tanks.\n\nAir power was a major concern of Germany and Britain between the wars. Trade in aircraft engines continued, with Britain selling hundreds of its best to German firms - which used them in a first generation of aircraft, and then improved on them much for use in German aircraft. These new inventions lead the way to major success for the Germans in World War II.\nGermany had always been and has continued to be in the forefront of internal combustion engine development. Göttingen was the world center of aerodynamics and fluid dynamics in general, at least up to the time when the highly dogmatic Nazi party came to power. This contributed to the German development of jet aircraft and of submarines with improved under-water performance.\n\nInduced nuclear fission was discovered in Germany in 1939 by Otto Hahn (and expatriate Jews in Sweden), but many of the scientists needed to develop nuclear power had already been lost, due to anti-Jewish and anti-intellectual policies.\n\nScientists have been at the heart of warfare and their contributions have often been decisive. As Ian Jacob, the wartime military secretary of Winston Churchill, famously remarked on the influx of refugee scientists (including 19 Nobel laureates), \"the Allies won the [Second World] War because our German scientists were better than their German scientists”.\n\nThe Allies of World War II cooperated extensively in the development and manufacture of new and existing technologies to support military operations and intelligence gathering during the Second World War. There are various ways in which the allies cooperated, including the American Lend-Lease scheme and hybrid weapons such as the Sherman Firefly as well as the British Tube Alloys nuclear weapons research project which was absorbed into the American-led Manhattan Project. Several technologies invented in Britain proved critical to the military and were widely manufactured by the Allies during the Second World War.\n\nThe origin of the cooperation stemmed from a 1940 visit by the Aeronautical Research Committee chairman Henry Tizard that arranged to transfer U.K. military technology to the U.S. in case of the successful invasion of the U.K. that Hitler was planning as Operation Sea Lion. Tizard led a British technical mission, known as the Tizard Mission, containing details and examples of British technological developments in fields such as radar, jet propulsion and also the early British research into the atomic bomb. One of the devices brought to the U.S. by the Mission, the resonant cavity magnetron, was later described as \"the most valuable cargo ever brought to our shores\".\n\nMilitary weapons technology experienced rapid advances during World War II, and over six years there was a disorientating rate of change in combat in everything from aircraft to small arms. Indeed, the war began with most armies utilizing technology that had changed little from World War I, and in some cases, had remained unchanged since the 19th century. For instance cavalry, trenches, and World War I-era battleships were normal in 1940, however within only six years, armies around the world had developed jet aircraft, ballistic missiles, and even atomic weapons in the case of the United States.\n\nThe best jet fighters at the end of the war easily outflew any of the leading aircraft of 1939, such as the Spitfire Mark I. The early war bombers that caused such carnage would almost all have been shot down in 1945, many by radar-aimed, proximity fuse-detonated anti-aircraft fire, just as the 1941 \"invincible fighter\", the Zero, had by 1944 become the \"turkey\" of the \"Marianas Turkey Shoot\". The best late-war tanks, such as the Soviet JS-3 heavy tank or the German Panther medium tank, handily outclassed the best tanks of 1939 such as Panzer IIIs. In the navy the battleship, long seen as the dominant element of sea power, was displaced by the greater range and striking power of the aircraft carrier. The chaotic importance of amphibious landings stimulated the Western Allies to develop the Higgins boat, a primary troop landing craft; the DUKW, a six-wheel-drive amphibious truck, amphibious tanks to enable beach landing attacks and Landing Ship, Tanks to land tanks on beaches. Increased organization and coordination of amphibious assaults coupled with the resources necessary to sustain them caused the complexity of planning to increase by orders of magnitude, thus requiring formal systematization giving rise to what has become the modern management methodology of project management by which almost all modern engineering, construction and software developments are organized.\n\nIn the Western European Theatre of World War II, air power became crucial throughout the war, both in tactical and strategic operations (respectively, battlefield and long-range). Superior German aircraft, aided by ongoing introduction of design and technology innovations, allowed the German armies to overrun Western Europe with great speed in 1940, largely assisted by lack of Allied aircraft, which in any case lagged in design and technical development during the slump in research investment after the Great Depression.\nSince the end of World War I, the French Air Force had been badly neglected, as military leaders preferred to spend money on ground armies and static fortifications to fight another World War I-style war. As a result, by 1940, the French Air Force had only 1562 planes and was together with 1070 RAF planes facing 5,638 Luftwaffe fighters and fighter-bombers. Most French airfields were located in north-east France, and were quickly overrun in the early stages of the campaign. The Royal Air Force of the United Kingdom possessed some very advanced fighter planes, such as Spitfires and Hurricanes, but these were not useful for attacking ground troops on a battlefield, and the small number of planes dispatched to France with the British Expeditionary Force were destroyed fairly quickly. Subsequently, the Luftwaffe was able to achieve air superiority over France in 1940, giving the German military an immense advantage in terms of reconnaissance and intelligence.\n\nGerman aircraft rapidly achieved air superiority over France in early 1940, allowing the Luftwaffe to begin a campaign of strategic bombing against British cities. Utilizing France's airfields near the English Channel the Germans were able to launch raids on London and other cities during the Blitz, with varying degrees of success.\n\nAfter World War I, the concept of massed aerial bombing—\"The bomber will always get through\"—had become very popular with politicians and military leaders seeking an alternative to the carnage of trench warfare, and as a result, the air forces of Britain, France, and Germany had developed fleets of bomber planes to enable this (France's bomber wing was severely neglected, whilst Germany's bombers were developed in secret as they were explicitly forbidden by the Treaty of Versailles).\n\nThe bombing of Shanghai by the Imperial Japanese Navy on January 28, 1932, and August 1937 and the bombings during the Spanish Civil War (1936–1939), had demonstrated the power of strategic bombing, and so air forces in Europe and the United States came to view bomber aircraft as extremely powerful weapons which, in theory, could bomb an enemy nation into submission on their own. As a result, the fear of bombers triggered major developments in aircraft technology.\n\nNazi Germany had put only one large, long-range strategic bomber (the Heinkel He 177 Greif, with many delays and problems) into production, while the America Bomber concept resulted only in prototypes. The Spanish Civil War had proved that tactical dive-bombing using Stukas was a very efficient way of destroying enemy troops concentrations, and so resources and money had been devoted to the development of smaller bomber craft. As a result, the Luftwaffe was forced to attack London in 1940 with heavily overloaded Heinkel and Dornier medium bombers, and even with the unsuitable Junkers Ju 87. These bombers were painfully slow—Italian engineers had been unable to develop sufficiently large piston aircraft engines (those that were produced tended to explode through extreme overheating), and so the bombers used for the Battle of Britain were woefully undersized. As German bombers had not been designed for long-range strategic missions, they lacked sufficient defenses. The Messerschmitt Bf 109 fighter escorts had not been equipped to carry enough fuel to guard the bombers on both the outbound and return journeys, and the longer-range Bf 110s could be outmanoeuvred by the short-range British fighters. (A bizarre feature of the war was how long it took to conceive of the Drop tank.) The air defense was well organized and equipped with effective radar that survived the bombing. As a result, German bombers were shot down in large numbers, and were unable to inflict enough damage on cities and military-industrial targets to force Britain out of the war in 1940 or to prepare for the planned invasion.\n\nBritish long-range bomber planes such as the Short Stirling had been designed before 1939 for strategic flights and given a large armament, but their technology still suffered from numerous flaws. The smaller and shorter ranged Bristol Blenheim, the RAF's most-used bomber, was defended by only one hydraulically operated machine-gun turret, and whilst this appeared sufficient, it was soon revealed that the turret was a pathetic defence against squadrons of German fighter planes. American bomber planes such as the B-17 Flying Fortress had been built before the war as the only adequate long-range bombers in the world, designed to patrol the long American coastlines. Defended by as many as six machine-gun turrets providing 360° cover, the B-17s were still vulnerable without fighter protection even when used in large formations.\n\nDespite the abilities of Allied bombers, though, Germany was not quickly crippled by Allied air raids. At the start of the war the vast majority of bombs fell miles from their targets, as poor navigation technology ensured that Allied airmen frequently could not find their targets at night. The bombs used by the Allies were very high-tech devices, and mass production meant that the precision bombs were often made sloppily and so failed to explode. German industrial production actually rose continuously from 1940 to 1945, despite the best efforts of the Allied air forces to cripple industry.\n\nSignificantly, the bomber offensive kept the revolutionary Type XXI U-Boat from entering service during the war. Moreover, Allied air raids had a serious propaganda impact on the German government, all prompting Germany to begin serious development on air defence technology—in the form of fighter planes.\n\nThe practical jet aircraft age began just before the start of the war with the development of the Heinkel He 178, the first true turbojet. Late in the war the Germans brought in the first operational Jet fighter, the Messerschmitt Me 262. However, despite their seeming technological edge, German jets were often hampered by technical problems, such as short engine lives, with the Me 262 having an estimated operating life of just ten hours before failing. German jets were also overwhelmed by Allied air superiority, frequently being destroyed on or near the airstrip. Other jet aircraft, such as the first and only Allied jet fighter of the war, the British Gloster Meteor, saw combat against German V-1 flying bombs but did not significantly distinguish themselves from top-line, late-war piston-driven aircraft.\n\nAircraft saw rapid and broad development during the war to meet the demands of aerial combat and address lessons learned from combat experience. From the open cockpit airplane to the sleek jet fighter, many different types were employed, often designed for very specific missions. Aircraft were used in anti-submarine warfare against German U-Boats, by the Germans to mine shipping lanes and by the Japanese against previously formidable Royal Navy battleships such as .\n\nDuring the war the Germans produced various Glide bomb weapons, which were the first smart bombs; the V-1 flying bomb, which was the first cruise missile weapon; and the V-2 rocket, the first ballistic missile weapon. The last of these was the first step into the space age as its trajectory took it through the stratosphere, higher and faster than any aircraft. This later led to the development of the Intercontinental ballistic missile (ICBM). Wernher Von Braun led the V-2 development team and later emigrated to the United States where he contributed to the development of the Saturn V rocket, which took men to the moon in 1969.\n\nThe laboratory of Ludwig Prandtl at University of Göttingen was the main center of theoretical and mathematical aerodynamics and fluid dynamics research from soon after 1904 to the end of World War II. Prandtl coined the term boundary layer and founded modern (mathematical) aerodynamics. The laboratory lost its dominance when the researchers were dispersed after the war.\n\nThe Axis countries had serious shortages of petroleum from which to make liquid fuel. The Allies had much more petroleum production. Germany, long before the war, developed a process to make synthetic fuel from coal. Synthesis factories were principal targets of the Oil Campaign of World War II.\n\nThe USA added tetra ethyl lead to its aviation fuel, with which it supplied Britain and other Allies. This octane enhancing additive allowed higher compression ratios, allowing higher efficiency, giving more speed and range to Allied Airplanes, and reducing the cooling load.\n\nThe Treaty of Versailles had imposed severe restrictions upon Germany constructing vehicles for military purposes, and so throughout the 1920s and 1930s, German arms manufacturers and the Wehrmacht had begun secretly developing tanks. As these vehicles were produced in secret, their technical specifications and battlefield potentials were largely unknown to the European Allies until the war actually began.\n\nFrench and British Generals believed that a future war with Germany would be fought under very similar conditions as those of 1914–1918. Both invested in thickly armoured, heavily armed vehicles designed to cross shell-damaged ground and trenches under fire. At the same time the British also developed faster but lightly armoured Cruiser tanks to range behind the enemy lines.\n\nOnly a handful of French tanks had radios, and these often broke as the tank lurched over uneven ground. German tanks were, on the contrary, all equipped with radios, allowing them to communicate with one another throughout battles, whilst French tank commanders could rarely contact other vehicles.\n\nThe Matilda Mk I tanks of the British Army were also designed for infantry support and were protected by thick armour. This was ideal for trench warfare, but made the tanks painfully slow in open battles. Their light cannons and machine-guns were usually unable to inflict serious damage on German vehicles. The exposed caterpillar tracks were easily broken by gunfire, and the Matilda tanks had a tendency to incinerate their crews if hit, as the petrol tanks were located on the top of the hull. By contrast the Infantry tank Matilda II fielded in lesser numbers was largely invulnerable to German gunfire and its gun was able to punch through the German tanks. However French and British tanks were at a disadvantage compared to the air supported German armoured assaults, and a lack of armoured support contributed significantly to the rapid Allied collapse in 1940.\n\nWorld War II marked the first full-scale war where mechanization played a significant role. Most nations did not begin the war equipped for this. Even the vaunted German Panzer forces relied heavily on non-motorised support and flank units in large operations. While Germany recognized and demonstrated the value of concentrated use of mechanized forces, they never had these units in enough quantity to supplant traditional units. However, the British also saw the value in mechanization. For them it was a way to enhance an otherwise limited manpower reserve. America as well sought to create a mechanized army. For the United States, it was not so much a matter of limited troops, but instead a strong industrial base that could afford such equipment on a great scale.\n\nThe most visible vehicles of the war were the tanks, forming the armored spearhead of mechanized warfare. Their impressive firepower and armor made them the premier fighting machine of ground warfare. However, the large number of trucks and lighter vehicles that kept the infantry, artillery, and others moving were massive undertakings also.\n\nNaval warfare changed dramatically during World War II, with the ascent of the aircraft carrier to the premier vessel of the fleet, and the impact of increasingly capable submarines on the course of the war. The development of new ships during the war was somewhat limited due to the protracted time period needed for production, but important developments were often retrofitted to older vessels. Advanced German submarine types came into service too late and after nearly all the experienced crews had been lost.\n\nIn addition to aircraft carriers, its assisting counterpart of destroyers were advanced as well. From the Imperial Japanese Navy, the Fubuki-class destroyer was introduced. The Fubuki class set a new standard not only for Japanese vessels, but for destroyers around the world. At a time when British and American destroyers had changed little from their un-turreted, single-gun mounts and light weaponry, the Japanese destroyers were bigger, more powerfully armed, and faster than any similar class of vessel in the other fleets. The Japanese destroyers of World War II are said to be the world's first modern destroyer.\n\nThe German U-boats were used primarily for stopping/destroying the resources from the United States and Canada coming across the Atlantic. Submarines were critical in the Pacific Ocean as well as in the Atlantic Ocean. Advances in submarine technology included the snorkel. Japanese defenses against Allied submarines were ineffective. Much of the merchant fleet of the Empire of Japan, needed to supply its scattered forces and bring supplies such as petroleum and food back to the Japanese Archipelago, was sunk. Among the warships sunk by submarines was the war's largest aircraft carrier, the \"Shinano\".\n\nThe Kriegsmarine introduced the pocket battleship to get around constraints imposed by the Treaty of Versailles. Innovations included the use of diesel engines, and welded rather than riveted hulls.\n\nThe most important shipboard advances were in the field of anti-submarine warfare. Driven by the desperate necessity of keeping Britain supplied, technologies for the detection and destruction of submarines was advanced at high priority. The use of ASDIC (SONAR) became widespread and so did the installation of shipboard and airborne radar. The Allies Ultra code breaking allowed convoys to be steered around German U-Boat wolfpacks.\n\nThe actual weapons; the guns, mortars, artillery, bombs, and other devices, were as diverse as the participants and objectives. A large array were developed during the war to meet specific needs that arose, but many traced their early development to prior to World War II.\nTorpedoes began to use magnetic detonators; compass-directed, programmed and even acoustic guidance systems; and improved propulsion. Fire-control systems continued to develop for ships' guns and came into use for torpedoes and anti-aircraft fire. Human torpedoes and the Hedgehog were also developed.\n\n\nNew production methods for weapons such as stamping, riveting, and welding came into being to produce the number of arms needed. Design and production methods had advanced enough to manufacture weapons of reasonable reliability such as the PPSh-41, PPS-42, Sten, Beretta Model 38, MP 40, M3 \"Grease Gun\", Gewehr 43, Thompson submachine gun and the M1 Garand rifle. Other Weapons commonly found during World War II include the American, Browning Automatic Rifle (BAR), M1 Carbine Rifle, as well as the Colt M1911 A-1; The Japanese Type 11 the Type 96 machine gun, and the Arisaka bolt-action rifles all were significant weapons used during the war.\n\nWorld War II saw the establishment of the reliable semi-automatic rifle, such as the American M1 Garand and, more importantly, of the first widely used assault rifles, named after the German \"sturmgewehrs\" of the late war. Earlier renditions that hinted at this idea were that of the employment of the Browning Automatic Rifle and 1916 Fedorov Avtomat in a \"walking fire\" tactic in which men would advance on the enemy position showering it with a hail of lead. The Germans first developed the FG 42 for its paratroopers in the assault and later the Sturmgewehr 44 (StG 44), the world's first assault rifle, firing an intermediate cartridge; the FG 42's use of a full-powered rifle cartridge made it difficult to control.\n\nDevelopments in machine gun technology culminated in the Maschinengewehr 42 (MG42) which was of an advanced design unmatched at the time. It spurred post-war development on both sides of the upcoming Cold War and is still used by some armies to this day including the German Bundeswehr's MG 3. The Heckler & Koch G3, and many other Heckler & Koch designs, came from its system of operation. The United States military meshed the operating system of the FG 42 with the belt feed system of the MG42 to create the M60 machine gun used in the Vietnam War.\n\nDespite being overshadowed by self-loading/automatic rifles and sub-machine guns, bolt-action rifles remained the mainstay infantry weapon of many nations during World War II. When the United States entered World War II, there were not enough M1 Garand rifles available to American forces which forced the US to start producing more M1903 rifles in order to act as a \"stop gap\" measure until sufficient quantities of M1 Garands were produced.\n\nDuring the conflict, many new models of bolt-action rifles were produced as a result of lessons learned from the First World War with the designs of a number of bolt-action infantry rifles being modified in order to speed up production as well as to make the rifles more compact and easier to handle. Examples of bolt-action rifles that were used during World War II include the German Mauser Kar98k, the British Lee–Enfield No.4, and the Springfield M1903A3. During the course of World War II, bolt-action rifles and carbines were modified even further to meet new forms of warfare the armies of certain nations faced e.g. urban warfare and jungle warfare. Examples include the Soviet Mosin–Nagant M1944 carbine, which were developed by the Soviets as a result of the Red Army's experiences with urban warfare e.g. the Battle of Stalingrad, and the British Lee–Enfield No.5 carbine, that were developed for British and Commonwealth forces fighting the Japanese in South-East Asia and the Pacific.\n\nWhen World War II ended in 1945, the small arms that were used in the conflict still saw action in the hands of the armed forces of various nations and guerrilla movements during and after the Cold War era. Nations like the Soviet Union and the United States provided many surplus, World War II-era small arms to a number of nations and political movements during the Cold War era as a pretext to providing more modern infantry weapons.\n\nThe massive research and development demands of the war included the Manhattan Project, the effort to quickly develop an atomic bomb, or nuclear fission warhead. It was perhaps the most profound military development of the war, and had a great impact on the scientific community, among other things creating a network of national laboratories in the United States. The British however started their own nuclear weapons program in 1940, being the first country to do so. However, due the potential radioactive fallout, the British considered the idea morally unacceptable and put it on hold. In 1947 the project was restarted and the first successful nuclear weapons test carried out on 3 October 1952 in Operation Hurricane and came info full service by 1955. Britain was also the first to come up with the idea of nuclear energy and hint at a potential for atomic weapons in 1933. It was patented in 1934, (British patent 630,726), which help to lead the way into the further research and later, the successful development of nuclear weapons.\n\nIn 1942, and with the threat of invasion by Germany still apparent, the United Kingdom dispatched around 20 British scientists and technical staff to America, along with their work, which had been carried out under the codename \"Tube Alloys\", to prevent the potential for vital information falling into enemy hands. The scientists formed the British contribution to the Manhattan Project, where their work on uranium enrichment was instrumental in jump-starting the project.\n\nThe invention of the atomic bomb meant that a single aircraft could carry a weapon so powerful it could burn down entire cities, making conventional warfare against a nation with an arsenal of them suicidal. Following the conclusion of the European Theater in May 1945, two atomic bombs were then employed against the Empire of Japan in August, hastening the end of the war, which averted the need for invading mainland Japan.\n\nThe strategic importance of the bomb, and its even more powerful fusion-based successors, did not become fully apparent until the United States lost its monopoly on the weapon in the post-war era. The Soviet Union developed and tested their first fire weapon in 1949, based partially on information obtained from Soviet espionage in the United States. Competition between the two superpowers played a large part in the development of the Cold War. The strategic implications of such a massively destructive weapon still reverberate in the 21st century.\n\nThere was also a German nuclear energy project, including talk of an atomic weapon. This failed for a variety of reasons, most notably German Antisemitism. Half of continental theoretical physicists—including Einstein, Bohr, Enrico Fermi, and Oppenheimer—who did much of their early study and research in Germany, were either Jewish or, in the case of Enrico Fermi, married to a Jew. Erwin Schrödinger had also left Germany for political reasons. When they left Germany, the only leading nuclear physicist left in Germany was Heisenberg, who apparently dragged his feet on the project, or at best lacked the high morale that characterized the Los Alamos work. He made some faulty calculations suggesting that the Germans would need significantly more heavy water than was necessary. Otto Hahn, the physical chemist who had the central part in the original discovery of fission, was another key figure in the project. The project was doomed due to insufficient resources, time, and a lack of Governmental interest.\n\nThe Empire of Japan was also developing an atomic Bomb, however, it floundered due to lack of resources despite gaining interest from the government.\n\nThe collaboration between the British and the Americans led to the 1958 US-UK Mutual Defence Agreement between the two nations, whereby American nuclear weapons technology was adapted for British use.\n\nElectronics rose to prominence quickly in World War II. The British developed and progressed electronic computers which were primarily used for breaking the “Enigma” codes, which were Nazi secret codes. These codes for radio messages were indecipherable to the Allies. However, the meticulous work of code breakers based at Britain’s Bletchley Park cracked the secrets of German wartime communication, and played a crucial role in the final defeat of Germany. Americans also used electronic computers for equations, such as battlefield equations, ballistics, and more. Numerous small digital computers were also used. From calculating tables, to mechanical trajectory calculators, to some of the most advanced electronic computers. Soldiers would usually carry most of the electronic devices in their pockets, but since technology has developed, digital computers started to increase in size, which spacious command and control centres would have. Initial control centers that were embarked on ships and aircraft that established the networked computing, is so essential to our daily lives. While prior to the war few electronic devices were seen as important pieces of equipment, by the middle of the war instruments such as the British invented radar and ASDIC (sonar) had become invaluable. Germany started the war ahead in some aspects of radar, but lost ground to research and development of the cavity magnetron in Britain and to later work at the \"Radiation Laboratory\" of the Massachusetts Institute of Technology. Half of the German theoretical physicists were Jewish and had emigrated or otherwise been lost to Germany long before WW II started.\n\nEquipment designed for communications and the interception of those communications became critical. The Germans widely relied on the Enigma coding machine for encrypting communications. The British developed a new method for decoding Enigma benefiting from information given to Britain by the Polish Cipher Bureau, which had been decoding early versions of Enigma before the war.\n\nRocketry was used greatly in World War II. There were many different inventions and advances in rocketry, such as:\n\nThe V-1, which is also known as the buzz bomb. This automatic aircraft is today known as a “cruise missile”. The V-1 was developed at Peenemünde Army Research Center by the Nazi German \"Luftwaffe\" during the Second World War. During initial development it was known by the codename \"Cherry Stone\". The first of the so-called \"Vergeltungswaffen\" series designed for terror bombing of London, the V-1 was fired from launch facilities along the French (Pas-de-Calais) and Dutch coasts. The first V-1 was launched at London on 13 June 1944), one week after (and prompted by) the successful Allied landings in Europe. At its peak, more than one hundred V-1s a day were fired at south-east England, 9,521 in total, decreasing in number as sites were overrun until October 1944, when the last V-1 site in range of Britain was overrun by Allied forces. After this, the V-1s were directed at the port of Antwerp and other targets in Belgium, with 2,448 V-1s being launched. The attacks stopped when the last launch site was overrun on 29 March 1945.\n\nThe V-2 (German: \"Vergeltungswaffe 2\", \"Retribution Weapon 2\"), technical name Aggregat-4 (\"A-4\"), was the world's first long-range guided ballistic missile. The missile with liquid-propellant rocket engine was developed during the Second World War in Germany as a \"vengeance weapon\", designed to attack Allied cities as retaliation for the Allied bombings against German cities. The V-2 rocket was also the first artificial object to cross the boundary of space.\n\nThese two rocketry advances took the lives of many civilians in London during the years 1944 and 1945.\n\nAfter the war, many of the inventions created during World War II that were invented for the troops were later sold to the civilian population.\n\nDuring 1932, the creator of M&M's, Forrest Mars Sr. moved to England, and began manufacturing Mars Bars for troops in the UK. During the Spanish Civil War, Forrest purportedly encountered troops eating small beads of chocolate that were encased in hard sugar shells. Upon returning to the US, he approached Bernie Murrie, the son of a Hershey's executive, to join him in his business venture as he anticipated a demand for chocolate and sugar during the war. After the design had been patented in 1941, a plant located in Newark, New Jersey, began production of M&Ms that year. The candies were sold exclusively to the military when the US joined the war and were used as an easy way to provide the troops with chocolate on the battlefield without it melting so easily. After the war, in 1946, the candies again became available for the civilian population.\n\n\n"}
{"id": "947383", "url": "https://en.wikipedia.org/wiki?curid=947383", "title": "Tensile structure", "text": "Tensile structure\n\nA tensile structure is a construction of elements carrying only tension and no compression or bending. The term tensile should not be confused with tensegrity, which is a structural form with both tension and compression elements.\nTensile structures are the most common type of thin-shell structures.\n\nMost tensile structures are supported by some form of compression or bending elements, such as masts (as in The O, formerly the Millennium Dome), compression rings or beams.\n\nA tensile membrane structure is most often used as a roof, as they can economically and attractively span large distances. Tensile membrane structures may also be used as complete buildings, with a few common applications being sports facilities, warehousing and storage buildings, and exhibition venues.\n\nThis form of construction has only become more rigorously analyzed and widespread in large structures in the latter part of the twentieth century. Tensile structures have long been used in tents, where the guy ropes and tent poles provide pre-tension to the fabric and allow it to withstand loads.\n\nRussian engineer Vladimir Shukhov was one of the first to develop practical calculations of stresses and deformations of tensile structures, shells and membranes. Shukhov designed eight tensile structures and thin-shell structures exhibition pavilions for the Nizhny Novgorod Fair of 1896, covering the area of 27,000 square meters. A more recent large-scale use of a membrane-covered tensile structure is the Sidney Myer Music Bowl, constructed in 1958.\n\nAntonio Gaudi used the concept in reverse to create a compression-only structure for the Colonia Guell Church. He created a hanging tensile model of the church to calculate the compression forces and to experimentally determine the column and vault geometries.\n\nThe concept was later championed by German architect and engineer Frei Otto, whose first use of the idea was in the construction of the West German pavilion at Expo 67 in Montreal. Otto next used the idea for the roof of the Olympic Stadium for the 1972 Summer Olympics in Munich.\n\nSince the 1960s, tensile structures have been promoted by designers and engineers such as Ove Arup, Buro Happold, Walter Bird of Birdair, Inc., Frei Otto, Mahmoud Bodo Rasch, Eero Saarinen, Horst Berger, Matthew Nowicki, Jörg Schlaich, the duo of Nicholas Goldsmith & Todd Dalland at FTL Design & Engineering Studio and David Geiger.\n\nSteady technological progress has increased the popularity of fabric-roofed structures. The low weight of the materials makes construction easier and cheaper than standard designs, especially when vast open spaces have to be covered.\n\n\n\n\nCommon materials for doubly curved fabric structures are PTFE-coated fiberglass and PVC-coated polyester. These are woven materials with different strengths in different directions. The warp fibers (those fibers which are originally straight—equivalent to the starting fibers on a loom) can carry greater load than the weft or fill fibers, which are woven between the warp fibers.\n\nOther structures make use of ETFE film, either as single layer or in cushion form (which can be inflated, to provide good insulation properties or for aesthetic effect—as on the Allianz Arena in Munich). ETFE cushions can also be etched with patterns in order to let different levels of light through when inflated to different levels. \n\nIn daylight, fabric membrane translucency offers soft diffused naturally lit spaces, while at night, artificial lighting can be used to create an ambient exterior luminescence. They are most often supported by a structural frame as they cannot derive their strength from double curvature.\n\nCables can be of mild steel, high strength steel (drawn carbon steel), stainless steel, polyester or aramid fibres. Structural cables are made of a series of small strands twisted or bound together to form a much larger cable. Steel cables are either spiral strand, where circular rods are twisted together and \"glued\" using a polymer, or locked coil strand, where individual interlocking steel strands form the cable (often with a spiral strand core).\n\nSpiral strand is slightly weaker than locked coil strand. Steel spiral strand cables have a Young's modulus, \"E\" of 150±10 kN/mm² (or 150±10 GPa) and come in sizes from 3 to 90 mm diameter. Spiral strand suffers from construction stretch, where the strands compact when the cable is loaded. This is normally removed by pre-stretching the cable and cycling the load up and down to 45% of the ultimate tensile load.\n\nLocked coil strand typically has a Young's Modulus of 160±10 kN/mm² and comes in sizes from 20 mm to 160 mm diameter.\n\nThe properties of the individuals strands of different materials are shown in the table below, where UTS is ultimate tensile strength, or the breaking load:\n\nAir-supported structures are a form of tensile structures where the fabric envelope is supported by pressurised air only.\n\nThe majority of fabric structures derive their strength from their doubly curved shape. By forcing the fabric to take on double-curvature the fabric gains sufficient stiffness to withstand the loads it is subjected to (for example wind and snow loads). In order to induce an adequately doubly curved form it is most often necessary to pretension or prestress the fabric or its supporting structure.\n\nThe behaviour of structures which depend upon prestress to attain their strength is non-linear, so anything other than a very simple cable has, until the 1990s, been very difficult to design. The most common way to design doubly curved fabric structures was to construct scale models of the final buildings in order to understand their behaviour and to conduct form-finding exercises. Such scale models often employed stocking material or tights, or soap film, as they behave in a very similar way to structural fabrics (they cannot carry shear).\n\nSoap films have uniform stress in every direction and require a closed boundary to form. They naturally form a minimal surface—the form with minimal area and embodying minimal energy. They are however very difficult to measure. For a large film, its weight can seriously affect its form.\n\nFor a membrane with curvature in two directions, the basic equation of equilibrium is:\n\nformula_1\n\nwhere:\n\n\nLines of principal curvature have no twist and intersect other lines of principal curvature at right angles.\n\nA geodesic or geodetic line is usually the shortest line between two points on the surface. These lines are typically used when defining the cutting pattern seam-lines. This is due to their relative straightness after the planar cloths have been generated, resulting in lower cloth wastage and closer alignment with the fabric weave.\n\nIn a pre-stressed but unloaded surface \"w\" = 0, so formula_2.\n\nIn a soap film surface tensions are uniform in both directions, so \"R\" = −\"R\".\n\nIt is now possible to use powerful non-linear numerical analysis programs (or finite element analysis) to formfind and design fabric and cable structures. The programs must allow for large deflections.\n\nThe final shape, or form, of a fabric structure depends upon:\n\nIt is important that the final form will not allow ponding of water, as this can deform the membrane and lead to local failure or progressive failure of the entire structure.\n\nSnow loading can be a serious problem for membrane structure, as the snow often will not flow off the structure as water will. For example, this has in the past caused the (temporary) collapse of the Hubert H. Humphrey Metrodome, an air-inflated structure in Minneapolis, Minnesota. Some structures prone to ponding use heating to melt snow which settles on them.\nThere are many different doubly curved forms, many of which have special mathematical properties. The most basic doubly curved from is the saddle shape, which can be a hyperbolic paraboloid (not all saddle shapes are hyperbolic paraboloids). This is a double ruled surface and is often used in both in lightweight shell structures (see hyperboloid structures). True ruled surfaces are rarely found in tensile structures. Other forms are anticlastic saddles, various radial, conical tent forms and any combination of them.\n\nPretension is tension artificially induced in the structural elements in addition to any self-weight or imposed loads they may carry. It is used to ensure that the normally very flexible structural elements remain stiff under all possible loads.\n\nA day to day example of pretension is a shelving unit supported by wires running from floor to ceiling. The wires hold the shelves in place because they are tensioned – if the wires were slack the system would not work.\n\nPretension can be applied to a membrane by stretching it from its edges or by pretensioning cables which support it and hence changing its shape. The level of pretension applied determines the shape of a membrane structure.\n\nThe alternative approximated approach to the form-finding problem solution is based on the total energy balance of a grid-nodal system. Due to its physical meaning this approach is called the Stretched Grid Method (SGM).\n\nA uniformly loaded cable spanning between two supports forms a curve intermediate between a catenary curve and a parabola. The simplifying assumption can be made that it approximates a circular arc (of radius \"R\").\n\nBy equilibrium:\n\nThe horizontal and vertical reactions :\n\nBy geometry:\n\nThe length of the cable:\n\nThe tension in the cable:\n\nBy substitution:\n\nThe tension is also equal to:\n\nThe extension of the cable upon being loaded is (from Hooke's Law, where the axial stiffness, \"k,\" is equal to formula_9):\n\nwhere \"E\" is the Young's modulus of the cable and \"A\" is its cross-sectional area.\n\nIf an initial pretension, formula_11 is added to the cable, the extension becomes:\n\nCombining the above equations gives:\n\nBy plotting the left hand side of this equation against \"T,\" and plotting the right hand side on the same axes, also against \"T,\" the intersection will give the actual equilibrium tension in the cable for a given loading \"w\" and a given pretension formula_11.\n\nA similar solution to that above can be derived where:\n\nBy equilibrium:\n\nBy geometry:\n\nThis gives the following relationship:\n\nAs before, plotting the left hand side and right hand side of the equation against the tension, \"T,\" will give the equilibrium tension for a given pretension, formula_11 and load, \"W\".\n\nThe fundamental natural frequency, \"f\" of tensioned cables is given by:\n\nwhere: \"T\" = tension in newtons, \"m\" = mass in kilograms and \"L\" = span length.\n\n\nConstruction Specifications Institute (CSI) Division 13 MasterFormat 2004 Edition:\n\n\nCSI MasterFormat 1995 Edition:\n\n\n\n\n"}
{"id": "551208", "url": "https://en.wikipedia.org/wiki?curid=551208", "title": "Thailand Science Park", "text": "Thailand Science Park\n\nThailand Science Park (TSP) is in Tha Khlong, Khlong Luang District, Pathum Thani Province near Rangsit, north of Bangkok. Managed by the National Science and Technology Development Agency (NSTDA), under the Ministry of Science and Technology. Thailand Science Park was set up in 2002. TSP is part of Thailand's efforts to strengthen its capabilities in research and innovation. It is the country's largest science and technology research park. \n\nCorporate tenants conducting research and development in Thailand Science Park receive maximum investment privileges from the Thailand Board of Investment (BOI).\n\nTSP houses NSTDA headquarters and four national research centers:\n\n\nFacilities include a library, an infirmary, a bank, a savings cooperative, a residence hall, a nursery (daycare center), a grocery store, two food courts, and a number of coffee shops and restaurants. In addition to advanced facilities and business space, the TSP offers a full range of value-added services to support technology businesses. \n\nThe park is near the Asian Institute of Technology, Sirindhorn International Institute of Technology, and Thammasat University. Also in the park is the Thailand Technology Information Access Center (now the Science and Technology Knowledge Service Center or STKS), a provider of on-line information services.\n\n\n"}
{"id": "13577532", "url": "https://en.wikipedia.org/wiki?curid=13577532", "title": "Tunnel finisher", "text": "Tunnel finisher\n\nA tunnel finisher is a machine that removes wrinkles from garments and is often used in the textile industry. As with other industrial pressing equipment, this machine is employed to improve the quality and look of a textile product. It has a chamber called a \"tunnel\" and includes a conveyor fed unit through which the garments are steamed and dried. The machine also features hook systems; air curtain entrance to eliminate moisture or condensation; cotton care and roller units; exhaust steam, and a preconditioning module.\n\nMost garments are shipped by sea freight from the country of production. They get very wrinkled because of the box packing being used. In the receiving country, they are unpacked and put on a clothes hanger. Those hangers are sent via automated transport through the tunnel with a speed up to 3,000 garments per hour. These garments are then sent to a room to be steamed and dried. \n\nThe machine processes each garment through several stages. First, the garment passes through a steam chamber to make the fabric moldable. Then wrinkles are removed by a strong hot air flow alongside the garments. Finally, the garment is dried by cooler air before it leaves the tunnel finisher. In the case of garments, smaller areas such as collars require further pressing using other equipment such as steam iron for a better finish.\n\nThe tunnel finisher is also used in laundries and dry cleaners to remove wrinkles from garments after washing or dry cleaning.\n\nTunnel finishers can be grouped into two different classifications, \"wide body\" or \"narrow body.\" \"Wide body\" machines are designed for high production finishing of blended garments wet-to-dry, damp-to-dry and or dry-to-dry. \"Narrow body\" machines are designed for shoulder-to-shoulder processing and are best suited for the dry-to-dry finishing of garments. However; they are capable of damp-to-dry finishing at slower production speeds. These units are ideal for dry cleaners, hotel laundries, institutional laundries and other on-premises laundry applications. The smaller capacity version of the tunnel finisher is called \"cabinet tunnel\" and this typically capable of automated processing of separate batches of 4 or 5 garments at the same time. The production capacity for this smaller equipment is 10 percent of the tunnel finisher.\n"}
{"id": "58641537", "url": "https://en.wikipedia.org/wiki?curid=58641537", "title": "ViaHero", "text": "ViaHero\n\nViaHero is a travel planning platform that allows travelers to curate personalized trips with the help of local experts. Founded in 2015 by Greg Buzulencia and Rachel Hawkes, ViaHero is based in New York City. \n\nViaHero was part of the AlphaLab accelerator in Pittsburgh, and received a funding of $25,000.\n\nViaHero, through their website, asks users for their travel preferences and matches them with a local resident of the area in which the user is planning to travel. The local, also called a Hero, creates a custom guidebook which can be accessed both online and offline through the ViaHero app. The user can also contact the Hero who will provide assistance in travel consulting including transport, accommodation, airport transfers, dinner reservations, bookings with tour guides, etc. for a pre-decided fee. The Heroes are carefully selected and are given training on a standardized trip planning process. ViaHero works on a basic plan and a premium plan model.\n\nGreg Buzulencia was passionate about helping friends in planning trips and discovering less traveled locations. He was an official “TripTik” planner for AAA at the age of 16. Upon continuous requests, he realized that travelers did not have the time or knowledge to plan personalized vacations. Greg then launched ViaHero in 2016 with Cuba as their first destination, to help create authentic international trips with guidance from local residents. Greg, along with Rachel Hawkes, built ViaHero to modernize the current travel agent offerings while including end-to-end trip planning.\n"}
{"id": "16945858", "url": "https://en.wikipedia.org/wiki?curid=16945858", "title": "WindShear", "text": "WindShear\n\nThe Windshear Full Scale Rolling Road Wind Tunnel is an automotive wind tunnel in Concord, North Carolina. \n\nIn January 2008 Wind Shear, a division of US machine tool builder Haas Automation, completed construction on one of the most advanced automotive wind tunnels in the world. The full-scale tunnel is located adjacent to Concord Regional Airport in Concord, North Carolina. The commercial operation was designed for vehicles from race industries: stock car, Formula One, Indy car, drag racing, as well as production car industries.\n\nWind Shear's tunnel is a closed air circuit, temperature-controlled system built around a rolling road. The rolling road, akin to a giant treadmill, is 10 ft wide by 29.5 ft long (3 m x 9 m) and accommodates full-size cars. Air and rolling road speeds are coordinated up to . Air temperature, critical to repeatable data collection, is maintained at a constant , plus or minus one degree. Air is moved through the massive air circuit at the maximum rate of per second by a motor and 29 carbon fiber blades in diameter.\n\nIn the wind tunnel industry, size is everything. The blockage effect is the condition where air flow in the wind tunnel is partially blocked by the vehicle. The blockage becomes more critical as the cross section of the test vehicle increases relative to the size of nozzle and airstream. As the vehicle increases in size relative to the nozzle, test data become less reliable as increased blockage effects the quality of the actual windstream. Windshear's solution was to build a sufficiently large air circuit. Nozzle size is a relatively large 180 square feet (16.7 square meters).\n\n\n\n"}
