{"id": "40663365", "url": "https://en.wikipedia.org/wiki?curid=40663365", "title": "AS-International", "text": "AS-International\n\nAS-International is the nonprofit trade association for AS-Interface (Actuator/Sensor-Interface, short: AS-i) users worldwide.\n\nAS-International is the sole owner of the \"registered trademark\" \"AS-Interface\", and only members may develop and sell products using \"AS-Interface\" technology. The association also controls the \"certification\" process, and grants the right to use the AS-Interface logo on printed and electronic material as well as the products themselves. AS-International represents a worldwide community of approximately 350 members. At this time, in addition to the headquarters in \"Gelnhausen\", Germany, 13 independently structured local organizations on three continents exist.\n\nIn 1990, eleven German (Balluff, Festo, ifm electronic, Leuze electronic, Pepperl+Fuchs, Sick, Siemens, Turck, Visolux (now part of the Pepperl+Fuchs group) and Swiss (Baumer, and Elesta) companies conceived a \"bus system\" for networking \"sensors\" and \"actuators\". These industrial companies, with different strategic and economic interests, and frequently competing, jointly conceived the AS-Interface technology. Funded in part by the \"BMBF\" (formerly BMFT- German Federal Ministry of Research and Technology,) their goal was the development of a simple and low-cost system. In 1991, after the completion of this work, an independent, \"nonprofit\" user organization, the \"Association for Promotion of Bus-compatible Interfaces e.V.\", now doing business as AS-International e.V., was founded. This was necessary to address legislation concerning \"anti-competitive business practices\". The developing companies effectively relinquished their rights in the core technology. Consequently and as stated in its constitution, AS-International is the owner and guardian of the \"trademark\" AS-Interface®, and owns the rights and the \"specification\" to the \"AS-Interface\" technology. Therefore, any company producing technology with \"AS-Interface\" products is required to be a member of AS-International Association. Additionally, the organization represents the technology in international regulatory bodies, organizes and hosts educational seminars, and promotes advancements of the technology. According to AS-International, the organization's mission is \"the promotion of bus-able interfaces for sensors and actuators.\"\n\nIn the fall of 1991, the detailed \"specification\" was approved, allowing AS-International members to start developing \"slave\" components.\n\nBy mid 1992, the \"electro-mechanical\" \"specification\" was released, defining the AS-Interface flat-cable, an essential component to addressing the goal of simple network installations.\n\nMid 1992, the \"master\" \"specification\" was released, resulting in the development of a number of bus masters with and without interfaces to other higher-level ‘’fieldbus’’ solutions.\n\nIn the fall of 1993, the first \"application-specific integrated circuit\" (ASIC) became available, simplifying \"slave\" development and passing performance testing.\n\nIn early 1994, the first fully functional circuit and \"prototypes\" became available.\n\nMid 1994, FZI (Forschungszentrum Informatik) in Karlsruhe – research institute for computer science in Karlsruhe) developed the first systems with \"master\".\n\nIn 1999, AS-Interface is standardized in EN 50295 and IEC 62026-2 (revised in 2012.)\n\nIn 2010, AS-International introduced AS-Interface Power 24V, enabling the use of almost any 24V power supply for AS-Interface.\n\nIn July 2012, the Developer Kit for AS-Interface to create a fully functional as-Interface network was released.\n\nAs a trade association based on common interest, AS-International offers its members: \n\n"}
{"id": "39271639", "url": "https://en.wikipedia.org/wiki?curid=39271639", "title": "A Boy and His Atom", "text": "A Boy and His Atom\n\nA Boy and His Atom is a 2013 stop-motion animated short film released on YouTube by IBM Research. The movie tells the story of a boy and a wayward atom who meet and become friends. It depicts a boy playing with an atom that takes various forms. One minute in length, it was made by moving carbon monoxide molecules with a scanning tunneling microscope, a device that magnifies them 100 million times. These two-atom molecules were moved to create images, which were then saved as individual frames to make the film. The movie has been recognized by the \"Guinness Book of World Records\" as the World's Smallest Stop-Motion Film.\n\nThe scientists at IBM Research – Almaden who made the film are moving atoms to explore the limits of data storage because, as data creation and consumption gets bigger, data storage needs to get smaller, all the way down to the atomic level. Traditional silicon transistor technology has become cheaper, denser and more efficient, but fundamental physical limitations suggest that scaling down is an unsustainable path to solving the growing Big Data dilemma. This team of scientists is particularly interested in starting on the smallest scale, single atoms, and building structures up from there. Using this method, IBM announced it can now store a single bit of information in just 12 atoms (current technology takes roughly one million atoms to store a single bit).\n\nAt the beginning of the film, subtitles introduce the audience to how and why the film was made. When the action starts, we see a boy meeting an atom. Hesitant at first, he dances for it to the film's musical accompaniment, then bounces it around like a ball. The atom drops to the ground and forms a trampoline, which the boy bounces on. The atom reforms, and the boy tosses it into the sky, flying past clouds and the word \"Think\", a longtime internal IBM motto.\n\n\"A Boy And His Atom\" was created by a team of IBM scientists – together with Ogilvy & Mather, IBM's longstanding advertising agency – at the company's Almaden Research Center in San Jose, California. Using a scanning tunneling microscope, Carbon monoxide molecules were manipulated into place on a copper substrate with a copper needle at a distance of 1 nanometer. They remain in place, forming a bond with the substrate because of the extremely low temperature of 5 K (, ) at which the device operates. The oxygen component of each molecule shows up as a dot when photographed by the scanning tunneling microscope, allowing the creation of images composed of many such dots.\n\nThe team created 242 still images with 65 carbon monoxide molecules. The images were combined to make a stop-motion film. Each frame measures 45 by 25 nanometers. It took four researchers two weeks of 18-hour days to produce the film.\n\nThe graphics and sound effects resemble those of early video games. \"This movie is a fun way to share the atomic-scale world,\" said project leader Andreas J. Heinrich. \"The reason we made this was not to convey a scientific message directly, but to engage with students, to prompt them to ask questions.\" In addition, the researchers created three still images to promote \"Star Trek Into Darkness\"—the Federation logo, the starship \"Enterprise\", and a Vulcan salute.\n\nGuinness World Records certified the movie as The World's Smallest Stop-Motion Film ever made. The film was accepted into the Tribeca Online Film Festival and shown at the New York Tech Meet-up and the World Science Festival. Now in the top 1% of all most-watched YouTube videos, the film surpassed a million views in 24 hours, and two million views in 48 hours, with more than 27,000 likes.\n\nWhile the film was used by the researchers as a fun way to get students interested in science, it grew out of work that could increase the amount of data computers could store. In 2012 they had demonstrated that they could store a bit of computer memory on a group of just 12 atoms instead of a million, the previous minimum. If it became commercially viable, \"You could carry around, not just two movies on your iPhone,\" Heinrich said in a companion video about the film's production, \"you could carry around every movie ever produced.\"\n\n\n"}
{"id": "8292534", "url": "https://en.wikipedia.org/wiki?curid=8292534", "title": "Andy Rathbone", "text": "Andy Rathbone\n\nAndy Rathbone is the author of a number of ...for Dummies books about Microsoft Windows as well as other computing books.\n\nRathbone was born in San Diego, California. He attended San Diego State University, majoring in comparative literature, and graduated in 1986. He lives with his wife Tina in Ocean Beach, a neighborhood in San Diego, California.\n\nThe character of Mark \"The Rat\" Ratner in the movie \"Fast Times at Ridgemont High\" was modeled on Rathbone after writer Cameron Crowe befriended him during Crowe's research for the original book at San Diego's Clairemont High School. Rathbone's portrayal in the film was featured in an article titled \"Geek God\" published in the March 13, 1995, issue of \"People Weekly\" magazine.\n\nBefore becoming an author, Rathbone was a reporter for the \"La Jolla Light\" newspaper, was an editor at \"ComputorEdge Magazine\", and has freelanced for \"PC World\", \"ComputerWorld\" and \"CompuServe\".\n\nIn 1992, he wrote his first \"For Dummies\" book, \"Windows For Dummies\", which was a \"New York Times\" bestseller. Since then, he has published some 50 computer books. His latest book is \"Windows 10 For Dummies.\" Several of his \"For Dummies\" books have made \"USA Today\"'s bestseller list.\n\n"}
{"id": "4318", "url": "https://en.wikipedia.org/wiki?curid=4318", "title": "Big Dig", "text": "Big Dig\n\nThe Central Artery/Tunnel Project (CA/T), known unofficially as the Big Dig, was a megaproject in Boston that rerouted the Central Artery of Interstate 93, the chief highway through the heart of the city, into the 1.5-mile (2.4 km) Thomas P. O'Neill Jr. Tunnel. The project also included the construction of the Ted Williams Tunnel (extending Interstate 90 to Logan International Airport), the Leonard P. Zakim Bunker Hill Memorial Bridge over the Charles River, and the Rose Kennedy Greenway in the space vacated by the previous I-93 elevated roadway. Initially, the plan was also to include a rail connection between Boston's two major train terminals. Planning began in 1982; the construction work was carried out between 1991 and 2006; and the project concluded on December 31, 2007 when the partnership between the program manager and the Massachusetts Turnpike Authority ended.\n\nThe Big Dig was the most expensive highway project in the US, and was plagued by cost overruns, delays, leaks, design flaws, charges of poor execution and use of substandard materials, criminal arrests, and one death. The project was originally scheduled to be completed in 1998 at an estimated cost of $2.8 billion (in 1982 dollars, US$6.0 billion adjusted for inflation ). However, the project was completed in December 2007 at a cost of over $14.6 billion ($8.08 billion in 1982 dollars, meaning a cost overrun of about 190%) . \"The Boston Globe\" estimated that the project will ultimately cost $22 billion, including interest, and that it would not be paid off until 2038. As a result of a death, leaks, and other design flaws, Bechtel and Parsons Brinckerhoff—the consortium that oversaw the project—agreed to pay $407 million in restitution and several smaller companies agreed to pay a combined sum of approximately $51 million.\n\nThe Rose Fitzgerald Kennedy Greenway is a roughly series of parks and public spaces, which are the final part of the Big Dig after Interstate 93 was put underground. The Greenway was named in honor of Kennedy family matriarch Rose Fitzgerald Kennedy, and was officially dedicated on July 26, 2004.\n\nThis project was developed in response to traffic congestion on Boston's historically tangled streets which were laid out long before the advent of the automobile. As early as 1930 the city's Planning Board recommended a raised express highway running north-south through the downtown district in order to draw traffic off the city streets. Commissioner of Public Works William Callahan promoted plans for an elevated expressway which eventually was constructed between the downtown area and the waterfront. Governor John Volpe interceded in the 1950s to change the design of the last section of the Central Artery putting it underground through the Dewey Square Tunnel. While traffic moved somewhat better, the other problems remained. There was chronic congestion on the Central Artery (I-93), an elevated six-lane highway through the center of downtown Boston, which was, in the words of Pete Sigmund, \"like a funnel full of slowly-moving, or stopped, cars (and swearing motorists).\"\nIn 1959, the 1.5-mile-long (2.4 km) road section carried approximately 75,000 vehicles a day, but by the 1990s, this had grown to 190,000 vehicles a day. Traffic jams of 16 hours were predicted for 2010.\n\nThe expressway had tight turns, an excessive number of entrances and exits, entrance ramps without merge lanes, and as the decades passed, had continually escalating vehicular traffic that was well beyond its design capacity. Local businesses again wanted relief, city leaders sought a reuniting of the waterfront with the city, and nearby residents desired removal of the matte green-painted elevated road which mayor Thomas Menino called Boston's \"other Green Monster\". MIT engineers Bill Reynolds and (eventual state Secretary of Transportation) Frederick P. Salvucci envisioned moving the whole expressway underground.\n\nAnother important motivation for the final form of the Big Dig was the abandonment of the Massachusetts Department of Public Works' intended expressway system through and around Boston. The Central Artery, as part of Mass. DPW's Master Plan of 1948, was originally planned to be the downtown Boston stretch of Interstate 95, and was signed as such; a bypass road called the Inner Belt, was subsequently renamed Interstate 695. (The law establishing the Interstate highway system was enacted in 1956.) The Inner Belt District was to pass to the west of the downtown core, through the neighborhood of Roxbury and the cities of Brookline, Cambridge, and Somerville. Earlier controversies over impact of the Boston extension of the Massachusetts Turnpike, particularly on the heavily populated neighborhood of Brighton, and the additional large amount of housing that would have had to be destroyed led to massive community opposition to both the Inner Belt and the Boston section of I-95.\n\nBuilding demolition and land clearances for I-95 through the neighborhoods of Roxbury, Jamaica Plain, and Roslindale led to secession threats by Hyde Park, Boston's youngest and southernmost neighborhood. By 1972, with only a minimum of work done on the I-95 right of way and none on the potentially massively disruptive Inner Belt, Governor Francis Sargent put a moratorium on highway construction within the MA-128 corridor, except for the final short stretch of Interstate 93. In 1974, the remainder of the Master Plan was canceled, leaving Boston with a severely overstressed expressway system for the existing traffic.\n\nWith ever-increasing traffic volumes funneled onto I-93 alone, the Central Artery became chronically gridlocked. The Sargent moratorium led to the rerouting of I-95 away from Boston around the MA-128 beltway and the conversion of the cleared land in the southern part of the city into the Southwest Corridor linear park, as well as a new right-of-way for the Orange Line subway and Amtrak. Parts of the planned I-695 right-of-way remain unused and under consideration for future mass-transit projects.\n\nThe original 1948 Master Plan included a Third Harbor Tunnel plan that was hugely controversial in its own right, because it would have disrupted the Maverick Square area of East Boston. It was never built.\n\nA major reason for the all-day congestion was that the Central Artery carried not only north–south traffic, but much east–west traffic as well. Boston's Logan Airport lies across Boston Harbor in East Boston, and before the Big Dig the only access from downtown was through the paired Callahan and Sumner tunnels. Traffic on the major highways from west of Boston—the Massachusetts Turnpike and Storrow Drive—mostly traveled on portions of the Central Artery to reach these tunnels. Getting between the Central Artery and the tunnels involved short diversions onto city streets, increasing local congestion.\n\nA number of public transportation projects were included as part of an environmental mitigation for the Big Dig. The most expensive was the building of the Phase II Silver Line tunnel under Fort Point Channel, done in coordination with Big Dig construction. Silver Line buses now use this tunnel and the Ted Williams Tunnel to link South Station and Logan Airport.\n\n, promised projects to extend the Green Line beyond Lechmere, to connect the Red and Blue subway lines, and to restore the Green Line streetcar service to the Arborway in Jamaica Plain have not been completed. Construction of the extension beyond Lechmere has begun. The Red and Blue subway line connection underwent initial design, but no funding has been designated for the project. The Arborway Line restoration has been abandoned, following a final court decision in 2011.\n\nThe original Big Dig plan also included the North-South Rail Link, which would have connected North and South Stations (the major passenger train stations in Boston), but this aspect of the project was ultimately dropped by the state transportation administration early in the Dukakis administration. Negotiations with the federal government had led to an agreement to widen some of the lanes in the new harbor tunnel, and accommodating these would require the tunnel to be deeper and mechanically-vented; this left no room for the rail lines, and having diesel trains (then in use) passing through the tunnel would have substantially increased the cost of the ventilation system.\n\nThe project was conceived in the 1970s by the Boston Transportation Planning Review to replace the rusting elevated six-lane Central Artery. The expressway separated downtown from the waterfront, and was increasingly choked with bumper-to-bumper traffic. Business leaders were more concerned about access to Logan Airport, and pushed instead for a third harbor tunnel. In their second terms, Michael Dukakis (governor) and Fred Salvucci (secretary of transportation) came up with the strategy of tying the two projects together—thereby combining the project that the business community supported with the project that they and the City of Boston supported.\n\nPlanning for the Big Dig as a project officially began in 1982, with environmental impact studies starting in 1983. After years of extensive lobbying for federal dollars, a 1987 public works bill appropriating funding for the Big Dig was passed by the US Congress, but it was vetoed by President Ronald Reagan for being too expensive. When Congress overrode the veto, the project had its green light and ground was first broken in 1991.\n\nIn 1997, the state legislature created the Metropolitan Highway System and transferred responsibility for the Central Artery and Tunnel \"CA/T\" Project from the Massachusetts Highway Department and the Massachusetts Governor's Office to the Massachusetts Turnpike Authority (MTA).\nThe MTA, which had little experience in managing an undertaking of the scope and magnitude of the CA/T Project, hired a joint venture to provide preliminary designs, manage design consultants and construction contractors, track the project's cost and schedule, advise MTA on project decisions, and (in some instances) act as the MTA's representative. Eventually, MTA combined some of its employees with joint venture employees in an integrated project organization. This was intended to make management more efficient, but it hindered MTA's ability to independently oversee project activities because MTA and the joint venture had effectively become partners in the project.\n\nIn addition to political and financial difficulties, the project faced several environmental and engineering obstacles.\nThe downtown area through which the tunnels were to be dug was largely landfill, and included existing Red Line and Blue Line subway tunnels as well as innumerable pipes and utility lines that would have to be replaced or moved. Tunnel workers encountered many unexpected geological and archaeological barriers, ranging from glacial debris to foundations of buried houses and a number of sunken ships lying within the reclaimed land.\n\nThe project received approval from state environmental agencies in 1991, after satisfying concerns including release of toxins by the excavation and the possibility of disrupting the homes of millions of rats, causing them to roam the streets of Boston in search of new housing. By the time the federal environmental clearances were delivered in 1994, the process had taken some seven years, during which time inflation greatly increased the project's original cost estimates.\n\nReworking such a busy corridor without seriously restricting traffic flow required a number of state-of-the-art construction techniques. Because the old elevated highway (which remained in operation throughout the construction process) rested on pylons located throughout the designated dig area, engineers first utilized slurry wall techniques to create concrete walls upon which the highway could rest. These concrete walls also stabilized the sides of the site, preventing cave-ins during the continued excavation process.\n\nThe multi-lane interstate highway also had to pass under South Station's seven railway tracks, which carried over 40,000 commuters and 400 trains per day. To avoid multiple relocations of train lines while the tunneling advanced, as had been initially planned, a specially designed jack was constructed to support the ground and tracks to allow the excavation to take place below. Construction crews also used ground freezing (an artificial induction of permafrost) to help stabilize surrounding ground as they excavated the tunnel. This was the largest tunneling project undertaken beneath railway lines anywhere in the world. The ground freezing enabled safer, more efficient excavation, and also assisted in environmental issues, as less contaminated fill needed to be exported than if a traditional cut-and-cover method had been applied.\n\nOther challenges included existing subway tunnels crossing the path of the underground highway. To build slurry walls past these tunnels, it was necessary to dig beneath the tunnels and to build an underground concrete bridge to support the tunnels' weight, without interrupting rail service.\n\nThe project was managed by the Massachusetts Turnpike Authority, with the Big Dig and the Turnpike's Boston Extension from the 1960s being financially and legally joined by the legislature as the Metropolitan Highway System. Design and construction was supervised by a joint venture of Bechtel Corporation and Parsons Brinckerhoff. Because of the enormous size of the project—too large for any company to undertake alone—the design and construction of the Big Dig was broken up into dozens of smaller subprojects with well-defined interfaces between contractors. Major heavy-construction contractors on the project included Jay Cashman, Modern Continental, Obayashi Corporation, Perini Corporation, Peter Kiewit Sons' Incorporated, J. F. White, and the Slattery division of Skanska USA. (Of those, Modern Continental was awarded the greatest gross value of contracts, joint ventures included.)\n\nThe nature of the Charles River crossing had been a source of major controversy throughout the design phase of the project. Many environmental advocates preferred a river crossing entirely in tunnels, but this, along with 27 other plans, was rejected as too costly. Finally, with a deadline looming to begin construction on a separate project that would connect the Tobin Bridge to the Charles River crossing, Salvucci overrode the objections and chose a variant of the plan known as \"Scheme Z\". This plan was considered to be reasonably cost-effective, but had the drawback of requiring highway ramps stacked up as high as immediately adjacent to the Charles River.\n\nThe city of Cambridge objected to the visual impact of the chosen Charles River crossing design. The city sued to revoke the project's environmental certificate and forced the project planners to redesign the river crossing again.\nSwiss engineer Christian Menn took over the design of the bridge. He suggested a cradle cable-stayed bridge that would carry ten lanes of traffic. The plan was accepted and construction began on the Leonard P. Zakim Bunker Hill Memorial Bridge. The bridge employed an asymmetrical design and a hybrid of steel and concrete was used to construct it. The distinctive bridge is supported by two forked towers connected to the span by cables and girders. It was the first bridge in the country to employ this method and it was, at the time, the widest cable-stayed bridge in the world, having since been surpassed by the Eastern span replacement of the San Francisco–Oakland Bay Bridge, although that structure is actually a self-anchored suspension bridge, not a cable-stayed bridge.\n\nMeanwhile, construction continued on the Tobin Bridge approach. By the time all parties agreed on the I-93 design, construction of the Tobin connector (today known as the \"City Square Tunnel\" for a Charlestown area it bypasses) was far along, significantly adding to the cost of constructing the US Route 1 interchange and retrofitting the tunnel. \n\nBoston blue clay and other soils extracted from the path of the tunnel were used to cap many local landfills, fill in the Granite Rail Quarry in Quincy, and restore the surface of Spectacle Island in the Boston Harbor Islands National Recreation Area.\n\nThe Storrow Drive Connector, a companion bridge to the Zakim, began carrying traffic from I-93 to Storrow Drive in 1999. The project had been under consideration for years, but was opposed by the wealthy residents of the Beacon Hill neighborhood. However, it finally was accepted because it would funnel traffic bound for Storrow Drive and downtown Boston away from the mainline roadway. The Connector ultimately used a pair of ramps that had been constructed for Interstate 695, enabling the mainline I-93 to carry more traffic that would have used I-695 under the original Master Plan.\n\nWhen construction began, the project cost, including the Charles River crossing, was estimated at $5.8 billion. Eventual cost overruns were so high that the chairman of the Massachusetts Turnpike Authority, James Kerasiotes, was fired in 2000. His replacement had to commit to an $8.55 billion cap on federal contributions. The total expenses eventually passed $15 billion. Interest brought this cost to $21.93 billion.\n\nSeveral unusual engineering challenges arose during the project, requiring unusual solutions and methods to address them.\n\nAt the beginning of the project, engineers had to figure out the safest way to build the tunnel without endangering the existing elevated highway above. Eventually, they created horizontal braces as wide as the tunnel, then cut away the elevated highway's struts, and lowered it onto the new braces.\n\nOn January 18, 2003, the opening ceremony was held for the I-90 Connector Tunnel, extending the Massachusetts Turnpike (Interstate 90) east into the Ted Williams Tunnel, and onwards to Boston Logan International Airport. The Ted Williams tunnel had been completed and was in limited use for commercial traffic and high-occupancy vehicles since late 1995. The westbound lanes opened on the afternoon of January 18 and the eastbound lanes on January 19.\n\nThe next phase, moving the elevated Interstate 93 underground, was completed in two stages: northbound lanes opened in March 2003 and southbound lanes (in a temporary configuration) on December 20, 2003. A tunnel underneath Leverett Circle connecting eastbound Storrow Drive to I-93 North and the Tobin Bridge opened December 19, 2004, easing congestion at the circle. All southbound lanes of I-93 opened to traffic on March 5, 2005, including the left lane of the Zakim Bridge, and all of the refurbished Dewey Square Tunnel.\n\nBy the end of December 2004, 95% of the Big Dig was completed. Major construction remained on the surface, including construction of final ramp configurations in the North End and in the South Bay interchange, and reconstruction of the surface streets.\n\nThe final ramp downtown — exit 20B from I-93 south to Albany Street — opened January 13, 2006.\n\nIn 2006, the two Interstate 93 tunnels were dedicated as the Thomas P. O'Neill Jr. Tunnel, after the former Democratic speaker of the House of Representatives from Massachusetts who pushed to have the Big Dig funded by the federal government.\n\nThe Commonwealth of Massachusetts was required under the Federal Clean Air Act to mitigate air pollution generated by the highway improvements. Secretary of Transportation Fred Salvucci signed an agreement with the Conservation Law Foundation in 1990 enumerating 14 specific projects the state agreed to build. This list was affirmed in a 1992 lawsuit settlement.\n\nProjects which have been completed include:\n\nAs of 2014, several mitigation projects were incomplete:\n\nSome projects, such as restoration of Green Line \"E\" Arborway service, were removed from the list of mitigation projects and replaced with other projects with similar air-quality improvements.\n\nSome surface treatments, that were part of the original project plan, were dropped due to the massive cost overruns on the highway portion of the project. For example, the North Point Park was created as part of the project, but it ended without constructing pedestrian bridges to neighboring parks. The bridges were later funded by the American Recovery and Reinvestment Act of 2009.\n\nWhile not a legally mandated requirement, public art was part of the urban design planning process (and later design development work) through the Artery Arts Program. The intent of the program was to integrate public art into highway infrastructure (retaining walls, fences, and lighting) and the essential elements of the pedestrian environment (walkways, park landscape elements, and bridges). As overall project costs increased, the Artery Arts Program was seen as a potential liability, even though there was support and interest from the public and professional arts organizations in the area.\n\nAt the beginning of the highway design process, a temporary arts program was initiated, and over 50 proposals were selected. However, development began on only a few projects before funding for the program was cut. Permanent public art that was funded includes: super graphic text and facades of former West End houses cast into the concrete elevated highway abutment support walls near North Station by artist Sheila Levrant de Bretteville; Harbor Fog, a sensor-activated mist, light and sound sculptural environment by artist Ross Miller in parcel 17; a historical sculpture celebrating the 18th and 19th century shipbuilding industry and a bust of shipbuilder Donald McKay in East Boston; blue interior lighting of the Zakim Bridge; and the Miller's River Littoral Way walkway and lighting under the loop ramps north of the Charles River.\n\nExtensive landscape planting, as well as a maintenance program to support the plantings, was requested by many community members during public meetings. \n\nThe Big Dig untangled the co-mingled traffic from the Massachusetts Turnpike and the Sumner and Callahan Tunnels. While only one net lane in each direction was added to the north–south I-93, several new east–west lanes became available. East–west traffic on the Massachusetts Turnpike/I-90 now proceeds directly through the Ted Williams Tunnel to Logan Airport and Route 1A beyond. Traffic between Storrow Drive and the Callahan and Sumner Tunnels still uses a short portion of I-93, but additional lanes and direct connections are provided for this traffic.\n\nThe result was a 62% reduction in vehicle hours of travel on I-93, the airport tunnels, and the connection from Storrow Drive, from an average 38,200 hours per day before construction (1994–1995) to 14,800 hours per day in 2004–2005, after the project was largely complete. The savings for travelers was estimated at $166 million annually in the same 2004–2005 time frame. Travel times on the Central Artery northbound during the afternoon peak hour were reduced 85.6%.\n\nA 2008 \"Boston Globe\" report asserted that waiting time for the majority of trips actually increased as a result of demand induced by the increased road capacity. Because more drivers were opting to use the new roads, traffic bottlenecks were only pushed outward from the city, not reduced or eliminated (although some trips are now faster). The report states, \"Ultimately, many motorists going to and from the suburbs at peak rush hours are spending more time stuck in traffic, not less.\" The \"Globe\" also asserted that their analysis provides a fuller picture of the traffic situation than a state-commissioned study done two years earlier, in which the Big Dig was credited with helping to save at least $167 million a year by increasing economic productivity and decreasing motor vehicle operating costs. That study did not look at highways outside the Big Dig construction area and did not take into account new congestion elsewhere.\n\nAs part of the project, an elaborate Operations Control Center (OCC) control room was constructed in South Boston. Staffed on a \"24/7/365\" basis, this center monitors and reports on traffic congestion, and responds to emergencies. Continuous video surveillance is provided by hundreds of cameras, and thousands of sensors monitor traffic speed and density, air quality, water levels, temperatures, equipment status, and other conditions inside the tunnel. The OCC can activate emergency ventilation fans, change electronic display signs, and dispatch service crews when necessary.\n\nAs far back as 2001, Turnpike Authority officials and contractors knew of thousands of leaks in ceiling and wall fissures, extensive water damage to steel supports and fireproofing systems, and overloaded drainage systems. A $10 million contract, Riwaj signed off as a cost overrun, was used to repair these leaks. Many of the leaks were a result of Modern Continental and other subcontractors failing to remove gravel and other debris before pouring concrete. This information was not made public; engineers at MIT (volunteer students and professors) performed several experiments and found serious problems with the tunnel.\n\nOn September 15, 2004, a major leak in the Interstate 93 north tunnel forced the closure of the tunnel while repairs were conducted. This also forced the Turnpike Authority to release information regarding its non-disclosure of prior leaks. A follow-up reported on \"extensive\" leaks that were more severe than state authorities had previously acknowledged. The report went on to state that the $14.6 billion tunnel system was riddled with more than 400 leaks. A \"Boston Globe\" report, however, countered that by stating there were nearly 700 leaks in a single section of tunnel beneath South Station. Turnpike officials also stated that the number of leaks being investigated was down from 1,000 to 500.\n\nThe problem of leaks is further aggravated by the fact that many of them involve corrosive salt water. This is caused by the proximity of Boston Harbor and the Atlantic Ocean, causing a mix of salt and fresh water leaks in the tunnel. The situation is made worse by road salt spread in the tunnel to melt ice during freezing weather, or brought in by vehicles passing through. Salt water and salt spray are well-known issues that must be dealt with in any marine environment. It has been reported that \"hundreds of thousands of gallons of salt water are pumped out monthly\" in the Big Dig, and a map has been prepared showing \"hot spots\" where water leakage is especially serious. Salt-accelerated corrosion has caused ceiling light fixtures to fail (see below), but can also cause rapid deterioration of embedded rebar and other structural steel reinforcements holding the tunnel walls and ceiling in place.\n\nThe much larger than expected volume of water that must be continuously pumped consumes a correspondingly larger amount of electrical power, and will cause the pumps to wear out much sooner than originally estimated.\n\nMassachusetts State Police searched the offices of Aggregate Industries, the largest concrete supplier for the underground portions of the project, in June 2005. They seized evidence that Aggregate delivered concrete that did not meet contract specifications. In March 2006 Massachusetts Attorney General Tom Reilly announced plans to sue project contractors and others because of poor work on the project. Over 200 complaints were filed by the state of Massachusetts as a result of leaks, cost overruns, quality concerns, and safety violations. In total, the state has sought approximately $100 million from the contractors ($1 for every $141 spent).\n\nIn May 2006, six employees of the company were arrested and charged with conspiracy to defraud the United States. In July 2007, Aggregate Industries settled the case with an agreement to pay $50 million. $42 million of the settlement went to civil cases and $8 million was paid in criminal fines. The company will provide $75 million in insurance for maintenance as well as pay $500,000 toward routine checks on areas suspected to contain substandard concrete.\n\nA fatal accident raised safety questions and closed part of the project for most of the summer of 2006. On July 10, 2006, concrete ceiling panels and debris weighing and measuring fell on a car traveling on the two-lane ramp connecting northbound I-93 to eastbound I-90 in South Boston, killing Milena Del Valle, who was a passenger, and injuring her husband, Angel Del Valle, who was driving. Immediately following the fatal ceiling collapse, Governor Mitt Romney ordered a \"stem-to-stern\" safety audit conducted by the Illinois engineering firm of Wiss, Janney, Elstner Associates, Inc. to look for additional areas of risk. Said Romney: \"We simply cannot live in a setting where a project of this scale has the potential of threatening human life, as has already been seen\". The collapse and closure of the tunnel greatly snarled traffic in the city. The resulting traffic jams are cited as contributing to the death of another person, a heart attack victim who died en route to Boston Medical Center when his ambulance was caught in one such traffic jam two weeks after the collapse. On September 1, 2006, one eastbound lane of the connector tunnel was re-opened to traffic.\n\nFollowing extensive inspections and repairs, Interstate 90 east- and westbound lanes reopened in early January 2007. The final piece of the road network, a high occupancy vehicle lane connecting Interstate 93 north to the Ted Williams Tunnel, reopened on June 1, 2007.\n\nOn July 10, 2007, after a lengthy investigation, the National Transportation Safety Board found that epoxy glue used to hold the roof in place during construction was not appropriate for long-term bonding. This was determined to be the cause of the roof collapse. The Power-Fast Epoxy Adhesive used in the installation was designed for short-term loading, such as wind or earthquake loads, not long-term loading, such as the weight of a panel.\n\nPowers Fasteners, the makers of the adhesive, revised their product specifications on May 15, 2007, to increase the safety factor from 4 to 10 for all of their epoxy products intended for use in overhead applications. The safety factor on Power-Fast Epoxy was increased from 4 to 16. On December 24, 2007, the Del Valle family announced they had reached a settlement with Powers Fasteners that would pay the family $6 million. In December 2008, Power Fasteners agreed to pay $16 million to the state to settle manslaughter charges.\n\nPublic safety workers have called the walkway safety handrails in the Big Dig tunnels \"ginsu guardrails\", because the squared-off edges of the support posts have caused mutilations and deaths of passengers ejected from crashed vehicles. After an eighth reported death involving the safety handrails, MassDOT officials announced plans to cover or remove the allegedly dangerous fixtures, but only near curves or exit ramps. This partial removal of hazards has been criticized by a safety specialist, who suggests that the handrails are just as dangerous in straight sections of the tunnel.\n\nIn March 2011, it became known that senior MassDOT officials had failed to disclose an issue with the lighting fixtures in the O'Neill tunnel. In early February 2011, a maintenance crew found a fixture lying in the middle travel lane in the northbound tunnel. Assuming it to be simple road debris, the maintenance team picked it up and brought it back to its home facility. The next day, a supervisor passing through the yard realized that the fixture was not road debris but was in fact one of the fixtures used to light the tunnel itself. Further investigation revealed that the fixture's mounting apparatus had failed, due to galvanic corrosion of incompatible metals, caused by having aluminum in direct contact with stainless steel, in the presence of salt water. The electrochemical potential difference between stainless steel and aluminum is in the range of 0.5 to 1.0V, depending on the exact alloys involved, and can cause considerable corrosion within months under unfavorable conditions.\n\nAfter the discovery of the reason why the fixture had failed, a comprehensive inspection of the other fixtures in the tunnel revealed that numerous other fixtures were also in the same state of deterioration. Some of the worst fixtures were temporarily shored up with plastic ties. Moving forward with temporary repairs, members of the MassDOT administration team decided not to let the news of the systemic failure and repair of the fixtures be released to the public or to Governor Deval Patrick's administration.\n\n, it appeared that all of the 25,000 light fixtures would have to be replaced, at an estimated cost of $54 million. The replacement work is mostly done at night, and requires lane closures or occasional closing of the entire tunnel for safety, and was estimated to take up to 2 years to complete. , replacement of the light fixtures is still ongoing.\n\n\n"}
{"id": "3153239", "url": "https://en.wikipedia.org/wiki?curid=3153239", "title": "Card catalog (cryptology)", "text": "Card catalog (cryptology)\n\nThe card catalog, or \"catalog of characteristics,\" in cryptography, was a system designed by Polish Cipher Bureau mathematician-cryptologist Marian Rejewski, and first completed about 1935 or 1936, to facilitate decrypting German Enigma ciphers.\nThe Polish Cipher Bureau used the theory of permutations to start breaking the Enigma cipher in late 1932. The Bureau recognized that the Enigma machine's doubled-key (see Grill (cryptology)) permutations formed cycles, and those cycles could be used to break the cipher. With German cipher keys provided by a French spy, the Bureau was able to reverse engineer the Enigma and start reading German messages. At the time, the Germans were using only 6 \"steckers\", and the Polish grill method was feasible. On 1 August 1936, the Germans started using 8 \"steckers\", and that change made the grill method less feasible. The Bureau needed an improved method to break the German cipher.\n\nAlthough the \"steckers\" would change which letters were in a doubled-key's cycle, the \"steckers\" would not change the number of cycles or the length of those cycles. Steckers could be ignored. Ignoring the mid-key turnovers, the Enigma machine had only distinct settings of the three rotors, and the three rotors could only be arranged in the machine ways. That meant there were only 105456 likely doubled-key permutations. The Bureau set about determining and cataloging the characteristic of each of those likely permutations. Each letter of the key could be one of partition number 13 = 101 possible values, and the 3 letters of the key meant there were 1030301 possible keys. On average, a key would find one setting of the rotors, but it might find several possible settings.\n\nThe Polish cryptanalyst could then collect enough traffic to determine all the cycles in a daily key. That usually took about 60 messages. The result might be:\nHe would use the lengths of the cycles (13;10-3;10-2-1) to look up the wheel order (II I III) and starting rotor positions in the card catalog. He would then use an Enigma to compute the un-\"steckered\" cycles:\nBy comparing the \"steckered\" cycles from the German traffic and the un-\"steckered\" cycles, the cryptanalyst can determine the \"steckers\". In the example, the \"CF\" permutation has codice_1 and codice_2. That requires that codice_3 is un\"stecker\"ed and a W-Z \"stecker\". The cycles can then be aligned on codice_3 and W-Z to determine other \"steckered\" and un-\"steckered\" letters.\nWhere codice_8 is a known un-\"steckered\" letter, codice_9 is a known \"steckered\" letter, and codice_10 is a newly discovered \"stecker\".\nRepetition produces the \"steckers\" A-M, F-I, N-V, P-S, T-U, W-Z.\n\nPreparation of the card catalog, using the cyclometer that Rejewski had invented about 1934 or 1935, was a laborious task that took over a year's time. But once the catalog was complete, obtaining Enigma daily keys was a matter of some fifteen minutes.\nWhen the Germans changed the Enigma machine's \"reflector,\" or \"reversing drum,\" on 1 November 1937, the Cipher Bureau was forced to start over again with a new card catalog: \"a task,\" writes Rejewski, \"which consumed, on account of our greater experience, probably somewhat less than a year's time.\"\n\nOn 15 September 1938 the Germans completely changed the procedure for enciphering message keys, rendering the card-catalog method useless. This spurred the invention of Rejewski's cryptologic bomb and Henryk Zygalski's \"perforated sheets.\"\n\n"}
{"id": "1550184", "url": "https://en.wikipedia.org/wiki?curid=1550184", "title": "Cocktail umbrella", "text": "Cocktail umbrella\n\nA cocktail umbrella is a small umbrella or parasol made from paper, paperboard, and a toothpick, used as a garnish or decoration in cocktails, desserts or other food and beverages.\n\nThe umbrella is fashioned out of paper, which can be patterned, with cardboard ribs. The ribs are made from cardboard in order to provide flexibility and to hinge so the umbrella can be pulled shut much like an ordinary umbrella. A small plastic retaining ring is often fashioned against the stem, a toothpick, in order to prevent the umbrella from folding up spontaneously. A sleeve of folded newspaper is located under the collar or base of the cocktail umbrella and is made out of recycled paper from either China, India or Japan. As a result, they indicate their country of origin.\n\nMost of the drinks that cocktail umbrellas are found in are tropical drinks. However, they are not all tiki drinks, as some would call them. To be considered a tiki drink, the cocktail must contain rum and exotic fruit juices. Following this definition, drinks such as pina colodas would be considered tiki drinks, but ones like the Hawaiian margarita would only be considered tropical. This is due to the fact that the Hawaiian margarita contains tequila instead of rum.\n\nNobody is quite certain exactly when the cocktail umbrella came into use. One possible source is Donn Beach, owner of the Hollywood, California-based restaurant and bar chain Don the Beachcomber. According to cocktail historian Dale DeGroff, Beach started the trend in 1932 after spending much of his time collecting things from around the world, most notably from the South Pacific. Beach sold his merchandise, including the cocktail umbrellas, to Victor Bergeron, owner of the Emeryville, California-based bar chain Trader Vic’s. According to Bergeron’s son Joe, Trader Vic’s used the paper parasols until their production was halted by World War II.\n\nAccording to Hawaiian-themed author Rick Carroll, Hawaiian bartender Harry K. Yee of the Hilton Waikiki was the first to use a paper parasol in a beverage in 1959 during the Tiki culture craze of the 1950s. Yee stated that he initially would use a sugar cane stick as a garnish for his tropical cocktails, but upon seeing how guests would set the sticks in ashtrays and dirty them, switched to Vanda orchids in 1955. In 1959, Yee switched to the cocktail umbrella for reasons unknown. Some speculate that it’s because the bar already stocked the umbrellas as toothpicks or decoration, so they were more readily available.\n\nAnother theory exists that Beach met Yee while vacationing in Hawaii after World War II. It is there that Beach and Yee traded cocktail ideas, recipes, and somewhere along the line, cocktail umbrellas. Afterwards, both of the bartenders began to use the umbrellas in tropical drinks.\n\nCocktail umbrellas are a Great Depression-era invention, but they didn't take off in popularity until after World War II. According to an Eater article published in 2015, many people, during the difficult times faced during the Great Depression, saw the South Pacific as \"a place of exotic abandon, where you didn't have to work for a living.\" Many poor Americans saw tiki culture as an escape from reality. With the rise of the middle class after World War II came families with disposable income. That, combined with Hawaii's statehood and the rise of commercial air travel in the late 1950s, led to an explosion in the popularity of tiki culture dubbed the \"tiki craze\". Tiki bars like Trader Vic's and Don the Beachcomber took advantage of the tiki craze, inventing slews of cocktails with a key identifying factor: a cocktail umbrella. The cocktail umbrella became synonymous with tiki cocktails, so much so that the drinks are often called \"umbrella drinks\".\n\nThe true purpose of the cocktail umbrella is unknown. Some bartenders say that the cocktail umbrella is only decorative. Other bartenders have argued that the umbrella provides shade that slows the melting of ice when the drink is served outdoors. However, the temperature outside matters more than direct sunlight when it comes to the melting of ice. The shade from the cocktail umbrella would do nothing to slow this down. Another hypothesis about the cocktail umbrella’s purpose is that its absence can lead to faster evaporation of alcohol due to direct sunlight when the drink is severed outdoors. Chemists reject this idea and explain that the presence of a cocktail umbrella has no effect on this. In the past, cocktail umbrellas have served as a gimmick to draw women into bars that were mostly frequented by men. However, this purpose has faded in current times.\n\nCurrently, the cocktail umbrella has become an important part of the drinks it is found on. It has become a garnish that is essential to the identities of these drinks. Cocktail garnishes started being used in the 19th century, with non-edible garnishes being introduced after prohibition. Two examples of these non-edible garnishes are cocktail umbrellas and swizzle sticks. Since their introduction, cocktail umbrellas use as a garnish has become a large part of their purpose. As a garnish, they complete the drink's presentation and identify it as tropical. However, cocktail umbrellas differ from other drink garnishes. Unlike foods commonly used as garnishes, such as cherries, olives, or citrus, cocktail umbrellas do not add to the flavor of the drink and have a purely aesthetic purpose.\n\nAlternative uses for cocktail umbrellas also exist. They can be used as toothpicks and may have been used as hat decorations in the past. It is also possible to use them as hair decorations and place cards to tell people where to sit at a hosted event.\n\nDrinks That Commonly use a Cocktail Umbrella:\n\n•Lava Lava\n\n•Hawaiian Margarita\n\n•Haupia\n\n•Lava Pit\n\n•Piña Colada\n\n•Kona Coffee-Tini\n\n•Mac Nut Martini\n\n•Mai Tai\n\n•Clipper-Tini\nOther Uses:\n\nThough the most common use for the cocktail umbrella is as a garnish in drinks there are many other uses people have found. As a decoration piece people have used the umbrellas to make wreaths, table centerpieces, lanterns, and even hair pieces. People have also found ways to turn cocktail umbrellas into art, as is the case with Dutch trio We Make Carpets. We Make Carpets makes creative carpets out of different everyday objects. For this project, they decided to use 6,000 cocktail umbrellas in their recent creation \"Umbrella Carpet 2.\" This unique use of cocktail umbrellas was displayed at a 2016 design exhibition in the Stedelijk Museum in Amsterdam.\n\nThe cocktail umbrellas can be seen used in most cocktail lounges, bars, restaurants, and luaus. The most frequent location you can spot the umbrella is restaurants and bars that maintain an \"island\" theme such as Hawaii.\n\n\n"}
{"id": "34900000", "url": "https://en.wikipedia.org/wiki?curid=34900000", "title": "Digital firm", "text": "Digital firm\n\nThe Digital Firm is an organization that has enabled core business relationships through digital networks. These digital networks are supported by enterprise class technology platforms that have been leveraged within an organization to support critical business functions and services. Some examples of these technology platforms are Customer Relationship Management (CRM), Supply Chain Management (SCM), Enterprise Resource Planning (ERP), Knowledge Management System (KMS), Enterprise Content Management (ECM), and Warehouse Management System (WMS) among others. The purpose of these technology platforms is to digitally enable seamless integration and information exchange within the organization to employees and outside the organization to customers, suppliers, and other business partners.\n\nThe term \"Digital Firm\" originated, as a concept in a series of Management Information Systems (MIS) books authored by Kenneth C. Laudon and it provides a new way to describe organizations that operate differently than the traditional brick and mortar business as a result of broad sweeping changes in technology and global markets. Digital firms place an emphasis on the digitization of business processes and services through sophisticated technology and information systems. These information systems create opportunities for digital firms to decentralize operations, accelerate market readiness and responsiveness, enhance customer interactions, as well as increase efficiencies across a variety of business functions.\n\nTechnology adoption has been increasing as digital firms continually look to achieve greater levels cost savings, competitive advantage, and operational performance optimization. As organizations adopt technology, the internal appetite for additional technologies increases and in some cases accelerates. This acceleration of technology adoption by digital firms creates a \"digital divide\" as emerging technology is absorbed at varying rates across organizations. This technology divergence can affect competitive dynamics in the market place between firms that achieve operational benefits from the technology and firms which have yet to adapt.\n\nWhile the growth of new technology consumption is not uniform across organizations, the trend for business-driven investment in technology across all markets has and continues to increase. During the span of 1990 to 2006, the gross U.S. domestic investment in information and communications technology, as measured by the U.S. Census Bureau, increased by 170%.\n\nThe market for Enterprise Resource Planning (ERP) systems and other packaged applications started to grow substantially during the 90's to the point that the ERP market alone accounts for approximately $25 billion. According to surveys conducted in 2002, nearly \"75% of global Fortune 1000 firms had implemented SAP’s ERP suite\".\n\nThrough digital networks and information systems, the digital firm is able to operate core business services and functions continuously and more efficiently. This digital enablement of business processes creates highly dynamic information systems allowing for more efficient and productive management of an organization.\n\nAdditionally, digital enablement of core business functions and services provides an organization with opportunities to:\n\n\nTechnology and information systems serve many critical roles in a digital firm by providing technology-driven capabilities that increase operational performance. For example, digital networks and information systems allow organizations to connect and integrate supply chains in ways that are real-time, uninterrupted and highly responsive to market conditions.\n\nAnother example of an information system that can increase an organization's performance awareness and management capabilities is a Real-Time Business Intelligence (RTBI) system. A RTBI system can provide a highly responsive and strategic decision support platform for an organization to analyze operational events as they occur. RTBI systems often work closely with Organizational Risk Management (ORM) systems in this capacity to increase capabilities around monitoring operational performance and assessing operational risks. These types of information systems can increase an organization's capabilities to effectively manage performance and productivity.\n\nThe three main enterprise information systems that can positively affect an organization's performance and productivity are:\n\nERP deployments can be complex and require a significant shift in business operations for an organization but the benefits can be substantial.\n\nAfter implementation of an ERP system within an organization there are measurable performance and productivity gains that can be directly correlated to the ERP system go-live event. This study conducted a detailed analysis of the ERP data produced and found that there was a direct causal relationship between ERP systems and performance gains in an organization.\n\nOrganizations that deploy ERP systems typically, based on performance and productivity gains, also implement both of the following enterprise platforms as well.\n\nOrganizations leverage CRM systems to improve the overall management of their relationships with customers. CRM systems operate as enterprise platforms that provide digital firms with opportunities to closely manage all aspects of interactions with customers through customer-oriented business processes.\n\nOrganizations which implement CRM systems may encounter some lag time until the CRM productivity affects are fully realized in the firm based on studies. However, the lag effects are difficult to measure and based in part on the organization's ability to leverage the new CRM system and adapt to the changes in business operations as a result.\n\nStudies of organizations that implemented SCM systems to improve supply chain management capabilities found that those systems had a significant impact on productivity and performance within the organization. Additionally, the implementation of SCM and CRM systems differed from an ERP implementation in that organizational performance could be directly correlated \"with both the initial purchase and go-live event\".\n\nSCM and CRM systems are often viewed as \"extended enterprise systems\" due to the way that they integrate with ERP systems and the benefits that they bring to organizations.\n\n"}
{"id": "31956396", "url": "https://en.wikipedia.org/wiki?curid=31956396", "title": "Digital scent technology", "text": "Digital scent technology\n\nDigital scent technology (or olfactory technology) is the engineering discipline dealing with olfactory representation. It is a technology to sense, transmit and receive scent-enabled digital media (such as web pages, video games, movies and music). This sensing part of this technology works by using olfactometers and electronic noses.\n\nIn the late 1950s, Hans Laube invented the Smell-O-Vision, a system which released odor during the projection of a film so that the viewer could \"smell\" what was happening in the movie. The Smell-O-Vision faced competition with AromaRama, a similar system invented by Charles Weiss that emitted scents through the air-conditioning system of a theater. \"Variety\" dubbed the competition \"the battle of the smellies\".\n\nSmell-O-Vision did not work as intended. According to a \"Variety\" review of the mystery comedy film \"Scent of Mystery\" (1960), which featured the one and only use of Smell-O-Vision, aromas were released with a distracting hissing noise and audience members in the balcony complained that the scents reached them several seconds after the action was shown on the screen. In other parts of the theater, the odors were too faint, causing audience members to sniff loudly in an attempt to catch the scent. These technical problems were mostly corrected after the first few showings, but the poor word of mouth, in conjunction with generally negative reviews of the film itself, led to the decline of Smell-O-Vision.\n\nIn 1999, DigiScents developed a computer peripheral device called iSmell, which was designed to emit a smell when a user visited a web site or opened an email. The device contained a cartridge with 128 \"primary odors\", which could be mixed to replicate natural and man-made odors. DigiScents had indexed thousands of common odors, which could be coded, digitized, and embedded into web pages or email. After $20 million in investment, DigiScents was shut down in 2001 when it was unable to obtain the additional funding it required.\n\nIn 2000, AromaJet developed a scent-generating device prototype called Pinoke. No new announcements have been made since December 2000.\n\nIn 2003, TriSenx (founded in 1999) launched a scent-generating device called Scent Dome, which by 2004 was tested by the UK internet service provider Telewest. This device was about the size of a teapot and could generate up to 60 different smells by releasing particles from one or more of 20 liquid-filled odor capsules. Computers fitted with a Scent Dome unit used software to recognize smell identifying codes embedded in an email or web page.\n\nIn 2004, Tsuji Wellness and France Telecom developed a scent-generating device called Kaori Web, which comes with 6 different cartridges for different smells. The Japanese firm, K-Opticom, had placed special units of this device in their internet cafes and other venues until the end of the experiment on March 20, 2005. Also in 2004, the Indian inventor Sandeep Gupta founded SAV Products, LLC and claimed to show a scent-generating device prototype at CES 2005.\n\nIn 2005, researchers from the University of Huelva developed XML Smell, a protocol of XML that can transmit smells. The researchers also developed a scent-generating device and worked on miniaturising its size. Also in 2005, Thanko launched P@D Aroma Generator, a USB device that comes with 3 different cartridges for different smells.\n\nIn 2005, Japanese researchers announced that they are working on a 3D television with touch and smell that would be commercially available on the market by the year 2020.\n\nDuring ThinkNext 2010, the Israeli company Scentcom featured a demo of its scent-generating device.\n\nIn June 2011, a press release from the University of California, San Diego Jacobs School of Engineering announced a paper published in Angewandte Chemie describing an optimization and minitaturization of a component that can select and release scents from 10,000 odors, that is intended to be part of a Digital scent solution for TVs and phones.\n\nIn October 2012, Aromajoin, a Japanese company, released a small-sized product named Aroma Shooter which contains 6 different solid-type scents.\n\nIn March 2013, a group of Japanese researchers unveiled a prototype invention they dubbed a \"smelling screen\". The device combines a digital display with four small fans that direct an emitted odor to a specific spot on the screen. The fans operate at a very low speed, making it difficult for the user to perceive airflow; instead he or she perceives the smell as coming directly out of the screen and object displayed at that location.\n\nIn July 2013, Raul Porcar (Spain), engineer and inventor developed and patented Olorama Technology, a wireless system with the aim to incorporate scents into movies, Virtual Reality, and all kind of audiovisual experiences.:\n\nIn December 2013 Amos Porat inventor and CTO Of scent2you Israel Company has built several prototypes that can control scents.\n\nAt GDC 2015, FeelReal unveiled its odor generator VR peripheral.\n\nIn 2016 Surina Hariri, Nur Ain Mustafa, Kasun Karunanayaka and Adrian David Cheok from Imagineering Institute, Iskandar Puteri, Malaysia experimented with Electrical stimulation of olfactory receptors.\n\nAt CEATEC 2016, Aromajoin unveiled the first wearable scent device, Aroma Shooter Mini, which can be connected and controlled from PCs and smartphones. Besides, the company also introduced a demo scent-enabled chatting app named AromaMessage in the event.\n\nCurrent obstacles of mainstream adoption include the timing and distribution of scents, a fundamental understanding of human olfactory perception, the health dangers of synthetic odours, and other hurdles.\n\n"}
{"id": "27237024", "url": "https://en.wikipedia.org/wiki?curid=27237024", "title": "Dragon (remote sensing)", "text": "Dragon (remote sensing)\n\nDragon refers to any of several remote sensing image processing\nsoftware packages. This software provides capabilities for displaying,\nanalyzing, and interpreting digital images\nfrom earth satellites and raster data files\nthat represent spatially distributed data. All the Dragon packages\nderive from code created by Goldin-Rudahl\nSystems, Incorporated, and focus on geography education:\n\n\nThe initial version of Dragon was released in 1987 and ran on the MS-DOS\noperating system. Dragon was the first commercial remote sensing software\npackage designed to use only the native capabilities of off-the-shelf personal\ncomputers. At the time Dragon was developed, other PC remote sensing products\nsuch as Erdas required expensive special purpose graphics\ndevices. Dragon was intended to be used for education in geography, geology,\nforestry and other disciplines that use spatial information; thus it was very\nimportant to minimize the costs of required hardware. The first version\nof Dragon ran on a basic IBM-PC with two floppy disks and a\nfour-color or gray-level graphics display. Alternatively, it could use any of several models of Japanese PC.\n\nThe MS-DOS phase of Dragon development focused on trying to squeeze\nfunctionality into very limited disk and memory space, and to get full-color\nimage display using rapidly changing graphics\nhardware with no standardized drivers. The VESA display standard was a\nturning point in making full-color display functionality available in\nMS-DOS. This VESA/SVGA/MS-DOS version of Dragon can still be adapted\nfor embedded systems use.\n\nThe move to Microsoft Windows 95/98 was painful because these\noperating systems did not provide true multitasking. Unfortunately this phase\ncoincided with the publication of the well-known Gibson and Powers textbook\n(Gibson,2000) which included a copy of the Windows 95 Dragon. With the advent\nof Windows NT and successors (Windows 2000, XP, Vista, etc.), it became\npossible to create a Windows version of Dragon that allowed simultaneous\ndisplay of and interaction with multiple images.\n\nIn 2004, funding became available from Thailand to create a free educational\nversion of the software which became known as OpenDragon. This project lasted\nfor three years. The software is still available at no cost in Thailand,\nLaos, Cambodia and Vietnam (although it has only been translated into Thai).\n\nAfter funding for OpenDragon was discontinued, Dragon Professional was\ndeveloped to reach beyond the customary educational users. New personal\ncomputer capabilities, which by then extended to gigabytes of memory and\nhundreds of gigabytes of disk storage, all at low cost, made it possible to\nstore and process the very large data sets produced by twenty-first-century\nhigh-resolution satellites.\n\nDragon Professional required major changes in the user interaction model,\nwhich previously had assumed a 1-to-1 relationship between the image on the\nscreen and the sensor data. At the same time, image processing operations such\nas selection of ground control points require access to individual data\nelements (pixels) selected from the more than 30 million available in a\ntypical full-scene image. Thus, the appearance and behavior of Dragon\nProfessional are quite different from OpenDragon/Dragon Academic.\n\nAsian dragons are considered symbolic of wisdom and knowledge, unlike the\nferocious western dragons. Thus, the name Dragon/ips(r) or Dragon Image\nProcessing System is intended to imply wisdom in the knowledge of and\nintelligent use of the world in which we live.\n\nBecause the expected user is assumed to be relatively untrained, Dragon pays\nmore attention to the user experience than to having a large selection of\npossibly obscure processing operations. Within the user interface, which has\nbeen translated into several languages, context-sensitive help explains every\nuser choice, and reasonable defaults are provided where possible. The User\nManual (English only) details all processing algorithms.\n\nThe software provides a fairly conventional set of remote sensing operations,\nwhich are intended to be those which a student of geography arguably ought to\nknow. These include:\n\n\nIn order to provide interoperability with other software packages, and to permit users\nto add their own custom processing operations, all important file formats are\ndocumented and an API called the Programmer's Toolkit is available.\n\n\n\n"}
{"id": "2760129", "url": "https://en.wikipedia.org/wiki?curid=2760129", "title": "Drawer dishwasher", "text": "Drawer dishwasher\n\nA dishdrawer is a type of dishwashing machine invented, designed and manufactured by Fisher & Paykel. They are available under several brands depending on geographic location Fisher & Paykel, Kenmore Appliances, KitchenAid, and Bauknecht following a distribution agreement with Whirlpool.\n\nIn 1987 Fisher & Paykel staff engineer Adrian Sargeant and designer Phil Brace developed the DishDrawer concept. A DishDrawer is based on a design similar to filing cabinet with each dishwasher having two fully independent cabinets (or drawers). The DishDrawer was exhibited in 1996 at Domotechnica and was launched in 1997. By this time Fisher & Paykel had spent $10 million on the machine's development.\n\nThey are marketed as either an individual drawer, for small apartments or homes or double units which have two separate drawers stacked together. Each drawer is independent, allowing different wash settings to be used on different loads. It also allows dishes to be washed in one drawer as the other is filled. And is potentially more energy efficient. \n\nDishDrawer washers have capacity restrictions due to their lower height, limiting the size of dishes that can be placed inside. In response to this, Fisher & Paykel have added 'Tall' models to their DishDrawer line up.\nEarly models were subject to complaints about the noise that they made when washing. This has been improved on with later models.\n\nThe concept behind the dishdrawer is \"to use compact technologies to provide a very space efficient dishwasher\". \"The dishdrawer caused quite a stir when it was launched and has been going from strength to strength ever since\". \"The drawer action means you load the dishwasher from the top which is much easier on the back. Fisher and Paykel have made several dish drawer configurations with single and double varieties available. This means you can install two single dishdrawers side by side toward the top of the bench for easy access or you can install a double dishdrawer where the two drawers are stacked. DishDrawers provide plenty of flexibility\" \n\n"}
{"id": "884103", "url": "https://en.wikipedia.org/wiki?curid=884103", "title": "Faint Object Camera", "text": "Faint Object Camera\n\nThe Faint Object Camera (FOC) was a camera installed on the Hubble Space Telescope from launch in 1990 until 2002. It was replaced by the Advanced Camera for Surveys. In December 1993, Hubble's vision was corrected on STS-61 by installing COSTARS, which corrected the problem with Hubble's mirror before it reached an instrument like FOC. Later instruments had this correction built in, which is why it was possible to later remove COSTARS itself and replace it with a new science instrument.\n\nThe camera was built by Dornier GmbH and was funded by the European Space Agency. The unit actually consists of two complete and independent camera systems designed to provide extremely high resolution, exceeding 0.05 arcseconds. It is designed to view very faint UV and optical light from 115 to 650 nanometers in wavelength. FOC has been compared to a \"telephoto\" lens, providing a high resolution in a small field of view. FOC could distinguish between two points 0.05 arc-seconds apart.\n\nThe camera was designed to operate at low, medium, or high resolution. The angular resolution and field of view at each resolution were as follows:\n\n"}
{"id": "7237326", "url": "https://en.wikipedia.org/wiki?curid=7237326", "title": "Free viewpoint television", "text": "Free viewpoint television\n\nFree viewpoint television (FTV) is a system for viewing natural video, allowing the user to interactively control the viewpoint and generate new views of a dynamic scene from any 3D position.\nThe equivalent system for computer-simulated video is known as virtual reality. With FTV, the focus of attention can be controlled by the viewers rather than a director, meaning that each viewer may be observing a unique viewpoint. It remains to be seen how FTV will affect television watching as a group activity.\n\nSystems for rendering arbitrary views of natural scenes have been well known in the computer vision community for a long time but only in recent years has the speed and quality reached levels that are suitable for serious consideration as an end user system.\n\nProfessor Masayuki Tanimoto from Nagoya University (Japan) has done much to promote the use of the term \"free viewpoint television\" and has published many papers on the ray space representation, although other techniques can be, and are used for FTV.\n\nQuickTime VR might be considered a predecessor to FTV.\n\nIn order to acquire the views necessary to allow a high quality rendering of the scene from any angle, several cameras are placed around the scene; either in a studio environment or an outdoor venue, such as a sporting arena for example. The output Multiview Video (MVV) must then be packaged suitably so that the data may be compressed and also so that the users' viewing device may easily access the relevant views to interpolate new views.\n\nIt is not enough to simply place cameras around the scene to be captured. The geometry of the camera set up must be measured by a process known in computer vision as \"camera calibration.\" Manual alignment would be too cumbersome so typically a \"best effort\" alignment is performed prior to capturing a test pattern that is used to generate calibration parameters.\n\nRestricted free viewpoint television views for large environments can be captured from a single location camera system mounted on a moving platform. Depth data must also be captured, which is necessary to generate the free viewpoint. The Google Street View capture system is an example with limited functionality. The first full commercial implementation, iFlex, was delivered in 2009 by Real Time Race.\n\nMultiview video capture varies from partial (usually about 30 degrees) to complete (360 degrees) coverage of the scene. Therefore, it is possible to output stereoscopic views suitable for viewing with a 3D display or other 3D methods. Systems with more physical cameras can capture images with more coverage of the viewable scene, however, it is likely that certain regions will always be occluded from any viewpoint. A larger number of cameras should make it possible to obtain high quality output because less interpolation is needed.\n\nMore cameras mean that efficient coding of the Multiview Video is required. This may not be such a big disadvantage as there are representations that can remove the redundancy in MVV; such as inter view coding using MPEG-4 or Multiview Video Coding, the ray space representation, geometry videos, etc.\n\nIn terms of hardware, the user requires a viewing device that can decode MVV and synthesize new viewpoints, and a 2D or 3D display.\n\nThe Moving Picture Experts Group (MPEG) has normalized Annex H of MPEG-4 AVC in March 2009 called Multiview Video Coding after the work of a group called '3DAV' (3D Audio and Visual) headed by Aljoscha Smolic at the Heinrich-Hertz Institute.\n\n\n"}
{"id": "49473613", "url": "https://en.wikipedia.org/wiki?curid=49473613", "title": "Functional analog (electronic)", "text": "Functional analog (electronic)\n\nFunctional analogs (or functional analogues) are entities (models, representations, etc.) that can be replaced, to fulfill the same function. When the entities in question are formally represented by black boxes, the concept of \"analog\" is related to \"same behavior\": they take the same output sequence when submitted to the same input sequence.\n\nAnalogical models are used in a method of representing a ‘target system’ by another, more understandable or analysable system. Two systems have \"analog functions\" (see illustration) if the black box representation of both can be the same.\n"}
{"id": "1633417", "url": "https://en.wikipedia.org/wiki?curid=1633417", "title": "George Devol", "text": "George Devol\n\nGeorge Charles Devol Jr. (February 20, 1912 – August 11, 2011) was an American inventor known for developing Unimate, the first material handling robot employed in industrial production work.\n\nBorn in Louisville, Kentucky in 1912, Devol was interested from boyhood in all things electrical and mechanical such as boats, airplanes, and engines.\n\nChoosing to forego higher education, in 1932 Devol went into business, forming United Cinephone to produce variable area recording directly onto film for the new sound motion pictures (\"talkies\"). However, he later learned that companies like RCA and Western Electric were working in the same area, and decided to discontinue the product.\n\nIn 1939, Devol applied for a patent for proximity controls for use in laundry press machines, based on a radio frequency field. This control would automatically open and close laundry presses when workers approached the machines. Once the war began, Devol was advised by the patent office that his patent application would be placed on hold for the duration of the conflict.\n\nAround the time the World War II began, Devol sold his interest in United Cinephone and approached Sperry Gyroscope to see if they were interested in his ideas on radar technology. He was retained by Sperry as manager of the Special Projects Department that developed radar devices and microwave test equipment.\n\nLater in the war, he approached Auto-Ordnance Company regarding products that company could produce aside from their primary product line, which were Thompson submachine guns. Devol told them that the field of radar counter-measures was about to emerge as an urgently needed defense technology.\n\nIn 1943, he organized General Electronics Industries in Greenwich, Connecticut, as a subsidiary of the Auto Ordinance Corporation. General Electronics produced counter-radar devices until the end of the War. General Electronics was one of the largest producers of radar and radar counter-measure equipment for the U.S. Navy, U.S. Army Air Force and other government agencies. The company's radar counter-measure systems were on allied planes on D-Day.\n\nOver a difference of opinion regarding the future of certain projects, Devol resigned from Auto Ordinance and joined RCA. After a short stint as eastern sales manager of electronics products, which he felt \"wasn't his ball of wax\", Devol left RCA to develop ideas that eventually led to the patent application for the first industrial robot. In 1946 he applied for a patent on a magnetic recording system for controlling machines and a digital playback device for machines.\n\nDevol was part of the team that developed the first commercial use of microwave oven technology, the Speedy Weeny, which automatically cooked and dispensed hotdogs in places such as Grand Central Terminal.\n\nIn the early 1950s, Devol licensed his digital magnetic recording device to Remington Rand of Norwalk, CT and became manager of their magnetics department. There he worked with a team to develop his magnetic recording system for business data applications. He also worked on developing the first high-speed printing systems. While the magnetic recording system proved too slow for business data, Devol's invention was re-purposed as a machine control that would eventually become the \"brains\" of the Unimate robot.\n\nIn the 1940s, Devol wasn't thinking about robots. Instead, he was focusing on manipulators and his patent on magnetic recording devices. He felt the world was ready for new ideas as he saw the introduction of automation into factories during this time.\n\nIn 1954, Devol applied for patent on Programmed Article Transfer that introduced the concept of \"Universal Automation\" or Unimation; was issued in 1954. At the suggestion of Devol's wife, Evelyn, the word \"Unimate\" was coined to define the product, much the same as George Eastman had coined Kodak.\nWhen he filed the patent for a programmable method for transferring articles, he wrote, \"the present invention makes available for the first time a more or less general purpose machine that has universal application to a vast diversity of applications where cyclic digital control is desired.\"\n\nAfter applying for this seminal patent — which had not a single prior citation — Devol searched for a company willing to give him financial backing to develop his programmable articles transfer system. He talked with many major corporations in the United States during his search. Through family connections, Devol obtained an audience with a partner in the firm Manning, Maxwell and Moore. Joseph F. Engelberger, at that time, was chief of engineering in the aircraft products division at Manning, Maxwell and Moore in Stratford, Connecticut. Engelberger was very interested, and Devol agreed to license Manning, Maxwell and Moore his patent and some future patents in the field. However, Manning, Maxwell and Moore was sold that year and its aircraft division is to be closed.\n\nThis development prompted Engelberger to seek a backer to buy out the aircraft division and found one in Consolidated Diesel Electronic (Condec), which agreed to put up the financing for the continued development of the robot. This new Condec division was called Unimation Incorporated with Joseph Engelberger as its president.\n\nThe first Unimate prototypes were controlled by vacuum tubes used as digital switches though later versions used transistors. Further, the \"off-the-shelf\" parts available in the late 1950s, such as digital encoders, were not adequate for the Unimate's purpose, and as a result, with Devol's guidance and a team of skilled engineers, Unimation designed and machined practically every part in the first Unimates. Devol also invented a variety of new technologies, including a unique rotating drum memory system with data parity controls.\n\nIn 1960, Devol personally sold the first Unimate robot, which was shipped in 1961 to General Motors. GM first used the machine for die casting handling and spot welding. The first Unimate robot was installed at GM's Inland Fisher Guide Plant in Ewing Township, New Jersey in 1961 to lift hot pieces of metal from a die-casting machine and stack them. Soon companies such as Chrysler, Ford, and Fiat saw the necessity for large Unimate purchases.\n\nApproximately $5 million was spent to develop the first Unimate. In 1966, after many years of market surveys and field tests, full-scale production began in Connecticut. Unimation's first production robot was a materials handling robot and was soon followed by robots for welding and other applications.\n\nIn 1975, Unimation showed its first profit. In 1978, the PUMA (Programmable Universal Machine for Assembly) robot was developed by Unimation from Vicarm (Victor Scheinman) and with support from General Motors.\n\nIn 2005, Popular Mechanics magazine selected Devol's Unimate as one of the Top 50 Inventions of the Past 50 Years.\n\nDevol later obtained patents on visual and tactile sensors for robots, coaxial connectors, non-refillable containers, and magnetostrictive manipulators or \"micro robotics\", a field he created.\n\n\nDevol died on August 11, 2011, aged 99, at his home in Wilton, Connecticut. He was survived by two daughters, two sons, five grandchildren and five great-grandchildren. His grave is in Bald Hill Cemetery (Wilton, Ct).\n"}
{"id": "40346741", "url": "https://en.wikipedia.org/wiki?curid=40346741", "title": "Harvard-MIT Data Center", "text": "Harvard-MIT Data Center\n\nThe Harvard-MIT Data Center (HMDC) provides multi-disciplinary information technology support for social science research and education at Harvard and MIT. Established in the early 1960s the HMDC was meant to be the original data center for political and social science at Harvard University, and over time it has evolved into an information technology service provider that transcends many educational fields.\n\nThe HMDC offers the following services:\n\nIn the early 1960s the HMDC, originally known as the Government Data Center, was established as part of a national movement for all universities to collect, consolidate, and share social science research data. This movement eventually became known as the Interuniversity Consortium for Political and Social Research (ICPSR), the largest collection of social science data in the world. In the early days associates of the Government Data Center were responsible for managing the distribution of ICPSR tapes housed in Harvard's Office of Information Technology. In 1987 all holdings within the facility were transferred to the Faculty of Arts and Sciences Department of Government (located in Harvard's Littauer building) and in recognition of the widespread use of the facility's data by Harvard scholars the name was changed to the Harvard Data Center. At this time some of the earliest local computer networks, which contained statistical software and computing resources, were established; in addition, associates began transitioning the facility's holdings from tape to more modern media. In the early 1990s associates of the Harvard Data Center played a major role in a National Science Foundation (NSF) grant that established a research training program in political economy for various educational institutions. Later on, in 1996, facility associates entered into an agreement with MIT to extend services to MIT users, thus changing the name to the Harvard-MIT Data Center (HMDC). In 1999 HMDC associates were awarded a multimillion dollar grant by the NSF and five other funding agencies to create an open-source, digital library for quantitative research data; associates of the facility continue to receive additional grants and funding support from vendors, such as the NSF and the Library of Congress, to continue their research and development projects. In 2005, after the facility was transferred into Harvard's new Center for Government and International Studies complex, the HMDC became a founding member of the Institute for Quantitative Social Science (IQSS), and in 2007 HMDC associates launched their new online data center, the Dataverse Network repository. Today, the HMDC continues to serve the social science community by providing technology support for research, education, and administration.\n\n"}
{"id": "11733066", "url": "https://en.wikipedia.org/wiki?curid=11733066", "title": "Hidden innovation", "text": "Hidden innovation\n\nHidden innovationor Invisible innovation refers to innovation that is not captured or recognised by traditional indicators such as research and development (R&D) spending or number of patents. The term generally refers to innovation that takes place outside science & technology sectors, which are the primary sectors which invest in formal R&D and patents. For example, although technological innovations are often developed in the oil & gas sector through oil exploration activities, these innovations are unaccounted for in innovation metrics because oil exploration is not counted as formal R&D. Other types of innovation, including social innovation, can be classed as hidden innovation.\n\nAlthough originally coined in the 90s by Diana Hicks and Sylvan Katz in their research on the hidden research system involving hospital researchers, and Mike Hopkins research on genetic testing within the UK healthcare system, the concept of hidden innovation has most recently been promoted by NESTA, in their \"Innovation Gap\" Report, published October 2006. A later report, called \"Hidden innovation\", further expands on the concept and identifies four types of hidden innovation:\n\n\n"}
{"id": "48581239", "url": "https://en.wikipedia.org/wiki?curid=48581239", "title": "Holus", "text": "Holus\n\nHolus is a holographic product under development by H+Technology. The concept was first developed in 2013, before funding via Kickstarter meant the product could be taken to market. The purpose of Holus is to simulate holographic experiences and is technically different from typical hologram stickers found on credit cards and currency notes.\n\nHolus has been criticized by some commentators as a revamping Pepper's ghost, a 19th-century optical trick.\n\nHolus was developed in late 2013 by a team in Vancouver, British Columbia, Canada.\n\nShortly before H+ Tech began looking for funding for the device, Holus won a number of awards for its design. This included he Vancouver User Experience Award in the non-profit category for partnering up with Ronald McDonald House to build Magic Room and the Peoples Choice Award to achieve excellence in joy, elegance, and creativity.\n\nIts first major coverage came from a review by the Massachusetts Institute of Technology in early 2015. At the time, the technology was demonstrated to bring animals to life within the 3D glass box. The product was referred to in the review as roughly the \"size of a microwave\". The concept went on to win two awards at the NextBC awards in Canada in early 2015.\n\nIn order to build mass versions of the product, a Kickstarter campaign was launched in order to take the idea to market. It used a similar technology to the optical illusion known as Pepper's ghost. This drew criticism from some during its Kickstarter campaign. It launched its Kickstarter campaign in June 2015 and generated twice its target of $40,000 within the first 48 hours.\n\nThe technology is similar to the technology used to display the music artists Tupac Shakur and Michael Jackson. Since then the technology has advanced, with a number of startups entering the market. One of these was H+ Technology, who first began working on the technology in early 2013. The aim of the product at the time has remained the same until today, to produce 3D technology that can be used in the home on a tabletop.\n\nOther reviews likened it to the technology used in the Star Wars films when the character R2-D2 projected a hologram call on a small platform.\n\nDue to the technology being in its infancy, the media has covered the R&D of the product and its potential. Spatial light modulators have been mentioned as one potential development on future versions of Holus. The University of British Columbia and Simon Fraser University have both assisted with the research work of such displays.\n"}
{"id": "11240976", "url": "https://en.wikipedia.org/wiki?curid=11240976", "title": "Imperial Innovations", "text": "Imperial Innovations\n\nImperial Innovations is a UK technology commercialisation and investment company, based in London. Imperial Innovations is traded on the Alternative Investment Market of the London Stock Exchange.\n\nImperial Innovations was founded in 1986 as a department of Imperial College London, later becoming a wholly owned subsidiary of the College.\n\nIn July 2006, shares in the company were admitted to trading on the Alternative Investment Market of the London Stock Exchange. Since 2006, Imperial Innovations has raised over £300 million to invest in early-stage technology businesses.\n\nOn 4 November 2010 Prime Minister David Cameron revealed in a speech given in East London that Imperial Innovations had agreed to advise on the creation of an accelerator space for spinout companies at the Queen Elizabeth Olympic Park as part of the new East London Tech City hub.\n"}
{"id": "1809995", "url": "https://en.wikipedia.org/wiki?curid=1809995", "title": "Instant payment notification", "text": "Instant payment notification\n\nInstant payment notification (IPN) is a method for online retailers to automatically track purchases and other server-to-server communication in real time. This allows E-commerce systems the opportunity to store payment transactions, order information and other sales internally. IPN messages can represent payment success or failures, order transaction status changes, accounting ledger information and many others depending on the payment gateway. \n\nThe payments industry is an evolving market, technology like IPN and instant payment are now used in the retail market and in the domestic sphere, but they are expected to evolve into the corporate, B2B segment and cross-border space. \n\nIPN is used by merchant to automate backend functions related to: the end user account creation, order tracking, customer and merchant notifications related to acquired services.\nWhen an E-commerce system requests a resource from a payment gateway, like a new invoice or bill for goods, the request must contain a URL endpoint representing a script or program to handle returning notifications. IPN messages are then sent to the retailer's E-commerce system by HTTP POST as the resource is updated by the gateway. \n\nThe IPN handler usually performs standard actions like validating the message, updating inventory levels in the E-commerce system, notifying customers of successful or failed payments, etc. Depending on the retailer's business requirements and the level of sophistication of the E-commerce software, some or all of the IPN messages can be handled or ignored. \nServer-side scripting languages such as PHP and ASP that power most E-commerce systems are event driven and make no distinction between a user-generated event or a machine-generated event. Utilizing this fact, IPN messages facilitate the coordination of the order state changes between the ecommerce system and the payment gateway handling the order. \n\n"}
{"id": "1752490", "url": "https://en.wikipedia.org/wiki?curid=1752490", "title": "Instrument approach", "text": "Instrument approach\n\nIn aviation, an instrument approach or instrument approach procedure (IAP) is a series of predetermined maneuvers for the orderly transfer of an aircraft under instrument flight conditions from the beginning of the initial approach to a landing or to a point from which a landing may be made visually. These approaches are approved in the United States by the FAA or the United States Department of Defense for the military. The ICAO defines an instrument approach as a series of predetermined maneuvers by reference to flight instruments with specific protection from obstacles from the initial approach fix, or where applicable, from the beginning of a defined arrival route to a point from which a landing can be completed and thereafter, if landing is not completed, to a position at which holding or enroute obstacle clearance criteria apply.\n\nThere are three categories of instrument approach procedures: precision approach (PA), approach with vertical guidance (APV), and non-precision approach (NPA). A precision approach uses a navigation system that provides course and glidepath deviation. Examples include precision approach radar (PAR), instrument landing system (ILS), and GBAS landing system (GLS). An approach with vertical guidance also uses a navigation system for course and glidepath deviation, just not to the same standards as a PA. Examples include baro-VNAV, localizer type directional aid (LDA) with glidepath, LNAV/VNAV and LPV. A non-precision approach uses a navigation system for course deviation but does not provide glidepath information. These approaches include VOR, NDB and LNAV. PAs and APVs are flown to a decision height/altitude (DH/DA), while non-precision approaches are flown to a minimum descent altitude (MDA).\n\nIAP charts are aeronautical charts that portray the aeronautical data that is required to execute an instrument approach to an airport. Besides depicting topographic features, hazards and obstructions, they depict the procedures and airport diagram. Each procedure chart uses a specific type of electronic navigation system such as an NDB, TACAN, VOR, ILS/MLS and RNAV. The chart name reflects the primary navigational aid (NAVAID), if there is more than one straight-in procedure or if it is just a circling-only procedure. A communication strip on the chart lists frequencies in the order they are used. Minimum, maximum and mandatory altitudes are depicted in addition to the minimum safe altitude (MSA) for emergencies. A cross depicts the final approach fix (FAF) altitude on NPAs while a lightning bolt does the same for PAs. NPAs depict the MDA while a PA shows both the decision altitude (DA) and decision height (DH). Finally, the chart depicts the missed approach procedures in plan and profile view, besides listing the steps in sequence.\n\nBefore satellite navigation was available for civilian aviation, the requirement for large land-based navigation aid (NAVAID) facilities generally limited the use of instrument approaches to land-based (i.e. asphalt, gravel, turf, ice) runways (and those on aircraft carriers). GNSS technology allows, at least theoretically, to create instrument approaches to any point on the Earth's surface (whether on land or water); consequently, there are nowadays examples of water aerodromes (such as Rangeley Lake Seaplane Base in Maine, United States) that have GNSS-based approaches.\n\nAn instrument approach procedure may contain up to five separate segments, which depict course, distance, and minimum altitude. These segments are\n\n\nWhen an aircraft is under radar control, air traffic control (ATC) may replace some or all of these phases of the approach with radar vectors (ICAO radar vectoring is the provision of navigational guidance to aircraft in the form of specific headings, based on the use of radar). ATC will use an imaginary \"approach gate\" when vectoring aircraft to the final approach course. This gate will be 1 nautical mile (NM) from the FAF and at least 5 NM from the landing threshold. Outside radar environments, the instrument approach starts at the IAF.\n\nThough ground-based NAVAID approaches still exist, the FAA is transitioning to approaches which are satellite-based (RNAV). Additionally, in lieu of the published approach procedure, a flight may continue as an IFR flight to landing while increasing the efficiency of the arrival with either a contact or visual approach.\n\nA visual approach is an ATC authorization for an aircraft on an IFR flight plan to proceed visually to the airport of intended landing; it is not an instrument approach procedure.\n\nA visual approach may be requested by the pilot or offered by ATC. Visual approaches are possible when weather conditions permit continuous visual contact with the destination airport. They are issued in such weather conditions in order to expedite handling of IFR traffic. The ceiling must be reported or expected to be at least 1000 feet AGL (above ground level) and the visibility is at least 3 SM (statute miles).\n\nA pilot may accept a visual approach clearance as soon as the pilot has the destination airport in sight. According to ICAO Doc. 4444, it is enough for a pilot to see the terrain to accept a visual approach. The point is that if a pilot is familiar with the terrain in the vicinity of the airfield he/she may easily find the way to the airport having the surface in sight. \nATC must ensure that weather conditions at the airport are above certain minima (in the U.S., a ceiling of 1000 feet AGL or greater and visibility of at least 3 statute miles) before issuing the clearance. According to ICAO Doc. 4444, it is enough if the pilot reports that in his/her opinion the weather conditions allow a visual approach to be made. In general, the ATC gives the information about the weather but it's the pilot who makes a decision if the weather is suitable for landing. Once the pilot has accepted the clearance, he/she assumes responsibility for separation and wake turbulence avoidance and may navigate as necessary to complete the approach visually. According to ICAO Doc. 4444, ATC continues to provide separation between the aircraft making a visual approach and other arriving and departing aircraft. The pilot may get responsible for the separation with preceding aircraft in case he/she has the preceding aircraft in sight and is instructed so by ATC.\nIn the United States, it is required that an aircraft have the airport, the runway, or the preceding aircraft in sight. It is not enough to have the terrain in sight (see #Contact approach).\n\nWhen a pilot accepts a visual approach, the pilot accepts responsibility for establishing a safe landing interval behind the preceding aircraft, as well as responsibility for wake-turbulence avoidance, and to remain clear of clouds.\n\nA contact approach that may be asked for by the pilot (but not offered by ATC) in which the pilot has 1 SM flight visibility and is clear of clouds and is expected to be able to maintain those conditions all the way to the airport. Obstruction clearances and VFR traffic avoidance become the pilot's responsibility.\n\nA visual approach that has a specified route the aircraft is to follow to the airport. Pilots must have a charted visual landmark or a preceding aircraft in sight, and weather must be at or above the published minimums. Pilots are responsible for maintaining a safe approach interval and wake turbulence separation.\n\nThese approaches include both ground-based and satellite-based systems and include criteria for terminal areas (TAAs), basic approach criteria, and final approach criteria. The TAA is a transition from the en route structure to the terminal environment which provides minimum altitudes for obstacle clearance. The TAA is a \"T\" or \"basic T\" design with left and right base leg IAFs on initial approach segments perpendicular to the intermediate approach segment where there is a dual purpose IF/IAF for a straight-in procedure (no procedure turn [NoPT]), or hold-in-lieu-of procedure-turn (HILO) course reversal. The base leg IAFs is 3 to 6 NM from the IF/IAF. The basic-T is aligned with the runway centerline, with the IF 5 NM from the FAF, and the FAF is 5 NM from the threshold.\n\nThe RNAV approach chart will have four lines of approach minimums corresponding to LPV, LNAV/VNAV, LNAV, and Circling. This allows GPS or WAAS equipped aircraft to use the LNAV MDA using GPS only, if WAAS becomes unavailable.\n\nThese are the most precise and accurate approaches. A runway with an ILS can accommodate 29 arrivals per hour. ILS systems on two or three runways increase capacity with parallel (dependent) ILS, simultaneous parallel (independent) ILS, precision runway monitor (PRM), and converging ILS approaches. ILS approaches have three classifications, CAT I, CAT II, and CAT III. CAT II and CAT III require additional certification for operators, pilots, aircraft and equipment, with CAT III used mainly by air carriers and the military. Simultaneous parallel approaches require runway centerlines to be between 4,300 and 9,000 feet apart, plus a \"dedicated final monitor controller\" to monitor aircraft separation. Simultaneous close parallel (independent) PRM approaches must have runways separation to be between 3,400 and 4,300 feet. Simultaneous offset instrument approaches (SOIAs) apply to runways separated by 750–3,000 feet. A SOIA uses an ILS/PRM on one runway and an LDA/PRM with glideslope for the other.\n\nThese approaches use VOR facilities on and off the airport and may be supplemented with DME and TACAN.\n\nThese approaches use NDB facilities on and off the airport and may be supplemented with a DME. These approaches are gradually being phased out.\n\nThis will be either a precision approach radar (PAR) or an airport surveillance radar (ASR) approach. Information is published in tabular form. The PAR provides vertical and lateral guidance plus range. The ASR only provides heading and range information.\n\nThese approaches include a localizer approach, localizer/DME approach, localizer back course approach, and a localizer-type directional aid (LDA). In cases where an ILS is installed, a back course may be available in conjunction with the localizer. Reverse sensing occurs on the back course using standard VOR equipment. With a horizontal situation indicator (HSI) system, reverse sensing is eliminated if it is set appropriately to the front course.\n\nThis type of approach is similar to the ILS localizer approach, but with less precise guidance.\n\nNon-precision systems provide lateral guidance (that is, heading information), but do not provide vertical guidance (i.e., altitude and/or glidepath guidance).\n\nPrecision approach systems provide both lateral (heading) and vertical (glidepath) guidance.\n\nICAO defines decision altitude/decision height as a specified altitude or height (A/H) in the precision approach at which a missed approach must be initiated if the required visual reference to continue the approach has not been established. A decision height (DH) or decision altitude (DA) is a specified lowest height or altitude in the approach descent at which, if the required visual reference to continue the approach (such as the runway markings or runway environment) is not visible to the pilot, the pilot must initiate a missed approach. The specific values for DH and/or DA at a given airport are established with intention to allow a pilot sufficient time to safely re-configure an aircraft to climb and execute the missed approach procedures while avoiding terrain and obstacles. A DH/DA denotes the altitude in which a missed approach procedure must be started, it does not preclude the aircraft from descending below the prescribed DH/DA. A decision height is measured AGL (above ground level) while a decision altitude is measured above MSL (mean sea level). They are used for precision approaches.\n\nThe minimum descent altitude (MDA) is the lowest altitude, expressed in feet above mean sea level, to which descent is authorized on final approach or during circle-to-land maneuvering in execution of a standard instrument approach procedure where no electronic glideslope is provided. Unlike with DH or DA, a missed approach need not be initiated immediately upon reaching the altitude. A pilot flying a non-precision approach may descend to the MDA and maintain it until reaching the missed approach point (MAP), then initiate a missed approach if the required visual reference was not obtained. An aircraft must not descend below the MDA until visual reference is obtained, and which the aircraft can land while performing normal maneuvers, which differs slightly from a DH/DA in that while the missed approach procedure must be initiated at or prior to the DH/DA, because of its vertical momentum, during a precision approach an aircraft may end up descending slightly below the DH/DA during the course of the missed approach.\n\nIf a runway has both precision and non-precision approaches defined, the MDA of the non-precision approach is almost always greater than the DH/DA of the precision approach, because of the lack of vertical guidance on the non-precision approach: the actual difference depends on the accuracy of the navaid upon which the approach is based, with ADF approaches and SRAs tending to have the highest MDAs.\n\nAn instrument approach wherein final approach is begun without first having executed a procedure turn, not necessarily completed with a straight-in landing or made to straight-in landing minimums. A direct instrument approach requires no procedure turn or any other course reversal procedures for alignment (usually indicated by \"NoPT\" on approach plates), as the arrival direction and the final approach course are not too different from each other. The direct approach can be finished with a straight-in landing or circle-to-land procedure.\n\nSome approach procedures do not permit straight-in approaches unless the pilots are being radar vectored. In these situations, pilots are required to complete a procedure turn (PT) or other course reversal, generally within 10 NM of the PT fix, to establish the aircraft inbound on the intermediate or final approach segment. When conducting any type of approach, if the aircraft is not lined up for a straight-in approach, then a course reversal might be necessary. The idea of a course reversal is to allow sufficiently large changes in the course flown (in order to line the aircraft up with the final approach course), without taking too much space horizontally and while remaining within the confines of protected airspace. This is accomplished in one of three ways: a procedure turn, a holding pattern, or a teardrop course reversal.\n\nCircle-to-land is a maneuver initiated by the pilot to align the aircraft with a runway for landing when a straight-in landing from an instrument approach is not possible or is not desirable, and only after ATC authorization has been obtained and the pilot has established and maintains required visual reference to the airport. A circle-to-land maneuver is an alternative to a straight-in landing. It is a maneuver used when a runway is not aligned within 30 degrees of the final approach course of the instrument approach procedure or the final approach requires 400 feet (or more) of descent per nautical mile, and therefore requires some visual maneuvering of the aircraft in the vicinity of the airport after the instrument portion of the approach is completed to align the aircraft with the runway for landing.\n\nIt is very common for a circle-to-land maneuver to be executed during a straight-in approach to a different runway, e.g., an ILS approach to one runway, followed by a low-altitude transition, ending in a landing on another (not necessarily parallel) runway. This way, approach procedures to one runway can be used to land on any runway at the airport, as the other runways might lack instrument procedures or their approaches cannot be used for other reasons (traffic considerations, navigation aids being out of service, etc.).\n\nCircling to land is considered more difficult and less safe than a straight-in landing, especially under instrument meteorological conditions because the aircraft is at a low altitude and must remain within a short distance from the airport in order to be assured of obstacle clearance (often within a couple of miles, even for faster aircraft). The pilot must maintain visual contact with the airport at all times; loss of visual contact requires execution of a missed approach procedure.\n\nPilots should be aware that there are significant differences in obstacle clearance criteria between procedures designed in accordance with ICAO PANS-OPS and US TERPS. This is especially true in respect of circling approaches where the assumed radius of turn and minimum obstacle clearance are markedly different.\n\nA visual maneuver by a pilot performed at the completion of an instrument approach to permit a straight-in landing on a parallel runway not more than 1,200 feet to either side of the runway to which the instrument approach was conducted.\n\nA useful formula pilots use to calculate descent rates (for the standard 3° glide slope):\n\nor\nFor other glideslope angles:\nwhere rate of descent is in feet per minute, and ground speed is in knots.\n\nThe latter replaces \"tan α\" (see below) with \"α/60\", which has an error of about 5% up to 10°.\n\nExample:\n\nThe simplified formulas above are based on a trigonometric calculation:\n\nwhere:\n\nExample:\n\nSpecial considerations for low visibility operations include improved lighting for the approach area, runways, and taxiways, and the location of emergency equipment. There must be redundant electrical systems so that in the event of a power failure, the back-up takes over operation of the required airport instrumentation (e.g., the ILS and lighting). ILS critical areas must be free from other aircraft and vehicles to avoid multipathing.\n\nIn the United States, the requirements and the standards for establishing instrument approaches at an airport are contained in the FAA Order 8260.3 \"United States Standard for Terminal Instrument Procedures (TERPS)\". ICAO publishes requirements in the ICAO Doc 8168 \"Procedures for Air Navigation Services – Aircraft Operations (PANS-OPS), Volume II: Construction of Visual and Instrument Flight Procedures\".\n\nMountain airports such as Reno–Tahoe International Airport (KRNO) offer significantly different instrument approaches for aircraft landing on the same runway, but from opposite directions. Aircraft approaching from the north must make visual contact with the airport at a higher altitude than a flight approaching from the south, because of rapidly rising terrain south of the airport. This higher altitude allows a flight crew to clear the obstacle if a landing is not feasible. In general, each specific instrument approach specifies the minimum weather conditions that must be present in order for the landing to be made.\n\n\n\n\n"}
{"id": "24002209", "url": "https://en.wikipedia.org/wiki?curid=24002209", "title": "Kamal Quadir", "text": "Kamal Quadir\n\nKamal S. Quadir is a Bangladeshi American entrepreneur and artist best known for introducing e-commerce in Bangladesh by founding CellBazaar, an electronic marketplace which, after reaching 4 million users, was acquired by Norwegian telecommunications operator Telenor in 2010. CellBazaar later was rebranded as ekhanei.com.\n\nQuadir is currently heading the company bKash, which provides financial services through a network of community-based agents and existing technology, including mobile phones. bKash is world’s second largest and fastest growing mobile financial services company.\n\nQuadir is a founding member of Open World Initiatives, a Lausanne, Switzerland-based organization of young thinkers. He is involved with Anwarul Quadir Foundation which recognises innovations in developing countries. He is a First Mover Fellow of The Aspen Institute. In 2009, TED selected Quadir a TED Fellow and the World Economic Forum recognised him as a Young Global Leader.\n\nQuadir was an intern at Insight Venture Partners in New York, led the Business Development Division of Occidental Petroleum's initiative in Bangladesh and worked for New York City's Chamber of Commerce. He was also the co-founder and creative director of \"GlobeKids Inc.\", an animation company.\n\nQuadir has a BA from Oberlin College and an MBA from the MIT Sloan School of Management.\n\nHe is also an artist whose art works are in the permanent collection of the Bangladesh National Museum and the Liberation War Museum.\n\n\n\n"}
{"id": "57347026", "url": "https://en.wikipedia.org/wiki?curid=57347026", "title": "Love chair", "text": "Love chair\n\nThe Love chair (French Siege d'amour) was a device created by a French furniture manufacturer in the early 20th century to allow the corpulent British King Edward VII to have sex with two or more women simultaneously.\n\nThe chair was first created by the Soubrier furniture manufacturer, which has now (as of 2018) become the current owner of the device. The King's waist measured 48 inches (122 cm) shortly before his coronation at which point the practice of visiting brothels may have stopped. Prior to this, however, the King was a regular visitor at \"Le Chabanais\" in Paris, a French brothel, and the chair was designed specifically for the King's visits there. The chair allowed him to indulge his sexual fantasies without crushing his female partners.\n"}
{"id": "747715", "url": "https://en.wikipedia.org/wiki?curid=747715", "title": "Lynn Conway", "text": "Lynn Conway\n\nLynn Ann Conway (born January 2, 1938) is an American computer scientist, electrical engineer, inventor, and transgender activist.\n\nConway is notable for a number of pioneering achievements, including the Mead & Conway revolution in VLSI design, which incubated an emerging electronic design automation industry. She worked at IBM in the 1960s and is credited with the invention of generalized dynamic instruction handling, a key advance used in out-of-order execution, used by most modern computer processors to improve performance.\n\nConway grew up in White Plains, New York. Conway was shy and experienced gender dysphoria as a child. She became fascinated and engaged by astronomy (building a reflector telescope one summer) and did well in math and science in high school. Conway entered MIT in 1955, earning high grades but ultimately leaving in despair after an attempted gender transition in 1957–58 failed due to the medical climate at the time. After working as an electronics technician for several years, Conway resumed education at Columbia University's School of Engineering and Applied Science, earning B.S. and M.S.E.E. degrees in 1962 and 1963.\n\nConway was recruited by IBM Research in Yorktown Heights, New York in 1964, and was soon selected to join the architecture team designing an advanced supercomputer, working alongside John Cocke, Herbert Schorr, Ed Sussenguth, Fran Allen and other IBM researchers on the Advanced Computing Systems (ACS) project, inventing multiple-issue out-of-order dynamic instruction scheduling while working there. The Computer History Museum has stated that \"the ACS machines appears to have been the first superscalar design, a computer architectural paradigm widely exploited in modern high-performance microprocessors.\"\n\nAfter learning of the pioneering research of Harry Benjamin in treating transsexuals and realising that genital affirmation surgery was now possible, Conway sought his help and became his patient. After suffering from severe depression from gender dysphoria, Conway contacted Benjamin, who agreed to providing counseling and prescribe hormones. Under Benjamin's care, Conway began her gender transition.\n\nWhile struggling with life in a male role, Conway had been married to a woman and had two children. Under the legal constraints then in place, after transitioning she was denied access to their children.\n\nAlthough she had hoped to be allowed to transition on the job, IBM fired Conway in 1968 after she revealed her intention to transition to a female gender role.\n\nUpon completing her transition in 1968, Conway took a new name and identity, and restarted her career in \"stealth-mode\" as a contract programmer at Computer Applications, Inc. She went on to work at Memorex during 1969–1972 as a digital system designer and computer architect.\n\nConway joined Xerox PARC in 1973, where she led the \"LSI Systems\" group under Bert Sutherland. Collaborating with Carver Mead of Caltech on VLSI design methodology, she co-authored \"Introduction to VLSI Systems\", a groundbreaking work that would soon become a standard textbook in chip design, used in over 100 universities by 1983. The book and early courses were the beginning of the Mead & Conway revolution in VLSI system design.\n\nIn 1978, Conway served as visiting associate professor of EECS at MIT, teaching a now famous VLSI design course based on a draft of the Mead–Conway text. The course validated the new design methods and textbook, and established the syllabus and instructor's guidebook used in later courses all around the world.\n\nAmong Conway's contributions were invention of dimensionless, scalable design rules that greatly simplified chip design and design tools, and invention of a new form of internet-based infrastructure for rapid-prototyping and short-run fabrication of large numbers of chip designs. The new infrastructure was institutionalized as the MOSIS system in 1981. Since then, MOSIS has fabricated more than 50,000 circuit designs for commercial firms, government agencies, and research and educational institutions around the world. Prominent VLSI researcher Charles Seitz commented that \"MOSIS represented the first period since the pioneering work of Eckert and Mauchley on the ENIAC in the late 1940s that universities and small companies had access to state-of-the-art digital technology.\"\n\nThe research methods used to develop the Mead–Conway VLSI design methodology and the MOSIS prototype are documented in a 1981 Xerox report and the Euromicro Journal. The impact of the Mead–Conway work is described and time-lined in a number of historical overviews of computing. Conway and her colleagues have compiled an online archive of original papers that documents much of that work.\n\nIn the early 1980s, Conway left Xerox to join DARPA, where she was a key architect of the Defense Department's Strategic Computing Initiative, a research program studying high-performance computing, autonomous systems technology, and intelligent weapons technology.\n\nIn a \"USA Today\" article about Conway's joining DARPA, Mark Stefik, a Xerox scientist who worked with her, said \"Lynn would like to live five lives in the course of one life\" and that she's \"charismatic and very energetic\". Douglas Fairbairn, a former Xerox associate, said \"She figures out a way so that everybody wins.\"\n\nAs sociologist Thomas Streeter discusses in The Net Effect: \"By taking this job, Conway was demonstrating that she was no antiwar liberal. (In response to critics, she has said, 'if you have to fight, and sometimes you must in order to deal with bad people, history tells us that it really helps to have the best weapons available)\". But Conway carried a sense of computers as tools for horizontal communications that she had absorbed at PARC right into DARPA - at one of the hottest moments of the cold war.\"\n\nConway joined the University of Michigan in 1985 as professor of electrical engineering and computer science, and associate dean of engineering. There she worked on \"visual communications and control probing for basic system and user-interface concepts as applicable to hybridized internet/broadband-cable communications\". She retired from active teaching and research in 1998, as professor emerita at Michigan.\n\nIn the fall of 2012, the IEEE published a special issue of the \"IEEE Solid-State Circuits Magazine\" devoted to Lynn Conway's career, including a career memoir by Lynn and peer commentaries by Chuck House, former Director of Engineering at HP, Carlo Séquin, Professor of EECS at U.C. Berkeley, and Ken Shepard, Professor of Electrical Engineering and Biomedical Engineering at Columbia University.\n\n\"Clearly a new paradigm had emerged . . . Importantly, imaginative support in terms of infrastructure and idea dissemination proved as valuable as the concepts, tools, and chips. The \"electronic book\" and the \"foundry\" were both prescient and necessary, providing momentum and proof-points.\" Jim Gibbons, former Dean of Engineering at Stanford University, further states that Lynn Conway, from his perspective, \"...was the singular force behind the entire \"foundry\" development that emerged.\" Ken Shepard stated that \"Lynn's amazing story of accomplishment and personal triumph in the face of personal adversity and overt discrimination should serve as an inspiration to all young engineers.\"\n\nWhen nearing retirement, Conway learned that the story of her early work at IBM might soon be revealed through the investigations of Mark Smotherman that were being prepared for a 2001 publication. She began quietly coming out as a trans woman in 1999 to friends and colleagues about her past gender transition, using her personal website to tell the story in her own words. Her story was then more widely reported in 2000 in profiles in \"Scientific American\" and the \"Los Angeles Times\".\n\nAfter going public with her story, she began work in transgender activism, intending to \"illuminate and normalize the issues of gender identity and the processes of gender transition\". She has worked to protect and expand the rights of transgender people. She has provided direct and indirect assistance to numerous other transgender women going through transition and maintains a well-known website providing emotional and medical resources and advice. Parts have been translated into most of the world's major languages. She maintains a listing of many successful post-transition trangender people, to, in her words \"provide role models for individuals who are facing gender transition\". Her website also provides current news related to transgender issues and information on sex reassignment surgery for transsexual women, facial feminization surgery, academic inquiries into the prevalence of transsexualism and transgender and transsexual issues in general.\n\nShe has also strongly advocated for equal opportunities and employment protections for transgender people in high-technology industry, and for elimination of the pathologization of transgender people by the psychiatric community.\n\nConway has been a prominent critic of the Blanchard, Bailey, and Lawrence theory of male-to-female transsexualism that all trans women are motivated either by feminine homosexuality or autogynephilia. She was also a key person in the campaign against J. Michael Bailey's book \"The Man Who Would Be Queen\". Conway and McCloskey wrote letters to Northwestern University, accusing Bailey of \"conducting intimate research observations on human subjects without telling them that they were objects of the study.\" American bioethicist Alice Dreger in her book \"Galilieo's Middle Finger\" criticized Conway for filing a lawsuit against Bailey which had \"no legal basis,\" referring to her allegation that Bailey lacked a license as a clinical psychologist when he wrote letters in support of a young transwoman seeking to transition. According to Dreger, as Bailey did not receive compensation for his services, he would not have needed a license in Illinois, and was \"completely forthright in his letters supporting the women, both about the fact that he had only had brief conversations with them (as opposed to having provided them with extensive counseling) and about his own qualifications and expertise...[and] even attached copies of his CV.\" As Dreger argues, \"presumably all this was why [Illinois] never bothered to pursue the charge.\" In response, Conway argued that Dreger \"deflects attention away from Bailey's book and the massive trans community protest, and caricatures the entire controversy as nothing more than a vicious effort by three rather witch-like women to 'ruin the life' of a brilliant scientist. In doing so, she stoops to new lows as a dirty-trickster by misquoting sources, exploiting sleazy innuendos and fabricating entire story-episodes in order to defame the three women.\"\n\nConway was a cast member in the first all-transgender performance of \"The Vagina Monologues\" in Los Angeles in 2004, and appeared in a LOGO-Channel documentary film about that event entitled \"Beautiful Daughters.\"\n\nIn 2009, Conway was named one of the \"Stonewall 40 trans heroes\" on the 40th anniversary of the Stonewall riots by the International Court System, one of the oldest and largest predominantly gay organizations in the world, and the National Gay and Lesbian Task Force.\n\nIn 2013, with support from many prominent thought-leaders in high-technology, Conway and her colleague Leandra Vicci of the University of North Carolina at Chapel Hill successfully lobbied the Board of Directors of the Institute of Electrical and Electronic Engineers (IEEE) for transgender inclusion in the IEEE's Code of Ethics. That Code, known within the profession as much as a code of honor as one of ethics, became fully LGBT inclusive in January 2014, thus impacting the world's largest engineering professional society, with 425,000 members in 160 countries. In 2014, \"Time Magazine\" named Lynn as one of \"21 Transgender People Who Influenced American Culture.\" In 2015 she was selected for inclusion in \"The Trans100\".\n\nIn 1987, Conway met her husband Charles \"Charlie\" Rogers, a professional engineer who shares her interest in the outdoors, including whitewater canoeing and motocross racing. They soon started living together, and bought a house with of meadow, marsh, and woodland in rural Michigan in 1994. On August 13, 2002, they were married. In 2014, the University of Michigan's \"The Michigan Engineer\" alumni magazine documented the connections between Conway's engineering explorations and the adventures in her personal life.\n\nConway has received a number of awards and distinctions:\n\n\n"}
{"id": "7822632", "url": "https://en.wikipedia.org/wiki?curid=7822632", "title": "MTV-1", "text": "MTV-1\n\nThe MTV-1 Micro TV was the second model of a near pocket-sized television. The first was the Panasonic IC model TR-001 introduced in 1970. The MTV-1 was developed by Clive Sinclair (Sinclair Radionics Ltd). It was shown to the public at trade shows in London and Chicago in January, 1977, and released for sale in 1978. Development spanned 10 years and included a cash infusion of (about ) from the UK government in 1976.\n\nThe MTV-1 used a German AEG Telefunken black-and-white, electrostatic cathode ray tube (CRT), the smallest CRT built into a commercially available product, and included a rechargeable 4-AA-cell NiCad battery pack. It measured and weighed . It was able to receive either PAL or NTSC transmissions on VHF or UHF. A Welsh company, Wolsey Electronics, manufactured it for Sinclair. Custom ICs made by Texas Instruments and Sinclair contributed to its small size and low power consumption.\n\nThe original (about ) price tag proved to be too high to sell many of them, and Sinclair lost over in 1978, eventually selling its remaining inventory to liquidators at greatly reduced prices.\n\nThe MTV-1B, released later in 1978 at the much lower price of , was able to receive only British and South African UHF PAL signals.\n\n\n"}
{"id": "57214851", "url": "https://en.wikipedia.org/wiki?curid=57214851", "title": "Manijeh Razeghi", "text": "Manijeh Razeghi\n\nManijeh Razeghi is an Iranian-american scientist. A pioneer in the field of semiconductors and optoelectronic devices, her work at the core of 20th century optical fiber telecommunications resulted in the development of new technologies.\n\nIn 1991, she moved to the US to become the Walter P. Murphy Professor and Director of the Center for Quantum Devices, Department of Electrical Engineering and Computer Science, Northwestern University. In 2018, Razeghi won the Ben Franklin Medal for Electrical Engineering for \"for the realization of high-power terahertz frequency sources operating at room temperature using specially designed and manufactured semiconductor lasers, which enables a new generation of imagers, chemical/biological sensors, and ultra-broadband wireless communication systems.\" She developed lasers that can detect explosives and pathogens as well as electronic devices that will eventually deliver turbo-charged, super-fast WiFi.\n\nShe holds 55 patents and has published 18 books and more than 1000 papers.\n\n"}
{"id": "907134", "url": "https://en.wikipedia.org/wiki?curid=907134", "title": "Microwave Data Systems", "text": "Microwave Data Systems\n\nMicrowave Data Systems, Inc. (MDS) is a company that makes wireless communications equipment for the industrial market. MDS was a subsidiary of Moseley Associates and was acquired, in January 2007, by General Electric Multilin, becoming known as GE MDS, LLC.\n\nProducts include \"point-to-point\" and \"point-to-multipoint\" radios that operate in the licensed and unlicensed bands. The company is known particularly for long range communications, interference rejection, industrial quality, and high reliability.\n\n\n\n"}
{"id": "1641090", "url": "https://en.wikipedia.org/wiki?curid=1641090", "title": "NaPTAN", "text": "NaPTAN\n\nThe National Public Transport Access Node (NaPTAN) database is a UK nationwide system for uniquely identifying all the points of access to public transport in the UK. The database is closely associated with the National Public Transport Gazetteer.\n\nEvery UK railway station, coach terminus, airport, ferry terminal, bus stop, taxi rank or other place where public transport can be joined or left is allocated a unique NaPTAN identifier. The relationship of the stop to a City, Town, Village or other locality can be indicated through an association with elements of the National Public Transport Gazetteer.\n\nThere is a CEN standardisation initiative, Identification of Fixed Objects In Public Transport ('IFOPT'), to develop NaPTAN concepts into a European standard for stop identification as an extension to Transmodel, the European standard for Public Transport information.\n\nThe ability to identify and locate stops in relation to topography, both consistently and economically, is fundamental to modern computer based systems that provide passenger information and manage public transport networks. Stop data is needed by journey planners, scheduling systems, real-time systems, for transport planning, performance monitoring, and for many other purposes. Digitalising a nation's public transport stops is an essential step in creating a national information infrastructure.\n\nIn the UK NaPTAN has enabled the creation of the Transport Direct Portal, a UK nationwide system for multi-modal journey planning. NaPTAN also underpins TransXChange, the UK standard for bus schedules, which is used for the Electronic Registration of Bus Services.\n\nNaPTAN comprises several distinct elements \n\nNaPTAN identfiers are designed to be used within the UK's Digital National Framework a system of unique persistent reference for shareable information resources of all types managed by the Ordnance Survey.\n\nNaPTAN includes on a related standard - the UK National Public Transport Gazetteer.\n\nThe National Public Transport Access Node database holds a current copy of all UK stops. Stops are submitted by PTEs to a central authority which consolidates the stops and distributes them back to users. There are some 450,000 stop points in the current database.\n\nThe NaPTAN database is maintained centrally under contract to the Department of Transport.\n\nNaPTAN data is described by a NaPTAN XML Schema. This can be used to describe NaPTAN data when exchanging it between systems as XML documents. It is versioned so that different generations of data can be managed. See http://www.dft.gov.uk/naptan/schema/2.4/NaPTAN.xsd\n\nThe NPTG & NaPTAN data conforms to a family of consistent, interlocking data models. The models are described in the NPTG & NaPTAN Schema Guide in UML notation.\n\nNaPTAN identifiers are a systematic way of identifying of all UK points of access to public transport or \"Stop points\").\n\n\nNaPTAN stop points have a number of text descriptor elements associated with them: not just a name, but also additional labels and distinguishing identifiers that will help users to recognise them. These elements can be combined in different ways to provide presentations of names useful for many different contexts, for example on maps, stop finders, timetables etc., and on mobile devices\n\n\nThe Purpose of these descriptors is to create an iterative level of detail i.e. \"Country - County - Locality - Street - Name - Identifier\". All of this information should be included but it is up to the user of the data to decide how much data is relevant for the task in hand.\n\nEvery NaPTAN point includes geospatial coordinates specified in both Ordnance Survey National Grid format and as WGS84 latitide and longitude pairs. This allows NaPTAN points to be projected on maps and to be associated with other information layers such as the Integrated Transport Network of the Ordnance Survey.\n\nThe National Public Transport Gazetteer is closely associated with the NaPTAN dataset and contains details of every City, Town, Village, suburb in Great Britain (i.e., UK but not including Northern Ireland). This dataset is based on usage of names, rather than legal definitions and so includes local informal names for places as well as their official names.\n\n\n\n"}
{"id": "1344213", "url": "https://en.wikipedia.org/wiki?curid=1344213", "title": "Nephoscope", "text": "Nephoscope\n\nNephoscope is an instrument for measuring the altitude, direction, and velocity of clouds. \nThere are several types of nephoscopes:\n\nAlso, a type of nephoscope was invented by Mikhail Pomortsev in 1894 in Russia.\n\nThe nephoscope is used as a grid to measure the clouds.It is also used to find the movements of clouds.\n\nA ray of light of known velocity is emitted from nephoscope, which will strike with the base of the clouds. The travel time of the receiving signal is used to get the height of the clouds. As\ns = vt\nt1+ t2/ 2\n\n"}
{"id": "19186857", "url": "https://en.wikipedia.org/wiki?curid=19186857", "title": "OSAMI-D", "text": "OSAMI-D\n\nThe research project OSAMI-D is the German subproject of the European ITEA \"2\" project OSAMI (Open Source AMbient Intelligence).\n\nThe aim of the international project OSAMI is the design of a basic, widely applicable SOA-oriented component platform, its development, test and its provision as open source software. The project consists of a number of national subprojects, each focussing on a certain field of application. The German subproject OSAMI-D, funded by the BMBF, the German ministry of education and research, under reference number 01 IS 08003, contributes to the e-Health domain. The main objectives are interoperability, maintainability, reliability, as well as automated configuration and management of medical devices and services to provide new forms of healthcare to diseased and convalescent people. The advantages of these technical contributions will be demonstrated by means of an e-Health application which supports ambulant cardiologic rehabilitation.<br>\nThe software component platform specified by the OSGi Alliance forms the technical basis of the OSAMI platform. It provides lifecycle management for software components as well as local service interactions as defined in service-oriented architectures and will be combined with the Web Services approach, in particular DPWS / WS4D in order to implement distributed, dynamically configurable, vendor-neutral and device-independent solutions.\n\nThe main objective of OSAMI is to connect technologically vertical markets on the basis of open source components and, hence, to facilitate the market entry for small and medium-sized enterprises (SME). \n\n\n\n\n"}
{"id": "591288", "url": "https://en.wikipedia.org/wiki?curid=591288", "title": "Old Father Time", "text": "Old Father Time\n\nFather Time (commonly known as Old Father Time) is a weather vane at Lord's Cricket Ground, London, in the shape of Father Time removing the bails from a wicket. The weathervane is a total of tall, with the figure of Father Time standing at . It was given to Lord's in 1926 by the architect of the Grandstand, Sir Herbert Baker.\n\nAlthough frequently referred to as 'Old' Father Time in television and radio broadcasts, 'Old' is not part of the official title.\n\nThe symbolism of the figure derives from Law 16(3) of the Laws of Cricket: \"After the call of Time, the bails shall be removed from both wickets.\"\n\nOld Father Time was originally located atop the old Grand Stand. It was wrenched from its perch during the Blitz, when it became entangled in the steel cable of a barrage balloon, but was repaired and returned to its previous position. In 1992 the weather vane was struck by lightning, and the subsequent repairs were featured on children's television programme \"Blue Peter.\" Old Father Time was permanently relocated to the Mound Stand in 1996, when the Grand Stand was demolished and rebuilt.\n\nFather Time was again damaged in March 2015 by the high winds of Cyclone Niklas which required extensive repair by specialists. \n\nIn 1969 Old Father Time became the subject of a poem, \"Lord's Test\", by the Sussex and England cricketer John Snow.\n\n"}
{"id": "1218564", "url": "https://en.wikipedia.org/wiki?curid=1218564", "title": "Paratransit", "text": "Paratransit\n\nParatransit is recognized in North America as special transportation services for people with disabilities, often provided as a supplement to fixed-route bus and rail systems by public transit agencies. Paratransit services may vary considerably on the degree of flexibility they provide their customers. At their simplest they may consist of a taxi or small bus that will run along a more or less defined route and then stop to pick up or discharge passengers on request. At the other end of the spectrum—fully demand responsive transport—the most flexible paratransit systems offer on-demand call-up door-to-door service from any origin to any destination in a service area. In addition to public transit agencies, Paratransit services are operated by community groups or not-for-profit organizations, and for-profit private companies or operators.\n\nTypically, minibuses are used to provide paratransit service. Most paratransit vehicles are equipped with wheelchair lifts or ramps to facilitate access.\n\nIn the United States, private transportation companies often provide paratransit service in cities and metropolitan areas under contract to local public transportation agencies. Transdev, First Transit and MV Transportation are among the largest private contractors of paratransit services in the United States and Canada.\n\n\"Definition: any type of public transportation that is distinct from conventional transit, such as flexibly scheduled and routed services such as airport limousines, carpools, etc.\nEtymology: para- 'alongside of' + transit\"\nThe use of \"paratransit\" (\"para transit\", \"para-transit\") has evolved and taken on two somewhat separate broad sets of meaning and application.\n\nThe more general meaning involved projects starting in the early 1970s, documented by the Urban Institute in the 1974 book \"Para-transit: Neglected options for urban mobility\", followed a year later by the first international overview, \"Paratransit: Survey of International Experience and Prospects\". Robert Cervero's 1997 book, \"Paratransit in America: Redefining Mass Transportation\", embraced this wider definition of paratransit, arguing that America's mass transit sector should enlarge to include micro-vehicles, minibuses, and shared-taxi services found in many developing cities. Paratransit, as an alternative mode of flexible passenger transportation that does not follow fixed routes or schedules, are common and often offer the only mechanized mobility options for the poor in many parts of the developing world.\n\nSince the early 1980s, particularly in North America, the term began to be used increasingly to describe the second meaning: special transport services for people with disabilities. In this respect, paratransit has become a subsector and business in its own right.\n\nThe term \"paratransit\" is rarely used outside of North America.\n\nIn 2013, the Canadian Urban Transit Association compared the eligibility requirements of paratransit services in Canada and the United States .\n\nAnnually, the Canadian Urban Transit Association publishes a fact book providing statistics for all of the Ontario specialized public transit services as of 2015 there were 79 in operation .\n\nBefore passage of the Americans with Disabilities Act of 1990 (ADA), paratransit was provided by not-for-profit human service agencies and public transit agencies in response to the requirements in Section 504 of the Rehabilitation Act of 1973. Section 504 prohibited the exclusion of the disabled from \"any program or activity receiving federal financial assistance\". In Title 49 Part 37 (49 CFR 37) of the Code of Federal Regulations, the Federal Transit Administration defined requirements for making buses accessible or providing complementary paratransit services within public transit service areas.\n\nMost transit agencies did not see fixed route accessibility as desirable and opted for a flexible system of small paratransit vehicles operating parallel to a system of larger, fixed-route buses. The expectation was that the paratransit services would not be heavily used, making a flexible system of small vehicles a less expensive alternative for accessibility than options with larger, fixed-route vehicles. This however ended up not being the case. Often paratransit services were being filled up to their capacity. In some cases, leaving individuals who were in need of the door to door service provided by paratransit unable to utilize it due to the fact that disabled people who could use fixed-route vehicles also found themselves using these paratransit services.\n\nWith the passage of the ADA, Section 504 of the Rehabilitation Act was extended to include \"all\" activities of state and local government. Its provisions were not limited to programs receiving federal funds and applied to all public transit services, regardless of how the services were funded or managed. Title II of the ADA also more clearly defined a disabled person's right to equal participation in transit programs, and the provider's responsibility to make that participation possible.\n\nIn revisions to Title 49 Part 37, the Federal Transit Administration defined the combined requirements of the ADA and the Rehabilitation Act for transit providers. These requirements included \"complementary\" paratransit to destinations within 3/4 mile of all fixed routes (49 CFR 37.131) and submission of a plan for complying with complementary paratransit service regulations (49 CFR 37.135). Paratransit service is an unfunded mandate.\n\nUnder the ADA, complementary paratransit service is required for passengers who are 1) Unable to navigate the public bus system, 2) unable to get to a point from which they could access the public bus system, or 3) have a temporary need for these services because of injury or some type of limited duration cause of disability (49 CFR 37.123). Title 49 Part 37 details the eligibility rules along with requirements governing how the service must be provided and managed. In the United States, paratransit service is now highly regulated and closely monitored for compliance with standards set by the Federal Transit Administration (FTA).\n\nAs the ADA became effective in 1992 (49 CFR 37.135), the FTA required transit systems in the United States to plan and begin implementing ADA compliant services, with full implementation by 1997 (49 CFR 37.139). During this period, paratransit demand and services rapidly expanded. This growth led to many new approaches to manage and provide these services. Computerized reservation, scheduling and dispatching for paratransit have also evolved substantially and are now arguably among the most sophisticated management systems available in the world of rubber tire transit (land-based non-rail public transit).\n\nSince the passage of the ADA, paratransit service has grown rapidly as a mode of public transit in the United States. Continued growth can be expected due to the aging of baby boomers and disabled Iraq War veterans. The growth of the number of people requiring paratransit has resulted in an increase in cost for the paratransit industry to maintain these services. The results of this rising cost are the paratransit industry trying to get individuals to move from a reliance on paratransit vehicles to fixed-route vehicles. Due to the push to have paratransit vehicles being the main method of transportation for disabled individuals prior to the passing of the ADA, the paratransit industry is finding it hard to get individuals to switch over to fixed route transportation.\n\nBeginning in 2004, the bus, rail and motor coach trade magazine \"Metro Magazine\" began conducting annual surveys of public and private paratransit providers:\n\nUS Government Accountability Office GAO released a report in November 2012 for the Federal Transit Administration which \"examined: (1) the extent of compliance with ADA paratransit requirements, (2) changes in ADA paratransit demand and costs since 2007, and (3) actions transit agencies are taking to help address changes in the demand for and costs of ADA paratransit service.\" The report found that \"average number of annual ADA paratransit trips provided by a transit agency increased 7 percent from 2007 to 2010\" and that the average cost of providing a paratransit trip is \"an estimated three and a half times more expensive than the average cost of $8.15 to provide a fixed-route trip.\"\n\nThe Maryland Transit Administration reported paratransit ridership increases of 15% in fiscal 2012, with double-digit increases expected in fiscal 2013 and 2014. The cost of providing paratransit service is considerably higher than traditional fixed-route bus service, with Maryland's \"Mobility\" service reporting per-passenger costs of over $40 per trip in 2010.\nParatransit ridership growth of more than 10% per year was reported in the District of Columbia metropolitan area for 2006 through 2009. \nWashington Metropolitan Area Transit Authority's MetroAccess service in Washington, D.C. conducted a peer review of large urban paratransit systems in the US in 2009:\nIn response to increasing ridership and costs of providing paratransit service, WMATA made two significant changes beginning in 2010: the paratransit service area was reduced from jurisdictional boundaries to the ADA requirement of within a 3/4 mile corridor of fixed-route services; and, fares were linked to WMATA's fixed route services and charged to the ADA allowable maximum of two times the fastest equivalent bus or rail fare.\nThese changes helped result in the first-ever reduction in the number of year-over-year trips between 2011 and 2012.\n\nThe complicated nature of providing paratransit service in accordance with ADA guidelines led to the development of sophisticated software for the industry.\n\nIntelligent transportation systems technologies, primarily GPS, mobile data terminals, digital mobile radios, and cell phones, and scheduling, dispatching and call reservation software are now in use increasingly in North America and Europe. Interactive voice response systems and web-based initiatives are the next technology innovation anticipated for paratransit services.\n\nAdvanced analytics is another field being applied to paratransit operations. Some companies are beginning to integrate cloud computing models to find operational efficiencies and cost savings for smaller paratransit service providers.\n\nThere is no legislation providing details on paratransit standards, but the Canadian Urban Transit Association has provide voluntary guidelines for member transit agency to use to determine para transit needs and standards. TTC however offers the service.\n\nIn the United Kingdom, services are called community transport and provided locally. The Community Transport Association \n\nIn Hong Kong, Rehabus service is provided by the Hong Kong Society for Rehabilitation.\n\nThe New Zealand Transport Agency provides a comprehensive list of options in the country, including Total Mobility (TM) in Auckland\n\nIn Australia, Disability Standards for Accessible Public Transport under subsection 31 (1) of the Disability Discrimination Act of 1992 mandated that as of 2002 \"all new public transport conveyances, premises and infrastructure must comply with the transport standards. Facilities already in operation at that time have between five and thirty years to comply with the standards.\"\n\nIn some parts of the world, transportation services for the elderly and disabled are obtainable through Share Taxi options, often without formal government involvement.\n\nParatransit systems in many developing world cities are operated by individuals and small business. The fragmented, intensely competitive nature of the industry makes government regulation and control much harder than traditional public transport. Government authorities have cited problems with unsafe vehicles and drivers as justifying efforts to regulate and \"formalize\" paratransit operations. However, these efforts have been limited by ignorance on the part of regulatory authorities and mistrust between\nauthorities and operators.\n\n\n"}
{"id": "25047321", "url": "https://en.wikipedia.org/wiki?curid=25047321", "title": "Passenger service system", "text": "Passenger service system\n\nA passenger service system (PSS) is a series of critical systems used by airlines. The PSS usually comprises an airline reservations system, an airline inventory system and a departure control system (DCS).\n\nGenerally the PSS is made up of modules that are used to manage different parts of the airline’s business.\n\nThe airline reservations system is the system that allows an airline to sell their inventory (seats). It contains information on schedules and fares and contains a database of reservations (or passenger name records) and of tickets issued (if applicable).\n\nThe airline inventory system may or may not be integrated with the reservation system. The system contains all the airline’s flights and the available seats. The main function of the inventory system is to define how many seats are available on a particular flight by opening or closing an individual booking class in accordance with rules defined by the airline.\n\nThe departure control system is the system used by airlines and airports to check-in a passenger. The DCS is connected to the reservation system enabling it to check who has a valid reservation on a flight. The DCS is used to enter information required by customs or border security agencies and to issue the boarding document. In addition the DCS may also be used to dispatch cargo and to optimize aircraft weight and balance.\n"}
{"id": "42055694", "url": "https://en.wikipedia.org/wiki?curid=42055694", "title": "Philips DP70", "text": "Philips DP70\n\nThe DP70 is a model of motion picture projector, of which approximately 1,500 were manufactured by the Electro-Acoustics Division of Philips between 1954 and about 1968. It is notable for having been the first mass-produced theater projector in which 4/35 and 5/70 prints could be projected by a single machine, thereby enabling wide film to become a mainstream exhibition technology, for its recognition in the 1963 Academy Awards, which led to it being described as \"the only projector to win an Oscar\" (though this is technically incorrect, because the award was actually a Class 2 Oscar Plaque), and for its longevity: a significant number remain in revenue-earning service at the time of writing (February 2014).\n\nSmall-scale attempts had been made to use wide film for commercial theater exhibition around the time of the conversion to sound, of which Fox Grandeur was technologically the closest to the format the DP70 was designed to facilitate the launch of, two decades later. One of the reasons these early systems failed to establish wide film as an industry standard was that the projectors developed for them were incompatible with the existing 4/35 standard. In order to be able to project both, therefore, theaters had to be equipped with two sets of projectors, which involved significant extra cost and in some cases architectural modifications to projection booths.\n\nThe DP70 (DP stands for \"Double Projector\") was invented and developed by a team headed by Jan-Jacob Kotte of Philips between 1952–54, as part of the Todd-AO system. A core objective of the project was to create a single machine that could project both the Todd-AO 5/70 format and the 4/35 format which was, and was likely to remain, the dominant standard for theater exhibition.\n\nThe first DP70s were exported from The Netherlands to the United States in the fall of 1954, and were used for the roadshow release of the feature that was made to launch Todd-AO, \"Oklahoma!\" DP70s were used exclusively as part of the Todd-AO system for the first few years, but were eventually sold independently by Philips and its resellers to theaters worldwide. The DP70 was widely praised for its versatility, reliability and ease of use, which was recognized by the Academy in 1963. During the 1960s DP70 installations appeared throughout the world, mainly in prestige, downtown first run venues. As a result of the machine's success, dual gauge projectors were quickly developed and launched by Philips's main competitors, notably Cinemeccanica of Italy and Century of the United States.\n\nAlmost 60 years after the first DP70s shipped from the factory, a significant number remain in regular, commercial use worldwide. In 1972 the cinema division of Philips was bought out by Kinoton, a German company that had handled European sales and support for Philips cinema products since 1949. After-sales support for the DP70 passed to Kinoton at that point, which continued to manufacture and distribute replacement parts until the company was wound up in April 2014. Accessories and modifications are available (some of them were made by Kinoton, others by aftermarket manufacturers) that will enable the DP70 to project every 5/70 and 4/35 format that was ever used on a significant scale, most notably for the 35mm digital optical sound systems launched in the 1990s, e.g. Dolby Digital, and for 70mm DTS.\n\nThough the projection of film itself in mainstream, first run theaters has been superseded almost completely by digital projection at the time of writing, DP70s remain in service in cinematheque-type venues that specialize in showing repertory and archive titles. Theaters in which DP70s are still running today include the Egyptian Theatre in Hollywood, the Gartenbaukino in Vienna, the Pictureville cinema at the UK's National Media Museum and in Stockholm.\n\nThe fact that the DP70 was a Dutch machine developed specifically for a customer in the United States resulted in it being known by several different names. The DP70 was Philips's original model name for the projector, and this is what projectionists in Europe tend to call it. In the United States, the American Optical Company (the AO in Todd-AO) used the Philips factory model number, EL4000/01 (the 60 Hz variant - the 50 Hz one, for sale in European markets, was model no. EL4000/00), as their catalog number for the machine. It was eventually marketed independently of Todd-AO by Norelco (a contraction of \"North American Philips Electrical Company\", i.e. the brand name used by Philips in the US). The DP70 was originally sold in the United States simply as the \"Norelco Universal 70/35mm Motion Picture Projector\". After its acknowledgment in the 1963 Oscars, Norelco rebranded it as the AA (Academy Award). An improved version of the projector was also launched in 1963, which was branded as the AAII in the US.\n\nAll the projector mechanisms were built at the Philips factory in Eindhoven, though much of the peripheral hardware for the machines that were exported to the United States, e.g. bases and reel magazines, was manufactured locally, initially by the American Optical Company and later by Ballantyne.\n\nThe DP70 consists of a monocoque, cast iron chassis containing the mechanism, which is completely oil-immersed on the non-operating side. Jan-Jacob Kotte believed that the use of heavier materials to absorb vibration reduced instability in the projected image, and this is certainly reflected in the design of the DP70: a complete outfit, including the bases and reel magazines, weighs 1,004 lbs - literally half a ton! The DP70 was also significantly more expensive than any single gauge theater projector on the market: a US customer was quoted $6,225 for one in 1966 ($44,942 in 2014, adjusted for CPI inflation), which was around the cost of a typical three-bedroom suburban home at the time. The price asked for a double set without lenses asked in West-Germany in 1956 was 42,510.00 Marks, which at the rate of exchange at that time (4:1), related to 10,627.50 $ for the pair, or 5,313.75 $ per projector. That price was higher, than 35mm only projectors, but still reasonably low if compared to competitors like Bauer U2. The low price and reasonable quality was an important part of the success. Because the DP70 was built to commission for a customer in the United States, it is a very unusual example of a European-designed piece of industrial machinery with fasteners that have SAE rather than metric dimensions.\n\nDetachable reel magazines can be fitted (e.g. to enable the projection of nitrate film in accordance with safety regulations), or removed (e.g. to enable projection using an external film transport device such as a platter or tower, or to fit an external audio reader) as needed. Special fire trap rollers for nitrate were also available, to comply with fire regulations in some jurisdictions. Separate magnetic and analog optical audio heads are built into the mechanism itself, enabling all 5/70 magnetic and 4/35 optical formats to be projected without the need to adjust or replace any audio components. The change of gauge is done by swapping some gate components, pad roller assemblies, reel spindles and the lens, and in some cases making minor adjustments to the lamphouse. This procedure can be completed by a competent projectionist in 5–10 minutes, which is a major reason for the DP70's popularity with venues that show 4/35 and 5/70 prints interchangeably.\n\nBecause the DP70 was intended for use with 70mm film and in large theaters with a long throw to a big screen, several features were included to disperse the intense heat generated by the more powerful lamps with which it was often used. The gate assembly includes copper components which are silver-plated, and a liquid-cooled plate that is fitted with a water pipe surrounding the aperture opening. The use of liquid recirculating equipment and distilled water was encouraged. A single-blade shutter designed to rotate at very high speed (up to 3,600 RPM) doubles as a cooling fan. Unlike other projectors of its hi-power kind, the DP 70 did not offer forced-air cooling of the film itself, which is known to be a key feature to prevent overheating of the film and the major nitrate-fire prevention measure. A water cooled pre-shield just assists in keeping the gate and metal parts cold, not the film.\n\nThe DP70 as shipped from the factory was equipped for dual speed operation, at 24fps (the frame rate at which almost all 4/35 prints with a combined soundtrack are projected at) and 30fps, the frame rate used in the original Todd-AO system. The original version of the DP70 used separate drive motors for 24fps and 30fps operation, whereas the AAII had a single motor and a dual locking pulley mechanism on the main drive shaft to change the speed of the mechanism. There were also several other more minor changes in the AAII.\n\nBecause many of the DP70s remaining in use today are in repertory venues that screen a wide range of formats, including prints of silent movies that require a lower frame rate than 24, a lot of them have now received aftermarket modifications that will typically enable any speed between 16 and 30. Since US market DP70 seem to be factory equipped with synchronous motors that lock their speed (1,800 RPM in the USA) to the AC line frequency (60 Hz in the USA), the most common way of doing this has been to add a variable frequency solid state AC inverter (Baldor Inverter) to drive the 24fps motor. Most European units were sold employing asynchronous motors, which added slip to the synchronous speed (1500 RPM at 50 Hz) to achieve the required 24 frames (1440 RPM) and 30 frames (1800 rpm), without gearing requirements in the original construction.\n\nBelton, John, \"Widescreen Cinema\" (Cambridge, MA: Harvard University Press, 1992), \n\n"}
{"id": "316760", "url": "https://en.wikipedia.org/wiki?curid=316760", "title": "Polymer-bonded explosive", "text": "Polymer-bonded explosive\n\nA polymer-bonded explosive, also called PBX or plastic-bonded explosive, is an explosive material in which explosive powder is bound together in a matrix using small quantities (typically 5–10% by weight) of a synthetic polymer. PBXs are normally used for explosive materials that are not easily melted into a casting, or are otherwise difficult to form. PBX was first developed in 1952 in Los Alamos National Laboratory, as RDX embedded in polystyrene with dioctyl phthalate plasticizer. HMX compositions with teflon-based binders were developed in 1960s and 1970s for gun shells and for Apollo Lunar Surface Experiments Package (ALSEP) seismic experiments, although the latter experiments are usually cited as using hexanitrostilbene (HNS).\n\nPolymer-bonded explosives have several potential advantages:\n\nFluoropolymers are advantageous as binders due to their high density (yielding high detonation velocity) and inert chemical behavior (yielding long shelf stability and low aging). They are however somewhat brittle, as their glass transition temperature is at room temperature or above; this limits their use to insensitive explosives (e.g. TATB) where the brittleness does not have detrimental effect to safety. They are also difficult to process.\n\nElastomers have to be used with more mechanically sensitive explosives, e.g. HMX. The elasticity of the matrix lowers sensitivity of the bulk material to shock and friction; their glass transition temperature is chosen to be below the lower boundary of the temperature working range (typically below -55 °C). Crosslinked rubber polymers are however sensitive to aging, mostly by action of free radicals and by hydrolysis of the bonds by traces of water vapor. Rubbers like Estane or hydroxyl-terminated polybutadiene (HTPB) are used for these applications extensively. Silicone rubbers and thermoplastic polyurethanes are also in use.\n\nFluoroelastomers, e.g. Viton, combine the advantages of both.\n\nEnergetic polymers (e.g. nitro or azido derivates of polymers) can be used as a binder to increase the explosive power in comparison with inert binders. Energetic plasticizers can be also used. The addition of a plasticizer lowers the sensitivity of the explosive and improves its processibility.\n\nExplosive yields can be affected by the introduction of mechanical loads or the application of temperature; such damages are called insults. The mechanism of a thermal insult at low temperatures on an explosive is primarily thermomechanical, at higher temperatures it is primarily thermochemical.\n\nThermomechanical mechanisms involve stresses by thermal expansion (namely differential thermal expansions, as thermal gradients tend to be involved), melting/freezing or sublimation/condensation of components, and phase transitions of crystals (e.g. transition of HMX from beta phase to delta phase at 175 °C involves a large change in volume and causes extensive cracking of its crystals).\n\nThermochemical changes involve decomposition of the explosives and binders, loss of strength of binder as it softens or melts, or stiffening of the binder if the increased temperature causes crosslinking of the polymer chains. The changes can also significantly alter the porosity of the material, whether by increasing it (fracturing of crystals, vaporization of components) or decreasing it (melting of components). The size distribution of the crystals can be also altered, e.g. by Ostwald ripening. Thermochemical decomposition starts to occur at the crystal nonhomogeneities, e.g. intragranular interfaces between crystal growth zones, on damaged parts of the crystals, or on interfaces of different materials (e.g. crystal/binder). Presence of defects in crystals (cracks, voids, solvent inclusions...) may increase the explosive's sensitivity to mechanical shocks.\n\n"}
{"id": "544694", "url": "https://en.wikipedia.org/wiki?curid=544694", "title": "Publix", "text": "Publix\n\nPublix Super Markets, Inc., commonly known as Publix, is an employee-owned, American supermarket chain headquartered in Lakeland, Florida. Founded in 1930 by George W. Jenkins, Publix is a private corporation that is wholly owned by present and past employees and members of the Jenkins family. \n\nPublix stands as one of the largest U.S. regional grocery chains. Locations are found as far north as Spotsylvania, Virginia, as far south as Key West, Florida, while the westernmost location is in Mobile, Alabama. Today, the state of Florida still has the largest number of stores, with 787, about two-thirds of the outlets. As of August 2018, Publix employs about 193,000 people at its 1,231 retail locations, cooking schools, corporate offices, 9 grocery distribution centers, and 11 manufacturing facilities. The manufacturing facilities produce its dairy, deli, bakery, and other food products.\n\nPublix is ranked No. 47 on \"Fortune\" magazine's list of 100 Best Companies to Work For 2018, down from No. 21 in 2017, and was ranked No. 7 on \"Forbes\" 2017 list of America's Largest Private Companies and is the largest in Florida. \"Fortune\" ranked Publix #1 on their 2018 list of World's Most Admired Companies in the Food & Drug Stores sector. The company's 2017 sales totaled US$34.6 billion, with profits of $2.3 billion, ranking No. 88 on \"Fortune\" magazine's Fortune 500 list of U.S. companies by revenue for 2017. According to the National Retail Federation, based on 2016 revenue, Publix is the fifteenth-largest U.S. retailer. Publix stock is only available for purchase by eligible active employees and non-employee members of its Board of Directors.\n\nGeorge Jenkins opened the first Publix market in Winter Haven, Florida, on September 6, 1930 - a 27 ft by 65 ft building at 199 West Central Avenue. In 1934, that store made $120,000 in sales. In 1935, he opened a second market, the Economy Food Store, also in Winter Haven. Despite the Great Depression, his stores were financially successful.\n\nIn 1940, Jenkins, affectionately called \"Mr. George\" by his employees, mortgaged an orange grove to build Florida's first supermarket. His \"food palace\" had piped-in music, air conditioning, cold cases for frozen and refrigerated items, in-store doughnut and flower shops, and electric-eye automatic doors. During World War II, material shortages prevented him from building additional stores. In 1945, Jenkins purchased the 19-store All American chain of food stores and converted them into Publix Super Markets.\n\nIn 1951, Publix moved its headquarters from Winter Haven to Lakeland, Florida, and built its first distribution warehouse there. At the same time, they began to close the All American stores, replacing them with Publix markets. In 1956, Publix achieved $50 million in sales, and $1 million in profit. In 1957, the donut shop in each store was expanded into a full-service bakery.\n\nBy 1959, Publix was the dominant supermarket chain in Central Florida, and began expansion to South Florida. In 1963, the company built a distribution center in Miami, and began providing deli services. In 1970, sales surpassed $500 million; they reached $1 billion in 1974, when the chain expanded to include Jacksonville, Florida.\n\nIn 1982, the company launched the \"Presto!\" ATM network; it soon installed ATMs in every Publix. Sales exceeded $5 billion in 1989.\n\nIn 1983, Carol Jenkins Barnett joined the Publix Board of Directors and served in that role until 2016. During her time at Publix, the company grew into the largest supermarket chain in Florida, expanded into five other states, and recorded $32.5 billion in sales in 2015.\nPublix Super Markets bought 49 Florida stores from Albertsons. The deal was announced on June 9, 2008, and was completed on September 9, 2008. It included 15 locations in North Florida, 30 in Central Florida, and four in South Florida. The sale allowed Publix to operate four stores in a new market area for the company, Escambia County, Florida (the Pensacola area).\n\nOn February 5, 2009, Publix opened its 1,000th store in St. Augustine, Florida, allowing the company to become one of only five U.S. grocery retailers to operate that many stores. The St. Augustine store is among Publix's first stores designed to be energy-efficient. The store includes motion sensor lights throughout the store, including on the freezer doors, and an overhead light system that can be controlled by each department.\n\nThe first Publix outside Florida opened in Savannah, Georgia, in 1991; distribution and manufacturing facilities in Dacula, Georgia (a northeastern suburb of Atlanta) soon followed, as it began to expand into metro Atlanta in 1993. Publix further expanded into South Carolina (1993), Alabama (1996), Tennessee (2002), North Carolina (2014) and Virginia (2017).\n\nIn 2011, Publix announced it was expanding into North Carolina, initially by opening stores in the Charlotte metropolitan area, and later announced construction of a new store in Asheville. The first Charlotte-area Publix stores (on the South Carolina side of the metropolitan area, opened in 2012); the first North Carolina Publix store opened in Ballantyne in 2014. Concurrently, Publix purchased seven Charlotte-area locations from competitor BI-LO stores. Publix completed the purchase of property in Boone, North Carolina on November 20, 2015 with plans to open in 2017.\n\nIn February 2016, Publix announced their entry into the Virginia market, with the signing of two store leases, the first in Bristol scheduled to open in 2017 and the second in metropolitan Richmond scheduled for 2018. In July 2016, it was announced that Publix had entered into a purchase agreement with Ahold and Delhaize Group for 10 Martin's Food Markets locations in the Richmond market as part of the divestiture of stores to gain clearance from the Federal Trade Commission for the impending Ahold/Delhaize merger.\n\nIn April 2016, Ed Crenshaw, grandson of founder George Jenkins, retired from his position as CEO. President Todd Jones, a 36-year Publix veteran whose first job was as a Front Service Clerk (bagger), has taken on Ed's responsibilities as CEO, marking the first time that someone outside the Jenkins family is in charge of the company. Ed Crenshaw will remain with Publix as Chairman of the Board of Directors.\n\nEach store provides products and services in its grocery, deli, bakery, produce, floral, meat, and seafood departments. Some stores have valet parking, cafés, sushi bars, pharmacy departments, and/or a liquor store.\n\nThe customer service counter also provides check cashing, money orders, Western Union services, Rug Doctor rentals, and lottery tickets. Some stores also provide DVD rental services. In December 2005, Publix discontinued its photo processing service, replacing it with an online or mail-order service via the Snapfish program. The Snapfish agreement has since been terminated, and Publix no longer offers photo services.\n\nPublix operates 11 cooking schools under the Aprons name.\nThe schools offer cooking demonstrations in which customers are encouraged to sample easy-to-make, nutritious dishes prepared at in-store kiosks and take a recipe card with them. All recipes are developed in-house, using easy-to-prepare or prepackaged ingredients, often available at the Aprons kiosk.\n\nIn 2005 Publix introduced its Aprons make-ahead meals concept. Customers could purchase meals that they could assemble in-store or, for an extra charge, an Aprons associate would prepare and assemble the meals. These were standalone stores located in Jacksonville and Lithia, Florida. In summer 2009, Publix closed both locations citing lack of customer interest.\n\nGreenWise Market is a concept the company introduced in response to the increase in the number and profitability of health food stores. GreenWise Markets were created to increase awareness of nutrition; it focuses on organic and natural items. These stores are similar to the Whole Foods Market chain. In addition to organic and traditional products, GreenWise Markets include salad and hot bars. The first six stores were set to be in Palm Beach Gardens, Boca Raton, Vero Beach, Tampa, Naples, and Coral Springs, Florida.\n\nThe first GreenWise Market opened on September 27, 2007 in Palm Beach Gardens. The second Publix GreenWise Market opened in Boca Raton on May 29, 2008, located in Boca Village Square. The third Publix GreenWise Market opened November 6, 2008, in Tampa's Hyde Park neighborhood.\nFrom 2008 to 2016, the company focused on a \"hybrid\" concept instead, integrating the GreenWise concept into traditional Publix stores. Approximately half of locations built since 2008 are considered hybrid stores.\n\nIn 2017, the company announced they would resume building standalone GreenWise locations, the first of which will be near the campus of Florida State University in Tallahassee, opening in 2018.\n\nPublix operates seven stores, branded \"Publix Sabor\" (\"sabor\" is Spanish for \"flavor\"), which cater to Hispanic Americans living in Florida and offer products for Hispanics. Located in Miami-Dade County in Greater Miami, the seven themed stores are spread between Miami and Hialeah. They have since been closed and replaced by newly built locations or merged with existing stores that are not part of the Sabor sub-brand. Two other Publix Sabor locations in Kissimmee and Lake Worth\n\nPublix Sabor locations have bilingual English-Spanish employees, open seating cafés, and a wider selection of prepared foods from the deli and bakery catering to Hispanic flavors. Publix offers cafés and hot foods because many Hispanic Americans grew up in foreign cities which had open public squares where people socialize and eat.\n\nThe first Publix in-store pharmacy was opened on October 30, 1986, in Altamonte Springs, Florida. By 1995, one-third of Publix stores had a pharmacy and today, approximately 90% of Publix stores include a pharmacy. Publix Pharmacies consistently ranked number one for customer satisfaction in supermarket pharmacies in several surveys conducted by independent research companies.\n\nPublix announced in August 2007, that it would offer several types of antibiotics free to its customers. Customers must have a prescription; they are given a maximum of a two-week supply. Several medical professionals expressed concerns that this could contribute to an overuse of antibiotics which leads to antibiotic resistance, a serious public health concern. These medications include:\n\nThese antibiotics are offered to customers regardless of their prescription insurance provider. Doxycycline Hyclate was removed from the list because of cost increases. In May 2014, Cephalexin was removed from the list due to cost increases.\n\nIn March 2010, Publix announced the launch of another free prescription, Metformin for Type II Diabetes, the generic of Glucophage. Publix provides the medication in 500 mg, 850 mg, and 1000 mg strengths. The only restriction is a 90-day supply or up to 360 500-mg, 270 850-mg, or 225 1000-mg tablets, but refills are not limited.\n\nIn August 2011, Publix began offering Lisinopril, an ACE inhibitor that is used to prevent, treat, or improve symptoms of high blood pressure, certain heart conditions, diabetes, and certain chronic kidney conditions, as another free prescription. Customers can get a 90-day supply of this prescription for free at any Publix Pharmacy, up to a maximum of 180 tablets. Lisinopril-HCTZ combination products are excluded.\n\nIn May 2014, Publix began offering Amlodipine, a calcium channel blocker used to treat high blood pressure and chest pain (angina) as a free medication. Customers can get a 90-day supply of this medication (up to 180 2.5-mg or 5-mg tablets, or 90 10-mg tablets) free of charge.\n\nMontelukast, a medicine used for the treatment of allergies and asthma, was added to the free medication program in February 2017. 90-day supplies of 4- or 5-mg chewable tablets for children, or 10-mg oral tablets for adults, are available with a doctor's prescription.\n\nIn early 2006, Publix and The Little Clinic signed an exclusive agreement to open medical clinics within Publix stores. The first clinics were opened in the Atlanta, Miami, Orlando, and Tampa markets in the first half of 2006. The Little Clinic health-care centers were staffed by nurse practitioners who can write prescriptions, provide diagnosis and treatment of common ailments and minor injuries, and offer wellness care like physicals, screenings, and vaccinations. Effective May 9, 2011, Publix closed the Little Clinics in its stores in order to focus on its core pharmacy and grocery business.\n\nPublix and BayCare Health System announced a collaboration to provide telehealth and telemedicine services at specialized pharmacies in four Tampa Bay-area counties in March 2017. Pharmacies participating in the program have private rooms for patients to speak with a board-certified physician in BayCare's network via teleconferencing, plus diagnostic tools that can be used by the patient, with or without assistance from pharmacy staff. Doctors will be able to perform basic exams and write prescriptions for minor illnesses and conditions for patients.\n\nAfter PublixDirect, Publix made a second attempt in 2010 at e-commerce with the introduction of Publix Curbside. Customers had the ability to browse and purchase groceries online, then drive to a participating location where an associate will have selected their items and would bring them out to the buyer's vehicle. Announced as a pilot program with locations in the Atlanta area and Tampa, the program was ended in January 2012 after its performance reportedly did not meet expectations.\n\nThe company later resurrected its curbside concept, this time using its delivery partner, Instacart, to manage the online ordering portion of the service. Currently in a trial stage, the second iteration of Publix Curbside began with two pilot locations in the greater Tampa area in September 2017, and is expected to expand to the greater Atlanta area by the end of the year.\n\nIn July 2016, Publix announced another pilot program with Instacart to offer online shopping and delivery services in the greater Miami area. Customers in 37 ZIP codes from Hallandale Beach to South Miami are able to participate in the program. Not all products available at stores, such as tobacco, gift cards, prescriptions, and age-restricted items, are able to be delivered by the service. Beer and wine can be delivered in Florida and North Carolina only.\n\nAs of February 2017, Instacart deliveries from Publix are available in the metro areas of Atlanta, Charlotte, Fort Lauderdale, Miami, Orlando, Raleigh, Tampa, Jacksonville, and Nashville, as determined by ZIP code.\n\nLater in 2017, Publix announced its intent to expand its delivery program, and expects to have the service available from more than 90 percent of stores by the end of the year.\n\nIn response to other grocery stores' aggressive discounting across the Florida market, Publix opened its first Food World store in September 1970 in Orlando, Florida. The store marked the first under the Food World banner for Publix and would become the first of 22 more of the type.\n\nIn November 1977, in Lakeland, Florida, Publix opened the Lake Miriam Food World, which, at 57,000 sq. ft., was its largest store in the company and also the largest store in the southeast. The store was the company's first to feature barcode scanners.\n\nThe brand was retired in 1985 because the stores were unable to turn a profit for Publix or give workers a percentage of their store's profits.\n\nStarting in 2001, Publix operated 14 PIX (stylized in all-capitals) gasoline-convenience stores in Florida, Georgia, and Tennessee. Locations were limited during the trial period of the concept. In 2014, all Florida and Georgia locations were sold to Circle K, the sole Tennessee location was sold to another entity, and the concept was discontinued. The locations were converted to other brands, as Publix retains the rights to \"PIX.\"\n\nIn 2002, Publix invested in the Lakeland-based restaurant chain Crispers, which concentrates on health-conscious fare. It increased its stake in 2004 before purchasing the remainder of the company in 2007. In May 2011, Publix announced it had sold the Crispers chain to Healthy Food Concepts LLC. The stores had not performed well during the downturn and in recent years Publix closed several units, leaving the chain with 36 stores when the sale was announced.\n\nPublix tested the market response to liquor stores in the late 1980s, but closed its test sites in 1989. It re-entered the liquor sales market again in 2003 and has met with success since. The liquor store is in an area accessed via a separate entrance as required by local laws, modeled after many other grocery chains.\n\nIn September 2010, Publix reported it started adding Blockbuster DVD rental kiosks to its stores, with the movie rentals starting at $1 per day. In 2010, Publix completed its rollout of Blockbuster Express kiosks to its stores.\n\nIn 2012, NCR sold its entertainment division, which includes the Blockbuster Express kiosks, to Coinstar, the owner of the Redbox DVD rental kiosks. Blockbuster Express machines were replaced with Redbox machines in most stores by the end of 2012.\n\nIn December 2016, Publix opened its first in-store Starbucks location in the Orlando area, with five more opening throughout 2017.\n\n\"Presto!\" is an automated teller machine (ATM) network owned and operated by Publix Super Markets. There are over 1,100 \"Presto!\" ATMs in Florida, Alabama, Georgia, North Carolina, South Carolina, and Tennessee, all located at Publix retail stores. This network includes point of sale (POS) capabilities, meaning that debit, credit, electronic benefit transfer (EBT) cash, or EBT food stamp cards can be used to make purchases at any Publix store.\n\nThe company, founded in 1930, has never had a layoff.\n\nIn 1995 Publix was sued \"for sex discrimination in job assignments, promotions and allocation of hours\" and settled for $81.5 million in 1997. Publix had claimed that the suit was simply an effort by the United Food and Commercial Workers to unionize the company, but the judge ruled in favor of the plaintiffs and required Publix to \"correct some of its statements.\"\n\nPublix announced that effective January 1, 2015, health coverage would be available to same-sex couples regardless of place of marriage, as long as they are legally married.\n\nPublix regularly conducts charity drives raising money and food for such charities as Special Olympics, March of Dimes, Children's Miracle Network, United Way, as well as various local food banks and soup kitchens such as Our Father's House Soup Kitchen and Second Harvest North Florida.\n\nOn May 25, 2018, student activists and survivors of the Stoneman Douglas High School shooting, including David Hogg, planned a boycott of Publix, which would have included \"die-in\" protests at several Publix supermarkets, because of the $670,000 that the company made to Adam Putnam, a Republican candidate for Florida governor and self-described \"proud NRA sell-out\" who opposed new state gun restrictions created in response to the school shooting. Moments before the protests began, the company announced that it would suspend corporate-funded political contributions and reevaluate their political funding practices. Despite the company's announcement, David Hogg led a die-in on May 25, 2018 at a Publix supermarket with parents and students from Stoneman Douglas High School for 720 seconds, the approximate number of school shootings in recent history. Six days after halting political contributions, the Florida Retail Federation, a trade group heavily funded (>80% in 2017) by Publix, donated an additional $100,000 to Putnam's Florida Grown political action committee.\n\nPublix stock is private and restricted: it can only be purchased by current employees or board members, and cannot be sold to anyone without first being offered back to Publix for repurchase.\n\nStock was made available to associates in 1959, originally priced at $10.00 per share.\nEmployees can acquire stock through three programs: an ESOP \"PROFIT\" plan, 401(k) \"SMART\" (Saving Makes A Richer Tomorrow) plan, and an employee stock purchase plan.\n\nAs of August 1, 2018, Publix stock is valued at $42.55 per share. Publix stock is quoted on the US OTC market under the code PUSH. It is listed on the 2016 Fortune 500 list at #87.\n\nDistribution centers are located in:\n\n\nManufacturing facilities are located in:\n\n\nIn 2003, Publix supported a successful bill that prevents owners from suing if their land is polluted by dry cleaning chemicals dumped on an adjacent property, if the adjacent property owners are on a state clean-up list. Publix lost a 2001 lawsuit filed by an owner whose property had been contaminated in this manner.\n\nOn October 4, 2005, Publix sued Visa and MasterCard, citing unfair business practices over their unannounced and non-negotiable increases in merchant account fees. Wal-Mart won a similar lawsuit against Visa in 2004.\n\nIn 2014, Publix was fined by the Board of Human Rights of Broward County, Florida for discrimination involved in the termination of an LGBT employee. Upon appeal, the 17th Circuit Court found that the decision by the Board of Human Rights of Broward County was \"not supported by competent, substantial evidence\" and quashed the order.\n\nPublix has won various local, regional, and national industry and philanthropic awards, including:\n\n"}
{"id": "2806550", "url": "https://en.wikipedia.org/wiki?curid=2806550", "title": "SK Hynix", "text": "SK Hynix\n\nSK Hynix Inc. () is a South Korean memory semiconductor supplier of dynamic random-access memory (DRAM) chips and flash memory chips. Hynix is the world's second-largest memory chipmaker (after Samsung Electronics) and the world's fifth-largest semiconductor company. Founded as Hyundai Electronic Industrial Co., Ltd. in 1983 and known as Hyundai Electronics, the company has manufacturing sites in Korea, the United States, China and Taiwan. In 2012, when SK Telecom became its major shareholder, Hynix merged to SK Group (the third largest conglomerate in South Korea). The company's shares are traded on the Korea Stock Exchange, and the Global Depository shares are listed on the Luxembourg Stock Exchange.\n\nHynix memory is used by Apple in some of their iMac, MacBook and MacBook Pro computers. Apple's A9 chipset is onboard together with an SK Hynix RAM module believed to be \"likely the same 2 GB LPDDR4 mobile DRAM found in the iPhone 6s\". Hynix memory is also used by Asus in their Google-branded Nexus 7 tablet (both 2012 and 2013 models, respectively), an OEM provider for IBM System x servers, and is used in desktop PCs and laptops as well as the Asus Eee PC. Dell, HP Inc. and Hewlett Packard Enterprise (formerly Hewlett-Packard) have also used Hynix memory as OEM equipment. Other products that use Hynix memory include DVD players, cellular phones, set-top boxes, personal digital assistants, networking equipment, and hard disk drives.\n\n\nHynix produces a variety of semiconductor memories, such as:\n\n\n"}
{"id": "8848961", "url": "https://en.wikipedia.org/wiki?curid=8848961", "title": "Solander box", "text": "Solander box\n\nA Solander box (\"S\" may also be in lowercase), or clamshell case (mainly in American English), is a book-form case used for storing manuscripts, maps, prints, documents, old and precious books, etc. It is commonly used in archives, print rooms and libraries. It is named after the Swedish botanist Daniel Solander (1733–1782), who is credited with its construction while working at the British Museum, where he catalogued the natural history collection between 1763 and 1782.\n\nThe case is usually constructed of hardcover or wood, and has a hinged lid connected to its base. Both lid and bottom sections of the box have three fixed side sections or \"lips\"; the lid is slightly larger so that the side pieces \"nest\" when the case is closed. The fourth \"spine\" side has flexible joints where it joins the main top and bottom pieces and so goes flat onto the surface where the box is opened. The front-edge of the case often contains a clasp for closure. The exterior is covered with heavy paper, fabric or leather, and its interior may be lined with padded paper or felt, especially if made for a book. All materials should be acid-free for conservation. The depth of the box is normally about five inches, if it is not made for a specific object, and various standard sizes are made, with traditional names including \"royal\", \"imperial\", \"elephant\" and others. Ones for very old books will typically be custom made to an exact size. The boxes are stored flat, and are strong enough to be kept in small stacks.\n\nA simplified and undecorated form of Solander box termed a \"phase box\" is used for temporary storage of books during conservation work. These are constructed from plain mounting board. Although undecorated, the materials should still be of archival grade.\n\n"}
{"id": "41364583", "url": "https://en.wikipedia.org/wiki?curid=41364583", "title": "Soliton Technologies", "text": "Soliton Technologies\n\nSoliton is an Indian technology company that offers research, development and consulting services in the areas of Test & Measurement automation, Machine Vision, and Embedded Systems. It is India's first and largest manufacturer of smart cameras and machine vision components. It was founded in 1998 by Dr Ganesh Devaraj and is headquartered in the city of Bengaluru. It has its other office locations in Coimbatore, Wisconsin, Texas and California. It is the market leader in machine vision products and solutions. It is the main sponsor and the core member of TIFAC Center Of Relevance and Excellence (CORE) in Machine Vision, Chennai.\n\nSoliton was founded in 1998 by Dr Ganesh Devaraj. The inspiration to start Soliton apparently came when he read the book ‘’, while working as a scientist at VI Engineering, Michigan, after obtaining a doctorate in Theoretical Physics from the University of Michigan, Ann Arbor. Soliton started as an Alliance Member of National Instruments, in the United States. In 2004, Soliton started a product division for manufacturing machine vision cameras that could address the need in the manufacturing industry for sophisticated quality control tools. Soliton has since extended its services from offering solutions for stated automation needs, to offering consulting and R&D services for the purpose of addressing the core needs of clients to build machine vision and automation products.\n\nSoliton has over 18 years of experience developing LabVIEW based automation solutions, and has one of the largest team of NI certified LabVIEW Developers and Architects in the world. India's first National Instruments Certified LabVIEW Developer, Asia's first Certified TestStand Developer,and Asia's first Certified TestStand Architect is from Soliton.\n\nSoliton products include PRIM (Print Color Registration Control System) to detect newspaper offset, CASH to detect counterfeit currency, Spot-IT, India's first smart camera, NEO, India's first user configurable smart camera, High speed pencil Inspection system, etc.\n\nNUERA [https://www.solitontech.com/smart-camera-support/doc/home/ <nowiki>[12]</nowiki>] machine vision smart camera is a compact, modular and a complete vision system . With inbuilt high performance quad-core ARM processor, it is capable of running image processing algorithms & user application logic. NUERA provides flexibility to choose the image sensor resolution, lens, filter, lighting, and connectivity interfaces as per the requirement. Complete SDK (C/C++, Python) is provided including project templates and example codes to speed up the application development. It can be used for Pharma Packaging Inspection (1D, 2D barcode), OCR/OCV, Direct part Marking Inspection.\n\nNeo is the first user configurable smart camera fully designed in India. It has a built-in image sensor and processor with integrated LED lights. This camera is configurable for a range of machine vision based poka-yoke applications. The heart of this system is the Soliton Vision library which consists of machine vision algorithms optimized for its processor architecture. NEO was showcased in the TI Developer's Conference 2008. At that point of time, Soliton Technologies was the only Indian company to present in TI Developer's Conference.\n"}
{"id": "1871162", "url": "https://en.wikipedia.org/wiki?curid=1871162", "title": "Spin–orbit interaction", "text": "Spin–orbit interaction\n\nIn quantum physics, the spin–orbit interaction (also called spin–orbit effect or spin–orbit coupling) is a relativistic interaction of a particle's spin with its motion inside a potential. A key example of this phenomenon is the spin–orbit interaction leading to shifts in an electron's atomic energy levels, due to electromagnetic interaction between the electron's magnetic dipole, its orbital motion, and the electrostatic field of the positively charged nucleus. This phenomenon is detectable as a splitting of spectral lines, which can be thought of as a Zeeman effect product of two relativistic effects: the apparent magnetic field seen from the electron perspective and the magnetic moment of the electron associated with its intrinsic spin. A similar effect, due to the relationship between angular momentum and the strong nuclear force, occurs for protons and neutrons moving inside the nucleus, leading to a shift in their energy levels in the nucleus shell model. In the field of spintronics, spin–orbit effects for electrons in semiconductors and other materials are explored for technological applications. The spin–orbit interaction is one cause of magnetocrystalline anisotropy and the spin Hall effect.\n\nFor atoms, energy level split produced by the spin-orbit interaction is usually of the same order in size to the relativistic corrections to the kinetic energy and the zitterbewegung effect. The addition of these three corrections is known as the fine structure. Note that the spin-orbit effect is due to the electrostatic field of the electron and not the magnetic field created by its orbit. The interaction between the magnetic field created by the electron and the magnetic moment of the nucleus is a slighter correction to the energy levels known as the hyperfine structure.\n\nThis section presents a relatively simple and quantitative description of the spin–orbit interaction for an electron bound to a hydrogen-like atom, up to first order in perturbation theory, using some semiclassical electrodynamics and non-relativistic quantum mechanics. This gives results that agree reasonably well with observations. \n\nA rigorous calculation of the same result would use relativistic quantum mechanics, using Dirac equation, and would include many-body interactions. Achieving an even more precise result would involve calculating small corrections from quantum electrodynamics.\n\nThe energy of a magnetic moment in a magnetic field is given by\n\nwhere μ is the magnetic moment of the particle, and B is the magnetic field it experiences.\n\nWe shall deal with the magnetic field first. Although in the rest frame of the nucleus, there is no magnetic field acting on the electron, there \"is\" one in the rest frame of the electron (see classical electromagnetism and special relativity). Ignoring for now that this frame is not inertial, in SI units we end up with the equation\n\nwhere v is the velocity of the electron, and E is the electric field it travels through. Here, in the non-relativistic limit, we assume that the Lorentz factor formula_3. Now we know that E is radial, so we can rewrite formula_4.\nAlso we know that the momentum of the electron formula_5. Substituting this in and changing the order of the cross product gives\n\nNext, we express the electric field as the gradient of the electric potential formula_7. Here we make the central field approximation, that is, that the electrostatic potential is spherically symmetric, so is only a function of radius. This approximation is exact for hydrogen and hydrogen-like systems. Now we can say that\n\nwhere formula_9 is the potential energy of the electron in the central field, and \"e\" is the elementary charge. Now we remember from classical mechanics that the angular momentum of a particle formula_10. Putting it all together, we get\n\nIt is important to note at this point that B is a positive number multiplied by L, meaning that the magnetic field is parallel to the orbital angular momentum of the particle, which is itself perpendicular to the particle's velocity.\n\nThe magnetic moment of the electron is\n\nwhere formula_13 is the spin angular-momentum vector, formula_14 is the Bohr magneton, and formula_15 is the electron-spin g-factor. Here formula_16 is a negative constant multiplied by the spin, so the magnetic moment is antiparallel to the spin angular momentum.\n\nThe spin–orbit potential consists of two parts. The Larmor part is connected to the interaction of the magnetic moment of the electron with the magnetic field of the nucleus in the co-moving frame of the electron. The second contribution is related to Thomas precession.\n\nThe Larmor interaction energy is\n\nSubstituting in this equation expressions for the magnetic moment and the magnetic field, one gets\n\nNow we have to take into account Thomas precession correction for the electron's curved trajectory.\n\nIn 1926 Llewellyn Thomas relativistically recomputed the doublet separation in the fine structure of the atom. Thomas precession rate formula_19 is related to the angular frequency of the orbital motion formula_20 of a spinning particle as follows:\n\nwhere formula_22 is the Lorentz factor of the moving particle. The Hamiltonian producing the spin \nprecession formula_19 is given by\n\nTo the first order in formula_25, we obtain\n\nThe total spin–orbit potential in an external electrostatic potential takes the form \nThe net effect of Thomas precession is the reduction of the Larmor interaction energy by factor 1/2, which came to be known as the \"Thomas half\".\n\nThanks to all the above approximations, we can now evaluate the detailed energy shift in this model. Note that \"L\" and \"S\" are no longer conserved quantities. In particular, we wish to find a new basis that diagonalizes both \"H\" (the non-perturbed Hamiltonian) and Δ\"H\". To find out what basis this is, we first define the total angular momentum operator\n\nTaking the dot product of this with itself, we get\n\n(since L and S commute), and therefore\n\nIt can be shown that the five operators \"H\", \"J, L, S\", and \"J\" all commute with each other and with Δ\"H\". Therefore, the basis we were looking for is the simultaneous eigenbasis of these five operators (i.e., the basis where all five are diagonal). Elements of this basis have the five quantum numbers: \"formula_31\" (the \"principal quantum number\"), formula_32 (the \"total angular momentum quantum number\"), \"formula_33\" (the \"orbital angular momentum quantum number\"), \"formula_34\" (the \"spin quantum number\"), and formula_35 (the \"\"z\" component of total angular momentum\").\n\nTo evaluate the energies, we note that\n\nfor hydrogenic wavefunctions (here formula_37 is the Bohr radius divided by the nuclear charge \"Z\"); and\n\nWe can now say that\n\nwhere\n\nFor the exact relativistic result, see the solutions to the Dirac equation for a hydrogen-like atom.\n\nA crystalline solid (semiconductor, metal etc.) is characterized by its band structure. While on the overall scale (including the core levels) the spin–orbit interaction is still a small perturbation, it may play a relatively more important role if we zoom in to bands close to the Fermi level (formula_41). The atomic formula_42 (spin-orbit) interaction, for example, splits bands that would be otherwise degenerate, and the particular form of this spin–orbit splitting (typically of the order of few to few hundred millielectronvolts) depends on the particular system. The bands of interest can be then described by various effective models, usually based on some perturbative approach. An example of how the atomic spin–orbit interaction influences the band structure of a crystal is explained in the article about Rashba and Dresselhaus interactions.\n\nIn crystalline solid contained paramagnetic ions, e.g. ions with unclosed d or f atomic subshell, localized electronic states exist. In this case, atomic-like electronic levels structure is shaped by intrinsic magnetic spin–orbit interactions and interactions with crystalline electric fields. Such structure is named the fine electronic structure. For rare-earth ions the spin–orbit interactions are much stronger than the CEF interactions. The strong spin–orbit coupling makes \"J\" a relatively good quantum number, because the first excited multiplet is at least ~130 meV (1500 K) above the primary multiplet. The result is that filling it at room temperature (300 K) is negligibly small. In this case, a (2\"J\" + 1)-fold degenerated primary multiplet split by an external crystal electric field (CEF) can be treated as the basic contribution to the analysis of such systems' properties. In the case of approximate calculations for basis formula_43, to determine which is the primary multiplet, the Hund principles, known from atomic physics, are applied:\n\n\nThe \"S\", \"L\" and \"J\" of the ground multiplet are determined by Hund's rules. The ground multiplet is 2\"J\" + 1 degenerated – its degeneracy is removed by CEF interactions and magnetic interactions. CEF interactions and magnetic interactions resemble, somehow, Stark and Zeeman effect known from atomic physics. The energies and eigenfunctions of the discrete fine electronic structure are obtained by diagonalization of the (2\"J\" + 1)-dimensional matrix. The fine electronic structure can be directly detected by many different spectroscopic methods, including the inelastic neutron scattering (INS) experiments. The case of strong cubic CEF (for 3\"d\" transition-metal ions) interactions form group of levels (e.g. \"T\", \"A\"), which are partially split by spin–orbit interactions and (if occur) lower-symmetry CEF interactions. The energies and eigenfunctions of the discrete fine electronic structure (for the lowest term) are obtained by diagonalization of the (\"2L + 1)(2S + 1)\"-dimensional matrix. At zero temperature (\"T\" = 0 K) only the lowest state is occupied. The magnetic moment at \"T\" = 0 K is equal to the moment of the ground state. It allows evaluate the total, spin and orbital moments. The eigenstates and corresponding eigenfunctions formula_44 can be found from direct diagonalization of Hamiltonian matrix containing crystal field and spin–orbit interactions. Taking into consideration the thermal population of states, the thermal evolution of the single-ion properties of the compound is established. This technique is based on the equivalent operator theory defined as the CEF widened by thermodynamic and analytical calculations defined as the supplement of the CEF theory by including thermodynamic and analytical calculations.\n\nHole bands of a bulk (3D) zinc-blende semiconductor will be split by formula_45 into heavy and light holes (which form a formula_46 quadruplet in the formula_47-point of the Brillouin zone) and a split-off band (formula_48 doublet). Including two conduction bands (formula_49 doublet in the formula_47-point), the system is described by the effective eight-band model of Kohn and Luttinger. If only top of the valence band is of interest (for example when formula_51, Fermi level measured from the top of the valence band), the proper four-band effective model is\n\nwhere formula_53 are the Luttinger parameters (analogous to the single effective mass of a one-band model of electrons) and formula_54 are angular momentum 3/2 matrices (formula_55 is the free electron mass). In combination with magnetization, this type of spin–orbit interaction will distort the electronic bands depending on the magnetization direction, thereby causing magnetocrystalline anisotropy (a special type of magnetic anisotropy).\nIf the semiconductor moreover lacks the inversion symmetry, the hole bands will exhibit cubic Dresselhaus splitting. Within the four bands (light and heavy holes), the dominant term is\n\nwhere the material parameter formula_57 for GaAs (see pp. 72 in Winkler's book, according to more recent data the Dresselhaus constant in GaAs is 9 eVÅ; the total Hamiltonian will be formula_58). Two-dimensional electron gas in an asymmetric quantum well (or heterostructure) will feel the Rashba interaction. The appropriate two-band effective Hamiltonian is\n\nwhere formula_60 is the 2 × 2 identity matrix, formula_61 the Pauli matrices and formula_62 the electron effective mass. The spin–orbit part of the Hamiltonian, formula_63 is parametrized by formula_64, sometimes called the Rashba parameter (its definition somewhat varies), which is related to the structure asymmetry. \n\nAbove expressions for spin–orbit interaction couple spin matrices formula_65 and formula_66 to the quasi-momentum formula_67, and to the vector potential formula_68 of an AC electric field through the Peierls substitution formula_69. They are lower order terms of the Luttinger–Kohn k·p perturbation theory in powers of formula_70. Next terms of this expansion also produce terms that couple spin operators of the electron coordinate formula_71. Indeed, a cross product formula_72 is invariant with respect to time inversion. In cubic crystals, it has a symmetry of a vector and acquires a meaning of a spin–orbit contribution formula_73 to the operator of coordinate. For electrons in semiconductors with a narrow gap formula_74 between the conduction and heavy hole bands, Yafet derived the equation\nwhere formula_76 is a free electron mass, and formula_77 is a formula_77-factor properly renormalized for spin–orbit interaction. This operator couples electron spin formula_79 directly to the electric field formula_80 through the interaction energy formula_81.\n\nElectric dipole spin resonance (EDSR) is the coupling of the electron spin with an oscillating electric field. Similar to the electron spin resonance (ESR) in which electrons can be excited with an electromagnetic wave with the energy given by the Zeeman effect, in EDSR the resonance can be achieved if the frequency is related to the energy band split given by the spin-orbit coupling in solids. While in ESR the coupling is obtained via the magnetic part of the EM wave with the electron magnetic moment, the ESDR is the coupling of the electric part with the spin and motion of the electrons. This mechanism has been proposed for controlling the spin of electrons in quantum dots and other mesoscopic systems.\n\n\n\n"}
{"id": "41309994", "url": "https://en.wikipedia.org/wiki?curid=41309994", "title": "Sterculia foetida", "text": "Sterculia foetida\n\nSterculia foetida is a soft wooded tree that can grow up to 35 metres (115 ft.) tall. It was described in 1753 by Carl Linnaeus. Common names for the plant are the bastard poon tree, java olive tree, hazel sterculia, and wild almond tree. This is the type species of the genus \"Sterculia\" and both names mean bad-smelling: the origin of \"Sterculia\" comes from the Roman god, Sterquilinus, who was the god of fertilizer or manure.\n\nThe branches of \"Sterculia foetida\" are arranged in whorls; they spread horizontally. The tree's bark is smooth and grey. The leaves are placed at the end of branchlets; they have 125–230 mm long petioles; the blades are palmately compound, containing 7-9 leaflets. The leaflets are elliptical, 100–170 mm long, and shortly petioluled The petioles are the source of the foul smell of the plant. The flowers are arranged in panicles, 100–150 mm long. The green or purple flowers are large and unisexual as the tree is dioecious (male and female flowers are found on different trees). The calyx is dull orange and is divided into five sepals, each one 10-13 mm long. The fruit consists of four to five follicles, each follicle generally containing 10-15 seeds. The follicles are scarlet when ripe.\nIn India, flowers appear in March, and the leaves appear between March and April. At Hyderabad (India), flowering was observed in September-October (2015) with ripened fruits on the top part and young green fruits at the lower branches. The fruit is ripe in February (11 months after the flowers appeared).\n\nThe oil of \"Sterculia foetida\" has been found to be comparable to sunflower, soybean, and rapeseed oils for the use of biofuels. \"Sterculia foetida\" oil contains cyclopropene fatty acids such as 8,9 methylene-heptadec-8-enoic acid (malvalic acid) and 9,10-methylene-ocadec-9-enoic acid (sterculic acid). The flash point, iodine value, free fatty acid count, phosphorus content, cloud point, pour point, viscosity at 40 °C, oxidative stability at 110 °C, density, and trace metal count are all within ASTM and EN specifications.\n\nEvidence suggests that the seeds of \"Sterculia foetida\" are edible, but they should be roasted prior to eating.\n\n\"Sterculia foetida\" has been found in many areas. These aforementioned areas are India, Taiwan, Indochina, the Philippines (where it is known as kalumpang), United States (Hawaii), Indonesia, Ghana, Australia, Mozambique, and Togo.\n"}
{"id": "22023441", "url": "https://en.wikipedia.org/wiki?curid=22023441", "title": "Sustainable flooring", "text": "Sustainable flooring\n\nSustainable flooring is produced from sustainable materials (and by a sustainable process) that reduces demands on ecosystems during its life-cycle. This includes harvest, production, use and disposal. It is thought that sustainable flooring creates safer and healthier buildings and guarantees a future for traditional producers of renewable resources that many communities depend on. Several initiatives have led the charge to bring awareness of sustainable flooring as well as healthy buildings (air quality). Below are examples of available, though sometimes less well-known, eco-friendly flooring options. The Asthma and Allergy Foundation of America recommends those with allergies to dust or other particulates choose flooring with smooth surfaces – such as hardwood, vinyl, linoleum tile or slate.\n\nIn the U.S., the Building for Energy and Environmental Sustainability (BEES) program of the National Institute of Standards and Technology (NIST) provides a one-stop source of life cycle assessment-based information about flooring options. Life cycle comparisons of flooring alternatives by research groups around the world consistently show bio-based flooring products to have lower environmental impacts than other types of flooring. The life cycle environmental impacts associated with producing and using flooring alternatives such as cork, linoleum, and solid wood are clearly lower than other alternatives. Wool carpeting and composite marble exhibit the greatest impacts, and impacts linked to typical carpeting used in residential structures are higher than those shown in the BEES system due to the use of a pad under the carpet layer.\n\nThe development of life cycle assessment methodology in the early 1990s has shown the environmental advantages of wood and wood-based products.\n\nWood is a unique and renewable material. Trees absorb carbon during their growing cycle, and this carbon remains stored in products like wood flooring during its service life, thus keeping it out of the atmosphere. At the end of its service life, wood can be reused (in which case the carbon continues to be stored in the wood) or used for fuel.\n\nA life cycle assessment of flooring materials made of solid wood, linoleum and vinyl found the wood flooring had lower energy use and carbon dioxide emissions. It also performed better in environmental impact categories such as resource use, environmental toxin emissions, air pollution emissions and waste generation.\n\nWhen reclaimed wood is used for wood flooring, it is taken for reuse from many different sources, including old warehouses, boxcars, coal mines, gymnasiums, homes, wine barrels, historic barns, and more. Wood can also be recovered from rivers in the form of fallen trees along with logs that were once sent downstream by lumber mills.\n\nUsing reclaimed wood can earn credits towards achieving LEED project certification. Because reclaimed wood is considered recycled content, it meets the Materials & Resources criteria for LEED certification and because some reclaimed lumber products are FSC certified, they can qualify for LEED credits under the \"certified wood\" category Besides qualifying for LEED points, reclaimed wood is drawing an increasing number of home and business owners, architects, and contractors to choosing reclaimed wood flooring for a few significant reasons:\n\n· Environmental responsibility and sustainability\n\n· Old growth wood is present in reclaimed examples in a time when it is increasingly difficult to source newly cut old growth lumber due to environmental protections and diminished availability\n\n· Old growth, tight growth rings found in many historic structures adds durability and aesthetic value to flooring\n\n· Client demand for green building materials has greatly increased since 2015\n\n· Developing countries are showing significant increases in green building certification\n\nBamboo flooring is made from a fast-growing renewable \"timber\" (bamboo is actually a grass). It is natural anti-bacterial, water-resistant and extremely durable. DIY installation is easy, as bamboo flooring is available with tongue-and-groove technology familiar in hardwood/laminate alternatives. Bamboo flooring is often more expensive than laminate, though it is generally cheaper than traditional hardwood flooring. Some bamboo floors are less sustainable than others, as they contain the toxic substance formaldehyde (rather than natural-base adhesives).\n\nCork flooring is made by removing the bark of the Cork Oak \"(Quercus Suber)\" without harming the tree (if harvested correctly); as such, it is a renewable and sustainable resource. It is naturally anti-microbial and has excellent insulation properties, ensuring minimal heat loss and comfortable warm walking surface. Cork is resilient and 'springs back' preventing imprints due to heavy traffic and furniture, it also provides excellent noise insulation. While cork itself is low in volatile organic compounds (VOC) emissions, it is important to check the finish applied. Cork is not suitable for bathrooms, as it absorbs moisture.\n\nLinoleum is made from dried and milled flax seeds mixed with other plant material (pine resins, wood flour, ground cork) with a jute backing, all completely natural materials which come from renewable sources and are 100% biodegradable. All by products and waste is milled and used. Linoleum does not fade, as the pigments are embedded in the structure. It is anti-static, repelling dirt, dust and other small particles, making it hypoallergenic – for this reason it is often used by people with respiratory issues (asthma, allergies). It is also fire-resistant and does not require additional fire-retardants finish.\n\nRubber flooring used to be made from a rubber tree, a 100% renewable resource. Today styrene-butadiene rubber (SBR), a general-purpose synthetic rubber, produced from a copolymer of styrene and butadiene is used for \"rubber flooring\" It is easy to install and maintain, is anti-static and provides effective sound insulation and vibration reduction.\nRubber flooring is also resistant to fading and cigarette burns. Most rubber flooring is made from synthetic rubber, which is not a sustainable product.\n\nThere are carpets that are sustainable, using natural fibers such as cotton, sisal, wool, jute and coconut husk. Handmade Citapore rugs include a wide range of sustainable flooring material as these rugs are generally made from cotton (both virgin and recycled), jute, rayon and cotton chennile. It is also possible to have carpet made completely from recycled polyethylene terephthalate used for food/drink containers. Recycled nylon is also a common material used and the process takes carpet made with nylon 6 fibers and recycles it into brand new nylon carpet. This process can be repeated numerous times and in 2009 alone, Shaw's Evergreen facility recycled over 100 million pounds of carpet. This is sustainable and it reduces material sent to landfill; further it uses dyeing methods that are less polluting and require less energy than other flooring. This flooring is sustainable when used alongside eco-friendly adhesive, as some products may have toxic finishes added (stain/fireproofing) that are not considered sustainable.\n\nCoconut timber is a hardwood substitute from coconut palm trees. Coconut palm wood flooring is cheaper than teak, with the wood hardness comparable to mahogany. Coconut palm wood is made from matured (60 to 80 years old) coconut palm trees that no longer bear fruits.\n\n\n"}
{"id": "46734304", "url": "https://en.wikipedia.org/wiki?curid=46734304", "title": "Telstra Business Awards", "text": "Telstra Business Awards\n\nThe Telstra Business Awards is an Australian awards programme started in 1992. The Telstra Business Awards is an independent Awards programme designed to recognise and promote excellence, best practice and innovation in the business community.\n\n\n\n\n\n\n\nThe Westpac Business Owner of the Year and Hudson Business Award National were presented in 2006 for the first and last times in the Telstra Business Awards to date. \n\n\n\n\n\n\n\nThe Telstra Australian Private and Corporate Sector Award was presented in 2007 for the first and last time in the Telstra Business Awards to date. \n\n\n\n\n\n\n\nThe Telstra Social Responsibility Award was inaugurated in 2008. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis was the last year in which the Telstra Australian Innovation Award was presented at the Telstra Business Awards. \n\n\n\n\n\n\n\n\nThe Telstra Australian Start Up Award was inaugurated in 2012. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Telstra Australian Regional Business Award was not awarded in 2016, while the Telstra Australian Start Up Award became the 'New Business' Award, and the Telstra Australian Charity Award was inaugurated. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "372525", "url": "https://en.wikipedia.org/wiki?curid=372525", "title": "The Spike (book)", "text": "The Spike (book)\n\nThe Spike is a 1997 book by Damien Broderick exploring the future of technology, and in particular the concept of the technological singularity.\n\nA revised and updated edition was published in 2001 as \"The Spike: How Our Lives Are Being Transformed by Rapidly Advancing Technologies\", New York: Tom Doherty Associates, 2001, he pbk. Library of Congress T14.B75 2001.\n\n"}
{"id": "22936371", "url": "https://en.wikipedia.org/wiki?curid=22936371", "title": "Tolerance coning", "text": "Tolerance coning\n\nTolerance coning is the engineering discipline of creating a budget of all tolerances that potentially add/subtract to affect adequacy of a particular parameter. This is particularly critical where stages of design/manufacture precede test/use.\n\nFor example, when setting a test limit for a measurement on each manufactured item of some type, to assure that no bad items are shipped, the limit must be tighter than the requirement to allow for the worst case sum of measurement inaccuracies (e.g. equipment, test fixture etc.). The design of the item thus has to take into account not only the product requirement but also the test tolerances. The buildup of this budget is tolerance coning.\nElectronics engineers intuitively do tolerance coning and tend to formalise it for critical parameters. However it is also relevant to other engineering disciplines.\n\n"}
{"id": "21160576", "url": "https://en.wikipedia.org/wiki?curid=21160576", "title": "Tourism Radio", "text": "Tourism Radio\n\nTourism Radio is a travel technology company based in Cape Town, South Africa, which produces location-based technology used in customized GPS in-car devices and IOS and Android mobile applications. The technology allows the user to take a tour of a city or pre-defined area, with geo-located, pre-recorded audio travel information triggering automatically as the user approaches them. The Tourism Radio in-car device plugs into a standard vehicle cigarette lighter and broadcasts a selection of local and international music along with location-specific travel information about the area the vehicle is driving through. Currently, there are approximately 3,300 of these devices used in South Africa, Namibia, New Zealand and Angola. Distribution of the devices to tourists is done in partnership with local Destination Marketing Organisations (DMOs) and car/motorhome rental agencies. The iOS and Android applications do not broadcast a selection of music, but use the same geo-location principle to give the user relevant travel information about the area they are in. This includes information about a general area or suburb, as well as information about specific points of interest, such as museums, art galleries, historical attractions, restaurants, nature walks, beaches, etc.\n\nStarted in Cape Town, South Africa in 2005 by founder and current CEO, Mark Allewell, Tourism Radio was originally created on the concept of touring a city without a tour guide. Allewell used his home town of Cape Town as his starting point, and worked on the development of a GPS device which would play information on points of interest as the user approached them. Months of research and development led to the creation of a bespoke, in-car device. This could play through an ordinary car radio sound system and would play audio information about important sights as they were passed by the vehicle. After a positive local and international response, the company was bought by a German angel investor and expanded operations to Spain and New Zealand. Tourism Radio has since focused on becoming a key player in the digital world by creating mobile apps, with a keen focus on free-to-download audio city guides, for iOS and Android devices.These apps served as a proving ground for Tourism Radio, and led to the development of white label apps for DMOs and corporate entities looking to target the tourism industry. These use Tourism Radio’s technology and are coupled with client-provided content to create company-specific apps.\n\nTourism Radio offers the use of its location-based technology in form of the in-car Tourism Radio devices and its iOS and Android applications.The company has patented its real-world and mobile geo-location technology in New Zealand and Europe, with patents pending in North America and elsewhere.Tourism Radio also features as application software on the Renault R-Link in-car multimedia system. Tourism Radio uses the R-Link's inbuilt GPS system to provide drivers with information about tourist attractions in their vicinity.The Tourism Radio mobile applications for Android and iOS allow the user to download a travel guide and then use it completely offline.\n\n\n"}
{"id": "3986055", "url": "https://en.wikipedia.org/wiki?curid=3986055", "title": "Unimate", "text": "Unimate\n\nUnimate was the first industrial robot,\nwhich worked on a General Motors assembly line at the Inland Fisher Guide Plant in Ewing Township, New Jersey, in 1961.\n\nIt was invented by George Devol in the 1950s using his original patent filed in 1954 and granted in 1961 (). The patent begins:\nThe present invention relates to the automatic operation of machinery, particularly the handling apparatus, and to automatic control apparatus suited for such machinery.\nDevol, together with Joseph Engelberger, his business associate, started the world's first robot manufacturing company, Unimation.\n\nThe machine undertook the job of transporting die castings from an assembly line and welding these parts on auto bodies, a dangerous task for workers, who might be poisoned by toxic fumes or lose a limb if they were not careful.\n\nThe original Unimate consisted of a large computer-like box, joined to another box and was connected to an arm, with systematic tasks stored in a drum memory.\n\nThe Unimate also appeared on \"The Tonight Show\" hosted by Johnny Carson on which it knocked a golf ball into a cup, poured a beer, waved the orchestra conductor's baton and grasped an accordion and waved it around.\n\nIn 2003 the Unimate was inducted into the Robot Hall of Fame.\n\nFictional robots called \"Unimate\", designed by the character \"Alan von Neumann, Jr.\", appeared in comic books from DC Comics.\n\n"}
{"id": "4440608", "url": "https://en.wikipedia.org/wiki?curid=4440608", "title": "Utilization", "text": "Utilization\n\nUtilization may refer to:\n\nIn engineering utilization or operational stage of a system or equipment life-cycle is a period of time when their quality is realized in practical use to achieve intended objectives and supported by accomplishment of storage, maintenance, repair, etc. Subsequently, the term \"utilization\" may have two meaning depending on context: 1) a process of actual use of a system or equipment - in case of simple products and 2) a stage of a complex product life-cycle - in case of use within the product life-cycle management processes. \n\nIn equipment and tool rental companies, utilization is the primary method by which asset performance is measured and business success determined. In basic terms it is a measure of the actual revenue earned by assets against the potential revenue they could have earned. Rental utilization is divided into a number of different calculations, and not all companies work precisely the same way. In general terms however there are 2 key calculations: the physical utilization on the asset, which is measured based on the number of available days for rental against the number of days actually rented. (This may also be measured in hours for certain types of equipment), and the financial utilization on the asset (referred to in North America as $ Utilization) which is measured as the rental revenue achieved over a period of time against the potential revenue that could have been achieved based on a target or standard, non-discounted rate. Physical utilization is also sometimes referred to as spot utilization, where a rental company looks at its current utilization of assets based on a single moment in time (e.g. now, 9 am today, etc.).\n\nUtilization calculations may be varied based on many different factors. For example: \n\nUtilization in this context is heavily linked to profitability. Low physical utilization may be mitigated by keeping rental rates high, high physical utilization normally justifies keeping rental rates lower. Different types of equipment may also alter the relationship between rates and utilization.\n\nCredit utilization is defined as the ratio of an entity's credit card and revolving loan balances to their credit limits; in other words, it represents the amount of their credit limit that is being used. Credit utilization is one of several factors typically used by credit scoring models because it is often correlated with lending risk.\n"}
{"id": "2841596", "url": "https://en.wikipedia.org/wiki?curid=2841596", "title": "Washdown", "text": "Washdown\n\nWashdown (also wash down) is the process of high-pressure cleaning with water and/or chemicals in industries such as food and beverage and pharmaceuticals. A washdown is usually a manual operation and is designed to kill bacteria and other microorganisms. Automatic cleaning of industrial equipment is often carried out by clean-in-place (CIP) and/or steam-in-place (SIP) operations. CIP(CIP) systems often make use of programmable controllers and SCADA software.\n\nWashdown is the process of using a stream of water to clean a flat or nearly flat outside surface. Typically the area cleaned is a large expanse of concrete or asphalt. The area is cleaned of dirt and debris by the use of the force and dissolving power of stream of water as projected from a hose. Generally, one person holds and aims the hose, though if the hose is longer than 300 feet then another person may be required to help move the hose itself. Water is typically squirted from the hose at a pressure of 300 psi. The hose itself can vary in length from 75 to 425 feet. Though typically the hose when used by a single person is 300 feet long. The hose itself carries water from a water main which attaches to the hose through what is called a “stinger“. The stinger screws into the water main pipe and is held there.\n\nAlso, the stinger as it is pushed down into the water supply pipe opens a seal which releases the water into the hose. At the other end of the hose a nozzle regulates the flow of water and allows for a large fanned-out spray, as well as a more concentrated stream termed a needlepoint.\n"}
