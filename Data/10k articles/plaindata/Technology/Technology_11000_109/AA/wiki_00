{"id": "5205110", "url": "https://en.wikipedia.org/wiki?curid=5205110", "title": "Abram Lyle", "text": "Abram Lyle\n\nAbram Lyle (14 December 1820 – 30 April 1891) is noted for founding the sugar refiners \" Abram Lyle & Sons\" which merged with the company of his rival Henry Tate to become Tate & Lyle in 1921.\n\nHe was born on 14 December 1820 in the seaport of Greenock, Renfrewshire, in Scotland, and at twelve years old became an apprentice in a lawyer's office. He then joined his father's cooperage businesses and in partnership with a friend, John Kerr, developed a shipping business, making the Lyle fleet one of the largest in Greenock. The area was heavily involved in the sugar trade with the West Indies, and his business included transporting sugar.\n\nTogether with four partners he purchased the \"Glebe Sugar Refinery\" in 1865, and so added sugar refining to his other business interests. When John Kerr, the principal partner, died in 1872, Lyle sold his shares and began the search for a site for a new refinery.\n\nTogether with his three sons he bought two wharves at Plaistow in East London in 1881 to construct a refinery for producing Golden Syrup. The site happened to be around from the sugar refinery of his rival, Henry Tate. In the first year Lyle's refinery showed a loss of £30,000, with economies being made by asking staff to wait for their wages on occasion, but eventually the business came to dominate the United Kingdom market for \"Golden Syrup\".\n\n\nThe brand, sold in a distinctive green and gold lidded tin with an image of a lion surrounded by bees, is believed to be Britain's oldest. The design of the tin decoration, which includes a biblical quotation, has remained almost unchanged since 1885.\n\nIn the Book of Judges, Samson was travelling to the land of the Philistines in search of a wife. During the journey he killed a lion, and on his return past the same spot he noticed that a swarm of bees had formed a comb of honey in the carcass. Samson later turned this into a riddle at a wedding: \"Out of the eater came forth meat and out of the strong came forth sweetness\".() While no one is sure why this particular quotation was chosen, it has been suggested that it refers either to the strength of the Lyle company which delivers the sweet syrup or possibly even to the trademark tins in which the syrup was sold.\n\nSugar refineries belonging to Tate & Lyle continued as a major industry in Greenock (but with difficulties) until the 1980s, then declining sugar consumption and a shift away from cane sugar led to closure of the last refinery in 1997. There is still a warehouse that was used in the past to store sugar in the town's \"Ocean Terminal\".\n\nLyle was Provost of Greenock from 1876 to 1879. An elder of St Michael's Presbyterian Church in Greenock, Lyle himself chose the biblical quotation for the syrup tins. He was a pious man and a strict teetotaller, who once declared that he would \"rather see a son of his carried home dead than drunk\".\n\nLyle was the son of Abram Lyle and Mary Campbell. He married Mary Park, daughter of William Park, on 14 December 1846 and the couple had five sons and one daughter:\n\nLyle died on 30 April 1891. He has a large memorial in Greenock Cemetery.\n\n"}
{"id": "52204514", "url": "https://en.wikipedia.org/wiki?curid=52204514", "title": "Anker (electronics)", "text": "Anker (electronics)\n\nAnker is a brand of Chinese electronics company Anker Innovations (formerly Oceanwing). based in Shenzhen, Guangdong. The brand is known for producing computer and mobile peripherals, especially power banks.\n\nSteven Yang started Anker in 2011 after working as software engineer for Google in California. He moved to Shenzhen, China. In early 2012 \"Google\"'s then-head of sales in China, Dongping Zhao, was hired and as of 2018 is president of \"Anker Innovations\".\n\nIn 2012, Anker switched its focus from replacement laptop batteries to smartphone battery chargers and wall chargers.\n\nAnker maintains subsidiaries in California, Germany, the United Kingdom, Japan and China as well as contractors in South Korea, Kuwait, Saudi Arabia and Romania. Before 2017 Anker devices were almost exclusively sold at Amazon Marketplace.\n\nAnker charging devices use its \"PowerIQ\" proprietary technology, which detects the connected device and automatically optimizes the current delivered, thereby reducing the amount of time for a device to reach full charge. Many of Anker's charging devices also use various fast-charging technologies, such as Quick Charge. Anker was mentioned in 2015 as the leading brand of mobile chargers on Amazon.com.\n\nOutside of its core competency, Anker also incorporates other product lines:\n\nIn 2014, an Indiegogo campaign to promote a set of magnetic accessories called \"Zolo\" was launched. Due to prolonged manufacturing challenges the project was set on hold and 1863 backers were offered a 200% refund.\n\nIn 2016, a batch of Anker USB-C charging cables suffered a manufacturing defect that could potentially cause damage to connected hardware, leading to a recall.\n"}
{"id": "46672899", "url": "https://en.wikipedia.org/wiki?curid=46672899", "title": "AppsFlyer", "text": "AppsFlyer\n\nAppsFlyer is a SaaS mobile marketing analytics and attribution platform, headquartered in San Francisco, California with offices around the world.\n\nAppsFlyer is a privately held company founded in 2011 by Oren Kaniel, its current CEO, and CTO Reshef Mann. AppsFlyer received its first funding from the Microsoft Ventures Accelerator program.\n\nFollowing the Microsoft Ventures Accelerator, AppsFlyer raised $7 million in Series A funding from Pitango and Magma partners. Using this funding, AppsFlyer developed its proprietary NativeTrack technology to attribute app installs with last click attribution. It also opened offices in New York, San Francisco, Bangkok, Beijing, Berlin, London, Tokyo, Bangalore, Kiev, Seoul and Buenos Aires.\n\nAppsFlyer became a Facebook MMP on 26 December 2012 and a Twitter Official Partner on 30 June 2014. Its platform is now integrated with over 2000 ad networks, including: Yahoo, Google, Bing, iAd, and Criteo.\n\nIn January 2015, AppsFlyer announced that it had raised $20 million in Series B funding from Fidelity Growth Partners Europe, which has helped accelerate the company’s growth. Following 2 years of development, AppsFlyer announced on 17 January 2017 that it had raised an additional $56 million in Series C financing, bringing its total funding to $84 million. The round was led by new investors Qumra Capital, as well as Goldman Sachs Private Capital Investing (PCI), Deutsche Telekom Capital Partners (DTCP) and Pitango Growth.\n\nIn March 2015, AppsFlyer became one of the first members of Fiksu's Preferred Partner Program for Mobile Ad Campaign Tracking.\n\nIn July 2017, Social commerce platform KASKUS partnered with AppsFlyer to bring more accessibility and convenience to users of its social platform.\n\nIn February 2018, AppsFlyer opened a new US headquarters in San Francisco.\n\nIn May 2018, AppsFlyer acquired the development team Yodas. \n\nThe AppsFlyer Software Development Kit is available for iPhone, Android, and Windows Phone apps. In 2014, AppsFlyer developed a complementary product called OneLink which provides deep linking support for custom media sources. In November 2014 TechCrunch article, AppsFlyer was identified by Apptopia as the most widely used SDK for analytics after Flurry.\n\nOnce the AppsFlyer SDK is installed in a mobile app, it provides advertisers with conversion data for their user acquisition and retention campaigns. Advertisers login to their dashboard, and can monitor which media source was responsible for the mobile activity. Based on this information, advertisers then are able to optimize their advertising budget.\n\nAppsFlyer has the following features to help support advertisers' decision-making: Retention reports, Cohort analysis, TV app ad attribution, Bundled app tracking for iOS, OneLink, its mobile deep linking solution, Rich in-app events and install tracking. AppsFlyer's product dashboard is currently in English, Chinese, Japanese, French and Spanish.\n\n\n"}
{"id": "11630614", "url": "https://en.wikipedia.org/wiki?curid=11630614", "title": "Atomtronics", "text": "Atomtronics\n\nAtomtronics is an emerging sub-field of ultracold atomic physics which encompasses a broad range of topics featuring guided atomic matter waves. The systems typically include components analogous to those found in electronic or optical systems, such as beam splitters and transistors. Applications range from studies of fundamental physics to the development of practical devices.\n\nAtomtronics is a contraction of \"atom\" and \"electronics\", in reference to the creation of atomic analogues of electronic components, such as transistors and diodes, and also electronic materials such as semiconductors. The field itself has considerable overlap with atom optics and quantum simulation, and is not strictly limited to the development of electronic-like components.\n\nThree major elements are required for an atomtronic circuit. The first is a Bose-Einstein condensate, which is needed for its coherent and superfluid properties, although an ultracold Fermi gas may also be used for certain applications. The second is a tailored trapping potential, which can be generated optically, magnetically, or using a combination of both. The final element is a method to induce movement of atoms within the potential, which can be achieved in a number of ways. For example, a transistor-like atomtronic circuit may be realized by a ring-shaped trap divided into two by two moveable weak barriers, with the two separate parts of the ring acting as the drain and the source, and the barriers acting as the gate. As the barriers move, atoms flow from the source to the drain.\n\nThe field of atomtronics is still very young. Any schemes realized thus far are proof-of-principle. Applications include:\n\nObstacles to the development of practical sensing devices are largely due to the technical challenges of creating Bose-Einstein condensates, as they require bulky lab-based setups not easily suitable for transportation, although creating portable experimental setups is an active area of research.\n\n"}
{"id": "2770849", "url": "https://en.wikipedia.org/wiki?curid=2770849", "title": "Auto mechanic", "text": "Auto mechanic\n\nAn auto mechanic (automotive technician in most of North America, car mechanic in British English, and motor mechanic in Australian English) is a mechanic with a variety of automobile makes or either in a specific area or in a specific make of automobile. In repairing cars, their main role is to diagnose the problem accurately and quickly. They often have to quote prices for their customers before commencing work or after partial disassembly for inspection. Their job may involve the repair of a specific part or the replacement of one or more parts as assemblies.\n\nBasic vehicle maintenance is a fundamental part of a mechanic's work in modern industrialized countries while in others they are only consulted when a vehicle is already showing signs of malfunction. Preventative maintenance is also a fundamental part of a mechanic's job, but this is not possible in the case of vehicles that are not regularly maintained by a mechanic. One misunderstood aspect of preventative maintenance is \"scheduled replacement\" of various parts, which occurs before failure to avoid far more expensive damage. Because this means that parts are replaced before any problem is observed, many vehicle owners will not understand why the expense is necessary.\n\nWith the rapid advancement in technology, the mechanic's job has evolved from purely mechanical, to include electronic technology. Because vehicles today possess complex computer and electronic systems, mechanics need to have a broader base of knowledge than in the past.\n\nDue to the increasingly labyrinthine nature of the technology that is now incorporated into automobiles, most automobile dealerships and independent workshops now provide sophisticated diagnostic computers to each technician, without which they would be unable to diagnose or repair a vehicle.\n\nIn the United States, many programs and schools offer training for those interested in pursuing competencies as automotive mechanics or technicians. Areas of training include automobile repair and maintenance, collision repair, painting and restoring, electronics, air-conditioning and heating systems, and truck and diesel mechanics. The National Automotive Technicians Education Foundation (NATEF) is responsible for evaluating technician training programs against standards developed by the automotive industry. NATEF accredits programs in four different categories: automotive, collision, trucks (diesel technology) and alternative fuels. NATEF lists secondary and post secondary schools with accredited programs on their website.\n\nSome mechanics are ASE certified, which is a standardized method of testing skill and knowledge level. While it's not required by law for a mechanic to be certified, some companies only hire or promote employees who have passed ASE tests. The technology used in automobiles changes very rapidly and the mechanic must be prepared to learn these new technologies and systems. The auto mechanic has a physically demanding job, often exposed to extreme temperatures, lifting heavy objects and staying in uncomfortable positions for extended periods. They also may deal with exposure to toxic chemicals.\n\nThe internet is being applied to the field increasingly often, with mechanics providing advice on-line. Mechanics themselves now regularly use the internet for information to help them in diagnosing and/or repairing vehicles. A popular resource for this type of research is the video website YouTube, where professional and DIY mechanics alike film and share their experiences on vehicle diagnostics and repair. Paper based service manuals for vehicles have become significantly less prevalent with computers that are connected to the Internet taking their position, giving quick access to a plethora of technical manuals and information.\n\nA mechanic usually works from the workshop in which the (well equipped) mechanic has access to a vehicle lift to access areas that are difficult to reach when the car is on the ground. Beside the workshop bound mechanic, there are mobile mechanics like those of the UK Automobile Association (the AA) which allow the car owner to receive assistance without the car necessarily having to be brought to a garage.\n\nA mechanic may opt to engage in other careers related to his or her field. Teaching of automotive trade courses, for example, is almost entirely carried out by qualified mechanics in many countries.\n\nThere are several other trade qualifications for working on motor vehicles, including \"panel beater\", \"spray painter\", \"body builder\" and \"motorcycle mechanic\". In most developed countries, these are separate trade courses, but a qualified tradesperson from one can change to working as another. This usually requires that they work under another tradesperson in much the same way as an apprentice.\n\nAuto body repair involves less work with oily and greasy parts of vehicles, but involves exposure to particulate dust from sanding bodywork and potentially toxic chemical fumes from paint and related products. Salespeople and dealers often also need to acquire an in-depth knowledge of cars, and some mechanics are successful in these roles because of their knowledge. Auto mechanics also need to stay updated with all the leading car companies as well as new launching cars. One has to study continuously on new technology engines and their work systems.\n\nPit crews for motor racing are a specialized form of work undertaken by some mechanics. It is sometimes portrayed as glamorous in movies and television and is considered prestigious in some parts of the automotive industry. Working in a pit crew in professional racing circuits is potentially dangerous and very stressful work due to the tight margins for error, and the potential financial losses and gains by the racing teams, but a pit crew mechanics pay is usually high to reflect the extra skill/stress levels.\n\n"}
{"id": "31976229", "url": "https://en.wikipedia.org/wiki?curid=31976229", "title": "Automated restaurant", "text": "Automated restaurant\n\nAutomated restaurant or robotic restaurant is a restaurant that uses robots to do tasks such as delivering food and drinks to the tables and/or to cook the food.\n\nRestaurant automation means the use of restaurant management system to automate the major operations of a restaurant establishment. Even in the early 1970s, a number of restaurants served foods solely through vending machines. Called automats, or in Japan shokkenki, customers ordered their foods directly through the machines. Across Europe, McDonald's has already begun implementing 7,000 touch screen kiosks that will handle cashiering duties.\n\nMore recently, restaurants are opening that have completely or partially automated their services. These may include taking of orders, preparing of food, serving and billing. A few fully automated restaurants operate without any human intervention whatsoever. Robots are designed to help and sometimes replace human labour (such as waiters and chefs). Automation of restaurants also allows the option for greater customization of an order.\n\nAutomated restaurants have been opening in many countries. Examples include Fritz's Railroad Restaurant in Kansas City, Missouri, United States, Výtopna (Railway Restaurant), a franchise of various restaurants and coffeehouses in Czech Republic, Bagger's Restaurant in Nuremberg, Germany, FuA-Men Restaurant, a ramen restaurant located in Nagoya, Japan, Hajime Robot Restaurant, a Japanese restaurant in Bangkok, Thailand, Fōster Nutrition, in Buenos Aires, Argentina, the Dalu Robot Restaurant in Jinan, China Haohai Robot Restaurant in Harbin, China and Robot Kitchen Restaurant in Hong Kong. Robot Chacha, the first robot restaurant of India is due to open in capital city, New Delhi. MIT graduates opened Spyce Kitchens in downtown Boston, Massachusetts, United States in 2018.\n\n\n"}
{"id": "27719906", "url": "https://en.wikipedia.org/wiki?curid=27719906", "title": "Bakki shower", "text": "Bakki shower\n\nBakki showers are a type of trickle filtration.\n\nFirst developed by the Momotaro Japanese Koi farm, the bakki shower attempts to simulate natures way of filtering water. Similar to a mountain stream where water cascades over rocks.\n\nBiofilms and algae grow on the rocks and feed on the nutrients in the stream, essentially filtering the water. In the case of the Bakki shower, ceramic is placed in trays and the water pumped to the top of the filter and trickles through the media, eventually flowing back to the pond.\n\nMomotaro Koi Farms developed the bakki shower with stainless steel trays. Since that time many \"DIY bakki showers\" have been designed with variations on them. Plastic and fiberglass boxes or crates have been used with success.\n\nIn all its simplicity there is a lot of higher chemistry going on between the bio film and the water column. The biochemistry is better explained in this book : \"Biology of the nitrogen cycle\" By Hermann Bothe, Stuart John Ferguson, William Edward Newton\n\nThe advantages of this type of filtration are:\n\nThis method of off-gassing and aeration is used widely in aquaculture.\n\nThe choices of Filter media vary greatly. Each filter media has different properties and therefore different performance. Here are some photos of different types:\n"}
{"id": "25037528", "url": "https://en.wikipedia.org/wiki?curid=25037528", "title": "Bernhard Hassenstein", "text": "Bernhard Hassenstein\n\nBernhard Hassenstein (31 May 1922 – 16 April 2016) was a German biologist and behaviorist.\n\nBernhard Hassenstein was a student of behavioral physiologist Erich von Holst and one of the leading researchers in the fields of behavioral biology and bio-cybernetics. His scientific work includes substantial contributions to the understanding of motion perception in insects and color vision in humans.\n\nFrom 1939 to 1949, Hassenstein studied biology, physics, and chemistry in Berlin, Göttingen, and Heidelberg. During his military service in 1943, he met Werner E. Reichardt, who eventually became his academic partner. In 1948, he worked as an assistant at the Max Planck Institute for Marine Biology in Wilhelmshaven. From 1954-1958, he worked at the Zoophysiological Institute of the University of Tübingen.\n\nIn 1958, Hassenstein worked with physicist Werner Reichardt and engineer Hans Wenking to found the world's first working group on cybernetics at the Max Planck Institute for Biology in Tübingen. In 1960, he was appointed Professor of Zoology at the University of Freiburg.\n\nHassenstein retired in 1984. He reportedly died on 16 April 2016 in Freiburg, at the age of 93.\n\n\n\n\n\n\nUniversity of Freiburg Faculty of Biology\n"}
{"id": "23467149", "url": "https://en.wikipedia.org/wiki?curid=23467149", "title": "Bio-Synthesis, Inc.", "text": "Bio-Synthesis, Inc.\n\nBio-Synthesis, Inc. (BSI) is a biotechnology company headquartered in Lewisville, Texas. It is a provider of custom and catalog peptides, custom oligos, antibodies, organic synthesis, and analytical services. Biomedical researchers worldwide in universities, biotech companies, private clinics, and government agencies use products from Bio-Synthesis, Inc. in studies ranging from PCR diagnostics to cancer research and the Human Genome Project.\n\nThe DNA Identity Testing Center, a subsidiary of Bio-Synthesis, Inc. (BSI), works with private citizens, legal representatives, and law enforcement providing services in personal and legal paternity tests, family relationship DNA tests, immigration DNA tests, forensic DNA tests, ancestry DNA tests, and identity DNA tests.\n\nFounded in 1984, Bio-Synthesis, Inc. was known as OCS Laboratories and was one of the first companies providing commercially available synthetic oligonucleotides to the biomedical research community worldwide.\n\nIt was the first producer of commercially available synthetic DNA and became a producer of synthetic peptides in 1985, and became the only company to provide both synthetic DNA and peptide under one roof. Also in 1985 the process, now known as PCR, was discovered by Mullis \"et al.\" A key activity for Bio-Synthesis was to synthesize large number of PCR primer thus assisting and solidifying the early adoption of this now common and crucial process in biology.\n\nIn 1988, Bio-Synthesis helped in the synthesis and characterization of a new class of peptides with novel antimicrobial properties discovered at the NIH. \n\nIn 1989, OCS became incorporated as Bio-synthesis, Inc. and moved its laboratories to Lewisville, Texas.\n\nIn 1993, Bio-Synthesis was one of the first peptide synthesis companies to acquire a Finnigan MALDI-TOF mass spectrometer for the accurate quality control of synthetic peptides produced in-house.\n\nIn 1994, Bio-synthesis pioneered the use of molecular methods for HLA analysis which is applied in organ matching for transplantation purposes. Later in the same year Bio-Synthesis held the first major HLA DNA typing workshop with the attendance of HLA laboratory directors from around the country in conjunction with University of North Texas in Denton Texas.\n\nIn 1995, Bio-Synthesis, Inc. introduced DNA identity testing to its broad range of molecular diagnostic services. Today, the DNA Identity Testing Center of BSI uses advanced techniques and automated systems to produce the most accurate DNA testing results allowed by current scientific technologies. The DNA Identity Testing Center has its own staff of scientific professionals whose focus is to ensure the quality and integrity of all tests and services.\n\nIn 2000, Bio-Synthesis, Inc. perform the genetic analysis close to 500 Chinese nationals that were soliciting, under the right of abode, the right to move to Hong Kong by proving that their biological fathers were Hong Kong residents.\n\nIn June 2007, Bio-Synthesis, Inc. unveiled a new online software tool called Protein Lounge which brought all of the vital and necessary databases to one place. The Protein Database contained all of the necessary information for proteins, such as datasheets, reviews, signaling pathway relation, disease relation, sequences, publications and reagent links. The database offered the most comprehensive gene/protein sequence pages which have all pertinent information needed to analyze any sequence.\n\nIn January 2008, Bio-Synthesis, Inc. announced an exclusive scientific collaboration with Dr. Dante Marciani, a world-renowned expert in immune agonists. The collaboration was focused on proprietary novel glycosides that stimulate innate immunity while taking advantage of the synergistic effects between innate and adaptive immunity. In addition, the collaboration extended to proprietary compounds that down regulate The immunity, an area of significance in the treatment of chronic inflammatory conditions. \n\nIn May 2009, Bio-Synthesis, Inc. began Mitochondrial, or DNA, testing and offered it commercially. The DNA Identity Testing Laboratory at Bio-Synthesis, Inc. (BSI) is currently sequencing DNA samples for maternal lineage and criminal investigations.\n\nIn February 2010, Bio-synthesis has been awarded a 2-year, $590,000, Small Business and Innovation Research (SBIR) Phase 1 grant from the National Institute of Allergy and Infectious Diseases (NIAID) of the National Institutes of Health (NIH). The company will use the proceeds to further the development of its proprietary adjuvants or immune agonists that carry in a single molecule the various determinants needed to stimulate both innate and adaptive immunities, causing synergistic effects on T cell immunity.\n\nIn March 2010, Bio-Synthesis launched a new custom service, PenetraINS, whereby two different molecules are joined together via selective chemical coupling for the purpose of up or down regulating gene expression in mammalian organisms.\n\n\n"}
{"id": "297203", "url": "https://en.wikipedia.org/wiki?curid=297203", "title": "Blast furnace", "text": "Blast furnace\n\nA blast furnace is a type of metallurgical furnace used for smelting to produce industrial metals, generally pig iron, but also others such as lead or copper. \"Blast\" refers to the combustion air being \"forced\" or supplied above atmospheric pressure.\n\nIn a blast furnace, fuel (coke), ores, and flux (limestone) are continuously supplied through the top of the furnace, while a hot blast of air (sometimes with oxygen enrichment) is blown into the lower section of the furnace through a series of pipes called tuyeres, so that the chemical reactions take place throughout the furnace as the material falls downward. The end products are usually molten metal and slag phases tapped from the bottom, and flue gases exiting from the top of the furnace. The downward flow of the ore and flux in contact with an upflow of hot, carbon monoxide-rich combustion gases is a countercurrent exchange and chemical reaction process.\n\nIn contrast, air furnaces (such as reverberatory furnaces) are naturally aspirated, usually by the convection of hot gases in a chimney flue. According to this broad definition, bloomeries for iron, blowing houses for tin, and smelt mills for lead would be classified as blast furnaces. However, the term has usually been limited to those used for smelting iron ore to produce pig iron, an intermediate material used in the production of commercial iron and steel, and the shaft furnaces used in combination with sinter plants in base metals smelting.\n\nCast iron has been found in China dating to the 5th century BC, but the earliest extant blast furnaces in China date to the 1st century AD and in the West from the High Middle Ages. They spread from the region around Namur in Wallonia (Belgium) in the late 15th century, being introduced to England in 1491. The fuel used in these was invariably charcoal. The successful substitution of coke for charcoal is widely attributed to English inventor Abraham Darby in 1709. The efficiency of the process was further enhanced by the practice of preheating the combustion air (hot blast), patented by Scottish inventor James Beaumont Neilson in 1828.\n\nArchaeological evidence shows that bloomeries appeared in China around 800 BC. Originally it was thought that the Chinese started casting iron right from the beginning, but this theory has since been debunked by the discovery of 'more than ten' iron digging implements found in the tomb of Duke Jing of Qin (d. 537 BC), whose tomb is located in Fengxiang County, Shaanxi (a museum exists on the site today). There is however no evidence of the bloomery in China after the appearance of the blast furnace and cast iron. In China blast furnaces produced cast iron, which was then either converted into finished implements in a cupola furnace, or turned into wrought iron in a fining hearth.\n\nAlthough cast iron farm tools and weapons were widespread in China by the 5th century BC, employing workforces of over 200 men in iron smelters from the 3rd century onward, the earliest extant blast furnaces were built date to the Han Dynasty in the 1st century AD. These early furnaces had clay walls and used phosphorus-containing minerals as a flux. Chinese blast furnaces ranged from around two to ten meters in height, depending on the region. The largest ones were found in modern Sichuan and Guangdong, while the 'dwarf\" blast furnaces were found in Dabieshan. In construction, they are both around the same level of technological sophistication \n\nThe effectiveness of the Chinese blast furnace was enhanced during this period by the engineer Du Shi (c. AD 31), who applied the power of waterwheels to piston-bellows in forging cast iron. Donald Wagner suggests that early blast furnace and cast iron production evolved from furnaces used to melt bronze. Certainly, though, iron was essential to military success by the time the State of Qin had unified China (221 BC). Usage of the blast and cupola furnace remained widespread during the Song and Tang Dynasties. By the 11th century, the Song Dynasty Chinese iron industry made a switch of resources from charcoal to coke in casting iron and steel, sparing thousands of acres of woodland from felling. This may have happened as early as the 4th century AD.\n\nThe primary advantage of the early blast furnace was in large scale production and making iron implements more readily available to peasants. Cast iron is more brittle than wrought iron or steel, which required additional fining and then cementation or co-fusion to produce, but for menial activities such as farming it sufficed. By using the blast furnace, it was possible to produce larger quantities of tools such as ploughshares more efficiently than the bloomery. In areas where quality was important, such as warfare, wrought iron and steel were preferred. Nearly all Han period weapons are made of wrought iron or steel, with the exception of axe-heads, of which many are made of cast iron.\n\nBlast furnaces were also later used to produce gunpowder weapons such as cast iron bomb shells and cast iron cannons during the Song dynasty.\n\nThe simplest forge, known as the Corsican, was used prior to the advent of Christianity. Examples of improved bloomeries are the (sometimes called wolf-furnace) or the Catalan forge, which remained until the beginning of the 19th century. The Catalan forge was invented in Catalonia, Spain, during the 8th century. Instead of using natural draught, air was pumped in by a \"trompe\", resulting in better quality iron and an increased capacity. This pumping of airstream in with bellows is known as \"cold blast\", and it increases the fuel efficiency of the bloomery and improves yield. The Catalan forges can also be built bigger than natural draught bloomeries.\n\nThe oldest known blast furnaces in the West were built in Dürstel in Switzerland, the Märkische Sauerland in Germany, and at Lapphyttan in Sweden, where the complex was active between 1205 and 1300. At Noraskog in the Swedish parish of Järnboås, there have also been found traces of blast furnaces dated even earlier, possibly to around 1100. These early blast furnaces, like the Chinese examples, were very inefficient compared to those used today. The iron from the Lapphyttan complex was used to produce balls of wrought iron known as osmonds, and these were traded internationally – a possible reference occurs in a treaty with Novgorod from 1203 and several certain references in accounts of English customs from the 1250s and 1320s. Other furnaces of the 13th to 15th centuries have been identified in Westphalia.\n\nThe technology required for blast furnaces may have either been transferred from China, or may have been an indigenous innovation. Al-Qazvini in the 13th century and other travellers subsequently noted an iron industry in the Alburz Mountains to the south of the Caspian Sea. This is close to the silk route, so that the use of technology derived from China is conceivable. Much later descriptions record blast furnaces about three metres high. As the Varangian Rus' people from Scandinavia traded with the Caspian (using their Volga trade route, it is possible that the technology reached Sweden by this means. High quality ores, water power for bellows for blast and wood for charcoal are readily obtainable in Sweden. However, since blast furnace has also been invented independently in Africa by the Haya people, it is more likely the process has been invented in Scandinavia independently. The step from bloomery to true blast furnace is not big. Simply just building a bigger furnace and using bigger bellows to increase the volume of the blast and hence the amount of oxygen leads inevitably into higher temperatures, bloom melting into liquid iron and, cast iron flowing from the smelters. Already the Vikings are known to have used double bellows, which greatly increases the volumetric flow of the blast.\n\nThis Caspian region may also separately be the technological source for at furnace at Ferriere, described by Filarete. Water-powered bellows at in northern Italy in 1226 in a two-stage process. In this, the molten iron was tapped twice a day into water thereby granulating it.\n\nOne means by which certain technological advances were transmitted within Europe was a result of the General Chapter of the Cistercian monks. This may have included the blast furnace, as the Cistercians are known to have been skilled metallurgists. According to Jean Gimpel, their high level of industrial technology facilitated the diffusion of new techniques: \"Every monastery had a model factory, often as large as the church and only several feet away, and waterpower drove the machinery of the various industries located on its floor.\" Iron ore deposits were often donated to the monks along with forges to extract the iron, and within time surpluses were being offered for sale. The Cistercians became the leading iron producers in Champagne, France, from the mid-13th century to the 17th century, also using the phosphate-rich slag from their furnaces as an agricultural fertilizer.\n\nArchaeologists are still discovering the extent of Cistercian technology. At Laskill, an outstation of Rievaulx Abbey and the only medieval blast furnace so far identified in Britain, the slag produced was low in iron content. Slag from other furnaces of the time contained a substantial concentration of iron, whereas Laskill is believed to have produced cast iron quite efficiently. Its date is not yet clear, but it probably did not survive until Henry VIII's Dissolution of the Monasteries in the late 1530s, as an agreement (immediately after that) concerning the \"smythes\" with the Earl of Rutland in 1541 refers to blooms. Nevertheless, the means by which the blast furnace spread in medieval Europe has not finally been determined.\n\nDue to the casting of cannon, the blast furnace came into widespread use in France in the mid 15th century.\n\nThe direct ancestor of these used in France and England was in the Namur region in what is now Wallonia (Belgium). From there, they spread first to the Pays de Bray on the eastern boundary of Normandy and from there to the Weald of Sussex, where the first furnace (called Queenstock) in Buxted was built in about 1491, followed by one at Newbridge in Ashdown Forest in 1496. They remained few in number until about 1530 but many were built in the following decades in the Weald, where the iron industry perhaps reached its peak about 1590. Most of the pig iron from these furnaces was taken to finery forges for the production of bar iron.\n\nThe first British furnaces outside the Weald appeared during the 1550s, and many were built in the remainder of that century and the following ones. The output of the industry probably peaked about 1620, and was followed by a slow decline until the early 18th century. This was apparently because it was more economic to import iron from Sweden and elsewhere than to make it in some more remote British locations. Charcoal that was economically available to the industry was probably being consumed as fast as the wood to make it grew. The Backbarrow blast furnace built in Cumbria in 1711 has been described as the first efficient example.\n\nThe first blast furnace in Russia opened in 1637 near Tula and was called the Gorodishche Works. The blast furnace spread from here to the central Russia and then finally to the Urals.\n\nIn 1709, at Coalbrookdale in Shropshire, England, Abraham Darby began to fuel a blast furnace with coke instead of charcoal. Coke's initial advantage was its lower cost, mainly because making coke required much less labor than cutting trees and making charcoal, but using coke also overcame localized shortages of wood, especially in Britain and on the Continent. Metallurgical grade coke will bear heavier weight than charcoal, allowing larger furnaces. A disadvantage is that coke contains more impurities than charcoal, with sulfur being especially detrimental to the iron's quality. Coke's impurities were more of a problem before hot blast reduced the amount of coke required and before furnace temperatures were hot enough to make slag from limestone free flowing. (Limestone ties up sulfur. Manganese may also be added to tie up sulfur).\n\nCoke iron was initially only used for foundry work, making pots and other cast iron goods. Foundry work was a minor branch of the industry, but Darby's son built a new furnace at nearby Horsehay, and began to supply the owners of finery forges with coke pig iron for the production of bar iron. Coke pig iron was by this time cheaper to produce than charcoal pig iron. The use of a coal-derived fuel in the iron industry was a key factor in the British Industrial Revolution. Darby's original blast furnace has been archaeologically excavated and can be seen in situ at Coalbrookdale, part of the Ironbridge Gorge Museums. Cast iron from the furnace was used to make girders for the world's first iron bridge in 1779. The Iron Bridge crosses the River Severn at Coalbrookdale and remains in use for pedestrians.\n\nThe steam engine was applied to power blast air, overcoming a shortage of water power in areas where coal and iron ore were located. The cast iron blowing cylinder was developed in 1768 to replace the leather bellows, which wore out quickly. The steam engine and cast iron blowing cylinder led to a large increase in British iron production in the late 18th century.\n\nHot Blast was the single most important advance in fuel efficiency of the blast furnace and was one of the most important technologies developed during the Industrial Revolution. Hot blast was patented by James Beaumont Neilson at Wilsontown Ironworks in Scotland in 1828. Within a few years of the introduction, hot blast was developed to the point where fuel consumption was cut by one-third using coke or two-thirds using coal, while furnace capacity was also significantly increased. Within a few decades, the practice was to have a \"stove\" as large as the furnace next to it into which the waste gas (containing CO) from the furnace was directed and burnt. The resultant heat was used to preheat the air blown into the furnace.\n\nHot blast enabled the use of raw anthracite coal, which was difficult to light, to the blast furnace. Anthracite was first tried successfully by George Crane at Ynyscedwyn Ironworks in south Wales in 1837. It was taken up in America by the Lehigh Crane Iron Company at Catasauqua, Pennsylvania, in 1839. Anthracite use declined when very high capacity blast furnaces requiring coke were built in the 1870s.\n\nThe blast furnace remains an important part of modern iron production. Modern furnaces are highly efficient, including Cowper stoves to pre-heat the blast air and employ recovery systems to extract the heat from the hot gases exiting the furnace. Competition in industry drives higher production rates. The largest blast furnaces have a volume around and can produce around of iron per week.\n\nThis is a great increase from the typical 18th-century furnaces, which averaged about per year. Variations of the blast furnace, such as the Swedish electric blast furnace, have been developed in countries which have no native coal resources.\n\nBlast furnaces are currently rarely used in copper smelting, but modern lead smelting blast furnaces are much shorter than iron blast furnaces and are rectangular in shape. The overall shaft height is around 5 to 6 m. Modern lead blast furnaces are constructed using water-cooled steel or copper jackets for the walls, and have no refractory linings in the side walls. The base of the furnace is a hearth of refractory material (bricks or castable refractory). Lead blast furnaces are often open-topped rather than having the charging bell used in iron blast furnaces.\n\nThe blast furnace used at the Nyrstar Port Pirie lead smelter differs from most other lead blast furnaces in that it has a double row of tuyeres rather than the single row normally used. The lower shaft of the furnace has a chair shape with the lower part of the shaft being narrower than the upper. The lower row of tuyeres being located in the narrow part of the shaft. This allows the upper part of the shaft to be wider than the standard.\n\nThe blast furnaces used in the Imperial Smelting Process (\"ISP\") were developed from the standard lead blast furnace, but are fully sealed. This is because the zinc produced by these furnaces is recovered as metal from the vapor phase, and the presence of oxygen in the off-gas would result in the formation of zinc oxide.\n\nBlast furnaces used in the ISP have a more intense operation than standard lead blast furnaces, with higher air blast rates per m of hearth area and a higher coke consumption.\n\nZinc production with the ISP is more expensive than with electrolytic zinc plants, so several smelters operating this technology have closed in recent years. However, ISP furnaces have the advantage of being able to treat zinc concentrates containing higher levels of lead than can electrolytic zinc plants.\n\nModern furnaces are equipped with an array of supporting facilities to increase efficiency, such as ore storage yards where barges are unloaded. The raw materials are transferred to the stockhouse complex by ore bridges, or rail hoppers and ore transfer cars. Rail-mounted scale cars or computer controlled weight hoppers weigh out the various raw materials to yield the desired hot metal and slag chemistry. The raw materials are brought to the top of the blast furnace via a skip car powered by winches or conveyor belts.\n\nThere are different ways in which the raw materials are charged into the blast furnace. Some blast furnaces use a \"double bell\" system where two \"bells\" are used to control the entry of raw material into the blast furnace. The purpose of the two bells is to minimize the loss of hot gases in the blast furnace. First, the raw materials are emptied into the upper or small bell which then opens to empty the charge into the large bell. The small bell then closes, to seal the blast furnace, while the large bell rotates to provide specific distribution of materials before dispensing the charge into the blast furnace. A more recent design is to use a \"bell-less\" system. These systems use multiple hoppers to contain each raw material, which is then discharged into the blast furnace through valves. These valves are more accurate at controlling how much of each constituent is added, as compared to the skip or conveyor system, thereby increasing the efficiency of the furnace. Some of these bell-less systems also implement a discharge chute in the throat of the furnace (as with the Paul Wurth top) in order to precisely control where the charge is placed.\n\nThe iron making blast furnace itself is built in the form of a tall structure, lined with refractory brick, and profiled to allow for expansion of the charged materials as they heat during their descent, and subsequent reduction in size as melting starts to occur. Coke, limestone flux, and iron ore (iron oxide) are charged into the top of the furnace in a precise filling order which helps control gas flow and the chemical reactions inside the furnace. Four \"uptakes\" allow the hot, dirty gas high in carbon monoxide content to exit the furnace throat, while \"bleeder valves\" protect the top of the furnace from sudden gas pressure surges. The coarse particles in the exhaust gas settle in the \"dust catcher\" and are dumped into a railroad car or truck for disposal, while the gas itself flows through a venturi scrubber and/or electrostatic precipitators and a gas cooler to reduce the temperature of the cleaned gas.\n\nThe \"casthouse\" at the bottom half of the furnace contains the bustle pipe, water cooled copper tuyeres and the equipment for casting the liquid iron and slag. Once a \"taphole\" is drilled through the refractory clay plug, liquid iron and slag flow down a trough through a \"skimmer\" opening, separating the iron and slag. Modern, larger blast furnaces may have as many as four tapholes and two casthouses. Once the pig iron and slag has been tapped, the taphole is again plugged with refractory clay.\n\nThe tuyeres are used to implement a hot blast, which is used to increase the efficiency of the blast furnace. The hot blast is directed into the furnace through water-cooled copper nozzles called tuyeres near the base. The hot blast temperature can be from 900 °C to 1300 °C (1600 °F to 2300 °F) depending on the stove design and condition. The temperatures they deal with may be 2000 °C to 2300 °C (3600 °F to 4200 °F). Oil, tar, natural gas, powdered coal and oxygen can also be injected into the furnace at tuyere level to combine with the coke to release additional energy and increase the percentage of reducing gases present which is necessary to increase productivity.\n\nBlast furnaces operate on the principle of chemical reduction whereby carbon monoxide, having a stronger affinity for the oxygen in iron ore than iron does, reduces the iron to its elemental form. Blast furnaces differ from bloomeries and reverberatory furnaces in that in a blast furnace, flue gas is in direct contact with the ore and iron, allowing carbon monoxide to diffuse into the ore and reduce the iron oxide to elemental iron mixed with carbon. The blast furnaces operates as a countercurrent exchange process whereas a bloomery does not. Another difference is that bloomeries operate as a batch process while blast furnaces operate continuously for long periods because they are difficult to start up and shut down. (See: Continuous production) Also, the carbon in pig iron lowers the melting point below that of steel or pure iron; in contrast, iron does not melt in a bloomery.\n\nCarbon monoxide also reduces silica which has to be removed from the pig iron. The silica is reacted with calcium oxide (burned limestone) and forms a slag which floats to the surface of the molten pig iron. The direct contact of flue gas with the iron causes contamination with sulfur if it is present in the fuel. Historically, to prevent contamination from sulfur, the best quality iron was produced with charcoal.\n\nThe downward moving column of ore, flux, coke or charcoal and reaction products must be porous enough for the flue gas to pass through. This requires the coke or charcoal to be in large enough particles to be permeable, meaning there cannot be an excess of fine particles. Therefore, the coke must be strong enough so it will not be crushed by the weight of the material above it. Besides physical strength of the coke, it must also be low in sulfur, phosphorus, and ash. This necessitates the use of metallurgical coal, which is a premium grade due to its relative scarcity.\n\nThe main chemical reaction producing the molten iron is:\n\nThis reaction might be divided into multiple steps, with the first being that preheated blast air blown into the furnace reacts with the carbon in the form of coke to produce carbon monoxide and heat:\n\nThe hot carbon monoxide is the reducing agent for the iron ore and reacts with the iron oxide to produce molten iron and carbon dioxide. Depending on the temperature in the different parts of the furnace (warmest at the bottom) the iron is reduced in several steps. At the top, where the temperature usually is in the range between 200 °C and 700 °C, the iron oxide is partially reduced to iron(II,III) oxide, FeO.\n\nAt temperatures around 850 °C, further down in the furnace, the iron(II,III) is reduced further to iron(II) oxide:\n\nHot carbon dioxide, unreacted carbon monoxide, and nitrogen from the air pass up through the furnace as fresh feed material travels down into the reaction zone. As the material travels downward, the counter-current gases both preheat the feed charge and decompose the limestone to calcium oxide and carbon dioxide:\n\nThe calcium oxide formed by decomposition reacts with various acidic impurities in the iron (notably silica), to form a fayalitic slag which is essentially calcium silicate, CaSiO:\n\nAs the iron(II) oxide moves down to the area with higher temperatures, ranging up to 1200 °C degrees, it is reduced further to iron metal:\n\nThe carbon dioxide formed in this process is re-reduced to carbon monoxide by the coke:\n\nThe temperature-dependent equilibrium controlling the gas atmosphere in the furnace is called the Boudouard reaction:\n\nThe \"pig iron\" produced by the blast furnace has a relatively high carbon content of around 4–5%, making it very brittle, and of limited immediate commercial use. Some pig iron is used to make cast iron. The majority of pig iron produced by blast furnaces undergoes further processing to reduce the carbon content and produce various grades of steel used for construction materials, automobiles, ships and machinery.\n\nAlthough the efficiency of blast furnaces is constantly evolving, the chemical process inside the blast furnace remains the same. According to the American Iron and Steel Institute: \"Blast furnaces will survive into the next millennium because the larger, efficient furnaces can produce hot metal at costs competitive with other iron making technologies.\" One of the biggest drawbacks of the blast furnaces is the inevitable carbon dioxide production as iron is reduced from iron oxides by carbon and as of 2016, there is no economical substitute – steelmaking is one of the largest industrial contributors of the CO emissions in the world (see greenhouse gases).\n\nThe challenge set by the greenhouse gas emissions of the blast furnace is being addressed in an ongoing European Program called ULCOS (Ultra Low CO Steelmaking). Several new process routes have been proposed and investigated in depth to cut specific emissions ( per ton of steel) by at least 50%. Some rely on the capture and further storage (CCS) of , while others choose decarbonizing iron and steel production, by turning to hydrogen, electricity and biomass. In the nearer term, a technology that incorporates CCS into the blast furnace process itself and is called the Top-Gas Recycling Blast Furnace is under development, with a scale-up to a commercial size blast furnace under way. The technology should be fully demonstrated by the end of the 2010s, in line with the timeline set, for example, by the EU to cut emissions significantly. Broad deployment could take place from 2020 on. \n\nStone wool or rock wool is a spun mineral fibre used as an insulation product and in hydroponics. It is manufactured in a blast furnace fed with diabase rock which contains very low levels of metal oxides. The resultant slag is drawn off and spun to form the rock wool product. Very small amounts of metals are also produced which are an unwanted by-product and run to waste.\n\nFor a long time, it was normal procedure for a decommissioned blast furnace to be demolished and either be replaced with a newer, improved one, or to have the entire site demolished to make room for follow-up use of the area. In recent decades, several countries have realized the value of blast furnaces as a part of their industrial history. Rather than being demolished, abandoned steel mills were turned into museums or integrated into multi-purpose parks. The largest number of preserved historic blast furnaces exists in Germany; other such sites exist in Spain, France, the Czech Republic, Japan, Luxembourg, Poland, Romania, Mexico, Russia and the United States.\n\nThe blast furnace gas can be used to generate heat. So by reducing the constituents in blast furnace gas we can increase the calorific value and can be used to generate heat and rise the temperature in any furnace.\n\nThe gas cleaning system contains two stages, the coarse cleaning system and the fine cleaning system.\n\nIn the coarse cleaning system a dust catcher is used. A dust catcher is a cylindrical steel structure with conical top and bottom sections. It is also lined with refractory bricks.\n\nThe principle of the dust catcher is that the dust-laden gas is given a sudden reverse in speed and direction. Because of their mass, the coarse dust particles cannot change their velocity easily, and hence settle to the bottom.\n\n\n"}
{"id": "2284882", "url": "https://en.wikipedia.org/wiki?curid=2284882", "title": "Clay Shirky", "text": "Clay Shirky\n\nClay Shirky (born 1964) is an American writer, consultant and teacher on the social and economic effects of Internet technologies and journalism.\n\nHe has a joint appointment at New York University (NYU) as a Distinguished Writer in Residence at the Arthur L. Carter Journalism Institute and Assistant Arts Professor in the New Media focused graduate Interactive Telecommunications Program (ITP). His courses address, among other things, the interrelated effects of the topology of social networks and technological networks, how our networks shape culture and vice versa.\n\nHe has written and been interviewed about the Internet since 1996. His columns and writings have appeared in \"Business 2.0\", \"The New York Times\", the \"Wall Street Journal\", the \"Harvard Business Review\" and \"Wired\". Shirky divides his time between consulting, teaching, and writing on the social and economic effects of Internet technologies. His consulting practice is focused on the rise of decentralized technologies such as peer-to-peer, web services, and wireless networks that provide alternatives to the wired client–server infrastructure that characterizes the World Wide Web. He is a member of the Wikimedia Foundation's advisory board. In \"The Long Tail\", Chris Anderson calls Shirky \"a prominent thinker on the social and economic effects of Internet technologies.\"\n\nAfter graduating from Yale University with a Bachelor of Arts degree in fine art in 1986, he moved to New York. In the 1990s he founded the Hard Place Theater, a theatre company that produced non-fiction theater using only found materials such as government documents, transcripts and cultural records and also worked as a lighting designer for other theater and dance companies, including the Wooster Group, Elevator Repair Service and Dana Reitz. During this time, Shirky was vice-president of the New York chapter of the Electronic Frontier Foundation, and wrote technology guides for Ziff Davis. He appeared as an expert witness on cyberculture in \"Shea v. Reno\", a case cited in the U. S. Supreme Court's decision to strike down the Communications Decency Act in 1996.\n\nShirky was the first Professor of New Media in the Media Studies department at Hunter College, where he developed the MFA in Integrated Media Arts program.\n\nIn the Fall of 2010, Shirky was a visiting Morrow Lecturer at Harvard University's John F. Kennedy School of Government instructing a course titled: \"New Media and Public Action\".\n\nIn his book \"Here Comes Everybody\", Shirky explains how he has long spoken in favor of crowdsourcing and collaborative efforts online. He uses the phrase \"the Internet runs on love\" to describe the nature of such collaborations. In the book, he discusses the ways in which the action of a group adds up to something more than just aggregated individual action borrowing the phrase \"more is different\" from physicist Philip Warren Anderson.\n\nShirky asserts that collaborative crowdsourced work results from \"a successful fusion of a plausible promise, an effective tool, and an acceptable bargain with the users.\" He states that the promise of what the user will get out of participating in a project leads to a person's desire to get involved. Collaborators will then choose the best social networking tool to do the job. One that \"must be designed to fit the job being done, and it must help people do something they actually want to do.\" The bargain, Shirky states, defines what collaborators expect from each other's participation in the project. Shirky's 'Promise, Tool, Bargain' premise restates aspects of the Uses and Gratifications Theory of mass media research.\n\nHe points to four key steps. The first is sharing, a sort of “me-first collaboration” in which the social effects are aggregated after the fact; people share links, URLs, tags, and eventually come together around a type. This type of sharing is a reverse of the so-called old order of sharing, where participants congregate first and then share (examples include Flickr, and Delicious). The second is conversation, that is, the synchronization of people with each other and the coming together to learn more about something and to get better at it. The third is collaboration, in which a group forms under the purpose of some common effort. It requires a division of labor, and teamwork. It can often be characterized by people wanting to fix a market failure, and is motivated by increasing accessibility.\n\nThe fourth and final step is collective action, which Shirky says is “mainly still in the future.” The key point about collective action is that the fate of the group as a whole becomes important.\n\nShirky also introduces his theory of mass amateurization:\nCombined with the lowering of transaction costs associated with creating content, \"mass amateurization\" of publishing changes the question from \"Why publish this?\" to \"Why not?\" Tied to \"mass amateurization\" is the idea of \"publish-then-filter\" which is now required due to the mere size and amount of material being created on a daily basis. Shirky calls this \"mass amateurization of filtering\" a forced move. He uses the Portland Pattern Repository, which introduced the wiki concept that inspired Wikipedia, as an example of this new marriage of mass content creation and mass filtering.\n\nIn 2010 Shirky published \"\" which expands on themes introduced in \"Here Comes Everybody\". The book follows concepts he introduced in a Web 2. 0 conference presentation April 23, 2008 called \"Gin, Television, and Social Surplus\", Herein he popularizes the concept of \"cognitive surplus\", the time freed from watching television which can be enormously productive when applied to other social endeavors. Technology has turned many past consumers into producers. This new production capacity, combined with humanity's willingness to share, can change society if applied to \"civic\" endeavors.\n\nShirky introduces Cognitive Surplus as a continuation of his work in \"Here Comes Everybody\". \"This book picks up where that one left off, starting with the observation that the wiring of humanity lets us treat free time as a shared global resource, and lets us design new kinds of participation and sharing that take advantage of that resource.\"\n\nShirky has also written about \"algorithmic authority,\" which describes the process through which unverified information is vetted for its trustworthiness through multiple sources.\n\nIn July 2005, Shirky gave a talk titled \"Institutions vs collaboration\" as a part of TEDGlobal 2005. This presentation reveals many of the ideas and concepts that would ultimately be presented in Here Comes Everybody and in future TED talks. Shirky compares the \"coordination costs\" between groups formed under traditional institutions and those formed by groups which \"build cooperation into the infrastructure.\" Classic institutions have to create economic, management, legal and physical structures and inherently, by creating these rigid structures, must exclude large numbers of people. Companies like Flickr, however, having built \"cooperation into the infrastructure\" of their company, do not have to build massive infrastructure nor exclude large groups of potential contributors.\n\nShirky states that since many social systems follow the Pareto principle wherein 20% of contributors account for 80% of contributions, traditional institutions lose out of the long tail of contributors by turning only the few that dominate the distribution into employees. The \"cooperative infrastructure\" model escapes having to lose this resource. Shirky presents an \"institution as enabler\" and \"institution as obstacle\" concept. The relatively small number of high-volume contributors can be assimilated, as employees, into the old-style corporate model and thus can live in an \"institution-as-enabler world\". The long tail of contributors, however, who make few and infrequent contributions, see institutions as an obstacle as they would never have been hired, therefore, disenfranchised. Shirky argues that an idea or contribution may be infrequent and significant. Furthermore, all of the long tail contributors, taken in aggregate, can be substantial.\n\nOne pitfall of the \"mass amateurs\" creating their own groups is that not all niches that are filled will be positive ones; Shirky presents pro-ana groups as an example. Shirky closes by stating that the migration from institutions to self-organizing, collaborative groups will be incomplete and will not end in a utopian society. Rather, chaos will follow as was created by the advent of the printing press before it, and that this period of transition will last roughly fifty years.\n\nShirky claims that our actions and behavior are generated by convenience. Writer and analyst Megan Garber writes: “The more people we have participating in media, and the more people we have consuming it—and the more people we have, in particular, creating it—the better. Not because bigger is implicitly better than the alternative compact, but because abundance changes the value proposition of media as a resource.\" \n\nAccording to Jay Baer by making collaboration more convenient for the user, it will eventually become a more commonplace. Further, enhancing the outcome of collaboration will instill motivation within the users.\n\nAccording to Audrey Tang, Shirky has pioneered the notion of a \"cognitive surplus\", to describe the way that time spent on the internet can have an increasing social value.\n\nIn June 2009, Shirky participated in a TED@State talk titled \"How cellphones, Twitter and Facebook can make history\" aka \"How social media can make history.\" In the talk, he explains that this is the first time in history that communication is possible from many to many. In the past, communication to a large group excluded the possibility of having a conversation, and having a conversation meant not interacting with a group and instead was necessarily a one-to-one structure. Shirky labels this incongruous exchange as asymmetric. In Shirky's view, this feature is one of the main reasons that the internet revolution is different from communication revolutions that preceded it.\n\nThe second difference between the twentieth and twenty-first century communication revolution, Shirky states, is now all media is digitized. This means that the Internet now encapsulates all forms of media from the past and the medium itself has become the site of exchange, not just a means of exchange.\n\nFinally, the Internet allows people to create content, thus the line between producers and consumers has become blurred. As Shirky puts it, \"Every time a new consumer joins this media landscape, a new producer joins as well.\" Even countries like China, as Shirky gives as an example, go to great lengths to control information exchange on the Internet but are having trouble as the \"amateurization\" of media creation has effectively turned every owner of a cellphone and Twitter account into a journalist. The populace as a whole, Shirky claims, is a force much harder to control than a handful of professional news sources. He compares the \"Great Firewall of China\" to the Maginot Line as both were built to protect from external threats but that is not where the majority of content is being created in this new media landscape.\n\nAs an example of the potential of this two-way, collaborative environment Shirky believes we are now living in, he presents as a case study MyBarackObama.com. Over the issue of the Foreign Intelligence Surveillance Act, members of the website were upset over Obama's announcement that he was changing his stance and that now he was going to sign the bill \"that granted immunity for possibly warrantless spying on American persons.\" Despite the disagreement between the President and the posters opposed to his altered view, Shirky cites the mere fact that the President posted a reply to their concerns, instead of persecuting/ignoring the group, as hope for the future of this new form of mass media.\n\nIn April 2010, Kevin Kelly cited the phrase \"Institutions will try to preserve the problem to which they are the solution\", and called it the \"Shirky Principle\", as the phrasing reminded him of the clarity of the Peter Principle.\n\nIn June 2010, Shirky participated in TED@Cannes wherein he spoke about \"cognitive surplus\" and its role furthering \"communal\" and \"civic value\". The talk was titled, \"How cognitive surplus will change the world,\" and the possibility for change, which Shirky presents, runs the spectrum at one end with \"communal value\" being increased and at the other end with \"civic value\" being furthered. Digital technology has allowed human generosity and \"the world's free time and talents,\" which Shirky calls \"cognitive surplus\", to combine and create a new form of creative expression. This creative expression can take the form of lolcats or endeavors such as Ushahidi; the former Shirky says increases \"communal value\", \"it is created by the participants for each other\" for simple amusement, whereas the latter he cites furthers \"civic value\" meaning the group action is taken to benefit society as a whole.\n\nShirky then presents the view that society lives under \"social constraint\" and that these \"social constraints\" can create a culture that is \"more generous than\" the environment created by \"contractual constraints\" alone. Understanding where the economic or \"contractual\" motivation of a situation ends and where the \"social\" part begins, Shirky claims is key when designing to maximize generosity. This being the case, to have society use its \"trillion hours a year of participatory value\" to advance \"civic value\", society itself simply needs to prize, and collectively praise, endeavors like Ushahidi.\n\nClay Shirky wrote an essay about the aspects of online community building through broadcast media. As members of a broad social community and users of media outlets, Shirky suggests ways in which we can build up this type of society.\n\nShirky suggests five different things to think about when dealing with broadcast media outlets:\nAudiences are built. Communities grow.\nCommunities face a tradeoff between size and focus.\nParticipation matters more than quality.\nYou may own the software, but the community owns itself.\nThe community will want to build. Help it, or at least let it.\n\nIn March 2011, Shirky responded to questions raised by Evgeny Morozov about consulting he had done for the Libyan government. Morozov tweeted \"With Clay Shirky consulting the Libyan govt, it's now clear why dictators are so smart about the Web\". Shirky explained he had been invited in 2007 to speak in Boston to Libya's IT Minister. Shirky stated the talk was \"about using social software to improve citizen engagement in coastal towns. The idea was that those cities would be more economically successful if local policies related to the tourist trade were designed by the locals themselves.\" Shirky added that nothing came of the project beyond his initial talk. He defended his underlying desire to expand representative government in Libya and concluded that \"the best reason to believe that social media can aid citizens in their struggle to make government more responsive is that both citizens and governments believe that.\"\n\nIn January 2012, at TED Salon NY, Shirky gave a talk titled \"Why SOPA is a bad idea.\" He cites SOPA as a way for traditional, mass media producers to \"raise the cost of copyright compliance to the point where people simply get out of the business of offering it as a capability to amateurs.\" After an offending internet site is identified, with the identification process itself not specified in the bill, the targeted site will be removed from the Domain Name System (DNS). Shirky claims since you can still use the static IP address of the site in question, removal from DNS is futile. He identifies the Audio Home Recording Act of 1992 as a law that was able to delineate between sharing with your friends as being legal and selling for commercial gain as illegal. Unsatisfied, media companies, Shirky claims, continued to push government to create more sweeping legislation which would hinder any form of sharing. This pressure, in 1998, created the Digital Millennium Copyright Act. It was now legal for media companies to sell uncopyable material although uncopyable digital material does not exist. To remedy this fact, Shirky states that media companies now tried to break consumer's computer hardware to create the illusion that the media they purchased was indeed uncopyable.\nWhereas DMCA was \"surgical,\" SOPA is \"nuclear\" since the law stipulates any sites pointing to \"illegal\" content may be censored. Ultimately, Shirky points out the public-at-large is by far the largest producers of content and they are the ones that which will be censured. They will be presumed guilty until they can prove the content they published is not illegal. This turns the American legal system on its head. He closes by encouraging Americans to contact their senators and congressmen and reminding them they prefer \"not to be treated like a thief.\"\n\nOn June 29, 2012, Shirky participated in Session 12: Public Sphere of TEDGlobal 2012. Shirky made the observation that many of the technological advancements in communication throughout history, from the printing press to the television, were heralded as harbingers of world peace yet ended up creating greater dissent. \"The more ideas there are in circulation, the more ideas there are for any individual to disagree with.\" However, Shirky claims, with this increased \"arguing,\" comes an increased \"speed\" of information exchange. Shirky cites \"The Invisible College\" as an example of a group that was able to utilize this effect created by the printing press, via the scientific journal, to help launch the scientific revolution.\n\nHe then states we are in a similar period today with open-source programmers and their use of distributed version control or DVCS. DVCS, he argues, allows for \"more arguments\" to be made into \"better arguments\". DVCS also allows for \"cooperation without coordination\" which Shirky states is \"the big change\". He then suggests that DVCS fits naturally with law as it, and software development, are \"dependency-related.\" Shirky presents another application for DVCS – drafting legislation. He cites Open Legislation, a listing of legislative information from the New York State Senate and Assembly, as an early step in that direction.\n\nThe talk culminates with Shirky posing the open question of whether or not government will transition from striving towards one-way transparency to mutual collaboration and suggests if it does, there is already a \"new form of arguing\" centered around DVCS to aid the transition.\n\n\n\n\n"}
{"id": "22826526", "url": "https://en.wikipedia.org/wiki?curid=22826526", "title": "Collis Mill, Great Thurlow", "text": "Collis Mill, Great Thurlow\n\nCollis Mill is a Grade II* listed smock mill at Great Thurlow, Suffolk, England which has been restored.\n\n\"Collis Mill\" was originally built at Slough, Berkshire. It was moved to Great Thurlow in 1807 replacing an earlier post mill. Thomas Hunt, the Soham millwright, carried out repairs to the mill in 1914. It ceased working by wind in 1915 and the sails were removed from 1920. The mill was worked by a portable steam engine until 1937. The derelict mill was capless in 1959 when it was purchased by R A Vestey for restoration as a visual amenity. Restoration was completed in 1962.\n\n\"Collis Mill\" is a three storey smock mill on a single storey brick base. It has four Common sails and the pepperpot cap is winded by a fantail. It has two pairs of underdrift millstones.\n\nReference for above:-\n\n"}
{"id": "9913921", "url": "https://en.wikipedia.org/wiki?curid=9913921", "title": "Customer Access and Retrieval System", "text": "Customer Access and Retrieval System\n\nThe Customer Access and Retrieval System, commonly known as CARS, is one of the many Telephone Company Customer Applications used by Mobile network operators, which is mostly used in the Western United States. The system stores bill and credit information, equipment information, carrier billing information, customer contact notes and payment history. The CARS interface is quite similar to the Billing and Order Support System (BOSS) interface.\n\nUser IDs are six characters and normally begin with a 'B' for business. Passwords (usually referred to as lockwords in the western United States) are from four to either characters and must contain at least one alpha and one numeric character. Passwords expire after 30 days. The system also asks for a Project Code (use 'M'), a Group Code (use 'G') and a Position Number. The Position Number consists of a pair of two character fields. The first two characters are the office code and the second two characters identify the individual employee.\n\nTelephone Company Customer Applications\n"}
{"id": "3497359", "url": "https://en.wikipedia.org/wiki?curid=3497359", "title": "Design for manufacturability", "text": "Design for manufacturability\n\nDesign for manufacturability (also sometimes known as design for manufacturing or DFM) is the general engineering practice of designing products in such a way that they are easy to manufacture. The concept exists in almost all engineering disciplines, but the implementation differs widely depending on the manufacturing technology. DFM describes the process of designing or engineering a product in order to facilitate the manufacturing process in order to reduce its manufacturing costs. DFM will allow potential problems to be fixed in the design phase which is the least expensive place to address them. Other factors may affect the manufacturability such as the type of raw material, the form of the raw material, dimensional tolerances, and secondary processing such as finishing.\n\nDepending on various types of manufacturing processes there are set guidelines for DFM practices. These DFM guidelines help to precisely define various tolerances, rules and common manufacturing checks related to DFM.\n\nWhile DFM is applicable to the design process, a similar concept called DFSS (Design for Six Sigma) is also practiced in many organizations.\n\nIn the PCB design process, DFM leads to a set of design guidelines that attempt to ensure manufacturability. By doing so, probable production problems may be addressed during the design stage.\n\nIdeally, DFM guidelines take into account the processes and capabilities of the manufacturing industry. Therefore, DFM is constantly evolving.\n\nAs manufacturing companies evolve and automate more and more stages of the processes, these processes tend to become cheaper. DFM is usually used to reduce these costs. For example, if a process may be done automatically by machines (i.e. SMT component placement and soldering), such process is likely to be cheaper than doing so by hand.\n\nAchieving high-yielding designs, in the state of the art VLSI technology has become an extremely challenging task due to the miniaturization as well as the complexity of leading-edge products. Here, the DFM methodology includes a set of techniques to modify the design of integrated circuits (IC) in order to make them more manufacturable, i.e., to improve their functional yield, parametric yield, or their reliability.\n\nTraditionally, in the prenanometer era, DFM consisted of a set of different methodologies trying to enforce some soft (recommended) design rules regarding the shapes and polygons of the physical layout of an integrated circuit. These DFM methodologies worked primarily at the full chip level. Additionally, worst-case simulations at different levels of abstraction were applied to minimize the impact of process variations on performance and other types of parametric yield loss. All these different types of worst-case simulations were essentially based on a base set of worst-case (or corner) SPICE device parameter files that were intended to represent the variability of transistor performance over the full range of variation in a fabrication process.\n\nThe most important yield loss models (YLMs) for VLSI ICs can be classified into several categories based on their nature.\n\nAfter understanding the causes of yield loss, the next step is to make the design as resistant as possible. Techniques used for this include:\nAll of these require a detailed understanding of yield loss mechanisms, since these changes trade off against one another. For example, introducing redundant vias will reduce the chance of via problems, but increase the chance of unwanted shorts. Whether this is good idea, therefore, depends on the details of the yield loss models and the characteristics of the particular design.\n\nThe objective is to design for lower cost. The cost is driven by time, so the design must minimize the time required to not just machine (remove the material), but also the set-up time of the CNC machine, NC programming, fixturing and many other activities that are dependent on the complexity and size of the part.\n\nUnless a 5th-Axis is used, a CNC can only approach the part from a single direction. One side must be machined at a time (called an operation or Op). Then the part must be flipped from side to side to machine all of the features. The geometry of the features dictates whether the part must be flipped over or not. The more Ops (flip of the part), the more expensive the part because it incurs substantial \"Set-up\" and \"Load/Unload\" time.\n\nEach operation (flip of the part) has set-up time, machine time, time to load/unload tools, time to load/unload parts, and time to create the NC program for each operation. If a part has only 1 operation, then parts only have to be loaded/unloaded once. If it has 5 operations, then load/unload time is significant.\n\nThe low hanging fruit is minimizing the number of operations (flip of the part) to create significant savings. For example, it may take only 2 minutes to machine the face of a small part, but it will take an hour to set the machine up to do it. Or, if there are 5 operations at 1.5 hours each, but only 30 minutes total machine time, then 7.5 hours is charged for just 30 minutes of machining.\n\nLastly, the volume (number of parts to machine) plays a critical role in amortizing the set-up time, programming time and other activities into the cost of the part. In the example above, the part in quantities of 10 could cost 7–10X the cost in quantities of 100.\n\nTypically, the law of diminishing returns presents itself at volumes of 100–300 because set-up times, custom tooling and fixturing can be amortized into the noise.\n\nThe most easily machined types of metals include aluminum, brass, and softer metals. As materials get harder, denser and stronger, such as steel, stainless steel, titanium, and exotic alloys, they become much harder to machine and take much longer, thus being less manufacturable. Most types of plastic are easy to machine, although additions of fiberglass or carbon fiber can reduce the machinability. Plastics that are particularly soft and gummy may have machinability problems of their own.\n\nMetals come in all forms. In the case of aluminum as an example, bar stock and plate are the two most common forms from which machined parts are made. The size and shape of the component may determine which form of material must be used. It is common for engineering drawings to specify one form over the other. Bar stock is generally close to 1/2 of the cost of plate on a per pound basis. So although the material form isn't directly related to the geometry of the component, cost can be removed at the design stage by specifying the least expensive form of the material.\n\nA significant contributing factor to the cost of a machined component is the geometric tolerance to which the features must be made. The tighter the tolerance required, the more expensive the component will be to machine. When designing, specify the loosest tolerance that will serve the function of the component. Tolerances must be specified on a feature by feature basis. There are creative ways to engineer components with lower tolerances that still perform as well as ones with higher tolerances.\n\nAs machining is a subtractive process, the time to remove the material is a major factor in determining the machining cost. The volume and shape of the material to be removed as well as how fast the tools can be fed will determine the machining time. When using milling cutters, the strength and stiffness of the tool which is determined in part by the length to diameter ratio of the tool will play the largest role in determining that speed. The shorter the tool is relative to its diameter the faster it can be fed through the material. A ratio of 3:1 (L:D) or under is optimum. If that ratio cannot be achieved, a solution like this depicted here can be used. For holes, the length to diameter ratio of the tools are less critical, but should still be kept under 10:1.\n\nThere are many other types of features which are more or less expensive to machine. Generally chamfers cost less to machine than radii on outer horizontal edges. 3D interpolation is used to create radii on edges that are not on the same plane which incur 10X the cost. Undercuts are more expensive to machine. Features that require smaller tools, regardless of L:D ratio, are more expensive.\n\nThe concept of Design for Inspection (DFI) should complement and work in collaboration with Design for Manufacturability (DFM) and Design for Assembly (DFA) to reduce product manufacturing cost and increase manufacturing practicality. There are instances when this method could cause calendar delays since it consumes many hours of additional work such as the case of the need to prepare for design review presentations and documents. To address this, it is proposed that instead of periodic inspections, organizations could adopt the framework of empowerment, particularly at the stage of product development, wherein the senior management empowers the project leader to evaluate manufacturing processes and outcomes against expectations on product performance, cost, quality and development time. Experts, however, cite the necessity for the DFI because it is crucial in performance and quality control, determining key factors such as product reliability, safety, and life cycles. For an aerospace components company, where inspection is mandatory, there is the requirement for the suitability of the manufacturing process for inspection. Here, a mechanism is adopted such as an inspectability index, which evaluates design proposals. Another example of DFI is the concept of cumulative count of conforming chart (CCC chart), which is applied in inspection and maintenance planning for systems where different types of inspection and maintenance are available. \n\nAdditive manufacturing broadens the ability of a designer to optimize the design of a product or part (to save materials for example). Designs tailored for additive manufacturing are sometimes very different from designs tailored for machining or forming manufacturing operations.\n\nIn addition, due to some size constraints of additive manufacturing machines, sometimes the related bigger designs are split into smaller sections with self-assembly features or fasteners locators.\n\n\n\n\n"}
{"id": "27636179", "url": "https://en.wikipedia.org/wiki?curid=27636179", "title": "Encyclopædia Britannica Ultimate Reference Suite", "text": "Encyclopædia Britannica Ultimate Reference Suite\n\nEncyclopædia Britannica Ultimate Reference Suite is an encyclopædia based on the \"Encyclopædia Britannica\" and published by Encyclopædia Britannica, Inc..\n\nThe DVD contains over 100,000 articles, an atlas, around 35,000 media files (images, video and audio) and a dictionary and thesaurus based on Merriam-Webster.\n\n\"Encyclopædia Britannica Ultimate Reference Suite\" received the 2004 Distinguished Achievement Award from the Association of Educational Publishers. Its predecessor, \"Britannica\" DVD, received Codie awards in 2000, 2001 and 2002.\n\nThere is no official release of \"Britannica\" for the Linux operating system; however, a script is provided that can help experienced users run \"Encyclopædia Britannica 2004 Ultimate Reference Suite\" DVD (and other 2004 editions of \"Britannica\") on Linux, with some limitations (for example the dictionary, Flash/QuickTime presentations, and content update functions do not work, and preferences must be edited manually). This script specifically requires version 1.3.1 of JRE, but can usually be made to work with newer versions if the version check is commented out.\n\n"}
{"id": "7385837", "url": "https://en.wikipedia.org/wiki?curid=7385837", "title": "Entertainment Consumers Association", "text": "Entertainment Consumers Association\n\nEntertainment Consumers Association (ECA) is a United States-based non-partisan, non-government, non-profit organization dedicated to the interests of individuals who play computer and video games in the United States and Canada.\n\nMr. Hal Halpin, a game industry veteran and former president of the Interactive Entertainment Merchants Association (IEMA) – now called the Entertainment Merchants Association (EMA) – founded ECA in July 2006. The concept of the ECA was born following an IEMA board of directors meeting, in which Halpin recognized a need for consumer representation. The association was launched as a means for consumer rights advocacy following a string of anti-games legislation aimed at criminalizing the sale of certain video games. Although publishers were effectively represented by Entertainment Software Association (ESA) and retailers by Entertainment Merchants Association (EMA), consumers of video games were virtually unrepresented until the launch of ECA.\n\nECA is an ardent supporter of consumer rights and advocacy, specifically in defending and advancing the interests of gamers. The organization does this through a variety of initiatives including netroots and lobbying efforts at the state and national governmental level, an activity permitted by its 501(c)(4) status. ECA also coalition builds with like-minded organizations including First Amendment advocacy groups and parallel trade associations. The ECA is non-partisan and does not support, oppose or give money to any candidates or political parties.\n\nThe ECA Member division negotiates and offers reduced rates for members with various companies that sell game-related merchandise and services including; magazine and premium website subscriptions, discounts on game rentals and purchases and free or discounted admission to trade shows, conferences and concerts, etc. They provide programs for reduced-cost medical and life insurance, financial aid, tuition assistance and scholarship opportunities for members as well as career advice, job boards, resume writing aid and discussion forums and boards.\n\nThe association distinguished itself early by weighing in publicly on issues that the parallel trade associations did not, including standing in defense of the game Mass Effect and its developer, BioWare, during the related controversy surrounding supposed sexualization of the product. ECA issued a press statement calling on FOX News to retract the misleading story. ECA also was a founding member of the Gamers for Net Neutrality initiative, which sought to educate and empower gamer consumers about the issues surrounding network neutrality as it relates to online gaming. Partnering with MoveOn.org, SaveTheInternet.com, and Games for Change, the coalition provides an educational area on ECA’s website as well as digital advocacy tools for gamers. The association also established several other digital advocacy sub-groups including Gamers for Digital Rights, Gamers for Universal Broadband. Membership is not required to participate in any of the three grass roots initiatives.\n\nOn May 12, 2010 the ECA announced that they would be submitting an amicus curiae (friend of the court) document in support of the gaming industry in the upcoming Schwarzenegger v. EMA First Amendment case. The organization also stated that they intend to amend a consumer petition to their brief to request that the court find that games should continue to enjoy the same First Amendment protections as music and movies and not be legislated and regulated like alcohol, tobacco and firearms.\n\nECA was a coalition partner with Reddit, Google, EFF, Public Knowledge, Major League Gaming, Demand Progress and others in opposing the Stop Online Piracy Act (SOPA) and its counterpart, the Protect Intellectual Property Act (PIPA). The association also stood opposed to the Copyright Modernization Act (C11) in Canada and the Anti-Counterfeiting Trade Agreement (ACTA), internationally.\n\nThe ECA merged a number of long-standing staple brands when forming the organization which lent it early credibility and built-in expertise in the respective fields. Among the more prominent brands was GamePolitics.com, a blog originally written and maintained by Dennis McCauley, now run by game journalist Pete Gallagher, the former Editor-in-Chief of GameDaily.com. GP, as it had come to be known in the business and by the site’s fans, is an information portal for all matters related to game legislation and grass roots lobbying initiatives.\n\nThe organization also publishes a daily email-based electronic newsletter, ECA Today, which is mailed nightly to all members. The newsletter informs and educates gamers about current and potential anti-games legislation, and acts as a call to arms in the association’s grass roots lobbying initiatives employing electronic advocacy. ECA also emails out a monthly members-only newsletter which keeps members abreast of the efforts being undertaken and advises the membership of new partnerships and coalitions it has joined. And the final two products are GameJobs.com, an interactive entertainment industry job board, and Video Game Yellow Pages (VGYP), which has served for over ten years as an online directory information for the games business.\n\nOn December 5, 2007 the ECA announced that the association was launching another publication, called GameCulture. Journalist and co-author of , Aaron Ruby, was hired on to be the Editor-in-Chief. The association felt the need to launch the site as a resource for promoting gaming in a more positive light and addressing the ways in which gamers and gaming have impacted broader society. In September 2009, GameCulture added veteran game journalist, John Keefer to its ranks who was followed by New Zealand-based writer, Julie Gray, in January 2010. Popular web comic, Experience Points, penned by Scott Johnson, moved from its original home at Crispy Gamer to GameCulture.\n\nOn December 2, 2009, controversy arose regarding the ECA’s membership cancellation policy, in which the association’s membership terms and conditions were changed without notifying ECA users. The change was made due to an exploit in a partner’s coupon codes. The cancellation policy change temporarily required that members mail a physical letter requesting cancellation while the association upgraded their systems. There were also complaints about the change in the terms and conditions being made without notifying the membership, which struck some members as ironic given the ECA’s stance regarding End User License Agreements.\nThe three-week ordeal ended on December 24, 2009 once the promised new modules went public giving members online account termination and an online auto-renewal opt-out functionality similar to Xbox Live and ECA’s listing with the CT Better Business Bureau was raised to an A-.\n\nOn February 6, 2013, the ECA announced via Facebook their appointment of \"Gerard Williams\" aka \"The Hip Hop Gamer\". as their video game ambassador. The appointment was received with negative reception by gamers and gaming press who felt that his reputation of verbally/physically threatening others and use of the word 'faggot' would hurt the image and cause of gamers as a consumer group. Heather Ellertson, the ECA's Vice President of Marketing sent out a press release saying that Gerard was in the process of turning his life around and they \"wanted to give him the opportunity to be a voice for gamers and a positive role model for gaming\". The ECA & 'Hip Hop Gamer Inc' then announced via their Facebook page that all complaints, comments and criticisms should be directed to The Hip Hop Gamer himself and that he did not represent the views of any of their partners or sponsors. Gerard Williams then published a video explaining that he had only used the word 'faggot' because someone told him to 'shut up' during a Sony press event.\n\nThe ECA receives financial support from its dues-paying membership – individuals who pay $19.99 annually ($14.99 for students and members of the military). The association claims not to accept funding from industry partners, nor does it permit game publisher advertising on any of its websites or publications, though open job placements on the ECA’s GameJobs.com are paid-for by game industry companies. The organisation also receives additional pro bono legal assistance from Hughes Hubbard & Reed. Additionally, the association lists the brands and companies which are marketing partners with the ECA on their website. Most provide discounts and special promotions, but none provide funding.\n\n\n"}
{"id": "11860131", "url": "https://en.wikipedia.org/wiki?curid=11860131", "title": "Federation architecture", "text": "Federation architecture\n\nFederation architecture is the architectural style in Australia that was prevalent from around 1890 to 1915. The name refers to the Federation of Australia on 1 January 1901, when the Australian colonies collectively became the Commonwealth of Australia.\n\nThe architectural style had antecedents in the Queen Anne style and Edwardian style of the United Kingdom, combined with various other influences like the Arts and Crafts style. Other styles also developed, like the Federation Warehouse style, which was heavily influenced by the Romanesque Revival style. In Australia, Federation architecture is generally associated with cottages in the Queen Anne style, but some consider that there were twelve main styles that characterized the Federation period.\n\nThe Federation period overlaps the Edwardian period, which was so named after the reign of King Edward VII (1901–1910); however, as the style preceded and extended beyond Edward's reign, the term \"Federation architecture\" was coined in 1969.\n\nFederation architecture has many similarities to Edwardian Baroque architecture; however, there are significant differences that distinguish the Federation architecture style from the Edwardian Baroque architecture style, particularly due to the embracing of Australiana themes and the use of the verandah in domestic settings. Australian flora and fauna are prominently featured, and stylised images of the New South Wales waratah, flannel flower, Queensland firewheel tree, and other flowers, and the kangaroo, kookaburra, and lyrebird, were common. The Coat of Arms, and rising sun, representing a new dawn in the country of Australia, also appeared regularly on gables.\n\nMany Federation buildings, both residential and non-residential, are listed on the Register of the National Estate because of their heritage values.\n\nGardens of the period were complex and contained many elements—generally a wider variety of plants than is seen in contemporary plantings, pergolas, rose arches, gazebos and summerhouses. Wooden lattice fences were used to partition parts of the garden off, particularly the front from the more private back. Garden paths could be straight or gently curved, and often edged with glazed edging tiles or bricks, and made of tiles, packed gravel or bricks. patterns for brick paving include stretcher bond, herringbone and basketweave. Asphalt and concrete were not used.\n\nPlants were selected to produce year-round colour and interest in the local climate conditions. Initially, evergreen trees were used, but the denseness of shade led to increasing popularity of deciduous trees such as jacaranda, flowering plum and peppercorn. Palms often framed the garden vista, and the native Cootamundra wattle was popular, as were shrubs such as camellias and standard roses. Conservatories contained begonias and adiantum ferns.\n\nThere are twelve styles that predominated in the Federation period:\n\nOf the twelve Federation styles, there are four that were mainly used in residential architecture. They are Federation Queen Anne style, Federation Filigree style, Federation Arts and Craft style, and Federation Bungalow style.\n\nThe Federation Queen Anne style was designed to embrace the outdoor lifestyles of the Australian people. Most homes have asymmetric gables, white-painted window frames, front verandas with decorative timber features, tiling on the patio floor and entry paths. The brickwork is usually a deep red or dark brown, often with a mix of the two. The roofs are typically terracotta tiles with decorative gables (sometimes adorned with finials), motifs, timber features, tall chimneys and fretwork. Decorative leadlight windows are also common, as are circular windows (known as bulls-eye windows). Federation homes also have decorative internal features in the plasterwork, high ceilings and timber features.\n\nSome outstanding examples are West Maling, Penshurst Avenue, Penshurst, New South Wales; Turramurra Ingleholme, Boomerang Street, Turramurra, New South Wales (former home of architect John Sulman); and Caerleon, Bellevue Hill, the first Queen Anne home in Australia. The Federation Queen Anne style was the most popular residential style in Australia between 1890 and 1910.\n\nThe Federation Filigree style is common in the hotter parts of Australia, especially in the north, since it is designed to create shade while allowing for the free flow of air. It is a common sight in Queensland and is sometimes known as the Queensland style. Some outstanding examples are Belltrees House, Scone, New South Wales; private home, Roderick Street, Ipswich, Queensland; and terrace of homes, east side of High Street, Millers Point, New South Wales.\n\nThe Federation Arts and Crafts style had its origins in England, where architects were reacting to the impersonal nature of the Industrial Revolution. Crafts and handiwork were emphasised to give architecture the \"human touch\". These influences were absorbed into Federation Australia, where the resulting buildings were generally small-scale to medium-scale and predominantly residential. Outstanding examples are Glyn, Kooyong road, Toorak, Victoria; The Crossways, Martin Road, Centennial Park, New South Wales; and Erica, Appian Way, Burwood, New South Wales.\n\nThe Federation Bungalow style was the Australian response to the bungalow style that was developed in America by people like Gustav Stickley. It can be seen as a transition phase between the Federation Queen Anne style and the California Bungalow style that took on later. Stylistically, it exploited the qualities of the bungalow while frequently retaining the flair and idiosyncrisies of the Queen Anne style, although usually in simplified form. Outstanding examples are Nee Morna, Nepean Highway, Sorrento, Victoria; Blythewood, Beecroft Road, Cheltenham New South Wales; and The Eyrie, Fox Valley Road, Wahroonga, New South Wales.\n\nFederation non-residential buildings can be in any of the twelve styles. The following gallery shows some examples of non-residential buildings.\n\nDuring the early-1990s, many of the design elements that characterised the Federation architecture of old were popularised in mainstream architecture. This Federation revival form is also known as \"mock Federation\" or \"faux Federation\". The style was widespread within the realm of residential housing (especially in new development suburbs) and for apartment buildings; however, smaller shopping centres and other public buildings also made use of the revival style that retained widespread popularity until the early 2000s. Suburbs of Sydney that developed in the 1990s—such as Cherrybrook, Castle Hill, and Menai—are notable in the sense that large tracts of these developments contain almost exclusively Federation revival homes.\n\nThe construction of Federation revival architecture varied little from that of other basic styles, with the Federation elements merely forming the facade and decorating elements of the building. For example, the typical brick and roof tile construction, hexagonal turrets, ornate gable work, finials, prominent verandah, steep pitched roofs, and faceted bay windows served to parallel the traditional Federation architecture.\n\nNotable Federation architects in Australia include:\n\n\n\n"}
{"id": "291437", "url": "https://en.wikipedia.org/wiki?curid=291437", "title": "Glow stick", "text": "Glow stick\n\nA glow stick is a self-contained, short-term light-source. It consists of a translucent plastic tube containing isolated substances that, when combined, make light through chemiluminescence, so it does not require an external energy source. The light cannot be turned off and can only be used once. Glow sticks are often used for recreation, but may also be relied upon for light during military, police, fire, or EMS operations.\n\nBis(2,4,5-trichlorophenyl-6-carbopentoxyphenyl)oxalate, trademarked \"Cyalume\", was invented in 1969 by Michael M. Rauhut and Laszlo J. Bollyky of American Cyanamid, based on work by Edwin A. Chandross of Bell Labs. Other early work on chemiluminescence was carried out at the same time, by researchers under Herbert Richter at China Lake Naval Weapons Center.\n\nSeveral US patents for \"glow stick\" type devices were received by various inventors. Bernard Dubrow and Eugene Daniel Guth patented a packaged chemiluminescent material in June 1965 (Patent 3,774,022). In October 1973, Clarence W. Gilliam, David Iba Sr., and Thomas N. Hall were registered as inventors of the Chemical Lighting Device (Patent 3,764,796). In June 1974, a patent for a Chemiluminescent Device was issued with Herbert P. Richter and Ruth E. Tedrick listed as the inventors (Patent 3,819,925).\n\nIn January 1976, a patent was issued for the Chemiluminescent Signal Device, with Vincent J. Esposito, Steven M. Little, and John H. Lyons listed as the inventors (Patent 3,933,118). This patent recommended a single glass ampoule that is suspended in a second substance, that when broken and mixed together, provide the chemiluminescent light. The design also included a stand for the signal device so it could be thrown from a moving vehicle and remain standing in an upright position on the road. The idea was this would replace traditional emergency roadside flares and would be superior, since it was not a fire hazard, would be easier and safer to deploy, and would not be made ineffective if struck by passing vehicles. This design, with its single glass ampoule inside a plastic tube filled with a second substance that when bent breaks the glass and then is shaken to mix the substances, most closely resembles the typical glow stick sold today.\n\nIn December 1977, a patent was issued for a Chemical Light Device with Richard Taylor Van Zandt as the inventor (Patent 4,064,428). This design alteration features a steel ball that shatters the glass ampoule when the glow stick is exposed to a predetermined level of shock; an example of its use being that an arrow can be flown dark but illuminate its landing location upon sudden deceleration.\n\nGlow sticks are waterproof, do not use batteries, generate negligible heat, are inexpensive, and are reasonably disposable. They can tolerate high pressures, such as those found under water. They are used as light sources and light markers by military forces, campers, and recreational divers.\n\nGlowsticking is the use of glow sticks in dancing. This is one of their most widely known uses in popular culture, as they are frequently used for entertainment at parties (in particular raves), concerts, and dance clubs. They are used by marching band conductors for evening performances; glow sticks are also used in festivals and celebrations around the world. Glow sticks also serve multiple functions as toys, readily visible night-time warnings to motorists, and luminous markings that enable parents to keep track of their children. Yet another use is for balloon-carried light effects. Glow sticks are also used to create special effects in low light photography and film.\n\nThe Guinness Book of Records says the world's largest glow stick was cracked at tall. It was created using Plexiglass by KNIXS GmbH (all Germany) in Darmstadt Weiterstadt, Germany, on 29 June 2009.\n\nGlow sticks emit light when two chemicals are mixed. The reaction between the two chemicals is catalyzed by a base, usually sodium salicylate. The sticks consist of a tiny, brittle container within a flexible outside\ncontainer. Each container holds a different solution. When the outer container is flexed, the inner container breaks, allowing the solutions to combine, causing the necessary chemical reaction. After breaking, the tube is shaken to thoroughly mix the components.\n\nThe glow stick contains two chemicals, a base catalyst, and a suitable dye (sensitizer, or fluorophor). This creates an exothermic reaction. The chemicals inside the plastic tube are a mixture of the dye, the base catalyst, and diphenyl oxalate. The chemical in the glass vial is hydrogen peroxide. By mixing the peroxide with the phenyl oxalate ester, a chemical reaction takes place, yielding two moles of phenol and one mole of peroxyacid ester (1,2-dioxetanedione). The peroxyacid decomposes spontaneously to carbon dioxide, releasing energy that excites the dye, which then relaxes by releasing a photon. The wavelength of the photon—the color of the emitted light—depends on the structure of the dye. The reaction releases energy mostly as light, with very little heat. The reason for this is that the reverse [2 + 2] photocycloadditions of 1,2-dioxetanedione is a forbidden transition (it violates Woodward–Hoffmann rules) and cannot proceed through a regular thermal mechanism.\nBy adjusting the concentrations of the two chemicals and the base, manufacturers can produce glow sticks that either glow brightly for a short amount of time or more dimly for an extended length of time. This also allows design of glow sticks that perform satisfactorily in hot or cold climates, by compensating for the temperature dependence of reaction. At maximum concentration (typically only found in laboratory settings), mixing the chemicals results in a furious reaction, producing large amounts of light for only a few seconds. The same effect can be achieved by adding copious amounts of sodium salicyate or other bases. Heating a glow stick also causes the reaction to proceed faster and the glow stick to glow more brightly for a brief period. Cooling a glow stick slows the reaction a small amount and causes it to last longer, but the light is dimmer. This can be demonstrated by refrigerating or freezing an active glow stick; when it warms up again, it will resume glowing. The dyes used in glow sticks usually exhibit fluorescence when exposed to ultraviolet radiation—even a spent glow stick may therefore shine under a black light.\n\nThe light intensity is high immediately after activation, then exponentially decays. Leveling of this initial high output is possible by refrigerating the glow stick before activation.\n\nA combination of two fluorophores can be used, with one in the solution and another incorporated to the walls of the container. This is advantageous when the second fluorophore would degrade in solution or be attacked by the chemicals. The emission spectrum of the first fluorophore and the absorption spectrum of the second one have to largely overlap, and the first one has to emit at shorter wavelength than the second one. A downconversion from ultraviolet to visible is possible, as is conversion between visible wavelengths (e.g., green to orange) or visible to near-infrared. The shift can be as much as 200 nm, but usually the range is about 20-100 nm longer than the absorption spectrum. Glow sticks using this approach tend to have colored containers, due to the dye embedded in the plastic. Infrared glow sticks may appear dark-red to black, as the dyes absorb the visible light produced inside the container and reemit near-infrared.\n\nOn the other hand, various colors can also be achieved by simply mixing several fluorophores within the solution to achieve the desired effect. These various colors can be achieved due to the principles of additive color. For example, a combination of red, yellow, and green fluorophores is used in orange light sticks, and a combination of several fluorescers is used in white light sticks. \n\n\nIn glow sticks, phenol is produced as a byproduct. It is advisable to keep the mixture away from skin and to prevent accidental ingestion if the glow stick case splits or breaks. If spilled on skin, the chemicals could cause slight skin irritation, swelling, or, in extreme circumstances, vomiting and nausea. Some of the chemicals used in older glow sticks were thought to be potential carcinogens. The sensitizers used are polynuclear aromatic hydrocarbons, a class of compounds known for their carcinogenic properties.\n\nDibutyl phthalate, an ingredient sometimes used in glow sticks, has raised some health concerns. While there is no evidence that dibutyl phthalate poses any major health risk, it was put on California's list of suspected teratogens in 2006.\n\nGlow sticks contain ingredients that act as a plasticizer. This means that if a glow stick leaks onto anything plastic it can liquefy it.\n\n\n"}
{"id": "1721949", "url": "https://en.wikipedia.org/wiki?curid=1721949", "title": "Heterojunction bipolar transistor", "text": "Heterojunction bipolar transistor\n\nThe heterojunction bipolar transistor (HBT) is a type of bipolar junction transistor (BJT) which uses differing semiconductor materials for the emitter and base regions, creating a heterojunction. The HBT improves on the BJT in that it can handle signals of very high frequencies, up to several hundred GHz. It is commonly used in modern ultrafast circuits, mostly radio-frequency (RF) systems, and in applications requiring a high power efficiency, such as RF power amplifiers in cellular phones. The idea of employing a heterojunction is as old as the conventional BJT, dating back to a patent from 1951. Detailed theory of heterojunction bipolar transistor was developed by Herbert Kroemer in 1957.\n\nThe principal difference between the BJT and HBT is in the use of differing semiconductor materials for the emitter-base junction and the base-collector junction, creating a heterojunction. The effect is to limit the injection of holes from the base into the emitter region, since the potential barrier in the valence band is higher than in the conduction band. Unlike BJT technology, this allows a high doping density to be used in the base, reducing the base resistance while maintaining gain. The efficiency of the heterojunction is measured by the Kroemer factor. Kroemer was awarded a Nobel Prize in 2000 for his work in this field at the University of California, Santa Barbara.\n\nMaterials used for the substrate include silicon, gallium arsenide, and indium phosphide, while silicon / silicon-germanium alloys, aluminum gallium arsenide / gallium arsenide, and indium phosphide / indium gallium arsenide are used for the epitaxial layers. Wide-bandgap semiconductors such as gallium nitride and indium gallium nitride are especially promising.\n\nIn SiGe graded heterostructure transistors, the amount of germanium in the base is graded, making the bandgap narrower at the collector than at the emitter. That tapering of the bandgap leads to a field-assisted transport in the base, which speeds transport through the base and increases frequency response.\n\nDue to the need to manufacture HBT devices with extremely high-doped thin base layers, molecular beam epitaxy is principally employed. In addition to base, emitter and collector layers, highly doped layers are deposited on either side of collector and emitter to facilitate an ohmic contact, which are placed on the contact layers after exposure by photolithography and etching. The contact layer underneath the collector, named subcollector, is an active part of the transistor.\n\nOther techniques are used depending on the material system. IBM and others use UHV CVD for SiGe; other techniques used include MOVPE for III-V systems.\n\nNormally the epitaxial layers are lattice matched (which restricts the choice of bandgap etc.). If they are near-lattice-matched the device is pseudomorphic, and if the layers are unmatched (often separated by a thin buffer layer) it is metamorphic.\n\nA pseudomorphic heterojunction bipolar transistor developed at the University of Illinois at Urbana-Champaign, built from indium phosphide and indium gallium arsenide and designed with compositionally graded collector, base and emitter, was demonstrated to cut off at a speed of 710 GHz.\n\nBesides being record breakers in terms of speed, HBTs made of InP/InGaAs are ideal for monolithic optoelectronic integrated circuits. A PIN-type photo detector is formed by the base-collector-subcollector layers. The bandgap of InGaAs works well for detecting 1550 nm-wavelength infrared laser signals used in optical communication systems. Biasing the HBT to obtain an active device, a photo transistor with high internal gain is obtained. Among other HBT applications are mixed signal circuits such as analog-to-digital and digital-to-analog converters.\n\n\n"}
{"id": "40956538", "url": "https://en.wikipedia.org/wiki?curid=40956538", "title": "History of bitcoin", "text": "History of bitcoin\n\nBitcoin is a cryptocurrency, a digital asset designed to work as a medium of exchange that uses cryptography to control its creation and management, rather than relying on central authorities. The presumed pseudonymous Satoshi Nakamoto integrated many existing ideas from the cypherpunk community when creating bitcoin. Over the course of bitcoin's history, it has undergone rapid growth to become a significant currency both on and offline from the mid 2010s, some businesses began accepting bitcoin in addition to traditional currencies.\n\nPrior to the release of bitcoin there were a number of digital cash technologies starting with the issuer based ecash protocols of David Chaum and Stefan Brands. Adam Back developed hashcash, a proof-of-work scheme for spam control. The first proposals for distributed digital scarcity based cryptocurrencies were Wei Dai's b-money and Nick Szabo's bit gold. Hal Finney developed reusable proof of work (RPOW) using hashcash as its proof of work algorithm.\n\nIn the bit gold proposal which proposed a collectible market based mechanism for inflation control, Nick Szabo also investigated some additional aspects including a Byzantine fault-tolerant agreement protocol based on quorum addresses to store and transfer the chained proof-of-work solutions, which was vulnerable to Sybil attacks, though.\n\nOn 18 August 2008, the domain name bitcoin.org was registered. Later that year on 31 October, a link to a paper authored by Satoshi Nakamoto titled \"Bitcoin: A Peer-to-Peer Electronic Cash System\" was posted to a cryptography mailing list. This paper detailed methods of using a peer-to-peer network to generate what was described as \"a system for electronic transactions without relying on trust\". On 3 January 2009, the bitcoin network came into existence with Satoshi Nakamoto mining the \"genesis block of bitcoin\" (block number 0), which had a reward of 50 bitcoins. Embedded in the coinbase of this block was the text:\nThe Times 03/Jan/2009 Chancellor on brink of second bailout for banks.\nThe text refers to a headline in \"The Times\" published on 3 January 2009. This note has been interpreted as both a timestamp of the genesis date and a derisive comment on the instability caused by fractional-reserve banking.\n\nThe first open source bitcoin client was released on 9 January 2009, hosted at SourceForge.\n\nOne of the first supporters, adopters, contributor to bitcoin and receiver of the first bitcoin transaction was programmer Hal Finney. Finney downloaded the bitcoin software the day it was released, and received 10 bitcoins from Nakamoto in the world's first bitcoin transaction on 12 January 2009. Other early supporters were Wei Dai, creator of bitcoin predecessor \"b-money\", and Nick Szabo, creator of bitcoin predecessor \"bit gold\".\n\nIn the early days, Nakamoto is estimated to have mined 1 million bitcoins. Before disappearing from any involvement in bitcoin, Nakamoto in a sense handed over the reins to developer Gavin Andresen, who then became the bitcoin lead developer at the Bitcoin Foundation, the 'anarchic' bitcoin community's closest thing to an official public face.\n\nThe value of the first bitcoin transactions were negotiated by individuals on the bitcoin forum with one notable transaction of 10,000 BTC used to indirectly purchase two pizzas delivered by Papa John's.\n\nOn 6 August 2010, a major vulnerability in the bitcoin protocol was spotted. Transactions weren't properly verified before they were included in the transaction log or \"blockchain\", which let users bypass bitcoin's economic restrictions and create an indefinite number of bitcoins. On 15 August, the vulnerability was exploited; over 184 billion bitcoins were generated in a transaction, and sent to two addresses on the network. Within hours, the transaction was spotted and erased from the transaction log after the bug was fixed and the network forked to an updated version of the bitcoin protocol. This was the only major security flaw found and exploited in bitcoin's history.\n\n\"Satoshi Nakamoto\" is presumed to be a pseudonym for the person or people who designed the original bitcoin protocol in 2008 and launched the network in 2009. Nakamoto was responsible for creating the majority of the official bitcoin software and was active in making modifications and posting technical information on the bitcoin forum. There has been much speculation as to the identity of Satoshi Nakamoto with suspects including Dai, Szabo, and Finney and accompanying denials. The possibility that Satoshi Nakamoto was a computer collective in the European financial sector has also been discussed.\n\nInvestigations into the real identity of Satoshi Nakamoto were attempted by \"The New Yorker\" and \"Fast Company\". \"The New Yorker's\" investigation brought up at least two possible candidates: Michael Clear and Vili Lehdonvirta. \"Fast Company\"'s investigation brought up circumstantial evidence linking an encryption patent application filed by Neal King, Vladimir Oksman and Charles Bry on 15 August 2008, and the bitcoin.org domain name which was registered 72 hours later. The patent application (#20100042841) contained networking and encryption technologies similar to bitcoin's, and textual analysis revealed that the phrase \"... computationally impractical to reverse\" appeared in both the patent application and bitcoin's whitepaper. All three inventors explicitly denied being Satoshi Nakamoto.\n\nIn May 2013, Ted Nelson speculated that Japanese mathematician Shinichi Mochizuki is Satoshi Nakamoto. Later in 2013 the Israeli researchers Dorit Ron and Adi Shamir pointed to Silk Road-linked Ross William Ulbricht as the possible person behind the cover. The two researchers based their suspicion on an analysis of the network of bitcoin transactions. These allegations were contested and Ron and Shamir later retracted their claim.\n\nNakamoto's involvement with bitcoin does not appear to extend past mid-2010. In April 2011, Nakamoto communicated with a bitcoin contributor, saying that he had \"moved on to other things\".\n\nStefan Thomas, a Swiss coder and active community member, graphed the time stamps for each of Nakamoto's 500-plus bitcoin forum posts; the resulting chart showed a steep decline to almost no posts between the hours of 5 a.m. and 11 a.m. Greenwich Mean Time. Because this pattern held true even on Saturdays and Sundays, it suggested that Nakamoto was asleep at this time, and the hours of 5 a.m. to 11 a.m. GMT are midnight to 6 a.m. Eastern Standard Time (North American Eastern Standard Time). Other clues suggested that Nakamoto was British: A newspaper headline he had encoded in the genesis block came from the UK-published newspaper \"The Times\", and both his forum posts and his comments in the bitcoin source code used British English spellings, such as \"optimise\" and \"colour\".\n\nAn Internet search by an anonymous blogger of texts similar in writing to the bitcoin whitepaper suggests Nick Szabo's \"bit gold\" articles as having a similar author. Nick denied being Satoshi, and stated his official opinion on Satoshi and bitcoin in a May 2011 article.\n\nIn a March 2014 article in \"Newsweek,\" journalist Leah McGrath Goodman doxed Dorian S. Nakamoto of Temple City, California, saying that Satoshi Nakamoto is the man's birth name. Her methods and conclusion drew widespread criticism.\n\nIn June 2016, the London Review of Books published a piece by Andrew O'Hagan about Nakamoto. The real identity of Satoshi Nakamoto still remains a matter of dispute.\n\nBased on bitcoin's open source code, other cryptocurrencies started to emerge.\n\nThe Electronic Frontier Foundation, a non-profit group, started accepting bitcoins in January 2011, then stopped accepting them in June 2011, citing concerns about a lack of legal precedent about new currency systems. The EFF's decision was reversed on 17 May 2013 when they resumed accepting bitcoin.\n\nIn June 2011, WikiLeaks and other organizations began to accept bitcoins for donations.\n\nIn January 2012, bitcoin was featured as the main subject within a fictionalized trial on the CBS legal drama \"The Good Wife\" in the third-season episode \"Bitcoin for Dummies\". The host of CNBC's \"Mad Money\", Jim Cramer, played himself in a courtroom scene where he testifies that he doesn't consider bitcoin a true currency, saying \"There's no central bank to regulate it; it's digital and functions completely peer to peer\".\n\nIn September 2012, the Bitcoin Foundation was launched to \"accelerate the global growth of bitcoin through standardization, protection, and promotion of the open source protocol\". The founders were Gavin Andresen, Jon Matonis, Patrick Murck, Charlie Shrem, and Peter Vessenes.\n\nIn October 2012, BitPay reported having over 1,000 merchants accepting bitcoin under its payment processing service. In November 2012, WordPress had started accepting bitcoins.\n\nIn February 2013, the bitcoin-based payment processor Coinbase reported selling US$1 million worth of bitcoins in a single month at over $22 per bitcoin. The Internet Archive announced that it was ready to accept donations as bitcoins and that it intends to give employees the option to receive portions of their salaries in bitcoin currency.\n\nIn March, the bitcoin transaction log called the \"blockchain\" temporarily split into two independent chains with differing rules on how transactions were accepted. For six hours two bitcoin networks operated at the same time, each with its own version of the transaction history. The core developers called for a temporary halt to transactions, sparking a sharp sell-off. Normal operation was restored when the majority of the network downgraded to version 0.7 of the bitcoin software. The Mt. Gox exchange briefly halted bitcoin deposits and the exchange rate briefly dipped by 23% to $37 as the event occurred before recovering to previous level of approximately $48 in the following hours. In the US, the Financial Crimes Enforcement Network (FinCEN) established regulatory guidelines for \"decentralized virtual currencies\" such as bitcoin, classifying American bitcoin miners who sell their generated bitcoins as Money Service Businesses (or MSBs), that may be subject to registration and other legal obligations.\n\nIn April, payment processors \"BitInstant\" and \"Mt. Gox\" experienced processing delays due to insufficient capacity resulting in the bitcoin exchange rate dropping from $266 to $76 before returning to $160 within six hours. Bitcoin gained greater recognition when services such as OkCupid and Foodler began accepting it for payment.\n\nOn 15 May 2013, the US authorities seized accounts associated with Mt. Gox after discovering that it had not registered as a money transmitter with FinCEN in the US.\n\nOn 17 May 2013, it was reported that BitInstant processed approximately 30 percent of the money going into and out of bitcoin, and in April alone facilitated 30,000 transactions,\n\nOn 23 June 2013, it was reported that the US Drug Enforcement Administration listed 11.02 bitcoins as a seized asset in a United States Department of Justice seizure notice pursuant to 21 U.S.C. § 881. This marked the first time a government agency claimed to have seized bitcoin.\n\nIn July 2013, a project began in Kenya linking bitcoin with M-Pesa, a popular mobile payments system, in an experiment designed to spur innovative payments in Africa. During the same month the Foreign Exchange Administration and Policy Department in Thailand stated that bitcoin lacks any legal framework and would therefore be illegal, which effectively banned trading on bitcoin exchanges in the country. According to Vitalik Buterin, a writer for Bitcoin Magazine, \"bitcoin's fate in Thailand may give the electronic currency more credibility in some circles\", but he was concerned it didn't bode well for bitcoin in China.\n\nOn 6 August 2013, Federal Judge Amos Mazzant of the Eastern District of Texas of the Fifth Circuit ruled that bitcoins are \"a currency or a form of money\" (specifically securities as defined by Federal Securities Laws), and as such were subject to the court's jurisdiction, and Germany's Finance Ministry subsumed bitcoins under the term \"unit of account\" a financial instrument though not as e-money or a functional currency, a classification nonetheless having legal and tax implications.\n\nIn October 2013, the FBI seized roughly 26,000 BTC from website Silk Road during the arrest of alleged owner Ross William Ulbricht. Two companies, Robocoin and Bitcoiniacs launched the world's first bitcoin ATM on 29 October 2013 in Vancouver, BC, Canada, allowing clients to sell or purchase bitcoin currency at a downtown coffee shop. Chinese internet giant Baidu had allowed clients of website security services to pay with bitcoins.\n\nIn November 2013, the University of Nicosia announced that it would be accepting bitcoin as payment for tuition fees, with the university's chief financial officer calling it the \"gold of tomorrow\". During November 2013, the China-based bitcoin exchange BTC China overtook the Japan-based Mt. Gox and the Europe-based Bitstamp to become the largest bitcoin trading exchange by trade volume.\n\nIn December 2013, Overstock.com announced plans to accept bitcoin in the second half of 2014. \nOn 5 December 2013, the People's Bank of China prohibited Chinese financial institutions from using bitcoins. After the announcement, the value of bitcoins dropped, and Baidu no longer accepted bitcoins for certain services. Buying real-world goods with any virtual currency had been illegal in China since at least 2009.\n\nIn January 2014, Zynga announced it was testing bitcoin for purchasing in-game assets in seven of its games. That same month, The D Las Vegas Casino Hotel and Golden Gate Hotel & Casino properties in downtown Las Vegas announced they would also begin accepting bitcoin, according to an article by \"USA Today\". The article also stated the currency would be accepted in five locations, including the front desk and certain restaurants. The network rate exceeded 10 petahash/sec. TigerDirect and Overstock.com started accepting bitcoin.\n\nIn early February 2014, one of the largest bitcoin exchanges, Mt. Gox, suspended withdrawals citing technical issues. By the end of the month, Mt. Gox had filed for bankruptcy protection in Japan amid reports that 744,000 bitcoins had been stolen. Months before the filing, the popularity of Mt. Gox had waned as users experienced difficulties withdrawing funds.\n\nIn June 2014 the network exceeded 100 petahash/sec. On 18 June 2014, it was announced that bitcoin payment service provider BitPay would become the new sponsor of St. Petersburg Bowl under a two-year deal, renamed the Bitcoin St. Petersburg Bowl. Bitcoin was to be accepted for ticket and concession sales at the game as part of the sponsorship, and the sponsorship itself was also paid for using bitcoin.\n\nIn July 2014 Newegg and Dell started accepting bitcoin.\n\nIn September 2014 TeraExchange, LLC, received approval from the U.S.Commodity Futures Trading Commission \"CFTC\" to begin listing an over-the-counter swap product based on the price of a bitcoin. The CFTC swap product approval marks the first time a U.S. regulatory agency approved a bitcoin financial product.\n\nIn December 2014 Microsoft began to accept bitcoin to buy Xbox games and Windows software.\n\nIn 2014, several lighthearted songs celebrating bitcoin such as the \"Ode to Satoshi\" have been released.\n\nA documentary film, \"The Rise and Rise of Bitcoin\", was released in 2014, featuring interviews with bitcoin users, such as a computer programmer and a drug dealer.\n\nIn January 2015 Coinbase raised 75 million USD as part of a Series C funding round, smashing the previous record for a bitcoin company. Less than one year after the collapse of Mt. Gox, United Kingdom-based exchange Bitstamp announced that their exchange would be taken offline while they investigate a hack which resulted in about 19,000 bitcoins (equivalent to roughly US$5 million at that time) being stolen from their hot wallet. The exchange remained offline for several days amid speculation that customers had lost their funds. Bitstamp resumed trading on 9 January after increasing security measures and assuring customers that their account balances would not be impacted.\n\nIn February 2015, the number of merchants accepting bitcoin exceeded 100,000.\n\nIn October 2015, a proposal was submitted to the Unicode Consortium to add a code point for the bitcoin symbol.\n\nIn January 2016, the network rate exceeded 1 exahash/sec.\n\nIn March 2016, the Cabinet of Japan recognized virtual currencies like bitcoin as having a function similar to real money. Bidorbuy, the largest South African online marketplace, launched bitcoin payments for both buyers and sellers.\n\nIn April 2016, Steam started accepting bitcoin as payment for video games and other online media.\n\nIn July 2016, researchers published a paper showing that by November 2013 bitcoin commerce was no longer driven by \"sin\" activities but instead by legitimate enterprises. Uber switched to bitcoin in Argentina after the government blocked credit card companies from dealing with Uber.\n\nIn August 2016, a major bitcoin exchange, Bitfinex, was hacked and nearly 120,000 BTC (around $60m) was stolen.\n\nIn September 2016, the number of bitcoin ATMs had doubled over the last 18 months and reached 771 ATMs worldwide.\n\nIn November 2016, the Swiss Railway operator SBB (CFF) upgraded all their automated ticket machines so that bitcoin could be bought from them using the scanner on the ticket machine to scan the bitcoin address on a phone app.\n\nBitcoin generates more academic interest year after year; the number of Google Scholar articles published mentioning bitcoin grew from 83 in 2009, to 424 in 2012, and 3580 in 2016. Also, the academic Ledger (journal) published its first issue. It is edited by Peter Rizun.\n\nThe number of businesses accepting bitcoin continued to increase. In January 2017, NHK reported the number of online stores accepting bitcoin in Japan had increased 4.6 times over the past year. BitPay CEO Stephen Pair declared the company's transaction rate grew 3× from January 2016 to February 2017, and explained usage of bitcoin is growing in B2B supply chain payments.\n\nBitcoin gains more legitimacy among lawmakers and legacy financial companies. For example, Japan passed a law to accept bitcoin as a legal payment method, and Russia has announced that it will legalize the use of cryptocurrencies such as bitcoin. And Norway’s largest online bank, Skandiabanken, integrates bitcoin accounts.\n\nIn March 2017, the number of GitHub projects related to bitcoin passed 10,000.\n\nExchange trading volumes continue to increase. For the 6-month period ending March 2017, Mexican exchange Bitso saw trading volume increase 1500%.\nBetween January and May 2017 Poloniex saw an increase of more than 600% active traders online and regularly processed 640% more transactions.\n\nIn June 2017, the bitcoin symbol was encoded in Unicode version 10.0 at position U+20BF (₿) in the Currency Symbols block.\n\nUp until July 2017, bitcoin users maintained a common set of rules for the cryptocurrency. On 1 August 2017 bitcoin split into two derivative digital currencies, the bitcoin (BTC) chain with 1 MB blocksize limit and the Bitcoin Cash (BCH) chain with 8 MB blocksize limit. The split has been called the \"Bitcoin Cash hard fork\".\n\nOn 6 December 2017 the software marketplace Steam announced that it would no longer accept bitcoin as payment for its products, citing slow transactions speeds, price volatility, and high fees for transactions.\n\nOn 22 January 2018, South Korea brought in a regulation that requires all the bitcoin traders to reveal their identity, thus putting a ban on anonymous trading of bitcoins.\n\nOn 24 January 2018, the online payment firm Stripe announced that it would phase out its support for bitcoin payments by late April 2018, citing declining demand, rising fees and longer transaction times as the reasons.\n\nAmong the factors which may have contributed to this rise were the European sovereign-debt crisis particularly the 2012–2013 Cypriot financial crisis statements by FinCEN improving the currency's legal standing and rising media and Internet interest.\n\nUntil 2013, almost all market with bitcoins were in United States dollars (US$).\n\nAs the market valuation of the total stock of bitcoins approached US$1 billion, some commentators called bitcoin prices a bubble. In early April 2013, the price per bitcoin dropped from $266 to around $50 and then rose to around $100. Over two weeks starting late June 2013 the price dropped steadily to $70. The price began to recover, peaking once again on 1 October at $140. On 2 October, The Silk Road was seized by the FBI. This seizure caused a flash crash to $110. The price quickly rebounded, returning to $200 several weeks later. The latest run went from $200 on 3 November to $900 on 18 November. Bitcoin passed US$1,000 on 28 November 2013 at Mt. Gox.\n\nPrices fell to around $400 in April 2014, before rallying in the middle of the year. They then declined to not much more than $200 in early 2015.\n\nA fork referring to a blockchain is defined variously as a blockchain split into two paths forward, or as a change of protocol rules. Accidental forks on the bitcoin network regularly occur as part of the mining process. They happen when two miners find a block at a similar point in time. As a result, the network briefly forks. This fork is subsequently resolved by the software which automatically chooses the longest chain, thereby orphaning the extra blocks added to the shorter chain (that were dropped by the longer chain).\n\nOn 12 March 2013, a bitcoin miner running version 0.8.0 of the bitcoin software created a large block that was considered invalid in version 0.7 (due to an undiscovered inconsistency between the two versions).\nThis created a split or \"fork\" in the blockchain since computers with the recent version of the software accepted the invalid block and continued to build on the diverging chain, whereas older versions of the software rejected it and continued extending the blockchain without the offending block.\nThis split resulted in two separate transaction logs being formed without clear consensus, which allowed for the same funds to be spent differently on each chain. In response, the Mt. Gox exchange temporarily halted bitcoin deposits. The exchange rate fell 23% to $37 on the Mt. Gox exchange but rose most of the way back to its prior level of $48.\n\nMiners resolved the split by downgrading to version 0.7, putting them back on track with the canonical blockchain.\nUser funds largely remained unaffected and were available when network consensus was restored. The network reached consensus and continued to operate as normal a few hours after the split.\n\nOn 18 March 2013, the Financial Crimes Enforcement Network (or FinCEN), a bureau of the United States Department of the Treasury, issued a report regarding centralized and decentralized \"virtual currencies\" and their legal status within \"money services business\" (MSB) and Bank Secrecy Act regulations. It classified digital currencies and other digital payment systems such as bitcoin as \"virtual currencies\" because they are not legal tender under any sovereign jurisdiction. FinCEN cleared American users of bitcoin of legal obligations by saying, \"A user of virtual currency is not an MSB under FinCEN's regulations and therefore is not subject to MSB registration, reporting, and recordkeeping regulations.\" However, it held that American entities who generate \"virtual currency\" such as bitcoins are money transmitters or MSBs if they sell their generated currency for national currency: \"...a person that creates units of convertible virtual currency and sells those units to another person for real currency or its equivalent is engaged in transmission to another location and is a money transmitter.\" This specifically extends to \"miners\" of the bitcoin currency who may have to register as MSBs and abide by the legal requirements of being a money transmitter if they sell their generated bitcoins for national currency and are within the United States. Since FinCEN issued this guidance, dozens of virtual currency exchangers and administrators have registered with FinCEN, and FinCEN is receiving an increasing number of suspicious activity reports (SARs) from these entities.\n\nAdditionally, FinCEN claimed regulation over American entities that manage bitcoins in a payment processor setting or as an exchanger: \"In addition, a person is an exchanger and a money transmitter if the person accepts such de-centralized convertible virtual currency from one person and transmits it to another person as part of the acceptance and transfer of currency, funds, or other value that substitutes for currency.\"\n\nIn summary, FinCEN's decision would require bitcoin exchanges where bitcoins are traded for traditional currencies to disclose large transactions and suspicious activity, comply with money laundering regulations, and collect information about their customers as traditional financial institutions are required to do.\n\nPatrick Murck of the Bitcoin Foundation criticized FinCEN's report as an \"overreach\" and claimed that FinCEN \"cannot rely on this guidance in any enforcement action\".\n\nJennifer Shasky Calvery, the director of FinCEN said, \"Virtual currencies are subject to the same rules as other currencies. ... Basic money-services business rules apply here.\"\n\nIn its October 2012 study, \"Virtual currency schemes\", the European Central Bank concluded that the growth of virtual currencies will continue, and, given the currencies' inherent price instability, lack of close regulation, and risk of illegal uses by anonymous users, the Bank warned that periodic examination of developments would be necessary to reassess risks.\n\nIn 2013, the U.S. Treasury extended its anti-money laundering regulations to processors of bitcoin transactions.\n\nIn June 2013, Bitcoin Foundation board member Jon Matonis wrote in \"Forbes\" that he received a warning letter from the California Department of Financial Institutions accusing the foundation of unlicensed money transmission. Matonis denied that the foundation is engaged in money transmission and said he viewed the case as \"an opportunity to educate state regulators.\"\n\nIn late July 2013, the industry group Committee for the Establishment of the Digital Asset Transfer Authority began to form to set best practices and standards, to work with regulators and policymakers to adapt existing currency requirements to digital currency technology and business models and develop risk management standards.\n\nIn 2014, the U.S. Securities and Exchange Commission filed an administrative action against Erik T. Voorhees, for violating Securities Act Section 5 for publicly offering unregistered interests in two bitcoin websites in exchange for bitcoins.\n\nBitcoins can be stored in a bitcoin cryptocurrency wallet. Theft of bitcoin has been documented on numerous occasions. At other times, bitcoin exchanges have shut down, taking their clients' bitcoins with them. A \"Wired\" study published April 2013 showed that 45 percent of bitcoin exchanges end up closing.\n\nOn 19 June 2011, a security breach of the Mt. Gox bitcoin exchange caused the nominal price of a bitcoin to fraudulently drop to one cent on the Mt. Gox exchange, after a hacker used credentials from a Mt. Gox auditor's compromised computer illegally to transfer a large number of bitcoins to himself. They used the exchange's software to sell them all nominally, creating a massive \"ask\" order at any price. Within minutes, the price reverted to its correct user-traded value. Accounts with the equivalent of more than US$8,750,000 were affected.\n\nIn July 2011, the operator of Bitomat, the third-largest bitcoin exchange, announced that he had lost access to his wallet.dat file with about 17,000 bitcoins (roughly equivalent to US$220,000 at that time). He announced that he would sell the service for the missing amount, aiming to use funds from the sale to refund his customers.\n\nIn August 2011, MyBitcoin, a now defunct bitcoin transaction processor, declared that it was hacked, which caused it to be shut down, paying 49% on customer deposits, leaving more than 78,000 bitcoins (equivalent to roughly US$800,000 at that time) unaccounted for.\n\nIn early August 2012, a lawsuit was filed in San Francisco court against Bitcoinica a bitcoin trading venue claiming about US$460,000 from the company. Bitcoinica was hacked twice in 2012, which led to allegations that the venue neglected the safety of customers' money and cheated them out of withdrawal requests.\n\nIn late August 2012, an operation titled Bitcoin Savings and Trust was shut down by the owner, leaving around US$5.6 million in bitcoin-based debts; this led to allegations that the operation was a Ponzi scheme. In September 2012, the U.S. Securities and Exchange Commission had reportedly started an investigation on the case.\n\nIn September 2012, Bitfloor, a bitcoin exchange, also reported being hacked, with 24,000 bitcoins (worth about US$250,000) stolen. As a result, Bitfloor suspended operations. The same month, Bitfloor resumed operations; its founder said that he reported the theft to FBI, and that he plans to repay the victims, though the time frame for repayment is unclear.\n\nOn 3 April 2013, Instawallet, a web-based wallet provider, was hacked, resulting in the theft of over 35,000 bitcoins which were valued at US$129.90 per bitcoin at the time, or nearly $4.6 million in total. As a result, Instawallet suspended operations.\n\nOn 11 August 2013, the Bitcoin Foundation announced that a bug in a pseudorandom number generator within the Android operating system had been exploited to steal from wallets generated by Android apps; fixes were provided 13 August 2013.\n\nIn October 2013, Inputs.io, an Australian-based bitcoin wallet provider was hacked with a loss of 4100 bitcoins, worth over A$1 million at time of theft. The service was run by the operator TradeFortress. Coinchat, the associated bitcoin chat room, has been taken over by a new admin.\n\nOn 26 October 2013, a Hong-Kong based bitcoin trading platform owned by Global Bond Limited (GBL) vanished with 30 million yuan (US$5 million) from 500 investors.\n\nMt. Gox, the Japan-based exchange that in 2013 handled 70% of all worldwide bitcoin traffic, declared bankruptcy in February 2014, with bitcoins worth about $390 million missing, for unclear reasons. The CEO was eventually arrested and charged with embezzlement.\n\nOn 3 March 2014, Flexcoin announced it was closing its doors because of a hack attack that took place the day before. In a statement that once occupied their homepage, they announced on 3 March 2014 that \"As Flexcoin does not have the resources, assets, or otherwise to come back from this loss [the hack], we are closing our doors immediately.\" Users can no longer log into the site.\n\nChinese cryptocurrency exchange Bter lost $2.1 million in BTC in February 2015.\n\nThe Slovenian exchange Bitstamp lost bitcoin worth $5.1 million to a hack in January 2015.\n\nThe US-based exchange Cryptsy declared bankruptcy in January 2016, ostensibly because of a 2014 hacking incident; the court-appointed receiver later alleged that Cryptsy's CEO had stolen $3.3 million.\n\nIn May 2016, Gatecoin closed temporarily after a breach had caused a loss of about $2 million in cryptocurrency. It subsequently relaunched its exchange in August 2016 and is slowly reimbursing its customers.\n\nIn August 2016, hackers stole some $72 million in customer bitcoin from the Hong-Kong-based exchange Bitfinex.\n\nIn December 2017, hackers stole 4,700 bitcoins from NiceHash a platform that allowed users to sell hashing power. The value of the stolen bitcoins totaled about $80M.\n\nOn December 19, 2017, Yapian, a company that owns the Youbit cryptocurrency exchange in South Korea, filed for bankruptcy following a hack, the second in eight months.\n\nIn 2012, the Cryptocurrency Legal Advocacy Group (CLAG) stressed the importance for taxpayers to determine whether taxes are due on a bitcoin-related transaction based on whether one has experienced a \"realization event\": when a taxpayer has provided a service in exchange for bitcoins, a realization event has probably occurred and any gain or loss would likely be calculated using fair market values for the service provided.\"\n\nIn August 2013, the German Finance Ministry characterized bitcoin as a unit of account, usable in multilateral clearing circles and subject to capital gains tax if held less than one year.\n\nOn 5 December 2013, the People's Bank of China announced in a press release regarding bitcoin regulation that whilst individuals in China are permitted to freely trade and exchange bitcoins as a commodity, it is prohibited for Chinese financial banks to operate using bitcoins or for bitcoins to be used as legal tender currency, and that entities dealing with bitcoins must track and report suspicious activity to prevent money laundering. The value of bitcoin dropped on various exchanges between 11 and 20 percent following the regulation announcement, before rebounding upward again.\n\nBitcoin's blockchain can be loaded with arbitrary data. In 2018 researchers from RWTH Aachen University and Goethe University identified 1,600 files added to the blockchain, 59 of which included links to unlawful images of child exploitation, politically sensitive content, or privacy violations. \"Our analysis shows that certain content, eg, illegal pornography, can render the mere possession of a blockchain illegal.\"\n\nInterpol also sent out an alert in 2015 saying that \"the design of the blockchain means there is the possibility of malware being injected and permanently hosted with no methods currently available to wipe this data\".\n\n"}
{"id": "23443160", "url": "https://en.wikipedia.org/wiki?curid=23443160", "title": "InLiving", "text": "InLiving\n\nInLiving is an educational game focusing on independent living. Aimed at young people aged 13–25, the game has been developed by Creative North Studios and Kirklees Neighbourhood Housing for use by housing organisations as a tool to deliver key information to prospective and current tenants. The game helps players understand the challenges they may face when moving into a property for the first time. The game was launched on 5 June 2008.\n\nThe game was originally developed for mobile phones and currently supports over 500 different devices. A web-based version of the game is currently in development due for release late 2009.\n\nThe game has had positive reception since its launch in June 2008 and was soon featured in \"Inside Housing\" in the 'What Works' section. The InLiving project is also supported as part of the Innovation Exchange programme.\n\nSeveral other games have been developed based on the InLiving (Erudite) Platform. Two such games are HouseM8 and StreetM8.\n\n"}
{"id": "44706880", "url": "https://en.wikipedia.org/wiki?curid=44706880", "title": "Iron hand (prosthesis)", "text": "Iron hand (prosthesis)\n\nIron hands are metal prostheses for hands and upper extremities from the middle ages and early modern period. These designs combined cosmetic and functional properties. The most famous example of an iron hand was made around the year 1530, being the second prosthetic hand made for the German knight Götz von Berlichingen.\n\nMost iron hands are based on the same constructive principles, although there are considerable differences in complexity. Fingers can be flexed passively (for example using the healthy hand) and are locked in place by a ratchet mechanism, similar to those of contemporary flintlocks. Extension of the fingers works by spring pressure.\n\n"}
{"id": "144146", "url": "https://en.wikipedia.org/wiki?curid=144146", "title": "List of programming languages", "text": "List of programming languages\n\nThe aim of this list of programming languages is to include all notable programming languages in existence, both those in current use and historical ones, in alphabetical order. Dialects of BASIC, esoteric programming languages, and markup languages are not included. \n"}
{"id": "2108110", "url": "https://en.wikipedia.org/wiki?curid=2108110", "title": "List of welding processes", "text": "List of welding processes\n\nThis is a list of welding processes, separated into their respective categories. The associated \"N reference numbers\" (second column) are specified in ISO 4063 (in the European Union published as \"EN ISO 4063\"). Numbers in parentheses are obsolete and were removed from the current (1998) version of ISO 4063. The AWS reference codes of the American Welding Society are commonly used in North America.\n\n\n"}
{"id": "11727138", "url": "https://en.wikipedia.org/wiki?curid=11727138", "title": "Madia (furniture)", "text": "Madia (furniture)\n\nMadia was a piece of furniture used during the High Renaissance period. It stored food and dishes. The madia was a service piece, meaning it was not made for looks, it was made to hold objects and used for a purpose. It would usually be found in the kitchen, not in an open area.\n\nIn Italy, the Madia was also a kitchen piece that was used to make bread. The Madia had two parts, the first was a cupboard bottom that could be used to store items. The top part could be lifted up. The dough would be placed in the top portion where it could rise, and then would be baked. \n\n"}
{"id": "7627165", "url": "https://en.wikipedia.org/wiki?curid=7627165", "title": "Magic Bullet (appliance)", "text": "Magic Bullet (appliance)\n\nThe Magic Bullet is a compact blender sold by Homeland Housewares, a division of the American company Alchemy Worldwide, and sold in over 50 countries. It is widely marketed through television advertisements and infomercials and sold in retail stores under the \"As seen on TV\" banner. A feature-limited retail version not under this banner called the \"Magic Bullet Single Shot+\" is also available.\n\nSince the introduction of the Magic Bullet, other incarnations include the Magic Bullet To Go, the Magic Bullet Mini, the Bullet Express, the Baby Bullet, the NutriBullet, the Party Bullet and the Dessert Bullet.\n\nThe Magic Bullet is a personal blender that is designed to be used as a space saving replacement for other appliances such as a blender, food processor, and electric juicer. The name is derived from the ogive-shaped curve of the blending cups. The entire Magic Bullet system consists of an electric blender base with a number of attachments. Some of the attachments are included with the product, and they are:\n\nOther add-ons can be purchased separately. These consist of:\n\nIn May 2018, Fox affiliate KTTV in Los Angeles obtained test videos from NutriBullet which appeared to show the machine exploding in different situations, and some consumers told FOX11 they were injured by using the blender. So far, 14 people have sued the company saying they were cut or burned when their NutriBullet exploded. The company has denied responsibility for the consumers’ injuries.\n\nThe appliance is used by attaching a blade attachment to the desired cup and fitting the assembly upside down on top of the base. The base contains the motor that turns the blade, which is inside the cup. When one applies pressure to the top of the unit, the blade spins. If one turns the cup to lock into the base, it will continue to spin until it is disengaged.\n\nThe Magic Bullet is known for its 30-minute infomercial, broadcast mostly in the early hours of the morning.\n\nThe design for the Magic Bullet and its attachments is registered with the United States Patent and Trademark Office to Lenny Sands, CEO and founding partner of Alchemy Worldwide.\n\nHomeland Housewares has introduced add-ons and different versions of the Magic Bullet including the Bullet2Go (with various accessories), the Bullet Express, and a Fat Burning Boost supplement to be used in conjunction with the Magic Bullet. In 2015, Magic Bullet introduced an app that offers breakfast, lunch and dinner recipes to Magic Bullet users. \n\nIn October 2017, the Bluetooth-enabled NutriBullet Balance was released. The blender works with a companion app which allows users to track calories from ingredients processed using the machine.\n\nThe Magic Bullet has been replicated and imitated on more than one occasion. Homeland Housewares, LLC, is a member of eBay’s Verified Rights Owner (VeRO) program and has created a Consumer Counterfeit Watch web page in order to help educate consumers regarding these issues.\n"}
{"id": "5547199", "url": "https://en.wikipedia.org/wiki?curid=5547199", "title": "MiniTSFO", "text": "MiniTSFO\n\nThe Mini TSFO (Training Set, Fire Observation) was the first artillery call-for-fire simulation designed for the personal computer. It was started in 1985 as an outgrowth of a Field Artillery Officer Advanced Course battlefield research project at the U.S. Army Field Artillery School (USAFAS) to develop a concept for incorporating PCs into artillery training, and was completed in 1986. It replaced summer artillery live fire training for cadets at West Point in 1986 and 1987.\n\nOne of the USAFAS students involved in the battlefield research project, Captain (later Colonel) Bill Erwin, volunteered to continue to develop the concept into an actual application during his follow-on assignment to the USAFAS Directorate of Training and Doctrine.\n\nVersions:\n\nMINITSFO - Original version using a Digital Message Device (DMD), virtual map, and CGA graphics.\n\nVTSFO - Replaced the DMD with keyboard interface for use at West Point.\n\nNGFTSFO - Naval Gunfire version using EGA graphics, actual scenes from San Clemente Island training area, and incorporating effects of dispersion between rounds.\n\nAs originally envisioned, the MiniTSFO would be a complete system that required only a computer to play. This meant that there had to be some way to see targets on-screen, some way to locate the target coordinates such as a map, and some way to call for fire. This resulted in a combination of three screens which the user could flip back-and-forth by using function keys. The first screen showed a simutated view through an AN/GVS-5 laser rangefinder. The map screen was a depiction of the fictional German town of Nitzburg and surrounding area. Lastly, the way to enter firing commands was a virtual AN/PSG-2 Digital Message Device (DMD).\n\nThe virtual DMD required extensive programming in order to simulate the actual operation of a real DMD and consumed most of the program code space.\n\nThe MiniTSFO was originally coded in BASICA, an interpreted version of BASIC available on IBM PCs. As the design of the program was pushing the limits of BASICA, Microsoft introduced the QuickBASIC compiler. This allowed the MiniTSFO to grow beyond the memory limits of BASICA and structured programming allowed additional complexity.\n\nIn its initial design, the MiniTSFO drew all screens from program code. It wasn't long before the limitations of this approach became obvious and so the screens completed to date were captured and imported into PC Paintbrush to be edited. This allowed the additional of details that would have been too tedious to incorporate through code and also allowed the target screens to be easily edited to add additional types of targets.\n\nTo allow the target screens to be easily changed to provide additional challenges, the target locations and descriptions were read in from an initialization file when the MiniTSFO was started.\n\nThe MiniTSFO used a virtual DMD to control fire missions. The user was presented with one of five random target screens. The user would then determine how to engage the target by either grid location, polar coordinates, or shifting from a known point by flipping back and forth between the target screen and the map. Once the target was located, the user would go to the virtual DMD and enter a fire mission just as a real FO would do with a real DMD. Messages back and forth between the user and the fire direction center (shot and splash) would precede the depiction of an artillery round bursting where the user had described the target location. The user would then use the virtual DMD to adjust the fire onto the target, fire for effect, and then give end-of-mission details. After the end of the fire mission, the user would be critiqued on several factors such as initial target location, description, how many rounds were needed to fire-for-effect, and overall mission completion.\n\nThe best available graphic card for a PC at that time was the CGA which allowed for 320x200 resolution in only four colors. This meant that the graphics for the MiniTSFO, while state-of-the-art for the time, would be considered very crude by today's standards.\n\nThis was the classic edition with the DMD removed and a keyboard interface used instead. NCOs training cadets at West Point decided that teaching them to use a DMD was too time-consuming, and wanted them to just call the fire mission information to the NCO in charge, who would then enter it into the MiniTSFO. This was the only difference from the classic edition.\n\nIn 1987, the Joint Munitions Effectiveness Group became concerned that existing simulations such as the TSFO gave an unrealistic picture of effects on a target because the effect of dispersion between rounds was not taken into account. In other words, if you fired 10 rounds at a target, not all the rounds would hit in the same spot. Most would hit close to the target and a couple might be quite a distance away in a pattern along the gun-to-target line. This can be computed using a Gaussian distribution function.\n\nEfforts to include this effect in the TSFO were deemed too expensive and lengthy. One of the members of the group was familiar with the MiniTSFO and suggested that it be modified. Additionally, since the original MiniTSFO supported the Army call for fire process, it was deemed appropriate to have the new version support the Naval Gunfire (NGF) process.\n\nWork began immediately. By this time, the Extended Graphics Adapter (EGA) was commonly available for the PC offering 640/350 resolution and a palette of 16 colors. Photographs of the San Clemente Island impact area were scanned in, incorporated into PC Paintbrush for cleanup, and realistic targets added. An actual map of the area was also scanned in, providing additional realism.\n\nThe virtual DMD was replaced by a fire mission workup board, as was used by Naval Gunfire Liaison Officers. Once the target was located, the user would flip to the workup board, enter the mission information, and submit to the ship.\n\nCommunications back and forth were through a scrolling strip of text at the bottom of the screen. The actual transmissions that would be used in a real naval gunfire mission kept the user informed on the progress of the mission.\n\nBesides the enhanced graphics and dispersion, several additional enhancements were made. The ship's location would move during the conduct of the fire mission and the user would see the pattern of rounds on the ground move along with the ship as its direction to the target shifted. The initialization file listed the direction, speed, and weapon type for each available firing platform. Also, the effects on the target would be judged on how close the rounds actually came to the target based on the caliber of the weapon.\n\nDevelopment of expansion cards for PCs which allowed communication with the TACFIRE system raised the possibility of using an actual DMD to replace the virtual DMD in the MiniTSFO and also of using the MiniTSFO's virtual DMD to communicate with a real TACFIRE system.\n\nFor several years, the Field Artillery School used the MiniTSFO as an example of the school's leadership in adopting computer technology. An overview of the MiniTSFO was briefed to all pre-command classes for Field Artillery battalion and brigade commanders as well as visiting dignitaries. It also provided some inspiration for the developers of the Guardfist II, a much more capable system. Some artillerymen used it to keep their skills up (the director of the USAFAS Gunnery Department used it before meeting with lieutenants in basic artillery training so that he could impress them with his skills with a DMD). In retrospect, it was a good introduction to what was to come later.\n\n"}
{"id": "40277431", "url": "https://en.wikipedia.org/wiki?curid=40277431", "title": "Ministry of Science and Technology (Bangladesh)", "text": "Ministry of Science and Technology (Bangladesh)\n\nThe Ministry of Science and Technology (; \"Bijñāna ō prayukti Montronaloya\") (abbreviated as MOST) is a ministry of the government of Bangladesh which coordinates science and technology activities in Bangladesh.\n\nThe principle agenda of the Ministry of Science and Technology is socio-economic development of Bangladesh through research, development, extension and successful utilization of Science and Technology.\n\n\n"}
{"id": "38966621", "url": "https://en.wikipedia.org/wiki?curid=38966621", "title": "Monokub", "text": "Monokub\n\nMonokub () is a computer motherboard based on the Russian Elbrus 2000 computer architecture, which form the basis for the Monoblock PC office workstation. \n\nThe motherboard has a miniITX formfactor and contains a single Elbrus-2C+ microprocessor with a clock frequency of 500 MHz. The memory controller provides a dual-channel memory mode. The board has two DDR2-800 memory slots, which enables up to 16 GB of RAM memory (using ECC modules). It also supports expansion boards using PCI Express x16 bus. In addition there is an on-board Gigabit Ethernet interface, 4 USB 2.0, RS-232 interface, DVI connector and audio input/output ports.\n"}
{"id": "44869510", "url": "https://en.wikipedia.org/wiki?curid=44869510", "title": "Mrkšina crkva printing house", "text": "Mrkšina crkva printing house\n\nThe Mrkšina crkva printing house () was a printing house established in 1562 in the Monastery of Mrkšina Crkva in village Kosjerić in Ottoman Serbia (today a town in Serbia). This printing house was operational until 1566 and printed two srbulje books, The Four Gospels (Четворојеванђеље) in 1562 and \"The Flower Triod\" (Триод Цветни) in 1566. In 1567 the Ottomans destroyed the church and its printing house.\n\nToday the exact location of this printing house is unknown. There are different theories about exact location of the monastery and its printing house, but all of them agree that it was somewhere near Kosjerić. One of the most possible locations is above the county house and mouth of the river Kladoroba and Skrapež.\n\nThe books created in the printing house were edited and printed by Hieromonk Mardarije. According to some sources, it was Mardarije who inspired first Dmitrović and then Gundulić to invest in a printing business and organized all activities during the set up of the printing house in Belgrade. Some sources explain that Mardarije moved a printing press from Belgrade to Mrkšina crkva monastery. Two books were printed in the Mrkšina crkva printing house:\n\nThe Flower Triod is recognizable because in this book Mardarije preferred to use figural motifs instead of the ornaments. This book was printed on 1 September 1566 by Hieromonk Mardarije, priest Živko and deacon Radul. This was the last book printed in the Mrkšina crkva printing house in 1566.\n\nIn 1567 the Ottomans burned the church and its printing house, while the monks fled to Rača monastery, similar to monks from Rujno Monastery printing house who fled there after 1537. After the printing house became defunct in 1566, no other such operation existed in Ottoman Serbia until 1831.\n\nTwo copies of the Four Gospels printed at Mrkšina crkva are still preserved in the Monastery of the Holy Trinity of Pljevlja, one copy in Monastery Ždrebaonik and two in Cetinje Monastery. A street in Kosjerić heading to the supposed place of the Mrkšina crkva is named Zoupan Mrkša.\n\n\n"}
{"id": "2850198", "url": "https://en.wikipedia.org/wiki?curid=2850198", "title": "Noseclip", "text": "Noseclip\n\nA noseclip or nose clip is a device made of wire covered in rubber or of plastic, and is worn by people during a wide range of aquatic activities such as kayaking, freediving, recreational swimming, synchronized swimming and waterdance. It is designed to hold the nostrils closed to prevent water from entering, or air from escaping. Some types of nose clips also feature a long band to keep the clip around the neck while it is not being used.\n"}
{"id": "7212867", "url": "https://en.wikipedia.org/wiki?curid=7212867", "title": "Ohio Supercomputer Center", "text": "Ohio Supercomputer Center\n\nEstablished in 1987, the Ohio Supercomputer Center (OSC) is a partner of Ohio universities and industries, providing researchers with high performance computing, advanced cyberinfrastructure, research and computational science education services.\n\nOSC is an organizational member of the Ohio Board of Regents’ Ohio Technology Consortium and works with an array of statewide/regional/national communities, including education, academic research, industry, and state government. The Center's research programs are primarily aligned with three of several key areas of research identified by the state to be well positioned for growth and success: biosciences, advanced materials and energy/environment.\n\nProminent program areas at OSC currently include:\n\nOSC is funded through the Ohio Board of Regents by the state operating and capital budgets of the Ohio General Assembly. OSC offices are located on the West Campus of The Ohio State University in Columbus, Ohio, United States.\n\nOSC was established by the Ohio Board of Regents in 1987 as a statewide resource designated to place Ohio's research universities and private industry in the forefront of computational research. Also in 1987, the OSC networking initiative — known today as OARnet — provided the first network access to the Center’s first Cray supercomputer.\n\nIn 1988, OSC launched the Center’s Industrial Interface Program to serve businesses interested in accessing the supercomputer. Battelle Memorial Institute, located just south of Ohio State, became OSC’s first industrial user. Today, the Center continues to offer HPC services to researcher in industry, primarily through its AweSim industrial engagement program.\n\nIn the summer of 1989, 20 talented high school students attended the first Governor’s Summer Institute. Today, OSC offers summer STEM education programs through Summer Institute and Young Women's summer Institute, which began in 2000.\n\nLater in the fall of 1989, OSC engineers installed a $22 million Cray Y-MP8/864 system, which was deemed the largest and fastest supercomputer in the world for a short time. The seven-ton system was able to calculate 200 times faster than many mainframes at that time.\n\nDirectors of the Center:\n\nProduction systems (Feb. 2014) include:\n\n"}
{"id": "3846227", "url": "https://en.wikipedia.org/wiki?curid=3846227", "title": "PSivida", "text": "PSivida\n\nEyePoint Pharmaceuticals, Inc. (formerly pSivida Corp.) pSivida is a Watertown, Massachusetts company specialising in the application of microelectromechanical systems (MEMS) and nanotechnology to drug delivery.\n\npSivida obtained porous silicon technology from the British government Defence Evaluation and Research Agency (DERA, now QinetiQ). QinetiQ continues to be a strategic partner..In April 2018, pSivida purchased eye products firm Icon Bioscience.\n\n\n"}
{"id": "43696932", "url": "https://en.wikipedia.org/wiki?curid=43696932", "title": "Pacific Rim Uprising", "text": "Pacific Rim Uprising\n\nPacific Rim: Uprising is a 2018 American science fiction film directed by Steven S. DeKnight (in his feature-film directorial debut), and written by DeKnight, Emily Carmichael, Kira Snyder, and T.S. Nowlin. It is the sequel to the 2013 film \"Pacific Rim\", with Guillermo del Toro, the director of the original, serving as a producer. The sequel stars John Boyega (also making his producer debut), as well as Scott Eastwood, Cailee Spaeny, Jing Tian, Adria Arjona, and Zhang Jin, with Rinko Kikuchi, Charlie Day, and Burn Gorman returning from the original film. Set in the year 2035, the plot follows humanity again fighting Kaiju, the giant monsters set on destroying the world.\n\nPrincipal photography began in November 2016 in Australia. The film was released in the United States on March 23, 2018, by Universal Pictures, in 2D, Real D 3D, IMAX 3D, and IMAX, and grossed $290 million worldwide. It received mixed reviews from critics; with some calling the film inferior to del Toro's and criticizing the scope, pacing, story, as well as the absent characters from the previous film and the underdeveloped new characters, while others praised the visual effects and performances of Boyega and Spaeny.\n\nIt is 2035, ten years after the Battle of the Breach, in which the interdimensional portal created by the Precursors (through which they sent marauding Kaiju) was closed. Former Jaeger pilot Jake Pentecost – son of Battle of the Breach hero Stacker Pentecost – makes a living by stealing and selling Jaeger parts on the black market in the Los Angeles area. After he tracks part of a disabled Jaeger's power core to the secret workshop of fifteen year-old Jaeger enthusiast Amara Namani, both are arrested by the Pan-Pacific Defense Corps (PPDC) after an altercation between Amara's small, single-pilot Jaeger \"Scrapper\" and the Police Jaeger \"November Ajax\". Jake's adoptive sister and PPDC Secretary General Mako Mori gives Jake a choice between prison or returning to PPDC as an instructor, with Amara as his recruit.\n\nArriving at the China Shatterdome, Jake starts training Jaeger program cadets with his estranged former co-pilot Nate Lambert. Nate and Mako reveal to him that the Jaeger program is threatened by Shao Corporation's new drone program, which offers to mass-produce Jaeger drones developed by Shao Liwen and Dr. Newton Geiszler. Mako is due to deliver a final assessment to determine the approval of the drones at a PPDC council meeting in Sydney, but is killed by rogue Jaeger \"Obsidian Fury\" before she can report. Her death prompts the PPDC council to immediately authorize and deploy the drones. \n\nMoments before her death, Mako had transmitted the location of a defunct Jaeger production facility in Siberia. Jake and Nate travel there in \"Gipsy Avenger\", but \"Obsidian Fury\" destroys the complex and engages them in battle. Upon destroying its reactor, they find that \"Obsidian Fury\" was piloted by a Kaiju's secondary brain, which testing shows was grown on Earth.\n\nWhen the drones reach their respective locations, their piloting operations are taken over by cloned Kaiju brains secretly mounted onboard. The Kaiju-Jaeger hybrids simultaneously attack all Pacific Rim Shatterdomes, inflicting heavy casualties on PPDC forces and incapacitating almost all Jaegers. Hermann Gottlieb and Geiszler try to disable the drones, but Hermann discovers that Geiszler is behind the attack when he commands the Kaiju-Jaeger hybrids to open multiple breaches across the Pacific Rim. Geiszler‘s mind has been taken over by the Precursors, who forged a link when he originally drifted with a Kaiju brain (first film). Although Shao is able to destroy the drones, closing the breaches, three Kaijus had already emerged and reached Tokyo. The team realizes that the Precursors' true goal is to activate the Ring of Fire by detonating Mount Fuji with the Kaijus' blood, spreading toxic gas into the atmosphere and wiping out all life on Earth while terraforming the planet for Precursor colonization.\n\nThe cadets are mobilized while Gottlieb and Shao repair the PPDC's four salvageable Jaegers; Jake and Nate’s \"Gipsy Avenger\" is joined by \"Guardian Bravo\" (piloted by cadets Suresh and Ilya), \"Saber Athena\" (cadets Renata and Ryoichi) and the three-pilot \"Bracer Phoenix\" (cadets Amara, Viktoria and Jinhai). Gottlieb develops Kaiju-blood-powered rockets, which launch the team to Tokyo. Although the Jaegers initially repel the three Kaiju, Geiszler merges them into a Mega-Kaiju using robotic parasites from one of Shao's factories. The Mega-Kaiju quickly overpowers the Jaegers, injures Nate, kills Suresh, and destroys three of the four Jaegers, leaving \"Gipsy Avenger\" as the only operational Jaeger. Jake and Amara pilot it against the Mega-Kaiju, with Shao remote piloting \"Scrapper\" and aiding them by locating a rocket and welding it to \"Gipsy\", which sends the Jaeger (with \"Scrapper\" holding on) into the atmosphere and free-falling back to Earth, colliding into the Mega-Kaiju and killing it, while Jake and Amara survive by transferring to \"Scrapper\". Geiszler, angered by the Mega-Kaiju's failure, tries to flee, but is subdued by Nate.\n\nIn custody of the PPDC, Geiszler threatens that Precursors will keep sending attacks, but Jake replies that humanity will soon be the ones attacking the Precursors.\n\nIn 2012, prior to the first film's release, del Toro noted that he had ideas for a sequel, noting in 2014 that he had been working on a script with Zak Penn for several months. In June 2014, del Toro stated that he would direct the sequel, and that it would be released by Universal Pictures, Legendary's new financing and distribution partner, on April 7, 2017. In July 2015, it was reported that filming was expected to begin in November, though production was halted following conflicts between Universal and Legendary. As the sequel's future became unclear, Universal indefinitely delayed the film. Still determined to have the film made, del Toro kept working and by that October announced that he had presented the studio with a script and a budget.\n\nAfter the sale of Legendary to Chinese Wanda Group for $3.5 billion, observers noted an increased likelihood of \"Pacific Rim 2\"s production being revitalized because the first film was so successful in China.\n\nIn February 2016, the studio, and del Toro himself via Twitter, announced that Steven S. DeKnight would take over directing duties, with a new script written by Jon Spaihts, marking DeKnight's feature directorial debut. del Toro remained on the project as a producer. Derek Connolly was brought in on May 12, 2016, to rewrite the script again.\n\nCast announcements began in June 2016, with John Boyega accepting a role, and news that Scott Eastwood was in talks appearing later that month. Further announcements took place in September and November. A notable absence from the cast was Charlie Hunnam, who could not join the project because of his scheduling conflicts with \"\".\n\nPrincipal photography on the film began on November 9, 2016, in Australia. On December 14, 2016, the official title was revealed to be \"Pacific Rim Uprising\". In February 2017, three new Jaegers for the film were revealed. On March 8, 2017, filming started in China. The Battle of Tokyo sequence was filmed in Seoul and Busan in South Korea using drones. Filming was completed on March 30, 2017.\n\nComposer John Paesano was originally slated to be writing the score for the film, replacing the first film's composer Ramin Djawadi. However, in January 2018, it was announced that Paesano had been replaced by Lorne Balfe. The soundtrack was digitally released on March 23, 2018 by Milan Records with the physical format being released later on April 6, 2018.\n\nThe visual effects are done by DNEG (Double Negative), Atomic Fiction, Blind LTD and Territory Studio, with Peter Chiang and Jim Berney serving as visual effects supervisors. Production designer Stefan Dechant tried to push \"the look and the feel\" of the Jaegers, stating that while in the original film they resembled tanks, the new generation of robots tried to be more like a fighter jet. The robots became more acrobatic and had silhouettes and color schemes that allowed them to become distinct and recognizable. Artists from Industrial Light & Magic, who made the effects in the first film, helped develop Gipsy Avenger, a \"sleek and advanced\" upgraded version of Gipsy Danger which DeKnight often compared to the \"USS Enterprise\" in being \"awe-inspiring\" given its status as the Jaeger flagship.\n\nLegendary Comics released \"Pacific Rim: Aftermath\" on January 17. 2018. The six-issue comic book series serves as a bridge between the two films.\n\n\"Pacific Rim Uprising\" was released on March 23, 2018 in the United States, in 3D, IMAX, and IMAX 3D, by Universal Pictures. Originally scheduled for release on April 7, 2017, the date was postponed multiple times. The film was pushed back to August 4, 2017, then to February 23, 2018, and one final time to March 23.\n\n\"Pacific Rim Uprising\" was released on Digital on June 5, 2018. A 4K Blu-ray, a Blu-ray 3D, a Blu-ray and a DVD was released on June 19, 2018.\n\n\"Pacific Rim Uprising\" grossed $59.6 million in the United States and Canada, and $230.9 million in other territories, for a worldwide total of $290.5 million to a production budget between $150–176 million.\n\nIn the United States and Canada, \"Pacific Rim Uprising\" was released alongside \"Midnight Sun\", \"Sherlock Gnomes\", \"Unsane\", and \"Paul, Apostle of Christ\", and was projected to gross $22–29million from 3,703 theaters in its opening weekend. The film made $2.35million from Thursday night previews, down from the original's $3.5million, and $10.4 million on its first day (including previews). It went on to debut to $28 million, becoming the first film to dethrone \"Black Panther\" (which made $16.7 million in its sixth week) for the top spot. It fell 67% to $9.2 million in its second weekend, finishing 5th.\n\nIn Korea, the film ranked first on March 22, with 82,486 admissions. In China, the film opened at number one, grossing $21.36million on its first day and $25.84million on its second, for a two-day gross of $48.59million. It went on to have a debut of $65 million in the country, as well as $6.9 million in Korea, $6.8 million in Russia and $4.9 million in Mexico, for an international opening weekend of $122.5 million.\n\nOn review aggregator website Rotten Tomatoes, the film holds an approval rating of 43% based on 211 reviews, and an average rating of 5/10. The website's critical consensus reads, \"\"Pacific Rim Uprising\" won't win any points for subtlety or originality, but it delivers enough of the rock 'em-sock 'em robots-vs.-kaiju thrills that fans of the original will be looking for.\" On Metacritic, the film has a weighted average score of 44 out of 100, based on 44 critics, indicating \"mixed or average reviews\". Audiences polled by CinemaScore gave the film an average grade of \"B\" on an A+ to F scale, down from the first film's \"A–\", while PostTrak reported filmgoers gave it an overall positive score of 78%.\n\nMark Kennedy of \"Associated Press\" called the film \"cheer-at-the-screen fun\" and awarded it 3.5 out of 4 stars, lauding Boyega's performance and his chemistry with Spaeny, while also commending DeKnight for using daylight instead of the rainy night settings of del Toro. Mel Evans of \"Metro\" gave the film 4 out of 5 stars, calling it \"very loud, mighty fun, but not much more,\" while also applauding Boyega for his performance and noting his chemistry with Eastwood. Ethan Sacks of \"New York Daily News\" gave the film 3 out of 5 stars, and was also positive of Boyega's and Spaeny's performances, comparing Boyega's character to Han Solo. However, he criticized the dense backstories of the characters, noting that, \"a movie about massive monster-fighting robots doesn't need so much engineering.\"\n\nRichard Roeper of the \"Chicago Sun-Times\" gave the film 2 out of 4 stars, saying: \"The climactic battle drags on forever and looks like a high-tech update of a monster movie clash of the titans from a half-century ago. Even the sight of the residents of Tokyo scrambling for their lives as a giant lizard monster stomps through the city serves only as a reminder we're sitting through a glorified B-movie with nothing new to say.\" David Ehrlich of IndieWire gave the film a \"C–\", calling it a \"generic and diverting sequel that corrects some of the original's biggest mistakes while also highlighting some of its more eccentric charms.\"\n\nCary Darling of the \"Houston Chronicle\" gave it 1.5 out of 5 stars and called it the worst film of 2018 so far, being especially critical of its bland premise and pandering attempt at international marketability. Darling concluded, \"\"Pacific Rim Uprising\" is a lot like the city-crunching monsters it stars: big, loud and as dull-witted as Homer Simpson roused from a medically induced coma. It's a rote, paint-by-numbers blockbuster that would be offensive in its mediocrity if it also weren't so relentlessly uninspired,\" and \"all that's left is the robot brawling and the marketing.\" Ignatiy Vishnevetsky of \"The A.V. Club\" called the film an \"impersonal sequel,\" stating \"simply put, it lacks its predecessor's curiosity about its world—its fascination with colorful backdrops and machines. Del Toro's movie [...] had an idealistic vision for its anime-influenced hobby-store pursuits [...] \"Pacific Rim: Uprising\" offers only its spare parts.\" Similarly, Mick LaSalle of the \"San Francisco Chronicle\" noted that \"DeKnight doesn't attempt to invest his monsters with majesty, the way Guillermo del Toro did in the previous film. With DeKnight it's just a lot of pounding, smashing and driving, purely functional.\"\n\nIn October 2017, five months before the film’s release, DeKnight stated \"If enough people show up to this, we've already talked about the plot of the third movie, and how the end of the third movie would expand the universe to a \"Star Wars\"/\"Star Trek\"-style [franchise or series] where you can go in many, many different directions... You can go main canon, you can go spin-offs, you can go one-offs. Yeah, that's the plan.\" DeKnight also talked about the possibility of a crossover with the MonsterVerse, as co-writer T.S. Nowlin is a member of its writers room.On November 8, 2018, Netflix announced an original anime series [14][15] that will expand upon the story of the first two live action movies.\n\n"}
{"id": "7659331", "url": "https://en.wikipedia.org/wiki?curid=7659331", "title": "Papyrus sanitary pad", "text": "Papyrus sanitary pad\n\nA papyrus sanitary pad, or Makapad, is a sanitary napkin made from papyrus, a natural material. It is reported to be 75 percent cheaper than a conventional pad and thus an advantage to the poor, as well as being highly absorbent.\n\nThe pads, developed by Dr. Moses Kizza Musaazi at Uganda's Makerere University Department of Technology, are targeted especially at rural primary school girls who have started menstruating. These girls would normally find it difficult to attend school if they had no pads, and often cannot afford conventional pads. These sanitary pads also provide employment opportunities for the women of the rural communities in Uganda. They are made entirely by hand, and using local materials.\n\nThis was part of the UN Refugee Agency (UNHCR) program to provide locally made Makapads to women and girls throughout refugee camps in Uganda. \n\nThe pads are made out of dry papyrus and mixed with waste paper and then stitched with non-woven absorbent piece of cloth. They are made almost entirely from natural materials, with a few percent of non-woven material and polythene paper. These pads are biodegradable and environment-friendly. \n\n"}
{"id": "24309243", "url": "https://en.wikipedia.org/wiki?curid=24309243", "title": "Plascore Incorporated", "text": "Plascore Incorporated\n\nPlascore Incorporated manufactures honeycomb core, cleanrooms, and composite panels marketed under the brand Plascore. Honeycomb is used in aerospace, marine, military, safety, transportation, and other applications. When honeycomb is sandwiched between two surfaces, it effectively creates a distance between the two surfaces more or less like an I-beam. The resulting composite structure exhibits a high strength-to-weight ratio and shear strength. Shear strength is the measured ability of a material to resist structural failure. Plascore honeycomb is designed to increase shear strength while adding minimal additional weight. Plascore is a global organization, with a 185,000 sq. ft. headquarters and three additional manufacturing facilities in Zeeland, Michigan. Those three facilities are 50,000, 40,000 and 80,000 sq. ft. in size. The company also has an 85,000 sq. ft. manufacturing plant in Waldlaubersheim, Germany; and sales offices throughout the world. Plascore is AS/EN/JISQ9100, ISO 14001:2004, and ISO 9001:2008 Certified, and the company's PP Honeycomb has received a Lloyds Register Certificate of Approval of a Core Material. Plascore is also AS9100 certified, an industry standard required by the majority of major aircraft manufacturers that provides verification of consistent aerospace product quality.\n\nPlascore was founded in 1977 as a manufacturer of honeycomb core sold to various value added manufacturers. By the 1980s, Plascore’s product development and manufacturing capabilities led the company into value-added markets, including cleanroom walls, ceiling and door systems for semi-conductor and pharmaceutical sectors; panels for transportation products and building materials and energy absorbers for various markets.\n\nIn 2002, Plascore received an award from CERN for its role in the design and manufacture of large honeycomb panels used in detection chambers.\n\nIn August 2009, Plascore received tax abatements from the city of Zeeland for new equipment and machinery installed at its facilities in Zeeland.\n\nWith the increase in fuel and the growing need for lighter structure products, honeycomb gained increasing popularity in the marine industry, where it allows boat designers to add strength to hull manufacturing and also save weight, for example by using thin layers of stone laminated to honeycomb for luxury yacht countertops rather than solid stone.\n\nIn 2012, Plascore constructed a new facility for increased Nomex manufacturing capabilities.\n\n"}
{"id": "24499728", "url": "https://en.wikipedia.org/wiki?curid=24499728", "title": "Psychological manipulation", "text": "Psychological manipulation\n\nPsychological manipulation is a type of social influence that aims to change the behavior or perception of others through abusive, deceptive, or underhanded tactics. By advancing the interests of the manipulator, often at another's expense, such methods could be considered exploitative, abusive, devious, and deceptive. \n\nSocial influence is not necessarily negative. For example, people such as friends, family and doctors, can try to persuade to change clearly unhelpful habits and behaviors. Social influence is generally perceived to be harmless when it respects the right of the influenced to accept or reject it, and is not unduly coercive. Depending on the context and motivations, social influence may constitute underhanded manipulation.\n\nAccording to psychology author George K. Simon, successful psychological manipulation primarily involves the manipulator:\n\nConsequently, the manipulation is likely to be accomplished through covert aggressive (relational aggressive or passive aggressive) means.\n\nHarriet B. Braiker (2004) identified the following ways that manipulators control their victims:\n\nSimon identified the following manipulative techniques:\n\n\nAccording to Braiker's self-help book, manipulators exploit the following vulnerabilities (buttons) that may exist in victims:\n\nAccording to Simon, manipulators exploit the following vulnerabilities that may exist in victims:\n\nManipulators generally take the time to scope out the characteristics and vulnerabilities of their victims.\n\nKantor advises in his book \"The Psychopathology of Everyday Life: How to Deal with Manipulative People\" that vulnerability to psychopathic manipulators involves being too:\n\nManipulators can have various possible motivations, including but not limited to:\n\nBeing manipulative is in Factor 1 of the Hare Psychopathy Checklist (PCL).\n\nThe workplace psychopath may often rapidly shift between emotions – used to manipulate people or cause high anxiety.\n\nThe authors of the book \"\" describe a five phase model of how a typical workplace psychopath climbs to and maintains power. In phase three (manipulation) - the psychopath will create a scenario of \"psychopathic fiction\" where positive information about themselves and negative disinformation about others will be created, where your role as a part of a network of pawns or patrons will be utilised and you will be groomed into accepting the psychopath's agenda.\n\nCorporate jargon Variously known as corporate speak, corporate lingo, business speak, business jargon, management speak, workplace jargon, or commercialese, is the jargon often used in large corporations, bureaucracies, and similar workplaces.[1][2] The use of corporate jargon, also known as \"corporatese\", is criticised for its lack of clarity as well as for its tedium, making meaning and intention opaque and understanding difficult.\nSophism In modern useage sophist and sophistry are redefined and used disparagingly. A sophism is a specious argument for displaying ingenuity in reasoning or for deceiving someone. A sophist is a person who reasons with clever but fallacious and deceptive arguments. \n\nImpression Management (unethical). In business, \"managing impressions\" normally \"involves someone trying to control the image that a significant stakeholder has of them\". The ethics of impression management has been hotly debated on whether we should see it as an effective self-revelation or as cynical manipulation.\n\nAccording to Kernberg, antisocial, borderline, and narcissistic personality disorders are all organized at a borderline level of personality organization, and the three share some common characterological deficits and overlapping personality traits, with deceitfulness and exceptional manipulative abilities being the most common traits among antisocial and narcissism. Borderline is emphasized by unintentional and dysfunctional manipulation, but stigma towards borderlines being deceitful still wrongfully persists. Antisocials, borderlines, and narcissists are often pathological liars. Other shared traits may include pathological narcissism, consistent irresponsibility, machiavellianism, lack of empathy, cruelty, meanness, impulsivity, proneness to self-harm and addictions, interpersonal exploitation, hostility, anger and rage, vanity, emotional instability, rejection sensitivity, perfectionism, and the use of primitive defence mechanisms that are pathological and narcissistic. Common narcissistic defences include splitting, denial, projection, projective identification, primitive idealization and devaluation, distortion (including exaggeration, minimization and lies), and omnipotence.\n\nPsychologist Marsha M. Linehan has stated that people with borderline personality disorder often exhibit behaviors which are not truly manipulative, but are erroneously interpreted as such. According to her, these behaviors often appear as unthinking manifestations of intense pain, and are often not deliberate as to be considered truly manipulative. In the DSM-V, manipulation was removed as a defining characteristic of borderline personality disorder.\n\nManipulative behavior is intrinsic to narcissists, who use manipulation to obtain power and narcissistic supply. Those with antisocial personalities will manipulate for material items, power, revenge, and a wide variety of other reasons.\n\nPeople with histrionic personality disorder are usually high-functioning, both socially and professionally. They usually have good social skills, despite tending to use them to manipulate others into making them the center of attention.\n\nMachiavellianism is a term that some social and personality psychologists use to describe a person's tendency to be unemotional, and therefore able to detach themselves from conventional morality and hence to deceive and manipulate others. In the 1960s, Richard Christie and Florence L. Geis developed a test for measuring a person's level of Machiavellianism (sometimes referred to as the \"Machiavelli test\").\n\n\n\n"}
{"id": "11881862", "url": "https://en.wikipedia.org/wiki?curid=11881862", "title": "RADiations Effects on Components and Systems", "text": "RADiations Effects on Components and Systems\n\nThe RADECS association is a non-profit professional organization that promotes basic and applied research in the field of radiation and its effects on materials, components and systems. The acronym RADECS stands for \"RADiations Effects on Components and Systems\".\n\nIts activities are as follows:\n\nThe first “Radiation and its Effects on Components and Systems\" (RADECS) conference was held in Montpellier, France in 1989 as a French national conference. In 1991, the members of the organizing committee expanded the scope of RADECS to become a European conference. Since then, the RADECS Conference and RADECS Workshop have run in alternate years.\n\nThe RADECS conference and workshops address technical issues related to radiation effects on devices, integrated circuits, sensors, and systems, as well as radiation hardening, testing, and environmental modeling methods. Papers from the events are published in a biennial issue of the IEEE Transactions on Nuclear Science journal.\n\n"}
{"id": "1452173", "url": "https://en.wikipedia.org/wiki?curid=1452173", "title": "RapidIO", "text": "RapidIO\n\nThe RapidIO architecture is a high-performance packet-switched, \ninterconnect technology. RapidIO supports messaging, read/write and cache coherency semantics. RapidIO fabrics guarantee in-order packet delivery, enabling power- and area- efficient protocol\nimplementation in hardware. Based on industry-standard electrical specifications such as those for Ethernet, \nRapidIO can be used as a chip-to-chip, board-to-board, and chassis-to-chassis interconnect. The protocol is marketed as: \"RapidIO - the unified fabric for Performance Critical Computing\", and is used in many applications such as Data Center & HPC, Communications Infrastructure, Industrial Automation and Military & Aerospace that are constrained by at least one of size, weight, and power (SWaP).\n\nRapidIO has its roots in energy-efficient, high-performance computing.\nThe protocol was originally designed by Mercury Computer Systems and Motorola (Freescale) as a replacement for Mercury's RACEway proprietary bus and Freescale's PowerPC bus. The RapidIO Trade Association was formed in February 2000, and included telecommunications and storage OEMs as well as FPGA, processor, and switch companies. \nThe protocol was designed to meet the following objectives:\n\nThe RapidIO Specification Revision 1.1, released in 2001, defined a wide, parallel bus. This specification did not achieve extensive commercial adoption.\n\nThe RapidIO Specification Revision 1.2, released in 2002, defined a serial interconnect based on the XAUI physical layer. Devices based on this specification achieved significant commercial success within wireless baseband, imaging and military compute.\nThe RapidIO Specification Revision 2.0, released in 2008, added more port widths (2×, 8×, and 16×) and increased the maximum lane speed to 6.25 GBd / 5 Gbit/s. Revision 2.1 has repeated and expanded the commercial success of the 1.2 specification.\n\nThe RapidIO Specification Revision 3.0, released in 2013, has the following changes and improvements compared to the 2.x specifications:\n\nThe RapidIO Specification Revision 4.0 was released in 2016. has the following changes and improvements compared to the 3.x specifications:\n\nRapidIO fabrics enjoy dominant market share in global deployment of cellular infrastructure 3G, 4G & LTE networks with millions of RapidIO ports shipped into wireless base stations worldwide. RapidIO fabrics were originally designed to support connecting different types of processors from different manufacturers together in a single system. This flexibility has driven the widespread use of RapidIO in wireless infrastructure equipment where there is a need to combine heterogeneous, DSP, FPGA and communication processors together in a tightly coupled system with low latency and high reliability.\n\nData Center and HPC Analytics systems have been deployed using a RapidIO 2D Torus Mesh Fabric, that provides a high speed general purpose interface among the system cartridges for applications that benefit from high bandwidth, low latency node-to-node communication. The RapidIO 2D Torus unified fabric is routed as a torus ring configuration connecting up to 45 server cartridges capable of providing 5Gbs per lane connections in each direction to its north, south, east and west neighbors. This allows the system to meet many unique HPC applications where efficient localized traffic is needed.\n\nAlso, using an open modular data center and compute platform, a heterogeneous HPC system has showcased the low latency attribute of RapidIO to enable real-time analytics. In March 2015 a top-of-rack switch was announced to drive RapidIO into mainstream data center applications.\n\nThe interconnect or \"bus\" is one of the critical technologies in the design and development of spacecraft avionic systems that dictates its architecture and level of complexity. There are a host of existing architectures that are still in use given their level of maturity. These existing systems are sufficient for a given type of architecture need and requirement. Unfortunately, for next generation missions a more capable avionics architecture is desired; which is well beyond the capabilities levied by existing architectures. A viable option toward the design and development of these next generation architectures is to leverage existing commercial protocols capable of accommodating high levels of data transfer.\n\nIn 2012, RapidIO was selected by the Next Generation Spacecraft Interconnect Standard (NGSIS) working group to serve as the foundation for standard communication interconnects to be used in spacecraft. The NGSIS is an umbrella standards effort that includes RapidIO Version 3.1 development, and a box level hardware standards effort under VITA 78 called SpaceVPX or High ReliabilityVPX. The NGSIS requirements committee developed extensive requirements criteria with 47 different elements for the NGSIS interconnect. Independent trade study results by NGSIS member companies demonstrated the superiority of RapidIO over other existing commercial protocols, such as InfiniBand, Fibre Channel, and 10G Ethernet. As a result, the group decided that RapidIO offered the best overall interconnect for the needs of next-generation spacecraft.\n\nThe RapidIO Specification Revision 3.1, released in 2014, was developed through a collaboration between the RapidIO Trade Association and NGSIS. Revision 3.1 has the following enhancements compared to the 3.0 specification:\n\nThe RapidIO Specification Revision 4.0 was released in 2016. has the following changes and improvements compared to the 3.x specifications:\n\nThe RapidIO roadmap aligns with Ethernet PHY development. RapidIO specifications for 50 GBd and higher links are under investigation.\n\n\nThe RapidIO protocol is defined in a 3-layered specification:\n\nSystem specifications include:\n\nThe RapidIO electrical specifications are based on industry-standard Ethernet and Optical Interconnect Forum standards:\n\nThe RapidIO PCS/PMA layer supports two forms of encoding/framing:\n\nEvery RapidIO processing element transmits and receives three kinds of information: Packets, control symbols, and an idle sequence.\n\nEvery packet has two values that control the physical layer exchange of that packet. The first is an acknowledge ID (ackID), which is the link-specific, unique, 5-, 6-, or 12-bit value that is used to track packets exchanged on a link. Packets are transmitted with serially increasing ackID values. Because the ackID is specific to a link, the ackID is not covered by CRC, but by protocol. This allows the ackID to change with each link it passes over, while the packet CRC can remain a constant end-to-end integrity check of the packet. When a packet is successfully received, it is acknowledged using the ackID of the packet. A transmitter must retain a packet until it has been successfully acknowledged by the link partner.\n\nThe second value is the packet's physical priority. The physical priority is composed of the Virtual Channel (VC) identifier bit, the Priority bits, and the Critical Request Flow (CRF) bit. The VC bit determines if the Priority and CRF bits identify a Virtual Channel from 1 to 8, or are used as the priority within Virtual Channel 0. Virtual Channels are assigned guaranteed minimum bandwidths. Within Virtual Channel 0, packets of higher priority can pass packets of lower priority. Response packets must have a physical priority higher than requests in order to avoid deadlock.\n\nThe physical layer contribution to RapidIO packets is a 2-byte header at the beginning of each packet that includes the ackID and physical priority, and a final 2-byte CRC value to check the integrity of the packet. Packets larger than 80 bytes also have an intermediate CRC after the first 80 bytes. With one exception a packet's CRC value(s) acts as an end-to-end integrity check.\n\nRapidIO control symbols can be sent at any time, including within a packet. This gives RapidIO the lowest possible in-band control path latency, enabling the protocol to achieve high throughput with smaller buffers than other protocols.\n\nControl symbols are used to delimit packets (Start of Packet, End of Packet, Stomp), to acknowledge packets (Packet Acknowledge, Packet Not Acknowledged), reset (Reset Device, Reset Port) and to distribute events within the RapidIO system (Multicast Event Control Symbol). Control symbols are also used for flow control (Retry, Buffer Status, Virtual Output Queue Backpressure) and for error recovery.\n\nThe error recovery procedure is very fast. When a receiver detects a transmission error in the received data stream, the receiver causes its associated transmitter to send a Packet Not Accepted control symbol. When the link partner receives a Packet Not Accepted control symbol, it stops transmitting new packets and sends a Link Request/Port Status control symbol. The Link Response control symbol indicates the ackID that should be used for the next packet transmitted. Packet transmission then resumes.\n\nThe IDLE sequence is used during link initialization for signal quality optimization. It is also transmitted when the link does not have any control symbols or packets to send.\n\nEvery RapidIO endpoint is uniquely identified by a Device Identifier (deviceID). Each RapidIO packet contains two device IDs. The first is the destination ID (destID), which indicates where the packet should be routed. The second is the source ID (srcID), which indicates where the packet originated. When an endpoint receives a RapidIO request packet that requires a response, the response packet is composed by swapping the srcID and destID of the request.\n\nRapidIO switches use the destID of received packets to determine the output port or ports that should forward the packet. Typically, the destID is used to index into an array of control values. The indexing operation is fast and low cost to implement. RapidIO switches support a standard programming model for the routing table, which simplifies system control.\n\nThe RapidIO transport layer supports any network topology, from simple trees and meshes to n-dimensional hypercubes, multi-dimensional toroids, and more esoteric architectures such as entangled networks.\n\nThe RapidIO transport layer enables hardware virtualization (for example, a RapidIO endpoint can support multiple device IDs). Portions of the destination ID of each packet can be used to identify specific pieces of virtual hardware within the endpoint.\n\nThe RapidIO logical layer is composed of several specifications, each providing packet formats and protocols for different transaction semantics.\n\nThe logical I/O layer defines packet formats for read, write, write-with-response, and various atomic transactions. Examples of atomic transactions are set, clear, increment, decrement, swap, test-and-swap, and compare-and-swap.\n\nThe Messaging specification defines Doorbells and Messages. Doorbells communicate a 16-bit event code. Messages transfer up to 4KiB of data, segmented into up to 16 packets each with a maximum payload of 256 bytes. Response packets must be sent for each Doorbell and Message request. The response packet status value indicates done, error, or retry. A status of retry requests the originator of the request to send the packet again. The logical level retry response allows multiple senders to access a small number of shared reception resources, leading to high throughput with low power.\n\nThe Flow Control specification defines packet formats and protocols for simple XON/XOFF flow control operations. Flow control packets can be originated by switches and endpoints. Reception of a XOFF flow control packet halts transmission of a flow or flows until an XON flow control packet is received or a timeout occurs. Flow Control packets can also be used as a generic mechanism for managing system resources.\n\nThe Globally Shared Memory specification defines packet formats and protocols for operating a cache coherent shared memory system over a RapidIO network.\n\nThe Data Streaming specification supports messaging with different packet formats and semantics than the Messaging specification. Data Streaming packet formats support the transfer of up to 64K of data, segmented over multiple packets. Each transfer is associated with a Class of Service and Stream Identifier, enabling thousands of unique flows between endpoints.\n\nThe Data Streaming specification also defines Extended Header flow control packet formats and semantics to manage performance within a client-server system. Each client uses extended header flow control packets to inform the server of the amount of work that could be sent to the server. The server responds with extended header flow control packets that use XON/XOFF, rate, or credit based protocols to control how quickly and how much work the client sends to the server.\n\nSystems with a known topology can be initialized in a system specific manner without affecting interoperability. The RapidIO system initialization specification supports system initialization when system topology is unknown or dynamic. System initialization algorithms support the presence of redundant hosts, so system initialization need not have a single point of failure.\n\nEach system host recursively enumerates the RapidIO fabric, seizing ownership of devices, allocating device IDs to endpoints and updating switch routing tables. When a conflict for ownership occurs, the system host with the larger deviceID wins. The \"losing\" host releases ownership of its devices and retreats, waiting for the \"winning\" host. The winning host completes enumeration, including seizing ownership of the losing host. Once enumeration is complete, the winning host releases ownership of the losing host. The losing host then discovers the system by reading the switch routing tables and registers on each endpoint to learn the system configuration. If the winning host does not complete enumeration in a known time period, the losing host determines that the winning host has failed and completes enumeration.\n\nSystem enumeration is supported in Linux by the RapidIO subsystem.\n\nRapidIO supports high availability, fault tolerant system design, including hot swap. The error conditions that require detection, and standard registers to communicate status and error information, are defined. A configurable isolation mechanism is also defined so that when it is not possible to exchange packets on a link, packets can be discarded to avoid congestion and enable diagnosis and recovery activities. In-band (port-write packet) and out-of-band (interrupt) notification mechanisms are defined.\n\nThe RapidIO specification does not discuss the subjects of form factors and connectors, leaving this to specific application-focussed communities. RapidIO is supported by the following form factors:\n\n\nProcessor-agnostic RapidIO support is found in the Linux kernel.\n\nThe RapidIO interconnect is used extensively in the following applications:\n\nRapidIO is expanding into supercomputing, server, and storage applications.\n\nPCI Express is targeted at the host to peripheral market, as opposed to embedded systems. Unlike RapidIO, PCIe is not optimized for peer-to-peer multi processor networks. PCIe is ideal for host to peripheral communication. PCIe does not scale as well in large multiprocessor peer-to-peer systems, as the basic PCIe assumption of a \"root complex\" creates fault tolerance and system management issues.\n\nAnother alternative interconnect technology is Ethernet. Ethernet is a robust approach to linking computers over large geographic areas, where network topology may change unexpectedly, the protocols used are in flux, and link latencies are large. To meet these challenges, systems based on Ethernet require significant amounts of processing power, software and memory throughout the network to implement protocols for flow control, data transfer, and packet routing. RapidIO is optimized for energy efficient, low latency, processor-to-processor communication in fault tolerant embedded systems that span geographic areas of less than one kilometre.\n\nSpaceFibre is a competing technology for space applications.\n\nTime-Triggered Ethernet is a competing technology for more complex backplane (VPX) and backbone applications for space (launchers and human-rated integrated avionics).\n\n\n"}
{"id": "10538907", "url": "https://en.wikipedia.org/wiki?curid=10538907", "title": "SITAOnAir", "text": "SITAOnAir\n\nSITAOnAir is a company that enables airline passengers to use their mobile phones and laptops for calls, text messaging, emails and Internet browsing. By May 2014, SITAOnAir’s services were available in over 60 countries and used by 22 airlines flying over five continents.\n\nThe company is a fully owned subsidiary of SITA, originally incorporated as OnAir as a joint venture with Airbus in February 2005. In February 2013, Airbus sold its 33% final stake to SITA. The company is headquartered in Geneva, Switzerland, and has operations in Seattle and sales offices in London, Singapore and Dubai.\n\nSITAOnAir offers many services which aircraft operators can use together or separately:\n\nAll three services share the same satellite connection to the ground. SITAOnAir (then OnAir) was the first company to provide integrated GSM and inflight wifi services, with Oman Air as the launch airline in March 2010. SITAOnAir’s technology has been certified for use on many types of aircraft – both private and commercial jets including Boeing and Airbus – for short and long haul. In most cases, it is available for linefit or retrofit.\n\nSITAOnAir's onboard system is based on a Wi-Fi access point and/or a GSM picocell connected to the ground through a satellite link. All onboard equipment, except for the satellite antenna, fits into a hand-luggage compartment.\n\nSITAOnAir offers the satellite connectivity required for its onboard services using Inmarsat's SwiftBroadband service operating in the L band and prospectively Inmarsat's K band-based Global Xpress system which is expected to be launched in 2015. However customers are free to use third-party services for the backhaul to the ground. 90% of the onboard equipment can be used with any radio link, for example other satellite networks or a direct air-to-ground link.\n\nDue to the limited bandwidth of only 864 kbit/s offered by SITAOnAir today and its inability to feed in live television in February 2013 Emirates, one of SITAOnAir's largest customers, has selected a K band-based service provided by SITAOnAir's rival Panasonic Avionics (\"Global Communications Suite\" (GCS)) for its fleet of Boeing 777s and future Airbus A380 deliveries, while existing Airbus A380s for now remain connected through SITAOnAir's service.\n\nA satellite data unit (SDU) manufactured by Thales and branded \"TopConnect\" establishes a backhaul link to the ground through Inmarsat's SwiftBroadband geostationary satellite constellation operating in the L band around 1500 MHz which allows the use of electronically steerable antennas mounted atop the aircraft fuselage and encased within a fiberglass, RF-transparent radome\nthat have a low profile compared to systems operating in the K band or K band which today still require mechanically steerable antennas with a significantly higher profile. Thus drag and fuel costs are reduced allowing economical operation even on smaller aircraft like business or regional jets.\nInmarsat's SwiftBroadband system covers much of the planet except for the polar regions above −82 and below +82 degrees latitude and currently provides symmetric data rates of up to 432 kbit/s per channel dependent on signal quality and overall load on the satellite's spotbeam serving the corresponding geographical area. Currently the Thales SDU can bond two channels resulting in a maximum bandwidth of 864 kbit/s.\n\nThere is also a clear upgrade path from SwiftBroadband to Inmarsat's Global Xpress system, a constellation of three K band satellites which will come on stream in 2014-2015 and will globally provide downlink bandwidths of up to 50 Mbit/s.\nSITAOnAir was appointed as distribution partner for Inmarsat's Global Xpress service in November 2011.\n\nA server installed onboard manages the satellite connection and routes the data traffic while also compressing and decompressing all data transmitted, including GSM phone calls that are recoded using the AMR codec which reduces bandwidth while maintaining a voice quality virtually indistinguishable from the native GSM codec.\n\nBroadband Internet access (Internet OnAir) is provided by Wi-Fi access points. In order to access SITAOnAir's Wi-Fi Internet service (Internet SITAOnAir) passengers need to bring a Wi-Fi compatible device. Access can usually be purchased on board.\n\nA picocell operating according to the GSM-1800 standard provides a GSM network (Mobile OnAir) enabling voice telephony, SMS and narrowband Internet access (GPRS). The GSM signal is distributed by a leaky line antenna, essentially a coaxial cable with a slotted shielding through which RF signals are radiated. This coaxial cable is installed above the ceiling panels along the whole aircraft cabin and provides a uniform linear coverage of the aircraft cabin at very low radiation power levels. In order to prevent handsets from connecting to terrestrial networks which would lead to high transmission power levels a so-called \"network control unit\" (NCU) installed onboard generates a broadband noise floor which is being emitted through the existing leaky line antenna masking reception of terrestrial mobile networks within the aircraft. These measures ensure that handsets can only connect to the onboard GSM network and will then operate with the lowest possible transmission power level (GSM-1800 power control level 15, nominal output power of 0 dBm) which results in significantly lower radiation levels than those experienced on average when using a mobile phone with terrestrial networks on the ground.\nThe GSM network is being realized based on Monaco Telecom's core network. It uses the MCC / MNC tuple 901-15 assigned to SITA, one of SITAOnAir's two owners, in March 2005.\n\nSITAOnAir's inflight cellphone service (Mobile OnAir) requires a mobile phone supporting the GSM-1800 standard, also called DCS (Digital Cellular Service), which most modern GSM phones support as well as a SIM card from a network operator having a roaming agreement with Monaco Telcom. So-called quad-band handsets always support GSM-1800 and so are compatible with Mobile OnAir.\n\nThe system can also provide IP-based connectivity to existing in-flight entertainment systems which allows news content to be fed in and messaging services as well as Internet access to be offered on in-seat units.\n\nSITAOnAir's customers which have been publicly announced are:\n\nAirlines:\n\nCruise ship operators:\n\nPrivate jet operators and VIP:\n\n\n"}
{"id": "15105257", "url": "https://en.wikipedia.org/wiki?curid=15105257", "title": "Sex pillow", "text": "Sex pillow\n\nA sex pillow is a specially-designed and typically firm pillow used to enhance sexual intercourse. An ordinary firm pillow, however, may be used in place of a special one. Some contain a high-density urethane core to balance firm support with softness. In addition to more common pillow shapes, there are wedge-shaped, ramp-shaped, prism-shaped, etc. pillows which lend themselves to various sexual positions, some of which might be difficult or uncomfortable without them.\n\nA pillow can be used to provide support when one partner has back pain. A very common use of a sex pillow is to place it under the woman's buttocks or hips before assuming the missionary position; this pivots the pelvis and increases depth of penetration.\n"}
{"id": "884087", "url": "https://en.wikipedia.org/wiki?curid=884087", "title": "Space Telescope Imaging Spectrograph", "text": "Space Telescope Imaging Spectrograph\n\nThe Space Telescope Imaging Spectrograph (STIS) is a spectrograph, also with a camera mode, installed on the Hubble Space Telescope. Aerospace engineer Bruce Woodgate of the Goddard Space Flight Center was the principal investigator and creator of the STIS. It operated continuously from 1997 until a power supply failure in August 2004. After repairs, it began operating again in 2009. The spectrograph has made many important observations, including the first spectrum of the atmosphere of an extrasolar planet, HD 209458b.\n\nThe STIS was installed on Hubble in 1997 during its second servicing mission (STS-82) by Mark Lee and Steven Smith, replacing the High Resolution Spectrograph and the Faint Object Spectrograph. It was designed to operate for five years. On August 3, 2004 an electronic failure rendered STIS inoperable, ending its use 2 years beyond its predicted lifespan. In order to bring it back to operational status, the instrument was repaired by space shuttle astronauts during STS-125, Servicing Mission 4, launched on May 11, 2009. The crew did a long (many hour) EVA to repair the instrument.\n\nThe STIS has three 1024×1024 detector arrays. The first is a charge-coupled device with a 52×52 arc-second field of view, covering the visible and near-infrared spectrum from 200 nm to 1030 nm. The other two detectors are Multi-Anode Multichannel Arrays, each with a 25×25 arc-second field of view. One is CsTe, and covers the near-UV between 160 nm and 310 nm. The other is CsI and covers the far-UV between 115 nm and 170 nm. STIS is both a spectrograph and an imaging camera, and is focused on ultraviolet light.\n\n\nOn its 20th anniversary (1997-2017) NASA noted a selection of discoveries and/or observations conducted with STIS:\n\n\n"}
{"id": "50629979", "url": "https://en.wikipedia.org/wiki?curid=50629979", "title": "Trapiche", "text": "Trapiche\n\nA trapiche is a mill made of wooden rollers used to extract juice from fruit, originally olives, and since the Middle Ages, sugar cane as well. By extension the word is also sometimes applied to the location of the mill, whether the workshop or the entire plantation.\n\nThe word has its origin in the Latin \"trapetum\" that means oil mill. From the Sicilian language \"trapetto\" the term, crossing the Mozarab Valencia, with its typical change of termination to «-ig» via the Catalan language (\"trapig\" -Gandía, 1536-, \"trapitz de canyamel\" -Mallorca, 1466-) has arrived to the other languages of the Iberian peninsula as \"trapiche.\" In the documents of the Duke of Gandía from the beginning of the fifteen century, one can see the term «trapig de canyamel», as a synecdoche to indicate the whole village engenho. According to Herrera: \"..es de notar que antiguamente no auuia azucar,ſino en Valencia\" (\"note that in the old days there was no sugar except in Valencia\").\n\nIn the late 15th century, the horizontal two-roller engenho or trapiche transferred seamlessly from the Portuguese in the Madeira Islands to the Canary Islands just as the Castilians (not yet known as Spanish), still struggled to control the Guanches, the rebellious indigenous Canarians. They were, in fact, the first coerced workers of the fledgling sugar industry on these islands. As the Iberians colonized the archipelagos off the coast of West Africa they relocated here most of the Mediterranean agricultural industry making of these islands the center of technological advancement in the Atlantic World. And in a matter of two decades after Christopher Columbus touched down on the Bahamas, just across the ocean, the trapiche followed European colonists to the Caribbean. The first stop was the island of Hispaniola.\n\nThe trapiche's arrival to the Caribbean coincided with three crucial events in the early history of the Americas. They were the dramatic decline of the indigenous population, the arrival of the first enslaved Africans to the Americas and the sudden drop in the production of gold. While large numbers of colonists sought to escape the ensuing desolation and migrated to settle and desolate in turn other territories, those who stayed on Hispaniola turned to the sugar industry hustled at first by a mixture of enslaved indigenous people and Africans (ladinos and bozales). In a few more years, as the indigenous population retrieved, enslaved Blacks made up the bulk if not all of the coerced workers. With the promise of personal wealth implied in the system of slavery and with the advice of Canarian experts colonists began establishing some types of engenhos as early as 1514. According to Cronistas de Indias (Chroniclers of the Indies), Bartolomé de las Casas and Gonzalo Fernández de Oviedo y Valdés, it was Gonzales de Veloso (also, Gonzalez Veloso and Gonzalo de Vellosa) who built in what today is San Cristobal the first two-roller trapiche pulled by horses on Hispaniola. From there, it turned up on the Island of San Juan Bautista (Puerto Rico) and later in Cuba.\n\nThough most current examples of trapiches in the Spanish Caribbean are of the three-rollers, according to scholar Anthony R. Stevens-Acevedo, the horizontal two-roller trapiche was the type used in the Caribbean throughout the end of the 16th century. As this piece of technology moved south to Tierra Firme (South America), the trapiche not only acquired a new roller, but it also erected all three of them to become a more efficient instrument of the expanding sugar industry. In this more elaborate shape, it soon returned to the Caribbean as the backbone of the sugar engenho.\n\nNowadays, the majority of the ingenios in Argentina or (engenhos in Brazil), use a trapiche to grind the sugarcane and extract its juice. They used water vapor as a driving force for mechanisms. In Latin America one can see small and transportable \"street trapiches\" handled by just one person. They can be installed almost anywhere to produce fresh cane juice. Its manufacture is artisanal, having even wooden gears.\n\nIn Argentina, Bolivia and Chile the term also applies to a type of mill used to reduce to dust different kinds minerals. In the seventeenth century, these facilities and the raw material (ore, wages, the lease of the site and water, buildings ...) needed a considerable investment, the major part of it held by wealthy colonial elite.\n\nAmong the general mechanisms by which the Chilean economic life developed in the Colony, the trapiches were a highly profitable investment. On the other hand, the perception of metals as means of payment for its use, offered a source of profitability, as they were connected to the commercial circuit of gold but outside the margins of local production centers.\n\n"}
{"id": "37042066", "url": "https://en.wikipedia.org/wiki?curid=37042066", "title": "Trazzler", "text": "Trazzler\n\nTrazzler is a travel destination app that specializes in unique and local destinations. The initial concept was developed by Adam Rugel and Biz Stone in 2006 at Twitter's original offices under the name \"71 miles\". More than 10,000 writers and photographers have contributed and more than $350,000 in freelance contracts have been issued as a result of Trazzeler's weekly writing and photography contests. Investors in the company include SV Angel, AOL Founder Steve Case, and the Twitter founders, Evan Williams, Jack Dorsey, and Biz Stone. The company's partners are the City of Chicago, Hawaii Tourism Authority, Fairmont Hotels & Resorts, Salon.com, and Air New Zealand. Trazzler is designed for use on the iOS, Android, and Facebook.\n"}
{"id": "2317480", "url": "https://en.wikipedia.org/wiki?curid=2317480", "title": "Tripod (photography)", "text": "Tripod (photography)\n\nIn photography, a tripod is used to stabilize and elevate a camera, a flash unit, or other photographic equipment. All photographic tripods have three legs and a mounting head to couple with a camera. The mounting head usually includes a thumbscrew that mates to a female threaded receptacle on the camera, as well as a mechanism to be able to rotate and tilt the camera when it is mounted on the tripod. Tripod legs are usually made to telescope, in order to save space when not in use. Tripods are usually made from aluminum, carbon fiber, steel, wood or plastic.\n\nTripods are used for both still and motion photography to prevent camera movement. They are necessary when slow-speed exposures are being made, or when lenses of extreme focal length are used, as any camera movement while the shutter is open will produce a blurred image. In the same vein, they reduce camera shake, and thus are instrumental in achieving maximum sharpness. A tripod is also helpful in achieving precise framing of the image, or when more than one image is being made of the same scene, for example when bracketing the exposure. Use of a tripod may also allow for a more thoughtful approach to photography. For all of these reasons, a tripod of some sort is often necessary for professional photography as well as certain video uses. Tripods are also used as an alternative to C-Stands to photographic accessories.\n\nFor maximum strength and stability, most photographic tripods are braced around a center post, with collapsible telescoping legs and a telescoping section at the top that can be raised or lowered. At the top of the tripod is the head, which includes the camera mount (usually a detachable plate with a thumbscrew to hold on to the camera), several joints to allow the camera to pan, rotate and tilt, and usually a handle to allow the operator to do so without jostling the camera. Some tripods also feature integrated remote controls to control a camcorder or camera, though these are usually proprietary to the company that built the camera. Materials used in the construction of tripod or monopod legs include metal (typically bare or painted aluminum), wood and carbon fiber-reinforced plastics, among others.\n\nPer ISO 1222:2010, the current tripod screw thread standard for attaching the camera calls for a 1/4-20 UNC or 3/8-16 UNC thread. Most consumer cameras are fitted with 1/4-20 UNC threads. Larger, professional cameras and lenses may be fitted with 3/8-16 UNC threads, plus a removable 1/4-20 UNC adapter, allowing them to be mounted on a tripod using either standard.\n\nHistorically, The Royal Photographic Society recommended the thread standard for attaching older cameras to tripods was 3/16-24 BSW (3/16 inch nominal diameter, 24 threads per inch), or 1/4-20 BSW for smaller cameras and 3/8-16 BSW for larger cameras and pan/tilt heads. In this application, the BSW and UNC thread profiles are similar enough that one can mount a modern camera on a legacy tripod and vice versa. The UNC threads are a 60-degree angle and flattened, whereas the BSW are a 55-degree angle and rounded crest. However, at least one English manufacturer uses No.1 B.A. (British Association) for its tripod mount thread.\n\nThere are several types of tripods. The least expensive, generally made of aluminum tubing and costing less than US$50, is used primarily for consumer still and video cameras; these generally come with an attached head and rubber feet. The head is very basic, and often not entirely suitable for smooth panning of a camcorder. A common feature, mostly designed for still cameras, allows the head to flip sideways 90 degrees to allow the camera to take pictures in portrait format rather than landscape. Often included is a small pin on the front of the mounting screw that is used to stabilize camcorders. This is not found on the more expensive photographic tripods.\n\nMore expensive professional tripods are sturdier, stronger, and usually come with no integrated head. The separate heads allow a tripod-head combination to be customized to the photographer's needs. There are expensive carbon fiber tripods, used for applications where the tripod needs to be lightweight. Many tripods, even some relatively inexpensive ones, also include leveling indicators for the legs of the tripod and the head.\n\nMany of the more expensive tripods have additional features, such as a reversible center post so that the camera may be mounted between the legs, allowing for shots from low positions, and legs that can open to several different angles.\nSmall \"tabletop\" tripods (sometimes called \"tablepods\") are also available, ranging from relatively flimsy models costing less than US$20, to professional models that can cost up to US$800 and can support up to 68 kg (150 lb). They are used in situations where a full sized tripod would be too bulky to carry. An alternative is a \"clamp-pod\", which is a ball head attached to a C-clamp.\n\nAnother technique involves forming a string triangle held taut around the two feet of the photographer and linked to the camera. This \"negative\" string \"tripod\" can stabilize the camera sufficiently to use a shutter speed three stops slower.\n\nThe head is the part of the tripod that attaches to the camera and allows it to be aimed. It may be integrated into the tripod, or a separate part. There are generally two different types of heads available.\nA ball head utilizes a ball joint to allow movement of all axes of rotation from a single point. Some ball heads also have a separate panoramic rotation axis on the base of the head. The head has two main parts, the \"ball\", which attaches to the camera and the \"socket\", which attaches to the tripod. The camera is attached to the ball via quick release plate, or a simple UNC 1/4\"-20 screw. The socket is where the ball rotates in, and also contains the controls for locking the ball. The socket has a slot on the side, to allow the camera to be rotated to the portrait orientation. Ball heads come in varying styles of complexity. Some have only one control for both ball and pan lock. While others have individual controls for the ball, pan, and also ball friction. Ball heads are used when a free-flow movement of the camera is needed. They are also more stable and can hold heavier loads, than pan-tilt heads. However, ball heads have the disadvantage that only one control is available to allow or prevent movement of all axes of rotation, so if the camera is tilted on one axis, there may be a risk of tilting on the other axes as well. When a movement of one, or two axes or rotation is needed, a pan-tilt head is used.\nThe pan-tilt head has separate axes and controls for tilting and panning, so that a certain axis can be controlled without risk of affecting the other axes. These heads come in two types, 2-way and 3-way. 2-way heads have 2 axes and controls, one for panoramic rotation, and one for front tilt. 3-way heads have 3 axes and controls, one for panoramic rotation, front tilt, and lateral tilt. The controls on these heads are usually handles that can be turned, to loosen or tighten the certain axis. This allows movement in one, a few, or none of the axes. When the movement of all axes of rotation is needed, a ball head is used. There are some pan-tilt heads that use gears, for precision control of each axis. This is helpful for some types of photography, such as macro photography.\n\nOther head types include the gimbal head, fluid head, gear head, alt-azimuth, and equatorial heads. Fluid heads and gear heads move very smoothly, avoiding the jerkiness caused by the stick-slip effect found in other types of tripod heads. Gimbal heads are single-axis heads used in order to allow a balanced movement for camera and lenses. This proves useful in wildlife photography as well as in any other case where very long and heavy telephoto lenses are adopted: a gimbal head rotates a lens around its center of gravity, thus allowing for easy and smooth manipulation while tracking moving subjects.\n\nIn place of or to supplement a tripod, some photographers use a one-legged telescoping stand called a monopod for convenience in setup and breakdown. A monopod requires the photographer to hold the camera in place, but because the monopod reduces the number of degrees of freedom of the camera, and also because the photographer no longer has to support the full weight of the camera, it can provide some of the same stabilization advantages as a tripod.\n\nFor low-angle shots particularly in cinematography, short tripods with fixed length legs and no center column may be used. The lowest of these is called a low hat, with a slightly higher version referred to as a hi hat.\n\nA travel tripod is one that has been designed to fit certain criteria including lightness and rigidity to give the photographer the freedom to travel and carry his/her tripod for extended periods. It must be small and light enough to be carried as hand baggage yet has enough strength to support a professional DSLR and fast telephoto lens. Typical specifications for a travel tripod legs would be: Weight (without head): 2 to 4 lbs (0.9 to 1.8 kg), Height (contracted): 12 to 20 inches (30 to 50cm), Height (extended): - 60 to 70 inches (152 to 178cm) and Max Load: 5 to 10 lbs (2.3 to 4.6 kg).\n\n"}
{"id": "47137356", "url": "https://en.wikipedia.org/wiki?curid=47137356", "title": "Warehouse execution system", "text": "Warehouse execution system\n\nWarehouse Execution Systems (WES) are computerized systems used in distribution operations (Logistics) and are functionally equivalent to a manufacturing execution system or MES. Distribution operations are a form of a manufacturing operation that receive, store and track inbound material and then select and combine (assemble) various materials to form a finished product, order, or shipment.\n\nFeatures of both a WES and MES track and control work process. WES software organizes, sequences and synchronizes work resources necessary to complete the assembly and shipment of finished product. WES works in real time to enable the control of multiple elements of the production process (e.g. inputs, personnel, machines and support services) where changing conditions in one work process may require changing the directions of upstream or downstream processes (reactive).\n\nThe WES is an intermediate step between an enterprise resource planning (ERP) system, warehouse management system WMS and the resources necessary to perform the various tasks. These resources include workers as well as process control systems. The WES communicates with resources to collect information and direct work effort.\n\nWES emerged as a hybrid system that combined specific WMS functionality with Warehouse Control System (WCS) functionality for automated warehouses. WCS is the software that controls the movement of cases, cartons, totes or pallets on conveyor and sortation systems. In automated warehouses that deploy those types of material handling equipment, WES adds business process logic to the almost real-time WCS data. Because WES is tightly integrated with automated systems such as conveyors, sortation, pick-to-light, etc., it has near real-time visibility into bottleneck issues.\n\nWES also tends to be more “productized” than WCS – more WMS-type features are fully developed and often come standard. Vendors typically offer modular software solutions that together form a comprehensive WES. The modules cover WMS, conveyor control, advanced analytics/reporting, and integration functionalities. Typical WES packages contain a suite of best practice methods for warehouse operations – e.g. picking and packing – that users can configure based on a workflow. This allows management of a process without customizing the product.\n\nTypical warehouse functionality such as replenishment and order picking take advantage of real-time conditions to optimize productivity. Whereas typical WMS functionality calls for planning and releasing orders to be picked, a WES is focused on executing these tasks based on existing facility conditions. As a result, warehouses become more flexible and agile in response.\n\nA WES has the ability to span across multiple areas of warehouse functionality that are traditionally managed by a variety of specialized software systems. WES can be deployed to encompass warehouse management functionality, warehouse control system functionality, material handling equipment (MHE) control, business intelligence and integration with host ERP systems. Encompassing this broad range of functionality is a distinct advantage for WES. As a result, the WES can leverage its visibility of lower level warehouse data to quickly adapt functionality needs for current conditions. This is especially true in facilities with automated systems. The WES can utilize its WCS roots to access connections to advanced picking and sortation systems thus offering an agile approach to optimizing operations in near real-time.\n\nAnother benefit of leveraging the visibility of lower level data across a broad range of warehouse functionality is the ability to provide unprecedented automated business intelligence. WES’ access to and collection of data from various warehouse points can be utilized to provide not only advanced reporting and live dashboard functionality but business intelligence tools such as predictive analysis, prescriptive analysis, and issue detection. The WES can feed data into its business intelligence engine to be mined in near real-time so that DC operations can move beyond just being agile in response to changing conditions, to being proactive in making adjustments before conditions change. WES data can be analyzed to identify trends and predict operational conditions. For example, if operation peaks occur at the end of every month, warehouses can use WES feedback to ramp up staffing and equipment needs more efficiently to reduce overall costs. WES data can also be used to predict issues such as potential stock-outs or order fulfillment delays. Issue detection can also relate to preventative maintenance of warehouse equipment such as lift trucks, conveyor systems, etc. To illustrate this point, through analyzing vast amounts of data, the WES can predict when a conveyor motor may need to be replaced or when a lift truck may need servicing to reduce downtime. By collecting and analyzing data from various lower level warehouse points and taking proactive action, operation leads can use this functionality – which is unique to a WES – to make their facilities more efficient, safe and responsive to increasing customer service requirements.\n\nThere are a number of voices in the material handling industry that disagree with the creation of a separate term to define WES functionality.\n\n\"The new term being used by material handling software providers, integrators, and pretty much anyone associated with the warehousing industry is Warehouse Execution System (WES). According to a March article in DC Velocity, Warehouse Execution Systems can replace both the WMS and WCS by maintaining warehouse inventory records and driving the mechanization. It certainly seems like a very tasty fry sauce! What’s important to remember when exploring options for a Warehouse Execution System is that the functionality is the same functionality that has always been offered! The key to finding the best system for your facility is not a flashy new name offering a combination or best of both worlds mix of solutions, but rather a modularized solution that allows you to define the functionality required.\"\n\n\"Why invent a new term? ... there is no such thing as a WES, WCS, or WMS. (There is) simply Warehouse Software functionality needed to suit a business requirement. If the term WES fits fine, but as soon as you draw too many boundaries you create is another layer of interface and support.\"\n\nRight or wrong to create a new term, the term WES is being used in the material handling and distribution industry. The creation of the term arose as a \"gap\" was identified in the previous divisions of functionality required to operate a distribution facility. \"As the demands of omni-channel distribution continue to pressure facilities and supply chains to run more efficiently, many distributors are realizing that the current functionalities of WMS and WCS are insufficient to align, automate, and synchronize the discrete processes needed to optimally control their order-fulfillment needs.\"\n\nA common occurrence in a large distribution center is that there are multiple varying types of large equipment and work processes provided by multiple vendors. The control of these complex and sometimes intricate functions is provided by the equipment supplier that control software is often called Warehouse Control System or WCS software. This situation leads to some facilities have multiple WCS software packages running. Without a WES layer this presents a terminology problem: The operation of all the equipment and process functions need to be coordinated and synchronized to provide the required facility objectives. This will require either a \"Master WCS\" or the WMS or ERP system to perform the function. A WES is the master WCS.\n\nAs in a MES Manufacturing Execution System the line between higher level and lower level control is blurred and not well defined, so it is with a WES. Manufacturing operations are significantly more mature than distribution when considering lean practices. This is evidenced by the total lack of a Wikipedia article on \"Lean distribution\". Manufacturing operations face the same challenges of distribution operations having many different work cells or processes controlled by vendor provided control systems. A special category of software was created for manufacturing to integrate the overall production and coordinate the efforts of all production resources.\n\nOne of the byproducts of the rise of e-commerce, is the need for software to support the complexities of omni-channel fulfillment. “The omni-channel environment places enormous pressure on DCs to keep up with higher order volumes including rapid processing of small e-commerce orders.” The market for WES solutions continue to grow as DC operators seek productivity and support a high throughput.\n\nEmerging functionality such as waveless processing is drawing attention to WES. Waveless processing requires constantly taking customer orders from a host system and releasing the work to the warehouse floor efficiently. This is in direct contrast to the typical WMS approach of batching the orders in waves. Waveless processing also allows orders to be worked as received, ensuring a faster turnaround time, and takes advantage of the WES grasp of near real-time information.\n\nEnterprise Resource Planning ERP\n\nLabor Management System LMS\n\nWarehouse Management System WMS\n\nOrder Management System OMS\n\nTransportation Management System TMS\n\nYard Management System YMS\n\nWarehouse Control System WCS\n\nLow level machine control software: This software directly interfaces (through I/O) to equipment providing information to and taking directions from higher level systems. In many operations this functionality is provided by Programmable logic controller often referred to as PLC logic. Other operations may use a general purpose computers coupled to industrial Input/Output systems to do the control.\n"}
{"id": "48326079", "url": "https://en.wikipedia.org/wiki?curid=48326079", "title": "Water vapor windows", "text": "Water vapor windows\n\nIn the Earth's atmosphere, water vapor absorbs many wavelengths of IR energy, while others are not absorbed. Those sections of the Electromagnetic spectrum that it does not absorb, are like windows in the atmosphere, allowing electromagnetic energy to flow freely in and out of the system. Like a window, that allows light to enter and leave. Originally discovered by John Tyndall, most of the infrared coming from the Universe is blocked, absorbed by water vapor (and other greenhouse gases)in the Earth's atmosphere. Those wavelength ranges that can partially reach the surface, are coming through what is called 'water vapor windows'. These windows are how Astronomers can view the Universe with IR telescopes, called Infrared astronomy. These windows also allow orbiting satellites to measure the IR energy leaving the planet, the SSTs and other important matters. See Electromagnetic_absorption_by_water#Atmospheric_effects.\n"}
{"id": "34385", "url": "https://en.wikipedia.org/wiki?curid=34385", "title": "Yeast", "text": "Yeast\n\nYeasts are eukaryotic, single-celled microorganisms classified as members of the fungus kingdom. The first yeast originated hundreds of millions of years ago, and 1,500 species are currently identified. They are estimated to constitute 1% of all described fungal species. Yeasts are unicellular organisms which evolved from multicellular ancestors, with some species having the ability to develop multicellular characteristics by forming strings of connected budding cells known as pseudohyphae or false hyphae. Yeast sizes vary greatly, depending on species and environment, typically measuring 3–4 µm in diameter, although some yeasts can grow to 40 µm in size. Most yeasts reproduce asexually by mitosis, and many do so by the asymmetric division process known as budding.\n\nYeasts, with their single-celled growth habit, can be contrasted with molds, which grow hyphae. Fungal species that can take both forms (depending on temperature or other conditions) are called dimorphic fungi (\"dimorphic\" means \"having two forms\").\n\nBy fermentation, the yeast species \"Saccharomyces cerevisiae\" converts carbohydrates to carbon dioxide and alcohols – for thousands of years the carbon dioxide has been used in baking and the alcohol in alcoholic beverages. It is also a centrally important model organism in modern cell biology research, and is one of the most thoroughly researched eukaryotic microorganisms. Researchers have used it to gather information about the biology of the eukaryotic cell and ultimately human biology. Other species of yeasts, such as \"Candida albicans\", are opportunistic pathogens and can cause infections in humans. Yeasts have recently been used to generate electricity in microbial fuel cells, and produce ethanol for the biofuel industry.\n\nYeasts do not form a single taxonomic or phylogenetic grouping. The term \"yeast\" is often taken as a synonym for \"Saccharomyces cerevisiae\", but the phylogenetic diversity of yeasts is shown by their placement in two separate phyla: the Ascomycota and the Basidiomycota. The budding yeasts (\"true yeasts\") are classified in the order Saccharomycetales, within the phylum Ascomycota.\n\nThe word \"yeast\" comes from Old English \"gist\", \"gyst\", and from the Indo-European root \"yes-\", meaning \"boil\", \"foam\", or \"bubble\". Yeast microbes are probably one of the earliest domesticated organisms. Archaeologists digging in Egyptian ruins found early grinding stones and baking chambers for yeast-raised bread, as well as drawings of 4,000-year-old bakeries and breweries. In 1680, Dutch naturalist Anton van Leeuwenhoek first microscopically observed yeast, but at the time did not consider them to be living organisms, but rather globular structures. Researchers were doubtful whether yeasts were algae or fungi, but in 1837 Theodor Schwann recognized them as fungi.\n\nIn 1857, French microbiologist Louis Pasteur proved in the paper \"Mémoire sur la fermentation alcoolique\" that alcoholic fermentation was conducted by living yeasts and not by a chemical catalyst. Pasteur showed that by bubbling oxygen into the yeast broth, cell growth could be increased, but fermentation was inhibited – an observation later called the \"Pasteur effect\".\n\nBy the late 18th century, two yeast strains used in brewing had been identified: \"Saccharomyces cerevisiae\" (top-fermenting yeast) and \"S. carlsbergensis\" (bottom-fermenting yeast). \"S. cerevisiae\" has been sold commercially by the Dutch for bread-making since 1780; while, around 1800, the Germans started producing \"S. cerevisiae\" in the form of cream. In 1825, a method was developed to remove the liquid so the yeast could be prepared as solid blocks. The industrial production of yeast blocks was enhanced by the introduction of the filter press in 1867. In 1872, Baron Max de Springer developed a manufacturing process to create granulated yeast, a technique that was used until the first World War. In the United States, naturally occurring airborne yeasts were used almost exclusively until commercial yeast was marketed at the Centennial Exposition in 1876 in Philadelphia, where Charles L. Fleischmann exhibited the product and a process to use it, as well as serving the resultant baked bread.\n\nYeasts are chemoorganotrophs, as they use organic compounds as a source of energy and do not require sunlight to grow. Carbon is obtained mostly from hexose sugars, such as glucose and fructose, or disaccharides such as sucrose and maltose. Some species can metabolize pentose sugars such as ribose, alcohols, and organic acids. Yeast species either require oxygen for aerobic cellular respiration (obligate aerobes) or are anaerobic, but also have aerobic methods of energy production (facultative anaerobes). Unlike bacteria, no known yeast species grow only anaerobically (obligate anaerobes). Most yeasts grow best in a neutral or slightly acidic pH environment.\n\nYeasts vary in regard to the temperature range in which they grow best. For example, \"Leucosporidium frigidum\" grows at , \"Saccharomyces telluris\" at , and \"Candida slooffi\" at . The cells can survive freezing under certain conditions, with viability decreasing over time.\n\nIn general, yeasts are grown in the laboratory on solid growth media or in liquid broths. Common media used for the cultivation of yeasts include potato dextrose agar or potato dextrose broth, Wallerstein Laboratories nutrient agar, yeast peptone dextrose agar, and yeast mould agar or broth. Home brewers who cultivate yeast frequently use dried malt extract and agar as a solid growth medium. The antibiotic cycloheximide is sometimes added to yeast growth media to inhibit the growth of \"Saccharomyces\" yeasts and select for wild/indigenous yeast species. This will change the yeast process.\n\nThe appearance of a white, thready yeast, commonly known as kahm yeast, is often a byproduct of the lactofermentation (or pickling) of certain vegetables, usually the result of exposure to air. Although harmless, it can give pickled vegetables a bad flavor and must be removed regularly during fermentation.\n\nYeasts are very common in the environment, and are often isolated from sugar-rich materials. Examples include naturally occurring yeasts on the skins of fruits and berries (such as grapes, apples, or peaches), and exudates from plants (such as plant saps or cacti). Some yeasts are found in association with soil and insects. The ecological function and biodiversity of yeasts are relatively unknown compared to those of other microorganisms. Yeasts, including \"Candida albicans\", \"Rhodotorula rubra\", \"Torulopsis\" and \"Trichosporon cutaneum\", have been found living in between people's toes as part of their skin flora. Yeasts are also present in the gut flora of mammals and some insects and even deep-sea environments host an array of yeasts.\n\nAn Indian study of seven bee species and 9 plant species found 45 species from 16 genera colonise the nectaries of flowers and honey stomachs of bees. Most were members of the genus \"Candida\"; the most common species in honey stomachs was \"Dekkera intermedia\" and in flower nectaries, \"Candida blankii\". Yeast colonising nectaries of the stinking hellebore have been found to raise the temperature of the flower, which may aid in attracting pollinators by increasing the evaporation of volatile organic compounds. A black yeast has been recorded as a partner in a complex relationship between ants, their mutualistic fungus, a fungal parasite of the fungus and a bacterium that kills the parasite. The yeast has a negative effect on the bacteria that normally produce antibiotics to kill the parasite, so may affect the ants' health by allowing the parasite to spread.\n\nCertain strains of some species of yeasts produce proteins called yeast killer toxins that allow them to eliminate competing strains. (See main article on killer yeast.) This can cause problems for winemaking but could potentially also be used to advantage by using killer toxin-producing strains to make the wine. Yeast killer toxins may also have medical applications in treating yeast infections (see \"Pathogenic yeasts\" section below).\n\nMarine yeasts, defined as the yeasts that are isolated from marine environments, are able to grow better on a medium prepared using seawater rather than freshwater (). The first marine yeasts were isolated by Bernhard Fischer in 1894 from the Atlantic Ocean, and those were identified as Torula sp. and Mycoderma sp. . Following this discovery, various other marine yeasts have been isolated from around the world from different sources, including seawater, seaweeds, marine fish and mammals . Among these isolates, some marine yeasts originated from terrestrial habitats (grouped as facultative marine yeast), which were brought to and survived in marine environments. The other marine yeasts were grouped as obligate or indigenous marine yeasts, which confine to marine habitats . However, no sufficient evidence has been found to explain the indispensability of seawater for obligate marine yeasts . It has been reported that marine yeasts are able to produce many bioactive substances, such as amino acids, glucans, glutathione, toxins, enzymes, phytase and vitamins with potential application in the food, pharmaceutical, cosmetic and chemical industries as well as for marine culture and environmental protection . Marine yeast was successfully used to produce bioethanol using seawater-based media which will potentially reduce the WF of bioethanol .\n\nYeasts, like all fungi, may have asexual and sexual reproductive cycles. The most common mode of vegetative growth in yeast is asexual reproduction by budding. Here, a small bud (also known as a bleb), or daughter cell, is formed on the parent cell. The nucleus of the parent cell splits into a daughter nucleus and migrates into the daughter cell. The bud continues to grow until it separates from the parent cell, forming a new cell. The daughter cell produced during the budding process is generally smaller than the mother cell. Some yeasts, including \"Schizosaccharomyces pombe\", reproduce by fission instead of budding, thereby creating two identically sized daughter cells.\n\nIn general, under high-stress conditions such as nutrient starvation, haploid cells will die; under the same conditions, however, diploid cells can undergo sporulation, entering sexual reproduction (meiosis) and producing a variety of haploid spores, which can go on to mate (conjugate), reforming the diploid.\n\nThe haploid fission yeast \"Schizosaccharomyces pombe\" is a facultative sexual microorganism that can undergo mating when nutrients are limiting. Exposure of \"S. pombe\" to hydrogen peroxide, an agent that causes oxidative stress leading to oxidative DNA damage, strongly induces mating and the formation of meiotic spores. The budding yeast \"Saccharomyces cerevisiae\" reproduces by mitosis as diploid cells when nutrients are abundant, but when starved, this yeast undergoes meiosis to form haploid spores. Haploid cells may then reproduce asexually by mitosis. Katz Ezov et al. presented evidence that in natural \"S. cerevisiae\" populations clonal reproduction and selfing (in the form of intratetrad mating) predominate. In nature, mating of haploid cells to form diploid cells is most often between members of the same clonal population and out-crossing is uncommon. Analysis of the ancestry of natural \"S. cerevisiae\" strains led to the conclusion that out-crossing occurs only about once every 50,000 cell divisions. These observations suggest that the possible long-term benefits of outcrossing (e.g. generation of diversity) are likely to be insufficient for generally maintaining sex from one generation to the next. Rather, a short-term benefit, such as recombinational repair during meiosis, may be the key to the maintenance of sex in \"S. cerevisiae\".\n\nSome pucciniomycete yeasts, in particular species of \"Sporidiobolus\" and \"Sporobolomyces\", produce aerially dispersed, asexual ballistoconidia.\n\nThe useful physiological properties of yeast have led to their use in the field of biotechnology. Fermentation of sugars by yeast is the oldest and largest application of this technology. Many types of yeasts are used for making many foods: baker's yeast in bread production, brewer's yeast in beer fermentation, and yeast in wine fermentation and for xylitol production. So-called red rice yeast is actually a mold, \"Monascus purpureus\". Yeasts include some of the most widely used model organisms for genetics and cell biology.\n\nAlcoholic beverages are defined as beverages that contain ethanol (CHOH). This ethanol is almost always produced by fermentation – the metabolism of carbohydrates by certain species of yeasts under anaerobic or low-oxygen conditions. Beverages such as mead, wine, beer, or distilled spirits all use yeast at some stage of their production. A distilled beverage is a beverage containing ethanol that has been purified by distillation. Carbohydrate-containing plant material is fermented by yeast, producing a dilute solution of ethanol in the process. Spirits such as whiskey and rum are prepared by distilling these dilute solutions of ethanol. Components other than ethanol are collected in the condensate, including water, esters, and other alcohols, which (in addition to that provided by the oak in which it may be aged) account for the flavour of the beverage.\n\nBrewing yeasts may be classed as \"top-cropping\" (or \"top-fermenting\") and \"bottom-cropping\" (or \"bottom-fermenting\"). Top-cropping yeasts are so called because they form a foam at the top of the wort during fermentation. An example of a top-cropping yeast is \"Saccharomyces cerevisiae\", sometimes called an \"ale yeast\". Bottom-cropping yeasts are typically used to produce lager-type beers, though they can also produce ale-type beers. These yeasts ferment well at low temperatures. An example of bottom-cropping yeast is \"Saccharomyces pastorianus\", formerly known as \"S. carlsbergensis\".\n\nDecades ago, taxonomists reclassified \"S. carlsbergensis\" (uvarum) as a member of \"S. cerevisiae\", noting that the only distinct difference between the two is metabolic. Lager strains of \"S. cerevisiae\" secrete an enzyme called melibiase, allowing them to hydrolyse melibiose, a disaccharide, into more fermentable monosaccharides. Top- and bottom-cropping and cold- and warm-fermenting distinctions are largely generalizations used by laypersons to communicate to the general public.\n\nThe most common top-cropping brewer's yeast, \"S. cerevisiae\", is the same species as the common baking yeast. Brewer's yeast is also very rich in essential minerals and the B vitamins (except B). However, baking and brewing yeasts typically belong to different strains, cultivated to favour different characteristics: baking yeast strains are more aggressive, to carbonate dough in the shortest amount of time possible; brewing yeast strains act slower but tend to produce fewer off-flavours and tolerate higher alcohol concentrations (with some strains, up to 22%).\n\n\"Dekkera/Brettanomyces\" is a genus of yeast known for its important role in the production of 'lambic' and specialty sour ales, along with the secondary conditioning of a particular Belgian Trappist beer. The taxonomy of the genus \"Brettanomyces\" has been debated since its early discovery and has seen many reclassifications over the years. Early classification was based on a few species that reproduced asexually (anamorph form) through multipolar budding. Shortly after, the formation of ascospores was observed and the genus \"Dekkera\", which reproduces sexually (teleomorph form), was introduced as part of the taxonomy. The current taxonomy includes five species within the genera of \"Dekkera/Brettanomyces\". Those are the anamorphs \"Brettanomyces bruxellensis\", \"Brettanomyces anomalus\", \"Brettanomyces custersianus\", \"Brettanomyces naardenensis\", and \"Brettanomyces nanus\", with teleomorphs existing for the first two species, \"Dekkera bruxellensis\" and \"Dekkera anomala\". The distinction between \"Dekkera\" and \"Brettanomyces\" is arguable, with Oelofse et al. (2008) citing Loureiro and Malfeito-Ferreira from 2006 when they affirmed that current molecular DNA detection techniques have uncovered no variance between the anamorph and teleomorph states. Over the past decade, \"Brettanomyces\" spp. have seen an increasing use in the craft-brewing sector of the industry, with a handful of breweries having produced beers that were primarily fermented with pure cultures of \"Brettanomyces\" spp. This has occurred out of experimentation, as very little information exists regarding pure culture fermentative capabilities and the aromatic compounds produced by various strains. \"Dekkera\"/\"Brettanomyces\" spp. have been the subjects of numerous studies conducted over the past century, although a majority of the recent research has focused on enhancing the knowledge of the wine industry. Recent research on eight \"Brettanomyces\" strains available in the brewing industry focused on strain-specific fermentations and identified the major compounds produced during pure culture anaerobic fermentation in wort.\n\nYeast is used in winemaking, where it converts the sugars present (glucose and fructose) in grape juice (must) into ethanol. Yeast is normally already present on grape skins. Fermentation can be done with this endogenous \"wild yeast\", but this procedure gives unpredictable results, which depend upon the exact types of yeast species present. For this reason, a pure yeast culture is usually added to the must; this yeast quickly dominates the fermentation. The wild yeasts are repressed, which ensures a reliable and predictable fermentation.\n\nMost added wine yeasts are strains of \"S. cerevisiae\", though not all strains of the species are suitable. Different \"S. cerevisiae\" yeast strains have differing physiological and fermentative properties, therefore the actual strain of yeast selected can have a direct impact on the finished wine. Significant research has been undertaken into the development of novel wine yeast strains that produce atypical flavour profiles or increased complexity in wines.\n\nThe growth of some yeasts, such as \"Zygosaccharomyces\" and \"Brettanomyces\", in wine can result in wine faults and subsequent spoilage. \"Brettanomyces\" produces an array of metabolites when growing in wine, some of which are volatile phenolic compounds. Together, these compounds are often referred to as \"\"Brettanomyces\" character\", and are often described as \"antiseptic\" or \"barnyard\" type aromas. \"Brettanomyces\" is a significant contributor to wine faults within the wine industry.\n\nResearchers from the University of British Columbia, Canada, have found a new strain of yeast that has reduced amines. The amines in red wine and Chardonnay produce off-flavors and cause headaches and hypertension in some people. About 30% of people are sensitive to biogenic amines, such as histamines.\n\nYeast, the most common one being \"S. cerevisiae\", is used in baking as a leavening agent, where it converts the food/fermentable sugars present in dough into the gas carbon dioxide. This causes the dough to expand or rise as gas forms pockets or bubbles. When the dough is baked, the yeast dies and the air pockets \"set\", giving the baked product a soft and spongy texture. The use of potatoes, water from potato boiling, eggs, or sugar in a bread dough accelerates the growth of yeasts. Most yeasts used in baking are of the same species common in alcoholic fermentation. In addition, \"Saccharomyces exiguus\" (also known as \"S. minor\"), a wild yeast found on plants, fruits, and grains, is occasionally used for baking. In breadmaking, the yeast initially respires aerobically, producing carbon dioxide and water. When the oxygen is depleted, fermentation begins, producing ethanol as a waste product; however, this evaporates during baking.\n\nIt is not known when yeast was first used to bake bread. The first records that show this use came from Ancient Egypt. Researchers speculate a mixture of flour meal and water was left longer than usual on a warm day and the yeasts that occur in natural contaminants of the flour caused it to ferment before baking. The resulting bread would have been lighter and tastier than the normal flat, hard cake.\n\nToday, there are several retailers of baker's yeast; one of the earlier developments in North America is Fleischmann's Yeast, in 1868. During World War II, Fleischmann's developed a granulated active dry yeast which did not require refrigeration, had a longer shelf life than fresh yeast, and rose twice as fast. Baker's yeast is also sold as a fresh yeast compressed into a square \"cake\". This form perishes quickly, so must be used soon after production. A weak solution of water and sugar can be used to determine whether yeast is expired. In the solution, active yeast will foam and bubble as it ferments the sugar into ethanol and carbon dioxide. Some recipes refer to this as proofing the yeast, as it \"proves\" (tests) the viability of the yeast before the other ingredients are added. When a sourdough starter is used, flour and water are added instead of sugar; this is referred to as proofing the sponge.\n\nWhen yeast is used for making bread, it is mixed with flour, salt, and warm water or milk. The dough is kneaded until it is smooth, and then left to rise, sometimes until it has doubled in size. The dough is then shaped into loaves. Some bread doughs are knocked back after one rising and left to rise again (this is called dough proofing) and then baked. A longer rising time gives a better flavour, but the yeast can fail to raise the bread in the final stages if it is left for too long initially.\n\nSome yeasts can find potential application in the field of bioremediation. One such yeast, \"Yarrowia lipolytica\", is known to degrade palm oil mill effluent, TNT (an explosive material), and other hydrocarbons, such as alkanes, fatty acids, fats and oils. It can also tolerate high concentrations of salt and heavy metals, and is being investigated for its potential as a heavy metal biosorbent. \"Saccharomyces cerevisiae\" has potential to bioremediate toxic pollutants like arsenic from industrial effluent. Bronze statues are known to be degraded by certain species of yeast. Different yeasts from Brazilian gold mines bioaccumulate free and complexed silver ions.\n\nThe ability of yeast to convert sugar into ethanol has been harnessed by the biotechnology industry to produce ethanol fuel. The process starts by milling a feedstock, such as sugar cane, field corn, or other cereal grains, and then adding dilute sulfuric acid, or fungal alpha amylase enzymes, to break down the starches into complex sugars. A glucoamylase is then added to break the complex sugars down into simple sugars. After this, yeasts are added to convert the simple sugars to ethanol, which is then distilled off to obtain ethanol up to 96% in purity.\n\n\"Saccharomyces\" yeasts have been genetically engineered to ferment xylose, one of the major fermentable sugars present in cellulosic biomasses, such as agriculture residues, paper wastes, and wood chips. Such a development means ethanol can be efficiently produced from more inexpensive feedstocks, making cellulosic ethanol fuel a more competitively priced alternative to gasoline fuels.\n\nA number of sweet carbonated beverages can be produced by the same methods as beer, except the fermentation is stopped sooner, producing carbon dioxide, but only trace amounts of alcohol, leaving a significant amount of residual sugar in the drink.\n\nYeast is used in nutritional supplements, especially those marketed to vegans. It is often referred to as \"nutritional yeast\" when sold as a dietary supplement. Nutritional yeast is a deactivated yeast, usually \"S.  cerevisiae\". It is naturally low in fat and sodium as well as an excellent source of protein and vitamins, especially most B-complex vitamins (contrary to some claims, it contains little or no vitamin B ), as well as other minerals and cofactors required for growth. Some brands of nutritional yeast, though not all, are fortified with vitamin B, which is produced separately by bacteria.\n\nIn 1920, the Fleischmann Yeast Company began to promote yeast cakes in a \"Yeast for Health\" campaign. They initially emphasized yeast as a source of vitamins, good for skin and digestion. Their later advertising claimed a much broader range of health benefits, and was censured as misleading by the Federal Trade Commission. The fad for yeast cakes lasted until the late 1930s.\n\nNutritional yeast has a nutty, cheesy flavor and is often used as an ingredient in cheese substitutes. Another popular use is as a topping for popcorn. It can also be used in mashed and fried potatoes, as well as in scrambled eggs. It comes in the form of flakes, or as a yellow powder similar in texture to cornmeal. In Australia, it is sometimes sold as \"savoury yeast flakes\". Though \"nutritional yeast\" usually refers to commercial products, inadequately fed prisoners have used \"home-grown\" yeast to prevent vitamin deficiency.\n\nSome probiotic supplements use the yeast \"S. boulardii\" to maintain and restore the natural flora in the gastrointestinal tract. \"S. boulardii\" has been shown to reduce the symptoms of acute diarrhea, reduce the chance of infection by \"Clostridium difficile\" (often identified simply as C. difficile or C. diff), reduce bowel movements in diarrhea-predominant IBS patients, and reduce the incidence of antibiotic-, traveler's-, and HIV/AIDS-associated diarrheas.\n\nYeast is often used by aquarium hobbyists to generate carbon dioxide (CO) to nourish plants in planted aquaria. CO levels from yeast are more difficult to regulate than those from pressurized CO systems. However, the low cost of yeast makes it a widely used alternative.\n\nYeast extract is the common name for various forms of processed yeast products that are used as food additives or flavours. They are often used in the same way that monosodium glutamate (MSG) is used and, like MSG, often contain free glutamic acid. The general method for making yeast extract for food products such as Vegemite and Marmite on a commercial scale is to add salt to a suspension of yeast, making the solution hypertonic, which leads to the cells' shrivelling up. This triggers autolysis, wherein the yeast's digestive enzymes break their own proteins down into simpler compounds, a process of self-destruction. The dying yeast cells are then heated to complete their breakdown, after which the husks (yeast with thick cell walls that would give poor texture) are separated. Yeast autolysates are used in Vegemite and Promite (Australia); Marmite (the United Kingdom); the unrelated Marmite (New Zealand); Vitam-R (Germany); and Cenovis (Switzerland).\n\nSeveral yeasts, in particular \"S. cerevisiae\" and \"S. pombe\", have been widely used in genetics and cell biology, largely because they are simple eukaryotic cells, serving as a model for all eukaryotes, including humans, for the study of fundamental cellular processes such as the cell cycle, DNA replication, recombination, cell division, and metabolism. Also, yeasts are easily manipulated and cultured in the laboratory, which has allowed for the development of powerful standard techniques, such as yeast two-hybrid, synthetic genetic array analysis, and tetrad analysis. Many proteins important in human biology were first discovered by studying their homologues in yeast; these proteins include cell cycle proteins, signaling proteins, and protein-processing enzymes.\n\nOn 24 April 1996, \"S. cerevisiae\" was announced to be the first eukaryote to have its genome, consisting of 12 million base pairs, fully sequenced as part of the Genome Project. At the time, it was the most complex organism to have its full genome sequenced, and the work seven years and the involvement of more than 100 laboratories to accomplish. The second yeast species to have its genome sequenced was \"Schizosaccharomyces pombe\", which was completed in 2002. It was the sixth eukaryotic genome sequenced and consists of 13.8 million base pairs. As of 2014, over 50 yeast species have had their genomes sequenced and published.\n\nGenomic and functional gene annotation of the two major yeast models can be accessed via their respective model organism databases: SGD and PomBase.\n\nVarious yeast species have been genetically engineered to efficiently produce various drugs, a technique called metabolic engineering. \"S. cerevisiae\" is easy to genetically engineer; its physiology, metabolism and genetics are well known, and it is amenable for use in harsh industrial conditions. A wide variety of chemical in different classes can be produced by engineered yeast, including phenolics, isoprenoids, alkaloids, and polyketides. About 20% of biopharmaceuticals are produced in \"S. cerevisiae\", including insulin, vaccines for hepatitis, and human serum albumin.\n\nSome species of yeast are opportunistic pathogens that can cause infection in people with compromised immune systems. \"Cryptococcus neoformans\" and \"Cryptococcus gattii\" are significant pathogens of immunocompromised people. They are the species primarily responsible for cryptococcosis, a fungal disease that occurs in about one million HIV/AIDS patients, causing over 600,000 deaths annually. The cells of these yeast are surrounded by a rigid polysaccharide capsule, which helps to prevent them from being recognised and engulfed by white blood cells in the human body.\n\nYeasts of the genus \"Candida\", another group of opportunistic pathogens, cause oral and vaginal infections in humans, known as candidiasis. \"Candida\" is commonly found as a commensal yeast in the mucous membranes of humans and other warm-blooded animals. However, sometimes these same strains can become pathogenic. The yeast cells sprout a hyphal outgrowth, which locally penetrates the mucosal membrane, causing irritation and shedding of the tissues. The pathogenic yeasts of candidiasis in probable descending order of virulence for humans are: \"C. albicans\", \"C. tropicalis\", \"C. stellatoidea\", \"C. glabrata\", \"C. krusei\", \"C. parapsilosis\", \"C. guilliermondii\", \"C. viswanathii\", \"C. lusitaniae\", and \"Rhodotorula mucilaginosa\". \"Candida glabrata\" is the second most common \"Candida\" pathogen after \"C. albicans\", causing infections of the urogenital tract, and of the bloodstream (candidemia).\n\nYeasts are able to grow in foods with a low pH (5.0 or lower) and in the presence of sugars, organic acids, and other easily metabolized carbon sources. During their growth, yeasts metabolize some food components and produce metabolic end products. This causes the physical, chemical, and sensible properties of a food to change, and the food is spoiled. The growth of yeast within food products is often seen on their surfaces, as in cheeses or meats, or by the fermentation of sugars in beverages, such as juices, and semiliquid products, such as syrups and jams. The yeast of the genus \"Zygosaccharomyces\" have had a long history as spoilage yeasts within the food industry. This is mainly because these species can grow in the presence of high sucrose, ethanol, acetic acid, sorbic acid, benzoic acid, and sulphur dioxide concentrations, representing some of the commonly used food preservation methods. Methylene blue is used to test for the presence of live yeast cells. In oenology, the major spoilage yeast is \"Brettanomyces bruxellensis\".\n\n\n\n"}
{"id": "6936890", "url": "https://en.wikipedia.org/wiki?curid=6936890", "title": "Zielgerät 1229", "text": "Zielgerät 1229\n\nThe ZG 1229 Vampir\" 1229 (ZG 1229), also known in its code name Vampir\", was an active infrared device developed for the Wehrmacht for the Sturmgewehr 44 assault rifle during World War II, intended primarily for night use.\n\nThe ZG 1229 Vampir weighed in at 2.25 kilograms (about 5 lbs.) and was fitted with lugs on the StG 44 at C.G. Haenel at Suhl, the weapons production facility. The grenadier carrying this was known as a Nachtjäger (night-hunter). As well as the sight and infrared spotlight, there was a 13.5 kilogram (about 30 lbs.) wooden cased battery for the light, and a second battery fitted inside a gas mask container to power the image converter. This was all strapped to a Tragegestell 39 (pack frame 1939). The searchlight consisted of a conventional tungsten light source shining through a filter permitting only infrared light. The sensor operated in the upper infrared (light) spectrum rather than in the lower infrared (heat) spectrum and was, therefore, not sensitive to body heat.\n\nVampir gear was first used in combat in February 1945. However, small arms infrared device introduction took place in early 1944. 310 units were delivered to the Wehrmacht at the final stages of the war. Eastern Front veteran reports consist of snipers shooting at night with the aid of 'peculiar non-shining torches coupled with enormous optical sights' mounted on their rifles. Similar infrared gear was fitted both to MG34 and MG42 machine guns.\n\nMuch of the plot of Stephen Hunter's novel \"The Master Sniper\" revolves around the development and employment of the \"Vampir\" system. It is also referenced in Hunter's book \"Black Light\".\n\nOne of the weapons in the 2008 videogame \"\" is equipped with a fictional development of the Vampir sight.\n\n"}
