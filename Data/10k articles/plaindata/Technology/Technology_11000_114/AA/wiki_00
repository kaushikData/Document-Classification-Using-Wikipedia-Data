{"id": "44578960", "url": "https://en.wikipedia.org/wiki?curid=44578960", "title": "3D Content Retrieval", "text": "3D Content Retrieval\n\nA 3D Content Retrieval system is a computer system for browsing, searching and retrieving three dimensional digital contents (e.g.: Computer-aided design, molecular biology models, and cultural heritage 3D scenes, etc.) from a large database of digital images. The most original way of doing 3D content retrieval uses methods to add description text to 3D content files such as the content file name, link text, and the web page title so that related 3D content can be found through text retrieval. Because of the inefficiency of manually annotating 3D files, researchers have investigated ways to automate the annotation process and provide a unified standard to create text descriptions for 3D contents. Moreover, the increase in 3D content has demanded and inspired more advanced ways to retrieve 3D information. Thus, shape matching methods for 3D content retrieval have become popular. Shape matching retrieval is based on techniques that compare and contrast similarities between 3D models.\n\nDerive a high level description (e.g.: a skeleton) and then find matching results\n\nThis method describes 3D models by using a skeleton. The skeleton encodes the geometric and topological information in the form of a skeletal graph and uses graph matching techniques to match the skeletons and compare them. However, this method requires a 2-manifold input model, and it is very sensitive to noise and details. Many of the existing 3D models are created for visualization purposes, while missing the input quality standard for the skeleton method. The skeleton 3D retrieval method needs more time and effort before it can be used widely.\n\nCompute a feature vector based on statistics\n\nUnlike Skeleton modeling, which requires a high quality standard for the input source, statistical methods do not put restriction on the validity of an input source. Shape histograms, feature vectors composed of global geo-metic properties such as circularity and eccentricity, and feature vectors created using frequency decomposition of spherical functions are common examples of using statistical methods to describe 3D information.\n\n2D projection method \n\nSome approaches use 2D projections of a 3D model, justified by the assumption that if two objects are similar in 3D, then they should have similar 2D projections in many directions. Prototypical Views and Light field description are good examples of 2D projection methods.\n\nIn Purdue University, researchers led by Professor Karthik Ramani at the Research and Education Center for Information created a 3D search engine called the 3D Engineering Search System (3DESS). It is designed to find computer-generated engineering parts.\n\nThe mechanism behind this search engine is that it starts from an algorithm which can transform query drawing to voxels, then extracts the most important shape information from the voxels by using another algorithm called thinning, and formulates a skeleton of the object’s outlines and topology. After that, 3DESS will develop a skeletal graph to render the skeleton, using three common topological constructs: loops, edges, and nodes. The processed common constructs graph can reduce the data amount to represent an object, and it is easier to store and index the description in a database.\n\nAccording to the lead professor, 3DESS can also describe objects using feature vectors, such as volume, surface area, etc. The system processes queries by comparing their feature vectors or skeletal graphs with data stored in the database. When the system retrieves models in response to the query, users can pick whichever object looks more similar to what they want and leave feedback.\n\nChallenges associated with 3D shape-based similarity queries\n\nWith the skeleton modeling 3D retrieval method, figuring out an efficient way to index 3D shape descriptors is very challenging because 3D shape indexing has very strict criteria. The 3D models must be quick to compute, concise to store, easy to index, invariant under similarity transformations, insensitive to noise and small extra features, robust to arbitrary topological degeneracies, and discriminating of shape differences at many scales.\n\n3D search and retrieval with multimodal support challenges\n\nIn order to make the 3D search interface simple enough for novice users who know little on 3D retrieval input source requirements, a multimodal retrieval system, which can take various types of input sources and provide robust query results, is necessary. So far, only a few approaches have been proposed. In Funkhouser et al. (2003), the proposed “Princeton 3D search engine” supports 2D sketches, 3D sketches, 3D models and text as queries. In Chen et al. (2003), he designed a 3D retrieval system that intakes 2D sketches and retrieves for 3D objects. Recently, Ansary et al. (2007) proposed a 3D retrieval framework using 2D photographic images, sketches, and 3D models.\n"}
{"id": "7535306", "url": "https://en.wikipedia.org/wiki?curid=7535306", "title": "Aerobic treatment system", "text": "Aerobic treatment system\n\nAn aerobic treatment system or ATS, often called (incorrectly) an aerobic septic system, is a small scale sewage treatment system similar to a septic tank system, but which uses an aerobic process for digestion rather than just the anaerobic process used in septic systems. These systems are commonly found in rural areas where public sewers are not available, and may be used for a single residence or for a small group of homes.\n\nUnlike the traditional septic system, the aerobic treatment system produces a high quality secondary effluent, which can be sterilized and used for surface irrigation. This allows much greater flexibility in the placement of the leach field, as well as cutting the required size of the leach field by as much as half.\n\nThe ATS process generally consists of the following phases:\n\n\nThe disinfecting stage is optional, and is used where a sterile effluent is required, such as cases where the effluent is distributed above ground. The disinfectant typically used is tablets of calcium hypochlorite, which are specially made for waste treatment systems. The tablets are intended to break down quickly in sunlight. Stabilized forms of chlorine persist after the effluent is dispersed, and can kill plants in the leach field.\n\nSince the ATS contains a living ecosystem of microbes to digest the waste products in the water, excessive amounts of items such as bleach or antibiotics can damage the ATS environment and reduce treatment effectiveness. Non-digestible items should also be avoided, as they will build up in the system and require more frequent sludge removal.\n\nSmall scale aerobic systems generally use one of two designs, fixed-film systems, or continuous flow, suspended growth aerobic systems (CFSGAS). The pre-treatment and effluent handling are similar for both types of systems, and the difference lies in the aeration stage.\n\nFixed film systems use a porous medium which provides a bed to support the biomass film that digests the waste material in the wastewater. Designs for fixed film systems vary widely, but fall into two basic categories (though some systems may combine both methods). The first is a system where the media is moved relative to the wastewater, alternately immersing the film and exposing it to air, while the second uses a stationary media, and varies the wastewater flow so the film is alternately submerged and exposed to air. In both cases, the biomass must be exposed to both wastewater and air for the aerobic digestion to occur. The film itself may be made of any suitable porous material, such as formed plastic or peat moss. Simple systems use stationary media, and rely on intermittent, gravity driven wastewater flow to provide periodic exposure to air and wastewater. A common moving media system is the rotating biological contactor (RBC), which uses disks rotating slowly on a horizontal shaft. Approximately 40 percent of the disks are submerged at any given time, and the shaft rotates at a rate of one or two revolutions per minute.\n\nCFSGAS systems, as the name implies, are designed to handle continuous flow, and do not provide a bed for a bacterial film, relying rather on bacteria suspended in the wastewater. The suspension and aeration are typically provided by an air pump, which pumps air through the aeration chamber, providing a constant stirring of the wastewater in addition to the oxygenation. A medium to promote fixed film bacterial growth may be added to some systems designed to handle higher than normal levels of biomass in the wastewater.\n\nAnother increasingly common use of aerobic treatment is for the remediation of failing or failed anaerobic septic systems, by retrofitting an existing system with an aerobic feature. This class of product, known as aerobic remediation, is designed to remediate biologically failed and failing anaerobic distribution systems by significantly reducing the biochemical oxygen demand (BOD5) and total suspended solids (TSS) of the effluent. The reduction of the BOD5 and TSS reverses the developed bio-mat. Further, effluent with high dissolved oxygen and aerobic bacteria flow to the distribution component and digest the bio-mat.Doing so on single tank systems where solids do not have anywhere to settle, or there is no a clarifying area can do damage to the field lines as the solid matter is stirred up in the tank.\n\nComposting toilets are designed to treat only toilet waste, rather than general residential waste water, and are typically used with water-free toilets rather than the flush toilets associated with the above types of aerobic treatment systems. These systems treat the waste as a moist solid, rather than in liquid suspension, and therefore separate urine from feces during treatment to maintain the correct moisture content in the system. An example of a composting toilet is the clivus multrum (Latin for 'inclined chamber'), which consists of an inclined chamber that separates urine and feces and a fan to provide positive ventilation and prevent odors from escaping through the toilet. Within the chamber, the urine and feces are independently broken down not only by aerobic bacteria, but also by fungi, arthropods, and earthworms. Treatment times are very long, with a minimum time between removals of solid waste of a year; during treatment the volume of the solid waste is decreased by 90 percent, with most being converted into water vapor and carbon dioxide. Pathogens are eliminated from the waste by the long durations in inhospitable conditions in the treatment chamber.\n\nThe aeration stage and the disinfecting stage are the primary differences from a traditional septic system; in fact, an aerobic treatment system can be used as a secondary treatment for septic tank effluent. These stages increase the initial cost of the aerobic system, and also the maintenance requirements over the passive septic system. Unlike many other biofilters, aerobic treatment systems require a constant supply of electricity to drive the air pump increasing overall system costs. The disinfectant tablets must be periodically replaced, as well as the electrical components (air compressor) and mechanical components (air diffusers). On the positive side, an aerobic system produces a higher quality effluent than a septic tank, and thus the leach field can be smaller than that of a conventional septic system, and the output can be discharged in areas too environmentally sensitive for septic system output. Some aerobic systems recycle the effluent through a sprinkler system, using it to water the lawn where regulations approve.\n\nSince the effluent from an ATS is often discharged onto the surface of the leach field, the quality is very important. A typical ATS will, when operating correctly, produce an effluent with less than 30 mg/liter BOD5, 25 mg/L TSS, and 10,000 cfu/mL fecal coliform bacteria. This is clean enough that it cannot support a biomat or \"slime\" layer like a septic tank.\n\nATS effluent is relatively odorless; a properly operating system will produce effluent that smells musty, but not like sewage. Aerobic treatment is so effective at reducing odors, that it is the preferred method for reducing odor from manure produced by farms.\n\n\n"}
{"id": "7815656", "url": "https://en.wikipedia.org/wiki?curid=7815656", "title": "Archive Corp.", "text": "Archive Corp.\n\nArchive Corporation was a computer tape drive manufacturer, based in Costa Mesa, California that was acquired by Conner Peripherals in 1993. \n\nOf particular note are the Archive DDS tape drives produced for Silicon Graphics that could also read and write Digital Audio Tapes.\n\nPrior to this, Archive was a leading vendor of the very popular QIC (Quarter-inch cartridge) format which was a popular distribution format for UNIX workstations and servers. For example, Sun-3 (Motorola 68k family) and Sun-4 (SPARC) software was most commonly distributed on QIC media before the CD-ROM became more cost-effective. Archive was probably better known for their QIC drives.\n\n\n"}
{"id": "18962267", "url": "https://en.wikipedia.org/wiki?curid=18962267", "title": "Axe", "text": "Axe\n\nAn axe (British English) or ax (American English; see spelling differences) is an implement that has been used for millennia to shape, split and cut wood; to harvest timber; as a weapon; and as a ceremonial or heraldic symbol. The axe has many forms and specialised uses but generally consists of an axe head with a handle, or \"helve\".\n\nBefore the modern axe, the stone-age hand axe was used from 1.5 million years BP without a handle. It was later fastened to a wooden handle. The earliest examples of handled axes have heads of stone with some form of wooden handle attached (hafted) in a method to suit the available materials and use. Axes made of copper, bronze, iron and steel appeared as these technologies developed. Axes are usually composed of a head and a handle.\n\nThe axe is an example of a simple machine, as it is a type of wedge, or dual inclined plane. This reduces the effort needed by the wood chopper. It splits the wood into two parts by the pressure concentration at the blade. The handle of the axe also acts as a lever allowing the user to increase the force at the cutting edge—not using the full length of the handle is known as choking the axe. For fine chopping using a side axe this sometimes is a positive effect, but for felling with a double bitted axe it reduces efficiency.\n\nGenerally, cutting axes have a shallow wedge angle, whereas splitting axes have a deeper angle. Most axes are double bevelled, i.e. symmetrical about the axis of the blade, but some specialist broadaxes have a single bevel blade, and usually an offset handle that allows them to be used for finishing work without putting the user's knuckles at risk of injury. Less common today, they were once an integral part of a joiner and carpenter's tool kit, not just a tool for use in forestry. A tool of similar origin is the billhook.\n\nMost modern axes have steel heads and wooden handles, typically hickory in the US and ash in Europe and Asia, although plastic or fibreglass handles are also common. Modern axes are specialised by use, size and form. Hafted axes with short handles designed for use with one hand are often called hand axes but the term hand axe refers to axes without handles as well. Hatchets tend to be small hafted axes often with a hammer on the back side (the poll). As easy-to-make weapons, axes have frequently been used in combat.\n\nInitially axes were tools of stone called hand axes, used without handles (hafts), and had knapped (chipped) cutting edges of flint or other stone. Stone axes made with \"ground\" cutting edges were first developed sometime in the late Pleistocene in Australia, where ground-edge axe fragments from sites in Arnhem Land date back at least 44,000 years; ground-edge axes were later invented independently in Japan sometime around 38,000 BP, and are known from several Upper Palaeolithic sites on the islands of Honshu and Kyushu. In Europe, however, the innovation of ground edges occurred much later, in the Neolithic period ending 4,000 to 2,000 BC. The first true hafted axes are known from the Mesolithic period (c. 6000 BC). Few wooden hafts have been found from this period, but it seems that the axe was normally hafted by wedging. Birch-tar and raw-hide lashings were used to fix the blade.\n\nSometimes a short section of deer antler (an \"antler sleeve\") was used, which prevented the splitting of the haft and softened the impact on the stone blade itself, helping absorb the impact of each axe blow and lessening the chances of breaking the handle. The antler was hollowed out at one end to create a socket for the axehead. The antler sheath was then either perforated and a handle inserted into it or set in a hole made in the handle instead.\n\nThe distribution of stone axes is an important indication of prehistoric trade. Thin sectioning is used to determine the provenance of the stone blades. In Europe, Neolithic \"axe factories\", where thousands of ground stone axes were roughed out, are known from many places, such as:\n\n\nStone axes are still produced and in use today in parts of Papua, Indonesia. The Mount Hagen area of Papua New Guinea was an important production centre.\n\nFrom the late Neolithic/Chalcolithic onwards, axes were made of copper or copper mixed with arsenic. These axes were flat and hafted much like their stone predecessors. Axes continued to be made in this manner with the introduction of Bronze metallurgy. Eventually the hafting method changed and the flat axe developed into the \"flanged axe\", then palstaves, and later winged and socketed axes.\n\nThe Proto-Indo-European word for \"axe\" may have been \"*pelek'u-\" (Greek \"pelekus\" πέλεκυς, Sanskrit \"parashu\", see also Parashurama), but the word was probably a loan, or a Neolithic \"wanderwort\", ultimately related to Sumerian \"balag\", Akkadian \"pilaku-\".\n\nAt least since the late Neolithic, elaborate axes (battle-axes, T-axes, etc.) had a religious significance and probably indicated the exalted status of their owner. Certain types almost never show traces of wear; deposits of unshafted axe blades from the middle Neolithic (such as at the Somerset Levels in Britain) may have been gifts to the deities.\nIn Minoan Crete, the double axe (labrys) had a special significance, used by priestesses in religious ceremonies. The symbol refers to deification ceremonies; part of the leaping over the bull symbol also found at Crete; whereby aspirant becomes able to speak as a god to create any reality; the symbol being a sky map.\n\nIn 1998 a labrys, complete with an elaborately embellished haft, was found at Cham-Eslen, Canton of Zug, Switzerland. The haft was 120 cm long and wrapped in ornamented birch-bark. The axe blade is 17.4 cm long and made of antigorite, mined in the Gotthard-area. The haft goes through a biconical drilled hole and is fastened by wedges of antler and by birch-tar. It belongs to the early Cortaillod culture.\n\nIn folklore, stone axes were sometimes believed to be thunderbolts and were used to guard buildings against lightning, as it was believed (mythically) that lightning never struck the same place twice. This has caused some skewing of axe distributions.\n\nSteel axes were important in superstition as well. A thrown axe could keep off a hailstorm, sometimes an axe was placed in the crops, with the cutting edge to the skies to protect the harvest against bad weather. An upright axe buried under the sill of a house would keep off witches, while an axe under the bed would assure male offspring.\n\nBasques, Australians and New Zealanders have developed variants of rural sports that perpetuate the traditions of log cutting with axe. The Basque variants, splitting horizontally or vertically disposed logs, are generically called \"aizkolaritza\" (from \"aizkora\": axe).\n\nIn Yorùbá mythology, the oshe (double-headed axe) symbolises Shango, Orisha (god) of thunder and lightning. It is said to represent swift and balanced justice. Shango altars often contain a carved figure of a woman holding a gift to the god with a double-bladed axe sticking up from her head.\n\nThe Arkalochori Axe is a bronze, Minoan, axe from the second millennium BC thought to be used for religious purposes. Inscriptions on this axe have been compared with other ancient writing systems.\n\nThe axe has two primary components: the axe \"head\", and the \"haft\".\n\nThe \"axe head\" is typically bounded by the \"bit\" (or blade) at one end, and the \"poll\" (or butt) at the other, though some designs feature two bits opposite each other. The top corner of the bit where the cutting edge begins is called the \"toe\", and the bottom corner is known as the \"heel\". Either side of the head is called the \"cheek\", which is sometimes supplemented by \"lugs\" where the head meets the haft, and the hole where the haft is mounted is called the \"eye\". The part of the bit that descends below the rest of the axe-head is called the beard, and a \"bearded axe\" is an antiquated axe head with an exaggerated beard that can sometimes extend the cutting edge twice the height of the rest of the head.\n\nThe \"axe haft\" is sometimes called the handle. Traditionally, it was made of a resilient hardwood like hickory or ash, but modern axes often have hafts made of durable synthetic materials. Antique axes and their modern reproductions, like the tomahawk, often had a simple, straight haft with a circular cross-section that wedged onto the axe-head without the aid of wedges or pins. Modern hafts are curved for better grip and to aid in the swinging motion, and are mounted securely to the head. The \"shoulder\" is where the head mounts onto the haft, and this is either a long oval or rectangular cross-section of the haft that is secured to the axe head with small metal or wooden wedges. The \"belly\" of the haft is the longest part, where it bows in gently, and the throat is where it curves sharply down to the short \"grip\", just before the end of the haft, which is known as the \"knob\".\n\n\n\n\nHammer axes (or axe-hammers) typically feature an extended poll, opposite the blade, shaped and sometimes hardened for use as a hammer. The name axe-hammer is often applied to a characteristic shape of perforated stone axe used in the Neolithic and Bronze Ages. Iron axe-hammers are found in Roman military contexts, e.g. Cramond, Edinburgh, and South Shields, Tyne and Wear.\n\n\n\n\n\n\n"}
{"id": "3594475", "url": "https://en.wikipedia.org/wiki?curid=3594475", "title": "Barro negro pottery", "text": "Barro negro pottery\n\nBarro negro pottery (\"black clay\") is a style of pottery from Oaxaca, Mexico, distinguished by its color, sheen and unique designs. Oaxaca is one of few Mexican states which is characterized by the continuance of its ancestral crafts, which are still used in everyday life. Barro negro is one of several pottery traditions in the state, which also include the glazed green pieces of Santa María Atzompa; however, barro negro is one of the best known and most identified with the state. It is also one of the most popular and appreciated styles of pottery in Mexico. The origins of this pottery style extends as far back as the Monte Alban period and for almost all of this pottery's history, had been available only in a grayish matte finish. In the 1950s, a potter named Doña Rosa devised a way to put a black metallic like sheen onto the pottery by polishing it before firing. This look has made the pottery far more popular. From the 1980s to the present, an artisan named Carlomagno Pedro Martínez has promoted items made this way with barro negro sculptures which have been exhibited in a number of countries.\n\nThe origins of barro negro pottery extend over centuries, with examples of it found at archeological sites, fashioned mostly into jars and other utilitarian items. It has remained a traditional crafts of the Zapotecs and Mixtecs of the Central Valleys area to the present day. Originally barro negro pottery was matte and grayish. In this form, the pottery is very sturdy, allowing it to be hit without breaking.\n\nIn the 1950s, Doña Rosa Real discovered that she could change the color and shine of the pieces by polishing the clay pieces and firing them at a slightly lower temperature. Just before the formed clay piece is completely dry, it is polished with a quartz stone to compress the surface. After firing, the piece emerges a shiny black instead of a dull gray. This innovation makes the pieces more breakable, but it has made the pottery far more popular with Mexican folk art collectors, including Nelson Rockefeller, who promoted it in the United States. The popularity stems from the look, rather than wearability, so many pieces are produced now for decorative purposes rather than utility. Doña Rosa died in 1980, but the tradition of making the pottery is being carried on by her daughter and grandchildren who stage demonstrations for tourists in their local potters' workshop. The workshop is still in the family home, where shelves and shelves of shiny black pieces for sale line the inner courtyard. Despite being the origin of black polished clay, pieces at this workshop are much cheaper than in other parts of Mexico.\n\nAnother important person in the development and promotion of barro negro is Carlomagno Pedro Martinez. He was born in San Bartolo Coyotepec into a pottery-making family. He was named after Charlemagne by his grandmother, who was an admirer of the king. From a young age, he showed talent in fashioning figures in clay. When he was grown, he attended the Fine Arts Workshop of Rufino Tamayo in Oaxaca city. He has become the first potter/sculptor in the medium, winning his first recognition in 1985 for his work. His fame increased with his development of human skulls made of barro negro in the years that followed. Each piece Carlomagno makes is unique, but themes such as oral histories, indigenous legends, Christianity and death, called \"our grandmother,\" recur. In Mexico, he has exhibited his work in dozens of expositions and has won three national-level awards. His work has been featured in five books. Martinez’s work has been exhibited in the United States, Colombia, Argentina, Lebanon, Germany, Spain, and Japan, with one of his latest exhibits in New York in 2008. In that same year, he created a mural in barro negro at the Baseball Academy in San Bartolo Coyotepec sponsored by the Alfredo Harp Helú Foundation.\n\nThe craft is made in San Bartolo Coyotepec and a large number of small communities in the surrounding valley, where the clay that gives it its color is found. This community is located south of the city of Oaxaca, with about 600 families in the area dedicated to the craft. In addition to a number of family workshops, including Doña Rosa's, the Mercado de Artesanias in an important attraction which brings visitors from many parts of Mexico and other countries. A group of fourteen people exhibit and sell barro negro objects. Some of these products include vases, animal figures and jars. Demonstrations of pottery making are held as well. In 2005, the Museo Estatal de Arte Popular de Oaxaca(State Museum of Popular Art) was opened here. It has one of its three halls dedicated to barro negro, with pieces from the Monte Alban era to the present day. In 2009, San Bartolo Coyotepec held its first Feria Artesanal de Barro Negro (Crafts Fair of Barro Negro) with the participation of over 150 artisans.\n\nMany different kinds of objects are made of barro negro including pots, whistles, flutes, bells, masks, lamps, animal figures with most being of a decorative nature and not for the storage of food and water. One exception to this is the use of cantaros from San Bartolo Coyotepec to age and store mezcal at many distilleries. These large jars are not polished and retain the ancient gray matte, which allows them to be resistant to liquid. Another quality the gray matte version has is that it can be struck similar to a bell, and the cantaros are also used as musical instruments. The sound produced is crystalline.\n\nAnother famous barro negro object is the \"mezcal monkey\" (chango mezcalero). This is a vessel created for the alcoholic liquor mezcal in the shape of a monkey. It is made to hold between 700 ml and 1 liter of the liquid with a cork or corncob stopper. It is either painted in bright colors or left grayish with detailed etchings. Valente Nieto, the sole surviving progeny of Doña Rosa states that his family created the mezcal monkey and no one else. He claims that his father was a gifted sculptor, and mezcal owners came to their property requesting novelty bottles for the alcoholic beverage. The monkey as well as other animal shapes were created. However, another family, that of Marcelo Simon Galan, also claim to have created the container. His surviving granddaughter says that he made the shape at the request of a customer. An example of Galan’s work is on display at the Museo de Arte Popular de Oaxaca in San Bartolo Coyotepec.\n\nThe color of barro negro is due to the properties of the clay, and is not colored. The earth used to extract the clay is cleaned to remove impurities, which can take a month of soaking and settling out the clay from the rest of the soil. After this process, each piece takes about twenty days to complete.\n\nTraditionally, the clay is molded on plates balanced on rocks to that they can be spun by hand. Modern potters’ tools are not used. Large pieces, such as cantaros are fashioned from the bottom up adding clay as the piece grows. After it is shaped, the pieces are set to dry in a well-insulated room to protect them from sudden changes in temperature. Drying can take up to three weeks. If the piece is to be polished so that it turns out shiny black when finished, it is polished when the piece is almost dry. The surface of the piece is lightly moistened and then rubbed with a curved quartz stone. This compacts the surface of the clay and creates the metallic sheen and dark color during firing. This is also the stage when decorative accents such as clay flowers or small handles are added. The designs of barro negro objects are unique to this area.\n\nThe pieces are then fired in underground pits or above ground kilns, using wood fires that heat the objects to between 700 and 800 °C. When they emerge, the polished pieces are a shiny black and the unpolished ones have a grey matte finish.\n"}
{"id": "10682567", "url": "https://en.wikipedia.org/wiki?curid=10682567", "title": "Bead breaker", "text": "Bead breaker\n\nInvented in 1984 by David Allen Dayton. A bead breaker is a tool used for separating tires from rims. The inner-most diameter of the tire that interfaces with a wheel is called the Tire Bead. The tire bead is a thicker section of rubber, and is reinforced with braided steel cable, called the Bead Bundle. The inner-most surface of the tire bead creates the air seal between the tire and rim on a radial-ply tire and a bias-ply tire. Often, the bead can become frozen to the rim after rusting or corrosion occurs, requiring the use of a bead breaker in order to be removed.\n\nA need was experienced by avid 4x4 enthusiasts and overland travelers for a simple tool to effectively and efficiently remove the tire from the rim of a wheel, in cases where a tire requires repairs to the inside. On ATV's, as well as motorcycles, passenger vehicles, trucks, many industrial and offroad vehicles, as well as light aircraft have an additional feature on the rim called the Bead Retainer. The bead retainer is a bump like feature that prevents the tire bead from slipping inward on the rim and losing the air seal. On ATV's (and UTV's/side-by-sides) this bead retainer is substantially large because these tires are often run at very low air pressures (~5 psi) and experience severe side loading forces from typical trail riding. The large bead retainer is necessary to prevent unintentional dislocation of the tire bead from the bead seat. Further, if an ATV tire does lose pressure from a puncture, the large bead retainer will keep the Tire in place for low speed riding allowing the operator to navigate to the trail head without the tire coming completely off of the rim. The large bead retainers of ATV's do such a good job keeping the tire in place during operating conditions, it also prevents the tire from being easily changed with normal tools even in a shop setting.\n\nThere are three categories of tire bead breaker designs: 1) \"Shoe and Lever mechanism\", 2) \"Plier-type mechanism\", 3) \"Clamp and Ram mechanism\". The Shoe and Lever mechanism performs well on many motorcycle tires, passenger vehicle tires and trailers, however with more difficult-to-change tires it often fails because of the nature of the design. The shoe pushes down on the tire sidewall next to the tire bead, and if the bead is stuck, the shoe will simply slide down the sidewall. Further the Shoe and Lever mechanism is quite large, and is not a good portable solution for on-the-trail tire changes. Advantages are typically low cost, often less than $50, and can be found manufactured by many companies. The Plier-Type tool has a spike that pushes in between the tire bead and lip of the rim, and can actually put a separating force on the bead bundle. However the pivot point results in an arc that quickly transitions to pushing on the sidewall. The Plier-type mechanism is more effective than the Shoe and Lever, however it has some disadvantages. It must be adjusted for different diameter wheels, it makes metal on metal contact with the rim, and can easily scratch and mar the surface of aluminum or painted wheels, it takes a bit of coordination and balance to use as you have to stand on the tire and use your body weight to operate the tool, and it is also fairly large, but ultimately is portable and is often used at race tracks for pit-stop tire changes. Ultimately, this type of bead breaker also struggles to work reliably on very stubborn ATV tires, 4x4 trucks, and tractors. The cost of the Plier-type is variable, form $75 to $200, and is manufactured by multiple companies. Lastly, the Clamp-and-Ram mechanism tool is by far the most effective and compact tool for breaking beads on ATV's and other difficult-to-change tires. This style of tool has a pointed foot that enters the space between the tire bead and lip of the rim, and it uses a padded clamp that is driven with a wrench to get pulled in all the way with very little user effort. The Ram foot then gets actuated by another bolt, with little effort by the user, and it completely breaks the bead loose in one motion. Because the tool is clamped in place, it cannot slide down the sidewall and damage the tire. The ram foot pushes directly on the steel bead bundle of the tire, and therefore works well on old and deteriorated tires. \n\nTire changers have a semi-automated bead loosening system for removing tires, but due to the high cost and lack of portability these are were not suitable. In addition beads frozen by heat and rust to the rim must often be broken free manually. In these case the bead breaker is ideal. Used like a chisel, a bead breaker leverages a mechanical advantage to drive the bead away from the rim.\n"}
{"id": "4724890", "url": "https://en.wikipedia.org/wiki?curid=4724890", "title": "Big bang adoption", "text": "Big bang adoption\n\nBig bang adoption or direct changeover is the adoption type of the instant changeover, when everybody associated with the old system moves to the fully functioning new system on a given date.\n\nWhen a new system needs to be implemented in an organization, there are three different ways to adopt this new system: the big bang adoption, phased adoption and parallel adoption. In case of parallel adoption the old and the new system are running parallel, so all the users can get used to the new system, and meanwhile do their work using the old system. Phased adoption means that the adoption will happen in several phases, so after each phase the system is a little nearer to be fully adopted. With the big bang adoption, the switch between using the old system and using the new system happens at one single date, the so-called instant changeover of the system. Everybody starts to use the new system at the same date and the old system will not be used anymore from that moment on.\n\nThe big bang adoption type is riskier than other adoption types because there are fewer learning opportunities incorporated in the approach, so more preparation is needed to get to the big bang. This preparation will be described below, illustrated by the process-data model of the big bang adoption.\n\nSeveral concepts are used in this entry. The definitions of these concepts are given in the table below to make the use of them clear.\n\nOnce the management has decided to use the big bang method, and supports the changes which are needed for this, the real changing process can start. This process comprises several steps: converting the system, releasing parts of the system and training the future users.\n\nThe activities in the process are explained in the table below, to state them clearly. The concepts that are used to execute the activities are in capitals.\n\nAt first, a planning for the whole adoption process is needed. Making a planning allows future users to know what will happen and when they should expect certain changes, which avoids unnecessary uncertainties and therefore creates a better working atmosphere. The planning also makes clear when the real adoption takes place and gives the future users the opportunity to get ready for this change. The model below shows that the activities (in the grey box) lead to outcomes (in the boxes next to the grey box) to be able to have a partial outcome: the converted system\n\nWhen the planning is made and everyone knows what is expected from them, the technical changeover can start. First the old data needs to be converted into a form which is able to work with the data in the new system (Koop, Rooimans and de Theye, 2003). Then this data needs to be loaded into the new system, which results in the so-called loaded data. This loaded data needs to be tested to check the efficiency of the data and to test the level of understanding of the future users. Off-line trials need to be executed to check whether the system and the users can work together. Not only do the efficiency and the understanding need to be tested, but the validity needs to be tested to make the level of data validation clear. ). If the data is not valid, the management need to determine the changes again and the organisation will have to prepare a different way of executing the Big bang adoption (See Adoption; Prepare an organization for adoption).\n\nIf all the data is valid, separate parts of the system can be released. The database which is converted from the old database needs to be released, so the new data is accessible. Next, the produced application needs to be released, so the new application can also be used. The infrastructure of the whole new system also needs to be released, so that it is clear what the system will look like and how everything is connected (Koop, Rooimans and de Theye, 2003). Important to note is that in this phase only separate parts are released, which don’t form the new system yet, but only parts of it. Note that all of this happens off-line: only the system developers see this, the users are still working on the old system. The model above shows what activities need to be executed (in the grey box) by the system controller, to get the outcomes that lead to the released parts. If the release of the parts failed, the management need to determine new changes again (See Adoption; Prepare an organization for adoption).\n\nIf the release of the separate parts succeeded, the next step can be taken: prepare the users. To be able to introduce the whole new system, i.e. to adopt it, all users need to be trained in working with the new system. Without huge consequences for the production level of an organization, training everyone is only possible if there is a buffer of experienced staff who can take over the daily work of the users that need to be trained. This means that for all the people that need to be trained, there will be staff available who can take over the work, so there won’t be an enormous delay of work. When this buffer is created, the users can be trained . The human resources department will create the buffer of experienced staff (activity in the grey box) by inviting applicants for the buffer. Then the users can be trained and the trained users can be listed, so a user preparation report can be written.\n\nBut training the future users properly is not as easy as it seems, as the FoxMeyer case illustrates (Scott, Vessey, 2000). This company used the big bang method to implement an Enterprise Resource Planning (ERP) system. Wrong trainings were given, the assumption was made that users already knew enough about it and wrong skills were taught. Dow Corning also had big problems with acquiring the necessary skills during their big bang ERP implementation (Scott, Vessey, 2000). Using a new system demands various skills and knowledge, which in several cases seem to be underrated by the (change) managers.\n\nThere are several techniques to implement a new system. The adoption phase is only one phase of the whole implementation. Regatta (Koop, Rooimans and de Theye, 2003) is for example a method which is developed to implements systems. This method, developed by Sogeti, treats a changeover as a project and focuses several stages of this project, for example the preparation phase () of an adoption and on the acceptance of an implementation method (). SAP Implementation is another technique specialized in implementing and adopting SAP AG software, which is divided into several techniques.\n\nBecause of the instant changeover, everything must be done in a fixed time schedule. This is a risky operation. The organization might not be ready yet for this, an incorrect dataset might be used, or the information system can get stuck, because of a lack of experience and start up problems. Also an incapable fall-back method can be a risk in implementing a system using the Big Bang (Koop, Rooimans and de Theye, 2003).\n\nThe 1986 the London Stock Exchange closed on Friday night and the computers were all switched on the following Monday morning. It has been alleged that this caused large losses.\n\nDow Corning formerly used systems that were focused on specific departments. The management decided that they wanted to become a truly global company, that would use only one information system: an Enterprise Resource Planning (ERP)-system. To adopt this new ERP-system, they used the big bang adoption type and they spent considerable time and effort reexamining its business processes. The company was prepared for the adoption and first conducted three pilot implementations, before using the new system across the global organization. Second, FoxMeyer adopted an ERP-system with ambitious warehouse automation software, using the big bang adoption to gain competitive advantage. But FoxMeyer seemed to have an overoptimistic management with unrealistic expectations: the change was too big and too drastic. This resulted in very high work pressure to meet the deadlines for all the employees. So unrealistic expectations of the management are also a risk (Scott, Vessy, 2000).\n\nDow Corning monitored the progress constantly and made decisions to make sure that the deadlines would be met. This was only possible with feedback and good communication. FoxMeyer failed in having communication and attention that was necessary to be able to give fast and effective feedback. They instead tried to minimize problems by ignoring them, and gave discouraging criticism, which resulted in ambiguous feedback. This hindered organizational learning, something which is very important during an organizational change. So bad communication and ambiguous feedback are also risks when adopting a system with the big bang (Scott, Vessey, 2000).\n\nAnother risky strategy is to focus only on the outcome, not on how to achieve this outcome and underrating the learning process for users. It is very hard to plan learning or knowledge, though these are necessary to be able to execute the big bang changeover.\n\n\n"}
{"id": "2246651", "url": "https://en.wikipedia.org/wiki?curid=2246651", "title": "Blow fill seal", "text": "Blow fill seal\n\nBlow-Fill-Seal (BFS) technology is a manufacturing technique used to produce small, (0.1mL) and large volume, (500mL +) liquid-filled containers. Originally developed in Europe in the 1930s, it was introduced in the United States in the 1960s, but over the last 20 years it has become more prevalent within the pharmaceutical industry and is now widely considered to be the superior form of aseptic processing by various medicine regulatory agencies including the U.S. Food and Drug Administration (FDA) in the packaging of pharmaceutical and healthcare products. \n\nThe basic concept of BFS is that a container is formed, filled, and sealed in a continuous process without human intervention, in a sterile enclosed area inside a machine. Thus this technology can be used to aseptically manufacture sterile pharmaceutical liquid dosage forms. \n\nThe process is multi-stepped: first, pharmaceutical-grade plastic resin is vertically heat extruded through a circular throat to form a hanging tube called the parison. This extruded tube is then enclosed within a two-part mould, and the tube is cut above the mould. The mould is transferred to the filling zone, or sterile filling space, where filling needles (mandrels) are lowered and used to inflate the plastic to form the container within the mould. Following the formation of the container, the mandrel is used to fill the container with liquid. Following filling the mandrels are retracted and a secondary top mould seals the container. All actions take place inside a sterile shrouded chamber inside the machine. The product is then discharged to a non-sterile area for labeling, packaging and distribution. \n\nBlow-fill-seal technology reduces personnel intervention making it a more robust method for the aseptic preparation of sterile pharmaceuticals. BFS is used for the filling of vials for parenteral preparations and infusions, eye drops, and inhalation products. Generally the plastic containers are made up of polyethylene and polypropylene. Polypropylene is more commonly used to form containers which are further sterilised by autoclaving as polypropylene has greater thermostability.\n\n\n"}
{"id": "1259108", "url": "https://en.wikipedia.org/wiki?curid=1259108", "title": "Blu-ray Disc Association", "text": "Blu-ray Disc Association\n\nThe Blu-ray Disc Association (BDA) is the industry consortium that develops and licenses Blu-ray Disc technology and is responsible for establishing format standards and promoting business opportunities for Blu-ray Disc. The BDA is divided into three levels of membership: the Board of Directors, Contributors, and General Members.\n\nThe \"Blu-ray Disc founder group\" was started on 20 May 2002 by MIT and nine leading electronic companies: Sony, Panasonic, Pioneer, Philips, Thomson, LG Electronics, Hitachi, Sharp, and Samsung Electronics.\nIn order to enable more companies to participate, it announced in May 2004 that it would form the Blu-ray Disc Association, which was inaugurated on 4 October 2004.\n\nThe Blu-ray Disc Association website describes the role of the Board of Directors as follows: \"Companies participating in the Board of Directors are active participants in the format creation and key BDA activities. They are selected from the Contributors by-election. The board sets an overall strategy and approves key issues. A board member can participate in all activities and attend all meetings. The Blu-ray Disc Founder companies will make up the initial Board of Directors.\"\n\nThe current 20 board members (as of November 2016) are the following:\n\n\nThe role of contributors as described by the Blu-ray Disc Association website:\n\"Contributors are active participants in the format creation and other key BDA activities. They can be elected to become a member of the Board of Directors. A contributor can attend general meetings and seminars, and can participate in Technical Expert Groups (TEGs), regional Promotion Team activities and most of the Compliance Committee (CC) activities. Membership requires execution of Contribution Agreement and must be approved by the Board of Directors.\nAnnual fee: $ 20,000\"\n\nThe contributors as of December 2017 are:\n\n\n\n"}
{"id": "28917701", "url": "https://en.wikipedia.org/wiki?curid=28917701", "title": "Boku mobile payments", "text": "Boku mobile payments\n\nBoku Inc. provides a mobile payments platform headquartered in San Francisco, CA, enabling consumers to pay for goods and services using their mobile phone number, with purchases billed to a consumer's mobile network operator bill. This form of mobile payment, also referred to as carrier billing, allows unbanked consumers who may have a mobile phone, but no credit card or traditional bank account, to make purchases online, as well as banked customers who simply find carrier billing a simpler way to transact.\nThe majority of transactions processed through Boku carrier billing are for digital and virtual goods and services, including music streaming, online video, digital subscription services, mobile device app stores, social and free-to-play games, virtual goods, social networks and other online experiences. Boku focuses on making the mobile phone number a viable payment option not only for virtual goods, but for digital goods (i.e. MP3's, eBooks, movies, etc.) and physical goods.\n\nOriginally called Vidicom Ltd, and founded in Chesterfield, Derbyshire, U.K. in 2003 by Glyn Smith, Thomas Kirk and original angel investors Dr Steven Gillam, Philip Meakin and Chris Harris. A small number of investors joined before Vidicom Ltd was convinvced by non exec board Chairman Mark Berntstein to seek funding from venture capitalists in Silicon Valley for its application \"Mobillcash\". Vidicom succsefully secured funding from Khosla Ventures (US) and Index Ventures (UK) Marc Britto and Ron Hirson's services were secured and Vidicom Ltd incorporated Boku inc as a Delaware company in 2009 and reestablished itself as Boku Inc. Boku launched in the USA in 2009 with the internal acquisition of Mobilcash and new acquisition of Paymo – a provider in the mobile payments industry – as well as $13 million USD in investment from venture capital firms.\n\nA year later, Boku added $25 million USD in a series C round, led by DAG Ventures.\n\nIn 2012, Boku raised another $35 million USD round to be used to continue building out its payments business globally, led by NEA and existing venture capital investors.\n\nThe last round, $13.75 million USD was completed in 2016 and led by a consortium of UK investors and GMO Payment Gateway to fund carrier connections for global expansion. \nIn total, Boku has raised more than $91M in venture capital funding from Benchmark Capital, Index Ventures, Khosla Ventures, DAG Ventures, Andreessen Horowitz, New Enterprise Associates, Telefónica, and GMO Payment Gateway.\n\nBoku went public at the London Stock exchange (AIM) on November 20, 2017 with a valuation of 125 GBP Million. \n\nBoku is connected to more than 173 carriers and has launched in 50+ markets offering a bank grade payment system to the largest global companies.\n\nIn November 2013, Boku announced the acquisition of Qubecell, India’s largest aggregator in carrier billing to grow India as its engineering and technology hub.\n\nIn 2015, Boku acquired Mobileview to strengthen its footprint in Italy, and Germany-based Mopay, one of its biggest competitors.\n\nAs of September 2016, Boku has offices in San Francisco, US (Americas HQ), London, UK (EMEA headquarters), Singapore (APAC headquarters) and several other countries (India, Germany, France, China, Taiwan, Japan, Italy, Latvia, Brazil)\n\nBoku is the largest independent carrier billing company in the world. The platform is used by digital marketplaces including Apple Inc., Google Play Store, Sony PlayStation Store, Windows Store, Facebook App Center, and Spotify. \nIn addition, Boku has established partnerships with game publishers and online services. Many digital and gaming companies enable Boku's service, including Electronic Arts, Riot Games, Tencent, IGG, King.com, Bigpoint, Goodgame Studios among others. \nLeading payment service providers (Adyen, Ingenico, Optimal Payments, Xsolla, and others) choose to offer Boku to their clients as alternative payment option.\n\nBoku provides businesses with the ability to put charges on end users’ phone bill. While the original payment panel product is still in place, Boku has expanded its capabilities to credit card like functionalities like charge and refund to enable cross platform purchases. \nEnhanced features, such as phone on file or header enrichment, offer consumers and merchants a more convenient way to pay.\nBoku also offers a subscription product, which gives merchants greater control and flexibility compared to existing market solutions.\n\nIn 2011, Mobile Trax awarded Boku their Mobility Award. In 2011 and 2013, Forbes selected Boku for their 25 most promising companies in America list. In 2014, Boku won the “Best Alternative Payments” award at 2014 Payment Awards\n\n"}
{"id": "29102432", "url": "https://en.wikipedia.org/wiki?curid=29102432", "title": "Bottom hole assembly", "text": "Bottom hole assembly\n\nA bottom hole assembly (BHA) is a component of a drilling rig. It is the lowest part of the drill string, extending from the bit to the drill pipe. The assembly can consist of drill collars, subs such as stabilizers, reamers, shocks, hole-openers, and the bit sub and bit. \n\nThe characteristics of the BHA help to determine the borehole shape, direction and other geometric characteristics.\n\nThe BHA is used to help the drilling process; the proper selection of the right BHA would go a long way in ensuring high rate of penetration (ROP) and thus help drill quickly and efficiently. This would lead to lowered drilling costs.\n\nRotary assemblies are commonly used where formations are predictable and the rig economics are an issue. In such an assembly the weight of the drill collars gives the BHA the tendency to sag or flex to the low side of the hole, collar stiffness length and stabilizer diameter and placement are engineered as a means of controlling the flex of the BHA. this will bring about the desired hold, build or drop tendency. \n\nThe ability to vary the directional tendency of the assembly comes from varying the weight on the bit. A fixed assembly has only one directional tendency. The weight on bit allows you to tune that tendency. \n\nThe bottom-hole assembly (BHA) can be:\n\nBottom-hole assemblies are also described as\n\nThis assembly is used to build angle. The assembly usually incorporates a near bit stabilizer. Behind this would be a selection of drill collars and Heavy Weight Drill Pipe (HWDP). The length of the section AFTER the near bit stabilizer would determine the extent of the angle build rate. \n\nAfter the appropriate length of pipe after the near bit stabilizer you would place a drill string stabilizer. \n\nIn short the longer the gap between the near bit and the drill string stabilizer the greater the angle building rate. Care must be taken to not have this section too long as the pipe may sag too much and rub against the borehole wall. This could result in key seating and pipe digging into the borehole wall.\n\nThis assembly is usually used to maintain borehole angle. This assembly is very rigid allowing little movement of the bit. \n\nSuch an assembly would mean that the stabilizers are closely packed: a near bit stabilizer within 0–30 feet of the drill bit, and two more spaced 30 and 60 feet beyond it. If a short drill collar is used, then the stabilizers can be even closer together. \n\nThe shorter the distance between the stabilizers means that the drill collars bend less and the weight on bit (WOB) pushes directly on the bit, hence maintaining the angle.\n\nA pendulum assembly is used to reduce the angle build rate or reduce well-bore angle.\n\nIn this assembly there is no near bit assembly. The front portion of the (BHA) is allowed to hang as a result of its own weight. In such a case the first stabilizer is placed 30–45 feet behind the bit. \n\nThis hanging means that there is a force acting on the low side of the hole, which causes the deviation. In the case of a straight hole then the bit simply continues downward.\n\nThere are three types of BHA configurations. These configurations addressed are usually concerned with the use or layout of drill collars, heavy weight drill pipe and standard drill pipe.\n\nType 1, standard simple configuration, uses only drill pipe and drill collars. In this instance the drill collars provide the necessary weight on the bit.\n\nType 2 uses heavy weight drill pipe as a transition between the drill collars and the drill pipe. Weight on bit is achieved by the drill collars. \n\nType 3 uses the drill collars to achieve directional control. The heavy weight drill pipe applies the weight on the bit. Such a layout promotes faster rig floor BHA handling. It may also reduce the tendency for differential sticking.\n\nIn most cases the above three types of configurations usually apply to straight/vertical wellbores at most low to medium angle wellbores. For high angle and horizontal wellbore careful weight control of the BHA is a must. In this instance the weight may be applied by running the drill pipe in compression in the high angle section. The high angle may help to stabilize the drill pipe allowing it to carry some compression.\n\nA stabilizer is used within a column of drill collars. They help guide the bit in the hole. They play a major part in directional drilling as it helps determine the well-bore path and angle. \n\nIt is used to\n\nSolid stabilizers have no moving or replaceable parts. The blades and the mandrel can be one piece (integral) or welded to the mandrel (weld-on/welded blade). The blades may be either straight or spiral. The working surface can be impregnated with tungsten carbide or diamonds inserts.\n\nReplaceable blade stabilizers can maintain full gauge stabilization, but their blades can be changed with tools no machining or welding required\n\nSleeve type stabilizers have replaceable sleeves that can be changed in the field. The sleeves are either rotating or non-rotating. \n\nReamers are stabilizers that have cutting elements embedded on their fins, and are used to maintain a gauged well-bore. They can be used to drill out doglegs and key-seats in hard formations. Due to the cutting ability of the reamer the bit does less work in maintaining well-bore gauge and more work drilling.\n\nAn underreamer is used to enlarge the well-bore, usually beginning at some point under the surface. It does this by utilizing expandable cutters that only deploy at the designated time or depth. It is not to be confused with hole opening which occurs from the surface and in most cases the hole opener tool is of a fixed diameter. \n\nThe underreamer utilizes an increase in mud pressure or flow rate to deploy the expandable cutters. A corresponding pressure drop across the tool would indicate that the tool has fully deployed.\n"}
{"id": "53810765", "url": "https://en.wikipedia.org/wiki?curid=53810765", "title": "Carpet hanger", "text": "Carpet hanger\n\nThe outdoor carpet hanger (also carpet stand or carpet rack) is a construction to hang carpets for cleaning with the help of carpet beaters. It is known in Germany, Poland, Lithuania, the Czech Republic, Finland, Russia, and other countries.\n\nIt was a small center of social life. German writers Walter Benjamin and Erich Kästner described hangers as important places during their childhood. Children may use it as a playground, as a soccer goal, as a drumming implement, etc.\n\n"}
{"id": "37418105", "url": "https://en.wikipedia.org/wiki?curid=37418105", "title": "Catalytic combustion", "text": "Catalytic combustion\n\nCatalytic combustion is a chemical process which uses a catalyst to speed desired oxidation reactions of a fuel and so reduce the formation of undesired products, especially pollutant nitrogen oxide gases (NO) far below what can be achieved without catalysts. The process was discovered in the 1970s at Engelhard Corp.\n\nCatalysts may be used to control combustion reactions in the following ways:\n\nCatalytic combustion was developed by Dr. William C. Pfefferle of Engelhard Corp by 1975. He co-founded a company, Precision Combustion, in 1986 to develop catalytic combustors for gas turbines. Pfefferle holds more than ninety United States patents related to catalytic combustion. Other early work was carried out by researchers at Acurex, Westinghouse, NASA and the United States Air Force. The technique was revisited in the 1990s, leading to two types of catalytic system: Catalytica's fuel-lean approach, and Precision Combustion's fuel-rich approach.\n\n"}
{"id": "5134337", "url": "https://en.wikipedia.org/wiki?curid=5134337", "title": "Cold-cranking simulator", "text": "Cold-cranking simulator\n\nThe cold-cranking simulator (CCS) is a device used to determine the low temperature performance of lubricants, when starting a cold engine (i.e. cold-cranking). In this condition, the only energy available to turn the engine comes from the starter motor and the battery, and it has been widely assumed that the system acts as a constant power viscometer. The use of this device for this purpose is standardized as ASTM D5293.\n\nThe cold-cranking simulator was invented developed by Dr. Dae Sik Kim of Esso Research and Engineering Company in 1964. The first prototype was built on his apartment kitchen table with Unimat, a miniature lathe/milling machine, to minimize and avoid proper company procedures. He reported the results of his developmental work, titled: \"Results of Cold Cranking Simulator and a Comment\" at SAE Fuels and Lubricants Meeting in Palmer house, Chicago on May 18, 1965. Although the device was initially called \"Kimometer\", he refused to put his name on it and he named it for what it was intended.\n\nCold-cranking simulator simulates rheological process of \"an average engine\" during cold starting. The engine's starter motor was replaced with a small series wound universal motor, a typical sewing machine motor, and the engine, with a specially designed cold cylinder and an insulated cylindrical rotor with a pair of parallel flats. The sample oil is continuously sheared under a periodically varying shear rate, lower when the flats pass. Oils in real engines are similarly sheared, high in the journal bearings, oscillatory on piston rings and low in galley. Most developmental work went into proper sizing of the flat to simulate relative shear rate distribution in an \"average engine\". Both an engine and simulator is calibrated with a set of Newtonian standard crank case oils with known viscosities.\n\nWhen SAE and ASTM decided to use the simulator for their future standard instrument, Esso R & E Company gave a free exclusive license to Cannon Instrument Co of State College, PA to avoid conflict of interest.\n\nDuring past four decades many marginal improvements are being made but the basic design and idea remains. Various generations of the CCS have been made over the years, with the latest Cannon CCS-2100 utilising Peltier cooling and an associated chiller to operate essentially the same instrument as the original 1960s design.\n\nIn the late 1980s Ravenfield Designs, Heywood, England, redesigned the entire system from the ground up, utilising a novel system to accurately model the old instruments and created a new machine offering higher repeatability and reproducibility than former methods. The Ravenfield apparatus, designated Model CS is markedly smaller than the Cannon apparatus, incorporating the cooler, the PC, the instrument and sample pumping in a 600 mm square footprint.\n\nThe Society of Automotive Engineers adopted the CCS test as part of the J300 specification, and is the subject of ASTM test method D5293.\n"}
{"id": "7636864", "url": "https://en.wikipedia.org/wiki?curid=7636864", "title": "Defense Nuclear Facilities Safety Board", "text": "Defense Nuclear Facilities Safety Board\n\nThe Defense Nuclear Facilities Safety Board is an independent agency of the United States government based in Washington, D.C. Established in 1988, the DNFSB oversees the nuclear weapons complex administered by the U.S. Department of Energy. The DNFSB is independent of the Department of Energy. The DNFSB's most important power is its ability to give recommendations to the Secretary of Energy.\n\n\n"}
{"id": "31051865", "url": "https://en.wikipedia.org/wiki?curid=31051865", "title": "Djinn chair", "text": "Djinn chair\n\nThe Djinn chair is an important design of the \"Modernist\" style, created by French designer Olivier Mourgue. Originally called the \"Low fireside chair\", it is also commonly referred to as the \"2001\" chair, because of its prominent appearance in the film \"\".\n\nThe Djinn chair is one element of a series of Djinn furniture designed by noted French designer Olivier Mourgue. In 1964 the first piece of the series, a chaise longue, was introduced by \"Airborne\" in Merignac Cedex, France. The rest of the series was released in 1965, and includes the iconic \"Low fireside chair\", a two seat sofa, and a foot stool. The set remained in production until 1976. \n\nThe name \"Djinn\" refers to an Islamic spirit capable of changing shape. The design's low profile was an attempt to emulate the informal lifestyle of the time. The set was introduced during a time when growing interest in Eastern mysticism was influencing Western decorative arts.\n\nThe stretch jersey covering used for the Djinn series did not wear well. Worse yet, the polyether foam used in construction of Djinn chairs has a tendency to degrade over time, causing the pieces to become unusable. \"Things should have a short life,\" Mr. Mourgue said in 1965, when he was 26.\n\nDjinn chairs were sold worldwide, and in Europe they have become valuable designer collectibles. Values in Europe remain varied depending on condition, with professionally restored chairs selling for $1,000 or more, and sofas for more than $1,800. In the United States the chairs remain largely forgotten, and little effort at restoration has occurred.\n\nA 1964-1965 green Djinn Chaise Longue is in the permanent collection of the Museum of Modern Art in New York City. It was donated by George Tanier, Inc. in 1966 \n\n"}
{"id": "35756238", "url": "https://en.wikipedia.org/wiki?curid=35756238", "title": "Eagle Vision 1 (Commercial Satellite Imagery)", "text": "Eagle Vision 1 (Commercial Satellite Imagery)\n\nEagle Vision One (EV1) is a military based Commercial Satellite Imagery (CSI) capability programmed and funded by HQ AF/A2QS (Air Force ISR Innovations). It is located at Ramstein Air Base, Germany, and is staffed by intelligence personnel assigned to the 24th Intelligence Squadron (24IS). The mission of EV1 is to maintain a deployable commercial satellite imagery system ready to support contingency operations, theater security cooperation events, and disaster relief efforts. The EV1 system consists of the two elements: a Data Acquisition Segment (DAS) which includes a direct downlink antenna and a computer server shelter that collects and processes imagery into a standard format, and a transit-cased Data Integration Segment (DIS) that processes the standard format products into useful products.\n\nEV1 works with satellite vendors from France (Spot 4 & 5), Canada (RADARSAT 1 & 2), and India (CARTOSAT1 & 2) to provide unclassified imagery to customers.\n\nEV1 is part of a larger Eagle Vision enterprise that consists of four other units stationed in South Carolina, Alabama, California, and Hawaii.\n\nNewest tools at EV1: EVR2EST is an unclassified imagery server used to upload and share imagery products. See the link below to view the EVR2EST server.\n\nhttps://docs.google.com/open?id=0B7sPKUxQw6P5eEJIRDI2elVHY2M\nhttp://www.disam.dsca.mil/pubs/v.23_4/hartmetz.pdf\nhttps://web.archive.org/web/20150409020541/https://fas.org/irp/program/process/eagle_vision.htm\nhttp://findarticles.com/p/articles/mi_m0IAJ/is_4_23/ai_78803060/?tag=content;col1\n"}
{"id": "1521032", "url": "https://en.wikipedia.org/wiki?curid=1521032", "title": "Fetoscopy", "text": "Fetoscopy\n\nFetoscopy is an endoscopic procedure during pregnancy to allow surgical access to the fetus, the amniotic cavity, the umbilical cord, and the fetal side of the placenta. A small (3–4 mm) incision is made in the abdomen, and an endoscope is inserted through the abdominal wall and uterus into the amniotic cavity. Fetoscopy allows for medical interventions such as a biopsy (tissue sample) or a laser occlusion of abnormal blood vessels (such as chorioangioma) or the treatment of spina bifida. \n\nFetoscopy is usually performed in the second or third trimester of pregnancy. The procedure can place the fetus at increased risk of adverse outcomes, including fetal loss or preterm delivery, so the risks and benefits must be carefully weighed in order to protect the health of the mother and fetus(es). The procedure is typically performed in an operating room by an obstetrician-gynecologist.\n\nIn 1945, Björn Westin published a study which documented his use of a panendoscope to directly observe embryos. In 1966, Agüero et al published a study which used hysteroscopy to observe various features of the fetus, cervix, and uterus. In 1972, Carlo Valenti of the SUNY Downstate Medical Center recorded a technique which he called \"endoamnioscopy\", which allowed for direct visualization of the developing fetus. Gallinat made the first attempt to standardize these techniques in 1978.\n\nBecause of the invasiveness of these procedures and the high risk they posed to the fetus, they were largely discarded in favor of transvaginal sonography until the 1990s. By that time, smaller instruments had been developed which reduced the risk to the fetus and provided a better visual for the physician. This in turn allowed for the development of techniques for surgical interventions such as biopsy. By 1993, authors such as Cullen, Ghirardini, and Reece had referred to this technique as \"fetoscopy\".\n\nThe field of minimally-invasive surgical fetoscopy has continued to develop since the 2000s. Physicians such as Michael Belfort and Ruben Quintero have used the technique to remove tumors and correct spina bifida on fetuses within the uterus.\n\nFetoscopy is a surgical procedure which may involve the use of a fibreoptic device called a fetoscope. Some confusion may arise from the use of specialized forms of stethoscopes, including Pinard horns and Doppler wands, to audibly monitor fetal heart rate (FHR). These audio diagnostic tools are also called \"fetoscopes\" but are not related to fetoscopy.\n\n\n"}
{"id": "9979470", "url": "https://en.wikipedia.org/wiki?curid=9979470", "title": "Fragmentation (weaponry)", "text": "Fragmentation (weaponry)\n\nFragmentation is the process by which the casing of an artillery or mortar shell, rocket, missile, bomb, grenade, etc. is shattered by the detonation of the explosive filler.\n\nThe correct term for these pieces is \"fragmentation\" (sometimes shortened to frag); \"shards\" or \"splinters\" can be used for non-preformed fragments. Preformed fragments can be of various shapes (spheres, cubes, rods, etc.) and sizes, and are normally held rigidly within some form of matrix or body until the high explosive (HE) filling is detonated. The resulting high-velocity fragments produced by either method are the main lethal mechanisms of these weapons, rather than the heat or overpressure caused by detonation, although offensive grenades are often constructed without a frag matrix.\n\nThese casing pieces are often incorrectly referred to as \"shrapnel\" (particularly by non-military media sources).\n\nThe use of fragmentation in bombs dates to the 14th century, and appears in the Ming Dynasty text \"Huolongjing\". The fragmentation bombs were filled with iron pellets and pieces of broken porcelain. Once the bomb explodes, the resulting shrapnel is capable of piercing the skin and blinding enemy soldiers. \n\nThe modern fragmentation grenade was developed during the 20th century. The Mills bomb, first adopted in 1915 by the British army, is an early fragmentation grenade used in World War I. The Mk 2 grenade was a fragmentation grenade adopted by the American military based on the Mills bomb, and was in use during World War II.\n\nThe term \"shrapnel\" is often incorrectly used to refer to fragments produced by \"any\" explosive weapon. However, the shrapnel shell, named for Major General Henry Shrapnel of the British Royal Artillery, predates the modern high-explosive shell and operates via an entirely different process.\n\nA shrapnel shell consists of a shell casing filled with steel or lead balls suspended in a resin matrix, with a small explosive charge at the base of the shell. When the projectile is fired, it travels a pre-set distance along a ballistic trajectory, then the fuse ignites a relatively weak secondary charge (often black powder or cordite) in the base of the shell. This charge fractures the matrix holding the balls in place and expels the nose of the shell to open a path for the balls, which are then propelled out of the front of the shell without rupturing the casing (which falls to earth harmlessly and can be retrieved and reused). These balls continue onward to the target, spreading out in a cone-shaped pattern at ground level, with most of their energy coming from the original velocity of the shell itself rather than the lesser force of the secondary charge that freed them from the shell. Since the cone of impact is relatively small, no more than 10 to 15 times the diameter of the shell, true shrapnel shells needed to be carefully sighted and judiciously used in order to maximize their impact on the enemy.\n\nIn contrast, a high-explosive shell contains a relatively large and energetic secondary charge of high explosive (known as a burster charge) which, when ignited by the fuse, produces a powerful supersonic shock wave that shatters the entire shell casing into many fragments that fly in all directions. The use of high explosives with a fragmenting case improves efficiency as well as propelling a larger number of fragments at a higher velocity over a much wider area (40-60 times the diameter of the shell), giving high-explosive shells a vastly superior battlefield lethality that was largely impossible before the Industrial Era. World War I was the first major conflict in which HE shells were the dominant form of artillery; the failure to adapt infantry tactics to the massive increase in lethality they produced was a major element in producing the ghastly subterranean stalemate conditions of trench warfare, in which neither side could risk movement above ground without the guarantee of instant casualties from the constant, indiscriminate hail of HE shell fragments.\n\nOne easy comparison between fragmenting HE and shrapnel shells would be to imagine a shell of each type standing stationary and base-first on the ground; a high-explosive shell would be equally lethal if detonated in this state vs. detonating on impact after being fired, whereas a shrapnel shell would ineffectually fire its contents only a few feet into the air in a cone-shaped pattern (while the casing itself remained intact). However, the reduced area of effect of shrapnel shells can be exploited, such as in the creeping barrage tactics of World War I, where shrapnel shells were able to be used much closer to friendly infantry than HE shells could be.\n\nAnother artillery round very similar to (and directly predating) shrapnel is canister shot, sometimes called case shot. Canister shot is a simple sheet metal casing filled with steel or lead balls. Upon firing, the casing fractures as it exits the muzzle of the gun and dispenses the balls from the gun’s muzzle much like a large shotgun. The practical differences between canister and shrapnel is that canister produces projectile separation immediately on detonation, and thus is only effective at short range (less than ), whereas a shrapnel shell can travel hundreds of meters downrange before the fuse activates the explosive charge and separates the projectiles from the case, thus making it effective at much longer ranges.\n"}
{"id": "44140260", "url": "https://en.wikipedia.org/wiki?curid=44140260", "title": "Fulton MX991/U Flashlight", "text": "Fulton MX991/U Flashlight\n\nThe Fulton MX991/U Flashlight (or sometimes known as a GI Flashlight, Army flashlight, moonbeam (Marine Corps) or simply MX991/U) is a model of angle-head flashlight currently manufactured by Fulton Industries which was issued to soldiers during the Vietnam War, and has since remained in service with the United States Army.\n\nDuring World War II, a request was made for a type of flashlight to be developed and used by soldiers being deployed by the US military. The TL-122 series of angle-head flashlights was developed by multiple manufacturers, most notably Bright Star, and issued to soldiers. It was used widely throughout the war, and the design remained fairly consistent throughout the following years. \n\nDuring the onset of the Vietnam War, a contract was offered to a manufacturer who could develop an updated version of the TL-122 series flashlights with improvements for the modern military requirements. The result was the MX-991/U angle head flashlight, and the contract was secured by Fulton Industries, and for a brief period, G.T. Price. Currently, Fulton Industries continues to manufacture the flashlight in various colors and modifications. These are available to civilians, law enforcement, and the military.\n\nIn recent years, many unnamed manufacturers from Asia have developed unofficial reproductions of the MX-991/U flashlight. Often the variants are physically and internally identical, with the exclusion of the \"MX-991/U\" printed into the side of the flashlight. These clones often include varying colours not available by Fulton, such as blaze orange, and also include different coloured lens in the tailcap such as yellow and green alongside the standard colours.\n\nThe MX-991/U flashlight is an inexpensive, waterproof angle head flashlight that uses two D-cell batteries (military BA-30). It uses a standard incandescent bulb, and features a high-impact plastic body, a belt/equipment clip so that it may be fastened to a belt or strap, a tailcap lanyard ring, a multi-mode switch and tailcap with a storage compartment which houses multiple colored plastic lenses which is not available on the telecommunications variant of the flashlight.\n\nThe multi-mode switch consists of three settings: Off, Signal, and On. When the switch is set to Signal, the user is able to press and hold a button located just above the switch to turn the light on, and it will switch off again when the button is released. This allows users to signal using Morse code. When the switch is moved up to the On position, the flashlight remains on without any further user interaction. The modern MX-991/U also features a switch guard which was not present in previous variants as issued in the Vietnam War. The switch guard was suggested as an improvement from soldiers during the war, and later added to prevent unintentional operation of the flashlight. This helped to prevent soldiers from accidentally giving their position away in the dark should equipment, clothing etc press the signal button or move the switch.\n\nThe tailcap consists of two compartments. The first houses the spring and keeps the batteries inside the flashlight body. Under the spring, a small plastic component houses a spare flashlight bulb. The second part of the tailcap consists of a small compartment that houses five plastic lens.\n\nThe nosecap of the flashlight has the ability to be unscrewed, and a custom lens can be fitted. The flashlight contains five lens in the tailcap, consisting of two red lens, blue lens, white lens, and diffuser lens (earlier three red and no blue-green). This enabled soldiers to send signals using different colors, or to cast the light in different methods. As the flashlight could not be focused/unfocused, the diffuser lens was used to spread the light in such a fashion that it would throw out a wide glow of light, as opposed to a narrow, focused beam.\n\nFulton currently manufactures the flashlight in varying color combinations, often to designate a special use or model:\n\n\nThe flashlight is often sold at military disposal stores, and is popularly considered by many to be an ideal flashlight for camping or power outages, due to its low price, utility and durable body. The flashlight can also accept custom bulbs such as modern LED lamp, which greatly extend the battery life and provide brighter light than the standard incandescent bulbs. It is still widely recognized or known as a \"GI Flashlight\" for its military history or a \"Boy Scout Flashlight\" in referral to its resemblance to the brass stamped angle-head flashlights issued to Boy Scouts in the mid-century.\n\n\n"}
{"id": "2523651", "url": "https://en.wikipedia.org/wiki?curid=2523651", "title": "Gambrel", "text": "Gambrel\n\nA gambrel or gambrel roof is a usually symmetrical two-sided roof with two slopes on each side. (The usual architectural term in eighteenth-century England and North America was \"Dutch roof.\") The upper slope is positioned at a shallow angle, while the lower slope is steep. This design provides the advantages of a sloped roof while maximizing headroom inside the building's upper level and shortening what would otherwise be a tall roof. The name comes from the Medieval Latin word \"gamba\", meaning \"horse's hock or leg\". The term \"gambrel\" is of American origin, the older, European name being a curb (kerb, kirb) roof. Europeans historically did not distinguish between a gambrel roof and a mansard roof but called both types a mansard. In the United States, various shapes of gambrel roofs are sometimes called Dutch gambrel or Dutch Colonial gambrel with bell-cast eaves, Swedish ~, German ~, English ~ , French ~, or New England gambrel.\n\nThe cross-section of a gambrel roof is similar to that of a mansard roof, but a gambrel has vertical gable ends instead of being hipped at the four corners of the building. A gambrel roof overhangs the façade, whereas a mansard normally does not.\n\n\"Gambrel\" is a Norman English word, sometimes spelled gambol such as in the 1774 Boston carpenters' price book (revised 1800). Other spellings include gamerel, gamrel, gambril, gameral, gambering, cambrel, cambering, chambrel referring to a wooden bar used by butchers to hang the carcasses of slaughtered animals. Butcher's gambrels, later made of metal, resembled the two-sloped appearance of a gambrel roof when in use. Gambrel is also a term for the joint in the upper part of a horse's hind leg, the hock. \n\nIn 1858, Oliver Wendell Holmes, Sr. wrote:\nAn earlier reference from the \"Dictionary of Americanisms\", published in 1848, defines \"gambrel\" as \"A hipped roof of a house, so called from the resemblance to the hind leg of a horse which by farriers is termed the \"gambrel\".\" Websters Dictionary also confusingly used the term \"hip\" in the definition of this roof.\n\nThe term is also used for a single mansard roof in France and Germany. In Dutch the term 'two-sided mansard roof' is used for gambrel roofs.\n\nThe origin of the gambrel roof form in North America is unknown. The oldest known gambrel roof in America was on the second Harvard Hall at Harvard University built in 1677. Possibly the oldest surviving house in the U.S. with a gambrel roof is the c. 1677–78 Peter Tufts House. The oldest surviving framed house in North America, the Fairbanks House, has an ell with a gambrel roof, but this roof was a later addition.\n\nClaims to the origin of the gambrel roof form in North America include: 1) Spanish, Portuguese, Dutch, and English mariners and traders had visited or settled into the area of southeast Asia now called Indonesia prior to permanent European settlement in America. In Indonesia, they saw dwellings with a roof style where the end of a roof started as a hip and finished as a gable end at the ridge. The gable end was an opening, to allow smoke to dissipate from the cooking fires. This roof design was brought back to Europe and the American Colonies, and adapted to local conditions. The roof style is still in use around the world today; 2) Seamen who traveled to the Netherlands brought the design back to North America; 3) or practical reasons such as a way to allow wider buildings, the use of shorter rafters, or to avoid taxes.\n\n "}
{"id": "10727643", "url": "https://en.wikipedia.org/wiki?curid=10727643", "title": "Gelato Federation", "text": "Gelato Federation\n\nThe Gelato Federation (usually just Gelato) was a \"global technical community dedicated to advancing Linux on the Intel Itanium platform through collaboration, education, and leadership.\" Formed in 2001, membership included more than seventy academic and research organizations around the world, including several that operated Itanium-based supercomputers on the Top500 list. The organization was active in projects to enhance the Linux kernel for Itanium and GCC for Itanium. The organization took its name from the Italian dessert gelato, paying homage to this by naming sub-projects Gelato Vanilla and Gelato Coconut for varieties of the dessert.\n\nIn late 2001, representatives from seven organizations met with Hewlett-Packard. The institutions were the Bioinformatics Institute, Singapore; Groupe ESIEE, France; Hewlett-Packard Company; National Center for Supercomputing Applications, USA; Tsinghua University, China; University of Illinois at Urbana-Champaign, USA; University of New South Wales, Australia; and University of Waterloo, Canada. These were the founding members of Gelato. \n\nRepresentatives from these organizations met twice a year. The first few meetings (in Palo Alto, California 2001 and Paris 2002) were primarily a \"strategy council meeting\" where the by-laws and charter were hammered out.\n\nThe Sydney meeting in October 2002 was the first that included a day of technical presentations. These became a regular feature of the meetings, eventually expanded to conferences, and thus the two conferences each year were entirely composed of technical presentations by vendors and members.\n\nThe organization apparently ceased operation in 2009.\n\nThe federation grew markedly after its inception. By April 2007, there were more than 70 members and sponsors around the world. Members were institutions, but there were a few individuals who, because of their contribution to IA-64 on Linux or to Gelato, were made Honorary Members. These included Clemens C. J. Roothaan (who contributed to the Itanium math libraries and floating point unit), Brian Lynn (the original HP representative), David Mosberger-Tang (original porter of Linux to IA-64) and Jean-Pol Taffin (ex-general secretary of ESIEE, and very influential in the early days of Gelato).\n\nInstitutional members were sponsored by an IA-64 vendor, or came in on their own. Sponsored members typically had specific projects in mind.\n\nThe Gelato ICE: Itanium Conference & Expo alternated between San Jose, California and somewhere else in the world, often in Southeast Asia or Europe. Gelato conferences were where most of the collaboration and cooperation between members were established, and where Intel revealed some of their future strategy for the Itanium-based platform. The last conference was held in Singapore in October 2007.\n\nApart from the Members' activities, Gelato funded a Central Operations (hosted at the University of Illinois at Urbana-Champaign). Central Operations, in addition to running the twice-a-year meetings, tried to coordinate and manage a number of projects. These included:\n\nGelato was funded by HP, Intel, BP, Itanium Solutions Alliance, and SGI. Gelato Central Operations was housed at the Coordinated Science Lab at the University of Illinois.\n\n"}
{"id": "3661487", "url": "https://en.wikipedia.org/wiki?curid=3661487", "title": "Graco (baby products)", "text": "Graco (baby products)\n\nGraco (pronounced gray-co) is an American baby products company based in High Point, North Carolina. It is owned and operated by Newell Brands. Graco was founded in 1942 in Philadelphia, Pennsylvania, by Russell Gray and Robert Cone (hence the name) as Graco Metal Products, a company that fabricated machine and car parts. Rex Thomas (one of two engineers hired to come up with a sustainable product) watched his wife sitting on the porch, rocking their baby in a swing with a string tied to it, while she read a book. Rex went into work the next day and said \"why don't we make an automatic baby swing.\" After 18 months of research and development, the Swyngnomatic - the world's first wind-up, automatic baby swing—was born in 1955, designed by company engineer Dave Saint. In 1987 the company pioneered the invention of the Pack N' Play Portable Playard, the world's first portable playard (designed by Nate Saint, Dave Saint's son).\n\nIn 1998, Graco acquired Century, a car seat manufacturer and introduced the SnugRide infant car seat, which has become America's top-selling infant car seat line. Century actually introduced the travel system where a car seat could fit on a stroller for easy transportation. That same year, Graco was acquired by Rubbermaid (the company became Newell Rubbermaid in 1999 and Newell Brands in 2016). In 2002, Graco launched the TurboBooster, a booster seat designed to help parents stay in compliance with many states' passage of laws requiring children to stay in a car seat longer. In 2007, the company purchased established German baby product brand, Teutonia. The same year, the Nautilus® 3-in-1car seat debuted, earning the \"Best Bet\" designation from Insurance Institute for Highway Safety. Then, in 2008, Graco acquired Aprica Kassai®, the number one selling baby brand in Japan. In 2014, Newell Rubbermaid acquired American baby brand, Baby Jogger.\n\nGraco offers products including car seats, travel systems, strollers, high chairs, playards and baby swings.\n\n"}
{"id": "55321521", "url": "https://en.wikipedia.org/wiki?curid=55321521", "title": "Gravity battery", "text": "Gravity battery\n\nA gravity battery is a type of mechanical battery that stores gravitational potential energy, by raising a mass, allowing that energy to be released (typically converted to electrical energy via a generator), by allowing the mass to fall.\n\nSuch devices have been considered as part of an sustainability/renewable energy drives, addressing the problem of electrical energy storage.\n\nThe technique was introduced with the pendulum clock.\n\nSimilar devices have been proposed on a smaller scale to power a single light bulb in developing countries.\n\nFor grid scale storage, weights suspended by cables may generate power as they drop to the bottom of the sea, or from a crane. Heavy trains run uphill can release that potential by using regenerative braking going downhill.\n"}
{"id": "42338952", "url": "https://en.wikipedia.org/wiki?curid=42338952", "title": "Griggs apparatus", "text": "Griggs apparatus\n\nGriggs apparatus, also referred to as a Griggs rig, is a modified piston cylinder high pressure apparatus used to create an environment of high pressure, high temperature and to impart a deviatoric stress on a sample of material. It was conceived in the 1960s.\n\nSample sizes range depending on the specific Griggs apparatus but generally can be up to approximately 150 mm3, and temperatures of up to 1600 K along with pressures of approximately 3 GPa can be achieved.\n\nThe Griggs apparatus was conceived by David Griggs in the mid 1960s during his time at the University of California at Los Angeles (UCLA). Since the inception of the Griggs apparatus it has become the work horse of many rock deformation labs world-wide, and has also helped elucidate numerous facets of plastic deformation in crystalline materials including the hydrolytic weakening of quartz.\n\nThe Griggs machine utilizes the same principle that other high pressure apparatuses (such as the diamond anvil cell) use to create elevated pressure on a specimen.\n\nBy generating a nominal force, in the case of the Griggs machine through a hydraulic ram, a greater force can then be applied to the sample by decreasing the area of subsequent pistons in series with the ram and in contact with the sample.\n\nThe sample assembly is constructed of multiple cylindrical sleeves which are placed in the opening in the pressure vessel or “bomb”. The outermost sleeve is typically composed from NaCl which is used to transfer the vertical load applied from the steel piston into a confining pressure on the sample at the center of the assembly. NaCl is used since it is relatively weak and it assists in the transfer of stress. Directly inboard of the outer NaCl sleeve is a ceramic support sleeve with a graphite sleeve inside of it, which is used for resistive heating of the sample. The inner most sleeve which houses the sample along with the upper and lower alumina pistons, is commonly also composed of NaCl. In addition to this arrangement, the inner sleeve can also be composed of a ternary eutectic salt mixture which is called a molten salt cell. The advantage to the molten salt cell is the salt mixture melts at moderate temperatures which allows for a true hydrostatic pressure to be applied the sample. When using a molten salt cell it also becomes necessary to add an additional nickel capsule to contain the salt mixture in order to prevent damage to other parts of the sample assembly. Temperature is monitored by a side entry thermocouple(s) that penetrate the wall of the graphite furnace and are directly adjacent to the sample, and are typically threaded through protective mullite insulation. The deviatoric stress is transferred to the sample through the σ1 piston. This piston is orientated in series with the upper piston, sample and lower piston within the inner sleeve which all sits on top of a tungsten carbide bottom piston.\n\nA Griggs apparatus has the ability to create and maintain confining pressure on a sample while separately being able to strain the sample. Confining pressure is generated by advancing a hydraulic ram either by using a hand operated lever pump or a servo controlled syringe pump. The advancing ram then compresses the outer tungsten carbide piston (σ3) that in turn depresses the Pb plug at the top of the sample assembly and then in turn stresses the NaCl pressure medium. Deviatoric stress is created by a mechanical drive train that is powered by an electric motor that sits on top of the apparatus. When the electric motor is powered it engages with a set of gears that allow for variable strain rates to be chosen that range from 10sec-1 down to 10sec-1. \nTo eliminate the torque from the advancing drive train produced by the gear set, a recirculating ball screw is located in between the gear set and deformation piston (σ1) that provides the axial load. Connected in series with the deformation piston is also an external load cell which measures the load applied to the inner (σ1). The outer (σ3) and inner (σ1) pistons that sit above the sample are two separate pieces, this allows for the advancement of the deformation piston with the drive train without altering the confining pressure which is operated with the hydraulic ram.\n\nThe design of the hydraulic ram produces an error in pressure measurements that are calculated from utilizing the oil pressure within the ram. Inside of the ram there is a large O-ring that seals oil in either the upper or lower portion of the ram. As the ram is pressurized and oil is transferred from the lower reservoir to the upper friction is produced by the motion of the O-ring along the inside wall of the ram. Since the ram needs to overcome the force of the friction to continue advancing, pressure measurements calculated using the oil pressure within the ram includes a contribution from the internal friction. The contribution of frictional force on pressure calculations can be described by the following relationship:\n\nCalculated pressure = Confining pressure + Internal friction within the ram.\n\nThe total contribution of this friction to the value of measured pressure is different for each specific Griggs machine, but it has been shown that deviations between actual and measured pressure can be up to 10%.\n\n\n"}
{"id": "42644090", "url": "https://en.wikipedia.org/wiki?curid=42644090", "title": "Highscreen", "text": "Highscreen\n\nHighscreen is a consumer electronics brand selling budget smartphones in Russia. The brand debuted in the 1990s, when the German computer store Vobis sold fully assembled PCs under the Highscreen brand.\n"}
{"id": "55861172", "url": "https://en.wikipedia.org/wiki?curid=55861172", "title": "IKeyMonitor", "text": "IKeyMonitor\n\niKeyMonitor is a monitoring program for iPhone and Android phones and tablets. It is designed for parental control and employee monitoring. Featured as a parental control tool, iKeyMonitor can be used to remotely monitor activities on children's phones, such as SMS, voice messages, calls, social chats, GPS and more information.\n\nAwosoft Technology Co., Ltd was founded in 2005. The company is dedicated to the development of software solutions for phones and computers to keep track of the activities of children and/or employees from a central location. Awosoft helps to monitor and record children/employees' activity on their phones/Macs including chat logs, emails, web history, application usage and more. Awosoft has since been branded as Easemon. In 2012, Awosoft released iKeyMonitor jailbreak spy for iPhone. Support for Android devices was released in 2014 with Android 2.3.x support. In 2017 the iKeyMonitor iPhone spy no jailbreak service was added and the iSpyTracker cloud panel app for iKeyMonitor was released.\n\niKeyMonitor works on jailbroken and non-jailbroken iPhone/iPad, rooted and non-rooted Android phones and tablets.\n\niKeyMonitor allows:\n\n\n"}
{"id": "30411849", "url": "https://en.wikipedia.org/wiki?curid=30411849", "title": "Kingfisher International", "text": "Kingfisher International\n\nKingfisher International Pty Ltd is an Australian manufacturer of fiber optic test and measurement equipment, located in Mulgrave, Victoria. As of 2005, the company had twenty-two employees.\n\nThe company was founded in Melbourne in 1986. as an international electronics trading firm, in a spare bedroom, by Rosmin Robertson (née Jaffer) and Bruce Robertson, Bruce developed the ground-breaking KI2000 Optical Light Source and KI020 Optical Talk Set.\n\nBruce Robertson's technical involvement in fiber optics started 1980 - 1985, when he was a researcher at GEC's Hirst Research Centre in the UK, developing novel fiber optic cable designs, manufacturing processes, fiber optic sensors & instrumentation, and was awarded various patents.\n\nIn 1988, the business moved to a large garage, further contracts were won, and an Australian Federal R&D Grant was received for early development work on optical power meters, which culminated in the release of the KI6000 series power meters soon afterwards.\n\nIn 1991, Kingfisher had grown into its first factory in Rowville. Kingfisher then located in facility in Scoresby from 2001 to May 2015, when it moved to new premises at 720 Springvale Road, Mulgrave Victoria 3170.\n\nThe company has always made its products in Australia.\n\nKingfisher is one of the oldest fiber optic test companies, and is regarded by industry elders as having a significant influence on the development of the industry. Kingfisher products are used by professional technicians when installing and maintaining fiber optic cabling and systems, and its fiber optic test equipment range includes such items as, optical power meters, optical light sources, optical loss test sets, optical test and inspection kits, variable optical attenuators, inspection microscopes and various optical fault locators.\n\nThe company has worldwide distribution channels, and currently participates in various national and international standards development groups.\n\nSince 2014, Kingfisher International has been wholly owned and managed by co-founder Bruce Robertson. \n\nVarious industry recognitions include:\n\n"}
{"id": "456619", "url": "https://en.wikipedia.org/wiki?curid=456619", "title": "Kryptos", "text": "Kryptos\n\nKryptos is a sculpture by the American artist Jim Sanborn located on the grounds of the Central Intelligence Agency (CIA) in Langley, Virginia. Since its dedication on November 3, 1990, there has been much speculation about the meaning of the four encrypted messages it bears. Of the four messages, the first three have been solved, while the fourth message remains as one of the most famous unsolved codes in the world. The sculpture continues to be of interest to cryptanalysts, both amateur and professional, who are attempting to decipher the fourth passage. The artist has so far given two clues to this passage.\n\nThe main part of the sculpture is located in the northwest corner of the New Headquarters Building courtyard, outside of the Agency's cafeteria. The sculpture comprises four large copper plates with other elements consisting of water, wood, plants, red and green granite, white quartz, and petrified wood. The most prominent feature is a large vertical S-shaped copper screen resembling a scroll or a piece of paper emerging from a computer printer, half of which consists of encrypted text. The characters are all found within the 26 letters of the Latin alphabet, along with question marks, and are cut out of the copper plates. The main sculpture contains four separate enigmatic messages, three of which have been deciphered.\n\nIn addition to the main part of the sculpture, Jim Sanborn also placed other pieces of art at the CIA grounds, such as several large granite slabs with sandwiched copper sheets outside the entrance to the New Headquarters Building. Several morse code messages are found on these copper sheets, and one of the stone slabs has an engraving of a compass rose pointing to a lodestone. Other elements of Sanborn's installation include a landscaped garden area, a fish pond with opposing wooden benches, a reflecting pool, and other pieces of stone including a triangle shaped black stone slab.\n\nThe name \"Kryptos\" comes from the ancient Greek word for \"hidden\", and the theme of the sculpture is \"Intelligence Gathering\".\n\nThe cost of the sculpture in 1988 was US $250,000 (worth US $501,000 in 2016).\n\nThe ciphertext on the left-hand side of the sculpture (as seen from the courtyard) of the main sculpture contains 869 characters in total : 865 letters and 4 question marks.\n\nIn April 2006, however, Sanborn released information stating that a letter was omitted from this side of \"Kryptos\" \"for aesthetic reasons, to keep the sculpture visually balanced\".\n\nThere are also three misspelled words in the plaintext of the deciphered first three passages, which Sanborn has said was intentional, and three letters (YAR) near the beginning of the bottom half of the left side are the only characters on the sculpture in superscript.\n\nThe right-hand side of the sculpture comprises a keyed Vigenère encryption tableau, consisting of 867 letters.\n\nOne of the lines of the Vigenère tableau has an extra character (L). Bauer, Link and Molle suggest that this may be a reference to the Hill cipher as an encryption method for the fourth passage of the sculpture.\n\nSanborn worked with a retiring CIA employee named Ed Scheidt, Chairman of the CIA Office of Communications, to come up with the cryptographic systems used on the sculpture.\n\nSanborn has revealed that the sculpture contains a riddle within a riddle, which will be solvable only after the four encrypted passages have been deciphered.\n\nHe has given conflicting information about the sculpture's answer, saying at one time that he gave the complete solution to the then-CIA director William Webster during the dedication ceremony; but later, he also said that he had not given Webster the entire solution. He did, however, confirm that within the passage of the plaintext of the second message which reads \"Who knows the exact location? Only WW.\", \"WW\" was intended to refer to William Webster.\n\nSanborn also confirmed that should he die before the entire sculpture becomes deciphered, there will be someone able to confirm the solution.\n\nThe first person to announce publicly that he had solved the first three passages was Jim Gillogly, a computer scientist from southern California, who deciphered these passages using a computer, and revealed his solutions in 1999.\n\nAfter Gillogly's announcement, the CIA revealed that their analyst David Stein also had solved the same passages in 1998 using pencil and paper techniques, although at the time of his solution the information was only disseminated within the intelligence community and no public announcement was made until July 1999.\n\nThe NSA also claimed that some of their employees had solved the same three passages, but would not reveal names or dates until March 2000, when it was learned that an NSA team led by Ken Miller, along with Dennis McDaniels and two other unnamed individuals, had solved passages 1–3 in late 1992.\n\nIn 2013, in response to a Freedom of Information Act request by Elonka Dunin, the NSA released documents which show the NSA became involved in attempts to solve the \"Kryptos\" puzzle in 1992, following a challenge by Bill Studeman, then Deputy Director of the CIA. The documents show that by June 1993, a small group of NSA cryptanalysts had succeeded in solving the first three passages of the sculpture.\n\nThe above attempts to solve \"Kryptos\" found that passage 2 ended with WESTIDBYROWS, but in 2005, Monet Friedrich, a logician, philosopher, and computer scientist from Vancouver, British Columbia, Canada, determined that another possible plaintext was: WESTXLAYERTWO.\n\nIn 2006, Sanborn announced that he had made an error in passage 2, and confirmed that the last passage of the plaintext was WESTXLAYERTWO, and not WESTIDBYROWS. Sanborn had inadvertently omitted a letter S in the crypt text. By rotating the keyword to BSCISSAA it decrypts to WESTPLAYERTWO. The significance of this is that WESTXLAYERTWO is the original plain text. It was intentional that WESTIDBYROWS was discovered at a later stage by following a clue to manipulate the letter(s) X.\n\nThe following are the solutions of passages 1–3 of the sculpture.\n\nMisspellings present in the text are included verbatim.\n\nMethod : Vigenère\n\nKeywords: Kryptos, Palimpsest\n\nBETWEEN SUBTLE SHADING AND THE ABSENCE OF LIGHT LIES THE NUANCE OF IQLUSION\n\nMethod : Vigenère\n\nKeywords: Kryptos, Abscissa\n\nIT WAS TOTALLY INVISIBLE HOWS THAT POSSIBLE ? THEY USED THE EARTHS MAGNETIC FIELD X THE INFORMATION WAS GATHERED AND TRANSMITTED UNDERGRUUND TO AN UNKNOWN LOCATION X DOES LANGLEY KNOW ABOUT THIS ? THEY SHOULD ITS BURIED OUT THERE SOMEWHERE X WHO KNOWS THE EXACT LOCATION ? ONLY WW THIS WAS HIS LAST MESSAGE X THIRTY EIGHT DEGREES FIFTY SEVEN MINUTES SIX POINT FIVE SECONDS NORTH SEVENTY SEVEN DEGREES EIGHT MINUTES FORTY FOUR SECONDS WEST X LAYER TWO\n\nOn April 19, 2006, Sanborn contacted an online community dedicated to the \"Kryptos\" puzzle to inform them that the accepted solution to passage 2 was incorrect.\nHe said that he made an error in the sculpture by omitting an \"X\" used to separate sentences, for aesthetic reasons, and that the deciphered text that ended \"...FOUR SECONDS WEST ID BY ROWS\" should actually be \"...FOUR SECONDS WEST X LAYER TWO\".\n\nThe coordinates mentioned in the plaintext: are for a point that is approximately 150 feet southeast of the sculpture.\n\nMethod : Transposition\n\nSLOWLY DESPARATLY SLOWLY THE REMAINS OF PASSAGE DEBRIS THAT ENCUMBERED THE LOWER PART OF THE DOORWAY WAS REMOVED WITH TREMBLING HANDS I MADE A TINY BREACH IN THE UPPER LEFT HAND CORNER AND THEN WIDENING THE HOLE A LITTLE I INSERTED THE CANDLE AND PEERED IN THE HOT AIR ESCAPING FROM THE CHAMBER CAUSED THE FLAME TO FLICKER BUT PRESENTLY DETAILS OF THE ROOM WITHIN EMERGED FROM THE MIST X CAN YOU SEE ANYTHING Q ?\n\nThis is a paraphrased quotation from Howard Carter's account of the opening of the tomb of Tutankhamun on November 26, 1922, as described in his 1923 book \"The Tomb of Tutankhamun\". The question with which it ends is asked by Lord Carnarvon, to which Carter (in the book) famously replied \"wonderful things\". In the November 26, 1922 field notes, however, his reply was, \"Yes, it is wonderful.\".\n\nMethod(s) : Unknown.\n\nNo solution to Part 4 has been publicly acknowledged by either Jim Sanborn or Ed Scheidt to be correct.\n\nWhen commenting in 2006 about his error in passage 2, Sanborn said that the answers to the first three passages contain clues to the fourth passage.\nIn November 2010, Sanborn released a clue, publicly stating that \"NYPVTT\", the 64th-69th letters in passage four, become \"BERLIN\" after decryption.\n\nSanborn gave \"The New York Times\" another clue in November 2014: the letters \"MZFPK\", the 70th-74th letters in passage four, become \"CLOCK\" after decryption. The 74th letter is K in both the plaintext and ciphertext, meaning that it is possible for a character to encrypt to itself. This means it does not have a weakness, where a character could never be encrypted as itself, that was known to be inherent in the German Enigma machine. It is believed that the \"BERLINCLOCK\" plaintext may be a direct reference to the Berlin Clock.\n\nSanborn further stated that in order to solve passage 4, \"You'd better delve into that particular clock,\" but added, \"There are several really interesting clocks in Berlin.\"\n\n\"Kryptos\" was the first cryptographic sculpture made by Sanborn.\n\nAfter producing \"Kryptos\" he went on to make several other sculptures with codes and other types of writing, including one entitled \"Antipodes\", which is at the Hirshhorn Museum in Washington, D.C., an \"Untitled Kryptos Piece\" that was sold to a private collector, and \"Cyrillic Projector\", which contains encrypted Russian Cyrillic text that included an extract from a classified KGB document.\n\nThe cipher on one side of \"Antipodes\" repeats the text from \"Kryptos\". Much of the cipher on \"Antipodes\" other side is duplicated on \"Cyrillic Projector\". The Russian portion of the cipher found on \"Cyrillic Projector\" and \"Antipodes\" was solved in 2003 by Frank Corr and Mike Bales independently from each other with translation from Russian plaintext provided by Elonka Dunin.\n\n\"Ex Nexum\" was installed in 1997 at Little Rock Old U.S. Post Office & Courthouse\n\nSome additional sculptures by Sanborn include Native American texts: \"Rippowam\" was installed at the University of Connecticut, in Stamford in 1999, while \"Lux\" was installed in 2001 at an old US Post Office building in Fort Myers, Florida.\n\n\"Indian Run\" is located next to the US Federal Courthouse in Beltsville, Maryland and contains a bronze cylinder perforated with the text of the Iroquois Book of the Great Law.\nThis document includes the contribution of the indigenous peoples to the United States legal system. \nThe text is written in Onondaga and was transcribed from the ancient oral tradition of five Iroquois nations.\n\n\"A Comma, A\" was installed at the Plaza in front of the new library at the University of Houston, in Houston, TX in 2004, and \"Radiance\" was installed at the Department of Energy, Coast, and Environment, Louisiana State University, Baton Rouge in 2008.\n\nThe dust jacket of the US version of Dan Brown's novel \"The Da Vinci Code\" contains two references to \"Kryptos\" - one on the back cover (coordinates printed light red on dark red, vertically next to the blurbs) is a reference to the coordinates mentioned in the plaintext of passage 2 (see above), except the degrees digit is off by one. When Brown and his publisher were asked about this, they both gave the same reply: \"The discrepancy is intentional\". The coordinates were part of the first clue of the second \"Da Vinci Code\" WebQuest, the first answer being \"Kryptos\". The other reference is hidden in the brown \"tear\" artwork—upside-down words which say \"Only WW knows\", which is another reference to the second message on \"Kryptos\".\n\n\"Kryptos\" features in Dan Brown's 2009 novel \"The Lost Symbol\".\n\nA small version of \"Kryptos\" appears in the season 5 episode of \"Alias\" \"S.O.S.\". In it, Marshall Flinkman, in a small moment of comic relief, says he has cracked the code just by looking at it during a tour visit to the CIA office. The solution he describes sounds like the solution to the first two parts.\n\nA picture of \"Kryptos\" appears in the season 2 episode of \"The King of Queens\" \"\". A framed pictured of \"Kryptos\" hangs on the wall by the door.\n\nThe progressive metal band Between the Buried and Me has a reference to \"Kryptos\" in their song \"Obfuscation\" from their 2009 album, \"The Great Misdirect\".\n\n\n\n\n"}
{"id": "1957072", "url": "https://en.wikipedia.org/wiki?curid=1957072", "title": "List of World War II weapons", "text": "List of World War II weapons\n\nWorld War II saw rapid technological innovation in response to the needs of the various combatants. Many different weapons systems evolved as a result.\n\nNote: This list does not consist of all weapons used by all countries in World War II. \n\n"}
{"id": "15653040", "url": "https://en.wikipedia.org/wiki?curid=15653040", "title": "List of countries by number of mobile phones in use", "text": "List of countries by number of mobile phones in use\n\nThis list ranks the countries of the world by the number of mobile phones in use. Note that it is not the number of phone devices that are being given here, but the number of phone numbers in a country.\nIn some countries, one person might have two mobile phones.\n\n\n"}
{"id": "24995456", "url": "https://en.wikipedia.org/wiki?curid=24995456", "title": "Meadow (calf)", "text": "Meadow (calf)\n\nMeadow is a Black Angus calf who is believed to be the first bovine calf fitted with double prosthetics.\n\nMeadow was born in northeastern New Mexico. She lost both of her back hooves to frostbite and was found badly wounded on a neighbor's property by Nancy Dickenson, who has helped over a dozen other animals that have been injured. After seeing the serious injuries of the wounded calf, Dickenson wanted to give her the ability to walk again.\n\nAfter consulting with veterinarians to ensure Meadow could lead a quality life, the doctors amputated a portion of the calf's hind legs and fitted her with new prosthetics. Colorado State veterinarian Dr. Robert Callan stated that he believes this to be the first bovine calf to be fitted with double prosthetics. He has based this claim with talks he has had with other veterinarian clinics and schools.\n\nThe procedure took place at the Colorado State University's (CSU) Veterinary Teaching Hospital. The procedure involved five anesthesiologists, expert large animal veterinarians, surgeons, and CSU veterinary students. The reason for the large number of anesthesiologists was due to the number of difficulties involved in anesthetizing cattle. The most difficult of these is keeping the animal from aspirating its stomach contents while put asleep during surgery.\n\nWhen a person undergoes surgery, they first go a period of time without eating or drinking before an elective surgery. This allows the patient's stomach to empty and decreases the opportunity of aspirating stomach content while placed under an anesthetizing agent. These precautions can not be undertaken when dealing with livestock. The calf's stomach has compartments that work \"like a giant fermenting vat\". This means that the stomach can never be emptied. Because of this, the food material within the stomach is frequently regurgitated back into the mouth. Coupled with a large amount of saliva, this placed the calf at a high risk of aspiration of this material. This in return could lead to an airway obstruction and later pneumonia.\n\nMeadow has since returned from the successful operation and prosthetic fittings. Dickenson has said she is adjusting well to her new legs as she recuperates at her new home at Twin Willows Ranch. She can be seen running and grazing, and is said to be \"making friends with goats in a nearby pen.\"\n"}
{"id": "31789966", "url": "https://en.wikipedia.org/wiki?curid=31789966", "title": "Microwave engineering", "text": "Microwave engineering\n\nMicrowave engineering pertains to the study and design of microwave circuits, components, and systems. Fundamental principles are applied to analysis, design and measurement techniques in this field. The short wavelengths involved distinguish this discipline from Electronic engineering. This is because there are different interactions with circuits, transmissions and propagation characteristics at microwave frequencies.\n\nSome theories and devices that pertain to this field are antennas, radar, transmission lines, space based systems (remote sensing), measurements, microwave radiation hazards and safety measures.\n\nDuring World War II microwave engineering played a significant role in developing radar that could accurately locate enemy ships and planes with a focused beam of EM radiation. The foundations of this discipline are found in Maxwell's equations and the work of Heinrich Hertz, William Thomson's waveguide theory, J.C. Bose, the klystron from Russel and Varian Bross, as well as contributions from Perry Spencer, and others.\n\nMicrowave is a term used to identify electromagnetic waves above 10 megahertz (1 Gigahertz) up to 300 Gigahertz because of the short physical wavelengths of these frequencies. Short wavelength energy offers distinct advantages in many applications. For instance, sufficient directivity can be obtained using relatively small antennas and low-power transmitters. These characteristics are ideal for use in both military and civilian radar and communication applications. Small antennas and other small components are made possible by microwave frequency applications. The size advantage can be considered as part of a solution to problems of space, or weight, or both. Microwave frequency usage is significant for the design of shipboard radar because it makes possible the detection of smaller targets. Microwave frequencies present special problems in transmission, generation, and circuit design that are not encountered at lower frequencies. Conventional circuit theory is based on voltages and currents while microwave theory is based on electromagnetic fields.\n\nApparatus and techniques may be described qualitatively as \"microwave\" when the wavelengths of signals are roughly the same as the dimensions of the equipment, so that lumped-element circuit theory is inaccurate. As a consequence, practical microwave technique tends to move away from the discrete resistors, capacitors, and inductors used with lower frequency radio waves. Instead, distributed circuit elements and transmission-line theory are more useful methods for design and analysis. Open-wire and coaxial transmission lines give way to waveguides and stripline, and lumped-element tuned circuits are replaced by cavity resonators or resonant lines. Effects of reflection, polarization, scattering, diffraction and atmospheric absorption usually associated with visible light are of practical significance in the study of microwave propagation. The same equations of electromagnetic theory apply at all frequencies.\n\nThe microwave engineering discipline has become relevant as the microwave domain moves into the commercial sector, and no longer only applicable to 20th and 21st century military technologies. Inexpensive components and digital communications in the microwave domain have opened up areas pertinent to this discipline. Some of these areas are radar, satellite, wireless radio, optical communication, faster computer circuits, and collision avoidance radar.\n\nColleges and universities offer microwave engineering.\n\nThe University of Massachusetts Amherst provides research and educational programs in microwave remote sensing, antenna design and communications systems. Courses and project work are offered leading toward graduate degrees. Specialties include microwave and RF integrated circuit design, antenna engineering, computational electromagnetics, radiowave propagation, radar and remote sensing systems, image processing, and THz imaging.\n\nTufts University offers a \"Microwave and Wireless Engineering\" certificate program as part of its graduate studies programs. It can be applied toward a master's degree in electrical engineering. The student must have an appropriate bachelor's degree to enroll in this program.\n\nAuburn University offers research for the microwave arena. Wireless Engineering Research and Education Center is one of three research centers. The university also offers a Bachelor of Wireless Engineering degree with a Wireless Electrical Engineering major.\n\nBradley University offers an undergraduate and a graduate degree in its Microwave and Wireless Engineering Program. It has an Advanced Microwave Laboratory, a Wireless Communication Laboratory and other facilities related to research.\n\nThere are professional societies pertinent to this discipline:\n\nThe IEEE Microwave Theory and Techniques Society (MTT-S) \"promotes the advancement of microwave theory and its applications...\". The society also publishes peer reviewed journals, and one magazine.\n\nThere are peer reviewed journals and other scholarly periodicals that cover topics that pertains to microwave engineering. Some of these are IEEE Transactions on Microwave Theory and Techniques, IEEE Microwave and Wireless Components Letters, Microwave Magazine, IET Microwaves, Antennas & Propagation, and Microwave Journal.\n\n"}
{"id": "52303799", "url": "https://en.wikipedia.org/wiki?curid=52303799", "title": "Multi-Evaporator System", "text": "Multi-Evaporator System\n\nA Multi-evaporator system is a vapor-compression refrigeration system generally consisting of four major components:\n\nSometimes in a refrigerator several loads are varied. Refrigerators used to function at different loads operated under different condition of temperatures and pressures. There may be arrangements possible for multi evaporators on the basis of single or multi compressors. If refrigerant from each evaporator compressed in the same single compressor then it is called as Multi-evaporator single-compressor system.\n\nA two evaporator single compressor with \"individual expansion valves\" for each evaporator after passing through the back pressure valve enters into the compressors and hence there is a significant rise in temperature is observed. \nThis system helps in dropping the pressure from high pressure evaporators with the help of back pressure valves. The high pressure ratio is obtained which necessarily compresses the vapor to high extent from the higher temperature evaporators to condenser temperature. This kind of refrigerator has greater application in load varying purpose. Moreover, high value of COP and better operating economy is observed.\n\nMass flow rate through evaporators :-\n\nm=Heat through evaporator 1(Q)÷(specific enthalpy difference in evaporator 1)\n\nm=Heat through evaporator 1(Q)÷(specific enthalpy difference in evaporator 2))\n\nThe net Work done is :-\n"}
{"id": "1898450", "url": "https://en.wikipedia.org/wiki?curid=1898450", "title": "Opaque predicate", "text": "Opaque predicate\n\nIn computer programming, an opaque predicate is a predicate—an expression that evaluates to either \"true\" or \"false\"—for which the outcome is known by the programmer \"a priori\", but which, for a variety of reasons, still needs to be evaluated at run time. Opaque predicates have been used as watermarks, as it will be identifiable in a program's executable. They can also be used to prevent an overzealous optimizer from optimizing away a portion of a program. Another use is in obfuscating the control or dataflow of a program to make reverse engineering harder.\n\n"}
{"id": "51160", "url": "https://en.wikipedia.org/wiki?curid=51160", "title": "Photophone", "text": "Photophone\n\nThe photophone is a telecommunications device that allows transmission of speech on a beam of light. It was invented jointly by Alexander Graham Bell and his assistant Charles Sumner Tainter on February 19, 1880, at Bell's laboratory at 1325 L Street in Washington, D.C. Both were later to become full associates in the Volta Laboratory Association, created and financed by Bell.\n\nOn June 3, 1880, Bell's assistant transmitted a wireless voice telephone message from the roof of the Franklin School to the window of Bell's laboratory, some 213 meters (about 700 ft.) away.\n\nBell believed the photophone was his most important invention. Of the 18 patents granted in Bell's name alone, and the 12 he shared with his collaborators, four were for the photophone, which Bell referred to as his \"greatest achievement\", telling a reporter shortly before his death that the photophone was \"the greatest invention [I have] ever made, greater than the telephone\".\n\nThe photophone was a precursor to the fiber-optic communication systems that achieved worldwide popular usage starting in the 1980s. The master patent for the photophone ( \"Apparatus for Signalling and Communicating, called Photophone\") was issued in December 1880, many decades before its principles came to have practical applications.\n\nThe photophone was similar to a contemporary telephone, except that it used modulated light as a means of wireless transmission while the telephone relied on modulated electricity carried over a conductive wire circuit.\n\nBell's own description of the light modulator:\n\nThe brightness of a reflected beam of light, as observed from the location of the receiver, therefore varied in accordance with the audio-frequency variations in air pressure—the sound waves—which acted upon the mirror.\n\nIn its initial form, the photophone receiver was also non-electronic, using the photoacoustic effect. Bell found that many substances could be used as direct light-to-sound transducers. Lampblack proved to be outstanding. Using a fully modulated beam of sunlight as a test signal, one experimental receiver design, employing only a deposit of lampblack, produced a tone that Bell described as \"painfully loud\" to an ear pressed close to the device.\n\nIn its ultimate electronic form, the photophone receiver used a simple selenium cell photodetector at the focus of a parabolic mirror. The cell's electrical resistance (between about 100 and 300 ohms) varied inversely with the light falling upon it, i.e., its resistance was higher when dimly lit, lower when brightly lit. The selenium cell took the place of a carbon microphone—also a variable-resistance device—in the circuit of what was otherwise essentially an ordinary telephone, consisting of a battery, an electromagnetic earphone, and the variable resistance, all connected in series. The selenium modulated the current flowing through the circuit, and the current was converted back into variations of air pressure—sound—by the earphone.\n\nIn his speech to the American Association for the Advancement of Science in August 1880, Bell gave credit for the first demonstration of speech transmission by light to Mr. A.C. Brown of London in the Fall of 1878.\n\nBecause the device used radiant energy, the French scientist suggested that the invention should not be named 'photophone', but 'radiophone', as its mirrors reflected the Sun's radiant energy in multiple bands including the invisible infrared band. Bell used the name for a while but it should not be confused with the later invention \"radiophone\" which used radio waves.\n\nWhile honeymooning in Europe with his bride Mabel Hubbard, Bell likely read of the newly discovered property of selenium having a variable resistance when acted upon by light, in a paper by Robert Sabine as published in \"Nature\" on 25 April 1878. In his experiments, Sabine used a meter to see the effects of light acting on selenium connected in a circuit to a battery. However Bell reasoned that by adding a telephone receiver to the same circuit he would be able to hear what Sabine could only see.\n\nAs Bell's former associate, Thomas Watson, was fully occupied as the superintendent of manufacturing for the nascent Bell Telephone Company back in Boston, Massachusetts, Bell hired Charles Sumner Tainter, an instrument maker who had previously been assigned to the U.S. 1874 Transit of Venus Commission, for his new 'L' Street laboratory in Washington, at the rate of $15 per week.\n\nOn February 19, 1880 the pair had managed to make a functional photophone in their new laboratory by attaching a set of metallic gratings to a diaphragm, with a beam of light being interrupted by the gratings movement in response to spoken sounds. When the modulated light beam fell upon their selenium receiver Bell, on his headphones, was able to clearly hear Tainter singing \"Auld Lang Syne\".\n\nIn an April 1, 1880 Washington, D.C. experiment, Bell and Tainter communicated some along an alleyway to the laboratory's rear window. Then a few months later on June 21 they succeeded in communicating clearly over a distance of some 213 meters (about 700 ft.), using plain sunlight as their light source, practical electrical lighting having only just been introduced to the U.S. by Edison. The transmitter in their latter experiments had sunlight reflected off the surface of a very thin mirror positioned at the end of a speaking tube; as words were spoken they cause the mirror to oscillate between convex and concave, altering the amount of light reflected from its surface to the receiver. Tainter, who was on the roof of the Franklin School, spoke to Bell, who was in his laboratory listening and who signaled back to Tainter by waving his hat vigorously from the window, as had been requested.\n\nThe receiver was a parabolic mirror with selenium cells at its focal point. Conducted from the roof of the Franklin School to Bell's laboratory at 1325 'L' Street, this was the world's first formal wireless telephone communication (away from their laboratory), thus making the photophone the world's earliest known voice wireless telephone systems, at least 19 years ahead of the first spoken radio wave transmissions. Before Bell and Tainter had concluded their research in order to move on to the development of the Graphophone, they had devised some 50 different methods of modulating and demodulating light beams for optical telephony.\n\nThe telephone itself was still something of a novelty, and radio was decades away from commercialization. The social resistance to the photophone's futuristic form of communications could be seen in an August 1880 \"New York Times\" commentary:\n\nHowever at the time of their February 1880 breakthrough, Bell was immensely proud of the achievement, to the point that he wanted to name his new second daughter \"Photophone\", which was subtly discouraged by his wife Mabel Bell (they instead chose \"Marian\", with \"Daisy\" as her nickname). He wrote somewhat enthusiastically:\n\nBell transferred the photophone's intellectual property rights to the American Bell Telephone Company in May 1880. While Bell had hoped his new photophone could be used by ships at sea and to also displace the plethora of telephone lines that were blooming along busy city boulevards, his design failed to protect its transmissions from outdoor interferences such as clouds, fog, rain, snow and such, that could easily disrupt the transmission of light. Factors such as the weather and the lack of light inhibited the use of Bell's invention. Not long after its invention laboratories within the Bell System continued to improve the photophone in the hope that it could supplement or replace expensive conventional telephone lines. Its earliest non-experimental use came with military communication systems during World War I and II, its key advantage being that its light-based transmissions could not be intercepted by the enemy.\n\nBell pondered the photophone's possible scientific use in the spectral analysis of artificial light sources, stars and sunspots. He later also speculated on its possible future applications, though he did not anticipate either the laser or fiber-optic telecommunications:\n\nAlthough Bell Telephone researchers made several modest incremental improvements on Bell and Tainter's design, Marconi's radio transmissions started to far surpass the maximum range of the photophone as early as 1897 and further development of the photophone was largely arrested until German-Austrian experiments began at the turn of the 20th century.\n\nThe German physicist Ernst Ruhmer believed that the increased sensitivity of his improved selenium cells, combined with the superior receiving capabilities of professor H. T. Simon's \"speaking arc\", would make the photophone practical over longer signalling distances. Ruhmer carried out a series of experimental transmissions along the Havel river and on Lake Wannsee from 1901 to 1902. He reported achieving sending distances under good conditions of 15 kilometers (9 miles), with equal success during the day and at night. He continued his experiments around Berlin through 1904, in conjunction with the German Navy, which supplied high-powered searchlights for use in the transmissions.\n\nThe German Siemens & Halske Company boosted the photophone's range by utilizing current-modulated carbon arc lamps which provided a useful range of approximately . They produced units commercially for the German Navy, which were further adapted to increase their range to using voice-modulated ship searchlights.\n\nBritish Admiralty research during WWI resulted in the development of a vibrating mirror modulator in 1916. More sensitive molybdenite receiver cells, which also had greater sensitivity to infra-red radiation, replaced the older selenium cells in 1917. The United States and German governments also worked on technical improvements to Bell's system.\n\nBy 1935 the German Carl Zeiss Company had started producing infra-red photophones for the German Army's tank battalions, employing tungsten lamps with infra-red filters which were modulated by vibrating mirrors or prisms. These also used receivers which employed lead sulfide detector cells and amplifiers, boosting their range to under optimal conditions. The Japanese and Italian armies also attempted similar development of lightwave telecommunications before 1945.\n\nSeveral military laboratories, including those in the United States, continued R&D efforts on the photophone into the 1950s, experimenting with high-pressure vapour and mercury arc lamps of between 500 and 2,000 watts power.\n\nOn March 3, 1947, the centenary of Alexander Graham Bell's birth, the Telephone Pioneers of America dedicated a historical marker on the side of one of the buildings, the Franklin School, which Bell and Sumner Tainter used for their first formal trial involving a considerable distance. Tainter had originally stood on the roof of the school building and transmitted to Bell at the window of his laboratory. The marker did not acknowledge Tainter's scientific and engineering contributions.\n\nOn February 19, 1980, exactly 100 years to the day after Bell and Tainter's first photophone transmission in their laboratory, staff from the Smithsonian Institution, the National Geographic Society and AT&T's Bell Labs gathered at the location of Bell’s former 1325 'L' Street Volta Laboratory in Washington, D.C. for a commemoration of the event.\n\nThe Photophone Centenary commemoration had first been proposed by electronics researcher and writer Forrest M. Mims, who suggested it to Dr. Melville Bell Grosvenor, the inventor's grandson, during a visit to his office at the National Geographic Society. The historic grouping later observed the centennial of the photophone's first successful laboratory transmission by using Mims hand-made demonstration photophone, which functioned similar to Bell and Tainter's model.\n\nMims also built and provided a pair of modern hand-held battery-powered LED transceivers connected by of optical fiber. The Bell Labs' Richard Gundlach and the Smithsonian's Elliot Sivowitch used the device at the commemoration to demonstrate one of the photophone's modern-day descendants. The National Geographic Society also mounted a special educational exhibit in its Explorer's Hall, highlighting the photophone's invention with original items borrowed from the Smithsonian Institution.\n\n Footnotes \n\n"}
{"id": "273703", "url": "https://en.wikipedia.org/wiki?curid=273703", "title": "Power management", "text": "Power management\n\nPower Management is a feature of some electrical appliances, especially copiers, computers, GPUs and computer peripherals such as monitors and printers, that turns off the power or switches the system to a low-power state when inactive. In computing this is known as PC power management and is built around a standard called ACPI. This supersedes\nAPM. All recent (consumer) computers have ACPI support.\n\nIn the military, \"Power Management\" often refers to suites of equipment which permit soldiers and squads to share diverse energy sources, powering often incompatible equipment.\n\nPC power management for computer systems is desired for many reasons, particularly:\n\nLower power consumption also means lower heat dissipation, which increases system stability, and less energy use, which saves money and reduces the impact on the environment.\n\nThe power management for microprocessors can be done over the whole processor, or in specific components, such as cache memory and main memory.\n\nWith dynamic voltage scaling and dynamic frequency scaling, the CPU core voltage, clock rate, or both, can be altered to decrease power consumption at the price of potentially lower performance. This is sometimes done in real time to optimize the power-performance tradeoff.\n\nExamples:\n\nAdditionally, processors can selectively power off internal circuitry (power gating). For example:\n\nIntel VRT technology split the chip into a 3.3V I/O section and a 2.9V core section. The lower core voltage reduces power consumption.\n\nARM's big.LITTLE architecture can migrate processes between faster \"big\" cores and more power efficient \"LITTLE\" cores.\n\nWhen a computer system hibernates it saves the contents of the RAM to disk and powers down the machine. On startup it reloads the data. This allows the system to be completely powered off while in hibernate mode. This requires a file the size of the installed RAM to be placed on the hard disk, potentially using up space even when not in hibernate mode. Hibernate mode is enabled by default in some versions of Windows and can be disabled in order to recover this disk space.\n\nGraphics processing unit (GPUs) are used together with a CPU to accelerate computing in variety of domains revolving around scientific, analytics, engineering, consumer and enterprise applications.\nAll of this do come with some drawbacks, the high computing capability of GPUs comes at the cost of high power dissipation. A lot of research has been done over the power dissipation issue of GPUs and a lot of different techniques have been proposed to address this issue.\nDynamic voltage scaling/Dynamic frequency scaling(DVFS) and clock gating are two commonly used techniques for reducing dynamic power in GPUs.\n\nExperiments show that conventional processor DVFS policy can achieve power reduction of embedded GPUs with reasonable performance degradation. New directions for designing effective DVFS schedulers for heterogeneous systems are also being explored. A heterogeneous CPU-GPU architecture, GreenGPU is presented which employs DVFS in a synchronized way, both for GPU and CPU. GreenGPU is implemented using the CUDA framework on a real physical testbed with Nvidia GeForce GPUs and AMD Phenom II CPUs. Experimentally it is shown that the GreenGPU achieves 21.04% average energy saving and outperforms several well-designed baselines.\nFor the mainstream GPUs which are extensively used in all kinds of commercial and personal applications several DVFS techniques exist and are built into the GPUs alone, AMD PowerTune and AMD ZeroCore Power are the two dynamic frequency scaling technologies for AMD graphic cards. Practical tests showed that reclocking a Geforce GTX 480 can achieve a 28% lower power consumption while only decreasing performance by 1% for a given task.\n\nA lot of research has been done on the dynamic power reduction with the use of DVFS techniques. However, as technology continues to shrink, leakage power will become a dominant factor. Power gating is a commonly used circuit technique to remove leakage by turning off the supply voltage of unused circuits. Power gating incurs energy overhead; therefore, unused circuits need to remain idle long enough to compensate this overheads.\nA novel micro-architectural technique for run-time power-gating caches of GPUs saves leakage energy. Based on experiments on 16 different GPU workloads, the average energy savings achieved by the proposed technique is 54%.\nShaders are the most power hungry component of a GPU, a predictive shader shut down power gating technique achieves up to 46% leakage reduction on shader processors.\nThe Predictive Shader Shutdown technique exploits workload variation across frames to eliminate leakage in shader clusters. Another technique called Deferred Geometry Pipeline seeks to minimize leakage in fixed-function geometry units by utilizing an imbalance between geometry and fragment computation across batches which removes up to 57% of the leakage in the fixed-function geometry units. A simple time-out power gating method can be applied to non-shader execution units which eliminates 83.3% of the leakage in non-shader execution units on average.\nAll the three techniques stated above incur negligible performance degradation, less than 1%.\n\n\n"}
{"id": "4037385", "url": "https://en.wikipedia.org/wiki?curid=4037385", "title": "Professional audio", "text": "Professional audio\n\nProfessional audio, abbreviated as pro audio, refers to both an activity and a category of high quality, studio-grade audio equipment. Typically it encompasses sound recording, sound reinforcement system setup and audio mixing, and studio music production by trained sound engineers, audio engineers, record producers, and audio technicians who work in live event support and recording using audio mixers, recording equipment and sound reinforcement systems. In contrast, consumer audio equipment is a lower grade of gear which is used by regular people for the reproduction of sound in a private home on a home stereo or home cinema system.\n\nProfessional audio can include, but is not limited to broadcast radio, audio mastering in a recording studio, television studio, and sound reinforcement such as a live concert, DJ performances, audio sampling, public address system set up, surround sound design in movie theatres, and design and setup of piped music in hotels and restaurants. Professional audio equipment is sold at professional audio stores and music stores. While consumer electronics stores sell some of the same categories of equipment (e.g., power amplifiers and subwoofer cabinets), the equipment that consumer stores sells is a lower consumer-grade type of equipment, which does not meet the standards for low noise and low distortion that are required in pro audio applications.\n\nThe term \"professional audio\" has no precise definition, but it typically includes:\nCompared to consumer-grade audio equipment, professional audio equipment tends to have such characteristics as:\n\nThe broadcast quality of professional audio equipment is on a par with that of consumer high-end audio and hi-fi equipment, but is more likely to be designed purely on sound engineering principles and owes little to the consumer-oriented audiophile sub-culture.\n\nA professional audio store is a retail establishment that sells, and in many cases rents, expensive, high-end sound recording equipment (microphones, audio mixers, digital audio recorders, speakers and surround sound speakers, monitor speakers) and sound reinforcement system gear (e.g., speaker enclosure cabinets, stage monitor speakers, power amplifiers, subwoofer cabinets) and accessories used in both settings, such as microphone stands. Some pro audio stores also sell video equipment, such as video projectors, as this equipment is commonly used in live audio settings (e.g., business presentations and conventions). Some pro audio stores also sell and/or rent DJ gear (record turntables, DJ mixers) and the stage lighting equipment used in rock concerts, dance clubs, raves and theater/musical theater shows.\n\n"}
{"id": "44121317", "url": "https://en.wikipedia.org/wiki?curid=44121317", "title": "Prontohotel", "text": "Prontohotel\n\nProntoHotel is a free hotel travel metasearch engine founded in 2007 with headquarters in Rome, Italy.\n\nThe site compares prices for over 550,000 hotels by searching multiple travel booking sites, including Booking.com and Expedia. The site is available in 220 countries and in 20 languages.\n\nProntoHotel was founded in May 2007 by two Italian experts in online travel and marketing: Paolo Mazzara and Simone Giacco.\n\nTheir goal was to meet the increasing needs of travelers to find a simple travel comparison site to help cut down on time spent online researching hotels and rates.\n\nThe beta version of ProntoHotel launched in 2008 with a small number of partnerships with online travel agencies and hotel chains in just the big-name European destinations, such as Rome, Paris, London, Berlin, and Amsterdam.\n\nIn 2009, the company implemented new software features, including the ability to filter results by a hotel’s location within a particular area of the city, desired price range per night, and proximity to a specific address.\n\nBy 2011, the database of hotels grew to over 200,000 and the portal was opened up to additional markets, including Spain (prontohotel.es), France (prontohotel.fr), and Germany (prontohotel.de).\n\nIn 2013, ProntoHotel added a number of partner websites and doubled the amount of hotels in its database. The company also created the ProntoHotel Price Index (PHPI), which analyzes the hotel market and issues reports based on time period and region.\n\nIn 2014, ProntoHotel was nominated for the World Travel Awards in the Travel Technology category for “World’s Leading Hotel Comparison Website” along with Kayak.com, Hipmunk, HotelsCombined, Momondo, and Trivago. That same year, the site’s database included 550,000 hotels, 20 languages, and 35 currencies.\n\nProntoHotel operates on its own proprietary hotel metasearch engine software. The system searches the results from a database of 50 partner travel websites (hotel chain websites, independent hotel websites, and Online Travel Agency (OTA)), and aggregates the rates into a condensed list of accommodation options (including hotels, bed and breakfasts, hostels, and resorts) that meet the search criteria. \n\nThe hotel price comparison engine lists amenities of each property, including consumer ratings, industry awards, bathroom type, meeting facilities, and dining options, so that users may have the necessary facts to compare accommodations. Further information about each property is provided, including hotel rating, amenities, photos, location, and original hotel reviews written by previous ProntoHotel users.\n\nSearch results may be viewed in two formats: map view or list view. Filters help rank hotels according to price, star rating, and popularity, as well as geographical parameters such as proximity to a certain attraction, position within a neighborhood, and distance from the city center. \n\nThe hotel comparison site offers two features to further personalize search results: one is to remove undesirable hotels from the list of options and the other is to add hotels of interest to a “favorite list.” \n\nOnce a hotel and rate are chosen, the travel technology redirects to the third-party vendor selling that deal. The transaction is performed directly with that supplier and all further business is conducted through that vendor. \n"}
{"id": "5222704", "url": "https://en.wikipedia.org/wiki?curid=5222704", "title": "Racking", "text": "Racking\n\nRacking, often referred to as Soutirage or Soutirage traditionnel (meaning racking in French), also filtering or fining, is a method in wine production of moving wine from one barrel to another using gravity rather than a pump, which can be disruptive to a wine. The process is also known as \"Abstich\" in German and \"travaso\" in Italian.\n\n\"Alexis Lichine's Encyclopedia of Wines and Spirits\" defines racking as \"siphoning wine or beer off the lees (in the case of wine) or trub (in the case of beer), into a new, clean barrel or other vessel.\" Racking allows clarification and aids in stabilization. Wine that is allowed to age on the lees often develops \"off-tastes.\" A racking hose or tube is used and can be attached to a racking cane to make the task easier. The racking process is repeated several times during the aging of wine.\n\nRacking or soutirage is a traditional method in wine production of moving wine from one barrel to another using gravity rather than a pump. The process is repeated when the casks are moved to the second-year cellar. Soutirage was developed in the Bordeaux region of France in the 19th century at a time when there was no electricity to power pumps. Many estates such in Bordeaux and some estates in Pomerol and St. Emilion still employ this labor-intensive method. During aging, the wine is decanted several times from barrel to barrel. This process softens tannins, clarifies the wine and enhances aromatic qualities. According to Oz Clarke, \"traditionally the wine is 'racked' or drawn from barrel or tank to another empty one on a number of occasions (called soutirage). This helps clarify and freshen the wine by removing the fine lees or sediment which forms and provides a tiny amount of oxygen to help the aging process. The wine may also be 'fined' (\"collage\") using egg white or other materials to settle out particles in suspension which are then removed through further racking.\" Egg white is often applied to each barrel during the process.\n\nA racking hose is a flexible, plastic hose, used to siphon wine or beer from one vessel to another. It is used in both racking and bottling operations. A racking cane is a rigid tube, often bent or \"L\"-shaped, that is attached to the racking hose to make racking easier. A protective cap is placed over the lower end of the cane that allows liquid to be drawn into the cane from above rather than below while keeping most large solids out. The cap allows the tip of the cane to be lowered close to the lees without unduly disturbing them. The lower tip of the racking cane should initially be held about midway between the surface and the lees and gradually lowered as the volume decreases due to the siphoning.\n\n"}
{"id": "338740", "url": "https://en.wikipedia.org/wiki?curid=338740", "title": "Rational Software", "text": "Rational Software\n\nRational Machines was founded by Paul Levy and Mike Devlin in 1981 to provide tools to expand the use of modern software engineering practices, particularly explicit modular architecture and iterative development. It changed its name in 1994 to Rational Software, and was sold for US$2.1 billion (equivalent to current US$) to IBM on February 20, 2003.\n\nFirst released in 1985, the Rational Environment was an integrated development environment for the Ada programming language, which provided good support for abstraction through strong typing. Its goal was to provide the productivity benefits associated with academic single-user programming environments to teams of developers developing mission-critical applications that could execute on a range of computing platforms.\n\nThe Rational Environment was organized around a persistent intermediate representation (DIANA), providing users with syntactic and semantic completion, incremental compilation, and integrated configuration management and version control. To overcome a conflict between strong typing and iterative development that produced recompilation times proportional to system size rather than size-of-change, the Rational Environment supported the definition of subsystems with explicit architectural imports and exports; this mechanism later proved useful in protecting application architectures from inadvertent degradation. The Environment's Command Window mechanism made it easy to directly invoke Ada functions and procedures, which encouraged developer-driven unit testing.\n\nThe Rational Environment ran on custom hardware, the Rational R1000, which implemented a high-level architecture optimized for execution of Ada programs in general and the Rational Environment in particular. The horizontally-microprogrammed R1000 provided two independent 64-bit data paths, permitting simultaneous computation and type checking. Memory was organized as a single-level store; a 64-bit virtual address presented to the memory system either immediately returned data, or triggered a page fault handled by the processor's microcode.\n\nThe company's name was later changed from \"Rational Machines\" to Rational Software Corporation (RATL) to avoid emphasizing this proprietary hardware when Rational merged with Verdix Corporation, a public company that developed Ada compilers, on 30 March 1994.\n\nRational provided code generators and the cross-debuggers for then-popular instruction set architectures such as the VAX, Motorola 68000, and x86; much of this was accomplished through a partnership with Tartan Labs, founded by Bill Wulf to commercialize his work on optimizing code generators semi-automatically produced from architecture descriptions (PQCC).\n\nRational's field Practices underlying the later Rational Unified Process (RUP) - iterative development, component-based architecture, modelling, continuous developer-driven testing, requirements management, and automated testing—are all traceable to this experience base.\n\nIn 1990, Rational launched three parallel development efforts: re-implementation of the Rational Environment (for Ada) to run on Unix-based workstations from Sun and IBM, development of a comparable Rational Environment for C++ to run on Unix-based workstations from Sun and IBM, and development of a workstation-hosted modeling tool called Rose that supported a graphical notation developed by Grady Booch. Apex, the Rational Environment for Ada, was launched on Sun and IBM Unix platforms in 1993, and the Rational Environment for C++ followed on the same platforms a year later. A version of Apex that ran on Microsoft Windows NT was successfully developed and released by Rational's Bangalore team.\n\nRose 1.0 was introduced at OOPSLA in 1992, but performed poorly in multiple dimensions and was withdrawn from the market.\n\nThe development of Rose 2.0 combined a Windows-based Booch notation editor called Object System Designer (acquired from Wisconsin-based Palladio) with a new intermediate representation, and with new semantic analysis, code generation, and reverse engineering capabilities. The latter, which allowed prospective customers to analyze existing C++ code to produce \"as-built\" navigable class diagrams, helped overcome Rational's late re-entry into the market for object-oriented modeling tools. Rose 2.0 ran on Windows PCs and on several Unix-based workstations.\n\nIn 1994, Rational merged with Verdix, a public company that produced a wide array of Ada compilers targeted to many architecture/OS combinations. The resulting entity was named \"Rational Software\", and promptly integrated the Rational Ada and C++ environments with the code generators and runtimes developed by Verdix.\n\nIn 1995, James Rumbaugh joined the company, and Rational acquired Ivar Jacobson's firm Objectory AB from Ericsson. With Grady Booch already aboard, this brought within one company three of the leading object-oriented software methodologists. These three experts attempted to unify their work. To eliminate the method fragmentation that they concluded was impeding commercial adoption of modeling tools, they developed Unified Modeling Language (UML), which provided a level playing field for all tool vendors. It was this collaboration effort that earned Rumbaugh, Jacobson and Booch the moniker \"The Three Amigos\" within the software engineering industry. At its 1.0 release, the Unified Modeling Language was contributed to the Object Management Group, which has managed its subsequent development.\n\nPhilippe Kruchten, a Rational techrep, was tasked with the assembly of an explicit process framework for modern software engineering. This effort combined the HTML-based process delivery mechanism employed by Objectory with Rational's 15-year experience base in working with customers developing significant software systems. The resulting \"Rational Unified Process\" (RUP) completed a strategic tripod:\n\nThe momentum generated by Rose and the UML enabled Rational to establish a partnership with Windows platform developers. Rational's aim was to secure Microsoft's public support for visual modeling.\n\nRational peaked at US$850M in revenues (current equivalent US$) and 4000 employees. After the dot-com crash, its revenues declined to $650M, but it was dominant, profitable, and cash-rich (~$600M) when its founders chose to sell the company to IBM for $2.1B (current equivalent US$). The acquisition was announced on 6 December 2002 and was completed before the market opened 21 February 2003.\n\n\n\n"}
{"id": "21498620", "url": "https://en.wikipedia.org/wiki?curid=21498620", "title": "Shai Reshef", "text": "Shai Reshef\n\nShai Reshef born September 11, 1955 is an Israeli educational entrepreneur, founder and president of University of the People, the world's first non-profit, tuition-free, accredited online university. His TED Talk, entitled \"An ultra-low-cost college degree\", has been viewed more than 5 million times.\n\nReshef earned an MA in Chinese Politics from the University of Michigan. From 1989 to 2005, Reshef served as Chief Executive Officer and then Chairman of the Kidum Group, a for-profit educational services and test preparation company. Reshef joined Kidum in 1989 when it was a single product company with revenues of $100,000. Under his leadership, Kidum grew to become a company with annual revenues in excess of $25 million, with over 1,000 employees and 50,000 students a year. In 2005, he sold the company to Kaplan, Inc., one of the world’s largest education companies and a subsidiary of the Washington Post.\n\nBetween 2001 and 2004, while continuing as the chairman of Kidum, Reshef lived in the Netherlands where he chaired KIT eLearning, a subsidiary of Kidum. KIT became the eLearning partner of the University of Liverpool in 1999, and was the first online university outside of the United States, providing MBA and M.Sc. degrees in Information Technology. In 2004, the company was acquired by Laureate Education, Inc. and changed its name to Laureate Online Education. It maintains its partnership with the University of Liverpool under its new name.\n\nIn January 2009, Reshef founded University of the People (UoPeople), a non-profit, accredited online academic institution that seeks to increase the availability of higher education by offering tuition-free degrees.\n\nReshef is married and has four children. He lives in Pasadena, California.\n\n\n"}
{"id": "589997", "url": "https://en.wikipedia.org/wiki?curid=589997", "title": "Silicon Fen", "text": "Silicon Fen\n\nSilicon Fen (sometimes known as the Cambridge Cluster) is the name given to the region around Cambridge, England, which is home to a large cluster of high-tech businesses focusing on software, electronics and biotechnology. Many of these businesses have connections with the University of Cambridge, and the area is now one of the most important technology centres in Europe.\n\nIt is called \"Silicon Fen\" by analogy with Silicon Valley in California, because it lies at the southern tip of the English Fenland. The interest in technology in the area started with Acorn Computers.\n\nMore than 1000 high-technology companies established offices in the area during the five years preceding 1998. Some early successful businesses were Advanced RISC Machines and Cambridge Display Technology. In 2004, 24% of all UK venture capital (8% of all the EU's) was received by Silicon Fen companies, according to the Cambridge Cluster Report 2004 produced by Library House and Grant Thornton.\n\nThe so-called \"Cambridge phenomenon\", giving rise to start-up companies in a town previously only having a little light industry in the electrical sector, is usually dated to the founding of the Cambridge Science Park in 1970: this was an initiative of Trinity College, Cambridge University and moved away from a traditional low-development policy for Cambridge.\n\nThe characteristic of Cambridge is small companies (as few as three people, in some cases) in sectors such as computer-aided design. Over time the number of companies has grown; it has not proved easy to count them, but recent estimates have placed the number anywhere between 1,000 and 3,500 companies. They are spread over an area defined perhaps by the CB postcode or 01223 telephone area code, or more generously in an area bounded by Ely, Newmarket, Saffron Walden, Royston and Huntingdon.\n\nIn 2000, then Chancellor of the Exchequer Gordon Brown set up a research partnership between MIT and Cambridge University, the Cambridge–MIT Institute, in order to increase international collaboration between the two universities and to strengthen the economic success of Silicon Fen.\n\nIn February 2006, the Judge Business School, Cambridge University reported estimates that suggested that at that time, there were around 250 active start-ups directly linked to the University, valued at around US$6 billion. Only a tiny proportion of these companies have so far grown into multinationals: ARM, Autonomy Corporation and AVEVA are the most obvious examples, and more recently CSR has seen rapid growth due to the uptake of Bluetooth.\n\nIt was found in 2012 that strong employment growth was hampered due to the concentration on research and development. This was because of limited competition in manufacturing capability and its consequent cost.\n\nRecent work by Cambridge Ahead, the business and academic membership organisation dedicated to the successful growth of the city region in the long-term, has shown that in 2015-16, growth of Cambridge companies continued at around 7% on a one, three and five-year view. Global turnover of Cambridge companies increased by 7.6% to £35.7bn, up from £33bn the previous year, and global employment grew by 7.6% to 210,292. The number of companies with their home base within 20 miles of Cambridge has grown from 22,017 to 24,580.\n\nWhat is striking about the data is that 2015-16 was just another typical 7% growth year for Cambridge. Over the past five years (2010-11 to 2015-16) the turnover of Cambridge companies has grown by 7.5% per annum, and employment by 6.6% per annum.\n\nThe growth of Silicon Fen is best revealed by the Cambridge Cluster Map, a new data tool that accurately maps the growth of the Cambridge sub-region over time and reveals the true extent of the success of the ‘Cambridge Phenomenon’. The map was created and launched in July 2016 by Cambridge Ahead in partnership with Barclays, working with the Centre for Business Research (CBR) at the University of Cambridge, and is an updated version of the previous Cluster Map covering the Tech sector launched in 2012.\n\nThe new dataset identifies and locates companies and certain research facilities that are active within a 20-mile radius of the centre of the city and shows three types of organisations: Cambridge-based companies, Cambridge-active companies, and Non-corporate Knowledge-intensive (KI) organisations.\n\nThe region has one of the most flexible job markets in the technology sector, and people are often employed by other companies after a start-up fails. Although everyone wants their company to succeed, failures are tolerated, indeed almost expected.\n\nOne reason for the area's success is that after some time such an employment market is self-sustaining, since employees are willing to move to an area that promises a future beyond any one company. Another factor is the high degree of 'networking', enabling people across the region to find partners, jobs, funding, and know-how. Organisations have sprung up to facilitate this process, for example the Cambridge Network.\n\nAnother factor is the academic pre-eminence of Cambridge University, which is one of the top 5 universities in the world, a high standard of living available in the county, and good transport links, for example to London and with Cambridge Airport having a full service business jet centre. Many graduates from the university choose to stay on in the area, giving local companies a rich pool of talent to draw upon . The high-technology industry has little by way of competition, unlike say in Oxfordshire where many other competing industries exist. Because Cambridgeshire was not until recently a high-technology centre, commercial rents were generally lower than in other parts of the UK, giving companies a head-start on those situated in other more expensive regions; this has, however, recently changed and Cambridgeshire now has one of the highest costs of living in the UK outside London.\n\n\n\n"}
{"id": "10241472", "url": "https://en.wikipedia.org/wiki?curid=10241472", "title": "Subsoiler", "text": "Subsoiler\n\nA subsoiler or flat lifter is a tractor-mounted farm implement used for deep tillage, loosening and breaking up soil at depths below the levels worked by moldboard ploughs, disc harrows, or rototillers. Most such tools will break up and turn over surface soil to a depth of , whereas a subsoiler will break up and loosen soil to twice those depths. Typically a subsoiler mounted on a compact utility tractor will reach depths of about and typically have only one thin blade with a sharpened tip.\n\nThe subsoiler is a tillage tool which will improve growth in all crops where soil compaction is a problem. In agriculture angled wings are used to lift and shatter the hard pan that builds up due to compaction. The design provides deep tillage, loosening soil depth is deeper than a tiller or plough is capable of reaching. Agricultural subsoilers, according to the Unverferth Company, can disrupt hardpan ground down to depths.\n\nVarious manufacturers' brochures claim that crops perform well during hot and dry seasons because roots penetrate soil layers deeper to reach moisture and nutrients. Brochures further claim that in wet conditions, the water passes more easily through the shattered areas, reducing the possibility of crops drowning.\n\nAgricultural subsoiler implements will have multiple deeper reaching blades; each blade is called a scarifier or shank. Purdue University's Deptartment of Agriculture indicates that common subsoilers for agricultural use are available with three, five or seven shanks. Subsoilers can be up to wide, some models are towed behind tractors while others are mounted to the three-point hitch.\n\nOne type of subsoiler has a torpedo-shaped tip and is called a mole plough because the tip describes a path much like the burrow that a mole creates. Mole ploughs are used to create tile drainage, with or without tiles or tile line added. A form of this implement (with a single blade), a pipe-and-cable-laying plough, is used to lay buried cables or pipes, without the need to dig a deep trench and re-fill it.\n\n"}
{"id": "9604195", "url": "https://en.wikipedia.org/wiki?curid=9604195", "title": "SunRiver Data Systems", "text": "SunRiver Data Systems\n\nSunRiver Data Systems was a division of SunRiver Corporation, a private company founded in 1986 in Jackson, Mississippi by four electrical engineers (Ronnie Hughes, Bill Long, Kester Rice, and Gerald Youngblood), all former employees of Diversified Technology, Inc. Initially funded by a local businessman, the company moved to Austin, Texas in 1989 after acquiring venture capital financing from Sevin Rosen Funds and Austin Ventures.\n\nSunRiver developed the first Fiber Optic Station, a color graphics terminal which relied on a proprietary, patented, \"bus extension\" technology in which the parallel data bus of the multi-user computer is serialized and then reconstituted in the terminal device. Custom LSI chips handled both ends of the connection, which later was converted from fiber optic cable using ST connectors, to Category 5 cable. The Cygna 386 Fiber Optic Station was supported with native drivers by the early DR-DOS operating system produced by Digital Research, as well as SCO (Santa Cruz Operation) XENIX and AT&T UNIX. The Cygna technology did not enjoy extensive commercial success, however, likely due to inherent speed limitations and the popularity of a competing approach offered by Citrix which was later adopted by Microsoft.\n\nSunRiver Data Systems later became Boundless Technologies after simultaneously becoming a public corporation and acquiring the Applied Digital Data Systems division of AT&T in 1994. By 1997, all of the original founders had left the company. Sometime after departing Boundless, Youngblood founded FlexRadio, and Long founded Viridian Gold; Rice and Hughes have continued to work in the semiconductor industry. In 2003, Boundless Corporation filed for Chapter 11 bankruptcy protection, was acquired by the majority public stockholders and moved from Hauppauge, NY to Farmingdale, NY, where it produced terminal emulation software, thin client terminals and replacements for old-style text computer terminals. After a series of 2006 transactions the bankrupt but public Boundless Corporation was merged into the seeking-to-be public Haitian Consulting, a manufacturer of fine chemicals. Ironically, Boundless had become a public company by a similar transaction in 1994 when it used the public company, All Quotes Inc., as a merger vehicle to the public market.\n\n"}
{"id": "1747550", "url": "https://en.wikipedia.org/wiki?curid=1747550", "title": "Technology education", "text": "Technology education\n\nTechnology education is the study of technology, in which students \"learn about the processes and knowledge related to technology\". As a field of study, it covers the human ability to shape and change the physical world to meet needs, by manipulating materials and tools with techniques. It addresses the disconnect between wide usage and the lack of knowledge about technical components of technologies used and how to fix them. This emergent discipline seeks to contribute to the learners' overall scientific and technological literacy.\n\nTechnology education should not be confused with educational technology. Educational technology focuses on a more narrow subset of technology use that revolves around the use of technology in and for education as opposed to technology education's focus on technology's use in general.\n\nTechnology education is an offshoot of the Industrial Arts tradition in the United States and the Craft teaching or vocational education in other countries. In 1980, through what was called the \"Futuring Project\", the name of \"industrial arts education\" was changed to be \"technology education\" in New York State; the goal of this movement was to increase students' technological literacy. Since the nature of technology education is significantly different from its predecessor, Industrial Arts teachers underwent inservice education in the mid-1980s while a Technology Training Network was also established by the New York State Education Department (NYSED). \n\nIn Sweden, technology as a new subject emerged from the tradition of crafts subjects while in countries like Taiwan and Australia, its elements are discernible in historical vocational programs.\n\nIn the 21st century, Mars suit design was utilized as a topic for technology education. Technical education is entirely different from general education\n\nTeachThought, a private entity, described technology education as being in the “status of childhood and bold experimentation.” A survey of teachers across the United States by an independent market research company found out that 86 percent of teacher-respondents agree that technology must be used in the classroom. 96 percent say it promotes engagement of students and 89% agree technology improves student outcomes. Technology is present in many education systems. As of July 2018, American public schools provide one desktop computer for every five students and spend over $3 billion annually on digital content. In school year 2015-2016, the government conducted more state-standardized testing for elementary and middle levels through digital platforms instead of the traditional pen and paper method.\n\nThe digital revolution offers fresh learning prospects. Students can learn online even if they are not inside the classroom. Advancement in technology entails new approaches of combining present and future technological improvements and incorporating these innovations into the public education system. Technology space in education is huge. It advances and evolves rapidly. In the United Kingdom, computer technology helped elevate standards in different schools to confront various challenges. The UK adopted the “Flipped Classroom” concept after it become popular in the United States. The idea is to reverse conventional teaching methods through he delivery of instructions online and outside of traditional classrooms.\n\nIn Europe, the European Commission espoused a Digital Education Plan in January 2018. The program consists of 11 initiatives that support utilization of technology and digital capabilities in education development. The Commission also adopted an action plan called the Staff Working Document which details its strategy in implementing digital education. This plan includes three priorities formulating measures to assist European Union member-states to tackle all related concerns. The whole framework will support the European Qualifications Framework for Lifelong Learning and European Classification of Skills, Competences, Qualifications, and Occupations.\n\nIn East Asia, The World Bank co-sponsored a yearly (two-day) international symposium In October 2017 with South Korea’s Ministry of Education, Science, and Technology and the World Bank to support education and ICT concerns for industry practitioners and senior policymakers. Participants plan and discuss issues in use of new technologies for schools within the region.\n\n"}
{"id": "480015", "url": "https://en.wikipedia.org/wiki?curid=480015", "title": "Traffic analysis", "text": "Traffic analysis\n\nTraffic analysis is the process of intercepting and examining messages in order to deduce information from patterns in communication, which can be performed even when the messages are encrypted. In general, the greater the number of messages observed, or even intercepted and stored, the more can be inferred from the traffic. Traffic analysis can be performed in the context of military intelligence, counter-intelligence, or pattern-of-life analysis, and is a concern in computer security.\n\nTraffic analysis tasks may be supported by dedicated computer software programs. Advanced traffic analysis techniques may include various forms of social network analysis.\n\nTraffic analysis method can be used to break the anonymity of anonymous networks, e.g., TORs . There are two methods of traffic-analysis attack, passive and active. \n\n\nIn a military context, traffic analysis is a basic part of signals intelligence, and can be a source of information about the intentions and actions of the target. Representative patterns include:\n\n\nThere is a close relationship between traffic analysis and cryptanalysis (commonly called codebreaking). Callsigns and addresses are frequently encrypted, requiring assistance in identifying them. Traffic volume can often be a sign of an addressee's importance, giving hints to pending objectives or movements to cryptanalysts.\n\nTraffic-flow security is the use of measures that conceal the presence and properties of valid messages on a network to prevent traffic analysis. This can be done by operational procedures or by the protection resulting from features inherent in some cryptographic equipment. Techniques used include:\n\n\nTraffic-flow security is one aspect of communications security.\n\nThe Communications' Metadata Intelligence, or COMINT metadata is a term in communications intelligence (COMINT) referring to the concept of producing intelligence by analyzing only the technical metadata, hence, is a great practical example for traffic analysis in intelligence.\n\nWhile traditionally information gathering in COMINT is derived from intercepting transmissions, tapping the target's communications and monitoring the content of conversations, the metadata intelligence is not based on content but on technical communicational data.\n\nNon-content COMINT is usually used to deduce information about the user of a certain transmitter, such as locations, contacts, activity volume, routine and its exceptions.\n\nFor example, if a certain emitter is known as the radio transmitter of a certain unit, and by using direction finding (DF) tools, the position of the emitter is locatable; hence the changes of locations can be monitored. That way we're able to understand that this certain unit is moving from one point to another, without listening to any orders or reports. If we know that this unit reports back to a command on a certain pattern, and we know that another unit reports on the same pattern to the same command, then the two units are probably related, and that conclusion is based on the metadata of the two units' transmissions, and not on the content of their transmissions.\n\nUsing all, or as much of the metadata available is commonly used to build up an Electronic Order of Battle (EOB) – mapping different entities in the battlefield and their connections. Of course the EOB could be built by tapping all the conversations and trying to understand which unit is where, but using the metadata with an automatic analysis tool enables a much faster and accurate EOB build-up that alongside tapping builds a much better and complete picture.\n\n\n\nTraffic analysis is also a concern in computer security. An attacker can gain important information by monitoring the frequency and timing of network packets. A timing attack on the SSH protocol can use timing information to deduce information about passwords since, during interactive session, SSH transmits each keystroke as a message. The time between keystroke messages can be studied using hidden Markov models. Song, \"et al.\" claim that it can recover the password fifty times faster than a brute force attack.\n\nOnion routing systems are used to gain anonymity. Traffic analysis can be used to attack anonymous communication systems like the Tor anonymity network. Adam Back, Ulf Möeller and Anton Stiglic present traffic analysis attacks against anonymity providing systems \n. Steven J. Murdoch and George Danezis from University of Cambridge presented \nresearch showing that traffic-analysis allows adversaries to infer which nodes relay the anonymous streams. This reduces the anonymity provided by Tor. They have shown that otherwise unrelated streams can be linked back to the same initiator.\n\nRemailer systems can also be attacked via traffic analysis. If a message is observed going to a remailing server, and an identical-length (if now anonymized) message is seen exiting the server soon after, a traffic analyst may be able to (automatically) connect the sender with the ultimate receiver. Variations of remailer operations exist that can make traffic analysis less effective.\n\nIt is difficult to defeat traffic analysis without both encrypting messages and masking the channel. When no actual messages are being sent, the channel can be masked\n. \"It is very hard to hide information about the size or timing of messages. The known solutions require Alice to send a continuous stream of messages at the maximum bandwidth she will ever use...This might be acceptable for military applications, but it is not for most civilian applications.\" The military-versus-civilian problems applies in situations where the user is charged for the volume of information sent.\n\nEven for Internet access, where there is not a per-packet charge, ISPs make statistical assumption that connections from user sites will not be busy 100% of the time. The user cannot simply increase the bandwidth of the link, since masking would fill that as well. If masking, which often can be built into end-to-end encryptors, becomes common practice, ISPs will have to change their traffic assumptions.\n\n\n\n"}
{"id": "34019230", "url": "https://en.wikipedia.org/wiki?curid=34019230", "title": "Velleman", "text": "Velleman\n\nVelleman is a Belgian producer and distributor of electronics, in particular for hobbyists. In a blog post introducing the products, RadioShack claimed Velleman to be \"the undisputed leader in do-it-yourself kits and components\".\n\nThe company was founded in 1975 as a family-owned maker of do-it-yourself electronic kits, and incorporated as Velleman NV in the 1980s. It is headquartered in Gavere in East Flanders, 10 km south-west of Ghent, has 165 employees worldwide and a turnover of 37 million euros (2009).\n\n\n"}
{"id": "2348419", "url": "https://en.wikipedia.org/wiki?curid=2348419", "title": "Videx", "text": "Videx\n\nVidex, Inc. is a Corvallis, Oregon manufacturer of computer hardware such as access control products and data collection terminals. Its initial success came with the first release of the $345 Videoterm (80 column) display card in March 1980 and the $149 shift and custom keyboard mapping Enhancer II terminal card in November 1981, both for Apple II computers. Later, in 1984, it released its $379 UltraTerm expansion card boasting high-definition 96-pixel characters and up to 128 x 32 character display. These products became obsolete when Apple released the Apple IIe with most of the 80-column card hardware built-in - only a much simpler and cheaper RAM card was then required.\n\nVidex also produced software, including Desktop Calendar for the Apple Lisa.\n"}
{"id": "8640180", "url": "https://en.wikipedia.org/wiki?curid=8640180", "title": "Washington Iron Works Skidder", "text": "Washington Iron Works Skidder\n\nThe Washington Iron Works Skidder is a steam-powered logging skidder, or cable winch. It was imported to Australia in the 1920s and was initially used to move the large Jarrah logs from the forests of Western Australia. It was later sold to the Forests Commission of Victoria for salvaging timber after the 1939 Black Friday fires.\n\nThe winch is located at Swifts Creek, Victoria, and is a unique part of Victoria's cultural heritage and logging history; left intact with engine, spars and cabling still rigged for work it is the only steam-powered engine of its kind in Australia. It is listed on the Victorian Heritage Register, and managed by the Department of Environment, Land, Water and Planning (DELWP).\n\nWashington Iron Works was a company in Seattle, Washington, founded by John M. Frink, that built these steam skidders. The company was active from 1882 until the 1980s when its various divisions – manufacturing cranes, logging equipment, and presses – were gradually sold off. The Works closed in 1986.\n\nWashington Iron Works engines revolutionized steam logging in the 1920s and 1930s. The steam-powered winches were mounted on heavy log skid frames which allowed the winch to be transported to new sites. Many Washington skidders can still be seen in North America.\n\nThe Washington Winch operated either a \"high lead\" or a \"skyline\" system. The high lead system was not often used as it only partially lifted the logs off the ground, which caused the logs to become caught in rocks, and left behind a \"snig track\", which deepened over time. The skyline system involved two large spars (trees) used to create a \"flying fox\" to lift logs over the rough ground. Riggers climbed 60m up a large tree and headed the trunk, which would cause the tree to sway violently. They then secured the spar with guy cables and attached the tackle. This operation was dangerous and physically demanding, often taking a full day, with lunch sent up on a rope. The Washington Winch was used to harvest Alpine Ash which would then be used for high-value products such as furniture, flooring and architraves.\n\n\n"}
