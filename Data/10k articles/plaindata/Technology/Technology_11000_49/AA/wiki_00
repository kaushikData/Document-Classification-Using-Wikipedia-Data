{"id": "1025612", "url": "https://en.wikipedia.org/wiki?curid=1025612", "title": "AbioCor", "text": "AbioCor\n\nAbioCor was a total artificial heart (TAH) developed by the Massachusetts-based company AbioMed. It was fully implantable within a patient, due to a combination of advances in miniaturization, biosensors, plastics and energy transfer. The AbioCor ran on a rechargeable source of power. The internal battery was charged by a transcutaneous energy transmission (TET) system, meaning that no wires or tubes penetrated the skin, reducing the risk of infection. However, because of its size, this heart was only compatible with men who had a large frame. It had a product life expectancy of 18 months.\n\nAbioCor was surgically introduced into 15 total patients, 14 of them during a clinical trial and one after FDA approval. However, due to insufficient evidence of its efficacy, AbioMed abandoned further development of the product.\n\nAbioMed, Inc. began development of the AbioCor device in the 1990s, beginning animal studies in 1998 in preparation to demonstrate readiness for formal clinical trials in humans. On January 30, 2001, the FDA granted AbioMed an investigational device exemption (IDE) for implantation into humans via a clinical trial. This opened the door for the first implantation of the AbioCor into Robert Tools on July 2, 2001. He lived for 151 days before having a fatal cerebrovascular accident. Time magazine awarded the AbioCor its Invention of the Year award in late 2001.\n\nThe second patient, Tom Christerson, who was given less than a 20 percent chance of surviving 30 days at the time of his surgery, lived for 512 days after receiving the AbioCor, dying on February 7, 2003 due to the wearing out of an internal membrane of the AbioCor. An additional 12 patients had the device implanted into 2004, resulting in an average life span of less than five months among all 14 patients. In some cases the device extended survival by several months, allowing the patients to spend valuable time with family and friends. In two cases, the device extended survival by 10 and 17 months respectively, and one patient was discharged from the hospital to go home. For a patient to be eligible for implantation with the AbioCor, the person must have had severe heart failure (with failure of both ventricles) and had to be likely to die within two weeks without transplantation.\n\nThough the device was initially rejected by FDA Circulatory System Devices Panel in 2005 for Humanitarian Device Exemption (HDE) status, it was eventually approved by the Food and Drug Administration on September 5, 2006 for HDE status. However, only one patient received the AbioCor after approval, a \"76-year-old man with congestive heart failure, who did not qualify for a heart transplant.\"\n\nIn August 2012, key AbioCor researcher and developer David Lederman died from pancreatic cancer.\n\nThe company also had plans to improve the AbioCor with a second version based upon the AbioCor ventricles and the Penn State energy converter. It was expected to last for five years, more than triple the life expectancy of AbioCor. The company stated it would be 30 percent smaller than the original model, and it could be implanted in smaller men and women. Additional modifications were planned to reduce the patient's risk of stroke, which was a concern of the FDA. , AbioCor II has not come to fruition, however. Additionally, the AbioCor product has been removed from the AbioMed website, with several news agencies reporting in 2015 that the company had quietly abandoned further development of the device.\n\nThe AbioCor heart is featured in the 2009 film \"\", when it is transplanted into the main character Chev Chelios's (Jason Statham) chest after he had been abducted by Chinese mobsters in the very beginning of the movie. However, the heart depicted in the film has a much lower battery life but gives Chev superhuman athleticism when fully charged (for dramatic purposes). The model of the heart in the movie is called AviCor.\n\n\n"}
{"id": "56008250", "url": "https://en.wikipedia.org/wiki?curid=56008250", "title": "Alice Cornwell", "text": "Alice Cornwell\n\nAlice Ann Cornwell or Alice Whiteman or Alice Robinson (1 January 1852 – 7 January 1932) was a British goldmining industrialist and newspaper proprietor. She made her fortune from gold and floated her company on the London Stock Exchange. She was a confident business person investing in several companies including owning the Sunday Times\n\nCornwell was born in West Ham in 1852 to Jemima and George Cornwell. She and her family emigrated to New Zealand when she was nine. Her father was an engineer.\n\nHer first marriage was with the much older John Whiteman. He was a politician and publican. This marriage resulted in a legal separation before she left for England, but it was not ended until Whiteman died. \n\nHer fortune was made when she returned from England after education at the Royal Academy of Music to Australia. Her father was a successful engineer who was then prospecting but he was not making a profit. Cornwell studied his ground and convinced others that a major find lay beneath his land. Shafts were created where she had indicated and gold was reputedly found within 30 centimetres of where she had said it would be.\n\nIn 1886 she returned to London as a reputed millionaire, although this may have been an exaggeration. Despite not being able to enter London's club's because of her gender she floated the \"Midas Mine\" on the London Stock Exchange. Moreover she also created the British and Australian Mining Trust and Investment Company. The purpose of this company was to allow people to invest money directly in Australian mines. \n\nCornwell bought the Sunday Times in 1887 from Colonel George FitzGeorge who an illegitimate member of the Royal Family. Her purpose was promote her new company and it was a gift to her lover Frederick Stannard (‘Phil’) Robinson. In 1888 her friend Fergus Hume wrote a novel, \"Madame Midas\" about a \"Mrs Villiers\" which was obviously based on Cornwell.\n\nCornwell sold the \"Sunday Times\" in 1893 to Frederick Beer, who already owned \"Observer\". Beer appointed his wife, Rachel Sassoon Beer, as editor.\n\nCornwell's estranged first husband died in 1893 and she married Robinson in 1894.\n\nCornwell died in 1932 in Hove.\n"}
{"id": "35737867", "url": "https://en.wikipedia.org/wiki?curid=35737867", "title": "Applied Thermal Engineering", "text": "Applied Thermal Engineering\n\nApplied Thermal Engineering is a peer-reviewed scientific journal publishing original articles concerning all aspects of the thermal engineering of advanced processes, including process integration, intensification, and development, together with the application of thermal equipment in conventional process plant, which includes its use for heat recovery. The current editor-in-chief is T. S. Zhao. The journal was established in 1981 as \"Journal of Heat Recovery Systems\" and renamed to \"Heat Recovery Systems and CHP\" in 1987. It obtained its current title in 1996.\n\nAccording to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 3.771. According to SCOPUS, the journal's h-index is 121.\n"}
{"id": "46991115", "url": "https://en.wikipedia.org/wiki?curid=46991115", "title": "Autographic Register", "text": "Autographic Register\n\nAn Autographic Register is a business machine invented in 1883 by James C. Shoup. The device consisted two separate rolls of paper interleaved with carbon paper. Usually one or both of the rolls would be preprinted with form information. To operate the machine the user would write, for example, a sales receipt and the machine automatically produced a copy. The crank on the machine ejected the records and moved a blank form into view. The original receipt produced would go to the user and the copy was filed. Shoup founded the Autographic Register Company in Hoboken, NJ to manufacture his invention.\n\nThe Autographic Register was an advance over use of separate forms and carbon paper as it guaranteed that the copy was made and kept the forms in relative alignment. A number of advancements were soon made, including the use of sprocket-fed paper, invented by Theodore Schirmer. This helped avoid slippage and misalignment of forms, allowing more copies to be produced simultaneously. In 1912 Schirmer founded the Standard Register Company in Dayton, Ohio.\n\nThe invention of sprocket-fed paper later found use in computer printers.\n\nAlthough the Autographic Register has been largely replaced by newer technology it remains in use as of 2015.\n"}
{"id": "48923021", "url": "https://en.wikipedia.org/wiki?curid=48923021", "title": "Bidji", "text": "Bidji\n\nThe Bidji project is a set of tools for data transformation and code generation.\n\nThe project is a mix between Apache Ant and the FreeMarker template engine. They combine together very well to offer maximum flexibility for code generation for any computer language (java, php, c++, etc.) or text documentation (docbook, markdown, plantuml, graphviz, etc.).\n"}
{"id": "51714024", "url": "https://en.wikipedia.org/wiki?curid=51714024", "title": "Billionaire space race", "text": "Billionaire space race\n\nThe billionaire space race is the intense rivalry in NewSpace by recent space entrepreneurs, who entered the space industry as billionaires from other industries, particularly computing. This private industry space race of the 21st century involves sounding rockets to the ignorosphere (mesosphere and thermosphere), orbital launch rockets, and suborbital tourist spaceflights.\n\nAmongst the billionaires entering into New Space, are:\n\nThe groundwork for the billionaire space race and NewSpace was arguably laid by Peter Diamandis, an American entrepreneur. In the 1980s, he founded an American national student space society, the Students for the Exploration and Development of Space (SEDS). Later, Jeff Bezos became a chapter president of SEDS. In the 1990s, Diamandis, dissolute with the state of space development, decided to spur it on and spark the suborbital space tourism market, by initiating a prize, the X Prize. This led to Paul Allen becoming involved in the competition, creating the Scaled Composites Tier One platform of SpaceShipOne and White Knight One which won the Ansari X-Prize in the 2000s. The technology of the winning entrant was then licensed by Richard Branson's Virgin Group as a basis to found Virgin Galactic. The base techniques of Tier One also form the basis for Stratolaunch Systems of Vulcan Aerospace. The billionaire space race shows the aims of billionaires extend beyond just fulfilling government contracts, with their own gilding of the space age, in extending capabilities and their own luster. Elon Musk has expressed excitement for a new space race..\n\nSpaceX and Blue Origin have had a long history of conflict. There is a vision of a hare versus a tortoise, for the two respectively, from Blue Origin's standpoint. Blue Origin and SpaceX have had dueling press releases that compete with each other's announcements and events.\n\nSpaceX and Blue Origin battled for the right to lease SLC 39A, the rocket launch platform that was used to launch the Apollo moon missions. SpaceX won the lease in 2013, but Blue Origin filed suit in court against that. It is currently in the hands of SpaceX, while Blue Origin rented SLC 36 instead.\n\nSpaceX filed suit against Blue Origin to invalidate their patent on landing rockets aboard ships at sea. They won their court fight in 2014. SpaceX had been attempting to land rockets at sea since 2014, finally succeeding in 2016, before Blue Origin ever even built a sea-going platform to land rockets onto.\n\nSpaceX and Blue Origin got into a Twitter battle about the meaning of a used rocket, landed rocket, spacerocket, at the end of 2015, when New Shepard successfully landed, after a suborbital jaunt into space. SpaceX had previously launched and landed its Grasshopper rocket multiple times without reaching space. Then SpaceX landed a Falcon 9 first stage, which had been used to launch a satellite into orbit, prompting more Twitter battle at the start of 2016.\n\nIn late 2016, Blue Origin announced the New Glenn, directly competing against SpaceX's Falcon Heavy, with a larger rocket but lower payload.\n\nAt the 2016 International Astronautical Congress in Guadalajara, Mexico, Blue Origin President Rob Meyerson elaborated on the Bezos vision previously outlined in the New Glenn announcement. The Blue Origin New Armstrong would be similar in function to the SpaceX Interplanetary Transport System that Elon Musk unveiled at the same conference.\n\nBlue Origin and Virgin Galactic are in the same market, suborbital space tourism, with New Shepard and SpaceShipTwo (Tier 1b), respectively. They are in a race to be first to launch paying customers on short spaceshots, with rival technological philosophies of space capsules and spaceplanes, respectively\n\nVirgin Galactic and SpaceX support competing communications satellite constellations. Each would launch their satellites on their own launchers, LauncherOne and Falcon 9/Falcon Heavy respectively. The satellite systems would compete for the same market and same bandwidth and go head-to-head at the ITU for satellite frequency licenses.\n\nVulcan Aerospace subsidiary Stratolaunch Systems plans to air-launch satellite launcher rockets, the same profile as planned by Virgin Galactic for its LauncherOne operations. While LauncherOne is already under development and launch aircraft procured (once White Knight Two, now 747 Cosmic Girl), the Scaled Composites \"Roc\" Model 351 is still being developed and the rocket to mate to it has yet to be selected.\n\nThe billionaire space race and the rise of Space 2.0 has impacted on \"Old Space\", the old space hands of the pre-existing aerospace sector. The disruptive forces of NewSpace may wash away some of the standing members of the community. In particular SpaceX and Blue Origin represent existential threats to the old boys club and staid storied enterprises of aerospace. SpaceX challenges all members of the old guard with the pricing of its launch vehicles, being lower than the previous low cost leader, the Chinese launch services. It greatly undercuts the monopolistic leader of the European and American space launch services sectors, respectively, ArianeSpace and United Launch Alliance (an alliance of Boeing and Lockheed Martin). The position of Blue Origin is that of a future threat. Both SpaceX and Blue Origin put forth reusable launch vehicles, forcing ULA and Ariane to design next generation rockets with partial reusability features to attempt to reduce costs to be competitive with the new twin threats, using the Vulcan and Ariane 6 respectively. Boeing has also fired shots across the bow of SpaceX (and others) in the race to Mars, intending to arrive first. Boeing and SpaceX both are exploring sub-orbital transport with hypersonic rocket transports, SpaceX basing it on reusable Falcon 9 technology, which would compete with transcontinental and transoceanic air travel.\n\n\n"}
{"id": "16413876", "url": "https://en.wikipedia.org/wiki?curid=16413876", "title": "Biotechnology consulting", "text": "Biotechnology consulting\n\nBiotechnology consulting (or biotech consulting) refers to the practice of assisting organizations involved in research and commercialization of biotechnology in improving their methods and efficiency of production, and approaches to R&D. This assistance is usually provided in the form of specialized technological advice and sharing of expertise. Both start-up and established organizations would hire biotechnology consultants mainly to receive an independent and professional advice from key opinion leaders, individuals with extensive knowledge and experience in a particular area of biotechnology or biological sciences, and, often, to outsource their projects for implementation by well qualified individuals. Large management consulting firms would often be able to provide technological advice as well, depending on the qualifications of their consulting team. With the growth of pharmaceutical companies, biotechnology consulting has recently developed into an industry of its own and separated from the management consulting industry that traditionally also provides technological advice on R&D projects to various industries. This has also been fueled by the impact various conflicts of interests can have on commercialization when biotechnology organizations contract services from academic institutions or government scientists\n\nThis is exemplified by the successful emergence of a large number of consulting companies dedicated exclusively to servicing the biotech industry. Occasionally, university professors and Phd students engage in biotechnology consulting, either commercially or free of charge.\n\nA special type of consulting is patent strategy and management consulting or simply patent consulting which specifically emphasizes on the scope of patent rights versus R&D in industry. It also assets successful commercialization of patentable matter. The primary aim of patent consulting company is to assist various small, medium and large corporation in realizing their research project toward successful patent registration with minimized danger of infringement and other risks that patent registrations may be subjected to prior to commercialization. One example of patent consulting firm is \"The Patent World\".\n"}
{"id": "36425706", "url": "https://en.wikipedia.org/wiki?curid=36425706", "title": "Bram Lebo", "text": "Bram Lebo\n\nBram Lebo is a Canadian entrepreneur, media proprietor and business consultant.\n\nLebo studied law at the Osgoode Hall Law School and was called to the Ontario Bar in 1994. That same year Lebo moved to the Netherlands where he completed his MBA and Master of Business Informatics from the Rotterdam School of Management at Erasmus University.\nIn the mid-1990s, Lebo worked for DigiCash, a pioneering electronic currency corporation based in Amsterdam, followed by three years as a business developer and strategist for multinationals in Europe.\nIn 2000, Lebo founded Expatica, an online newspaper for English-speakers living in Europe, based in Amsterdam with operations in four other countries. Expatica grew to become the most widely read English-language publication in Continental Europe, in any medium, rivaling major newspapers in readership.\nLebo later returned to Canada and worked for the Government of Ontario, and as of 2008 served as the Director of Marketing and Brand Development for ServiceOntario, a Government of Ontario initiative to give Ontarians an easy, cost-effective way to access government services. In 2009 he co-founded Gravitas Business Architects.\nIn recent years Lebo has worked as a radio host at Canoe FM (CKHA), \"the Voice of Haliburton County\" in Ontario's \"cottage country\", hosting the shows \"County Hot Seat\" and \"Half Facts\".\nCurrently Lebo is publisher of and a columnist at The Highlander newspaper and serves on the Board of the Haliburton Highlands Chamber of Commerce \n\nLebo authored \"The Law Students' Guide to Articling and Summer Positions in Canada \" ( (0-920722-53-9)).\n"}
{"id": "44079682", "url": "https://en.wikipedia.org/wiki?curid=44079682", "title": "Cardiac Pacemakers, Inc.", "text": "Cardiac Pacemakers, Inc.\n\nCardiac Pacemakers, Inc.(CPI), doing business as Guidant Cardiac Rhythm Management, manufactured implantable cardiac rhythm management devices, such as pacemakers and defibrillators. It also sold insulin pumps controlled by microprocessors and various equipments to regulate heart rhythm. In addition, Cardiac Pacemakers, Inc. developed therapies for the treatment of irregular heartbeats. The company was founded in 1971 and is based in St. Paul, Minnesota. Cardiac Pacemakers, Inc. operates as a subsidiary of Boston Scientific Corporation.\n\nCPI was a highly successful start up venture, increasing sales from zero in 1972 to over $47 million and highly profitable when it was acquired by Eli Lilly and Company in 1978 for $127 million. During the 1970s, Lilly acquired other medical device companies including IVAC of San Diego, Physio Control of Redmond WA, and Advanced Cardiovascular Systems of Santa Clara, CA. These companies formed the core product/therapy areas of the Medical Devices Division of Eli Lilly and Company.\n\nCPI designed and manufactured the worlds first pacemaker with a lithium anode and a lithium-iodide electrolyte solid-state battery. An improved heart pacer including the conventional combination of a pulse generator, electrode means, and electrode leads coupling the pulse generator to the electrodes, wherein the battery power source of the pulse generator is a solid-state battery with a lithium anode and a lithium-iodide electrolyte.(see Wilson Greatbatch, ed.). The pacer structure is enclosed in a hermetically sealed metallic enclosure, with means being provided in the enclosure for passing electrode leads in sealed relationship therethrough. The outer surface of the casing is polished metal*, and is continuous in all areas. In certain instances, the continuity may be with the exception of the zone through which the external electrode leads pass.\n"}
{"id": "35541763", "url": "https://en.wikipedia.org/wiki?curid=35541763", "title": "Conversion as a service", "text": "Conversion as a service\n\nIn the e-commerce industry, conversion as a service is a method of online conversion optimization that is a customized intersection of art and technology that combines analytics, behavioral targeting, software, style, and business rules to exact success. This approach advocates a holistic approach to achieve an improvement in online conversion.\n\nConversion as a Service (Caas) allows clients to take advantage of the accumulated expertise of a vendor, rather than either developing and maintaining the expertise in-house or outsourcing the expertise through a third-party services firm. Opting for vendor expertise ensures a knowledge base for the lifetime of the relationship and real-time program optimization.\n\nCRO is a process of improving the performance of your website using user feedback and visitor analytics. There are some metrics on your website which you want to improve, normally called as key performance indicators. These factors can be improved via CRO. Most of the time these metrics are associated with acquisition of new customers, downloads, sign up’s etc.In short,it is a process of improving the percentage of passive visitors into valuable consumers.\n\nIn the process of CRO, what visitors are searching and looking while they surf your site is found out and that information to provide to them. Depending on what metrics you want to improve CRO may take varied forms.An example would be placing strong call to actions on a high traffic but under optimized page. In some cases, it refers to removal of unnecessary and complicated steps from conversion funnel which hinder conversions.\n\nPerformance measurement is key to Conversion as a Service. In pay-for-performance models, the success or failure of a solution is not based on whether providers successfully deploy software, but on whether or not the solution delivers desired measurable results. In this model, providers and clients align their efforts to meet a set of agreed-upon metrics that determine whether or not the relationship is valuable.\n\nIn most cases, the metrics used in online commerce revolve around gains in incremental revenue directly attributable to a solution provider. The specific metric can vary, whether it is an increase in average order value, up-sell, or customer satisfaction or a decrease in cart abandonment or call center traffic, the solution provider must be able to demonstrate that an increase in revenue can be tied to the presence of their solution.\n\nThere are three goals for clarity in Pay-For-Performance Models:\n\n\n\n"}
{"id": "44204924", "url": "https://en.wikipedia.org/wiki?curid=44204924", "title": "Crypto Wars", "text": "Crypto Wars\n\nThe Crypto Wars is an unofficial name for the U.S. and allied governments' attempts to limit the public's and foreign nations' access to cryptography strong enough to resist decryption by national intelligence agencies (especially USA's NSA).\n\nIn the early days of the Cold War, the U.S. and its allies developed an elaborate series of export control regulations designed to prevent a wide range of Western technology from falling into the hands of others, particularly the Eastern bloc. All export of technology classed as 'critical' required a license. CoCom was organized to coordinate Western export controls.\n\nTwo types of technology were protected: technology associated only with weapons of war (\"munitions\") and dual use technology, which also had commercial applications. In the U.S., dual use technology export was controlled by the Department of Commerce, while munitions were controlled by the State Department. Since in the immediate post WWII period the market for cryptography was almost entirely military, the encryption technology (techniques as well as equipment and, after computers became important, crypto software) was included as a Category XIII item into the United States Munitions List. The multinational control of the export of cryptography on the Western side of the cold war divide was done via the mechanisms of CoCom.\n\nBy the 1960s, however, financial organizations were beginning to require strong commercial encryption on the rapidly growing field of wired money transfer. The U.S. Government's introduction of the Data Encryption Standard in 1975 meant that commercial uses of high quality encryption would become common, and serious problems of export control began to arise. Generally these were dealt with through case-by-case export license request proceedings brought by computer manufacturers, such as IBM, and by their large corporate customers.\n\nEncryption export controls became a matter of public concern with the introduction of the personal computer. Phil Zimmermann's PGP cryptosystem and its distribution on the Internet in 1991 was the first major 'individual level' challenge to controls on export of cryptography. The growth of electronic commerce in the 1990s created additional pressure for reduced restrictions. Shortly afterward, Netscape's SSL technology was widely adopted as a method for protecting credit card transactions using public key cryptography.\n\nSSL-encrypted messages used the RC4 cipher, and used 128-bit keys. U.S. government export regulations would not permit crypto systems using 128-bit keys to be exported. At this stage Western governments had, in practice, a split personality when it came to encryption; policy was made by the military cryptanalysts, who were solely concerned with preventing their 'enemies' acquiring secrets, but that policy was then communicated to commerce by officials whose job was to support industry.\n\nThe longest key size allowed for export without individual license proceedings was 40 bits, so Netscape developed two versions of its web browser. The \"U.S. edition\" had the full 128-bit strength. The \"International Edition\" had its effective key length reduced to 40 bits by revealing 88 bits of the key in the SSL protocol. Acquiring the 'U.S. domestic' version turned out to be sufficient hassle that most computer users, even in the U.S., ended up with the 'International' version, whose weak 40-bit encryption could be broken in a matter of days using a single personal computer. A similar situation occurred with Lotus Notes for the same reasons.\n\nLegal challenges by Peter Junger and other civil libertarians and privacy advocates, the widespread availability of encryption software outside the U.S., and the perception by many companies that adverse publicity about weak encryption was limiting their sales and the growth of e-commerce, led to a series of relaxations in US export controls, culminating in 1996 in President Bill Clinton signing the Executive order 13026 transferring the commercial encryption from the Munition List to the Commerce Control List. Furthermore, the order stated that, \"the software shall not be considered or treated as 'technology'\" in the sense of Export Administration Regulations. This order permitted the United States Department of Commerce to implement rules that greatly simplified the export of proprietary and open source software containing cryptography, which they did in 2000.\n\nAs of 2009, non-military cryptography exports from the U.S. are controlled by the Department of Commerce's Bureau of Industry and Security. Some restrictions still exist, even for mass market products, particularly with regard to export to \"rogue states\" and terrorist organizations. Militarized encryption equipment, TEMPEST-approved electronics, custom cryptographic software, and even cryptographic consulting services still require an export license (pp. 6–7). Furthermore, encryption registration with the BIS is required for the export of \"mass market encryption commodities, software and components with encryption exceeding 64 bits\" (). In addition, other items require a one-time review by or notification to BIS prior to export to most countries. For instance, the BIS must be notified before open-source cryptographic software is made publicly available on the Internet, though no review is required. Export regulations have been relaxed from pre-1996 standards, but are still complex. Other countries, notably those participating in the Wassenaar Arrangement, have similar restrictions.\n\nUntil 1996, the UK government withheld export licenses from exporters unless they used weak ciphers or short keys, and generally discouraged practical public cryptography. A debate about cryptography for the NHS brought this out in the open.\n\nThe Clipper chip was a chipset for mobile phones made by the NSA in the 1990s, which implemented encryption with a backdoor for the US government. The US government tried to get phone manufacturers to adopt the chipset, but without success, and the program was finally defunct by 1996.\n\nA5/1 is a stream cipher used to provide over-the-air communication privacy in the GSM cellular telephone standard.\n\nSecurity researcher Ross Anderson reported in 1994 that \"there was a terrific row between the NATO signal intelligence agencies in the mid-1980s over whether GSM encryption should be strong or not. The Germans said it should be, as they shared a long border with the Warsaw Pact; but the other countries didn't feel this way, and the algorithm as now fielded is a French design.\"\n\nAccording to professor Jan Arild Audestad, at the standardization process which started in 1982, A5/1 was originally proposed to have a key length of 128 bits. At that time, 128 bits was projected to be secure for at least 15 years. It is now estimated that 128 bits would in fact also still be secure as of 2014. Audestad, Peter van der Arend, and Thomas Haug say that the British insisted on weaker encryption, with Haug saying he was told by the British delegate that this was to allow the British secret service to eavesdrop more easily. The British proposed a key length of 48 bits, while the West Germans wanted stronger encryption to protect against East German spying, so the compromise became a key length of 56 bits.\n\nThe widely used DES encryption algorithm was originally planned by IBM to have a key size of 128 bits; NSA lobbied for a key size of 48 bits. The end compromise were a key size of 64 bits, 8 of which were parity bits, to make an effective key security parameter of 56 bits. DES was considered insecure as early as 1977, and documents leaked in the 2013 Snowden leak shows that it was in fact easily crackable by the NSA, but was still recommended by NIST. The DES Challenges were a series of brute force attack contests created by RSA Security to highlight the lack of security provided by the Data Encryption Standard. As part of the successful cracking of the DES-encoded messages, EFF constructed a specialized DES cracking computer nicknamed Deep Crack.\n\nThe successful cracking of DES likely helped gather both political and technical support for more advanced encryption in the hands of ordinary citizens. In 1997, NIST began a competition to select a replacement for DES, resulting in the publication in 2000 of the Advanced Encryption Standard (AES). NSA considers AES strong enough to protect information classified at the Top Secret level.\n\nFearing widespread adoption of encryption, the NSA set out to stealthily influence and weaken encryption standards and obtain master keys—either by agreement, by force of law, or by computer network exploitation (hacking).\n\nAccording to the \"New York Times\": \"But by 2006, an N.S.A. document notes, the agency had broken into communications for three foreign airlines, one travel reservation system, one foreign government’s nuclear department and another’s Internet service by cracking the virtual private networks that protected them. By 2010, the Edgehill program, the British counterencryption effort, was unscrambling VPN traffic for 30 targets and had set a goal of an additional 300.\"\n\nAs part of Bullrun, NSA has also been actively working to \"Insert vulnerabilities into commercial encryption systems, IT systems, networks, and endpoint communications devices used by targets\". \"The New York Times\" has reported that the random number generator Dual_EC_DRBG contains a back door from the NSA, which would allow the NSA to break encryption relying on that random number generator. Even though Dual_EC_DRBG was known to be an insecure and slow random number generator soon after the standard was published, and the potential NSA backdoor was found in 2007, and alternative random number generators without these flaws were certified and widely available, RSA Security continued using Dual_EC_DRBG in the company's BSAFE toolkit and Data Protection Manager until September 2013. While RSA Security has denied knowingly inserting a backdoor into BSAFE, it has not yet given an explanation for the continued usage of Dual_EC_DRBG after its flaws became apparent in 2006 and 2007, however it was reported on December 20, 2013 that RSA had accepted a payment of $10 million from the NSA to set the random number generator as the default. Leaked NSA documents state that their effort was “a challenge in finesse” and that “Eventually, N.S.A. became the sole editor” of the standard.\n\nBy 2010, the NSA had developed “groundbreaking capabilities” against encrypted Internet traffic. A GCHQ document warned however “These capabilities are among the Sigint community’s most fragile, and the inadvertent disclosure of the simple ‘fact of’ could alert the adversary and result in immediate loss of the capability.” Another internal document stated that “there will be NO ‘need to know.’” Several experts, including Bruce Schneier and Christopher Soghoian, have speculated that a successful attack against RC4, a 1987 encryption algorithm still used in at least 50 per cent of all SSL/TLS traffic is a plausible avenue, given several publicly known weaknesses of RC4. Others have speculated that NSA has gained ability to crack 1024-bit RSA and Diffie–Hellman public keys. A team of researchers have pointed out that there is wide reuse of a few non-ephemeral 1024 bit primes in Diffie–Hellman implementations, and that NSA having done precomputation against those primes in order to break encryption using them in real time is very plausibly what NSA's \"groundbreaking capabilities\" refer to.\n\nThe Bullrun program is controversial, in that it is believed that NSA deliberately inserts or keeps secret vulnerabilities which affect both law-abiding US citizens as well as NSA's targets, under its NOBUS policy. In theory, NSA has two jobs: prevent vulnerabilities that affect the US, and find vulnerabilities that can be used against US targets; but as argued by Bruce Schneier, NSA seems to prioritize finding (or even creating) and keeping vulnerabilities secret. Bruce Schneier has called for the NSA to be broken up so that the group charged with strengthening cryptography is not subservient to the groups that want to break the cryptography of its targets.\n\nAs part of the Snowden leaks, it became widely known that intelligence agencies could bypass encryption of data stored on Android and iOS smartphones by legally ordering Google and Apple to bypass the encryption on specific phones. Around 2014, as a reaction to this, Google and Apple redesigned their encryption so that they did not have the technical ability to bypass it, and it could only be unlocked by knowing the user's password.\n\nVarious law enforcements officials, including the Obama administration's Attorney General Eric Holder responded with strong condemnation, calling it unacceptable that the state could not access alleged criminals' data even with a warrant. One of the more iconic responses being the chief of detectives for Chicago’s police department stating that \"Apple will become the phone of choice for the pedophile\". Washington Post posted an editorial insisting that \"smartphone users must accept that they cannot be above the law if there is a valid search warrant\", and after agreeing that backdoors would be undesirable, suggested implementing a \"golden key\" backdoor which would unlock the data with a warrant.\n\nFBI Director James Comey cited a number of cases to support the need to decrypt smartphones. Interestingly, in none of the presumably carefully handpicked cases did the smartphone have anything to do with the identification or capture of the culprits, and FBI seems to have been unable to find any strong cases supporting the need for smartphone decryption.\n\nBruce Schneier has labelled the right to smartphone encryption debate \"Crypto Wars II\", while Cory Doctorow called it \"Crypto Wars redux\".\n\nLegislators in the US states of California and New York have proposed bills to outlaw the sale of smartphones with unbreakable encryption. As of February 2016, no bills have been passed.\n\nIn February 2016 the FBI obtained a court order demanding that Apple create and electronically sign new software which would enable the FBI to unlock an iPhone 5c it recovered from one of the shooters in the 2015 terrorist attack in San Bernardino, California. Apple has challenged the order. In the end the FBI hired a third party to crack the phone. \"See\" FBI–Apple encryption dispute.\n\nIn April 2016, Dianne Feinstein and Richard Burr sponsored an overly vague bill that would be likely to criminalise all forms of strong encryption.\n\nIn October 2017, Deputy Attorney General Rod Rosenstein called for responsible encryption as a solution to the ongoing problem of \"going dark\". This refers to wiretapping court orders and police measures increasingly becoming ineffective as strong end-to-end encryption are increasingly added to widespread messenger products. Responsible encryption means that companies need to introduce key escrow that allows them to provide their customers with a way to recover their encrypted data if they forget their password, so that it is not lost forever. According to Rosenstein's reasoning, it would be irresponsible to leave the user helpless in such a case. As a pleasant side effect, this would allow a judge to issue a search warrant instructing the company to decrypt the data, which the company would then be able to comply with. In contrast to previous proposals, the decentral storage of key recovery material by companies instead of government agencies would be an additional safeguard.\n\nIn 2015 the head of the NSA, Admiral Michael S. Rogers suggested to further decentralize the key escrow by introducing \"front doors\" instead of back doors into encryption. This way, the key would be split into two halves, with one half being kept by the government authorities and the other by the company responsible for the encryption product. Thus, the government would still have to get a search warrant to obtain the other half of the key from the company, and the company would be unable to abuse the key escrow to access the user's data, since it would lack the other half of the key kept by the government. Experts were not impressed.\n\nIn 2018, the NSA promoted the use of \"lightweight encryption\", in particular its ciphers Simon and Speck, for Internet of Things devices. However, the attempt to have those ciphers standardized by ISO failed because of severe criticism raised by the board of cryptography experts which provoked fears that the NSA had non-public knowledge of how to break them.\n\nFollowing the 2015 \"Charlie Hebdo\" shooting, a terrorism attack, former UK Prime Minister David Cameron called for outlawing non-backdoored cryptography, saying that there should be no \"means of communication\" which \"we cannot read\". US president Barack Obama sided with Cameron on this.\n\n"}
{"id": "3568143", "url": "https://en.wikipedia.org/wiki?curid=3568143", "title": "DARPA XG", "text": "DARPA XG\n\nThe neXt Generation program or XG is a technology development project sponsored by DARPA's Strategic Technology Office, with the goals to \"develop both the enabling technologies and system concepts to dynamically redistribute allocated spectrum along with novel waveforms in order to provide dramatic improvements in assured military communications in support of a full range of worldwide deployments.\"\n\nIn the Wireless World Research Forum of 27 October 2003, Preston Marshall, program manager of DARPA XG Program, said \"The primary product of the XG program is not a new radio, but a set of advanced technologies for dynamic spectrum\naccess.\"\n\n\n"}
{"id": "11525168", "url": "https://en.wikipedia.org/wiki?curid=11525168", "title": "De-asphalter", "text": "De-asphalter\n\nA de-asphalter is a unit in a crude oil refinery or bitumen upgrader that separates asphalt from crude oil or bitumen. \n\nThe de-asphalter unit is usually placed after the vacuum distillation tower. It is usually a solvent de-asphalter unit, SDA. The SDA separates the asphalt from the feedstock because light hydrocarbons will dissolve aliphatic compounds but not asphaltenes. The output from the de-asphalter unit is de-asphalted oil (\"DAO\") and asphalt. \n\nDAO from propane de-asphalting has the highest quality but lowest yield, whereas using pentane may double or triple the yield from a heavy feed, but at the expense of contamination by metals and carbon residues that shorten the life of downstream cracking catalysts. If the solvent is butane the unit will be referred to as a butane de-asphalter (\"BDA\") and if the solvent is propane, it will be called a propane de-asphalter (\"PDA\") unit.\n\n"}
{"id": "37916680", "url": "https://en.wikipedia.org/wiki?curid=37916680", "title": "Faithful amplification", "text": "Faithful amplification\n\nIn electronics, faithful amplification is the amplification of a signal, particularly a weak one, by a triode or a transistor such that the signal changes in amplitude but not in shape. In order to achieve this with a bipolar transistor, the transistor is biased. Faithful amplification can only occur on transistors with a forward biased emitter-base junction, a reverse biased collector-base junction, and proper zero signal collector current. Without the correct bias, the transistor will not operate efficiently and cause its output to distort.\n\n"}
{"id": "16135584", "url": "https://en.wikipedia.org/wiki?curid=16135584", "title": "Fire Equipment Manufacturers' Association", "text": "Fire Equipment Manufacturers' Association\n\nFounded in 1930, The Fire Equipment Manufacturers’ FEMA is an international, non-profit trade association dedicated to manufacturing commercial fire protection equipment to serve as the first line of defense against fire in its early stages. The association centers its efforts around the key premise that safety to life is best achieved through the implementation of a “balanced fire protection design” – a concept in which a proactive safety plan does not rely on any single safeguard.\n\nThe Fire Equipment Manufacturers' Association works in conjunction with the National Fire Protection Association (NFPA), International Code Council, local, state, national officials to advance positive fire and building codes, laws and Underwriters Laboratories, Inc. regarding relevant safety standards.\n\nMember companies unite to save lives and protect property through education and awareness, advancement\nof the fire equipment marketplace and improvement in regulatory requirements.\n\nThe Fire Equipment Manufacturers’ Association is organized into three divisions:\n\nMember companies work in smaller product-specific teams, as well as collectively, to impact\nindustry issues and outcomes.\n\n\n"}
{"id": "11117031", "url": "https://en.wikipedia.org/wiki?curid=11117031", "title": "Fire art", "text": "Fire art\n\nFire art is a piece of art that uses active flames as an essential part of the piece. The piece may either use flame effects as part of a sculpture, or be a choreographed performance of fire effects as the piece burns; the latter being almost a type of performance art. \n\nFire can be a compelling medium for artists and viewers. It has a direct effect on its surroundings, consuming materials and giving heat and light. It is constantly moving and can appear to be alive. There is also the inherent perceived danger of fire.\n\nFire artists use various fuels and colorants to achieve desired effects. The choice of fuel is largely dependent on the effects desired. In large stationary pieces, propane gas is a popular fuel. Gas escaping from its container can be set alight. Different plumbing, pressures, nozzles alter the nature of the flames. Controlling how the propane mixes with the oxygen in the air is important. \n\nLiquid fuels like alcohol, methanol, kerosene, mixes of kerosene and diesel, and others, can produce spectacular results. Extra precautions must be taken with liquids because they fall and splash, while gasses usually dissipate. \n\nMost colorants are solids ground into a powder. As the powder is heated, it begins to oxidize and burns a color particular to the chemical used. Some colorants can be dissolved in alcohol or water or other liquids to facilitate their dispersion in the fire. Some colorants are toxic and should not be used in proximity to people. \n\n\n"}
{"id": "26520670", "url": "https://en.wikipedia.org/wiki?curid=26520670", "title": "GCV Infantry Fighting Vehicle", "text": "GCV Infantry Fighting Vehicle\n\nThe Ground Combat Infantry Fighting Vehicle was an infantry fighting vehicle being developed for the U.S. Army. The program originated as the lead vehicle of the U.S. Army's Ground Combat Vehicle program coordinated by TACOM and spawned a parallel program coordinated by DARPA. The purpose of the program was to replace existing armored personnel carriers and infantry fighting vehicles in U.S. Army service. The DARPA project aimed to have the vehicle designed by 2015. Derivatives of the vehicle based on a common chassis—such as tanks and ambulances—were expected to be manufactured. It replaced the previous attempt at a next-generation infantry transport, the XM1206 Infantry Carrier Vehicle. The Ground Combat Vehicle program was cancelled in February 2014.\n\nThe Army emphasized affordability, rapid deployment, and low risk technology for the GCV. The Army required that all aspects of the Ground Combat Vehicle be at technology readiness level 6. The shortfalls of rapid deployment would be mitigated through an incremental addition of components as technology matures. The Army provided details from the Manned Ground Vehicle effort to utilize on the GCV. The GCV was required to have better protection than any vehicle in the military's inventory.\n\nGeneral Peter W. Chiarelli said that the \"four main fundamentals\" of the vehicle were: The ability to carry 12 soldiers and operate in all forms of combat; have significant protection; and deliver the first production vehicle by 2018.\n\nThe IFV would be modular and networked and offer improved survivability, mobility, and power management functions. The GCV family would use technologies pioneered with the IFV lead vehicle effort.\nThe Mounted Soldier System (MSS) was being developed for GCV crew members. MSS worked as a force multiplier enhancing situation awareness, comfort, and safety. Dismounted leaders will utilize the Ground Soldier Systems.\n\nThe IFV would be operable with the current Battle Command Control and Communications Suite but would gradually use a more revolutionary networked integration system. The system would support integration with unmanned systems, and dismounted soldiers, providing adaptive access points and connectivity. The new network concept called for decentralization of decision making.\n\nThe Mounted Soldier System was to enhance situational awareness through wireless communications and input from vehicle sensors and external sources such as other vehicles.\n\nThe IFV would provide exportable electrical power, and battery charging capability for soldier systems.\n\nThermal management and acoustic noise reduction would be utilized to avoid detection. The vehicle would be able to avoid threats by laying obscurants. An array of hit avoidance systems would be leveraged and the Army offered the various active protection systems developed for the manned ground vehicle program. The GCV enabled the detection and neutralization of mines at standoff ranges. The vehicle was also to be equipped with an engagement detection system. The Army required the IFV to have the passive blast protection level equal to the MRAP. The Army made available the composition of the armor of the manned ground vehicle program. A transparent armor shield would provide protection for the vehicle commander when exposed through the turret. Personnel would leverage harnesses and restraints to mitigate trauma. In addition, a Vehicle Health Management System would provide vehicle diagnostic monitoring systems for commanders. A fire suppression system and ammunition detonation protection would be utilized for damage control.\n\nThe Mounted Soldier System would protect crew members from ballistic, thermal, and CBRN threats. The Mounted Soldier System incorporated fire retardant systems such as the Improved Combat Vehicle Crewman Coverall and undergarments, facewear, gloves, and footwear. Ballistic protection would come from the Combat Vehicle Crewman Helmet, eyewear, a maxillofacial shield, and improvements to body armor. A secondary squad egress was to be provided for the squad to exit in emergencies.\n\nThe Infantry Fighting Vehicle variant was intended to fill the infantry transport role in Heavy Brigade Combat Teams replacing the aging M113 APC, M2 Bradley, and M1126 Infantry Carrier Vehicle. It was the U.S. Army's intention that the IFV replace the M113 APC in the near term, and the M2 Bradley and M1126 ICV in the midterm.\n\nIn the U.S. Army, as part of the ongoing restructuring, Heavy Brigade Combat Team Brigades would have an arsenal of 62 IFV's, battalions would have 29, and platoons would have 4. Platoons were to be led by platoon leader GCV which would be accompanied by platoon medic, forward observer, Radio Transmission Operator, and other attachments and would command three other GCVs.\n\nThe Army placed importance on the GCV's ability to carry a full nine-man squad. Numerous Army studies have concluded that a squad, containing two fireteams, should be composed of nine to eleven soldiers. These numbers allow the squad to accomplish the fire and maneuver doctrine, and for squad resilience, lethality, and leader span of control. The M2 Bradley cannot carry a complete squad from one vehicle, creating risk when transitioning from mounted to dismounted operations. The Bradley's lower carrying capacity was accepted for greater (than previous vehicles) mounted lethality and cost savings, leading to squads being broken apart for transport. A GCV with a nine-man squad would have allowed the squad leader to control and communicate with the squad while mounted, simplify the transition to dismounted operations in complex terrain, and allow the squad to conduct independent fire and maneuver immediately upon dismount. Replacing the Bradley on a one-for-one basis would have four GCVs per mechanized infantry platoon carrying one full nine-man squad in a single vehicle, with three vehicles carrying squads and one carrying the platoon's organic and attached enablers.\n\nThe Ground Combat Vehicle was envisioned to be a model of acquisition reform. The initial program was canceled a year into development and was soon replaced with a new program better emphasizing affordability.\n\nIn the initial plan, the first variant of the vehicle was to be prototyped in 2015 and fielded by 2017. The U.S. military planned on procuring 1,450 IFVs at a total program cost of $40 billion. The program was abruptly canceled in August 2010, before any contracts were awarded.\n\nAn Army presentation in March revealed that TARDEC, ARL, and TRADOC - ARCIC had partnered to analyze the survivability of the army's \"Ground Combat Vehicle\". Army Chief of Staff Robert Gates announced his intention of halting funding for the XM1206 Infantry Carrier Vehicle of the FCS manned ground vehicle program in April 2009. In late May, Army and Department of Defense representatives outlined plans for the cancellation of Future Combat Systems and the initiation of the Ground Combat Vehicle program in its place. On 15 and 16 June, a blue-ribbon panel convened in Washington D.C. to determine the requirements for the Ground Combat Vehicle. It was concluded at this meeting that an Infantry Fighting Vehicle was to be the first vehicle variant fielded. Defense contractors were not allowed to attend but at least six in attendance were employed by defense companies that eventually bid on the GCV contract. On 23 June, Future Combat Systems was formally dissolved and many programs including the Manned Ground Vehicle program were canceled with it. On 19 October, contractors turned up for a U.S. Army organized industry day event in Dearborn, Michigan to learn about the requirements. In late October PEO Integration was established to oversee subsystems of BCT Modernization including the GCV. On 24 November, a second industry day was held in Warren, Michigan.\n\nAfter much delay, reviews necessary for continuation were held throughout February, in Washington D.C. The GCV review was officially passed on 25 February and a request for proposal (RfP) was issued the same day. It was revealed in the RfP that the GCV would be a cost-plus contract. Companies had 60 days to respond, but this offer was extended an additional 25 days. In May, a \"red team\" was formed to curtail the GCVs 7-year development schedule. By the 21 May deadline, four proposals were submitted. On 1 July, management of the GCV was transferred from PEO Integration to PEO Ground Combat Systems with Andrew DiMarco as project manager.\n\nFor fiscal year 2011, the U.S. Army intended to spend $934 million of the $2.5 billion allocated for BCT Modernization to develop the GCV. Reportedly, $100 million was removed from the yet to be approved budget but the budget continued to reported as $934 million.\n\nOn 25 August the Army retracted its request for proposals after the red team assembled in May recommended that the Army either upgrade the existing ground vehicle fleet or rewrite the requirements.\n\nThe Technology Development Phase (or Milestone A) was to begin with the award of up to three vehicle contracts awarded in late Fiscal Year 2010 under the Technology Development Phase Contract. A Preliminary Design Review would follow in mid FY 2012. The U.S. military planned to spend $7.6 billion during Milestone A.\n\nThe Engineering and Manufacturing Development Phase (or Milestone B) was to begin with two prototype development contracts awarded in the beginning of Fiscal Year 2013 under the Engineering & Manufacturing Development Contract. Shortly thereafter, an Interim Critical Design Review would follow in Mid-FY 2013. After a nearly two-year manufacturing period the first prototypes would be manufactured Mid-FY 2015 after which a Critical Design Review and a Production Readiness Review would occur in FY 2015 and FY 2016 respectively.\n\nThe Low Rate Initial Production Phase (or Milestone C) was to begin with a low-rate production contract awarded in mid Fiscal Year 2016 under the Low Rate Initial Production (LRIP) contract. Less than two years after the contract award LRIP would begin. After more testing a battalion-sized team would be attained in FY 2018 followed by a brigade-sized arsenal in FY 2019.\n\nIf a Full-Rate Production Decision was attained, full-rate production would begin. The U.S. military planned on procuring 1,450 IFVs at a total program cost of $40 billion.\n\nThere were four known competing contractors for the Ground Combat Vehicle contract.\n\n\n\n\nIn September, Alion Science and Technology was awarded a $23,828,000 contract modification for the development of systems supporting GCV development. This contract was tendered by the U.S. Air Force and $2,180,000 in funds was obligated at the time of the award. An industry day was held 1 October in Dearborn, Michigan. The Army reduced its requested FY 2011 budget to $462 million. Advanced Defense Vehicle Systems, General Dynamics Land Systems, and BAE Systems announced their intention of re-competing soon after the cancellation. A revised RfP was to be issued around 27 October 2010. Military officials met on 20 October to discuss delaying the RfP to allow leaders time to deliberate about requirements. The panel recommended releasing the RfP without delay but George Casey said he would need time to commit to a decision. Senior leaders at the meeting felt that the 27 October target could be met. The National Commission on Fiscal Responsibility and Reform suggested deferring development of the GCV until after 2015.\nThe revised RfP was issued on 30 November. ADVS announced its decision to not submit a proposal. ADVS decision not to compete was stated to be that the vehicle's slow procurement timeline was not suited to \"ADVS’ rapid development and fielding capabilities\".\n\nUp to three cost-plus contracts were to be awarded nine months after the RfP was released. An acquisition decision memorandum on 17 August allowed the program to award technology development contracts. It also initiated two reviews of alternatives including a revised analysis of alternatives and an analysis of non-developmental vehicles. The 18 August, the Army awarded technology development contracts to only BAE and GDLS. BAE was awarded $450 million while GDLS was awarded $440 million. SAIC followed up with a bid protest on 26 August further delaying GCV development. It believed the evaluations process was flawed and the evaluation took factors into consideration that were not stated in the request for proposal.\n\n$884 million was requested by the U.S. Army to fund the GCV in FY 2012. The technology development phase was to be a 24 months long, 3 months shorter than the previous plan. The Engineering and Manufacturing Development phase was to be 48 months long. The Army planned on acquiring 1,874 GCVs to replace Bradleys in 16 active and 8 National Guard Heavy Brigade Combat Teams.\n\nTesting of commercially available combat vehicles began in May 2012 at Fort Bliss and White Sands Missile Range to prepare the Army for Milestone B. The Non-Developmental Vehicle analysis assessed five vehicles, the M2A3 Bradley, Namer, CV-9035, a double v-hulled M1126 Infantry Carrier Vehicle and a turretless Bradley. The tests, completed on May 25, were carried out to determine what vehicle variants and configurations fulfill the Army's needs. The Army found that although the vehicles assessed met some GCV requirements, no currently fielded vehicle met enough without needing significant redesign.\n\nThere were three known competing contractors for the Ground Combat Vehicle contract.\n\nA Milestone C decision could have been made in 2019.\n\nIn November 2012, estimates of the GCV's weight, depending on armor packages, put the General Dynamics entry vehicle at 64–70 tons, and the BAE Systems entry vehicle at 70–84 tons. This made the planned infantry fighting vehicle designs heavier than the M1 Abrams tank. The reason was the vehicle had to have enough armor to protect a squad of nine troops from all battlefield threats (from rocket-propelled grenades to IEDs) as good as or better than other vehicles can protect against specific threats individually. This worked against the vehicle; as weight increases, cost goes up and maneuverability goes down. The contractors worked to bring the weight down. The Army maintained that heavy armor was needed to protect the squad from acceleration forces that come with an underside blast, and that thicker underbelly plates and V-shaped hulls do not give enough protection. More armor would come from the vehicle being larger for more internal space for the soldiers, and to allow for features such as floating floors for blast deflection and extra headroom. The Army also said heavy weight would not affect deployability because the Bradley it was planned replace already requires strategic airlift transport aircraft.\n\nBoth contractors claimed their designs were below the 70–84 tons expectation of what the GCV will weigh. BAE's vehicle weighed 60–70 tons, based on modular armor package, and a 20 percent margin for weight increase the Army had planned for future upgrades would bring it up to 84 tons. General Dynamic's vehicle with a diesel engine weighed 62 tons in its most heavily armored configuration, which increased to 76 tons with the 20 percent future upgrade margin. Removing protection for easier air transportation would have reduced it to 56 tons. The Army's consideration to slow down the GCV development program gave time to the companies to refine their designs and reduce weight. One way would have been to reduce squad size. A nine-man squad has been identified as best for being able to fight with the possibility of taking casualties with single-vehicle transportability. With a three-man crew, the GCV had to carry 12 men. A greater number of lighter IFVs that carry fewer soldiers would have similar carrying capacity and combined costs and weight to planned GCV numbers. Another way would be an advance in armor designs. Lighter and stronger armor materials had not made radical progressions in recent history, and domestic active protection intercept systems were not yet mature. Foreign systems like the Israeli Trophy had seen combat but cannot yet intercept tank shells. The GCV program originally included an APS, but was then delayed as a feature for later upgrades. The last effort to replace the Bradley was with Future Combat Systems, which developed a vehicle that relied on sensors to avoid danger and an APS in place of heavy armor. It was too ambitious for the time and the vehicle's weight had grown from 19 tons to 30 tons by the time it was cancelled.\n\nThe BAE Systems Ground Combat Vehicle design had a steel-core hull and an integrated electronic network capability with embedded intelligence, surveillance, and reconnaissance equipment. Its turret was unmanned. The centerpiece of the vehicle was its simplified drive train. It was propelled by a Hybrid Electric Drive (HED), which was developed by Northrop Grumman, that produced 1,100 kW of electricity. Advantages to it are fewer components and lower volume and weight compared to current power plants. The transmission was 40 percent smaller and the drive train had half the moving parts. The hybrid drive train cost 5 percent more than a mechanical system, but had a 20 percent reduction in life-cycle cost. The electric drive allows for smoother low-speed operation and less noise. The vehicle burned 20 percent less fuel while running, with 4.61 gallons (17.45 liters) per hour used while stationary. It had a top speed of 43 mph (70 km/h), could go from 0 to 20 mph (32.18 km/hr) in 7.8 seconds, and had a range of with a 255-gallon fuel capacity. Disadvantages to the BAE design included a weight of 70 tons and fuel efficiency of only 0.73 mpg. It was argued that big, heavy vehicles are not practical in urban combat and that the infrastructure of urban and third-world countries should limit the vehicle's weight to 45 tons. Others said that urban warfare tactics have become so lethal that only vehicles of this size can survive. BAE integrated the Artis Iron Curtain active protection system to defeat incoming rockets and missiles before they can hit the vehicle. The Army conducted tests on the system in April 2013, and the it successfully passed all tests. A prototype system for the vehicle to drive in low visibility conditions was also tested. A Humvee with blacked-out windows drove through a smoke-filled mock town with the system safely, even though visibility was completely obscured. In August 2013, the BAE GCV's hybrid electric drive completed 2,000 miles of testing on a fully integrated “Hotbuck” mobility platform. The Hotbuck is a stationary test stand that simulates real-life environments and terrain and puts actual miles on the HED system. Under BAE's own timeline, the testing was completed four months ahead of schedule. Developing and testing actual hardware was not a program requirement for the Technology Development (TD) phase, but BAE Systems chose to demonstrate the fuel efficiency and performance of a hybrid system.\n\nAlthough dramatic funding cuts for the GCV program in January 2014 put the very completion of the acquisition effort in jeopardy, funding remained for research on a hybrid-electric propulsion system. The BAE GCV's hybrid-electric engine is more fuel efficient, has fewer moving parts, and has faster acceleration than ordinary engines. While powering a vehicle concept that reached 70 tons proved impractical, its benefits of providing power for onboard electronics, silent overwatch, and short, stealthy movements are still promising. BAE has pledged to support future Army developmental efforts with technologies from their GCV entry. On 18 July 2014, BAE Systems was awarded a $7.9 million study contract for technical, cost, and risk assessments to utilize the GCV TD phase integrated hybrid-electric propulsion and mobility subsystems Automotive Test Rig (ATR) and the hybrid-electric integrated propulsion subsystem (Hotbuck) for the Future Fighting Vehicle (FFV) effort.\n\nOn 31 October 2013, General Dynamics successfully completed a preliminary design review of their GCV IFV design. Subsystem and component design reviews were held from August to October of that year and led to the four-day PDR. General Dynamics demonstrated their vehicle met Tier 1 affordability, reliability, and other requirements. The success of the PDR meant that the General Dynamics GCV IFV could be expected to be operationally effective and suitable.\n\n\n"}
{"id": "10078385", "url": "https://en.wikipedia.org/wiki?curid=10078385", "title": "GPS wildlife tracking", "text": "GPS wildlife tracking\n\nGPS wildlife tracking is a process whereby biologists, scientific researchers or conservation agencies can remotely observe relatively fine-scale movement or migratory patterns in a free-ranging wild animal using the Global Positioning System and optional environmental sensors or automated data-retrieval technologies such as Argos satellite uplink, mobile data telephony or GPRS and a range of analytical software tools.\n\nA GPS-enabled device will normally record and store location data at a pre-determined interval or on interrupt by an environmental sensor. These data may be stored pending recovery of the device or relayed to a central data store or internet-connected computer using an embedded cellular (GPRS), radio, or satellite modem. The animal's location can then be plotted against a map or chart in near real-time or, when analysing the track later, using a GIS package or custom software. \n\nWhile GPS tracking devices may also be attached to domestic animals such as pets, pedigree livestock and working dogs, and similar systems are used in fleet management of vehicles, wildlife tracking can place additional constraints on size and weight and may not allow for post-deployment recharging or replacement of batteries or correction of attachment.\n\nAs well as allowing in-depth study of animal behaviour and migration, the high-resolution tracks available from a GPS-enabled system can potentially allow for tighter control of animal-borne communicable diseases such as the H5N1 strain of avian influenza.\n\nCollar attachment is the primary attachment technique where the subject has a suitable body type and behaviour. Tracking collars would normally be used on the animal's neck (assuming the head has a larger circumference than the neck) but also on a limb, perhaps around an ankle. Suitable animals for neck attachment would include primates, large cats, some bears etc. Limb attachment would work well in animals such as kiwi, where the foot is much larger than the ankle.\n\nHarness attachments may used in situations where collar attachment is not suitable, such as animals whose neck diameter may exceed that of the head. Examples of this type of animal may include pigs, Tasmanian devils, etc. \nLarge, long-necked, birds such as the greylag goose may also need to be fitted with a harness to prevent removal of the tag by the subject.\n\nDirect attachment is used on animals where a collar cannot be used, such as birds, reptiles and marine mammals. \n\nIn the case of birds, the GPS unit must be very lightweight to avoid interfering with the bird's ability to fly or swim. The device is usually attached by gluing or, for short deployments, taping to the bird. The unit will then naturally fall off when the bird next moults.\n\nIn the case of reptiles such as crocodiles and turtles, gluing the unit onto the animal's skin or carapace using epoxy (or similar material) is the most common method and minimises discomfort.\n\nIn deployments on marine mammals such as phocids or otariids, the device would be glued to the fur and fall off during the annual moult. Units used with turtles or marine animals have to resist the corrosive effects of sea water and be waterproof to pressures of up to 200bar.\n\nOther applications include rhinoceros tracking, for which a hole may be drilled in the animal's horn and a device implanted. Compared to other methods, implanted transmitters may suffer from a reduced range as the large mass of the animal's body can absorb some transmitted power.\n\nThere are also GPS implants for large snakes, such as ones offered by Telemetry Solutions.\n\nDuty Cycle Scheduling - GPS devices typically record data about the animal's exact location and store readings at pre-set intervals known as duty-cycles.\nBy setting the interval between readings, the researcher is able to determine the lifespan of the device - very frequent readings drain battery power more rapidly, whereas longer intervals between readings might provide lower resolution but over a longer deployment. \n\nRelease Timers - Some devices can be programmed to drop off at a set time/date rather than requiring recapture and manually retrieval. Some may also be fitted with a low-power radio receiver allowing a remote signal to trigger the automatic release. \n\nLocational data provided by GPS devices can be displayed using GIS packages such as the open-source GRASS or plotted and prepared for display on the World Wide Web using packages such as Generic Mapping Tools (GMT), FollowDem (developed by Ecrins national Park to track ibex) or Maptool.\n\nStatistical software such as R can be used to display and examine data and may reveal behavioural patterns or trends.\n\nGPS tracking devices have been linked to an Argos Platform Transmitter Terminal (PTT) enabling them to transmit data via the Argos System, a scientific satellite system which has been in use since 1978. Users can download their data directly from Argos via telnet and process the raw data to extract their transmitted information.\n\nWhere satellite uplink fails due to antenna damage, it may be possible to intercept the underpowered transmission locally using a satellite uplink receiver.\n\nGPS location data can be transmitted via the GSM mobile/cell phone network, using SMS messages or internet protocols over a GPRS session.. The EPASTO GPS is dedicated to follow and locate cow.\n\nGPS data may be transmitted via short-range radio signals and decoded using a custom receiver.\n\n"}
{"id": "45636090", "url": "https://en.wikipedia.org/wiki?curid=45636090", "title": "Gopal Palpodi", "text": "Gopal Palpodi\n\nGopal Palpodi or Gopal Tooth Powder is a brand of tooth powder, popular in Tamil Nadu state of India. It is also popular among the Tamil diaspora in Sri Lanka, Malaysia and Singapore .\n\nThe brand is manufactured by the Madurai-based S.P.S. Jayam and Company. It is prepared with a combination of herbs. The trademark \"Gopal Tooth Powder\" was first registered on 30 April 1947.\n"}
{"id": "1365624", "url": "https://en.wikipedia.org/wiki?curid=1365624", "title": "Handle", "text": "Handle\n\nA handle is a part of, or attachment to, an object that can be moved or used by hand. The design of each type of handle involves substantial ergonomic issues, even where these are dealt with intuitively or by following tradition. Handles for tools are an important part of their function, enabling the user to exploit the tools to maximum effect. Package handles allow for convenient carrying of packages.\n\nThe three nearly universal requirements of are:\n\nOther requirements may apply to specific handles:\n\nOne major category of handles are \"pull\" handles, where one or more hands grip the handle or handles, and exert force to shorten the distance between the hands and their corresponding shoulders. The three criteria stated above are universal for pull handles.\n\nMany pull handles are for lifting, mostly on objects to be carried.\n\nHorizontal pull handles are widespread, including drawer pulls, handles on latchless doors and the outside of car doors. The inside controls for opening car doors from inside are usually pull handles, although their function of permitting the door to be \"pushed\" open is accomplished by an internal unlatching linkage.\n\nPull handles are also a frequent host of common door handle bacteria such as e-coli, fungal or other viral infections.\n\nTwo kinds of pull handles may involve motion in addition to the hand-focused motions described:\n\nAnother category of hand-operated device requires grasping (but not pulling) and rotating the hand and either the lower arm or the whole arm, about their axis. When the grip required is a fist grip, as with a door handle that has an arm rather than a knob to twist, the term \"handle\" unambiguously applies. Another clear case is a rarer device seen on mechanically complicated doors like those of airliners, where (instead of the whole hand moving down as it also rotates, on the door handles just described) the axis of rotation is between the thumb and the outermost fingers, so the thumb moves up if the outer fingers move down.\n\nThe handles of bicycle grips, club-style weapons, shovels and spades, axes, hammers, mallets and hatchets, baseball bats, rackets, golf clubs, and croquet mallets involve a greater range of ergonomic issues.\n"}
{"id": "38754240", "url": "https://en.wikipedia.org/wiki?curid=38754240", "title": "Hydrothermal liquefaction", "text": "Hydrothermal liquefaction\n\nHydrothermal liquefaction (HTL) is a thermal depolymerization process used to convert wet biomass into crude-like oil -sometimes referred to as bio-oil or biocrude- under moderate temperature and high pressure. The crude-like oil (or bio-oil) has high energy density with a lower heating value of 33.8-36.9 MJ/kg and 5-20 wt% oxygen and renewable chemicals.\n\nThe reaction usually involves homogeneous and/or heterogeneous catalysts to improve the quality of products and yields. Carbon and hydrogen of an organic material, such as biomass, peat or low-ranked coals (lignite) are thermo-chemically converted into hydrophobic compounds with low viscosity and high solubility. Depending on the processing conditions, the fuel can be used as produced for heavy engines, including marine and rail or upgraded to transportation fuels, such as diesel, gasoline or jet-fuels.\n\nAs early as the 1920s, the concept of using hot water and alkali catalysts to produce oil out of biomass was proposed. This was the foundation of later HTL technologies that attracted research interest especially during the 1970s oil embargo. It was around that time that a high-pressure (hydrothermal) liquefaction process was developed at the Pittsburgh Energy Research Center (PERC) and later demonstrated (at the 100 kg/h scale) at the Albany Biomass Liquefaction Experimental Facility at Albany, Oregon, US. In 1982, Shell Oil developed the HTU™ process in the\nNetherlands. Other organizations that have previously demonstrated HTL of biomass include Hochschule für Angewandte Wissenschaften Hamburg, Germany, SCF Technologies in Copenhagen, Denmark, EPA’s Water Engineering Research Laboratory, Cincinnati, Ohio, USA, and Changing World Technology Inc. (CWT), Philadelphia, Pennsylvania, USA. Today, technology companies such as Licella/Ignite Energy Resources (Australia), Altaca Energy (Turkey), Steeper Energy (Denmark, Canada), and Nabros Energy (India) continue to explore the commercialization of HTL.\n\nIn hydrothermal liquefaction processes, long carbon chain molecules in biomass are thermally cracked and oxygen is removed in the form of HO (dehydration) and CO (decarboxylation). These reactions result in the production of high H/C ratio bio-oil. Simplified descriptions of dehydration and decarboxylation reactions can be found in the literature (e.g. Asghari and Yoshida (2006) and Snåre et al. (2007))\n\nMost applications of hydrothermal liquefaction operate at temperatures between 250-550C and high pressures of 5-25 MPa as well as catalysts for 20–60 minutes, although higher or lower temperatures can be used to optimize gas or liquid yields, respectively. At these temperatures and pressures, the water present in the biomass becomes either subcritical or supercritical, depending on the conditions, and acts as a solvent, reactant, and catalyst to facilitate the reaction of biomass to bio-oil.\n\nThe exact conversion of biomass to bio-oil is dependent on several variables:\n\nTheoretically, any biomass can be converted into bio-oil using hydrothermal liquefaction regardless of water content, and various different biomasses have been tested, from forestry and agriculture residues, sewage sludges, food process wastes, to emerging non-food biomass such as algae. The composition of cellulose, hemicellulose, protein, and lignin in the feedstock influence the yield and quality of the oil from the process.\n\nTemperature plays a major role in the conversion of biomass to bio-oil. The temperature of the reaction determines the depolymerization of the biomass to bio-oil, as well as the repolymerization into char. While the ideal reaction temperature is dependent on the feedstock used, temperatures above ideal lead to an increase in char formation and eventually increased gas formation, while lower than ideal temperatures reduce depolymerization and overall product yields.\n\nSimilarly to temperature, the rate of heating plays a critical role in the production of the different phase streams, due to the prevalence of secondary reactions at non-optimum heating rates. Secondary reactions become dominant in heating rates that are too low, leading to the formation of char. While high heating rates are required to form liquid bio-oil, there is a threshold heating rate and temperature where liquid production is inhibited and gas production is favored in secondary reactions.\n\nPressure (along with temperature) determines the super- or subcritical state of solvents as well as overall reaction kinetics and the energy inputs required to yield the desirable HTL products (oil, gas, chemicals, char etc.).\n\nHydrothermal liquefaction is a fast process, resulting in low residence times for depolymerization to occur. Typical residence times are measured in minutes (15 to 60 minutes); however, the residence time is highly dependent on the reaction conditions, including feedstock, solvent ratio and temperature. As such, optimization of the residence time is necessary to ensure a complete depolymerizaiton without allowing further reactions to occur.\n\nWhile water acts as a catalyst in the reaction, other catalysts can be added to the reaction vessel to optimize the conversion. Previously used catalysts include water-soluble inorganic compounds and salts, including KOH and NaCO, as well as transition metal catalysts using Ni, Pd, Pt, and Ru supported on either carbon, silica or alumina. The addition of these catalysts can lead to an oil yield increase of 20% or greater, due to the catalysts converting the protein, cellulose, and hemicellulose into oil. This ability for catalysts to convert biomaterials other than fats and oils to bio-oil allows for a wider range of feedstock to be used.\n\nBiofuels that are produced through hydrothermal liquefaction are carbon neutral, meaning that there are no net carbon emissions produced when burning the biofuel. The plant materials used to produce bio-oils use photosynthesis to grow, and as such consume carbon dioxide from the atmosphere. The burning of the biofuels produced releases carbon dioxide into the atmosphere, but is nearly completely offset by the carbon dioxide consumed from growing the plants, resulting in a release of only 15-18 g of CO/kWh or energy produced. This is substantially lower than the releases rate of fossil fuel technologies, which can range from releases of 955 g/kWh (coal), 813 g/kWh (oil), and 446 g/kWh (natural gas). Recently, Steeper Energy announced that the Carbon Intensity (CI) of its Hydrofaction™ oil is 15 COeq/MJ according to GHGenius model (version 4.03a), while diesel fuel is 93.55 COeq/MJ.\n\nHydrothermal liquefaction is a clean process that doesn't produce harmful compounds, such as ammonia, NO, or SO. Instead the heteroatoms, including nitrogen, sulfur, and chlorine, are converted into harmless byproducts such as N and inorganic acids that can be neutralized with bases.\n\nThe HTL process differs from pyrolysis as it can process wet biomass and produce a bio-oil that contains approximately twice the energy density of pyrolysis oil. Pyrolysis is a related process to HTL, but biomass must be processed and dried in order to increase the yield. The presence of water in pyrolysis drastically increases the heat of vaporization of the organic material, increasing the energy required to decompose the biomass. Typical pyrolysis processes require a water content of less than 40% to suitably convert the biomass to bio-oil. This requires considerable pretreatment of wet biomass such as tropical grasses, which contain a water content as high as 80-85%, and even further treatment for aquatic species, which can contain higher than 90% water content.\n\nThe HTL oil can contain up to 80% of the feedstock carbon content (single pass). HTL oil has good potential to yield bio-oil with \"drop-in\" properties that can be directly distributed in existing petroleum infrastructure.\n\n"}
{"id": "4587303", "url": "https://en.wikipedia.org/wiki?curid=4587303", "title": "Insight Technology", "text": "Insight Technology\n\nInsight Technology, Inc. is an optical device manufacturer based in Londonderry, New Hampshire, USA.\n\nInsight Technology builds firearm accessories such as tactical flashlights and laser aiming modules, for military and civilian markets. The company is best known for making the AN/PEQ-2 and AN/PEQ-6 laser sights used by some branches of the United States armed forces.\n\nInsight XTI Procyon Tactical Light\n\nThe Insight XTI Procyon is a tactical light designed to mount on the accessory rail. The light is provided by a 125 lumen LED, and the housing is made of anodized aluminum. The XTI Procyon is water resistant to 15 feet.\n\nInsight ISM Integrated Sighting Module\n\nThe Insight Integrated Sighting Module (ISM) is a non-magnified red dot sight equipped with an integral infrared target illumination laser and visible laser sight. The ISM was originally produced in tandem with the XM8 Assault Rifle, with the intention of becoming a standard attachment. However, even as the XM8 project was cancelled, Insight continued development of the ISM sight until it was released in early 2007. The ISM can fit any MIL-STD 1913 rail.\n\n"}
{"id": "25021893", "url": "https://en.wikipedia.org/wiki?curid=25021893", "title": "Internet Security Alliance", "text": "Internet Security Alliance\n\nInternet Security Alliance (ISAlliance) was founded in 2001 as a non-profit collaboration between the Electronic Industries Alliance (EIA), a federation of trade associations, and Carnegie Mellon University's CyLab, focussing on cyber security. The ISAlliance is a forum for information sharing and leadership on information security, and it lobbies for corporate security interests.\n\n\nThe ISAlliance has proposed a \"Cybersecurity Social Contract\" that offers an \"action plan\" to protect the United States from cyber attacks.\n\nWhile the ISAlliance is physically based in the United States, its membership operates internationally and the goal is international security for all its trusted partners. The ISAlliance has member companies on four continents. There has always been a non-U.S. based company on the Executive Committee. The ISAlliance believes the international communication it fosters is critical to the long-term success of achieving greater information security. Moreover, international representation and voice more realistically addresses the many difficult problems which face all users of the Internet.\n\nPublished in 2009, \"The Financial Impact of Cyber Risk\" is the first known guidance document to approach the financial impact of cyber risks from the perspective of core business functions. It provides guidance to CFOs and their colleagues responsible for legal issues, business operations and technology, privacy and compliance, risk assessment and insurance, and corporate communications.\n\n"}
{"id": "24551539", "url": "https://en.wikipedia.org/wiki?curid=24551539", "title": "Journal of Materials Science: Materials in Electronics", "text": "Journal of Materials Science: Materials in Electronics\n\nThe Journal of Materials Science: Materials in Electronics is a peer-reviewed scientific journal published by Springer Science+Business Media. It is an offshoot of the \"Journal of Materials Science\", focusing specifically on materials used in electronics. The editor-in-chief is Arthur F.W. Willoughby (University of Southampton).\n\nThis journal was originally published quarterly by Chapman & Hall from May 1990. This frequency was changed to six issues per year from 1994 to 1998, and then nine issues per year from 1999 to 2000. From 2001, the journal has been published monthly.\n\nThe journal is abstracted andindexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2012 impact factor of 1.486.\n\n"}
{"id": "2145387", "url": "https://en.wikipedia.org/wiki?curid=2145387", "title": "Kisekae Set System", "text": "Kisekae Set System\n\nKisekae Set System (commonly known as KiSS) is a blending of art with computers originally designed to allow creation of virtual \"paper dolls\". Kisekae is short for \"kisekae ningyou\"; a Japanese term meaning \"dress-up dolls\". Unlike \"computer art\" which creates or displays traditional art via a computer, KiSS uses the computer as the medium, allowing the art to be not only animated, but also interactive.\n\nKiSS is an Open standard which has to some extent been implemented on many platforms, including several PDAs. It has also been implemented in Java and on the web.\n\nKiSS originated in Japan in 1991 with \"dolls\" based on shōjo manga characters.\n\nThe original dolls, a series of simple, static images, could be moved about and layered on top of one another to look as if the doll image was wearing the clothing. Using computer graphics had the advantage over traditional paper dolls in allowing multiple layers to move in unison, including visually separate pieces, giving an illusion of depth not possible with physical paper.\n\nThe initial viewer software was designed for NEC PC-9800 series using a palette of 16 colours to display the doll. Shortly after, an enhanced standard was put forward (General Specification 2 known as 'KiSS/GS2') which included support for VGA cards and 256 or multiple 16 colour palettes. This standard is still the basis of KiSS, but several additional specifications have been incorporated into viewers since then, in particular \"French KiSS\", generally called FKiSS, for controlling interactivity and animation and \"Cherry KiSS\" (i.e. CKiSS) for 32-bit \"true\" colour support.\n\nBy the late 1990s KiSS had spread from the Japanese BBS communities internationally via the Internet with artists creating \"dolls\", programmers creating support tools, and fans appearing worldwide.\n\nNote that although KiSS sets are often referred to generically as 'dolls' they are not confined to dress-up — in fact they can be anything and there are \"build-your-own\" faces, wedding cakes, dollhouses, battleships, as well as puzzles, games and much more. Nonetheless such \"unusual\" sets are sometimes referred to as \"aberrant\" KiSS.\n\nA KiSS set consists of many files of a number of different formats. These are packaged for distribution as a single set or 'doll' in LZH format (a preferred archive format in Japan) which viewer programs can read as a whole to obtain the individual files.\n\nMost files are 'cel' files which are raw, uncompressed graphics data analogous to animation cels. KiSS/GS2 specification cels also require a KCF (KiSS Colour File) as a palette, but CKiSS specification cels do not. A KCF also can control background colour and contain multiple palettes that can be swapped for lighting and colour change effects. All KiSS binary files (KCF, standard and CKiSS cels) since KiSS/GS2 share a common 32 byte binary header record identifying the size, type and format of KiSS data they contain.\n\nA configuration file is also required to control field size, layering, cel position, use of palettes, and interaction and animation events.\n\nIn addition Midi files for music and WAV files for sound clips may be used, and generally some form of text documentation is included by the artist.\n\nKiSS sets are allowed to acquire resources from other KiSS sets by a process called 'Expansion'. This allows new versions of a doll without incorporating the original cells into the new set, meaning that earlier versions did not have to be replaced, and different artists could add to the doll without confusion as to who the original artist was. This dates from some of the earliest viewers, but the details of loading an expansion set remain somewhat viewer dependent.\n\nA number of features have been added to KiSS but never formally incorporated into the main KiSS format. For compatibility and to hide them from viewers that don't support them they are disguised as comments in the configuration file. Each type of extension (except user grouping) was initially introduced in Japan, however all (except Cherry KiSS) have later been extended by international viewers.\n\n'French' KiSS (or 'FKiSS') is an event driven scripting language created as an experimental add-on to the KiSS/GS2 specifications. It was introduced in Japan to allow animation and greater interactivity in KiSS. It was the first extension, and intended only for testing but it proved so popular that it became entrenched as is. All FKiSS directives appear preceded in the first column of their configuration line by:\nThe \";\" normally indicates the beginning of a comment, which originally hid the directive if a viewer didn't handle FKiSS although it is now standard in all viewers.\n\nFKiSS itself has been extended several times:\n\nThese are additions to the cell definitions to control start up properties. They appear as a comment at the end of the cell definition that immediately starts with a % and a code. The first (%t - to control initial transparency) was added when the first level of FKiSS was finalized. Other properties added with FKiSS4 include display status (%u), clickability (%g) and offset overrides (%x and %y).\n\nThese are comment added to the configuration to suggest to the viewer program how best to automatically display the set. Originally used in Japan to indicate other KiSS sets of which the current one is an expansion (;INCLUDE -- i.e. where to find referenced resources not included in the set), later viewers use them to indicate optimal settings for the set being loaded (;HINT).\n\nCommonly called 'CKiSS', this is an extension to the binary data header record, and unlike other extensions makes no changes to the configuration file. It is a specification allowing a cell file to contain raw 24-bit colour data and an 8 bit alpha channel for variable transparency. CKiSS cells tend to use a lot of disk space compared to palette-based cels, and do not compress well, so they are used sparingly by most artists.\n\nUser groupings were added along with FKiSS4 to simplify controlling large numbers of cells (or uniquely identify specific cells) for testing and animation.\n\nThere are many programs on most platforms which can convert from standard graphics formats (most commonly BMP, GIF or PSD files) to KiSS cel and KCF files, allowing the artist to create the original images with any freeware or proprietary graphics program. In addition, GIMP is a fully featured graphics program which can open and save CEL files directly, leaving no need for conversion.\n\nThe configuration file is written with a text editor (standard as part of any Operating System software). Once the basic files are created a KiSS viewer is used to display and fine tune the set, then an archiver with LZH capability is used for packaging. All the software needed is freely available on the internet, as are detailed tutorials for KiSS creation.\n\nThe modern KiSS community on the internet resembles the dolling community with which there is a degree of overlap, though the two are distinct and each is protective of its own art. However, since KiSS art is more specialized the KiSS community is centralized around the largest archive of dolls on the internet, the BiG KiSS Page. Unfortunately in recent years bandwidth costs have forced the BKP to allow most doll downloads by subscription only, which has had negative impacts on the size of the active community.\n\nBecause being able to dress a doll implies being able to undress as well there has always been a subgenre of 'adult' KiSS which exists independently of the main community.\n"}
{"id": "56070662", "url": "https://en.wikipedia.org/wiki?curid=56070662", "title": "LG K10 (2018)", "text": "LG K10 (2018)\n\nThe 2018 LG K10 is an upcoming mid-range Android phone developed and manufactured by LG Electronics and was announced on Mobile World Congress in February 2018, alongside the 2018 LG K8. The phone will be released in April 2018. Notable changes compared to the predecessor include a redesigned form factor and some hardware upgrades. The model will be sold with three variants, namely the base LG K10, the low-end LG K10α (K10 Alpha) and the high-end LG K10+.\n\nThe 2018 model sports a metal design, compared to the predecessor. Similar to the previous model, it has the same processor and 720p screen.\n\nThe LG K10α has the downgraded camera, with 8 MP rear and 5 MP front cameras, compared to K10 and K10's 13 MP rear and 8 MP or 5 MP front camera. They can record 1080p at 30fps of video. The LG K10+'s storage and RAM was increased to 32 GB and 3 GB respectively, compared to LG K10 and K10α's 16 GB of storage and 2 GB RAM. They can be expandable of up to 400 GB of microSD card.The battery of 2018 models has upgraded to 3000 mAh, compared to the predecessor's 2800 mAh battery. \n\nIt is 100 euros in Iran. \n"}
{"id": "43815192", "url": "https://en.wikipedia.org/wiki?curid=43815192", "title": "Liftware", "text": "Liftware\n\nLiftware is a brand name for a spoon designed to counteract the tremor associated with medical conditions such as Parkinson's disease or essential tremors. The company which designed the projects, Lift Labs, was founded by Anupam Pathak, a University of Michigan Ph.D. student.\n\nThe device works by detecting tremors with an accelerometer then responding to them with an actuator. The product first became available in December 2013.\n\nGoogle launched its version of the spoon in November 2014, priced at $195.\n"}
{"id": "14759475", "url": "https://en.wikipedia.org/wiki?curid=14759475", "title": "MacMinute", "text": "MacMinute\n\nMacMinute was the name of a web site that provided news and information focused on Apple Inc and the Macintosh Operating system. It was founded by Canadian businessman Stan Flack in 2001 to \"keep you up-to-date on everything that is going on in the world of Macintosh as soon as it happens\". On the opening day, May 9, 2001 , the update to the Mac OS X to 10.0.3 was announced.\n\nPrior to creating MacMinute Stan Flack founded the MacCentral news website which he sold to MacWorld on June 1, 1999. This website was eventually folded into the MacWorld brand name and the last vestiges of MacCentral disappeared with the incorporation of the MacCentral Forum into the MacWorld forums in December 2007.\n\nMacMinute was noted for its up to the minute digest of current breaking news about everything connected to the Macintosh and Apple products but one of its more important functions was to serve as a forum for the Macintosh community. This MacMinute Reader Café is an eclectic mix of Macintosh users who provide and share expertise in all aspects of the Macintosh platform.\n\nMr. Flack died of natural causes, from complications to a pre-existing condition, in his home on Prince Edward Island, Canada on April 14, 2008.\n\nOn May 22, 2008, the Flack family announced the closing of the Macminute website and forums. Some forum members decide to move to MacCentral Cafe as it reminds them of the single forum Stan Flack started with at the MacCentral news website.\n\nOn June 5, 2008 it is announced on the Macminute news site that MacTech's Community News Scan and Macsimum News will act as caretakers to the archives of MacMinute News and Forums. They also announced they would continue to host the MacMinute Forums which are now up and running smoothly as of November 2008. Stan's Lounge is crowded with MacCentral and MacMinute faithful in honor of Stan Flack and all that he did for the Macintosh community.\n\n\n\n"}
{"id": "20261", "url": "https://en.wikipedia.org/wiki?curid=20261", "title": "Machete", "text": "Machete\n\nA machete (; ) is a broad blade used either as an implement like an axe, or in combat like a short sword. The blade is typically long and usually under thick. In the Spanish language, the word is a diminutive form of the word \"macho\", which was used to refer to sledgehammers. In the English language, an equivalent term is matchet, though it is less commonly used. In the English-speaking Caribbean, such as Jamaica, Barbados, Guyana, and Grenada and in Trinidad and Tobago, the term \"cutlass\" is used for these agricultural tools.\n\nIn various tropical and subtropical countries, the machete is frequently used to cut through rainforest undergrowth and for agricultural purposes (e.g. cutting sugar cane). Besides this, in Latin America a common use is for such household tasks as cutting large foodstuffs into pieces—much as a cleaver is used—or to perform crude cutting tasks, such as making simple wooden handles for other tools. It is common to see people using machetes for other jobs, such as splitting open coconuts, yard work, removing small branches and plants, chopping animals' food, and clearing bushes.\n\nMachetes are often considered tools and used by adults. However, many hunter–gatherer societies and cultures surviving through subsistence agriculture begin teaching babies to use sharp tools, including machetes, before their first birthdays.\n\nBecause the machete is common in many tropical countries, it is often the weapon of choice for uprisings. For example, the Boricua Popular Army are unofficially called \"macheteros\" because of the machete-wielding laborers of sugar cane fields of past Puerto Rico.\n\nMany of the killings in the 1994 Rwandan genocide were performed with machetes, and they were the primary weapon used by the Interahamwe militias there. Machetes were also a distinctive tool and weapon of the Haitian \"Tonton Macoute\".\n\nIn 1762, the Kingdom of Great Britain invaded Cuba in the Battle of Havana, and peasant guerrillas led by Pepe Antonio, a Guanabacoa councilman, used machetes in the defense of the city. The machete was also the most iconic weapon during the independence wars in that country (1868–1898), although it saw limited battlefield use. Carlos Manuel de Céspedes, owner of the sugar refinery \"La Demajagua\" near Manzanillo, freed his slaves on 10 October 1868. He proceeded to lead them, armed with machetes, in revolt against the Spanish government. The first cavalry charge using machetes as the primary weapon was carried out on 4 November 1868 by Máximo Gómez, a sergeant born in the Dominican Republic, who later became the general in chief of the Cuban Army.\n\nThe machete was (and still is) a common side arm and tool for many ethnic groups in West Africa. Machetes in this role are referenced in Chinua Achebe's \"Things Fall Apart\".\n\nSome countries have a name for the blow of a machete; the Spanish \"machetazo\" is sometimes used in English. In the British Virgin Islands, Grenada, Jamaica, Saint Kitts and Nevis, Barbados, Saint Lucia, and Trinidad and Tobago, the word \"planass\" means to hit someone with the flat of the blade of a machete or cutlass. To strike with the sharpened edge is to \"chop\". Throughout the Caribbean, the term 'cutlass' refers to a laborers' cutting tool.\n\nThe Brazilian Army's Instruction Center on Jungle Warfare developed a machete with a blade in length and a very pronounced clip point. This machete is issued with a 5-inch Bowie knife and a sharpening stone in the scabbard; collectively called a \"jungle kit\" (\"Conjunto de Selva\" in Portuguese); it is manufactured by Indústria de Material Bélico do Brasil (IMBEL).\n\nMany fictitious slashers have used it as a weapon in horror movies, the most notorious being Jason Voorhees, from the \"Friday the 13th\" movie series.\n\nThe tsakat is used primarily in southern Armenia and Artsakh when clearing areas or hiking. It's especially well suited for clearing the plentiful blackberry plants in these regions.\n\nThe \"panga\" or \"tapanga\" is a variant used in East and Southern Africa. This name may be of Swahili etymology; not to be confused with the Panga fish. The \"panga\" blade broadens on the backside and has a length of . The upper inclined portion of the blade may be sharpened.\n\nThis tool has been used as a weapon: during the Mau Mau Uprising; in the Rwandan Genocide; in South Africa particularly in the 1980s and early 1990s when the former province of Natal was wracked by conflict between the African National Congress and the Zulu-nationalist Inkatha Freedom Party.\n\nIn the Philippines, the \"bolo\" is a very similar tool, but with the blade swelling just before the tip to make the knife even more efficient for chopping. Variations include the longer and more pointed \"iták\" intended for combat; this was used during the Philippine Revolution against the Spanish colonial authorities, later becoming a signature weapon of guerrillas in the Philippine–American War. Filipinos still use the \"bolo\" for everyday tasks, such as clearing vegetation and chopping various large foodstuffs. These are also commonly found in most Filipino kitchens, with some sets displayed on the walls and other sets for less practical use. The \"bolo\" is also used in training in \"eskrima\", the indigenous martial art of the Philippines.\n\nOther similar tools include the \"parang\" and the \"golok\" (from Malaysia and Indonesia); however, these tend to have shorter, thicker blades with a primary grind, and are more effective on woody vegetation. The Nepalese \"kukri\" is a curved blade that is often used for similar tasks.\n\nIn Thailand, more variations exist, such as the \"e-nep\", or \"nep\", which translates as \"leaf\" (มีดเหน็บ). It may resemble some forms of Muslim blades like the \"jambiya\", or the Nepali \"khukuri\", having aspects of both with the up-swept tip and protruding belly. Another design found in Thailand is the \"e-toh\", which is prominent in Southern China, Laos, and other northern parts of South East Asia. Generally, \"e-tohs\" must have forward weighted tips, and are used around the home for splitting stove wood or chopping bone. The Chinese \"dao\", with its triangular tip, is found in Thailand as the \"hua-tad\" (หัวแตด), which translates roughly as \"head chopper.\" The most common blade in Thailand is called the \"pra\", (พร้า) it can describe long straight designs, or billhook designs. The primary purpose of a \"pra\" is farm work and clearing vegetation.\n\nIn the various regions of Ecuador, it is still used as an everyday tool in agricultural labors, such as clearing, chopping, cutting and felling. In the Pacific coast region, the machete has a long history of use and can be seen as part of the everyday dress of the rural male inhabitants, especially in the provinces of Manabi, Los Rios and Guayas. In its day, the machete and the skills related to it were seen as a token of manliness, and it was carried, sword-like, in ornamented sheaths made out of leather or in sashes around the waist. Its use was not limited to agriculture: it also had a double role as a ready-to-hand weapon for self-defense or attack. Although modern laws in Ecuador now prohibit its use as a weapon, there are still cases of vicious fighting or intimidation related to it. Being a part of the male dress, it also has a part in the cultural expressions of the coastal rural regions of Ecuador, such as dances, horse taming contests and skill exhibitions.\n\nIn the southern Brazilian state of Rio Grande do Sul, the machete made by Spanish is largely used. It is used to clear paths through the bush, and was used to fight against the Brazilian Empire in the Ragamuffin War. There, the machete is called \"facão\" or \"facón\" (literally \"big knife\"). Today, this region has a dance called the \"dança dos facões\" (machetes' dance) in which the dancers, who also always men, knock their machetes while dancing, simulating a battle. \"Maculelê\", an Afro-Brazilian dance and martial art, can also be performed with \"facões\". This practice began in the city of Santo Amaro, Bahia, in the northeastern part of the country.\n\nIn southern Mexico and Central America it is widely used to clear bush and often hundreds of \"macheteros\" are contracted to assist in clearing paths for the construction of new roads or structures. Many people in the rural regions own machetes to clear the constant overgrowth of jungle bush. In the recent drug cartel wars of the region, many homicides and decapitations are suspected of being committed with machetes or similar tools.\nThe \"taiga\" is a machete of Russian origin that combines the functions of machetes, axes, knives, saws, and shovels into one tool. It is easily distinguished by the large swell at the end of the blade to facilitate chopping. The \"taiga\" is used by military air and special forces, including the \"Spetsnaz\".\n\nThe modern machete is very similar to some forms of the medieval falchion, a short sword popular from the 13th century onwards. The cutting edge of the falchion is curved, widening toward the point, and has a straight, unsharpened back edge. The machete differs from the falchion mainly in the lack of a guard and a simpler hilt, though some machetes do have a guard for greater hand protection during work.\n\nThe \"kopis\" is an ancient Greek weapon comparable to the machete. The \"makhaira\" is also similar, but was intended primarily to be a weapon rather than a tool.\n\nThe \"seax\" is a Germanic weapon that is also similar in function, although different in shape.\n\nThe \"kukri\" is a Nepalese curved blade used for many purposes similar to the machete.\n\nThe \"parang\" is a Malaysian knife that many machetes are based on.\n\nThe \"grosse messer\" is a large \"medieval\" knife, employed both as a tool and as a weapon.\n\nThe \"dao\" is a traditional Chinese weapon resembling the machete. It is also known as \"The General of All Weapons\". \n\nThe fascine knife is a somewhat similar tool and weapon used by European armies throughout the late 18th to early 20th centuries. The Spanish Army called its fascine knives \"machetes\". Whereas infantry were usually issued short sabres as side arms, engineers and artillerymen often received fascine knives, as besides being side arms they also served as useful tools for the construction of fortifications and other utilitarian tasks. They differ from machetes in that they generally have far thicker, tapered blades optimized for chopping European vegetation (the thin, flat blade of the machete is better for soft plants found in tropical environments), sword-like hilts and guards, and sometimes a sawback-blade. Some later models could be fixed to rifles as bayonets as well.\n\nThe katana, typically acquired through trade, was used by the Ainu people in a machete-like fashion rather than a weapon as it was originally intended to be.\n\nBoth the materials used and the shape of the machete itself are important to make a good machete. In the past, the most famous manufacturer of machetes in Latin America and the Spanish-speaking Caribbean was Collins Company of Collinsville, Connecticut. The company was founded as Collins & Company in 1826 by Samuel W. Collins to make axes. Its first machetes were sold in 1845 and became so famous that all good machetes were called \"un Collins\". In the English-speaking Caribbean, Robert Mole & Sons of Birmingham, England, was long considered the manufacturer of agricultural cutlasses of the best quality. Some Robert Mole blades survive as souvenirs of travelers to Trinidad, Jamaica, and, less commonly, St. Lucia.\n\nSince the 1950s, however, manufacturing shortcuts have resulted in a quality decline of machetes. Today, most modern factory-made machetes are of very simple construction, consisting of a blade and full-length tang punched from a single piece of flat steel plate of uniform thickness (and thus lack a primary grind), and a simple grip of two plates of wood or plastic bolted or riveted together around the tang. Finally, both sides are ground down to a rough edge so that the purchaser can sharpen the blade to their specific geometry using a file. These machetes are occasionally provided with a simple cord loop as a sort of lanyard, and a canvas scabbard—although in some regions where machetes are valuable, commonly used tools, the users may make decorative leather scabbards for them.\n\nToughness is important because of the twisting and impact forces that the relatively thin blade may encounter, while edge retention is secondary. Medium to high carbon spring steels, such as 1050 to 1095, are well suited to this application (with better machetes using the latter), and are relatively easy to sharpen. Most stainless steel machetes should be avoided, as many high-carbon stainless-steel machetes cannot stand up to repeated impacts, and will easily break if abused.\n\nIn comparison to most other knives, which are commonly heat treated to a very high degree of hardness, many machete blades are tempered to maximum toughness, often nearly spring tempered. This results in a tougher blade, more resistant to chipping and breaking, with an edge that is easier to sharpen but does not retain sharpness as well, due to its lower hardness.\n\nA properly constructed machete will have a convex or flat primary bevel from the spine to the edge, which is formed by a secondary bevel. Better machetes will also have a slight distal taper.\n\nColombia is the largest exporter of machetes worldwide.\n\nThe flag of Angola features a machete, along with a cog-wheel.\n\nThe machete is also a performance weapon used in variations of the Brazilian martial dance called \"maculelê\", often practiced by practitioners of \"capoeira\". In the Mexican state of Durango, the folkloric dance called \"Danza de los Machetes\" consists of blind-folded dancers juggling machetes and pitching them at increasing speeds between one another.\n\nTraditional forms of fencing with machetes include Colombian grima in Colombia, \"Juego del garrote\" in Venezuela, and \"tire machèt\" in Haiti.\n\n"}
{"id": "46596478", "url": "https://en.wikipedia.org/wiki?curid=46596478", "title": "Mammoth plate", "text": "Mammoth plate\n\nA mammoth plate is a photographic plate that is usually 18 x 21 inches, but may vary in size from 15 by 18 inches to 22 by 25 inches. There is no official sizing or naming production chart for these glass negatives, only historical record. Before photographic enlargers were developed, photographers used mammoth plates to make large prints that were precisely the same size as the negative from which they were made. Notably, American landscape photographer Carleton Watkins derived his detailed images of the American West, namely his views of Yosemite commissioned by the California State Geological Survey, from mammoth plates.\n\nThere is a proposed naming for very large plates under the listing \"Mega Mammoth\".\n\n"}
{"id": "47141966", "url": "https://en.wikipedia.org/wiki?curid=47141966", "title": "Manual focus override", "text": "Manual focus override\n\nIn photography, manual focus override, also known as full-time manual focus, allows manual intervention in the autofocus acquisition process simply by turning the focus ring on a photographic lens.\n\nThere are a number of technologies used to implement this feature. Pentax uses the trademark \"Quick Shift\" to refer to its implementations of manual focus override.\n"}
{"id": "58402686", "url": "https://en.wikipedia.org/wiki?curid=58402686", "title": "Michell structures", "text": "Michell structures\n\nMichell structures are structures that are optimal based on the criteria defined by A.G.M. Michell in his seminal 1904 paper. Michell states that \"“a frame (today called truss) (is optimal) attains the limit of economy of material possible in any frame-structure under the same applied forces, if the space occupied by it can be subjected to an appropriate small deformation, such that the strains in all the bars of the frame are increased by equal fractions of their lengths, not less than the fractional change of length of any element of the space.”\"\n\nThe above conclusion is based on the Maxwell load-path theorem:\n\nformula_1\n\nWhere formula_2 is the tension value in any tension element of length formula_3, formula_4 is the compression value in any compression element of length formula_5 and formula_6 is a constant value which is based on external loads applied to the structure.\n\nBased on the Maxwell load-path theorem, reducing load path of tension members formula_7 will reduce by the same value the load path of compression elementsformula_8 for a given set of external loads. Structure with minimum load path is one having minimum compliance compliance (having minimum weighted deflection in the points of applied loads weighted by the values of these loads). In consequence Michell structures are minimum compliance trusses.\n\n1. All bars of a truss are subject to a load of the same sign (tension or compression).\n\nRequired volume of material is the same for all possible cases for a given set of loads. Michell defines minimum required volume of material to be:\nformula_9\n\nWhere formula_10 is the allowable stress in the material.\n\n2. Mixed tension and compression bars\n\nMore general case are frames which consist of bars that both before and after the appropriate deformation, form curves of orthogonal systems. A two-dimensional orthogonal system remains orthogonal after stretching one series of curves and compressing the other with equal strain if and only if the inclination between any two adjacent curves of the same series is constant throughout their length. This requirement results with the perpendicular series of cures to be either:\n\na) systems of tangents and involutes or\n\nb) systems of intersecting logarithmic spirals.\n\nNote that straight line or a circle are special cases of a logarithmic spiral.\n\nMichell provided several examples of optimum frames:\n\nIn recent years a lot of studies have been done on discrete optimum trusses. In spite of Michell trusses being defined for continuum (infinite number of members) these are sometimes called Michell trusses as well. Significant contribution to the topic of discrete optimum trusses had William Prager who used the method of the circle of relative displacements to arrive with optimal topology of such trusses (typically cantilevers). To recognize Prager’s contribution discrete Michell trusses are sometimes called Prager trusses. Later geometry of cantilevered Prager trusses has been formalized by Mazurek, Baker and Tort who noticed certain geometrical relationships between members of optimal discrete trusses for 3 point or 3 force problems.\n"}
{"id": "30551191", "url": "https://en.wikipedia.org/wiki?curid=30551191", "title": "NCL Eclipse", "text": "NCL Eclipse\n\nNCL Eclipse is a plugin for Eclipse IDE aiming at supporting the development of Nested Context Language applications. NCL is the declarative standard language for ISDB-Tb (International System for Digital Broadcast Terrestrial Brazilian) and also is ITU-T standard for IPTV systems. NCL Eclipse was first developed by Laws Lab and it is currently jointly maintained by Laws and TeleMídia Labs.\n\n\"NCL Eclipse\" is free software, available at Brazilian Public Software Portal under GNU GPLv2 license.\n\nAs an Eclipse IDE plug-in NCL Eclipse can be easily integrated with others plug-ins---for instance, those supporting other ISDB-Tb and ITU-T standard languages (such as Lua and Java).\n\nThe first stable version of NCL Eclipse was named \"NCL Eclipse 1.0\". This version has included support to syntax highlighting, folding (which allows the author to hide parts of source code according with his needs), wizards to create simple documents, auto-formatting, document, marking error validation, contextual content suggestion and an outline view (which shows the document content as a tree). To provide the marking error validation, all NCL Eclipse versions uses the NCL Validator (validation framework of NCL documents). This first version was very well accepted by the community of NCL developers, which gave several feedbacks. The NCL Eclipse evolution is strongly based on these feedbacks.\n\nThe NCL Eclipse 1.1, 1.2, and 1.3 have focused mainly on fixing bugs from the previous version. The next big changes came with 1.4 version of NCL Eclipse. That version brings program visualization, media previews and hypertext navigation. Additionally, a new plug-in aimed to integrate NCL Eclipse with NCL Club was included in the same package. As pointed out before, the integration with NCL Club allows for beginners to start learning NCL based on real-world examples. The internationalization support for English, Spanish and Portuguese was also included in this version.\n\nThe latest stable, and current version, is \"NCL Eclipse 1.5\". This version contains some improvements in source code. As new feature, this version came with support to semi-automatic error correction and option to run the NCL document, provided by a virtual machine running the Ginga-NCL emulator.\n\n"}
{"id": "34283923", "url": "https://en.wikipedia.org/wiki?curid=34283923", "title": "National Authority for Remote Sensing and Space Sciences", "text": "National Authority for Remote Sensing and Space Sciences\n\nNational Authority for Remote Sensing and Space Sciences (NARSS) is the pioneering Egyptian institution in the field of satellite remote sensing.\n\nNARSS is an outgrowth of Remote Sensing Center, established in 1971 as an American-Egyptian joint project, they were affiliated with the Egyptian Academy of Scientific Research and Technology. In 1994 the authority was established as an organization under the State Ministry of Scientific Research to promote the use of state of the art space technology for the development of the country and introducing High Tech capabilities to regional planning and other applications.\n\nNARSS includes two major sectors: Remote Sensing and Space Sciences. The sector of remote sensing works on the use of data provided by earth observation satellites and various airborne sensors to produce maps and spatial data for the evaluation and monitoring of natural resources, natural hazards and management environmental. The sector of space sciences is concerned with the development of sensors for earth observation to be mounted on satellites and with all the problems involved with monitoring communication with satellites and retrieving the information for processing, and ultimately on launching an Egyptian remote sensing satellite.\n\nBoth the headquarters and the New Cairo space control station are located in Cairo. The receiving station is located in Aswan.\n\nEgyptSat 1 was a remote sensing satellite that was launched on 17 April 2007 at Baikonur Cosmodrome. Control and communication was lost July 2010.The satellite weighed 100 kg and contained a multi-spectral imager for Earth observations.\n\nEgyptSat 2 was an imaging satellite built by RSC Energia that launched on 16 April 2014 at the Baikonur Cosmodrome. The satellite was lost in April 2014 when a dual failure in the flight control system occurred.\n\n"}
{"id": "2996004", "url": "https://en.wikipedia.org/wiki?curid=2996004", "title": "Nokia Pop-Port", "text": "Nokia Pop-Port\n\nThe Pop-Port interface (originally code-named \"Tomahawk\") was a proprietary plug-in port for accessories and data synchronisation, available with many Nokia mobile phones. The port consists of one metal pin on either end, and a plastic tab containing thirteen contacts. Pop-Port-like interfaces first appeared in Nokia phones since circa 1996, but the Pop-Port was standardised as a single interface in 2002.\n\nBy early 2007 the Pop-Port was fully replaced by the industry standard USB (miniUSB, and later by microUSB) sockets for data services and a 4-part 2.5mm or 3.5mm \"standard\" TRRS socket for audio. Nokia had been equipping certain devices with one of these connectors as alternatives from about 2004.\n\nNokia filed \"Pop-Port\" as a trademark in the United States on September 3, 2002.\n\nThe port carries signals for hands-free microphone, stereo speakers, FBus Rx/Tx or USB signals for the phones supporting them, power output for feeding the accessories that do not have their own batteries, and the Accessory Control Interface (ACI), a bidirectional serial control bus for connection and authentication of phone accessories, with a specific ASIC inside accessories and a proprietary protocol. It is also used to upgrade USB-enabled phones' software using a specific USB data cable and the Nokia Software Updater. Earlier cables connected to RS-232 but later was replaced by USB.\n\nA common problem with Pop-port was that contacts often lost connection, thus resulting in drop-outs in audio (when a hands-free device is used) or an unstable data connection (when a USB cable is used). This was a common problem when listening to music from the phone while the phone was in a pocket. The more stable 2.5 mm and 3.5 mm audio sockets aren't prone to such problems.\n\nThe contacts are exposed to dust and dirt, more so than those of a TRRS socket. This, combined with the small size of the contacts, prevented connections in some cases. Also, the plug's 'hook' tended to lose its hooking capability, making it even easier to accidentally lose connection. The data cables had to be original in most cases.\n\n\n\n"}
{"id": "45389413", "url": "https://en.wikipedia.org/wiki?curid=45389413", "title": "Numerical modeling in echocardiography", "text": "Numerical modeling in echocardiography\n\nNumerical manipulation of Doppler parameters obtain during routine Echocardiography has been extensively utilized to non-invasively estimate intra-cardiac pressures, in many cases removing the need for invasive cardiac catheterization.\n\nEchocardiography uses ultrasound to create real-time anatomic images of the heart and its structures. Doppler echocardiography utilizes the Doppler principle to estimate intracardiac velocities. Via the modified Bernoulli equation, velocity is routinely converted to pressure gradient for use in clinical cardiology decision making.\n\nA broad discipline of mathematical modeling of intracardiac velocity parameters for pulmonary circulation and aortic Doppler for aortic stenosis have been investigated. Diasatolic dysfunction algorithms use complex combinations of these numeric models to estimate intra-cardiac filling pressures. Shunt defects have been studied using the Relative Atrial Index.\n\n\n"}
{"id": "190260", "url": "https://en.wikipedia.org/wiki?curid=190260", "title": "OpenDoc", "text": "OpenDoc\n\nOpenDoc is a multi-platform software componentry framework standard created by Apple for compound documents, intended as an alternative to Microsoft's Object Linking and Embedding (OLE). Active development was discontinued in March 1997.\n\nThe core idea of OpenDoc is to create small, reusable components, responsible for a specific task, such as text editing, bitmap editing, or browsing an FTP server. OpenDoc provides a framework in which these components can run together, and a document format for storing the data created by each component. These documents can then be opened on other machines, where the OpenDoc frameworks substitute suitable components for each part, even if they are from different vendors. In this way users can \"build up\" their documents from parts. Since there is no main application and the only visible interface is the document itself, the system is known as \"document centered\".\n\nAt its inception, it was envisioned that OpenDoc would allow smaller, third-party developers to enter the then-competitive office software market, able to build one good editor instead of having to provide a complete suite.\n\nOpenDoc was initially created by Apple in 1992, after Microsoft approached Apple asking for input on a proposed OLE II project. Apple had been experimenting with software components internally for some time, based on the initial work done on its Publish and Subscribe linking model and the AppleScript scripting language, which in turn was based on the HyperCard programming environment. Apple reviewed the Microsoft prototype and document and returned a list of problems they saw with the design. Microsoft and Apple, who were very competitive at the time, were unable to agree on common goals and did not work together.\n\nAt about the same time, a group of third-party developers had met at the Apple Worldwide Developers Conference (WWDC '91) and tried to hammer out a standardized document format, based conceptually on the Electronic Arts Interchange File Format (IFF). Apple became interested in this work, and soon dedicated some engineers to the task of building, or at least documenting, such a system. Initial work was published on the WWDC CDs, as well as a number of follow-up versions on later developer CDs. A component document system would only work with a known document format that all the components could use, and so soon the standardized document format was pulled into the component software effort. The format quickly changed from a simple one using tags to a very complex object oriented persistence layer called Bento.\n\nInitially the effort was codenamed \"Exemplar\", then \"Jedi\", \"Amber\", and eventually \"OpenDoc\".\n\nApple was also involved in the Taligent project during some of this period, which offered somewhat similar functionality although based on very different underlying mechanisms. While OpenDoc was still being developed, Apple confused things greatly by suggesting that it should be used by people porting existing software only, and new projects should instead be based on Taligent since that would be the next OS. Taligent was considered the future of the Mac platform, and work on other tools like MacApp were considerably deprioritized. When Taligent died in 1995, Apple was left with no major ongoing efforts along these lines, and OpenDoc became the future of Mac development.\n\nStarting in 1992, Apple had also been involved in an effort to replace MacApp development framework with a cross-platform solution known as Bedrock, from Symantec. Symantec's Think C was rapidly becoming the tool of choice for development on the Mac. Apple had been working with them to port their tools to the PowerPC when they learned of Symantec's internal porting tools. Apple proposed merging existing MacApp concepts and code with Symantec's to produce an advanced cross-platform system. Bedrock began to compete with OpenDoc as \"the\" solution for future development.\n\nAs OpenDoc gained currency within Apple, the company started to push Symantec into including OpenDoc functionality in Bedrock. Symantec was uninterested in this, and eventually gave up on the effort, passing the code to Apple. Bedrock was in a very early state of development at this point, in spite of 18 months of work, as the development team at Symantec suffered continual turnover. Apple proposed that the code would be used for OpenDoc programming, but nothing was ever heard of this again, and Bedrock disappeared.\n\nAs a result of Taligent and Bedrock both being the \"next big thing\", little effort had been expended on updating MacApp. As these two projects died, this left Apple with only OpenDoc as a modern OO-based programming system.\n\nThe development team realized in mid-1992 that an industry coalition was needed to promote the system, and created the \"Component Integration Laboratories\" (\"CI Labs\") with IBM and WordPerfect. IBM introduced the System Object Model (SOM) shared library system to the project, which became a major part of Apple's future efforts, in and out of OpenDoc. In 1996 the project was adopted by the Object Management Group, in part due to SOM's use of Common Object Request Broker Architecture (CORBA), maintained by the OMG.\n\nOpenDoc was one of Apple's earliest experiments with open standards and collaborative development methods with other companies. Apple and its partners never publicly released the source code, but did make the complete source available to developers for feedback and for testing and debugging purposes.\n\nOpenDoc was initially released to run on classic Mac OS System 7.5. From IBM’s involvement in Taligent, there was an implementation of OpenDoc in OS/2 Warp 4.\n\nThe WAV word processor was a semi-successful OpenDoc word processor from Digital Harbor; the Numbers & Charts package was a spreadsheet and 3D real-time charting solution from Adrenaline Software; and the Cyberdog web browser was created by Apple as an OpenDoc application. Lexi from Soft-Linc, Inc. was a linguistic package containing a spell checker, thesaurus and a simple translation tool which WAV and other components used. The Nisus Writer software by Nisus incorporated OpenDoc, but its implementation was hopelessly buggy. Bare Bones Software tested the waters by making its BBEdit Lite freeware text editor available as an OpenDoc editor component. RagTime, a completely integrated office package with spreadsheet, publishing and image editing was ported to OpenDoc shortly before OpenDoc was cancelled. Apple's 1996 release of ClarisWorks 5.0 (the predecessor of AppleWorks) was planned to support OpenDoc components, but this was dropped.\n\nAnother OpenDoc container application, called \"Dock'Em\", was written by MetaMind Software under a grant from the National Science Foundation and commissioned by The Center for Research in Math and Science Education, headquartered at San Diego State University. The goal was to allow multimedia content to be included in documents describing curriculum.\n\nA number of physics simulations were written by MetaMind Software and by Russian software firm Physicon (OpenTeach) as OpenDoc parts. Physics curricula for high school and middle school used them as their focus. With the demise of OpenDoc, the simulations were rewritten as Java applets and are still available from the Center under the title of \"The Constructing Physics Understanding (CPU) Project\" by Dr. Fred Goldberg.\n\nComponents of the E-Slate educational microworlds' platform were originally implemented as OpenDoc parts in C++ on both MacOS and Windows, reimplemented later on (after the demise of OpenDoc) as Java applets and eventually as JavaBeans.\n\nOpenDoc's flexibility came at a cost. OpenDoc components were invariably large and slow. For instance, opening a simple text editor part would often require 2 megabytes of RAM or more, whereas the same editor written as a standalone application could be as small as 32 KB. This initial overhead became less important as the number of documents open increased, since the basic cost was for shared libraries which implemented the system, but it was large compared to entry level machines of the day. Many developers felt that the extra overhead was too large, and since the operating system did not include OpenDoc capability, the memory footprint of their OpenDoc based applications appeared unacceptably large. In absolute terms, the one-time library overhead was approximately 1 megabyte of RAM, which at the time was nearly half of a low-end desktop computer's entire RAM complement.\n\nAnother issue was that OpenDoc had little in common with most \"real world\" document formats, and so OpenDoc documents could really only be used by other OpenDoc machines. Although one would expect some effort to allow the system to export to other formats, this was often impractical because each component held its own data. For instance, it took significant effort for the system to be able to turn a text file with some pictures into a Microsoft Word document, both because the text editor had no idea what was in the embedded objects, and because the proprietary Microsoft format was undocumented and required reverse engineering.\n\nAnother problem was the fact that each part saved its data within Bento (the former name of an OpenDoc compound document file format) in its own internal binary format, and it was very common to find one component could not open a document created by another, even though the internal data represented similar objects (spreadsheet data for instance). OpenDoc attempted to solve this problem by allowing developers to store multiple formats to represent the same document object. For instance, it was both possible and encouraged to store a common format like JPEG along with editable binary format, but in practice few developers followed this recommendation. This problem was not unique to OpenDoc, and in fact was also experienced by the Microsoft equivalent, Object Linking and Embedding (OLE). Indeed, many years later, XML documents which attempt to perform embedding of other XML formats also encounter similar issues.\n\nIt also appears that OpenDoc was a victim of an oversold concept, that of compound documents. Only a few specific examples are common, for instance most word processors and page layout programs include the ability to include graphics, and spreadsheets are expected to handle charts.\n\nBut certainly the biggest problem with the project was that it was part of a very acrimonious competition between OpenDoc consortium members and Microsoft. The members of the OpenDoc alliance were all trying to obtain traction in a market rapidly being dominated by Microsoft Office. As the various partners all piled in their own pet technologies in hopes of making it an industry standard, OpenDoc grew increasingly unwieldy. At the same time, Microsoft used the synergy between the OS and applications divisions of the company to make it effectively mandatory that developers adopt the competing OLE technology. In order to obtain a Windows 95 compliance logo from Microsoft, one had to meet certain interoperability tests which were quite difficult to meet without adoption of OLE technology, even though the technology was largely only useful in integrating with Microsoft Office. OpenDoc was forced to create an interoperability layer in order to allow developers to even consider adoption, and this added a great technical burden to the project.\n\nOpenDoc had several hundred developers signed up but the timing was poor. Apple was rapidly losing money at the time and many in the industry press expected the company to fail.\n\nOpenDoc was soon discontinued, with Steve Jobs (who had been at NeXT during this development) noting that they \"put a bullet through [OpenDoc's] head\", and most of the Apple Advanced Technology Group was laid off in a big reduction in force in March 1997. Other sources noted that Microsoft hired away three ClarisWorks developers who were responsible for OpenDoc integration into ClarisWorks.\n\nAppleShare IP Manager from versions 5.0 to 6.2 relied on OpenDoc, but AppleShare IP 6.3, the first Mac OS 9 compatible version (released in 1999), eliminated the reliance on OpenDoc. Apple officially relinquished the last trademark on the name OpenDoc on June 11, 2005.\n\n\n"}
{"id": "10296894", "url": "https://en.wikipedia.org/wiki?curid=10296894", "title": "Pedialyte", "text": "Pedialyte\n\nPedialyte is an oral electrolyte solution manufactured by Abbott Laboratories and marketed for use in children. It was invented by Dr. Gary Cohen of Swampscott, Massachusetts.\n\nPedialyte is designed to promote rehydration and electrolyte replacement in ill children. It \"meets the requirements of the American Academy of Pediatrics (AAP) Committee on Nutrition to help prevent dehydration in infants and children.\"\n\nPedialyte is lower in sugars than most sports drinks, containing 100 calories per liter compared to approximately 240 in Gatorade. It contains more sodium (1,035 milligrams per liter vs. 465 mg/L in Gatorade) and potassium (780 milligrams per liter vs. 127 mg/L in Gatorade). Pedialyte does not contain sucrose, because this sugar has the potential to make diarrhea worse by drawing water into the intestine, increasing the risk of dehydration. In its flavored formulations, Pedialyte uses the synthetic sweeteners sucralose and acesulfame potassium.\n\nPedialyte has become a hydration alternative to sports drinks for some athletes. \n\nPedialyte has become a popular drink for people suffering from hangovers, with one third of its sales coming from adults. There has been a 57% increase in its use by adults since 2012. As a result, Pedialyte has begun a marketing campaign promoting the use of Pedialyte by hungover adults.\n\nPedialyte is similar to rehydration fluids used by the World Health Organization (WHO) such as \"New Oral Rehydration Solution\" (N-ORS), that are used during the outbreak of illnesses such as cholera and rotavirus. Similar products include Lytren, NormaLyte, Gastrolyte, Ricelyte, Resol, and Drip Drop.\n\n"}
{"id": "15291863", "url": "https://en.wikipedia.org/wiki?curid=15291863", "title": "Real-time Control System Software", "text": "Real-time Control System Software\n\nThe Real-time Control System (RCS) is a software system developed by NIST based on the Real-time Control System Reference Model Architecture, that implements a generic Hierarchical control system. The RCS Software Library is an archive of free C++, Java and Ada code, scripts, tools, makefiles, and documentation developed to aid programmers of software to be used in real-time control systems (especially those using the Reference Model Architecture for Intelligent Systems Design).\n\nRCS has been used in automated manufacturing, robotics, and automated vehicle research at NIST. The software consists of a C++ library and GUI and configuration tools written a variety of software languages. The Software Library is offering the following RCS tools:\n\n\n\n\n"}
{"id": "26123", "url": "https://en.wikipedia.org/wiki?curid=26123", "title": "Real-time operating system", "text": "Real-time operating system\n\nA real-time operating system (RTOS) is an operating system (OS) intended to serve real-time applications that process data as it comes in, typically without buffer delays. Processing time requirements (including any OS delay) are measured in tenths of seconds or shorter increments of time.\nA real time system is a time bound system which has well defined fixed time constraints. Processing must be done within the defined constraints or the system will fail. \nThey either are event driven or time sharing. Event driven systems switch between tasks based on their priorities while time sharing systems switch the task based on clock interrupts. Most RTOS’s use a pre-emptive scheduling algorithm.\n\nA key characteristic of an RTOS is the level of its consistency concerning the amount of time it takes to accept and complete an application's task; the variability is \"jitter\". A \"hard\" real-time operating system has less jitter than a \"soft\" real-time operating system. The chief design goal is not high throughput, but rather a guarantee of a soft or hard performance category. An RTOS that can usually or \"generally\" meet a \"deadline\" is a soft real-time OS, but if it can meet a deadline deterministically it is a hard real-time OS.\n\nAn RTOS has an advanced algorithm for scheduling. Scheduler flexibility enables a wider, computer-system orchestration of process priorities, but a real-time OS is more frequently dedicated to a narrow set of applications. Key factors in a real-time OS are minimal interrupt latency and minimal thread switching latency; a real-time OS is valued more for how quickly or how predictably it can respond than for the amount of work it can perform in a given period of time.\n\nSee the comparison of real-time operating systems for a comprehensive list. Also, see the list of operating systems for all types of operating systems.\n\nThe most common designs are\n\nTime sharing designs switch tasks more often than strictly needed, but give smoother multitasking, giving the illusion that a process or user has sole use of a machine.\n\nEarly CPU designs needed many cycles to switch tasks during which the CPU could do nothing else useful. For example, with a 20 MHz 68000 processor (typical of the late 1980s), task switch times are roughly 20 microseconds. (In contrast, a 100 MHz ARM CPU (from 2008) switches in less than 3 microseconds.) Because of this, early OSes tried to minimize wasting CPU time by avoiding unnecessary task switching.\n\nIn typical designs, a task has three states:\n\nMost tasks are blocked or ready most of the time because generally only one task can run at a time per CPU. The number of items in the ready queue can vary greatly, depending on the number of tasks the system needs to perform and the type of scheduler that the system uses. On simpler non-preemptive but still multitasking systems, a task has to give up its time on the CPU to other tasks, which can cause the ready queue to have a greater number of overall tasks in the ready to be executed state (resource starvation).\n\nUsually the data structure of the ready list in the scheduler is designed to minimize the worst-case length of time spent in the scheduler's critical section, during which preemption is inhibited, and, in some cases, all interrupts are disabled. But the choice of data structure depends also on the maximum number of tasks that can be on the ready list.\n\nIf there are never more than a few tasks on the ready list, then a doubly linked list of ready tasks is likely optimal. If the ready list usually contains only a few tasks but occasionally contains more, then the list should be sorted by priority. That way, finding the highest priority task to run does not require iterating through the entire list. Inserting a task then requires walking the ready list until reaching either the end of the list, or a task of lower priority than that of the task being inserted.\n\nCare must be taken not to inhibit preemption during this search. Longer critical sections should be divided into small pieces. If an interrupt occurs that makes a high priority task ready during the insertion of a low priority task, that high priority task can be inserted and run immediately before the low priority task is inserted.\n\nThe critical response time, sometimes called the flyback time, is the time it takes to queue a new ready task and restore the state of the highest priority task to running. In a well-designed RTOS, readying a new task will take 3 to 20 instructions per ready-queue entry, and restoration of the highest-priority ready task will take 5 to 30 instructions.\n\nIn more advanced systems, real-time tasks share computing resources with many non-real-time tasks, and the ready list can be arbitrarily long. In such systems, a scheduler ready list implemented as a linked list would be inadequate.\n\nSome commonly used RTOS scheduling algorithms are:\n\nA multitasking operating system like Unix is poor at real-time tasks. The scheduler gives the highest priority to jobs with the lowest demand on the computer, so there is no way to ensure that a time-critical job will have access to enough resources. Multitasking systems must manage sharing data and hardware resources among multiple tasks. It is usually unsafe for two tasks to access the same specific data or hardware resource simultaneously. There are three common approaches to resolve this problem:\n\nGeneral-purpose operating systems usually do not allow user programs to mask (disable) interrupts, because the user program could control the CPU for as long as it wishes. Some modern CPUs don't allow user mode code to disable interrupts as such control is considered a key operating system resource. Many embedded systems and RTOSs, however, allow the application itself to run in kernel mode for greater system call efficiency and also to permit the application to have greater control of the operating environment without requiring OS intervention.\n\nOn single-processor systems, an application running in kernel mode and masking interrupts is the lowest overhead method to prevent simultaneous access to a shared resource. While interrupts are masked and the current task does not make a blocking OS call, the current task has \"exclusive\" use of the CPU since no other task or interrupt can take control, so the critical section is protected. When the task exits its critical section, it must unmask interrupts; pending interrupts, if any, will then execute. Temporarily masking interrupts should only be done when the longest path through the critical section is shorter than the desired maximum interrupt latency. Typically this method of protection is used only when the critical section is just a few instructions and contains no loops. This method is ideal for protecting hardware bit-mapped registers when the bits are controlled by different tasks.\n\nWhen the shared resource must be reserved without blocking all other tasks (such as waiting for Flash memory to be written), it is better to use mechanisms also available on general-purpose operating systems, such as a mutex and OS-supervised interprocess messaging. Such mechanisms involve system calls, and usually invoke the OS's dispatcher code on exit, so they typically take hundreds of CPU instructions to execute, while masking interrupts may take as few as one instruction on some processors.\n\nA (non-recursive) mutex is either locked or unlocked. When a task has locked the mutex, all other tasks must wait for the mutex to be unlocked by its \" owner\" - the original thread. A task may set a timeout on its wait for a mutex. There are several well-known problems with mutex based designs such as priority inversion and deadlocks.\n\nIn priority inversion a high priority task waits because a low priority task has a mutex, but the lower priority task is not given CPU time to finish its work. A typical solution is to have the task that owns a mutex at, or 'inherit,' the priority of the highest waiting task. But this simple approach gets more complex when there are multiple levels of waiting: task \"A\" waits for a mutex locked by task \"B\", which waits for a mutex locked by task \"C\". Handling multiple levels of inheritance causes other code to run in high priority context and thus can cause starvation of medium-priority threads.\n\nIn a deadlock, two or more tasks lock mutex without timeouts and then wait forever for the other task's mutex, creating a cyclic dependency. The simplest deadlock scenario occurs when two tasks alternately lock two mutex, but in the opposite order. Deadlock is prevented by careful design.\n\nThe other approach to resource sharing is for tasks to send messages in an organized message passing scheme. In this paradigm, the resource is managed directly by only one task. When another task wants to interrogate or manipulate the resource, it sends a message to the managing task. Although their real-time behavior is less crisp than semaphore systems, simple message-based systems avoid most protocol deadlock hazards, and are generally better-behaved than semaphore systems. However, problems like those of semaphores are possible. Priority inversion can occur when a task is working on a low-priority message and ignores a higher-priority message (or a message originating indirectly from a high priority task) in its incoming message queue. Protocol deadlocks can occur when two or more tasks wait for each other to send response messages.\n\nSince an interrupt handler blocks the highest priority task from running, and since real time operating systems are designed to keep thread latency to a minimum, interrupt handlers are typically kept as short as possible. The interrupt handler defers all interaction with the hardware if possible; typically all that is necessary is to acknowledge or disable the interrupt (so that it won't occur again when the interrupt handler returns) and notify a task that work needs to be done. This can be done by unblocking a driver task through releasing a semaphore, setting a flag or sending a message. A scheduler often provides the ability to unblock a task from interrupt handler context.\n\nAn OS maintains catalogues of objects it manages such as threads, mutexes, memory, and so on. Updates to this catalogue must be strictly controlled. For this reason it can be problematic when an interrupt handler calls an OS function while the application is in the act of also doing so. The OS function called from an interrupt handler could find the object database to be in an inconsistent state because of the application's update. There are two major approaches to deal with this problem: the unified architecture and the segmented architecture. RTOSs implementing the unified architecture solve the problem by simply disabling interrupts while the internal catalogue is updated. The downside of this is that interrupt latency increases, potentially losing interrupts. The segmented architecture does not make direct OS calls but delegates the OS related work to a separate handler. This handler runs at a higher priority than any thread but lower than the interrupt handlers. The advantage of this architecture is that it adds very few cycles to interrupt latency. As a result, OSes which implement the segmented architecture are more predictable and can deal with higher interrupt rates compared to the unified architecture.\n\nSimilarly the System Management Mode on x86 compatible Hardware can take very much time before it returns control to the operating system. It is generally wrong to write real-time software for x86 Hardware.\n\nMemory allocation is more critical in a real-time operating system than in other operating systems.\n\nFirst, for stability there cannot be memory leaks (memory that is allocated but not freed after use). The device should work indefinitely, without ever needing a reboot. For this reason, dynamic memory allocation is frowned upon. Whenever possible, all required memory allocation is specified statically at compile time.\n\nAnother reason to avoid dynamic memory allocation is memory fragmentation. With frequent allocation and releasing of small chunks of memory, a situation may occur where available memory is divided into several sections and the RTOS is incapable of allocating a large enough continuous block of memory, although there is enough free memory. Secondly, speed of allocation is important. A standard memory allocation scheme scans a linked list of indeterminate length to find a suitable free memory block, which is unacceptable in an RTOS since memory allocation has to occur within a certain amount of time.\n\nBecause mechanical disks have much longer and more unpredictable response times, swapping to disk files is not used for the same reasons as RAM allocation discussed above.\n\nThe simple fixed-size-blocks algorithm works quite well for simple embedded systems because of its low overhead.\n"}
{"id": "4136796", "url": "https://en.wikipedia.org/wiki?curid=4136796", "title": "Réseau plate", "text": "Réseau plate\n\nThe fiducial markers on a Réseau plate are sometimes called reticles. The Latin root of fiducial means \"trust, confidence\"; fiducial markers are trustworthy reference points. The Latin root of reticle means \"small meshwork bag, small net\"; a grid of reticles has a net-like appearance.\n\nIf the film negative is properly printed the fiducial markers will be evenly spaced on the positive image. Uneven spacing reveals that the image has been distorted. Identical spacing on adjacent images (i.e. from a series of aerial photos) ensures that printing magnification is consistent. Irregularities in the grid can also result from incorrect positioning of the film in the camera, physical distortion of the film media in the development process, and (if digitally scanned) the nonlinearity of the image scanner. These errors may be visually imperceptible but limit the precision of measurements. Because the position of the marks is very precisely known, the image can easily be corrected to account for these effects. When used for photogrammetry, multiple images can be combined using the fiducial marks and knowledge of the camera geometry to enable measurement of distances between objects in the images.\n\nThe Hasselblad lunar surface data camera was fitted with a Réseau plate. The Réseau plate was made of glass and was fitted to the back of the camera body, extremely close to the film plane. The plate was 5.4 × 5.4 cm in the film plane, which was the useful exposure area on the 70 mm film.\n\nThe Réseau plate was engraved with a 5 × 5 grid of crosses. The intersections of the crosses were 10 mm apart and accurately calibrated to a tolerance of 0.002 mm. Except for the double-sized central cross, each of the four arms on a cross was 1 mm long and 0.02 mm wide. The crosses (also known as fiducials) were recorded on every exposed frame and provided a means of determining angular distances between objects in the field-of-view.\n\nWhen the Hasselblad lunar surface data camera was fitted with a 60 mm lens, the images of the Réseau crosses on the film have an apparent separation of 10.3 degrees. With a 500 mm lens fitted, the apparent separation is 1.24 degrees.\n"}
{"id": "36533289", "url": "https://en.wikipedia.org/wiki?curid=36533289", "title": "Sabina Jeschke", "text": "Sabina Jeschke\n\nSabina Jeschke (born 27 July 1968 in Kungälv, Sweden) is a German university professor for information sciences in mechanical engineering at the RWTH Aachen University. As of 10 November 2017, she was named member of the management board of Deutschen Bahn AG for digitalization and technology. She is also the director of the Cybernetics Lab IMA/ZLW & IfU.\nIn the summer semester of 2017, she is on sabbatical leave to develop her research in the area of artificial consciousness (artificial / machine consciousness), and is involved in building a think tank \"Strong Artificial Intelligence\" at the Volvo Car Corporation in Göteborg. Since May 2015, Jeschke has been a member of the supervisory board of Körber AG, since April 2012 chairman of the board of VDI Aachen\n\nJeschke studied physics and computer sciences at the Technische Universität Berlin, Germany. She completed her student research project during a research stay at the NASA Ames Research Center Moffett Field, California, USA under the direction of William (Bill) Borucki. \nDuring her doctoral studies she worked as a scientific researcher at the Institut für Mathematik at the Technische Universität Berlin and as an assistant professor/instructor at the Georgia Institute of Technology in Atlanta, US.\n\nIn 2004 she obtained her doctorate degree (Dr. rer. nat.) with honors. After two years, Sabina Jeschke became a for New Media in Mathematics and Natural Sciences at the Technische Universität Berlin. As from 2007 she held a full professorship at the Institute for IT Service Technologies (IITS) and simultaneously became the director of the Central Information Technology Services (IT-Center) (RUS) at the University of Stuttgart, Germany. Additionally she was a visiting professor and head of the former Center for Multimedia in Teaching and Research (MuLF) now innoCampus at the Technische Universität Berlin.\nIn 2009 she followed the call of the RWTH Aachen University and became a professor and the director of the Institute Cluster IMA/ZLW & IfU at the Faculty of Mechanical Engineering. Until 2010 she additionally functioned as a visiting professor with scientific direction of research projects at the Institut für Technische Optik (ITO) of the University of Stuttgart, Germany. From October 2011 to September 2016, she was vice dean of the Faculty of Mechanical Engineering at the RWTH Aachen University.\n\nSome of the main areas of her research are robotics and automation technology (heterogeneous and cooperative robotics, web services in robotics), traffic and mobility (autonomous and semi-autonomous transport systems, international logistics, car2car & car2X models), Internet of Things and cyber-physical systems (industry 4.0, semantic web services), Artificial Intelligence (data integration and data mining, multi-agency systems, cognitive computing), human-machine interaction (virtual and remote laboratories, intelligent training environments), innovation research (innovation fields, trend monitoring) and information management (integration technologies for information services, innovative learning and teaching concepts).\n\nShe is a member and expert in numerous committees and commissions, inter alia Senior Member of the Institute of Electrical and Electronics Engineers (IEEE) since 2011 and a member of the Program Committee \"Robotics and Automation\" of the German Aerospace Center (DLR) since 2017. In July 2014, she was honored by the German Society of Computer Science (GI) with the recognition of Germany's digital minds. For her contributions to a modern engineering education, she was awarded the Golden Nikola Tesla Chain of the International Society for Engineering Pedagogy (IGIP) in September 2015. Together with the Carologistics team of RWTH Aachen University and the University of Applied Sciences of Aachen she won the World Championship title in the RoboCup Logistics League 2014, 2015 and 2016 for three years in a row.\n\n\n"}
{"id": "22318440", "url": "https://en.wikipedia.org/wiki?curid=22318440", "title": "Sex machine", "text": "Sex machine\n\nA sex machine, also known as a fucking machine, is a mechanical device used to simulate human sexual intercourse or other sexual activity.\n\nDevices can be penetrative or extractive. A typical penetrative machine works by the transfer of rotational or reciprocating force from a motor to a directional motion on a shaft, which is tipped by a dildo. A hand-held modified reciprocating saw device is sometimes called a fucksaw, a hand-held modified drill motor rotating device is sometimes called a drilldo, and a modified jigsaw is called a jillsaw. An extractive device works like a milking machine and can be attached to the penis, breast, or other body part.\n\nThe vibrator was originally invented for the treatment of hysteria in Victorian women through medical orgasm induced by clitoral massage. These early mechanical devices were much larger and more powerful than the modern vibrators and were first used by physicians and became popular in bath houses in Europe and the US towards the beginning of the 20th century. More compact, electrically powered versions later briefly appeared as health aids in department store catalogs.\n\nModern automated erotic stimulation devices differ from vibrators because they penetrate as well as throb. These devices are sometimes used as part of auto-erotic or partnered bondage play. Teledildonics combine use of various sex machines and a web interface, used remotely by a partner. Modern sex machines on the market include vacuum pumps, instruments that deliver calibrated electrical shocks to the nipples and genitals, and life size inflatable male and female dolls with penetrable and vibrating orifices.\n\nIn 2009, a woman from Maryland required a medevac after the blade of a homemade sex-machine cut through the plastic dildo and caused severe vaginal injuries.\n\nIn the 2008 film \"Burn After Reading\", US Treasury agent Harry Pfarrer builds a pedal-powered \"dildo chair\". The pornographic website Fucking Machines has been featured in the mainstream press as a source of information and depictions of uses.\n\nIn 2011, J. Michael Bailey provided a forum for a live demonstration of a sex-machine device to his class at Northwestern University, which led to international press coverage, questions about appropriate college coursework, and questions about academic freedom vis-a-vis tenure.\n\n"}
{"id": "51584665", "url": "https://en.wikipedia.org/wiki?curid=51584665", "title": "Silterra Malaysia", "text": "Silterra Malaysia\n\nSilterra Malaysia Sdn. Bhd. is a Malaysian semiconductor manufacturer founded in November 1995. Silterra Malaysia Sdn. Bhd. was formerly known as Wafer Technology (Malaysia) Sdn. Bhd. and changed its name to Silterra Malaysia Sdn. Bhd. in December 1999. The company was founded in 1995 and is based in Kulim, Malaysia with sales and marketing offices in San Jose, California; and Hsinchu, Taiwan. Silterra is started as a project of strategic national interest to promote front-end semiconductor manufacturing and a catalyst for high technology investments in Malaysia. Since its inception, Silterra has served many top-tier global fabless design and product companies covering the consumer electronics, communications & computing, and mobile device market segments.\nThe company was owned by Khazanah Nasional. Najib says government open to selling Silterra Malaysia at Kedah stake to foreign investors.\n\n"}
{"id": "44447573", "url": "https://en.wikipedia.org/wiki?curid=44447573", "title": "Sustainable Land Use Forum", "text": "Sustainable Land Use Forum\n\nThe Sustainable Land Use Forum (SLUF) is a local Ethiopian non-governmental organization established in 1995 in Addis Ababa. It is a network of like-minded organizations under the name “NOVIB Partners Forum on Sustainable Land Use in Ethiopia and Eritrea”. The organisation was established due to data received from a 1994 study exposing severe land degradation and unsustainable population dynamics in Ethiopia and Eritrea. Partnerships with Eritrean organizations were discontinued due to the war between the two countries in 1998. \n\nNOVIB has been gradually expanding the scope, geographic locations and diversity of its programs in Ethiopia, paving the way for SLUF in February 2002 and renamed as Sustainable Land Use Forum. In accordance with the Charities and Societies Proclamation No. 621/2009, SLUF was re-registered as an Ethiopian Residents Charities Consortium with certificate no. 0523 on November 2009.\n\nSLUF is a membership organization where membership is open to all organizations engaged in Sustainable Land Use and Natural Resources Management. As of 2014 it had 29 members consisting of 3 international and 26 local NGOs. One of the international NGOs is an Honorary Member.\n"}
{"id": "22824759", "url": "https://en.wikipedia.org/wiki?curid=22824759", "title": "The Middle (TV series)", "text": "The Middle (TV series)\n\nThe Middle is an American sitcom about a lower middle class family living in Indiana facing the day-to-day struggles of home life, work, and raising children. The series premiered on September 30, 2009, on the ABC network and concluded on May 22, 2018. The series features \"Everybody Loves Raymond\" actress Patricia Heaton and \"Scrubs\" actor Neil Flynn. \"The Middle\" was created by former \"Roseanne\" and \"Murphy Brown\" writers Eileen Heisler and DeAnn Heline of Blackie and Blondie Productions. The show is produced by Warner Bros. Television and Blackie and Blondie Productions. \"The Middle\" was praised by television critics and earned numerous award nominations.\n\nA spin-off of the series centered around Eden Sher's character Sue Heck was set to launch in 2019. However, the pilot was passed on by ABC, but is being shopped to other networks.\n\nThe series features Frances \"Frankie\" Heck (Patricia Heaton), a middle class, middle-aged, Midwestern woman and her husband Mike (Neil Flynn), who reside in the small fictional town of Orson, Indiana based on the real town of Jasper, Indiana. They are the parents of three children, Axl (Charlie McDermott), Sue (Eden Sher), and Brick (Atticus Shaffer).\n\nThe series is narrated by Frankie, initially an under-performing salesperson at a used-car dealership and later a dental assistant. Her stoic husband Mike manages a local quarry and serves as a stabilizing influence in the family, though Frankie complains about his lack of affection at times. The kids are quite different from one another: oldest son Axl, a popular but lazy teenager, does well in sports but not in academics; daughter Sue is an enthusiastic young teen but chronically unsuccessful and socially awkward; and youngest son Brick is an intelligent but introverted compulsive reader with odd behavioral traits loosely hinted to derive from Asperger syndrome.\n\n\nThe series was originally developed in the 2006–07 development cycle for ABC and was to star Ricki Lake as Frankie. Atticus Shaffer was the only actor to retain his role when the show was re-developed. ABC later ordered a second, cast-contingent pilot order tied to Patricia Heaton being cast in the leading role for the 2008-09 development cycle. The series was created by Eileen Heisler and DeAnn Heline (who is from Muncie, Indiana) and the pilot was directed by Julie Anne Robinson.\n\nThe show was originally set to take place in Jasper, Indiana, though the setting was changed to the fictional Orson, Indiana on the advice of attorneys. However, Orson is based on, and presumed to be located near Jasper. The show was filmed in Stage 31 at the Warner Bros. Ranch, with the house's exterior on the ranch's Blondie Street. Set director Julie Fanton shops at traditionally mid-western places, such as Target and Kohl's, so the show appears to have a realistic middle-class look.\n\nThe series was picked up for a full season of 24 episodes after airing just two episodes. On January 12, 2010, ABC Entertainment President Steve McPherson announced that he was renewing \"The Middle\" for a second season. The show was renewed for a third season. The third season premiered with a one-hour episode on September 21, 2011. On May 10, 2012, ABC renewed the show for a fourth season, which premiered with a one-hour special on September 26, 2012. The show was renewed for a fifth season on May 10, 2013. ABC confirmed on May 9, 2014 that the series was picked up for a sixth season of 22 episodes, and officially ordered an additional two episodes in October of that year, bringing the season six total to 24.\n\nOn May 8, 2015, ABC officially picked up the series for a seventh season, renewing the contracts of the main cast at the same time. ABC renewed the series for season eight with a 22-episode order, later expanded to 23 episodes in December 2016.\n\nThe series was renewed for a ninth season on January 25, 2017, with filming beginning on August 15, 2017. On August 2, 2017, it was announced that the series would end after its ninth season, at the request of the series' creators. The one-hour series finale aired on May 22, 2018.\n\n\"The Middle\" premiered in the U.S. on September 30, 2009, on ABC. The series aired on City (formerly Citytv) in Canada from the third to ninth season. Previously, the show aired on A (now CTV Two) during its first season. In Australia, the show premiered on December 7, 2009, on Nine Network. The New Zealand premiere was on May 8, 2010, on TV2. In India, the show premiered on January 5, 2015 on Romedy Now. In the UK, it premiered on August 29, 2010, on Sky1. Season 5 premiered on Comedy Central, UK on January 21, 2014, and in Ireland on April 16, 2014, on TV3. It has been adapted in Hindi on Reliance Broadcast Network comedy channel BIG MAGIC as \"Tedi Medi Family\". It is also broadcast on the Neox Channel in Spain and Warner Channel in Latin America and Brazil.\n\nOn March 6, 2012, it was announced that ABC Family (now Freeform) obtained the rights to \"The Middle\", which began airing the series on September 9, 2013.\n\nHallmark Channel also acquired \"The Middle\" for syndication, which began airing in March 2014. However, the series left the network in August 2018. In addition, the series debuted in local syndication on September 16, 2013. As of September 2017, the show is no longer available for local syndication.\n\nIn region 1, seasons 1–4 have had an official retail release. Starting with season 5, the series has been released as a MOD DVD-R via the Warner Archive Collection. The Canadian releases continue to be traditionally manufactured and sold, but are otherwise identical to their American counterparts. Distribution for regions 2 and 4 ended after the fourth season.\n\n\"The Middle\" has received positive reviews from critics, citing its unique and original characters along with critics praising the show's consistent standard and its realistic portrayal of lower-middle-class families. It holds a score of 71 out of 100 on the review aggregator website Metacritic. Critics praise the show's good timing, writing, and acting, with Robert Bianco of \"USA Today\" saying, \"...This series seems to more assuredly offer a first-class version of what so many viewers say they want: a humorous, heartfelt, realistic look at middle-class, middle-America family life.\" The praise has been, so far, consistent, with \"Entertainment Weekly\"s Ken Tucker saying that, in season two, \"The Middle\" continues to be \"...a rock-solid show, the saga of a family struggling to keep their heads above the choppy economic waters...\"\n\nIn the 2009–2010 season, \"The Middle\" ranked number six on Metacritic's \"Best Reviewed New Network Show\" list. Airing behind the quickly cancelled \"Hank\" during its first season, ratings were not initially impressive, averaging fewer than 7 million viewers. At the start of the 2010/2011 season, ABC moved the show to the beginning of its prime time block (8:00 pm EDT) and ratings increased substantially, with the show usually ranking second in its time slot to CBS's \"Survivor\".\n\nIn 2016, Bob Sassone of \"Esquire\" published an article called \"\"The Middle\" Is the Best TV Show You're Not Watching\" where he expresses the dissatisfaction of the series not having received nominations for several awards and not receiving the deserved attention of the critics, saying that \"\"The Middle\" is the finest American sitcom on TV right now\". After ABC confirmed that the ninth season of The Middle would be the last, Devon Ivie of Vulture published that \"\"The Middle\" Is One of TV’s Most Underrated Gems\" saying that \"I’ll miss the midwestern comfort of The Middle tremendously\" and gave five reasons why readers should give to the show a chance to charm them.\n\nSeason 3's \"Halloween II\" was the most watched episode of the series, viewed by 10.16 million viewers.\n\nIn 2011, \"The Middle\" received a Gracie Award for Outstanding Comedy Series. The 1st Critics' Choice Television Awards nominated the series for Best Comedy Series, Patricia Heaton for Best Actress in a Comedy Series, and Eden Sher for Best Supporting Actress in a Comedy Series.\n\nOn May 30, 2018, it was reported by \"Variety\" that a spin-off was being eyed following the cancellation of \"Roseanne\". Almost two months later, on July 20, 2018, in an interview with TVLine's Michael Ausiello, Sher revealed that ABC had ordered a pilot for the potential series.<ref name=\"SueSpinoff\"/ It was noted that the potential spin-off would be set a few years after the parent series ended, and follow Sue Heck as an adult. The spin-off was officially ordered on August 13, 2018, and will follow \"the twentysomething adventures of eternal optimist Sue Heck as she leaves the small town of Orson to navigate the ups and downs of a career and young adulthood in the big city of Chicago\". As of October 5, 2018, the pilot was being filmed. The series was originally to be titled \"Sue Sue in the City\", but this decision was later reversed and the series remains untitled for the time being. On November 21, 2018, TV Line reported that the proposed spin-off would not be moving forward at ABC. The spinoff is being shopped to other networks.\n\nOn October 5, 2018, it was announced that major recurring character Brad Bottig, played by Brock Ciarlleli, had joined the cast as a series regular. A few days later on October 8, 2018, it was reported that Kimberley Crossman would join the cast as Remi, a hotel chef still recovering from a messy breakup that ended with her boyfriend driving away with their food truck and taking all of her dreams with it. It was also revealed that Sue would find herself working at the same hotel as Remi. On October 10, 2018, Finesse Mitchell joined the cast as Hudson, a bartender with a big heart who works at the same hotel as Sue. The following day, it was announced that \"Silicon Valley\"'s Chris Diamantopoulos would play Sue's “mercurial, charming and rich” boss Nick and newcomer Aaron Branch playing Otis, the hotel’s naïve but endearing bellhop.\n\n"}
{"id": "57466488", "url": "https://en.wikipedia.org/wiki?curid=57466488", "title": "Trimethylenetetrathiafulvalenedithiolate", "text": "Trimethylenetetrathiafulvalenedithiolate\n\nTrimethylenetetrathiafulvalenedithiolate (tmdt) is a ligand used in the making of metal organic electric conductors. It normally has a charge of −2. Known compounds include Ni(tmdt), Pt(tmdt), Pd(tmdt), Au(tmdt),\n\nThe Trimethylenetetrathiafulvalenedithiolate ion is based on fulvalene, but with the four atoms adjacent to the bridging double bond replace with sulfur yielding tetrathiafulvalene, at one end of the pair of rings is another 5 member ring attached by adding three carbon atoms (the trimethylene part), and the other side of the fulvalene has two sulfur atoms, that bond to the metal ion.\n\nThe gold compound has an antiferromagnetic transition at .\n\nSome of the sulfur atoms can be replaced by selenium to yield similar conducting compounds.\n"}
{"id": "15499016", "url": "https://en.wikipedia.org/wiki?curid=15499016", "title": "UsiXML", "text": "UsiXML\n\nUsiXML (USer Interface eXtensible Markup Language) is an XML-based markup language for defining user interfaces on computers.\n\nUsiXML is a specification language for user interface design. It allows the designer to describe a user interface at different levels of abstraction. In other words, you can specify a UI in terms of: functionality (task analysis), the object it manipulates, or in a more concrete way, user interface.\n\nThe UsiXML language is currently being submitted for a standardisation plan to the W3C.\n\nAnother work with the same purpose is UIML.\n\nThere are plenty of tools that can be found for UsiXML.\nThey include: a translator from UsiXML specification to Flash (FlashiXML), a tool for drawing/sketching user interfaces (SketchiXML), a tool for task analysis (idealXML).\n\n"}
{"id": "11586643", "url": "https://en.wikipedia.org/wiki?curid=11586643", "title": "Well services", "text": "Well services\n\nWell services is a department within a petroleum production company through which matters concerning existing wells are handled. Having a shared well services department for all (or at least multiple) assets operated by a company is seen as advantageous as it allows the pooling of talent, experience and resources for managing wells.\n\nThe term may sometimes be used to encompass the larger section of the industry responsible for wells including the supplier companies as well the operating company's wells department.\n\nA well is initially drilled and completed under the control of the drilling and completions department operating under the request of the asset. Once the well is completed, control is transferred to the asset's production team, who will operate the well as appropriate for their purposes. Should any issues of well integrity or any requirement for well work arise, the asset will refer the issue to the well services. During interventions, control of affected well is handed over from production to the well services crew at the well site, a practical action involving transferring control lines from the production control panel to the well services control panel.\n\nWhen well work is required, it is the responsibility of the WOE to assemble the team and arrange their dispatch to the well site. The team will consist of a well services supervisor and other operators. The well services supervisor is a dedicated worker who is sent to oversee well services operations at well sites and take responsibility for all well services personnel. At offshore sites, there will commonly be two, to cover both day shift and night shift. The other operators will usually consist of personnel from supplier companies, who are trained in the relevant field, such as wireline, coiled tubing, wellhead maintenance, etc.\n\n\n"}
{"id": "9511813", "url": "https://en.wikipedia.org/wiki?curid=9511813", "title": "Zero waste agriculture", "text": "Zero waste agriculture\n\nZero waste agriculture is a type of sustainable agriculture which optimizes use of the five natural kingdoms, i.e. plants, animals, bacteria, fungi and algae, to produce biodiverse-food, energy and nutrients in a synergistic integrated cycle of profit making processes where the waste of each process becomes the feedstock for another process.\n\nThe integration of shallow microaglal oxidisation ponds was demonstrated by Golueke & Oswald in the 1960s. The widespread global implementation of these systems can be largely credited to Prof George Lai Chan (02 March 1924 Mauritius-08 October 2016 Mauritius) from ZERI. Zero waste agriculture is now practiced in China (ecological farming), Columbia (integrated food & waste management systems) & Fiji (integrated farming systems), India (integrated biogas farming), South Africa (BEAT Coop & African Agroecological Biotechnology Initiative) and Mauritius. The Brazilian government has adopted integrated farming system as a major social technology for the uplifting of marginalized and subsistence farmers through coordination with TECPAR. \n\nZero waste agriculture combines mature ecological farming practices that delivers an integrated balance of job creation, poverty relief, food security, energy security, water conservation, climate change relief, land security & stewardship.\n\nZero waste agriculture is optimally practiced on small 1-5 ha sized family owned and managed farms and it complements traditional farming & animal husbandry as practiced in most third world communities. Zero Waste Agriculture also preserves local indigenous systems and existing agrarian cultural values and practices.\n\nZero waste agriculture presents a balance of economically, socially and ecologically benefits as it:\n\nIn sunny climates, a one hectare zero waste farm can produce over 1000 litres of oil in a year from the chlorella microalgae grown on biogas digester effluent in a 500m shallow pond. The nutritive high protein waste from the oil extraction process can be used as an animal feed.\n\n"}
